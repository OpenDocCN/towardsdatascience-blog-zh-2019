<html>
<head>
<title>Reinforcement Learning and Deep Reinforcement Learning with Tic Tac Toe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用井字游戏进行强化学习和深度强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-and-deep-reinforcement-learning-with-tic-tac-toe-588d09c41dda?source=collection_archive---------9-----------------------#2019-01-28">https://towardsdatascience.com/reinforcement-learning-and-deep-reinforcement-learning-with-tic-tac-toe-588d09c41dda?source=collection_archive---------9-----------------------#2019-01-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d6089903c8701f7fb84a20c377a5b670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mSmsjPEHeQwzw6JHwX6p1A.png"/></div></div></figure><p id="b3a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我想分享我在井字游戏中实现强化学习和深度强化学习方法的项目。</p><p id="a734" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该文章包含:</p><p id="b2a3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">1.将博弈严格定义为马尔可夫决策过程。</p><p id="cdab" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.如何实现称为 TD(0)的强化学习方法，以创建一个在游戏的每个状态下都发挥最佳行动的代理。</p><p id="e224" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">3.如何实现深度强化学习，这与第 2 节非常相似，但这里我使用了神经网络来学习价值函数(什么是价值函数将在后面定义)。</p><p id="5647" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以让我们开始…</p><h1 id="dc4c" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">井字游戏的马尔可夫决策过程</strong></h1><p id="7a08" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">为了简单起见，让我们将第一个玩家定义为玩 X 符号的玩家，将第二个玩家定义为玩 O 符号的玩家，让我们将注意力集中在 X 玩家上。</p><p id="1dbd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先要注意这不是一个对称博弈，X 有优势，因为他先玩，他最多有 5 次机会，而 O 最多有 4 次机会。</p><p id="c507" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">MDP(马尔可夫决策过程)是一个 4 元组，它包含:</p><ol class=""><li id="5378" class="lz ma iq ka b kb kc kf kg kj mb kn mc kr md kv me mf mg mh bi translated"><strong class="ka ir">状态设置</strong></li></ol><p id="9511" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当轮到玩家 X 出牌时，他的状态集包含所有的棋盘，所以在我们的例子中，状态集包含所有具有相同数量的 X 和 O 的棋盘。例如，空板处于 set 状态。此外，当游戏结束时，所有的棋盘也在状态集中，它们被称为结束状态。</p><p id="28c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将把一个状态写成一个 9 字符串，其中包含数字 1-9，从左上到右下代表棋盘上的空白点，用 X 和 O 代替玩家所在位置的数字。例如，字符串 X234O6789 表示 X 在左上角播放，O 在中间播放的状态。</p><p id="77a4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2。</strong> <strong class="ka ir">动作设定(一)</strong></p><p id="29a3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">动作集包含了玩家 X 可以玩的所有选项，也就是特定状态下的所有数字。</p><p id="8e44" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里要注意的是，玩家 X 出牌后的棋盘不是玩家 X 的状态集中的状态，因为 X 不能在这个棋盘上执行某个动作(轮到 O 了)。该动作将导致的新状态将是玩家 O 完成其回合后的棋盘。</p><p id="3bf4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 3。概率函数(P) </strong></p><p id="b581" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">概率函数，定义通过采取行动从<em class="mi"> S </em>到<em class="mi"> S </em>的转移概率<em class="mi">a-</em><em class="mi">p:s✕s✕a→【0，1】。在我们的例子中，S 取决于 O 的话轮。我们可以把它想成:X 采取一个动作，环境做出反应，在 O 的回合后返回一个新的状态。每个可能的状态都有自己的概率，而概率取决于 O 的玩家。</em></p><p id="02db" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里需要注意的是，我们的目标是找到一个函数，将一个状态映射到 X 可以采取的最佳动作。这个动作显然取决于 O 怎么玩(因为它改变了概率函数)。举例来说，如果我们和一个人类玩家或者一个同样随机的玩家进行游戏，那么这个动作可能会有所不同。</p><p id="755b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如，假设我们开始了一个新游戏(新一集)，状态是空板，我们需要执行一个动作。我们有 9 个选择。假设现在，我们选择在棋盘中间玩。现在参与人 O 有 8 个选择。如果 O 是一个同样随机的玩家，他将以同样的概率从 8 个选择中挑选一个。对于那些熟悉条件概率的人，我们可以这样写:</p><p id="80eb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="mi"> P(S =O234X6789|S =0123456789，a = 5)= P(S = 1 o 34 x 6789 | S = 0123456789，a=5) = …=1/8 </em></p><p id="23c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于人类玩家来说，在角落玩比在边缘中间玩更有可能，所以概率函数会不同。</p><p id="345c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 4。奖励(R) </strong></p><p id="13f2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">奖励:我们将它定义为 1 如果 X 赢了，-1 如果 O 赢了，0.5 如果是平局，否则 0。</p><h2 id="9052" class="mj kx iq bd ky mk ml dn lc mm mn dp lg kj mo mp lk kn mq mr lo kr ms mt ls mu bi translated">政策和价值功能</h2><p id="52f7" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">策略是从状态集映射到动作集的功能。最优策略是使每个州的未来回报最大化的策略。</p><p id="a5c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当从状态<em class="mi"> S </em>开始并遵循策略π时，策略π的价值函数是将状态<em class="mi"> S </em>映射到期望回报(未来回报的平均值)的函数。最优价值函数是最优策略的价值函数。</p><p id="e694" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的目标是学习这个最优值函数，然后根据这个函数作用于每个状态。但首先，让我们假设我们已经知道这个函数，我们如何从一个给定的状态中选择我们的最优行动？</p><p id="95bc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们假设对手，O 玩家是一个理性的玩家，在 X 的回合之后，他会采取使下一个状态的价值最小化(从而使他的价值最大化)的行动。我们将选择使所有可能的下一个状态的最小值最大化的动作。</p><p id="a562" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">比如在空板，X 有 9 个选项可以玩。每个选项可以导致 8 种不同的状态(O 有 8 个选项可以玩)。对于每个 X 移动，我们将从下一个状态计算 8 个可能的值，并记住这些值的最小值。我们将有 9 个价值，我们将决定采取最大化这 9 个价值的行动。</p><h1 id="d686" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">强化学习代理</h1><p id="64b8" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我们现在的目标是学习最优值函数，并创建一个根据该函数进行最优移动的代理。为了学习价值函数，我们将使用 TD(0)方法:</p><p id="7cd8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们将每个状态的值函数初始化为 0。对于游戏访问的每一个状态，我们都会按照更新后的规则更新前一个状态的值函数:<em class="mi">v(s)= v(s)+α(v(s ')+R-v(s))</em></p><p id="83ce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="mi"> s </em>是要更新的状态(前一状态)<em class="mi">s’</em>是当前状态，<em class="mi"> R </em>是奖励，<em class="mi"> α </em>是学习率。我们将选择我们的下一步棋作为概率为 0.8 的最优棋和概率为 0.2 的随机棋，以鼓励探索。</p><p id="0ccd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">每学习 100 集，我们将随机播放 100 集最佳动作，以检查我们的学习。</p><p id="f126" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在下图中，我们可以看到学习过程。左图对应 X 玩家，右图对应 O 玩家。开始时，我们有两个随机的玩家，由于游戏的性质，X 赢的多一点。在学习过程之后，我们可以看到每个玩家几乎总是在学习和获胜。</p><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/663d7d3375df805efdd0e9c11c364bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xm3fc47y_x6mIrR_09oRNw.png"/></div></div></figure><h1 id="af9f" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">深度强化学习代理</h1><p id="3acf" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">现在我们可以尝试用不同的方法来学习价值函数。我们不需要学习一个将每个状态映射到一个值的值函数，而是可以尝试将这个函数学习为一个神经网络，它接受一个状态并返回一个值。</p><p id="1773" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">馈送给网络的状态是长度为 9 的向量。x 标记为 1，空点标记为 0，O 标记为-1。</p><p id="efe4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该网络包含两个隐藏的密集层，第一层包含 27 个具有 Relu 激活功能的神经元，第二层包含 18 个具有 Relu 激活功能的神经元。网络的最后一层是具有 1 个输出神经元的密集层，该输出神经元具有线性激活函数。</p><p id="cad7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了训练网络，我们为每个状态计算一个目标值:<em class="mi">target = v(s)+α(v(s ')+R-v(s))</em>其中<em class="mi"> v(s) </em>和<em class="mi"> v(s') </em>是从神经网络本身计算的。每次目标计算后，执行一次随机梯度下降迭代。这就是为什么在这个井字游戏的特殊例子中，与以前的方法相比，需要更多的片段来训练网络以执行良好的结果。</p><p id="de2d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种方法被称为深度强化学习，因为我们在这里使用的是深度学习方法。</p><p id="dce5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了实现神经网络，我使用了 Keras 框架。</p><p id="d6cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下图显示了深度学习方法的学习过程:</p><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/e27270f7d5cdfc7f11130bac228ef848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*alZQ_XSF_xFsIQDpV0W3qw.png"/></div></div></figure><h1 id="a2f5" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">摘要</h1><p id="5082" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">在这篇文章中，我们看到了两种创建完美井字游戏玩家的方法。第一种方法使用直接的方法——在每个状态学习状态的值。这种方式需要多次访问每个状态，以便了解完整的值函数。在这种情况下是可能的，因为游戏没有很多状态。</p><p id="60e6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第二种方式是一种更通用的方式:训练一个将状态映射到值的神经网络。神经网络学习闭合状态(如旋转、镜像等。)有相同的价值观。这意味着我们不需要访问所有国家。然而，因为我们使用随机梯度下降，我们需要播放更多的剧集，以便学习一个完美的神经网络。</p><p id="ad1e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">原代码可以在:<a class="ae na" href="https://github.com/giladariel/TicTacToe_RL.git" rel="noopener ugc nofollow" target="_blank">https://github.com/giladariel/TicTacToe_RL.git</a>找到</p><p id="7fc7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你现在可以自己和代理人比赛，看看你是否能赢。</p></div></div>    
</body>
</html>