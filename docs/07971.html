<html>
<head>
<title>This Is How Reinforcement Learning Works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这就是强化学习的工作原理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/this-is-how-reinforcement-learning-works-5080b3a335d6?source=collection_archive---------10-----------------------#2019-11-03">https://towardsdatascience.com/this-is-how-reinforcement-learning-works-5080b3a335d6?source=collection_archive---------10-----------------------#2019-11-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ea71" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">(什么会让你建立你的第一个人工智能)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3fdb5cc46ad9c712e89af088b5c23f0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eJ6IBLzHNx5HwVRs.jpg"/></div></div></figure><p id="af72" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">2017 年底，谷歌推出了<a class="ae lq" href="https://www.theguardian.com/technology/2017/dec/07/alphazero-google-deepmind-ai-beats-champion-program-teaching-itself-to-play-four-hours" rel="noopener ugc nofollow" target="_blank"> AlphaZero </a>，这是一个人工智能系统，它从头开始自学如何在四个小时内掌握国际象棋、围棋和日本象棋。</p><p id="fc5d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">短时间的训练足以让 AlphaZero 击败世界冠军国际象棋程序。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/2e5cf2a6533ca870354b5e54205416b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*VSiH_FG_2HXcHTDRM7pg1Q.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">(Andriy Popov / Alamy Stock Photo)</figcaption></figure><p id="8c4c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最近，OpenAI 证明了强化学习不仅仅是虚拟任务的工具。<a class="ae lq" href="https://openai.com/blog/solving-rubiks-cube/" rel="noopener ugc nofollow" target="_blank"> Dactyl </a>，它的拟人机器人手已经学会自己解魔方。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/58c00bae85d4f62b5baef07a049edeb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1BotCfxyKyqVlLaM.gif"/></div></div></figure><p id="f8e6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">谷歌 AlphaZero 和 OpenAI Dactyl 是<strong class="kw iu">强化学习</strong>算法，除了游戏规则之外没有任何领域知识。一些人工智能专家认为，这种方法是实现人类或超人人工智能的最可行的策略(AGI)。</p><p id="3a3b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们之前的文章中，我们介绍了强化学习的构建模块。</p><div class="lx ly gp gr lz ma"><a rel="noopener follow" target="_blank" href="/dont-ever-ignore-reinforcement-learning-again-4d026ee81371"><div class="mb ab fo"><div class="mc ab md cl cj me"><h2 class="bd iu gy z fp mf fr fs mg fu fw is bi translated">不要再忽视强化学习了</h2><div class="mh l"><h3 class="bd b gy z fp mf fr fs mg fu fw dk translated">监督或无监督学习并不代表一切。每个人都知道。开始使用 OpenAI 健身房。</h3></div><div class="mi l"><p class="bd b dl z fp mf fr fs mg fu fw dk translated">towardsdatascience.com</p></div></div><div class="mj l"><div class="mk l ml mm mn mj mo ks ma"/></div></div></a></div><p id="a10f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将深入探讨人工智能代理使用的机制，以教会他们自己如何采取正确的行动流程来实现一个全球回报的目标。</p><h1 id="9b38" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">什么是策略？</h1><p id="4f98" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">让我们考虑一下<a class="ae lq" href="https://gym.openai.com/envs/FrozenLake-v0/" rel="noopener ugc nofollow" target="_blank"> OpenAI 冰冻湖</a>，一个简单的环境，其中代理控制一个角色在网格世界中的移动。网格的一些瓦片是可行走的，其他的导致代理人掉进水里。代理人因找到一条通往目标方块的可行走路径而获得奖励。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1997cb03094b04a4b471b195e9e5ee01.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/0*J7zgU5BiKxd0Tj-b.png"/></div></figure><p id="48f4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">即使对于这样相当简单的环境，我们也可以有各种各样的策略。例如，代理可以总是向前移动，或者随机选择一个动作，或者通过检查先前的向前动作是否失败来尝试绕过障碍，或者甚至有趣地旋转来娱乐。</p><p id="5fb3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">策略的直观定义是，它是一些控制代理行为的规则集。不同的政策可以给我们不同的回报，这使得找到一个好的政策变得很重要。</p><p id="ae1e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">形式上，策略被定义为每个可能状态的动作的概率分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/dba48ad9a8664746aca54d64f3da998e.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*Gq_k4IZfzs1lysQZS1WgWw.png"/></div></figure><p id="8fe5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最优策略𝛑*是使期望值函数<em class="no"> V </em>最大化的策略:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/4ea95dcac34989b1a5c43e5a4f43f027.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*mps9llXm_RHyQpLShVOpbw.png"/></div></figure><p id="49a4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">价值函数</strong> <em class="no"> V(s) </em>是状态<em class="no"> s </em>贴现后的预期长期回报，与短期回报相对。价值函数表示代理所处的状态有多好。它等于代理人从该状态开始的预期总报酬。换句话说，在状态<em class="no"> s </em>中采取行动<em class="no"> a </em>的一步奖励的总奖励通过<em class="no">v(𝑠</em>定义。</p><h1 id="0d43" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">如何选择最佳动作？</h1><p id="b136" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">值函数取决于代理选择要执行的操作的策略。学习最优策略需要我们使用所谓的<strong class="kw iu">贝尔曼方程</strong>。</p><p id="4372" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们通过考虑下面的例子来直观地介绍一下贝尔曼方程。代理可以执行动作<em class="no"> 1、2、… </em>或<em class="no"> N </em>。这将把代理带到一个未来状态<em class="no"> S1、S2、… </em>或<em class="no"> SN </em>。代理将相应地获得奖励<em class="no"> r1、r2、… </em>或<em class="no"> rN </em>。每个未来状态的预期长期回报将是<em class="no"> V1、V2、… </em>或<em class="no"> VN </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/60bee8b571df49f5bbd2687202d68a51.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*eSb4SCZw1RVep5yuBADM2g.png"/></div></figure><p id="94bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果代理在处于状态<em class="no"> S0 </em>时采取动作<em class="no"> a=i </em>，则状态<em class="no"> S0 </em>的预期长期回报或值由下面的等式给出，其中γ是常数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0073f5043f05afb1d9aabe8c7042687e.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*0d7wwT6MJ-HDcyQlm7pQEQ.png"/></div></figure><p id="2ef6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最优策略将帮助代理选择最佳的可能<br/>动作。为此，代理需要计算每个可能动作的结果值<em class="no"> a=1，2，…，N </em>，并选择最大可能结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b18d0589e0ca4e5e41fad2f376971359.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*mK9GehomsOf5gYJVgYZp_g.png"/></div></figure><p id="9576" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以上方程称为<strong class="kw iu">确定性贝尔曼方程</strong>。如果对于一个给定的动作，主体可以以不同的概率达到一个以上的未来状态，那么它可以成为一个随机方程。这种情况如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/5874bb7d7193aeec116e3bf3da5140b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*cqQeFHMBN7b8xK05ear5fQ.png"/></div></figure><p id="a7e8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这种一般情况下产生的<strong class="kw iu">随机贝尔曼方程</strong>如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/7ba9e1a1f9d5c9e26610d48e14e0ee49.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*JJhNXspp0TJ4cysGCDgo-Q.png"/></div></figure><p id="3dd0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们提供了一个贝尔曼方程的实现，以选择给定状态下的最佳可能动作。您需要加载必要的必备库，如我们之前的文章所述。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="b50b" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">AI 智能体如何通过值迭代学习？</h1><p id="3259" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">在上一节中，我们解释了如何找到提供最大长期价值的最佳行动。如果我们能对所有的状态都这样做，我们将获得价值函数。我们还将知道在每种状态下应该执行什么操作(最优策略)。这个算法叫做<strong class="kw iu">值迭代</strong>。</p><p id="e463" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="no">值迭代</em>算法随机选择一个初始值函数。然后，它在迭代过程中计算新的改进的价值函数，直到它达到最佳价值函数。最后，它从最优值函数中导出最优策略。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/c44e0d53e3b05bd8eac517bcd2b8d244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*0xv18qqputfX8QfsHoFnJg.png"/></div></figure><p id="7a05" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在 4x4 的冰湖环境中，<em class="no">值迭代</em>算法在所有 16 个状态和 4 个可能的动作上循环，以探索给定动作的奖励，并计算最大可能的动作/奖励，并将其存储在向量 V[s]中。该算法迭代，直到 V[s]不再显著改善。</p><p id="0bfb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最佳策略<em class="no"> P </em>则是每次都采取行动，以达到具有最高<em class="no"> V </em>值的状态。</p><p id="48e9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的函数在 4x4 冰湖环境中实现了值迭代算法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/06c9a21d050910240b20a41bd6510136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*hKDNAdZcuCPNjLRGs0h5IA.png"/></div></figure><p id="f7be" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上面得到的数组说明了价值迭代函数如何成功地计算出 16 个州的长期回报。该算法进行了 7 次迭代。</p><p id="0987" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">代表一个洞的状态(<em class="no"> H </em>)的值为 0，而代表冻结块的状态(<em class="no"> F </em>)的值较高，尤其是如果这些块处于朝向目标<em class="no"> G </em>的有希望的路径上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/abdfeeb61610cf9e591dd84c61a8e10a.png" data-original-src="https://miro.medium.com/v2/resize:fit:82/format:webp/0*BUxf0gwc0WNBmPIT.png"/></div></figure><p id="248f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的函数提供了使用热图的可视化结果。箭头显示了使用最优策略<em class="no"> P </em>提供最佳总报酬的行动流程。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/6a4482c08b797f0dbbb44516c029c8bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*Yudh4f5RMnD0ixLNhG9m4w.png"/></div></figure><p id="bfe5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面我们在一个 8×8 的冰湖环境下运行数值迭代算法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b14a088393559975ad37f5e11ce7ce19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*j16Ln0PyyZq5GpUplLaTgA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/dc61e79d51cc8c19444daf9bde6312bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*bRq3mTgCt5b-yiyuj19Qeg.png"/></div></figure><h1 id="6ace" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">AI 智能体如何通过策略迭代学习？</h1><p id="4cf9" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">在上一节中，我们介绍了价值迭代算法，并举例说明了一个代理人将如何走过一个冻结的湖洞，以达到目标。</p><p id="183a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<strong class="kw iu">策略迭代</strong>算法中，我们从随机策略而不是随机值函数开始。然后我们找到该政策的价值函数。接下来，我们基于先前的值函数找到新的(改进的)策略。经过多次迭代，这将产生一个最优策略。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/97ca4c24a1c7644b6cbc7280119d87eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-WB8lFt0XYa-qKnJGxGuw.png"/></div></div></figure><p id="f7bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的函数实现了策略迭代算法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f30b2af383832af24d609cad9807213a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*J_GY15yh-kcS8lAEvpyWog.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/b4081b4ca1232a7e6f6ea8c0eb6df7e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*u37ryn1kq85mJ-0H8QvEAg.png"/></div></figure><p id="c07f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们所观察到的，两种算法产生了相同的结果。虽然值迭代算法在每次迭代中不断改进值函数，直到值函数收敛，但是策略改进定理向我们保证由策略迭代算法找到的策略比原始随机策略更好。</p><p id="19d4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这两种方法实现了相同的目标，策略迭代在计算上更加高效。</p><h1 id="de39" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">如何调 AI 智能体的学习率？</h1><p id="caec" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">值和策略迭代算法都依赖于超参数γ (gamma ),它定义了值或策略更新期间的学习率。</p><p id="6791" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这里，我们尝试不同的 gamma 值，并讨论 gamma 对训练的影响。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/0b5419dae38372b8fb3b8c58a0a331e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_AsHNvgjRETH1I4_KhznHw.png"/></div></div></figure><p id="8283" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些图显示了值如何随着 gamma 的增加而增加。不同的 gamma 值(0-10)会产生不同的策略。较低的 gamma 值将更重视短期回报，而较高的 gamma 值将更重视长期收益。</p><p id="011f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最佳伽玛取决于任务的领域。在冰湖的情况下，寻求短期收益是没有意义的(例如，落入洞中导致的负面奖励实际上不如后来收到的相同惩罚有价值，在很长一段时间没有下落之后)。我们更希望尽可能地向前看。</p><h1 id="7b4b" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">结论</h1><p id="2fb0" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">在本文中，我们提供了如何实现值迭代和策略迭代算法的实践技巧，以便在强化学习中找到最佳策略。</p><p id="5c3c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Andrej Kaparthy 的这篇文章提供了关于这些技术的进一步见解。</p><p id="66c4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当代理知道关于环境模型的足够细节时，值和策略迭代算法都工作。在自动驾驶、医疗或股票交易等几个场景中，学习或提供过渡模型可能很难。在这种情况下，无模型方法更合适。我们没有在本文中涉及它们。</p><p id="f88a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Q-learning 是一种无模型学习，当代理不知道环境模型，但必须利用其与环境交互的历史通过反复试验来发现策略时使用。</p><p id="8ae4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">状态-行动-奖励-状态-行动(SARSA)是另一种算法，其中代理与环境交互并根据采取的行动更新策略。<a class="ae lq" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/" rel="noopener ugc nofollow" target="_blank">本帖</a>将给出更多关于无模型算法的有趣见解。</p><p id="cc8f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感谢阅读。</p><p id="ce4e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你还饿吗？下面查看我最全面最简单的深度学习入门。</p><div class="lx ly gp gr lz ma"><a rel="noopener follow" target="_blank" href="/why-deep-learning-works-289f17cab01a"><div class="mb ab fo"><div class="mc ab md cl cj me"><h2 class="bd iu gy z fp mf fr fs mg fu fw is bi translated">深度学习为什么有效:解决一个农民的问题</h2><div class="mh l"><h3 class="bd b gy z fp mf fr fs mg fu fw dk translated">在开始是神经元:梯度下降，反向传播，回归，自动编码器，细胞神经网络…</h3></div><div class="mi l"><p class="bd b dl z fp mf fr fs mg fu fw dk translated">towardsdatascience.com</p></div></div><div class="mj l"><div class="og l ml mm mn mj mo ks ma"/></div></div></a></div></div></div>    
</body>
</html>