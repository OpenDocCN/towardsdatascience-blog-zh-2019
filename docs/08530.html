<html>
<head>
<title>Analysis of Tweets on the Hong Kong Protest Movement 2019 with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 分析 2019 年香港抗议运动的推文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/analysis-of-tweets-on-the-hong-kong-protest-movement-2019-with-python-a331851f061?source=collection_archive---------21-----------------------#2019-11-18">https://towardsdatascience.com/analysis-of-tweets-on-the-hong-kong-protest-movement-2019-with-python-a331851f061?source=collection_archive---------21-----------------------#2019-11-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/16839da264530f34bf443bc260b28186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*W1TEmxQc8E0rRWN7"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@yulokchan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Joseph Chan</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="kd ke kf"><p id="9b11" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">免责声明:本文无意对香港当前形势作任何形式的政治或社会评论。所做的分析完全基于手头数据集的推断。</p></blockquote><p id="c459" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">最近，在完成了吴恩达教授的 Coursera 深度学习课程后，我有动力做一个关于情感分析的宠物项目，其中一个专业是序列模型。我写这篇文章是为了巩固和分享我的学习和代码。</p><p id="6d1f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">随着香港抗议运动已经发生了将近 6 个月，我突然有了一个想法，收集关于抗议的 Twitter 推文，并将其用于这个项目。例如，我不想使用 Kaggle 上容易获得的现有(可能已经清理过的)数据集。我想这是一个让我把手弄脏并学习抓取推文的过程的机会。</p><p id="48b1" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">该项目的目标是发现:</p><ol class=""><li id="8cef" class="li lj iq kj b kk kl ko kp lf lk lg ll lh lm le ln lo lp lq bi translated">微博对抗议的总体看法，特别是，这些微博对中国中央政府、香港政府和警方的立场/观点是什么</li><li id="4339" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">twitter 用户的人口统计数据</li><li id="a9c3" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">标签的流行度</li><li id="6e37" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">顶级用户和一般用户的行为</li><li id="bce9" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">每日热门推文</li></ol><p id="b63d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">本文的结构将不同于通常的教程，在通常的教程中，流程是数据清理和预处理，接着是探索性的数据分析，然后是模型训练和调优。在这里，我们希望读者首先关注数据分析和可视化。数据清理和预处理步骤将在后面介绍。您可以从这个<a class="ae kc" href="https://github.com/leowgriffin/tweets_analysis_hkprotests_2019" rel="noopener ugc nofollow" target="_blank">库</a>访问源代码。</p><h1 id="48f1" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">使用 Tweepy 抓取 Twitter 消息</h1><p id="d1ba" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">因为抓取 Twitter tweets 也不是本文的重点，所以我另外写了一篇文章详细描述抓取的过程。如果你需要一步一步的指导，点击这个<a class="ae kc" href="https://medium.com/@leowgriffin/scraping-tweets-with-tweepy-python-59413046e788?source=friends_link&amp;sk=b037451da958081d793746387bc40ab6" rel="noopener">链接</a>。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="ea18" class="lw lx iq bd ly lz ng mb mc md nh mf mg mh ni mj mk ml nj mn mo mp nk mr ms mt bi translated">探索性数据分析</h1><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/21fc2826c4c1cc8338104afe9ff04b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*o7_kKUi_5gE37WN4s7Bywg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Source: <a class="ae kc" href="https://miro.medium.com/max/810/1*p3Ste5R_iJzi5IcSmFkmtg.png" rel="noopener">https://miro.medium.com/max/810/1*p3Ste5R_iJzi5IcSmFkmtg.png</a></figcaption></figure><p id="0fad" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">让我们用常用的数据可视化库——seaborn 和 matplotlib——来探索和可视化已经处理过的推文。</p><h2 id="a88f" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">1.WordCloud——在关于抗议的推文中找到的流行词汇的快速预览</h2><p id="7f6d" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">首先，我们使用一个词云，它可以立即向我们显示在与抗议相关的推文中使用最多的词。所需的代码如下:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="7d26" class="nq lx iq od b gy oh oi l oj ok">from wordcloud import WordCloud<br/>import matplotlib.pyplot as plt</span><span id="bd84" class="nq lx iq od b gy ol oi l oj ok">def show_wordcloud(data, title = None):<br/>    wordcloud = WordCloud(<br/>        background_color = 'white',<br/>        max_words = 200,<br/>        max_font_size = 40, <br/>        scale = 3,<br/>        random_state = 42<br/>    ).generate(str(data))</span><span id="c728" class="nq lx iq od b gy ol oi l oj ok">fig = plt.figure(1, figsize = (15, 15))<br/>    plt.axis('off')<br/>    if title: <br/>        fig.suptitle(title, fontsize = 20)<br/>        fig.subplots_adjust(top = 2.3)</span><span id="d775" class="nq lx iq od b gy ol oi l oj ok">plt.imshow(wordcloud)<br/>plt.show()<br/>    <br/># print wordcloud<br/>show_wordcloud(data['cleaned_text'])</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi om"><img src="../Images/c53dab55a184398fade456a2ca4f3864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IbgGogH9fN7F7heqh97LwQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Our word cloud for the top 100 words found in the tweets</figcaption></figure><p id="a36a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们为前 100 个单词生成了一个单词云，其中一个单词越受欢迎，该单词在单词云中就越大(您可以通过更改“max_words”的值来调整该参数)。</p><blockquote class="kd ke kf"><p id="4b35" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">一些很快打动我们的词是:厌恶、警察、消防员、抗议者、催泪瓦斯、公民、失败、信任等等。总的来说，如果没有推特的背景，我们无法确定每个词，作为它自己，是否代表对政府或抗议者的负面或正面情绪。但对于我们这些一直关注社交媒体和新闻的人来说，对警察的反对声很大。</p></blockquote><h2 id="daa5" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">2.正面情绪的数量与负面情绪的数量</h2><p id="527c" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">接下来，我们看看正面和负面推文的分布情况。基于 NLTK Vader-Lexicon 库的 SentimentIntensityAnalyzer，这个分析器检查一个句子的情感，关于它是积极的、中性的还是消极的。</p><p id="41f3" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们可以用下面的方式来解释这种情绪。如果一种情绪是积极的，这可能意味着它是亲政府和/或警察的。然而，负面情绪可能意味着它是反政府和/或反警察的，是支持抗议者的。</p><p id="2546" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">分析器为每个句子返回 4 个分数，即“肯定”、“否定”、“中性”和“复合”。分数“复合”返回范围为[-1，1]的句子的总体情感。出于我们当前的目的，我们使用“复合”分数将每条推文分为 5 类，并为每类分配一个数值范围:</p><ol class=""><li id="ba99" class="li lj iq kj b kk kl ko kp lf lk lg ll lh lm le ln lo lp lq bi translated">非常正的“5”——[0.55，1.00]</li><li id="8ede" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">正' 4' — [0.10，0.55]</li><li id="5fff" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">中性' 3 '——(-0.10，0.10)</li><li id="cd4f" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">负“2”——(-0.55，-0.10)</li><li id="a8d4" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">非常负的“1”——[-1.00，-0.55]</li></ol><p id="eafa" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">注意:中性情绪的值范围更严格。</p><blockquote class="kd ke kf"><p id="a947" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">事实证明，由于抗议的性质，使用基于规则的方法分析推文的情绪是极其不准确的。每条推文的情绪可以是关于政府或抗议者的。另一方面，在诸如酒店评论的其他情况下，每个评论的情感分析是关于酒店的，而不是给出评论的酒店客人的。因此，很明显，好的情绪分数意味着对该酒店的评论是好的，而坏的情绪分数意味着对该酒店的评论是坏的。然而，在我们目前的案例研究中，一条推文的良好情绪得分可能意味着支持一方，也可能意味着反对/否定另一方。这将在下面显示出来。</p></blockquote><p id="b712" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kj ir">根据数据的“复合”分数，将数据分类:</strong></p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="d925" class="nq lx iq od b gy oh oi l oj ok"># Focus on 'compound' scores<br/># Create a new column called 'sentiment_class'<br/>sentimentclass_list = []</span><span id="fe2b" class="nq lx iq od b gy ol oi l oj ok">for i in range(0, len(data)):<br/>    <br/>    # current 'compound' score:<br/>    curr_compound = data.iloc[i,:]['compound']<br/>    <br/>    if (curr_compound &lt;= 1.0 and curr_compound &gt;= 0.55):<br/>        sentimentclass_list.append(5)<br/>    elif (curr_compound &lt; 0.55 and curr_compound &gt;= 0.10):<br/>        sentimentclass_list.append(4)<br/>    elif (curr_compound &lt; 0.10 and curr_compound &gt; -0.10):<br/>        sentimentclass_list.append(3)<br/>    elif (curr_compound &lt;= -0.10 and curr_compound &gt; -0.55):<br/>        sentimentclass_list.append(2)<br/>    elif (curr_compound &lt;= -0.55 and curr_compound &gt;= -1.00):<br/>        sentimentclass_list.append(1)</span><span id="0a4d" class="nq lx iq od b gy ol oi l oj ok"># Add the new column 'sentiment_class' to the dataframe<br/>data['sentiment_class'] = sentimentclass_list</span><span id="0777" class="nq lx iq od b gy ol oi l oj ok"># Verify if the classification assignment is correct:<br/>data.iloc[0:5, :][['compound', 'sentiment_class']]</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi on"><img src="../Images/037be6c78342898d8cfcbcf0872a0e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*ee1td9e4NOE7BsY5l8kA9w.png"/></div></figure><p id="4cb0" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们制作了一个 seaborn countplot 来显示数据集中情感类别的分布:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="43f4" class="nq lx iq od b gy oh oi l oj ok">import seaborn as sns</span><span id="1e57" class="nq lx iq od b gy ol oi l oj ok"># Distribution of sentiment_class<br/>plt.figure(figsize = (10,5))<br/>sns.set_palette('PuBuGn_d')<br/>sns.countplot(data['sentiment_class'])<br/>plt.title('Countplot of sentiment_class')<br/>plt.xlabel('sentiment_class')<br/>plt.ylabel('No. of classes')<br/>plt.show()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/f74eb8a5c6c77dab8cf95cdc1dd8d365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*vbS8E03Cu0UylVsDVCp_2g.png"/></div></figure><h2 id="d7cf" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">让我们来看看每个情感类中的一些推文:</h2><ol class=""><li id="248e" class="li lj iq kj b kk mu ko mv lf op lg oq lh or le ln lo lp lq bi translated"><strong class="kj ir"> 10 条被归类为“负面情绪”的随机推文——1 类和 2 类</strong></li></ol><pre class="nm nn no np gt oc od oe of aw og bi"><span id="8773" class="nq lx iq od b gy oh oi l oj ok"># Display full text in Jupyter notebook:<br/>pd.set_option('display.max_colwidth', -1)</span><span id="3e18" class="nq lx iq od b gy ol oi l oj ok"># Look at some examples of negative, neutral and positive tweets</span><span id="0844" class="nq lx iq od b gy ol oi l oj ok"># Filter 10 negative original tweets:<br/>print("10 random negative original tweets and their sentiment classes:")<br/>data[(data['sentiment_class'] == 1) | (data['sentiment_class'] == 2)].sample(n=10)[['text', 'sentiment_class']]</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi os"><img src="../Images/572fcd06eae06219e940dd2b5a069b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ebv6ODOmLLDl-IoUloQEcw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">10 random tweets with negative sentiments. This means that the tweets are SUPPOSED to show support towards the protestors but not seem to the Hong Kong government and/or police.</figcaption></figure><blockquote class="kd ke kf"><p id="c8db" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">很明显，这些推文是关于谴责所谓的警察暴力，争取国际支持——特别是来自美国的支持——以及报道警察反对抗议者的活动。</p></blockquote><p id="85ca" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">2.<strong class="kj ir"> 10 条被归类为“中性情绪”的随机推文——第三类</strong></p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="b7ff" class="nq lx iq od b gy oh oi l oj ok"># Filter 10 neutral original tweets:<br/>print("10 random neutral original tweets and their sentiment classes:")<br/>data[(data['sentiment_class'] == 3)].sample(n=10)[['text', 'sentiment_class']]</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ot"><img src="../Images/3c15cb8717ea743ee8c801e502b76c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxUvaxddr_yoPoyv1lioYw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">10 random tweets with neutral sentiments. This means that the tweets are SUPPOSED to show support neither towards the protestors nor the Hong Kong government and/or police.</figcaption></figure><blockquote class="kd ke kf"><p id="b27c" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">除了最后一条索引为 114113 的推文，大多数推文的立场应该是中立的。但考虑到背景，可以推断这些推文是为了支持抗议者和他们的事业。</p></blockquote><p id="fe79" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">3.<strong class="kj ir"> 20 条随机推文被归类为“积极情绪”——第 4 类和第 5 类</strong></p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="d831" class="nq lx iq od b gy oh oi l oj ok"># Filter 20 positive original tweets:<br/>print("20 random positive original tweets and their sentiment classes:")<br/>data[(data['sentiment_class'] == 4) | (data['sentiment_class'] == 5)].sample(n=20)[['text', 'sentiment_class']]</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ot"><img src="../Images/85cc4e1f4bcc071b6159df81e8ffb51e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T40A1TjVjoROapovGkU6Qw.png"/></div></div></figure><blockquote class="kd ke kf"><p id="9569" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">随机挑选了 20 条推文，但几乎所有推文的情绪都是负面的，这意味着它们反对香港政府和/或警方。快速观察发现，这些推文涵盖了以下主题:在美国通过香港民主和人权法；取消香港政客的奖学金；总体上支持香港抗议者。</p><p id="59f4" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">这支持了之前提出的论点，即在这种情况下，使用 Vader 词典库对推文进行基于规则的情感分析在识别真正的积极情感方面是不准确的，给我们留下了许多假阳性。它没有检查和考虑推文的背景。大多数“积极情绪”的推文包含更多“积极”的词语，而不是“消极”的词语，但它们实际上是对抗议者及其事业的支持，而不是对香港政府和/或中国的支持。</p></blockquote><h2 id="1a0d" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">3.标签的流行度</h2><p id="8d90" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">回想一下，这些推文是使用预定义的搜索词搜集的，其中包含与抗议活动相关的特定标签列表。此外，推文还可以包含搜索项中未定义的其他标签，只要推文包含由搜索项定义的标签。</p><p id="c481" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在这一部分，我们想找出 Twitter 用户在他们的推文中使用的最受欢迎和最不受欢迎的标签。</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="be75" class="nq lx iq od b gy oh oi l oj ok"># the column data['hashtags'] returns a list of string(s) for each tweet. Build a list of all hashtags in the dataset.</span><span id="c67f" class="nq lx iq od b gy ol oi l oj ok">hashtag_list = []</span><span id="a4bd" class="nq lx iq od b gy ol oi l oj ok">for i in range(0, len(data)):<br/>    # Obtain the current list of hashtags<br/>    curr_hashtag = data.iloc[i, :]['hashtags']<br/>    <br/>    # Extract and append the hashtags to 'hashtag_list':<br/>    for j in range(0, len(curr_hashtag)):<br/>        hashtag_list.append(curr_hashtag[j])</span></pre><p id="3d3d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">使用的 hashtags 的总数可以通过以下方式确定:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="8c2d" class="nq lx iq od b gy oh oi l oj ok"># No. of hashtags<br/>print('No. of hashtags used in {} tweets is {}'.format(len(data), len(hashtag_list)))</span></pre><blockquote class="kd ke kf"><p id="7b70" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在 233651 条推文中使用的标签数量是 287331</p></blockquote><p id="516d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们构建了一个简单的数据框架用于可视化:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="2eb6" class="nq lx iq od b gy oh oi l oj ok">df_hashtag = pd.DataFrame(<br/>    {'hashtags': hashtag_list}<br/>)</span><span id="100c" class="nq lx iq od b gy ol oi l oj ok">print(df_hashtag.head())<br/>print('Shape of df_hashtag is:', df_hashtag.shape)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/6b418423f678718e27fbf666c62d6bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*ikig02VZfSyhrA2mStzILw.png"/></div></figure><p id="fef2" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kj ir">基本视觉化:使用的前 15 个标签</strong></p><p id="bc2c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">让我们来看看用户使用的前 15 个标签</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="f163" class="nq lx iq od b gy oh oi l oj ok"># Define N to be the top number of hashtags<br/>N = 15<br/>top_hashtags = df_hashtag.groupby(['hashtags']).size().reset_index(name = 'counts').sort_values(by = 'counts', ascending = False).head(N)<br/>print(top_hashtags)</span><span id="8886" class="nq lx iq od b gy ol oi l oj ok"># seaborn countplot on the top N hashtags<br/>plt.figure(figsize=(30,8))<br/>sns.set_palette('PuBuGn_d')<br/>sns.barplot(x = 'hashtags', y = 'counts', data = top_hashtags)<br/>plt.title('Barplot of Top ' + str(N) + ' Hashtags used')<br/>plt.xlabel('Hashtags')<br/>plt.ylabel('Frequency')<br/>plt.show()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi os"><img src="../Images/9d7ed492bb780a7006403014959ffbab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JT1jpj9Ccj5xPIk7bk9mLw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">A Seaborn bar plot for the all-time top 15 hashtags used by users.</figcaption></figure><blockquote class="kd ke kf"><p id="4337" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">不出所料，15 个标签中有 14 个包含关键词“香港”和“香港”，因为用户使用它们来识别他们与香港和抗议活动有关的推文。唯一与众不同的标签是#中国。</p><p id="a9d5" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">有 6 个标签明确表示支持抗议者，并谴责香港警方的行动和行为——这些标签是:#fightforfreedom、#freehongkong、#fightforfreedom、#hkpoliceterrorism、#hkpolicestate 和#hkpolicebrutality。</p></blockquote><p id="0618" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kj ir">高级可视化:过去 7 天前 10 个标签的时间序列(不完全是时间序列…) </strong></p><p id="a3df" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们希望看到从 2019 年 11 月 3 日开始收集数据时开始的标签使用量的增长。随着时间的推移，一个或几个标签变得越来越流行了吗？让我们来看看:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="4a1b" class="nq lx iq od b gy oh oi l oj ok">from datetime import datetime</span><span id="f43f" class="nq lx iq od b gy ol oi l oj ok">ind_to_drop = []<br/>date = []</span><span id="bb7e" class="nq lx iq od b gy ol oi l oj ok"># First find out which 'tweetcreatedts' is not a string or in other weird formats<br/>for i in range(0, len(data)):<br/>    ith_date_str = data.iloc[i,:]['tweetcreatedts']<br/>    ith_match = re.search(r'\d{4}-\d{2}-\d{2}', ith_date_str)<br/>    if ith_match == None:<br/>        ind_to_drop.append(i)<br/>    else:<br/>        continue</span><span id="c87a" class="nq lx iq od b gy ol oi l oj ok"># Drop these rows using ind_to_drop<br/>data.drop(ind_to_drop, inplace = True)</span><span id="f601" class="nq lx iq od b gy ol oi l oj ok"># Create a new list of datetime date objects from the tweets:<br/>for i in range(0, len(data)):<br/>    ith_date_str = data.iloc[i, :]['tweetcreatedts']<br/>    ith_match = re.search(r'\d{4}-\d{2}-\d{2}', ith_date_str)<br/>    ith_date = datetime.strptime(ith_match.group(), '%Y-%m-%d').date()<br/>    <br/>    date.append(ith_date)<br/>    <br/># Size of list 'date'<br/>print('Len of date list: ', len(date))</span></pre><blockquote class="kd ke kf"><p id="9697" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">日期列表长度:233648</p></blockquote><pre class="nm nn no np gt oc od oe of aw og bi"><span id="90f2" class="nq lx iq od b gy oh oi l oj ok"># Append 'date' to dataframe 'data' as 'dt_date' aka 'datetime_date'<br/>data['dt_date'] = date</span><span id="53e3" class="nq lx iq od b gy ol oi l oj ok"># Check to see that we have the correct list of dates from the dataset<br/>data['dt_date'].value_counts()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/4e9a0fa12929d28c6e3198f2d682fee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*0HtOf-pSnEIFb69q9bAESg.png"/></div></figure><pre class="nm nn no np gt oc od oe of aw og bi"><span id="5864" class="nq lx iq od b gy oh oi l oj ok"># Create a new dataframe first<br/>timeseries_hashtags = pd.DataFrame(columns = ['hashtags', 'count', 'date', 'dayofnov'])</span><span id="1412" class="nq lx iq od b gy ol oi l oj ok"># Obtain a set of unique dates in 'date' list:<br/>unique_date = np.unique(date)</span></pre><p id="7e2e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们定义了一个函数，允许您选择要显示的前 N 个标签，以及最近 T 天的数据，而不是自 2019 年 11 月 3 日以来的每天数据(虽然您可以选择，但这会使图变得混乱)</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="1428" class="nq lx iq od b gy oh oi l oj ok">def visualize_top_hashtags(main_df, timeseries_df, N, T, unique_dates):<br/>    # main_df - main dataframe 'data'<br/>    # timeseries_df - a new and empty dataframe to store the top hashtags <br/>    # N - number of top hashtags to consider<br/>    # T - number of days to consider<br/>    # unique_dates - list of unique dates available in the table<br/>    <br/>    # Returns:<br/>    # timeseries_df<br/>    <br/>    # Start counter to keep track of number of days already considered<br/>    counter = 1</span><span id="2789" class="nq lx iq od b gy ol oi l oj ok"># Starting from the latest date in the list<br/>    for ith_date in reversed(unique_dates):<br/>        # Check if counter exceeds the number of days required, T:<br/>        if counter &lt;= T:<br/>            <br/>            # Filter tweets created on this date:<br/>            ith_date_df = main_df[main_df['dt_date'] == ith_date]</span><span id="3fea" class="nq lx iq od b gy ol oi l oj ok"># From this particular df, build a list of all possible hashtags:<br/>            ith_hashtag_list = []</span><span id="b96e" class="nq lx iq od b gy ol oi l oj ok">for i in range(0, len(ith_date_df)):<br/>                # Obtain the current list of hashtags:<br/>                curr_hashtag = ith_date_df.iloc[i,:]['hashtags']</span><span id="6ec3" class="nq lx iq od b gy ol oi l oj ok"># Extract and append the hashtags to 'hashtag_list':<br/>                for j in range(0, len(curr_hashtag)):<br/>                    ith_hashtag_list.append(curr_hashtag[j])</span><span id="0273" class="nq lx iq od b gy ol oi l oj ok"># Convert the list into a simple DataFrame<br/>            ith_df_hashtag = pd.DataFrame({<br/>                    'hashtags': ith_hashtag_list<br/>            })</span><span id="4ede" class="nq lx iq od b gy ol oi l oj ok"># Obtain top N hashtags:<br/>            ith_top_hashtags = ith_df_hashtag.groupby(['hashtags']).size().reset_index(name = 'count').sort_values(by = 'count', ascending = False).head(N)</span><span id="e931" class="nq lx iq od b gy ol oi l oj ok"># Add date as a column<br/>            ith_top_hashtags['date'] = ith_date<br/>            ith_top_hashtags['dayofnov'] = ith_date.day</span><span id="4ae7" class="nq lx iq od b gy ol oi l oj ok"># Finally, concat this dataframe to timeseries_hashtags<br/>            timeseries_df = pd.concat([timeseries_df, ith_top_hashtags], axis = 0)</span><span id="e9f9" class="nq lx iq od b gy ol oi l oj ok"># Increase counter by 1<br/>            counter += 1<br/>        <br/>        else: # break the for loop<br/>            break<br/>    <br/>    print('The newly created timeseries_hashtag of size {} is: '.format(timeseries_df.shape))<br/>    timeseries_df.reset_index(inplace = True, drop = True)<br/>    <br/>    # Visualization<br/>    plt.figure(figsize=(28,12))<br/>    ax = sns.barplot(x = 'hashtags', <br/>                   y = 'count',<br/>                   data = timeseries_df,<br/>                   hue = 'dayofnov')</span><span id="5392" class="nq lx iq od b gy ol oi l oj ok"># plt.xticks(np.arange(3, 6, step=1))<br/>    # Moving legend box outside of the plot<br/>    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)<br/>    # for legend text<br/>    plt.setp(ax.get_legend().get_texts(), fontsize='22')<br/>    # for legend title<br/>    plt.setp(ax.get_legend().get_title(), fontsize='32') <br/>    plt.xlabel('Top Hashtags')<br/>    plt.ylabel('Count of Hashtags')<br/>    plt.title('Top ' + str(N) + ' Hashtags per day')<br/>    sns.despine(left=True, bottom=True)<br/>    plt.xticks(rotation = 45)<br/>    plt.show()<br/>    <br/>    return timeseries_df</span></pre><p id="c58a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们终于可以制作剧情了:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="78d6" class="nq lx iq od b gy oh oi l oj ok">timeseries_hashtags = visualize_top_hashtags(main_df = data,<br/>                       timeseries_df = timeseries_hashtags,<br/>                       N = 10,<br/>                       T = 7,<br/>                       unique_dates = unique_date)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ow"><img src="../Images/8281191da94adb6cdef04a535bc18eef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u4UsmLU6pT4am3WPEEjyTA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Top 10 hashtags over the last 7 days from 16th November 2019.</figcaption></figure><p id="a697" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我认为用条形图查看每个标签的趋势比用散点图更容易，即使它会显示时间序列，因为当有这么多颜色和类别(标签)时，很难将每个点与图例相关联。</p><blockquote class="kd ke kf"><p id="a64a" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">自 2019 年 11 月 16 日以来，过去 7 天每天排名前 10 位的标签显示了该运动常用的标签-#香港、#香港抗议、#香港警察、#standwithhongkong 和#香港警察。</p><p id="8150" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">除了常见的标签，可视化功能可以揭示独特和重大的事件/事件，因为用户在谈论这些事件时会在他们的推文中使用这些标签。</p><p id="5828" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi">Throughout the period, we see sudden appearance of hashtags such as #blizzcon2019 (not shown because of different parameters N and T), #周梓樂 (represented by squares in above graph), #japanese, #antielab, #pla, and #hkhumanrightsanddemocracyact.</p></blockquote><p id="e48e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这些标签与重要的里程碑相关联:</p><ol class=""><li id="03f7" class="li lj iq kj b kk kl ko kp lf lk lg ll lh lm le ln lo lp lq bi translated"># blizzcon 2019/blizzcon 19—<a class="ae kc" href="https://www.scmp.com/tech/apps-social/article/3035987/will-blizzcon-become-latest-battleground-hong-kong-protests" rel="noopener ugc nofollow" target="_blank">https://www . scmp . com/tech/apps-social/article/3035987/will-blizzcon-begin-最新-战场-香港-抗议</a></li><li id="4067" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi">#周梓樂 — <a class="ae kc" href="https://www.bbc.com/news/world-asia-china-50343584" rel="noopener ugc nofollow" target="_blank">https://www.bbc.com/news/world-asia-china-50343584</a></li><li id="ed01" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi">#hkust (related to #周梓樂) — <a class="ae kc" href="https://www.hongkongfp.com/2019/11/08/hong-kong-students-death-prompts-fresh-anger-protests/" rel="noopener ugc nofollow" target="_blank">https://www.hongkongfp.com/2019/11/08/hong-kong-students-death-prompts-fresh-anger-protests/</a></li><li id="2c52" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">#日本人(一名日本人被误认为是中国大陆人，因此遭到抗议者攻击)——<a class="ae kc" href="https://www3.nhk.or.jp/nhkworld/en/news/20191112_36/#:~:targetText=Hong%20Kong%20media%20reported%20on,take%20pictures%20of%20protest%20activities" rel="noopener ugc nofollow" target="_blank">https://www3 . NHK . or . jp/NHK world/en/news/2019 11 12 _ 36/#:~:target text = Hong % 20 kong % 20 media % 20 reported % 20 on，take % 20pictures % 20of %抗议% 20 活动</a>。</li><li id="a94e" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">#antielab(抗议者抵制选举，因为 Joshua Wong 被禁止参加选举)——<a class="ae kc" href="https://www.scmp.com/news/hong-kong/politics/article/3035285/democracy-activist-joshua-wong-banned-running-hong-kong" rel="noopener ugc nofollow" target="_blank">https://www . scmp . com/news/hong-hong kong/politics/article/3035285/democracy-activity-Joshua-Wong-banted-running-hong-hong kong</a></li><li id="8052" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">#pla(中国人民解放军(pla)驻香港部队帮助清理和清除被抗议者封锁和破坏的街道)——<a class="ae kc" href="https://www.channelnewsasia.com/news/asia/china-s-pla-soldiers-help-clean-up-hong-kong-streets-but-12099910" rel="noopener ugc nofollow" target="_blank">https://www . channel news Asia . com/news/Asia/China-s-PLA-soldiers-help-clean-up-Hong Kong-streets-but-12099910</a></li></ol><blockquote class="kd ke kf"><p id="f155" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">除了通常的标签外，预计未来重大事件将继续引发新的标签。</p></blockquote><h2 id="7584" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">4.最受欢迎的推文</h2><p id="b5fc" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">在这一部分，我们将重点关注最受欢迎的推文。有两个指标可以帮助我们做到这一点——一条推文的转发数和收藏数。不幸的是，我们只能提取 retweet 计数，因为在从。json 格式(知道怎么做的请随时留言评论！).</p><p id="4c66" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们将执行以下操作:</p><ul class=""><li id="adf9" class="li lj iq kj b kk kl ko kp lf lk lg ll lh lm le ox lo lp lq bi translated">有史以来的前 N 条推文</li><li id="e5f0" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ox lo lp lq bi translated">特定日期的前 N 条推文</li><li id="5dc9" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ox lo lp lq bi translated">过去 T 天的前 N 条推文</li></ul><p id="261a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kj ir">史上十大推文</strong></p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="5187" class="nq lx iq od b gy oh oi l oj ok"># Convert the data type of the column to all int using pd.to_numeric()</span><span id="bd48" class="nq lx iq od b gy ol oi l oj ok">print('Current data type of "retweetcount" is:',data['retweetcount'].dtypes)</span><span id="cb0b" class="nq lx iq od b gy ol oi l oj ok">data['retweetcount'] = pd.to_numeric(arg = data['retweetcount'])</span><span id="47ec" class="nq lx iq od b gy ol oi l oj ok">print('Current data type of "retweetcount" is:',data['retweetcount'].dtypes)</span></pre><blockquote class="kd ke kf"><p id="6da8" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">“retweetcount”的当前数据类型为:object <br/>当前数据类型为:int64</p></blockquote><p id="57f8" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们定义了一个函数来提取前 10 条推文:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="4277" class="nq lx iq od b gy oh oi l oj ok">def alltime_top_tweets(df, N):<br/>    # Arguments:<br/>    # df - dataframe<br/>    # N - top N tweets based on retweetcount<br/>    <br/>    # Sort according to 'retweetcount'<br/>    top_tweets_df = df.sort_values(by = ['retweetcount'], ascending = False)<br/>    # Drop also duplicates from the list, keep only the copy with higher retweetcount<br/>    top_tweets_df.drop_duplicates(subset = 'text', keep = 'first', inplace = True)<br/>    # Keep only N rows<br/>    top_tweets_df = top_tweets_df.head(N)<br/>    <br/>    # Print out only important details <br/>    # username, tweetcreatedts, retweetcount, original text 'text'<br/>    return top_tweets_df[['username', 'tweetcreatedts', 'retweetcount', 'text']]</span><span id="42f6" class="nq lx iq od b gy ol oi l oj ok">print('All-time top 10 tweets:')<br/>print('\n')<br/>alltime_top_tweets(data, 10)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oy"><img src="../Images/c97d374e5e8a0bf81b4725df259524c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*syky69t_cxNpnIAthcoWpA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">All-time top 10 tweets between 3rd Nov to 16th Nov 2019.</figcaption></figure><p id="6832" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kj ir">某一天的前 10 条推文</strong></p><p id="f2ac" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们还可以创建另一个函数来提取某一天的前 N 条推文:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="a0cd" class="nq lx iq od b gy oh oi l oj ok">def specified_toptweets(df, spec_date, N):<br/>    # Arguments<br/>    # df - dataframe<br/>    # N - top N tweets<br/>    # date - enter particular date in str format i.e. '2019-11-02'<br/>    <br/>    # Specific date<br/>    spec_date = datetime.strptime(spec_date, '%Y-%m-%d').date()<br/>    <br/>    # Filter df by date first<br/>    date_df = df[df['dt_date'] == spec_date ]<br/>    <br/>    # Sort according to 'retweetcount'<br/>    top_tweets_date_df = date_df.sort_values(by = ['retweetcount'], ascending = False)<br/>    # Drop also duplicates from the list, keep only the copy with higher retweetcount<br/>    top_tweets_date_df.drop_duplicates(subset = 'text', keep = 'first', inplace = True)<br/>    # Keep only N rows<br/>    top_tweets_date_df = top_tweets_date_df.head(N)<br/>    <br/>    print('Top ' + str(N) + ' tweets for date ' + str(spec_date) + ' are:')<br/>    # Print out only important details <br/>    # username, tweetcreatedts, retweetcount, original text 'text'<br/>    return top_tweets_date_df[['username', 'tweetcreatedts', 'retweetcount', 'text']]</span></pre><p id="5862" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">让我们试试 2019 年 11 月 5 日:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="1756" class="nq lx iq od b gy oh oi l oj ok">specified_toptweets(data, '2019-11-05', 10)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oz"><img src="../Images/ea28881f4e9ebecc0767fc3edfa6ab15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uvjl4ZpTtZ6oakjHqZfn8g.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Top 10 tweets for 5th Nov 2019.</figcaption></figure><p id="9f47" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kj ir">过去 5 天的前两条推文</strong></p><p id="7f9d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">好了，最后，我们还可以使用以下函数提取最近 T 天的前 N 条推文:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="7f9c" class="nq lx iq od b gy oh oi l oj ok">def past_toptweets(df, T, N, unique_date):<br/>    # Arguments:<br/>    # df - dataframe 'data'<br/>    # T - last T days <br/>    # N - top N tweets<br/>    # List of all unique dates in dataset<br/>    <br/>    # Create a df to store top tweets for all T dates, in case there is a need to manipulate this df<br/>    past_toptweets_df = pd.DataFrame(columns = ['username', 'tweetcreatedts', 'retweetcount', 'text'])<br/>    print(past_toptweets_df)<br/>    <br/>    # Filter data according to last T dates first:<br/>    # Do a check that T must not be greater than the no. of elements in unique_date<br/>    if T &lt;= len(unique_date):<br/>        unique_date = unique_date[-T:] # a list<br/>    else:<br/>        raise Exception('T must be smaller than or equal to the number of dates in the dataset!')<br/>    <br/>    # Print out top N for each unique_date one after another, starting from the latest:<br/>    for ith_date in reversed(unique_date):<br/>        # Filter tweets created on this date:<br/>        ith_date_df = df[df['dt_date'] == ith_date]<br/>        <br/>        # Sort according to 'retweetcount'<br/>        top_tweets_date_df = ith_date_df.sort_values(by = ['retweetcount'], ascending = False)<br/>        # Drop also duplicates from the list, keep only the copy with higher retweetcount<br/>        top_tweets_date_df.drop_duplicates(subset = 'text', keep = 'first', inplace = True)<br/>        # Keep only N rows<br/>        top_tweets_date_df = top_tweets_date_df.head(N)<br/>        # Keep only essential columns<br/>        top_tweets_date_df = top_tweets_date_df[['username', 'tweetcreatedts', 'retweetcount', 'text']]<br/>        <br/>        # Append top_tweets_date_df to past_toptweets_df<br/>        past_toptweets_df = pd.concat([past_toptweets_df, top_tweets_date_df], axis = 0)<br/>        <br/>        # Print out the top tweets for this ith_date<br/>        print('Top ' + str(N) + ' tweets for date ' + str(ith_date) + ' are:')<br/>        # print only essential columns:<br/>        print(top_tweets_date_df)<br/>        print('\n')<br/>    <br/>    return past_toptweets_df</span><span id="51ce" class="nq lx iq od b gy ol oi l oj ok">past_toptweets(data, T = 5, N = 2, unique_date = unique_date)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pa"><img src="../Images/b64f3524bb99df44cd3ee790634817d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZErEMJgLl_7-KlojUNKlA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Top 2 tweets for the last 5 days from 12 Nov to 16 Nov 2019.</figcaption></figure><blockquote class="kd ke kf"><p id="7475" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">函数“past_toptweets”的一个缺陷是，它可以返回完全相同的推文。例如，第一天的热门推文可以在随后的几天被其他用户转发。这个函数可以选择这样的 tweet，因为还没有实现逻辑来只考虑不是从较早日期选择的 tweet。</p></blockquote><h2 id="b0d5" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">5.Twitter 用户的行为</h2><p id="3ee9" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated"><strong class="kj ir">每日推文数量</strong></p><p id="d69d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">让我们来看看每日推文数量的趋势。我们将建立另一个数据框架，用于绘制可视化。</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="8a7a" class="nq lx iq od b gy oh oi l oj ok">top_user_df = pd.DataFrame(columns = ['username', 'noTweets', 'noFollowers', 'dt_date'])</span><span id="53d7" class="nq lx iq od b gy ol oi l oj ok"># Convert datatype of 'totaltweets' to numeric<br/>pd.to_numeric(data['totaltweets'])</span><span id="118c" class="nq lx iq od b gy ol oi l oj ok">for ith_date in unique_date:<br/>    print('Current loop: ', ith_date)<br/>    <br/>    temp = data[data['dt_date'] == ith_date]<br/>    <br/>    # pd.DataFrame - count number of tweets tweeted in that day - noTweets<br/>    temp_noTweets = temp.groupby(['username']).size().reset_index(name = 'noTweets').sort_values(by = 'username', ascending = False)<br/>    <br/>    # pd.Series - count max followers - might fluctuate during the day<br/>    temp_noFollowing = temp.groupby(['username'])['followers'].max().reset_index(name = 'noFollowers').sort_values(by = 'username', ascending = False)['noFollowers']<br/>    <br/>    # *** NOT WORKING<br/>    # pd.Series - count max totaltweets - might fluctuate during the day. Note this is historical total number of tweets ever since the user is created.<br/>    # temp_noTotaltweets = temp.groupby(['username'])['totaltweets'].max().reset_index(name = 'noTotaltweets').sort_values(by = 'username', ascending = False)['noTotaltweets']<br/>    <br/>    # Concat series to temp_noTweets, which will be the main df<br/>    final = pd.concat([temp_noTweets, temp_noFollowing], axis = 1) # add as columns<br/>    final['dt_date'] = ith_date<br/>    <br/>    print(final)<br/>    <br/>    # Append 'final' dataframe to top_user_df<br/>    top_user_df = pd.concat([top_user_df, final])</span></pre><p id="2185" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">绘制可视化图:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="841d" class="nq lx iq od b gy oh oi l oj ok"># hue = retweetcount and followers, totaltweets<br/>f, axes = plt.subplots(3, 1, figsize = (22,22))<br/>sns.set_palette('PuBuGn_d')<br/>sns.stripplot(x = 'dt_date', y = 'noTweets', data = top_user_df, jitter = True, ax = axes[0], size = 6, alpha = 0.3)<br/>sns.boxplot(y = 'dt_date', x = 'noTweets', data = top_user_df, orient = 'h', showfliers=False, ax = axes[1])<br/>sns.boxplot(y = 'dt_date', x = 'noTweets', data = top_user_df, orient = 'h', showfliers=True, fliersize = 2.0, ax = axes[2])</span><span id="1f5c" class="nq lx iq od b gy ol oi l oj ok"># Axes and titles for each subplot<br/>axes[0].set_xlabel('Date')<br/>axes[0].set_ylabel('No. of Tweets')<br/>axes[0].set_title('No. of Tweets Daily')</span><span id="577e" class="nq lx iq od b gy ol oi l oj ok">axes[1].set_xlabel('No. of Tweets')<br/>axes[1].set_ylabel('Date')<br/>axes[1].set_title('No. of Tweets Daily')</span><span id="6ded" class="nq lx iq od b gy ol oi l oj ok">axes[2].set_xlabel('Date')<br/>axes[2].set_ylabel('No. of Tweets')<br/>axes[2].set_title('No. of Tweets Daily')</span><span id="211c" class="nq lx iq od b gy ol oi l oj ok">plt.show()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pb"><img src="../Images/1b482b5630e6c734ab1a7af68585b814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pLJWJQMfxj-Y6g_QKUmviQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Top: Seaborn strip plot showing the number of tweets daily; middle: Seaborn box plot showing the distribution of tweets per user daily WITHOUT outliers; bottom: Seaborn box plot showing the distribution of tweets per user daily WITH outliers.</figcaption></figure><blockquote class="kd ke kf"><p id="7636" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">从 Seaborn box 图和 strip 图中，我们看到数据集中的大多数用户一天都不会发很多微博。从带状图中，我们可能无法辨别数据集中的异常值，并且可能认为大多数用户每天发的微博数量在 1 到 30 条以上。</p><p id="73bb" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">然而，盒子情节告诉我们一个不同的故事。可视化中间的第一个方框图显示，大多数用户大约发了 1 到 8 条微博。另一方面，在可视化底部的第二个方框图中显示了许多异常值。这些用户发了很多推文，从 10 条起。在考虑的时间段内，至少有 7 个用户每天至少发 100 条以上的推文。</p></blockquote><p id="60ec" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kj ir">每日发推次数最多的前 5 名用户</strong></p><p id="6797" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">让我们通过找出谁是顶级用户来进一步放大。</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="284a" class="nq lx iq od b gy oh oi l oj ok"># To change the number of users, adjust the value in head()<br/># top_user_df.set_index(['dt_date', 'username']).sort_values(by = ['dt_date','noTweets'], ascending = False)<br/>user_most_tweets_df = top_user_df.sort_values(by = ['dt_date', 'noTweets'], ascending = False, axis = 0).groupby('dt_date').head(5)</span><span id="2a49" class="nq lx iq od b gy ol oi l oj ok"># Extract 'days' out of dt_date so we can plot a scatterplot<br/># Will return an int:<br/>user_most_tweets_df['dayofNov'] = user_most_tweets_df['dt_date'].apply(lambda x: x.day)<br/>user_most_tweets_df['noTweets'] = user_most_tweets_df['noTweets'].astype(int)</span><span id="a549" class="nq lx iq od b gy ol oi l oj ok"># Plot 2 subplots<br/># 1st subplot - show who are the users who tweeted the most<br/># 2nd subplot - trend in number of tweets<br/>f, axes = plt.subplots(2, 1, figsize = (20,20))<br/>f = sns.scatterplot(x = 'dayofNov', y = 'noTweets', hue = 'username', data = user_most_tweets_df, size = 'noFollowers', sizes = (250, 1250), alpha = 0.75, ax = axes[0])<br/>sns.lineplot(x = 'dayofNov', y = 'noTweets', data = user_most_tweets_df, markers = True)</span><span id="48dd" class="nq lx iq od b gy ol oi l oj ok"># Axes and titles for each subplot<br/># First subplot<br/>axes[0].set_xlabel('Day in Nov')<br/>axes[0].set_ylabel('No. of Tweets')<br/>axes[0].set_title('Most no. of tweets daily')</span><span id="fd8a" class="nq lx iq od b gy ol oi l oj ok"># Legends for first subplot<br/>box = f.get_position()<br/>f.set_position([box.x0, box.y0, box.width * 1.0, box.height]) # resize position</span><span id="5f9b" class="nq lx iq od b gy ol oi l oj ok"># Put a legend to the right side<br/>f.legend(loc='center right', bbox_to_anchor=(1.5, 0.5), ncol=4)</span><span id="201a" class="nq lx iq od b gy ol oi l oj ok"># Second subplot<br/>axes[1].set_xlabel('Date')<br/>axes[1].set_ylabel('No. of Tweets')<br/>axes[1].set_title('Trend of no. of tweets by top users')</span><span id="b4f0" class="nq lx iq od b gy ol oi l oj ok">plt.show()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pc"><img src="../Images/0fc19280bf0e10d43df3acdb7a021c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-l18NkxxLeyya67rlmi9Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Daily activities of top 5 users</figcaption></figure><h2 id="d47f" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">6.Twitter 用户的人口统计数据</h2><p id="0cde" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated"><strong class="kj ir">推特用户的位置</strong></p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="b317" class="nq lx iq od b gy oh oi l oj ok">location = data['location']<br/>print('No. of distinct locations listed by twitter users is:', len(location.value_counts()))<br/>unique_locations = location.value_counts()</span><span id="b389" class="nq lx iq od b gy ol oi l oj ok"># Remove n.a.<br/>unique_locations = pd.DataFrame({'locations': unique_locations.index,<br/>                                'count': unique_locations.values})<br/>unique_locations.drop(0, inplace = True)</span><span id="c1df" class="nq lx iq od b gy ol oi l oj ok"># See top few locations<br/>unique_locations.sort_values(by = 'count', ascending = False).head(10)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pd"><img src="../Images/eed8f7b0ea26a11ff19ccf3c4cb336ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FkEAO05QZmyutnYYVDlCg.png"/></div></div></figure><blockquote class="kd ke kf"><p id="2008" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">预计会看到这些用户中的许多人声称居住在香港，因为这些用户应该更接近地面。因此，他们可以通过亲眼所见迅速传播消息。</p></blockquote><p id="74da" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们将把“香港”从视觉化中剔除，并专注于其余位置的分布:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="41d1" class="nq lx iq od b gy oh oi l oj ok"># To remove 香港<br/>hk_chinese_word = unique_locations.iloc[1,0]</span><span id="a981" class="nq lx iq od b gy ol oi l oj ok"># Obtain the row index of locations that contain hong kong:<br/>ind_1 = unique_locations[unique_locations['locations'] == 'hong kong'].index.values[0]<br/>ind_2 = unique_locations[unique_locations['locations'] == 'hk'].index.values[0]<br/>ind_3 = unique_locations[unique_locations['locations'] == 'hong kong '].index.values[0]<br/>ind_4 = unique_locations[unique_locations['locations'] == 'hongkong'].index.values[0]<br/>ind_5 = unique_locations[unique_locations['locations'] == hk_chinese_word].index.values[0]<br/>ind_6 = unique_locations[unique_locations['locations'] == 'kowloon city district'].index.values[0]</span><span id="a9e9" class="nq lx iq od b gy ol oi l oj ok">list_ind = [ind_1,ind_2,ind_3,ind_4,ind_5, ind_6]</span><span id="8ef3" class="nq lx iq od b gy ol oi l oj ok"># Drop these rows from unique_locations<br/>unique_loc_temp = unique_locations.drop(list_ind)</span><span id="e47b" class="nq lx iq od b gy ol oi l oj ok"># Focus on top 20 locations first<br/># Convert any possible str to int/numeric first<br/>count = pd.to_numeric(unique_loc_temp['count'])<br/>unique_loc_temp['count'] = count<br/>unique_loc_temp = unique_loc_temp.head(20)</span><span id="0bc3" class="nq lx iq od b gy ol oi l oj ok"># Plot a bar plot<br/>plt.figure(figsize=(16,13))<br/>sns.set_palette('PuBuGn_d')<br/>sns.barplot(x = 'count', y = 'locations', orient = 'h',data = unique_loc_temp)<br/>plt.xlabel('Count')<br/>plt.ylabel('Locations')<br/>plt.title('Top 20 Locations')<br/>plt.show()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pe"><img src="../Images/93985cbaf437dc507ad45642fe9c49dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JBqO45aKTD_ERxu80n4AnA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Top 20 locations of users</figcaption></figure><blockquote class="kd ke kf"><p id="aec0" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">快速统计一下排名前 20 位的地点(不包括香港),就会发现这些地点大多来自西方国家。我们看到了预期中的国家，如美国、加拿大、英国和澳大利亚，那里的一些人和政治家也在关注抗议活动，并公开反对执政的政府和警察。</p></blockquote><p id="1731" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kj ir">拥有最多关注者的前 30 名用户</strong></p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="7f35" class="nq lx iq od b gy oh oi l oj ok"># Reuse code from top_user_df<br/># Sort according to noFollowers<br/>top_user_df = top_user_df.sort_values(by = 'noFol lowers', ascending = False)<br/>user_most_followers = top_user_df.groupby('username')['noFollowers', 'dt_date'].max().sort_values(by = 'noFollowers', ascending = False)<br/>user_most_followers['username'] = user_most_followers.index<br/>user_most_followers.reset_index(inplace = True, drop = True)</span><span id="d026" class="nq lx iq od b gy ol oi l oj ok"># plot chart<br/>plt.figure(figsize = (25, 8))<br/>sns.set_palette('PuBuGn_d')<br/>sns.barplot(x = 'noFollowers', y = 'username', orient = 'h', data = user_most_followers.head(30))<br/>plt.xlabel('No. of Followers')<br/>plt.ylabel('Usernames')<br/>plt.title('Top Twitter Accounts')<br/>plt.show()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pf"><img src="../Images/90729d2e08113b026c52241c569bd115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-n5YEU-56LluydaW7j5DAg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Seaborn bar chart for top 30 twitter accounts in terms of number of followers.</figcaption></figure><blockquote class="kd ke kf"><p id="2d36" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在排名前 30 位的账户中，大多数都属于新闻机构或媒体，如法新社、CGTNOfficial、EconomicTimes 和 ChannelNewsAsia。其余属于记者和作家等个人。约书亚·王(Joshua Wong)的账户是名单中唯一可以被确认为抗议活动一部分的账户。</p></blockquote><p id="f8a5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kj ir">大客户活动</strong></p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="bbfb" class="nq lx iq od b gy oh oi l oj ok">user_most_followers_daily = top_user_df.sort_values(by = ['dt_date', 'noFollowers'], ascending = False, axis = 0).groupby('dt_date').head(5)<br/>print(user_most_followers_daily)</span><span id="514d" class="nq lx iq od b gy ol oi l oj ok"># Extract 'days' out of dt_date so we can plot a scatterplot<br/># Will return an int:<br/>user_most_followers_daily['dayofNov'] = user_most_followers_daily['dt_date'].apply(lambda x: x.day)<br/>user_most_followers_daily['noFollowers'] = <br/>user_most_followers_daily['noFollowers'].astype(int)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pg"><img src="../Images/3438fc93392d0e06184d3a9f3636e4e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yHKrEfMCkRUnbMIUeJlLUg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Top 5 users and their tweets per day</figcaption></figure><pre class="nm nn no np gt oc od oe of aw og bi"><span id="e6d1" class="nq lx iq od b gy oh oi l oj ok">f, axes = plt.subplots(1, 1, figsize = (15,10))<br/>f = sns.scatterplot(x = 'dayofNov', y = 'noTweets', hue = 'username',data = user_most_followers_daily, size = 'noFollowers', sizes=(50, 1000))</span><span id="f1ed" class="nq lx iq od b gy ol oi l oj ok"># Axes and titles for each subplot<br/># First subplot<br/>axes.set_xlabel('Day in Nov')<br/>axes.set_ylabel('No. of Tweets')<br/>axes.set_title('Daily activity of users with most number of followers')</span><span id="cebb" class="nq lx iq od b gy ol oi l oj ok"># Legends for first subplot<br/>box = f.get_position()<br/>f.set_position([box.x0, box.y0, box.width * 1, box.height]) # resize position</span><span id="e11a" class="nq lx iq od b gy ol oi l oj ok"># Put a legend to the right side<br/>f.legend(loc='center right', bbox_to_anchor=(1.5, 0.5), ncol=3)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ph"><img src="../Images/c3fd5ce059cf5610ac4e4d6838318e91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wnj9gMjv3g9NgJT7azTNIg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Daily activity of users with most number of followers. The size of each point is proportional to the number of followers.</figcaption></figure><blockquote class="kd ke kf"><p id="ac26" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">尽管这些顶级账户有很多粉丝，但他们平均每天发布的推文数量不到 10 条。这种活动与“T4”栏目下每日发推次数最多的前 5 名用户相比就相形见绌了。</p></blockquote><h2 id="6ae5" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">7.最常提及的用户名</h2><p id="4ded" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">我们能从这些推文中发现抗议运动中更多受欢迎的人物吗？Twitter 用户可能会标记这些人，以告知他们当地正在发生的事件。他们的背景可以是律师、立法者、政治家、记者，甚至抗议领袖。</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="e915" class="nq lx iq od b gy oh oi l oj ok">def find_users(df):<br/>    # df: dataframe to look at<br/>    # returns a list of usernames<br/>    <br/>    # Create empty list<br/>    list_users = []<br/>    <br/>    for i in range(0, len(df)):<br/>        users_ith_text = re.findall('@[^\s]+', df.iloc[i,:]['text'])<br/>        # returns a list<br/>        # append to list_users by going through a for-loop:<br/>        for j in range(0, len(users_ith_text)):<br/>            list_users.append(users_ith_text[j])<br/>    <br/>    return list_users</span><span id="76a2" class="nq lx iq od b gy ol oi l oj ok"># Apply on dataframe data['text']<br/>list_users = find_users(data)</span><span id="14cf" class="nq lx iq od b gy ol oi l oj ok">mentioned_users_df = pd.DataFrame({<br/>    'mentioned_users': list_users<br/>})</span><span id="7dfb" class="nq lx iq od b gy ol oi l oj ok">mentionedusers = mentioned_users_df.groupby('mentioned_users').size().reset_index(name = 'totalcount').sort_values(by = 'totalcount', ascending = False)<br/>mentionedusers.head()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/0fa2370a30f0932db7755211e7019112.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*Z1pA7lUAxXbfg2ex5JmN3g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Most mentioned users</figcaption></figure><pre class="nm nn no np gt oc od oe of aw og bi"><span id="59bd" class="nq lx iq od b gy oh oi l oj ok">plt.figure(figsize=(30,8))<br/>sns.set_palette('PuBuGn_d')<br/>sns.barplot(x = 'mentioned_users', y = 'totalcount', data = mentionedusers.head(15))<br/>plt.xlabel('Mentioned users in tweets')<br/>plt.ylabel('Number of times')<br/>plt.title('Top users and how many times they were mentioned in tweets')<br/>plt.show()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pj"><img src="../Images/8aa932ade6f170f2a5abcf9138b5af83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*clBGA-ZyO4Or6zs7VM5nBw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Most mentioned users in tweets from 3rd Nov to 16th Nov 2019.</figcaption></figure><blockquote class="kd ke kf"><p id="770c" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在 15 个被提及最多的用户中，大部分(如果不是全部的话)都与香港和抗议运动直接相关。在谷歌上对这些用户进行快速搜索，结果显示他们要么支持抗议者和抗议活动，要么反对香港政府和警方。总而言之:</p></blockquote><ol class=""><li id="d116" class="li lj iq kj b kk kl ko kp lf lk lg ll lh lm le ln lo lp lq bi translated">@ Solomon yue——与美国通过的《香港人权与民主法案》有关的美籍华人政治家</li><li id="25c4" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">@ joshuawongcf——当地抗议领袖，原计划参加选举，但被禁止。</li><li id="d916" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">@ GOVUK——威胁要制裁香港官员对抗议的处理</li><li id="2557" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">@ HawleyMO——美国政治家</li><li id="08a5" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">@ heather wheeler——亚太事务部长，就拟议中的制裁致函香港政府官员。</li></ol><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pk"><img src="../Images/7702235a83389986ea559b807fc8b218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*39DOFz7zKxaaSQ1U"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@jon_chng?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jonathan Chng</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="a5e1" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">EDA 的结论</h1><p id="d7d6" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">总而言之，从 2019 年 11 月 3 日开始到 2019 年 11 月 16 日的 14 天内，超过 20 万条关于 2019 年香港抗议运动的推文被删除。所需的主要步骤是设置 Twitter API 调用；对推文进行清理和处理；和创造海洋视觉效果。该项目的数据分析/可视化集中在几个主题上:</p><ol class=""><li id="755e" class="li lj iq kj b kk kl ko kp lf lk lg ll lh lm le ln lo lp lq bi translated">最受欢迎的词有一个词云</li><li id="2fa3" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">用来自 NLTK 的 Vader-Lexicon 进行情感分析</li><li id="647d" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">标签的流行度</li><li id="13b7" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">最受欢迎的推文</li><li id="183b" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">Twitter 用户的活动</li><li id="385e" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">Twitter 用户的人口统计</li><li id="1002" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">最常提及的用户名</li></ol><h2 id="cab7" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">标签的有用性</h2><blockquote class="kd ke kf"><p id="a341" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">值得注意的是，通过监测用户使用的标签的每日受欢迎程度和趋势，有可能确定抗议活动中的重要里程碑。理论上，这意味着人们可以简单地监控标签，而不用阅读新闻或滚动社交媒体来了解抗议运动的最新动态。</p></blockquote><h2 id="5c74" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">热门推文的有用性</h2><blockquote class="kd ke kf"><p id="e9e5" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">最受欢迎的推文也有助于揭示普通推特用户对运动的持续情绪。我们可以这样理解。当关于某个事件或内容的推文被许多人转发时，这可能意味着这些人对该信息产生了共鸣，并希望与尽可能多的人分享。例如,《香港人权和民主法案》就是这样一个热门话题。最受欢迎的推文也可以提供关于一天的主要话题/事件的进一步细节和粒度。在上面标题为“<strong class="kj ir">最受欢迎的推文</strong>”的部分显示的证据中，我们看到这些推文经常涉及警察暴力和涉嫌不当使用武力的案件。</p></blockquote><h2 id="e001" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">个人观察</h2><blockquote class="kd ke kf"><p id="45a8" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">完成这个项目后，我更加相信，对一个话题/想法的整体情绪可能取决于社交媒体平台，即 Twitter、脸书、微博等。</p><p id="de44" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">到目前为止，我们已经从这些推特上看到了香港抗议运动的整体情绪，对香港政府和警察的负面情绪压倒一切，但对抗议者的积极和支持。同样，在微博和其他中国媒体网站等平台上，我们可能会得到相反的反应，这些平台对香港政府和警方表示支持和赞扬。</p><p id="bde2" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">然而，也有可能是我错了，因为搜索词中使用了标签，所以推文收集中存在缺陷。我们在搜索中使用的标签都是关于警察负面的，比如#hkpolicebrutality。使用它的人显然是用它来谴责这些所谓的暴行。回想起来，考虑# support hong kong police # supporthkgovt # supporthkpolice 等标签会更公平。我将把这个留给读者去探索这个元素。</p></blockquote><h2 id="9ffb" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">基于规则的情感分析的缺点</h2><blockquote class="kd ke kf"><p id="1c10" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">我们在上面使用 NLTLK 的 Vader 库进行的基本情绪分析揭示了大量的误报——在仔细检查这些被评为对政府和警察积极的随机推文后，它们实际上要么是对他们消极，要么是支持抗议者的事业。因此，我们需要转向深度学习技术，这将为我们提供更好、更可靠的结果。</p><p id="0a87" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">覆盖深度学习工作不在本项目的范围和目的之内。在分类一个小型推特数据集方面还需要做更多的工作，以便它可以用于预训练模型的迁移学习。</p><p id="b8a5" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">然而，根据我们手头的推文，抗议者和他们的事业得到了压倒性的支持，但公众对警察的暴行和不当行为表示强烈抗议。任何将推文分为正面或负面情绪的尝试，最终都可能导致对香港政府和警方负面情绪的高度倾斜分布。因此，用深度学习来预测情绪可能是不值得的。在我看来，相关 Twitter 推文的情绪很大程度上是负面的。</p></blockquote></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="9cf5" class="lw lx iq bd ly lz ng mb mc md nh mf mg mh ni mj mk ml nj mn mo mp nk mr ms mt bi translated">清理和预处理推文</h1><p id="4ba1" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">从这里开始，对这个项目的数据清理流程感兴趣的读者可以继续阅读本文的剩余部分。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pl"><img src="../Images/3ac80ae73cf917abe6c1c6aedcb4ab29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3ARpqyyZvEzfMVXF"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@zhenhappy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">pan xiaozhen</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="d718" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">导入库和数据集</h2><p id="0758" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">在另一个 Jupyter 笔记本中:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="7869" class="nq lx iq od b gy oh oi l oj ok"># Generic ones<br/>import numpy as np<br/>import pandas as pd<br/>import os</span><span id="581f" class="nq lx iq od b gy ol oi l oj ok"># Word processing libraries<br/>import re<br/>from nltk.corpus import wordnet<br/>import string<br/>from nltk import pos_tag<br/>from nltk.corpus import stopwords<br/>from nltk.tokenize import WhitespaceTokenizer<br/>from nltk.stem import WordNetLemmatizer</span><span id="6121" class="nq lx iq od b gy ol oi l oj ok"># Widen the size of each cell<br/>from IPython.core.display import display, HTML<br/>display(HTML("&lt;style&gt;.container { width:95% !important; }&lt;/style&gt;"))</span></pre><p id="ad95" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">每一轮 tweet 抓取都会创建一个. csv 文件。各读各的。先将 csv 文件转换成数据帧:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="188a" class="nq lx iq od b gy oh oi l oj ok"># read .csv files into Pandas dataframes first<br/>tweets_1st = pd.read_csv(os.getcwd() + '/data/raw' + '/20191103_131218_sahkprotests_tweets.csv', engine='python')<br/>..<br/>..<br/>tweets_15th = pd.read_csv(os.getcwd() + '/data/raw' + '/20191116_121136_sahkprotests_tweets.csv', engine='python')</span><span id="c7b3" class="nq lx iq od b gy ol oi l oj ok"># Check the shape of each dataframe:<br/>print('Size of 1st set is:', tweets_1st.shape)</span><span id="30fe" class="nq lx iq od b gy ol oi l oj ok"># You can also check out the summary statistics:<br/>print(tweets_1st.info())</span></pre><p id="8ee1" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">将所有数据帧连接成一个数据帧:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="ad01" class="nq lx iq od b gy oh oi l oj ok"># Concat the two dataset together:<br/>data = pd.concat([tweets_1st, tweets_2nd, tweets_3rd, tweets_4th, tweets_5th, tweets_6th, tweets_7th, tweets_8th, tweets_9th, tweets_10th, tweets_11th, tweets_12th, tweets_13th, tweets_14th, tweets_15th], axis = 0)</span><span id="b3c9" class="nq lx iq od b gy ol oi l oj ok">print('Size of concatenated dataset is:', data.shape)</span><span id="4fc0" class="nq lx iq od b gy ol oi l oj ok"># Reset_index<br/>data.reset_index(inplace = True, drop = True)<br/>data.head()<br/>print(data.info())</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/fd67884621c03aa286bc5db4641b52f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*cKEaNuARW8AAV2Ga3tAWdQ.png"/></div></figure><p id="fad8" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">您将在数据框中看到的片段:</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pn"><img src="../Images/2e8ac554cf671d0f513bfcb8987f61c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lI5wyvxZpUA5wy_C4QXag.png"/></div></div></figure><h2 id="979c" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">检查重复条目并删除它们</h2><p id="1769" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">由于我们是在彼此靠近的地方进行抓取，所以只要它们在 search_date 起 7 天的搜索窗口内，就有可能抓取到相同的推文。我们从数据集中删除这些重复的行。</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="97ce" class="nq lx iq od b gy oh oi l oj ok"># Let's drop duplicated rows:<br/>print('Initial size of dataset before dropping duplicated rows:', data.shape)<br/>data.drop_duplicates(keep = False, inplace = True)</span><span id="fc29" class="nq lx iq od b gy ol oi l oj ok">print('Current size of dataset after dropping duplicated rows, if any, is:', data.shape)</span></pre><blockquote class="kd ke kf"><p id="195c" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">删除重复行之前数据集的初始大小:(225003，11) <br/>删除重复行(如果有)之后数据集的当前大小为:(218652，11)</p></blockquote><h2 id="d8be" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">删除非英语单词/符号</h2><p id="3879" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">由于可能会删除日常英语对话中使用的非英语单词，如姓名等，因此按中文过滤可能会更好。</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="52a1" class="nq lx iq od b gy oh oi l oj ok"># Remove empty tweets<br/>data.dropna(subset = ['text'], inplace = True)</span><span id="a8c8" class="nq lx iq od b gy ol oi l oj ok"># The unicode accounts for Chinese characters and punctuations.<br/>def strip_chinese_words(string):<br/>    # list of english words<br/>    en_list = re.findall(u'[^\u4E00-\u9FA5\u3000-\u303F]', str(string))<br/>    <br/>    # Remove word from the list, if not english<br/>    for c in string:<br/>        if c not in en_list:<br/>            string = string.replace(c, '')<br/>    return string</span><span id="7e70" class="nq lx iq od b gy ol oi l oj ok"># Apply strip_chinese_words(...) on the column 'text'<br/>data['text'] = data['text'].apply(lambda x: strip_chinese_words(x))<br/>data.head()</span></pre><h2 id="8878" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">提取每条推文中提到的 Twitter 用户名</h2><p id="3836" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">我们希望从每条推文中获得有用的信息，因为我们可以分析谁是抗议运动中的热门人物。</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="84f8" class="nq lx iq od b gy oh oi l oj ok"># Define function to sieve out <a class="ae kc" href="http://twitter.com/users" rel="noopener ugc nofollow" target="_blank">@users</a> in a tweet:<br/>def mentioned_users(string):<br/>    usernames = re.findall('@[^\s]+', string)<br/>    return usernames</span><span id="0c41" class="nq lx iq od b gy ol oi l oj ok"># Create a new column and apply the function on the column 'text'<br/>data['mentioned_users'] = data['text'].apply(lambda x: mentioned_users(x))<br/>data.head()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi po"><img src="../Images/abd2610a402b63a9d51b8b7157289bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eKWBSTgAKZuzwv3i5De_Nw.png"/></div></div></figure><h2 id="9ebc" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">主文本清理和预处理</h2><p id="2b96" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">从每个文本中删除和提取中文单词和用户名后，我们现在可以做繁重的工作了:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="0f06" class="nq lx iq od b gy oh oi l oj ok"># Define Emoji_patterns<br/>emoji_pattern = re.compile("["<br/>         u"\U0001F600-\U0001F64F"  # emoticons<br/>         u"\U0001F300-\U0001F5FF"  # symbols &amp; pictographs<br/>         u"\U0001F680-\U0001F6FF"  # transport &amp; map symbols<br/>         u"\U0001F1E0-\U0001F1FF"  # flags (iOS)<br/>         u"\U00002702-\U000027B0"<br/>         u"\U000024C2-\U0001F251"<br/>         "]+", flags=re.UNICODE)</span><span id="a9b6" class="nq lx iq od b gy ol oi l oj ok"># Define the function to implement POS tagging:<br/>def get_wordnet_pos(pos_tag):<br/>    if pos_tag.startswith('J'):<br/>        return wordnet.ADJ<br/>    elif pos_tag.startswith('V'):<br/>        return wordnet.VERB<br/>    elif pos_tag.startswith('N'):<br/>        return wordnet.NOUN<br/>    elif pos_tag.startswith('R'):<br/>        return wordnet.ADV<br/>    else:<br/>        return wordnet.NOUN</span><span id="017a" class="nq lx iq od b gy ol oi l oj ok"># Define the main function to clean text in various ways:<br/>def clean_text(text):<br/>    <br/>    # Apply regex expressions first before converting string to list of tokens/words:<br/>    # 1. remove <a class="ae kc" href="http://twitter.com/usernames" rel="noopener ugc nofollow" target="_blank">@usernames</a><br/>    text = re.sub('@[^\s]+', '', text)<br/>    <br/>    # 2. remove URLs<br/>    text = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', '', text)<br/>    <br/>    # 3. remove hashtags entirely i.e. #hashtags<br/>    text = re.sub(r'#([^\s]+)', '', text)<br/>    <br/>    # 4. remove emojis<br/>    text = emoji_pattern.sub(r'', text)<br/>    <br/>    # 5. Convert text to lowercase<br/>    text = text.lower()<br/>    <br/>    # 6. tokenise text and remove punctuation<br/>    text = [word.strip(string.punctuation) for word in text.split(" ")]<br/>    <br/>    # 7. remove numbers<br/>    text = [word for word in text if not any(c.isdigit() for c in word)]<br/>    <br/>    # 8. remove stop words<br/>    stop = stopwords.words('english')<br/>    text = [x for x in text if x not in stop]<br/>    <br/>    # 9. remove empty tokens<br/>    text = [t for t in text if len(t) &gt; 0]<br/>    <br/>    # 10. pos tag text and lemmatize text<br/>    pos_tags = pos_tag(text)<br/>    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]<br/>    <br/>    # 11. remove words with only one letter<br/>    text = [t for t in text if len(t) &gt; 1]<br/>    <br/>    # join all<br/>    text = " ".join(text)<br/>    <br/>    return(text)</span><span id="aefe" class="nq lx iq od b gy ol oi l oj ok"># Apply function on the column 'text':<br/>data['cleaned_text'] = data['text'].apply(lambda x: clean_text(x))<br/>data.head()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi os"><img src="../Images/a220ce06821ba4f7b9cdb763aeb19063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2F52xegA4cSBlbZqDVAg8A.png"/></div></div></figure><pre class="nm nn no np gt oc od oe of aw og bi"><span id="7302" class="nq lx iq od b gy oh oi l oj ok"># Check out the shape again and reset_index<br/>print(data.shape)<br/>data.reset_index(inplace = True, drop = True)</span><span id="8fc3" class="nq lx iq od b gy ol oi l oj ok"># Check out data.tail() to validate index has been reset<br/>data.tail()</span></pre><h2 id="b3d5" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">处理列“hashtags”</h2><p id="3795" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">列“hashtags”的数据类型最初是 string，所以我们需要将其转换为 Python 列表。</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="dfbb" class="nq lx iq od b gy oh oi l oj ok"># Import ast to convert a string representation of list to list<br/># The column 'hashtags' is affected<br/>import ast</span><span id="e27c" class="nq lx iq od b gy ol oi l oj ok"># Define a function to convert a string rep. of list to list<br/>## Function should also handle NaN values after conversion<br/>def strlist_to_list(text):<br/>    <br/>    # Remove NaN<br/>    if pd.isnull(text) == True: # if true<br/>        text = ''<br/>    else:<br/>        text = ast.literal_eval(text)<br/>    <br/>    return text</span><span id="c4e6" class="nq lx iq od b gy ol oi l oj ok"># Apply strlist_to_list(...) to the column 'hashtags'<br/># Note that doing so will return a list of dictionaries, where there will be one dictionary for each hashtag in a single tweet.<br/>data['hashtags'] = data['hashtags'].apply(lambda x: strlist_to_list(x))</span><span id="2bba" class="nq lx iq od b gy ol oi l oj ok">data.head()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ot"><img src="../Images/7e217e8cb492822105f9b197ae0117b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uskduNC1hgdcNnp5qroRbw.png"/></div></div></figure><p id="7f89" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">因为每个“hashtag”条目包含一个字典列表，所以我们需要遍历列表来提取每个 hashtag:</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="2310" class="nq lx iq od b gy oh oi l oj ok"># Define a function to perform this extraction:<br/>def extract_hashtags(hashtag_list):<br/>    # argument:<br/>    # hashtag_list - a list of dictionary(ies), each containing a hashtag<br/>    <br/>    # Create a list to store the hashtags<br/>    hashtags = []<br/>    <br/>    # Loop through the list:<br/>    for i in range(0, len(hashtag_list)):<br/>        # extract the hashtag value using the key - 'text'<br/>        # For our purposes, we can ignore the indices, which tell us the position of the hashtags in the string of tweet<br/>        # lowercase the text as well<br/>        hashtags.append(hashtag_list[i]['text'].lower())<br/>        <br/>    return hashtags</span><span id="9959" class="nq lx iq od b gy ol oi l oj ok"># Apply function on the column - data['hashtags']<br/>data['hashtags'] = data['hashtags'].apply(lambda x: extract_hashtags(x))</span><span id="621e" class="nq lx iq od b gy ol oi l oj ok"># Check out the updated column 'hashtags'<br/>print(data.head()['hashtags'])</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pg"><img src="../Images/23186e14171e4d629d3fb4f386c542c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uetvuON9GOhiG3rtfKbo0w.png"/></div></div></figure><h2 id="c566" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">处理列“位置”</h2><pre class="nm nn no np gt oc od oe of aw og bi"><span id="95a5" class="nq lx iq od b gy oh oi l oj ok"># Replace NaN (empty) values with n.a to indicate that the user did not state his location<br/># Define a function to handle this:<br/>def remove_nan(text):<br/>    if pd.isnull(text) == True: # entry is NaN<br/>        text = 'n.a'<br/>    else:<br/>        # lowercase text for possible easy handling<br/>        text = text.lower()<br/>        <br/>    return text</span><span id="d8fb" class="nq lx iq od b gy ol oi l oj ok"># Apply function on column - data['location']<br/>data['location'] = data['location'].apply(lambda x: remove_nan(x))</span><span id="18f5" class="nq lx iq od b gy ol oi l oj ok"># Check out the updated columns<br/>print(data.head()['location'])</span><span id="0a9b" class="nq lx iq od b gy ol oi l oj ok"># Let's take a quick look at the value_counts()<br/>data['location'].value_counts()</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/ecdc414553c96e74cd4df04b846f03b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*daDjNoZttwZkb1FwGmonQw.png"/></div></figure><blockquote class="kd ke kf"><p id="3f52" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">不出所料，大多数推文是由来自/在香港的用户发布的。由于这些是每条推文用户的位置，现在确定实际的人口统计数据还为时过早。我们以后再处理这个问题。</p></blockquote><h2 id="9aab" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">处理列“acctdesc”</h2><p id="aedc" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">我们通过删除 NaN 值并用字符串“n.a”替换它们来清理这个列 twitter 用户的帐户描述。</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="4c5c" class="nq lx iq od b gy oh oi l oj ok"># Apply the function already defined above: remove_nan(...)<br/># Apply function on column - data['acctdesc']<br/>data['acctdesc'] = data['acctdesc'].apply(lambda x: remove_nan(x))</span><span id="0e08" class="nq lx iq od b gy ol oi l oj ok"># Check out the updated columns<br/>print(data.head()['acctdesc'])</span></pre><h1 id="b51d" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">特征工程——基于规则的文字处理</h1><p id="8065" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">到目前为止，我们已经删除了重复的行，提取了重要的信息，如标签，提到了用户和用户的位置，还清理了推文。在这一节中，我们将重点放在基于规则的文字处理来进行情感分析。一旦我们有了所有的要素，探索性的数据可视化将在以后完成。</p><h2 id="8097" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">使用 NLTK Vader_Lexicon 库从推文中产生情感</h2><p id="b024" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">我们使用 NLTK 的 Vader_lexicon 库为每条推文生成情感。维德使用词汇来确定推文中的哪些词是积极的或消极的。然后，它将返回一组关于文本的积极、消极和中立性的 4 个分数，以及文本是积极还是消极的总分数。我们将定义以下内容:</p><ol class=""><li id="9da5" class="li lj iq kj b kk kl ko kp lf lk lg ll lh lm le ln lo lp lq bi translated">积极—“积极”</li><li id="5a0f" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">消极——“消极”</li><li id="dc9e" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">中立——“新”</li><li id="64f7" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">总分—“复合”</li></ol><pre class="nm nn no np gt oc od oe of aw og bi"><span id="2367" class="nq lx iq od b gy oh oi l oj ok"># Importing VADER from NLTK<br/>from nltk.sentiment.vader import SentimentIntensityAnalyzer</span><span id="0c21" class="nq lx iq od b gy ol oi l oj ok"># Create a sid object called SentimentIntensityAnalyzer()<br/>sid = SentimentIntensityAnalyzer()</span><span id="e739" class="nq lx iq od b gy ol oi l oj ok"># Apply polarity_score method of SentimentIntensityAnalyzer()<br/>data['sentiment'] = data['cleaned_text'].apply(lambda x: sid.polarity_scores(x))</span><span id="c08a" class="nq lx iq od b gy ol oi l oj ok"># Keep only the compound scores under the column 'Sentiment'<br/>data = pd.concat([data.drop(['sentiment'], axis = 1), data['sentiment'].apply(pd.Series)], axis = 1)</span></pre><h2 id="973f" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">提取附加特征——每条推文中的字符数和字数</h2><pre class="nm nn no np gt oc od oe of aw og bi"><span id="69f9" class="nq lx iq od b gy oh oi l oj ok"># New column: number of characters in 'review'<br/>data['numchars'] = data['cleaned_text'].apply(lambda x: len(x))</span><span id="d7fa" class="nq lx iq od b gy ol oi l oj ok"># New column: number of words in 'review'<br/>data['numwords'] = data['cleaned_text'].apply(lambda x: len(x.split(" ")))</span><span id="bec5" class="nq lx iq od b gy ol oi l oj ok"># Check the new columns:<br/>data.tail(2)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oz"><img src="../Images/0ec5f038f85736ed0c794636981550fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WWImSQIKhS5sXvw7X-5fsw.png"/></div></div></figure><h2 id="e6e3" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">单词嵌入—使用 Gensim 训练 Doc2Vec</h2><p id="a211" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">单词嵌入涉及将文本语料库中的单词映射到数字向量，其中共享相似上下文的相似单词也将具有相似向量。它涉及一个浅层两层神经网络，该网络训练一个称为嵌入矩阵的矩阵/张量。通过取嵌入矩阵和语料库中每个词的一热向量表示的矩阵乘积，我们获得嵌入向量。</p><p id="c785" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们将使用 Gensim——一个开源 Python 库——来生成 doc2vec。</p><blockquote class="kd ke kf"><p id="4570" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">注意:应该在 word2vec 上使用 doc2vec 来获得“文档”的向量表示，在本例中，是整个 tweet。Word2vec 只会给我们一个 tweet 中单词的向量表示。</p></blockquote><pre class="nm nn no np gt oc od oe of aw og bi"><span id="fb7e" class="nq lx iq od b gy oh oi l oj ok"># Import the Gensim package<br/>from gensim.test.utils import common_texts<br/>from gensim.models.doc2vec import Doc2Vec, TaggedDocument</span><span id="6054" class="nq lx iq od b gy ol oi l oj ok">documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data["cleaned_text"].apply(lambda x: x.split(" ")))]</span><span id="4564" class="nq lx iq od b gy ol oi l oj ok"># Train a Doc2Vec model with our text data<br/>model = Doc2Vec(documents, vector_size = 10, window = 2, min_count = 1, workers = 4)</span><span id="2d5a" class="nq lx iq od b gy ol oi l oj ok"># Transform each document into a vector data<br/>doc2vec_df = data["cleaned_text"].apply(lambda x: model.infer_vector(x.split(" "))).apply(pd.Series)<br/>doc2vec_df.columns = ["doc2vec_vector_" + str(x) for x in doc2vec_df.columns]<br/>data = pd.concat([data, doc2vec_df], axis = 1)</span><span id="4316" class="nq lx iq od b gy ol oi l oj ok"># Check out the newly added columns:<br/>data.tail(2)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pq"><img src="../Images/73ccc99e62cdc1dc59f167bb9a23dcae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NUDAOHTpTXkGyaEnEu5Sng.png"/></div></div></figure><h2 id="2e79" class="nq lx iq bd ly nr ns dn mc nt nu dp mg lf nv nw mk lg nx ny mo lh nz oa ms ob bi translated">计算 TD-IDF 列</h2><p id="445e" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">接下来，我们将使用 sklearn 库计算评论的 TD-IDF。TD-IDF 代表<em class="ki">词频-逆文档频率</em>，用于反映一个词在集合或语料库中对一个文档的重要程度。TD-IDF 值与单词在文档中出现的次数成比例地增加，并被语料库中包含该单词的文档的数量所抵消，这有助于调整某些单词通常出现更频繁的事实。</p><ol class=""><li id="4613" class="li lj iq kj b kk kl ko kp lf lk lg ll lh lm le ln lo lp lq bi translated">术语频率—术语在文档中出现的次数。</li><li id="0d8a" class="li lj iq kj b kk lr ko ls lf lt lg lu lh lv le ln lo lp lq bi translated">逆文档频率—逆文档频率因子，减少文档集中出现频率很高的术语的权重，增加出现频率很低的术语的权重。</li></ol><p id="148d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">由于 NLTK 不支持 TF-IDF，我们将使用 Python sklearn 库中的 tfidfvectorizer 函数。</p><pre class="nm nn no np gt oc od oe of aw og bi"><span id="9483" class="nq lx iq od b gy oh oi l oj ok">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="a692" class="nq lx iq od b gy ol oi l oj ok"># Call the function tfidfvectorizer<br/># min_df is the document frequency threshold for ignoring terms with a lower threshold.<br/># stop_words is the words to be removed from the corpus. We will check for stopwords again even though we had already performed it once previously.<br/>tfidf = TfidfVectorizer(<br/>    max_features = 100,<br/>    min_df = 10,<br/>    stop_words = 'english'<br/>)</span><span id="554d" class="nq lx iq od b gy ol oi l oj ok"># Fit_transform our 'review' (the corpus) using the tfidf object from above<br/>tfidf_result = tfidf.fit_transform(data['cleaned_text']).toarray()</span><span id="a2f9" class="nq lx iq od b gy ol oi l oj ok"># Extract the frequencies and store them in a temporary dataframe<br/>tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())</span><span id="48a1" class="nq lx iq od b gy ol oi l oj ok"># Rename the column names and index<br/>tfidf_df.columns = ["word_" + str(x) for x in tfidf_df.columns]<br/>tfidf_df.index = data.index</span><span id="5ea1" class="nq lx iq od b gy ol oi l oj ok"># Concatenate the two dataframes - 'dataset' and 'tfidf_df'<br/># Note: Axis = 1 -&gt; add the 'tfidf_df' dataframe along the columns  or add these columns as columns in 'dataset'.<br/>data = pd.concat([data, tfidf_df], axis = 1)</span><span id="fd86" class="nq lx iq od b gy ol oi l oj ok"># Check out the new 'dataset' dataframe<br/>data.tail(2)</span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ot"><img src="../Images/c59e9eba0631c5bd7f53f09279302a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QOziZpajCz1LqxufNxoVFg.png"/></div></div></figure></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="9128" class="lw lx iq bd ly lz ng mb mc md nh mf mg mh ni mj mk ml nj mn mo mp nk mr ms mt bi translated">关闭</h1><p id="1822" class="pw-post-body-paragraph kg kh iq kj b kk mu km kn ko mv kq kr lf mw ku kv lg mx ky kz lh my lc ld le ij bi translated">我希望你获得了和我一样多的见识。欢迎留下评论来分享你的想法，或者就任何技术方面或我对数据的分析来纠正我。</p><p id="2a78" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">感谢您花时间阅读这篇冗长的文章。</p></div></div>    
</body>
</html>