<html>
<head>
<title>Retinal Vasculature Segmentation with a U-Net Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 U-Net 结构的视网膜血管分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/retinal-vasculature-segmentation-with-a-u-net-architecture-d927674cf57b?source=collection_archive---------9-----------------------#2019-05-18">https://towardsdatascience.com/retinal-vasculature-segmentation-with-a-u-net-architecture-d927674cf57b?source=collection_archive---------9-----------------------#2019-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="4077" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">视网膜脉管系统所展示的结构推断出关于诸如早产儿(RoP)、糖尿病性视网膜病(DR)、青光眼、高血压和与年龄相关的黄斑变性(AMD)等广泛的视网膜病理的关键信息。这些病理是失明的主要原因。视网膜脉管系统的精确分割对于各种眼科诊断和治疗过程是重要的。</p><p id="0210" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在过去的几十年里，许多研究都集中在开发用于视网膜血管分割的自动化和精确的技术上。随着近年来机器学习、深度学习和计算机视觉的兴起，研究人员已经找到了应用这些技术为医学、生物学和医疗保健中存在的问题提供解决方案的方法。</p><p id="c278" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这些技术中，U-Net 是一种有趣的深度学习网络架构。它是由 O. Ronneberger，P. Fischer 和 T. Brox 在 2015 年开发的，可以归类为用于生物医学图像分割的全卷积神经网络(CNN)。U-Net 论文的作者写了以下内容。</p><blockquote class="ko"><p id="e883" class="kp kq it bd kr ks kt ku kv kw kx kn dk translated"><em class="ky">…在本文中，我们提出了一种网络和训练策略，它依赖于对数据扩充的大量使用，以更有效地使用可用的带注释的样本。该架构由一个捕获上下文的收缩路径和一个支持精确定位的对称扩展路径组成……</em></p></blockquote><p id="831f" class="pw-post-body-paragraph jq jr it js b jt kz jv jw jx la jz ka kb lb kd ke kf lc kh ki kj ld kl km kn im bi translated">U-Net 不需要大量的训练数据，这使得它非常适合于生物医学图像分析，因为生物医学领域中的图像相对稀缺。在本文中，我们将讨论如何编写一个简单的 U-Net 架构来解决视网膜血管分割问题，以及如何评估算法的性能。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/7c8f34e0f17ed3829e56aaef465916dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*Jc75nDkqvUfMk18s61PBAQ.jpeg"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">A Retinal Image</figcaption></figure><p id="4c87" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我已经使用了“<a class="ae lq" href="https://www.isi.uu.nl/Research/Databases/DRIVE/" rel="noopener ugc nofollow" target="_blank">驱动:用于血管提取的数字视网膜图像</a>”数据集来训练网络。在数据集中，有两个文件夹，即“培训”和“测试”。“训练”文件夹包含 20 幅视网膜图像及其血管遮罩。“训练”文件夹中的 17 幅图像及其血管遮罩被用作训练集。剩余的 3 幅图像及其血管掩模作为验证集。测试文件夹包含 20 幅图像和两种类型的血管屏蔽(1st_manual 和 2nd_manual)。1st_manual vessel masks 被视为黄金标准，以便在评估性能时可以将人工注释(2nd_manual)与黄金标准进行比较。将 20 个图像及其血管掩模(1st_manual)作为测试数据。视网膜图像是 3 通道图像(RGB ),而它们的血管掩模是二进制图像。驱动器上的原始图像尺寸为 565 × 584。在将训练集、验证集和测试集保存在“. hdf5”文件中之前，它们的大小被调整为 512 × 512。</p><p id="84c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下图展示了我们将要考虑的 U-Net 架构。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/57a331d7b50a6f797d45f24b798a6c04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YstzNvuyRT6C3ibfAySDGw.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">The U-Net framework</figcaption></figure><p id="2a49" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下要点包含了我们可以用来训练模型的 U-Net 架构。架构是用 keras 写的。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="lw lx l"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">U-Net architecture coded in python (with Keras)</figcaption></figure><p id="9133" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于我们的训练集相当小，使用数据扩充技术来增加训练数据的大小是有帮助的。为此，我们可以使用 keras 的 ImageDataGenerator 类。它使您能够配置图像准备和增强。这个类的伟大之处在于它能够在模型拟合过程中创建扩充数据。数据生成器实际上是一个迭代器，当模型拟合算法请求时，它返回成批的增强图像。</p><p id="8e56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了准备用于训练的数据，我们必须首先在区间[0，1]内重新调整它们。当扩充我们的数据时，我们可以使用随机旋转。这些随机旋转的角度范围可以在数据生成器中指定。这将使我们的模型<strong class="js iu">旋转不变</strong>，因为模型将看到不同方向的图像。此外，我们可以使用水平和垂直随机移动作为增强技术。通过在具有不同垂直和/或水平位移的图像上训练我们的模型，我们可以使我们的模型<strong class="js iu">平移不变</strong>。缩放是我们可以使用的另一种增强技术。这将使我们的模型<strong class="js iu">比例不变</strong>。我们可以如下配置上述图像数据准备和增强技术。</p><pre class="lf lg lh li gt ly lz ma mb aw mc bi"><span id="ad4c" class="md me it lz b gy mf mg l mh mi">datagen_args = dict(<br/>             rescale=1./255,<br/>             rotation_range=90,<br/>             width_shift_range=0.1,<br/>             height_shift_range=0.1,<br/>             zoom_range=0.2<br/>)</span></pre><p id="086c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在数据准备和扩充期间，我们必须确保蒙版得到的变化与我们应用于图像的变化相同。下面的函数会解决这个问题。</p><pre class="lf lg lh li gt ly lz ma mb aw mc bi"><span id="b8a9" class="md me it lz b gy mf mg l mh mi">def get_generator(self, images, masks):<br/>    image_datagen = ImageDataGenerator(**datagen_args)<br/>    mask_datagen = ImageDataGenerator(**datagen_args)<br/><br/>    seed = 1<br/><br/>    image_generator = image_datagen.flow(images, seed=seed)<br/>    mask_generator = mask_datagen.flow(masks, seed=seed)<br/><br/>    return zip(image_generator, mask_generator)</span></pre><p id="0119" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们可以定义模型的训练例程。我们将使用学习率为 0.0001 的<strong class="js iu"> Adam 优化器</strong>。损失函数将是<strong class="js iu">二进制交叉熵</strong>，因为我们正在处理像素标记问题。(血管区域= 1，非血管区域= 0)。我们将训练 50 个时期的模型，同时每个时期有 200 个步骤，我们的批量大小是 32。这样，由于我们之前定义的图像增强配置，该模型在每个时期将看到 32 × 200 = 6400 幅图像。每当在给定时期结束时损失得到改善，我们就将我们的模型权重保存到“. hdf5”文件中。此外，我们将实现一个有 10 个周期的耐心(没有改善的周期数，在此之后训练将被停止)的早期停止。</p><pre class="lf lg lh li gt ly lz ma mb aw mc bi"><span id="4e20" class="md me it lz b gy mf mg l mh mi">compile_args = dict(<br/>            optimizer=Adam(lr=1e-4),<br/>            loss='binary_crossentropy',<br/>            metrics=['accuracy']<br/>)</span><span id="ed29" class="md me it lz b gy mj mg l mh mi">earlystopper = EarlyStopping(<br/>            patience=10,<br/>            verbose=1<br/>)</span><span id="5353" class="md me it lz b gy mj mg l mh mi">model_checkpoint = ModelCheckpoint(<br/>            self.model_path,<br/>            monitor='loss',<br/>            verbose=1,<br/>            save_best_only=True<br/>)</span><span id="ba3d" class="md me it lz b gy mj mg l mh mi">model.compile(**self.compile_args)</span><span id="b9e8" class="md me it lz b gy mj mg l mh mi">train_generator = self.get_generator(x_train, y_train)<br/>val_generator = self.get_generator(x_val, y_val)</span><span id="45e6" class="md me it lz b gy mj mg l mh mi">model.fit_generator(<br/>            train_generator,<br/>            epochs=50,<br/>            steps_per_epoch=200,<br/>            verbose=1,<br/>            callbacks=[model_checkpoint, earlystopper],<br/>            validation_data=val_generator,<br/>            validation_steps=10<br/>)</span></pre><p id="3108" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练完成后，我们可以评估我们的模型。为了评估该模型，我们可以使用性能指标，如 F1 分数、准确度分数、受试者-操作者曲线(ROC) AUC 和精确度-召回率(PR)曲线 AUC。绘制 PR 和 ROC 曲线可以很好地了解模型性能。</p><p id="4c8a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们还可以绘制一个选定的视网膜图像(来自测试集)，它的血管掩模由人类注释者创建，它的血管掩模由我们的 U-Net 模型预测。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mk"><img src="../Images/2e0f54d6ebc7b62a113711ad1ca5a41b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L_5yIbLq6xZ7Q5-d3zTiRw.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">From the left : Original retinal image, vessel mask annotated by a human and the predicted vessel mask using the U-Net model</figcaption></figure><p id="a28c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章到此结束。请在评论中告诉我你的问题。U 网快乐！</p><p id="7bfd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参考资料:</p><ol class=""><li id="9a68" class="ml mm it js b jt ju jx jy kb mn kf mo kj mp kn mq mr ms mt bi translated"><a class="ae lq" href="https://machinelearningmastery.com/image-augmentation-deep-learning-keras/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/image-augment-deep-learning-keras/</a></li></ol></div></div>    
</body>
</html>