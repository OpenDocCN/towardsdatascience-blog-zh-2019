<html>
<head>
<title>Recurrent Neural Network-Head to Toe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络-从头到脚</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recurrent-neural-network-head-to-toe-d58ff2f2dab3?source=collection_archive---------12-----------------------#2019-10-24">https://towardsdatascience.com/recurrent-neural-network-head-to-toe-d58ff2f2dab3?source=collection_archive---------12-----------------------#2019-10-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="08de" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经元是人脑的组成部分。它可以在微秒内分析复杂的信号，并向神经系统发送信号来执行任务。每个神经元的结构都是相同的，这意味着神经元之间的结构层不会改变。让这些层连续，它可以很容易地复制我们的大脑。这些连续的“层”帮助我们进行日常活动、复杂的决策和语言处理。</p><p id="2c76" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是，如何在这些层面上概括我们的问题呢？概括起来需要什么样的模型？</p><p id="ed55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">答案以参数共享的形式出现在研究人员面前。它有助于将模型扩展和应用到不同形式的数据。这是通过将输出的成员共享为输出的先前成员的函数来实现的。输出的成员由相同的更新规则产生。理解这种计算结构的更简单的方法是使用<em class="ko">‘展开计算图’</em>。图形的展开导致共享结构中参数的深层网络。</p><p id="670d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">考虑动力系统的经典形式:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/ad6e468cfdb7a53ddd8da7d9473ed4d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/1*E9vqXECq5PnNciQsRa62Xw.gif"/></div></div></figure><p id="0da7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">展开这个动力系统的结构，我们得到:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/fb81f392539a5e54a3112d837bc506f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*tDdvNcYryeY5SPCoprNuOg.gif"/></div></figure><p id="0b73" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如你所见，这个等式不再涉及递归。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/c641796f02940f6dc934e1fca02cd97c.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/1*EUds4Pn88g8bzpihUCy_SA.gif"/></div></figure><p id="2f6f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">递归神经网络可以以不同的方式建立，其中一些也可以有隐藏单元。当训练递归神经网络基于过去的输入来执行时，摘要是有损耗的，因为我们将任意长度的序列映射到向量<em class="ko"/><strong class="js iu"><em class="ko">【h(t)</em></strong><em class="ko">。</em>根据手头的任务，我们也可以选择哪些过去的输入，我们可以有选择地保留过去序列的某些方面。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/a9b70973410572f3857b1d862b3f1826.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*AmYJRvLVlSsm30Q-BqEIsA.png"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk"><strong class="bd li">Unfolded graph: A recurrent network with no outputs, it processes the information from the input x by incorporating it into the state h that is passed forward through time.</strong></figcaption></figure><p id="de59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以用函数<strong class="js iu"> <em class="ko"> g(t) </em> </strong>来表示 t 步之后展开的递归。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lj"><img src="../Images/8d09858d8bc2c38391da386914d1b14a.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/1*Lh7zzbfauWW3YymAtUGhhA.gif"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">The function of past sequence takes g(t) as input.</figcaption></figure><p id="1278" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">展开过程具有一些主要优势，并导致使模型<em class="ko"> f </em>无处不在成为可能的因素，这进一步允许一般化。</p><p id="f7c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko"> a)不管输入序列的长度如何，模型具有相同的输入大小。</em></p><p id="808f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko"> b)有可能在每个时间步使用具有相同参数的相同转移函数 f，因为它是从一个状态到另一个状态指定的。</em></p><p id="0822" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">展开图通过显示信息流动的路径，说明了明确描述和信息在时间上向前和向后流动的思想。</p><p id="9402" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些想法对于构建递归神经网络非常重要，因为 RNN 在每个时间步都产生输出，并且隐藏单元之间的连接可以通过读取整个序列产生输出，然后产生单个输出。这导致了一个结论，即任何可以被图灵机 计算的函数都可以被有限大小的递归神经网络计算。正是这种利用过去的输出和隐藏层连接的性质，使得 RNN 取得了今天的成就。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/c1ea5fc145585ecc2dcd94cd4892ad59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*w2a6YkGEXA_ONuJUHjpkTg.gif"/></div></figure></div><div class="ab cl ll lm hx ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="im in io ip iq"><h2 id="4632" class="ls lt it bd lu lv lw dn lx ly lz dp ma kb mb mc md kf me mf mg kj mh mi mj mk bi translated"><strong class="ak">递归神经网络</strong></h2><p id="4653" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">有了图形展开和参数共享的知识，我们现在开发 RNN。我们假设双曲正切激活函数。考虑输出的一个自然方法是给出非标准化的对数概率，我们将<em class="ko"> softmax </em>作为<em class="ko">后处理步骤</em>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/9153716b412c531bb3d961f97b4715a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*f2risth5cbWSb1KghGdCqA.png"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">Computation Graph to compute training loss of recurrent neural network.The sequence of output values o is compared to the training targets y, this leads to the computation of the loss function. We assume o is the unnormalised log probabilities. The <strong class="bd li">loss function</strong> <strong class="bd li">L</strong> internally computes <strong class="bd li">y^ = softmax(o)</strong> and compares this to target y.The RNN has input to hidden connections parameterised by a weight matrix <strong class="bd li">U</strong>, parameterised by a weight matrix <strong class="bd li">W</strong>, and hidden to output connection parameterised by a weight matrix <strong class="bd li">V</strong>.</figcaption></figure><blockquote class="mr ms mt"><p id="75e1" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">RNN 的计算可以分解为三个参数块:</p><p id="87a7" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">1.从输入到隐藏状态</p><p id="91e0" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">2.从以前的隐藏状态到现在的隐藏状态</p><p id="5474" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">3.从隐藏状态到输出</p><p id="5df8" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">这些块中的每一个都与单独的权重矩阵相关联。当网络展开时，这些块中的每一个都对应于浅层变换(影响单个层的变换)。</p></blockquote><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/a467c3d02e9de7dd1cc6d5a9790eb662.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*FjZbXsx3o0-QJ9qtpNqW2A.png"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi my"><img src="../Images/e8d3bfc1257f556b73fb430cc9e9a267.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*vi2JVoa0SBsQ7zVUaBeWGA.png"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/8048b28f81327ad5a136acad31ef681a.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*8qKjKPgaMQYG7Ag2bvu-NA.png"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">The above equations specify forward propagation of this model,forward propagation begins with a specification of initial state <strong class="bd li">h(0)</strong> for each time step from <strong class="bd li">t = 1 to t= T</strong></figcaption></figure><p id="d8ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是 RNN 的图像，它将输入序列映射到相同长度的输出序列。总损失是一段时间内损失的总和。<em class="ko"> L(t) </em>是<em class="ko"> y(t)的负对数似然。</em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi na"><img src="../Images/b90619a13b42eb1607d6780df7802c24.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*5_NE2Vd7OAUIHuB30beCmw.png"/></div></figure><p id="7233" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">梯度计算包括从左到右的前向传播，随后是从右到左的反向传播。在前向传播中计算的状态必须在反向传播中使用，因此需要存储它们。通过输出<em class="ko"> o(t) </em>应用的反向传播算法被称为<em class="ko">‘通过时间的反向传播’。</em>通过递归神经网络计算梯度非常简单。人们简单地应用广义反向传播算法来展开计算图。通过反向传播获得的梯度然后可以用于训练 RNN。</p><p id="64c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们在<em class="ko"> Pytorch 中制作<em class="ko"> RNN </em>模型。</em>我们在这里生成伪代码，我将在我的 GitHub 链接中贴出完整的引文:<em class="ko"> </em> <a class="ae nb" href="https://github.com/theAkhileshRai/ML_Made_Easy/blob/master/VanillaRNNEarly" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> <em class="ko">链接</em> </strong> </a></p><h2 id="b8fe" class="ls lt it bd lu lv lw dn lx ly lz dp ma kb mb mc md kf me mf mg kj mh mi mj mk bi translated"><em class="nc"> Pytorch MNIST 培训</em></h2><p id="42b9" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">首先，我们导入类和<em class="ko"> MNIST 数据集。</em></p><blockquote class="mr ms mt"><p id="fbd4" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">导入 torch <br/>导入 torchvision <br/>导入 torch.nn 作为 nn <br/>导入 torchvision.transforms 作为转换<br/>导入 torchvision.datasets 作为数据集<br/>从 matplotlib 导入 numpy 作为 np <br/>导入 pyplot 作为 plt</p><p id="247d" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">n _ epochs = 3<br/>batch _ size _ train = 64<br/>batch _ size _ test = 1000<br/>learning _ rate = 0.01<br/>momentum = 0.5<br/>log _ interval = 10</p><p id="7c40" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">random _ seed = 1<br/>torch . backends . cud nn . enabled = False<br/>torch . manual _ seed(random _ seed)</p><p id="c11a" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">train _ loader = torch . utils . data . data loader(dsets。MNIST('/Users/akhileshrai/Downloads '，train=True，download=True，<br/>transform = torch vision . transforms . compose([<br/>torch vision . transforms . totensor()，<br/>torch vision . transforms . normalize(<br/>(0.1307)，(0.3081)，)<br/>))，<br/> batch_size=batch_size_train，shuffle=True)</p><p id="4ca4" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">test _ loader = torch . utils . data . data loader(<br/>dsets)。MNIST('/Users/akhileshrai/Downloads '，train=False，download=True，<br/>transform = torch vision . transforms . compose([<br/>torch vision . transforms . totensor()，<br/>torch vision . transforms . normalize(<br/>(0.1307)，(0.3081)，)<br/>))，<br/> batch_size=batch_size_test，shuffle=True)</p><p id="3438" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">examples = enumerate(test _ loader)<br/>batch _ idx，(example_data，example_targets) = next(示例)</p><p id="6a38" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">将 matplotlib.pyplot 作为 plt 导入</p><p id="d4a4" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">fig = plt.figure() <br/>对于范围(6)中的 I:<br/>PLT . subplot(2，3，I+1)<br/>PLT . tight _ layout()<br/>PLT . im show(example _ data[I][0]，cmap='gray '，interpolation = ' none ')<br/>PLT . title(" Number:{ } "。格式(example _ targets[I])<br/>PLT . x ticks([])<br/>PLT . y ticks([])<br/>打印(图)</p></blockquote><p id="17c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">数据加载器</em>加载<em class="ko"> MNIST </em>数据集。数据集被下载到上述文件夹中。这些变换首先用于将数据转换为张量，然后将数据归一化。</p><p id="afa1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们通过打印测试数据集的样本来看看同样的情况。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/63678f06901983cd0105c86b3fa7d786.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*4zdVKak5b6ugGb3sF4FKNA.png"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">Samples of the MNIST Dataset</figcaption></figure><p id="0caa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们根据上面所示的架构来准备我们的神经网络:</p><blockquote class="mr ms mt"><p id="7226" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">RNNModel 类(nn。模块):<br/> def __init__(self，input_dim，hidden_dim，layer_dim，output _ dim):<br/>super(rnn model，self)。__init__() <br/> #隐藏维度<br/> self.hidden_dim = hidden_dim</p><p id="0ed3" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#隐藏层数<br/> self.layer_dim = layer_dim</p><p id="2a46" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#构建您的 RNN <br/> # batch_first=True 导致输入/输出张量的形状为<br/> # (batch_dim，seq_dim，input_dim) <br/> # batch_dim =每批的样本数<br/> self.rnn = nn。RNN(input_dim，hidden_dim，layer_dim，batch_first=True，非线性='tanh ')</p><p id="f50e" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#读出层<br/> self.fc = nn。线性(隐藏尺寸，输出尺寸)</p><p id="0e42" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">def forward(self，x): <br/> #用零初始化隐藏状态<br/> # (layer_dim，batch_size，hidden _ dim)<br/>h0 = torch . zeros(self . layer _ dim，x.size(0)，self.hidden_dim)。requires_grad_()</p><p id="1390" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#我们需要分离隐藏状态以防止爆炸/消失梯度<br/> #这是通过时间截断反向传播的一部分(BPTT) <br/> out，hn = self.rnn(x，h0.detach())</p><p id="36be" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#索引上一时间步的隐藏状态<br/> # out.size() → 100，28，10 <br/> # out[:，-1，:] → 100，10 →只想要上一时间步的隐藏状态！<br/> out = self.fc(out[:，-1，:)<br/> # out.size() → 100，10 <br/>返回 out</p><p id="cbbd" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">输入 _ 尺寸= 28 <br/>隐藏 _ 尺寸= 100 <br/>图层 _ 尺寸= 3 <br/>输出 _ 尺寸= 10</p><p id="f875" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">model = RNNModel(输入尺寸，隐藏尺寸，图层尺寸，输出尺寸)</p><p id="b305" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">print(model)<br/>print(len(list(model . parameters())))<br/>for I in range(len(list(model . parameters()))):<br/>print(list(model . parameters())【I】。size())</p></blockquote><p id="2916" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的模型有 3 个隐藏层，每层 100 个隐藏神经元，接受 28 维的输入数据，同时输出 10 维的数据。我们假设的激活函数是<em class="ko">双曲线</em> ' <em class="ko"> tanh' </em>。<em class="ko">随机梯度下降</em>用于在每次迭代中寻找单个例子的成本函数的梯度，而不是所有例子的成本函数的梯度之和。</p><p id="a7c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，该模型经过<em class="ko"> 5000 次迭代训练，</em>一种称为早期停止的方法用于防止模型“过度拟合”。</p><p id="7db8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">提前停止</em> </strong>是通过计算验证损失完成的优化技术。如果验证损失在指定的迭代次数内没有减少，则模型停止其训练。</p><blockquote class="mr ms mt"><p id="d6d5" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">learning _ rate = 0.01<br/>min _ val _ loss = NP。INF<br/>epochs _ no _ improve = 0<br/>n _ epoch _ stop = 2<br/>early _ stop = False</p><p id="5ad4" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">optimizer = torch . optim . SGD(model . parameters()、lr=learning_rate) <br/> #展开步骤数<br/>criteria = nn。交叉入口型()</p><p id="0f7d" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#展开的步骤数<br/>seq_dim = 28<br/>early _ stop = False<br/>ITER = 0<br/>for epoch in range(num _ epochs):<br/><br/>val _ loss = 0<br/>for I，(images，labels)in enumerate(train _ loader):<br/>#将图像加载为具有梯度累积能力的 torch 张量<br/> images = images.view(-1，seq _ dim，input_dim)。requires_grad_()</p><p id="0945" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#清除梯度 w.r.t .参数<br/> optimizer.zero_grad()</p><p id="b23d" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#向前传递以获取输出/logits<br/># outputs . size()→100，10 <br/>输出=模型(图像)</p><p id="8222" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#计算损失:softmax →交叉熵损失<br/>损失=标准(输出，标签)</p><p id="0911" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#获取梯度 w.r.t .参数<br/> loss.backward()</p><p id="185b" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#更新参数<br/>optimizer . step()<br/><br/>val _ loss+= loss<br/>val _ loss = val _ loss/len(train _ loader)<br/>#如果验证损失最小<br/>如果 val _ loss&lt;min _ val _ loss:<br/>#保存模型<br/># torch . Save(model)<br/>epochs _ no _ improve = 0<br/>min _ val _ loss = val _ loss<br/><br/>否则</p><p id="c77e" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">如果 iter % 500 == 0: <br/> #计算精度<br/>正确= 0 <br/>总计= 0 <br/> #迭代测试数据集<br/> #检查<strong class="js iu">提前停止</strong>条件<br/>如果 epochs _ no _ improve = = n _ epochs _ stop:<br/>打印('提前停止！')<br/>early _ stop = True<br/>break<br/>else:<br/>continue<br/>break<br/>if early _ stop:<br/>print(" Stopped ")<br/>break<br/><br/><br/><br/>对于图像，test_loader 中的标签:<br/># Resize images<br/>images = images . view(-1，seq_dim，input_dim)</p><p id="2713" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#仅向前传递以获取逻辑/输出<br/>输出=模型(图像)</p><p id="2463" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#从最大值得到预测值<br/> _，predicted = torch . max(outputs . data，1)</p><p id="ba45" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#标签总数<br/> total += labels.size(0)</p><p id="ec05" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">#正确预测总数<br/>正确+=(预测==标签)。总和()</p><p id="2b89" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">准确度= 100 *正确/总计</p><p id="19c9" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated"># Print Loss <br/> print('迭代:{}。损失:{}。准确性:{} '。format(iter，loss.item()，accuracy))</p></blockquote><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/9f36157142364c9050013a33c3354959.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*fYG6FZofpR8uCwLpQcSejQ.png"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">Description: Training the RNN using early Early Stopping.</figcaption></figure><p id="4f1e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">模型调查及结论:</strong></p><p id="303e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该模型很好地检测了与先前数字无关的手写数字。如果这些数字是跨序列相关的(长期和短期相关)，该模型不会做得很好。</p><p id="4877" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过时间的反向传播算法是昂贵的，因为在正向传递中计算的状态将被存储，直到它们在反向传递中被重用，所以存储器成本也是<em class="ko"> O(T)。</em></p><p id="9fe1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">RNNs 可能面临的另一个问题是“悬崖”。非线性函数会出现这种情况，因为它们的导数的大小可能非常大，也可能非常小。<strong class="js iu"> <em class="ko">【裁剪渐变】</em> </strong> <em class="ko">是一种技术，用于使渐变下降更合理，并限制步长。</em></p></div><div class="ab cl ll lm hx ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="im in io ip iq"><p id="8950" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">我希望这能让你了解 rnn 是如何形成的，它们的直觉是如何塑造深度学习世界的。</em></p><p id="8df6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">一些让你入门的好的研究论文和书籍:</em></p><ol class=""><li id="8ef6" class="nf ng it js b jt ju jx jy kb nh kf ni kj nj kn nk nl nm nn bi translated"><em class="ko"> Alex Graves: </em> <a class="ae nb" href="http://Generating Sequences With Recurrent Neural Networks" rel="noopener ugc nofollow" target="_blank"> <em class="ko">用递归神经网络生成序列</em> </a> <em class="ko">。</em></li><li id="ef41" class="nf ng it js b jt no jx np kb nq kf nr kj ns kn nk nl nm nn bi translated">深度学习:伊恩·古德菲勒</li><li id="a23a" class="nf ng it js b jt no jx np kb nq kf nr kj ns kn nk nl nm nn bi translated">用于模式识别的神经网络。</li><li id="7189" class="nf ng it js b jt no jx np kb nq kf nr kj ns kn nk nl nm nn bi translated"><em class="ko"> Y .本吉奥；P. SimardP. Frasconi: </em> <a class="ae nb" href="https://ieeexplore.ieee.org/abstract/document/279181" rel="noopener ugc nofollow" target="_blank"> <em class="ko">学习具有梯度下降的长期依赖关系是困难的</em> </a></li></ol><p id="4a7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一如既往地欢迎评论。</p></div></div>    
</body>
</html>