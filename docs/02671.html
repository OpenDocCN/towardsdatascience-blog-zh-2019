<html>
<head>
<title>Review: Maxout Network (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:最大输出网络(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-maxout-network-image-classification-40ecd77f7ce4?source=collection_archive---------18-----------------------#2019-05-01">https://towardsdatascience.com/review-maxout-network-image-classification-40ecd77f7ce4?source=collection_archive---------18-----------------------#2019-05-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e42d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">作者:<strong class="ak">伊恩·j·古德菲勒，</strong>大卫·沃德-法利，迈赫迪·米尔扎，<strong class="ak">亚伦·库维尔，约舒阿·本吉奥</strong></h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/77d70a2cba42b7a8cdc38c365e47971d.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/0*KKyKZddnzzDe3a7M.jpg"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">The book “</strong><a class="ae ks" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank"><strong class="bd kr">Deep learning</strong></a><strong class="bd kr">”</strong></figcaption></figure><p id="70b7" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi lp translated"><span class="l lq lr ls bm lt lu lv lw lx di">在</span>这个故事中，对<strong class="kv ir">蒙特利尔大学</strong>的<strong class="kv ir"> Maxout 网络</strong>进行了简要回顾。<strong class="kv ir">第一作者 Ian J. GoodFellow </strong>，也是生成对抗网络(GAN)的发明者。而<strong class="kv ir">最后一位作者 Yoshua Bengio </strong>，今年(2019)刚刚拿到了最近的图灵奖，也就是“计算的诺贝尔奖”。这两位作者加上倒数第二位作者<strong class="kv ir">亚伦·库维尔</strong>，三位作者一起，还出版了<strong class="kv ir">本书</strong> <a class="ae ks" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir">深度学习</strong></a><strong class="kv ir"/>，通过出版社 MIT Press，于 2016 年出版。而本文针对 maxout 网络，发表在<strong class="kv ir"> 2013 ICML </strong>上，引用超过<strong class="kv ir"> 1500 次</strong>。(<a class="ly lz ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----40ecd77f7ce4--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="497a" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">概述</h1><ol class=""><li id="2d7f" class="mz na iq kv b kw nb kz nc lc nd lg ne lk nf lo ng nh ni nj bi translated"><strong class="kv ir">最大输出</strong></li><li id="fbbe" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo ng nh ni nj bi translated"><strong class="kv ir">关于</strong> <a class="ae ks" rel="noopener" target="_blank" href="/review-nin-network-in-network-image-classification-69e271e499ee"> <strong class="kv ir"> NIN </strong> </a>中 Maxout 的解释</li><li id="f387" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo ng nh ni nj bi translated"><strong class="kv ir">结果</strong></li></ol></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="d322" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">1.最大输出</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/0f61ebb80b2c3eed77ca224cda29d034.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*OpU5Ce-PIgyttn-b6esWOg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">An MLP containing two maxout units</strong></figcaption></figure><ul class=""><li id="eac3" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated">给定一个输入<em class="nu"> x </em>，或隐藏层的状态<em class="nu"> v </em>，<em class="nu"> z </em>为:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c6a7ba748eaf0bacb0a22cee4151593c.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*RhTQd1TK0pb4v-zDxo0Gww.png"/></div></figure><ul class=""><li id="b95d" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated">并且使用了一种新型的激活功能:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi nw"><img src="../Images/95f2daafd8c2318cc8822cb96f9d9f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*E5P9nC2jNuqdH-6h726lEQ.png"/></div></div></figure><ul class=""><li id="351a" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated">最后<em class="nu"> g </em>是:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b05cb15aefb7349b0cd66d4217ffc204.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*JPajRmXTdh8czlxErwCLdw.png"/></div></figure><ul class=""><li id="ed8e" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated">背后的理念是:</li></ul><blockquote class="oc od oe"><p id="0824" class="kt ku nu kv b kw kx jr ky kz la ju lb of ld le lf og lh li lj oh ll lm ln lo ij bi translated"><strong class="kv ir">任何连续的 PWL 函数都可以表示为两个凸 PWL 函数的差。</strong></p></blockquote><ul class=""><li id="a3ce" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated"><strong class="kv ir">任何连续函数都可以用分段线性函数任意逼近。</strong></li><li id="f5e1" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">并且可以通过具有两个隐藏单元<em class="nu"> h1 </em> ( <em class="nu"> v </em>)和<em class="nu"> h2 </em> ( <em class="nu"> v </em>)的 maxout 网络来实现，具有足够大的<em class="nu"> k </em>。</li><li id="db9e" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">并且发现<strong class="kv ir">一个两隐单元 maxout 网络可以很好地逼近紧域上任意连续函数<em class="nu"> f </em> ( <em class="nu"> v </em>)。</strong></li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="b481" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">2.中<a class="ae ks" rel="noopener" target="_blank" href="/review-nin-network-in-network-image-classification-69e271e499ee">最大输出的解释</a></h1><ul class=""><li id="3ad6" class="mz na iq kv b kw nb kz nc lc nd lg ne lk nf lo nt nh ni nj bi translated">由于<a class="ae ks" rel="noopener" target="_blank" href="/review-nin-network-in-network-image-classification-69e271e499ee"> NIN </a>在实验结果部分与 Maxout 进行了深入的比较，<a class="ae ks" rel="noopener" target="_blank" href="/review-nin-network-in-network-image-classification-69e271e499ee"> NIN </a>也为 Maxout 网络做了一点说明。</li><li id="fc57" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated"><strong class="kv ir">通过仿射特征图的最大池化减少了特征图的数量</strong>(仿射特征图是线性卷积的直接结果，没有应用激活函数)。</li><li id="ffd5" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">线性函数上的最大化使得分段线性逼近器能够逼近任何凸函数。</li><li id="69c0" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">最大输出网络更有效，因为它可以分离位于凸集内的概念。</li><li id="8f97" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated"><strong class="kv ir">然而，最大输出网络强加了潜在概念的实例位于输入空间的凸集中的先验，这不一定成立。</strong></li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="7fa7" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated"><strong class="ak"> 3。结果</strong></h1><h2 id="bcb8" class="oi mi iq bd mj oj ok dn mn ol om dp mr lc on oo mt lg op oq mv lk or os mx ot bi translated">3.1.MNIST</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/9ddc237ed6bc920a571c6fa3511fcc2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*6iu4Ei01Ouu9s-olbtxb6w.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Test error on permutation invariant MNIST</strong></figcaption></figure><ul class=""><li id="d60a" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated">MNIST (LeCun 等人，1998 年)数据集由手写数字 0-9 的 28×28 像素灰度图像组成，有 60，000 个训练样本和 10，000 个测试样本。</li><li id="27c9" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">最后 10，000 个训练样本用作验证集。</li><li id="d865" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">训练由两个密集连接的最大输出层和跟随其后的 softmax 层组成的模型。</li><li id="1964" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">获得了 0.94%的测试误差，这是不使用无监督预训练的最好结果。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/08d2227aa9097791a925c71a61bafa7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*8Uw-1gJGkD0NNclwvCpphA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Test error on MNIST</strong></figcaption></figure><ul class=""><li id="a3e9" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated">使用三个卷积最大输出隐藏层(在最大输出层之上具有空间最大汇集),其后是密集连接的 softmax 层。</li><li id="3198" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">测试集错误率为 0.45%，这是最好的结果。</li></ul><h2 id="addc" class="oi mi iq bd mj oj ok dn mn ol om dp mr lc on oo mt lg op oq mv lk or os mx ot bi translated">3.2.CIFAR-10</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/d96a9c5a49353b751393589b3a88df14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*QihrCLQ7o9ZR6G0juBrgQg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Test error on CIFAR-10</strong></figcaption></figure><ul class=""><li id="9e66" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated">CIFAR-10 数据集(Krizhevsky &amp; Hinton，2009 年)由 32 × 32 幅彩色图像组成，这些图像来自 10 个类别，分为 50，000 幅训练图像和 10，000 幅测试图像。</li><li id="495a" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">所使用的模型由三个卷积最大输出层、全连接最大输出层和全连接 softmax 层组成。</li><li id="972e" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">测试集误差为 11.68%。</li><li id="31f3" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">通过数据扩充，即平移和水平反射，获得了 9.38%的测试集误差。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/188e06d600ca4beb0bb29c862cbe539e.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*3gkPWqu-KstYGC9jkSWumA.png"/></div></figure><ul class=""><li id="eca2" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated">借助 dropout，CIFAR-10 上的验证集误差降低了 25%以上。</li></ul><h2 id="2774" class="oi mi iq bd mj oj ok dn mn ol om dp mr lc on oo mt lg op oq mv lk or os mx ot bi translated">3.3.西发尔-100</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi oy"><img src="../Images/17b2c9f81389f1cdb28431f153d57f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ru7BlP-T9HXX28Rn.gif"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">CIFAR-100</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/b0474089eaef423d22d5d8843def7305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*nnPAbvFF0mlCo_QCQhJnvg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Test error on CIFAR-100</strong></figcaption></figure><ul class=""><li id="8251" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated">CIFAR-100 (Krizhevsky &amp; Hinton，2009 年)数据集的大小和格式与 CIFAR-10 数据集相同，但包含 100 个类，每个类的标记示例数只有后者的十分之一。</li><li id="8490" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">测试误差为 38.57%。</li></ul><h2 id="56d7" class="oi mi iq bd mj oj ok dn mn ol om dp mr lc on oo mt lg op oq mv lk or os mx ot bi translated">3.4.街景门牌号(SVHN)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/d5c5da8f0b8a503233a4b503e6a0c623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*UlTIfvnHV7qwykrj2Z_FJQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Test error on SVHN</strong></figcaption></figure><ul class=""><li id="e8ab" class="mz na iq kv b kw kx kz la lc nq lg nr lk ns lo nt nh ni nj bi translated">每幅图像的大小为 32×32，任务是对图像中心的数字进行分类。</li><li id="24e9" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">在训练集中有 73，257 个数字，在测试集中有 26，032 个数字，以及 531，131 个额外的、难度稍低的示例，用作额外的训练集。</li><li id="7bc2" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">从训练集中选择每类 400 个样本，从额外集中选择每类 200 个样本。训练和额外集合的剩余数字用于训练。</li><li id="8187" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">所使用的模型包括三个卷积 maxout 隐藏层和一个密集连接的 maxout 层，后面是一个密集连接的 softmax 层。</li><li id="2dbf" class="mz na iq kv b kw nk kz nl lc nm lg nn lk no lo nt nh ni nj bi translated">获得了 2.47%的测试误差。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><p id="6eba" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">通读这篇论文，看看作者如何利用神经网络来实现上述命题是很有趣的。在文章的最后，我们还对 Maxout 网络相对于其他激活函数(如 Tanh 或 ReLU)的消融进行了研究。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h2 id="efe4" class="oi mi iq bd mj oj ok dn mn ol om dp mr lc on oo mt lg op oq mv lk or os mx ot bi translated">参考</h2><p id="64a7" class="pw-post-body-paragraph kt ku iq kv b kw nb jr ky kz nc ju lb lc pb le lf lg pc li lj lk pd lm ln lo ij bi translated">【2013 ICML】【Maxout】<br/><a class="ae ks" href="https://arxiv.org/abs/1302.4389" rel="noopener ugc nofollow" target="_blank">Maxout 网</a></p><h2 id="c72f" class="oi mi iq bd mj oj ok dn mn ol om dp mr lc on oo mt lg op oq mv lk or os mx ot bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kt ku iq kv b kw nb jr ky kz nc ju lb lc pb le lf lg pc li lj lk pd lm ln lo ij bi translated">)(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(在)(这)(些)(事)(上)(了)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(不)(会)(想)(到)(这)(些)(事)(了)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(上)(,)(她)(们)(们)(还)(是)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(,)(她)(们)(还)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(们)(还)(没)(有)(什)(么)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">物体检测<br/></strong><a class="ae ks" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae ks" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae ks" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae ks" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae ks" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae ks" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae ks" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae ks" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae ks" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae ks" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4">G-RMI</a>][<a class="ae ks" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae ks" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3</a>[<a class="ae ks" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>[<a class="ae ks" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a>[<a class="ae ks" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a></p><p id="6582" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">语义切分<br/></strong><a class="ae ks" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae ks" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae ks" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae ks" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae ks" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae ks" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae ks" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae ks" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae ks" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae ks" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">生物医学图像分割<br/></strong>[<a class="ae ks" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae ks" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae ks" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae ks" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1">M FCN<br/></a></p><p id="3134" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">实例分割<br/> </strong> [ <a class="ae ks" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></p><p id="58de" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir"/><br/><a class="ae ks" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">【DeepPose】</a><a class="ae ks" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">【汤普森 NIPS'14】</a><a class="ae ks" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">【汤普森 CVPR'15】</a></p></div></div>    
</body>
</html>