<html>
<head>
<title>A High-Level Guide to Autoencoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器高级指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-high-level-guide-to-autoencoders-b103ccd45924?source=collection_archive---------10-----------------------#2019-08-22">https://towardsdatascience.com/a-high-level-guide-to-autoencoders-b103ccd45924?source=collection_archive---------10-----------------------#2019-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d9b7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个自动编码器工具箱，从最基本的到最花哨的。</h2></div><p id="c28d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在机器学习和人工智能的奇妙世界中，存在一种称为自动编码器的结构。自动编码器是一种神经网络，它是无监督学习(或者对某些人来说，半无监督学习)的一部分。有许多不同类型的自动编码器用于许多目的，有些是生成式的，有些是预测式的，等等。本文将为您提供一个工具箱和不同类型的自动编码器指南。</p><h1 id="50eb" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">传统自动编码器(AE)</h1><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/92987a6d2c66c5372ff0df2575a55596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldhBBbw6iFbEc5EkLAgKYg.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">The basic autoencoder</figcaption></figure><p id="29b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自动编码器的基本类型如上图所示。它由输入层(第一层)、隐藏层(黄色层)和输出层(最后一层)组成。网络的目标是输出层与输入层完全相同。隐藏图层用于特征提取或识别决定结果的特征。从第一层到隐藏层的过程称为编码。从隐藏层到输出层的过程称为解码。编码和解码的过程是自动编码器的特别之处。黄色层有时被称为瓶颈隐藏层。</p><p id="7267" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这里，有一堆不同类型的自动编码器。</p><h1 id="8c79" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">问题:隐藏图层过多</h1><p id="0f18" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">我们首先要看的是解决过度完整的隐藏层问题。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mo"><img src="../Images/e5e8d4e928fb5be86b5bc42565e26236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLmp5ozxunmdPseuyX_WuQ.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Overcomplete Hidden Layers</figcaption></figure><p id="3ec9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">需要注意的重要部分是，隐藏层比输入/输出层多。如果他们的数据中有比平常更多的特征，他们将需要这个。这样做的主要问题是输入可以不加任何改变地通过；就不会有任何真正的<em class="mp">特征的提取</em>。看看下面的例子:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mq"><img src="../Images/8a9351b999e277b432c531aeb116c41e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kNTLYQm9FlXslYVUo9QrEw.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Grey nodes are not used; the blue nodes are not at all altered.</figcaption></figure><p id="932c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输入层和输出层之间没有发生真正的变化；他们只是保持不变。此外，两个隐藏层节点根本没有被使用。为了避免这种情况，至少有三种方法:</p><h2 id="d55c" class="mr lc iq bd ld ms mt dn lh mu mv dp ll ko mw mx ln ks my mz lp kw na nb lr nc bi translated">解决方案 A:稀疏自动编码器</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nd"><img src="../Images/37597a392ddc9dd19abd6a7d78a076f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YoseSRJ0HssJor64ypBb6w.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Sparse Autoencoders</figcaption></figure><p id="6cdb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，稀疏自动编码器能够“淘汰”隐藏层中的一些神经元，迫使自动编码器使用它们所有的神经元。它不再仅仅通过某些节点来记忆输入，因为在每次运行中，这些节点可能不是活动的。</p><h2 id="b3b6" class="mr lc iq bd ld ms mt dn lh mu mv dp ll ko mw mx ln ks my mz lp kw na nb lr nc bi translated">解决方案 B:降噪自动编码器(DAE)</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ne"><img src="../Images/fa77fd50b28bcecc27908bddd600285c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BN7Gr1zby87mIMc7lzEoSA.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Some of the inputs are turned into 0.</figcaption></figure><p id="9c1c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们改变了稀疏自动编码器中的隐藏层。另一种选择是改变输入。在去噪自动编码器中，一些输入被(随机地)归零。一旦它被馈通，输出就与原始(非零)输入进行比较。这与稀疏自动编码器的目的相似，但是这一次，清零的编码器位于不同的位置。</p><h2 id="a630" class="mr lc iq bd ld ms mt dn lh mu mv dp ll ko mw mx ln ks my mz lp kw na nb lr nc bi translated">解决方案 C:收缩式自动编码器(CAE)</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nf"><img src="../Images/164a11527bca3c5e4d495695f60cb92b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dT-BbzhcdJvHJ2U64xqXEA.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">The darkened circles represent a slightly modified output.</figcaption></figure><p id="ee52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们改变了输入层，隐藏层，现在我们将改变输出层。自动编码器通过一种叫做反向传播的方法进行训练；在收缩式自动编码器中执行这种算法时，输出会略有改变，但不会完全归零(就像过去的算法一样)。反向传播时对输出层的这种改变阻止了纯粹的记忆。</p><h2 id="176e" class="mr lc iq bd ld ms mt dn lh mu mv dp ll ko mw mx ln ks my mz lp kw na nb lr nc bi translated">为什么要解决这个问题？</h2><p id="afe7" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">这些网络的一些实际应用包括标记用于分割的图像数据、对图像去噪(对此的明显选择是 DAE)、检测异常值以及填充图像中的间隙。这些应用程序中的许多还与 SAE 一起工作，这将在下面解释。</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><p id="74b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">别再提那个问题了。现在我们要提到其他更强大的自动编码器的变体。</p><h1 id="12a5" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">堆叠式自动编码器(SAE)</h1><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nn"><img src="../Images/7e6cd33adb8a2326a602d9aeacefbc9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ImIMckxBvxVSCTlc.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Source: <a class="ae no" href="https://www.researchgate.net/figure/Stacked-autoencoders-architecture_fig21_319524552" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/figure/Stacked-autoencoders-architecture_fig21_319524552</a></figcaption></figure><p id="a3fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">堆叠自动编码器开始看起来很像神经网络。本质上，SAE 是许多自动编码器与多层编码和解码放在一起。这允许算法有更多的层、更多的权重，并且很可能最终变得更健壮。</p><p id="a4de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不仅要了解 SAE，还要了解其他形式的 AEs，这些层也可以是卷积层和解卷积层；这样更便于图像处理。</p><p id="a7c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SAE 和 AEs 的一些用途通常包括分类和调整图像大小。这是自动编码器已知的特征提取工具的两个实际应用；特征提取的任何其他用途对于自动编码器都是有用的。</p><h1 id="a05d" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">可变自动编码器(VAE)</h1><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi np"><img src="../Images/56e717a3a9201c462693669620157d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rTeNcw5CDu_IsvC7.jpg"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Source: <a class="ae no" href="http://kvfrans.com/variational-autoencoders-explained/" rel="noopener ugc nofollow" target="_blank">http://kvfrans.com/variational-autoencoders-explained/</a></figcaption></figure><p id="418c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这可能是自动编码器最常用的变体:生成式。它也比其他的要复杂得多。</p><p id="89e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，VAEs 类似于 SAE，但是它们能够分离解码器。在中间，有两个向量，然后结合成一个潜在向量。这是什么意思？嗯，如果理论上从 SAE 中提取瓶颈隐藏层，并要求它生成给定随机向量的图像，很可能会产生噪声。有很多随机性，只有某些区域是提供真实图像的矢量。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nq"><img src="../Images/34c12ba0ff806e065253bad585406344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vrMbPTdno3KMTN5MJK1yJg.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">The yellow sectors are good vectors. All of the pink region will just give noise.</figcaption></figure><p id="55ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于获得图像生成向量的机会很小，均值和标准差有助于将这些黄色区域压缩成一个称为潜在空间的区域。然后，对其进行采样，以产生最终图像。潜在空间内的一切都应该产生图像。</p><p id="4f87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这里，我们可以取出编码部分，结果应该是一个生成器。</p><h1 id="b206" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">解开变分自动编码器</h1><p id="385a" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">这是 VAEs 的决胜，略有变化。这将基本上允许每个向量控制图像的一个(且仅一个)特征。</p><p id="7796" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看看下面的例子:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nr"><img src="../Images/ce580cb27dbf558fe39ad0df14e7214b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IVQUTGedYmmmjr0H0HKkuQ.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">The two to the left use disentangled VAEs, and the one to the left is a normal VAE. (Source: <a class="ae no" href="https://arxiv.org/pdf/1707.08475.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1707.08475.pdf</a>)</figcaption></figure><p id="dd93" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，在“解开”选项中，只有一个特征被更改(例如，左转、右转、距离等。).然而，在纠缠中，似乎有许多特征同时发生变化。单个变化改变单个特征的能力是解开 VAEs 的要点。</p><p id="1414" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">做到这一点的方法是向原始值添加另一个参数，该参数将考虑模型随着输入向量的每次变化而变化的程度。从那里开始，权重将相应地调整。</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><p id="17cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而且…现在就这样了。我希望你喜欢这个工具箱。我用了大量的文章和视频，都是很好的读物。如果有任何我可以改进的地方，或者如果你有任何意见或建议，我很乐意听到你的反馈。谢谢大家！</p><h2 id="a99e" class="mr lc iq bd ld ms mt dn lh mu mv dp ll ko mw mx ln ks my mz lp kw na nb lr nc bi translated">使用的来源</h2><p id="2d41" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated"><em class="mp">注:除非另有说明，所有图片均由本人设计。</em></p><div class="ns nt gp gr nu nv"><a href="https://stackoverflow.com/questions/49296951/neural-networks-difference-between-deep-autoencoder-and-stacked-autoencoder#targetText=As%20I%20understand%20it%2C%20the,greedy%2C%20layer%2Dwise%20approach." rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">神经网络——深度自动编码器和堆叠式自动编码器的区别</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">该领域的术语没有固定的、清晰的和明确的定义，不同的研究可能意味着不同的…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">stackoverflow.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj md nv"/></div></div></a></div><div class="ns nt gp gr nu nv"><a href="https://www.hindawi.com/journals/mpe/2018/5105709/" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">基于堆叠式自动编码器的深度神经网络实现齿轮箱故障诊断</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">机械故障诊断在现代制造业中至关重要，因为早期检测可以避免一些故障的发生</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">www.hindawi.com</p></div></div><div class="oe l"><div class="ok l og oh oi oe oj md nv"/></div></div></a></div><div class="ns nt gp gr nu nv"><a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">教程-什么是变分自动编码器？-贾恩·阿尔托萨尔</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">为什么深度学习研究人员和概率机器学习人员在讨论变分时会感到困惑…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">jaan.io</p></div></div></div></a></div><div class="ns nt gp gr nu nv"><a href="http://kvfrans.com/variational-autoencoders-explained/" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">解释了各种自动编码器</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">在我上一篇关于生成对立网络的文章中，我介绍了一个简单的方法来训练一个可以…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">kvfrans.com</p></div></div><div class="oe l"><div class="ol l og oh oi oe oj md nv"/></div></div></a></div><p id="b5aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">https://www.youtube.com/watch?v=9zKuYvjFFS8<a class="ae no" href="https://www.youtube.com/watch?v=9zKuYvjFFS8" rel="noopener ugc nofollow" target="_blank"/></p><p id="a617" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">【https://www.youtube.com/watch?v=fcvYpzHmhvA T4】</p><p id="deb0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae no" href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf" rel="noopener ugc nofollow" target="_blank">http://www . jmlr . org/papers/volume 11/Vincent 10a/Vincent 10a . pdf</a></p><p id="8845" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae no" href="https://arxiv.org/pdf/1707.08475.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1707.08475.pdf</a></p></div></div>    
</body>
</html>