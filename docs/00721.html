<html>
<head>
<title>Model-Free Prediction: Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无模型预测:强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-free-prediction-reinforcement-learning-507297e8e2ad?source=collection_archive---------11-----------------------#2019-02-03">https://towardsdatascience.com/model-free-prediction-reinforcement-learning-507297e8e2ad?source=collection_archive---------11-----------------------#2019-02-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="24e1" class="pw-subtitle-paragraph jo ip iq bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">第 4 部分:利用蒙特卡罗学习、时间差学习和 TD( <strong class="ak"> λ) </strong>的无模型预测</h2></div><p id="88d0" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">之前，我们研究了通过动态规划来解决已知的<em class="lc"> MDP </em>的规划。在本帖中，我们将使用无模型预测来估计未知 MDP 的价值函数。也就是说，我们将着眼于一个未知的 MDP 的政策评估。这一系列的博客文章包含了大卫·西尔弗在<a class="ae ld" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="noopener ugc nofollow" target="_blank">关于强化学习的介绍</a>中解释的概念总结。</p><p id="813d" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">零件:<a class="ae ld" rel="noopener" target="_blank" href="/reinforcement-learning-an-introduction-to-the-concepts-applications-and-code-ced6fbfd882d">1</a><a class="ae ld" rel="noopener" target="_blank" href="/getting-started-with-markov-decision-processes-reinforcement-learning-ada7b4572ffb">2</a><a class="ae ld" rel="noopener" target="_blank" href="/planning-by-dynamic-programming-reinforcement-learning-ed4924bbaa4c">3【T114】…</a></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/b314d8a6d89a240b7faf99601870744d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KJHmpFoppnX5FIWG"/></div></div></figure><p id="fd61" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">将对无模型预测<strong class="ki ir"> <em class="lc"> </em> </strong>的三种主要方法进行说明:</p><ul class=""><li id="c393" class="lq lr iq ki b kj kk km kn kp ls kt lt kx lu lb lv lw lx ly bi translated">蒙特卡罗学习</li><li id="3500" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb lv lw lx ly bi translated">时差学习</li><li id="557d" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb lv lw lx ly bi translated">TD(λ)</li></ul><p id="7e10" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这篇文章主要着眼于在未知的 MDP 中评估一个给定的政策，而不是寻找最优政策。</p><h1 id="ef04" class="me mf iq bd mg mh mi mj mk ml mm mn mo jx mp jy mq ka mr kb ms kd mt ke mu mv bi translated">蒙特卡罗学习</h1><p id="125b" class="pw-post-body-paragraph kg kh iq ki b kj mw js kl km mx jv ko kp my kr ks kt mz kv kw kx na kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">蒙特卡洛</em> </strong>方法<strong class="ki ir"> <em class="lc"> </em> </strong>都是直接从剧集经验中学习的<strong class="ki ir"> <em class="lc">无模型</em> </strong>。蒙特卡洛从<em class="lc">全集</em>中学习<em class="lc">无引导。MC 的一个缺点是它只能应用于所有情节都必须终止的情节马尔可夫决策过程。</em></p><blockquote class="nb nc nd"><p id="c658" class="kg kh lc ki b kj kk js kl km kn jv ko ne kq kr ks nf ku kv kw ng ky kz la lb ij bi translated"><strong class="ki ir"> <em class="iq">无模型:</em> </strong>对 MDP 转场/奖励一无所知<br/> <strong class="ki ir"> <em class="iq">自举</em> </strong> <em class="iq"> : </em>更新涉及到一个估计</p></blockquote><h2 id="86e8" class="nh mf iq bd mg ni nj dn mk nk nl dp mo kp nm nn mq kt no np ms kx nq nr mu ns bi translated">蒙特卡洛政策评估</h2><p id="8744" class="pw-post-body-paragraph kg kh iq ki b kj mw js kl km mx jv ko kp my kr ks kt mz kv kw kx na kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">目标</em> </strong>:给定一个策略<em class="lc"> π，</em>从几集经验中学习<em class="lc"> v_π(策略值)</em>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7b4a8c759c18e6e562e28b1ea816efb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*LEYsZ_h2FcAMXNaui8A2Qg.jpeg"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Given policy <em class="jn">π with each state, action and associated reward for taking that action</em></figcaption></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/75ae9e0ed73f84192773bc279516e4bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*L4DrShkGC682FW9arfzpUg.jpeg"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Recall: return is the total discounted reward</figcaption></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c4d3b6981eaef7a2da318df7bf1f8a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*7aH-6kJyFVu2kA0tDIPJqQ.jpeg"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Recall: value function is the expected return</figcaption></figure><p id="eafa" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">蒙特卡罗策略评估</em> </strong>用<em class="lc">经验均值收益</em>代替预期收益。评估一个状态下策略的价值函数的两种方法是使用<strong class="ki ir"> <em class="lc">首次访问蒙特卡罗策略评估</em> </strong>或<strong class="ki ir"> <em class="lc">每次访问蒙特卡罗策略评估。</em> </strong></p><p id="dfc3" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">首次访问蒙特卡洛政策评估</em> </strong></p><ol class=""><li id="cf1a" class="lq lr iq ki b kj kk km kn kp ls kt lt kx lu lb oa lw lx ly bi translated">评估给定策略的状态值</li><li id="a116" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb oa lw lx ly bi translated">第一个<strong class="ki ir"><em class="lc"/></strong>时步(<em class="lc"> t) </em>那个状态(<em class="lc"> s) </em>在一集里被访问</li><li id="cfaf" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb oa lw lx ly bi translated">增量计数器:<em class="lc"> N(s) ← N(s) + 1 </em></li><li id="6fd7" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb oa lw lx ly bi translated">增量总回报:<em class="lc"> S(s) ← S(s) + Gₜ </em></li><li id="ea05" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb oa lw lx ly bi translated">价值由平均收益估计:<em class="lc"> V(s) = S(s)/N(s) </em></li><li id="1815" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb oa lw lx ly bi translated"><em class="lc"> V(s) → v_π(s) </em>为<em class="lc"> N(s) → ∞ </em></li></ol><p id="48d5" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"><em class="lc"/></strong>蒙地卡罗政策评估</p><ol class=""><li id="84e4" class="lq lr iq ki b kj kk km kn kp ls kt lt kx lu lb oa lw lx ly bi translated">评估给定策略的状态值</li><li id="2356" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb oa lw lx ly bi translated"><strong class="ki ir"> <em class="lc">每一个</em></strong><em class="lc">t)</em>那个状态(<em class="lc"> s) </em>都是在一个情节中被访问</li><li id="b0a6" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb oa lw lx ly bi translated">增量计数器:<em class="lc"> N(s) ← N(s) + 1 </em></li><li id="617d" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb oa lw lx ly bi translated">增量总回报:<em class="lc"> S(s) ← S(s) + Gₜ </em></li><li id="dd94" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb oa lw lx ly bi translated">价值由平均收益估计:<em class="lc"> V(s) = S(s)/N(s) </em></li><li id="d699" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb oa lw lx ly bi translated"><em class="lc"> V(s) → v_π(s) </em>为<em class="lc"> N(s) → ∞ </em></li></ol><p id="3c1f" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在上述两种评估方法中，我们必须跟踪算法的统计数据。也就是说，我们只能在完成所有剧集后计算其价值。为了解决这个问题，我们可以使用<strong class="ki ir"> <em class="lc">增量均值</em> </strong>等式来增量更新该值。</p><blockquote class="nb nc nd"><p id="0885" class="kg kh lc ki b kj kk js kl km kn jv ko ne kq kr ks nf ku kv kw ng ky kz la lb ij bi translated"><strong class="ki ir">增量均值<br/> </strong>一个序列的均值₁，₂，…x₁，x₂，…可以增量计算。</p></blockquote><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/015fa9ac0538608d087aade7c317707f.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*fYQdwcQ1c_fzzby4hNKn1w.jpeg"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Incremental Mean</figcaption></figure><p id="cb68" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">增量蒙特卡罗更新<br/> </em> </strong>更新<em class="lc"> V(s) </em>增量后集<em class="lc"> S₁，A₁，R₂，…，Sₜ.</em>为每个状态<em class="lc"> Sₜ </em>带回车<em class="lc"> Gₜ : </em></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a4002ff8ebd480965174699e863aad87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*SyUV5mnNpX3lYRlPysCJ2w.jpeg"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Replace step 3–5 with the above. <strong class="bd od">(G<em class="jn">ₜ</em> − V(S<em class="jn">ₜ</em>)) </strong>can be viewed as the error between the return and the mean at time step <em class="jn">t</em></figcaption></figure><p id="8d14" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在非平稳问题中(事物四处漂移，你不需要记住很久以前发生的事情)，我们可以使用移动平均方法，即忘记旧的情节。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/be3136ac996425c4a763741a0b93fd3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*-omqhY4fmfHGB6C_VH7wuw.jpeg"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Incremental Monte-Carlo updates</figcaption></figure><h1 id="f9ab" class="me mf iq bd mg mh mi mj mk ml mm mn mo jx mp jy mq ka mr kb ms kd mt ke mu mv bi translated">时差学习</h1><p id="af95" class="pw-post-body-paragraph kg kh iq ki b kj mw js kl km mx jv ko kp my kr ks kt mz kv kw kx na kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">时间差</em> </strong>是<em class="lc">无模型的。</em>时间差分法直接从经验/与环境的相互作用中学习。时态差从不完整的剧集中学习，通过<strong class="ki ir"> <em class="lc">自举</em> </strong>(更新值函数的猜测)<strong class="ki ir">。</strong></p><p id="b5fa" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在 MC 和 TD 中，目标都是从政策<em class="lc"> π </em>下的经验中在线学习<em class="lc"> v_π </em>。<br/>如果我们要应用<strong class="ki ir"> <em class="lc">增量每次访问蒙特卡罗</em> </strong>我们就朝着<strong class="ki ir">实际</strong>返回<strong class="ki ir"> <em class="lc"> Gₜ </em> </strong>更新值<em class="lc"> V(Sₜ】</em></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/be3136ac996425c4a763741a0b93fd3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*-omqhY4fmfHGB6C_VH7wuw.jpeg"/></div></figure><p id="fd81" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">最简单的时差学习算法，<strong class="ki ir"><em class="lc">TD(0)</em></strong><em class="lc"/>随着我们更新值<em class="lc"> V(Sₜ) </em>趋向于一个<strong class="ki ir">估计值</strong>返回<strong class="ki ir"><em class="lc">rₜ₊₁+γv(sₜ₊₁)</em></strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi of"><img src="../Images/d59cd236921f3395ca0592ba8749d9ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*FBkFD0DikIFa9Qwc2xsxTA.jpeg"/></div></figure><p id="0d00" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><em class="lc">【rₜ₊₁+γv(sₜ₊₁】)</em>是<em class="lc"> </em> <strong class="ki ir"> <em class="lc"> TD 目标</em></strong><em class="lc">【δₜ=rₜ₊₁+γv(sₜ₊₁)-v(sₜ】)</em>是<em class="lc"> </em> <strong class="ki ir"> <em class="lc"> TD 误差</em> </strong> <em class="lc">。</em></p><p id="ec6b" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">TD 学习<em class="lc">立即更新值函数</em>，这允许它在知道每一步之后的最终结果 之前学习<strong class="ki ir"> <em class="lc">，不像 MC 必须等到剧集结束之后才知道返回。TD 工作在<strong class="ki ir"> <em class="lc">持续(非终止)环境</em> </strong>中，而 MC 只工作在阶段性(终止)环境/完整序列中。</em></strong></p><p id="0d91" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">举例说明 TD 和 MC 之间的区别，如果我们试图预测在沿途的每个州开车回家需要多长时间。在<strong class="ki ir"> MC </strong>中，我们会给每个状态分配我们在旅程结束时得到的值(实际结果)。<br/>在<strong class="ki ir"> TD </strong>中，我们将使用下一个状态对当前状态的影响(估计结果)来更新每个状态的值。</p><p id="dd66" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在偏差和方差之间有一个权衡。<strong class="ki ir"> MC </strong>使用<strong class="ki ir">返回<em class="lc"> Gₜ </em> </strong>时有<em class="lc">高方差</em>和<em class="lc">零偏</em>，这依赖于<em class="lc">的许多随机动作</em>、<em class="lc">过渡</em>和<em class="lc">奖励</em>。因此，即使在函数逼近的情况下，它也具有良好的收敛性，并且对初值不敏感。</p><p id="14be" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> TD </strong>有<em class="lc">低方差</em>和<em class="lc">部分偏差为</em><strong class="ki ir"><em class="lc">TD 目标</em> </strong> <em class="lc"> </em>取决于<em class="lc">一个随机动作，跃迁</em>和<em class="lc">奖励。</em>通常比 MC 更有效率。<em class="lc"> TD(0) </em>收敛于<em class="lc"> v_π(s) </em>但不总是用函数逼近。与 MC 不同，它对初始值更敏感。</p><h2 id="5e19" class="nh mf iq bd mg ni nj dn mk nk nl dp mo kp nm nn mq kt no np ms kx nq nr mu ns bi translated"><strong class="ak"> <em class="jn">批量 MC 和</em>TD</strong></h2><p id="3a63" class="pw-post-body-paragraph kg kh iq ki b kj mw js kl km mx jv ko kp my kr ks kt mz kv kw kx na kz la lb ij bi translated">所以我们看到 MC 和 TD 收敛:<em class="lc"> V(s) → v_π(s) </em>作为经验<em class="lc"> → ∞ <br/> </em>但是实际上我们不可能永远进行下去，那么这些算法对于有限经验的批量求解是如何收敛的呢？</p><p id="0587" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">假设我们有两个状态<em class="lc"> A，B </em>有<em class="lc">无贴现</em>和<em class="lc"> 8 </em>集经验。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f492823bfc84cd78669db2e874c5e634.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*KizivSRCjGUcw2GqWiRMuA.jpeg"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">AB Example</figcaption></figure><p id="035f" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">状态<em class="lc"> A </em>的值是多少。<em class="lc"> V(一)</em> <em class="lc">？<br/> </em> <strong class="ki ir"> MC </strong>收敛于<em class="lc">以最小均方误差最佳拟合观察回报</em>的解。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a6b81205337ab8472dc517e1e9628364.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*zu1Ik4wXY7xOhSU6WwjXfA.jpeg"/></div></figure><p id="b8ce" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此<em class="lc"> V(A)=0。</em>因为状态<em class="lc"> A </em>唯一一次出现在一集里是在 return 为 0 的时候。</p><p id="3139" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> TD(0) </strong>收敛到<em class="lc">最大似然马尔可夫模型的解。</em>这是最符合数据的 MDP 解决方案。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/fef015b9c363da1278b9ccfb97f7d3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*4axAwYjZrQeOLz3c-B0XYw.jpeg"/></div></figure><p id="f98b" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此<em class="lc"> V(A)=0.75 </em>。因为我们在 8 集里得到了 6 集的奖励。与 MC 不同，TD 利用了马尔可夫特性。</p><h2 id="82de" class="nh mf iq bd mg ni nj dn mk nk nl dp mo kp nm nn mq kt no np ms kx nq nr mu ns bi translated">备份方法之间的比较</h2><p id="818f" class="pw-post-body-paragraph kg kh iq ki b kj mw js kl km mx jv ko kp my kr ks kt mz kv kw kx na kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">蒙特卡罗备份:<br/> </em> </strong>状态的值<em class="lc"> Sₜ </em>只能在到达终端状态时计算</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/be3136ac996425c4a763741a0b93fd3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*-omqhY4fmfHGB6C_VH7wuw.jpeg"/></div></figure><p id="6f74" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">时间差 TD(0)备份:</em> </strong> <br/>仅使用一步前瞻来计算状态值<em class="lc"> Sₜ </em>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi of"><img src="../Images/d59cd236921f3395ca0592ba8749d9ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*FBkFD0DikIFa9Qwc2xsxTA.jpeg"/></div></figure><p id="ef1a" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">动态编程备份:<br/> </em> </strong>在<em class="lc"> Sₜ </em>处的值通过一步查看每个可能的状态来计算，并计算出期望值。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b7ed64b403de876e0acf7d318e2c9f68.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*CdTNNYkhMc_0DATGTH3_pQ.jpeg"/></div></figure><p id="f12d" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc"> n 步返回<br/> </em> </strong>一种介于 TD(0)和 MC 之间的方法，这里我们有 n 步时差学习。因此，该值将通过向前看 n 步并应用时间差学习方法来计算。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/8e8253b44882daf30f97a332411a44f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*eqi9yOPIG-wpWcmJK1zdAw.jpeg"/></div></figure><h1 id="4951" class="me mf iq bd mg mh mi mj mk ml mm mn mo jx mp jy mq ka mr kb ms kd mt ke mu mv bi translated">TD(λ)</h1><p id="29f2" class="pw-post-body-paragraph kg kh iq ki b kj mw js kl km mx jv ko kp my kr ks kt mz kv kw kx na kz la lb ij bi translated">我们可以不用查看每个 n 步回报<em class="lc"> Gₜ⁽ⁿ⁾ </em>，而是使用一个衰减加权和来组合所有 n 步回报，称为<strong class="ki ir"><em class="lc">λ-回报</em> </strong>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a9914dceb6bb2380c1672ca1056446fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*swfHh7IC9YbdPPh4D30KPQ.jpeg"/></div></figure><h2 id="03fe" class="nh mf iq bd mg ni nj dn mk nk nl dp mo kp nm nn mq kt no np ms kx nq nr mu ns bi translated"><strong class="ak"> <em class="jn">前视 TD(λ) </em> </strong></h2><p id="6ba5" class="pw-post-body-paragraph kg kh iq ki b kj mw js kl km mx jv ko kp my kr ks kt mz kv kw kx na kz la lb ij bi translated">现在可以使用<strong class="ki ir"> <em class="lc">前视 TD(λ) </em> </strong>计算状态下的值</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b56b7f780c011959889f91e34736a2b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*Nd_aYgVFkPoIanCYns_yxg.jpeg"/></div></figure><p id="f580" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">前视着眼于未来以计算λ回报，并向其更新价值函数，并且只能从完整的剧集中计算。</p><h2 id="cd96" class="nh mf iq bd mg ni nj dn mk nk nl dp mo kp nm nn mq kt no np ms kx nq nr mu ns bi translated">后视 TD(λ)</h2><p id="d243" class="pw-post-body-paragraph kg kh iq ki b kj mw js kl km mx jv ko kp my kr ks kt mz kv kw kx na kz la lb ij bi translated">后向视图提供了从不完整序列在线更新每一步值的机制。我们为每个状态 s 保留一个<strong class="ki ir"> <em class="lc">合格轨迹</em> </strong>，并与<em class="lc"> TD 误差</em> <em class="lc"> δₜ </em>和<em class="lc">合格轨迹 eₜ(s】</em>成比例地为每个状态<em class="lc"> s </em>更新<em class="lc"> V(s) </em>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4b5f9edd67e7720ea4ab562d01dd6c33.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*LxY_-LgN6FcuJs3NgqULGw.jpeg"/></div></figure><blockquote class="nb nc nd"><p id="079b" class="kg kh lc ki b kj kk js kl km kn jv ko ne kq kr ks nf ku kv kw ng ky kz la lb ij bi translated"><strong class="ki ir">合格追踪</strong> <br/>合格追踪结合了<strong class="ki ir">频率启发式</strong>和<strong class="ki ir">新近启发式</strong>。<br/> - <strong class="ki ir">频率启发式</strong>:将信用分配给最频繁的状态<br/> - <strong class="ki ir">最近启发式</strong>:将信用分配给最近的状态</p></blockquote><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/7f842c4177b580c20c0318627a0ec5b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*DaKcgWwekjnXzPJsVHpDxw.jpeg"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eligibility Trace Equations</figcaption></figure><h1 id="b8d4" class="me mf iq bd mg mh mi mj mk ml mm mn mo jx mp jy mq ka mr kb ms kd mt ke mu mv bi translated">摘要</h1><p id="0d83" class="pw-post-body-paragraph kg kh iq ki b kj mw js kl km mx jv ko kp my kr ks kt mz kv kw kx na kz la lb ij bi translated">我们已经研究了各种用于无模型预测的方法<strong class="ki ir"> <em class="lc"> </em> </strong>，例如蒙特卡罗学习、时间差学习和 TD(λ)。当给定一个策略时，这些方法允许我们找到一个状态的值。在下一篇文章中，我们将使用无模型方法寻找最优策略。</p><h1 id="a1e6" class="me mf iq bd mg mh mi mj mk ml mm mn mo jx mp jy mq ka mr kb ms kd mt ke mu mv bi translated">参考</h1><ul class=""><li id="7a30" class="lq lr iq ki b kj mw km mx kp op kt oq kx or lb lv lw lx ly bi translated"><a class="ae ld" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf" rel="noopener ugc nofollow" target="_blank">关于 RL 的 UCL 课程——第 4 讲</a></li><li id="c033" class="lq lr iq ki b kj lz km ma kp mb kt mc kx md lb lv lw lx ly bi translated">《强化学习导论》，萨顿和巴尔托，1998 年</li></ul></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><p id="f3dd" class="pw-post-body-paragraph kg kh iq ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如果你喜欢这篇文章，并想看到更多，不要忘记关注和/或留下掌声。</p></div></div>    
</body>
</html>