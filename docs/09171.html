<html>
<head>
<title>Regression — Why Mean Square Error?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归——为什么是均方差？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/https-medium-com-chayankathuria-regression-why-mean-square-error-a8cad2a1c96f?source=collection_archive---------1-----------------------#2019-12-05">https://towardsdatascience.com/https-medium-com-chayankathuria-regression-why-mean-square-error-a8cad2a1c96f?source=collection_archive---------1-----------------------#2019-12-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/82c65d96e11aadda2a0087bb39dfb3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JqMXRx_n2iHDLkPU"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@edgr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edu Grande</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="kd"><p id="104e" class="ke kf iq bd kg kh ki kj kk kl km kn dk translated">我如何知道我的算法的正确损失函数是什么？因为如果我选择了错误的损失函数，我会得到错误的解。</p></blockquote></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><p id="85a7" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>机器学习中，我们的主要目标是最小化由损失函数定义的误差。每种算法都有不同的误差测量方法。在这篇文章中，我将介绍回归算法中使用的一些基本损失函数，以及它们为什么会这样。我们开始吧。</p><p id="c5b1" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">假设我们有两个损失函数。两个函数将具有不同的最小值。因此，如果你优化了错误的损失函数，你就会得到错误的解，这就是我的损失函数中的最优点或权重的优化值。或者我们可以说我们在解决错误的优化问题。因此，我们需要找到适当的损失函数，我们将最小化。</p><h2 id="0227" class="mb mc iq bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">1.误差总和(标准误差):</h2><p id="0922" class="pw-post-body-paragraph kv kw iq kx b ky mu la lb lc mv le lf lg mw li lj lk mx lm ln lo my lq lr kn ij bi translated">让我们从考虑最基本的损失函数开始，它只不过是每次迭代中的误差之和。误差将是预测值和实际值之间的差异。因此，损失函数将由下式给出:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/3fb52a8e07212599e4d611d00f927e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*b6_10TrW8tGDdxM9IRLlfQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Ŷ is the predicted value; Y is the actual value</figcaption></figure><p id="5fc4" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">其中求和从 n=1 到 N，N 是数据集中实例的数量。现在考虑下面这条符合我们 3 个数据点的线:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/26ab216baf0094ae6cb6c9047db51af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*ibIXV393YaydRr-__m41Pg.png"/></div></figure><p id="9709" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">当然不是你可能会说的最佳拟合线！但是根据这个损失函数，这条线是最佳拟合线，因为误差几乎为 0。对于点 3，由于预测值较低，误差为负。而对于点 1，误差是正的，并且大小几乎相同。对于点 2，它是 0。将所有这些相加将导致总误差为 0！但错误肯定不止于此。如果误差为 0，那么算法将假设它已经收敛，而实际上它并没有收敛——<em class="nf">，并且将提前退出</em>。它将显示一个非常小的误差值，而实际上该值要大得多。所以你怎么能说这是错误的路线呢？你真的不能。你只是选错了损失函数。</p><h2 id="51a9" class="mb mc iq bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">2.绝对误差之和(SAE):</h2><p id="fc68" class="pw-post-body-paragraph kv kw iq kx b ky mu la lb lc mv le lf lg mw li lj lk mx lm ln lo my lq lr kn ij bi translated">SE 肯定不是我们想要使用的损失函数。所以让我们稍微改变一下来克服它的缺点。让我们只取所有迭代的误差绝对值。这应该能解决问题..对吗？还是没有？这是损失函数的样子:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/da846e3d521bf8dc80c79a7b167216e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*wkXQQKLasbX-EKx8Jq9XhA.png"/></div></figure><p id="4bc9" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">所以现在误差项不会相互抵消，实际上会相加。这个函数有什么潜在的问题吗？嗯，是的。这个损失函数不是<a class="ae kc" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-4/v/differentiability" rel="noopener ugc nofollow" target="_blank"> <em class="nf">在 0 处可微</em> </a> <em class="nf"> </em>。损失函数的图形将是:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/e76ee779f555b12d293c54b8b342ffb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*tHeOi51cGuvQb9oK6ICpGQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Y axis is the loss function</figcaption></figure><p id="f322" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">导数<em class="nf">在 0 处将不存在</em>。我们需要对函数求导，并使其等于 0，以找到最佳点。这在这里是不可能的。我们无法解决这个问题。</p><h2 id="6af7" class="mb mc iq bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">3.误差平方和(SSE):</h2><p id="b5e5" class="pw-post-body-paragraph kv kw iq kx b ky mu la lb lc mv le lf lg mw li lj lk mx lm ln lo my lq lr kn ij bi translated">所以让我们用平方代替绝对值。损失函数现在将变成:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d8f54d01a2f2169462d258c17f2de373.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*KSPNQrfzvtF00c9qHS5TsQ.png"/></div></figure><p id="b3b1" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">它在所有点上都是可微的，并且给出非负误差。但你可能会说，为什么我们不能达到更高阶，比如 4 阶左右。那么，如果我们考虑采用 4 阶损失函数，结果会是怎样:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/8ba042f9b7a9b16b6984b615c78d53c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*3-53lIJJaxz_5lp97M2gWw.png"/></div></figure><p id="95fd" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">因此它的<em class="nf">梯度</em>将在 3 点消失。所以它也会有局部最小值——这不是我们的最优解。我们需要找到全局最小值的点来找到最优解。所以让我们坚持正方形本身。</p><p id="998a" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated"><strong class="kx ir"> 4。均方差(MSE): </strong></p><p id="edb8" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">现在考虑我们用 SSE 作为损失函数。因此，如果我们有一个 100 点的数据集，我们的上证指数就是 200。如果我们将数据点增加到 500，我们的 SSE 将会增加，因为平方误差现在将增加到 500 个数据点。所以假设它变成了 800。如果我们再增加数据点的数量，我们的 SSE 会进一步增加。够公平吗？绝对不行！</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/77318808d018a0f0c97d39bdd6032d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*hcRa5Mg9Yee0MBtagXAqFg.png"/></div></figure><p id="ffe6" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">误差应该随着我们样本数据的增加而减少，因为我们数据的分布越来越窄(指正态分布)。我们的数据越多，误差就越小。但在上交所，情况完全相反。最后，我们的勇士——均方差出场了。它的表述是:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/862b48add911f0cc299792926b6404ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*zfMHgFXHk2X2nNa8idN_Hg.png"/></div></figure><p id="2d69" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">我们取上证指数的平均值。因此，数据越多，总误差 MSE 就越小。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0b424f576dfe12af3a857d16d4d34524.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*UijhqE3WkAoV7EZy9aMRzQ.png"/></div></figure><p id="abd7" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">这里你可以看到，随着我们的算法获得越来越多的经验，误差在减少。无论是 R、Python 还是 MATLAB，均方误差都被用作评估大多数回归算法性能的默认指标。</p><p id="214c" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated"><strong class="kx ir"> 5。均方根误差(RMSE): </strong></p><p id="a028" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">MSE 的唯一问题是丢失的次数多于数据的次数。由于我的数据是 1 阶的，损失函数的 MSE 是 2 阶的。所以我们不能直接把数据和误差联系起来。因此，我们取 MSE 的根，即均方根误差:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/327622d64e77c601979832d488cdd8d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*u9H5TCCnUL8h3hHsEc0SxQ.png"/></div></figure><p id="acfb" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">这里，我们不改变损失函数，解决方案仍然是相同的。我们所做的就是通过求根来降低损失函数的阶数。</p><h2 id="f798" class="mb mc iq bd md me mf dn mg mh mi dp mj lg mk ml mm lk mn mo mp lo mq mr ms mt bi translated">6.胡伯损失:</h2><p id="7f79" class="pw-post-body-paragraph kv kw iq kx b ky mu la lb lc mv le lf lg mw li lj lk mx lm ln lo my lq lr kn ij bi translated">Huber 损失结合了 MSE 和 MAE(平均绝对误差)的最佳特性。对于较小的误差，它是二次的，否则是线性的(对于它的梯度也是类似的)。它由它的<em class="nf">变量</em>参数识别:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e411e0a038e9acc923fbf9f88f462f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*hhDmQQQd_nHVXnc4OK-Lug.png"/></div></figure><p id="6c0f" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated"><a class="ae kc" href="https://en.wikipedia.org/wiki/Huber_loss" rel="noopener ugc nofollow" target="_blank"> Huber loss </a>比 MSE 对数据中的异常值更不敏感或更稳健。它在 0 也是可微的。基本上是绝对误差，误差小了就变成二次了。误差必须小到二次，这取决于一个可以调整的超参数𝛿(δ)。当𝛿 ~ 0 时，胡伯损失接近 MAE，当𝛿 ~ ∞(大数)时，接近 MSE。)</p></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><p id="66b4" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr kn ij bi translated">这就是本文的全部内容。一定要看看 Neptune 上这个令人惊叹的博客，了解关于<a class="ae kc" href="https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide" rel="noopener ugc nofollow" target="_blank">性能指标</a>的更多细节。请在下面评论你的想法！</p></div></div>    
</body>
</html>