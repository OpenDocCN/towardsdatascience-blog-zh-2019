<html>
<head>
<title>An Example of Hyperparameter Optimization on XGBoost, LightGBM and CatBoost using Hyperopt</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Hyperopt 在 XGBoost、LightGBM 和 CatBoost 上优化超参数的示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e?source=collection_archive---------1-----------------------#2019-08-01">https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e?source=collection_archive---------1-----------------------#2019-08-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="83db" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">额外奖励:Hyperopt-Sklearn</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b2de75e6aeb8b5cdc3f79a0be835e7d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8QISTMyQ2VkeagBDZvVqWA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: Pexels</figcaption></figure><p id="fc67" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我保证，这次会比上一次更高级:</p><div class="lu lv gp gr lw lx"><a rel="noopener follow" target="_blank" href="/a-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd iu gy z fp mc fr fs md fu fw is bi translated">配对交易中强化学习的温和实现</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">TensorFlow 中结构化编程的一个例子</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">towardsdatascience.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml ks lx"/></div></div></a></div><h1 id="fc28" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">介绍</h1><h2 id="bee7" class="ne mn it bd mo nf ng dn ms nh ni dp mw lh nj nk my ll nl nm na lp nn no nc np bi translated">梯度推进决策树(GBDT)</h2><p id="5d3d" class="pw-post-body-paragraph ky kz it la b lb nq ju ld le nr jx lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated"><strong class="la iu">梯度推进</strong>是<strong class="la iu">决策树</strong>上的附加训练技术。<em class="nw"> XGBoost </em>  <em class="nw"> </em>的<a class="ae nv" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank">官方页面给出了非常清晰的概念解释。基本上，不是运行静态的单个决策树或随机森林，而是迭代地添加新的树<strong class="la iu"/>直到无法实现进一步的改进。除了正则化之外，集合技术对于防止过拟合是至关重要的。尽管该模型可能非常强大，但仍有许多超参数需要微调。</a></p><h2 id="a10e" class="ne mn it bd mo nf ng dn ms nh ni dp mw lh nj nk my ll nl nm na lp nn no nc np bi translated">XGBoost、LightGBM 和 CatBoost</h2><p id="b12c" class="pw-post-body-paragraph ky kz it la b lb nq ju ld le nr jx lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">这些是众所周知的梯度增强包。与通过遍历所有要素找到最佳分割的传统 GBDT 方法相比，这些包实现了基于直方图的方法，将要素分组到条柱中，并在条柱级别而不是要素级别执行分割。另一方面，他们也倾向于忽略稀疏输入。这些显著提高了它们的计算速度(更多细节见<a class="ae nv" href="http://mlexplained.com/2018/01/05/lightgbm-and-xgboost-explained/" rel="noopener ugc nofollow" target="_blank">此处</a>)。需要注意的几个关键点:</p><p id="d0db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> XGBoost </strong>:著名的 Kaggle 中奖包。树的生长是基于逐层的树修剪(树在一层的所有节点上生长),使用来自分裂的<a class="ae nv" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html#learn-the-tree-structure" rel="noopener ugc nofollow" target="_blank">信息增益</a>,为此需要对样本进行预分类，以便在每一步中计算所有可能分裂的最佳分数，因此比较耗时。</p><p id="afbe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">等级和树叶(树从特定的树叶开始生长)训练都是可用的。它允许用户选择一种叫做<strong class="la iu">基于梯度的单侧采样(GOSS) </strong>的方法，这种方法根据最大梯度和一些具有较小梯度的随机样本来分割样本。背后的假设是梯度越小的数据点训练得越好。另一个关键算法是<strong class="la iu">专有特征捆绑(EFB) </strong>，它研究特征的稀疏性，并将多个特征组合成一个，而不会丢失任何信息，因为它们在一起决不非零。这些使得 LightGBM 比 XGBoost 更快。</p><p id="f62c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> CatBoost </strong>:专为分类数据训练设计，也适用于回归任务。GPU 上的速度号称是这些库中<a class="ae nv" href="https://github.com/catboost/catboost/issues/505" rel="noopener ugc nofollow" target="_blank">最快的</a>。在<a class="ae nv" href="https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html#algorithm-main-stages_cat-to-numberic" rel="noopener ugc nofollow" target="_blank">中将分类特征转换为数字</a>中有多种方法。其速度的关键与两个 Os 挂钩:<a class="ae nv" href="https://www.quora.com/Lets-say-that-I-want-to-build-a-decision-tree-ensemble-that-can-make-predictions-very-quickly-Is-there-a-library-that-can-help-me-build-such-an-efficient-ensemble" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">遗忘树</strong> </a>和<strong class="la iu">有序推进</strong>。<em class="nw">不经意树</em>是指采用对称二进制分裂的逐层树构建(即每层上的每片叶子都被一个特征分裂)，而<em class="nw">有序提升</em>应用排列和顺序目标编码来转换分类特征。更多详情请参见此处<a class="ae nv" href="https://catboost.ai/news/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus" rel="noopener ugc nofollow" target="_blank">此处</a>和<a class="ae nv" rel="noopener" target="_blank" href="/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2">此处</a>。</p><h2 id="455a" class="ne mn it bd mo nf ng dn ms nh ni dp mw lh nj nk my ll nl nm na lp nn no nc np bi translated">贝叶斯优化</h2><p id="a56c" class="pw-post-body-paragraph ky kz it la b lb nq ju ld le nr jx lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">与作为强力方法的 GridSearch <strong class="la iu"> </strong>或纯粹随机的 RandomSearch 相比，<em class="nw">经典</em> <strong class="la iu">贝叶斯优化</strong>通过高斯过程逼近目标函数(即随机样本被迭代地抽取(基于序列模型的优化(SMBO))并且样本之间的函数输出被置信区域逼近)，在搜索最优参数时结合了随机性和后验概率分布。将在置信区域的高均值和方差处从参数空间中提取新样本，用于勘探和开发。查看此以获得更多解释。</p><h2 id="7f87" class="ne mn it bd mo nf ng dn ms nh ni dp mw lh nj nk my ll nl nm na lp nn no nc np bi translated">远视</h2><p id="21c5" class="pw-post-body-paragraph ky kz it la b lb nq ju ld le nr jx lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">Hyperopt 是一个用于搜索空间优化的 python 库。目前它提供了两种优化算法:<strong class="la iu"> 1。随机搜索</strong>和<strong class="la iu"> 2。Parzen 估计器树(TPE </strong>)，这是一种贝叶斯方法，它使用<em class="nw"> P(x|y) </em>而不是<em class="nw"> P(y|x) </em>，在计算预期改善时，它基于对由阈值而不是一个阈值分隔的两个不同分布的近似(参见<a class="ae nv" rel="noopener" target="_blank" href="/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">和</a>)。它曾经适应高斯过程和回归树，但现在这些不再被实现。</p><div class="kj kk kl km gt ab cb"><figure class="nx kn ny nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/9d1ee4ec8d87443c5bb06f2adaf50324.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*oUMDBugjwi3K31I-v2Yg_A.png"/></div></figure><figure class="nx kn od nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/5e039ff7125d0d5f58db2ee6a1df9af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*5gY5IdU6PO4JCqQoEDtdMA.png"/></div></figure></div><div class="ab cb"><figure class="nx kn oe nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/74a4faaf52081d5720288e22e47def0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*1_PdcS7tBUi2LTDkzeLRuQ.png"/></div></figure><figure class="nx kn of nz oa ob oc paragraph-image"><img src="../Images/20ede6fc0c973087e798c20cabf8f757.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*NRQSqkJdSfuw_KsI0Kfk2A.jpeg"/><figcaption class="ku kv gj gh gi kw kx bd b be z dk og di oh oi">UL: <a class="ae nv" href="https://devblogs.nvidia.com/catboost-fast-gradient-boosting-decision-trees/" rel="noopener ugc nofollow" target="_blank">Feature binning</a>; UR: <a class="ae nv" href="https://rohitgr7.github.io/" rel="noopener ugc nofollow" target="_blank">Tree growing</a>; BL: <a class="ae nv" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">Bayesian Optimizing</a> ; BR: <a class="ae nv" href="https://www.kaggle.com/c/home-credit-default-risk/discussion/59806" rel="noopener ugc nofollow" target="_blank">Gradient-based One-Side Sampling</a></figcaption></figure></div><h1 id="6347" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">履行</h1><h2 id="46e3" class="ne mn it bd mo nf ng dn ms nh ni dp mw lh nj nk my ll nl nm na lp nn no nc np bi translated">装置</h2><p id="a2ab" class="pw-post-body-paragraph ky kz it la b lb nq ju ld le nr jx lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">查看以下安装指南:</p><ul class=""><li id="7531" class="oj ok it la b lb lc le lf lh ol ll om lp on lt oo op oq or bi translated"><a class="ae nv" href="https://xgboost.readthedocs.io/en/latest/build.html" rel="noopener ugc nofollow" target="_blank"> XGBoost </a></li><li id="5bbe" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><a class="ae nv" href="https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html" rel="noopener ugc nofollow" target="_blank">灯 GBM </a></li><li id="952a" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><a class="ae nv" href="https://catboost.ai/docs/concepts/installation.html" rel="noopener ugc nofollow" target="_blank"> CatBoost </a></li><li id="35c1" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><a class="ae nv" href="https://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank">远视</a></li></ul><h2 id="507a" class="ne mn it bd mo nf ng dn ms nh ni dp mw lh nj nk my ll nl nm na lp nn no nc np bi translated">远视示例</h2><p id="7f11" class="pw-post-body-paragraph ky kz it la b lb nq ju ld le nr jx lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated"><code class="fe ox oy oz pa b">fmin()</code>是 hyperopt 中用于优化的主要功能。它接受四个基本参数并输出优化的参数集:</p><ol class=""><li id="d7d0" class="oj ok it la b lb lc le lf lh ol ll om lp on lt pb op oq or bi translated">目标函数— <code class="fe ox oy oz pa b">fn</code></li><li id="4e10" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt pb op oq or bi translated">搜索空间— <code class="fe ox oy oz pa b">space</code></li><li id="0daf" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt pb op oq or bi translated">搜索算法— <code class="fe ox oy oz pa b">algo</code></li><li id="edb8" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt pb op oq or bi translated">(最大)评估数量— <code class="fe ox oy oz pa b">max_evals</code></li></ol><p id="ecea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也可以将一个 Trials 对象传递给<code class="fe ox oy oz pa b">trials</code>参数，它跟踪整个过程。为了进行试验，目标函数的输出必须是至少包括关键字<code class="fe ox oy oz pa b">'loss'</code>和<code class="fe ox oy oz pa b">'status'</code>的字典，这两个关键字分别包含结果和优化状态。可以通过以下方式提取临时值:</p><ul class=""><li id="5380" class="oj ok it la b lb lc le lf lh ol ll om lp on lt oo op oq or bi translated"><code class="fe ox oy oz pa b">trials.trials</code> -字典列表包含所有相关信息</li><li id="b30b" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">trials.results</code> -收集函数输出的字典列表</li><li id="3cdc" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">trials.losses()</code> -损失清单(每次‘ok’试验的浮动)</li><li id="9659" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">trials.statuses()</code> -状态字符串列表</li><li id="a1d3" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">trials.vals</code> -采样参数字典</li></ul><p id="bc8d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看下面的例子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Example of hyperopt implementation</figcaption></figure><div class="kj kk kl km gt ab cb"><figure class="nx kn pe nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/88f0037e8e6997595d72e11fc31d1c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*CrMFlaDfT_CZ-8qYpRRL4w.png"/></div></figure><figure class="nx kn pf nz oa ob oc paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/19b38f3c085a1c95f95885101f34d395.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*k3CW3uAd3nqJSJoHegLZuw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk pg di ph oi">Example of hyperopt implementation - progress and the corresponding results</figcaption></figure></div><p id="0fa3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">优化后的<em class="nw"> x </em>为<em class="nw"> 0.5000833960783931 </em>，接近理论值<em class="nw"> 0.5 </em>。正如你可能注意到的，样本在最小值附近更加浓缩。如果您将<code class="fe ox oy oz pa b">algo</code>切换到使用随机采样的<code class="fe ox oy oz pa b">hyperopt.rand.suggest</code>，那么这些点将在<code class="fe ox oy oz pa b">hp.uniform</code>下更均匀地分布。</p><p id="8548" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有几件事需要澄清:</p><p id="4b59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">搜索算法</strong>:或者<code class="fe ox oy oz pa b">hyperopt.tpe.suggest</code>或者<code class="fe ox oy oz pa b">hyperopt.rand.suggest</code></p><p id="dee8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">搜索空间:</strong> <code class="fe ox oy oz pa b">hp.uniform('x', -1, 1)</code>用标签‘x’定义一个搜索空间，它将在-1 和 1 之间被均匀采样。目前由 hyperopt 的优化算法识别的随机表达式是:</p><ul class=""><li id="043a" class="oj ok it la b lb lc le lf lh ol ll om lp on lt oo op oq or bi translated"><code class="fe ox oy oz pa b">hp.choice(label, options)</code>:选项的<strong class="la iu">指标</strong></li><li id="5285" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">hp.randint(label, upper)</code>:随机整数<em class="nw">【0，上)</em></li><li id="ef84" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">hp.uniform(label, low, high)</code>:低/高值一致</li><li id="5459" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">hp.quniform(label, low, high, q)</code> : <code class="fe ox oy oz pa b">round(uniform(.)/q)*q</code>(注意该值给出的是一个<a class="ae nv" href="https://github.com/hyperopt/hyperopt/issues/253" rel="noopener ugc nofollow" target="_blank">浮点数，而不是整数</a>)</li><li id="2c4d" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">hp.loguniform(label, low, high)</code> : <code class="fe ox oy oz pa b">exp(uniform(low, high)/q)*q</code></li><li id="6cc5" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">hp.qloguniform(label, low, high, q)</code> : <code class="fe ox oy oz pa b">round(loguniform(.))</code></li><li id="ec10" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">hp.normal(label, mu, sigma)</code>:正态分布抽样</li><li id="e2ca" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">hp.qnormal(label, mu, sigma, q)</code> : <code class="fe ox oy oz pa b">round(normal(nu, sigma)/q)*q</code></li><li id="d4a5" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">hp.lognormal(label, mu, sigma)</code> : <code class="fe ox oy oz pa b">exp(normal(mu, sigma)</code></li><li id="ed03" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">hp.qlognormal(label, mu, sigma, q)</code> : <code class="fe ox oy oz pa b">round(exp(normal(.))/q)*q</code></li></ul><p id="aa5b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">详见<a class="ae nv" href="https://github.com/hyperopt/hyperopt/wiki/FMin" rel="noopener ugc nofollow" target="_blank">本</a>。</p><p id="0195" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想从远视空间采样，你可以调用<code class="fe ox oy oz pa b">hyperopt.pyll.stochastic.sample(space)</code>，其中<code class="fe ox oy oz pa b">space</code>是上面的<code class="fe ox oy oz pa b">hp</code>空间之一。</p><h2 id="b230" class="ne mn it bd mo nf ng dn ms nh ni dp mw lh nj nk my ll nl nm na lp nn no nc np bi translated">使用 Hyperopt 优化 XGBoost、LightGBM 和 CatBoost</h2><p id="8dd8" class="pw-post-body-paragraph ky kz it la b lb nq ju ld le nr jx lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">下面是本文中的主要示例。所有三个 boosting 库都有一些相似的接口:</p><ul class=""><li id="c135" class="oj ok it la b lb lc le lf lh ol ll om lp on lt oo op oq or bi translated"><strong class="la iu">训练:</strong>T9】</li><li id="1ac0" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><strong class="la iu">交叉验证:</strong> <code class="fe ox oy oz pa b">cv()</code></li><li id="5aa0" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><strong class="la iu">sci kit-学习 API: </strong> <br/> -回归器:<code class="fe ox oy oz pa b">XGBRegressor()</code>，<code class="fe ox oy oz pa b">LGBMRegressor()</code>，<code class="fe ox oy oz pa b">CatBoostRegressor()</code>，<br/> -分类器:<code class="fe ox oy oz pa b">XGBClassifier()</code>，<code class="fe ox oy oz pa b">LGBMClassifier()</code>，<code class="fe ox oy oz pa b">CatBoostClassifier()</code></li></ul><p id="0b81" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下示例使用回归器接口。让我们首先为所有三个库定义参数空间(<code class="fe ox oy oz pa b">reg_params</code>用于对象实例化；<code class="fe ox oy oz pa b">fit_params</code>为<code class="fe ox oy oz pa b">fit()</code>功能):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="d1bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，许多参数共享公共的参数名或别名。有关更多信息，请查看以下链接和相应的 API 页面:</p><ul class=""><li id="1aec" class="oj ok it la b lb lc le lf lh ol ll om lp on lt oo op oq or bi translated"><a class="ae nv" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">XG boost</a>【X】</li><li id="dd11" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><a class="ae nv" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" rel="noopener ugc nofollow" target="_blank">light GBM</a>【L】</li><li id="b69e" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><a class="ae nv" href="https://catboost.ai/docs/concepts/python-reference_parameters-list.html" rel="noopener ugc nofollow" target="_blank">CatBoost</a>【C】</li></ul><p id="d38b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一些基本参数:</p><ul class=""><li id="027b" class="oj ok it la b lb lc le lf lh ol ll om lp on lt oo op oq or bi translated"><code class="fe ox oy oz pa b">learning rate</code>【X/L/C】:学习率(别名:<code class="fe ox oy oz pa b">eta</code>)</li><li id="85ab" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">max_depth</code>【X/L/C】:树木的最大深度</li><li id="e34d" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">n_estimators</code>【X/L/C】:升压迭代次数</li><li id="5562" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">min_child_weight</code>【X/L】:一个孩子所需的最小体重总和</li><li id="3308" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">min_child_samples</code>【信用证】:一叶数据的最小数量</li><li id="1259" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">subsample</code>【X/L/C】:训练实例的子样本比率(注意，对于 CatBoost，只有在选择了泊松或伯努利<code class="fe ox oy oz pa b">bootstrap_type</code>时，才能使用该参数)</li><li id="b2e6" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">colsample_bytree</code>【X/L】:树木建筑中柱子的子样比</li><li id="1493" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">colsample_bylevel</code>【X/C】:树型建筑中各层柱子的子样比</li><li id="93a7" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">colsample_bynode</code>【X】:各节点列的子样率</li><li id="ac00" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">tree_method</code>【X】:树形构造法</li><li id="9c41" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">boosting</code>【L】:树形构造法</li><li id="42f8" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">boosting_type</code> [C]: <code class="fe ox oy oz pa b">Ordered</code>表示有序升压，或者<code class="fe ox oy oz pa b">Plain</code>表示经典</li><li id="60c5" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">early_stopping_rounds</code>【X/L/C】:用于<code class="fe ox oy oz pa b">fit()</code>的参数——如果验证数据的一个指标在最后<code class="fe ox oy oz pa b">early_stopping_rounds</code>轮中没有改善，则停止训练</li><li id="10e5" class="oj ok it la b lb os le ot lh ou ll ov lp ow lt oo op oq or bi translated"><code class="fe ox oy oz pa b">eval_metric</code>【X/L/C】:验证数据的评估指标</li></ul><p id="8372" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有关 CatBoost 中分类特征设置的更多设置，请查看参数页面中的 CTR 设置。</p><p id="cfea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在设置参数之后，我们可以创建一个类<code class="fe ox oy oz pa b">HPOpt</code>,它用训练和测试数据实例化，并提供训练函数。这里我只包括回归变量的例子。您可以在课程中添加自己的分类、训练或交叉验证功能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="be95" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，给定预定义的数据帧<code class="fe ox oy oz pa b">x_train</code>、<code class="fe ox oy oz pa b">x_test</code>、<code class="fe ox oy oz pa b">y_train</code>、<code class="fe ox oy oz pa b">y_test</code>，我们可以通过调用<code class="fe ox oy oz pa b">process()</code>来运行优化过程:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div></figure><h1 id="b843" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">额外奖励:Hyperopt-Sklearn</h1><p id="9a1e" class="pw-post-body-paragraph ky kz it la b lb nq ju ld le nr jx lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated"><a class="ae nv" href="https://hyperopt.github.io/hyperopt-sklearn/" rel="noopener ugc nofollow" target="_blank"> Hyperopt-Sklearn </a>是一个非常高级的优化包，目前仍在构建中。让我们来看一个官方例子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">source: <a class="ae nv" href="https://hyperopt.github.io/hyperopt-sklearn/" rel="noopener ugc nofollow" target="_blank">https://hyperopt.github.io/hyperopt-sklearn/</a></figcaption></figure><p id="1fd1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可能会好奇什么是<code class="fe ox oy oz pa b">any_classifier</code>？如果我们检查它的<a class="ae nv" href="https://github.com/hyperopt/hyperopt-sklearn/blob/master/hpsklearn/components.py" rel="noopener ugc nofollow" target="_blank"> GitHub </a>库:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="18bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">似乎涵盖了<em class="nw"> SVM </em>、<em class="nw"> KNN </em>、<em class="nw">随机森林</em>甚至<em class="nw"> XGBoost </em>等多个分类器和回归器。正如官方页面所说:</p><blockquote class="pi pj pk"><p id="0fb8" class="ky kz nw la b lb lc ju ld le lf jx lg pl li lj lk pm lm ln lo pn lq lr ls lt im bi translated">hyperopt 中可用的任何搜索算法都可以用来驱动估计器。也可以提供自己的算法或混合使用不同的算法。</p></blockquote><p id="7957" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基本上，它几乎为您搜索所有内容，包括模型和超参数。虽然这听起来很方便，但在把一切交给电脑之前，我会三思而行。但是，如果数据集和参数空间的大小不是很大，那么值得一试。</p><h1 id="9c49" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">最后</h1><p id="e389" class="pw-post-body-paragraph ky kz it la b lb nq ju ld le nr jx lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">这是对主要 boosting 库和 hyperopt 的介绍。在文档中有更多关于并行运行的主题，可以提高 GPU 和 MongoDB for hyperopt 的计算速度，您可能也会有所启发。</p></div></div>    
</body>
</html>