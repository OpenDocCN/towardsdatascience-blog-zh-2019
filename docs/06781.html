<html>
<head>
<title>Learning To Solve a Rubik’s Cube From Scratch using Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用强化学习从头开始学习解魔方</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-to-solve-a-rubiks-cube-from-scratch-using-reinforcement-learning-381c3bac5476?source=collection_archive---------11-----------------------#2019-09-27">https://towardsdatascience.com/learning-to-solve-a-rubiks-cube-from-scratch-using-reinforcement-learning-381c3bac5476?source=collection_archive---------11-----------------------#2019-09-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/da038eab6d9d9c57070a58261cb483bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-70oIxlNkFLP1aKLxiXeaA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@olav_ahrens?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Olav Ahrens Røtne</a> on <a class="ae kc" href="https://unsplash.com/s/photos/rubik?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="bbf4" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">动机:</h1><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/5d143d14fa49e262853dca0957d82b6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/1*QAfD1M2LIsI7yCvGJVuq1w.gif"/></div></figure><p id="d7c2" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">魔方是一种有 6 个面的 3D 拼图，每个面通常有 9 个 3×3 布局的贴纸，拼图的目标是达到每个面只有一种独特颜色的已解状态。一个 3x3x3 魔方的可能状态有<a class="ae kc" href="https://en.wikipedia.org/wiki/Quintillion" rel="noopener ugc nofollow" target="_blank">万亿次</a>的数量级，其中只有一个被认为是“已解”状态。这意味着任何试图解立方体的强化学习代理的输入空间都是 huuuuuge。</p><h1 id="a7f7" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">数据集:</h1><p id="4374" class="pw-post-body-paragraph lg lh iq li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md ij bi translated">我们使用 python 库<a class="ae kc" href="https://github.com/adrianliaw/pycuber" rel="noopener ugc nofollow" target="_blank"> pycuber </a>来表示一个立方体，并且只考虑四分之一旋转(90°)移动。这里没有使用注释数据，所有样本都是作为一个状态序列生成的，该序列从已解状态开始，然后被反转(以便它进入已解状态)，然后这些序列就是用于训练的序列。</p><h1 id="f38e" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">问题设置:</h1><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/64d0ca44bc817568cdd736905911468b.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*ykp6WMl-_pdUujDM6LVZ3A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Flattened Cube</figcaption></figure><p id="13f0" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">从一个像上面例子一样的随机洗牌的立方体中，我们想要学习一个模型，它能够从集合{<strong class="li ir">‘F’</strong>:0，<strong class="li ir">‘B’</strong>:1，<strong class="li ir">‘U’</strong>:2，<strong class="li ir">‘D’</strong>:3，<strong class="li ir">‘L’</strong>:4，<strong class="li ir">‘R’</strong>:5，<strong class="li ir">【F’</strong>:6， <strong class="li ir"> "L'" </strong> : 10，<strong class="li ir"> "R'" </strong> : 11} <em class="mk">(定义见</em><a class="ae kc" href="https://ruwix.com/the-rubiks-cube/notation/" rel="noopener ugc nofollow" target="_blank"><em class="mk">【https://ruwix.com/the-rubiks-cube/notation/】</em></a><em class="mk">)</em>进入如下所示的求解状态:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/6fdf450faa472e45e9ff59e8176fe8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*xw8mFRbD2SL3u8siEdOnwA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Flattened solved Cube</figcaption></figure><h1 id="bb45" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">型号:</h1><p id="734e" class="pw-post-body-paragraph lg lh iq li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md ij bi translated">这里实现的想法大多来自论文<a class="ae kc" href="https://arxiv.org/abs/1805.07470" rel="noopener ugc nofollow" target="_blank">在没有人类知识的情况下解魔方</a></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mm"><img src="../Images/2c4c6df1317b71ecb7c058e18337f700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SERfeZTXvRTFJzVolMF-Rw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure from “Solving the Rubik’s Cube Without Human Knowledge” by McAleer et al.</figcaption></figure><p id="cf6e" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">所实施的 RL 方法被称为自动教学迭代或 ADI。如上图所示(来自论文)。我们通过从已解决状态往回走来构建到已解决状态的路径，然后我们使用完全连接的网络来学习路径中每个中间状态的“策略”和“值”。学习目标被定义为:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/7197b6f5f42d62e44741bb58dd268af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*hAKkIF7_sxdnh0VWkn7OQw.png"/></div></figure><p id="6fb8" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">其中“A”是引言中定义的所有 12 个可能动作的变量，R(A(x_i，A))是通过从状态 x_i 采取动作 A 获得的回报。如果我们到达已解决状态，我们定义 R = 0.4，否则为-0.4。奖励是网络在训练期间得到的唯一监督信号，该信号可能被多个“动作”决策延迟，因为具有正奖励的唯一状态是最终解决的状态。</p><p id="5253" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">在推理时，我们使用价值策略网络来指导我们对已解决状态的搜索，以便我们可以通过减少“值得”采取的行动的数量来在尽可能短的时间内解决难题。</p><h1 id="7c71" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结果:</h1><p id="75b1" class="pw-post-body-paragraph lg lh iq li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md ij bi translated">我用这个实现得到的结果并不像论文中呈现的那样壮观。据我所知，这个实现可以很容易地解决离已解决状态 6、7 或 8 步的立方体，但在此之后就困难多了。<br/>网络运行示例:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/47b10b18eecde6b20e1ca109e5d173b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/1*gypdUTAgrYZV7sTMnUSywA.gif"/></div></figure><h1 id="822a" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结论:</h1><p id="34f9" class="pw-post-body-paragraph lg lh iq li b lj me ll lm ln mf lp lq lr mg lt lu lv mh lx ly lz mi mb mc md ij bi translated">这是强化学习的一个非常酷的应用，用来解决一个组合问题，其中有大量的状态，但没有使用任何先验知识或特征工程。</p><p id="3f66" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">重现结果的代码可在:<a class="ae kc" href="https://github.com/CVxTz/rubiks_cube" rel="noopener ugc nofollow" target="_blank">https://github.com/CVxTz/rubiks_cube</a>获得</p></div></div>    
</body>
</html>