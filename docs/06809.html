<html>
<head>
<title>LSTM for time series prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时间序列预测的 LSTM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lstm-for-time-series-prediction-de8aeb26f2ca?source=collection_archive---------1-----------------------#2019-09-28">https://towardsdatascience.com/lstm-for-time-series-prediction-de8aeb26f2ca?source=collection_archive---------1-----------------------#2019-09-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1945d602e0f88b8be3cca8ec4db644ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NLZasQ-lDm-vfJSY.jpg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Time series prediction <a class="ae kf" href="https://www.pexels.com/photo/photograph-of-a-document-1418347/" rel="noopener ugc nofollow" target="_blank">Photo by rawpixel.com from Pexels</a></figcaption></figure><p id="8cba" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用神经网络(NN)来预测市场上的股票价格运动的想法与神经网络一样古老。凭直觉，只看过去似乎很难预测未来的价格走势。有很多关于如何预测价格走势或其威力的教程，把问题简单化了。我决定尝试用 LSTM 预测成交量加权平均价格，因为这看起来很有挑战性，也很有趣。</p><p id="98cc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇博客文章中，我将使用 PyTorch 对比特币交易数据训练一个长短期记忆神经网络(LSTM ),并使用它来预测未见过的交易数据的价格。我在寻找中级教程时遇到了相当大的困难，其中有一个训练 LSTM 进行时间序列预测的可重复示例，所以我整理了一个<a class="ae kf" href="https://romanorac.github.io/assets/notebooks/2019-09-27-time-series-prediction-with-lstm.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>来帮助你开始。</p><h2 id="48b9" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated"><strong class="ak">这里有几个你可能会感兴趣的链接:</strong></h2><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="a134" class="le lf it mc b gy mg mh l mi mj">- <a class="ae kf" href="https://trymito.io/" rel="noopener ugc nofollow" target="_blank">Complete your Python analyses 10x faster with Mito</a> [Product]</span><span id="cf78" class="le lf it mc b gy mk mh l mi mj">- <a class="ae kf" href="https://aigents.co/skills" rel="noopener ugc nofollow" target="_blank">Free skill tests for Data Scientists &amp; ML Engineers</a> [Test]</span><span id="1e41" class="le lf it mc b gy mk mh l mi mj">- <a class="ae kf" href="https://imp.i115008.net/c/2402645/1116216/11298" rel="noopener ugc nofollow" target="_blank">All New Self-Driving Car Engineer Nanodegree</a><strong class="mc iu"> </strong>[Course]</span></pre><p id="3897" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ml">你愿意多读一些这样的文章吗？如果是这样，你可以点击上面的任何链接来支持我。其中一些是附属链接，但你不需要购买任何东西。</em></p><h1 id="bc75" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">正在加载必要的依赖项</h1><p id="6e4a" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">让我们导入将要用于数据操作、可视化、模型训练等的库。我们将使用 PyTorch 库来训练 LSTM。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="1964" class="le lf it mc b gy mg mh l mi mj"><strong class="mc iu">%</strong>matplotlib inline</span><span id="7e13" class="le lf it mc b gy mk mh l mi mj">import glob<br/>import matplotlib<br/>import numpy <strong class="mc iu">as</strong> np<br/>import pandas <strong class="mc iu">as</strong> pd<br/>import sklearn<br/>import torch</span></pre><h1 id="a105" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">加载数据</h1><p id="b3e0" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">我们要分析 BitMex 的 XBTUSD 交易数据。每日文件公开<a class="ae kf" href="https://public.bitmex.com/?prefix=data/trade/" rel="noopener ugc nofollow" target="_blank">下载</a>。我没有费心编写自动下载数据的代码，我只是简单地点击几次来下载文件。</p><p id="48d1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们列出所有文件，将它们读取到一个 pandas 数据框架中，并通过 XBTUSD 符号过滤交易数据。按时间戳对数据帧进行排序很重要，因为有多个每日文件，这样它们就不会混淆。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="6783" class="le lf it mc b gy mg mh l mi mj">files <strong class="mc iu">=</strong> sorted(glob<strong class="mc iu">.</strong>glob('data/*.csv.gz'))</span><span id="a7b7" class="le lf it mc b gy mk mh l mi mj">df <strong class="mc iu">=</strong> pd<strong class="mc iu">.</strong>concat(map(pd<strong class="mc iu">.</strong>read_csv, files))</span><span id="4522" class="le lf it mc b gy mk mh l mi mj">df <strong class="mc iu">=</strong> df[df<strong class="mc iu">.</strong>symbol <strong class="mc iu">==</strong> 'XBTUSD']</span><span id="e8fb" class="le lf it mc b gy mk mh l mi mj">df<strong class="mc iu">.</strong>timestamp <strong class="mc iu">=</strong> pd<strong class="mc iu">.</strong>to_datetime(df<strong class="mc iu">.</strong>timestamp<strong class="mc iu">.</strong>str<strong class="mc iu">.</strong>replace('D', 'T')) # covert to timestamp type</span><span id="2426" class="le lf it mc b gy mk mh l mi mj">df <strong class="mc iu">=</strong> df<strong class="mc iu">.</strong>sort_values('timestamp')</span><span id="ee0a" class="le lf it mc b gy mk mh l mi mj">df<strong class="mc iu">.</strong>set_index('timestamp', inplace<strong class="mc iu">=</strong>True) # set index to timestamp</span><span id="6718" class="le lf it mc b gy mk mh l mi mj">df<strong class="mc iu">.</strong>head()</span></pre><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/9b0867884817499a24aaf8f052f70e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGmW77cEsoJkua2NdmFxSw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">BitMex trade data</figcaption></figure><p id="e66c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每行代表一种交易:</p><ul class=""><li id="e465" class="nj nk it ki b kj kk kn ko kr nl kv nm kz nn ld no np nq nr bi translated">以微秒为单位的时间戳精度，</li><li id="bbaa" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated">交易合约的符号，</li><li id="8ff6" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated">交易的一方，买或卖，</li><li id="f234" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated">大小代表合约数量(交易的美元数量)，</li><li id="c807" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated">合同价格，</li><li id="2cbf" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated">tickDirection 描述了自上次交易以来价格的上涨/下跌，</li><li id="927c" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated">trdMatchID 是唯一的交易 ID，</li><li id="bdaf" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated">grossValue 是交换的 satoshis 的数量，</li><li id="9a1f" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated">家名义是交易中 XBT 的数量，</li><li id="2fe1" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated">外币名义金额是交易中的美元金额。</li></ul><p id="7e1d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用 3 列:时间戳，价格和外国名义。</p><h1 id="7736" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">数据预处理</h1><p id="51c4" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">让我们以 1 分钟的时间间隔计算<a class="ae kf" href="https://www.investopedia.com/terms/v/vwap.asp" rel="noopener ugc nofollow" target="_blank">成交量加权平均价格(VWAP) </a>。我们根据预先定义的时间间隔对交易进行分组的数据表示叫做时间棒。这是表示交易数据进行建模的最佳方式吗？根据洛佩兹·德·普拉多的说法，市场上的交易并不是随时间均匀分布的。有一些活动频繁的时段，例如在期货合约到期之前，在预定义的时间间隔内对数据进行分组会在一些时间条内对数据进行过采样，而在其他时间条内对数据进行欠采样。<a class="ae kf" rel="noopener" target="_blank" href="/financial-machine-learning-part-0-bars-745897d4e4ba">金融机器学习第 0 部分:条</a>是洛佩兹·德·普拉多的书<a class="ae kf" href="https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089" rel="noopener ugc nofollow" target="_blank">金融机器学习书籍</a>第二章的一个很好的总结。时间条可能不是最好的数据表示，但无论如何我们都要使用它们。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="2ad7" class="le lf it mc b gy mg mh l mi mj">df_vwap <strong class="mc iu">=</strong> df<strong class="mc iu">.</strong>groupby(pd<strong class="mc iu">.</strong>Grouper(freq<strong class="mc iu">=</strong>"1Min"))<strong class="mc iu">.</strong>apply(<br/>    <strong class="mc iu">lambda</strong> row: pd<strong class="mc iu">.</strong>np<strong class="mc iu">.</strong>sum(row<strong class="mc iu">.</strong>price <strong class="mc iu">*</strong> row<strong class="mc iu">.</strong>foreignNotional) <strong class="mc iu">/</strong> pd<strong class="mc iu">.</strong>np<strong class="mc iu">.</strong>sum(row<strong class="mc iu">.</strong>foreignNotional))</span></pre><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/3c36036f1658f8d62b4528b3b3ba9eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_9EspAShYUy0sZH9.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Time bars showing XBTUSD VWAP from 1st of August till the 17th of September 2019</figcaption></figure><p id="2759" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该图显示了 2019 年 8 月 1 日至 9 月 17 日 VWAP 的时间条。我们将把数据的第一部分用于训练集，中间部分用于验证集，最后一部分用于测试集(竖线是分隔符)。我们可以观察 VWAP 的波动性，那里的价格在 8 月上旬达到最高点，在 8 月底达到最低点。高点和低点在训练集中被捕获，这很重要，因为模型很可能在看不见的 VWAP 区间上工作得不好。</p><h1 id="7c2f" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">缩放数据</h1><p id="bf45" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">为了帮助 LSTM 模型更快地收敛，缩放数据是很重要的。输入中的大值可能会降低学习速度。我们将使用 sklearn 库中的 StandardScaler 来缩放数据。定标器安装在训练集上，用于转换验证集和测试集上的未知交易数据。如果我们在所有数据上拟合标量，模型将会过度拟合，并且它将在该数据上获得良好的结果，但是在真实世界的数据上性能会受到影响。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="0b5e" class="le lf it mc b gy mg mh l mi mj">from sklearn.preprocessing import StandardScaler</span><span id="24dd" class="le lf it mc b gy mk mh l mi mj">scaler <strong class="mc iu">=</strong> StandardScaler()<br/>train_arr <strong class="mc iu">=</strong> scaler<strong class="mc iu">.</strong>fit_transform(df_train)<br/>val_arr <strong class="mc iu">=</strong> scaler<strong class="mc iu">.</strong>transform(df_val)<br/>test_arr <strong class="mc iu">=</strong> scaler<strong class="mc iu">.</strong>transform(df_test)</span></pre><h1 id="4532" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">转换数据</h1><p id="7653" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">缩放后，我们需要将数据转换成适合用 LSTM 建模的格式。我们将长数据序列转换成许多较短的序列(每个序列有 100 个时间条),每个时间条移动一个时间条。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="3b83" class="le lf it mc b gy mg mh l mi mj">from torch.autograd import Variable</span><span id="a78b" class="le lf it mc b gy mk mh l mi mj"><strong class="mc iu">def</strong> <strong class="mc iu">transform_data</strong>(arr, seq_len):<br/>    x, y <strong class="mc iu">=</strong> [], []<br/>    <strong class="mc iu">for</strong> i <strong class="mc iu">in</strong> range(len(arr) <strong class="mc iu">-</strong> seq_len):<br/>        x_i <strong class="mc iu">=</strong> arr[i : i <strong class="mc iu">+</strong> seq_len]<br/>        y_i <strong class="mc iu">=</strong> arr[i <strong class="mc iu">+</strong> 1 : i <strong class="mc iu">+</strong> seq_len <strong class="mc iu">+</strong> 1]<br/>        x<strong class="mc iu">.</strong>append(x_i)<br/>        y<strong class="mc iu">.</strong>append(y_i)<br/>    x_arr <strong class="mc iu">=</strong> np<strong class="mc iu">.</strong>array(x)<strong class="mc iu">.</strong>reshape(<strong class="mc iu">-</strong>1, seq_len)<br/>    y_arr <strong class="mc iu">=</strong> np<strong class="mc iu">.</strong>array(y)<strong class="mc iu">.</strong>reshape(<strong class="mc iu">-</strong>1, seq_len)<br/>    x_var <strong class="mc iu">=</strong> Variable(torch<strong class="mc iu">.</strong>from_numpy(x_arr)<strong class="mc iu">.</strong>float())<br/>    y_var <strong class="mc iu">=</strong> Variable(torch<strong class="mc iu">.</strong>from_numpy(y_arr)<strong class="mc iu">.</strong>float())<br/>    <strong class="mc iu">return</strong> x_var, y_var</span><span id="5425" class="le lf it mc b gy mk mh l mi mj">seq_len <strong class="mc iu">=</strong> 100</span><span id="eaf7" class="le lf it mc b gy mk mh l mi mj">x_train, y_train <strong class="mc iu">=</strong> transform_data(train_arr, seq_len)<br/>x_val, y_val <strong class="mc iu">=</strong> transform_data(val_arr, seq_len)<br/>x_test, y_test <strong class="mc iu">=</strong> transform_data(test_arr, seq_len)</span></pre><p id="c647" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了训练集中的第一个和第二个序列。两个序列的长度都是 100 倍棒。我们可以观察到，两个序列的目标几乎与特征相同，不同之处在于第一个和最后一个时间条。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ny"><img src="../Images/fb91d70818c71d6a2978989c255bbf0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eHbMEwzlyRNkKnwj.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">The feature and target of the first and second sequence in the training set</figcaption></figure><p id="acb6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">LSTM 在训练阶段如何使用这个序列？</strong></p><p id="b634" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们关注第一个序列。该模型采用索引 0 处的时间条的特征，并尝试预测索引 1 处的时间条的目标。然后，它采用索引 1 处的时间条的特征，并试图预测索引 2 处的时间条的目标，等等。第二序列的特征从第一序列的特征偏移 1 个时间条，第三序列的特征从第二序列偏移 1 个时间条，等等。通过这一过程，我们得到了许多较短的序列，它们被一个时间条所移动。</p><p id="ae84" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，在分类或回归任务中，我们通常有一组特征和一个我们试图预测的目标。在这个 LSTM 的例子中，特征和目标来自相同的序列，唯一的区别是目标移动了 1 倍。</p><h1 id="a424" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">长短期记忆神经网络</h1><p id="7649" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">长短期记忆神经网络是一种递归神经网络(RNN)。rnn 使用先前的时间事件来通知后面的时间事件。例如，为了对电影中正在发生的事件进行分类，模型需要使用以前事件的信息。如果问题只需要最近的信息来执行当前的任务，那么 RNNs 工作得很好。如果问题需要长期依赖，RNN 将很难对其建模。LSTM 旨在学习长期依赖性。它能长时间记住信息。LSTM 是在 1997 年由舒米胡伯介绍的。要了解更多关于 LSTMs 的信息，请阅读一篇伟大的<a class="ae kf" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> colah 博客文章</a>，它提供了一个很好的解释。</p><p id="a9b1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的代码是时序预测的有状态 LSTM 的实现。它有一个 LSTMCell 单元和一个线性图层来模拟时间序列。该模型可以生成时间序列的未来值，并且可以使用教师强制(我将在后面描述这个概念)来训练它。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="de40" class="le lf it mc b gy mg mh l mi mj">import torch.nn <strong class="mc iu">as</strong> nn<br/>import torch.optim <strong class="mc iu">as</strong> optim<br/></span><span id="21c8" class="le lf it mc b gy mk mh l mi mj"><strong class="mc iu">class</strong> <strong class="mc iu">Model</strong>(nn<strong class="mc iu">.</strong>Module):<br/>    <strong class="mc iu">def</strong> <strong class="mc iu">__init__</strong>(self, input_size, hidden_size, output_size):<br/>        super(Model, self)<strong class="mc iu">.</strong>__init__()<br/>        self<strong class="mc iu">.</strong>input_size <strong class="mc iu">=</strong> input_size<br/>        self<strong class="mc iu">.</strong>hidden_size <strong class="mc iu">=</strong> hidden_size<br/>        self<strong class="mc iu">.</strong>output_size <strong class="mc iu">=</strong> output_size<br/>        self<strong class="mc iu">.</strong>lstm <strong class="mc iu">=</strong> nn<strong class="mc iu">.</strong>LSTMCell(self<strong class="mc iu">.</strong>input_size, self<strong class="mc iu">.</strong>hidden_size)<br/>        self<strong class="mc iu">.</strong>linear <strong class="mc iu">=</strong> nn<strong class="mc iu">.</strong>Linear(self<strong class="mc iu">.</strong>hidden_size, self<strong class="mc iu">.</strong>output_size)</span><span id="ee85" class="le lf it mc b gy mk mh l mi mj">    <strong class="mc iu">def</strong> <strong class="mc iu">forward</strong>(self, input, future<strong class="mc iu">=</strong>0, y<strong class="mc iu">=</strong>None):<br/>        outputs <strong class="mc iu">=</strong> []</span><span id="a80d" class="le lf it mc b gy mk mh l mi mj">        <em class="ml"># reset the state of LSTM</em><br/>        <em class="ml"># the state is kept till the end of the sequence</em><br/>        h_t <strong class="mc iu">=</strong> torch<strong class="mc iu">.</strong>zeros(input<strong class="mc iu">.</strong>size(0), self<strong class="mc iu">.</strong>hidden_size, dtype<strong class="mc iu">=</strong>torch<strong class="mc iu">.</strong>float32)<br/>        c_t <strong class="mc iu">=</strong> torch<strong class="mc iu">.</strong>zeros(input<strong class="mc iu">.</strong>size(0), self<strong class="mc iu">.</strong>hidden_size, dtype<strong class="mc iu">=</strong>torch<strong class="mc iu">.</strong>float32)</span><span id="9241" class="le lf it mc b gy mk mh l mi mj">        <strong class="mc iu">for</strong> i, input_t <strong class="mc iu">in</strong> enumerate(input<strong class="mc iu">.</strong>chunk(input<strong class="mc iu">.</strong>size(1), dim<strong class="mc iu">=</strong>1)):<br/>            h_t, c_t <strong class="mc iu">=</strong> self<strong class="mc iu">.</strong>lstm(input_t, (h_t, c_t))<br/>            output <strong class="mc iu">=</strong> self<strong class="mc iu">.</strong>linear(h_t)<br/>            outputs <strong class="mc iu">+=</strong> [output]</span><span id="89fb" class="le lf it mc b gy mk mh l mi mj">        <strong class="mc iu">for</strong> i <strong class="mc iu">in</strong> range(future):<br/>            <strong class="mc iu">if</strong> y <strong class="mc iu">is</strong> <strong class="mc iu">not</strong> None <strong class="mc iu">and</strong> random<strong class="mc iu">.</strong>random() <strong class="mc iu">&gt;</strong> 0.5:<br/>                output <strong class="mc iu">=</strong> y[:, [i]]  <em class="ml"># teacher forcing</em><br/>            h_t, c_t <strong class="mc iu">=</strong> self<strong class="mc iu">.</strong>lstm(output, (h_t, c_t))<br/>            output <strong class="mc iu">=</strong> self<strong class="mc iu">.</strong>linear(h_t)<br/>            outputs <strong class="mc iu">+=</strong> [output]<br/>        outputs <strong class="mc iu">=</strong> torch<strong class="mc iu">.</strong>stack(outputs, 1)<strong class="mc iu">.</strong>squeeze(2)<br/>        <strong class="mc iu">return</strong> outputs</span></pre><h1 id="8659" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">训练 LSTM</h1><p id="38e7" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">我们用 21 个隐藏单位训练 LSTM。使用的单位数量较少，这样 LSTM 就不太可能完全记住这个序列。我们使用均方误差损失函数和 Adam 优化器。学习率被设置为 0.001，并且每 5 个时期衰减一次。我们用每批 100 个序列训练该模型 15 个时期。从下面的图中，我们可以观察到训练和验证损失在第六个历元之后收敛。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/fa0d00c16a8c8c77d333176c59b9e923.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*tNJ4ShG5Z24W6ZSy.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Training and validation loss</figcaption></figure><p id="6323" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们在测试集上评估模型。future 参数设置为 5，这意味着模型输出它认为在接下来的 5 个时间条(在我们的示例中为 5 分钟)中的 VWAP。这将使价格变化在发生前的几个时间段内可见。</p><p id="0a7e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下图中，我们可以观察到预测值与 VWAP 的实际值非常接近，乍一看这似乎很好。但是 future 参数被设置为 5，这意味着橙色线应该在尖峰出现之前做出反应，而不是覆盖它。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/baae8b738f1b905356b9d25c33be0218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*C2lSTZMEBwQCKwJW.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Actual and predicted VWAP on the test set</figcaption></figure><p id="00df" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们放大尖峰时(一个在时间序列的开始，另一个在时间序列的结束)。我们可以观察到预测值模拟了实际值。当实际值改变方向时，预测值也跟着改变，这对我们没有太大帮助。同样的情况发生在我们增加未来参数的时候(就像不影响预测线一样)。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/ed7000aecbf98524c44c0f931d4c36eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OH47bcwIBMz_gfIG.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Two VWAP spikes with actual and predicted values</figcaption></figure><p id="6756" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们用模型为第一个测试序列生成 1000 个时间条，并比较预测的、生成的和实际的 VWAP。我们可以观察到，虽然模型输出预测值，但它们接近实际值。但是当它开始产生值时，输出几乎类似正弦波。经过一段时间后，数值收敛到 9600。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/e323e894f0970049d14a449f7fac790a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tS07k46u1vK3dKkV.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Predicted, generated and actual VWAP of the first test sequence</figcaption></figure><p id="bf08" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之所以会出现这种情况，是因为模型仅使用真实输入进行了训练，而从未使用生成的输入进行过训练。当模型从输入中获得生成的输出时，它在生成下一个值方面做得很差。教师强迫是处理这个问题的一个概念。</p><h1 id="67c0" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">教师强迫</h1><p id="ccc4" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated"><a class="ae kf" href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">教师强制</a>是一种训练递归神经网络的方法，它使用前一时间步的输出作为输入。当训练 RNN 时，它可以通过使用先前的输出作为当前输入来生成序列。在训练期间可以使用相同的过程，但是模型可能变得不稳定或者不收敛。教师强迫是在培训中解决这些问题的一种方法。它通常用在语言模型中。</p><p id="a844" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用教师强制的扩展，称为<a class="ae kf" href="https://arxiv.org/abs/1506.03099" rel="noopener ugc nofollow" target="_blank">预定抽样</a>。该模型将在训练期间使用其生成的输出作为具有一定概率的输入。起初，模型看到其生成输出的概率很小，然后在训练期间逐渐增加。注意，在这个例子中，我们使用随机概率，它在训练过程中不会增加。</p><p id="791b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用与之前相同的参数训练一个模型，但是启用了教师强制。在 7 个时期之后，训练和验证损失收敛。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/fbd1338aa87e09da7c5cf93cfba8c89f.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*udwcVg-u2KxmwmbT.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Training and validation loss with teacher forcing</figcaption></figure><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/834042fe6d5b88745dfeea5e61528c50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7GPSVY7Y3FULxB7i.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Actual and predicted VWAP on the test set with teacher forcing</figcaption></figure><p id="5819" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以观察到和以前相似的预测序列。当我们放大峰值时，可以观察到模型的类似行为，其中预测值模拟实际值。好像老师强迫并没有解决问题。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/77566549f667fd43ecc79e1d25d19e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*L1iesaeJiP2_B9h9.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Two VWAP spikes with actual and predicted values with teacher forcing</figcaption></figure><p id="7152" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们用教师强制训练的模型为第一个测试序列生成 1000 个时间条。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/fc2b8fb55ec8d25996b68fe80e9b6d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_3hn6PvPaHdfJjS9.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Predicted, generated and actual VWAP of the first test sequence with teacher forcing</figcaption></figure><p id="9a33" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过教师强制训练的模型生成的序列需要更长的时间来收敛。关于生成序列的另一个观察结果是，当它增加时，它将继续增加到某个点，然后开始减少，模式重复，直到序列收敛。该模式看起来像一个振幅递减的正弦波。</p><h1 id="ea2a" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">结论</h1><p id="86bb" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">这个实验的结果是模型的预测模拟了序列的实际值。第一个和第二个模型在价格变化发生之前不检测价格变化。添加另一个要素(如交易量)可能有助于模型在价格变化发生之前检测到价格变化，但随后模型将需要生成两个要素，以将这些要素的输出用作下一步的输入，这将使模型变得复杂。使用更复杂的模型(多个 LSTMCells，增加隐藏单元的数量)可能没有帮助，因为该模型具有预测 VWAP 时间序列的能力，如上图所示。更先进的教师强制方法可能会有所帮助，因此该模型将提高序列生成技能。</p><h1 id="8163" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">参考</h1><ul class=""><li id="50e9" class="nj nk it ki b kj nd kn ne kr oa kv ob kz oc ld no np nq nr bi translated"><a class="ae kf" href="https://github.com/pytorch/examples/tree/master/time_sequence_prediction" rel="noopener ugc nofollow" target="_blank">时间序列预测</a></li><li id="759f" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated"><a class="ae kf" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解 LSTM 网络</a></li><li id="1f3e" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated"><a class="ae kf" href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">什么是递归神经网络的教师强迫？</a></li><li id="1254" class="nj nk it ki b kj ns kn nt kr nu kv nv kz nw ld no np nq nr bi translated"><a class="ae kf" href="https://arxiv.org/abs/1506.03099" rel="noopener ugc nofollow" target="_blank">用递归神经网络进行序列预测的预定采样</a></li></ul><h1 id="f4f7" class="mm lf it bd lg mn mo mp lj mq mr ms lm mt mu mv lp mw mx my ls mz na nb lv nc bi translated">在你走之前</h1><p id="dea3" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">在<a class="ae kf" href="https://twitter.com/romanorac" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我，在那里我定期<a class="ae kf" href="https://twitter.com/romanorac/status/1328952374447267843" rel="noopener ugc nofollow" target="_blank">发布关于数据科学和机器学习的</a>消息。</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/b5d426b68cc5a21b1a35d0a157ebc4f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*69rP1pwjJi9mLSFE"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@cmhedger?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Courtney Hedger</a> on <a class="ae kf" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div>    
</body>
</html>