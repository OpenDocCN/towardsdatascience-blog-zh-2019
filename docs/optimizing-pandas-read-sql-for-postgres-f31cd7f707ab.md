# 为 Postgres 优化 pandas.read_sql

> 原文：<https://towardsdatascience.com/optimizing-pandas-read-sql-for-postgres-f31cd7f707ab?source=collection_archive---------4----------------------->

将 SQL 查询读入 Pandas 数据帧是一项常见的任务，可能会非常慢。根据所使用的数据库，这可能很难避免，但是对于我们这些使用 Postgres 的人来说，我们可以使用 COPY 命令大大加快速度。然而，有几种方法可以使用 COPY 命令将 SQL 中的数据放入 pandas，但需要不同的内存/速度权衡。在本文中，我们将测试几种不同的方法。

测试数据集只是样本 [Triage](http://github.com/dssg/triage) 预测表的前五百万行，这只是我手边的一个。我试图使用本地 Postgres 数据库中的 1300 万行，但是 pandas.read_sql 崩溃了，所以我决定将数据集降低到它可以作为基准处理的水平。

每种方法都包括三种统计数据:

**峰值内存**—SQL 读取代码期间使用的最高内存量。这是重要的一点，看看你的程序是否会崩溃！

**增量内存**—SQL 读取代码结束时仍在使用的内存量。理论上，这对于所有的方法都是一样的，但是内存泄漏会使不同的方法保留更多的内存。

**经过时间** —程序使用的时钟时间。

这里用的熊猫版本是 0.24.1。

首先，快速概述一下正在测试的不同方法:

*   pandas.read_sql —基线
*   临时文件—使用临时文件模块在磁盘上创建一个临时文件，以便在数据帧读入复制结果之前，将它们存放在其中
*   StringIO——使用 StringIO 代替磁盘；使用更多内存，但磁盘 I/O 更少
*   压缩 BytesIO，pandas 解压——用 BytesIO 代替 StringIO，压缩数据；应该使用更少的内存，但需要更长的时间
*   压缩字节，gzip 解压缩—与其他压缩字节相同，但是使用 GzipFile 而不是 pandas 来解压缩
*   压缩的临时文件——将压缩思想应用于磁盘文件；应该会减少所需的磁盘 I/O
*   压缩字节数，低压缩级别—尝试分割未压缩方法和压缩方法之间差异的较低压缩级别

## pandas.read_sql

这是基线。这里没什么特别的。

**峰值内存:3832.7 MiB /增量内存:3744.9 MiB /运行时间:35.91s**

## 使用临时文件

这是我们第一次尝试使用复制命令。来自 COPY 命令的数据必须使用 filehandle:还有比使用临时文件更简单的方法吗？

**峰值内存:434.3 MB /增量内存:346.6 MB /运行时间:8.93 秒**

那……好多了。对于运行时间比 read_sql 快得多，我并不感到惊讶，但我有点惊讶的是，内存使用量相差如此之大。不管怎样，我们继续吧

## 使用弦乐器

磁盘 I/O 可能很昂贵，尤其是取决于可用的磁盘类型。我们可以通过使用 StringIO 作为文件句柄来加速它吗？当然，这会占用更多的内存，但也许这是我们可以做的一个折衷。

**峰值内存:434.2 MB /增量内存:346.6 MB /运行时间:9.82 秒**

这是一个令人惊讶的结果。我本以为这会占用更多的内存，速度会更快，但事实并非如此。我的假设是，StringIO 使用的内存峰值最终会在数据帧创建过程中被一个峰值超过。

还要注意:增量内存与临时文件版本相同，这可能告诉我们，346.6 MB 是在没有任何内存泄漏的情况下内存基线的一个很好的参考。

## 使用压缩字节，熊猫解压。

我们能降低内存选项所需的内存吗？鉴于之前的结果，这可能看起来像一个傻瓜的差事，但我已经写了代码，所以我不会提前停止测试！Python 的 GzipFile 接口包装了一个 filehandle(在本例中是一个 BytesIO)并处理压缩。我们让 pandas 通过将“compression='gzip '”传递给 read_csv 来处理解压缩

**峰值内存:613.6 MB 增量内存:525.8 MB，耗时:1:30 分钟**

不好！与未压缩版本相比，它实际上使用了更多的内存(并泄漏了一些)。

## 使用压缩字节，Gzip 解压缩

和上一个一样，除了我们绕过熊猫的解压程序，以防它们带来问题。GzipFile 也可以为我们处理解压缩！

**峰值内存:504.6 MB 增量内存:416.8 MB，耗时:1:42m**

当然，这比熊猫的解压缩版本在内存方面要好，但是这仍然比未压缩的版本差。

## 使用压缩的临时文件

压缩思想也可以应用到以前的 tempfile 方法。在这种情况下，压缩应该可以帮助我们减少磁盘 I/O。

**峰值内存:517.2 MB 增量内存:429.5 MB，耗时:1 分 35 秒**

类似于其他 gzip 示例。不是一个好的选择。

## 使用压缩字节，低压缩级别

既然我们正在尝试，我们还有一个途径可以探索:gzip 压缩级别。前面所有示例的默认值是 9，这是可能的最高压缩率。在这样做的过程中，除了额外的时间之外，还可能需要额外的内存来进行压缩。如果我们将其中一个翻转到最低压缩级别(1)会怎么样？

**峰值内存:761.5 MB 增量内存:673.8 MB，运行时间:1 分 13 秒**

在时间上稍微好一点，但是在 RAM 上更差:看起来 gzipping 进程无论如何都要使用大量的内存，并且不能很好地传输。

# 结论

我们在这里学到了什么？

1.  pandas.read_sql 很烂，无论是时间还是内存。
2.  使用带有 COPY 的 StringIO 或 tempfile 可以执行类似的操作。很容易将 tempfile 称为赢家，但我要强调的是，这完全是基于对您来说便宜的东西。这个测试在我的笔记本电脑上运行，使用本地磁盘。根据设置的不同，使用磁盘 I/O 可能会更昂贵！但是这两个例子一般来说都非常快，应该是你的首选！
3.  压缩的想法似乎是在转移视线，至少在用 gzip 实现的时候是这样。GzipFile，它的内部我并不完全熟悉。可能有其他更复杂的方法可以工作(例如 zlib.compressobj，或者其他完全的压缩类型)，但是这里没有。