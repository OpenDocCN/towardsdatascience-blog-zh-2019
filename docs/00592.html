<html>
<head>
<title>ENet — A Deep Neural Architecture for Real-Time Semantic Segmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ENet——一种用于实时语义分割的深度神经架构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/enet-a-deep-neural-architecture-for-real-time-semantic-segmentation-2baa59cf97e9?source=collection_archive---------7-----------------------#2019-01-27">https://towardsdatascience.com/enet-a-deep-neural-architecture-for-real-time-semantic-segmentation-2baa59cf97e9?source=collection_archive---------7-----------------------#2019-01-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9be2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">论文摘要</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4b889408c2147102eb56e1972b458640.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aBW5dSNFJa3c1PVvsn3q9g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://www.youtube.com/watch?v=qWl9idsCuLQ" rel="noopener ugc nofollow" target="_blank"><strong class="bd kw">Fig 1. A conversation between a semantic segmented guy and a toon</strong></a></figcaption></figure><p id="af06" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="lt">这是一篇论文的论文摘要:<br/> </em> <strong class="kz ir"> <em class="lt"> ENet:一种用于实时语义分割的深度神经网络架构</em></strong><em class="lt"><br/>by</em><a class="ae kv" href="mailto:a.paszke@students.mimuw.edu.pl" rel="noopener ugc nofollow" target="_blank"><em class="lt">亚当·帕兹克</em> </a> <em class="lt"> <br/>论文:</em><a class="ae kv" href="https://arxiv.org/abs/1606.02147" rel="noopener ugc nofollow" target="_blank"><em class="lt">https://arxiv.org/abs/1606.02147</em></a></p><h1 id="ce7c" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">概观</h1><p id="8e45" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated"><strong class="kz ir"> <em class="lt"> ENet(高效神经网络)</em> </strong>提供实时执行逐像素语义分割的能力。ENet 的速度提高了 18 倍，所需的 FLOPs 减少了 75 倍，参数减少了 79 倍，精度与现有模型相当或更高。在 CamVid、CityScapes 和 SUN 数据集上进行了测试。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/de20fba27253dfcf4bdc6eefb135a7a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G3izIfZMo6UTkQ5aRk-0kQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 2. Semantic Segmentations using ENet</strong></figcaption></figure><h1 id="bd03" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">方法</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ece45e7b98e7dfd7590b4dd47c2559f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*CKuZqyLSc4U8BjG3sWZHew.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 3. ENet Architecture</strong></figcaption></figure><p id="bf26" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">以上是完整的网络架构。<br/>它分为几个阶段，如表中的水平线和每个块名后的第一个数字所示。<br/>报告输入图像分辨率为<code class="fe mt mu mv mw b">512 * 512</code>时的输出尺寸</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/ffc791ad6a93ca28b182955fb66106f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWYNKupbSTFFTOU7_7-sAg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 4. Each module of ENet in detail</strong></figcaption></figure><p id="fdde" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><br/>的直观表示:初始块如<em class="lt"> (a) <br/> - </em>所示，瓶颈块如<em class="lt"> (b) </em>所示</p><p id="3b6e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">每个瓶颈模块由:<br/> - 1x1 投影降维<br/> -一个主卷积层(<code class="fe mt mu mv mw b"><em class="lt">conv</em></code> <em class="lt">)(要么— </em> <strong class="kz ir"> <em class="lt">正则</em> </strong> <em class="lt">，</em> <strong class="kz ir"> <em class="lt">扩张</em> </strong> <em class="lt">或</em> <strong class="kz ir"> <em class="lt">满</em></strong><em class="lt">)(3x 3)</em><br/>-1x1 扩展<br/></p><p id="dadf" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果瓶颈是缩减采样，则最大池图层将被添加到主分支。此外，第一个 1x1 投影被替换为跨距=2 的 2x2 卷积。</p><p id="a37f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">它们将激活填充为零，以匹配特征映射的数量。</p><p id="fd49" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><code class="fe mt mu mv mw b">conv</code>有时是非对称卷积，即一系列<code class="fe mt mu mv mw b">5 * 1</code>和<code class="fe mt mu mv mw b">1 * 5</code>卷积。</p><p id="3eb6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于<code class="fe mt mu mv mw b">regularizer</code>，他们使用<em class="lt">空间落差</em> : <br/> -瓶颈前带<code class="fe mt mu mv mw b">p = 0.01</code>2.0<br/>-瓶颈后带<code class="fe mt mu mv mw b">p = 0.1</code></p><p id="b157" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">所以，</p><ol class=""><li id="f64f" class="my mz iq kz b la lb ld le lg na lk nb lo nc ls nd ne nf ng bi translated">阶段 1，2，3— <strong class="kz ir"> <em class="lt">编码器</em> </strong> —由 5 个瓶颈块组成(除了阶段 3 不下采样)。</li><li id="a2c1" class="my mz iq kz b la nh ld ni lg nj lk nk lo nl ls nd ne nf ng bi translated">阶段 4，5— <strong class="kz ir"> <em class="lt">解码器</em> </strong> —阶段 4 包含 3 个瓶颈，阶段 5 包含 2 个瓶颈</li><li id="7354" class="my mz iq kz b la nh ld ni lg nj lk nk lo nl ls nd ne nf ng bi translated">后跟一个<code class="fe mt mu mv mw b">fullconv</code>，它输出维度为<code class="fe mt mu mv mw b">C * 512 * 512</code>的最终输出，其中<code class="fe mt mu mv mw b">C</code>是滤波器的数量。</li></ol><p id="0d70" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">还有几个事实:<br/> -他们没有在任何投影中使用偏差项<br/> -在每个卷积层和激活之间，他们使用批量归一化<br/> -在解码器中，MaxPooling 被替换为 MaxUnpooling <br/> -在解码器中，Padding 被替换为没有偏差的空间卷积<br/> -在<em class="lt">最后一个</em> ( <em class="lt"> 5.0 </em>)上采样模块<br/>中没有使用池索引-网络的最后一个模块是完全卷积，它单独<br/> -每个分支都有一个空间落差，第一阶段为<code class="fe mt mu mv mw b">p = 0.01</code>，随后阶段为<code class="fe mt mu mv mw b">p = 0.1</code>。</p><h1 id="7d3d" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">结果</h1><p id="46e8" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">使用 SegNet[2]作为基准，在<br/> - <em class="lt"> CamVid </em>(道路场景)<br/> - <em class="lt">城市风景</em>(道路场景)<br/> - <em class="lt"> SUN RGB-D </em>(室内场景)<br/>上对 ENet 的性能进行基准测试，因为它是最快的分割模型之一。使用 cuDNN 后端使用 Torch7 库。</p><p id="814f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">推理速度是使用 NVIDIA Titan X GPU 以及在 NVIDIA TX1 嵌入式系统模块上记录的。输入图像大小为 640x360 时，达到 10fps 以上。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/e0ebb8cc9503f41b27e7f9e5e0f3c6a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ipLorVidmsCmecw7Kr_IwA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 5. Inference Time Comparison on the two different GPUs with SegNet as the baseline</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/8db8761768adbf1f1cdf6ceb17ac7ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VW5O9OHfTRjeV3JIc53onQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 6. Hardware requirements for SegNet and ENet</strong></figcaption></figure><h2 id="01f1" class="no lv iq bd lw np nq dn ma nr ns dp me lg nt nu mg lk nv nw mi lo nx ny mk nz bi translated">基准</h2><p id="8632" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated"><strong class="kz ir"> <em class="lt">用于亚当</em> </strong> <em class="lt">。</em> ENet 收敛速度非常快，在每个数据集上，使用 4 个 Titan X GPUs 的训练只需要大约 3-6 个小时。</p><p id="420b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">分两个阶段执行:<br/> -首先，他们训练编码器对输入图像的降采样区域进行分类。<br/> -然后追加解码器，训练网络执行上采样和逐像素分类。</p><p id="5943" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> <em class="lt">学习速率— 5e-4 <br/> L2 重量衰减 2e-4 <br/>批量大小 10 </em> </strong></p><p id="eb6b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">自定义等级称重方案定义为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/2d6accdb2aeacbf299c1455e10d72891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f3B35VTrazyB38IBdz_l8A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 7. The formula for the custom class weighing scheme</strong></figcaption></figure><p id="2a5a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">其中<strong class="kz ir"> <em class="lt"> c = 1.02 </em> </strong> <br/>并且类权重被限制在<strong class="kz ir"><em class="lt">【1，50】</em></strong>的区间内</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/5fb9a8f64560f11304cd737465c6e8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9eXaBmczBfLpK6FfqaNkJg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 8. Performance on CityScapes Dataset</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/0a66fa21dc6000126f107ca8868984e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yt3HJ3DBMlatHqg4CmtPMA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 9. Performance on CamVid Dataset</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/5a5e81ac0adb922c10bd44631e442927.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjx-_RJd2e3X8WRi0oGi9g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 10. Performance on SUN RGB-D dataset</strong></figcaption></figure><h2 id="912e" class="no lv iq bd lw np nq dn ma nr ns dp me lg nt nu mg lk nv nw mi lo nx ny mk nz bi translated">参考</h2><ol class=""><li id="20a6" class="my mz iq kz b la mm ld mn lg oe lk of lo og ls nd ne nf ng bi translated">A. Paszke，A. Chaurasia，S. Kim 和 E. Culurciello。Enet:用于实时语义分割的深度神经网络架构。arXiv 预印本 arXiv:1606.02147，2016。</li><li id="7f0d" class="my mz iq kz b la nh ld ni lg nj lk nk lo nl ls nd ne nf ng bi translated"><em class="lt"> V. Badrinarayanan，A. Kendall，R. Cipolla，“Segnet:用于图像分割的深度卷积编码器-解码器架构”，arXiv 预印本 arXiv:1511.00561，2015 年。</em></li></ol></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><p id="182c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我最近也转载了这篇论文，可以在这里找到:<a class="ae kv" href="https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation" rel="noopener ugc nofollow" target="_blank">https://github . com/iArunava/ENet-Real-Time-Semantic-Segmentation</a></p></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><blockquote class="oo"><p id="6fa6" class="op oq iq bd or os ot ou ov ow ox ls dk translated">感谢阅读！一定要看报纸！</p><p id="398b" class="op oq iq bd or os ot ou ov ow ox ls dk translated">如果我发现其他有趣的见解，我会更新的！</p></blockquote></div></div>    
</body>
</html>