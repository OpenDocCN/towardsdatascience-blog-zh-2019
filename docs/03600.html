<html>
<head>
<title>Neural Architecture Search (NAS)- The Future of Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经架构搜索(NAS)——深度学习的未来</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-architecture-search-nas-the-future-of-deep-learning-c99356351136?source=collection_archive---------3-----------------------#2019-06-08">https://towardsdatascience.com/neural-architecture-search-nas-the-future-of-deep-learning-c99356351136?source=collection_archive---------3-----------------------#2019-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fd2e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">作为初学者，你需要知道的是</h2></div><p id="50f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(本博客的修订版可以在<a class="ae le" href="https://theaiacademy.blogspot.com/2020/05/neural-architecture-search-nas-future.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">这里找到</strong> </a>)</p><p id="c749" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们大多数人可能都听说过 ResNet 的成功，她是 2015 年 ILSVRC 图像分类、检测和定位奖的获得者，也是 2015 年 COCO MS 检测和分割奖的获得者<strong class="kk iu">。这是一个巨大的建筑，到处都是跳跃连接。当我使用这个 ResNet 作为我的机器学习项目的预训练网络时，我想“怎么会有人提出这样的架构呢？''</strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl lk"><img src="../Images/e81579ed0e3284a30060dd279e31ddfd.png" data-original-src="https://miro.medium.com/v2/format:webp/1*6hF97Upuqg_LdsqWY6n_wg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Large Human Engineered Images Classification Architectures(<a class="ae le" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">image source</a>)</figcaption></figure><p id="5166" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不久之后，我得知许多工程师和科学家凭借他们多年的经验建造了这个建筑。更多的是直觉而不是完整的数学会告诉你“我们现在需要一个 5x5 滤波器来实现最佳精度”。对于图像分类任务，我们有很好的架构，但对于其他任务，我们必须花费大量精力来寻找一种具有合理性能的架构。如果我们能够像学习机器学习模型的参数一样自动化这个架构建模任务，那当然会更好。</p><p id="b170" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经架构搜索(NAS)，自动化架构工程的过程，即找到我们的机器学习模型的设计。我们需要为 NAS 系统提供数据集和任务(分类、回归等)，它将为我们提供体系结构。且当通过所提供数据集进行训练时，对于给定的任务，该架构将在所有其他架构中表现最佳。NAS 可以被视为 AutoML 的一个子领域，与超参数优化有很大的重叠。要了解 NAS，我们需要深入了解它在做什么。它通过遵循<strong class="kk iu">搜索策略</strong>从<strong class="kk iu">所有可能的架构</strong>中找到一个架构，这将<strong class="kk iu">最大化性能</strong>。下图总结了 NAS 算法。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/4f4be91478845cd2af1e6ca9dcb22504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cr1OHTq9lE4GDyOCYeknxQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Dimensions of NAS method(<a class="ae le" href="https://arxiv.org/pdf/1808.05377.pdf" rel="noopener ugc nofollow" target="_blank">reference</a>)</figcaption></figure><p id="47d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它有 3 个独立的维度:搜索空间、搜索策略和性能评估。</p><p id="f69c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">搜索空间</strong>定义了<strong class="kk iu"/>NAS 方法原则上可能发现什么样的神经架构。它可以是链状结构，其中层(n-1)的输出作为层(n)的输入。也可以是跳接的现代复杂架构(多分支网络)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/2bd369363d051ae45b02ffbf17683187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*rxBO_zugD4EGXTzwjPXoOQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">A chain-like and multi-branch network (<a class="ae le" href="https://arxiv.org/pdf/1808.05377.pdf" rel="noopener ugc nofollow" target="_blank">image source</a>)</figcaption></figure><p id="0a38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有时人们确实想使用手工制作的外部建筑(宏观建筑),带有重复的主题或单元。在这种情况下，外部结构是固定的，NAS 只搜索单元体系结构。这种类型的搜索被称为微搜索或小区搜索。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/aff733383f678ef032ce7528317cc945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*gLod-YUqTRInKm90QvI3fA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">left: The cell architecture right: cells are put in the handcrafted outer structure(<a class="ae le" href="https://arxiv.org/pdf/1808.05377.pdf" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><p id="9fc6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在许多 NAS 方法中，微观和宏观结构都以分层的方式进行搜索；它由几个层次的主题组成。第一层由一组基元操作组成，第二层由通过有向非循环图连接基元操作的不同基元组成，第三层基元编码如何连接第二层基元，依此类推。</p><p id="1dd1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解释<strong class="kk iu">搜索策略</strong>和<strong class="kk iu">性能估计，</strong>三种不同的 NAS 方法将在以下部分讨论。</p><p id="69c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">强化学习</strong></p><p id="0d19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强化学习是代理(程序)面临的问题，它必须通过与动态环境的试错交互来学习行为，以最大化某些回报。代理(根据由θ参数化的某个策略执行某个动作。然后，该代理根据对所采取行动的奖励来更新策略θ。在 NAS 的情况下，代理产生模型架构，<em class="ly">子网络</em> ( <em class="ly">动作)。</em>然后在数据集上训练模型，模型在验证数据上的表现作为奖励。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl lk"><img src="../Images/c34390454c131108f2d984041419939a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*hIif88uJ7Te8MJEhm40rbw.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">The controller plays the role of an agent and accuracy is taken as the reward(<a class="ae le" href="https://arxiv.org/pdf/1611.01578.pdf" rel="noopener ugc nofollow" target="_blank">reference</a>)</figcaption></figure><p id="0990" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，递归神经网络(RNN)被视为控制器或代理。它产生字符串，并随机地从该字符串构建模型。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lz"><img src="../Images/5f877432f7ecd08f4056d9a0b9b9c521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jsdG_98K-kCLyqCY4Hxp7w.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">RNN predicts filter height, filter width, stride height, stride width, and the number of filters for one layer and repeats in a form of string. Each prediction is carried out by a softmax classifier and then fed into the next time step as input. The RNN stops after generating a certain number of outputs. (<a class="ae le" href="https://arxiv.org/pdf/1611.01578.pdf" rel="noopener ugc nofollow" target="_blank">image source</a>)</figcaption></figure><p id="a201" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图显示了这种 RNN 的输出示例。卷积神经网络的每一层的超参数重复产生一定次数。然后根据 RNN 的输出建立模型，然后对模型进行训练并计算验证精度。</p><p id="00d7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，RNN 的参数充当了代理人的策略θ。生产管柱(高度、过滤器宽度、步幅高度等)是代理的动作。在这种强化方法中，模型在验证集上的表现是代理人的回报。RNN 以某种方式更新其参数θ，使得所提出的架构的预期验证精度最大化。</p><p id="2eb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过策略梯度方法训练 RNN，迭代更新策略θ。主要目标是最大化预期精度，即</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/51381c35a23cb61b370d7711284385c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*LQm7juMhHjdWpY0AHRsyMA.png"/></div></figure><p id="ead3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里<strong class="kk iu"> R </strong>是模型的精度，<strong class="kk iu"> a_1: T </strong>是 RNN( the action)生产的长度为<strong class="kk iu"> T </strong>的字符串，<strong class="kk iu"> θ </strong>是 RNN 的参数。</p><p id="6847" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于 J(θ)不可微，采用策略梯度法迭代更新θ。更新函数看起来像这样</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/b64698d1a6ef57048dcf37c05f7633d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*z7vnBUhIOkGkbO-R1FacgQ.png"/></div></figure><p id="20d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是原始方程的近似值。用采样代替 J(θ)的期望项。该更新方程的外部求和用于该采样，即从 RNN 采样的<strong class="kk iu"><em class="ly">【m】</em></strong>个模型，并取其平均值。<strong class="kk iu">P(a _ t | a _(t1):1；θc)，</strong>是根据 RNN 用参数<strong class="kk iu"> θ </strong> ( <a class="ae le" href="https://arxiv.org/pdf/1611.01578.pdf" rel="noopener ugc nofollow" target="_blank">参考</a> ) <strong class="kk iu">预测的策略<strong class="kk iu"> θ或</strong>，给定从<strong class="kk iu"> 1 到(t-1) </strong>的所有动作，在时间<strong class="kk iu">t</strong>采取动作<strong class="kk iu"> a_t </strong>的概率。</strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/0faca0650cb572bb947714bdb70af7d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*hHxhu85PiOjVphcFIDpO5Q.jpeg"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Example of strings produced by RNN to create a model(<a class="ae le" href="https://arxiv.org/pdf/1611.01578.pdf" rel="noopener ugc nofollow" target="_blank">reference</a>)</figcaption></figure><p id="a5a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">编码跳过连接有点棘手。为此，每层的 RNN 生成一个名为<strong class="kk iu">锚点</strong>的输出。锚点<strong class="kk iu">点</strong>用于指示跳过连接。在层<em class="ly"> N，</em>处，锚点<em class="ly"> </em>将包含<em class="ly">N-1</em>基于内容的 sigmoids，以指示需要连接的先前层。</p><p id="2bf7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">渐进式神经架构搜索(PNAS) </strong></p><p id="3fe8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PNAS 做了一个小区搜索，如本教程的搜索空间部分所讨论的。他们从块中构造单元，并通过以预定义的方式添加单元来构造完整的网络。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi md"><img src="../Images/72d01fe5d9f3cd8694b2f1372f49d3e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uJzNgTITofNC1P5ure2UdA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Hierarchy of model architecture(<a class="ae le" href="https://cs.jhu.edu/~cxliu/slides/pnas-talk-eccv.pdf" rel="noopener ugc nofollow" target="_blank">reference</a>)</figcaption></figure><p id="8ff0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">细胞以预定的数量串联起来形成网络。并且每个单元由几个块(原始论文中使用的 5 个)形成。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi me"><img src="../Images/154b6ffa269520c3e94635211d88cda0.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*p69zLeq5W3nY4GrxHVBoBA.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(<a class="ae le" href="https://cs.jhu.edu/~cxliu/slides/pnas-talk-eccv.pdf" rel="noopener ugc nofollow" target="_blank">image source</a>)</figcaption></figure><p id="205e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些块由预定义的操作组成。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mf"><img src="../Images/f13bae602c18da115efa914379b3b93f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dDkL1l47D6HwVcjRYqHosw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">structure of a block. The <strong class="bd mg">Combination </strong>function is just an element-wise addition (reference)</figcaption></figure><p id="492f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图中所示的操作在原始文件中使用，可以扩展。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mh"><img src="../Images/f977c18ed1ae1a4f5dd2cacecc445aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pq1bgTkcWfDOfE86NWbz5Q.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">the complete structure of a Cell(<a class="ae le" href="https://cs.jhu.edu/~cxliu/slides/pnas-talk-eccv.pdf" rel="noopener ugc nofollow" target="_blank">reference</a>)</figcaption></figure><p id="6a0d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">左侧显示了一个完整的示例。甚至在这个单元或微搜索中，总共有 10 个⁴有效组合要检查以找到最佳单元结构。</p><p id="7621" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，为了降低复杂性，首先只构建只有一个块的单元。这很容易，因为在上述操作中，只有 256 个不同的单元是可能的。然后，选择前<strong class="kk iu"> K 个</strong>表现最好的单元来扩展 2 个块单元，并且重复直到 5 个块。</p><p id="fced" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是对于一个合理的<strong class="kk iu"> K </strong>来说，要训练的 2 挡候选人太多了。作为这个问题的解决方案，训练了一个“廉价的”代理模型，它只需通过读取字符串(<em class="ly">单元格被编码为字符串</em>)来预测最终性能。这种训练的数据是在构建、训练和验证单元时收集的。</p><p id="3a40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，我们可以构建所有 256 个单块单元，并测量它们的性能。并用这些数据训练代理模型。然后使用该模型预测 2 个块单元的性能，而无需训练或测试它们。当然，代理模型应该能够处理可变大小的输入。然后选择由模型预测的前 K 个最佳性能的 2 个块单元。然后对这两个块单元进行训练，对“代理”模型进行微调，将这些单元扩展到 3 个块，并进行迭代，直到得到 5 个块单元。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mi"><img src="../Images/5653c1e448bb08fbcf99aa09c1323971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R3jY5TK6qmuzqOO0au6BVA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Steps in PNAS (reference)</figcaption></figure><p id="ff33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">差异化架构搜索(DARTS) </strong></p><p id="88a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经架构的搜索空间是离散的，即一个架构与另一个架构至少有一层或该层中的一些参数不同，例如，<em class="ly"> 5x5 </em>滤波器与<em class="ly"> 7x7 </em>滤波器。在这种方法中，连续松弛被应用于这种离散搜索，这使得能够进行直接的基于梯度的优化。</p><p id="651c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们搜索的单元可以被视为有向非循环图，其中每个节点<strong class="kk iu"> <em class="ly"> x </em> </strong>是潜在表示(例如卷积网络中的特征图)，并且每个有向<strong class="kk iu"> <em class="ly">边(I，j) </em> </strong>与一些操作<strong class="kk iu"> o(i，j) </strong>(卷积、最大池化等)相关联，这些操作变换<strong class="kk iu"><em class="ly"/></strong>并且在节点<strong class="kk iu">处存储潜在表示</strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/0c4060ba5ff883aa0cdb69a178704694.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*-RyYy4caGkegCZbH5qgjAg.png"/></div></figure><p id="0335" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个节点的输出可以通过左边的等式计算。节点是这样枚举的，从节点<strong class="kk iu"><em class="ly"/></strong>到<strong class="kk iu"><em class="ly">x(j)</em></strong>再到<strong class="kk iu"> i &lt; j </strong>有一条<strong class="kk iu"> <em class="ly">边(I，j) </em> </strong>。</p><p id="6227" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在连续松弛中，而不是在两个节点之间进行单一操作。使用每个可能操作的凸组合。为了在图中对此进行建模，在两个节点之间保留了多条边，每条边对应一个特定的操作。而且每条边还有一个权重<strong class="kk iu"> <em class="ly"> α </em> </strong>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/7453cc29f5dbd1dab7df445361250905.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*R84oGRqPaM-64K4BH1KznA.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">continuous relaxation of the discrete problem(<a class="ae le" href="https://arxiv.org/pdf/1806.09055" rel="noopener ugc nofollow" target="_blank">reference</a>)</figcaption></figure><p id="bf85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在<strong class="kk iu"> O(i，j) </strong>节点 x(i)和 x(j)之间的运算是一组运算的凸组合<strong class="kk iu"> o(i，j) </strong>其中<strong class="kk iu"> o(。)</strong> ϵ <strong class="kk iu"> S </strong>，其中 s 是所有可能操作的集合。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/fc8e777256d7b443fc27160f84cb3784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*ZDixS-R-GDw8gtO5R9A9QA.png"/></div></figure><p id="bb2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> O(i，j) </strong>的输出由左式<strong class="kk iu">计算。</strong></p><p id="5689" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练和验证损失分别用<strong class="kk iu"> <em class="ly"> L_train </em> </strong>和<strong class="kk iu"> <em class="ly"> L_val </em> </strong>表示。这两个损耗不仅由架构参数<strong class="kk iu"> α </strong>决定，还由网络中的权重<strong class="kk iu">‘w’</strong>决定。架构搜索的目标是找到最小化验证损失的<strong class="kk iu">α∫</strong><strong class="kk iu"><em class="ly">L _ val</em>(w∫，α∫)</strong>，其中通过最小化训练损失来获得与架构相关的权重<strong class="kk iu">‘w∫’</strong>。</p><p id="1b2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">w∫</strong>= arg min<strong class="kk iu"><em class="ly">L _ train</em>(w，α∫)</strong>。</p><p id="c535" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着一个以<strong class="kk iu"> <em class="ly"> α </em> </strong>为上层变量，以<strong class="kk iu"> <em class="ly"> w </em> </strong>为下层变量的双层优化问题:</p><p id="ece2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="ly">α*</em></strong>= arg min<strong class="kk iu"><em class="ly">L _ val</em>(w∫(α)，α) </strong></p><p id="82da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">s . t .<strong class="kk iu">w∫(α)</strong>= arg min<strong class="kk iu"><em class="ly">L _ train</em></strong>(w，α)</p><p id="c616" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练后，一些边的一些α变得比其它边大得多。为了导出该连续模型的离散架构，在两个节点之间，保留具有最大权重的唯一边。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mm"><img src="../Images/64a726ba81e54e5576d3da5b481f9dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0hN1wLBbc4gxTllRAVV-NA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">a) Operations on the edges are initially unknown. b) Continuous relaxation of the search space by placing a mixture of candidate operations on each edge c) some weights increases and some falls during bilevel optimization d) final architecture is constructed only by taking edges with the maximum weight between two nodes. (<a class="ae le" href="https://arxiv.org/pdf/1806.09055" rel="noopener ugc nofollow" target="_blank">reference</a>)</figcaption></figure><p id="513c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当这些细胞被发现时，它们就被用来构建更大的网络。</p><p id="0bce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有许多其他方法也用于神经结构搜索。他们取得了令人印象深刻的成绩。但是，我们仍然对回答“为什么一个特定的架构工作得更好？”这个问题知之甚少。我想我们正在回答这个问题。</p></div></div>    
</body>
</html>