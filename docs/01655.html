<html>
<head>
<title>How to implement Seq2Seq LSTM Model in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在 Keras 中实现 Seq2Seq LSTM 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639?source=collection_archive---------4-----------------------#2019-03-18">https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639?source=collection_archive---------4-----------------------#2019-03-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d3a8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如果你被尺寸问题困扰，这是给你的</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/79e263b3f8d7d32f1c38d4dd302fe5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*sVg3CU8qDpjmLGBspTnFUw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><a class="ae kr" href="https://www.techleer.com/articles/440-keras-deep-learning-for-python/" rel="noopener ugc nofollow" target="_blank">Keras: Deep Learning for Python</a></figcaption></figure><h1 id="8359" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">为什么需要看这个？</h1><p id="bb3a" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">当我第一次尝试为聊天机器人任务实现 seq2seq 时，我卡了很多次，特别是关于输入数据的维度和神经网络架构的输入层。</p><p id="1de4" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">现在我明白了，除非你对矩阵、张量等线性代数概念，或者 Keras API 是如何工作的有很深的理解，否则你会不断地得到错误(而且那太惨了！).</p><p id="4d1f" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">因此在本文中，我将通过仔细检查每个过程中的输入和输出，来解释 seq2seq for Keras 的完整分步指南。</p><p id="7f75" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">此外，我将提供一些可移植的原始函数，您可以在任何其他 NLP 项目中使用这些函数进行预处理。</p><p id="f48d" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们开始吧！</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h1 id="1310" class="ks kt iq bd ku kv ms kx ky kz mt lb lc jw mu jx le jz mv ka lg kc mw kd li lj bi translated">菜单</h1><ol class=""><li id="8dff" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf nc nd ne nf bi translated">Seq2Seq 文本生成模型是什么？</li><li id="07bb" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">任务定义和 Seq2Seq 建模</li><li id="a74d" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">Seq2Seq 中各层的尺寸</li><li id="ffc2" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">Seq2Seq 的预处理(在聊天机器人的情况下)</li><li id="080b" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">最简单的代码预处理:您现在就可以使用！</li></ol></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h1 id="6a73" class="ks kt iq bd ku kv ms kx ky kz mt lb lc jw mu jx le jz mv ka lg kc mw kd li lj bi translated">1.Seq2Seq 文本生成模型是什么？</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nl"><img src="../Images/9a68689217c6b73b6d087b2aebd2f89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LYGO4IxqUYftFdAccg5fVQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig A — Encoder-Decoder training architecture for NMT — image copyright@<a class="nq nr ep" href="https://medium.com/u/c3f8c66f5451?source=post_page-----6f355f3e5639--------------------------------" rel="noopener" target="_blank">Ravindra Kompella</a></figcaption></figure><blockquote class="ns"><p id="fbce" class="nt nu iq bd nv nw nx ny nz oa ob mf dk translated">Seq2Seq 是一种使用 RNN 的编码器-解码器模型。它可以作为机器交互和机器翻译的模型。</p></blockquote><p id="442f" class="pw-post-body-paragraph lk ll iq lm b ln oc jr lp lq od ju ls lt oe lv lw lx of lz ma mb og md me mf ij bi translated">通过学习大量的序列对，该模型从另一个生成一个。更友好地解释一下，Seq2Seq 的 I/O 如下:</p><ul class=""><li id="4c81" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated">输入:文本数据的句子，例如“你好吗？”</li><li id="ee8d" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf ok nd ne nf bi translated">输出:文本数据的句子，例如“还不错”</li></ul><p id="8e3c" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">这里是 Seq2Seq 的业务应用示例:</p><ul class=""><li id="9fa1" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated">一个聊天机器人(你可以从我的 GitHub 中找到它)</li><li id="324f" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf ok nd ne nf bi translated">机器翻译(可以从我的<a class="ae kr" href="https://github.com/samurainote/seq2seq_translate_slackbot" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中找到)</li><li id="4158" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf ok nd ne nf bi translated">问题回答</li><li id="ef16" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf ok nd ne nf bi translated">摘要文本摘要(可以从我的<a class="ae kr" href="https://github.com/samurainote/Text_Summarization_using_Bidirectional_LSTM" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中找到)</li><li id="4bb6" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf ok nd ne nf bi translated">文本生成(可以从我的<a class="ae kr" href="https://github.com/samurainote/Text_Generation_using_GRU" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中找到)</li></ul><p id="d535" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">如果你想了解更多关于 Seq2Seq 的信息，这里我有一个来自 Youtube 上微软的<a class="ae kr" href="https://www.youtube.com/channel/UCXvHuBMbgJw67i5vrMBBobA" rel="noopener ugc nofollow" target="_blank">机器学习的推荐。</a></p><p id="d0dc" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">现在我们可以理解这项技术的多功能性，所以让我们来看看整个过程！</p><p id="b39a" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="8f79" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">2.任务定义和 Seq2Seq 建模</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ol"><img src="../Images/33408ea7eec7bbd332c0efc34d72ce59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CkeGXClZ5Xs0MhBc7xFqSA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><a class="ae kr" href="https://www.oreilly.com/library/view/deep-learning-essentials/9781785880360/b71e37fb-5fd9-4094-98c8-04130d5f0771.xhtml" rel="noopener ugc nofollow" target="_blank">https://www.oreilly.com/library/view/deep-learning-essentials/9781785880360/b71e37fb-5fd9-4094-98c8-04130d5f0771.xhtml</a></figcaption></figure><p id="edad" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">为了训练我们的 seq2seq 模型，我们将使用<a class="ae kr" href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html" rel="noopener ugc nofollow" target="_blank">康奈尔电影对话语料库数据集</a>，该数据集包含 10，292 对电影角色之间超过 220，579 次对话交流。它涉及 617 部电影中的 9035 个角色。</p><p id="af80" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">这是数据集中的一段对话:</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="113c" class="or kt iq on b gy os ot l ou ov">Mike: <br/>"Drink up, Charley. We're ahead of you."</span><span id="b853" class="or kt iq on b gy ow ot l ou ov">Charley: <br/>"I'm not thirsty."</span></pre><p id="51fc" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">然后我们将这些对话对输入编码器和解码器。这意味着我们的神经网络模型有两个输入层，如下图所示。</p><p id="3e3d" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">这是我们这次的 Seq2Seq 神经网络架构:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">copyright Akira Takezawa</figcaption></figure><p id="0d76" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">让我们用 LSTM 来形象化我们的 Seq2Seq:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oz"><img src="../Images/ed10975c1befdb11a926efc5095c1ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7EUe3SzWJgDbJsI0BISIUQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">copyright Akira Takezawa</figcaption></figure><h1 id="95c9" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">3.Seq2Seq 中各层的尺寸</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/991c8d5c09b27799552964f8aac771b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*DRwOwEdpT9wygpfY1DVhVA.jpeg"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><a class="ae kr" href="https://bracketsmackdown.com/word-vector.html" rel="noopener ugc nofollow" target="_blank">https://bracketsmackdown.com/word-vector.html</a></figcaption></figure><p id="7375" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">“NLP 新手”的黑盒是这样的:</p><blockquote class="ns"><p id="c85d" class="nt nu iq bd nv nw pb pc pd pe pf mf dk translated">每一层如何编译数据并改变它们的数据维度？</p></blockquote><p id="4653" class="pw-post-body-paragraph lk ll iq lm b ln oc jr lp lq od ju ls lt oe lv lw lx of lz ma mb og md me mf ij bi translated">为了说明这一点，我将详细解释它是如何工作的。这些层可以分为 5 个不同的部分:</p><ol class=""><li id="3598" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf nc nd ne nf bi translated">输入层(编码器和解码器)</li><li id="8217" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">嵌入层(编码器和解码器)</li><li id="dbb0" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">LSTM 层(编码器和解码器)</li><li id="11c2" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">解码器输出层</li></ol><p id="c5e5" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们开始吧！</p><h1 id="a5b1" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">1.编码器和解码器的输入层(2D-&gt;2D)</h1><ul class=""><li id="01a8" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated"><strong class="lm ir">输入层维度:2D (sequence_length，无)</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="6fbf" class="or kt iq on b gy os ot l ou ov"># 2D<br/>encoder_input_layer = Input(shape=(sequence_length, ))<br/>decoder_input_layer = Input(shape=(sequence_length, ))</span></pre><p id="5545" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><em class="pg">注意:sequence_length 是 MAX_LEN 在预处理中通过填充统一的</em></p><ul class=""><li id="12fb" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入数据:2D(样本数量，最大序列长度)</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="38be" class="or kt iq on b gy os ot l ou ov"># Input_Data.shape = (150000, 15)</span><span id="dcc8" class="or kt iq on b gy ow ot l ou ov">array([[ 1, 32, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br/>       [ 123, 56, 3, 34, 43, 345, 0, 0, 0, 0, 0, 0, 0],<br/>       [ 3, 22, 1, 6543, 58, 6, 435, 0, 0, 0, 0, 0, 0],<br/>       [ 198, 27, 2, 94, 67, 98, 0, 0, 0, 0, 0, 0, 0],<br/>       [ 45, 78, 2, 23, 43, 6, 45, 0, 0, 0, 0, 0, 0]<br/>        ], dtype=int32)</span></pre><p id="4371" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><em class="pg">注意:样本数量可以是训练数据的长度(150000) </em></p><ul class=""><li id="d33e" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出数据:2D </strong></li></ul><p id="0e04" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><em class="pg">注意:</em> <code class="fe ph pi pj on b">Input()</code> <em class="pg">仅用于 Keras 张量实例化</em></p><p id="005d" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="97f0" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">2.编码器和解码器的嵌入层(2D-&gt;3D)</h1><ul class=""><li id="eb08" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated"><strong class="lm ir">嵌入层维度:2D (sequence_length，vocab_size) </strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="79ea" class="or kt iq on b gy os ot l ou ov">embedding_layer = Embedding(input_dim = <strong class="on ir">vocab_size</strong>,<br/>                            output_dim = <strong class="on ir">embedding_dimension</strong>, <br/>                            input_length = <strong class="on ir">sequence_length</strong>)</span></pre><p id="c997" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><em class="pg">注:</em><strong class="lm ir"><em class="pg">vocab _ size</em></strong><em class="pg">是数字的唯一字</em></p><ul class=""><li id="39a1" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入数据:2D (sequence_length，vocab_size) </strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="c9e7" class="or kt iq on b gy os ot l ou ov"># Input_Data.shape = (15, 10000)</span><span id="bb1f" class="or kt iq on b gy ow ot l ou ov">array([[ 1, 1, 0, 0, 1, 0, ...... 0, 0, 1, 0, 0, 0, 0],<br/>       [ 0, 0, 1, 0, 0, 1, ...... 0, 0, 0, 0, 0, 0, 1],<br/>       [ 0, 1, 0, 0, 0, 0, ...... 0, 0, 1, 0, 0, 0, 0],<br/>       [ 0, 1, 0, 0, 0, 1, ...... 0, 0, 0, 1, 0, 1, 0],<br/>       [ 0, 0, 1, 0, 1, 0, ...... 0, 0, 1, 0, 1, 0, 0]<br/>        ], dtype=int32)</span></pre><p id="3c37" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><em class="pg">注意:数据应该是一组单热向量</em></p><ul class=""><li id="975d" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出数据:3D(样本数，序列长度，嵌入尺寸)</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="aa25" class="or kt iq on b gy os ot l ou ov"># Output_Data.shape = (150000, 15, 50)</span><span id="60f4" class="or kt iq on b gy ow ot l ou ov">array([[[ 1, 1, 0, 0, ...... 0, 1, 0, 0],<br/>        [ 0, 0, 1, 0, ...... 0, 0, 0, 1],<br/>         ..., <br/>         ..., <br/>        [ 0, 1, 0, 0, ...... 1, 0, 1, 0],<br/>        [ 0, 0, 1, 0, ...... 0, 1, 0, 0]], <br/></span><span id="ebae" class="or kt iq on b gy ow ot l ou ov">       [[ 1, 1, 0, 0, ...... 0, 1, 0, 0],<br/>        [ 0, 0, 1, 0, ...... 0, 0, 0, 1],<br/>         ..., <br/>         ..., <br/>        [ 0, 1, 0, 0, ...... 1, 0, 1, 0],<br/>        [ 0, 0, 1, 0, ...... 0, 1, 0, 0]],</span><span id="c4b7" class="or kt iq on b gy ow ot l ou ov">        ....,] * 150000      , dtype=int32)</span></pre><p id="acfd" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><em class="pg">注:数据是嵌入 50 维的字</em></p><p id="1bde" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="3e81" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">3.编码器和解码器的 LSTM 层(3D-&gt;3D)</h1><p id="ebcc" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">LSTM 层的棘手论点是这两个:</p><blockquote class="ns"><p id="be19" class="nt nu iq bd nv nw pb pc pd pe pf mf dk translated">1.返回状态:</p><p id="0ba4" class="nt nu iq bd nv nw pb pc pd pe pf mf dk translated">是否随输出一起返回上一个状态</p><p id="ee83" class="nt nu iq bd nv nw pb pc pd pe pf mf dk translated"><strong class="ak"> 2。返回序列</strong>:</p><p id="4da8" class="nt nu iq bd nv nw pb pc pd pe pf mf dk translated">是否返回输出序列的最后一个输出或完整序列</p></blockquote><p id="528c" class="pw-post-body-paragraph lk ll iq lm b ln oc jr lp lq od ju ls lt oe lv lw lx of lz ma mb og md me mf ij bi translated">你可以从<a class="ae kr" href="https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/" rel="noopener ugc nofollow" target="_blank"> <strong class="lm ir">中找到很好的解释理解 Keras </strong> </a> <strong class="lm ir"> </strong>作者<a class="ae kr" href="https://machinelearningmastery.com/author/jasonb/" rel="noopener ugc nofollow" target="_blank"><strong class="lm ir">Jason Brownlee</strong></a>中 LSTMs 的返回序列和返回状态的区别。</p><ul class=""><li id="e105" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">图层维度:3D (hidden_units，sequence_length，embedding_dims) </strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="8f56" class="or kt iq on b gy os ot l ou ov"># HIDDEN_DIM = 20</span><span id="fa98" class="or kt iq on b gy ow ot l ou ov">encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)</span><span id="4b43" class="or kt iq on b gy ow ot l ou ov">decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)   <br/>decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])</span></pre><ul class=""><li id="41ae" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入数据:3D(样本数，序列长度，嵌入尺寸)</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="1f4c" class="or kt iq on b gy os ot l ou ov"># Input_Data.shape = (150000, 15, 50)</span><span id="be24" class="or kt iq on b gy ow ot l ou ov">array([[[ 1, 1, 0, 0, ...... 0, 1, 0, 0],<br/>        [ 0, 0, 1, 0, ...... 0, 0, 0, 1],<br/>         ..., <br/>         ..., <br/>        [ 0, 1, 0, 0, ...... 1, 0, 1, 0],<br/>        [ 0, 0, 1, 0, ...... 0, 1, 0, 0]],</span><span id="abc2" class="or kt iq on b gy ow ot l ou ov">       [[ 1, 1, 0, 0, ...... 0, 1, 0, 0],<br/>        [ 0, 0, 1, 0, ...... 0, 0, 0, 1],<br/>         ..., <br/>         ..., <br/>        [ 0, 1, 0, 0, ...... 1, 0, 1, 0],<br/>        [ 0, 0, 1, 0, ...... 0, 1, 0, 0]],</span><span id="f6d5" class="or kt iq on b gy ow ot l ou ov">          ....,] * 150000      , dtype=int32)</span></pre><p id="58e4" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><em class="pg">注:数据是嵌入 50 维的字</em></p><ul class=""><li id="3936" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出数据:3D(样本数，序列长度，隐藏单位)</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="0c36" class="or kt iq on b gy os ot l ou ov"># HIDDEN_DIM = 20<br/># Output_Data.shape = (150000, 15, 20)</span><span id="9342" class="or kt iq on b gy ow ot l ou ov">array([[[ 0.0032, 0.0041, 0.0021, .... 0.0020, 0.0231, 0.0010],<br/>        [ 0.0099, 0.0007, 0.0098, .... 0.0038, 0.0035, 0.0026],<br/>         ..., <br/>         ..., <br/>        [ 0.0099, 0.0007, 0.0098, .... 0.0038, 0.0035, 0.0026],<br/>        [ 0.0021, 0.0065, 0.0008, .... 0.0089, 0.0043, 0.0024]],</span><span id="4a01" class="or kt iq on b gy ow ot l ou ov">        [ 0.0032, 0.0041, 0.0021, .... 0.0020, 0.0231, 0.0010],<br/>        [ 0.0099, 0.0007, 0.0098, .... 0.0038, 0.0035, 0.0026],<br/>         ..., <br/>         ..., <br/>        [ 0.0099, 0.0007, 0.0098, .... 0.0038, 0.0035, 0.0026],<br/>        [ 0.0021, 0.0065, 0.0008, .... 0.0089, 0.0043, 0.0024]],</span><span id="77d0" class="or kt iq on b gy ow ot l ou ov">         ....,] * 150000      ,   dtype=int32)</span></pre><p id="5ad2" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><em class="pg">注:数据被 LSTM 整形为 20 维隐藏层</em></p><p id="54eb" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><strong class="lm ir">附加信息:</strong></p><p id="9f95" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">如果<code class="fe ph pi pj on b">return_state = False</code>和<code class="fe ph pi pj on b">return_sequences = False</code>:</p><ul class=""><li id="b60e" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出数据:2D(数量 _ 样本，隐藏 _ 单位)</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="adfd" class="or kt iq on b gy os ot l ou ov"># HIDDEN_DIM = 20<br/># Output_Data.shape = (150000, 20)</span><span id="6442" class="or kt iq on b gy ow ot l ou ov">array([[ 0.0032, 0.0041, 0.0021, .... 0.0020, 0.0231, 0.0010],<br/>       [ 0.0076, 0.0767, 0.0761, .... 0.0098, 0.0065, 0.0076],<br/>         ..., <br/>         ..., <br/>       [ 0.0099, 0.0007, 0.0098, .... 0.0038, 0.0035, 0.0026],<br/>       [ 0.0021, 0.0065, 0.0008, .... 0.0089, 0.0043, 0.0024]]<br/>       , dtype=float32)</span></pre><p id="5ea4" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="cf80" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">4.解码器输出层(3D-&gt;2D)</h1><ul class=""><li id="53c1" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated"><strong class="lm ir">输出层维度:2D (sequence_length，vocab_size) </strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="017e" class="or kt iq on b gy os ot l ou ov">outputs = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(decoder_outputs)</span></pre><p id="f25d" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><em class="pg">注意:时间分布密度</em>层<em class="pg">允许我们将一个层应用到输入</em>的每个时间片</p><ul class=""><li id="3ccd" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入数据:3D(样本数，序列长度，隐藏单元数)</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="721f" class="or kt iq on b gy os ot l ou ov"># HIDDEN_DIM = 20<br/># Input_Data.shape = (150000, 15, 20)</span><span id="2005" class="or kt iq on b gy ow ot l ou ov">array([[[ 0.0032, 0.0041, 0.0021, .... 0.0020, 0.0231, 0.0010],<br/>        [ 0.0099, 0.0007, 0.0098, .... 0.0038, 0.0035, 0.0026],<br/>         ..., <br/>         ..., <br/>        [ 0.0099, 0.0007, 0.0098, .... 0.0038, 0.0035, 0.0026],<br/>        [ 0.0021, 0.0065, 0.0008, .... 0.0089, 0.0043, 0.0024]],</span><span id="2d73" class="or kt iq on b gy ow ot l ou ov">[ 0.0032, 0.0041, 0.0021, .... 0.0020, 0.0231, 0.0010],<br/>        [ 0.0099, 0.0007, 0.0098, .... 0.0038, 0.0035, 0.0026],<br/>         ..., <br/>         ..., <br/>        [ 0.0099, 0.0007, 0.0098, .... 0.0038, 0.0035, 0.0026],<br/>        [ 0.0021, 0.0065, 0.0008, .... 0.0089, 0.0043, 0.0024]],</span><span id="e73d" class="or kt iq on b gy ow ot l ou ov">....,] * 150000      ,   dtype=int32)</span></pre><p id="189f" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><em class="pg">注:数据被 LSTM 整形为 20 维隐藏层</em></p><ul class=""><li id="98f0" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出数据:2D (sequence_length，vocab_size) </strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="c455" class="or kt iq on b gy os ot l ou ov"># Output_Data.shape = (15, 10000)</span><span id="8743" class="or kt iq on b gy ow ot l ou ov">array([[ 1, 1, 0, 0, 1, 0, ...... 0, 0, 1, 0, 0, 0, 0],<br/>       [ 0, 0, 1, 0, 0, 1, ...... 0, 0, 0, 0, 0, 0, 1],<br/>       [ 0, 1, 0, 0, 0, 0, ...... 0, 0, 1, 0, 0, 0, 0],<br/>       [ 0, 1, 0, 0, 0, 1, ...... 0, 0, 0, 1, 0, 1, 0],<br/>       [ 0, 0, 1, 0, 1, 0, ...... 0, 0, 1, 0, 1, 0, 0]<br/>        ], dtype=int32)</span></pre><p id="b72d" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">在数据通过这个完全连接的层之后，我们使用<strong class="lm ir">反向词汇</strong>，我将在后面解释它来从一个热点向量转换成单词序列。</p><p id="e6d0" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="8f6b" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">4.Seq2Seq 的整个预处理(在聊天机器人的情况下)</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/677ab0c056f2fe8a44db63f96bc0c77c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*ZX16gz2plU0AgIPDMPOYKA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Creating A Language Translation Model Using Sequence To Sequence Learning Approach</figcaption></figure><p id="d3f8" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">在开始 Seq2Seq 的预处理之前，我想提一下:</p><blockquote class="ns"><p id="aabe" class="nt nu iq bd nv nw pb pc pd pe pf mf dk translated">在数据预处理过程中，我们需要一些变量来定义我们的 Seq2Seq 神经网络的形状</p></blockquote><ol class=""><li id="8533" class="mx my iq lm b ln oc lq od lt pl lx pm mb pn mf nc nd ne nf bi translated">MAX_LEN:统一输入句子的长度</li><li id="b5a0" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">VOCAB_SIZE:决定句子的一个热点向量的维数</li><li id="a5d5" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">EMBEDDING_DIM:决定 Word2Vec 的尺寸</li></ol><p id="9e31" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><p id="8c13" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><strong class="lm ir">seq 2 seq 的预处理</strong></p><p id="4870" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">好了，请把这些信息记在心里，我们开始讲预处理。整个过程可以分为 8 个步骤:</p><ol class=""><li id="6bd1" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf nc nd ne nf bi translated">文本清理</li><li id="3aa0" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">为解码器输入放置<bos>标签和<eos>标签</eos></bos></li><li id="43e6" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">制作词汇表(VOCAB_SIZE)</li><li id="d422" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">将单词包标记为 id 包</li><li id="8085" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">填充(MAX_LEN)</li><li id="e402" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">单词嵌入(EMBEDDING_DIM)</li><li id="849a" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">重塑数据取决于神经网络的形状</li><li id="ea99" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">为培训和验证、测试拆分数据</li></ol><p id="545f" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="dd91" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated"><strong class="ak"> 1。文本清理</strong></h1><ul class=""><li id="3153" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated"><strong class="lm ir">功能</strong></li></ul><p id="159b" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我总是使用我自己的函数来清理 Seq2Seq 的文本:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div></figure><ul class=""><li id="9027" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="9999" class="or kt iq on b gy os ot l ou ov"># encoder input text data</span><span id="d5ad" class="or kt iq on b gy ow ot l ou ov">["Drink up, Charley. We're ahead of you.",<br/> 'Did you change your hair?',<br/> 'I believe I have found a faster way.']</span></pre><ul class=""><li id="0223" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="2110" class="or kt iq on b gy os ot l ou ov"># encoder input text data</span><span id="6be3" class="or kt iq on b gy ow ot l ou ov">['drink up charley we are ahead of you',<br/> 'did you change your hair',<br/> 'i believe i have found a faster way']</span></pre><p id="dcea" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="8173" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">2.为解码器输入放置<bos>标签和<eos>标签</eos></bos></h1><ul class=""><li id="b0c1" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated"><strong class="lm ir">功能</strong></li></ul><p id="cfe6" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><bos>表示“序列开始”，<eos>表示“序列结束”。</eos></bos></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div></figure><ul class=""><li id="a1b6" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="ff19" class="or kt iq on b gy os ot l ou ov"># decoder input text data</span><span id="0044" class="or kt iq on b gy ow ot l ou ov">[['with the teeth of your zipper',<br/>  'so they tell me',<br/>  'so  which dakota you from'],,,,]</span></pre><ul class=""><li id="0541" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="b261" class="or kt iq on b gy os ot l ou ov"># decoder input text data</span><span id="2906" class="or kt iq on b gy ow ot l ou ov">[['&lt;BOS&gt; with the teeth of your zipper &lt;EOS&gt;',<br/>  '&lt;BOS&gt; so they tell me &lt;EOS&gt;',<br/>  '&lt;BOS&gt; so  which dakota you from &lt;EOS&gt;'],,,,]</span></pre><p id="2929" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="e98a" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">3.制作词汇表(VOCAB_SIZE)</h1><ul class=""><li id="78e6" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated"><strong class="lm ir">功能</strong></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div></figure><ul class=""><li id="c550" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="12a9" class="or kt iq on b gy os ot l ou ov"># Cleaned texts</span><span id="8b75" class="or kt iq on b gy ow ot l ou ov">[['with the teeth of your zipper',<br/>  'so they tell me',<br/>  'so  which dakota you from'],,,,]</span></pre><ul class=""><li id="4eec" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="6cc6" class="or kt iq on b gy os ot l ou ov">&gt;&gt;&gt; word2idx</span><span id="f2c0" class="or kt iq on b gy ow ot l ou ov">{'genetically': 14816,<br/> 'ecentus': 64088,<br/> 'houston': 4172,<br/> 'cufflinks': 30399,<br/> "annabelle's": 23767,<br/> .....} # 14999 words</span><span id="268f" class="or kt iq on b gy ow ot l ou ov">&gt;&gt;&gt; idx2word</span><span id="57fc" class="or kt iq on b gy ow ot l ou ov">{1: 'bos',<br/> 2: 'eos',<br/> 3: 'you',<br/> 4: 'i',<br/> 5: 'the',<br/> .....} # 14999 indexs</span></pre><p id="f55b" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="f1ba" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">4.将单词包标记为 id 包</h1><ul class=""><li id="4b4f" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated">功能</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div></figure><ul class=""><li id="35b1" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="1eaa" class="or kt iq on b gy os ot l ou ov"># Cleaned texts</span><span id="c323" class="or kt iq on b gy ow ot l ou ov">[['with the teeth of your zipper',<br/>  'so they tell me',<br/>  'so  which dakota you from'],,,,]</span></pre><ul class=""><li id="1e46" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="4bc0" class="or kt iq on b gy os ot l ou ov"># decoder input text data</span><span id="a65c" class="or kt iq on b gy ow ot l ou ov">[[10, 27, 8, 4, 27, 1107, 802],<br/> [3, 5, 186, 168],<br/> [662, 4, 22, 346, 6, 130, 3, 5, 2407],,,,,]</span></pre><p id="42c6" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="68df" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">5.填充(MAX_LEN)</h1><ul class=""><li id="d01b" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated"><strong class="lm ir">功能</strong></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div></figure><ul class=""><li id="2435" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="d958" class="or kt iq on b gy os ot l ou ov"># decoder input text data</span><span id="a0a5" class="or kt iq on b gy ow ot l ou ov">[[10, 27, 8, 4, 27, 1107, 802],<br/> [3, 5, 186, 168],<br/> [662, 4, 22, 346, 6, 130, 3, 5, 2407],,,,,]</span></pre><ul class=""><li id="abac" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="5169" class="or kt iq on b gy os ot l ou ov"># MAX_LEN = 10<br/># decoder input text data </span><span id="fc3a" class="or kt iq on b gy ow ot l ou ov">array([[10, 27, 8, 4, 27, 1107, 802, 0, 0, 0],<br/>       [3, 5, 186, 168, 0, 0, 0, 0, 0, 0],<br/>       [662, 4, 22, 346, 6, 130, 3, 5, 2407, 0],,,,,]</span></pre><p id="d1a1" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="8d10" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">6.单词嵌入(EMBEDDING_DIM)</h1><ul class=""><li id="881d" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated"><strong class="lm ir">功能</strong></li></ul><p id="8b0e" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们使用手套的预训练和 Word2Vec 模型。我们可以通过 3 个步骤用手套创建嵌入层:</p><ol class=""><li id="9200" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf nc nd ne nf bi translated">从 XX 调用手套文件</li><li id="a2c5" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">从我们的词汇表中创建嵌入矩阵</li><li id="68c4" class="mx my iq lm b ln ng lq nh lt ni lx nj mb nk mf nc nd ne nf bi translated">创建嵌入层</li></ol><p id="5e87" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">我们来看看吧！</p><ul class=""><li id="6229" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">从 XX </strong>调用手套文件</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div></figure><ul class=""><li id="707a" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">从我们的词汇中创建嵌入矩阵</strong></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div></figure><ul class=""><li id="0357" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">创建嵌入层</strong></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="8dd2" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="cce5" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">7.将数据重塑为神经网络形状</h1><ul class=""><li id="a582" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated"><strong class="lm ir">功能</strong></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div></figure><ul class=""><li id="23c6" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="8813" class="or kt iq on b gy os ot l ou ov"># MAX_LEN = 10<br/># decoder input text data</span><span id="e78f" class="or kt iq on b gy ow ot l ou ov">array([[10, 27, 8, 4, 27, 1107, 802, 0, 0, 0],<br/>       [3, 5, 186, 168, 0, 0, 0, 0, 0, 0],<br/>       [662, 4, 22, 346, 6, 130, 3, 5, 2407, 0],,,,,]</span></pre><ul class=""><li id="c45d" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="be09" class="or kt iq on b gy os ot l ou ov"># output.shape (num_samples, MAX_LEN, VOCAB_SIZE)<br/># decoder_output_data.shape (15000, 10, 15000)</span><span id="91ba" class="or kt iq on b gy ow ot l ou ov">array([[[0., 0., 0., ..., 0., 0., 0.],<br/>        [0., 0., 0., ..., 0., 0., 0.],<br/>        [0., 0., 1., ..., 0., 0., 0.],<br/>        ...,<br/>        [1., 0., 0., ..., 0., 0., 0.],<br/>        [1., 0., 0., ..., 0., 0., 0.],<br/>        [1., 0., 0., ..., 0., 0., 0.]],</span><span id="5501" class="or kt iq on b gy ow ot l ou ov">        ..., ], , dtype=float32)</span></pre><p id="1e7b" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="6434" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">8.为培训和验证、测试拆分数据</h1><ul class=""><li id="672e" class="mx my iq lm b ln lo lq lr lt mz lx na mb nb mf ok nd ne nf bi translated"><strong class="lm ir">功能</strong></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ox oy l"/></div></figure><ul class=""><li id="8883" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输入</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="29a5" class="or kt iq on b gy os ot l ou ov">array([[[0., 0., 0., ..., 0., 0., 0.],<br/>        [0., 0., 0., ..., 0., 0., 0.],<br/>        [0., 0., 1., ..., 0., 0., 0.],<br/>        ...,<br/>        [1., 0., 0., ..., 0., 0., 0.],<br/>        [1., 0., 0., ..., 0., 0., 0.],<br/>        [1., 0., 0., ..., 0., 0., 0.]],</span><span id="580b" class="or kt iq on b gy ow ot l ou ov">..., ], , dtype=float32)</span></pre><ul class=""><li id="f38a" class="mx my iq lm b ln mg lq mh lt oh lx oi mb oj mf ok nd ne nf bi translated"><strong class="lm ir">输出</strong></li></ul><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="9922" class="or kt iq on b gy os ot l ou ov">array([[[0., 0., 0., ..., 0., 0., 0.],<br/>        [0., 0., 0., ..., 0., 0., 0.],<br/>        [0., 0., 1., ..., 0., 0., 0.]],</span><span id="a4f2" class="or kt iq on b gy ow ot l ou ov">..., ], , dtype=float32)</span></pre><p id="a095" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">现在，在将学习数据输入到我们的 LSTM 模型之前，我们已经完成了整个预处理过程。</p><p id="40ad" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">在这之后，你只需要喂他们，等待完成学习。</p><p id="46a6" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">感谢阅读，下一篇文章再见。</p><p id="35c1" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi">— — — — —</p><h1 id="9cf0" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">参考</h1><div class="po pp gp gr pq pr"><a href="https://chunml.github.io/ChunML.github.io/project/Sequence-To-Sequence/" rel="noopener  ugc nofollow" target="_blank"><div class="ps ab fo"><div class="pt ab pu cl cj pv"><h2 class="bd ir gy z fp pw fr fs px fu fw ip bi translated">使用序列到序列学习方法创建语言翻译模型</h2><div class="py l"><h3 class="bd b gy z fp pw fr fs px fu fw dk translated">大家好。距离我上一篇博客已经有很长时间了。这听起来像是借口，但我一直在挣扎…</h3></div><div class="pz l"><p class="bd b dl z fp pw fr fs px fu fw dk translated">chunml.github.io</p></div></div><div class="qa l"><div class="qb l qc qd qe qa qf kl pr"/></div></div></a></div><div class="po pp gp gr pq pr"><a href="http://sujitpal.blogspot.com/2016/10/deep-learning-models-for-question.html" rel="noopener  ugc nofollow" target="_blank"><div class="ps ab fo"><div class="pt ab pu cl cj pv"><h2 class="bd ir gy z fp pw fr fs px fu fw ip bi translated">基于 Keras 的问答深度学习模型</h2><div class="py l"><h3 class="bd b gy z fp pw fr fs px fu fw dk translated">上周，我参加了一个由我们搜索协会组织的关于问答的(公司内部)研讨会，其中…</h3></div><div class="pz l"><p class="bd b dl z fp pw fr fs px fu fw dk translated">sujitpal.blogspot.com</p></div></div></div></a></div><div class="po pp gp gr pq pr"><a href="https://nextjournal.com/gkoehler/machine-translation-seq2seq-cpu" rel="noopener  ugc nofollow" target="_blank"><div class="ps ab fo"><div class="pt ab pu cl cj pv"><h2 class="bd ir gy z fp pw fr fs px fu fw ip bi translated">使用序列对序列学习的机器翻译</h2><div class="py l"><h3 class="bd b gy z fp pw fr fs px fu fw dk translated">在本文中，我们正在训练一个基于两个长短期记忆(LSTM)层的递归神经网络(RNN)模型…</h3></div><div class="pz l"><p class="bd b dl z fp pw fr fs px fu fw dk translated">nextjournal.com</p></div></div><div class="qa l"><div class="qg l qc qd qe qa qf kl pr"/></div></div></a></div></div></div>    
</body>
</html>