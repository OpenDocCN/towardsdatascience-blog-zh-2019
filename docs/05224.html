<html>
<head>
<title>Singular Value Decomposition Example In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的奇异值分解示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/singular-value-decomposition-example-in-python-dab2507d85a0?source=collection_archive---------1-----------------------#2019-08-05">https://towardsdatascience.com/singular-value-decomposition-example-in-python-dab2507d85a0?source=collection_archive---------1-----------------------#2019-08-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/c2cce57edf3d06cec9c3adadb2523239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hunQi54bcXdWjPb-"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://www.pexels.com/photo/woman-writing-on-a-whiteboard-3862130/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/woman-writing-on-a-whiteboard-3862130/</a></figcaption></figure><div class=""/><p id="27c9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">奇异值分解(SVD)有着广泛的应用。这些包括降维、图像压缩和数据去噪。本质上，SVD 表明一个矩阵可以表示为其他三个矩阵的乘积。用数学术语来说，SVD 可以写成如下形式:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi le"><img src="../Images/60c08fb1cf12f683a61358561c3a26e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*P4VgGcZYQl04Jqa4NW9b4A.png"/></div></figure><p id="b9c5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="ki jk"> <em class="lj"> n </em> </strong>为行数(即样本数)<strong class="ki jk"> <em class="lj"> p </em> </strong>代表维数。</p><p id="8b59" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们有一个矩阵<strong class="ki jk"> <em class="lj">一个</em> </strong>。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lk"><img src="../Images/9f5d8b232366a15fba7fd1d7abb427eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:166/0*fSmI-1gALrBtGeZD.gif"/></div></div></figure><p id="9148" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了确定关联的<strong class="ki jk"><em class="lj"/></strong>矩阵，我们必须先求出矩阵<strong class="ki jk"><em class="lj"/></strong>的特征向量乘以矩阵<strong class="ki jk"><em class="lj"/></strong>的转置。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/e10b7cbcfa33ad69c12f00c4e515551f.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/0*QU7yKTR70WrSOfIS.gif"/></div></figure><p id="0a98" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/910a4c541f77eafd1d7b48d98def4940.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*_HhF-ho4X4th8XwsFWjUcg.png"/></div></figure><p id="64fd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下特征向量和特征值的定义:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/da1a69eb7af94ee17b976a129d9a67fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/format:webp/1*OUnqg479Fwqy-g_kU-493w.png"/></div></figure><p id="6bb6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">后者也可以表示为:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/4504fddb61046bc2dcd6818b016c7c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*lE0qLizoEA0fT6YUQSzkzw.png"/></div></figure><p id="0313" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，回到我们的例子:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/1debc84cf66ac59ba6cde531f797576f.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/0*EWT0glVlwjv1Qrrh.gif"/></div></figure><p id="0a91" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到了一个四次多项式。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/36fd30d6b1db6db9aff3c25c3389fc7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*8CpQG9mcdZ0Io5fe80uu_g.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/f24bf65f2d6af059b748ad7f98678569.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*h2MDyYEUfOVrATG7KTmSrA.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/0bf90b5050cb3611292123c4536e3c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*bPY-pIqmqNvDPbd8uh1Htw.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/08cb5865b91ba059c3bd274695fde780.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*2IzF3h3zwH3Yhjb7-dt-Hw.png"/></div></figure><p id="bc84" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">求解后，我们用其中一个特征值代替λ。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/42d9b9b1cde662432f5e8062e8388224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*FlKTGfKGCe7_JKuQaSUTBQ.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/8b86474e78e0752c92fbc0271c7a25c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*RyWYR6Uug41qm9Thiqpcug.png"/></div></figure><p id="01d5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在将矩阵乘以向量<strong class="ki jk"> <em class="lj"> x </em> </strong>之后，我们得到如下:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/3f7f3609d9a42a9822377b154602d8d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*XIkUv6Eyv5QA1uJ67VwQ_w.png"/></div></figure><p id="746a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在解方程时，我们得到:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lx"><img src="../Images/754e8f879ed84c751624804cd8676cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*yqx_NGXQ1n347kgDo82V9w.png"/></div></div></figure><p id="389c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们把特征向量代入<strong class="ki jk"><em class="lj"/></strong><em class="lj">的列。</em></p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/9ceb8085fdca74cbc6de22e0133603cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/0*jBlX_dIFgVW-4-p8.gif"/></div></figure><p id="ae5f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们对矩阵<strong class="ki jk"><em class="lj"/></strong>乘以矩阵<strong class="ki jk"><em class="lj"/></strong>的转置重复同样的过程。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lz"><img src="../Images/393558f8b9826491a7a047304a164c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/0*og0Ypo3IzeR0ZaBU.gif"/></div></div></figure><p id="1c48" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">求解后，我们得到了<strong class="ki jk"> <em class="lj"> V </em> </strong>的表达式。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/90c25ee0504dfad2fba8b4ec922bbbf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/0*_cFujQ92IHXvqhjZ.gif"/></div></figure><p id="aba8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，<strong class="ki jk"> <em class="lj"> S </em> </strong>是一个对角矩阵，其值是任一个的特征值的平方根</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/82e9acc3d8ed1031c3b72a3ae4f9c842.png" data-original-src="https://miro.medium.com/v2/resize:fit:174/format:webp/1*wTgxCHlqtkiRP6kdKnRhcw.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/e5305fb76c3b29a36ae0bc13f9ac1b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*eJv0z-cf5ECITlBrnNIpQQ.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi md"><img src="../Images/c798ebecf68a6b0265c07519022e481a.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/0*ssenk7T2Cxskt_cs.gif"/></div></figure><p id="efba" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是事情变得有趣的地方。我们知道所有三个矩阵的乘积等价于左边的矩阵。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi me"><img src="../Images/a4cbdb2343fa52bc95ec1c6d673a3f9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W5kjmtkMTQKNXCSjiyx0Zg.png"/></div></div></figure><p id="15d3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以排除特征，但仍然保持原始矩阵的近似值。假设矩阵<strong class="ki jk"><em class="lj"/></strong>是组成图像的列和行或像素的数据集，我们可以在理论上使用新形成的矩阵来训练模型，并达到相当的(如果不是更好的话)(由于维数灾难)精确度。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mf"><img src="../Images/c800c9a8e4856c84823b69a8ecf979c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T83WqpRLvyOmsMmHWjOdLw.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mg"><img src="../Images/e40b1c9d4ed45075fffbc4a9f72aa2a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B8nmtWyt8mOeI0DLqK3r6A.png"/></div></div></figure><h1 id="a6a5" class="mh mi jj bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">密码</h1><p id="7a7e" class="pw-post-body-paragraph kg kh jj ki b kj nf kl km kn ng kp kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">让我们看看如何在 Python 中应用奇异值分解。首先，导入以下库。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="69ec" class="np mi jj nl b gy nq nr l ns nt">import numpy as np<br/>from sklearn.datasets import load_digits<br/>from matplotlib import pyplot as plt<br/>from sklearn.decomposition import TruncatedSVD<br/>float_formatter = lambda x: "%.2f" % x<br/>np.set_printoptions(formatter={'float_kind':float_formatter})<br/>from sklearn.ensemble import RandomForestClassifier</span></pre><p id="0d56" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在接下来的教程中，我们将尝试对手写数字进行分类。幸运的是，<code class="fe nu nv nw nl b">scikit-learn</code>库提供了一个包装器函数，用于将数据集导入我们的程序。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="f41b" class="np mi jj nl b gy nq nr l ns nt">X, y = load_digits(return_X_y=True)</span></pre><p id="b589" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该数据集包含 1797 幅 8×8 的图像。如果你指定了<code class="fe nu nv nw nl b">return_X_y=True</code>,这个函数将把像素作为一维数组返回。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="ca69" class="np mi jj nl b gy nq nr l ns nt">X.shape</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/7308c224ed51c04b9c27f33f3302896e.png" data-original-src="https://miro.medium.com/v2/resize:fit:180/format:webp/1*S1tNhB5b8KYniFsS4bqjZw.png"/></div></figure><p id="0c99" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> <em class="lj"> y </em> </strong>包含每个数字的标签。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="1078" class="np mi jj nl b gy nq nr l ns nt">y</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/b28ad3ff8beb7016e9b4c5976c2c086d.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*ZZg9LPzUtVy_c6Fp8wh1nw.png"/></div></div></figure><p id="200d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看第一个数字。正如我们所看到的，它只是一个长度为 64 的数组，包含像素亮度。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="8a54" class="np mi jj nl b gy nq nr l ns nt">image = X[0]</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/672f68f4e3f7128b2c3611f49f0bfb07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*do1KCPC1VbpQIVK61X6YIQ.png"/></div></figure><p id="6d14" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们想使用<code class="fe nu nv nw nl b">matplotlib</code>查看图像，我们必须首先重塑数组。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="b562" class="np mi jj nl b gy nq nr l ns nt">image = image.reshape((8, 8))</span><span id="f3e4" class="np mi jj nl b gy oa nr l ns nt">plt.matshow(image, cmap = 'gray')</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/19e02b5e42e9c92f808468b5528ea573.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*Dcp9nmEVt-6XRhY2a4w91g.png"/></div></figure><p id="b3ef" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们将使用奇异值分解来看看我们是否能够仅使用每行的两个特征来重建图像。函数返回的<strong class="ki jk"> s </strong>矩阵必须使用<code class="fe nu nv nw nl b">diag</code>方法转换成对角矩阵。默认情况下，<code class="fe nu nv nw nl b">diag</code>将创建一个相对于原始矩阵为<em class="lj"> n x n </em>的矩阵。这导致了一个问题，因为矩阵的大小不再遵循矩阵乘法的规则，其中一个矩阵中的列数必须与另一个矩阵中的行数相匹配。因此，我们创建一个新的<em class="lj"> m x n </em>矩阵，并用对角矩阵填充它的第一个<em class="lj"> n x n </em>部分。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="4dc1" class="np mi jj nl b gy nq nr l ns nt">U, s, V = np.linalg.svd(image)</span><span id="84e9" class="np mi jj nl b gy oa nr l ns nt">S = np.zeros((image.shape[0], image.shape[1]))</span><span id="7425" class="np mi jj nl b gy oa nr l ns nt">S[:image.shape[0], :image.shape[0]] = np.diag(s)</span><span id="f1d4" class="np mi jj nl b gy oa nr l ns nt">n_component = 2</span><span id="1777" class="np mi jj nl b gy oa nr l ns nt">S = S[:, :n_component]<br/>VT = VT[:n_component, :]</span><span id="3cd3" class="np mi jj nl b gy oa nr l ns nt">A = U.dot(Sigma.dot(VT))</span><span id="9350" class="np mi jj nl b gy oa nr l ns nt">print(A)</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/94cd4447feb66744f117f9f728b9a9f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*fG89HFkPcIe3TVykMhM3Tw.png"/></div></figure><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="30c5" class="np mi jj nl b gy nq nr l ns nt">plt.matshow(A, cmap = 'gray')</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/5b937ab634ed9783d30bb69620d9b360.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*-EhCvx4x1ldZeZaBDrbtGQ.png"/></div></figure><p id="45d0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过取<strong class="ki jk"> U </strong>和<strong class="ki jk"> S </strong>矩阵的点积得到缩减的特征空间。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="eebc" class="np mi jj nl b gy nq nr l ns nt">U.dot(S)</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/37d026aab37ebea8a197c472f7f372b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*YZjCav5bB-OW1O8mWJJPew.png"/></div></figure><h2 id="e5de" class="np mi jj bd mj oe of dn mn og oh dp mr kr oi oj mv kv ok ol mz kz om on nd oo bi translated">原始与缩减的特征空间</h2><p id="30b3" class="pw-post-body-paragraph kg kh jj ki b kj nf kl km kn ng kp kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">让我们比较随机森林模型在使用原始手写数字训练时和使用从奇异值分解获得的缩减特征空间训练时的准确性。</p><p id="1d7b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过查看公开得分来衡量模型的准确性。如果你对 OOB 的概念不熟悉，我鼓励你看看兰登森林的这篇文章。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="f4b5" class="np mi jj nl b gy nq nr l ns nt">rf_original = RandomForestClassifier(oob_score=True)</span><span id="c2ad" class="np mi jj nl b gy oa nr l ns nt">rf_original.fit(X, y)</span><span id="3352" class="np mi jj nl b gy oa nr l ns nt">rf_original.oob_score_</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/7e5a7e53ba15e5d3ba21f5675f4f08b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*OLPz2a575JvcYTstMdZg8Q.png"/></div></figure><p id="cb45" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们用 2 个组件创建并装配一个<code class="fe nu nv nw nl b">TruncatedSVD</code>类的实例。值得一提的是，与前面的例子不同，我们使用的是 2/64 特性。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="bb75" class="np mi jj nl b gy nq nr l ns nt">svd = TruncatedSVD(n_components=2)</span><span id="92b6" class="np mi jj nl b gy oa nr l ns nt">X_reduced = svd.fit_transform(X)</span></pre><p id="72c7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">精简数据集中的每个图像(即行)包含 2 个特征。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="4b22" class="np mi jj nl b gy nq nr l ns nt">X_reduced[0]</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/47da4ffa927d39f2873fbeb6243729fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*2qQgbmf8k6D3v23lBQCqOw.png"/></div></figure><p id="7f25" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看一看图像，很难区分图像由什么数字组成，它很可能是 5 而不是 0。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="60a3" class="np mi jj nl b gy nq nr l ns nt">image_reduced = svd.inverse_transform(X_reduced[0].reshape(1,-1))</span><span id="5853" class="np mi jj nl b gy oa nr l ns nt">image_reduced = image_reduced.reshape((8,8))</span><span id="3f86" class="np mi jj nl b gy oa nr l ns nt">plt.matshow(image_reduced, cmap = 'gray')</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/0fa1dc17d60459cba6244a3583d7971b.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*9d5JWcl_aom3SwlgZz-ZLg.png"/></div></figure><p id="a15a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在精简的数据集上训练随机森林分类器后，我们获得了 36.7%的微弱准确率</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="dd7a" class="np mi jj nl b gy nq nr l ns nt">rf_reduced = RandomForestClassifier(oob_score=True)</span><span id="6c3d" class="np mi jj nl b gy oa nr l ns nt">rf_reduced.fit(X_reduced, y)</span><span id="7d5d" class="np mi jj nl b gy oa nr l ns nt">rf_reduced.oob_score_</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/667b42cd286559bed2fb169b6e807ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*2xhuvoy6qeiHf3eW3fxDTw.png"/></div></figure><p id="c28d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过取<code class="fe nu nv nw nl b">explained_variance_ratio_</code>属性的和得到总方差解释。我们通常希望达到 80%到 90%的目标。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="1c1f" class="np mi jj nl b gy nq nr l ns nt">svd.explained_variance_ratio_.sum()</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/353bcf4c507c31b0c43487cee336fa7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*TA3mQV_cwd1YMEwCkf3FCg.png"/></div></figure><p id="eb20" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们再试一次，只是这一次，我们使用 16 个组件。我们查看包含在 16 个特征中的信息量。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="caa5" class="np mi jj nl b gy nq nr l ns nt">svd = TruncatedSVD(n_components=16)</span><span id="0766" class="np mi jj nl b gy oa nr l ns nt">X_reduced = svd.fit_transform(X)</span><span id="29a4" class="np mi jj nl b gy oa nr l ns nt">svd.explained_variance_ratio_.sum()</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/4598773dab6464d494ccfbe19ce8ff39.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*v7Lp9g6MmopErD9z4HojZQ.png"/></div></figure><p id="6aeb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们获得了与使用原始图像训练的模型相当的准确度，并且我们使用了 16/64=0.25 的数据量。</p><pre class="lf lg lh li gt nk nl nm nn aw no bi"><span id="c230" class="np mi jj nl b gy nq nr l ns nt">rf_reduced = RandomForestClassifier(oob_score=True)</span><span id="82ab" class="np mi jj nl b gy oa nr l ns nt">rf_reduced.fit(X_reduced, y)</span><span id="d8e3" class="np mi jj nl b gy oa nr l ns nt">rf_reduced.oob_score_</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lo"><img src="../Images/f8642bd2d55d89c8ebc856b03a6c8112.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*D8V9ozIM0H74pz5BoSJZiA.png"/></div></div></figure></div></div>    
</body>
</html>