<html>
<head>
<title>Adversarial Attacks in Machine Learning and How to Defend Against Them</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的对抗性攻击及其防御</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adversarial-attacks-in-machine-learning-and-how-to-defend-against-them-a2beed95f49c?source=collection_archive---------2-----------------------#2019-12-19">https://towardsdatascience.com/adversarial-attacks-in-machine-learning-and-how-to-defend-against-them-a2beed95f49c?source=collection_archive---------2-----------------------#2019-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="74d4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">刘玲教授在 2019 年 IEEE 大数据大会上的主题演讲笔记</h2></div><p id="6a9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated">ig 数据驱动的机器学习和深度学习已经在许多领域取得了令人印象深刻的进展。一个例子是 ImageNet 的发布，它由 22，000 个类别的 1，500 多万张带标签的高分辨率图像组成，彻底改变了计算机视觉领域。<a class="ae ln" href="https://paperswithcode.com/sota/image-classification-on-imagenet" rel="noopener ugc nofollow" target="_blank">最先进的模型</a>已经在 ImageNet 数据集上实现了 98%的前五名准确率，所以看起来这些模型是万无一失的，不会出错。</p><p id="2488" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，对抗训练的最新进展发现，这是一种错觉。当面对对立的例子时，一个好的模型经常会出错。下图说明了这个问题:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/ff029bd67e093adba9525fa050b8b3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VMW1j6A_kh4LyuNFyEw3hw.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Image taken from <a class="ae ln" rel="noopener" target="_blank" href="/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa">https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa</a></figcaption></figure><p id="a7a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型最初对熊猫图片进行了正确的分类，但当一些人类察觉不到的噪声注入到图片中时，模型的最终预测变为另一种动物长臂猿，即使有如此高的置信度。对我们来说，似乎最初的和改变的图像是相同的，尽管它与模型完全不同。这说明了这些对抗性攻击造成的威胁—我们可能没有察觉到差异，因此我们无法判断对抗性攻击是否发生。因此，尽管模型的输出可能会改变，但我们无法判断输出是正确还是不正确。</p><p id="8fef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这形成了刘玲教授在 2019 年 IEEE 大数据大会上的主题演讲背后的动机，她在演讲中谈到了对抗性攻击的类型，对抗性例子是如何产生的，以及如何对抗这些攻击。事不宜迟，我将进入她的演讲的内容。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="3664" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">敌对攻击的类型</h1><p id="6e19" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">对抗性攻击分为两类——有针对性的攻击和无针对性的攻击。</p><p id="b18a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目标攻击有一个目标类别 Y，它希望目标模型 M 将类别 X 的图像 I 分类为。因此，目标攻击的目标是通过预测敌对示例 I，使 M 误分类为预期的目标类别 Y 而不是真实的类别 x。另一方面，无目标攻击不具有它想要模型将图像分类为的目标类别。相反，目标仅仅是通过预测对立的例子 I 作为一个类，而不是原始类 x，来使目标模型错误分类。</p><p id="9216" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">研究人员发现，一般来说，虽然无针对性的攻击不如有针对性的攻击，但耗时要少得多。尽管有针对性的攻击在改变模型预测方面更成功，但这是有代价的(时间)。</p><h1 id="171f" class="ml mm it bd mn mo ni mq mr ms nj mu mv jz nk ka mx kc nl kd mz kf nm kg nb nc bi translated">对立的例子是如何产生的？</h1><p id="0dc8" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">了解了有目标攻击和无目标攻击之间的区别后，我们现在来看看这些敌对攻击是如何进行的。在良性的机器学习系统中，训练过程寻求最小化目标标签和预测标签之间的损失，数学公式如下:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nn"><img src="../Images/4b76da8bf4bf6176de3cd168f34d8aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ArGd_Bw-1OaTbRyA.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Image credits to Professor Lise Getoor taken from <a class="ae ln" href="https://users.soe.ucsc.edu/~getoor/Talks/IEEE-Big-Data-Keynote-2019.pdf" rel="noopener ugc nofollow" target="_blank">https://users.soe.ucsc.edu/~getoor/Talks/IEEE-Big-Data-Keynote-2019.pdf</a></figcaption></figure><p id="e13a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在测试阶段，对学习的模型进行测试，以确定它能够多好地预测预测的标签。然后，通过目标标签和预测标签之间的损失之和来计算误差，数学公式如下:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="ab gu cl no"><img src="../Images/605b7dbc023ce5555e796ed97ab42d2c.png" data-original-src="https://miro.medium.com/v2/format:webp/0*M9lWNRAFupfRX1RX.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Image credits to Professor Lise Getoor taken from <a class="ae ln" href="https://users.soe.ucsc.edu/~getoor/Talks/IEEE-Big-Data-Keynote-2019.pdf" rel="noopener ugc nofollow" target="_blank">https://users.soe.ucsc.edu/~getoor/Talks/IEEE-Big-Data-Keynote-2019.pdf</a></figcaption></figure><p id="f0c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在对抗性攻击中，采取以下两个步骤:</p><ol class=""><li id="fafc" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated">查询输入从良性输入 x 变为 x’。</li><li id="576f" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">设定攻击目标，使得预测结果 H(x)不再是 y。损失从<em class="od"> l </em> (H(x ᵢ)，y ᵢ)变为<em class="od"> l </em> (H(x ᵢ)，y' ᵢ)其中 y' ᵢ ≠ y ᵢ.</li></ol><h2 id="2765" class="oe mm it bd mn of og dn mr oh oi dp mv kr oj ok mx kv ol om mz kz on oo nb op bi translated">对抗性干扰</h2><p id="2452" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">将查询输入从 x 改变到 x’的一种方式是通过称为“对抗扰动”的方法，其中计算扰动，使得预测将与原始标签不同。对于图像，这可能以像素噪声的形式出现，正如我们在上面的熊猫例子中看到的那样。无目标攻击的单一目标是最大化 H(x)和 H(x’)之间的损失，直到预测结果不是 y(真实标签)。定向攻击还有一个额外的目标，即不仅最大化 H(x)和 H(x’)之间的损耗，而且最小化 H(x’)和 y’之间的损耗，直到 H(x’)= y’而不是 y。</p><p id="7623" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对抗性扰动可以分为一步扰动和多步扰动。顾名思义，一步扰动只涉及一个阶段——添加一次噪声，仅此而已。另一方面，多步扰动是一种迭代攻击，每次都对输入进行小的修改。因此，单步攻击很快，但可能会添加过多的噪声，从而使人类更容易检测到这些变化。此外，它更重视 H(x)和 H(x’)之间损耗最大化的目标，而不太重视扰动量最小化。相反，多步攻击更具战略性，因为它每次都会引入少量扰动。然而，这也意味着这种攻击在计算上更加昂贵。</p><h1 id="8034" class="ml mm it bd mn mo ni mq mr ms nj mu mv jz nk ka mx kc nl kd mz kf nm kg nb nc bi translated">黑盒攻击与白盒攻击</h1><p id="014d" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">现在我们已经了解了对抗性攻击是如何产生的，一些敏锐的读者可能会意识到这些攻击的一个基本假设，即攻击目标预测模型 H 是对手已知的。只有当目标模型已知时，才可以通过改变输入来折衷产生对立的例子。然而，攻击者并不总是知道或者能够访问目标模型。这听起来像是抵御这些敌对攻击者的万全之策，但事实是黑盒攻击也非常有效。</p><p id="133e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">黑盒攻击基于对立示例的可转移性概念，即对立示例虽然是为了攻击代理模型 G 而生成的，但在攻击另一个模型 h 时可以取得令人印象深刻的结果。采取的步骤如下:</p><ol class=""><li id="5129" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated">攻击目标预测模型 H 是私人训练的，对手不知道。</li><li id="cb98" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">一个模仿 H 的代理模型 G 被用来产生对立的例子。</li><li id="7eeb" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">利用对抗性例子的可转移性，可以发起黑盒攻击来攻击 h。</li></ol><p id="d11e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种攻击可以在训练数据集已知或未知的情况下发起。在对手已知数据集的情况下，可以在与模型 H 相同的数据集上训练模型 G 来模仿 H。</p><p id="7b46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，当训练数据集未知时，对手可以利用<a class="ae ln" href="https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf" rel="noopener ugc nofollow" target="_blank">成员推理攻击</a>，从而训练一个攻击模型，其目的是将目标模型在训练输入上的行为与其在训练期间没有遇到的输入上的行为区分开。本质上，这变成了一个分类问题，以识别目标模型对其训练的输入和未训练的输入的预测之间的差异。这使得对手能够更好地了解训练模型 H 的训练数据集 D，使得攻击者能够基于真实的训练数据集生成影子数据集 S，以便训练替代模型 G。在 S 上训练 G，其中 G 模仿 H，S 模仿 D，然后可以对 H 发起黑盒攻击</p><h1 id="49e0" class="ml mm it bd mn mo ni mq mr ms nj mu mv jz nk ka mx kc nl kd mz kf nm kg nb nc bi translated">黑盒攻击的例子</h1><p id="abe2" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">现在我们已经看到了黑盒攻击与白盒攻击的不同之处，因为目标模型 H 对于对手来说是未知的，我们将讨论黑盒攻击中使用的各种策略。</p><h2 id="d58a" class="oe mm it bd mn of og dn mr oh oi dp mv kr oj ok mx kv ol om mz kz on oo nb op bi translated">身体攻击</h2><p id="4dba" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">将查询输入从 x 改变为 x’的一种简单方法是简单地添加一些物理上的东西(例如亮色)来扰乱模型。一个例子是 CMU 的<a class="ae ln" href="https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf" rel="noopener ugc nofollow" target="_blank">研究人员如何在对面部识别模型的攻击中给一个人戴上眼镜。下图说明了这次攻击:</a></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oq"><img src="../Images/a31fdcbd7f2d76ea85a9058427233f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0n11B2qrdiba9sQ16u6IBQ.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Image taken from <a class="ae ln" href="https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf</a></figcaption></figure><p id="834d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一行图像对应于通过添加眼镜而修改的原始图像，第二行图像对应于作为预期的错误分类目标的模仿目标。仅仅通过在原始图像上添加眼镜，面部识别模型就被欺骗，将顶行的图像分类为底行的图像。</p><p id="5cc2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个例子来自谷歌的<a class="ae ln" href="https://arxiv.org/pdf/1712.09665.pdf" rel="noopener ugc nofollow" target="_blank">研究人员，他们在输入图像中添加贴纸来改变图像的分类，如下图所示:</a></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi or"><img src="../Images/7cc18bfc29dc2df85177cdfa69c8dc26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHPzZOhZfoy2T8oDIq8Y6Q.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Image taken from <a class="ae ln" href="https://arxiv.org/pdf/1712.09665.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1712.09665.pdf</a></figcaption></figure><p id="7a3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些例子显示了这种物理攻击是多么有效。</p><h2 id="99be" class="oe mm it bd mn of og dn mr oh oi dp mv kr oj ok mx kv ol om mz kz on oo nb op bi translated">分发外(OOD)攻击</h2><p id="c470" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">实施黑盒攻击的另一种方式是通过分发外(OOD)攻击。机器学习中的传统假设是，所有的训练和测试样本都是从<strong class="kk iu">相同的</strong>分布中独立抽取的。在 OOD 攻击中，通过向模型提供来自训练数据集的不同分布的图像来利用这种假设，例如，将 TinyImageNet 数据馈送到 CIFAR-10 分类器，这将导致具有高置信度的不正确预测。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="52b1" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">如何才能信任机器学习？</h1><p id="52a9" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">既然我们已经看了各种类型的对抗性攻击，那么一个自然的问题就来了——如果我们的机器学习模型如此容易受到对抗性攻击，我们怎么能信任它们呢？</p><p id="8f36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Chow 等人在 2019 年题为“<a class="ae ln" href="https://arxiv.org/pdf/1908.07667.pdf" rel="noopener ugc nofollow" target="_blank">抗黑盒对抗性攻击的去噪和验证跨层集成</a>”的论文中提出了一种可能的方法。该方法的核心是使机器学习系统能够自动检测敌对攻击，然后通过使用去噪和验证集成来自动修复它们。</p><h2 id="aa69" class="oe mm it bd mn of og dn mr oh oi dp mv kr oj ok mx kv ol om mz kz on oo nb op bi translated">去噪集成</h2><p id="d078" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">首先，输入图像必须经过去噪集合，去噪集合尝试不同的方法来去除图像中的任何添加的噪声，例如添加高斯噪声。由于防御者不知道由对手添加到图像中的特定噪声，因此每次尝试去除每种类型的噪声时，都需要一组去噪器。</p><p id="6cf9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了去噪自动编码器的训练过程-原始图像被注入了攻击者可能注入的一些噪声，自动编码器尝试重建原始未损坏的图像。在训练过程中，目标是减少重建图像和原始图像之间的重建误差。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi os"><img src="../Images/4f886c28eef207f36f1424a3dcdbe7ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NvDnGvix_L55dVxqB9FNUg.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Image taken from <a class="ae ln" href="https://arxiv.org/pdf/1908.07667.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1908.07667.pdf</a></figcaption></figure><p id="7d28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过开发这些自动编码器的集合，每个自动编码器被训练以去除特定类型的噪声，希望被破坏的图像将被充分去噪，使得它接近原始未被破坏的图像，以允许图像分类。</p><h2 id="71de" class="oe mm it bd mn of og dn mr oh oi dp mv kr oj ok mx kv ol om mz kz on oo nb op bi translated">验证集成</h2><p id="6e45" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">在图像被去噪之后，它们然后经过验证集合，该验证集合检查由每个去噪器产生的每个去噪图像，然后对去噪图像进行分类。验证集成中的每个分类器对每个去噪图像进行分类，然后集成进行投票以确定图像所属的最终类别。这意味着尽管一些图像可能没有在去噪步骤中以正确的方式去噪，但是验证集合对所有去噪的图像进行投票，从而增加了做出更准确预测的可能性。</p><h2 id="e8dd" class="oe mm it bd mn of og dn mr oh oi dp mv kr oj ok mx kv ol om mz kz on oo nb op bi translated">多样性</h2><p id="6b85" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">发现去噪器和验证器的多样性是非常重要的，因为首先，敌对攻击者将更善于改变图像，因此需要能够处理各种破坏图像的去噪器的多样性组。接下来，还需要验证器多样化，以便它们可以生成多种分类，这样敌对攻击者就很难操纵它们，就像他们如何设法操纵我们信任并在机器学习中频繁使用的普通分类器一样。</p><p id="692a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这仍然是一个悬而未决的问题，因为在各种验证者做出所有这些决定之后，仍然有一个最终决策者需要决定听取谁的意见。最终的决策者需要保持群体中的多样性，这并不是一件容易的事情。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="f44f" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">结论</h1><p id="260c" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">我们已经研究了各种类型的对抗性攻击，以及防御这些攻击的有希望的方法。当我们实现机器学习模型时，这绝对是要记住的事情。我们不应该盲目地相信模型会产生正确的结果，而是需要防范这些对抗性的攻击，并在接受这些模型做出的决定之前总是三思而行。</p><p id="f83a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">非常感谢刘教授就机器学习中的这个紧迫问题发表了富有启发性的主旨演讲！</p></div></div>    
</body>
</html>