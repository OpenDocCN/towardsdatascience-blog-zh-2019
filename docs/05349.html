<html>
<head>
<title>Faster R-CNN for object detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于目标检测的快速 R-CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/faster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46?source=collection_archive---------0-----------------------#2019-08-09">https://towardsdatascience.com/faster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46?source=collection_archive---------0-----------------------#2019-08-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6538" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">技术论文摘要</h2></div><p id="e278" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">R-CNN 家族中使用最广泛的最先进版本——更快的 R-CNN 于 2015 年首次发布。本文是了解当今物体探测基础的系列文章的第三篇，也是最后一篇，阐述了更快的 R-CNN 探测管道的技术细节。回顾它的前辈，看看这些总结:<a class="ae lb" href="https://medium.com/@shilpa_47526/r-cnn-for-object-detection-a-technical-summary-9e7bfa8a557c" rel="noopener">地区与 CNN (R-CNN) </a>和<a class="ae lb" href="https://medium.com/@shilpa_47526/fast-r-cnn-for-object-detection-a-technical-summary-a0ff94faa022" rel="noopener">快速 R-CNN </a>。</p><p id="0e03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 R-CNN 系列论文中，版本之间的演变通常是在计算效率(整合不同的训练阶段)、减少测试时间和提高性能(mAP)方面。这些网络通常包括——A)生成“边界框”或图像中可能对象的位置的区域提议算法；b)通常使用 CNN 获得这些对象的特征的特征生成阶段；c)分类层，用于预测该对象属于哪个类别；以及 d)回归层，用于使对象边界框的坐标更加精确。</p><p id="fae3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">快速 R-CNN 网络中剩下的唯一独立部分是区域提议算法。R-CNN 和快速 R-CNN 都使用基于 CPU 的区域提议算法，例如，选择性搜索算法，每幅图像大约需要 2 秒钟，在 CPU 上运行。更快的 R-CNN [3]论文通过使用另一个卷积网络(RPN)来产生区域提议，解决了这个问题。这不仅将每个图像的区域提议时间从 2s 降低到 10ms，而且允许区域提议阶段与随后的检测阶段共享层，导致特征表示的整体改进。在本文的其余部分，“更快的 R-CNN”通常指的是一种检测管道，它使用 RPN 作为区域提议算法，使用<a class="ae lb" href="https://medium.com/@shilpa_47526/fast-r-cnn-for-object-detection-a-technical-summary-a0ff94faa022" rel="noopener">快速 R-CNN </a>作为检测器网络。</p><h1 id="b066" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">区域提案网络</h1><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/8925051c0053aa15ff03298bbae510ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S_-8lv4zP3W8IVfGP6_MHw.jpeg"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 1: The architecture of the region proposal network or RPN</figcaption></figure><h2 id="5ec6" class="mk ld iq bd le ml mm dn li mn mo dp lm ko mp mq lo ks mr ms lq kw mt mu ls mv bi translated">体系结构</h2><ul class=""><li id="d45d" class="mw mx iq kh b ki my kl mz ko na ks nb kw nc la nd ne nf ng bi translated">区域建议网络(RPN)从输入图像被馈送到骨干卷积神经网络开始。首先调整输入图像的大小，使其最短边为 600 像素，最长边不超过 1000 像素。</li><li id="0384" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">主干网络的输出特征(由 H x W 表示)通常比输入图像小得多，这取决于主干网络的跨度。对于本文中使用的两种可能的主干网(VGG、ZF 网络)，网络跨距为 16。这意味着主干输出特征中的两个连续像素对应于输入图像中相距 16 个像素的两个点。</li><li id="c38f" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">对于输出特征图中的每个点，网络必须了解输入图像中相应位置是否存在对象，并估计其大小。这是通过在主干网络的输出要素地图上的每个位置的输入图像上放置一组“锚点”来实现的。这些锚表示在该位置的各种尺寸和长宽比的可能物体。下图显示了 9 个可能的锚点，它们以 3 种不同的纵横比和 3 种不同的大小放置在输入图像上，用于输出特征地图上的点 A。对于帕斯卡挑战，所使用的锚具有 128、256、512 和 1:1、1:2 和 2:1 的 3 种比例的盒子面积。</li></ul><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi nm"><img src="../Images/0937d0e7f99e4c90b1e506659d597b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZF7pnTEhyOK8z25t_QZVw.jpeg"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 2: The possible anchors in the input image in a location corresponding to point A in the feature map.</figcaption></figure><ul class=""><li id="46a8" class="mw mx iq kh b ki kj kl km ko nn ks no kw np la nd ne nf ng bi translated">当网络移动通过输出特征图中的每个像素时，它必须检查跨越输入图像的这些<em class="nq"> k </em>对应锚点是否实际包含对象，并细化这些锚点的坐标以给出作为“对象提议”或感兴趣区域的边界框。</li><li id="90d9" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">首先，具有 512 个单元的 3×3 卷积被应用于主干特征图，如图 1 所示，以给出每个位置的 512-d 特征图。接下来是两个兄弟层:1 x 1 卷积层，18 个单元用于对象分类，1 x 1 卷积层，36 个单元用于边界框回归。</li><li id="3f4d" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">分类分支中的 18 个单元给出大小为(H，W，18)的输出。该输出用于给出主干特征图(尺寸:H×W)<strong class="kh ir">中的每个点在该点的所有 9 个锚点中是否包含对象</strong>的概率。</li><li id="f3c0" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">回归分支中的 36 个单元给出大小为(H，W，36)的输出。该输出用于给出主干特征图(大小:H×W)中每个点的 9 个锚的 4 个回归系数。这些回归系数用于改善包含对象的锚点的坐标。</li></ul><h2 id="69e2" class="mk ld iq bd le ml mm dn li mn mo dp lm ko mp mq lo ks mr ms lq kw mt mu ls mv bi translated">训练和损失函数</h2><ul class=""><li id="10fa" class="mw mx iq kh b ki my kl mz ko na ks nb kw nc la nd ne nf ng bi translated">输出的特征图由大约 40×60 个位置组成，总共对应 40*60*9 ~ 20k 个锚点。在列车运行时，所有跨越边界的锚都被忽略，因此它们不会造成损失。这使得每个图像大约有 6k 个锚。</li><li id="1a4f" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">如果一个锚满足以下两个条件中的任何一个，则该锚被认为是“正”样本——a)该锚具有最高的 IoU(交集/并集，一种重叠的度量);b)锚具有大于 0.7 的 IoU 和任何地面实况框。同一个 groundtruth 框可以导致多个锚点被分配阳性标签。</li><li id="1e3a" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">如果一个锚点的 IoU 与所有 groundtruth 框小于 0.3，则该锚点被标记为“负”。剩余的锚(既不是正面的也不是负面的)在 RPN 训练中被忽略。</li><li id="e083" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">用于训练 RPN 的每个小批量来自单个图像。从该图像中采样所有锚会使学习过程偏向负样本，因此随机选择 128 个正样本和 128 个负样本来形成该批，如果正样本数量不足，则用额外的负样本填充。</li><li id="ceb7" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">RPN 的训练损失也是多任务损失，由下式给出:</li></ul><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/be4d3ca8df183b7d9a6424a17171e8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*mq0jKvp4I1o1ecHBTzQLHA.png"/></div></figure><ul class=""><li id="21c7" class="mw mx iq kh b ki kj kl km ko nn ks no kw np la nd ne nf ng bi translated">这里<em class="nq"> i </em>是主播在 mini-batch 中的索引。分类损失<em class="nq"> L𝒸ₗₛ(pᵢ，pᵢ*) </em>是两类(对象对非对象)的对数损失。<em class="nq"> pᵢ </em>是主播<em class="nq"> i </em>的分类分支的输出分数，<em class="nq"> pᵢ* </em>是地面实况标签(1 或 0)。</li><li id="81fd" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">回归损失<em class="nq"> Lᵣₑ(tᵢ，tᵢ*) </em>仅在锚点实际包含一个对象时被激活，即基础事实<em class="nq"> pᵢ* </em>为 1。术语<em class="nq"> tᵢ </em>是回归层的输出预测，由 4 个变量组成【tₓ，tᵧ，t <em class="nq"> w </em>，tₕ].回归目标 tᵢ*计算如下—</li></ul><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f4488d6054c5ed8e59133dccdb27be64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*s6uMkokhzyGoMLSXLMm9Ug.png"/></div></figure><ul class=""><li id="7392" class="mw mx iq kh b ki kj kl km ko nn ks no kw np la nd ne nf ng bi translated">这里 x，y，w，h 对应于盒子中心的<em class="nq"> (x，y) </em>坐标和盒子的高度<em class="nq"> h </em>和宽度<em class="nq"> w </em>。xₐ，x*代表锚定框的坐标及其对应的地面真实边界框。</li><li id="3166" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">请记住，锚盒的所有<em class="nq"> k </em> ( <em class="nq"> = </em> 9)都有不同的回归量，它们不共享权重。因此，锚<em class="nq"> i </em>的回归损失应用于其相应的回归量(如果是正样本)。</li><li id="2552" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">在测试时，所学习的回归输出 tᵢ可以被应用于其相应的锚框(被预测为正的)，并且用于预测的对象提议边界框的<em class="nq"> x，y，w，h </em>参数可以从下式反算</li></ul><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c4301b5ed8681095d004a3333d7fa98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*8BnFj3-7f6h4xM-EUkeykw.png"/></div></figure><h2 id="1b9f" class="mk ld iq bd le ml mm dn li mn mo dp lm ko mp mq lo ks mr ms lq kw mt mu ls mv bi translated">测试时间详细信息</h2><p id="650f" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">在测试时，来自每个图像的 20k 个锚经过一系列后处理步骤，发送到对象提议边界框中。</p><ul class=""><li id="9fe6" class="mw mx iq kh b ki kj kl km ko nn ks no kw np la nd ne nf ng bi translated">回归系数被应用于锚点以进行精确定位。这给出了精确的边界框。</li><li id="d51e" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">所有的盒子都按照它们的<em class="nq"> cls </em>分数排列。然后，应用阈值为 0.7 的非最大抑制(NMS)。从上到下，所有与另一个边界框的 IoU 大于 0.7 的边界框都被丢弃。因此，最高得分的边界框被保留用于一组重叠的框。</li><li id="d7fa" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">这给出了每个图像大约 2k 个建议。</li><li id="82b6" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">跨界边界框被保留并被剪切到图像边界。</li><li id="f9aa" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">当使用这些对象建议来训练快速 R-CNN 检测流水线时，使用来自 RPN 的所有 2k 个建议。在用于快速 R-CNN 检测的测试时间，仅选择来自 RPN 的前 N 个提议。</li></ul><h1 id="abe4" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">对象检测:更快的 R-CNN (RPN +快速 R-CNN)</h1><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi nx"><img src="../Images/a146c1d1ae1db72e261c172de3fb960b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tTqg3W165itg-LVRFxHJfA.jpeg"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 3: The RPN for region proposals and Fast R-CNN as a detector in the Faster R-CNN detection pipeline</figcaption></figure><p id="e612" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更快的 R-CNN 架构包括作为区域提议算法的 RPN 和作为检测器网络的快速 R-CNN。</p><h2 id="17e4" class="mk ld iq bd le ml mm dn li mn mo dp lm ko mp mq lo ks mr ms lq kw mt mu ls mv bi translated">快速 R-CNN 作为快速 R-CNN 的检测器</h2><p id="ad02" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">快速 R-CNN 检测器还包括一个 CNN 主干、一个 ROI 池层和完全连接的层，后面是两个用于分类和边界框回归的兄弟分支，如图 3 所示。</p><ul class=""><li id="9e22" class="mw mx iq kh b ki kj kl km ko nn ks no kw np la nd ne nf ng bi translated">输入图像首先通过主干 CNN 得到特征图(特征大小:60，40，512)。除了测试时间效率，使用 RPN 作为建议生成器的另一个关键原因是 RPN 主干和快速 R-CNN 检测器主干之间<strong class="kh ir">重量共享的优势。</strong></li><li id="a81b" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">接下来，来自 RPN 的边界框提议被用于汇集来自主干特征地图的特征。这是由 ROI pooling 层完成的。ROI 汇集层本质上是通过以下方式工作的:a)从主干特征地图中提取与提议相对应的区域；b)将该区域分成固定数量的子窗口；c)在这些子窗口上执行最大汇集以给出固定大小的输出。要了解投资回报池层的细节及其优势，请阅读<a class="ae lb" href="https://medium.com/@shilpa_47526/fast-r-cnn-for-object-detection-a-technical-summary-a0ff94faa022" rel="noopener"> Fast R-CNN </a>。</li><li id="f4f3" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">ROI 池层的输出大小为(N，7，7，512 ),其中 N 是区域建议算法的建议数。在通过两个完全连接的图层后，这些要素将被送入兄弟分类和回归分支。</li><li id="b226" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">注意，这些分类和检测分支与 RPN 的不同。这里，分类层对于检测任务中的每个类(包括一个包罗万象的背景类)都有 C 个单元。这些特征通过 softmax 层得到分类分数，即一个建议属于每个类别的概率。回归层系数用于改进预测的边界框。这里的回归量是大小不可知的，(不同于 RPN)，但特定于每个类。也就是说，所有类都有单独的回归变量，每个变量有 4 个参数，对应于回归层中的 C*4 个输出单元。</li><li id="87a5" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">关于更快的 R-CNN 如何被训练和它的损失函数的更多细节参考<a class="ae lb" href="https://medium.com/@shilpa_47526/fast-r-cnn-for-object-detection-a-technical-summary-a0ff94faa022" rel="noopener">快速 R-CNN </a>。</li></ul><h2 id="911c" class="mk ld iq bd le ml mm dn li mn mo dp lm ko mp mq lo ks mr ms lq kw mt mu ls mv bi translated">4 步交替训练</h2><p id="02f9" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">为了迫使网络在 RPN 和检测器之间共享 CNN 主干的权重，作者使用了 4 步训练方法:</p><p id="f186" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">a)如上所述，独立训练 RPN。该任务的中枢 CNN 用来自为 ImageNet 分类任务训练的网络的权重初始化，然后为区域提议任务进行微调。<br/> b)快速 R-CNN 检测器网络也是独立训练的。该任务的中枢 CNN 用来自为 ImageNet 分类任务训练的网络的权重初始化，然后为对象检测任务进行微调。RPN 权重是固定的，并且来自 RPN 的建议被用于训练更快的 R-CNN。<br/>c)RPN 现在使用来自这个更快的 R-CNN 的权重进行初始化，并且针对区域提议任务进行微调。这一次，RPN 和检测器之间的公共层中的权重保持固定，只有 RPN 特有的层被微调。这是最终的 RPN。<br/> d)再次使用新的 RPN，快速 R-CNN 检测器被微调。同样，只有检测器网络特有的层被微调，而公共层权重是固定的。</p><p id="8365" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这给出了共享卷积层的更快的 R-CNN 检测框架。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ny"><img src="../Images/4797d9f3bb9dac81d01ba670be0198e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hLM5gqM7l4fXBvXDjM6khw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 4: Results of the Faster R-CNN detection framework with a VGG backbone</figcaption></figure><h1 id="a2f5" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结果</h1><ul class=""><li id="1137" class="mw mx iq kh b ki my kl mz ko na ks nb kw nc la nd ne nf ng bi translated">在 PASCAL 数据集上的所有实验中，选择快速 R-CNN 作为检测器。使用 RPN+ZF 主干作为建议网络(不与检测器共享权重)与使用“选择性搜索”(SS)作为区域建议算法的性能相当。这已经给了我们相当的结果，大大减少了检测时间。RPN+VGG 主干网仅作为一个具有非共享权重的提案网络，其表现略好于 SS 地区提案基线。当检测器使用共享权重时，RPN 中的 ZF 和 VGG 主干的性能都超过了 SS 基线。这与大量其他实验一起验证了 RPN 作为区域提议方法的使用。</li><li id="1d17" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">与选择性搜索的 1.8 秒相比，VGG RPN 的检测需要 198 毫秒。</li><li id="3e3a" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">执行的其他实验验证了 NMS 的使用，以及单独的分类和回归分支的使用。用于根据分数对提议进行排序的分类分支似乎是保持合理的高召回率与 IoU 重叠率的重要因素，即使当对象提议的数量减少时。</li><li id="e3d9" class="mw mx iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">在观察锚箱比例和长宽比的重要性的消融研究中，作者发现使用 3 个比例和一个长宽比几乎与 3 个比例和 3 个长宽比一样有效。根据任务和数据集，可以修改这些比率和比例。在每个位置使用单个锚点会导致地图显著下降。</li></ul><p id="d701" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就结束了更快的 R-CNN 论文的技术总结。希望你喜欢(理解)！欢迎在下面的评论中讨论或更正。</p><h1 id="8ca6" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">参考资料:</h1><p id="9b34" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">[1]任，何，罗斯吉斯克，<a class="ae lb" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">更快的 R-CNN:用区域提议网络实现实时目标检测</a>，NIPS'15 会议录<br/>【2】<a class="ae lb" href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" rel="noopener ugc nofollow" target="_blank">http://www . telesens . co/2018/03/11/Object-Detection-and-classification-using-R-CNNs/</a><br/>【3】<a class="ae lb" href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/object_localization_and_detection.html" rel="noopener ugc nofollow" target="_blank">https://Leonardo araujosantos . git books . io/artificial-intelligence/</a></p></div></div>    
</body>
</html>