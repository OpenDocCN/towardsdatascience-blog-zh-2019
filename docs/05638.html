<html>
<head>
<title>Evaluate Topic Models: Latent Dirichlet Allocation (LDA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">评估主题模型:潜在狄利克雷分配(LDA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=collection_archive---------0-----------------------#2019-08-19">https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=collection_archive---------0-----------------------#2019-08-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/d177a52c824c13a87c388b2122354525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iy0_QsAey-Q-Yi7AKSzmVg.jpeg"/></div></div></figure><h2 id="23d8" class="iz ja jb bd b dl jc jd je jf jg jh dk ji translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/in-depth-analysis/home" rel="noopener"> <strong class="ak">深度剖析</strong> </a></h2><div class=""/><div class=""><h2 id="7e4d" class="pw-subtitle-paragraph kh jk jb bd b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky dk translated">构建可解释主题模型的分步指南</h2></div></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><p id="d2ee" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">前言:</strong>本文旨在提供潜在主题的综合信息，不应被视为原创作品。这些信息和代码通过一些在线文章、研究论文、书籍和开源代码被重新利用</p></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><p id="642b" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在之前的<a class="ae mc" rel="noopener" target="_blank" href="/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0">文章</a>中，我介绍了主题建模的概念，并通过使用<em class="md"> Gensim </em>实现，在 python 中使用潜在的 Dirichlet 分配(LDA)方法来开发您的第一个主题模型。</p><p id="9570" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">基于这种理解，在本文中，我们将更进一步，概述通过主题一致性度量来定量评估主题模型的框架，并使用<em class="md"> Gensim </em>实现来共享 python 中的代码模板，以实现端到端的模型开发。</p><h1 id="385c" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">为什么要评估主题模型？</h1><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/0742252cc7819edf3d8768af0dc1f656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6uCZY_F4R0m0HEeDsslX1A.jpeg"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk"><a class="ae mc" href="https://tinyurl.com/y3xznjwq" rel="noopener ugc nofollow" target="_blank"><strong class="bd nf">https://tinyurl.com/y3xznjwq</strong></a></figcaption></figure><p id="f83a" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们知道，概率主题模型，如 LDA，是文本分析的流行工具，提供了语料库的预测和潜在主题表示。然而，有一个长期的假设，即这些模型发现的潜在空间通常是有意义和有用的，并且由于其无监督的训练过程，评估这些假设是具有挑战性的。此外，有一个没有黄金标准的主题列表来与每个语料库进行比较。</p><p id="128e" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">然而，同样重要的是确定一个经过训练的模型客观上是好是坏，以及有能力比较不同的模型/方法。要做到这一点，就需要一个客观的质量衡量标准。传统上，并且仍然对于许多实际应用来说，为了评估是否已经学习了关于语料库的“正确的东西”，使用隐含知识和“目测”方法。理想情况下，我们希望在一个可以最大化和比较的单一指标中捕获这些信息。</p><p id="b042" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">让我们大致了解一下评估中常用的方法:</p><p id="f665" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">眼球模型</strong></p><ul class=""><li id="7639" class="ng nh jb li b lj lk lm ln lp ni lt nj lx nk mb nl nm nn no bi translated">前 N 个单词</li><li id="d306" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb nl nm nn no bi translated">主题/文档</li></ul><p id="1d5b" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">内在评估指标</strong></p><ul class=""><li id="12f9" class="ng nh jb li b lj lk lm ln lp ni lt nj lx nk mb nl nm nn no bi translated">捕获模型语义</li><li id="1a8f" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb nl nm nn no bi translated">主题可解释性</li></ul><p id="78b3" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">人类的判断</strong></p><ul class=""><li id="a96b" class="ng nh jb li b lj lk lm ln lp ni lt nj lx nk mb nl nm nn no bi translated">什么是主题</li></ul><p id="499e" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">外部评估指标/任务评估</strong></p><ul class=""><li id="7b36" class="ng nh jb li b lj lk lm ln lp ni lt nj lx nk mb nl nm nn no bi translated">模型擅长执行预定义的任务吗，比如分类</li></ul><p id="76ef" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">自然语言是杂乱的，模糊的，充满了主观的解释，有时试图清除模糊会使语言变成一种不自然的形式。在本文中，我们将探讨更多关于主题一致性的内容，主题一致性是一种内在的评估标准，以及如何使用它来定量地证明模型选择的合理性。</p><h1 id="2adb" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">什么是话题连贯？</h1><p id="efb5" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">在理解话题连贯性之前，我们先来简单看一下困惑度。困惑度也是一种内在的评估度量，广泛用于语言模型评估。它捕捉了一个模型对它以前没有见过的新数据有多惊讶，并作为一个拒不接受的测试集的归一化对数似然性来衡量。</p><p id="b181" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">专注于对数似然部分，您可以将困惑度量视为测量一些新的看不见的数据给定之前学习的模型的可能性。也就是说，模型在多大程度上代表或再现了保留数据的统计数据。</p><p id="1ad6" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">然而，最近的研究表明，预测可能性(或等价地，困惑)和人类的判断往往不相关，甚至有时略有反相关。</p><blockquote class="nz oa ob"><p id="494b" class="lg lh md li b lj lk kl ll lm ln ko lo oc lq lr ls od lu lv lw oe ly lz ma mb ij bi translated">针对困惑进行优化可能不会产生人类可解释的主题</p></blockquote><p id="0cc3" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这种困惑测量的限制成为更多工作的动机，试图模拟人类的判断，从而<em class="md">主题连贯性。</em></p><p id="5916" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">主题连贯性的概念将许多测量方法结合到一个框架中，以评估由模型推断的主题之间的连贯性。但在此之前…</p><p id="16a8" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">什么是话题连贯？</strong></p><p id="505c" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">主题连贯性测量通过测量主题中高得分单词之间的语义相似度来对单个主题评分。这些度量有助于区分语义上可解释的主题和统计推断的工件。但是…</p><p id="e51b" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">什么是连贯？</strong></p><p id="a734" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">如果一组陈述或事实相互支持，就说它们是连贯的。因此，一个连贯的事实集可以在涵盖所有或大部分事实的上下文中进行解释。连贯事实集的一个例子是“这项运动是一项团队运动”，“这项运动是用球进行的”，“这项运动需要巨大的体力”</p><h1 id="9fd1" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">一致性度量</h1><p id="e47a" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">让我们快速看一下不同的一致性度量，以及它们是如何计算的:</p><ol class=""><li id="f9bc" class="ng nh jb li b lj lk lm ln lp ni lt nj lx nk mb of nm nn no bi translated"><strong class="li jl"/></li><li id="bcda" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><strong class="li jl"/></li><li id="f118" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><strong class="li jl"> <em class="md"> C_uci </em> </strong>测度是基于滑动窗口和给定顶词的所有词对的逐点互信息(PMI)</li><li id="17cf" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><strong class="li jl"> <em class="md"> C_umass </em> </strong>基于文档同现计数、一前分割和对数条件概率作为确认度量</li><li id="021c" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><strong class="li jl"> <em class="md"> C_npmi </em> </strong>是使用归一化逐点互信息(npmi)的 C_uci 一致性的增强版本</li><li id="34b3" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><strong class="li jl"> <em class="md"> C_a </em> </strong>基于上下文窗口、顶部单词的成对比较以及使用归一化逐点互信息(NPMI)和余弦相似度的间接确认测量</li></ol><p id="b561" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">当然，主题模型评估的概念和连贯性测量还有很多。但是，请记住本文的长度和目的，让我们将这些概念应用到开发一个至少比使用默认参数更好的模型中。此外，我们将重新利用已经可用的在线代码来支持这一练习，而不是重新发明轮子。</p></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><h1 id="7e00" class="me mf jb bd mg mh og mj mk ml oh mn mo kq oi kr mq kt oj ku ms kw ok kx mu mv bi translated">模型实现</h1><p id="8a34" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">完整的代码可以从 GitHub 上的<a class="ae mc" href="https://github.com/kapadias/medium-articles/blob/master/natural-language-processing/topic-modeling/Evaluate%20Topic%20Models.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本中获得</a></p><ol class=""><li id="b25d" class="ng nh jb li b lj lk lm ln lp ni lt nj lx nk mb of nm nn no bi translated">加载数据</li><li id="ade9" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated">数据清理</li><li id="2555" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated">短语建模:二元语法和三元语法</li><li id="6ec2" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated">数据转换:语料库和词典</li><li id="ec83" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated">基础模型</li><li id="419a" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated">超参数调谐</li><li id="4fee" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated">最终模型</li><li id="1ccb" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated">可视化结果</li></ol><h1 id="c140" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated"><strong class="ak">加载数据</strong></h1><figure class="mx my mz na gt is gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/e74d2aa1ea81a5831471aec05a0af2d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*TW24U1lZ3CaVc6jGzZ9yzA.png"/></div></figure><p id="9511" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">对于本教程，我们将使用 NIPS 会议上发表的论文数据集。NIPS 会议(神经信息处理系统)是机器学习社区最负盛名的年度活动之一。CSV 数据文件包含从 1987 年到 2016 年(29 年！).这些论文讨论了机器学习中的各种主题，从神经网络到优化方法，等等。</p><p id="f4cf" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">让我们从查看文件的内容开始</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="6233" class="or mf jb on b be os ot l ou ov">import zipfile<br/>import pandas as pd<br/>import os<br/><br/># Open the zip file<br/>with zipfile.ZipFile("./data/NIPS Papers.zip", "r") as zip_ref:<br/>    # Extract the file to a temporary directory<br/>    zip_ref.extractall("temp")<br/><br/># Read the CSV file into a pandas DataFrame<br/>papers = pd.read_csv("temp/NIPS Papers/papers.csv")<br/><br/># Print head<br/>papers.head()</span></pre><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ow"><img src="../Images/69b65fadb49c7bc7a3c0600a1d520015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q4CFMVFEqS1gDt_A3e-gmg.png"/></div></div></figure><h1 id="d748" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated"><strong class="ak">数据清理</strong></h1><p id="710d" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">因为这个分析的目标是执行主题建模，所以我们将只关注每篇论文的文本数据，而忽略其他元数据列</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="5dd5" class="or mf jb on b be os ot l ou ov"># Remove the columns<br/>papers = papers.drop(columns=['id', 'title', 'abstract', <br/>                              'event_type', 'pdf_name', 'year'], axis=1)<br/><br/># sample only 100 papers<br/>papers = papers.sample(100)<br/><br/># Print out the first rows of papers<br/>papers.head()</span></pre><p id="1a96" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">移除标点符号/小写字母</strong></p><p id="f649" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">接下来，让我们对 paper_text 列的内容进行简单的预处理，使它们更易于分析，并得到可靠的结果。为此，我们将使用正则表达式删除所有标点，然后将文本小写</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="493a" class="or mf jb on b be os ot l ou ov"># Load the regular expression library<br/>import re<br/><br/># Remove punctuation<br/>papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\.!?]', '', x))<br/><br/># Convert the titles to lowercase<br/>papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())<br/><br/># Print out the first rows of papers<br/>papers['paper_text_processed'].head()</span></pre><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ox"><img src="../Images/d4ec5c70a2126ad9def95e66a9da132a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EwKCmKLgCYRPkUoaJITdNg.png"/></div></div></figure><p id="7787" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">分词并进一步清理文字</strong></p><p id="5cec" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">让我们把每个句子标记成一个单词列表，去掉标点符号和不必要的字符。</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="526d" class="or mf jb on b be os ot l ou ov">import gensim<br/>from gensim.utils import simple_preprocess<br/><br/>def sent_to_words(sentences):<br/>    for sentence in sentences:<br/>        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations<br/><br/>data = papers.paper_text_processed.values.tolist()<br/>data_words = list(sent_to_words(data))<br/><br/>print(data_words[:1][0][:30])</span></pre><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/5ff3ea21ff7a305cb8a0f72dcfbaf7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*By8g6byGawpUwy6_kYvUrQ.png"/></div></div></figure><h1 id="2683" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">短语建模:二元和三元模型</h1><p id="5695" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">二元模型是在文档中频繁出现的两个词。三元模型是三个经常出现的词。我们例子中的一些例子有:“back_bumper”、“oil_leakage”、“maryland_college_park”等。</p><p id="1afe" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><a class="ae mc" href="https://radimrehurek.com/gensim/models/phrases.html" rel="noopener ugc nofollow" target="_blank"> Gensim 的短语模型</a>可以构建和实现二元模型、三元模型、四元模型等。短语的两个重要参数是最小计数和阈值。</p><blockquote class="nz oa ob"><p id="b362" class="lg lh md li b lj lk kl ll lm ln ko lo oc lq lr ls od lu lv lw oe ly lz ma mb ij bi translated">这些参数的值越高，单词就越难组合。</p></blockquote><pre class="mx my mz na gt om on oo bn op oq bi"><span id="7a80" class="or mf jb on b be os ot l ou ov"># Build the bigram and trigram models<br/>bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.<br/>trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  <br/><br/># Faster way to get a sentence clubbed as a trigram/bigram<br/>bigram_mod = gensim.models.phrases.Phraser(bigram)<br/>trigram_mod = gensim.models.phrases.Phraser(trigram)</span></pre><p id="2e3f" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">移除停用词，制作二元模型，并使其词条化</strong></p><p id="95e0" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">模型准备好了。让我们定义函数来删除停用词，制作三元模型和词汇化，并按顺序调用它们。</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="b8c1" class="or mf jb on b be os ot l ou ov"># NLTK Stop words<br/>import nltk<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords<br/><br/>stop_words = stopwords.words('english')<br/>stop_words.extend(['from', 'subject', 're', 'edu', 'use'])<br/><br/># Define functions for stopwords, bigrams, trigrams and lemmatization<br/>def remove_stopwords(texts):<br/>    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]<br/><br/>def make_bigrams(texts):<br/>    return [bigram_mod[doc] for doc in texts]<br/><br/>def make_trigrams(texts):<br/>    return [trigram_mod[bigram_mod[doc]] for doc in texts]<br/><br/>def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):<br/>    """https://spacy.io/api/annotation"""<br/>    texts_out = []<br/>    for sent in texts:<br/>        doc = nlp(" ".join(sent)) <br/>        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])<br/>    return texts_out</span></pre><p id="7677" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">让我们按顺序调用函数。</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="3594" class="or mf jb on b be os ot l ou ov">!python -m spacy download en_core_web_sm<br/>import spacy<br/><br/># Remove Stop Words<br/>data_words_nostops = remove_stopwords(data_words)<br/><br/># Form Bigrams<br/>data_words_bigrams = make_bigrams(data_words_nostops)<br/><br/># Initialize spacy 'en' model, keeping only tagger component (for efficiency)<br/>nlp = spacy.load("en_core_web_sm", disable=['parser', 'ner'])<br/><br/># Do lemmatization keeping only noun, adj, vb, adv<br/>data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])<br/><br/>print(data_lemmatized[:1][0][:30])</span></pre><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/67f528507e603c54295ceb5af301da22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XydCAX0C7UzgKYxgJ7_vyg.png"/></div></div></figure><h1 id="9178" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">数据转换:语料库和词典</h1><p id="5b1a" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">LDA 主题模型的两个主要输入是词典(id2word)和语料库。让我们创造它们。</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="9901" class="or mf jb on b be os ot l ou ov">import gensim.corpora as corpora<br/><br/># Create Dictionary<br/>id2word = corpora.Dictionary(data_lemmatized)<br/><br/># Create Corpus<br/>texts = data_lemmatized<br/><br/># Term Document Frequency<br/>corpus = [id2word.doc2bow(text) for text in texts]<br/><br/># View<br/>print(corpus[:1][0][:30])</span></pre><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/83828bc46b7a3722147ae725ded98b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LSn6MpqNhqoXRF6w9c6VBQ.png"/></div></div></figure><p id="5b56" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">Gensim 为文档中的每个单词创建一个唯一的 id。上面显示的产生的语料库是(word_id，word_frequency)的映射。</p><p id="9d64" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">例如，上面的(0，7)意味着，单词 id 0 在第一个文档中出现了七次。同样，单词 id 1 出现三次，依此类推</p><h1 id="c19e" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">基础模型</h1><p id="462e" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">我们拥有训练基本 LDA 模型所需的一切。除了语料库和词典，您还需要提供主题的数量。除此之外，alpha 和 eta 是影响主题稀疏性的超参数。根据 Gensim 文档，两者都默认为 1.0/num_topics prior(我们将对基本模型使用默认值)。</p><blockquote class="nz oa ob"><p id="09f2" class="lg lh md li b lj lk kl ll lm ln ko lo oc lq lr ls od lu lv lw oe ly lz ma mb ij bi translated"><strong class="li jl"> chunksize </strong>控制在训练算法中一次处理多少个文档。增加块大小将会加快训练速度，至少只要文档块容易放入内存。</p><p id="95e8" class="lg lh md li b lj lk kl ll lm ln ko lo oc lq lr ls od lu lv lw oe ly lz ma mb ij bi translated"><strong class="li jl">通道</strong>控制我们在整个语料库上训练模型的频率(设置为 10)。通行证的另一个词可能是“时代”。迭代在某种程度上是技术性的，但本质上它控制了我们对每个文档重复特定循环的频率。将“遍数”和“迭代数”设置得足够高很重要。</p></blockquote><pre class="mx my mz na gt om on oo bn op oq bi"><span id="7468" class="or mf jb on b be os ot l ou ov"># Build LDA model<br/>lda_model = gensim.models.LdaMulticore(corpus=corpus,<br/>                                       id2word=id2word,<br/>                                       num_topics=10, <br/>                                       random_state=100,<br/>                                       chunksize=100,<br/>                                       passes=10,<br/>                                       per_word_topics=True)</span></pre><p id="c905" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">查看 LDA 模型中的主题</strong></p><p id="df74" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">上面的 LDA 模型是用 10 个不同的主题构建的，其中每个主题是关键字的组合，并且每个关键字对主题有一定的权重。</p><p id="a216" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">您可以使用<em class="md">LDA _ model . print _ topics()\</em>查看每个主题的关键字以及每个关键字的权重(重要性)</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="838c" class="or mf jb on b be os ot l ou ov">from pprint import pprint<br/><br/># Print the Keyword in the 10 topics<br/>pprint(lda_model.print_topics())<br/>doc_lda = lda_model[corpus]</span></pre><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/e4590abc621c3c152269d80d7aed2762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NdkH1DKvdAyJiHGZTli48g.png"/></div></div></figure><p id="3f04" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">计算模型复杂度和一致性分数</strong></p><p id="5449" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">让我们计算基线一致性分数</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="a7b7" class="or mf jb on b be os ot l ou ov">from gensim.models import CoherenceModel<br/><br/># Compute Coherence Score<br/>coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')<br/>coherence_lda = coherence_model_lda.get_coherence()<br/>print('Coherence Score: ', coherence_lda)</span></pre><p id="a35b" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl"> <em class="md">连贯性得分:0.301 </em> </strong></p><h1 id="dcc2" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">超参数调谐</h1><p id="3e38" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">首先，让我们区分模型超参数和模型参数:</p><blockquote class="nz oa ob"><p id="bdf1" class="lg lh md li b lj lk kl ll lm ln ko lo oc lq lr ls od lu lv lw oe ly lz ma mb ij bi translated"><strong class="li jl">模型超参数</strong>可视为机器学习算法的设置，由数据科学家在训练前进行调整。例如随机森林中的树的数量，或者在我们的例子中，主题的数量 K</p><p id="765e" class="lg lh md li b lj lk kl ll lm ln ko lo oc lq lr ls od lu lv lw oe ly lz ma mb ij bi translated"><strong class="li jl">模型参数</strong>可以被认为是模型在训练期间学习的内容，例如给定主题中每个单词的权重</p></blockquote><p id="354d" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">现在我们已经有了默认 LDA 模型的基线一致性分数，让我们执行一系列敏感性测试来帮助确定以下模型超参数:</p><ol class=""><li id="3dd8" class="ng nh jb li b lj lk lm ln lp ni lt nj lx nk mb of nm nn no bi translated">主题数量(K)</li><li id="ce2b" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated">狄利克雷超参数α:文档主题密度</li><li id="30ee" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated">狄利克雷超参数β:词主题密度</li></ol><p id="99df" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们将按顺序执行这些测试，通过保持其他参数不变，一次执行一个参数，并在两个不同的验证语料库集上运行它们。我们将使用<strong class="li jl"> <em class="md"> C_v </em> </strong>作为我们选择的性能比较指标</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="c6e5" class="or mf jb on b be os ot l ou ov"># supporting function<br/>def compute_coherence_values(corpus, dictionary, k, a, b):<br/>    <br/>    lda_model = gensim.models.LdaMulticore(corpus=corpus,<br/>                                           id2word=dictionary,<br/>                                           num_topics=k, <br/>                                           random_state=100,<br/>                                           chunksize=100,<br/>                                           passes=10,<br/>                                           alpha=a,<br/>                                           eta=b)<br/>    <br/>    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')<br/>    <br/>    return coherence_model_lda.get_coherence()</span></pre><p id="244c" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">让我们调用这个函数，并在主题、alpha 和 beta 参数值的范围内迭代它</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="4f0c" class="or mf jb on b be os ot l ou ov">import numpy as np<br/>import tqdm<br/><br/>grid = {}<br/>grid['Validation_Set'] = {}<br/><br/># Topics range<br/>min_topics = 2<br/>max_topics = 11<br/>step_size = 1<br/>topics_range = range(min_topics, max_topics, step_size)<br/><br/># Alpha parameter<br/>alpha = list(np.arange(0.01, 1, 0.3))<br/>alpha.append('symmetric')<br/>alpha.append('asymmetric')<br/><br/># Beta parameter<br/>beta = list(np.arange(0.01, 1, 0.3))<br/>beta.append('symmetric')<br/><br/># Validation sets<br/>num_of_docs = len(corpus)<br/>corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), <br/>               corpus]<br/><br/>corpus_title = ['75% Corpus', '100% Corpus']<br/><br/>model_results = {'Validation_Set': [],<br/>                 'Topics': [],<br/>                 'Alpha': [],<br/>                 'Beta': [],<br/>                 'Coherence': []<br/>                }<br/><br/># Can take a long time to run<br/>if 1 == 1:<br/>    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))<br/>    <br/>    # iterate through validation corpuses<br/>    for i in range(len(corpus_sets)):<br/>        # iterate through number of topics<br/>        for k in topics_range:<br/>            # iterate through alpha values<br/>            for a in alpha:<br/>                # iterare through beta values<br/>                for b in beta:<br/>                    # get the coherence score for the given parameters<br/>                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, <br/>                                                  k=k, a=a, b=b)<br/>                    # Save the model results<br/>                    model_results['Validation_Set'].append(corpus_title[i])<br/>                    model_results['Topics'].append(k)<br/>                    model_results['Alpha'].append(a)<br/>                    model_results['Beta'].append(b)<br/>                    model_results['Coherence'].append(cv)<br/>                    <br/>                    pbar.update(1)<br/>    pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)<br/>    pbar.close()</span></pre><h1 id="1931" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">调查结果</h1><p id="5f19" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">让我们从确定主题的最佳数量开始。下表列出了两个验证集的主题数量的一致性分数 C_v，以及固定的 alpha = 0.01 和 beta = 0.1</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oz"><img src="../Images/a5c62b60458eed60fb5429ce6c218e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRLUGZh-Biyb_fNNnTYhig.png"/></div></div></figure><p id="99c7" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">随着一致性分数似乎随着主题数量的增加而增加，在变平或大幅下降之前，选择 CV 最高的模型可能更有意义。在这种情况下，我们选择 K=8</p><p id="e086" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">接下来，我们要选择最佳的α和β参数。虽然有其他复杂的方法来处理选择过程，但对于本教程，我们选择 K=8 时产生最大 C_v 得分的值</p><figure class="mx my mz na gt is gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/1af75efc7752f047704bfecfc07413cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*2vhzbzTOFLHnnc_qe7T6Jw.png"/></div></figure><blockquote class="nz oa ob"><p id="912d" class="lg lh md li b lj lk kl ll lm ln ko lo oc lq lr ls od lu lv lw oe ly lz ma mb ij bi translated">阿尔法=0.01</p><p id="4215" class="lg lh md li b lj lk kl ll lm ln ko lo oc lq lr ls od lu lv lw oe ly lz ma mb ij bi translated">β= 0.9</p><p id="565f" class="lg lh md li b lj lk kl ll lm ln ko lo oc lq lr ls od lu lv lw oe ly lz ma mb ij bi translated">K=8</p></blockquote><p id="7a00" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这大约产生。比基线得分提高了 17%</p><h1 id="1386" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">最终模型</h1><p id="3542" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">让我们使用上面选择的参数来训练最终的模型</p><pre class="mx my mz na gt om on oo bn op oq bi"><span id="a5f0" class="or mf jb on b be os ot l ou ov">num_topics = 8<br/><br/>lda_model = gensim.models.LdaMulticore(corpus=corpus,<br/>                                           id2word=id2word,<br/>                                           num_topics=num_topics, <br/>                                           random_state=100,<br/>                                           chunksize=100,<br/>                                           passes=10,<br/>                                           alpha=0.01,<br/>                                           eta=0.9)</span></pre><h1 id="d51e" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">可视化主题</h1><pre class="mx my mz na gt om on oo bn op oq bi"><span id="e4c8" class="or mf jb on b be os ot l ou ov">import pyLDAvis.gensim_models as gensimvis<br/>import pickle <br/>import pyLDAvis<br/><br/># Visualize the topics<br/>pyLDAvis.enable_notebook()<br/><br/>LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))<br/><br/># # this is a bit time consuming - make the if statement True<br/># # if you want to execute visualization prep yourself<br/>if 1 == 1:<br/>    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)<br/>    with open(LDAvis_data_filepath, 'wb') as f:<br/>        pickle.dump(LDAvis_prepared, f)<br/><br/># load the pre-prepared pyLDAvis data from disk<br/>with open(LDAvis_data_filepath, 'rb') as f:<br/>    LDAvis_prepared = pickle.load(f)<br/><br/>pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')<br/><br/>LDAvis_prepared</span></pre><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pb"><img src="../Images/5eb3f57d1a613627a9151a6a6f9f20e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6NCCBZjNqDasW7uLhog3hg.png"/></div></div></figure><h1 id="98d5" class="me mf jb bd mg mh mi mj mk ml mm mn mo kq mp kr mq kt mr ku ms kw mt kx mu mv bi translated">结束语</h1><p id="bbb1" class="pw-post-body-paragraph lg lh jb li b lj nu kl ll lm nv ko lo lp nw lr ls lt nx lv lw lx ny lz ma mb ij bi translated">我们从理解为什么评估主题模型是必要的开始。接下来，我们回顾了现有的方法，并触及了主题一致性的表面，以及可用的一致性度量。然后，我们使用 Gensim 实现构建了一个默认的 LDA 模型，以建立基线一致性得分，并回顾了优化 LDA 超参数的实用方法。</p><p id="e0d7" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">希望这篇文章能够揭示潜在的主题评估策略，以及其背后的直觉。</p><p id="1dcc" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li jl">参考文献:</strong></p><ol class=""><li id="7096" class="ng nh jb li b lj lk lm ln lp ni lt nj lx nk mb of nm nn no bi translated"><a class="ae mc" href="http://qpleple.com/perplexity-to-evaluate-topic-models/" rel="noopener ugc nofollow" target="_blank">http://qpleple.com/perplexity-to-evaluate-topic-models/</a></li><li id="b15f" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><a class="ae mc" href="https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020" rel="noopener ugc nofollow" target="_blank">https://www . Amazon . com/Machine-Learning-probability-Perspective-computing/DP/0262018020</a></li><li id="3112" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><a class="ae mc" href="https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models . pdf</a></li><li id="8337" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><a class="ae mc" href="https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/matti lyra/pydataberlin-2017/blob/master/notebook/evaluationunsupervisedmodels . ipynb</a></li><li id="b0a1" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><a class="ae mc" href="https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/" rel="noopener ugc nofollow" target="_blank">https://www . machine learning plus . com/NLP/topic-modeling-gensim-python/</a></li><li id="ab42" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><a class="ae mc" href="http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf" rel="noopener ugc nofollow" target="_blank">http://SVN . aksw . org/papers/2015/WSDM _ 话题 _ 评价/public.pdf </a></li><li id="6b0c" class="ng nh jb li b lj np lm nq lp nr lt ns lx nt mb of nm nn no bi translated"><a class="ae mc" href="http://palmetto.aksw.org/palmetto-webapp/" rel="noopener ugc nofollow" target="_blank">http://palmetto.aksw.org/palmetto-webapp/</a></li></ol></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><p id="0a37" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">感谢阅读。<em class="md">如果您有任何反馈，请对本文发表评论，在</em><a class="ae mc" href="https://www.linkedin.com/in/shashankkapadia/" rel="noopener ugc nofollow" target="_blank"><em class="md">LinkedIn</em></a><em class="md">上给我发消息，或者给我发电子邮件(shmkapadia[at]gmail.com) </em></p><p id="07de" class="pw-post-body-paragraph lg lh jb li b lj lk kl ll lm ln ko lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><em class="md">如果你喜欢这篇文章，请访问我的其他文章</em></p><div class="ip iq gp gr ir pc"><a rel="noopener follow" target="_blank" href="/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jl gy z fp ph fr fs pi fu fw jk bi translated">Python 中的主题建模:潜在狄利克雷分配(LDA)</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">如何开始使用 Python 中的 LDA 进行主题建模</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq ix pc"/></div></div></a></div><div class="ip iq gp gr ir pc"><a rel="noopener follow" target="_blank" href="/building-blocks-text-pre-processing-641cae8ba3bf"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jl gy z fp ph fr fs pi fu fw jk bi translated">构建块:文本预处理</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">本文是关于自然语言处理的后续文章的第二篇。这一系列…的目的</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pr l pn po pp pl pq ix pc"/></div></div></a></div><div class="ip iq gp gr ir pc"><a rel="noopener follow" target="_blank" href="/introduction-to-language-models-n-gram-e323081503d9"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jl gy z fp ph fr fs pi fu fw jk bi translated">语言模型简介:N-Gram</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">本文是关于自然语言处理的第三篇文章。这一系列…的目的</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="ps l pn po pp pl pq ix pc"/></div></div></a></div></div></div>    
</body>
</html>