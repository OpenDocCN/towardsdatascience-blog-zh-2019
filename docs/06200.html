<html>
<head>
<title>Bayesian Priors and Regularization Penalties</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯先验和正则化惩罚</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-priors-and-regularization-penalties-6d0054d9747b?source=collection_archive---------16-----------------------#2019-09-07">https://towardsdatascience.com/bayesian-priors-and-regularization-penalties-6d0054d9747b?source=collection_archive---------16-----------------------#2019-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0457" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">实证检验它们的等价性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0695605a5dee60cdfe7599df3461b681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DiBS5qrL8eW7wdVaxwXmTw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@introspectivedsgn?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Erik Mclean</a> on <a class="ae ky" href="https://unsplash.com/s/photos/dice?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="3889" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">执行机器学习的贝叶斯方法提供了几个优于其对应方的优势，特别是估计不确定性的能力和将上下文知识编码为先验分布的选项。那么，为什么它们没有得到更广泛的应用呢？</p><p id="bf77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了明确地被认为是一种升级，贝叶斯模型必须对所有流行的机器学习模型都有一个等效的理论公式，并恢复在其中观察到的那种预测性能。它们必须同样易于使用。</p><p id="9afd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些都是艰巨的挑战，有许多技术障碍需要克服。然而，我今天在这里不是为了对概率编程的状态进行哲学探讨。相反，我想花一些时间来探索贝叶斯方法的许多初学者教科书中提出的等价性:贝叶斯线性模型中系数的先验分布与正则化最小二乘回归中使用的惩罚项之间的等价性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/d3f248dba74e5637c4c0fe834eb8918a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*5UV5pIhxqhoQIv-VPJZPHg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The equivalence we will be exploring</figcaption></figure><p id="e233" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我发现这种二元性令人信服，因为就其本身而言，正规化似乎有点“杂乱无章”知道它可以在一个更大的框架内被理解和形式化是令人欣慰的，这是我认为值得凭经验探究的事情。</p><p id="c977" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种等价性有许多很好的理论处理方法，因此我选择通过改变正则化线性模型的调整参数来测试它，并查看最大系数的大小以及回归问题的误差是如何响应的。我用贝叶斯 GLM 做了同样的事情，调整了参数先验分布的方差。这是一次有趣的练习，我认为结果值得分享。</p><h2 id="a237" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">该理论</h2><p id="817b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我现在想强调的是，我在这里的目的不是向那些不熟悉的人介绍贝叶斯线性模型。如果你是概率编程新手，并且有 Python 经验，我推荐<a class="ae ky" href="https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/" rel="noopener ugc nofollow" target="_blank">针对黑客的贝叶斯方法</a>。无论如何，我的目标是提供另一个角度来看线性回归的两个公式是如何等价的。</p><p id="53a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我将简要概述线性回归的贝叶斯观点。在此公式中，响应变量<em class="mu"> Y </em>被视为随机变量，其平均值等于特征<em class="mu"> βX. </em>的加权和</p><p id="9b56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即<em class="mu">Y∞N(βX，σ)。</em>如果噪声项呈正态分布(<em class="mu"> Y=βX+ϵ，</em>与<em class="mu"> ϵ∼N(0，【σ】</em>)<em class="mu">，这相当于线性回归的标准公式。</em></p><p id="a90f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们可以指定参数<em class="mu"> β </em>的先验分布。常见的选择是高斯分布。如果我们将这个分布集中在 0 附近，这将表明我们期望参数很小。为该先验的标准差选择小值将对应于更紧密的分布，表明对小参数的更强的初始信念-类似于正则化最小二乘回归中的大惩罚。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/c710aa9b1a9c439fcaf3b2c3485641c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*iawRYZV-KhFOyuxK5iVoNg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">A plot showing the sampled posterior distributions of some model parameters and the “trace” of the sampling.</figcaption></figure><p id="0e72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在概率规划方法中，一个与我们感兴趣的所有随机变量(对于线性模型，系数和截距)的后验分布成比例的非标准化函数是根据贝叶斯定理从数据和先验中生成的。一种抽样算法，通常是<a class="ae ky" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="noopener ugc nofollow" target="_blank">马尔可夫链蒙特卡罗</a>的某种变体，然后生成这个后验估计。我使用<a class="ae ky" href="https://docs.pymc.io/" rel="noopener ugc nofollow" target="_blank"> PyMC3 </a>来构建我的贝叶斯模型，并对其参数的分布进行采样。</p><h2 id="8ca6" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">设置</h2><p id="0e5c" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我选择用 Kaggle 上的<a class="ae ky" href="https://www.kaggle.com/rayheberer/bayesian-priors-and-regularization-penalties" rel="noopener ugc nofollow" target="_blank">笔记本来进行我的小型实验。我的理由有两个:</a></p><ol class=""><li id="46ee" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">我希望其他人能够在不需要安装依赖项的情况下使用代码。</li><li id="2bf1" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">我想专注于比较我的模型的行为，Kaggle 提供了干净的数据和一个简单的导入方法</li></ol><p id="459b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我知道了我想要做的比较和情节，这是一个相当简单的过程。对于那些对技术细节感兴趣的人来说，笔记本的大部分代码都放在<a class="ae ky" href="https://www.kaggle.com/rayheberer/priors-penalties-functions" rel="noopener ugc nofollow" target="_blank">这个实用程序脚本</a>中。它包含了我的实验所需的主要成分:</p><ul class=""><li id="4006" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nk nc nd ne bi translated">一种迭代超参数、训练模型以及报告验证错误和系数幅度的方法。</li><li id="f79c" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nk nc nd ne bi translated">一个贝叶斯 GLM 的实现，带有一个 scikit-learn 风格的 API，可以插入到上面的循环中。</li><li id="a5f4" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nk nc nd ne bi translated">一个可以获取结果并生成线图的函数。我使用了我的文章中描述的那种模板，即以编程方式生成 matplotlib 子情节。</li></ul><h2 id="cd36" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">结果呢</h2><p id="886e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">下面是笔记本为岭(L2)回归和具有高斯先验的贝叶斯线性模型生成的图。如我们所料，使用具有适当范围的对数 x 轴，曲线非常相似。</p><div class="kj kk kl km gt ab cb"><figure class="nl kn nm nn no np nq paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/383bb9d3619c9f6800ef59d33d7ea4f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1i6cKVblbH_ggTKi35tHMA.png"/></div></figure><figure class="nl kn nr nn no np nq paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/4e2a86a2f27edb116075876046f210e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*cpeRg_N-5FLfI1ElsN9bOw.png"/></div></figure></div><p id="4937" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们到底看到了什么？首先，让我们回顾一下超参数。</p><p id="456d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mu"> Alpha </em>是控制岭回归中 L2 罚项的相对重要性的调整参数。损失函数由<em class="mu">L = MSE+</em>α<em class="mu">| |</em>|<strong class="lb iu"><em class="mu">θ</em></strong>|给出，其中<em class="mu"> MSE </em>是均方误差，<strong class="lb iu"> <em class="mu"> θ </em> </strong>是系数的向量。因此，当α较大时，损耗由这一项决定，将其降至最低的最佳方法是将所有系数设为 0。</p><p id="adcd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mu">适马</em>给出贝叶斯模型中系数先验分布的标准差。该先验被选择为正态(高斯)分布。如果它很小，那么我们的先验非常紧密地以零为中心，并且需要大量的证据(数据)来将参数的后验分布的质量从零移开。</p><p id="3f00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">知道了这些，上面的结果有意义吗？让我们看看:</p><ul class=""><li id="12eb" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nk nc nd ne bi translated">随着调节参数α的增加，L2 惩罚模型中的最大系数趋于零。</li><li id="e7da" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nk nc nd ne bi translated">因此，如果你一直只是猜测结果数据的平均值，误差会上升到你会得到的值。</li><li id="2728" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nk nc nd ne bi translated">同样的事情发生在贝叶斯模型中，因为先验的方差被设置得非常小。</li></ul><p id="fafb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管这两个模型背后的实现非常不同，一个依赖于优化，另一个依赖于采样，但我们在选择的两个度量中观察到几乎相同的行为。</p><h2 id="c6b2" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">结论</h2><p id="771a" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">贝叶斯线性模型通常作为寻求学习概率编程的入门材料，包含对频率主义统计学习模型的现有理解。我相信这是有效的，因为它允许一个人在现有知识的基础上构建新知识，甚至将已经理解的东西——也许只是许多工具中的一个——放入更广泛、理论上更令人满意的框架中。</p><p id="ced9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">贝叶斯线性模型中选择的参数的先验分布和正则化最小二乘回归中的惩罚项之间的关系已经是众所周知的。尽管如此，我觉得我能够通过实证检验调整每个模型的超参数的效果来对这种等价性有一个更直观的理解。我希望我的小实验能为你做同样的事情，并作为现有证据的补充。</p></div></div>    
</body>
</html>