# 机器学习安全性及其与模型可解释性的相互作用

> 原文：<https://towardsdatascience.com/machine-learning-security-and-its-interplay-with-model-interpretability-a13fa8362af8?source=collection_archive---------18----------------------->

![](img/670b4457e2491bb0b368a2d5781d65c2.png)

机器学习的发展和部署在许多方面带来了机遇和挑战:从高质量和相关数据集的管理，到可推广模型的开发，最后到所述模型的部署。在最后一个阵营中，公平性、问责制和透明度等问题是最近许多研究问题的核心(参见 FATML 社区[1])，以及可解释性和数据隐私。

模型安全性是另一个重要的考虑因素。在实践中，模型安全性旨在防止对手窃取已部署模型的模型参数(可通过 API 访问)，这种损失可能会产生竞争/法律/健壮性后果。这种担忧既影响初创公司，也影响公司，特别是 Clarifai、Google、Amazon 等较大的公司，以及其他盈利策略依赖于在其 API 生命周期内摊销模型培训成本的公司[2]。

当一家公司必须在允许访问一个非常强大的 API(带有关于模型输出的详细信息，包括类别标签、置信度得分等)和冒着被对手复制其模型的风险之间做出决定时，这种紧张关系确实令人着迷。即使折衷的解决方案建议 API 只提供有限的信息，法规要求部署的模型提供可解释的解释(法律要求[3])，这可能会再次促进模型窃取[4]。

最近，我和我的合作者介绍了梅斯:对后果性决策的模型不可知的反事实解释[5]。动机是提供一个全封装的方法，以满足对各种 ML 模型和距离度量(代表一组不同的现实世界应用)的可解释解释的法律要求。接下来，我很兴奋地探索反事实解释的潜力，同时提供高质量的解释和可证明的保证，而不透露太多关于模型的内部。实现这一点对于消费者和法律采用 ML 模型是必要的。

[1][https://www.fatml.org/](https://www.fatml.org/)

[2]https://arxiv.org/pdf/1609.02943.pdf

[3]https://arxiv.org/pdf/1711.00399.pdf

[https://arxiv.org/pdf/1807.05185.pdf](https://arxiv.org/pdf/1807.05185.pdf)

[https://arxiv.org/pdf/1905.11190.pdf](https://arxiv.org/pdf/1905.11190.pdf)

***注来自《走向数据科学》的编辑:*** *虽然我们允许独立作者根据我们的* [*规则和指导方针*](/questions-96667b06af5) *发表文章，但我们不认可每个作者的贡献。你不应该在没有寻求专业建议的情况下依赖一个作者的作品。详见我们的* [*读者术语*](/readers-terms-b5d780a700a4) *。*