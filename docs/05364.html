<html>
<head>
<title>Understanding Adam : how loss functions are minimized ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解亚当:损失函数如何最小化？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-adam-how-loss-functions-are-minimized-3a75d36ebdfc?source=collection_archive---------15-----------------------#2019-08-09">https://towardsdatascience.com/understanding-adam-how-loss-functions-are-minimized-3a75d36ebdfc?source=collection_archive---------15-----------------------#2019-08-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f8bc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Adam:一种随机优化方法</h2></div><h1 id="775f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">简介</strong></h1><p id="f80d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi lw translated"><span class="l lx ly lz bm ma mb mc md me di"> W </span>在使用基于 PyTorch 构建的 fast.ai 库时，我意识到迄今为止我从未与优化器进行过交互。由于 fast.ai 在调用 fit_one_cycle 方法时已经处理了它，所以我不需要参数化优化器，也不需要了解它是如何工作的。</p><p id="ef42" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">由于其简单性和速度，Adam 可能是机器学习中使用最多的优化器。它是由<strong class="lc iu">迪德里克·金玛</strong>和<strong class="lc iu">吉米·巴雷</strong>在 2015 年开发的，并在一篇名为<a class="ae mk" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank">亚当:随机优化方法</a>的论文中介绍。</p><p id="95d4" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">和往常一样，这篇博文是我写的一张备忘单，用来检查我对一个概念的理解。如果你发现一些不清楚或不正确的地方，不要犹豫，写在评论区。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ml"><img src="../Images/615889b8bf008b7f0a20b2476e7ffe2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GeKxS-nhrHgnfMAQ3yyMeQ.png"/></div></div></figure><h1 id="bd60" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">算法</h1><p id="5111" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">首先，Adam 指的是<strong class="lc iu">自适应矩估计。</strong>Adam 背后的基本原理是提出一种优化目标函数的高效算法。该研究论文主要研究高维参数空间中随机目标的<strong class="lc iu">优化。</strong></p><p id="6960" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">Adam 是随机梯度下降的修改版，这里就不解释了。对于 memo，随机梯度下降更新规则为:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/496af785f7e7d286a02a80b10a400a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*4nQKTtP2PnhgIxOQipix2g.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">The SGD’s update rule : here J is a cost function</figcaption></figure><p id="2019" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">Adam 算法的伪代码如下:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nc"><img src="../Images/f56c4ac08699e337e7ff536c4f30460a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1KupUACFU9_8x8ILbR2nKg.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Adam algorithm pseudo-code</figcaption></figure><p id="57af" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">每一步我都会讲清楚，但首先我需要介绍一些量。</p><p id="38a2" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">由于 Adam 是一个迭代过程，我们需要一个索引<strong class="lc iu"> t </strong>，它在每一步都增加 1。</p><p id="3b2c" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu"> α </strong>是步长，相当于随机梯度下降的学习速率。</p><p id="6e09" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu"> G(t) </strong>是目标函数<strong class="lc iu"> f </strong>在步骤<strong class="lc iu"> t </strong>相对于参数向量<strong class="lc iu"> θ的梯度。</strong></p><p id="c64c" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu"> M(t) </strong>是梯度<strong class="lc iu"> G(t) </strong>的一阶矩的指数移动平均线(EMA)。因此<strong class="lc iu"> β1 </strong>均线的指数衰减率。</p><p id="a2ec" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu"> V(t) </strong>是梯度<strong class="lc iu"> G(t) </strong>的二阶矩的指数移动平均。因此<strong class="lc iu"> β2 </strong>均线的指数衰减率。</p><p id="1c49" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">最后，<strong class="lc iu"> ε </strong>是一个非常小的超参数，防止算法被零除。</p><p id="1997" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu"> <em class="nd">为什么亚当用渐变 EMA 而不用渐变？</em> </strong></p><p id="69e3" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">当计算梯度时，一些小批量可能具有异常值，这可能产生大的信息梯度，因此通过计算每个小批量的梯度的 EMA，无信息梯度的影响被最小化。此外，均线是新旧梯度的加权平均值。如果 Adam 使用算术平均值，所有梯度(第 1 步和第 t 步之间)将具有相同的权重。</p><p id="f4fa" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">让我们注意到，<strong class="lc iu">均线作为梯度</strong>的估计，我们将在后面看到。</p><p id="6a53" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu"> <em class="nd">亚当的更新法则</em> </strong></p><p id="5966" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">如上所述，亚当的更新规则只不过是梯度下降的更新规则的修改版本。在 Adam 中，梯度<strong class="lc iu">的第一个矩的 EMA 被矩的第二个矩的平方根缩放，减去参数向量θ。</strong></p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/24d7c975984f6f5064424a14f3e164cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*g_xa7t0JbHgTpzCy2y-QpA.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Adam’s update rule</figcaption></figure><p id="84fc" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">在论文中，作者将其定义为<strong class="lc iu"> SNR </strong>(信噪比)。</p><p id="7b77" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">SNR 是将<strong class="lc iu">期望信号</strong>(这里是梯度，即目标函数曲线的方向)<strong class="lc iu">的水平与背景噪声</strong>(二阶梯度，即该方向周围的噪声)的水平进行比较的度量。</p><p id="06bf" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">平方根缩放来自 RMSProp (RMS 表示均方根)算法。这个想法是，由于梯度是在多个小批量上累积的，我们需要在每一步的梯度保持稳定。由于一个小批量可以具有与另一个小批量完全不同的数据样本，为了限制梯度的变化，梯度的一阶矩的 EMA 被二阶矩的 RMS 缩放。<strong class="lc iu">人们可以将这种技术视为一种标准化，即梯度除以一种标准偏差。</strong></p><p id="4408" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu"> <em class="nd">自动退火</em> </strong></p><p id="e6ea" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">SNR 越小，有效步长越接近于零，这意味着与实际信号相比存在大量噪声，因此一阶梯度的方向是否对应于最优方向的不确定性越大。这是一个理想的特性，因为步长较小，从而限制了偏离局部最优值。</p><p id="fccc" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">SNR 通常变得更接近于 0，趋向于最佳值，导致参数空间中的小有效步长。这使得<strong class="lc iu">能够更稳健、更快速地收敛</strong>到最优。</p><p id="69c7" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated"><strong class="lc iu"> <em class="nd">初始化偏置-校正</em> </strong></p><p id="80ac" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">由于 EMA 向量被初始化为 0 的向量，所以矩估计偏向于 0，尤其是在初始时间步长期间，尤其是当衰减率很小时(β接近于 1)。</p><p id="ba75" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">为了抵消这一点，力矩估计值被<strong class="lc iu">偏差修正</strong>。</p><p id="f63e" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">使用<strong class="lc iu"> v(t) </strong>和<strong class="lc iu"> v(t-1) </strong>之间的递归关系，可以得出:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nf"><img src="../Images/bb58358559b55471d2a720df96d026c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*7LvHiK9ymePZfBUEX1QCkg.png"/></div></div></figure><p id="20fb" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">在通过将它与期望值组合来使用这个等式之后，很快就可以得出，<strong class="lc iu"> E[v(t)]等于</strong></p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ddf72a8c212d40bbb5b13fbbc7a29f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*pCrVJka1YJl7MT8_g1xk0g.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Zeta is a residual parameter to adjust the equation</figcaption></figure><p id="930d" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">这就是为什么在每一步初始化权重时，<strong class="lc iu">e【v(t)】</strong>除以<strong class="lc iu"> (1-β2^t) </strong>。</p><p id="7ca5" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">对于第一梯度矩，遵循同样的推理。</p><h1 id="2c59" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">adamax:Adam 扩展</h1><p id="1c71" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Adamax 是 Adam 的扩展。它用一个加权无穷范数代替梯度的二阶矩。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nh"><img src="../Images/9ecc64310af262969dcc094e4f2bfc53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0hAfg6L50Z6IhLFCSYoAZg.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Adamax pseudo-code</figcaption></figure><p id="599d" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">使用这种变体是因为当特征矩阵稀疏时，作者已经提出了一种令人惊讶的稳定解决方案(像在嵌入中一样，考虑一下一键编码)。事实上，由于用于<strong class="lc iu"> u(t) </strong>的更新规则不仅仅依赖于梯度，所以该解决方案是健壮的。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e93e8d2e0d363294a4269505028eca73.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*oZjJ7MMyNDBjQf_aRcTpGA.png"/></div></figure><p id="f301" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">当<strong class="lc iu"> g(t) </strong>变得非常小时，由于 max 函数，更新规则选择<strong class="lc iu"> β2*u(t-1) </strong>。</p><h1 id="8da2" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结果</h1><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nj"><img src="../Images/3b147023395ec19c44217c1b8ee457e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1DY550UrQTpvYuWxWgOvNg.png"/></div></div></figure><p id="9480" class="pw-post-body-paragraph la lb it lc b ld mf ju lf lg mg jx li lj mh ll lm ln mi lp lq lr mj lt lu lv im bi translated">比较 MNIST 数据集上的结果，我们观察到 Adam 比 Adagrad、RMSProp 或 AdaDelta 收敛得更快且更接近最优。</p></div></div>    
</body>
</html>