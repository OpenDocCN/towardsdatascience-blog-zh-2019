<html>
<head>
<title>Everything You Ever Wanted to Know About K-Nearest Neighbors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你想知道的关于 K 近邻的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-k-nearest-neighbors-dab986e21b60?source=collection_archive---------20-----------------------#2019-12-05">https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-k-nearest-neighbors-dab986e21b60?source=collection_archive---------20-----------------------#2019-12-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bb03" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">五分钟后</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ab9ed370a21affbc7d4f596c19f0e405.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*t5j-q3pOgWYx5CMq"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@ninastrehl?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nina Strehl</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9fce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">k 近邻算法是最简单、最容易理解的机器学习算法之一。它可以用于分类和回归任务，但在分类中更常见，所以我们将从这个用例开始。不过，这些原则在这两种情况下都适用。</p><h1 id="b6db" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">下面是算法:</strong></h1><ol class=""><li id="7669" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu mu mv mw mx bi translated">定义<em class="my"> k </em></li><li id="beb7" class="mn mo it lb b lc mz lf na li nb lm nc lq nd lu mu mv mw mx bi translated">定义距离度量，通常是欧几里得距离</li><li id="c706" class="mn mo it lb b lc mz lf na li nb lm nc lq nd lu mu mv mw mx bi translated">对于一个新的数据点，找到最近的<em class="my"> k </em>个训练点，并以某种方式组合它们的类别——通常是投票——以获得一个预测的类别</li></ol><p id="2ecf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样！</p><p id="eac6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一些好处:</strong></p><ul class=""><li id="2d2f" class="mn mo it lb b lc ld lf lg li ne lm nf lq ng lu nh mv mw mx bi translated">不需要任何传统意义上的训练。你只需要一个快速的方法来找到最近的邻居。</li><li id="9ee5" class="mn mo it lb b lc mz lf na li nb lm nc lq nd lu nh mv mw mx bi translated">易于理解和解释。</li></ul><p id="cb5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">部分否定:</strong></p><ul class=""><li id="9c89" class="mn mo it lb b lc ld lf lg li ne lm nf lq ng lu nh mv mw mx bi translated">您需要定义 k，这是一个超参数，因此它可以通过交叉验证进行调整。<strong class="lb iu">k 值越高，偏差越大，k 值越小，方差越大。</strong></li><li id="f6ee" class="mn mo it lb b lc mz lf na li nb lm nc lq nd lu nh mv mw mx bi translated">您必须选择一个距离度量，根据度量的不同，可能会得到非常不同的结果。同样，您可以使用交叉验证来决定使用哪个距离。</li><li id="14fc" class="mn mo it lb b lc mz lf na li nb lm nc lq nd lu nh mv mw mx bi translated">它并没有真正提供哪些特性可能是重要的见解。</li><li id="914a" class="mn mo it lb b lc mz lf na li nb lm nc lq nd lu nh mv mw mx bi translated">由于维数灾难，它可能会受到高维数据的影响。基本上，维度的诅咒意味着在高维度中，很可能接近的点并不比平均距离近多少，这意味着接近并没有太大的意义。在高维度中，数据变得非常分散，这就造成了这种现象。网上有很多关于这方面的好资源，我就不多说了。</li></ul><p id="7bf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">基本假设:</strong></p><ul class=""><li id="a33f" class="mn mo it lb b lc ld lf lg li ne lm nf lq ng lu nh mv mw mx bi translated">对于我们的目标类或值，接近的数据点共享相似的值。</li></ul><h1 id="6a38" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">投票方式</strong></h1><ul class=""><li id="054a" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu nh mv mw mx bi translated"><strong class="lb iu">多数投票:</strong>选出<em class="my"> k n </em>个最近的邻居后，对这些邻居的班级进行“投票”。新的数据点被分类为邻居的大多数类。如果您正在进行二元分类，建议您使用奇数个邻居，以避免票数相等。然而，在一个多类问题中，更难避免平局。一个常见的解决方案是降低 k ，直到平局被打破。最常见类别的投票分数也可以作为该类别的概率得分。</li><li id="f632" class="mn mo it lb b lc mz lf na li nb lm nc lq nd lu nh mv mw mx bi translated"><strong class="lb iu">距离加权:</strong>不是直接对最近的邻居<em class="my"> k </em>进行投票，而是根据该实例与新数据点的距离对每一票进行加权。一种常见的加权方法是基于新数据点和训练点之间的距离。新的数据点被添加到总权重最大的类别中。这不仅减少了平局的机会，而且还减少了数据失真的影响。</li></ul><h1 id="8347" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">距离度量</h1><p id="1d43" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">欧几里德距离，或 2 范数，是一种非常常见的距离度量，用于<em class="my"> k- </em>最近邻。然而，可以使用任何标准。<em class="my">p</em>-定额定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/1c1ffeb5073b577fcbe88d6b6ec3eca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*B8m1CHA4tS5naMJpi9OPiw.png"/></div></div></figure><p id="5ae0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，对于<em class="my"> p </em>的任何值，您可以定义两点之间的距离:<em class="my"> x </em>和<em class="my"> y. </em></p><h1 id="8c27" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">搜索算法</h1><p id="1fe5" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">如上所述，除了准备一种快速查找相邻点的方法之外，没有真正的训练。假设您的数据集包含 2000 个点。对一个点的 3 个最近邻居的强力搜索不需要很长时间。但是如果数据集包含 2，000，000 个点，强力搜索可能会变得非常昂贵，尤其是在数据的维度很大的情况下。其他搜索算法为了更快的运行时间而牺牲了穷举搜索。KDTrees 或 Ball trees 等结构用于加快运行时间。虽然我们不会深入这些结构的细节，但了解它们以及它们如何优化您的运行时间(尽管训练时间确实会增加)是很重要的。</p><h1 id="9294" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">半径邻居分类器</h1><p id="b4d7" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">一个非常相关的算法是半径邻居分类器。这与最近邻分类器的想法相同，但不是查找<em class="my"> k- </em>最近邻，而是查找给定半径内的所有邻居。设置半径需要一些领域知识；如果您的点紧密地聚集在一起，您可能希望使用较小的半径来避免几乎每个点都投票。</p><h1 id="68cc" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">k-邻居回归量</h1><p id="d7eb" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">为了将我们的问题从分类转变为回归，我们所要做的就是找到第<em class="my"> k </em>个最近邻的加权平均值。我们使用与上述相同的加权方法，计算这些最接近值的加权平均值，而不是取多数类。在这种情况下,“多数投票”只是邻居的简单平均值，而“距离加权”是由距离加权的平均值。</p><h1 id="b71f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">代码！</h1><p id="970e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">让我们看看这一切是如何与一些 Python 代码结合在一起的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" href="https://gist.github.com/tfolkman/1f122eb101011edc4a1456c70aef07fd" rel="noopener ugc nofollow" target="_blank">https://gist.github.com/tfolkman/1f122eb101011edc4a1456c70aef07fd</a></figcaption></figure><p id="e9ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦运行，您将获得:</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="5c48" class="nt lw it np b gy nu nv l nw nx">Best Params: {'n_neighbors': 5, 'p': 1, 'weights': 'uniform'} <br/>Train F1: 0.9606837606837607 <br/>Test Classification Report:             <br/>                  precision    recall  f1-score   support            <br/>          0       0.97         0.88    0.93       43  <br/>          1       0.93         0.99    0.96       71  <br/>avg / total       0.95         0.95    0.95       114  <br/>Train Accuracy: 0.9494505494505494 <br/>Test accuracy: 0.9473684210526315</span></pre><p id="02a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还不错！希望这篇文章能帮助你更好地理解 K 近邻是如何工作的，以及如何在 Python 中实现它。如果你想学习更多的机器学习算法，一定要看看《Scikit-Learn 和 TensorFlow 的<a class="ae ky" href="https://amzn.to/38cz74r" rel="noopener ugc nofollow" target="_blank">动手机器学习》这本书。</a></p><p id="6f98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章也可以在这里找到<a class="ae ky" href="https://learningwithdata.com/posts/tylerfolkman/everything-you-ever-wanted-to-know-about-k-nearest-neighbors-dab986e21b60/" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="ba42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">加入我的</strong> <a class="ae ky" href="https://upscri.be/lg7gvt" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">邮箱列表</strong> </a> <strong class="lb iu">保持联系。</strong></p></div></div>    
</body>
</html>