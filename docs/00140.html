<html>
<head>
<title>PyTorch Autograd</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 亲笔签名</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95?source=collection_archive---------1-----------------------#2019-01-07">https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95?source=collection_archive---------1-----------------------#2019-01-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cc8b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解 PyTorch 魔法的核心</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b1faab1ba8da2c4de58afd028d62434a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7w9w0tGu5pPSBeS6FbbzEQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/a235791aa481266841cd80f787d3328e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*m8N1xnJZ4wjBj_3s4_e5RQ.jpeg"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><a class="ae kw" href="https://medium.com/@timetraveller1998/that-is-true-b02561bf9c1c?source=responses---------0---------------------" rel="noopener">In the process it never explicitly constructs the whole Jacobian. It’s usually simpler and more efficient to compute the JVP directly.</a></figcaption></figure><p id="34e3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae kw" href="https://medium.com/@timetraveller1998/that-is-true-b02561bf9c1c?source=responses---------0---------------------" rel="noopener">来源:https://www . cs . Toronto . edu/~ rgrosse/courses/CSC 321 _ 2018/slides/LEC 10 . pdf</a></p><p id="a0e3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae kw" href="http://bumpybrains.com/comics.php?comic=34" rel="noopener ugc nofollow" target="_blank">资料来源:http://bumpybrains.com/comics.php?comic=34</a></p><p id="ad6e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们只是同意，当涉及到大型神经网络时，我们都不擅长微积分。通过显式求解数学方程来计算如此大的复合函数的梯度是不切实际的，尤其是因为这些曲线存在于大量的维度中并且是不可能理解的。</p><blockquote class="lt"><p id="56a3" class="lu lv iq bd lw lx ly lz ma mb mc ls dk translated">为了处理 14 维空间中的超平面，想象一个 3 维空间并大声对自己说“14”。每个人都这样做——杰弗里·辛顿</p></blockquote><p id="2e87" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">这就是 PyTorch 的亲笔签名的来源。它抽象了复杂的数学，帮助我们“神奇地”用几行代码计算高维曲线的梯度。这篇文章试图描述亲笔签名的魔力。</p><h1 id="7842" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">PyTorch 基础</h1><p id="207c" class="pw-post-body-paragraph kx ky iq kz b la na jr lc ld nb ju lf lg nc li lj lk nd lm ln lo ne lq lr ls ij bi translated">在继续之前，我们需要了解一些基本的 PyTorch 概念。</p><p id="e30b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> Tensors </strong>:简单来说就是 PyTorch 中的一个 n 维数组。Tensors 支持一些额外的增强功能，这使它们独一无二:除了 CPU，它们还可以加载到 GPU 上进行更快的计算。在设置<code class="fe nf ng nh ni b">.requires_grad = True</code>时，它们开始形成一个反向图，跟踪应用于它们的每一个操作，以使用一种叫做动态计算图(DCG)的东西来计算梯度(在帖子中进一步解释)。</p><p id="7971" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="nj">在 PyTorch 的早期版本中，</em> <code class="fe nf ng nh ni b"><em class="nj">torch.autograd.Variable</em></code> <em class="nj">类用于创建支持梯度计算和操作跟踪的张量，但从 PyTorch v0.4.0 变量类开始，</em> <a class="ae kw" href="https://pytorch.org/blog/pytorch-0_4_0-migration-guide/" rel="noopener ugc nofollow" target="_blank"> <em class="nj">已被弃用。</em> </a> <em class="nj"> </em> <code class="fe nf ng nh ni b">torch.Tensor</code>和<code class="fe nf ng nh ni b">torch.autograd.Variable</code>现在是一个档次。更准确地说，<code class="fe nf ng nh ni b">torch.Tensor</code>能够追踪历史，并且表现得像旧的<code class="fe nf ng nh ni b">Variable</code></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Code to show various ways to create gradient enabled tensors</figcaption></figure><blockquote class="nm nn no"><p id="33fd" class="kx ky nj kz b la lb jr lc ld le ju lf np lh li lj nq ll lm ln nr lp lq lr ls ij bi translated"><strong class="kz ir"/></p></blockquote><p id="e00c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">亲笔签名:</strong>这个类是一个计算导数的引擎(更准确地说是雅可比向量积)。它记录了在梯度张量上执行的所有操作的图形，并创建了一个称为动态计算图的非循环图形。这个图的叶子是输入张量，根是输出张量。梯度的计算方法是从根到叶追踪图形，并使用链式法则将每个梯度相乘。</p><h1 id="a833" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">神经网络和反向传播</h1><p id="6675" class="pw-post-body-paragraph kx ky iq kz b la na jr lc ld nb ju lf lg nc li lj lk nd lm ln lo ne lq lr ls ij bi translated">神经网络只不过是经过微妙调整(训练)以输出所需结果的复合数学函数。调整或训练是通过一种叫做反向传播的非凡算法来完成的。反向传播用于计算损失相对于输入权重的梯度，以便稍后更新权重并最终减少损失。</p><blockquote class="lt"><p id="4cab" class="lu lv iq bd lw lx ly lz ma mb mc ls dk translated">在某种程度上，反向传播只是链式法则——杰瑞米·霍华德的别称</p></blockquote><p id="1a0d" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">创建和训练神经网络包括以下基本步骤:</p><ol class=""><li id="fdce" class="ns nt iq kz b la lb ld le lg nu lk nv lo nw ls nx ny nz oa bi translated">定义架构</li><li id="e507" class="ns nt iq kz b la ob ld oc lg od lk oe lo of ls nx ny nz oa bi translated">使用输入数据在架构上向前传播</li><li id="98b2" class="ns nt iq kz b la ob ld oc lg od lk oe lo of ls nx ny nz oa bi translated">计算损失</li><li id="faf8" class="ns nt iq kz b la ob ld oc lg od lk oe lo of ls nx ny nz oa bi translated"><strong class="kz ir">反向传播以计算每个权重的梯度</strong></li><li id="fb08" class="ns nt iq kz b la ob ld oc lg od lk oe lo of ls nx ny nz oa bi translated">使用学习率更新权重</li></ol><p id="6038" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">输入权重的微小变化引起的损失变化称为该权重的梯度，使用反向传播进行计算。然后，使用学习率将梯度用于更新权重，以总体减少损失并训练神经网络。</p><p id="030e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这是以迭代的方式完成的。对于每次迭代，都要计算几个梯度，并建立一个称为计算图的东西来存储这些梯度函数。PyTorch 通过构建一个动态计算图(DCG)来做到这一点。该图是在每次迭代中从头开始构建的，为梯度计算提供了最大的灵活性。例如，对于前向操作(函数)<code class="fe nf ng nh ni b">Mul</code>来说，称为<code class="fe nf ng nh ni b">MulBackward</code>的后向操作(函数)被动态集成到后向图中，用于计算梯度。</p><h1 id="79a3" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">动态计算图</h1><p id="04db" class="pw-post-body-paragraph kx ky iq kz b la na jr lc ld nb ju lf lg nc li lj lk nd lm ln lo ne lq lr ls ij bi translated">启用梯度的张量(变量)与函数(运算)结合起来创建动态计算图。数据流和应用于数据的操作是在运行时定义的，因此可以动态地构建计算图。此图由引擎盖下的亲笔签名类动态制作。<a class="ae kw" href="https://github.com/pytorch/pytorch/blob/master/docs/source/notes/autograd.rst" rel="noopener ugc nofollow" target="_blank">在开始训练之前，你不必对所有可能的路径进行编码——你跑的就是你与众不同的。</a></p><p id="719e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">两个张量相乘的简单 DCG 如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/0dbd7cd47e9af7a33e9c0f40b56fff26.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*jGo_2J9UQeynwG_3olUD4w.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">DCG with requires_grad = False (Diagram created using draw.io)</figcaption></figure><p id="c96c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">图中的每个虚线框是一个变量，紫色矩形框是一个操作。</p><p id="723c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">每个变量对象都有几个成员，其中包括:</p><p id="88ef" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">数据</strong>:变量保存的数据。<strong class="kz ir"> <em class="nj"> x </em> </strong>保存一个值等于 1.0 的 1x1 张量，而<strong class="kz ir"> <em class="nj"> y </em> </strong>保存 2.0。<strong class="kz ir"> z </strong>表示两者的乘积，即 2.0</p><p id="d9e6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> requires_grad </strong>:该成员，如果为 true，则开始跟踪所有的操作历史，并形成一个用于梯度计算的反向图。对于任意张量<strong class="kz ir"> <em class="nj"> a </em> </strong> <em class="nj"> </em>它可以被就地操作如下:<code class="fe nf ng nh ni b">a.requires_grad_(True).</code></p><p id="ecf0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> grad: </strong> grad 保存渐变的值。如果<code class="fe nf ng nh ni b">requires_grad</code>为假，它将保持一个 None 值。即使<code class="fe nf ng nh ni b">requires_grad</code>为真，它也将保持一个 None 值，除非从其他节点调用<code class="fe nf ng nh ni b">.backward()</code>函数。例如，如果您为某个变量<strong class="kz ir"> <em class="nj"> out </em> </strong>调用<code class="fe nf ng nh ni b">out.backward()</code>，该变量在其计算中涉及<strong class="kz ir"><em class="nj"/></strong>，那么<code class="fe nf ng nh ni b">x.grad</code>将持有<strong class="kz ir"> ∂out/∂x </strong>。</p><p id="26e7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> grad_fn: </strong>这是用于计算梯度的反向函数。</p><p id="a4e9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> is_leaf </strong>:如果:</p><ol class=""><li id="a8fe" class="ns nt iq kz b la lb ld le lg nu lk nv lo nw ls nx ny nz oa bi translated">它是由类似于<code class="fe nf ng nh ni b">x = torch.tensor(1.0)</code>或<code class="fe nf ng nh ni b">x = torch.randn(1, 1)</code>的函数显式初始化的(基本上是本文开头讨论的所有张量初始化方法)。</li><li id="cc4e" class="ns nt iq kz b la ob ld oc lg od lk oe lo of ls nx ny nz oa bi translated">它是在对所有具有<code class="fe nf ng nh ni b">requires_grad = False.</code>的张量进行运算后创建的</li><li id="609a" class="ns nt iq kz b la ob ld oc lg od lk oe lo of ls nx ny nz oa bi translated">它是通过在某个张量上调用<code class="fe nf ng nh ni b">.detach()</code>方法创建的。</li></ol><p id="9510" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在调用<code class="fe nf ng nh ni b">backward()</code>时，只为<code class="fe nf ng nh ni b">requires_grad</code>和<code class="fe nf ng nh ni b">is_leaf</code>都为真的节点填充渐变。渐变是从调用<code class="fe nf ng nh ni b">.backward()</code>的输出节点开始的，相对于其他叶节点。</p><p id="f165" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">打开<code class="fe nf ng nh ni b">requires_grad = True</code>时，PyTorch 将开始跟踪操作，并存储每个步骤的梯度函数，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/4f5606fc35a2b7e13a84ebc2bfc18bba.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*viCEZbSODfA8ZA4ECPwHxQ.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">DCG with requires_grad = True (Diagram created using draw.io)</figcaption></figure><p id="5e91" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">PyTorch 引擎下生成上图的代码是:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><a class="ae kw" href="https://medium.com/@timetraveller1998/that-is-true-b02561bf9c1c?source=responses---------0---------------------" rel="noopener">In the process it never explicitly constructs the whole Jacobian. It’s usually simpler and more efficient to compute the JVP directly.</a></figcaption></figure><p id="a3f3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae kw" href="https://medium.com/@timetraveller1998/that-is-true-b02561bf9c1c?source=responses---------0---------------------" rel="noopener">来源:https://www . cs . Toronto . edu/~ rgrosse/courses/CSC 321 _ 2018/slides/LEC 10 . pdf</a></p><p id="157d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了阻止 PyTorch 跟踪历史和形成后向图，可以将代码包装在<code class="fe nf ng nh ni b">with torch.no_grad():</code>中，这将使代码在不需要梯度跟踪时运行得更快。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="7769" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">Backward()函数</h1><p id="83bf" class="pw-post-body-paragraph kx ky iq kz b la na jr lc ld nb ju lf lg nc li lj lk nd lm ln lo ne lq lr ls ij bi translated">Backward 是这样一个函数，它实际上是通过将其参数(默认情况下是 1x1 单位张量)通过后向图一直传递到可从调用根张量追踪到的每个叶节点来计算梯度的。然后将计算出的梯度存储在每个叶节点的<code class="fe nf ng nh ni b">.grad</code>中。<em class="nj">记住，反向图形已经在正向传递过程中动态生成。Backward function 仅使用已经制作的图形计算梯度，并将它们存储在叶节点中。</em></p><p id="d860" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们分析下面的代码</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="2515" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">需要注意的一件重要事情是，当调用<code class="fe nf ng nh ni b">z.backward()</code>时，一个张量被自动作为<code class="fe nf ng nh ni b">z.backward(torch.tensor(1.0))</code>传递。<code class="fe nf ng nh ni b">torch.tensor(1.0)</code>是为终止链式法则梯度乘法而提供的外部梯度。该外部梯度作为输入传递给<code class="fe nf ng nh ni b">MulBackward</code>函数，以进一步计算<strong class="kz ir"> <em class="nj"> x </em> </strong>的梯度。传入<code class="fe nf ng nh ni b">.backward()</code>的张量的维数必须与正在计算梯度的张量的维数相同。例如，如果梯度使能张量 x 和 y 如下:</p><p id="9c9a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><code class="fe nf ng nh ni b">x = torch.tensor([0.0, 2.0, 8.0], requires_grad = True)</code></p><p id="4be3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><code class="fe nf ng nh ni b">y = torch.tensor([5.0 , 1.0 , 7.0], requires_grad = True)</code></p><p id="b5ec" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">和<code class="fe nf ng nh ni b">z = x * y</code></p><p id="7476" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然后，为了计算<code class="fe nf ng nh ni b">z</code>(1×3 张量)相对于<code class="fe nf ng nh ni b">x</code>或<code class="fe nf ng nh ni b">y</code>的梯度，需要将外部梯度传递给<code class="fe nf ng nh ni b"> z.backward()</code>函数，如下所示:<code class="fe nf ng nh ni b">z.backward(torch.FloatTensor([1.0, 1.0, 1.0])</code></p><blockquote class="nm nn no"><p id="725e" class="kx ky nj kz b la lb jr lc ld le ju lf np lh li lj nq ll lm ln nr lp lq lr ls ij bi translated"><code class="fe nf ng nh ni b"><em class="iq">z.backward()</em></code> <em class="iq">会给一个</em> <code class="fe nf ng nh ni b"><em class="iq">RuntimeError: grad can be implicitly created only for scalar outputs</em></code></p></blockquote><p id="aad6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">传递给后向函数的张量的作用类似于梯度加权输出的权重。从数学上来说，这是向量乘以非标量张量的雅可比矩阵(在本文中进一步讨论)，因此它几乎总是与张量<code class="fe nf ng nh ni b">backward </code>维数相同的单位张量，除非需要计算加权输出。</p><blockquote class="nm nn no"><p id="70a7" class="kx ky nj kz b la lb jr lc ld le ju lf np lh li lj nq ll lm ln nr lp lq lr ls ij bi translated">tldr:反向图是由自动签名的类在正向传递过程中自动动态创建的。<code class="fe nf ng nh ni b">Backward()</code>简单地通过将参数传递给已经制作好的反向图来计算梯度。</p></blockquote><h1 id="e695" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">数学——雅可比和向量</h1><p id="a56a" class="pw-post-body-paragraph kx ky iq kz b la na jr lc ld nb ju lf lg nc li lj lk nd lm ln lo ne lq lr ls ij bi translated">从数学上来说，亲笔签名的类只是一个雅可比矢量积计算引擎。一个<a class="ae kw" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant" rel="noopener ugc nofollow" target="_blank">雅可比矩阵</a>用非常简单的话来说就是一个表示两个向量所有可能偏导数的矩阵。它是一个向量相对于另一个向量的梯度。</p><blockquote class="nm nn no"><p id="705e" class="kx ky nj kz b la lb jr lc ld le ju lf np lh li lj nq ll lm ln nr lp lq lr ls ij bi translated">注意:在这个过程中 PyTorch 从不显式地构造整个 Jacobian。直接计算 JVP(雅可比向量积)通常更简单、更有效。</p></blockquote><p id="fc32" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果一个向量<strong class="kz ir">X =【x1，x2，…。xn] </strong>用于计算其他一些向量<strong class="kz ir"> f(X) = [f1，f2，…。fn] </strong>通过函数<strong class="kz ir"> f </strong>然后雅可比矩阵(<strong class="kz ir"> J </strong>)简单地包含所有偏导数组合如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/608bdb288aa66e3ab16c142c8d9f1cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*dOApiHjKt1QtPfgf4ZBDZw.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Jacobian matrix (Source: Wikipedia)</figcaption></figure><p id="b7e0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">以上矩阵表示<strong class="kz ir"> f(X) </strong>相对于<strong class="kz ir"> X </strong>的梯度</p><p id="146c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">假设 PyTorch 梯度启用张量<strong class="kz ir"> X </strong>为:</p><p id="218c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> X = [x1，x2，…..xn] </strong>(假设这是某个机器学习模型的权重)</p><p id="f568" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> X </strong>经过一些运算形成一个矢量<strong class="kz ir"> Y </strong></p><p id="e57e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> Y = f(X) = [y1，y2，…。ym] </strong></p><p id="91ea" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> Y </strong>然后用于计算标量损失<strong class="kz ir"> <em class="nj"> l. </em> </strong>假设向量<strong class="kz ir"> <em class="nj"> v </em> </strong>恰好是标量损失<strong class="kz ir"> <em class="nj"> l </em> </strong>的梯度，关于向量<strong class="kz ir"> Y </strong>如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/59addf442e7b3bb1f9e5c098df744713.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*PLhFxIHfMUervwt-z8G4Iw.png"/></div></figure><p id="f2f3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="nj">向量 v 被称为</em> <code class="fe nf ng nh ni b"><em class="nj">grad_tensor</em></code> <em class="nj">并作为自变量传递给</em> <code class="fe nf ng nh ni b"><em class="nj">backward()</em></code> <em class="nj">函数</em></p><p id="3057" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了得到损失<strong class="kz ir"> <em class="nj"> l </em> </strong>相对于权重<strong class="kz ir"> X </strong>的梯度，雅可比矩阵<strong class="kz ir"> J </strong>被向量乘以向量<strong class="kz ir"> <em class="nj"> v </em> </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/8b8443d603325c8a6de604444007472d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bAOzjl1KncUmXURr2nFvBA.png"/></div></div></figure><p id="334e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这种计算雅可比矩阵并将其与矢量<strong class="kz ir"> <em class="nj"> v </em> </strong>相乘的方法使得 PyTorch 能够轻松提供外部梯度，即使是非标量输出。</p><h1 id="63ea" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">进一步阅读</h1><p id="acb8" class="pw-post-body-paragraph kx ky iq kz b la na jr lc ld nb ju lf lg nc li lj lk nd lm ln lo ne lq lr ls ij bi translated"><a class="ae kw" href="https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html" rel="noopener ugc nofollow" target="_blank">反向传播:快速修改</a></p><p id="86b2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae kw" href="https://pytorch.org/docs/stable/autograd.html" rel="noopener ugc nofollow" target="_blank"> PyTorch:自动分化包—torch . autogradated</a></p><p id="eabe" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae kw" href="https://github.com/pytorch/pytorch/tree/master/torch/autograd" rel="noopener ugc nofollow" target="_blank">亲笔签名的源代码</a></p><p id="059a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae kw" href="https://www.youtube.com/watch?v=MswxJw-8PvE" rel="noopener ugc nofollow" target="_blank">视频:PyTorch 亲笔签名讲解——深度教程</a>作者<a class="ae kw" href="https://www.youtube.com/channel/UCBCWYxhwuebyRTf4v3GkBlQ" rel="noopener ugc nofollow" target="_blank">埃利奥特·韦特</a></p><blockquote class="lt"><p id="5348" class="lu lv iq bd lw lx ly lz ma mb mc ls dk translated">感谢您的阅读！欢迎在回复中表达任何疑问。</p></blockquote></div></div>    
</body>
</html>