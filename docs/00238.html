<html>
<head>
<title>Understanding how to explain predictions with “explanation vectors”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解如何用“解释向量”解释预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-how-to-explain-predictions-with-explanation-vectors-15e31e32bfc3?source=collection_archive---------17-----------------------#2019-01-10">https://towardsdatascience.com/understanding-how-to-explain-predictions-with-explanation-vectors-15e31e32bfc3?source=collection_archive---------17-----------------------#2019-01-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="87ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://medium.com/@pferrandohernandez/interpretable-machine-learning-an-overview-10684eaa1fd7" rel="noopener">在最近的一篇文章</a>中，我介绍了三种现有的方法来解释<strong class="jp ir">任何</strong>机器学习模型的个体预测。在帖子聚焦于<a class="ae kl" rel="noopener" target="_blank" href="/understanding-how-lime-explains-predictions-d404e5d1829c"> LIME </a>和<a class="ae kl" rel="noopener" target="_blank" href="/understanding-how-ime-shapley-values-explains-predictions-d75c0fceca5a"> Shapley values </a>之后，现在轮到<strong class="jp ir">解释向量</strong>了，这是一种由 David Baehrens、Timon Schroeter 和 Stefan Harmeling 在 2010 年提出的方法。</p><p id="ae42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们在提到的帖子中看到的，解释黑盒模型的决策意味着理解是什么输入特征使模型对正在解释的观察给出预测。</p><p id="a1f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">直观地说，如果某个特征值的微小变化导致模型输出的较大变化，则该特征会对模型决策产生很大影响，而如果该变量的较大变化几乎不影响模型输出，则该特征对预测几乎没有影响。</p><p id="5e26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于模型是一个标量函数，其梯度指向模型输出增长率最大的方向，因此它可用作特征影响的度量。在分类任务中，如果<em class="km"> c </em>是一个实例的预测类，则在该实例处评估的条件概率 P(Y≠ c|X=x)的梯度指向数据点必须移动以改变其预测标签的方向，因此它提供了对模型决策中最有影响的特征的定性理解。同样，尽管 Baehrens 等人将回归任务的推广工作留给了未来，但使用回归函数的梯度似乎是很自然的，因为它指出了数据点的局部小变化将意味着模型输出的大变化的方向。</p><p id="295c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，解释可以被定义为<strong class="jp ir">梯度向量</strong>，其表征了数据点必须如何移动以改变其预测，这就是为什么它们被称为<em class="km">解释向量</em>的原因。</p><h2 id="4b9f" class="kn ko iq bd kp kq kr dn ks kt ku dp kv jy kw kx ky kc kz la lb kg lc ld le lf bi translated">分类方法</h2><p id="9fda" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy li ka kb kc lj ke kf kg lk ki kj kk ij bi translated">Baehrens 等人提出，通过类别的条件概率的感兴趣点处的梯度来解释任何分类器的预测，该类别不是给定其特征值的预测类别。因此，解释是一个向量(称为<em class="km">解释向量</em>)，其表征了数据点必须如何移动以改变实例的预测标签。</p><p id="8178" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个定义可以直接应用于<em class="km">概率</em>(或<em class="km">软</em>)<em class="km"/>分类器，它们的输出可以解释为概率，因为它们显式地估计了类别条件概率。然而，<em class="km">硬</em>分类器(例如，支持向量机)直接估计决策规则，即，它们分配预测的类别标签，而不产生概率估计。对于这种类型的分类器，该方法提出使用核密度估计器(也称为 Parzen-Rosenblatt 窗口)来估计类条件概率，因此我们可以通过概率分类器来近似硬分类器。</p><p id="df67" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">概率分类器</strong></p><p id="ef5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设 X=ℝᵖ为特征空间。设 X=(X₁,…,Xₚ)∈ℝᵖ为<em class="km"> p </em>连续随机变量的向量，y 为{1，…，k}中可能取值(类标)的离散随机变量。设 P(X，Y)为联合分布，大部分时间未知。</p><p id="37ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设<em class="km">f</em>:ℝᵖ→【0,1]ᵏ使得 f₁(x)+f₂(x)+…+fₖ(x)=1，∀ x∈X，是被解释的“概率”分类器。此外，我们假设<em class="km"> f </em>的所有分量对于所有类别<em class="km"> k </em>以及在整个输入空间上相对于 X 是一阶可微的。最后，让 y∈ℝᵖ成为被解释的观察。</p><p id="57d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">预测<em class="km"> f(x) </em>的每个分量<em class="km"> fᵢ </em>都是条件概率 P(Y=i|X=x)的估计。然而，“概率”分类器可以使用贝叶斯规则变成“硬”分类器，贝叶斯规则是 0-1 损失函数的最佳决策规则:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/ed54731f6258c6e7f39139b888a39419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s5k3yjM1XB1EowmlBLYzyg.png"/></div></div></figure><p id="dc2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，{1- <em class="km"> fᵢ </em> (x)}是对 P(Y≠i|X=x)的估计。也就是说，预测的类别是具有最高概率的类别。</p><p id="d7db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据点 y∈ℝᵖ的<strong class="jp ir">解释向量</strong>定义为 y 的条件概率在 x=y 处的梯度不是给定 X=x 的预测类:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lx"><img src="../Images/a8640a7b45ce46867e27dad73ced7af0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rkPTXeasL8idMLT_lywbIQ.png"/></div></div></figure><p id="3697" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，解释向量ζ(y)是一个<em class="km"> p </em>维向量，其指向数据点必须被移动以改变其预测类别的方向。组件的正(负)符号意味着增加相应的特征将降低(提高)将<em class="km"> y </em>分配给{y}的概率。此外，分量的绝对值越大，该特征在类别标签预测中的影响越大。</p><p id="d3d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">解释向量的一个问题是它们可能变成零向量。如果这是因为局部最大值或最小值而发生的，我们可以从 Hessian 的特征向量中学习与模型决策相关的特征，即使我们不会获得它们的方向。然而，如果分类器结果是在<em class="km"> y </em>的某个邻域中平坦的概率分布，则不能获得有意义的解释。总之，解释向量方法非常适合输出类函数不完全平坦的概率的分类器。</p><p id="42ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在二元分类的情况下，Baehrens 等人将<em class="km">解释向量</em>定义为正类的学习模型的概率函数 P(Y = 1 | X = x)的局部梯度。形式上，数据点 y∈ℝᵖ的<em class="km">解释向量</em>是:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ly"><img src="../Images/cdb78162a2558b83d76ff0a53bf3cfaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xK2E709vpN7sOhDuhbtH_Q.png"/></div></div></figure><p id="f646" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="km"> f </em> : ℝᵖ→ [0，1]是一个二元分类器。</p><p id="c275" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意<em class="km"> f(x) </em>是对 P(Y = 1 | X = x)的估计。因此，解释向量指向从数据点到类别 1 的更高概率的最陡上升的方向。此外，每个分量的符号指示当<em class="km"> y </em>的相应特征局部增加时，预测为 1 的概率是增加还是减少。最后，请注意，当预测类别为 1 时，解释向量的一般定义将不同于二元分类的定义，在这种情况下，负版本-ζ(y)可能特别有用，因为它指示如何移动要分配给类别 0 的数据点。</p><p id="5d10" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图显示了如何应用解释向量来解释二元分类器的预测的示例:我们已标记了用于训练模型(在这种情况下，是高斯过程分类器)的训练数据(面板(a))，该模型为特征空间的每个数据点(面板(b))分配了正类中的概率。然后，我们计算我们想要解释的数据点的解释向量，以便理解是什么特征使得模型给出其预测。例如，在图(c)和(d)中，我们可以看到沿着假设和由数据产生的三角形的角的解释向量具有两个不同于零的分量，而沿着边的解释向量只有一个非零分量。因此，两个解释变量都会影响模型对沿假设边界和三角形拐角处观察值的决策，而只有一个特征与沿边观察值的模型预测相关。此外，解释向量的长度(面板(c))表示相关特征的重要程度，因此我们可以比较观察结果之间的解释。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lz"><img src="../Images/2321344ace1c05617349270183806945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIBsvzFVWgfsZOyWeQtGOw.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Example of how explanation vectors are applied to model predictions learned by a Gaussian Process Classifier (GPC), which provides probabilistic predictions. Panel (a) shows the training points and their labels (class +1 in red, class -1 in blue). Panel (b) shows the trained model, which assigns a probability of being in the positive class to every data point. Panels (c) and (d) show the explanation vectors and the direction of the explanation vectors, respectively, together with the contour map of the model. Source: <a class="ae kl" href="http://www.jmlr.org/papers/volume11/baehrens10a/baehrens10a.pdf" rel="noopener ugc nofollow" target="_blank">http://www.jmlr.org/papers/volume11/baehrens10a/baehrens10a.pdf</a></figcaption></figure><p id="8d51" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">硬分类器</strong></p><p id="752a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设 X=ℝᵖ为特征空间。设 X=(X₁,…,Xₚ)∈ℝᵖ为<em class="km"> p </em>个连续随机变量的向量，y 为{1，…，k}中可能取值(类标)的离散随机变量。设 P(X，Y)为联合分布，未知。</p><p id="e3bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设<em class="km"> f </em> : X=ℝᵖ → {1，…，k}为被解释的“硬”分类器。最后，让 y∈ℝᵖ成为被解释的观察。</p><p id="4745" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设 x、…、xⁿ∈ℝᵖ为训练点，分别贴上标签。设 Iᵢ = {j∈{1，…，n} | f(xʲ) = i}为预测标号为<em class="km"> i </em>的训练集的指标集。</p><p id="6b0f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每个类别<em class="km"> i </em>，条件概率 P(Y=i|X=x)可以通过以下核密度估计量的商来近似:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi me"><img src="../Images/6d5a2814d0dace4fc8346a88cc0ae938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9YTbHsj9HyjXpHGSX41bOg.png"/></div></div></figure><p id="4e1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们可以定义一个分类器，其组件是 P(Y=i|X=x)的估计值:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mf"><img src="../Images/df30bd41da07634f401879c605e9609e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7MU0bZNwF7KKcZ7uXtpyaA.png"/></div></div></figure><p id="93bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，我们可以使用经过训练的分类器<em class="km"> f </em>来生成尽可能多的标记数据，以构建这个近似(概率)分类器。现在我们有了一个概率分类器(它是我们最初的硬分类器的近似)，我们可以使用贝叶斯规则来获得预测的类，就像我们在上一节中所做的那样:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mg"><img src="../Images/f998f3aa19b9b4db78b59bff333f9a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3zT2_bf07w1zRcO1NESKPA.png"/></div></div></figure><p id="4cbd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们可以定义数据点 y∈ℝᵖ的<em class="km">估计解释向量</em>如下:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mh"><img src="../Images/d4f3f4c5a6185d122f8d3e93a99131a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gK2edhZBGk4ikSSgNhma-Q.png"/></div></div></figure><p id="7dc4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="km"> f </em> : ℝᵖ →{1，…，k}是我们的“硬”分类器。</p><p id="3b88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，使用实际预测类别<em class="km"> f(y) </em>来代替近似概率分类器的预测类别。这样做是为了确保估计的解释向量指向必须移动观察的方向，以改变由<em class="km"> f </em>预测的<em class="km">实际</em>标签，而不是由近似分类器(可能不同)分配的标签。</p><p id="4908" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，应当选择单个超参数σ，使得“新的”预测类别标签(即，由近似概率分类器最终分配的类别)尽可能地类似于由原始硬分类器<em class="km"> f </em>在测试集上做出的实际预测。形式上，如果 z，…，zᵐ∈ℝᵖ是测试数据点，f(z)，… ,f(zᵐ)是分类器预测的它们各自的标签。那么，σ的选择值为:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mi"><img src="../Images/c398b86972576b695710615bac6aaece.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XqU7v0caglWO2IPpdfe63Q.png"/></div></div></figure></div></div>    
</body>
</html>