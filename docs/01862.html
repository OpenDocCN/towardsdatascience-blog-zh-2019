<html>
<head>
<title>Quantum machine learning: distance estimation for k-means clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">量子机器学习:k 均值聚类的距离估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quantum-machine-learning-distance-estimation-for-k-means-clustering-26bccfbfcc76?source=collection_archive---------13-----------------------#2019-03-27">https://towardsdatascience.com/quantum-machine-learning-distance-estimation-for-k-means-clustering-26bccfbfcc76?source=collection_archive---------13-----------------------#2019-03-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="2145" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">几周前，我被邀请在我们学校的年度科技节上演示量子计算的一些应用。</p><p id="d261" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我选择专注于量子机器学习，在我的研究过程中，我发现了一些我认为超级酷的东西:一种完全不同的思考方式，即我们估计点之间距离的方式，这可以使量子 k-means 聚类算法比经典算法快得多。</p><p id="d452" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，在我们深入研究之前，让我们先快速回顾一下经典的 k-means 聚类算法。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h2 id="e72d" class="kv kw it bd kx ky kz dn la lb lc dp ld kb le lf lg kf lh li lj kj lk ll lm ln bi translated"><strong class="ak">(经典)k 均值聚类到底是什么意思？</strong></h2><p id="1fdc" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">K-means 聚类是一种分类算法，这意味着当我们向它提供数据时，它会尝试将每个数据点与其他一些具有类似属性的点放入一个组或类中。它也是一种无监督的学习算法，所以我们不需要使用我们的一些数据集来训练模型，或者帮助它学习数据的特征。</p><p id="7852" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们有一个包含汽车型号、发动机尺寸和离地间隙列表的小型数据集，并将其输入到 k-means 算法中，输入将是这样的:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/36f67f4c56bda34adb2c6ad222e186e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KbEkjZz60zWnr-vhBgu1Lg.jpeg"/></div></div></figure><p id="8ceb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的输出可能如下所示:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mf"><img src="../Images/5dcb447faebabee0fe8f26492f54f98f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rgEdQQS5MVZvLinV5Go7Ew.jpeg"/></div></div></figure><p id="8ec2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是算法是如何做到这一点的呢？</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h2 id="d6be" class="kv kw it bd kx ky kz dn la lb lc dp ld kb le lf lg kf lh li lj kj lk ll lm ln bi translated">那么它是如何工作的呢？</h2><p id="2c64" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">虽然这听起来非常复杂，但是 k-means 聚类背后的思想实际上非常简单。它只是把你要求它分类的每一个新数据点放入它最接近的组中。</p><p id="4e42" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将这一点形象化会更容易理解——红点会进入哪个星团？</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/236b5a33ea696295285b4c3ae0d7f03e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*s1-6OqanCHgVM5U_-qtk7w.png"/></div></figure><p id="f99d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看起来红点最接近蓝点，所以 k-means 聚类算法会将其放入蓝色聚类中。</p><p id="c580" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">不过，这又引出了另一个问题:判断图上两点之间的距离对我们来说很容易，但我们如何让计算机做同样的事情呢？</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h2 id="fb24" class="kv kw it bd kx ky kz dn la lb lc dp ld kb le lf lg kf lh li lj kj lk ll lm ln bi translated">比较距离</h2><p id="2f23" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">由于每个点都被表示为一组数字——每个数字都指定了该点在一个平面中的位置——计算机可以使用点之间的坐标差来找到点之间的距离。</p><p id="4537" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以把每一个聚类表示为一个位于该组“平均”位置的点。</p><p id="211e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们如何找到平均值？我们可以在一个维度上将属于一个簇的所有点的坐标相加，然后除以该组中的点数。如果我们对每个维度都这样做，那么这些坐标所代表的点就可以用作聚类的平均值或质心。如果我们用上面的例子来做这件事:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/da8d696ec4f31dc5acbabee70d361009.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*I-CkuYoXmmK4Qi490bXMVA.png"/></div></figure><p id="9d07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">彩色的 x 标记了每个簇的质心。</p><p id="b74d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，如果我们取代表红点的坐标和群质心之间的差，我们可以证实我们最初的猜测，红点属于蓝色星团:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/22c2579db912586dcbe6ff8916c9935b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*wI0RL5QLep7wNArxRomMNQ.png"/></div></figure><p id="d0d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">右下角的文本给出了从红点到每个簇的质心的欧几里德距离。正如我们所看到的，到蓝色质心的距离是最小的，这意味着红点确实会被归类到蓝色星团中。</p><p id="3a8d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们如何找到距离？我们用质心和红点的坐标作为向量。</p><p id="4d9a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，如果我们想计算到蓝色质心的距离，我们可以从蓝色质心的位置向量中减去红点的位置向量:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mh"><img src="../Images/35e899f4716fb871c0b7ae2fb7cee5a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JgWSdP80Cy0FP8Of"/></div></div></figure><p id="6675" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要计算距离，我们需要计算矢量的大小:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/b95310ae4fad472de7fcf9dd2f28a891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/0*VdP_CNZ28OfFduuu"/></div></figure><p id="5d55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">k-means 聚类算法会找到新点与每个质心之间的距离，然后将新点放入质心最近的聚类中。</p><p id="b58d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">超级简单吧？</p><p id="6edc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要点是，找到从一个新数据点到每个质心的距离是关键——之后的所有事情，如比较距离，并将新点分配给一个组，都容易得多。</p><p id="6d50" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">既然我们已经得到了经典的版本，让我们把事情推进到量子宇宙。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h2 id="7582" class="kv kw it bd kx ky kz dn la lb lc dp ld kb le lf lg kf lh li lj kj lk ll lm ln bi translated">距离估计</h2><p id="4711" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">我们知道，我们的算法需要测量到不同质心的距离:但我们如何将使用向量减法测量距离的想法转化为我们可以在量子计算机上轻松有效地执行的事情？</p><p id="6550" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有了经典的量子计算机，我们可以很容易地计算出欧几里得距离，但是试图在量子计算机上这样做要困难得多——而且需要的量子比特比我们能提供的还要多。其原因是，由于量子位的概率性质，相位差和概率振幅很容易测量，但像两个向量之间的距离这样的事情却不容易测量。</p><p id="64a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们只需要距离，这样我们就可以将数据点分配到不同的组，我们需要知道哪个聚类离数据点最近。如果我们有另一个更容易测量的参数，可以让我们计算出哪个质心最接近呢？</p><p id="fe78" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个参数不一定要和距离成正比，只要和它正相关就行。这是可行的，因为我们只需要最近的质心，而不需要到每个质心的精确距离，我们可以将新的数据点放在与我们的参数取的最小值相关联的群集中。</p><p id="bb90" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">幸运的是，当我们处理量子位时，有许多类似距离的参数可用，如两个(归一化)向量之间的内积，以及测量处于<code class="fe mj mk ml mm b">|0&gt;</code>或<code class="fe mj mk ml mm b">|1&gt;</code>状态的量子位的概率。</p><p id="1c08" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是我们如何使用两个向量之间的内积来帮助我们估计距离呢？</p><p id="552c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要做一点数学计算。如果你不喜欢，可以直接跳到下一节的结尾——你真正需要知道的是我们最终得出的可测量变量之间的关系。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h2 id="82b2" class="kv kw it bd kx ky kz dn la lb lc dp ld kb le lf lg kf lh li lj kj lk ll lm ln bi translated">关联内积和距离</h2><p id="3bcc" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">让我们从一个零初始化的辅助量子位和一个量子态ψ开始，它存储了我们想要估计的归一化向量:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/f070c580817355730033bc0edfb8dc18.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/0*WJehSCFjDHQuS0-9"/></div></figure><p id="8014" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以假设<code class="fe mj mk ml mm b">|x&gt;</code>是我们想要分类的新数据点的位置向量，而<code class="fe mj mk ml mm b">|y&gt;</code>是我们数据中一个聚类的质心。</p><p id="3357" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，我们对辅助量子位应用一个阿达玛门，把它推到叠加态:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/7276550e6e688cebc8ee7b5319b58507.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/0*sCaqvMSzar1clkHL"/></div></figure><p id="59d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了使状态ψ与我们的辅助量子位纠缠在一起，我们对辅助量子位上控制的ψ应用一个交换门:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/4bf7f62746f008717e772a28d09c71ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/0*mIGMsMm4bJ60MZ9c"/></div></figure><p id="dbb7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里的<code class="fe mj mk ml mm b"><em class="mq">F</em>(|Ψ&gt;) </code>只是一种更简单的方式来展示<code class="fe mj mk ml mm b">|Ψ&gt;</code>的交换版本，这是交换门应用于<code class="fe mj mk ml mm b">|Ψ&gt;</code>的结果。</p><p id="108e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们将另一个哈达玛门应用于辅助量子位:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/f69aa3453414933c5c06db840c314fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/0*POQcnbqC9zp5oOba"/></div></figure><p id="4d99" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里<code class="fe mj mk ml mm b"><em class="mq">I</em></code>代表恒等运算符— <code class="fe mj mk ml mm b"><em class="mq">I</em>(|Ψ&gt;)</code>简单来说就是给出<code class="fe mj mk ml mm b">|Ψ&gt;</code>。</p><p id="c5a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来的几个步骤可能看起来有点随机，但我们真正要做的是将我们现在拥有的等式转化为一种形式，它包含两个向量<code class="fe mj mk ml mm b">|x&gt;</code>和<code class="fe mj mk ml mm b">|y&gt;</code>的内积。</p><p id="f209" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们替换<code class="fe mj mk ml mm b">(<em class="mq">I — F</em>)</code>和<code class="fe mj mk ml mm b">(<em class="mq">I</em>+<em class="mq">F</em>)</code>，我们得到这个:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/2aa2e057b4d39159814c1d0896a04f39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/0*pmVp8-0cngnVnliC"/></div></figure><p id="0825" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以用一个叫做期望值的超级有用的概念来进一步简化这个问题(你可以在这里阅读这个概念)。这个想法是，我们可以将辅助量子位在<code class="fe mj mk ml mm b">|0&gt;</code>状态下被测量的概率表示为内积:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/e1c626243961c80444811141d902f39d.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/0*TZ5P1_w2TeoMDE8O"/></div></figure><p id="58d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们把<code class="fe mj mk ml mm b"><em class="mq">I</em></code> <em class="mq"> </em>和<code class="fe mj mk ml mm b"><em class="mq">F</em></code> <em class="mq"> </em>重新代入:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/072a0be114ff5f80372d167c49674684.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/0*YaFga2BE5Y2eO7GK"/></div></figure><p id="a05e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们展开内积，我们得到这个:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/8ec490a45873593f2cb3d5276f49c870.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/0*cvmgp957R9uksto9"/></div></figure><p id="d5a6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">自<code class="fe mj mk ml mm b">&lt;xy , xy&gt; = 1</code>和<code class="fe mj mk ml mm b">&lt;xy , yx&gt; = (&lt;x , y&gt;)^2</code>开始:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/9ee1c21ab2920b9bd96024cb91da3bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/0*_CAh4936KJE4jegU"/></div></figure><p id="7d91" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">太棒了——所以现在我们有了在<code class="fe mj mk ml mm b">|1&gt;</code>状态下测量辅助量子位的概率，这是我们很容易测量的，和我们想要找到两个向量之间距离的内积之间的关系。</p><p id="31e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在需要做的就是证明等式的右边与两个向量<code class="fe mj mk ml mm b">|x&gt;</code>和<code class="fe mj mk ml mm b">|y&gt;</code>之间的欧几里德距离正相关。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi my"><img src="../Images/250c74e372008ff3e06f5949d8cb1e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/0*Bvc_2dYbhIaeP9Gx"/></div></figure><p id="60fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们扩展矢量减法:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ca84f5773abb4679d8a545b256cc592d.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/0*Boogsp-R4s_EgZbI"/></div></figure><p id="f8dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为两个向量都是归一化的，所以它们的幅度是 1:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8059d5c93b455a345c9e9c233c70de20.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/0*C6Gt35fGhas2L-26"/></div></figure><p id="4d57" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们看看我们之前推导出的等式，它们看起来非常相似:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/9ee1c21ab2920b9bd96024cb91da3bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/0*_CAh4936KJE4jegU"/></div></figure><p id="7eb7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">酷！看起来我们有了一个容易测量的参数，可以用来代替测量距离。现在让我们使用 qiskit 实现我们的距离估计算法。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h2 id="205f" class="kv kw it bd kx ky kz dn la lb lc dp ld kb le lf lg kf lh li lj kj lk ll lm ln bi translated">从盒子到球体</h2><p id="ff6c" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">我们如何将我们想要估计的点之间距离的矢量坐标编码成量子位？</p><p id="7609" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们首先绘制几个类，它们的质心，以及我们想要分类的一个新点:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/cd1441f585e9c8c95ddce26807dcea14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-nZwHhsKpEP3AHtiCso5rQ.png"/></div></figure><p id="bb70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">红点是我们要分类的新数据点，彩色的 x 标记每个类的质心。</p><p id="da04" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要找到一种方法，将我们目前用来描述质心的直角坐标转换成球面坐标，这样我们就可以将图上的每个数据点表示为布洛赫球面上的另一个点(查看<a class="ae mt" href="http://akyrillidis.github.io/notes/quant_post_7" rel="noopener ugc nofollow" target="_blank">这篇</a>文章，了解量子位表示的超级平滑介绍):</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/362c327f62a90bc9ad4d42d8ed2c0af6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*DXQ6aIXTG0DWcCZH.png"/></div></figure><p id="0b5b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要转换数据点，我们需要执行以下步骤:</p><ul class=""><li id="565f" class="nc nd it js b jt ju jx jy kb ne kf nf kj ng kn nh ni nj nk bi translated">取一个零初始化的量子位，对其应用哈达玛门，移动代表量子位的向量，使其沿着 x 轴。</li><li id="1def" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated">根据对应于特征 1 的数据点的值设置φ。</li><li id="66c7" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated">根据对应于特征 2 的数据点的值设置θ。</li></ul><p id="e42a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">θ的范围可以从<code class="fe mj mk ml mm b">0</code>到<code class="fe mj mk ml mm b">π</code>弧度，φ的范围可以从<code class="fe mj mk ml mm b">0</code>到<code class="fe mj mk ml mm b">π</code>弧度。</p><p id="b7a2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为什么我们需要限制φ可以取值的范围？</p><p id="bb85" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设我们有两个数据点，当用一个量子位表示时，它们取相同的θ值和φ值<code class="fe mj mk ml mm b">0</code>和<code class="fe mj mk ml mm b">2π</code>。量子位将占据布洛赫球上的同一个点，即使它们代表的点具有不同的特征值，这将搞乱我们的算法——它只在布洛赫球上量子位之间的相位差与两个数据点之间的实际欧几里得距离大致成比例时才起作用。</p><p id="961a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用以下等式，我们可以轻松地将数据特征值映射到θ和φ值:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/64c2dde50d19c5d4a1e5fa3c1b5465ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/0*WYWyJkLSuCLwvrD8"/></div></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1baf8486cf65599b55a549793475f9ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/0*0OjPa8wPNMoxScnu"/></div></figure><p id="6fb9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里<code class="fe mj mk ml mm b">d_0</code>和<code class="fe mj mk ml mm b">d_1</code>代表数据特征 1 和 2 的值。如果这些没有意义，回到我们数据的散点图，看看每个特征的范围(-1 到 1)，以及θ和φ。</p><p id="d092" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">太好了！</p><p id="761a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们找到了一种方法，从将我们的数据绘制为图上的点，到使用量子位来表示它们——现在让我们把手弄脏，看看我们的算法是否真的有效。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h2 id="7449" class="kv kw it bd kx ky kz dn la lb lc dp ld kb le lf lg kf lh li lj kj lk ll lm ln bi translated">构建距离估计器</h2><p id="4218" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">你可以在这里得到我使用的数据。</p><p id="e417" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们已经看到了上面的数据图，以及我们需要分类的红点。为了方便起见，让我们再次绘制我们的数据，但是这里只显示了每个类的质心:</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="a5da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们得到了一个简洁、超级简单的图表:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/2f37086082889203f04ffed309605811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*_g4_-Y387Jf2oRdHCIEpXQ.png"/></div></figure><p id="8a2f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要比较从红点到每个质心的距离，所以我们必须将质心和红点的坐标编码到我们的量子位中。为此，我们使用上一节中提出的规则:</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="6abf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们从 qiskit 导入我们需要的所有模块，并创建寄存器来保存我们的量子位和结果:</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="a1d2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以使用 qiskit 提供的<strong class="js iu"> <em class="mq"> U3 </em> </strong>门<strong class="js iu"> <em class="mq"> </em> </strong>来实现我们需要执行的旋转，以对我们的数据点进行编码。<strong class="js iu"> <em class="mq"> U3 </em> </strong>门可以这样应用:</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="dfc0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这条指令将导致量子位从正 z 轴向外移动θ弧度，从正 x 轴向外移动φ弧度，就像我们之前看到的布洛赫球一样。</p><p id="d808" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们实现我们在连接辅助量子位的概率和两个向量之间的距离的部分中走过的过程:</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="91b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以检查新数据点属于哪个类。让我们也打印欧几里得距离，以了解到每个质心的距离之间的差异:</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="7446" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我运行了这个程序很多次，得到了这些结果:</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="b3d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它看起来很有效——超级酷！</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h2 id="079a" class="kv kw it bd kx ky kz dn la lb lc dp ld kb le lf lg kf lh li lj kj lk ll lm ln bi translated">包扎</h2><p id="a73d" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">厉害！</p><p id="ccd4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们构建了一个量子距离估计器，与 k-means 聚类算法一起使用，我认为这很棒。你可以在这里获得整个程序<a class="ae mt" href="https://github.com/SashwatAnagolum/DoNew/blob/master/kmeans/kmeans.py" rel="noopener ugc nofollow" target="_blank"/>—<strong class="js iu"/>随意在其他数据集上运行它，感受一下它是如何工作的。</p><p id="b8e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章只是我将发布的一个新系列的开始，涵盖了量子机器学习的不同方面——敬请关注！</p></div></div>    
</body>
</html>