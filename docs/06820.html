<html>
<head>
<title>Fuel Up the Deep Learning: Custom Dataset Creation with Web Scraping</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为深度学习加油:通过网络抓取创建自定义数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fuel-up-the-deep-learning-custom-dataset-creation-with-web-scraping-ba0f44414cf7?source=collection_archive---------12-----------------------#2019-09-28">https://towardsdatascience.com/fuel-up-the-deep-learning-custom-dataset-creation-with-web-scraping-ba0f44414cf7?source=collection_archive---------12-----------------------#2019-09-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/01b61dbfab9aa437d1950b1fbe66001e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*P4XxwIhUiCf9PFVF"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@loveleighmiles?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dev Leigh</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="79fa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我完全理解了深度学习的一件事。它需要大量的数据才能工作。没有数据，它只是一个高级算法。它不能单独产生任何有意义的输出。</p><p id="6a54" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当你从零开始开发深度学习模型时，这一点更加明显。</p><p id="db16" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上下文中，<a class="ae kf" href="https://opensource.google.com/projects/open-images-dataset" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu">开放图像数据集</strong> </a> <strong class="ki iu"> </strong>包含 900 万张已经用图像级标签和对象边界框进行了注释的图像。准备如此庞大的数据集需要付出巨大的努力。</p><p id="49b4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好消息是，你可以根据你的需要用相对较少的数据量<a class="ae kf" rel="noopener" target="_blank" href="/transfer-learning-from-pre-trained-models-f2393f124751"> <strong class="ki iu">微调一个预训练的模型</strong> </a>。</p><p id="abce" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在接下来的部分中，我将向您展示如何使用<strong class="ki iu"> Scrapy </strong>和<strong class="ki iu"> Python </strong>创建这样一个数据集。希望这能帮助你开始你的第一个机器学习项目。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/d77a198a83af66da8330241634698b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nfwOaN7rnSOsz9LrlTbPLw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@jontyson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jon Tyson</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2afc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设您想要开发一个机器学习模型，在提供的图像中检测衣服、鞋子、包和配件。您可能还想开发一个图像相似性模型，根据给定的输入图像为您提供相似衣服的图像。像购物一样，Pinterest、谷歌和许多其他公司提供的 look 功能。</p><p id="3a3d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这两种情况，您都需要大量与领域相关的图像和数据。网络抓取是一个工具，你可以用来克服这个困难。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lk"><img src="../Images/a8dab58030ec211c5863bd0d3103a223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*28ySaQD_LLhq_NpV"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@nordwood?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">NordWood Themes</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7f0c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我以一种结构化的方式来解释 web 报废流程，从而一步一步地指导您:</p><h1 id="d2f2" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">第一步:了解礼貌刮擦的规则</h1><p id="a375" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">在网络抓取中，你必须遵守几个规则，以免损害网站报废。在考虑网络抓取之前，请花时间去理解这些规则。这里有一个<a class="ae kf" href="https://blog.scrapinghub.com/2016/08/25/how-to-crawl-the-web-politely-with-scrapy" rel="noopener ugc nofollow" target="_blank">网站</a>用具体的例子来解释它们。</p><h1 id="4ff2" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">第二步:决定刮哪个网站</h1><p id="1ae3" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">这似乎是显而易见的，但这是第一步，其余的很大程度上取决于这一点。在投入时间和精力之前，想想你的兴趣是什么。这将有助于你缩小关注网站的选择范围。</p><p id="340c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我的情况下，由于我计划开发时尚的深度学习模型，我将寻找在线购物网站。</p><p id="1a4b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的数据来源将是服装购物网站，因为我们的领域与时尚有关。所以我决定刮一下<strong class="ki iu"> 'www2.hm.com/tr_tr '，</strong>来演示一下整个刮的过程。</p><p id="bb46" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你在另一个领域有一个特定的网站，你仍然可以按照这个例子来理解你可以很容易地应用到你的案例中的基础知识。</p><h1 id="dd80" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">第三步:了解网站的结构</h1><p id="8340" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">了解产品如何在<strong class="ki iu"> 'www2.hm.com/tr_tr' </strong>中列出和分页以加快抓取过程非常重要。在我们的例子中，产品分为女性、男性和儿童。网站上有一些页面，你可以在那里看到特定主要类别的所有产品。因此，我们不必搜索每个子类别，如男士衬衫、男鞋等。</p><p id="1611" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你查看那些页面的 Url，你可以看到使用了两个参数:<strong class="ki iu"> <em class="lj">【偏移量】</em> </strong>和<strong class="ki iu"> <em class="lj">【页面大小】。</em> </strong>我们将使用 offset 值来浏览产品列表，并使用 page-size 值来定义一页上要列出多少产品。更高的<strong class="ki iu">页面大小</strong>值需要更少的 Http 请求，因此减少了抓取时间。</p><blockquote class="mo mp mq"><p id="cd6c" class="kg kh lj ki b kj kk kl km kn ko kp kq mr ks kt ku ms kw kx ky mt la lb lc ld im bi translated"><a class="ae kf" href="https://www2.hm.com/tr_tr/erkek/urune-gore-satin-al/view-all.html?sort=stock&amp;image-size=small&amp;image=model&amp;offset=0&amp;page-size=72" rel="noopener ugc nofollow" target="_blank">https://www2 . hm . com/tr _ tr/erkek/u rune-gore-satin-al/view-all . html？sort = stock&amp;image-size = small&amp;image = model&amp;offset = 0&amp;page-size = 72</a></p></blockquote><h1 id="048a" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">第四步:理解 HTML 中的模式</h1><p id="445a" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">此时，我们应该检查列出了所有商品及其图片、产品名称和价格信息的 Html。</p><p id="bb0c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们理解了网站上使用的 Html 和 CSS 模式，那么我们就可以很容易地找到我们想要抓取的部分。</p><p id="b0b7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的例子中，您可以在下面的图片中看到，所有的条目都包含在具有<strong class="ki iu">“products-listing small”</strong>类名的<ul> Html 元素中。嵌套在那个&lt; ul &gt;中的每个&lt; li &gt; Html 元素，涵盖了与单个产品相关的细节。</ul></p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mu"><img src="../Images/02ca2b9f3cfa53f2f33d9b2b0126add1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJ3cQDiYVbEy4Ht_2eHXFw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Inspecting HTML patterns for Web Scrapping</figcaption></figure><p id="a5e0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们进入具有“product-item”类名的单个<li>元素的细节时，我们可以看到产品图片链接、产品名称和产品价格信息位于何处。您可以看到页面上显示的每个产品都使用了相同的 Html 模式。</li></p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/f3c4403f39a662ce280fc38f08fb1bfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TOAnSWrHFgY0lbTTM3swbg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Inspecting HTML patterns for Web Scrapping</figcaption></figure><h1 id="12d1" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">步骤 5:构建并运行 Scrapy Spider</h1><p id="ec84" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">假设你的电脑上已经安装并运行了 Python，让我们从安装<a class="ae kf" href="https://scrapy.org" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> Scrapy </strong> </a>开始。</p><p id="a800" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用您的 shell 终端安装 Scrapy 并生成您的 Scrapy 项目。</p><p id="7f29" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还需要安装<a class="ae kf" href="https://pypi.org/project/image/" rel="noopener ugc nofollow" target="_blank">图像库</a>。这是 Scrapy 图像管道所要求的，我们将使用它来下载产品图像。</p><pre class="lf lg lh li gt mw mx my mz aw na bi"><span id="7513" class="nb lm it mx b gy nc nd l ne nf">$ pip install scrapy<br/>$ pip install image<br/>$ scrapy startproject <strong class="mx iu">fashionWebScraping</strong><br/>$ cd <strong class="mx iu">fashionWebScraping</strong><br/>$ ls</span></pre><p id="7119" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">完成后，您将看到 scrapy.cfg、items.py、pipelines.py、middlewares.py 和 settings.py 文件被填充到文件夹中。一个空的<strong class="ki iu">‘蜘蛛’</strong>文件夹也出现在项目主文件夹中，用于存储我们的蜘蛛 python 文件。</p><p id="af5b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要修改<strong class="ki iu"> settings.py </strong>，<strong class="ki iu"> items.py </strong>并构建一个蜘蛛 python 文件开始抓取。</p><p id="bc5b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">建立 Scrapy 项目</strong></p><p id="7ebe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了设置项目，我将修改<strong class="ki iu"><em class="lj">setting . py</em></strong>by<strong class="ki iu"><em class="lj"/></strong>包括几个行项目来定义图像管道。然后我会改一些基本设置礼貌刮。可以参考<a class="ae kf" href="https://doc.scrapy.org/en/latest/topics/settings.html" rel="noopener ugc nofollow" target="_blank"> Scrapy 网站</a>了解更多设置。</p><pre class="lf lg lh li gt mw mx my mz aw na bi"><span id="854e" class="nb lm it mx b gy nc nd l ne nf">BOT_NAME = 'fashionWebScraping'</span><span id="0bbd" class="nb lm it mx b gy ng nd l ne nf">SPIDER_MODULES = ['fashionWebScraping.spiders']<br/>NEWSPIDER_MODULE = 'fashionWebScraping.spiders'</span><span id="b330" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu"># Crawl responsibly by identifying yourself <br/># (and your website) on the user-agent</strong><br/>USER_AGENT = 'erdemisbilen@gmail.com'</span><span id="9b6e" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu"># Obey robots.txt rules</strong><br/>ROBOTSTXT_OBEY = True</span><span id="dc7d" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu"># This to avoid hitting servers too hard</strong><br/>DOWNLOAD_DELAY = 5</span><span id="9328" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu"># Configure item pipelines</strong><br/>ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}<br/>IMAGES_STORE = '/Angular/fashionWebScraping/images_scraped'</span></pre><p id="6b7f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">用<em class="lj"> items.py </em> </strong>定义项目</p><blockquote class="mo mp mq"><p id="bde0" class="kg kh lj ki b kj kk kl km kn ko kp kq mr ks kt ku ms kw kx ky mt la lb lc ld im bi translated">为了定义通用的输出数据格式，Scrapy 提供了<code class="fe nh ni nj mx b"><a class="ae kf" href="https://doc.scrapy.org/en/1.0/topics/items.html#scrapy.item.Item" rel="noopener ugc nofollow" target="_blank"><strong class="ki iu"><em class="it">Item</em></strong></a></code>类。<code class="fe nh ni nj mx b"><a class="ae kf" href="https://doc.scrapy.org/en/1.0/topics/items.html#scrapy.item.Item" rel="noopener ugc nofollow" target="_blank"><strong class="ki iu"><em class="it">Item</em></strong></a></code>对象是用来收集抓取数据的简单容器。</p><p id="cb68" class="kg kh lj ki b kj kk kl km kn ko kp kq mr ks kt ku ms kw kx ky mt la lb lc ld im bi translated"><strong class="ki iu">出自</strong><a class="ae kf" href="https://docs.scrapy.org/en/latest/topics/items.html" rel="noopener ugc nofollow" target="_blank"><strong class="ki iu">scrapy.org</strong></a></p></blockquote><p id="ad6b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们在<strong class="ki iu"> <em class="lj"> items.py </em> </strong>中定义我们的项目:</p><pre class="lf lg lh li gt mw mx my mz aw na bi"><span id="fe6d" class="nb lm it mx b gy nc nd l ne nf">import scrapy</span><span id="ac26" class="nb lm it mx b gy ng nd l ne nf">class FashionwebscrapingItem(scrapy.Item):</span><span id="8f2a" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu">#items to store product details</strong><br/>  gender=Field()<br/>  productId=Field()<br/>  productName=Field()<br/>  priceOriginal=Field()<br/>  priceSale=Field()</span><span id="fb96" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu">#items to store links</strong><br/>  imageLink = Field()<br/>  productLink=Field()</span><span id="f602" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu">#item for company name</strong><br/>  company = Field()</span><span id="7793" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu">#items for image pipeline</strong><br/>  image_urls = scrapy.Field()<br/>  images = scrapy.Field()</span><span id="a63b" class="nb lm it mx b gy ng nd l ne nf">pass</span></pre><p id="f555" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">编写我们的蜘蛛</strong></p><blockquote class="mo mp mq"><p id="9256" class="kg kh lj ki b kj kk kl km kn ko kp kq mr ks kt ku ms kw kx ky mt la lb lc ld im bi translated">蜘蛛是定义如何抓取某个站点(或一组站点)的类，包括如何执行抓取(即跟随链接)以及如何从页面中提取结构化数据(即抓取项目)。换句话说，蜘蛛是为特定站点(或者，在某些情况下，一组站点)定义抓取和解析页面的自定义行为的地方。</p><p id="5dd7" class="kg kh lj ki b kj kk kl km kn ko kp kq mr ks kt ku ms kw kx ky mt la lb lc ld im bi translated"><strong class="ki iu">出自</strong><a class="ae kf" href="https://docs.scrapy.org/en/latest/topics/spiders.html" rel="noopener ugc nofollow" target="_blank"><strong class="ki iu">scrapy.org</strong></a></p></blockquote><pre class="lf lg lh li gt mw mx my mz aw na bi"><span id="814d" class="nb lm it mx b gy nc nd l ne nf"><strong class="mx iu">fashionWebScraping $</strong> scrapy genspider fashionHM hm.com<br/><em class="lj">Created spider 'fashionHM' using template 'basic' in module:<br/>fashionWebScraping.spiders.fashionHM</em></span></pre><p id="e50b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的 shell 命令创建了一个空的蜘蛛文件。让我们将代码写入我们的<strong class="ki iu"> <em class="lj"> fashionHM.py </em> </strong>文件:</p><pre class="lf lg lh li gt mw mx my mz aw na bi"><span id="3bbe" class="nb lm it mx b gy nc nd l ne nf">import scrapy<br/>from fashionWebScraping.items import FashionwebscrapingItem<br/>from scrapy.http import Request</span><span id="9c0f" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu">#to read from a csv file</strong><br/>import csv</span><span id="198d" class="nb lm it mx b gy ng nd l ne nf">class FashionhmSpider(scrapy.Spider):<br/> name = 'fashionHM'<br/> allowed_domains = ['www2.hm.com']<br/> start_urls = ['http://www2.hm.com/']</span><span id="3f52" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu"># This function helps us to scrape the whole content of the website<br/> # by following the links in a csv file.</strong><br/> def start_requests(self):</span><span id="7088" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu"># Read main category links from a csv file</strong> <br/>  with open("SpiderMainCategoryLinks.csv", "rU") as f:<br/>   reader=csv.DictReader(f)<br/>   <br/>   for row in reader:<br/>    url=row['url']</span><span id="8f71" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu"># Change the offset value incrementally <br/>    # to navigate through the     product list<br/>    # You can play with the range value <br/>    # according to maximum product quantity<br/></strong>    link_urls = [url.format(i*100) for i in range(0,1)]<br/>    <br/>    for link_url in link_urls:<br/>      print(link_url)<br/>      <br/><strong class="mx iu">      # Pass the each link containing 100 products, <br/>      # to parse_product_pages function with the gender metadata</strong></span><span id="c53f" class="nb lm it mx b gy ng nd l ne nf">request=Request(link_url, callback=self.parse_product_pages, meta={'gender': row['gender']})<br/>      yield request</span><span id="26bf" class="nb lm it mx b gy ng nd l ne nf"><strong class="mx iu"># This function scrapes the page with the help of xpath provided</strong><br/> def parse_product_pages(self,response):<br/>  item=FashionwebscrapingItem()<br/>  <br/> <strong class="mx iu"> # Get the HTML block where all the products are listed<br/>  # &lt;ul&gt; HTML element with the "products-listing small" class name</strong><br/>  content=response.xpath('//ul[@class="products-listing small"]')<br/>  <br/><strong class="mx iu">   # loop through the &lt;li&gt; elements with the <br/>   # "product-item" class name in the content</strong><br/>   for product_content in content.xpath('//li[@class="product-item"]'):<br/>    image_urls = []<br/>      <br/>    <strong class="mx iu"># get the product details and populate the items</strong><br/>item['productId']=product_content.xpath('.//article[@class="hm-product-item"]/@data-articlecode').extract_first()</span><span id="d5bf" class="nb lm it mx b gy ng nd l ne nf">item['productName']=product_content.xpath('.//a[@class="link"]/text()').extract_first()</span><span id="dc23" class="nb lm it mx b gy ng nd l ne nf">item['priceOriginal']=product_content.xpath('.//span[@class="price regular"]/text()').extract_first()</span><span id="1cf6" class="nb lm it mx b gy ng nd l ne nf">item['priceSale']=product_content.xpath('.//span[@class="price sale"]/text()').extract_first()</span><span id="4b05" class="nb lm it mx b gy ng nd l ne nf">if item['priceSale']==None:<br/>      item['priceSale']=item['priceOriginal']</span><span id="e3a0" class="nb lm it mx b gy ng nd l ne nf">item['productLink']="https://www2.hm.com"+product_content.xpath('.//a[@class="link"]/@href').extract_first()</span><span id="3326" class="nb lm it mx b gy ng nd l ne nf">item['imageLink']="https:"+product_content.xpath('.//img/@data-src').extract_first()<br/>     image_urls.append(item['imageLink'])<br/>     item['image_urls']=image_urls<br/>     item['company']="H&amp;M"<br/>     item['gender']=response.meta['gender']<br/>     <br/>     if item['productId']==None: <br/>      break<br/>     print(item['productId'])<br/>     <br/>     yield (item)</span><span id="7c6e" class="nb lm it mx b gy ng nd l ne nf">def parse(self, response):<br/>  pass</span></pre><p id="18bb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，从 CSV 文件中读取主类别链接，然后循环使用偏移值遍历链接，以浏览产品列表。</p><p id="06a7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的技巧是使用<strong class="ki iu"> XPath </strong>来定位我们想要抓取的 Html 部分。剩下的就很简单了。</p><p id="5360" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">慢慢来，研究一下<a class="ae kf" href="https://docs.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html" rel="noopener ugc nofollow" target="_blank"> XPath </a>，因为它是 web 报废的核心主题。它提供了在 HTML 中导航的工具。</p><p id="e81a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">运行我们的蜘蛛</strong></p><pre class="lf lg lh li gt mw mx my mz aw na bi"><span id="c6a1" class="nb lm it mx b gy nc nd l ne nf"><strong class="mx iu">$ </strong>scrapy crawl -o rawdata_HM.json -t json fashionHM</span></pre><p id="6774" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在几分钟内，这一行 shell 命令通过将图像和相关产品数据存储在一个 JSON 文件中，不仅生成了 10K 图像和相关产品数据。</p><p id="f729" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">像魔术一样！</p><h1 id="fd9f" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">结论</h1><p id="5c3a" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我们用 Scrapy 和 Python 经历了网页抓取的过程。如果你是<strong class="ki iu">数据科学</strong>或<strong class="ki iu">机器学习</strong>爱好者，掌握网页抓取的基础知识会对你有很大帮助。</p><p id="fa0c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在<a class="ae kf" href="https://github.com/eisbilen/fashionWebScraping" rel="noopener ugc nofollow" target="_blank">我的 GitHub 库</a>中找到项目代码。</p><p id="30b8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下载的服装图像可以用于训练与时尚相关的定制深度学习模型。在培训过程中使用这些图像之前，您可能需要对它们进行预处理和标记。</p><p id="69d6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不幸的是，这是深度学习最耗时和最困难的任务。</p><p id="3ca8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我的下一篇文章将详细讨论这个主题。</p></div></div>    
</body>
</html>