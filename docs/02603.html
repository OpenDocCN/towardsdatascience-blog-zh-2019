<html>
<head>
<title>This thing called Weight Decay</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这个东西叫做重量衰减</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab?source=collection_archive---------0-----------------------#2019-04-29">https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab?source=collection_archive---------0-----------------------#2019-04-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/25d0256f08fb92c81e60ab690fb792ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U8vqEJjpFfKYLvtEsR3dxA.png"/></div></div></figure><h2 id="b372" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">介绍</h2><p id="2c61" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">在我之前的文章中，我提到过<a class="ae lp" href="https://medium.com/@dipam44/data-augmentations-in-fastai-84979bbcefaa" rel="noopener">数据增强</a>有助于深度学习模型很好地推广。那是在数据方面。事情的模型方面呢？在训练我们的模型时，我们能做些什么来帮助他们更好地概括。</p><p id="cbb4" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">我们做<strong class="kw ir"> <em class="lv">重量衰减</em> </strong>。</p><h2 id="16d5" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">模型的参数</h2><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lw"><img src="../Images/0d14ff84476ac21c0081155a52a64056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NXjpInUYpoNyt0Nyb4Y2aA.png"/></div></div></figure><p id="b937" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">我们从上面的图片开始。我们看到我们有一堆数据点，我们不能用一条直线很好地拟合它们。因此，我们使用一个二次多项式来这样做。我们还注意到，如果增加多项式的次数超过某个点，那么我们的模型就会变得太复杂，并开始过度拟合。</p><p id="1c2c" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">这意味着为了防止过度拟合，我们不应该让我们的模型变得太复杂。不幸的是，这导致了深度学习中的一个误解，即我们不应该使用大量的参数(为了防止我们的模型变得过于复杂)。</p><h2 id="4c1a" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">重量衰减的起源</h2><p id="d1f3" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">首先，真实世界的数据不会像上面显示的那样简单。真实世界的数据是复杂的，为了解决复杂的问题，我们需要复杂的解决方案。</p><p id="19b0" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">拥有更少的参数只是防止我们的模型变得过于复杂的一种方式。但这实际上是一个非常有限的策略。更多的参数意味着我们神经网络各部分之间更多的相互作用。更多的互动意味着更多的非线性。这些非线性帮助我们解决复杂的问题。</p><p id="8ce6" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">然而，我们不希望这些交互失去控制。因此，如果我们惩罚复杂性。我们仍然会使用很多参数，但是我们会防止我们的模型变得太复杂。体重衰减的想法就是这样产生的。</p><p id="b2e4" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">在我关于<a class="ae lp" href="https://becominghuman.ai/collaborative-filtering-using-fastai-a2ec5a2a4049" rel="noopener ugc nofollow" target="_blank">协同过滤</a>的文章中，我们已经看到了权重衰减。事实上，fastai 库中的每个学习者都有一个参数，叫做权重衰减。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mb"><img src="../Images/5622b05d839ffdc36666d9195a27cfe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5ZPTQXooTntp2tSfjfYLQ.png"/></div></div></figure><h2 id="c22a" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">这个东西叫做重量衰减</h2><p id="7f8c" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">惩罚复杂性的一种方法是将我们所有的参数(权重)添加到损失函数中。嗯，那不太管用，因为有些参数是正的，有些是负的。那么如果我们把所有参数的平方加到损失函数中。我们可以这样做，但这可能会导致我们的损失变得如此巨大，以至于最好的模型是将所有参数设置为 0。</p><p id="a583" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">为了防止这种情况发生，我们将平方和乘以另一个更小的数。这个数字叫做<strong class="kw ir"> <em class="lv">重量衰减</em> </strong>或<code class="fe mc md me mf b">wd.</code></p><p id="eb5a" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">我们的损失函数现在看起来如下:</p><pre class="lx ly lz ma gt mg mf mh mi aw mj bi"><span id="27e1" class="jy jz iq mf b gy mk ml l mm mn">Loss = MSE(y_hat, y) + wd * sum(w^2)</span></pre><p id="b006" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">当我们使用梯度下降更新权重时，我们执行以下操作:</p><pre class="lx ly lz ma gt mg mf mh mi aw mj bi"><span id="509f" class="jy jz iq mf b gy mk ml l mm mn">w(t) = w(t-1) - lr * dLoss / dw</span></pre><p id="7312" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">既然我们的损失函数有两项，那么第二项的导数 w.r.t <code class="fe mc md me mf b">w</code>就是:</p><pre class="lx ly lz ma gt mg mf mh mi aw mj bi"><span id="3316" class="jy jz iq mf b gy mk ml l mm mn">d(wd * w^2) / dw = 2 * wd * w (similar to d(x^2)/dx = 2x)</span></pre><p id="030a" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">也就是说，从现在开始，我们不仅要从权重中减去<code class="fe mc md me mf b">learning rate * gradient</code>，还要减去<code class="fe mc md me mf b">2 * wd * w</code>。我们从原来的重量中减去一个常数乘以重量。这就是为什么它被称为重量衰减。</p><h2 id="dba0" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">决定 wd 的值</h2><p id="f54a" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">一般来说，<code class="fe mc md me mf b">wd = 0.1</code>工作得很好。然而，fastai 的人在这方面有点保守。因此 fastai 中重量衰减的默认值实际上是<code class="fe mc md me mf b">0.01</code>。</p><p id="1ae7" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">选择这个值的原因是，如果你的体重下降太多，那么无论你训练多少，这个模型都不会拟合得足够好，而如果你的体重下降太少，你仍然可以训练得很好，你只需要稍微早一点停止。</p><p id="1970" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">我已经在这个<a class="ae lp" href="https://www.kaggle.com/dipam7/multiclass-classification-and-weight-decay-fastai" rel="noopener ugc nofollow" target="_blank"> jupyter 笔记本</a>中演示了这个概念。</p><p id="0787" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">这是一个多类别(而不是<a class="ae lp" href="https://becominghuman.ai/multi-label-classification-using-fastai-e424d7e71dcc" rel="noopener ugc nofollow" target="_blank">多标签</a>)分类问题，我们试图预测植物幼苗的类别。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/4f9da98cc72c9fdd96cffd69ff4a9511.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*BzjROV_xZpLA3z5VOVE6Kw.png"/></div></figure><p id="6d38" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">我使用了 3 个重量衰减值，默认值<code class="fe mc md me mf b">0.01</code>，最佳值<code class="fe mc md me mf b">0.1</code>和一个大值<code class="fe mc md me mf b">10</code>。在第一种情况下，我们的模型需要更多的时期来适应。在第二种情况下，它工作得最好，而在最后一种情况下，即使在 10 个时期之后，它也从来没有完全适合。(参见差异 b/w 培训和验证损失。)</p><p id="82d7" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">这就是本文的全部内容。你可以在这里了解<a class="ae lp" href="https://becominghuman.ai/regularization-in-neural-networks-3b9687e1a68c" rel="noopener ugc nofollow" target="_blank">其他正规化技术</a>。</p><p id="8ea0" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">如果你想了解更多关于深度学习的知识，可以看看我在这方面的系列文章:</p><div class="mp mq gp gr mr ms"><a href="https://medium.com/@dipam44/deep-learning-series-30ad108fbe2b" rel="noopener follow" target="_blank"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd ir gy z fp mx fr fs my fu fw ip bi translated">深度学习系列</h2><div class="mz l"><h3 class="bd b gy z fp mx fr fs my fu fw dk translated">我所有关于深度学习的文章的系统列表</h3></div><div class="na l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">medium.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng jw ms"/></div></div></a></div><p id="4758" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">~快乐学习。</p></div></div>    
</body>
</html>