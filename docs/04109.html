<html>
<head>
<title>Python, Performance, and GPUs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python、性能和 GPU</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/python-performance-and-gpus-1be860ffd58d?source=collection_archive---------0-----------------------#2019-06-28">https://towardsdatascience.com/python-performance-and-gpus-1be860ffd58d?source=collection_archive---------0-----------------------#2019-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9289" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Python 中使用 GPU 加速器的状态更新</h2></div><p id="2f0f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">这篇博文是在最近的</em> <a class="ae lc" href="https://pasc19.pasc-conference.org/" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> PASC 2019 </em> </a> <em class="lb">大会上以谈话形式发表的。</em> <a class="ae lc" href="https://docs.google.com/presentation/d/e/2PACX-1vSajAH6FzgQH4OwOJD5y-t9mjF9tTKEeljguEsfcjavp18pL4LkpABy4lW2uMykIUvP2dC-1AmhCq6l/pub?start=false&amp;loop=false&amp;delayms=60000" rel="noopener ugc nofollow" target="_blank"> <em class="lb">演讲的幻灯片在这里</em> </a> <em class="lb">。</em></p><h1 id="e114" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">行动纲要</h1><p id="dd27" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">我们正在改进 Python 中可扩展 GPU 计算的状态。</p><p id="8f3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章展示了当前的状态，并描述了未来的工作。它还总结并链接了最近几个月的其他几篇博客文章，这些文章深入到不同的主题，供感兴趣的读者阅读。</p><p id="da3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">概括地说，我们简要地涵盖了以下几个类别:</p><ul class=""><li id="1b76" class="ma mb iq kh b ki kj kl km ko mc ks md kw me la mf mg mh mi bi translated">用 CUDA 写的 Python 库比如 CuPy 和 RAPIDS</li><li id="1724" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">Python-CUDA 编译器，特别是 Numba</li><li id="9a70" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">使用 Dask 扩展这些库</li><li id="9526" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">与 UCX 的网络通信</li><li id="b55d" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">用康达包装</li></ul><h1 id="e206" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">GPU 加速的 Python 库的性能</h1><p id="1832" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">对于 Python 程序员来说，获得 GPU 性能的最简单方法可能就是使用 GPU 加速的 Python 库。这些提供了一组通用的操作，它们被很好地调整和集成在一起。</p><p id="3c15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">许多用户知道用于深度学习的库，如 PyTorch 和 TensorFlow，但还有其他几个用于更通用计算的库。这些倾向于复制流行的 Python 项目的 API:</p><ul class=""><li id="7776" class="ma mb iq kh b ki kj kl km ko mc ks md kw me la mf mg mh mi bi translated">GPU 上的 numpy:<a class="ae lc" href="https://cupy.chainer.org/" rel="noopener ugc nofollow" target="_blank">CuPy</a></li><li id="f3c7" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">GPU 上的 Numpy(再次):<a class="ae lc" href="https://github.com/google/jax" rel="noopener ugc nofollow" target="_blank"> Jax </a></li><li id="caef" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">GPU 上的熊猫:<a class="ae lc" href="https://docs.rapids.ai/api/cudf/nightly/" rel="noopener ugc nofollow" target="_blank">激流 cuDF </a></li><li id="0444" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">sci kit-在 GPU 上学习:<a class="ae lc" href="https://docs.rapids.ai/api/cuml/nightly/" rel="noopener ugc nofollow" target="_blank">急流城累计</a></li></ul><p id="25ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些库构建了流行 Python 库的 GPU 加速变体，如 NumPy、Pandas 和 Scikit-Learn。为了更好地理解相对性能差异<a class="ae lc" href="https://github.com/pentschev" rel="noopener ugc nofollow" target="_blank"> Peter Entschev </a>最近整理了一个<a class="ae lc" href="https://github.com/pentschev/pybench" rel="noopener ugc nofollow" target="_blank">基准测试套件</a>来帮助进行比较。他制作了下图，展示了 GPU 和 CPU 之间的相对加速比:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mo"><img src="../Images/18d458ee485d27dfdbf0654084b699c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gS93S6LMioksAzln3Z0aIA.png"/></div></div></figure><p id="bc34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那里有很多有趣的结果。彼得在他的博客中对此进行了更深入的探讨。</p><p id="093b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，更广泛地说，我们看到了性能的可变性。我们关于 CPU 快慢的心理模型并不一定适用于 GPU。幸运的是，由于一致的 API，熟悉 Python 的用户可以很容易地试验 GPU 加速，而无需学习 CUDA。</p><h1 id="398c" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">Numba:将 Python 编译成 CUDA</h1><p id="64e0" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated"><em class="lb">另见本</em> <a class="ae lc" href="https://blog.dask.org/2019/04/09/numba-stencil" rel="noopener ugc nofollow" target="_blank"> <em class="lb">最近关于 Numba stencils </em> </a> <em class="lb">的博文以及附带的</em> <a class="ae lc" href="https://gist.github.com/mrocklin/9272bf84a8faffdbbe2cd44b4bc4ce3c" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> GPU 笔记本</em> </a></p><p id="c422" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像 CuPy 和 RAPIDS 这样的 GPU 库中的内置操作，涵盖了最常见的操作。然而，在现实世界的设置中，我们经常会发现需要编写一些自定义代码的混乱情况。在这些情况下，切换到 C/C++/CUDA 可能会很有挑战性，尤其是对于主要是 Python 开发人员的用户。这就是 Numba 的用武之地。</p><p id="ce92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Python 在 CPU 上也有同样的问题。用户通常懒得学习 C/C++来编写快速定制代码。为了解决这个问题，有像 Cython 或 Numba 这样的工具，它们让 Python 程序员无需学习 Python 语言以外的知识就能编写快速的数字代码。</p><p id="bc08" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，Numba 在 CPU 上将 For 循环风格的代码加速到 500 倍以下，从缓慢的 Python 速度提高到快速的 C/Fortran 速度。</p><pre class="mp mq mr ms gt na nb nc nd aw ne bi"><span id="9f8e" class="nf le iq nb b gy ng nh l ni nj"><strong class="nb ir">import</strong> <strong class="nb ir">numba</strong>  <em class="lb"># We added these two lines for a 500x speedup</em><br/><br/>@numba.jit    <em class="lb"># We added these two lines for a 500x speedup</em><br/><strong class="nb ir">def</strong> sum(x):<br/>    total = 0<br/>    <strong class="nb ir">for</strong> i <strong class="nb ir">in</strong> range(x.shape[0]):<br/>        total += x[i]<br/>    <strong class="nb ir">return</strong> total</span></pre><p id="de68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在不切换 Python 上下文的情况下下降到底层高性能代码的能力是有用的，特别是如果您还不知道 C/C++或者没有为您设置的编译器链的话(目前大多数 Python 用户都是这种情况)。</p><p id="eed4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种优势在 GPU 上更加明显。虽然许多 Python 程序员懂一点 C 语言，但很少有人懂 CUDA。即使他们这样做了，他们可能也很难设置编译器工具和开发环境。</p><p id="7fcb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">进入<a class="ae lc" href="https://numba.pydata.org/numba-doc/dev/cuda/index.html" rel="noopener ugc nofollow" target="_blank"> numba.cuda.jit </a> Numba 的 cuda 后端。Numba.cuda.jit 允许 Python 用户以交互方式创作、编译和运行用 Python 编写的 cuda 代码，而无需离开 Python 会话。这是一个在 Jupyter 笔记本中编写模板计算来平滑 2d 图像的图像:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nk"><img src="../Images/7c7426abaf0072a7b7809ed442a7b230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F7MaK0KaBB4sRaWxdbv31Q.png"/></div></div></figure><p id="d308" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是 Numba CPU/GPU 代码的简化对比，比较编程风格。GPU 代码的速度比单个 CPU 内核快 200 倍。</p><h1 id="b99b" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">CPU — 600 毫秒</h1><pre class="mp mq mr ms gt na nb nc nd aw ne bi"><span id="037c" class="nf le iq nb b gy ng nh l ni nj">@numba.jit<br/><strong class="nb ir">def</strong> _smooth(x):<br/>    out = np.empty_like(x)<br/>    <strong class="nb ir">for</strong> i <strong class="nb ir">in</strong> range(1, x.shape[0] - 1):<br/>        <strong class="nb ir">for</strong> j <strong class="nb ir">in</strong> range(1, x.shape[1] - 1):<br/>            out[i,j] = (x[i-1, j-1] + x[i-1, j+0] + x[i-1, j+1] +<br/>                        x[i+0, j-1] + x[i+0, j+0] + x[i+0, j+1] +<br/>                        x[i+1, j-1] + x[i+1, j+0] + x[i+1, j+1])//9</span><span id="3ee6" class="nf le iq nb b gy nl nh l ni nj">    <strong class="nb ir">return</strong> out</span></pre><p id="c425" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者如果我们使用花哨的 numba.stencil 装饰器…</p><pre class="mp mq mr ms gt na nb nc nd aw ne bi"><span id="0aa1" class="nf le iq nb b gy ng nh l ni nj">@numba.stencil<br/><strong class="nb ir">def</strong> _smooth(x):<br/>    <strong class="nb ir">return</strong> (x[-1, -1] + x[-1, 0] + x[-1, 1] +<br/>            x[ 0, -1] + x[ 0, 0] + x[ 0, 1] +<br/>            x[ 1, -1] + x[ 1, 0] + x[ 1, 1]) // 9</span></pre><h1 id="af8d" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">GPU — 3 毫秒</h1><pre class="mp mq mr ms gt na nb nc nd aw ne bi"><span id="0f6a" class="nf le iq nb b gy ng nh l ni nj">@numba.cuda.jit<br/><strong class="nb ir">def</strong> smooth_gpu(x, out):<br/>    i, j = cuda.grid(2)<br/>    n, m = x.shape<br/>    <strong class="nb ir">if</strong> 1 &lt;= i &lt; n - 1 <strong class="nb ir">and</strong> 1 &lt;= j &lt; m - 1:<br/>        out[i, j] = (x[i-1, j-1] + x[i-1, j] + x[i-1, j+1] +<br/>                     x[i  , j-1] + x[i  , j] + x[i  , j+1] +<br/>                     x[i+1, j-1] + x[i+1, j] + x[i+1, j+1]) // 9</span></pre><p id="7b28" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Numba.cuda.jit 已经存在很长时间了。它容易接近，成熟，玩起来很有趣。如果你有一台装有 GPU 的机器，并有一些好奇心，那么我们强烈建议你尝试一下。</p><pre class="mp mq mr ms gt na nb nc nd aw ne bi"><span id="2335" class="nf le iq nb b gy ng nh l ni nj">conda install numba<br/># or<br/>pip install numba</span><span id="15aa" class="nf le iq nb b gy nl nh l ni nj">&gt;&gt;&gt; <strong class="nb ir">import</strong> <strong class="nb ir">numba.cuda</strong></span></pre><h1 id="ad70" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">使用 Dask 扩展</h1><p id="bf2e" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">正如在以前的博客文章(<a class="ae lc" href="https://blog.dask.org/2019/01/03/dask-array-gpus-first-steps" rel="noopener ugc nofollow" target="_blank"> 1 </a>、<a class="ae lc" href="https://blog.dask.org/2019/01/13/dask-cudf-first-steps" rel="noopener ugc nofollow" target="_blank"> 2 </a>、<a class="ae lc" href="https://blog.dask.org/2019/03/04/building-gpu-groupbys" rel="noopener ugc nofollow" target="_blank"> 3 </a>、<a class="ae lc" href="https://blog.dask.org/2019/03/18/dask-nep18" rel="noopener ugc nofollow" target="_blank"> 4 </a>)中提到的，我们已经一般化了<a class="ae lc" href="https://dask.org/" rel="noopener ugc nofollow" target="_blank"> Dask </a>，不仅可以操作 Numpy 数组和 Pandas 数据帧，还可以操作任何看起来足够像 Numpy(像<a class="ae lc" href="https://cupy.chainer.org/" rel="noopener ugc nofollow" target="_blank"> CuPy </a>或<a class="ae lc" href="https://sparse.pydata.org/en/latest/" rel="noopener ugc nofollow" target="_blank"> Sparse </a>或<a class="ae lc" href="https://github.com/google/jax" rel="noopener ugc nofollow" target="_blank"> Jax </a>)或足够像 Pandas(像【T16 这很有效。下面是一个简短的视频，展示了 Dask array 并行计算一个 SVD，并展示了当我们用 CuPy 替换 Numpy 库时会发生什么。</p><figure class="mp mq mr ms gt mt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="2208" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们看到计算速度提高了大约 10 倍。最重要的是，我们能够在 CPU 实现和 GPU 实现之间切换，只需一行代码，但继续使用 Dask Array 的复杂算法，就像它的并行 SVD 实现一样。</p><p id="940f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们也看到了交流的相对放缓。总的来说，今天几乎所有重要的 Dask + GPU 工作都变得与通信密切相关。我们的计算速度已经够快了，交流的相对重要性已经显著增加。</p><h1 id="a1ab" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">与 UCX 的通信</h1><p id="f622" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated"><em class="lb">见</em> <a class="ae lc" href="https://developer.download.nvidia.com/video/gputechconf/gtc/2019/video/S9679/s9679-ucx-python-a-flexible-communication-library-for-python-applications.mp4" rel="noopener ugc nofollow" target="_blank"> <em class="lb">本谈</em> </a> <em class="lb">经</em> <a class="ae lc" href="https://github.com/Akshay-Venkatesh" rel="noopener ugc nofollow" target="_blank"> <em class="lb">或查看</em></a><a class="ae lc" href="https://www.slideshare.net/MatthewRocklin/ucxpython-a-flexible-communication-library-for-python-applications" rel="noopener ugc nofollow" target="_blank"/></p><p id="1cf2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">也参见</em> <a class="ae lc" href="https://blog.dask.org/2019/06/09/ucx-dgx" rel="noopener ugc nofollow" target="_blank"> <em class="lb">这篇最近关于 UCX 和达斯克</em> </a>的博文</p><p id="37e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们一直在用<a class="ae lc" href="https://github.com/rapidsai/ucx-py" rel="noopener ugc nofollow" target="_blank">的 UCX-Py </a>将<a class="ae lc" href="https://openucx.org/" rel="noopener ugc nofollow" target="_blank">的 OpenUCX </a>库集成到 Python 中。UCX 提供对 TCP、InfiniBand、共享内存和 NVLink 等传输的统一访问。UCX-Py 是第一次可以从 Python 语言轻松访问这些传输协议。</p><p id="63a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一起使用 UCX 和 Dask，我们能够获得显著的加速。以下是添加 UCX 前后的 SVD 计算轨迹:</p><p id="1bed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">在 UCX 之前</strong>:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi no"><img src="../Images/415c59dc8f616d788ca8a04d4cfd2e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZS-ZaR_7bTjY7EyfviTXCw.png"/></div></div></figure><p id="f4e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">UCX 之后</strong>:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi np"><img src="../Images/1384da09bf4d53c3260bd4a860b25231.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EsjVBGltjki3E1ujr9NVGw.png"/></div></div></figure><p id="e9f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管如此，这里仍有大量工作要做(上面链接的博客文章在未来工作部分有几个条目)。</p><p id="0015" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">人们可以用高度实验性的康达套装试用 UCX 和 UCX-Py:</p><pre class="mp mq mr ms gt na nb nc nd aw ne bi"><span id="6c03" class="nf le iq nb b gy ng nh l ni nj">conda create -n ucx -c conda-forge -c jakirkham/label/ucx cudatoolkit=9.2 ucx-proc=*=gpu ucx ucx-py python=3.7</span></pre><p id="b33e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们希望这项工作也将影响使用 Infiniband 的 HPC 系统上的非 GPU 用户，甚至是消费类硬件上的用户，因为他们可以轻松访问共享内存通信。</p><h1 id="50ed" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">包装</h1><p id="fafa" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">在之前的<a class="ae lc" href="https://matthewrocklin.com/blog/work/2018/12/17/gpu-python-challenges" rel="noopener ugc nofollow" target="_blank">博文</a>中，我们讨论了安装与系统上安装的 CUDA 驱动程序不匹配的 CUDA 支持包的错误版本所带来的挑战。幸运的是，由于 Anaconda 的 Stan Seibert 和 Michael Sarahan 最近的工作，Conda 4.7 现在有了一个特殊的元包，它被设置为已安装驱动程序的版本。这将使用户在未来安装正确的软件包变得更加容易。</p><p id="79d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Conda 4.7 刚刚发布，除了<code class="fe nq nr ns nb b">cuda</code>元包之外，还有许多新特性。你可以在这里了解更多信息<a class="ae lc" href="https://www.anaconda.com/how-we-made-conda-faster-4-7/" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="mp mq mr ms gt na nb nc nd aw ne bi"><span id="5466" class="nf le iq nb b gy ng nh l ni nj">conda update conda</span></pre><p id="8a8b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">今天在包装领域仍有大量的工作要做。每个构建 conda 包的人都用自己的方式，导致头痛和异构。这很大程度上是因为没有集中的基础设施来构建和测试支持 CUDA 的包，就像我们在<a class="ae lc" href="https://conda-forge.org/" rel="noopener ugc nofollow" target="_blank"> Conda Forge </a>所做的那样。幸运的是，Conda Forge 社区正在与 Anaconda 和 NVIDIA 合作来帮助解决这个问题，尽管这可能需要一些时间。</p><h1 id="4120" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">摘要</h1><p id="69a0" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">这篇文章更新了 Python 中 GPU 计算背后的一些努力。它也为将来的阅读提供了各种各样的链接。如果您想了解更多信息，我们在下面列出了它们:</p><ul class=""><li id="67d3" class="ma mb iq kh b ki kj kl km ko mc ks md kw me la mf mg mh mi bi translated"><a class="ae lc" href="https://docs.google.com/presentation/d/e/2PACX-1vSajAH6FzgQH4OwOJD5y-t9mjF9tTKEeljguEsfcjavp18pL4LkpABy4lW2uMykIUvP2dC-1AmhCq6l/pub?start=false&amp;loop=false&amp;delayms=60000" rel="noopener ugc nofollow" target="_blank">幻灯片</a></li><li id="dc2b" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">GPU 上的 numpy:<a class="ae lc" href="https://cupy.chainer.org/" rel="noopener ugc nofollow" target="_blank">CuPy</a></li><li id="5655" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">GPU 上的 Numpy(再次):<a class="ae lc" href="https://github.com/google/jax" rel="noopener ugc nofollow" target="_blank"> Jax </a></li><li id="ffdf" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">GPU 上的熊猫:<a class="ae lc" href="https://docs.rapids.ai/api/cudf/nightly/" rel="noopener ugc nofollow" target="_blank">激流 cuDF </a></li><li id="df5a" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">sci kit-在 GPU 上学习:<a class="ae lc" href="https://docs.rapids.ai/api/cuml/nightly/" rel="noopener ugc nofollow" target="_blank"> RAPIDS cuML </a></li><li id="b1a6" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated"><a class="ae lc" href="https://github.com/pentschev/pybench" rel="noopener ugc nofollow" target="_blank">基准测试套件</a></li><li id="c932" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated"><a class="ae lc" href="https://gist.github.com/mrocklin/9272bf84a8faffdbbe2cd44b4bc4ce3c" rel="noopener ugc nofollow" target="_blank">数字 CUDA JIT 笔记本</a></li><li id="06a7" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">关于 UCX 的演讲</li><li id="1222" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated"><a class="ae lc" href="https://blog.dask.org/2019/06/09/ucx-dgx" rel="noopener ugc nofollow" target="_blank">一篇关于 UCX 和达斯克的博文</a></li><li id="82f5" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated"><a class="ae lc" href="https://www.anaconda.com/how-we-made-conda-faster-4-7/" rel="noopener ugc nofollow" target="_blank">康达 4.7 </a></li></ul></div></div>    
</body>
</html>