<html>
<head>
<title>Hyperparameter Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调谐</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624?source=collection_archive---------1-----------------------#2019-02-16">https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624?source=collection_archive---------1-----------------------#2019-02-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="532f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">探索 Kaggle 的不要过度拟合 II 竞赛中超参数调整方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0599fa6dcc93c8169ab93dafbbb191c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ttMDNygtDy7hYAvPvRi6w.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/photos/BeDcRuoBzzw?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae ky" href="https://unsplash.com/search/photos/collection?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="41f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">卡格尔的不要过度适应 II 竞赛提出了一个有趣的问题。我们有 20，000 行连续变量，其中只有 250 行属于训练集。</p><p id="12e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">挑战在于不要吃太多。</p><p id="2d74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于如此小的数据集，甚至更小的训练集，这可能是一项艰巨的任务！</p><p id="5571" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将探索超参数优化作为一种防止过度拟合的方法。</p><p id="3626" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的笔记本可以在这里找到<a class="ae ky" href="https://www.kaggle.com/tboyle10/hyperparameter-tuning" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="657c" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">超参数调谐</h2><p id="a695" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" rel="noopener ugc nofollow" target="_blank">维基百科</a>声明“超参数调优就是为一个学习算法选择一组最优的超参数”。那么什么是<a class="ae ky" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">超参数</a>？</p><blockquote class="mt mu mv"><p id="fb7d" class="kz la mw lb b lc ld ju le lf lg jx lh mx lj lk ll my ln lo lp mz lr ls lt lu im bi translated">超参数是在学习过程开始之前设置其值的参数。</p></blockquote><p id="f9fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数的一些例子包括逻辑回归中的惩罚和随机梯度下降中的损失。</p><p id="6922" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" href="https://scikit-learn.org/stable/modules/grid_search.html#grid-search" rel="noopener ugc nofollow" target="_blank"> sklearn </a>中，超参数作为参数传递给模型类的构造函数。</p><h2 id="864b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">调优策略</h2><p id="afbb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们将探讨优化超参数的两种不同方法:</p><ul class=""><li id="56f4" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">网格搜索</li><li id="a676" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">随机搜索</li></ul><p id="c123" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将从准备数据开始，用默认的超参数尝试几种不同的模型。我们将从中选择两种最佳的超参数调节方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="c8b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们找到平均交叉验证分数和标准偏差:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8d21" class="lv lw it nr b gy nv nw l nx ny">Ridge<br/>CV Mean:  0.6759762475523124<br/>STD:  0.1170461756924883<br/><br/>Lasso<br/>CV Mean:  0.5<br/>STD:  0.0<br/><br/>ElasticNet<br/>CV Mean:  0.5<br/>STD:  0.0<br/><br/>LassoLars<br/>CV Mean: 0.5<br/>STD:  0.0<br/><br/>BayesianRidge<br/>CV Mean:  0.688224616492365<br/>STD:  0.13183095412112777<br/><br/>LogisticRegression<br/>CV Mean:  0.7447916666666667<br/>STD:  0.053735373404660246<br/><br/>SGDClassifier<br/>CV Mean:  0.7333333333333333<br/>STD:  0.03404902964480909</span></pre><p id="41c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们这里表现最好的模型是逻辑回归和随机梯度下降。让我们看看是否可以通过超参数优化来提高它们的性能。</p><h2 id="ad7d" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">网格搜索</h2><p id="4776" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">网格搜索是执行超参数优化的传统方式。它通过彻底搜索超参数的指定子集来工作。</p><p id="ffe9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用 sklearn 的<code class="fe nz oa ob nr b">GridSearchCV</code>，我们首先定义要搜索的参数网格，然后运行网格搜索。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div></figure><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ccf0" class="lv lw it nr b gy nv nw l nx ny">Fitting 3 folds for each of 128 candidates, totalling 384 fits</span><span id="858b" class="lv lw it nr b gy oc nw l nx ny">Best Score:  0.7899186582809224<br/>Best Params:  {'C': 1, 'class_weight': {1: 0.6, 0: 0.4}, 'penalty': 'l1', 'solver': 'liblinear'}</span></pre><p id="a2e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将交叉验证分数从 0.744 提高到了 0.789！</p><p id="b461" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网格搜索的好处是可以保证找到所提供参数的最佳组合。缺点是非常耗时且计算量大。</p><p id="13c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以用随机搜索来解决这个问题。</p><h2 id="76a2" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">随机搜索</h2><p id="cfd0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">随机搜索不同于网格搜索，主要在于它随机地而不是穷尽地搜索超参数的指定子集。主要的好处是减少了处理时间。</p><p id="6fae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，减少处理时间是有代价的。我们不能保证找到超参数的最佳组合。</p><p id="cb32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们用 sklearn 的<code class="fe nz oa ob nr b">RandomizedSearchCV</code>来试试随机搜索吧。与上面的网格搜索非常相似，我们在运行搜索之前定义了要搜索的超参数。</p><p id="f4fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里需要指定的一个重要的附加参数是<code class="fe nz oa ob nr b">n_iter</code>。这指定了随机尝试的组合数量。</p><p id="ef2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择太低的数字会降低我们找到最佳组合的机会。选择太大的数字会增加我们的处理时间。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div></figure><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="769a" class="lv lw it nr b gy nv nw l nx ny">Fitting 3 folds for each of 1000 candidates, totalling 3000 fits</span><span id="564b" class="lv lw it nr b gy oc nw l nx ny">Best Score:  0.7972911250873514<br/>Best Params:  {'penalty': 'elasticnet', 'loss': 'log', 'learning_rate': 'optimal', 'eta0': 100, 'class_weight': {1: 0.7, 0: 0.3}, 'alpha': 0.1}</span></pre><p id="7903" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们将交叉验证分数从 0.733 提高到 0.780！</p><h2 id="9957" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">结论</h2><p id="fdba" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在这里，我们探索了两种超参数化的方法，并看到了模型性能的改善。</p><p id="28ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这是建模中的一个重要步骤，但绝不是提高性能的唯一方法。</p><p id="49a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在以后的文章中，我们将探索防止过度拟合的其他方法，包括特征选择和集合。</p></div></div>    
</body>
</html>