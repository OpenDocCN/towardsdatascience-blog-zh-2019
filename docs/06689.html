<html>
<head>
<title>Probability Learning V: Naive Bayes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">概率学习 V:朴素贝叶斯</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/probability-learning-v-naive-bayes-7f1d0466f5f1?source=collection_archive---------23-----------------------#2019-09-24">https://towardsdatascience.com/probability-learning-v-naive-bayes-7f1d0466f5f1?source=collection_archive---------23-----------------------#2019-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c08c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">概率机器学习的朴素模型。还是没那么幼稚…</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/997666a60648f1aa940b26e9a0a670df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PilXexSefB8nSdqVeHZgSg.jpeg"/></div></div></figure><p id="496e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">亲爱的读者们，你们好。这是概率学习系列的第五篇文章。之前的帖子有:</p><ul class=""><li id="13b6" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/probability-learning-i-bayes-theorem-708a4c02909a">概率学习 I:贝叶斯定理</a></li><li id="c6cf" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp lv lw lx ly bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/probability-learning-ii-how-bayes-theorem-is-applied-in-machine-learning-bd747a960962">概率学习 II:贝叶斯定理如何应用于机器学习</a></li><li id="a877" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp lv lw lx ly bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/probability-learning-iii-maximum-likelihood-e78d5ebea80c">概率学习三:最大似然</a></li><li id="43e4" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp lv lw lx ly bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/probability-learning-iv-the-math-behind-bayes-bfb94ea03dd8">概率学习四:贝叶斯背后的数学</a></li></ul><p id="b61f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">我强烈鼓励你阅读它们</strong>，因为它们很有趣，充满了关于概率机器学习的有用信息。</p><p id="aa48" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在上一篇文章中，我们讨论了机器学习的贝叶斯定理背后的数学。这篇文章将描述这个定理的各种<strong class="kw iu">简化</strong>，这些<strong class="kw iu">使它更实用，更适用于现实世界的问题</strong>:这些简化被称为<strong class="kw iu">朴素贝叶斯</strong>。此外，为了澄清一切，我们将看到一个非常有说明性的例子，它展示了朴素贝叶斯如何应用于分类。</p><h1 id="71fc" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">为什么我们不总是使用贝叶斯？</h1><p id="8351" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">正如在以前的文章中提到的，<strong class="kw iu">贝叶斯定理告诉我们，当我们获得更多关于<em class="nc">某事</em>的证据时，如何逐步更新我们关于<em class="nc">某事</em>的知识</strong>。</p><p id="8ece" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们看到，在机器学习中，这通过更新新数据证据中的某些参数分布来反映。我们还看到了如何使用<strong class="kw iu">贝叶斯定理进行分类</strong>，方法是计算一个新数据点属于某个类别的概率，并将这个新点分配给报告最高概率的类别。我们提到过，这种方法的优势在于能够将先前的知识整合到我们的模型中。</p><p id="d2cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">让我们恢复一下最基本的贝叶斯公式:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/10e99063ec193ac95cb257bf6e86b2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*Otz1cC9Qlvh-1ZetbHuzkg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">General version of Bayes Formula</figcaption></figure><p id="fdea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该公式可以定制为计算一个<strong class="kw iu">数据点<em class="nc"> x </em>，</strong>属于某个<strong class="kw iu">类<em class="nc"> ci </em> </strong>的概率，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/6ad1e48e8731a8680a0a58e66488afe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*U9xVhoYdwUz_R_zzRQuDkg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Bayes formula particularised for class i and the data point x</figcaption></figure><p id="8788" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">像这样的方法可以用于分类:我们计算一个数据点属于每个可能类别的概率，然后将这个新点分配给产生最高概率的类别。<strong class="kw iu">这可用于二值和多值分类。</strong></p><p id="c05e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">当我们的模型中的数据点具有不止一个特征时，贝叶斯定理应用的问题</strong>就出现了:计算<strong class="kw iu">似然项<em class="nc"> P(x|ci) </em> </strong>并不简单。该术语说明了给定某个类别的数据点(由其特征表示)的概率。如果这些特征在它们之间是相关的，那么这个<strong class="kw iu">条件概率</strong>计算的计算量会非常大。此外，如果有许多特征，并且我们必须计算所有特征的<strong class="kw iu">联合概率</strong>，计算也可能相当广泛。</p><p id="f896" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是为什么我们不总是使用贝叶斯，而是有时不得不求助于更简单的替代方法。</p><h1 id="294d" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">那么什么是朴素贝叶斯呢？</h1><p id="a3bf" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated"><strong class="kw iu">朴素贝叶斯是贝叶斯定理的简化，它被用作二元多类问题的分类算法。</strong>之所以称之为幼稚，是因为它做出了一个非常重要但却不太真实的假设:数据点的所有特征都是相互独立的。通过这样做，它极大地简化了贝叶斯分类所需的计算，同时保持了相当不错的结果。这类算法通常被用作分类问题的基线。</p><p id="03cd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">让我们看一个例子来阐明这是什么意思</strong>、<strong class="kw iu">以及与贝叶斯</strong>的区别:假设你喜欢每天早上去你家旁边的公园散一会儿步。这样做了一段时间后，你开始遇到一个非常聪明的老人，他有时会和你走同样的路。当你遇到他时，他会用最简单的术语向你解释数据科学的概念，优雅而清晰地分解复杂的事物。</p><p id="f47c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，有些日子，当你出去散步，兴奋地想听到更多老人的消息时，他却不在了。那些日子你希望你从未离开过你的家，并感到有点难过。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/136f5081764c8b54bfe1d6aa0abbb287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*tOtmIDXSzqo_FDUhssiYwQ.gif"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">One of your lovely walks in the park</figcaption></figure><p id="cd2a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了解决这个问题，你在一周内每天都去散步，并记下每天的天气情况，以及老人是否外出散步。下表显示了您收集的信息。“散步”一栏指的是老人是否去公园散步。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/2c66a983ed70019af89ca9cec94f37b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Oyl5BvUcb-D0n9Ny60ujA.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Table with the information collected during one week</figcaption></figure><p id="4e45" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">利用这些信息，以及这位数据科学专家曾经提到过的一些东西，<strong class="kw iu">朴素贝叶斯分类算法</strong>，你会根据当天的天气情况计算出老人每天出去散步的概率，然后决定你是否认为这个概率足够高，让你出去尝试遇见这位睿智的天才。</p><p id="861c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，如果我们<strong class="kw iu">在字段值为“否”时将每个分类变量</strong>建模为 0，在字段值为“是”时建模为 1，那么我们表格的第一行将是:</p><blockquote class="nl nm nn"><p id="1ad6" class="ku kv nc kw b kx ky ju kz la lb jx lc no le lf lg np li lj lk nq lm ln lo lp im bi">111001 | 0</p></blockquote><p id="b635" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中竖线后的 0 表示目标标签。</p><p id="e5dc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">如果我们使用正态贝叶斯算法</strong>来计算每种可能的天气情况下每个类别(步行或不步行)的后验概率，我们将必须计算每个类别的 0 和 1 的每种可能组合的概率。在这种情况下，我们必须为每个类计算 2 的 6 次方个可能组合的概率，因为我们有 6 个变量。一般推理如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/95cabeff76f11cc6c7fa51d40f962a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*WF1ys--wTRccsgqlwnTeeA.png"/></div></div></figure><p id="76ba" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这有各种各样的问题:首先，我们需要大量的数据来计算每个场景的概率。然后，如果我们有这些可用的数据，计算将比其他类型的方法花费更长的时间，并且这个时间将随着变量或特征的数量而大大增加。最后，如果我们认为这些变量中的一些是相关的(例如，阳光与温度)，我们将不得不在计算概率时考虑这种关系，这将导致更长的计算时间。</p><p id="2490" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">朴素贝叶斯是如何修复这一切的？</strong>通过假设每个特征变量独立于其他变量:这意味着我们只需计算给定每个类别的每个单独特征的概率，将所需的计算从 2^n 减少到 2n。同时，这意味着我们不关心变量之间可能的关系，比如太阳和温度。</p><p id="c554" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们一步一步地描述它<strong class="kw iu">这样你们就能更清楚地看到我在说什么:</strong></p><ol class=""><li id="a8c9" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp ns lw lx ly bi translated">首先，我们使用上表计算每个类别的<strong class="kw iu">先验概率。</strong></li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/4e1331e3783704b2d37e64354953e8f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*amPQeMJ3sc-hR4KzkutsxA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Prior probabilities for each class</figcaption></figure><p id="19ee" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">2.然后，<strong class="kw iu">对于每个特征，我们计算给定每个类别的不同分类值的概率</strong>(在我们的示例中，我们只有“是”和“否”作为每个特征的可能值，但这可能因数据而异)。以下示例显示了特征“Sun”的这种情况。<strong class="kw iu">我们必须对每个特性都这样做。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/317a0fa6b88e1f1e180eb8923d508abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*Tz2v-vv26TG0XHwFZK-Prg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Probabilities of sun values given each class</figcaption></figure><p id="0526" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">3.现在，<strong class="kw iu">当我们获得一个新的数据点</strong>作为一组气象条件时，我们可以通过将给定类别的每个特征的个体概率与每个类别的先验概率相乘来计算每个类别的概率。然后，我们将这个新数据点分配给产生最高概率的类。</p><p id="990e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看一个例子。假设我们观察以下天气状况。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/645c0b85ffe5bba30dd481d56d9e6ada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QlEfz8YY6X6ZoHetMprJ2w.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">New data point</figcaption></figure><p id="678e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">首先，给定这些条件，我们将计算老人行走</strong>的概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a2e344dc51ad80c859091c3a57d8c971.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*unpYS29gWPqssgkiA3rUWw.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Probabilities needed to calculate the chance of the old man walking</figcaption></figure><p id="c05f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们做所有这些的乘积，<strong class="kw iu">我们得到 0.0217 </strong>。现在让我们做同样的事情，但是对于另一个目标类:不走。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1885df60ea9f99db29ed1e7bd4e7438d.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*JfvTMD4Sn2cn0B6HHPYdQQ.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Probabilities needed to calculate the chance of the old man not walking</figcaption></figure><p id="5b15" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样，如果我们做乘积，<strong class="kw iu">我们得到 0.00027 </strong>。现在，如果我们比较两种可能性(男人走路和男人不走路)，我们得到一个男人走路的更高的可能性，所以我们穿上运动鞋，拿一件外套以防万一(有云)，然后<strong class="kw iu">去公园</strong>。</p><p id="b795" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意在这个例子中我们没有任何概率等于零。这与我们观察到的具体数据点以及我们拥有的数据量有关。如果任何一个计算出的概率为零，那么整个乘积将为零，这是不太现实的。<strong class="kw iu">为了避免这些，使用了被称为平滑的技术</strong>，但是我们不会在这篇文章中讨论它们。</p><p id="8937" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">就是它！</strong>现在，当我们醒来，想要看到我们会发现老人在散步的机会时，我们所要做的就是像前面的例子一样，看看天气情况，做一个快速的计算！</p><h1 id="fa39" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结论</h1><p id="bf58" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们已经看到了如何使用贝叶斯定理的一些简化来解决分类问题。在下一篇文章中，我们将讨论朴素贝叶斯在自然语言处理中的应用。</p><p id="5862" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就这些，我希望你喜欢这个帖子。要获得更多关于数据科学和机器学习的资源，请查看下面的博客:<a class="ae lz" href="https://howtolearnmachinelearning.com/books/machine-learning-books/" rel="noopener ugc nofollow" target="_blank">如何学习机器学习</a>。谢谢，祝你阅读愉快！</p><h1 id="c5d6" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated"><strong class="ak">其他资源</strong></h1><p id="8f59" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">如果您渴望了解更多信息，您可以使用以下资源:</p><ul class=""><li id="b33b" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated"><a class="ae lz" href="https://medium.com/@mark.rethana/bayesian-statistics-and-naive-bayes-classifier-33b735ad7b16" rel="noopener">中型岗位覆盖贝叶斯和朴素贝叶斯</a>由<a class="ny nz ep" href="https://medium.com/u/99a3476135bb?source=post_page-----7f1d0466f5f1--------------------------------" rel="noopener" target="_blank">马克·蕾瑟娜</a></li><li id="915e" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp lv lw lx ly bi translated"><a class="ae lz" href="https://www.youtube.com/watch?v=CPqOCI0ahss&amp;t=213s" rel="noopener ugc nofollow" target="_blank"> Youtube 视频关于朴素贝叶斯分类器的类似例子</a></li><li id="a3fe" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp lv lw lx ly bi translated"><a class="ae lz" href="https://machinelearningmastery.com/naive-bayes-for-machine-learning/?source=post_page-----33b735ad7b16----------------------" rel="noopener ugc nofollow" target="_blank">机器学习大师关于朴素贝叶斯的帖子</a></li></ul><p id="734b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一如既往，有任何问题请联系我。祝你有美好的一天，继续学习。</p></div></div>    
</body>
</html>