<html>
<head>
<title>Spectral graph clustering and optimal number of clusters estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谱图聚类和最优聚类数估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spectral-graph-clustering-and-optimal-number-of-clusters-estimation-32704189afbe?source=collection_archive---------1-----------------------#2019-01-01">https://towardsdatascience.com/spectral-graph-clustering-and-optimal-number-of-clusters-estimation-32704189afbe?source=collection_archive---------1-----------------------#2019-01-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bfb4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">谱图聚类概述和特征间隙启发式算法的 python 实现</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/302a4fed4a672bda99514474ff2a44d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j1BIg64PHS-i19SeN0Z8kw.jpeg"/></div></div></figure><p id="5cd9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这篇文章解释了谱图聚类算法的功能，然后看了一个名为<em class="ln">自调优图聚类</em>的变体。这种调整的优点在于提供了对最佳聚类数的估计，以及对数据点之间的相似性度量的估计。接下来，我们将基于输入数据的拉普拉斯算子的连续特征值之间的最大距离，提供对数据集中最优聚类数的<strong class="kt ir">特征间隙启发式</strong>计算的实现。</p><p id="03af" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在让我们从介绍一些基本的图论概念开始。</p><h1 id="7cb5" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">邻接矩阵(A)</h1><p id="6194" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">给定一个具有<em class="ln"> n </em>个顶点和<em class="ln"> m </em>个节点的图，邻接矩阵是一个 n*n 的方阵，其性质为:</p><p id="712c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果节点 I 和节点 j 之间有边，则 A[i][j] = 1，否则为 0</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/349323c519e4d7a660fc3f48b39790ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UhN9SBh-dehsy3RaYDz0qQ.png"/></div></div></figure><p id="d538" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因为 A 是对称的，所以它的本征向量是实数且正交的(点积为 0)。</p><h1 id="fa33" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">度矩阵</h1><p id="a927" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">度矩阵是具有属性的 n*n 对角矩阵</p><p id="c14e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">d[i][i] =节点 I 中相邻边的数目或节点 I 的度<strong class="kt ir"/></p><p id="a96e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">d[i][j] = 0</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/4cae6aee4bd906b03d5db6c191e43c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v4UN6tekd0D1NYNSYtFy7A.png"/></div></div></figure><h1 id="2807" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">拉普拉斯矩阵</h1><p id="867b" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">拉普拉斯矩阵是 n*n 矩阵，定义为:<strong class="kt ir"> L = D -A </strong></p><p id="c034" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其<strong class="kt ir">特征值是正实数</strong>，而<strong class="kt ir">特征向量是实数且正交的</strong>(两个向量的点积为 0)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/4a6f12d9a1bd7dec661ef2be079f5ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SKwO_w0Xs1zRNv-C60RL2w.png"/></div></div></figure><h1 id="3bef" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated"><strong class="ak">电导</strong></h1><p id="68d2" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">相对于组的密度(指向簇外的边的数量除以簇中节点的度数之和)，衡量组与网络其余部分的连通性。电导越低，簇越好。</p><p id="77c9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">用 x 计算 A 的特征值和特征向量(用节点的值计算 n 维向量):<strong class="kt ir">A * x =λ* x</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mn"><img src="../Images/584166ebb385da4122ed4214e2abb165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CSdb0ZWwALcLsL4A7wG2BQ.png"/></div></div></figure><p id="2e0d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">表示图 G 的矩阵的谱是由相应特征值λI 排序的图的一组特征向量 xi</p><p id="e79c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">既然我们已经介绍了图论中最重要的构件，我们就可以总结谱聚类的步骤了:</p><ol class=""><li id="3c46" class="mo mp iq kt b ku kv kx ky la mq le mr li ms lm mt mu mv mw bi translated">计算输入图<strong class="kt ir"> G </strong>的拉普拉斯矩阵<strong class="kt ir"> L </strong></li><li id="6024" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">计算特征值(λ)和特征向量(x ),使得</li></ol><p id="3615" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">L * x =λ* x</strong></p><p id="c576" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">3.选择对应于最大特征值的 n 个特征向量，并将输入空间重新定义为 n 维子空间</p><p id="05fc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">4.使用各种聚类算法，例如 k-means，在这个子空间中寻找聚类</p><p id="0dca" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">也可以使用<strong class="kt ir">相似矩阵</strong>来代替上面定义的邻接矩阵，该矩阵确定我们的空间中的 2 个点有多接近或相似。按照 sklearn <a class="ae nc" href="https://scikit-learn.org/dev/modules/clustering.html" rel="noopener ugc nofollow" target="_blank">实施</a>中的定义:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="1ee2" class="ni lp iq ne b gy nj nk l nl nm">similarity = np.exp(-beta * distance / distance.std())</span></pre><p id="058a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">演示亲和力矩阵创建的一个很好的资源是<a class="ae nc" href="https://www.youtube.com/watch?v=P-LEH-AFovE" rel="noopener ugc nofollow" target="_blank">这个</a> youtube 视频。</p><p id="01ee" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">谱聚类和相似性传播都已经在 python 中实现。这个<a class="ae nc" href="https://github.com/ciortanmadalina/high_noise_clustering/blob/master/spectral_clustering.ipynb" rel="noopener ugc nofollow" target="_blank">木星笔记本</a>展示了它们的用法的快速演示。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="f827" class="ni lp iq ne b gy nj nk l nl nm">clustering = SpectralClustering(n_clusters=nb_clusters, assign_labels="discretize", random_state=0).fit(X)<br/>y_pred = clustering.labels_<br/>plt.title(f'Spectral clustering results ')<br/>plt.scatter(X[:, 0], X[:, 1], s=50, c = y_pred);</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mn"><img src="../Images/388c56443c1355a9b093be6e5091108e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIc4Ksd3fOzFHRFRT3qhlA.png"/></div></div></figure><p id="78a5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">众所周知，谱聚类是一种性能良好的技术，特别是在非高斯聚类的情况下，在这种情况下，最常见的聚类算法(如 K-Means)不能给出良好的结果。然而，需要给定预期的聚类数和相似性阈值的参数。</p><h1 id="1824" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated"><strong class="ak">自调谐光谱聚类</strong></h1><p id="5eb9" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">自调整谱聚类背后的思想是确定聚类的<strong class="kt ir">最佳数量</strong>以及在亲和矩阵的计算中使用的<strong class="kt ir">相似性度量</strong> <strong class="kt ir"> σi </strong>。</p><p id="dd9e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如本文<a class="ae nc" href="https://papers.nips.cc/paper/2619-self-tuning-spectral-clustering.pdf" rel="noopener ugc nofollow" target="_blank">所述</a>，亲和矩阵定义如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/c8a34eae69c3ba778e40584369885256.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*YgKmBEouzaf37lRJvAec5w.png"/></div></figure><p id="5aa0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其中 d(si，sj)是某个距离函数，通常只是向量 si 和 sj 之间的欧几里德距离。σ是比例参数，是点之间相似性的度量。通常是手动选择的。也可以通过使用不同的值多次运行聚类并选择产生最小失真聚类的值来自动设置。本文建议为每个数据点 si 计算局部缩放参数σi，而不是单个缩放参数。该论文提出分析每个点 si 的邻域，从而定义:<strong class="kt ir"> σi = d(si，sK) </strong>其中 sK 是点 si 的第 K 个邻域。下图说明了这一点，取自原始论文，K=7。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/5962fedb480ee1ae5fc681bfafbebbf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*t75UA4nbYBMh4amyBcaNHw.png"/></div></figure><p id="f50b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">具有局部缩放的亲和度矩阵可以如下实现:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/9bcc623eff0b83b0a6c617886608d9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*WRbwO7kgjWdXDlf_5Gqphg.png"/></div></figure><p id="e33a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">估计群集数量的第二种方法是分析特征值(L 的最大特征值将是数量为 1 的重复特征值，其重数等于组 C 的数量。这意味着可以通过计算等于 1 的特征值的数量来估计 C)。</p><p id="f8f5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如论文所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/5109f124e23b80131d4f3faeee057129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0SeKZtC3sgllvmyBMxhogQ.png"/></div></div></figure><p id="8f21" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">可以对特征向量进行另一种类型的分析，但这不在本文的讨论范围之内。</p><h1 id="b86c" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">寻找最佳聚类数的特征间隙启发式算法</h1><p id="a645" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">本文<a class="ae nc" href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Luxburg07_tutorial_4488%5B0%5D.pdf" rel="noopener ugc nofollow" target="_blank">是一篇关于谱聚类的教程——Ulrike von lux burg</a>提出了一种基于<strong class="kt ir">微扰理论</strong>和<strong class="kt ir">谱图论</strong>的方法来计算最佳聚类数。特征间隙启发法表明，聚类数 k 通常由使特征间隙(连续特征值之间的差)最大化的 k 值给出。这个特征间隙越大，理想情况的特征向量越接近，因此谱聚类工作得越好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/1007942145894aac168d92f4e057420b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W6e9KWe0uWNwEfCWPr_0dA.png"/></div></div></figure><p id="4c5b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这篇文章的代码库可以在<a class="ae nc" href="https://github.com/ciortanmadalina/high_noise_clustering/blob/master/spectral_clustering.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。</p><p id="15b0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">资源</strong></p><ol class=""><li id="1c74" class="mo mp iq kt b ku kv kx ky la mq le mr li ms lm mt mu mv mw bi translated"><a class="ae nc" href="https://www.youtube.com/watch?v=P-LEH-AFovE" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=P-LEH-AFovE</a></li><li id="8a39" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated"><a class="ae nc" href="https://papers.nips.cc/paper/2619-self-tuning-spectral-clustering.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/2619-self-tuning-spectral-clustering . pdf</a></li><li id="9456" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated"><a class="ae nc" href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Luxburg07_tutorial_4488%5b0%5d.pdf" rel="noopener ugc nofollow" target="_blank">http://www . kyb . mpg . de/file admin/user _ upload/files/publications/attachments/luxburg 07 _ tutorial _ 4488% 5b 0% 5d . pdf</a></li></ol></div></div>    
</body>
</html>