<html>
<head>
<title>How BERT leverage attention mechanism and transformer to learn word contextual relations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BERT 如何利用注意机制和转换器学习单词上下文关系</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb?source=collection_archive---------12-----------------------#2019-01-06">https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb?source=collection_archive---------12-----------------------#2019-01-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c6a6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">伯特简介</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5b821e25c3f9b9fa9fd5c13112065654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PJTsRgCNN2YrP_N9"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@weareambitious?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ambitious Creative Co. - Rick Barrett</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="630e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" rel="noopener" target="_blank" href="/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f"> ELMo </a>(来自语言模型的嵌入)和开放 AI <a class="ae ky" rel="noopener" target="_blank" href="/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b"> GPT </a>(生成式预训练转换器)之后，谷歌发布了一份新的最先进的 NLP 论文。他们将这种方法称为 BERT(来自变压器的双向编码器表示)。</p><p id="2b9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">开放人工智能 GPT 和伯特都使用变压器架构来学习文本表示。其中一个区别是 BERT 使用双向转换器(从左到右和从右到左方向)而不是双向转换器(从左到右方向)。另一方面，两者都使用双向语言模型来学习文本表示。然而，ELMo 使用浅层连接层，而 BERT 使用深层神经网络。</p><p id="67f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看完这篇帖子，你会明白:</p><ul class=""><li id="a628" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">伯特设计与建筑</li><li id="a74d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">模特培训</li><li id="585c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">实验</li><li id="7f06" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">履行</li><li id="3f92" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">拿走</li></ul><h1 id="8442" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">伯特设计与建筑</h1><h2 id="7180" class="nb mk it bd ml nc nd dn mp ne nf dp mt li ng nh mv lm ni nj mx lq nk nl mz nm bi translated">输入表示</h2><p id="ba3a" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">BERT 使用三种嵌入来计算输入表示。它们是标记嵌入、片段嵌入和位置嵌入。“CLS”是表示序列开始的保留标记，而“SEP”是分隔片段(或句子)的标记。这些输入是</p><ul class=""><li id="ae58" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">令牌嵌入:一般单词嵌入。简而言之，它用向量来表示 token(或 word)。你可以查看这个<a class="ae ky" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">故事</a>的细节。</li><li id="aa79" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">另一个词中的句子嵌入。如果输入包括两个句子，相应的句子嵌入将被分配给特定的单词。如果输入只包括一个句子，将使用一个且只有一个句子嵌入。在计算 BERT 之前学习片段嵌入。对于句子嵌入，你可以查看这个<a class="ae ky" rel="noopener" target="_blank" href="/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c">故事</a>的细节。</li><li id="8477" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">位置嵌入:指输入的标记序列。即使有 2 句话，也会累积位置。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/f8e04819a05bd70f45383f55b2ca5859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uofj0P5k41L-VRwbCNmRKA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">BERT Input Representation (Devlin et al., 2018)</figcaption></figure><h2 id="f32c" class="nb mk it bd ml nc nd dn mp ne nf dp mt li ng nh mv lm ni nj mx lq nk nl mz nm bi translated">培训任务</h2><p id="8a1a" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">说完输入表示，我就来介绍一下 BERT 是怎么训练的。它使用两种方式来实现它。第一个训练任务是掩蔽语言模型，而第二个任务是预测下一句话。</p><blockquote class="nt nu nv"><p id="d723" class="kz la nw lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated">掩蔽语言模型</p></blockquote><p id="a7b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个预训练任务是利用掩蔽语言模型(Masked LM)。与传统的方向模型不同，BERT 使用双向作为预训练目标。如果使用传统的方法来训练一个双向模型，每个单词将能够间接地看到“它自己”。因此，伯特使用了掩蔽语言模型(MLM)方法。通过随机屏蔽一些标记，使用其他标记来预测那些被屏蔽的标记以学习表示。与其他方法不同，BERT 预测屏蔽令牌而不是整个输入。</p><p id="8657" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，实验随机抽取 15%的令牌进行替换。然而，也有一些缺点。第一个缺点是掩码标记(实际标记将被该标记替换)在微调阶段和实际预测中不会出现。因此，Devlin 等人，所选择的用于屏蔽的令牌不会总是被屏蔽</p><ul class=""><li id="5e1c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">答:80%的时候，它会被替换为[MASK] token</li><li id="4c12" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">B: 10%的时候，它会被其他实际的代币代替</li><li id="d41e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">C: 10%的时间，它会保持原样。</li></ul><p id="abf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比如原句是“我在学 NLP”。假设“NLP”是用于屏蔽的选定令牌。然后 80%的时候会表现为“我在学【面具】(场景 A)。”我用 10%的时间学习 OpenCV(场景 B)。其余 10%的时间，它将显示为“我正在学习 NLP”(场景 C)。尽管随机替换(场景 B)会发生并可能损害句子的意思。但是它只有 1.5%(仅屏蔽了整个数据集中的 15%的标记和这 15%中的 10%)，作者认为它不会损害模型。</p><p id="0e35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个缺点是每批只有 15%的令牌被屏蔽(预测),训练需要更长的时间。</p><blockquote class="nt nu nv"><p id="77fe" class="kz la nw lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated">下一句预测</p></blockquote><p id="d186" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二个预训练任务是预测下一句话。这种方法克服了第一个任务的问题，因为它不能学习句子之间的关系。目标很简单。只区分第二句是不是下一句。举个例子，</p><p id="5f72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入 1:我正在学习 NLP。</p><p id="4650" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入 2: NLG 是 NLP 的一部分。</p><p id="1b1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预期的输出是 isNextSentence 或 notNextSentence。</p><p id="e036" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在为该任务生成训练数据时，将随机选择 50%的“notNextSentence”数据。</p><h1 id="5e7e" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">模特培训</h1><p id="7869" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">在 BERT 中采用两阶段训练。使用通用数据集执行第一次训练，并通过提供领域特定数据集对其进行微调。</p><h2 id="b5de" class="nb mk it bd ml nc nd dn mp ne nf dp mt li ng nh mv lm ni nj mx lq nk nl mz nm bi translated">训练前阶段</h2><p id="4040" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">在预训练阶段，从图书语料库(800 万字)(朱等，2015)和英文维基百科(2500 万字)中提取句子。</p><ul class=""><li id="8040" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">屏蔽 LM:每个序列将使用 512 个标记(2 个连接的句子)，每批有 256 个序列。设定大约 40 个时期来训练模型。该配置是:</li><li id="b308" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">学习率为 1e-4 的 Adam，β1 = 0.9，β2 = 0.999</li><li id="1b29" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">L2 重量衰减 0.01</li><li id="8bbc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">所有层的压差为 0.1</li><li id="fe42" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用 gelu 进行激活</li></ul><p id="9e2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，选择两个句子用于“下一句预测”预训练任务。另一个句子被随机选取并标记为“不是下一个句子”的概率为 50%,而另一个句子实际上是下一个句子的概率为 50%。</p><p id="57ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一步由谷歌研究团队完成，我们可以利用这个预先训练的模型，根据自己的数据进一步微调模型。</p><p id="1c2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nw">微调阶段</em> </strong></p><p id="6e66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在预训练阶段，仅改变了一些模型超参数，如批量大小、学习速率和训练次数，大多数模型超参数保持不变。在实验过程中，以下数值范围适用于各种任务:</p><ul class=""><li id="5033" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">批量:16 个，32 个</li><li id="cdff" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">学习率:5e-5，3e-5，2e-5</li><li id="dc47" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">历元数:3，4</li></ul><p id="ff05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">微调程序不同，取决于下游任务。</p><blockquote class="nt nu nv"><p id="644f" class="kz la nw lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated">分类</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/4aca81429e4db87af06e6f51a1e656ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*elCEAYhmi3UtSpBQeBFp6w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Single Sentence Classification Task (Devlin et al., 2018)</figcaption></figure><p id="4c3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于[CLS]令牌，它将作为最终隐藏状态被馈送。标签(C)概率用 softmax 计算。之后，对其进行微调，以最大化正确标签的对数概率。</p><blockquote class="nt nu nv"><p id="a598" class="kz la nw lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated">命名实体识别</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/0af652a493bcc84e4e5376483426a7f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*gjG8lrs18Z50iR8b1EOEpA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">NER Task (Devlin et al., 2018)</figcaption></figure><p id="371e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">令牌的最终隐藏表示将被馈送到分类层。预测时将考虑周围的单词。换句话说，分类只关注令牌本身，而没有条件随机场(CRF)。</p><h1 id="603d" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">实验</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/1b5ec0b84cb5e0d695fc11fd7520f307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1ksfLuI3vHxlWCJ0"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@_louisreed?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Louis Reed</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dbb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，与其他最先进的 NLP 模型相比，BERT 提供了最好的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/c4f838fe82db6f09354edc185a243925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qtt3bVG-3D9JSaciDrvP3A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Experiment Result on GLUE dataset (Devlin et al., 2018)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/b092374edaf9d1178d98595667af0118.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*iPDLrFbs7VKr7FDFaR8V4g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Experiment Result on SQuAD (Devlin et al., 2018)</figcaption></figure><h1 id="0e54" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">履行</h1><h2 id="f30d" class="nb mk it bd ml nc nd dn mp ne nf dp mt li ng nh mv lm ni nj mx lq nk nl mz nm bi translated">微调模型(再现实验)</h2><p id="fca8" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">在微调特定领域的数据集之前，我更喜欢先重现实验结果。你可以访问<a class="ae ky" href="https://github.com/google-research/bert#sentence-and-sentence-pair-classification-tasks" rel="noopener ugc nofollow" target="_blank">官方页面</a>或者跟随它的指示</p><ul class=""><li id="60d9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">执行这个<a class="ae ky" href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e" rel="noopener ugc nofollow" target="_blank">脚本</a>来下载数据集</li><li id="8308" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">下载<a class="ae ky" href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank">预培训的</a>型号(选定的“BERT-Base，Uncased”型号)</li><li id="44c6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">分配环境变量</li></ul><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="5230" class="nb mk it og b gy ok ol l om on">export BERT_BASE_DIR=/downloaded_model_path/bert<br/>export GLUE_DIR=/downloaded_data_path/glue<br/>export BERT_OUTPUT_DIR=/trained/model/bert/</span></pre><ul class=""><li id="8ea1" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">执行以下命令开始微调</li></ul><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="4e46" class="nb mk it og b gy ok ol l om on">python run_classifier.py \<br/>  --task_name=MRPC \<br/>  --do_train=true \<br/>  --do_eval=true \<br/>  --data_dir=$GLUE_DIR/MRPC \<br/>  --vocab_file=$BERT_BASE_DIR/vocab.txt \<br/>  --bert_config_file=$BERT_BASE_DIR/bert_config.json \<br/>  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \<br/>  --max_seq_length=128 \<br/>  --train_batch_size=32 \<br/>  --learning_rate=2e-5 \<br/>  --num_train_epochs=3.0 \<br/>  --output_dir=$BERT_OUTPUT_DIR</span></pre><ul class=""><li id="131b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">我用了一台 20 核 CPU 的机器来重现它，花了大约一个小时来完成微调。</li></ul><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="0de7" class="nb mk it og b gy ok ol l om on">INFO:tensorflow:***** Eval results *****<br/>INFO:tensorflow:  eval_accuracy = 0.84313726<br/>INFO:tensorflow:  eval_loss = 0.5097478<br/>INFO:tensorflow:  global_step = 343<br/>INFO:tensorflow:  loss = 0.5097478</span></pre><h2 id="4865" class="nb mk it bd ml nc nd dn mp ne nf dp mt li ng nh mv lm ni nj mx lq nk nl mz nm bi translated">提取固定向量</h2><p id="e9ee" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">而不是针对特定数据集微调预训练模型。我们还可以为下游任务提取一个固定的向量，这样更容易。这类似于埃尔莫所做的。</p><p id="f1ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以访问<a class="ae ky" href="https://github.com/google-research/bert#using-bert-to-extract-fixed-feature-vectors-like-elmo" rel="noopener ugc nofollow" target="_blank">官方页面</a>或跟随其指示</p><ul class=""><li id="b802" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">生成一个样本文件到当前方向</li></ul><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="aef8" class="nb mk it og b gy ok ol l om on">echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' &gt; input.txt</span></pre><ul class=""><li id="37f0" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">执行以下命令提取字符向量</li></ul><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="3fdc" class="nb mk it og b gy ok ol l om on">python extract_features.py \<br/>  --input_file=input.txt \<br/>  --output_file=$BERT_OUTPUT_DIR/output.jsonl \<br/>  --vocab_file=$BERT_BASE_DIR/vocab.txt \<br/>  --bert_config_file=$BERT_BASE_DIR/bert_config.json \<br/>  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \<br/>  --layers=-1,-2,-3,-4 \<br/>  --max_seq_length=128 \<br/>  --batch_size=8</span></pre><ul class=""><li id="086b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">输出文件包含以下对象</li></ul><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="173a" class="nb mk it og b gy ok ol l om on">- features<br/>  - token: Token value (e.g. Who)<br/>  - layers<br/>    - index: Layer number (from -1 to -4) per token<br/>    - values: Vector values. Default model dimension is 768</span></pre><h2 id="5bb7" class="nb mk it bd ml nc nd dn mp ne nf dp mt li ng nh mv lm ni nj mx lq nk nl mz nm bi translated">参数</h2><p id="b6b3" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">如果我们更多地了解如何改变参数，这将是有益的。以下是一些有用的参数解释:</p><p id="8265" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oo op oq og b">data_dir</code>:数据方向</p><p id="0d1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oo op oq og b">task_name</code>:具体用什么任务。特定任务处理器已准备就绪。可能的<code class="fe oo op oq og b">task_name</code>有“可乐”、“mnli”、“mrpc”、“xnli”。您可以通过扩展<code class="fe oo op oq og b">DataProcessor</code>类来实现自己的数据处理器。</p><p id="4940" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oo op oq og b">do_train</code>:包含训练步骤。必须启用<code class="fe oo op oq og b">do_train</code>、<code class="fe oo op oq og b">do_eval</code>或<code class="fe oo op oq og b">do_test</code>中的任何一个。</p><p id="908f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oo op oq og b">do_eval</code>:包含评估步骤。必须启用<code class="fe oo op oq og b">do_train</code>、<code class="fe oo op oq og b">do_eval</code>或<code class="fe oo op oq og b">do_test</code>中的任何一个。</p><p id="5f3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oo op oq og b">do_test</code>:包含测试步骤。必须启用<code class="fe oo op oq og b">do_train</code>、<code class="fe oo op oq og b">do_eval</code>或<code class="fe oo op oq og b">do_test</code>中的任何一个。</p><h1 id="d0ef" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae ky" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="a986" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><p id="9b12" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">Devlin J .，Chang M. W .，Lee K .，Toutanova K .，2018 年。<a class="ae ky" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a></p><p id="a1a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">张量流中的伯特</a>(原创)</p><p id="ac1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/huggingface/pytorch-pretrained-BERT" rel="noopener ugc nofollow" target="_blank">伯特在 PyTorch </a></p><p id="c079" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/soskek/bert-chainer" rel="noopener ugc nofollow" target="_blank">伯特在链器里</a></p><p id="cf8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a"> word2vec，glove 和 fastText Story(单词嵌入</a>)</p><p id="f63b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c">跳过思想故事(句子嵌入)</a></p><p id="6267" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f"> ELMo 的故事</a></p><p id="8df4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/named-entity-recognition-3fad3f53c91e"> NER 的故事</a></p><p id="85f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="http://peterbloem.nl/blog/transformers" rel="noopener ugc nofollow" target="_blank">变压器详解</a></p></div></div>    
</body>
</html>