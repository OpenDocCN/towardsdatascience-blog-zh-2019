<html>
<head>
<title>Understand Self-Attention in BERT Intuitively</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">直观地理解伯特的自我关注</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-self-attention-in-bert-intuitively-cd480cbff30b?source=collection_archive---------8-----------------------#2019-07-31">https://towardsdatascience.com/understand-self-attention-in-bert-intuitively-cd480cbff30b?source=collection_archive---------8-----------------------#2019-07-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="179d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释什么是自我关注中的查询向量、关键向量和价值向量，以及它们是如何工作的</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7f38e23c255e170ed41a425f17707932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kHud9r76PaIuu3Spb2JdKQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@stefanbc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Stefan Cosma</a> on <a class="ae ky" href="https://unsplash.com/search/photos/attention?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="bbde" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">先决条件</h1><p id="cbe9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本文的目标是进一步解释什么是自我关注中的查询向量、关键向量和价值向量。如果你忘记了一些概念，你可以通过阅读<a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">插图变压器</a>和<a class="ae ky" href="https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3" rel="noopener">解剖伯特第一部分:编码器</a>来回忆。</p><h1 id="55c4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是自我关注</h1><p id="a0ea" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在插图变压器的<a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">中的“高层次的自我关注”部分，它在跳到代码之前给出了一个非常清晰的概念。</a></p><blockquote class="mn mo mp"><p id="2aad" class="lr ls mq lt b lu mr ju lw lx ms jx lz mt mu mc md mv mw mg mh mx my mk ml mm im bi translated">当模型处理每个单词(输入序列中的每个位置)时，自我注意允许它查看输入序列中的其他位置，以寻找有助于更好地编码该单词的线索。</p></blockquote><p id="921b" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">让我重新措辞一下我的话。当模型处理一个句子时，<strong class="lt iu">自我注意允许句子中的每个单词查看其他单词，以更好地知道哪个单词对当前单词有贡献。</strong>更直观地说，我们可以认为<strong class="lt iu">“自我关注”</strong>是指句子会<strong class="lt iu">审视自身，以确定如何表示每个标记。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/715483fd362652d65c542efff18d8d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/0*5jF2DJuEEqrIxjI-.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">The Illustrated Transformer</a></figcaption></figure><p id="8a4d" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">例如，当模型处理上图右栏中的单词<code class="fe na nb nc nd b">it</code>时，自我注意查看其他单词以便更好地编码(理解<em class="mq">)当前单词<code class="fe na nb nc nd b">it</code>。这就是伯特如何根据上下文(句子)理解每个单词的神奇之处。<strong class="lt iu">一个单词在不同的句子(上下文)中可以有不同的含义，自我关注可以基于上下文单词为当前单词编码(理解)每个单词。</strong></em></p><h1 id="c3cf" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">直观理解自我关注中的 Q，K，V</h1><p id="9aae" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我将用上面两篇文章中的例子和图表来解释什么是 Q，K，v。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/effd0ab12a4e5184712595121dfd246c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/0*ogd2iGlkBBoYNL5E.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">taken from <a class="ae ky" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a></figcaption></figure><p id="5ba0" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">我们更简单地改写上述自我注意功能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nf"><img src="../Images/1351a273086bf729934f3af06b77cef0.png" data-original-src="https://miro.medium.com/v2/format:webp/1*V6LGUR-0NmlOGmm0TDAa5g.png"/></div></figure><p id="43a1" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">Q、K、V 分别称为查询、键、值。不要被他们的名字搞糊涂了。唯一的区别是三个代表的权重不同。你可以在下图中看到。</p><p id="2de2" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">但是直观上，我们可以认为查询(Q)代表我们在寻找什么样的信息，键(K)代表与查询的相关性，值(V)代表输入的实际内容。因为它们在模型中扮演不同的角色。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/be4461f3f0c3452d4c5a60deb19c6082.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/0*_nyMI3ri4aUTxlEw.png"/></div></figure><p id="20a0" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">这里 X 矩阵中的每一行表示一个记号的嵌入向量，所以这个 X 矩阵有 2 个记号。我们使用三个权重<code class="fe na nb nc nd b">W^Q, W^K, W^V</code>乘以相同的输入 X 得到 Q，K，v</p><h1 id="5cd1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">为什么要计算查询(Q)和键(K)的点积</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nf"><img src="../Images/1351a273086bf729934f3af06b77cef0.png" data-original-src="https://miro.medium.com/v2/format:webp/1*V6LGUR-0NmlOGmm0TDAa5g.png"/></div></figure><p id="afe2" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">首先，点积的物理意义可以表示两个向量之间的相似性。所以你可以认为 Q 和 K 的点积是为了得到不同 tokens 之间的<strong class="lt iu">相似度(注意力得分)</strong>。比如我们有一个句子，<code class="fe na nb nc nd b">Hello, how are you?</code>。句子长度为 6，嵌入维数为 300。所以 Q，K，V 的大小都是<code class="fe na nb nc nd b">(6, 300)</code>。</p><p id="9e93" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">Q 和 K 的矩阵相乘如下图(softmax 之后)。矩阵乘法是点生成的快速版本。但是基本思想是相同的，计算任意两个标记对之间的注意力分数。</p><p id="809e" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">注意力得分的大小是(6，6)。您可能已经注意到，关注分数的大小只与序列长度有关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/286767b31f1421d812e02048ed4669b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OAqt6LKykbUodxy9.png"/></div></figure><p id="2e96" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">每条线代表一个记号和其他记号之间的相似性。当当前令牌是<code class="fe na nb nc nd b">Hello</code>时，我们只需查看关注度矩阵中的第一行，就可以知道从<code class="fe na nb nc nd b">Hello</code>到其他令牌的相似度。比如<code class="fe na nb nc nd b">Hello</code>和<code class="fe na nb nc nd b">how</code>的相似度是<code class="fe na nb nc nd b">6.51*10e-40</code>。我们添加 softmax 来标准化数据。</p><h1 id="4a96" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">为什么我们需要值(V)？</h1><p id="f19a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">注意力得分矩阵可以表示任意两个标记对之间的相似性，但是由于缺少嵌入向量，我们不能用它来表示原句。这就是为什么我们需要 V，这里 V 还是代表原句。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/2af5c8ffd94376a704988006a721ed8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*1Pvqym1Kgw3r2YQs4WRRzw.png"/></div></figure><p id="11fb" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">我们用注意力得分(6x6)乘以 V(6300)，得到一个加权求和结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/4133ec09e2f5302b413ce48cee108f4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*B3Ifxeq8EqhukWytEwZFcw.png"/></div></figure><p id="b735" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">直观上，我们用嵌入向量来表示每个记号，并且我们假设它们彼此不相关。但是在与关注分数相乘之后，在 V 中嵌入向量的每个令牌(“Hello”)将根据从当前(“Hello”)到其他令牌的关注来调整其在每个维度(列)中的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/58ea6b5d3cb7c4123fce010fc9a146f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GZPqp4_y0T-Hkba2Bq4yKw.jpeg"/></div></div></figure><p id="37e2" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">我们可以认为关注度矩阵是一个<strong class="lt iu">过滤矩阵</strong>，可以让价值矩阵<strong class="lt iu">更加关注那些重要的词，而对不重要的词</strong>关注很少。</p><h1 id="cdcd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">为什么我们用不同的权重来得到 K 和 Q？</h1><p id="a15f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一些读者可能想知道为什么我们必须使用不同的权重来得到 K 和 Q？换句话说，我们可以通过 K 和 K 或者 Q 和 Q 的矩阵相乘来计算注意力得分，一个原因是两种计算不会有太大的差别，后一种也可以减少训练时的参数。</p><p id="5008" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">我觉得用不同的权重来得到 K 和 Q 是为了提高模型的泛化能力。因为有两个权重 W_K 和 W_Q，把同一个 X 投影到不同的空间。</p><p id="c763" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">如果我们用 K 和 K 之间的矩阵乘法，你会发现关注度得分矩阵会变成一个<a class="ae ky" href="https://www.wikiwand.com/en/Symmetric_matrix" rel="noopener ugc nofollow" target="_blank">对称矩阵</a>。因为它用了一个权重 W_K，把 X 投影到同一个空间。所以泛化能力会下降。在这种情况下，注意力分数过滤效果对于 value (V)来说不会很好。</p><blockquote class="mn mo mp"><p id="8773" class="lr ls mq lt b lu mr ju lw lx ms jx lz mt mu mc md mv mw mg mh mx my mk ml mm im bi translated"><strong class="lt iu"> <em class="it">查看我的其他帖子</em> </strong> <a class="ae ky" href="https://medium.com/@bramblexu" rel="noopener"> <strong class="lt iu"> <em class="it">中等</em> </strong> </a> <strong class="lt iu"> <em class="it">同</em> </strong> <a class="ae ky" href="https://bramblexu.com/posts/eb7bd472/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> <em class="it">一分类查看</em> </strong> </a> <strong class="lt iu"> <em class="it">！<br/>GitHub:</em></strong><a class="ae ky" href="https://github.com/BrambleXu" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="it">bramble Xu</em></strong></a><strong class="lt iu"><em class="it"><br/>LinkedIn:</em></strong><a class="ae ky" href="https://www.linkedin.com/in/xu-liang-99356891/" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="it">徐亮</em> </strong> </a> <strong class="lt iu"> <em class="it"> <br/>博客:</em></strong><a class="ae ky" href="https://bramblexu.com" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="it">bramble Xu</em></strong></a></p></blockquote><h1 id="529b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><div class="nl nm gp gr nn no"><a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">图示的变压器</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">讨论:黑客新闻(65 分，4 条评论)，Reddit r/MachineLearning (29 分，3 条评论)翻译…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">jalammar.github.io</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc ks no"/></div></div></a></div><div class="nl nm gp gr nn no"><a href="https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3" rel="noopener follow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">剖析 BERT 第 1 部分:编码器</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">这是由米盖尔·罗梅罗和弗朗西斯科·英厄姆共同撰写的《理解伯特》的 1/2 部分。如果你已经明白了…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">medium.com</p></div></div><div class="nx l"><div class="od l nz oa ob nx oc ks no"/></div></div></a></div><div class="nl nm gp gr nn no"><a href="https://medium.com/@Alibaba_Cloud/self-attention-mechanisms-in-natural-language-processing-9f28315ff905" rel="noopener follow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">自然语言处理中的自我注意机制</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">在过去的几年里，注意机制在各种自然语言处理中得到了广泛的应用</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">medium.com</p></div></div><div class="nx l"><div class="oe l nz oa ob nx oc ks no"/></div></div></a></div><div class="nl nm gp gr nn no"><a href="https://kexue.fm/archives/4765" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi">《Attention is All You Need》浅读（简介+代码） — 科学空间|Scientific Spaces</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk">2017 年中，有两篇类似同时也是笔者非常欣赏的论文，分别是 FaceBook 的《Convolutional Sequence to Sequence Learning》和 Google 的《Atten…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">科学. fm</p></div></div></div></a></div></div></div>    
</body>
</html>