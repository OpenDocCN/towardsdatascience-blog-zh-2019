# 机器学习算法中的参数。

> 原文：<https://towardsdatascience.com/parameters-in-machine-learning-algorithms-ba3e3f0e49a?source=collection_archive---------10----------------------->

![](img/4f37e080fa8aa7aec5f6cde04c38ae67.png)

Pic Credit: [https://mljar.com/blog](https://mljar.com/blog)

## 理解 ML 算法的初学者指南。

在我与海得拉巴[ISB](https://www.linkedin.com/school/indian-school-of-business/)的交往中，我有幸成为[沙伊莱什·库马尔](https://www.linkedin.com/in/shaileshk/)的学生。Shailesh 教授对于如何定义一名成功的数据科学家有着独特的观点:

*   数据科学家能够写出针对给定问题必须优化的目标函数。
*   数据科学家能够理解在求解目标函数时需要学习的自由参数的数量。
*   数据科学家能够理解控制模型复杂性的旋钮(或超参数)。

我写这篇文章是为了那些想了解参数在 ML 算法中的作用的人。需要求解的参数数量将直接影响训练过程的时间和输出。下面的信息对那些理解 ML 中各种算法的人是有用的。

*   *降维方法*

**主成分分析:**

PCA 用于输入数据的降维，方便快捷。PCA 算法的输出是在向量空间中正交的数据集。PCA 的目标函数可以写成 *argmax{W'CW}* 其中 C 是输入数据的协方差矩阵，它是对称的、半正定的。为 *W* 求解上述函数将导致 *W* 是矩阵 *C* 的特征向量。设数据是一个 *d 维*矩阵。 *C* 将会是*d * d .*PCA 中的参数数量由总共处于最大值*‘d’*的特征向量的数量给出。每个特征向量的维数为*‘1xd’，*，因此需要估计的总参数为*d * dx1 = d。*许多软件包也给出了特征值，即每个主成分解释的方差。由于所有特征值的总和必须等于数据中的总方差，因此存在用于估计特征值的 *d-1* 自由参数。旋钮是我们在不损失太多方差的情况下需要考虑的主成分数 *(k)* 。例如:MNIST 数据集中每个字符的数据被安排在一个 28×28 的图像中，该图像构成一个长度为 784 的向量。这一幅图像的协方差矩阵大小为 784x784，因此参数总数为 784*784+783。

**多维标度(MDS):**

MDS 的目标是将高维数据投射到低维表面。对于每对观察值，相似性距离*δ*作为算法的输入给出。结果将是一个 *x* 维空间中每个数据点的坐标向量。目标函数是最小化在 *x 维*空间中的投影距离 *delta_x* 与数据中每对点之间的实际距离 *delta* 的误差。.即 *argmin {(delta_x — delta) }。的编号。要估计的参数是的数量。数据点* *x* (您想要投影的尺寸)。举个例子:如果你想把 5 种不同的菜系投射到一个二维空间中。参数= 5*2 = 10。旋钮就是 *x.* 的大小*

*   *无监督学习方法*

**K 均值聚类:**

问题是为给定的输入数据集找到 K 个 T21 代表。这些代表被称为聚类中心(或)质心，并且被选择为使得在同一聚类中从每个点到其质心的距离最小。目标函数是 *argmin I(k)*{(x-m(k)) }* 其中 *I(k)* 是一个点属于聚类的指示函数 *k.* 模型参数除了聚类质心向量之外什么都不是。如果输入数据集为 *d* 维，则参数总数为 *k*d.* 旋钮为 *k* 的值，该值必须作为超级参数传递给算法。

**Parzen 窗口:**

Parzen 窗口是一种估计单个随机变量(单变量数据)密度的技术。数据的密度只不过是给定数据的真实概率密度函数(pdf)的近似值。然后汇总每个点的 Parzen 窗口估计值，以获得数据的密度估计值。目标函数是计算 *p(x) = SUM(k(x))。*在该模型中没有要学习的自由参数，但是您为每个数据点分配一个高斯(影响区域)，该高斯被称为核函数，其均值(即以数据点为中心)和方差( *sigma* )在定义核时已经指定。旋钮是 *sigma* 的值，它是 parzen 窗口算法的超级参数。

**单变量正态(Uni 高斯):**

UVN 建模基于基本假设，即输入数据仅由一维组成，其*均值(μ)和方差(σ)*将采用高斯概率密度函数(pdf)进行估计。然而，与上述方法不同，模型参数实际上是通过最大化(或最小化负值)似然函数或其对数似然函数来学习的。假设输入数据是独立同分布的样本。目标函数是*arg min-{ prod(1/sqrt(2 * pi * sigma)* e^-(x-mu)/sigma)}。*自由参数为*μ*和*σ。*这款没有旋钮。

**多变量正态分布(MVN/高斯混合分布):**

在上述模型中，用多变量数据集替换单变量数据，我们得到一个多变量正态分布数据，它具有一个完整的协方差矩阵 *(sigma)* 和一个均值向量 *(mu)。*目标是最大化给定输入数据集上的似然函数，假设多变量高斯分布的 pdf 由这里的[给出。](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)

对于一个 *d 维的输入数据，*协方差矩阵 *(sigma)* 在上三角形区域将有 *d*(d-1)/2* 个条目，对角线上有 *d* 个条目。估计协方差矩阵将涉及学习 *d*(d-1)/2 + d* 自由参数。估计平均向量*(μ)*需要学习 *d* 参数。因此，自由参数的总数为 *d*(d-1)/2 + 2d。*该型号没有任何旋钮。

*   *监督学习方法*

**感知器:**

一个简单的感知是一个单细胞神经元，可以在一个 *n* 维特征空间中分离两类。感知器是可以模拟两个类别之间的边界线(或平面)的区别分类器的一个例子。这条线的函数可以写成 *y = h(w'x+b)。*参数是神经元的权重( *w 和 b* )，总计 *n+1。*目标是最小化预期分类误差 aka as loss，可写成 *-SUM(y*log(h(w'x+b))。*计算损失函数的梯度，并使用梯度下降更新权重。模型的旋钮是 GD 算法中使用的*学习率(lr)* 。

**逻辑回归:**

逻辑回归的形式类似于感知器，即它可以解决两类问题。所使用的激活函数是由*h(w ' x+b)= 1/1+e^-(w'x+b).给出的*s 形**其余论点同上。

**神经网络:**

神经网络中的每个节点都可以理解为一个单独的逻辑回归。前馈神经网络被完全连接。在具有 2 个隐藏层的神经网络中，每个隐藏层具有 5 个神经元，参数的总数将是*5 *(n+1)+(5 * 5)+5 *输出*。目标函数是使用*交叉熵*损失最小化分类误差。使用从输出到输入的每个连续层的误差梯度的反向投影来调整权重。模型的旋钮或复杂性是隐藏层的数量和每个隐藏层中的单元的数量，这是设计时与学习速率(如果使用梯度下降来解决优化问题)一起考虑的因素，学习速率是超参数。

**朴素贝叶斯分类器:**

与上面不同，NB 是一个生成式分类器。朴素贝叶斯分类器的关键假设是特征是类条件独立的。NB 分类器对条件概率的贝叶斯公式起作用，即 *p(类/数据)~ p(类)* p(数据/类)。p(数据/类别)*根据关键假设进行估算。 *p(x1，x2，x3…/c)~ p(x1/c)* p(x2/c)* p(x3/c)…*的编号。估计 *p(x/c)* 所需的参数取决于特征的类型，即分类特征或数字特征。如果特征是分类的，那么你需要为它的所有级别建立概率值 *(l)* 。的编号。自由参数是( *l-1) * c …(l-1，因为所有级别的概率加起来是 1)。*如果特征是数字，那么你需要估计基础分布的参数，例如高斯分布的均值和方差。因此参数的数量会根据输入数据集而变化。没有最小化或最大化的目标，你只需要计算条件概率，建立先验，并使用贝叶斯规则对测试数据进行分类。然而，可以使用上面给出的相同目标函数，例如铰链损失或交叉熵损失，来调整后验概率，以更接近地反映基本事实。这种算法缺乏对模型复杂性的控制。

**K-最近邻:**

KNN 是一种懒惰的算法，也就是说，当一个需要分类的新数据点被呈现给算法时，它在推理时完成大部分工作。基于给定距离的阈值内的最近数据点，新数据点将被多数类标签分类。旋钮或模型复杂度是阈值距离，它是一个超参数。没有目标函数或参数。

**支持向量机:**

SVM 是一种特殊类型的判别分类器，其目标是最大化给定类别对之间的决策边界。最大化函数可以使用向量代数来导出为 *1/2*||w||，*，其中可以假设 *w* 是等式*y *(w’x-b)-1>= 0，*中的参数向量，该等式是可以分离给定类别对的线(或超平面)的等式。的编号。对于 *d-* 维输入数据集，需要求解的参数为 *d+1* 。旋钮或复杂度由成本参数(被认为是以升为单位的 *gamma* )给出。)这将允许对可能导致复杂的过拟合决策边界(当采用非线性*内核*时)的训练数据集点 San 的错误分类的一些容忍。