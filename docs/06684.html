<html>
<head>
<title>DeepLabv3: Semantic Image Segmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepLabv3:语义图像分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deeplabv3-c5c749322ffa?source=collection_archive---------18-----------------------#2019-09-24">https://towardsdatascience.com/deeplabv3-c5c749322ffa?source=collection_archive---------18-----------------------#2019-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="86d5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">来自谷歌的作者扩展了先前的研究，使用最先进的卷积方法来处理不同尺度图像中的对象[1]，在语义分割基准上击败了最先进的模型。</em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/17e6ca15a8a39c1dec565fdba616fb29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDzDAcCO8LgBHwfXxtDy8g.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">From Chen, L.-C., Papandreou, G., Schroff, F., &amp; Adam, H., 2017 [1]</figcaption></figure><h1 id="9f33" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">介绍</h1><p id="dc6f" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">使用深度卷积神经网络(DCNNs)分割图像中的对象的一个挑战是，随着输入特征图通过网络变得越来越小，关于更小尺度的对象的信息可能会丢失。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mi"><img src="../Images/989cefc33c70f038bd399b0f077efa11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rVUKcFDBQ1WzK4b4"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Figure 1. The repeated combination of pooling and striding reduces the spatial resolution of the feature maps as the input traverses through the DCNN.</figcaption></figure><p id="26a3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">DeepLab 的贡献在于引入了<em class="ko"> atrous 卷积、</em>或 expanded 卷积，以提取更密集的特征，从而更好地保留给定不同尺度对象的信息【2–3】。Atruos 卷积还有一个额外的参数<em class="ko"> r </em>，称为 ATR uos 速率，它对应于输入信号的采样步幅。在图 2 中，这相当于沿着每个空间维度在两个连续的滤波器值之间插入<em class="ko"> r </em> -1 个零。在这种情况下，<em class="ko"> r=2 </em>，因此每个滤波器值之间的零值数量为 1。这种方法的目的是灵活地修改滤波器的视野，并修改计算特征的密度，所有这些都通过改变<em class="ko"> r </em>来实现，而不是学习额外的参数。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/792026b9a6b2bb2743321182f2d10328.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*1lLBZCk0vDvMmbleR0ymHw.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Equation 1. The formulation for the location <em class="mk">i in the output feature map y, or in other words, one of the squares in the green matrix in Figure 2. x is the input signal, r is the atrous rate, w is the filter and k is the kernel.</em></figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/62328676b3369f347fe03fb3a143ed51.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*eGRy1Ydrts4Nibsh.gif"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Figure 2. A atruous 2D convolution using a 3 kernel with a atrous rate of 2 and no padding. From “<a class="ae mm" rel="noopener" target="_blank" href="/types-of-convolutions-in-deep-learning-717013397f4d">An Introduction to Different Types of Convolutions in Deep Learning</a>” by <a class="ae mm" href="https://towardsdatascience.com/@pietz?source=post_page-----717013397f4d----------------------" rel="noopener" target="_blank">Paul-Louis Pröve</a></figcaption></figure><p id="125a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作者调整了一个名为<em class="ko"> output_stride，</em>的值，它是输入图像分辨率和输出分辨率之间的比率。他们比较并结合了三种方法来创建他们的最终方法:<strong class="js iu">阿特鲁空间金字塔池(ASPP) </strong>。第一种是级联卷积，它是简单的相互进行的卷积。当这些卷积为<em class="ko"> r </em> =1 时，这是现成的卷积，详细信息被抽取，使分割变得困难。作者发现，使用 atrous 卷积通过允许在 DCNN 的更深块中捕获远程信息来改善这一点。图 3 示出了“普通”DCNN 与级联 DCNN 的比较，级联 DCNN 具有<em class="ko">r</em>T24】1 个卷积。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mn"><img src="../Images/eaa4b16dbc9cb2dbfaf1a9b029f01b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9oLh4-BIydN0JOv-cxyzIg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Figure 3. The top (a) is a regular CNN while the second uses atrous convolutions with <em class="mk">r&gt;1 in a cascading manner and with an output_stride of 16.</em></figcaption></figure><p id="5f40" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二种是多重网格方法，不同大小的网格层次结构(见图 4)。他们将多网格参数定义为一组 atrous rates ( <em class="ko"> r₁，r </em> ₂，<em class="ko"> r </em> ₃)，按顺序应用于三个区块。最终速率等于单位速率和相应速率的乘积。例如，在 output_stride 为 16，multi_grid 为(1，2，4)的情况下，块 4(见图 3)将有三个速率为 2 ⋅ (1，2，4) = (2，4，8)的卷积，</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mi"><img src="../Images/5ed1b4b8339934128e489b742cad647b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HruUhv03rfOUYdHX.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Figure 4. A multi-grid CNN architecture. From Ke, T., Maire, M., &amp; Yu, S.X., 2016 [4]</figcaption></figure><p id="52f6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作者的主要贡献是从[5]修改阿特鲁空间金字塔池(ASPP)，它在空间“金字塔”池方法中使用 Atrous 卷积，以包括<strong class="js iu">批量归一化</strong>和包括<strong class="js iu">图像级特征</strong>。如图 5 (b)所示，他们通过在最后一个特征图上应用全局平均池来实现这一点。然后，它们将结果馈送到具有 256 个滤波器的 1x1 卷积。他们最终将该特征双线性上采样到期望的空间维度。图 5 中的例子提供了两个输出。输出(a)是具有 multi_grid rates = (6，12，18)的 3×3 卷积。输出(b)是图像级特征。然后，网络连接这些输出，并在生成 logit 类输出的最终 1x1 卷积之前，通过 1x1 卷积传递它们。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mo"><img src="../Images/b1714af10cf31a45e9a78400527aa4c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nuEBRSqMoTXJVkxG5Uts3w.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Figure 5. Parallel modules with atrous convolutions. From Chen, L.-C., Papandreou, G., Schroff, F., &amp; Adam, H., 2017 [1].</figcaption></figure><h1 id="97e5" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">实验</h1><p id="7480" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">为了给他们的方法提供支持，他们将级联和多重网格网络与 ASPP 进行了比较。结果是:</p><ul class=""><li id="6aad" class="mp mq it js b jt ju jx jy kb mr kf ms kj mt kn mu mv mw mx bi translated"><strong class="js iu">输出步幅:</strong>他们发现，较大的分辨率，或较小的 output_stride，比没有 atrous 卷积或较大的 Output _ Stride 表现明显更好。他们还发现，当在比较 output_stride 为 8(更大的分辨率)和 output_stride 为 16 的验证集上测试这些网络时，output_stride 为 8 的网络性能更好。</li><li id="4cd2" class="mp mq it js b jt my jx mz kb na kf nb kj nc kn mu mv mw mx bi translated"><strong class="js iu">级联</strong>:与常规卷积相比，级联 atrous 卷积的结果提高了性能。然而，他们发现添加的块越多，改善的幅度就越小。</li><li id="713f" class="mp mq it js b jt my jx mz kb na kf nb kj nc kn mu mv mw mx bi translated"><strong class="js iu">多网格</strong>:相对于“普通”网络，他们的多网格架构结果确实有所改善，并且在第 7 块的(<em class="ko"> r₁，r </em> ₂，<em class="ko"> r </em> ₃) = (1，2，1)处表现最佳。</li><li id="0d3b" class="mp mq it js b jt my jx mz kb na kf nb kj nc kn mu mv mw mx bi translated"><strong class="js iu"> ASPP +多重网格+图像池</strong>:多重网格速率为(<em class="ko"> r₁，r </em> ₂，<em class="ko"> r </em> ₃) = (1，2，4)，使得 ASPP (6，12，18)模型在 7721 万时表现最佳。在具有多尺度输入的 COCO 数据集上，当 output_stride = 8 时，模型在 82.70 处测试，显示通过将 output_stride 从 16 改变为 8 而进一步改善。</li></ul><h1 id="f5fd" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">结论</h1><p id="4a91" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">作者提出了一种方法，通过向空间“金字塔”池 atrous 卷积层添加批处理和图像特征来更新 DeepLab 以前的版本。结果是，网络可以提取密集的特征图来捕捉长范围的上下文，从而提高分割任务的性能。他们提出的模型的结果在 PASCAL VOC 2012 语义图像分割基准上优于最先进的模型。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/ba1251ca4f4aa05ae3fe0f266e0cef2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*TLzUlT-LmIFgXML0-7PEOQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">From Chen, L.-C., Papandreou, G., Schroff, F., &amp; Adam, H., 2017 [1].</figcaption></figure><p id="ad3b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更多信息请访问他们的 GitHub:<a class="ae mm" href="https://github.com/tensorflow/models/tree/master/research/deeplab" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/tensor flow/models/tree/master/research/deep lab</a></p><h1 id="f2f2" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">参考</h1><p id="4c98" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">[1]陈，L.-C .，帕潘德里欧，g .，施罗夫，f .，&amp;亚当，H. (2017)。语义图像分割中阿特鲁卷积的再思考。从<a class="ae mm" href="http://arxiv.org/abs/1706.05587" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1706.05587</a>取回</p><p id="7a53" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2] Holschneider，Matthias &amp; Kronland-Martinet，Richard &amp; Morlet，j .和 Tchamitchian，Ph. (1989 年)。一种基于小波变换的实时信号分析算法。小波、时频方法和相空间。-1.286.10.1007/978–3–642–75988–8_28.</p><p id="7b08" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3] Giusti，a .，Cireş an，D. C .，Masci，j .，Gambardella，L. M .，&amp; Schmidhuber，J. (2013 年 9 月)。深度最大池卷积神经网络的快速图像扫描。在<em class="ko"> 2013 IEEE 国际图像处理会议</em>(第 4034–4038 页)。IEEE。</p><p id="11a6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[4]柯，t .梅尔，m .，，余，S.X. (2016)。多重网格神经结构。<em class="ko"> 2017 年 IEEE 计算机视觉与模式识别大会(CVPR) </em>，4067–4075。</p><p id="be42" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[5]陈、帕潘德里欧、科基诺斯、墨菲和尤耶。Deeplab:使用深度卷积网络、atrous 卷积和全连接 CRF 的语义图像分割。arXiv:1606.00915，2016。</p></div></div>    
</body>
</html>