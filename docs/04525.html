<html>
<head>
<title>Build the right Autoencoder — Tune and Optimize using PCA principles. Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建正确的自动编码器——使用 PCA 原理进行调整和优化。第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b?source=collection_archive---------2-----------------------#2019-07-12">https://towardsdatascience.com/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b?source=collection_archive---------2-----------------------#2019-07-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a6b5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在这里，我们将学习自动编码器的期望属性，这些属性源自它与 PCA 的相似性。由此，我们将在<a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6" rel="noopener">第二部分</a>中为自动编码器构建定制约束，以进行调整和优化。</h2></div><p id="799e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">&lt;<download the="" free="" book="" class="ae kf" href="https://www.understandingdeeplearning.com" rel="noopener ugc nofollow" target="_blank">了解深度学习，了解更多&gt; &gt;</download></p><p id="0163" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">深度学习 API 的可用性，如 Keras 和 TensorFlow，使模型建立和实验变得极其容易。然而，对基本面缺乏清晰的理解可能会让我们在最佳模式的竞赛中迷失方向。在这样的比赛中达到最佳模式是靠运气的。</p><p id="0987" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这里，我们将了解自动编码器所需的基本属性。这将为自动编码器调整和优化提供一个很好的指导方法。在第一部分中，我们将重点学习这些属性及其优点。在<a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6" rel="noopener">第二部分</a>，我们将开发自定义层和约束来合并属性。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="cbcc" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们将在这里学习的主要概念是，<strong class="ki ir">自动编码器与主成分分析(PCA) </strong>直接相关，这将使我们能够构建一个正确的<em class="lj">自动编码器。一个“正确的”自动编码器在数学上意味着一个适定的自动编码器。一个适定的模型更容易调整和优化。</em></p><p id="b6ee" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">自动编码器相对于<strong class="ki ir"> </strong> PCA <strong class="ki ir">、</strong></p><ul class=""><li id="7f2c" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated">线性激活的自动编码器近似于 PCA。数学上，最小化 PCA 建模中的重建误差与单层线性自动编码器相同。</li><li id="049f" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb lp lq lr ls bi translated">自动编码器将 PCA 扩展到非线性空间。换句话说，自动编码器是 PCA 的非线性扩展。</li></ul><p id="1ddf" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此，自动编码器应该理想地具有 PCA 的特性。这些属性是，</p><ul class=""><li id="aeca" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated"><strong class="ki ir">绑定权重:</strong>编码器和相应解码器层上的权重相等(在下一节的图 1 中阐明)。</li><li id="c5d6" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb lp lq lr ls bi translated"><strong class="ki ir">正交权重:</strong>每个权重向量都是相互独立的。</li><li id="8fff" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb lp lq lr ls bi translated"><strong class="ki ir">不相关特征:</strong>编码层的输出不相关。</li><li id="1642" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb lp lq lr ls bi translated"><strong class="ki ir">单位定额:</strong>一层上的权重有单位定额。</li></ul><p id="a290" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">然而，大多数教程中解释的自动编码器，例如<a class="ae kf" href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener ugc nofollow" target="_blank">在 Keras</a>【1】中构建自动编码器，没有这些属性。缺少这一点使它们不是最佳的。</p><p id="d294" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此，对于一个适定的自动编码器来说，结合这些属性是很重要的。通过合并它们，我们还将</p><ul class=""><li id="9eec" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated"><strong class="ki ir">有正规化。</strong>正交性和单位范数约束作为正则化。此外，正如我们将在后面看到的，绑定权重将网络参数的数量减少了近一半，这是另一种正则化类型。</li><li id="2a8b" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb lp lq lr ls bi translated"><strong class="ki ir">解决爆炸和消失的渐变。</strong>单位范数约束防止权重变大，因此解决了爆炸梯度问题。此外，由于正交性约束，只有重要的/信息性的权重是非零的。因此，在反向传播期间，足够的信息流过这些非零权重，从而避免消失梯度。</li><li id="0f2b" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb lp lq lr ls bi translated"><strong class="ki ir">有更小的网络</strong>:没有正交性，编码器有冗余的权重和特征。为了补偿冗余，增加了编码器的尺寸。相反，正交性确保每个编码特征都有一条唯一的信息——独立于其他特征。这消除了冗余，并且我们可以用较小的编码器(层)编码相同数量的信息。</li></ul><blockquote class="ly lz ma"><p id="91aa" class="kg kh lj ki b kj kk jr kl km kn ju ko mb kq kr ks mc ku kv kw md ky kz la lb ij bi translated">通过一个更小的网络，我们让 Autoencoder 更接近<strong class="ki ir">边缘计算</strong>。</p></blockquote></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="7ce1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">本文将通过展示</p><ol class=""><li id="4491" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb me lq lr ls bi translated">PCA 和 Autoencoder 之间的架构相似性，以及</li><li id="ebb6" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb me lq lr ls bi translated">传统自动编码器的次优性。</li></ol><p id="e544" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">本文将在<a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6" rel="noopener">第二部分</a>中继续，详细介绍优化自动编码器的步骤。在第二部分的<a class="ae kf" rel="noopener" target="_blank" href="/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6">中，我们发现优化将自动编码器重构误差提高了 50%以上。</a></p><p id="1c41" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">本文假设读者对 PCA 有基本的了解。如果不熟悉，请参考<a class="ae kf" href="https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0" rel="noopener">了解 PCA</a>【2】。</p><h1 id="2775" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">PCA 和自动编码器之间的架构相似性</h1><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi mx"><img src="../Images/991d61809ff6eace5976a449a965fe77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0KETnXWHct-bQsR2CoXLg.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Figure 1. Single layer Autoencoder vis-à-vis PCA.</figcaption></figure><p id="d437" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">为简单起见，我们将线性单层自动编码器与 PCA 进行比较。PCA 建模有多种算法。其中之一是通过最小化重建误差进行估计(参见[ <a class="ae kf" href="https://www.cs.cmu.edu/~mgormley/courses/10701-f16/slides/lecture14-pca.pdf" rel="noopener ugc nofollow" target="_blank"> 3 </a> ])。遵循该算法可以更清楚地理解 PCA 和自动编码器之间的相似之处。</p><p id="6f30" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">图 1 显示了单层线性自动编码器。如图底部所示，编码过程类似于 PC 变换。PC 变换将原始数据投影到主成分上，以产生正交特征，称为主得分。类似地，解码过程类似于从主要分数重构数据。在自动编码器和 PCA 中，可以通过最小化重建误差来估计模型权重。</p><p id="eba5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在下文中，我们将通过展示关键的自动编码器组件及其在 PCA 中的等效组件来进一步阐述图 1。</p><p id="4515" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">假设我们有带有<em class="lj"> p </em>特征的数据。</p><p id="dbee" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">输入层—数据样本。</strong></p><ul class=""><li id="f38c" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated">在自动编码器中，使用尺寸为<em class="lj"> p. </em>的<strong class="ki ir">输入层</strong>输入数据</li><li id="07d7" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb lp lq lr ls bi translated">在 PCA 中，数据作为样本输入。</li></ul><p id="19eb" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">编码——数据在主成分上的投影。</strong></p><ul class=""><li id="50c8" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated">编码层的大小为<em class="lj"> k </em>。在 PCA 中，<em class="lj"> k </em>表示选择的主成分(PCs)的数量。</li><li id="1f3a" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb lp lq lr ls bi translated">在两者中，我们都有<em class="lj"> k </em> &lt; <em class="lj"> p </em>进行降维。<em class="lj"> k ≥ p </em>导致过代表模型，因此(接近)零重建误差。</li><li id="ef39" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb lp lq lr ls bi translated">图 1 中编码层中的彩色单元是计算节点，其具有表示为<em class="lj">p</em>的权重，</li></ul><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nn"><img src="../Images/0faa9bb9b65e073d30ee7f493da1a0e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8k4NZx4ON771TNui3Zzvdw.png"/></div></div></figure><p id="f28d" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">也就是说，对于 1，…，<em class="lj"> k </em>中的每个编码节点，我们都有一个<em class="lj"> p </em>维的权重向量。这相当于 PCA 中的一个特征向量。</p><ul class=""><li id="02e1" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated">自动编码器中的编码层输出是，</li></ul><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi no"><img src="../Images/cdcc273c94c3f9b1f4566f3a56ab6330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3GNIDndjs8RG7X8tPELTw.png"/></div></div></figure><p id="d3d5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><em class="lj"> x </em>是输入，<em class="lj"> W </em>是权重矩阵。功能<em class="lj"> g </em>是一个激活功能。<em class="lj"> g </em> ( <em class="lj"> Wx </em>)是编码层的输出。如果激活是线性的，这相当于 PCA 中的主要分数。</p><p id="73f6" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">解码——从主要分数中重建数据。</strong></p><ul class=""><li id="1ed3" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated">自动编码器和 PCA 重建中解码层的大小必须是输入数据的大小</li><li id="f0ba" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb lp lq lr ls bi translated">在解码器中，数据从编码中被重构为，</li></ul><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi np"><img src="../Images/32b53e09c5051359330fd8f0d62e10a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xy64wbmVNElKldO3LTP11Q.png"/></div></div></figure><p id="baa6" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">类似地，在 PCA 中，它被重构为，</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi np"><img src="../Images/d3ed0c732d2a81f1a1a1786f454cbc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*es9x6VXJ1H4-OLaYfdSGWA.png"/></div></div></figure><p id="e938" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">注意，我们在等式中有<em class="lj">W’</em>。4 和等式中的<em class="lj"> W </em>。5.这是因为，默认情况下，编码器和解码器的权重不同。如果编码器和解码器权重与<em class="lj">绑定</em>，解码器和 PCA 重建将是相同的，即</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi np"><img src="../Images/363b3848fa0acb53735e2964e7396694.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-UVelnLz5_vJUiYDKg5rQ.png"/></div></div></figure><blockquote class="ly lz ma"><p id="eb07" class="kg kh lj ki b kj kk jr kl km kn ju ko mb kq kr ks mc ku kv kw md ky kz la lb ij bi translated">解码器单元中的多种颜色表示编码器中不同单元中的权重出现在解码器中的相同单元中。</p></blockquote><p id="b6c5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这就把我们带到了自动编码器和 PCA 之间的数学比较。</p><p id="3046" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">数学上，线性自动编码器将类似于 PCA if，</p><ul class=""><li id="46e1" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated"><strong class="ki ir">捆绑权重</strong>:在任何通用多层自动编码器中，编码器模块中的层<em class="lj"> l </em>的权重等于解码器中从末端开始的层<em class="lj"> l </em>的权重的转置。</li></ul><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi np"><img src="../Images/866af7e100d93dac60015d53a33b80de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKd74LI3e3Wj7G2cIGotjA.png"/></div></div></figure><ul class=""><li id="0949" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated"><strong class="ki ir">正交权重</strong>:编码层上的权重是正交的(参见等式。7b)。可以在中间编码器层上实施相同的正交性约束以进行规则化。</li></ul><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi np"><img src="../Images/b9290c67d9921f879c11ec707b588cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Giw-p6TnTZsqfnsWhEnuVw.png"/></div></div></figure><ul class=""><li id="da0e" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated"><strong class="ki ir">不相关特征</strong>:PCA 的输出，即主分数，是不相关的。因此，编码器的输出应该具有，</li></ul><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi np"><img src="../Images/c0ff0c93d393f655a3ca11f9f824d48e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Rel6Ja5sG5JxS0U54JhrQ.png"/></div></div></figure><ul class=""><li id="7279" class="lk ll iq ki b kj kk km kn kp lm kt ln kx lo lb lp lq lr ls bi translated"><strong class="ki ir">单位范数</strong>:PCA 中的特征向量被约束为具有单位范数。如果没有这个约束，我们将得不到一个合适的解，因为只要向量的范数增加，投影的方差就可能变得任意大。出于同样的原因，编码层上的权重应该是单位范数(参见等式 1)。7d)。这个约束也应该应用于其他中间层以进行正则化。</li></ul><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi np"><img src="../Images/c4922477633f9c17e1f9e09e52854203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*41D8G8F_-QbkKhXyc-g4kA.png"/></div></div></figure><h1 id="f35f" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">正则无约束自动编码器的次优性</h1><p id="2da6" class="pw-post-body-paragraph kg kh iq ki b kj nq jr kl km nr ju ko kp ns kr ks kt nt kv kw kx nu kz la lb ij bi translated">这里我们将在随机数据集上实现 PCA 和一个典型的无约束自动编码器。我们将展示它们的输出在上述各个方面的不同。这导致了次优的自动编码器。在这个讨论之后，我们将展示如何为<em class="lj">正确的</em>估计(<a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6" rel="noopener">第二部分</a>)约束自动编码器。</p><p id="9c4f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">完整的代码可在<a class="ae kf" href="https://github.com/cran2367/pca-autoencoder-relationship/blob/master/pca-autoencoder-relationship.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="8b5b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">加载库</strong></p><pre class="my mz na nb gt nv nw nx ny aw nz bi"><span id="ef7f" class="oa mg iq nw b gy ob oc l od oe"><strong class="nw ir">from</strong> <strong class="nw ir">numpy.random</strong> <strong class="nw ir">import</strong> seed<br/>seed(123)<br/><strong class="nw ir">from</strong> <strong class="nw ir">tensorflow</strong> <strong class="nw ir">import</strong> set_random_seed<br/>set_random_seed(234)<br/><br/><strong class="nw ir">import</strong> <strong class="nw ir">sklearn</strong><br/><strong class="nw ir">from</strong> <strong class="nw ir">sklearn</strong> <strong class="nw ir">import</strong> datasets<br/><strong class="nw ir">import</strong> <strong class="nw ir">numpy</strong> <strong class="nw ir">as</strong> <strong class="nw ir">np</strong><br/><strong class="nw ir">from</strong> <strong class="nw ir">sklearn.model_selection</strong> <strong class="nw ir">import</strong> train_test_split<br/><strong class="nw ir">from</strong> <strong class="nw ir">sklearn.preprocessing</strong> <strong class="nw ir">import</strong> StandardScaler, MinMaxScaler<br/><strong class="nw ir">from</strong> <strong class="nw ir">sklearn</strong> <strong class="nw ir">import</strong> decomposition<br/><strong class="nw ir">import</strong> <strong class="nw ir">scipy</strong><br/><br/><strong class="nw ir">import</strong> <strong class="nw ir">tensorflow</strong> <strong class="nw ir">as</strong> <strong class="nw ir">tf</strong><br/><strong class="nw ir">from</strong> <strong class="nw ir">keras.models</strong> <strong class="nw ir">import</strong> Model, load_model<br/><strong class="nw ir">from</strong> <strong class="nw ir">keras.layers</strong> <strong class="nw ir">import</strong> Input, Dense, Layer, InputSpec<br/><strong class="nw ir">from</strong> <strong class="nw ir">keras.callbacks</strong> <strong class="nw ir">import</strong> ModelCheckpoint, TensorBoard<br/><strong class="nw ir">from</strong> <strong class="nw ir">keras</strong> <strong class="nw ir">import</strong> regularizers, activations, initializers, constraints, Sequential<br/><strong class="nw ir">from</strong> <strong class="nw ir">keras</strong> <strong class="nw ir">import</strong> backend <strong class="nw ir">as</strong> K<br/><strong class="nw ir">from</strong> <strong class="nw ir">keras.constraints</strong> <strong class="nw ir">import</strong> UnitNorm, Constraint</span></pre><p id="5a01" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">生成随机数据</strong></p><p id="3e6a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们生成多元相关正态数据。数据生成的步骤在<a class="ae kf" href="https://github.com/cran2367/pca-autoencoder-relationship/blob/master/pca-autoencoder-relationship.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>中有详细说明。</p><pre class="my mz na nb gt nv nw nx ny aw nz bi"><span id="b654" class="oa mg iq nw b gy ob oc l od oe">n_dim = 5<br/>cov = sklearn.datasets.make_spd_matrix(n_dim, random_state=None)<br/>mu = np.random.normal(0, 0.1, n_dim)</span><span id="d7a1" class="oa mg iq nw b gy of oc l od oe">n = 1000</span><span id="330b" class="oa mg iq nw b gy of oc l od oe">X = np.random.multivariate_normal(mu, cov, n)</span><span id="cd71" class="oa mg iq nw b gy of oc l od oe">X_train, X_test = train_test_split(X, test_size=0.5, random_state=123)</span><span id="9a96" class="oa mg iq nw b gy of oc l od oe"># <em class="lj">Data Preprocessing<br/></em>scaler = MinMaxScaler()<br/>scaler.fit(X_train)</span><span id="c430" class="oa mg iq nw b gy of oc l od oe">X_train_scaled = scaler.transform(X_train)</span><span id="f3c6" class="oa mg iq nw b gy of oc l od oe">X_test_scaled = scaler.transform(X_test)</span></pre><p id="a7a1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">测试数据集将在<a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6" rel="noopener">第二部分</a>中用于比较自动编码器重建精度。</p><p id="c27a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">自动编码器和 PCA 型号</strong></p><p id="85ea" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们安装了一个单层线性自动编码器，编码维数为<strong class="ki ir">两个</strong>。我们还为 PCA 安装了两个组件<strong class="ki ir">。</strong></p><pre class="my mz na nb gt nv nw nx ny aw nz bi"><span id="7964" class="oa mg iq nw b gy ob oc l od oe"># <em class="lj">Fit Autoencoder</em><br/>nb_epoch = 100<br/>batch_size = 16<br/>input_dim = X_train_scaled.shape[1] <em class="lj">#num of predictor variables, </em><br/>encoding_dim = 2<br/>learning_rate = 1e-3<br/><br/>encoder = Dense(encoding_dim, activation="linear", input_shape=(input_dim,), use_bias = <strong class="nw ir">True</strong>) <br/>decoder = Dense(input_dim, activation="linear", use_bias = <strong class="nw ir">True</strong>)<br/><br/>autoencoder = Sequential()<br/>autoencoder.add(encoder)<br/>autoencoder.add(decoder)<br/><br/>autoencoder.compile(metrics=['accuracy'],<br/>                    loss='mean_squared_error',<br/>                    optimizer='sgd')<br/>autoencoder.summary()<br/><br/>autoencoder.fit(X_train_scaled, X_train_scaled,<br/>                epochs=nb_epoch,<br/>                batch_size=batch_size,<br/>                shuffle=<strong class="nw ir">True</strong>,<br/>                verbose=0)</span><span id="595c" class="oa mg iq nw b gy of oc l od oe"># <em class="lj">Fit PCA</em><br/>pca = decomposition.PCA(n_components=2)</span><span id="6f43" class="oa mg iq nw b gy of oc l od oe">pca.fit(X_train_scaled)</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi og"><img src="../Images/247cc68a0b427c3ecc395cfeaf431eda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ig22tU9Kl1x92eBBsM0UxQ.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Figure 2. Structure of the single-layer Autoencoder.</figcaption></figure><p id="50af" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">从视觉上看，这里开发的编码器-解码器结构如下面的图 3 所示。该图有助于理解权重矩阵是如何排列的。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oh"><img src="../Images/fdf3415d0f78a424f147146e87c2bc13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KhtGPm8IBLmOECbMnT20Nw.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Figure 3. A simple linear Autoencoder to encode a 5-dimensional data into 2-dimensional features.</figcaption></figure><p id="4a9e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">为了遵循 PCA 属性，图 3 中的自动编码器应该遵循等式中的条件。下面，我们将示出这种传统的自动编码器不满足它们中的任何一个。</p><h2 id="b882" class="oa mg iq bd mh oi oj dn ml ok ol dp mp kp om on mr kt oo op mt kx oq or mv os bi translated">1.捆绑重物</h2><p id="04d1" class="pw-post-body-paragraph kg kh iq ki b kj nq jr kl km nr ju ko kp ns kr ks kt nt kv kw kx nu kz la lb ij bi translated">正如我们在下面看到的，编码器和解码器的权重是不同的。</p><pre class="my mz na nb gt nv nw nx ny aw nz bi"><span id="209d" class="oa mg iq nw b gy ob oc l od oe">w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T  <em class="lj"># W in Figure 3.</em><br/>w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 2)  <em class="lj"># W' in Figure 3.</em><br/>print('Encoder weights <strong class="nw ir">\n</strong>', w_encoder)<br/>print('Decoder weights <strong class="nw ir">\n</strong>', w_decoder)</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ot"><img src="../Images/39a1e9561f8998f11d792e42578552f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9boACeXM_qkbqFbM_GfEmg.png"/></div></div></figure><h2 id="cac8" class="oa mg iq bd mh oi oj dn ml ok ol dp mp kp om on mr kt oo op mt kx oq or mv os bi translated"><strong class="ak"> 2。权重正交性</strong></h2><p id="6e72" class="pw-post-body-paragraph kg kh iq ki b kj nq jr kl km nr ju ko kp ns kr ks kt nt kv kw kx nu kz la lb ij bi translated">如下所示，与 PCA 权重(即特征向量)不同，编码器和解码器上的权重不是正交的。</p><pre class="my mz na nb gt nv nw nx ny aw nz bi"><span id="33c3" class="oa mg iq nw b gy ob oc l od oe">w_pca = pca.components_<br/>np.round(np.dot(w_pca, w_pca.T), 3)</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ou"><img src="../Images/73b08534e85f05389127a94fdf77e234.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLcz3VU-5E2LpXZulYmU2w.png"/></div></div></figure><pre class="my mz na nb gt nv nw nx ny aw nz bi"><span id="af11" class="oa mg iq nw b gy ob oc l od oe">np.round(np.dot(w_encoder, w_encoder.T), 3)</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ov"><img src="../Images/3b7dd257540da6c9603ef5bffe71c340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oDpoX1JIzE4j8rXOMJYn0w.png"/></div></div></figure><pre class="my mz na nb gt nv nw nx ny aw nz bi"><span id="f78d" class="oa mg iq nw b gy ob oc l od oe">np.round(np.dot(w_decoder, w_decoder.T), 3)</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ov"><img src="../Images/ce4a281cd37a7fec9a692451ed350714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5i5XGt8ArZumHcPnQY627g.png"/></div></div></figure><h2 id="52d3" class="oa mg iq bd mh oi oj dn ml ok ol dp mp kp om on mr kt oo op mt kx oq or mv os bi translated"><strong class="ak"> 3。特征关联</strong></h2><p id="5ac8" class="pw-post-body-paragraph kg kh iq ki b kj nq jr kl km nr ju ko kp ns kr ks kt nt kv kw kx nu kz la lb ij bi translated">在 PCA 中，特征是不相关的。</p><pre class="my mz na nb gt nv nw nx ny aw nz bi"><span id="691b" class="oa mg iq nw b gy ob oc l od oe">pca_features = pca.fit_transform(X_train_scaled)<br/>np.round(np.cov(pca_features.T), 5)</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ow"><img src="../Images/71bf34e562535aaaa61e67466667862b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*raYEPYm80cYKyCm0pgmwpw.png"/></div></div></figure><p id="6ee1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">但是编码特征是相关的。</p><pre class="my mz na nb gt nv nw nx ny aw nz bi"><span id="2e15" class="oa mg iq nw b gy ob oc l od oe">encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)<br/>encoded_features = np.array(encoder_layer.predict(X_train_scaled))<br/>print('Encoded feature covariance\n', np.cov(encoded_features.T))</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ox"><img src="../Images/2c0a82f054c1cd73d344bd4d130f8203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nBOKvPoicNuT0eRUi_Zrbg.png"/></div></div></figure><blockquote class="ly lz ma"><p id="9572" class="kg kh lj ki b kj kk jr kl km kn ju ko mb kq kr ks mc ku kv kw md ky kz la lb ij bi translated">权重非正交性和特征相关性是不期望的，因为它带来了包含在编码特征中的信息的冗余。</p></blockquote><h2 id="181c" class="oa mg iq bd mh oi oj dn ml ok ol dp mp kp om on mr kt oo op mt kx oq or mv os bi translated"><strong class="ak"> 4。单位定额</strong></h2><p id="39d6" class="pw-post-body-paragraph kg kh iq ki b kj nq jr kl km nr ju ko kp ns kr ks kt nt kv kw kx nu kz la lb ij bi translated">五氯苯甲醚重量的单位标准是 1。这是在 PCA 估计中应用的约束，以产生<em class="lj">正确的</em>估计<em class="lj">。</em></p><pre class="my mz na nb gt nv nw nx ny aw nz bi"><span id="00e8" class="oa mg iq nw b gy ob oc l od oe">print('PCA weights norm, \n', np.sum(w_pca ** 2, axis = 1))<br/>print('Encoder weights norm, \n', np.sum(w_encoder ** 2, axis = 1))<br/>print('Decoder weights norm, \n', np.sum(w_decoder ** 2, axis = 1))</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oy"><img src="../Images/2d4e7bec788a51bc21cf7a5a6dfa58ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KkOWRVbucu4Y0j0e3hC_jg.png"/></div></div></figure><p id="7c3a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae kf" href="https://github.com/cran2367/pca-autoencoder-relationship/blob/master/pca-autoencoder-relationship.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir"> Github 库</strong> </a></p><p id="772e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">完整的代码可在<a class="ae kf" href="https://github.com/cran2367/pca-autoencoder-relationship/blob/master/pca-autoencoder-relationship.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><div class="oz pa gp gr pb pc"><a href="https://github.com/cran2367/pca-autoencoder-relationship/blob/master/pca-autoencoder-relationship.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd ir gy z fp ph fr fs pi fu fw ip bi translated">cran 2367/PCA-自动编码器-关系</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">了解 PCA 和自动编码器之间的关系-cran 2367/PCA-自动编码器-关系</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">github.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq nh pc"/></div></div></a></div></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="0741" class="mf mg iq bd mh mi pr mk ml mm ps mo mp jw pt jx mr jz pu ka mt kc pv kd mv mw bi translated">结论</h1><p id="7a5c" class="pw-post-body-paragraph kg kh iq ki b kj nq jr kl km nr ju ko kp ns kr ks kt nt kv kw kx nu kz la lb ij bi translated">因此，自动编码器模型是不适定的。不适定模型没有稳健的估计。这不利地影响了它的测试精度，即新数据的重建误差。</p><p id="0064" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">最近的几项研究进展是建立和利用正交条件来提高深度学习模型的性能。一些研究方向参考[ <a class="ae kf" href="https://arxiv.org/pdf/1709.06079.pdf" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]和[ <a class="ae kf" href="https://arxiv.org/pdf/1609.07093.pdf" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]。</p><p id="2726" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在续集<a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6" rel="noopener">第二部分</a>中，我们将实现自定义约束，将上述从 PCA 得到的属性合并到自动编码器中。我们将会看到，添加约束改善了测试重构误差。</p><p id="303b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6" rel="noopener"> <em class="lj">去看续集，下集。</em>T25】</a></p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="a761" class="mf mg iq bd mh mi pr mk ml mm ps mo mp jw pt jx mr jz pu ka mt kc pv kd mv mw bi translated">参考</h1><ol class=""><li id="e5ac" class="lk ll iq ki b kj nq km nr kp pw kt px kx py lb me lq lr ls bi translated"><a class="ae kf" href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener ugc nofollow" target="_blank">在 Keras 中构建自动编码器</a></li><li id="8a76" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb me lq lr ls bi translated"><a class="ae kf" href="https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0" rel="noopener">理解主成分分析</a></li><li id="802d" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb me lq lr ls bi translated"><a class="ae kf" href="https://www.cs.cmu.edu/~mgormley/courses/10701-f16/slides/lecture14-pca.pdf" rel="noopener ugc nofollow" target="_blank">主成分分析:利用重构误差的算法(第 15 页)。</a></li><li id="c2e7" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb me lq lr ls bi translated">黄，雷等.<a class="ae kf" href="https://arxiv.org/pdf/1709.06079.pdf" rel="noopener ugc nofollow" target="_blank">正交权重归一化:深度神经网络中多重相关 stiefel 流形上的优化解。</a><em class="lj">第三十二届 AAAI 人工智能大会</em>。2018.</li><li id="0def" class="lk ll iq ki b kj lt km lu kp lv kt lw kx lx lb me lq lr ls bi translated">使用内省对抗网络的神经照片编辑。<em class="lj">arXiv 预印本 arXiv:1609.07093 </em> (2016)。</li></ol></div></div>    
</body>
</html>