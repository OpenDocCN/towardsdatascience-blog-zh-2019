<html>
<head>
<title>Why cautiously initializing deep neural networks matters?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么谨慎初始化深度神经网络很重要？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa?source=collection_archive---------7-----------------------#2019-04-18">https://towardsdatascience.com/what-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa?source=collection_archive---------7-----------------------#2019-04-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c802" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">明凯初始化背后的数学。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fa3b1a795446621d2833de2c559143c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3PIgxr4UmVY22icRdwegPA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Careful weight initialization expedites the convergence of neural nets. (photo by <a class="ae kv" href="https://unsplash.com/photos/LNDgBERq8Q0" rel="noopener ugc nofollow" target="_blank">@kimzy</a> on unsplash)</figcaption></figure><h2 id="90e5" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">介绍</h2><p id="c61b" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">深度学习最近的成功很大一部分归功于<strong class="lu ir"> ReLU </strong>激活功能。它在图像分类问题的深度细胞神经网络方面取得了最新的成果。在这篇博客中，我们将讨论一种鲁棒的权重初始化方法，它有助于更快地收敛更深层次的神经模型。<strong class="lu ir">何等</strong>在<a class="ae kv" href="https://arxiv.org/abs/1502.01852" rel="noopener ugc nofollow" target="_blank"> <strong class="lu ir">深入钻研整流器</strong> </a>论文(2015)中提出了这种方法。</p><p id="6879" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这篇博客的灵感来自于<a class="ae kv" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> Fast.ai </a>的《程序员深度学习》课程，第二部分，由<a class="ae kv" href="https://twitter.com/jeremyphoward" rel="noopener ugc nofollow" target="_blank">杰瑞米·霍华德</a>在 USF 教授。</p><h2 id="83be" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">神经网络中的参数</h2><p id="587b" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">神经网络的参数包括权重和偏差。这些数字首先被随机初始化。然后我们的模型学习它们，这意味着我们在反向过程中使用梯度来逐渐更新它们。</p><p id="4245" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">初始化参数最普遍的方法是使用<strong class="lu ir">高斯分布</strong>。该分布的平均值为 0，标准差为 1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/bcede76bfdc48f1dd17abb4074b3e033.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*dK_SbA-CgghK2YAMCOzB7g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://www.graphpad.com/guides/prism/7/curve-fitting/reg_how_to_gaussian.html" rel="noopener ugc nofollow" target="_blank">Bell Curve</a></figcaption></figure><p id="0b41" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果<strong class="lu ir"> m </strong>是输入尺寸并且<strong class="lu ir"> nh </strong>是隐藏单元的数量，那么权重可以被初始化为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/4d820e18d5fc3bd4470e0f0135ae7936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uA57G_S-H_BYYtM63cCqFw@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">random weight initialization in PyTorch</figcaption></figure><h2 id="4de6" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">为什么准确的初始化很重要？</h2><p id="9992" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated"><em class="ms">深度神经网络很难训练。</em>随机初始化参数，太小或太大都会有问题，而反向传播梯度一直到初始层。</p><p id="f2d7" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">当我们初始化权重过小时会发生什么(&lt;1)? Their gradient tends to get smaller as we move backward through the hidden layers, which means that neurons in the earlier layers learn much more slowly than neurons in later layers. This causes minor weight updates. This phenomenon is called <strong class="lu ir"> <em class="ms">消失渐变问题，其中权重消失为 0 </em> </strong> <em class="ms">)。</em></p><p id="449e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果我们初始化的权重太大(&gt; 1)怎么办？梯度在较早的层中变得更大，这导致超过最小值的极高权重更新。这就是<strong class="lu ir"> <em class="ms">爆炸梯度问题，权重爆炸到无穷大(</em> NaN <em class="ms"> ) </em> </strong> <em class="ms">)。</em>这两种情况都使得神经网络难以收敛。</p><p id="9004" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">下面是<strong class="lu ir"> Gloriot 等人</strong>在论文<a class="ae kv" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank">中进行的实验的图像，理解训练深度前馈神经网络</a>的困难。作者考虑了具有标准初始化的 5 层深度的神经网络。他们从正态分布初始化随机权重(0 均值和 1 方差)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/060296ff27939dae12f3c3bb483b155d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*49B_eRP0uICCOXLGUQZ1yQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">0 peak increases for higher layers (layer 4 and 5).</figcaption></figure><p id="bacf" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">上图描述了随着训练的进行(从第 1 层到第 5 层的正向传递),所有激活值的平均值在最后一层变得更小(消失到 0)。在第 5 层，它们几乎为 0。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/1901ff147cac322aff25bc9e5ea92f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GBuK0eWcN6kKQJGBfWVaMQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">0 peak decreases towards the end.</figcaption></figure><p id="78b3" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们反向计算梯度，从第 5 层到第 1 层。对于第一层，所有的梯度几乎消失了。<br/>不好的初始化真的会阻碍高度非线性系统的学习。由于随机初始化，第一层丢弃了关于输入图像的大部分信息。因此，即使我们广泛地训练后面的层，它们也没有足够的信息从输入图像中学习。</p><p id="290d" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">仔细的权重初始化防止了这两种情况的发生，并导致深度神经网络的更快收敛。</p><h2 id="bc00" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">输入数据的预处理</h2><p id="7777" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">当输入数据居中(平均值为 0，标准差为 1)时，神经网络工作得最好。因此，当输入值乘以权重值时，它们的激活程度保持在 1 级。它是做什么的？它有助于神经网络的优化。因为隐藏的激活功能不会那么快饱和。因此在学习早期不会给出接近零的梯度。</p><h2 id="4665" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">ReLU —整流器线性单元</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/697783ab34e80f432a206f0b293f8274.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*MVO9VbxIVIL0-KVs.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://ailephant.com/glossary/relu-function/" rel="noopener ugc nofollow" target="_blank">ReLu</a> Function Graph</figcaption></figure><p id="f192" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><em class="ms"> ReLU 是一个非线性</em> <strong class="lu ir"> <em class="ms">激活</em> </strong> <em class="ms">功能。ReLU 的定义是，</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/32840bab85069984bfb8eaf9b275051d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QvIrPYM923gMBtjOW08TlA@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://pouannes.github.io/blog/initialization/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/c338cb4bae31fe1c379de54965eefed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h8zidB1wuqw0Ap0fdiX7ig@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">ReLU in PyTorch</figcaption></figure><h2 id="e397" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">使用 ReLU 激活的优势-</h2><ol class=""><li id="d9bc" class="my mz iq lu b lv lw ly lz lf na lj nb ln nc mk nd ne nf ng bi translated">ReLU 解决了爆炸和消失渐变问题，因为它为所有大于 0 的输入输出 1 的恒定渐变</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/d689e1d16f57a6c6ac50e283e8217706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tQadJAXSx-Oag8oyB659ZA@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">a derivative of ReLU(<a class="ae kv" href="https://pouannes.github.io/blog/initialization/#bringing-forward-propagation-and-backward-propagation-together" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="fbce" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">2.这使得神经网络学习速度更快。并且还加快了训练过程的收敛。</p><p id="c3b4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">3.ReLU 比传统的 sigmoid 单元更快地给出更好的解决方案。</p><p id="e9f6" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">明凯等人(1)通过谨慎地对 ReLUs 的非线性进行建模，导出了一种合理的初始化方法，这种方法使得非常深的模型(&gt; 30 层)收敛。</p><h2 id="9a15" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">正向传播</h2><p id="3131" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">正向传递由来自与 ReLU 非线性耦合的顺序矩阵乘法(在层的输入和权重之间)的激活组成。其输出被传递到执行类似操作的连续层。</p><p id="5097" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">对于每个卷积层 l，响应在等式(1)之下，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f4258111407a8219a0208f7eed0373c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*HW5qys0RLEaoElOdnrIGtQ@2x.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">linear equation for convolutional layer l</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/14951194a26728c4cf3ec991dbeaee78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*eKh2yond4jnh_VKNMcuEUw@2x.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">in python</figcaption></figure><p id="70b2" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在哪里</p><ol class=""><li id="e1ea" class="my mz iq lu b lv ml ly mm lf nk lj nl ln nm mk nd ne nf ng bi translated">x_l = x_l 是<strong class="lu ir"> n 乘-l </strong>向量。(<strong class="lu ir"> _l </strong>到此表示<em class="ms">下标 l </em>起。)<br/> x 是<strong class="lu ir"> k x k(长、宽)</strong>和<strong class="lu ir"> c </strong>输入<strong class="lu ir">通道</strong>的一个<strong class="lu ir">输入</strong>图像。<br/>我们假设图像是正方形(l=b)。<br/> n 是输出中<strong class="lu ir">激活</strong>的次数。<strong class="lu ir"> n= k c </strong> <br/>如果 f 是前一层(l-1)的激活函数，我们有</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5e8a4922231d57b4d68d7e138d762379.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*cfVjP3cCZppSl18cNVNrLQ@2x.png"/></div></figure><ol class=""><li id="9910" class="my mz iq lu b lv ml ly mm lf nk lj nl ln nm mk nd ne nf ng bi translated">W_l = d 乘 n 个<strong class="lu ir">权重</strong>矩阵，其中<strong class="lu ir"> d </strong>是<strong class="lu ir">滤波器</strong>的个数。n 是 x 的长度，即 n = k<strong class="lu ir">T29】c。当前层 l 的通道与前一层(l-1)的滤波器相同。</strong></li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b628868b611d233659e941f05ab3b35e.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*cUjCmGFw6xq2JCDAfPi76Q@2x.png"/></div></figure><ol class=""><li id="a1f7" class="my mz iq lu b lv ml ly mm lf nk lj nl ln nm mk nd ne nf ng bi translated">b_l =偏置的矢量<strong class="lu ir">(初始化为 0)</strong></li><li id="9a92" class="my mz iq lu b lv np ly nq lf nr lj ns ln nt mk nd ne nf ng bi translated">y_l =权重和输入矩阵相乘并加上偏差后的结果向量。</li></ol><p id="21f6" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">假设</strong></p><ol class=""><li id="ef2c" class="my mz iq lu b lv ml ly mm lf nk lj nl ln nm mk nd ne nf ng bi translated">W_l 和 x_l 中的元素相互独立，共享同一分布。</li><li id="d057" class="my mz iq lu b lv np ly nq lf nr lj ns ln nt mk nd ne nf ng bi translated">x_l 和 w_l 相互独立。</li></ol><p id="7f01" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">卷积层为什么执行线性运算？</strong></p><p id="24f0" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">另外，如果您想知道卷积层如何像线性层一样执行线性方程。Conv 层做卷积。但是如果你关注马修·克莱恩史密斯的博客。你会明白卷积只是矩阵乘法，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/5dd28145c8c89031885d28702172ab20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1dNULVDtF86f9a3dcgxm3A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Convolutions are simply matrix multiplications.</figcaption></figure><p id="881c" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在回到我们的方程，如果我们求线性方程(1)的方差，我们得到方程(2)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/81b9c4848c8d14090b5e13e2daf9bb58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*a_5-98droORqYw87vkuA6g@2x.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">a variance of linear Eqn. (1)</figcaption></figure><p id="1ddb" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">其中 y_l，w_l，x_l 分别是<strong class="lu ir"> y_l，W_l，x_l </strong>中每个元素的随机变量。我们假设 w_l 零均值。代入方程。(2)我们将自变量乘积的方差作为等式。(3)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/8d1cbefbc7c3f97adb1376d91e1f7634.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*r1CAI5TfAU5Gyzp0rrhEbA@2x.png"/></div></figure><p id="48ca" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">但是我们是如何做到这一点的呢？对于相互独立的随机变量<strong class="lu ir"> x_l </strong>和<strong class="lu ir"> W_l </strong>。我们可以用期望的性质来证明这一点</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/4330e74f16e502195db6814b89269ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KY3D8-hmNb6uHnkGcYi5Jg@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="943e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">由于 w_l 的平均值为 0，即<strong class="lu ir"> E[w_l]=[E[w_l]] =0 </strong> <br/>这意味着在上述等式中。(A)，★评估为零。然后我们只剩下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/8dc9d8dadc5d65fc9ce0fac69e1ea575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C1w_xS2nYvVcbqAAmio7pA@2x.png"/></div></div></figure><p id="b59b" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">使用方差公式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/af83ab0d80b184f5a652cb46305bf2a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*1KUssQnL-Hz4uggzr1T_xw@2x.png"/></div></figure><p id="428f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">以及 E[w_l]=0 的事实我们可以得出<strong class="lu ir"> Var[w_l]=E[w _l]。<br/> </strong>有了这个结论，我们就可以替换方程中的 E[w _l]了。(B)用 Var[w _ l]得到，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/ce1f214ed1d3d63204bdc7229b9bd9cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8NtepCfepWxbDXc8N8vIAA@2x.png"/></div></div></figure><p id="6388" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">抓住你了。通过替换等式。转换成等式。(2)我们得到方程式。(3)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/8d1cbefbc7c3f97adb1376d91e1f7634.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*r1CAI5TfAU5Gyzp0rrhEbA@2x.png"/></div></figure><p id="2ed6" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">让我们把重点放在 E[x _l]项上。这里 E()代表给定变量的期望值，也就是它的均值。但是低于等式。不成立</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/cc1665e438cf3ab19717f46507cdf8f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*s2_g7atFgx0U_KaakXLDVw@2x.png"/></div></figure><p id="666e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">除非 x_l 的均值为零。x_l 不能有 0 均值，因为它是前一层(l-1)的 ReLU 激活函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/2f32742ba1de53f92b9aa4229b556503.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*pJ66RWSgNBMobGZ6be_olg@2x.png"/></div></figure><p id="82a5" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果我们进一步假设 w(L1)在 0 附近具有对称分布，并且 b(L1)= 0。那么 y(L1)具有零均值，并且在零附近具有对称分布。所以现在我们有下面的等式。(4)当 f 为 ReLU 时。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/edd207e48d283b4f553c66f7ae5dcacc.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*HPon-xjB3s1cZOQTMhoOVw@2x.png"/></div></figure><p id="360f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">为什么 ReLU 在输出中加入标量 1/2？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/3942acb825686608ecee68fbc23e2603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2suXWiMXWizzFHtxsHapw.png"/></div></div></figure><p id="81cc" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">对于 ReLUs 家族，我们有通用的激活函数，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/2572b42863d43a9989c22c948bdcd642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*UGocinJ9TuTC6JWbCkGZUg@2x.png"/></div></figure><p id="0843" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">其中，<br/> 1) y_i 是第 I 个通道上非线性激活 f 的输入，<br/> 2) a_i 是控制负部分斜率的系数。</p><p id="18cd" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir"> ReLu </strong>是 a_i=0 时得到的<strong class="lu ir"> </strong>。合成激活函数的形式为 f(y_i)=max(0，y_i)。</p><p id="8ec8" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">ReLU 激活是一个零阈值，它使网络具有稀疏表示。例如，在权重的统一初始化之后，大约 50%的隐藏单元连续输出值是实零。</p><p id="b688" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">Relu 丢失了大量信息(被零值替换),这影响了积极的数据压缩。要保留数据，您可以使用 PRelu 或 LRelu，它们将 a_i 的斜率添加到轴的负侧，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1f07445cf4d92c00c6866fee02cb72e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*5KKLVLZKEMvztV0kr5_acA@2x.png"/></div></figure><p id="7b04" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">ReLu 激活函数只保留正半轴值，所以我们有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/edd207e48d283b4f553c66f7ae5dcacc.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*HPon-xjB3s1cZOQTMhoOVw@2x.png"/></div></figure><p id="59dd" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">将其代入等式(3)，我们得到等式。(5)作为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/41850ee7ff2a6c1e88444996c57509e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*FAW0NGrANeqRAezpqGBmjw.png"/></div></figure><p id="452e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在我们有了一个等式。具有层 l 的激活和层(l -1)的激活。将 L 层放在一起，从最后一层<strong class="lu ir"> <em class="ms"> L </em> </strong> <em class="ms">，</em>开始，我们得到下面的乘积作为等式。(5)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/b211df8542139b11cfcc88933f1aba65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3Tg0yK0rRlhFOYVGXgqvQ.png"/></div></div></figure><p id="0a94" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">注</strong> : x_l 是网络的输入，这就是为什么上面的等式。从 l = 2 开始。这个产品是初始化设计的关键。适当的初始化方法应该避免指数地减小或放大输入信号的幅度。因此，我们希望我们的产品采取适当的标量(如 1)。<br/>正向传递的充分条件(1) 为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/d02157fecf7cb7a9d0d806d890fb1277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*p6cuidBKlR6Z4c41gAB00g@2x.png"/></div></figure><p id="2712" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这导致零均值高斯分布，其标准偏差为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/cdcddc25ace919d2869123d9467be830.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*6UthNZO8gMqLJwSWEcuBnA@2x.png"/></div></figure><p id="c149" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们也初始化 bias，b = 0。<br/>对于第一层(l = 1)，我们应该只有 n_1Var[w_1] = 1，因为没有对输入应用 ReLU，所以我们不需要将输入减半。小因子 1/2，如果只是存在于一层也无所谓。</p><h2 id="92a4" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">反向传播</h2><p id="e3dc" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">反向过程是从最后一层到第一层反向计算的梯度。<br/>对于反向传播，卷积层 l 的梯度由等式 1 给出。(6)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/9683250247eb30aa91e981da0314bca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*VUlDNFtiEC5a4uTkwaLvKQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">the gradient of convolutional layer l</figcaption></figure><p id="37a0" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在哪里，</p><ol class=""><li id="40e4" class="my mz iq lu b lv ml ly mm lf nk lj nl ln nm mk nd ne nf ng bi translated">Ŵ是一个 c_l-by- n̂_l 矩阵，其中滤波器以反向传播的方式重新排列。请注意，W_l 和 Ŵ可以互相改造。</li><li id="b8e2" class="my mz iq lu b lv np ly nq lf nr lj ns ln nt mk nd ne nf ng bi translated">x 是 c_l 乘 1 的向量，表示层 l 的像素处的梯度。<br/><strong class="lu ir">【x = ∂e/∂x】</strong></li><li id="66be" class="my mz iq lu b lv np ly nq lf nr lj ns ln nt mk nd ne nf ng bi translated">y 表示 d 通道中 k 乘 k 的协同定位像素(长 x 宽),并被整形为 k 乘 1 的向量。<strong class="lu ir">∮y = ∂e/∂y.</strong><br/>我们用以下等式表示响应中的连接数</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/bd847941ba470fba8fc1199b324fc3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*fr0UrpOxzGiEJUAB1zL7eg@2x.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">no. of activations</figcaption></figure><p id="e0b4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在反向投影过程中，我们在网络中反向移动，所以下面的等式不成立。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/52790160f602fe7b5d83fd3a93c9007b.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*_C_h7tPlu1K0DCNMovuesg@2x.png"/></div></figure><p id="be7c" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">假设</strong></p><ol class=""><li id="c462" class="my mz iq lu b lv ml ly mm lf nk lj nl ln nm mk nd ne nf ng bi translated">w_l 和 y_l 相互独立，包含正态分布的随机数。</li><li id="b05d" class="my mz iq lu b lv np ly nq lf nr lj ns ln nt mk nd ne nf ng bi translated">当 w_l 由围绕零的对称分布初始化时，x_l 对所有 l 的均值为零。</li></ol><p id="cb46" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在反向传播中，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/23d22ded786d9471943d9e794a94b98f.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*I-FYkxX7gXBYqNzdBnj7Aw@2x.png"/></div></figure><p id="9c52" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">其中 f′是激活函数 f. <br/>的导数对于<strong class="lu ir"> ReLU </strong>的情况，f′(y _ l)是零或一，它们的概率相等。Pr(0)=1/2，Pr(1)=1/2</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/d689e1d16f57a6c6ac50e283e8217706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tQadJAXSx-Oag8oyB659ZA@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">a derivative of ReLU</figcaption></figure><p id="8eb4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们再次假设 f′(y _ l)和 x_(l+1)是相互独立的。<br/>正如我们在等式中看到的。(4) ReLU 将标量 1/2 加到其输出上。因此，我们有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/a17e0e248da0c5029fc856fb4b35f93f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*FT1m0oasxwIpC91h5GAYog@2x.png"/></div></figure><p id="49a0" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">还有，因为δy _ l 有一个零均值，f′(y _ l)= f′(y _ l)。通过取方程平方的期望值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/50e6c6459094706fd6032bd5e542075d.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*71sn5uEeckGDDquJOTZVUg@2x.png"/></div></figure><p id="2a8e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们得到下面的等式。在这里，标量 1/2 是 ReLU 的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/5c0d348d36d2e59d75fbb080d87450fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*EnKwTrumuo_FTE8HqYFTeg@2x.png"/></div></figure><p id="7053" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">然后我们计算方程中梯度的方差。(6):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/b8607df3d0fa59fc8096f1299c7baad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*igNGjYYoPfmMfN-Jti-PfA.png"/></div></div></figure><p id="f6c9" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">将 L 层放在一起，方差(⇼x _ L)为等式。(7)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/f40a590d95f439682665cb33b0f47922.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jbEExGE5d9Pr5C3kET7Krw.png"/></div></div></figure><p id="0a22" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们必须定义一个<strong class="lu ir">充分条件(2) </strong>来确保梯度不是指数地大/小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/15e17f29802a387b94db5cc5a6a1a3e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*S5PyHzc75P9tWZ_5qu0Asw@2x.png"/></div></figure><p id="73c7" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">注意，反向传播和正向传递的唯一区别是我们有<strong class="lu ir"> n̂_l </strong>而不是<strong class="lu ir"> n_l </strong>。<br/> k 为图像尺寸，d 为输入通道。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/321e591ae2e0573ca8b3eb7267da8b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*sJ1c5kkLVFp7YX9JUnCxKA@2x.png"/></div></figure><p id="c3ba" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">当前层 c_l 的通道与前一层 d_(l-1)的滤波器相同。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/ef152befc0a88b3b003db0464b0e429d.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*26AUjRrmHy4w-NlPTRvoTQ@2x.png"/></div></figure><p id="fd48" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">此外，该条件满足零均值高斯分布，其标准偏差为，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/c4f5e71ccb0f257e1d1ca24806c20b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*biHzW9zsvEn7fwsGVf_b5Q@2x.png"/></div></figure><p id="e8f5" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">对于第一层(l = 1)，我们不需要计算 x1，因为它代表图像。单个层的因素不会使整个产品呈指数级变大/变小。</p><p id="9d43" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">使用条件(1)或(2)来使神经模型收敛而没有爆炸/消失梯度是安全的。</p><p id="dc9e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">假设我们替换等式(2)中的条件。(7)我们得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/34eb467ed000ca2ac1a256f7b2d495d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*WBhutex3l0LQsCRRMN_TSw@2x.png"/></div></figure><p id="73e6" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">然后在方程中。(5)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/efb3dde75e8d033303d3bd864aa0652c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*edca5KoDCrObnLmPre4BCw@2x.png"/></div></div></figure><p id="a983" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">要寻找的常数不是标量 1，而是 c2/dL。即网络开始和结束时的信道数量。这不是一个使神经网络面临爆炸或消失梯度问题的递减数字。根据作者，该方程适当地缩放向前和向后通道，使得神经网络有效地收敛。</p><h2 id="29a6" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">Xavier 初始化</h2><p id="9bcd" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">Xavier 和 Bengio 早些时候提出了“Xavier”初始化，这种方法的推导是基于激活是线性的假设。这个假设对于 relu 是无效的。明凯和 Xavier 初始化之间的主要区别在于明凯处理 ReLU 非线性。Xavier 方法使用下面的条件，该条件从标准分布初始化权重</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e00002e425009a986555afdc7900aae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*WWwcELqXL3HcXE-k0SCcqA@2x.png"/></div></figure><p id="9c9e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这里，I 只不过是我们的层索引 l。当根据<strong class="lu ir"> Xavier 的归一化初始化</strong>在网络中上下移动时，保持激活和反向传播梯度稳定的归一化因子是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/17fc80904adecf5ca0040e1395e1bb85.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*F9WXYrOPhhsVYCKULGfG2A@2x.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Xavier’s Normalized Initialization</figcaption></figure><p id="54a6" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们可以使用 PyTorch 中的 Xavier 公式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/f933ad0af0ca26f1c95f0d7414da46ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sGMyixSneGj8E8kNJ4MXtA@2x.png"/></div></div></figure><h2 id="4a49" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">结论</h2><p id="0678" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">下图比较了泽维尔和明凯初始化方法与 ReLU 非线性的收敛性。在 22 和 30(27 conv，3 fc)层深度神经网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/f5f06adf099d7a5bb2e9671c5995bc6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_fNpIu7yuGmUuruqa_sWjg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">no.of epochs vs error rate in 22 layers deep neural model with ReLU (<a class="ae kv" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="daa0" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">正如我们所看到的，两种 init 方法都有助于 22 层深度神经网络的收敛，而使用 ReLU。但是明凯比泽维尔更早开始降低错误率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/6d01946c3b07ee8beb8903bb66fa444a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UWRzCyljqTAUY3XX8-AwUg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">no.of epochs vs error rate in 30 layers deep neural model with ReLU activation</figcaption></figure><p id="7993" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">正如我们所看到的，只有明凯初始化方法能够使深度模型收敛。Xavier 的方法完全停止了学习，梯度逐渐减小，导致完全没有收敛。</p><p id="dc3d" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">下面是如何使用明凯初始化策略初始化权重</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/fcb9b3871daeebb672302ae81ee4449f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UzNscjbeg4-m8XgvTPMTng@2x.png"/></div></div></figure><p id="df29" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">torch.nn 使用下面的公式来演示明凯初始化</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/4b39531bd29437da73d49397b1bb5ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*iBEl7T3YVn_3A3b2tsQg0w@2x.png"/></div></figure><p id="38f3" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">用- <br/> 1 初始化有两种模式。<strong class="lu ir">扇入</strong>(缺省)-在正向传递中保持权重的大小。<br/> 2。<strong class="lu ir">扇出</strong> —保留后向通道中权重的大小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/4083595291ea1722fc0d9383225ea48c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ixuHJpr5sI2V0QDafm2_Yg@2x.png"/></div></div></figure><p id="abd1" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们使用扇出，因为我们需要反向传播梯度是稳定的。明凯初始化对于更深层次的校正神经网络是必须的。</p><p id="874c" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">希望你喜欢。</p><h2 id="f978" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">参考</h2><ol class=""><li id="7c36" class="my mz iq lu b lv lw ly lz lf na lj nb ln nc mk nd ne nf ng bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank">深入研究整流器:在 ImageNet 分类上超越人类水平的性能，明凯等人，微软，2015 年</a></li><li id="3b95" class="my mz iq lu b lv np ly nq lf nr lj ns ln nt mk nd ne nf ng bi translated"><a class="ae kv" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank">理解训练深度前馈神经网络的困难，Glorot 等人，蒙特利尔大学，2010 年</a></li><li id="1d4f" class="my mz iq lu b lv np ly nq lf nr lj ns ln nt mk nd ne nf ng bi translated"><a class="ae kv" href="https://pouannes.github.io/blog/initialization/" rel="noopener ugc nofollow" target="_blank"> Pierre Ouannes，深度神经网络如何初始化？泽维尔和明凯初始化，2019 年 3 月 22 日</a></li><li id="55b6" class="my mz iq lu b lv np ly nq lf nr lj ns ln nt mk nd ne nf ng bi translated"><a class="ae kv" href="https://towardsdatascience.com/@jamesdell" rel="noopener" target="_blank">詹姆斯·德林杰</a>，<a class="ae kv" rel="noopener" target="_blank" href="/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">神经网络中的权重初始化:从基础到明凯的旅程</a></li><li id="a3c4" class="my mz iq lu b lv np ly nq lf nr lj ns ln nt mk nd ne nf ng bi translated">Jefkine，<a class="ae kv" href="https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/" rel="noopener ugc nofollow" target="_blank">深网初始化</a> <a class="ae kv" href="https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/" rel="noopener ugc nofollow" target="_blank">整流器案例</a>，2016 年 8 月 8 日</li></ol></div></div>    
</body>
</html>