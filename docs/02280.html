<html>
<head>
<title>Postmortem of Artificial Neural Networks using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PyTorch 对人工神经网络进行事后分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/postmortem-of-artificial-neural-networks-using-pytorch-fd429d4f5a93?source=collection_archive---------21-----------------------#2019-04-15">https://towardsdatascience.com/postmortem-of-artificial-neural-networks-using-pytorch-fd429d4f5a93?source=collection_archive---------21-----------------------#2019-04-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="89b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我与<strong class="jp ir"> <em class="kl">机器学习</em> </strong>的第一次互动是在我需要完成一项任务的时候，我只是在互联网上搜索使用机器学习的方法。我最终使用 scikit-learn API 用于 ann。我完成了任务，但对后端发生的事情一点概念都没有。于是，后来我开始上 MOOCs，开始探索这个<strong class="jp ir"> <em class="kl">“黑匣子”背后的数学。</em> </strong></p><p id="b326" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在所有这些探索之后，当我回来使用 ann 时(这次是使用 Keras)，我发现非常令人不安的是，像<strong class="jp ir"> <em class="kl"> model.fit </em> </strong>这样简单的一行代码正在后端自行完成所有事情。这是我决定转向一个框架的时候，这个框架给了程序员更大的权力，其代码更具可读性。</p><p id="dc9e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇博客中，我将使用 PyTorch 用代码和数学来解释人工神经网络。所以没有任何进一步的拖延，我们开始吧。</p><h1 id="00fd" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak"> PyTorch </strong></h1><p id="a3ce" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">这是一个深度学习框架，在研究人员中非常受欢迎，主要是因为它非常 pythonic 化且易于理解的语法。</p><p id="a0ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如 PyTorch 教程[1]所述，典型的模型训练遵循以下六个步骤。</p><ol class=""><li id="6e3a" class="lp lq iq jp b jq jr ju jv jy lr kc ls kg lt kk lu lv lw lx bi translated">定义模型(带有所有可学习的参数)</li><li id="a4f2" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">迭代输入数据集</li><li id="fc83" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">前进传球</li><li id="9769" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">计算损失</li><li id="74d7" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">反向传播梯度</li><li id="22d9" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">更新权重。</li></ol><p id="dfaa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们回顾一下这些步骤背后的代码(PyTorch)和数学。</p><h1 id="9d3b" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">定义模型</h1><p id="f15b" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在传统的人工神经网络中，应该有一个输入层和一个输出层以及可选的隐藏层。输入图层的大小与数据集中每个输入的大小相同。输出层的大小与类的数量相同，对于隐藏层，大小可能因问题而异。</p><p id="2073" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将要使用的模型包含一个大小为 2 的输入层、一个大小为 3 的隐藏层和一个大小为 1 的输出层。下面是 PyTorch 中的代码。请注意，该代码还包含函数 forward。我们将在稍后的<strong class="jp ir"> <em class="kl">向前传球</em> </strong>部分中讨论这一点。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="0552" class="mm kn iq mi b gy mn mo l mp mq">import torch</span><span id="cb06" class="mm kn iq mi b gy mr mo l mp mq"># defining model architecture<br/>class ANN(torch.nn.Module):<br/>    d<em class="kl">ef __init__</em>(<em class="kl">self</em>):<br/>    super(NueralNetwork, <em class="kl">self</em>).__init__()<br/><br/>    <em class="kl">self</em>.fc1 = torch.nn.Linear(2,3)<br/>    <em class="kl">self</em>.fc2 = torch.nn.Linear(3,1)<br/><br/><em class="kl">def </em>forward(<em class="kl">self</em>, x):<br/>    x = torch.sigmoid(<em class="kl">self</em>.fc1(x))<br/>    x = torch.sigmoid(<em class="kl">self</em>.fc2(x))<br/>    <em class="kl">return </em>x</span></pre><p id="3f61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的模型有 13 个可学习的参数，包括 9 个权重参数(6 个用于隐藏层，3 个用于输出层)和 4 个偏差参数(3 个用于隐藏层，1 个用于输出层)。以下是该模型的图示。</p><figure class="md me mf mg gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ms"><img src="../Images/26a59cb19a9ee2cba0eff2add8103655.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*OmDAqwRVxhGxJrGp69B3MA.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Artificial Neural network Architecture [2]</figcaption></figure><h1 id="3edf" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">数据集</strong></h1><p id="acc0" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">第二步是准备数据集。在这篇博客中，我们的数据集是 XOR 门，它包含 4 行，每行有 2 个输入和 1 个输出。我们使用下面这段代码来定义它。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="0a8b" class="mm kn iq mi b gy mn mo l mp mq"><em class="kl"># setting up data<br/></em>X = torch.tensor(([0, 0],[0, 1],[1, 0], [1, 1]), dtype=torch.float)<br/>y = torch.tensor(([0], [1], [1], [0]), dtype=torch.float)</span></pre><h1 id="1645" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">向前传球</strong></h1><p id="9abb" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">从这里开始，我们坚持数据集的第一行，即 X[0] = [0，0]和 y[0] = [0]。数学上，向前传球使用以下公式计算:<strong class="jp ir">y^ = w *输入+ b </strong>其中‘w’和‘b’分别代表权重和偏差。</p><p id="1079" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用下面这段代码来查看我们的初始权重和偏差。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="127f" class="mm kn iq mi b gy mn mo l mp mq">nueralNet = ANN()<br/>print(nueralNet)<br/><br/>params = list(nueralNet.parameters())<br/><em class="kl"># print(params)<br/># print(nueralNet.fc2.weight)<br/><br/></em>print("Weight vector for layer 1")<br/>print(params[0])<br/>print("Bias vector for layer 1")<br/>print(params[1])<br/>print("Weight vector for layer 2")<br/>print(params[2])<br/>print("Bias vector for layer 2")<br/>print(params[3])<br/>print('==========================')<br/><br/>out = nueralNet(X[0])</span></pre><p id="ca0f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这会打印出以下内容</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="160f" class="mm kn iq mi b gy mn mo l mp mq">NueralNetwork(<br/>  (fc1): Linear(in_features=2, out_features=3, bias=True)<br/>  (fc2): Linear(in_features=3, out_features=1, bias=True)<br/>)<br/>Weight vector for layer 1<br/>Parameter containing:<br/>tensor([[-0.3122,  0.5551],<br/>        [-0.2819, -0.0666],<br/>        [ 0.6712, -0.2915]], requires_grad=True)<br/>Bias vector for layer 1<br/>Parameter containing:<br/>tensor([ 0.2816, -0.4757, -0.4125], requires_grad=True)<br/>Weight vector for layer 2<br/>Parameter containing:<br/>tensor([[-0.5761, -0.2721,  0.4413]], requires_grad=True)<br/>Bias vector for layer 2<br/>Parameter containing:<br/>tensor([-0.1019], requires_grad=True)</span></pre><p id="3f79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将数据集的第一个实例传递给模型的代码行执行向前传递的计算。这将运行前面在 ANN 类中定义的 forward 函数。正向传递中的 Sigmoid 是由以下公式给出的激活函数:</p><figure class="md me mf mg gt mt gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/83bddd7f0ff774056f9762f998c4cbf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:146/0*Hnuzvx0jFREQN6Ya.gif"/></div></figure><p id="99cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是向前传球背后的数学计算。</p><figure class="md me mf mg gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nf"><img src="../Images/d6f8380fbb8ddebd86de6f15bdb7c01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xpv1V6CXu1D8Bs402qn6ag.jpeg"/></div></div></figure><p id="b975" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这导致 y^ = 0.4112</p><h1 id="bca9" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">计算损失</strong></h1><p id="dba6" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在向前传递之后，我们根据原始标签评估由我们的模型创建的输出。这是通过计算损耗来实现的，损耗实质上是测量标签和当前输出之间的距离。在我们的实现中，我们将使用流行的均方误差。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="9e45" class="mm kn iq mi b gy mn mo l mp mq"><em class="kl">import </em>torch.optim <em class="kl">as </em>optim<br/><br/><em class="kl"># create your optimizer<br/></em>optimizer = optim.SGD(nueralNet.parameters(), lr=0.01)<br/>criterion = torch.nn.MSELoss()<br/><em class="kl"># in your training loop:<br/></em>optimizer.zero_grad()   <em class="kl"># zero the gradient buffers<br/></em>loss = criterion(out, y[0])<br/>print('Mean squared error or loss = ', loss)</span></pre><p id="abb4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数学上，它由下式给出</p><figure class="md me mf mg gt mt gh gi paragraph-image"><div class="ab gu cl ng"><img src="../Images/9e88407607846d617a90a7ff62ac2883.png" data-original-src="https://miro.medium.com/v2/1*3VJyfU1qBqoHwaDJm3KAKA.gif"/></div></figure><p id="de42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在我们的例子中，因为我们只有一个输入实例，所以损失是= 0.1691。</p><h1 id="d043" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">反向传播渐变</strong></h1><p id="9560" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在计算了损失之后，我们需要一个最小化它的方法来改进我们的方法。为此，我们首先计算梯度，然后将它们传播回来以更新可学习的参数。这是通过 PyTorch 中的以下代码完成的</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="8a5f" class="mm kn iq mi b gy mn mo l mp mq">loss.backward()<br/><br/>optimizer.step()    <em class="kl"># Does the weight update</em></span><span id="d6aa" class="mm kn iq mi b gy mr mo l mp mq">print(<strong class="mi ir">"Weight vector for layer 1"</strong>)<br/>print(params[0])<br/>print(<strong class="mi ir">"Bias vector for layer 1"</strong>)<br/>print(params[1])<br/>print(<strong class="mi ir">"Weight vector for layer 2"</strong>)<br/>print(params[2])<br/>print(<strong class="mi ir">"Bias vector for layer 2"</strong>)<br/>print(params[3])<br/>print(<strong class="mi ir">'=========================='</strong>)</span></pre><p id="fe7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我鼓励你通过这个链接(<a class="ae nh" href="https://www.youtube.com/watch?v=Ilg3gGewQ5U" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=Ilg3gGewQ5U</a>)来深入理解反向传播和权重更新的工作原理。作为一个有趣的练习，在这个例子中尝试计算更新后的梯度和参数。</p></div></div>    
</body>
</html>