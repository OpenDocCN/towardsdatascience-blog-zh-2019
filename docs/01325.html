<html>
<head>
<title>Using word2vec to Analyze News Headlines and Predict Article Success</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 word2vec 分析新闻标题并预测文章成功</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751?source=collection_archive---------2-----------------------#2019-03-03">https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751?source=collection_archive---------2-----------------------#2019-03-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="d40d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/in-depth-analysis/home" rel="noopener">深入分析</a></h2><div class=""/><div class=""><h2 id="4847" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">文章标题的单词嵌入能预测受欢迎程度吗？关于情绪和股票的关系，我们能了解到什么？word2vec 可以帮助我们回答这些问题，还有更多。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/fca4a49c4816f015134a124508968abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TofsJKSMNbQxc1h4A3Ed2w.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Sentiment distributions for popular news websites</figcaption></figure><p id="4ff6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">单词嵌入是表示单词以及文档(单词集合)中包含的潜在信息的一种强大方式。使用新闻文章标题的数据集，其中包括来源、情感、主题和受欢迎程度(份额数)的特征，我开始查看我们可以通过文章各自的嵌入来了解它们之间的关系。</p><p id="ff77" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">该项目的目标是:</p><ul class=""><li id="8555" class="ma mb iq lg b lh li lk ll ln mc lr md lv me lz mf mg mh mi bi translated">使用 NLTK 预处理/清理文本数据</li><li id="46e9" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz mf mg mh mi bi translated">使用 word2vec 创建单词和标题嵌入，然后使用 t-SNE 将它们可视化为集群</li><li id="7772" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz mf mg mh mi bi translated">形象化标题情绪和文章流行度的关系</li><li id="a756" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz mf mg mh mi bi translated">尝试从嵌入和其他可用特征预测文章流行度</li><li id="9abe" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz mf mg mh mi bi translated">使用模型堆叠来提高流行度模型的性能(这一步并不成功，但仍然是一个有价值的实验！)</li></ul><p id="4a03" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">整个笔记本托管在<a class="ae mo" href="https://nbviewer.jupyter.org/github/chambliss/Notebooks/blob/master/Word2Vec_News_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>，使用<a class="ae mo" href="https://nbviewer.jupyter.org/" rel="noopener ugc nofollow" target="_blank"> nbviewer </a>。</p><h1 id="a2b8" class="mp mq iq bd mr ms mt mu mv mw mx my mz kf na kg nb ki nc kj nd kl ne km nf ng bi translated">导入和预处理</h1><p id="16ac" class="pw-post-body-paragraph le lf iq lg b lh nh ka lj lk ni kd lm ln nj lp lq lr nk lt lu lv nl lx ly lz ij bi translated">我们将从进口开始:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="26c1" class="nr mq iq nn b gy ns nt l nu nv"><strong class="nn ja">import</strong> <strong class="nn ja">pandas</strong> <strong class="nn ja">as</strong> <strong class="nn ja">pd</strong><br/><strong class="nn ja">import</strong> <strong class="nn ja">gensim</strong><br/><strong class="nn ja">import</strong> <strong class="nn ja">seaborn</strong> <strong class="nn ja">as</strong> <strong class="nn ja">sns</strong><br/><strong class="nn ja">import</strong> <strong class="nn ja">matplotlib.pyplot</strong> <strong class="nn ja">as</strong> <strong class="nn ja">plt</strong><br/><strong class="nn ja">import</strong> <strong class="nn ja">numpy</strong> <strong class="nn ja">as</strong> <strong class="nn ja">np</strong><br/><strong class="nn ja">import</strong> <strong class="nn ja">xgboost</strong> <strong class="nn ja">as</strong> <strong class="nn ja">xgb</strong></span></pre><p id="cee8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后读入数据:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="c76e" class="nr mq iq nn b gy ns nt l nu nv">main_data = pd.read_csv('News_Final.csv')<br/>main_data.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/6198597707f8428bc47cfb9f2f8b8d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q7qgx7LUX2j0AwUvegDB0g.png"/></div></div></figure><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="3b42" class="nr mq iq nn b gy ns nt l nu nv"># Grab all the titles <br/>article_titles = main_data['Title']</span><span id="b234" class="nr mq iq nn b gy nx nt l nu nv"><em class="ny"># Create a list of strings, one for each title</em><br/>titles_list = [title <strong class="nn ja">for</strong> title <strong class="nn ja">in</strong> article_titles]<br/><br/><em class="ny"># Collapse the list of strings into a single long string for processing</em><br/>big_title_string = ' '.join(titles_list)<br/><br/><strong class="nn ja">from</strong> <strong class="nn ja">nltk.tokenize</strong> <strong class="nn ja">import</strong> word_tokenize<br/><br/><em class="ny"># Tokenize the string into words</em><br/>tokens = word_tokenize(big_title_string)<br/><br/><em class="ny"># Remove non-alphabetic tokens, such as punctuation</em><br/>words = [word.lower() <strong class="nn ja">for</strong> word <strong class="nn ja">in</strong> tokens <strong class="nn ja">if</strong> word.isalpha()]<br/><br/><em class="ny"># Filter out stopwords</em><br/><strong class="nn ja">from</strong> <strong class="nn ja">nltk.corpus</strong> <strong class="nn ja">import</strong> stopwords<br/>stop_words = set(stopwords.words('english'))<br/><br/>words = [word <strong class="nn ja">for</strong> word <strong class="nn ja">in</strong> words <strong class="nn ja">if</strong> <strong class="nn ja">not</strong> word <strong class="nn ja">in</strong> stop_words]<br/><br/><em class="ny"># Print first 10 words</em><br/>words[:10]</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/7259cc90f4b51f4da51bb3ab48fbd06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/format:webp/1*fvXkma60AP9rF1v5RK9plQ.png"/></div></figure><p id="cfd8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">接下来，我们需要加载预先训练好的 word2vec 模型。你可以在这里找到几款这样的<a class="ae mo" href="https://github.com/RaRe-Technologies/gensim-data" rel="noopener ugc nofollow" target="_blank">。由于这是一个新闻数据集，所以我使用了谷歌新闻模型，该模型被训练了大约 1000 亿个单词(哇)。</a></p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="9a40" class="nr mq iq nn b gy ns nt l nu nv"><em class="ny"># Load word2vec model (trained on an enormous Google corpus)</em><br/>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = <strong class="nn ja">True</strong>) <br/><br/><em class="ny"># Check dimension of word vectors</em><br/>model.vector_size</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/4bdd663d418f363714b249d5604f1d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:60/format:webp/1*FJ3as4yl9EL8hb5vJAw2oA.png"/></div></figure><p id="fab8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">所以模型会生成 300 维的词向量，我们要做的就是创建一个向量，让它通过模型。每个向量看起来像这样:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="4528" class="nr mq iq nn b gy ns nt l nu nv">economy_vec = model['economy']<br/>economy_vec[:20] <em class="ny"># First 20 components</em></span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/4caf90d6f07433306705795af51a0b02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*E5MqTRO275UCcGK9eHPOlQ.png"/></div></figure><p id="2c49" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">word2vec(可以理解)不能从一个不在其词汇表中的单词创建向量。正因为如此，我们需要在创建单词向量的完整列表时指定“if word in model.vocab”。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="d16f" class="nr mq iq nn b gy ns nt l nu nv"><em class="ny"># Filter the list of vectors to include only those that Word2Vec has a vector for</em><br/>vector_list = [model[word] <strong class="nn ja">for</strong> word <strong class="nn ja">in</strong> words <strong class="nn ja">if</strong> word <strong class="nn ja">in</strong> model.vocab]<br/><br/><em class="ny"># Create a list of the words corresponding to these vectors</em><br/>words_filtered = [word <strong class="nn ja">for</strong> word <strong class="nn ja">in</strong> words <strong class="nn ja">if</strong> word <strong class="nn ja">in</strong> model.vocab]<br/><br/><em class="ny"># Zip the words together with their vector representations</em><br/>word_vec_zip = zip(words_filtered, vector_list)<br/><br/><em class="ny"># Cast to a dict so we can turn it into a DataFrame</em><br/>word_vec_dict = dict(word_vec_zip)<br/>df = pd.DataFrame.from_dict(word_vec_dict, orient='index')</span><span id="bf92" class="nr mq iq nn b gy nx nt l nu nv">df.head(3)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/e8eecc794ecf351877fd3ede6ff9e58f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zor-xZZwdNzkaW3z9aoJ5Q.png"/></div></div></figure><h1 id="2ae0" class="mp mq iq bd mr ms mt mu mv mw mx my mz kf na kg nb ki nc kj nd kl ne km nf ng bi translated">基于 t-SNE 的维数约简</h1><p id="2abe" class="pw-post-body-paragraph le lf iq lg b lh nh ka lj lk ni kd lm ln nj lp lq lr nk lt lu lv nl lx ly lz ij bi translated">接下来，我们将使用 t-SNE 挤压(阅读:对这些单词向量进行降维)，看看是否有任何模式出现。如果你不熟悉 t-SNE 和它的解释，看看<a class="ae mo" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank">这篇关于 t-SNE 的优秀的、互动的 distill.pub 文章</a>。</p><p id="84d6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">摆弄 SNE 霸王龙的参数很重要，因为不同的值会产生非常不同的结果。我测试了 0 到 100 之间的几个困惑值，发现每次都产生大致相同的形状。我还测试了几个介于 20 和 400 之间的学习率，并决定保持默认的学习率(200)。</p><p id="4e7c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">出于可见性(和处理时间)的考虑，我使用了 400 个单词向量，而不是大约 20，000 个单词向量。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="722a" class="nr mq iq nn b gy ns nt l nu nv"><strong class="nn ja">from</strong> <strong class="nn ja">sklearn.manifold</strong> <strong class="nn ja">import</strong> TSNE<br/><br/><em class="ny"># Initialize t-SNE</em><br/>tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)<br/><br/><em class="ny"># Use only 400 rows to shorten processing time</em><br/>tsne_df = tsne.fit_transform(df[:400])</span></pre><p id="a453" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，我们准备绘制缩减的单词向量数组。我使用<code class="fe od oe of nn b">adjust_text</code>智能地将单词分开，以提高可读性:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="b059" class="nr mq iq nn b gy ns nt l nu nv">sns.set()</span><span id="ef51" class="nr mq iq nn b gy nx nt l nu nv"># Initialize figure<br/>fig, ax = plt.subplots(figsize = (11.7, 8.27))<br/>sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)<br/><br/><em class="ny"># Import adjustText, initialize list of texts</em><br/><strong class="nn ja">from</strong> <strong class="nn ja">adjustText</strong> <strong class="nn ja">import</strong> adjust_text<br/>texts = []<br/>words_to_plot = list(np.arange(0, 400, 10))<br/><br/><em class="ny"># Append words to list</em><br/><strong class="nn ja">for</strong> word <strong class="nn ja">in</strong> words_to_plot:<br/>    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))<br/>    <br/><em class="ny"># Plot text using adjust_text (because overlapping text is hard to read)</em><br/>adjust_text(texts, force_points = 0.4, force_text = 0.4, <br/>            expand_points = (2,1), expand_text = (1,2),<br/>            arrowprops = dict(arrowstyle = "-", color = 'black', lw = 0.5))<br/><br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/36db52a4f1daca2ab263b9ee928fca4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*xqWwkmbNgZ12O9JmlyJZsg.png"/></div></div></figure><p id="398a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果你有兴趣尝试一下<code class="fe od oe of nn b">adjust_text</code>来满足自己的绘图需求，你可以在这里找到它<a class="ae mo" href="https://github.com/Phlya/adjustText" rel="noopener ugc nofollow" target="_blank">。一定要用 came case<code class="fe od oe of nn b">adjustText</code>导入，请注意<code class="fe od oe of nn b">adjustText</code>目前不兼容<code class="fe od oe of nn b">matplotlib</code> 3.0 以上版本。</a></p><p id="4830" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">令人鼓舞的是，即使向量嵌入已经减少到二维，我们也看到某些项目聚集在一起。例如，我们在左/左上角有<strong class="lg ja">个月</strong>，我们在底部有<strong class="lg ja">个公司财务术语</strong>，我们在中间有更多的<strong class="lg ja">个通用的、非主题性的词</strong>(比如‘full’，‘really’，‘slew’)。</p><p id="90f5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">请注意，如果我们用不同的参数再次运行 t-SNE，我们可能会观察到一些类似的结果，但我们不能保证看到完全相同的模式。t-SNE 不是决定性的。相关地，聚类的紧密度和聚类之间的距离并不总是有意义的。它主要是作为一种探索工具，而不是相似性的决定性指标。</p><h1 id="6003" class="mp mq iq bd mr ms mt mu mv mw mx my mz kf na kg nb ki nc kj nd kl ne km nf ng bi translated">平均单词嵌入</h1><p id="d080" class="pw-post-body-paragraph le lf iq lg b lh nh ka lj lk ni kd lm ln nj lp lq lr nk lt lu lv nl lx ly lz ij bi translated">我们已经了解了单词嵌入如何应用于这个数据集。现在我们可以转移到一些更有趣的 ML 应用程序上:找到聚集在一起的标题，看看会出现什么样的模式。</p><p id="dfec" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们可以使用一个更简单的<a class="ae mo" href="https://stackoverflow.com/questions/45234310/doc2vec-worse-than-mean-or-sum-of-word2vec-vectors" rel="noopener ugc nofollow" target="_blank">(有时甚至更有效)</a>技巧:对每个文档中单词向量的嵌入进行平均，而不是使用 Doc2Vec，doc 2 vec 没有可用的预训练模型，因此需要漫长的训练过程。在我们的例子中，文档指的是标题。</p><p id="3f3c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们需要重做预处理步骤来保持标题完整——正如我们将看到的，这比拆分单词要复杂一些。谢天谢地，Dimitris Spathis <a class="ae mo" href="https://github.com/sdimi/average-word2vec/blob/master/notebook.ipynb" rel="noopener ugc nofollow" target="_blank">已经创建了一系列函数</a>，我发现它们非常适合这个用例。谢谢你，迪米特里斯！</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="1670" class="nr mq iq nn b gy ns nt l nu nv"><strong class="nn ja">def</strong> document_vector(word2vec_model, doc):<br/>    <em class="ny"># remove out-of-vocabulary words</em><br/>    doc = [word <strong class="nn ja">for</strong> word <strong class="nn ja">in</strong> doc <strong class="nn ja">if</strong> word <strong class="nn ja">in</strong> model.vocab]<br/>    <strong class="nn ja">return</strong> np.mean(model[doc], axis=0)<br/><br/><em class="ny"># Our earlier preprocessing was done when we were dealing only with word vectors</em><br/><em class="ny"># Here, we need each document to remain a document </em><br/><strong class="nn ja">def</strong> preprocess(text):<br/>    text = text.lower()<br/>    doc = word_tokenize(text)<br/>    doc = [word <strong class="nn ja">for</strong> word <strong class="nn ja">in</strong> doc <strong class="nn ja">if</strong> word <strong class="nn ja">not</strong> <strong class="nn ja">in</strong> stop_words]<br/>    doc = [word <strong class="nn ja">for</strong> word <strong class="nn ja">in</strong> doc <strong class="nn ja">if</strong> word.isalpha()] <br/>    <strong class="nn ja">return</strong> doc<br/><br/><em class="ny"># Function that will help us drop documents that have no word vectors in word2vec</em><br/><strong class="nn ja">def</strong> has_vector_representation(word2vec_model, doc):<br/>    <em class="ny">"""check if at least one word of the document is in the</em><br/><em class="ny">    word2vec dictionary"""</em><br/>    <strong class="nn ja">return</strong> <strong class="nn ja">not</strong> all(word <strong class="nn ja">not</strong> <strong class="nn ja">in</strong> word2vec_model.vocab <strong class="nn ja">for</strong> word <strong class="nn ja">in</strong> doc)<br/><br/><em class="ny"># Filter out documents</em><br/><strong class="nn ja">def</strong> filter_docs(corpus, texts, condition_on_doc):<br/>    <em class="ny">"""</em><br/><em class="ny">    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.</em><br/><em class="ny">    """</em><br/>    number_of_docs = len(corpus)<br/><br/>    <strong class="nn ja">if</strong> texts <strong class="nn ja">is</strong> <strong class="nn ja">not</strong> <strong class="nn ja">None</strong>:<br/>        texts = [text <strong class="nn ja">for</strong> (text, doc) <strong class="nn ja">in</strong> zip(texts, corpus)<br/>                 <strong class="nn ja">if</strong> condition_on_doc(doc)]<br/><br/>    corpus = [doc <strong class="nn ja">for</strong> doc <strong class="nn ja">in</strong> corpus <strong class="nn ja">if</strong> condition_on_doc(doc)]<br/><br/>    print("<strong class="nn ja">{}</strong> docs removed".format(number_of_docs - len(corpus)))<br/><br/>    <strong class="nn ja">return</strong> (corpus, texts)</span></pre><p id="8754" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在我们将使用这些来进行处理:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="935c" class="nr mq iq nn b gy ns nt l nu nv"><em class="ny"># Preprocess the corpus</em><br/>corpus = [preprocess(title) <strong class="nn ja">for</strong> title <strong class="nn ja">in</strong> titles_list]<br/><br/><em class="ny"># Remove docs that don't include any words in W2V's vocab</em><br/>corpus, titles_list = filter_docs(corpus, titles_list, <strong class="nn ja">lambda</strong> doc: has_vector_representation(model, doc))<br/><br/><em class="ny"># Filter out any empty docs</em><br/>corpus, titles_list = filter_docs(corpus, titles_list, <strong class="nn ja">lambda</strong> doc: (len(doc) != 0))</span><span id="7f3f" class="nr mq iq nn b gy nx nt l nu nv">x = []<br/><strong class="nn ja">for</strong> doc <strong class="nn ja">in</strong> corpus: <em class="ny"># append the vector for each document</em><br/>    x.append(document_vector(model, doc))<br/>    <br/>X = np.array(x) <em class="ny"># list to array</em></span></pre><h1 id="5aba" class="mp mq iq bd mr ms mt mu mv mw mx my mz kf na kg nb ki nc kj nd kl ne km nf ng bi translated">t-SNE，第二轮:文档向量</h1><p id="c918" class="pw-post-body-paragraph le lf iq lg b lh nh ka lj lk ni kd lm ln nj lp lq lr nk lt lu lv nl lx ly lz ij bi translated">现在我们已经成功地创建了文档向量数组，让我们看看用 t-SNE 绘制它们时是否能得到类似的有趣结果。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="9764" class="nr mq iq nn b gy ns nt l nu nv"><em class="ny"># Initialize t-SNE</em><br/>tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)<br/><br/><em class="ny"># Again use only 400 rows to shorten processing time</em><br/>tsne_df = tsne.fit_transform(X[:400])</span><span id="ea03" class="nr mq iq nn b gy nx nt l nu nv">fig, ax = plt.subplots(figsize = (14, 10))<br/>sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)<br/><br/><strong class="nn ja">from</strong> <strong class="nn ja">adjustText</strong> <strong class="nn ja">import</strong> adjust_text<br/>texts = []<br/>titles_to_plot = list(np.arange(0, 400, 40)) <em class="ny"># plots every 40th title in first 400 titles</em><br/><br/><em class="ny"># Append words to list</em><br/><strong class="nn ja">for</strong> title <strong class="nn ja">in</strong> titles_to_plot:<br/>    texts.append(plt.text(tsne_df[title, 0], tsne_df[title, 1], titles_list[title], fontsize = 14))<br/>    <br/><em class="ny"># Plot text using adjust_text</em><br/>adjust_text(texts, force_points = 0.4, force_text = 0.4, <br/>            expand_points = (2,1), expand_text = (1,2),<br/>            arrowprops = dict(arrowstyle = "-", color = 'black', lw = 0.5))<br/><br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/e0d83a24efa0db560429de742a1cade4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JibF2cykRgbFvwcZxCCw3w.png"/></div></div></figure><p id="134a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">挺有意思的！我们可以看到，t-SNE 将文档向量折叠到一个维度空间中，在这个空间中，文档根据其内容是与国家、世界领导人和外交事务有关，还是与技术公司有关而展开。</p><p id="2a9b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在让我们来探讨一下文章流行度。人们普遍认为，一篇文章的标题越煽情或越吸引人，它就越有可能被分享，对吗？接下来，我们将看看在这个特定的数据集中是否有这方面的证据。</p><h1 id="e428" class="mp mq iq bd mr ms mt mu mv mw mx my mz kf na kg nb ki nc kj nd kl ne km nf ng bi translated">流行度和情感分析</h1><p id="20e2" class="pw-post-body-paragraph le lf iq lg b lh nh ka lj lk ni kd lm ln nj lp lq lr nk lt lu lv nl lx ly lz ij bi translated">首先，我们需要删除所有没有流行度测量或来源的文章。流行度的零测量在该数据中表示为-1。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="6199" class="nr mq iq nn b gy ns nt l nu nv"><em class="ny"># Drop all the rows where the article popularities are unknown (this is only about 11% of the data)</em><br/>main_data = main_data.drop(main_data[(main_data.Facebook == -1) | <br/>                                     (main_data.GooglePlus == -1) | <br/>                                     (main_data.LinkedIn == -1)].index)<br/><br/><em class="ny"># Also drop all rows where we don't know the source</em><br/>main_data = main_data.drop(main_data[main_data['Source'].isna()].index)<br/><br/>main_data.shape</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/6108a2a7cbeb0b04729b7e24a18bb3ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*A8kyo6CGtD8u4p85xW9bXg.png"/></div></figure><p id="3e3d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们仍然有 81，000 篇文章要处理，所以让我们看看是否可以找到情绪和股票数量之间的关联。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="9c16" class="nr mq iq nn b gy ns nt l nu nv">fig, ax = plt.subplots(1, 3, figsize=(15, 10))<br/><br/>subplots = [a <strong class="nn ja">for</strong> a <strong class="nn ja">in</strong> ax]<br/>platforms = ['Facebook', 'GooglePlus', 'LinkedIn']<br/>colors = list(sns.husl_palette(10, h=.5)[1:4]) <br/><br/><strong class="nn ja">for</strong> platform, subplot, color <strong class="nn ja">in</strong> zip(platforms, subplots, colors):<br/>    sns.scatterplot(x = main_data[platform], y = main_data['SentimentTitle'], ax=subplot, color=color)<br/>    subplot.set_title(platform, fontsize=18)<br/>    subplot.set_xlabel('') <br/>    <br/>fig.suptitle('Plot of Popularity (Shares) by Title Sentiment', fontsize=24)<br/><br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/a99416260b822f137e9ef63e92a6999b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rhYJ7sMJxX75DnI99Q4XvA.png"/></div></div></figure><p id="2a7e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">很难确定这里是否有任何关系，因为一些文章在它们的份额计数方面是显著的异常值。让我们试着对 x 轴进行对数变换，看看我们是否能揭示任何模式。我们还将使用一个 regplot，因此<code class="fe od oe of nn b">seaborn</code>将为每个图覆盖一个线性回归。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="6c22" class="nr mq iq nn b gy ns nt l nu nv"><em class="ny"># Our data has over 80,000 rows, so let's also subsample it to make the log-transformed scatterplot easier to read</em><br/><br/>subsample = main_data.sample(5000)<br/><br/>fig, ax = plt.subplots(1, 3, figsize=(15, 10))<br/><br/>subplots = [a <strong class="nn ja">for</strong> a <strong class="nn ja">in</strong> ax]<br/><br/><strong class="nn ja">for</strong> platform, subplot, color <strong class="nn ja">in</strong> zip(platforms, subplots, colors):<br/>    <em class="ny"># Regression plot, so we can gauge the linear relationship</em><br/>    sns.regplot(x = np.log(subsample[platform] + 1), y = subsample['SentimentTitle'], <br/>                ax=subplot, <br/>                color=color,<br/>                <em class="ny"># Pass an alpha value to regplot's scatterplot call</em><br/>                scatter_kws={'alpha':0.5})<br/>    <br/>    <em class="ny"># Set a nice title, get rid of x labels</em><br/>    subplot.set_title(platform, fontsize=18)<br/>    subplot.set_xlabel('') <br/>    <br/>fig.suptitle('Plot of log(Popularity) by Title Sentiment', fontsize=24)<br/><br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/3c8239a145725a26a722621a07e4de14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*meozBLCDdbs0Q8x-shOIHA.png"/></div></div></figure><p id="b5fc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">与我们可能预期的相反(来自我们对高度情绪化、点击量大的标题的想法)，在这个数据集中，我们发现标题情绪和文章受欢迎程度(通过分享数量来衡量)之间没有关系。</p><p id="885e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了更清楚地了解流行度本身是什么样子，让我们按平台绘制一个最终的 log(流行度)图。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="3ff6" class="nr mq iq nn b gy ns nt l nu nv">fig, ax = plt.subplots(3, 1, figsize=(15, 10))<br/><br/>subplots = [a <strong class="nn ja">for</strong> a <strong class="nn ja">in</strong> ax]<br/><br/><strong class="nn ja">for</strong> platform, subplot, color <strong class="nn ja">in</strong> zip(platforms, subplots, colors):<br/>  <br/>    sns.distplot(np.log(main_data[platform] + 1), ax=subplot, color=color, kde_kws={'shade':<strong class="nn ja">True</strong>})<br/>    <br/>    <em class="ny"># Set a nice title, get rid of x labels</em><br/>    subplot.set_title(platform, fontsize=18)<br/>    subplot.set_xlabel('') <br/>    <br/>fig.suptitle('Plot of Popularity by Platform', fontsize=24)<br/><br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/094edd74edbea3deb1c22ee5f29831d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*butzdo-jj5ZSOG3eHonb6w.png"/></div></div></figure><p id="58aa" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">作为我们探索的最后一部分，让我们看看情绪本身。出版商之间似乎有所不同吗？</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="8971" class="nr mq iq nn b gy ns nt l nu nv"><em class="ny"># Get the list of top 12 sources by number of articles</em><br/>source_names = list(main_data['Source'].value_counts()[:12].index)<br/>source_colors = list(sns.husl_palette(12, h=.5))<br/><br/>fig, ax = plt.subplots(4, 3, figsize=(20, 15), sharex=<strong class="nn ja">True</strong>, sharey=<strong class="nn ja">True</strong>)<br/><br/>ax = ax.flatten()<br/><strong class="nn ja">for</strong> ax, source, color <strong class="nn ja">in</strong> zip(ax, source_names, source_colors):<br/>    sns.distplot(main_data.loc[main_data['Source'] == source]['SentimentTitle'],<br/>                               ax=ax, color=color, kde_kws={'shade':<strong class="nn ja">True</strong>})<br/>    ax.set_title(source, fontsize=14)<br/>    ax.set_xlabel('')<br/>    <br/>plt.xlim(-0.75, 0.75)<br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/fca4a49c4816f015134a124508968abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TofsJKSMNbQxc1h4A3Ed2w.png"/></div></div></figure><p id="5f8f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这些分布看起来相当相似，但是当它们都在不同的地块上时，很难说出<em class="ny">和</em>有多相似。让我们试着把它们都叠加在一个图上。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="56e3" class="nr mq iq nn b gy ns nt l nu nv"><em class="ny"># Overlay each density curve on the same plot for closer comparison</em><br/><br/>fig, ax = plt.subplots(figsize=(12, 8))<br/><br/><strong class="nn ja">for</strong> source, color <strong class="nn ja">in</strong> zip(source_names, source_colors):<br/>    sns.distplot(main_data.loc[main_data['Source'] == source]['SentimentTitle'],<br/>                               ax=ax, hist=<strong class="nn ja">False</strong>, label=source, color=color)<br/>    ax.set_xlabel('')<br/>    <br/>plt.xlim(-0.75, 0.75)<br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/cd81e9a98d38ff95c338a25d4bc6924d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q5jJhdYFpPTH-5UFQLDQcA.png"/></div></div></figure><p id="8a9c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们看到，来源对文章标题的情感分布非常相似——就正面或负面标题而言，似乎没有任何一个来源是异常的。相反，所有 12 个最常见的来源都以 0 为中心分布，尾部大小适中。但这能说明全部情况吗？让我们再来看看这些数字:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="feaf" class="nr mq iq nn b gy ns nt l nu nv"># Group by Source, then get descriptive statistics for title sentiment<br/>source_info = main_data.groupby('Source')['SentimentTitle'].describe()</span><span id="ab94" class="nr mq iq nn b gy nx nt l nu nv"># Recall that `source_names` contains the top 12 sources<br/># We'll also sort by highest standard deviation<br/>source_info.loc[source_names].sort_values('std', ascending=False)[['std', 'min', 'max']]</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/43f4d35a8503accf3f0fce71b8205f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*WXCAjBg_NDPNnA2P9w1zJw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">WSJ has both the highest standard deviation and the largest range.</figcaption></figure><p id="fc1c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们可以一眼看出，与其他任何顶级来源相比,《华尔街日报》的标准差最高，范围最大，最低情绪最低。这表明《华尔街日报》的文章标题可能异常负面。为了严格验证这一点，需要进行假设检验，这超出了本文的范围，但这是一个有趣的潜在发现和未来方向。</p><h1 id="9727" class="mp mq iq bd mr ms mt mu mv mw mx my mz kf na kg nb ki nc kj nd kl ne km nf ng bi translated">流行预测</h1><p id="fcb2" class="pw-post-body-paragraph le lf iq lg b lh nh ka lj lk ni kd lm ln nj lp lq lr nk lt lu lv nl lx ly lz ij bi translated">我们为建模准备数据的第一个任务是用各自的标题重新连接文档向量。幸运的是，当我们预处理语料库时，我们同时处理了<code class="fe od oe of nn b">corpus</code>和<code class="fe od oe of nn b">titles_list</code>，所以向量和它们所代表的标题仍然匹配。同时，在<code class="fe od oe of nn b">main_df</code>中，我们已经删除了所有流行度为-1 的文章，所以我们需要删除代表这些文章标题的向量。</p><p id="520e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在这台计算机上，按原样在这些巨大的向量上训练一个模型是不可能的，但我们会看看我们可以做些什么来降低维度。我还将从发布日开始设计一个新特性:“DaysSinceEpoch”，它基于 Unix 时间(在这里阅读更多<a class="ae mo" href="https://en.wikipedia.org/wiki/Unix_time" rel="noopener ugc nofollow" target="_blank"/>)。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="f38d" class="nr mq iq nn b gy ns nt l nu nv"><strong class="nn ja">import</strong> <strong class="nn ja">datetime</strong><br/><br/><em class="ny"># Convert publish date column to make it compatible with other datetime objects</em><br/><br/>main_data['PublishDate'] = pd.to_datetime(main_data['PublishDate'])<br/><br/><em class="ny"># Time since Linux Epoch</em><br/>t = datetime.datetime(1970, 1, 1)<br/><br/><em class="ny"># Subtract this time from each article's publish date</em><br/>main_data['TimeSinceEpoch'] = main_data['PublishDate'] - t<br/><br/><em class="ny"># Create another column for just the days from the timedelta objects </em><br/>main_data['DaysSinceEpoch'] = main_data['TimeSinceEpoch'].astype('timedelta64[D]')<br/><br/>main_data['TimeSinceEpoch'].describe()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/1a855124589dd86a64ba52f77c7177bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*_3UdVG_NO8PWTjRNuXZc8A.png"/></div></figure><p id="1693" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">正如我们所看到的，所有这些文章都是在 250 天之内发表的。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="3956" class="nr mq iq nn b gy ns nt l nu nv"><strong class="nn ja">from</strong> <strong class="nn ja">sklearn.decomposition</strong> <strong class="nn ja">import</strong> PCA<br/><br/>pca = PCA(n_components=15, random_state=10)<br/><br/><em class="ny"># as a reminder, x is the array with our 300-dimensional vectors</em><br/>reduced_vecs = pca.fit_transform(x)</span><span id="3726" class="nr mq iq nn b gy nx nt l nu nv">df_w_vectors = pd.DataFrame(reduced_vecs)<br/><br/>df_w_vectors['Title'] = titles_list</span><span id="0757" class="nr mq iq nn b gy nx nt l nu nv"># Use pd.concat to match original titles with their vectors<br/>main_w_vectors = pd.concat((df_w_vectors, main_data), axis=1)<br/><br/><em class="ny"># Get rid of vectors that couldn't be matched with the main_df</em><br/>main_w_vectors.dropna(axis=0, inplace=<strong class="nn ja">True</strong>)</span></pre><p id="aeae" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，我们需要删除非数字和非虚拟列，以便将数据输入到模型中。我们还将对<code class="fe od oe of nn b">DaysSinceEpoch</code>特性应用缩放，因为与减少的词向量、情感等相比，它在数量上要大得多。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="e3e0" class="nr mq iq nn b gy ns nt l nu nv"><em class="ny"># Drop all non-numeric, non-dummy columns, for feeding into the models</em><br/>cols_to_drop = ['IDLink', 'Title', 'TimeSinceEpoch', 'Headline', 'PublishDate', 'Source'] <br/><br/>data_only_df = pd.get_dummies(main_w_vectors, columns = ['Topic']).drop(columns=cols_to_drop)<br/><br/><em class="ny"># Standardize DaysSinceEpoch since the raw numbers are larger in magnitude </em><br/><strong class="nn ja">from</strong> <strong class="nn ja">sklearn.preprocessing</strong> <strong class="nn ja">import</strong> StandardScaler<br/><br/>scaler = StandardScaler()<br/><br/><em class="ny"># Reshape so we can feed the column to the scaler</em><br/>standardized_days = np.array(data_only_df['DaysSinceEpoch']).reshape(-1, 1)<br/>data_only_df['StandardizedDays'] = scaler.fit_transform(standardized_days)<br/><br/><em class="ny"># Drop the raw column; we don't need it anymore</em><br/>data_only_df.drop(columns=['DaysSinceEpoch'], inplace=<strong class="nn ja">True</strong>)<br/><br/><em class="ny"># Look at the new range</em><br/>data_only_df['StandardizedDays'].describe()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/7d76b0245ef6b3f65eb787526fcab117.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*iL6SL_xFPRSS2RgTK2AkgQ.png"/></div></figure><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="00bd" class="nr mq iq nn b gy ns nt l nu nv"># Get Facebook data only<br/>fb_data_only_df = data_only_df.drop(columns=['GooglePlus', 'LinkedIn'])</span><span id="217b" class="nr mq iq nn b gy nx nt l nu nv"># Separate the features and the response<br/>X = fb_data_only_df.drop('Facebook', axis=1)<br/>y = fb_data_only_df['Facebook']<br/><br/><em class="ny"># 80% of data goes to training</em><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)</span></pre><p id="07f4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我们对数据运行一个非优化的<code class="fe od oe of nn b">XGBoost</code>，看看它是如何开箱即用的。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="1245" class="nr mq iq nn b gy ns nt l nu nv"><strong class="nn ja">from</strong> <strong class="nn ja">sklearn.metrics</strong> <strong class="nn ja">import</strong> mean_squared_error<br/><br/><em class="ny"># Instantiate an XGBRegressor</em><br/>xgr = xgb.XGBRegressor(random_state=2)<br/><br/><em class="ny"># Fit the classifier to the training set</em><br/>xgr.fit(X_train, y_train)<br/><br/>y_pred = xgr.predict(X_test)<br/><br/>mean_squared_error(y_test, y_pred)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/ac23bdaec3d24963039df57e818b2191.png" data-original-src="https://miro.medium.com/v2/resize:fit:152/format:webp/1*5SyaBvtTGMDy_DX6-AkEBw.png"/></div></figure><p id="7df9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">至少可以说，结果平平。我们可以通过超参数调整来提高这种性能吗？我从<a class="ae mo" href="https://www.kaggle.com/jayatou/xgbregressor-with-gridsearchcv" rel="noopener ugc nofollow" target="_blank">这篇 Kaggle 文章</a>中提取并重新调整了一个超参数调整网格。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="b4f6" class="nr mq iq nn b gy ns nt l nu nv"><strong class="nn ja">from</strong> <strong class="nn ja">sklearn.model_selection</strong> <strong class="nn ja">import</strong> GridSearchCV<br/><br/><em class="ny"># Various hyper-parameters to tune</em><br/>xgb1 = xgb.XGBRegressor()<br/>parameters = {'nthread':[4], <br/>              'objective':['reg:linear'],<br/>              'learning_rate': [.03, 0.05, .07], <br/>              'max_depth': [5, 6, 7],<br/>              'min_child_weight': [4],<br/>              'silent': [1],<br/>              'subsample': [0.7],<br/>              'colsample_bytree': [0.7],<br/>              'n_estimators': [250]}<br/><br/>xgb_grid = GridSearchCV(xgb1,<br/>                        parameters,<br/>                        cv = 2,<br/>                        n_jobs = 5,<br/>                        verbose=<strong class="nn ja">True</strong>)<br/><br/>xgb_grid.fit(X_train, y_train)</span></pre><p id="1987" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">根据<code class="fe od oe of nn b">xgb_grid</code>，我们的最佳参数如下:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="9b2a" class="nr mq iq nn b gy ns nt l nu nv">{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 4, 'n_estimators': 250, 'nthread': 4, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.7}</span></pre><p id="24ae" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">用新参数再试一次:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="993a" class="nr mq iq nn b gy ns nt l nu nv">params = {'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 4, <br/>          'n_estimators': 250, 'nthread': 4, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.7}<br/><br/><em class="ny"># Try again with new params</em><br/>xgr = xgb.XGBRegressor(random_state=2, **params)<br/><br/><em class="ny"># Fit the classifier to the training set</em><br/>xgr.fit(X_train, y_train)<br/><br/>y_pred = xgr.predict(X_test)<br/><br/>mean_squared_error(y_test, y_pred)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/d294373451cd662627204e7c2413629b.png" data-original-src="https://miro.medium.com/v2/resize:fit:156/format:webp/1*HQrJ83qfLpFFUNVCioxYPw.png"/></div></figure><p id="ad17" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">大约好了 35，000，但我不确定这说明了很多。在这一点上，我们可以推断，当前状态的数据似乎不足以让这个模型运行。让我们看看我们是否可以用更多的功能工程来改进它:我们将训练一些分类器来区分两个主要的文章组:哑弹(0 或 1 份额)与非哑弹。</p><p id="d639" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这个想法是，如果我们可以给回归变量一个新的特征(文章将具有极低份额的概率)，它可能会在预测高度共享的文章方面表现得更好，从而降低这些文章的残值并减少均方误差。</p><h1 id="5e06" class="mp mq iq bd mr ms mt mu mv mw mx my mz kf na kg nb ki nc kj nd kl ne km nf ng bi translated">迂回:检测无用的文章</h1><p id="4475" class="pw-post-body-paragraph le lf iq lg b lh nh ka lj lk ni kd lm ln nj lp lq lr nk lt lu lv nl lx ly lz ij bi translated">从我们之前制作的对数转换图中，我们可以注意到，一般来说，有 2 个文章块:1 个簇在 0，另一个簇(长尾)从 1 开始。我们可以训练一些分类器来识别文章是否是“无用的”(在 0-1 股票箱中)，然后使用这些模型的预测作为最终回归的特征，这将预测概率。这叫做<strong class="lg ja">模型叠加</strong>。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="3bf8" class="nr mq iq nn b gy ns nt l nu nv"># Define a quick function that will return 1 (true) if the article has 0-1 share(s)<br/>def dud_finder(popularity):<br/>    if popularity &lt;= 1:<br/>        return 1<br/>    else:<br/>        return 0</span><span id="c652" class="nr mq iq nn b gy nx nt l nu nv"># Create target column using the function<br/>fb_data_only_df['is_dud'] = fb_data_only_df['Facebook'].apply(dud_finder)<br/>fb_data_only_df[['Facebook', 'is_dud']].head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi os"><img src="../Images/9d5fb046a63a348572659c1c329213af.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*sP_hHLpi6CSveSfx1iLV3w.png"/></div></figure><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="5db7" class="nr mq iq nn b gy ns nt l nu nv"># 28% of articles can be classified as "duds"<br/>fb_data_only_df['is_dud'].sum() / len(fb_data_only_df)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/cd2c2dd364eb06e78335b2568938e938.png" data-original-src="https://miro.medium.com/v2/resize:fit:92/format:webp/1*Qngc5R5DmKrS3j3taeyj9g.png"/></div></figure><p id="85f2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在我们已经有了无用的特征，我们将初始化分类器。我们将使用一个随机森林、一个优化的 xgb 分类器和一个 K-最近邻分类器。我将省去调优 XGB 的部分，因为它看起来与我们之前进行的调优基本相同。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="a9b9" class="nr mq iq nn b gy ns nt l nu nv">from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import train_test_split</span><span id="2aea" class="nr mq iq nn b gy nx nt l nu nv">X = fb_data_only_df.drop(['is_dud', 'Facebook'], axis=1)<br/>y = fb_data_only_df['is_dud']</span><span id="6817" class="nr mq iq nn b gy nx nt l nu nv"># 80% of data goes to training<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)</span><span id="3ab8" class="nr mq iq nn b gy nx nt l nu nv"># Best params, produced by HP tuning<br/>params = {'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 4, <br/>          'n_estimators': 200, 'nthread': 4, 'silent': 1, 'subsample': 0.7}</span><span id="2749" class="nr mq iq nn b gy nx nt l nu nv"># Try xgc again with new params<br/>xgc = xgb.XGBClassifier(random_state=10, **params)<br/>rfc = RandomForestClassifier(n_estimators=100, random_state=10)<br/>knn = KNeighborsClassifier()</span><span id="2abc" class="nr mq iq nn b gy nx nt l nu nv">preds = {}<br/>for model_name, model in zip(['XGClassifier', 'RandomForestClassifier', 'KNearestNeighbors'], [xgc, rfc, knn]):<br/>    model.fit(X_train, y_train)<br/>    preds[model_name] = model.predict(X_test)</span></pre><p id="e235" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">测试模型，获得分类报告:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="cfd7" class="nr mq iq nn b gy ns nt l nu nv">from sklearn.metrics import classification_report, roc_curve, roc_auc_score</span><span id="7853" class="nr mq iq nn b gy nx nt l nu nv">for k in preds:<br/>    print("{} performance:".format(k))<br/>    print()<br/>    print(classification_report(y_test, preds[k]), sep='\n')</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/98c502799f559036a00f0f50d7060ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*zHjjgUvi7ob3U3GV2KaKDg.png"/></div></figure><p id="5e89" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">f1 成绩最好的是 XGC，其次是 RF，最后是 KNN。然而，我们也可以注意到，在召回(成功识别哑弹)方面，KNN 实际上做得最好<em class="ny"/>。这就是为什么模型堆叠是有价值的——有时，即使是 XGBoost 这样优秀的模型也可能在像这样的任务中表现不佳，显然要识别的函数可以进行局部近似。包括 KNN 的预测应该会增加一些急需的多样性。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="2e7a" class="nr mq iq nn b gy ns nt l nu nv"># Plot ROC curves<br/>for model in [xgc, rfc, knn]:<br/>    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])<br/>    plt.plot([0, 1], [0, 1], 'k--')<br/>    plt.plot(fpr, tpr)<br/>    <br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.title('ROC Curves')<br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/0f6f63514c861e0a4e7144e6afb44068.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*gpqo3aEyi1sx7O-XNyltvQ.png"/></div></figure><h1 id="02f9" class="mp mq iq bd mr ms mt mu mv mw mx my mz kf na kg nb ki nc kj nd kl ne km nf ng bi translated">人气预测:第二轮</h1><p id="55fd" class="pw-post-body-paragraph le lf iq lg b lh nh ka lj lk ni kd lm ln nj lp lq lr nk lt lu lv nl lx ly lz ij bi translated">现在，我们可以对三个分类器的概率预测进行平均，并将其用作回归变量的一个特征。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="bec3" class="nr mq iq nn b gy ns nt l nu nv">averaged_probs = (xgc.predict_proba(X)[:, 1] +<br/>                  knn.predict_proba(X)[:, 1] + <br/>                  rfc.predict_proba(X)[:, 1]) / 3</span><span id="dc45" class="nr mq iq nn b gy nx nt l nu nv">X['prob_dud'] = averaged_probs<br/>y = fb_data_only_df['Facebook']</span></pre><p id="9a3b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">接下来是另一轮惠普调整，包括新的特性，我将不再赘述。让我们看看我们的表现如何:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="1438" class="nr mq iq nn b gy ns nt l nu nv">xgr = xgb.XGBRegressor(random_state=2, **params)</span><span id="0bf9" class="nr mq iq nn b gy nx nt l nu nv"># Fit the classifier to the training set<br/>xgr.fit(X_train, y_train)</span><span id="a499" class="nr mq iq nn b gy nx nt l nu nv">y_pred = xgr.predict(X_test)</span><span id="a2aa" class="nr mq iq nn b gy nx nt l nu nv">mean_squared_error(y_test, y_pred)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/c949d272e0b41a3313ed2e1b31fdeff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*hFfW0Dqkk6syd0pNZ3xiGA.png"/></div></figure><p id="93f6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">啊哦！这种性能本质上与我们进行任何模型堆叠之前是一样的。也就是说，我们可以记住，MSE 作为一种误差度量，往往会加重异常值。事实上，我们还可以计算平均绝对误差(MAE ),它用于评估具有显著异常值的数据的性能。在数学术语中，MAE 计算残差的<a class="ae mo" href="https://en.wikipedia.org/wiki/Norm_(mathematics)" rel="noopener ugc nofollow" target="_blank"> l1 范数</a>，本质上是绝对值，而不是 MSE 使用的 l2 范数。我们可以将 MAE 与 MSE 的平方根进行比较，后者也称为均方根误差(RMSE)。</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="5d1b" class="nr mq iq nn b gy ns nt l nu nv">mean_absolute_error(y_test, y_pred), np.sqrt(mean_squared_error(y_test, y_pred))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/e443bbe16f830eeb7f9f5636fe95c25d.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*QEGZmLn8zGUvJlhX10KmSg.png"/></div></figure><p id="b032" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">平均绝对误差只有 RMSE 的 1/3 左右！也许我们的模型没有我们最初想象的那么糟糕。</p><p id="cd65" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">作为最后一步，让我们根据 XGRegressor 来看看每个特性的重要性:</p><pre class="kp kq kr ks gt nm nn no np aw nq bi"><span id="361f" class="nr mq iq nn b gy ns nt l nu nv">for feature, importance in zip(list(X.columns), xgr.feature_importances_):<br/>    print('Model weight for feature {}: {}'.format(feature, importance))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/35a98f61c54f68ea746deb82d5434ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*DI5KI_Q2dvsiRP6gdeQpEQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Aha! prob_dud was found to be the most important feature.</figcaption></figure><p id="1f3a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">整洁！我们的模型发现<code class="fe od oe of nn b">prob_dud</code>是最重要的特性，我们的定制<code class="fe od oe of nn b">StandardizedDays</code>特性是第二重要的特性。(特征 0 到 14 对应于简化的标题嵌入向量。)</p><p id="ac2b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">尽管通过这一轮的模型堆叠，整体性能并没有得到改善，但是我们可以看到，我们确实成功地捕获了数据中可变性的一个重要来源，而模型正是利用了这个来源。</p><p id="0bfc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果我继续扩展这个项目以使模型更加准确，我可能会考虑用外部数据增加数据，包括通过宁滨或哈希将 Source 作为变量，在原始的 300 维向量上运行模型，并使用每篇文章在不同时间点的流行度的“时间切片”数据(该数据的配套数据集)来预测最终的流行度。</p><p id="2d53" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果您觉得这个分析很有趣，请随意使用代码，并对其进行进一步扩展！笔记本在这里是<a class="ae mo" href="https://nbviewer.jupyter.org/github/chambliss/Notebooks/blob/master/Word2Vec_News_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank"/>(注意，一些单元格的顺序可能与这里显示的略有不同)，这个项目使用的原始数据在这里是<a class="ae mo" href="https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms" rel="noopener ugc nofollow" target="_blank"/>。</p></div></div>    
</body>
</html>