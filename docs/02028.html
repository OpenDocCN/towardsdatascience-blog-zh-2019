<html>
<head>
<title>Normalization vs Standardization — Quantitative analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">标准化与规范化—定量分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf?source=collection_archive---------0-----------------------#2019-04-04">https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf?source=collection_archive---------0-----------------------#2019-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9f95" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">停止使用 Sklearn 的 StandardScaler 作为默认的特征缩放方法可以让你的精度提高 7%<strong class="ak">,即使你的超参数已经调好了！</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/eade161a14a0c4202d886a9c2ffe6551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZlwWGNhFco5bmpfwYyLCQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://365datascience.com/standardization/" rel="noopener ugc nofollow" target="_blank">https://365datascience.com/standardization/</a></figcaption></figure><p id="707c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个 ML 从业者都知道特征缩放是一个重要的问题(在这里阅读更多<a class="ae ky" href="https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e" rel="noopener"/>)。</p><p id="78fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">讨论最多的两种缩放方法是规范化和标准化。<strong class="lb iu"> <em class="lv">正常化</em> </strong>通常意味着将值重新调整到[0，1]的范围内。<strong class="lb iu"> <em class="lv">标准化</em> </strong>通常意味着重新调整数据，使平均值为 0，标准差为 1(单位方差)。</p><p id="7024" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇博客中，我进行了一些实验，并希望回答如下问题:</p><ol class=""><li id="07b0" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">我们应该总是缩放我们的特征吗？</li><li id="bfab" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">有没有单一的最佳缩放技术？</li><li id="4fc7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">不同的缩放技术如何影响不同的分类器？</li><li id="2825" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们应该考虑缩放技术作为我们模型的一个重要的超参数吗？</li></ol><p id="0500" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将分析在多个实验设置中对特征应用不同缩放方法的实验结果。</p><h1 id="85b3" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">竞赛表格</h1><ul class=""><li id="f201" class="lw lx it lb b lc nc lf nd li ne lm nf lq ng lu nh mc md me bi translated">0.我们为什么在这里？</li><li id="4f47" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">1.现成的分类器</li><li id="6e30" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">2.分类器+缩放</li><li id="9a68" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">3.分类器+缩放+ PCA</li><li id="b4d7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">4.分类器+缩放+ PCA +超参数调整</li><li id="5551" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">5.更多的数据集:</li><li id="1755" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">— 5.1 澳大利亚的降雨数据集</li><li id="bc03" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">— 5.2 银行营销数据集</li><li id="f86a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">— 5.3 收入分类数据集</li><li id="a20d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">— 5.4 收入分类数据集</li><li id="f91d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">结论</li></ul><h1 id="1862" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">0.我们为什么在这里？</h1><p id="a4d7" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">首先，我试图理解规范化和标准化之间的区别。因此，我遇到了 Sebastian Raschka 写的这个优秀的博客，它提供了满足我好奇心的数学背景。如果您不熟悉规范化或标准化概念，请花 5 分钟时间阅读本博客。 <br/>著名的 Hinton <a class="ae ky" href="https://www.youtube.com/watch?v=Xjtu1L7RwVM&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=26" rel="noopener ugc nofollow" target="_blank">在这里</a>也对处理使用梯度下降方法(如神经网络)训练的分类器时缩放特征的需要做了很好的解释。</p><p id="ddef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，我们抓了些数学，就这样？不完全是。当我检查流行的 ML 库 Sklearn 时，我看到有许多不同的缩放方法。有一个伟大的形象化的<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py" rel="noopener ugc nofollow" target="_blank">不同的定标器对有异常值的数据的影响</a>。但他们没有显示它如何影响不同分类器的分类任务。</p><p id="0265" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我看到很多 ML pipelines 教程使用 StandardScaler(通常称为 Z-score 标准化)或 MinMaxScaler(通常称为 min-max 归一化)来缩放特征。为什么没有人使用其他缩放技术进行分类？有没有可能 StandardScaler 或者 MinMaxScaler 是最好的缩放方法？<br/>我在教程中没有看到任何关于为什么或何时使用它们的解释，所以我想通过运行一些实验来研究这些技术的性能。<strong class="lb iu">这就是这款笔记本的全部内容</strong></p><h1 id="da4b" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">项目详情</h1><p id="2c40" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">像许多数据科学项目一样，让我们读取一些数据，并用几个现成的分类器进行实验。</p><h2 id="fb3f" class="nl ml it bd mm nm nn dn mq no np dp mu li nq nr mw lm ns nt my lq nu nv na nw bi translated">资料组</h2><p id="ef0d" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><a class="ae ky" href="https://www.kaggle.com/adx891/sonar-data-set" rel="noopener ugc nofollow" target="_blank">声纳</a>数据集。它包含 208 行和 60 个特征列。这是一项分类任务，以区分从金属圆柱体反弹的声纳信号和从大致圆柱形的岩石反弹的信号。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/62c45c34784544524d42aa66f92048a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qjuIaF6FRElpMniHloccQ.png"/></div></div></figure><p id="8eaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个平衡的数据集:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="7eb9" class="nl ml it nz b gy od oe l of og">sonar[60].value_counts() # 60 is the label column name</span><span id="51d7" class="nl ml it nz b gy oh oe l of og">M    111<br/>R     97</span></pre><p id="6f38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该数据集中的所有要素都在 0 到 1 之间，<strong class="lb iu">但是</strong>不能保证 1 是每个要素中的最大值或 0 是最小值。</p><p id="7c55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我选择这个数据集是因为，一方面，它很小，所以我可以很快地进行实验。另一方面，这是一个困难的问题，没有一个分类器达到接近 100%的准确率，所以我们可以比较有意义的结果。</p><p id="cb50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在最后一节中，我们将尝试更多的数据集。</p><p id="e1fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">代码</strong></p><p id="9c2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为预处理步骤，我已经计算了所有的结果(这需要一些时间)。所以我们只加载结果文件并使用它。</p><p id="ce6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">产生结果的代码可以在我的 GitHub 中找到:<br/><a class="ae ky" href="https://github.com/shaygeller/Normalization_vs_Standardization.git" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/shaygeller/Normalization _ vs _ standardization . git</a></p><p id="78ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我从 Sklearn 中挑选了一些最流行的分类模型，表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/780cc58ec9fdcefd7d943ff0d1cb2f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*G0DvYlXlKH5P5WyOjYs6iw.png"/></div></figure><p id="8c88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(MLP 是多层感知器，一种神经网络)</p><p id="2256" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我使用的定标器表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/4a5354ea25ba1213146d777ca1481cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*XU4abA9kv7Fohqk2WtQ2ag.png"/></div></figure><p id="8e23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">*不要混淆 Normalizer，上面列表中的最后一个缩放器和我之前讨论的最小-最大归一化技术。最小-最大归一化是列表中的第二个，命名为 MinMaxScaler。Sklearn 的规格化器类将样本分别规格化为单位范数。它不是基于列的，而是基于行的标准化技术。</p><h1 id="cb92" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">实验详情:</h1><ul class=""><li id="f1ff" class="lw lx it lb b lc nc lf nd li ne lm nf lq ng lu nh mc md me bi translated">当需要再现性时，使用相同的种子。</li><li id="b870" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">我随机将数据分成 80%-20%的训练测试集。</li><li id="fa8b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">所有结果都是来自<strong class="lb iu">训练集</strong>的 10 倍随机交叉验证分割的准确度分数。</li><li id="de88" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">我在这里不讨论测试集的结果。通常，测试集应该保持隐藏，并且我们关于分类器的所有结论应该仅来自交叉验证分数。</li><li id="cff6" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">在第 4 部分中，我执行了嵌套交叉验证。一个内部交叉验证使用 5 个随机分割进行超参数调整，另一个外部 CV 使用 10 个随机分割使用最佳参数获得模型得分。同样在这一部分中，所有数据仅取自训练集。一张图胜过千言万语:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/e5754e2034747b78f67633304da76c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7-Y--5i-Pc6VTL7EzY0lCA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://sebastianraschka.com/faq/docs/evaluate-a-model.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka.com/faq/docs/evaluate-a-model.html</a></figcaption></figure><h1 id="46ae" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">让我们阅读结果文件</h1><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="6724" class="nl ml it nz b gy od oe l of og">import os<br/>import pandas as pd</span><span id="1ce4" class="nl ml it nz b gy oh oe l of og">results_file = "sonar_results.csv"<br/>results_df = pd.read_csv(os.path.join("..","data","processed",results_file)).dropna().round(3)<br/>results_df</span></pre><h1 id="99de" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">1.现成的分类器</h1><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="bf4a" class="nl ml it nz b gy od oe l of og"><strong class="nz iu">import</strong> operator</span><span id="8dac" class="nl ml it nz b gy oh oe l of og">results_df.loc[operator.and_(results_df["Classifier_Name"].str.startswith("_"), <strong class="nz iu">~</strong>results_df["Classifier_Name"].str.endswith("PCA"))].dropna()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/f272d6daa1d24e59bc355a89fe6d7ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*PY8iPAFpK7RgJpfnqMEyPw.png"/></div></figure><p id="c294" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不错的结果。通过查看 CV_mean 列，我们可以看到目前 MLP 领先。SVM 表现最差。</p><p id="ded8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标准差也差不多，所以主要可以通过平均分来判断。以下所有结果将是 10 倍交叉验证随机分裂的平均分数。</p><p id="a44c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们看看不同的缩放方法如何改变每个分类器的分数</p><h1 id="770c" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">2.分类器+缩放</h1><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="7fbc" class="nl ml it nz b gy od oe l of og">import operator<br/>temp = results_df.loc[~results_df["Classifier_Name"].str.endswith("PCA")].dropna()<br/>temp["model"] = results_df["Classifier_Name"].apply(lambda sen: sen.split("_")[1])<br/>temp["scaler"] = results_df["Classifier_Name"].apply(lambda sen: sen.split("_")[0])</span><span id="ce77" class="nl ml it nz b gy oh oe l of og">def df_style(val):<br/>    return 'font-weight: 800'</span><span id="a532" class="nl ml it nz b gy oh oe l of og">pivot_t = pd.pivot_table(temp, values='CV_mean', index=["scaler"], columns=['model'], aggfunc=np.sum)<br/>pivot_t_bold = pivot_t.style.applymap(df_style,<br/>                      subset=pd.IndexSlice[pivot_t["CART"].idxmax(),"CART"])<br/>for col in list(pivot_t):<br/>    pivot_t_bold = pivot_t_bold.applymap(df_style,<br/>                      subset=pd.IndexSlice[pivot_t[col].idxmax(),col])<br/>pivot_t_bold</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b4db7776d716df799f86ec916ee06d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*O7f4vWIUgUvCpwxT24dsmg.png"/></div></figure><p id="e67f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一行，没有索引名称的一行，是没有应用任何缩放方法的算法。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="cd24" class="nl ml it nz b gy od oe l of og">import operator</span><span id="d31f" class="nl ml it nz b gy oh oe l of og">cols_max_vals = {}<br/>cols_max_row_names = {}<br/>for col in list(pivot_t):<br/>    row_name = pivot_t[col].idxmax()<br/>    cell_val = pivot_t[col].max()<br/>    cols_max_vals[col] = cell_val<br/>    cols_max_row_names[col] = row_name<br/>    <br/>sorted_cols_max_vals = sorted(cols_max_vals.items(), key=lambda kv: kv[1], reverse=True)</span><span id="70a6" class="nl ml it nz b gy oh oe l of og">print("Best classifiers sorted:\n")<br/>counter = 1<br/>for model, score in sorted_cols_max_vals:<br/>    print(str(counter) + ". " + model + " + " +cols_max_row_names[model] + " : " +str(score))<br/>    counter +=1</span></pre><p id="3c4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个模型的最佳分类器:</p><p id="a0be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.SVM +标准缩放器:0.849 <br/> 2。MLP +电源变压器-Yeo-Johnson : 0.839 <br/> 3。KNN + MinMaxScaler : 0.813 <br/> 4。LR +量化转换器-统一:0.808 <br/> 5。n b+ power transformer-Yeo-Johnson:0.752<br/>6。LDA+power transformer-Yeo-Johnson:0.747<br/>7。手推车+量化器-制服:0.74 <br/> 8。射频+归一化因子:0.723</p><h1 id="a06d" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">让我们分析结果</h1><ol class=""><li id="94be" class="lw lx it lb b lc nc lf nd li ne lm nf lq ng lu mb mc md me bi translated"><strong class="lb iu">没有一个单一的缩放方法可以统治所有的人。</strong></li><li id="3f5c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们可以看到缩放改善了结果。SVM、MLP、KNN 和 NB 从不同的扩展方法中得到了显著的提升。</li><li id="13b4" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">请注意，NB、RF、LDA、CART 不受某些缩放方法的影响。这当然与每个分类器的工作方式有关。树不受缩放的影响，因为分割标准首先对每个要素的值进行排序，然后计算分割的基尼\熵。一些缩放方法保持这种顺序，因此准确度分数没有变化。<br/> NB 不受影响，因为模型的先验由每个类中的计数决定，而不是由实际值决定。线性判别分析(LDA)使用类之间的变化来找到它的系数(检查<a class="ae ky" href="https://www.youtube.com/watch?v=azXCzI57Yfc" rel="noopener ugc nofollow" target="_blank">这个</a>，所以缩放也无关紧要。</li><li id="4a08" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">一些缩放方法，如 QuantileTransformer-Uniform，不保留每个特征中值的确切顺序，因此即使在上述分类器中分数也会发生变化，而这些分类器对其他缩放方法是不可知的。</li></ol><h1 id="be3d" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">3.分类器+缩放+PCA</h1><p id="8fdf" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们知道一些众所周知的 ML 方法如 PCA 可以从缩放中获益(<a class="ae ky" href="https://sebastianraschka.com/Articles/2014_about_feature_scaling.html" rel="noopener ugc nofollow" target="_blank">博客</a>)。让我们尝试将 PCA(n_components=4)添加到管道中，并分析结果。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="4a38" class="nl ml it nz b gy od oe l of og">import operator<br/>temp = results_df.copy()<br/>temp["model"] = results_df["Classifier_Name"].apply(lambda sen: sen.split("_")[1])<br/>temp["scaler"] = results_df["Classifier_Name"].apply(lambda sen: sen.split("_")[0])</span><span id="db19" class="nl ml it nz b gy oh oe l of og">def df_style(val):<br/>    return 'font-weight: 800'</span><span id="006a" class="nl ml it nz b gy oh oe l of og">pivot_t = pd.pivot_table(temp, values='CV_mean', index=["scaler"], columns=['model'], aggfunc=np.sum)<br/>pivot_t_bold = pivot_t.style.applymap(df_style,<br/>                      subset=pd.IndexSlice[pivot_t["CART"].idxmax(),"CART"])<br/>for col in list(pivot_t):<br/>    pivot_t_bold = pivot_t_bold.applymap(df_style,<br/>                      subset=pd.IndexSlice[pivot_t[col].idxmax(),col])<br/>pivot_t_bold</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/c137a9a348d0985be0d08ac769009237.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CUM03Zp2PN5s8DUbkgv-kA.png"/></div></div></figure><h1 id="3242" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">让我们分析结果</h1><ol class=""><li id="9221" class="lw lx it lb b lc nc lf nd li ne lm nf lq ng lu mb mc md me bi translated">大多数时候缩放方法使用 PCA 改进模型，<strong class="lb iu">但是</strong>没有具体的缩放方法负责。<br/>我们再来看“QuantileTransformer-Uniform”，这个方法得分最高的占大多数。<br/>在 LDA-PCA 中，它将结果从 0.704 提高到 0.783(准确度提高了 8%！)，但在 RF-PCA 中却让事情变得更糟，从 0.711 降到 0.668(准确率下降 4.35%！)<br/>另一方面，使用带有“QuantileTransformer-Normal”的 RF-PCA，将精度提高到 0.766(精度跃升 5%！)</li><li id="5c4d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们可以看到，PCA 只是提高了 LDA 和 RF，所以 PCA 不是一个神奇的解决方案。<br/>没事。我们没有超调 n_components 参数，即使我们这样做了，PCA 也不能保证改善预测。</li><li id="1345" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们可以看到，StandardScaler 和 MinMaxScaler 仅在 16 个案例中的 4 个案例中取得最佳成绩。所以我们应该仔细考虑选择什么样的缩放方法，即使是默认的缩放方法。</li></ol><p id="30b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">我们可以得出结论，尽管 PCA 是一种受益于缩放的已知成分，但没有一种缩放方法总能改善我们的结果，其中一些方法甚至会造成伤害(带标准缩放器的 RF-PCA)。</strong></p><p id="08d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">数据集在这里也是一个重要因素。为了更好地理解缩放方法对 PCA 的影响，我们应该对更多样化的数据集进行实验(类别不平衡、不同尺度的特征以及具有数值和分类特征的数据集)。我在第 5 节做这个分析。</strong></p><h1 id="9635" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">4.分类器+缩放+PCA+超参数调整</h1><p id="9927" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">对于给定的分类器，不同的缩放方法之间的准确度分数有很大的差异。人们可以假设，当超参数被调整时，缩放技术之间的差异将是微小的，我们可以使用 StandardScaler 或 MinMaxScaler，就像在网络上的许多分类管道教程中使用的那样。让我们检查一下！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/9daf1f8592c3e4adc889669564766727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cq-0CrKFMurnKqAiXZ03Qw.png"/></div></div></figure><p id="15c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一，NB 不在这里，那是因为 NB 没有参数可以调。</p><p id="4b16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，与上一步的结果相比，几乎所有的算法都受益于超参数调整。一个有趣的例外是 MLP，结果更糟。这可能是因为神经网络很容易过度拟合数据(特别是当参数的数量远大于训练样本的数量时)，我们没有仔细地提前停止以避免它，也没有应用任何正则化。</p><p id="d2d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，即使调整了超参数，使用不同缩放方法的结果之间仍然存在很大差异。如果我们将不同的缩放技术与广泛使用的标准缩放技术进行比较，当使用其他技术时，我们可以<strong class="lb iu">获得高达 7%的精度提升</strong> (KNN 列)。</p><p id="b88b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">从这一步得出的主要结论是，即使调整了超参数，改变缩放方法也会显著影响结果。因此，我们应该将标度方法视为我们模型的一个关键超参数。</strong></p><p id="ae41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第 5 部分包含对更多样化的数据集的更深入的分析。如果不想深究，可以直接跳到结论部分。</p><h1 id="8c7f" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">5.更多的数据集</h1><p id="4573" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">为了更好地理解和得出更一般化的结论，我们应该用更多的数据集进行实验。</p><p id="9bb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将在具有不同特征的几个数据集上应用分类器+缩放+PCA，如第 3 节，并分析结果。所有数据集均取自 Kaggel。</p><ul class=""><li id="f22d" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu nh mc md me bi translated">为了方便起见，我只从每个数据集中选择了数字列。在多元数据集(数值和分类要素)中，关于如何缩放要素一直存在争议。</li><li id="7cfe" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">我没有超调分类器的任何参数。</li></ul><h1 id="c653" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">5.1 澳大利亚的降雨数据集</h1><p id="2883" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><a class="ae ky" href="https://www.kaggle.com/jsphyg/weather-dataset-rattle-package#weatherAUS.csv" rel="noopener ugc nofollow" target="_blank">链接</a> <br/> <strong class="lb iu">分类任务</strong>:预测是不是要下雨了？<br/> <strong class="lb iu">度量</strong>:精度<br/> <strong class="lb iu">数据集形状</strong> : (56420，18) <br/> <strong class="lb iu">每个类的计数</strong> : <br/>否 43993 <br/>是 12427</p><p id="dbf9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个 5 行的例子，我们不能在一张图中显示所有的列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/3877591af4ae2c4b18e80c18e5388210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NoL1AoPJa4f0qowV_wxatA.png"/></div></div></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="63e6" class="nl ml it nz b gy od oe l of og">dataset.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/28cad56fd34c066d211df0f8cc5540e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ueOG8zIGopArwAKYh5gAYQ.png"/></div></div></figure><p id="bcac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们怀疑，由于特征的不同比例，缩放将改善分类结果(检查上表中的最小最大值，它甚至在一些其余的特征上变得更差)。</p><p id="6cf9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/596f4b2ef7a7a0f4b47376d0169d53b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cM5pzKhBp1dVSDYshfMV4g.png"/></div></div></figure><p id="6e16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果分析</strong></p><ul class=""><li id="e6a6" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu nh mc md me bi translated">我们可以看到 StandardScaler 从未得到最高分，MinMaxScaler 也是如此。</li><li id="ca89" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">我们可以看到标准定标器和其他方法之间的<strong class="lb iu">差异高达 20% </strong>。(购物车-PCA 列)</li><li id="8bad" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">我们可以看到缩放通常会改善结果。以 SVM 为例，<strong class="lb iu">从 78%跃升至 99%。</strong></li></ul><h1 id="09e5" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">5.2 银行营销数据集</h1><p id="3e22" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><a class="ae ky" href="https://www.kaggle.com/henriqueyamahata/bank-marketing" rel="noopener ugc nofollow" target="_blank">链接</a> <br/> <strong class="lb iu">分类任务</strong>:预测客户是否认购了定期存款？<br/> <strong class="lb iu">指标</strong> : AUC <strong class="lb iu">(数据不平衡)</strong> <br/> <strong class="lb iu">数据集形状</strong> : (41188，11) <br/> <strong class="lb iu">每个类的计数</strong> : <br/>否 36548 <br/>是 4640</p><p id="1105" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个 5 行的例子，我们不能在一张图中显示所有的列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/bb05d0fef297443e545ecdc85aafe9e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fzuln582evzvssUyszvc9g.png"/></div></div></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="f51e" class="nl ml it nz b gy od oe l of og">dataset.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/0f08c07c445154260235f618499cd6c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TkgNo_a2rekFe2BBzSZriQ.png"/></div></div></figure><p id="e4ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，不同比例的特征。</p><p id="5b02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/51b44d78e99749e0870629b189139c82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IXemV_c-mI260WD0Kfa1-Q.png"/></div></div></figure><p id="1e32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果分析</strong></p><ul class=""><li id="047f" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu nh mc md me bi translated">我们可以看到，在该数据集中，即使要素处于不同的比例上，使用 PCA 进行缩放并不总能改善结果。<strong class="lb iu">然而，</strong>每个 PCA 列中的第二好的分数非常接近最好的分数。这可能表明超调 PCA 的组件数量并使用缩放将比根本不缩放改善结果。</li><li id="abee" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">同样，没有一个单一的缩放方法脱颖而出。</li><li id="eb58" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">另一个有趣的结果是，在大多数模型中，所有的缩放方法都没有产生太大的影响(通常有 1%-3%的改善)。让我们记住，这是一个不平衡的数据集，我们没有超调参数。另一个原因是 AUC 分数已经很高了(~90%)，所以更难看到重大改善。</li></ul><h1 id="bcba" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">5.3 斯隆数字巡天 DR14 数据集</h1><p id="7cc6" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><a class="ae ky" href="https://www.kaggle.com/lucidlenn/sloan-digital-sky-survey" rel="noopener ugc nofollow" target="_blank">链接</a> <br/> <strong class="lb iu">分类任务</strong>:预测一个物体是星系、恒星还是类星体。<br/> <strong class="lb iu">度量</strong>:精度(多类)<br/> <strong class="lb iu">数据集形状</strong> : (10000，18) <br/> <strong class="lb iu">每个类的计数</strong> : <br/>星系 4998 <br/>恒星 4152 <br/>类星体 850</p><p id="0137" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个 5 行的例子，我们不能在一张图中显示所有的列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/2c78b357ae77f2efe38303c608c4f923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*61rtzzOrRXE6wl4RRCFgnw.png"/></div></div></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="672b" class="nl ml it nz b gy od oe l of og">dataset.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/e39f1d4a0a8c605a55aaec5c4dfb8863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKSKI_87l7MviX_BT25sCw.png"/></div></div></figure><p id="d2b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，不同比例的特征。</p><p id="e933" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/821360360d36ecfc6e2649dc384ffc72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAsv-KaQieauDnDO-x4C1A.png"/></div></div></figure><p id="d23b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果分析</strong></p><ul class=""><li id="f559" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu nh mc md me bi translated">我们可以看到缩放极大地改善了结果。我们可以期待它，因为它包含了不同尺度的特征。</li><li id="b01e" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">我们可以看到，当我们使用 PCA 时，RobustScaler 几乎总是获胜。这可能是由于该数据集中的许多异常值移动了 PCA 特征向量。另一方面，当我们不使用 PCA 时，那些异常值不会产生这样的影响。我们应该做一些数据探索来验证这一点。</li><li id="2ab0" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">如果我们将 StandardScaler 与另一种缩放方法进行比较，精确度会有 5%的差异。因此，这是需要试验多种缩放技术的另一个指标。</li><li id="c3bc" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">PCA 几乎总是受益于缩放。</li></ul><h1 id="f5c8" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">5.4 收入分类数据集</h1><p id="6f04" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><a class="ae ky" href="https://www.kaggle.com/lodetomasi1995/income-classification" rel="noopener ugc nofollow" target="_blank">链接</a> <br/> <strong class="lb iu">分类任务</strong>:预测收入是否&gt; 50K，&lt; =50K。<br/> <strong class="lb iu">度量</strong> : AUC <strong class="lb iu">(不平衡数据集)</strong> <br/> <strong class="lb iu">数据集形状</strong> : (32561，7) <br/> <strong class="lb iu">每个类的计数</strong>:<br/>&lt;= 50K 24720<br/>&gt;50K 7841</p><p id="6d85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个 5 行的例子，我们不能在一张图中显示所有的列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/9e53c3fa30053cde985b68af99dfc410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*5BgWgb3D5vsMU5SG6I1SsQ.png"/></div></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="1322" class="nl ml it nz b gy od oe l of og">dataset.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/572b49f8dd86680b860b3fc7bd6aedc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1iP0766U3-WIGEU-sGIzTg.png"/></div></div></figure><p id="4c7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，不同比例的特征。</p><p id="9fe2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/59f60361aeb0fe8620d916c5453ef8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jDGln1ifWDojhIRRZrhM7w.png"/></div></div></figure><p id="4d20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果分析</strong></p><ul class=""><li id="a18c" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu nh mc md me bi translated">同样，我们有一个不平衡的数据集，但我们可以看到缩放在改善结果方面做得很好(高达 20%！).这可能是因为与银行营销数据集相比，AUC 得分较低(~80%)，因此更容易看到重大改进。</li><li id="530c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">即使没有突出显示 StandardScaler(我在每个专栏中只突出显示了第一个最好的分数)，但在许多专栏中，它达到了与最好相同的结果，但并不总是如此。从运行时间的结果(没有出现在这里)，我可以告诉你，运行 StandatdScaler 比许多其他的 Scaler 快得多。因此，如果你急于得到一些结果，这可以是一个很好的起点。但是如果你想从你的模型中榨取每一个百分比，你可能想体验多种缩放方法。</li><li id="169b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">同样，没有单一的最佳扩展方法。</li><li id="4387" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">PCA 几乎总是受益于规模化</li></ul><h1 id="5139" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">结论</h1><ul class=""><li id="7a3f" class="lw lx it lb b lc nc lf nd li ne lm nf lq ng lu nh mc md me bi translated">尝试多种缩放方法可以显著提高您在分类任务中的得分，即使您的超参数已经调优。<strong class="lb iu">所以，你应该把缩放方法作为你的模型的一个重要的超参数。</strong></li><li id="1af3" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">缩放方法对不同的分类器有不同的影响。基于距离的分类器，如 SVM、KNN 和 MLP(神经网络)极大地受益于缩放。但是，即使树(CART，RF)不知道某些缩放方法，也可以从其他方法中受益。</li><li id="91c5" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">了解模型\预处理方法背后的数学基础是理解结果的最佳方式。(例如，树是如何工作的，为什么一些缩放方法不会影响它们)。当你的模型是随机森林时，如果你知道不应用 StandardScaler，也可以节省你很多时间。</li><li id="fcbe" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nh mc md me bi translated">像 PCA 这样的预处理方法，已知受益于缩放，但确实受益于缩放。<strong class="lb iu">如果不符合</strong>，可能是由于 PCA 的成分数参数设置错误、数据中存在异常值或缩放方法选择不当。</li></ul><p id="d370" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你发现一些错误或者有改进实验的覆盖面或有效性的建议，请通知我。</p></div></div>    
</body>
</html>