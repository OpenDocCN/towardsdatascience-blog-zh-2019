<html>
<head>
<title>Why exclude highly correlated features when building regression model ??</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么在建立回归模型时要排除高度相关的特征？？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-exclude-highly-correlated-features-when-building-regression-model-34d77a90ea8e?source=collection_archive---------4-----------------------#2019-08-23">https://towardsdatascience.com/why-exclude-highly-correlated-features-when-building-regression-model-34d77a90ea8e?source=collection_archive---------4-----------------------#2019-08-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/7910206dab36ecb01038003b6e0b1ddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*e8YsZ2_6gADo3qP_YyAkYg.png"/></div></figure><p id="6b40" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果您已经处理数据很长时间了，那么您一定知道一般的做法是在运行线性回归时排除高度相关的特征。本文的目的是解释为什么我们在构建简单的线性回归模型时需要避免高度相关的特性。在继续这篇文章之前，我强烈推荐你参考我关于<a class="ae ks" rel="noopener" target="_blank" href="/regression-explained-in-simple-terms-dccbcad96f61">回归</a>的文章。</p><h2 id="4d4c" class="kt ku iq bd kv kw kx dn ky kz la dp lb kf lc ld le kj lf lg lh kn li lj lk ll bi translated">什么是相关性？</h2><p id="0077" class="pw-post-body-paragraph ju jv iq jw b jx lm jz ka kb ln kd ke kf lo kh ki kj lp kl km kn lq kp kq kr ij bi translated">相关性仅仅意味着两个或更多事物之间的相互关系。考虑数据集中的数据点(xᵢ，yᵢ)，i = 1，2，…n。相关性的目的是观察大值“x”是否与大值“y”配对，小值“x”是否与小值“y”配对。如果没有，检查小值“x”是否与大值“y”成对出现，反之亦然。</p><p id="c804" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在统计学中，上述现象是用一个叫做相关系数的拟合函数来衡量的。衡量相关性的公式是</p><figure class="ls lt lu lv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lr"><img src="../Images/d10564a1bb512b534f8e69f1bcf783b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*jY2NVmatguKNF3yASRbI1A.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Correlation coefficient formula</figcaption></figure><p id="e936" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">x̄和̄y 分别代表 x 和 y。当相关系数为&lt; 0, we say that x and y are negatively correlated. If it is &gt; 0 时，两者正相关。相关系数在-1 和 1 之间变化。</p><p id="1a5c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">需要注意的最重要的一点是，相关性只度量两个变量之间的关联，而不度量因果关系。即，“y”的大值<strong class="jw ir">不是由“x”的大值引起的</strong>，反之亦然，而是恰好这样的数据对只存在于数据集中。</p><h2 id="8a63" class="kt ku iq bd kv kw kx dn ky kz la dp lb kf lc ld le kj lf lg lh kn li lj lk ll bi translated">为什么排除高度相关的特征？</h2><p id="b3a7" class="pw-post-body-paragraph ju jv iq jw b jx lm jz ka kb ln kd ke kf lo kh ki kj lp kl km kn lq kp kq kr ij bi translated">如果您还记得我上一篇关于<a class="ae ks" rel="noopener" target="_blank" href="/regression-explained-in-simple-terms-dccbcad96f61">回归</a>的文章，回归就是从训练数据中学习权重向量，并使用它来进行预测。获得权重向量的公式为</p><figure class="ls lt lu lv gt jr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/e10317b847e7395028f3b4fc46e068dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*HHcgT3vxC66qpDYVdjPzQQ.png"/></div></figure><p id="7372" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们有一个回归的概率观点，假设因变量“y”正态分布，方差为σ。在这种假设下，从数学上可以看出，上述权重向量 Wₗₛ的方差为</p><figure class="ls lt lu lv gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/07b61f52c4c73e77d5b226542b1efe04.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*h6KutxF-aDXedF8V5rQpmA.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Variance of Wₗₛ</figcaption></figure><p id="becc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了使模型足够稳定，上述方差应该很低。如果权重的方差很高，则意味着模型对数据非常敏感。如果方差很大，则权重与训练数据相差很大。这意味着模型可能无法很好地处理测试数据。所以，自然的问题是，</p><p id="1e78" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">wₗₛ的方差什么时候会大？</strong></p><p id="5cb1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在你应该猜到了，当我们有高度相关的特征时，Wₗₛ的方差会很大。是的，猜对了！！但是让我们看看这在数学上是如何正确的。任何 n×d 矩阵都可以分解为</p><figure class="ls lt lu lv gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/79ea3cd228af527dbf0909cc674b7f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*pjnfR1DPa_mAkJZe3XtGvg.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Singular Value Decomposition</figcaption></figure><p id="bf3e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">上述分解称为“奇异值分解”。上式中的“S”矩阵是非负对角矩阵。利用这种分解，Wₗₛ的方差可以改写为</p><figure class="ls lt lu lv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mh"><img src="../Images/dbf84ab0a5066541b1529f088934ec33.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*YbO_1ZymvlFFa_TDzAKaoA.png"/></div></div></figure><p id="08dd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">当数据集中的要素高度相关时,“S”矩阵中的值会很小。因此,“s”矩阵(上式中的 S^-2)的平方反比将很大，这使得 Wₗₛ的方差很大。</p><p id="7d86" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因此，如果两个要素高度相关，建议我们只在数据集中保留一个要素。我希望这篇文章是有帮助的。如果您有任何疑问，请在下面留下。</p></div></div>    
</body>
</html>