<html>
<head>
<title>Introduction to gradient boosting on decision trees with Catboost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Catboost 对决策树进行梯度提升的介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-gradient-boosting-on-decision-trees-with-catboost-d511a9ccbd14?source=collection_archive---------2-----------------------#2019-02-13">https://towardsdatascience.com/introduction-to-gradient-boosting-on-decision-trees-with-catboost-d511a9ccbd14?source=collection_archive---------2-----------------------#2019-02-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="7f00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">今天我想和大家分享我在<a class="ae kl" href="https://github.com/catboost" rel="noopener ugc nofollow" target="_blank">开源</a>机器学习库的经验，这个库是基于决策树的梯度推进，由俄罗斯搜索引擎公司 Yandex 开发的。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/d28adcf0eeac93ff5d0815503aed66ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q3seb0lpyZJCEfKDrGILTg.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Github profile according to the 12th of February 2020</figcaption></figure><p id="57d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">库是在 Apache 许可下发布的，并作为免费服务提供。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lc"><img src="../Images/a7660c45700341a21e9199bb87e53518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j7eILWmNFFFnwQpsA4GuqQ.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">‘Cat’, by the way, is a shortening of ‘category’, Yandex is enjoying the play on words.</figcaption></figure><p id="92ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可能熟悉梯度增强库，如 XGBoost、H2O 或 LightGBM，但在本教程中，我将快速概述梯度增强的基础，然后逐步转向更核心复杂的东西。</p><h1 id="3b78" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">决策树简介</h1><p id="5019" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">在谈论梯度推进之前，我将从决策树开始。树作为一种数据结构，在现实生活中有很多类比。它被用于许多领域，是决策过程的一个很好的代表。该树由根节点、决策节点和终端节点(不会被进一步分割的节点)组成。树通常是倒着画的，因为树叶在树的底部。决策树可以应用于回归和分类问题。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/a65bd31299d3022a1624478d315469d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*3LXQR_j8gyKOOp7I3j13Vw.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">A simple decision tree used in scoring classification problem</figcaption></figure><p id="f049" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在分类问题中，作为进行二元划分的标准，我们使用不同的指标——最常用的是基尼指数和交叉熵。基尼指数是对 K 个阶层总方差的一种衡量。在回归问题中，我们使用方差或与中位数的平均偏差</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/5e865db48a98303a9ca960a7b5dd42cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*NNIOPtPFhabnaURGMQRT-A.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">The functional whose value is maximized for finding the optimal partition at a given vertex</figcaption></figure><p id="2c30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">生长一棵树包括决定选择哪些特征和使用什么条件进行分割，以及知道何时停止。决策树往往非常复杂和过度拟合，这意味着训练集的误差将很低，但在验证集上却很高。更小的树和更少的分裂可能导致更低的方差和更好的解释，代价是一点点偏差。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/f4bcf2693060e5ec3522c9459c8df906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*RuEeJ99gttCqK5VM04z5Hw.png"/></div></figure><p id="1f31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">决策树在非线性依赖中表现出良好的结果。在上面的例子中，我们可以看到每个类的划分表面是分段常数，并且表面的每一边都平行于坐标轴，因为每个条件都将一个符号的值与阈值进行比较。</p><p id="98e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以用两种方法来避免过度拟合:添加停止标准，或者使用树修剪。停止标准有助于决定，我们是否需要继续划分树，或者我们可以停止，把这个顶点变成一片叶子。例如，我们可以在每个节点中设置多个对象。如果 m &gt; n，则继续划分树，否则停止。n = = 1——最差情况。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mj"><img src="../Images/5348e1866d2893f3fdebf9c51c77073c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YtIqJm0aYohiG692evAh_Q.png"/></div></div></figure><p id="b146" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">或者我们可以调整树的高度。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mk"><img src="../Images/78b9e5717802962662c9c2ea3527cb0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GUIBFES71W3PppRBfMf8Ug.png"/></div></div></figure><p id="6bc9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一种方法是树修剪——我们构建一棵过度拟合的树，然后根据选择的标准删除叶子。修剪可以从根部开始，也可以从叶子开始。从一棵“完全成长”的树上移除树枝——得到一系列逐渐修剪的树。在交叉验证中，我们比较了有分裂和没有分裂的过度拟合树。如果没有这个节点结果更好，我们就排除它。有许多用于优化性能的树修剪技术，例如，减少错误修剪和成本复杂性修剪，其中学习参数(alpha)用于衡量是否可以根据子树的大小删除节点。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ml"><img src="../Images/fe0ca1b7d99dee5297d0549a38646702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cOU9jKJgTScCVkHKN-uUPg.jpeg"/></div></div></figure><p id="5a8c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">决策树受到高方差的影响。这意味着，如果我们将训练数据随机分成两部分，并对这两部分都使用决策树，我们得到的结果可能会非常不同。</p><h1 id="9077" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">合奏</h1><p id="d7b5" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">然而，研究人员发现，结合不同的决策树可能会显示更好的结果。整体——当我们有一个 N 基算法时，最终算法的结果将是基算法结果的函数。我们结合一系列 k 学习模型来创建改进的模型。</p><p id="9f0f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有各种集成技术，如 boosting(用一组分类器进行加权投票)、bagging(对一组分类器的预测进行平均)和 stacking(组合一组异类分类器)。</p><p id="5c8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了构建树集成，我们需要在不同的样本上训练算法。但是我们不能在一台设备上训练它们。我们需要使用随机化在不同的数据集上训练分类。例如，我们可以使用 bootstrap。</p><p id="b653" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">误差的期望值是方差、偏差和噪声的总和。集合由具有低偏差和高方差的树组成。梯度推进算法的主要目标是保持构造具有低偏差的树。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mm"><img src="../Images/0087b86b50ed45da7be03ce387f8a383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N3Ff8UJBbjJZKh_tKD6Oqw.png"/></div></div></figure><p id="86f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，我们需要基于带有噪声的 10 个点来近似右图中的格林函数。在左图中，我们显示了在不同样本上训练的策略。右图用红线显示了平均多项式。</p><p id="a88c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，红色图形与绿色图形几乎相同，而算法分别与绿色函数有显著不同。下面的算法家族有<strong class="jp ir">低偏差，但高方差。</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mn"><img src="../Images/f422949aec035245f00f9d311fda0b97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bhUvfFyQm_SkCV-tlifWsw.png"/></div></div></figure><p id="c28c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">决策树的特点是低偏差但高方差，即使训练样本有很小的变化。总体方差是一个基本算法的方差除以算法数+基本算法之间的相关性。</p><h2 id="f48e" class="mo le iq bd lf mp mq dn lj mr ms dp ln jy mt mu lr kc mv mw lv kg mx my lz mz bi translated">随机森林算法</h2><p id="424d" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">为了减少基算法之间相关性的影响，我们可以使用<strong class="jp ir"> bagging </strong>算法和<strong class="jp ir">随机子空间方法。</strong>这种方法最显著的例子之一是随机森林分类器。这是一种基于随机子空间和 bagging 方法的算法，使用 CART 决策树作为基本算法。</p><p id="a0df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随机子空间方法有助于降低树之间的相关性，避免过度拟合。让我们仔细看看:假设我们有一个数据集，有 D 个特征，L 个对象和 N 个基树。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi na"><img src="../Images/3d8a8a43596394588be7c776d00cd4b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6rXidx45nUOVSs5IqYu5wQ.png"/></div></div></figure><p id="8a05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个基本算法都适合来自 bootstrap 的样本。</p><p id="2928" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们从 D 中随机选择 D 个特征，构建树直到停止准则(我前面提到过)。通常我们用低偏差建立过度拟合的树。</p><p id="ab40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回归问题的特征数 D 为 D/3，分类 sqrt 为(D)。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nb"><img src="../Images/6f4f2f4eab1f79944ed22196551d2ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I69uRUxCYOR2sGDIiZoGpQ.png"/></div></div></figure><p id="406b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">应该强调的是，每个<br/>再次选择一个大小为 d 的随机子集，这是分裂另一个顶点的时候。这是这种<br/>方法与随机子空间方法的主要区别，在随机子空间方法中，在构建基础算法之前选择一次随机特征子集。</p><p id="4fe6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">毕竟，我们应用 bagging 和平均每个基本算法的结果。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nc"><img src="../Images/c21d8a4b4a30bcbf87747f3b3926f326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lbKcS6Jl9e381NtPSjaag.png"/></div></div></figure><p id="f4c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随机森林具有各种优点，如对离群点不敏感，对大特征空间工作良好，难以通过添加更多的树来过度拟合。</p><p id="4adf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而有一个缺点，存储模型需要 O(NK)内存存储，其中<em class="nd"> K </em> —树的数量。这已经很多了。</p><h2 id="c59a" class="mo le iq bd lf mp mq dn lj mr ms dp ln jy mt mu lr kc mv mw lv kg mx my lz mz bi translated">助推</h2><p id="4b02" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">Boosting 是一种加权集成方法。每个基本算法都是一个接一个按顺序添加的。一系列<em class="nd"> N </em>分类器迭代学习。更新权重以允许后续分类器“更多地关注”被先前分类器错误分类的训练元组。加权投票不影响算法的复杂度，但平滑了基本算法的答案。</p><p id="ead0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nd">增压与装袋相比如何？</em> Boosting 侧重于错误分类的元组，它有使生成的复合模型过度适应此类数据的风险。</p><p id="1a64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于构建线性模型的贪婪算法</p><p id="1e32" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下每个算法都是为了纠正现有集合的错误而构建的。</p><ul class=""><li id="7569" class="ne nf iq jp b jq jr ju jv jy ng kc nh kg ni kk nj nk nl nm bi translated">用于阈值损失函数的不同近似</li></ul><p id="08be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以 MSE 作为损失函数的标准回归任务为例</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a884c801395805d469337add0c296c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*aJ05kGuCOSjg4F1X0uWbZg.png"/></div></figure><p id="d45f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通常作为基本算法，我们采用最简单的算法，例如我们可以采用一个短决策树</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/4a5815311ade459ff09887f0e12ee175.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*QzA4FWiW_Hz8Ql_l9SyiJQ.png"/></div></figure><p id="6de4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二种算法必须以最小化合成<em class="nd"> b1(x) </em>和<em class="nd"> b2(x) </em>的误差的方式拟合</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi np"><img src="../Images/b44e1dd8513e71e1fde55ee4de9b1cd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UQZ-1De_z5dHbFlb6QsPyg.png"/></div></div></figure><p id="2f20" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">至于<em class="nd"> bN(x) </em></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/6cd862d1d2f46c3bdbf5e8e1576c0444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*Q3jg0fcs4CxHzZ6fQLqt7A.png"/></div></figure><h2 id="e2fb" class="mo le iq bd lf mp mq dn lj mr ms dp ln jy mt mu lr kc mv mw lv kg mx my lz mz bi translated">梯度推进</h2><p id="b592" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">已知梯度推进是主要的集成算法之一。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nr"><img src="../Images/f83f56fadb88a217f7ff7d187ceaf167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nzXEJIu1wK_YNMBmmM0ntA.png"/></div></div></figure><p id="bae1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度推进算法采用梯度下降法优化损失函数。这是一种迭代算法，步骤如下:</p><ol class=""><li id="e234" class="ne nf iq jp b jq jr ju jv jy ng kc nh kg ni kk ns nk nl nm bi translated">初始化第一个简单算法<em class="nd"> b0 </em></li><li id="7ccf" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">在每次迭代中，我们生成一个移位向量<em class="nd"> s = (s1，..sl)。si — </em>训练样本上算法<em class="nd"> bN(xi) = si </em>的值</li></ol><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c8d4ace84d07381821d1eedc7c2fa8f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*gACz4AOb7jNlMnhkcy768w.png"/></div></figure><p id="8a27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.那么算法就是</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/ac756be7b8252f9dcc47f6460d1c5d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*RozR-qRYnNGOgNv5wewWnw.png"/></div></figure><p id="04f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.最后，我们将算法<em class="nd"> bN </em>添加到集成中</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/60a8b25f9a1f6070e860ecf239363a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*WDQaS7Y3unSlz2a3Oob_Ug.png"/></div></figure><p id="c7c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有几个可用的梯度增强库:XGBoost、H20、LightGBM。它们之间的主要区别在于树结构、特征工程和处理稀疏数据</p><h1 id="5b0d" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">Catboost</h1><p id="5511" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">Catboost 可以解决回归、分类、多类分类、排序等问题。模式因目标函数而异，我们试图在梯度下降过程中将其最小化。此外，Catboost 有预构建的<a class="ae kl" href="https://tech.yandex.com/catboost/doc/dg/concepts/loss-functions-docpage/" rel="noopener ugc nofollow" target="_blank">指标</a>来衡量模型的准确性。</p><p id="22d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 Catboost <a class="ae kl" href="https://catboost.ai/#benchmark" rel="noopener ugc nofollow" target="_blank">官方网站</a>上，你可以找到 Catboost (method)与主要基准的比较，或者你可以在 Neptune.ai <a class="ae kl" href="https://bit.ly/3OQC3aR" rel="noopener ugc nofollow" target="_blank">博客</a>上深入研究这个主题</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ob"><img src="../Images/6209d0da5daa016adea481ae4584fdab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tZDKh261_bqdAE9QqmH_zQ.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Figures in this table represent Logloss values (lower is better) for Classification mode.</figcaption></figure><p id="a542" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">百分比是根据优化的 CatBoost 结果测量的度量差异。</p><h1 id="b1b4" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">Catboost 优势</h1><p id="7bb6" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">Catboost 引入了以下算法进步:</p><ol class=""><li id="60e0" class="ne nf iq jp b jq jr ju jv jy ng kc nh kg ni kk ns nk nl nm bi translated">一种用于处理分类特征的创新算法。不需要自己对特性进行预处理—它是开箱即用的。对于具有分类特征的数据，与另一种算法相比，准确性会更好。</li><li id="3b32" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">实施<strong class="jp ir">有序升压</strong>，这是一种替代经典 bosting 算法的置换驱动算法。在小型数据集上，GB 很快就会被过度分配。在 Catboost 中，有一个针对这种情况的特殊修改。也就是说，在其他算法存在过度拟合问题的数据集上，您不会在 Catboost 上观察到相同的问题</li><li id="d4a0" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">快速易用的 GPU 训练。您可以通过<em class="nd"> pip-install </em>简单地安装它</li><li id="84d6" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">其他有用的特性:缺少值支持，可视化效果好</li></ol><h2 id="c569" class="mo le iq bd lf mp mq dn lj mr ms dp ln jy mt mu lr kc mv mw lv kg mx my lz mz bi translated">分类特征</h2><p id="6e30" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">分类特征是一组离散的值，称为<em class="nd">类别</em>，彼此不可比。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oc"><img src="../Images/ef0df887faf045ace7eb26c8989d30fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*voAg6dG_q_j1KVg9UYRVWA.png"/></div></div></figure><p id="e7b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Catboost 的主要优势是分类数据的智能预处理。您不必自己对数据进行预处理。对分类数据进行编码的一些最流行的做法是:</p><ol class=""><li id="da87" class="ne nf iq jp b jq jr ju jv jy ng kc nh kg ni kk ns nk nl nm bi translated">一键编码</li><li id="ac57" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">标签编码</li><li id="3a39" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">哈希编码</li><li id="82f2" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">目标编码</li><li id="af4e" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">等等..</li></ol><p id="9dc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于具有少量不同特征的分类特征，一键编码是一种流行的方法。Catboost 将<em class="nd"> one_hot_max_size </em>用于多个不同值小于或等于给定参数值的所有特征。</p><p id="e708" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在具有高基数的特征的情况下(例如，像“用户 ID”特征)，这样的技术导致不可行的大量新特征。</p><p id="1992" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一种流行的方法是通过<em class="nd">目标统计</em> (TS)对类别进行分组，这些统计估计了每个类别中的预期目标值。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi od"><img src="../Images/5d8524e1f62c1cbe77fd1c02c3eedbdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLbtj_bBYSGvsJCddRrwLg.png"/></div></div></figure><p id="aa60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种贪婪方法的问题是目标泄漏:使用前一个特征的目标来计算新特征。这导致了有条件的转移——对于训练和测试示例，分布是不同的。</p><p id="ffd9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">解决这个问题的标准方法是<em class="nd">保持 TS </em>和<em class="nd">留一</em>TS。但是他们仍然不能防止模型目标泄漏。</p><p id="6507" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">CatBoost 使用更有效的策略。它依赖于排序原则，被称为<em class="nd">基于目标与先验(TBS) </em>。它是受在线学习算法的启发，在线学习算法按时间顺序获取训练样本。每个示例的<em class="nd"> TS </em>的值仅依赖于观察到的历史。为了使这种思想适应标准的离线设置，我们引入了人工“时间”，即训练样本的随机排列σ。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oe"><img src="../Images/24bee91a971a5d713364c74be2b747a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XfV0SkW-7NRac768CKU9LQ.png"/></div></div></figure><p id="26cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 Catboost 中，数据被随机打乱，并且只对每个对象的历史数据计算平均值。数据可以被多次重组。</p><p id="aa07" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">CatBoost 的另一个重要细节是使用分类特征的组合作为额外的分类特征，这些分类特征在广告点击预测任务中捕获高阶依赖性，如用户 ID 和广告主题的联合信息。可能组合的数量随着数据集中分类要素的数量呈指数增长，不可能处理所有的组合。CatBoost 以贪婪的方式构造组合。也就是说，对于树的每次分裂，CatBoost 将当前树中先前分裂已经使用的所有分类特征(及其组合)与数据集中的所有分类特征相结合(连接)。组合被动态地转换成 TS。</p><h2 id="755f" class="mo le iq bd lf mp mq dn lj mr ms dp ln jy mt mu lr kc mv mw lv kg mx my lz mz bi translated">对抗梯度偏差</h2><p id="1189" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">CatBoost 实现了一种算法，允许对抗常见的梯度增强偏差。现有的实现面临统计问题，<em class="nd">预测偏移。</em>训练示例的分布<em class="nd"> F(x_k) | x_k </em>从测试示例<em class="nd"> x </em>的分布<em class="nd"> F(x) | x </em>转移。这个问题类似于上述分类变量预处理中出现的问题。</p><p id="9566" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Catboost 团队衍生了有序增强，这是标准梯度增强算法的一种修改，可以避免目标泄漏。CatBoost 有两种升压模式，<em class="nd">命令</em>和<em class="nd">普通</em>。后一种模式是内置有序 TS 的标准 GBDT 算法。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi of"><img src="../Images/66b439fc9b718936746d0ab62c04b9bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzYUvOkdstmTtZDjfB_ncg.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">You can find a detailed description of the algorithm in the paper <a class="ae kl" href="https://arxiv.org/abs/1706.09516" rel="noopener ugc nofollow" target="_blank">Fighting biases with dynamic boosting</a></figcaption></figure><p id="2484" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">CatBoost 使用不经意决策树，在树的整个级别上使用相同的分裂标准。这样的树是平衡的，不容易过度拟合，并允许在测试时显著加速预测。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/e5a7197ecaed687d8bb6fc4d9370655f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AjrRnwvBuu-zK8CvEfM29w.png"/></div></div></figure><p id="fe87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是不经意树评估在<a class="ae kl" href="https://github.com/catboost/catboost/blob/92fd122c98f273a6df77de72f3c4e3bc2e1f5ee2/catboost/libs/model/model_calcer.h#L37" rel="noopener ugc nofollow" target="_blank"> Catboost </a>中的实现:</p><pre class="kn ko kp kq gt og oh oi oj aw ok bi"><span id="ca74" class="mo le iq oh b gy ol om l on oo">int index = 0;<br/> for (int depth = 0; depth &lt; tree.ysize(); ++depth) {<br/>  index |= binFeatures[tree[depth]] &lt;&lt; depth;<br/> }<br/> result += Model.LeafValues[treeId][resultId][index];</span></pre><p id="0e6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如您所见，这段代码中没有“if”操作符。你不需要分支来评估一个健忘的决策树。</p><p id="1f5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">遗忘决策树可以描述为一个条件列表，每层一个条件。对于不经意树，你只需要评估所有树的条件，组成二进制向量，将这个二进制向量转换成数字，并通过等于这个数字的索引访问叶子数组。</p><p id="8a93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如在 LightGBM 中(XgBoost 有类似的实现)</p><pre class="kn ko kp kq gt og oh oi oj aw ok bi"><span id="b406" class="mo le iq oh b gy ol om l on oo">std::vector&lt;int&gt; left_child_;<br/>std::vector&lt;int&gt; right_child_;</span><span id="155e" class="mo le iq oh b gy op om l on oo">inline int NumericalDecision(double fval, int node) const { <br/>...<br/>  if (GetDecisionType(decision_type_[node], kDefaultLeftMask)) {<br/>   return left_child_[node];<br/>  } else {<br/>   return right_child_[node];<br/>  }<br/>...<br/>}<br/> <br/>inline int Tree::GetLeaf(const double* feature_values) const {<br/>...<br/> while (node &gt;= 0) {<br/>   node = NumericalDecision(feature_values[split_feature_[node]], node);<br/>  }<br/>...<br/>}</span></pre><p id="1274" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在有序增强模式中，在学习过程中，我们维护支持模型<em class="nd"> Mr，j </em>，其中<em class="nd"> Mr，j(i) </em>是基于排列<em class="nd"> σr </em>中的前 j 个示例的第<em class="nd"> i </em>个示例的当前预测。在算法的每次迭代<em class="nd"> t </em>中，我们从<em class="nd"> {σ1，.。。，σs} </em>并在此基础上构造一棵 Tt。首先，对于分类特征，所有的 TS 都是根据这个排列计算的。第二，排列影响树学习过程。</p><p id="5c42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于<em class="nd"> Mr，j(i)，</em>计算相应的梯度。在构建树时，我们根据余弦相似性来近似梯度<em class="nd"> G </em>，其中对于每个示例 I，我们基于之前的示例取梯度为<em class="nd">σs。</em>当树结构<em class="nd"> Tt </em>(即，分裂属性的序列)被构建时，我们使用它来提升所有模型<em class="nd">Mr’，j </em></p><p id="2378" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以在<a class="ae kl" href="https://arxiv.org/pdf/1706.09516.pdf" rel="noopener ugc nofollow" target="_blank">原始文件</a>或<a class="ae kl" href="https://github.com/catboost/catboost/blob/master/slides/CatBoostPaper_NIPS2018/Presentation.pdf" rel="noopener ugc nofollow" target="_blank"> NIPS 的 18 张幻灯片</a>中找到详细信息</p><h2 id="5e0e" class="mo le iq bd lf mp mq dn lj mr ms dp ln jy mt mu lr kc mv mw lv kg mx my lz mz bi translated">GPU 培训</h2><p id="cc10" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">CatBoost 可以在一台机器的几个 GPU 上高效训练。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oq"><img src="../Images/cc25cf0780d4d28a6a2113cd4760b1c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fe2OAxZpdZv0UKMcA21BMg.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Experimental result for different hardware</figcaption></figure><p id="fda1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">CatBoost 实现了良好的可扩展性。在采用 InfiniBand 的 16 个 GPU 上，CatBoost 的运行速度比 4 个 GPU 快大约 3.75 倍。对于更大的数据集，可伸缩性应该更好。如果有足够的数据，我们可以在缓慢的 1gb 网络上训练模型，因为两台机器(每台机器有两个卡)不会明显慢于一个 PCIe 根联合体上的 4 个 GPU。你可以在这篇 NVIDIA <a class="ae kl" href="https://devblogs.nvidia.com/catboost-fast-gradient-boosting-decision-trees/" rel="noopener ugc nofollow" target="_blank">文章</a>中了解更多信息</p><p id="65c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在所描述的优点中，还需要提到以下一个:</p><ol class=""><li id="50a8" class="ne nf iq jp b jq jr ju jv jy ng kc nh kg ni kk ns nk nl nm bi translated">过拟合检测器。通常在梯度推进中，我们将学习速率调整到稳定的精度。但是学习率越小，需要的迭代次数就越多。</li><li id="58f5" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">缺少变量。刚离开南</li><li id="9dcb" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">在 Catboost 中，您可以编写自己的损失函数</li><li id="8cb3" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">特征重要性</li><li id="0180" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk ns nk nl nm bi translated">CatBoost 为<a class="ae kl" href="https://catboost.ai/docs/concepts/python-installation.html#python-installation" rel="noopener ugc nofollow" target="_blank"> Python 包</a>提供工具，允许用不同的训练统计数据绘制图表。该信息可以在训练过程中和训练之后被访问</li></ol><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi or"><img src="../Images/60521f49654d7b1c9b0689da85f19fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e1pA1dT4jv6_jI8VYUw4yg.jpeg"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Monitor training in iPython Notebook using our visualization tool CatBoost Viewer.</figcaption></figure><p id="0a03" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Catboost 模型可以集成到 Tensorflow 中。例如，将 Catboost 和 Tensorflow 结合在一起是常见的情况。神经网络可用于梯度增强的特征提取。</p><p id="56f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，现在 Catboost 模型可以在 CoreML 的帮助下用于生产。</p><h1 id="1f4e" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">例子</h1><p id="7940" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">我创建了一个应用 Catboost 解决回归问题的<a class="ae kl" href="https://colab.research.google.com/drive/1Lte4Pb4lw_w7OGp2wqwS5a50ZgBIcW6j" rel="noopener ugc nofollow" target="_blank">例子</a>。我使用 Allstate 索赔严重程度的数据作为基础。</p><p id="b816" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在您的进一步研究中，请随意使用<a class="ae kl" href="https://colab.research.google.com/drive/1Lte4Pb4lw_w7OGp2wqwS5a50ZgBIcW6j#scrollTo=nPJ39y19C41z" rel="noopener ugc nofollow" target="_blank"> my colab </a>！</p><p id="06ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你也可以在 Catboost 官方的 github 中找到大量其他的<a class="ae kl" href="https://github.com/catboost/tutorials" rel="noopener ugc nofollow" target="_blank">例子</a></p><h1 id="a934" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">贡献</h1><p id="8193" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">如果你想让 CatBoost 变得更好:</p><ul class=""><li id="424a" class="ne nf iq jp b jq jr ju jv jy ng kc nh kg ni kk nj nk nl nm bi translated">查看<a class="ae kl" href="https://github.com/catboost/catboost/labels/help%20wanted" rel="noopener ugc nofollow" target="_blank">求助</a>问题，看看有哪些可以改进的地方，或者如果您需要什么，可以打开一个问题。</li><li id="0a2e" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk nj nk nl nm bi translated">将您的故事和经验添加到<a class="ae kl" href="https://github.com/catboost/catboost/blob/master/AWESOME.md" rel="noopener ugc nofollow" target="_blank"> Awesome CatBoost </a>中。</li><li id="bae6" class="ne nf iq jp b jq nt ju nu jy nv kc nw kg nx kk nj nk nl nm bi translated">要向 CatBoost 投稿，您需要首先阅读 CLA 文本，并在您的请求中添加您同意 CLA 条款的内容。更多信息可以在<a class="ae kl" href="https://github.com/catboost/catboost/blob/master/CONTRIBUTING.md" rel="noopener ugc nofollow" target="_blank"> CONTRIBUTING.md </a>找到<a class="ae kl" href="https://tech.yandex.com/catboost/doc/dg/concepts/development-and-contributions-docpage/" rel="noopener ugc nofollow" target="_blank">对贡献者的说明可以在</a>这里找到。</li></ul><p id="bc0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关注<a class="ae kl" href="https://twitter.com/kidrulit" rel="noopener ugc nofollow" target="_blank">推特</a>或微信(zkid18)来了解我的最新消息。</p></div></div>    
</body>
</html>