# 通往埃勒沃恩之路

> 原文：<https://towardsdatascience.com/the-road-to-erewohn-285197f90e1c?source=collection_archive---------25----------------------->

## 人工智能:一场无人问津的军备竞赛

![](img/1f9322ceb7cd251b4c09ad1ffa2b5b36.png)

现有的机器我都不害怕；我担心的是，他们正以超乎寻常的速度变得与现在大相径庭。在过去的任何时候，没有哪一类生物有如此快速的进步。在我们还能检查它的时候，难道不应该小心地观察和检查这个运动吗？(……)难道从事管理机器的人可能比从事管理人的人多吗？难道不是我们自己在创造地球上至高无上的继承者吗？每天增加他们的组织的美丽和精致，每天给他们更大的技能，提供越来越多的自我调节，自我行动的力量，这将优于任何智力？

塞缪尔·巴特勒，艾瑞沃恩。1872.

每一代人都认为他们面临的挑战是新的，但实际上很少是新的。这也是我们面临的人工智能(AI)发展的挑战。似乎他们已经预见到了 150 年前英国工业革命时期[的到来。在一部写于 19 世纪的小说中读到这些反思，肯定会让你心惊肉跳。](https://utopiaordystopia.com/tag/erewhon/)

我们关闭了[科幻小说](/science-non-fiction-e50b589aa435)，这是我们关于这个主题的观点的第一部分，尼克·博斯特罗姆[想知道](https://www.youtube.com/watch?v=pywF6ZzsghI)在超级智能系统到来之前，我们可以做些什么来增加我们作为一个物种的生存机会。现在，一些专家说这还很遥远，而另一些人则说我们可能就在这种情景的边缘，这将在未来的 20 到 40 年内发生。考虑到多达 30%的这些专家认为，当它到来时，结果对我们来说要么是坏的，要么是非常坏的，这肯定是我们应该谈论的事情。有 30%的几率属于濒危物种绝对不酷。1%或 2%，好吧，我可以接受，但是……30%？？

围绕人工智能发展的可能影响的辩论今天集中在非常严重的事情上，这些事情已经存在，当然值得辩论。我们在上一篇文章中已经提到了其中的一些:[正在中国部署的社会信用系统](https://www.wired.co.uk/article/china-social-credit-system-explained),[致命的自主武器](https://www.nytimes.com/2018/11/15/magazine/autonomous-robots-weapons.html)或者警方使用的面部识别软件中的[偏差效应](http://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212)。然而，事实上，在几年内终结者的未来可能会成为现实，但这一次没有约翰·康纳或阻力或任何希望，似乎并不困扰我们。

挺奇怪的。我们不都应该非常害怕吗？或者换一种说法:为什么几乎没人谈这个？

似乎有几个原因。首先，你会有这种“科幻小说”效应:人们倾向于认为那些敢于给出如此激进预测的人是不科学或轻率的。还有一些与人性有关的原因:我们天生就担心直接和具体的威胁，而不是未来和抽象的威胁。此外，我们的天性中也有一种“一厢情愿”的效应:如果他们告诉我们，有了人工智能，未来将会很美好，为什么要担心呢？让我们把这件事交给专家吧，他们肯定知道自己在做什么。想起来了吗？另一个可能的原因是，人们对技术进步有些厌倦:我们近年来看到了如此多的变化，以至于像 AlphaZero 这样的革命性产品几乎没有成为科技媒体的头条新闻。最后，也许其中一个关键原因是“军备竞赛”效应，在这种效应中，领导这场革命的代理人试图确保对人工智能的担忧不会设置任何额外的障碍，因为这将拖延他们，使他们的竞争对手受益。

先来看看那个“科幻”效果。直到不久前，具有人类智能的系统，更不用说人工超级智能的出现，被大多数专家认为是不真实的事件，或者至少是在很久以前值得担心的事情。然而，在过去的二十年里，这种情况发生了根本的变化。当雷·库兹韦尔在他的书[精神机器时代](https://en.wikipedia.org/wiki/The_Age_of_Spiritual_Machines)中声称机器将在 2029 年通过[图灵测试](https://en.wikipedia.org/wiki/Turing_test)时，说得委婉点，专家们的普遍反应是不相信。许多批评家认为他不是一个严肃的科学家，其他人则认为这个预测纯属无稽之谈。大多数人确信，如果它真的发生了，这确实是非常可疑的，数百年后才会发生。所以雷在 1999 年出版了这本书。在 2013 年[的一项调查](https://nickbostrom.com/papers/survey.pdf)，也就是说，不到 15 年后，这些专家的共识已经从“数百年后”下降到“到 2040 年代”雷说，如果他们继续保持这种节奏，他们最终会批评他是保守派，这是正确的。(事实上，2018 年在美国普通人群中进行的一项[调查](https://governanceai.github.io/US-Public-Opinion-Report-Jan-2019/)显示，到 2028 年，这种情况发生的可能性超过 50%)。

你必须非常小心地拒绝看起来像科幻小说的预测，因为为了让小说成为非小说，有时你必须以非常高的激励加入人才。人工智能领域的另一位主要研究人员 Stuart Russell[讲述了下面的故事](https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom):发现原子蕴含大量能量的科学家欧内斯特·卢瑟福勋爵在 1933 年说，“任何期望从这些原子的转化中获得能量的人都是在胡说八道。”第二天，另一位科学家读了这篇评论，一点也不喜欢。恼怒之下，他出去散步，突然想到了核连锁反应的想法。几年后，在赢得世界大战的激励下，这个想法被付诸实践，美国制造了原子弹。斯图亚特用这个故事作为例子，让我们学会谨慎。他指出，至于激励措施，数十亿美元正被投入到人工智能的开发中；不可思议的突破事实上可能随时发生，将之前的捕风捉影或科幻小说变成科学。

然而，很明显，这种“科幻小说”的效果仍然很强。尽管这十年来取得了所有的进步，尽管像[尼克·博斯特罗姆](https://en.wikipedia.org/wiki/Nick_Bostrom)这样的人所做的工作，尽管像[比尔·盖茨](https://www.washingtonpost.com/news/the-switch/wp/2015/01/28/bill-gates-on-dangers-of-artificial-intelligence-dont-understand-why-some-people-are-not-concerned/?noredirect=on&utm_term=.5f21221fc075)或[斯蒂芬·霍金](https://www.bbc.com/news/technology-30290540)这样可信的公众人物已经警告我们应该认真对待最坏的情况。但是盖茨和霍金发表声明已经有几年了，事实是他们想要发起的一般性辩论并没有发生。

离开学术界，走到大街上，会有更多相同的事情发生。严肃的媒体谈论我们已经存在的问题，但对几年后会发生什么却几乎只字不提。不出所料，DeepMind 只有在有体育角度的时候才会成为一些头条新闻和大量关注:在最后的挑战中，人对抗机器， [Lee Sedol vs. AlphaGo](https://www.alphagomovie.com/) 。当我们说巨大的关注时，这在某种程度上仅限于日本、中国和韩国，在这些国家，围棋(T2 战略棋类游戏 T3)不仅仅是一项全国性的运动。也许真的要就人工智能展开一场大规模的辩论，斯蒂芬·霍金和李·塞多尔的名字应该在头条上换成金·卡戴珊和弗洛伊德·梅威瑟的名字。

让我们转到下一个为什么今天这种争论几乎不存在的原因:人性。必须明白，从进化的角度来看，我们或多或少与十万年前的生物是一样的，因此我们这个物种被设计来生活在与今天非常不同的环境中。狩猎采集者，也就是智人在地球上 90%的时间里所做的，不必应对未来和抽象的威胁，而是非常直接和具体的威胁:我宁愿不爬上那棵树去找蛋，因为它太高了，我可能会掉下来；这蘑菇看起来不太好吃，我最好不要吃它；风带来了老虎的味道，我们跑吧！至于长期战略，狩猎采集者最多评估迁移到另一个地区，以提高他们的生存机会:天气开始变冷；寻找食物变得越来越困难；这里有太多的食肉动物。

当我们不再采集水果，不再成群结队地离开自己的领地，转而在民族国家的大城市出售金融衍生品，过着定居的生活时，对威胁的战略管理变得更加复杂。我们的进化准备不再相关，也许除了那些生活在这些大城市的某些街区的人。社会制度是分层的，关于什么是对整个社会的威胁以及如何避免这种威胁的决定被委托给来自上层的少数个人。这些更高的阶层变得越来越擅长于通过制造或放大威胁来管理人口，并根据什么最符合他们的利益来排除或压制其他人。

因此，毫不奇怪，在非常严重的威胁之后做出的一些最重要的决定并不完全是对整个社会最有利的决定。尽管有大量的信息和越来越可靠的预测工具。例如，让我们强调一下导致第一次世界大战大屠杀的一连串计算错误(特别指出在战争可能爆发之前传递给民众的普遍热情)。一个更紧迫的例子是:影响气候变化挑战决策的利益冲突(对我的股东来说，将我的部分利润投资于否认气候变化的游说团体比解决问题并不得不战略性地调整我的业务更有利可图，因此也更好。)

在许多情况下，无论是个人还是集体，人性的另一个变量也会发挥作用:“一厢情愿”效应，即希望事情会变好，并据此进行规划。在人工智能的发展中，似乎有很多这样的事情。可能的有利后果是如此诱人，以至于人们真的愿意相信我们会实现它们。这很好，我们都想要那些有利的结果。然而，对任何好父亲的书来说，最明智的建议之一(也是许多商业大师效仿的建议之一)是:“抱最好的希望，做最坏的准备”。思考和工作很棒，这样人工智能将会帮助我们生活得越来越好；但与此同时，有必要做好准备和计划，这样我们就不会从新技术的袋子里拿到那个黑球，那个意味着我们毁灭的黑球。

说到新技术，上面提到的我们似乎没有太关注人工智能革命的另一个原因恰恰是技术疲劳。发生这种情况的部分原因是指数增长曲线，这对我们来说可能已经有点困难了。技术发展越来越快，范围越来越广。我们很难注意到新的东西，被提供的信息淹没了。关于像 AlphaGo 这样的人工智能的革命性新闻在几个月后随着 [AlphaZero](https://www.technologyreview.com/s/609736/alpha-zeros-alien-chess-shows-the-power-and-the-peculiarity-of-ai/) 的出现而过时。谁能跟上？

我们似乎也没有做好准备。如果你年龄够大，你可能会记得，当你习惯了互联网和电子邮件时，手机及其应用程序和社交网络大量出现。当你最终拥抱另一项技术时，它带来了前所未有的大规模控制和操纵水平(正如我们在 [Bricks in your wall](https://medium.com/@borja_45971/bricks-in-your-wall-c5e2306502a0) 中指出的)。作家尤瓦尔·诺亚·哈拉里(Yuval Noah Harari)在[为《卫报》撰写的这篇专栏文章](https://www.theguardian.com/books/2018/sep/14/yuval-noah-harari-the-new-threat-to-liberal-democracy)中提出了巨大的问题:“在一个政府和企业可以攻击人类的时代，自由民主如何运作？“选民最清楚”和“顾客永远是对的”这两个信念还剩下什么？当你意识到你是一种可被黑客攻击的动物，你的心脏可能是政府特工，你的杏仁核可能为普京工作，你脑海中出现的下一个想法很可能是某种算法的结果，而这种算法比你自己更了解你，你该如何生活？”想想我们已经面临的这些挑战就让人不知所措。但这并不意味着我们不应该考虑明天我们将不得不面对的挑战，在它们碾过我们之前(是的，又有[那辆卡车的比喻](/science-non-fiction-e50b589aa435)！).

因此，我们得出了我们指出的最后一个原因:“军备竞赛”效应，这个术语似乎来自冷战。因为引领人工智能发展的是世界主要国家的政府，当然是由美国和中国领导的；以及大型科技公司，如谷歌、脸书、亚马逊、微软、IBM、百度或腾讯。演员有巨大的能力来影响一场辩论，并且有太多的利害关系来回避它。因为那些面临更多障碍(批评、监管之类的东西)的人很可能会在比赛中被拖延，从而给竞争对手带来优势。

想想吧。这场比赛的获胜者会得到什么奖励？如果一切顺利，这个系统将有能力变成超人，对于那些控制它的人来说，有能力或者负担得起。一个可能产生另一个物种的系统，一个可能像人类看待黑猩猩一样，或者更糟，像一个农民看待他的牲畜一样，将人类视为被忽略的系统。一个有可能把所有好处都给第一个人，而第二个人却一点好处都没有的体系。这是这场竞赛的关键所在。

我们之前谈到社会上层倾向于制造或放大最符合他们利益的威胁，忽视或压制阻碍他们的威胁。我们可以肯定地说，这场竞赛的参与者是那些更高阶层的精英。正如在任何比赛中发生的那样，为了第一个到达，你必须比其他人跑得更快。为此，你最好有最快的车，否则你将需要承担更多的风险来竞争。

这也许是最可怕的事情:对于这些参赛者来说，任何比他们先到终点的参赛者似乎都比冒更大的风险先到终点更具威胁，即使这意味着每个人都要拿到黑球。哦，好吧，运气不好，你打算怎么办？难怪他们都不喜欢人们大喊“灭绝！”他们会尽全力阻止这种情况发生。不是灭绝，而是人们的呼喊。

好了，这就是为什么你很可能看不到迎面驶来的卡车，直到它压在你身上的原因。现在让我们更仔细地看看那些参加比赛的人和他们在做什么。

首先，中国政府已经做出了明确而果断的承诺，要在未来几年加快步伐，并努力走在前列。这当然包括大型中国科技公司，如腾讯或百度，以及数百家获得巨额融资的新人工智能初创公司。与此一致的其他变量是庞大的人口，由于其控制的日益有效性，他们几乎没有抗议或反应的能力，以及为人工智能系统提供燃料的巨大数据收集能力。结果是[越来越像乔治·奥威尔的](https://www.nytimes.com/2018/07/08/business/china-surveillance-technology.html)[*1984*](https://en.wikipedia.org/wiki/Nineteen_Eighty-Four)反面乌托邦，一些“改进”似乎直接取自黑镜，如社会行为信用系统。至少他们不太在乎隐藏如果他们赢得比赛会发生什么。

现在美国是一个更加复杂的例子。有两个主要因素推动着人工智能的发展。第一个是五角大楼，有像现在著名的 Project Maven 这样的东西，旨在加速对军用无人机拍摄的图像进行分析，自动对物体和人的图像进行分类(花点时间想想它可能的用途)。其次，大型科技公司:首先是谷歌、脸书、亚马逊、微软和 IBM。我们说这种情况更复杂，因为在中国，所有参与者或多或少都是一致的，而在美国，五角大楼和大技术公司竞相吸引专业人才，同时还在数十亿美元的开发合同中进行合作。这导致了摩擦。例如，去年，面对自己员工的激进主义，[谷歌拒绝续签](https://www.nytimes.com/2018/06/01/technology/google-pentagon-project-maven.html)其 Project Maven 合同。正如[这篇文章所解释的](https://www.wired.com/story/inside-the-pentagons-plan-to-win-over-silicon-valleys-ai-experts/)，树各方利益冲突:一方面是专业人士，本案中的谷歌员工，他们不希望自己的工作被用来杀人；另一方面，这些公司从股东那里获得并维持数十亿美元的合同，不管承包商是谁(最终可能是中国，就像[项目蜻蜓](https://www.theverge.com/2018/11/28/18116447/google-employees-project-dragonfly-chinese-search-engine-protest-letter-400-signatures))；最后但并非最不重要的是，五角大楼，它希望利用这项技术继续拥有对美国对手的军事优势，为此，他们试图建立一座桥梁，以“帮助我们，我们是这里的好人”的角度来挽救与其他两个政党的分歧。

在大型科技公司中，谷歌是该领域最活跃的公司。事实上，据估计，80%的主要人工智能研究人员和开发人员(例如 Ray Kurzweil)为谷歌或其正在收购的一些领先公司工作，如波士顿动力或 DeepMind。后者的创始人兼首席执行官[德米斯·哈萨比斯](https://www.youtube.com/watch?v=iKCR60VMpBI)这样描述它的使命:“第一步，解决智能问题。第二步，用它来解决其他一切。”整个领域都有一个坚定的共识，即 DeepMind 是实现这一目标的最佳演员之一。他们在游戏中应用深度强化学习算法的进展是第一步中非常成功的一部分，Demmis 本人[预计未来几年将在科学和医学的更重要领域取得突破性进展](https://www.youtube.com/watch?v=XSGhzmA8WVk)。从这个意义上说，他们刚刚引入了 alpha fold(T7)，一个预测蛋白质结构的系统。

DeepMind 创始人同意谷歌收购的原因之一是，该协议包括建立一个道德委员会，以确保他们未来创造的技术不会被滥用。关于这个委员会， [Nick Bostrom 说](https://www.theguardian.com/technology/2017/jan/26/google-deepmind-ai-ethics-board)“在我看来，一个以‘解决智能问题’为雄心的组织有一个思考成功意味着什么的过程是非常合适的，尽管这是一个长期目标(……)提前开始学习这些内容比在考试前一天晚上做准备更好。”

在人工智能及其未来的使用中，肯定不缺少关于伦理和安全的董事会、委员会、研究所、论坛和其他倡议。大型科技公司创建了一个名为[人工智能伙伴关系的组织，以造福人类和社会](https://www.partnershiponai.org/)。埃隆马斯克促成了另一个，[开启 AI](https://openai.com/) 。在大学领域，牛津的[人类未来研究所](https://www.fhi.ox.ac.uk/)，伯克利的[机器智能研究所](https://intelligence.org/)和[人类兼容智能建筑中心](http://humancompatible.ai/)，剑桥的[存在风险研究中心](http://cser.org/)，或者哈佛和麻省理工学院的[生命未来研究所](http://thefutureoflife.org/)，已经负责发表了几篇重要论文和调查，这些论文和调查甚至不时会成为头条新闻。

但事实是，总体形势相当令人担忧，正如凯尔西·派珀在这篇文章中所说。他使用的类比非常清楚:“总体而言，该领域的[状态](https://arxiv.org/pdf/1805.01109.pdf)有点像几乎所有的气候变化研究人员都在专注于管理我们今天已经面临的干旱、野火和饥荒，只有一个小型骨干团队致力于预测未来，大约 50 名研究人员全职工作，制定扭转局面的计划。”

简而言之:人工智能发展中的伦理和安全缺乏国际层面的公共政策协调，没有关于其最深远影响的公开和普遍的辩论，发表的关键论文几乎没有超出学术领域，致力于长期研究和预防的资源非常荒谬。与此同时，获得人工智能的竞赛获得了数十亿美元的资金，这种人工智能将给予企业或地缘战略对手明确的竞争优势。

所以，用我们开始这一切的比喻来结束，这些人在一条充满他们没有地图的曲线的道路上进行一场 8 轴拖车卡车比赛。比赛的规则很简单:赢家通吃。速度越来越快，没有人打算减速。你走在那条路上，呆呆地看着你的手机。你的孩子正在前方某处玩耍，在一个弯道后面。

祝你有愉快的一天。

[Goonder](https://goonder.com/EN/) 2019