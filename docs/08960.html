<html>
<head>
<title>A 10-line proof of back propagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播的 10 行证明</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-10-line-proof-of-back-propagation-5a2cad1032c4?source=collection_archive---------18-----------------------#2019-11-29">https://towardsdatascience.com/a-10-line-proof-of-back-propagation-5a2cad1032c4?source=collection_archive---------18-----------------------#2019-11-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8991" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">没有求和也没有索引的矢量证明。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ab9bae0f3ba555e048a4cba2fed3ab55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*P2Rf_-5Atf4-54M9"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@timmarshall?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tim Marshall</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="599f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">TLDR</h1><p id="2b90" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">反向传播是深度学习的脊梁。虽然有大量关于这一主题的文献，但很少有人彻底解释反向传播所需的梯度公式(∂ <em class="mk">损耗</em> / ∂ <strong class="lq ir"> W </strong>)从何而来。即使他们解释了，数学往往会变得很长，到处都是指数，因为问题的维度很高:你有一个样本数量的指数，一个层数的指数和一个每层神经元数量的指数。</p><p id="bba5" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">所以每次我想更新我过去学到的机器学习概念时，我总是很难在不看教科书的情况下写出反向传播背后的数学。虽然我明白它是如何工作的，但我发现这些公式非常不直观，有时甚至令人困惑…</p><p id="3930" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">所以这篇文章的想法是用一种优雅的方式演示反向传播公式，只使用<strong class="lq ir">的矢量化演算:所以没有索引<em class="mk"> i，j，k，… </em>并且根本没有求和∑！</strong> <em class="mk">(我们将使用的唯一索引是表示层。)</em></p><h1 id="fdea" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="98da" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">假设我们有以下神经网络(NN)架构</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/5de1a9566e17f31db2b4730bf3f7a3fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*COY_5e_QOt17hKnnMuJ9-A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">NN with 2 hidden layers and an output of 3 classes</figcaption></figure><p id="6c5b" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们的神经网络有两个隐藏层和一个三类输出层。我们将对所有层使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax 激活</a>。</p><p id="8884" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">训练这个模型意味着最小化我们的损失函数，即交叉熵(对数损失)。由于我们有 3 个班，所以日志损失为:𝜉( <strong class="lq ir"> Y </strong>、<strong class="lq ir"> X </strong>、<strong class="lq ir"> W₁ </strong>、<strong class="lq ir"> W₂ </strong>、<strong class="lq ir"> W₃ </strong> ) = <em class="mk">总和</em> [ <strong class="lq ir"> Y </strong> ○ <em class="mk">日志</em>(<strong class="lq ir">x₃</strong>)】<br/>，其中:<em class="mk">总和</em>为所有"○"也被称为<a class="ae kv" href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)" rel="noopener ugc nofollow" target="_blank">哈达玛产品</a>。</p><p id="6848" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir"> X₀ </strong>是一个 n×3 矩阵，n 是最小批量的大小。<br/>T3】w₀是一个 3×5 的矩阵。它具有从层 0 到层 1 的过渡的权重。<strong class="lq ir"> <br/> X₁ </strong>是一个 n×5 矩阵。它表示第一层的数据转换。<strong class="lq ir"> <br/> W₁ </strong>是一个 5×4 矩阵。它具有从第 1 层到第 2 层的过渡的权重。<strong class="lq ir"> <br/> X₂ </strong>是一个 n×4 矩阵。它表示第二层的数据转换。<strong class="lq ir"> <br/> W₂ </strong>是一个 4×3 矩阵。它具有从第 2 层到第 3 层过渡的权重。<strong class="lq ir"> <br/> X₃ </strong>是一个 n×3 矩阵。它表示第三层的数据转换。<strong class="lq ir"> <br/> Y </strong>是一个 n×3 矩阵，3 是类的个数。</p><p id="e019" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">基本上，本文的目标是演示下面的差分表达式:(★)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/7cb6418f8e945235085a75119c325572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTOxLra2sxKZS_McfMyucg.png"/></div></div></figure><p id="d4ef" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">带着<strong class="lq ir">𝜹₂</strong>=<strong class="lq ir">y</strong>-<strong class="lq ir">x₃<br/></strong>和<strong class="lq ir"> 𝜹₁ </strong> = ( <strong class="lq ir"> 𝜹₂。w₂</strong>ᐪ<strong class="lq ir"/>)<strong class="lq ir"/>○<strong class="lq ir">x₂</strong>○(<strong class="lq ir">1-x₂</strong>)<br/>和<strong class="lq ir"> 𝜹₀ </strong> = ( <strong class="lq ir"> 𝜹₁。w₁</strong>ᐪ<strong class="lq ir"/>)<strong class="lq ir"/>○<strong class="lq ir">x₁</strong>○(<strong class="lq ir">1-x₁</strong>)。<br/>和○为<a class="ae kv" href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)" rel="noopener ugc nofollow" target="_blank">哈达玛产品</a>。<br/>和<strong class="lq ir"> : </strong>为<a class="ae kv" href="https://en.wikipedia.org/wiki/Frobenius_inner_product" rel="noopener ugc nofollow" target="_blank"> Frobenius 产品</a></p><p id="c992" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这种差异特别有趣，因为它揭示了反向传播期间权重更新所需的所有梯度</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/a6200aeadb7a4a2af71cc39392235e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B3uly7JHYljxdgQQ-hFk9A.png"/></div></div></figure><p id="6e66" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">因此，在每一步<strong class="lq ir">中，Wᵢ </strong>更新如下:【<strong class="lq ir">wᵢ</strong>=<strong class="lq ir">wᵢ</strong>+𝛼*<strong class="lq ir">xᵢ</strong>ᐪ<strong class="lq ir">𝜹ᵢ</strong><strong class="lq ir"/>𝛼为学习率。</p><p id="ceab" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">如果你对如何找到(★)感兴趣，那么这篇文章就是为你准备的！</p><h1 id="021c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">关键结果需要证明</h1><h2 id="6599" class="mt kx iq bd ky mu mv dn lc mw mx dp lg lx my mz li mb na nb lk mf nc nd lm ne bi translated">1.与多项式回归的联系</h2><p id="c43e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">实际上，如果你取我们网络的最后两层，它基本上是一个多项逻辑回归，X₂作为输入数据，X₃作为输出预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/306beac3aa095e4f7c135148d3ffa2ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tgmf61ado68GdDovOXZb-A.png"/></div></div></figure><p id="778d" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">以<a class="ae kv" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> 𝜎为 softmax 函数</a>，我们可以写出<strong class="lq ir"> X₃ </strong> =𝜎( <strong class="lq ir"> X₂W₂ </strong>。<br/>用<strong class="lq ir"> Z₂=X₂W₂，</strong>我们可以写成<strong class="lq ir"> X₃ </strong> =𝜎( <strong class="lq ir"> Z₂ </strong>。</p><p id="782c" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这样损失可以写成:<strong class="lq ir"> 𝜉 </strong> = <em class="mk">总和</em> [ <strong class="lq ir"> Y </strong> ○ <em class="mk">日志</em>(𝜎(<strong class="lq ir">z₂</strong>)]</p><p id="c852" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">所以如果我们同意这确实是一个<a class="ae kv" href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression" rel="noopener ugc nofollow" target="_blank">多项逻辑回归</a>问题，那么我们就有以下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/55b2065100876baa6c7c1198a9f61eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u0SJ_7yhrqywHnKNKVH6fw.png"/></div></div></figure><p id="43b9" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">为了使这篇文章简短，我们不会深入研究如何计算这些梯度的细节。但是如果你对这个证明感兴趣，那么让我知道，我会试着写一篇关于它的文章。</p><p id="e24a" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">另一个要注意的有趣的事情是，由于我们对所有层使用 softmax 激活，那么我们可以推断:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/4b30119f95ae3e8cd090b559ea1a87a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B6_W7XnJt8EaRNmAuGr9Jg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">for i in 1,2,3</figcaption></figure><h2 id="5db2" class="mt kx iq bd ky mu mv dn lc mw mx dp lg lx my mz li mb na nb lk mf nc nd lm ne bi translated">2.矩阵的微分函数</h2><p id="7e99" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这一部分相当重要，因为它将为我们提供一种优雅的方式来揭示最终的表达(★)。</p><p id="9c34" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir"> f:矩阵- &gt;标量</strong></p><p id="c78c" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们假设 f 是一个函数，它取一个矩阵，输出一个标量。如果 f 是可微的，那么:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/14e15132a664b2f0b88243efeab129ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZkcKFpr6IyNFw8YgJPFJ0g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">with X and D having the same shape</figcaption></figure><p id="ceb6" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">因此，f 的微分写为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/944940a7fea0f7640c5d93f9d13b8245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6OdY0yw-h7Aq1Cckjr7RBQ.png"/></div></div></figure><p id="0abb" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">以“:”为<a class="ae kv" href="https://en.wikipedia.org/wiki/Frobenius_inner_product" rel="noopener ugc nofollow" target="_blank"> Frobenius 产品</a>[<strong class="lq ir">a:b</strong>=<em class="mk">trace</em>(<strong class="lq ir">a</strong>ᐪ<strong class="lq ir">b</strong>)]。</p><p id="5d49" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir"> f:标量- &gt;标量，按元素应用于矩阵</strong></p><p id="42f7" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">让我们假设 f 是一个简单的函数，它接受一个标量并输出一个标量。现在让我们用 F 来表示这个函数，它将 F 独立地应用于矩阵中的每个元素。如果 f 是可微的，那么:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/fde64f98b8ebbbb1a435656f7bef7907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UGEIPXqsBfy8QWqCYug2xg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">with dF, dX and D having the same shape.</figcaption></figure><p id="76f1" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">因此，f 的微分写为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/f5db614720eeff559e6a38d103d1a615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CUTQgPqEiOVVZy1aJmxTVg.png"/></div></div></figure><p id="a999" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">“○”是我们之前看到的<a class="ae kv" href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)" rel="noopener ugc nofollow" target="_blank"> hadamard 产品</a>。</p><h2 id="35ad" class="mt kx iq bd ky mu mv dn lc mw mx dp lg lx my mz li mb na nb lk mf nc nd lm ne bi translated">3.Hadamard 和 Frobenius 产品的性质</h2><p id="4c0d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">设 A、B 和 C 是三个形状相同的矩阵。</p><ul class=""><li id="c119" class="nm nn iq lq b lr ml lu mm lx no mb np mf nq mj nr ns nt nu bi translated"><strong class="lq ir">哈达玛产品的特性:</strong></li></ul><p id="2271" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">除了矩阵之外，它与简单的标量乘法具有基本相同的性质。</p><p id="036c" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">可交换的<strong class="lq ir"><em class="mk">A</em></strong>○<strong class="lq ir"><em class="mk">B = B</em></strong>○<strong class="lq ir"><em class="mk">A</em></strong><br/>联想的<strong class="lq ir"><em class="mk">A</em></strong>○<strong class="lq ir"><em class="mk">(B</em></strong>○<strong class="lq ir"><em class="mk">C)=(A</em>○<strong class="lq ir"><em class="mk">B)</em></strong>○<strong class="lq ir"><em class="mk">C = A【A</em></strong></strong></p><ul class=""><li id="0a5e" class="nm nn iq lq b lr ml lu mm lx no mb np mf nq mj nr ns nt nu bi translated"><strong class="lq ir">Frobenius 产品的特性:</strong></li></ul><p id="4ddc" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">分配式超额加法<strong class="lq ir"><em class="mk">A:(B+C)= A:B+A:C</em></strong></p><p id="d06c" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">下一个性质特别有趣，可以通过用 trace 公式替换“:”运算符来简单证明。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/4128bc5f87b597764851bbaed22017ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cy8MPwmbsIMsRktQNmNi6w.png"/></div></div></figure><p id="2dfc" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir">两个产品的联合性能:</strong></p><p id="9f24" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">运算顺序:<br/>“○”比“:”优先级高，两者都比矩阵乘法优先级低。”<strong class="lq ir">即:</strong> <strong class="lq ir">.”&gt;"○"&gt;":</strong><br/>由此，<strong class="lq ir"><em class="mk">A:B</em></strong>○<strong class="lq ir"><em class="mk">C . D = A:(B</em></strong>○<strong class="lq ir"><em class="mk">(C . D))</em></strong></p><p id="961f" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这两个运算符之间最重要的属性如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/d2023f4847abc28d436fd23b69a2480f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1GA1Zj_gqaeXac2xh1oYeg.png"/></div></div></figure><p id="f7d2" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">并且可以简单地通过用 trace 公式替换“:”操作符来演示。</p><h1 id="5067" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">反向传播公式的证明</h1><h2 id="5cdc" class="mt kx iq bd ky mu mv dn lc mw mx dp lg lx my mz li mb na nb lk mf nc nd lm ne bi translated">1.区分损失</h2><p id="0def" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">如果我们合并 1.a 和 2.a，我们有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/3fb7eb0c5034076d9b9094d07b76f75f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B5o4y1lQGSKp5RW1ck8gIQ.png"/></div></div></figure><p id="1d0b" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">由于<strong class="lq ir"> <em class="mk"> Z </em> ₂ </strong>是一个矩阵乘法，其区别如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/5b89e97a1e80be559f7e475cc199a3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5S5fe5eTx0IAqScb2fdhmA.png"/></div></div></figure><p id="3b79" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">因此</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/066db321d179f44a1a04b641e16708be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_cdEKqNSgQtPZjy1K0epg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">First 4 lines of the proof</figcaption></figure><p id="cbca" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们现在有𝜉对 W₂和 X₂的 4 行证明。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/fe5ae2185698a0cc12ec04da840c82e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lzz9Ux6n0tV87gvW8nyMqg.png"/></div></div></figure><h2 id="bc29" class="mt kx iq bd ky mu mv dn lc mw mx dp lg lx my mz li mb na nb lk mf nc nd lm ne bi translated">2.区分隐藏层</h2><p id="0b7a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在让我们通过最后一个隐藏层反向传播。如果我们把 1.b 和 2.b 结合起来，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/b1d1eff50a788b93f6935ee38168d23e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tSDp_kXvmtPiG5BtD9Cw3Q.png"/></div></div></figure><p id="2b6d" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">这样<strong class="lq ir">𝜹₂w₂</strong>ᐪ<strong class="lq ir">:</strong>d<strong class="lq ir">x₂</strong>就变成了</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/dae223c872be7173c660358c88766bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QBiO3gi6OUaX5Y0zLLOapg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Last 6 lines of the proof</figcaption></figure><p id="60a0" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">现在我们有了<strong class="lq ir">𝜹₂w₂</strong>ᐪ<strong class="lq ir">:</strong>d<strong class="lq ir">x₂，</strong>共 6 行证明的微分。</p><p id="76b5" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">把这个放回 d𝜉给了我们</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/8d0ab5ab477c22b9a8b23906270b27e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7rZU86o4zLgWmxjZyUph3A.png"/></div></div></figure><p id="abd8" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir">原来其实就是这样。剩下的就是对剩下的层重复这个步骤。</strong></p><p id="97db" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">通过对<strong class="lq ir">𝜹₁w₁</strong>ᐪ<strong class="lq ir">:</strong>d<strong class="lq ir">x₁</strong>所做的与我们对<strong class="lq ir">𝜹₂w₂</strong>ᐪ<strong class="lq ir">:</strong>d<strong class="lq ir">x₂</strong>所做的相同，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/79cbb3403eac493afc2f801026514166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_DMevMAtDVQ08WCpTdL7g.png"/></div></div></figure><h2 id="643c" class="mt kx iq bd ky mu mv dn lc mw mx dp lg lx my mz li mb na nb lk mf nc nd lm ne bi translated">3.把所有的放在一起</h2><p id="ce8d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">最后，把所有东西都放回 d𝜉，我们得到了最终的等式(★)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/11d03d4375ae55ff9220af8fc1d758d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nR4bKhch1Zs_u7tXr6nxdg.png"/></div></div></figure><p id="759d" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">带着<strong class="lq ir">𝜹₂</strong>=<strong class="lq ir">y</strong>—<strong class="lq ir">x₃<br/></strong>和<strong class="lq ir"> 𝜹₁ </strong> = ( <strong class="lq ir"> 𝜹₂。w₂</strong>ᐪ<strong class="lq ir"/>)<strong class="lq ir"/>○<strong class="lq ir">g₁</strong>=(<strong class="lq ir">𝜹₂。w₂</strong>ᐪ<strong class="lq ir"/>)<strong class="lq ir"/>○<strong class="lq ir">x₂</strong>○(<strong class="lq ir">1-x₂</strong>)<br/>和<strong class="lq ir"> 𝜹₀ </strong> = ( <strong class="lq ir"> 𝜹₁。w₁</strong>ᐪ<strong class="lq ir"/>)<strong class="lq ir"/>○g<strong class="lq ir">₀</strong>=(<strong class="lq ir">𝜹₁。w₁</strong>ᐪ<strong class="lq ir"/>)<strong class="lq ir"/>○<strong class="lq ir">x₁</strong>○(<strong class="lq ir">1-x₁</strong>)</p><p id="cb2b" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">Tada！🎉</p><h1 id="729f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="09d5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">就我个人而言，我发现这种证明更优雅，更容易从记忆中重现，而不需要看教科书。我希望你也是这样，我希望你喜欢这篇文章。</p><h2 id="6f20" class="mt kx iq bd ky mu mv dn lc mw mx dp lg lx my mz li mb na nb lk mf nc nd lm ne bi translated">参考</h2><div class="og oh gp gr oi oj"><a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">多项式逻辑回归</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">在统计学中，多项逻辑回归是一种分类方法，将逻辑回归推广到…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">en.wikipedia.org</p></div></div><div class="os l"><div class="ot l ou ov ow os ox kp oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">Softmax 函数</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">在数学中，softmax 函数，也称为 softargmax 或归一化指数函数，[2] :198 是一个…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">en.wikipedia.org</p></div></div><div class="os l"><div class="oy l ou ov ow os ox kp oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">哈达玛乘积(矩阵)</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">在数学中，哈达玛积(也称为舒尔积或熵积)是一种二元运算…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">en.wikipedia.org</p></div></div><div class="os l"><div class="oz l ou ov ow os ox kp oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://en.wikipedia.org/wiki/Frobenius_inner_product" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">Frobenius 内积</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">在数学中，Frobenius 内积是一种二元运算，它取两个矩阵并返回一个数。这是…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">en.wikipedia.org</p></div></div></div></a></div></div></div>    
</body>
</html>