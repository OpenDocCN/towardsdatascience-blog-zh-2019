<html>
<head>
<title>Support Vector Machine — Formulation and Derivation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机——公式和推导</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machine-formulation-and-derivation-b146ce89f28?source=collection_archive---------7-----------------------#2019-09-24">https://towardsdatascience.com/support-vector-machine-formulation-and-derivation-b146ce89f28?source=collection_archive---------7-----------------------#2019-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/28b85a811378f3101ca5c72a6f144a6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V_iGqc5OCADAc9jkwgIM4w.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@andyjh07?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Andy Holmes</a> on <a class="ae kf" href="https://unsplash.com/s/photos/solar-system?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="kg"><p id="ad42" class="kh ki it bd kj kk kl km kn ko kp kq dk translated">预测机器学习中的定性反应称为<em class="kr">分类</em>。</p></blockquote><p id="9551" class="pw-post-body-paragraph ks kt it ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo kq im bi translated">SVM 或支持向量机是最大限度地提高利润率的分类器。在下面的例子中，分类器的目标是找到一条线或(n-1)维超平面，它将 n 维空间中的两个类分开。</p><p id="6a53" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">在下面给出的例子中，我们看到任何学习算法都会给出所提到的任何给定的行，但是什么可能是最好的行呢？</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lu"><img src="../Images/7e382dea68de09b94704a0d16dbf8090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d1Pycgqeeba1mJ0-faPOEQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">2 dimensional representation of binary classes</figcaption></figure><p id="8878" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">直观上，绿线看起来是最佳解决方案，因为它可能会对未来的测试数据集做出更好的预测。我们通过引入一个称为边缘的参数，即在没有任何训练样本的情况下，决策边界/分类器周围的带宽，来形式化分类器的<em class="lz">优度</em>的概念。</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/28664308b730f3947af474642e43ee5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*fYXRn8T4u-BTGxFelpC2yg.gif"/></div></figure><p id="672a" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">因此，目标是找到具有最大余量的决策边界。我们可以将边缘视为训练区域周围的区域，决策边界不能绕过该区域。随着半径的增加，可行域减小，它收敛到一条直线。</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/cf4f99c143bd9436086e765469eead3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*P0_fZcJ0okbwW2Pc-nk59g.gif"/></div></figure><blockquote class="kg"><p id="1b80" class="kh ki it bd kj kk kl km kn ko kp kq dk translated">注意:只有少数带有气泡的训练样本触及了决策边界。这些样本本质上被称为支持向量。</p></blockquote><figure class="mc md me mf mg ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mb"><img src="../Images/0b7d77b8d22e8ab1a92bc9364e0248ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z5sMm7_3zNl7KJC3SBdOmA.png"/></div></div></figure><h1 id="9804" class="mh mi it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated"><strong class="ak">打破维度诅咒</strong></h1><p id="0373" class="pw-post-body-paragraph ks kt it ku b kv nf kx ky kz ng lb lc ld nh lf lg lh ni lj lk ll nj ln lo kq im bi translated">1970 年，数学家<a class="ae kf" href="http://mathworld.wolfram.com/Vapnik-ChervonenkisDimension.html" rel="noopener ugc nofollow" target="_blank"> Vapnik 和 Chervonenkis </a>给出了 VC 维的概念，他们将未来测试误差(R(α))估计为训练误差和 VC 维的某个函数(单调递增函数)。</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/288fd891e273b3c75f5aa9d8c6b60dcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*UfHDEs2RDoHSl4tfnJrwdw.png"/></div></figure><p id="d1ca" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">VC 维数 h 被写成相对裕度平方的倒数和数据维数的最小值。因此，如果我们可以最大化相对裕度，我们将最小化它的平方反比，如果它低于数据的维度，h 将变得独立于维度。</p><blockquote class="nl nm nn"><p id="7890" class="ks kt lz ku b kv lp kx ky kz lq lb lc no lr lf lg np ls lj lk nq lt ln lo kq im bi translated">注意:相对裕度就是裕度除以包含所有训练点的圆的直径。</p></blockquote><h1 id="ecf0" class="mh mi it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated"><strong class="ak">SVM 的提法</strong></h1><div class="lv lw lx ly gt ab cb"><figure class="nr ju ns nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/4f3f23518fa7e5412e9a8374548e13f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*v0OUUim9Ur14Qsb904cMDQ.png"/></div></figure><figure class="nr ju nx nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/2e8bc3cb199b6102dc77cd08dd635a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*B8zpyNKq0GT_RGQpXQMEVg.png"/></div></figure></div><p id="ff32" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">我们使用方法 2 并将问题公式化为:</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d5fe62613598c85937f00cd2f6e3f0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*o1vjGJlmHI8KSJnZ7o8pMQ.png"/></div></figure><p id="71c5" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">对<a class="ae kf" href="https://en.wikipedia.org/wiki/Lagrange_multiplier" rel="noopener ugc nofollow" target="_blank">拉格朗日形式</a>中的常数进行积分，我们得到:</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nz"><img src="../Images/0d45c2628a2a61460934be96675abd33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jdRDCPXEnri4fJjTca3JGA.png"/></div></div></figure><p id="dc2a" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">拉格朗日乘数法规定，如前所述，对于 w 和 b，J 最小化，但是对于<strong class="ku iu"> α，J 必须<strong class="ku iu">最大化</strong>。</strong>J 代表的点称为<a class="ae kf" href="https://en.wikipedia.org/wiki/Saddle_point" rel="noopener ugc nofollow" target="_blank">鞍点</a>。</p><p id="12e2" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">函数 J 目前以它的<a class="ae kf" href="https://en.wikipedia.org/wiki/Duality_%28optimization%29" rel="noopener ugc nofollow" target="_blank">原始形式</a>表示，我们可以把它转换成它的对偶形式来求解。</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/2590ef869acebb0a4967bab1c1be0f10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HuB4SUa012SQo4dJiKNa4Q.png"/></div></div></figure><p id="90af" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">同样，从拉格朗日乘子的<a class="ae kf" href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions" rel="noopener ugc nofollow" target="_blank"> KKT 条件</a>中，我们可以说 J 函数中对应于拉格朗日乘子的所有项在最优时应该趋向于 0。</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/bf8c514648653dd6094fde4927cc4baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*KcMSC0EWhIf2HSYYwQ4qFA.png"/></div></figure><p id="604d" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">这意味着非零拉格朗日系数对应于支持向量数据点。利用上述等式，我们可以将 J 写成:</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/849beb14bb1ecabad8c331a95ec0dc7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gC6UiB4xeb83KjXjdsPagQ.png"/></div></div></figure><p id="f32a" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">Q(α)代表<strong class="ku iu"> <em class="lz">对偶形式</em> </strong> J 它只依赖于α，其余都是已知的标量。我们可以用任何 QP 优化来求解 Q(α)，这超出了本文的范围。在得到α之后，我们得到 w，从这点出发，任何一个支持向量都可以根据 KKT 条件得到 b。</p><h1 id="146f" class="mh mi it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">非线性可分离数据</h1><p id="937d" class="pw-post-body-paragraph ks kt it ku b kv nf kx ky kz ng lb lc ld nh lf lg lh ni lj lk ll nj ln lo kq im bi translated">我们研究了数据是线性可分的情况。现在，我们来看看数据可能不线性分离的情况，原因是</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/7d07ecf8988aaa765f7b496d6cd9fb2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tl3dQaEConfFoTVM0amXHA.png"/></div></div></figure><p id="22fc" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated"><strong class="ku iu">噪声数据</strong></p><p id="db6e" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">对于有噪声的数据，我们在估计/优化中引入一个训练误差参数。我们引入一个松弛变量，并增加额外的条件如下</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/76a66643a38ba7f13c06feadd2b4556a.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*MpajTvjdPFy_4gZ5Xr4meQ.png"/></div></figure><p id="4764" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">用拉格朗日系数重复同样的过程，我们得到</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div class="gh gi of"><img src="../Images/219b77a7c4206656af084d232a0cf346.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*aqAUF1-cIDhT6c-Dhx4p_A.png"/></div></figure><p id="53b7" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">唯一的区别是现在拉格朗日系数有了限制。参数 C 控制训练误差和 VC 维数之间的相对权重。</p><h1 id="fcd9" class="mh mi it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">非线性边界</h1><p id="fd81" class="pw-post-body-paragraph ks kt it ku b kv nf kx ky kz ng lb lc ld nh lf lg lh ni lj lk ll nj ln lo kq im bi translated">任何具有非线性边界的数据集，如果投影到更高维度，理论上都是线性可分的。</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/d3c0e46657ad5b9bf2b586a3892ab057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6WQ0sYegd3rLTvIINFlLKA.png"/></div></div></figure><p id="3ebc" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">因此 Q(α)可以写成:</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/8d2b8a1a81d2e7a116be06da4820e416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qvuzoVZqpA8pGQ4mAbQdeA.png"/></div></div></figure><p id="276a" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">我们可以将 w 和其他测试相位方程写成:</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/daed97af6032c6ca0a2280239277c350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uUX5Ftn533YQJC79a6FxgA.png"/></div></div></figure><h2 id="b75a" class="oj mi it bd mj ok ol dn mn om on dp mr ld oo op mv lh oq or mz ll os ot nd ou bi translated">内核技巧</h2><p id="4074" class="pw-post-body-paragraph ks kt it ku b kv nf kx ky kz ng lb lc ld nh lf lg lh ni lj lk ll nj ln lo kq im bi translated">我们可以看到，在训练和测试中，映射都以点积的形式出现。由于我们不知道映射，我们可以找到一个函数<strong class="ku iu"> K(x，y) </strong>它等价于映射的点积；我们可以避免显式映射到更高维度。</p><p id="7ffa" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">让我们举一个二次核的例子来更好地理解。</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ov"><img src="../Images/5fba0a912ad5bdf86e0ac43509401218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9qtSu8G4rE-hfwTyWwWvMQ.png"/></div></div></figure><p id="7696" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">我们看到，与映射然后相乘相比，核函数的计算复杂度更低。<br/>这个可以扩展到 n 维内核。因此，n 维映射/核可以表示为</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/9fa93ee93480ca1570bc578766c525c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*R1I7MilIpmQYKvlpCS6I7A.png"/></div></figure><blockquote class="nl nm nn"><p id="a726" class="ks kt lz ku b kv lp kx ky kz lq lb lc no lr lf lg np ls lj lk nq lt ln lo kq im bi translated">注意:添加两个有效内核也给了我们一个内核。这很容易证明。</p></blockquote><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ox"><img src="../Images/d6051bf8ae0a651aa6cfcfa89c35a837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W0f-y0htRkrrol8TSB1txA.png"/></div></div></figure><p id="95ab" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">因此，为了映射到极高的维度，我们可以将内核计算为:</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oy"><img src="../Images/c3c7a488557d0de17c84cbbca1a17d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zn4HZgBdGBW4GtgUzpxvKw.png"/></div></div></figure><h1 id="7229" class="mh mi it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">无限维度</h1><p id="fc86" class="pw-post-body-paragraph ks kt it ku b kv nf kx ky kz ng lb lc ld nh lf lg lh ni lj lk ll nj ln lo kq im bi translated">理论上，如果映射到无限维超平面，数据集将是线性可分的。因此，如果我们能找到一个能给出无限超平面映射乘积的核，我们的工作就完成了。</p><p id="1885" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">这里出现了<a class="ae kf" href="https://en.wikipedia.org/wiki/Mercer%27s_theorem" rel="noopener ugc nofollow" target="_blank"> Mercer 定理</a>，它陈述了当且仅当 K(X，Y)是对称的、连续的和正半定的(那么 Mercer 的条件)，它可以表示为</p><figure class="lv lw lx ly gt ju gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/a10b2017e7b25fdd21248b8f5bd9024f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*SPib3VkSXPSTJJqf8IbLUg.png"/></div></figure><p id="d91b" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">意味着高维映射的线性组合的存在是有保证的。因此，现在我们只需检查函数是否满足 Mercer 条件，就可以得到无限维的映射。</p><p id="67b3" class="pw-post-body-paragraph ks kt it ku b kv lp kx ky kz lq lb lc ld lr lf lg lh ls lj lk ll lt ln lo kq im bi translated">我以此结束我关于 SVM 的博客，这是我用过的最好的分类器之一，观看这个空间的更多内容。</p></div></div>    
</body>
</html>