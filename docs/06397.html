<html>
<head>
<title>The Ultimate Beginner’s Guide to Data Scraping, Cleaning, and Visualization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据搜集、清理和可视化初学者终极指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ultimate-beginners-guide-to-scraping-and-cleaning-twitter-data-a64e4aaa9343?source=collection_archive---------2-----------------------#2019-09-14">https://towardsdatascience.com/ultimate-beginners-guide-to-scraping-and-cleaning-twitter-data-a64e4aaa9343?source=collection_archive---------2-----------------------#2019-09-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="591c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何简单地通过清理和预处理数据，让您的模型从不起眼变得令人惊叹</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7573ce26f5ffb9d6a056fd8cae831260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZoMoOA3vqXtH2Imd11v9A.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://www.pexels.com/@tasveerwala?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Nitin Sharma </a>from <a class="ae ky" href="https://www.pexels.com/photo/grey-and-white-monkeys-sitting-near-tree-2861847/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="ebb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有一个结果可以接受但并不惊人的模型，看看你的数据吧！花时间以正确的方式清理和预处理您的数据可以让您的模型成为明星。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/651777a6797498a56d04040f1611710e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nWzs22HHAw0yEC5E1kUYQg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://www.pexels.com/@burst?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Burst </a>from <a class="ae ky" href="https://www.pexels.com/photo/adult-tan-and-white-french-bulldog-545063/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="9860" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更详细地了解搜集和预处理，让我们来看看“你就是你发的微博:通过 Twitter 使用检测社交媒体中的抑郁症”中的一些工作这样，我们可以真正检查抓取推文，然后清理和预处理推文的过程。我们还将做一些探索性的可视化，这是一种更好地了解您的数据的方式！我们将在这里做一些最基本的清理和预处理工作:当你建立你的模型时，真正把这些 Tweets 按顺序整理好是你自己的事！</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/you-are-what-you-tweet-7e23fb84f4ed"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">你在推特上说什么就是什么</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">通过 Twitter 使用检测社交媒体中的抑郁症</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ks lz"/></div></div></a></div><h1 id="82dd" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">一点背景</h1><p id="1344" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">超过 3 亿人患有抑郁症，只有一小部分人接受了适当的治疗。抑郁症是全球残疾的主要原因，每年有近 80 万人死于自杀。自杀是 15-29 岁人群的第二大死因。抑郁症的诊断(和随后的治疗)经常被延迟、不精确和/或完全错过。</p><p id="e539" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不一定要这样！社交媒体为转变早期抑郁症干预服务提供了前所未有的机会，特别是在年轻人中。</p><p id="170a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每秒钟，Twitter 上大约有 6000 条推文，相当于每分钟发送 35 万条推文，每天 5 亿条推文，每年约 2000 亿条推文。皮尤研究中心指出，目前，72%的公众使用某种类型的社交媒体。该项目捕捉并分析与抑郁症状的发作和持续相关的语言标记，以建立一种可以有效预测抑郁的算法。通过建立一种可以分析表现出自我评估抑郁特征的推文的算法，个人、父母、护理人员和医疗专业人员将有可能分析社交媒体帖子，以寻找标志着精神健康恶化的语言线索，这远远早于传统方法。分析社交媒体帖子中的语言标记可以进行低调的评估，这种评估可以补充传统服务，并且可以比传统方法更早地意识到抑郁迹象。</p><h1 id="98ad" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">我们从哪里开始？</h1><p id="1e97" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们需要数据！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/dade780cd8706d483fd408043be4dab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lJ6W1hxaVs-jwk0UmVPo3Q.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://www.pexels.com/@quang-nguyen-vinh-222549?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Quang Nguyen Vinh </a>from <a class="ae ky" href="https://www.pexels.com/photo/black-and-tan-smooth-chihuahua-in-blue-and-white-plastic-basket-2135383/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><h1 id="0bae" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">收集数据</h1><p id="1885" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">为了建立一个抑郁症检测器，需要两种推文:不一定表明抑郁症的随机推文和表明用户可能患有抑郁症和/或抑郁症状的推文。随机推文的数据集可以来源于 Kaggle 上可用的<a class="ae ky" href="https://www.kaggle.com/kazanova/sentiment140" rel="noopener ugc nofollow" target="_blank">sensition 140 数据集，但对于这种二进制分类模型，这种利用 sensition 140 数据集</a>并提供一组二进制标签的<a class="ae ky" href="https://www.kaggle.com/ywang311/twitter-sentiment/data" rel="noopener ugc nofollow" target="_blank">数据集被证明是建立稳健模型的最有效方法。没有公开可用的表明抑郁的推文数据集，所以使用 Twitter 抓取工具 TWINT 检索“抑郁”推文。手动检查抓取的推文的相关性(例如，推文表明情绪而非经济或气氛抑郁)，并清理和处理推文。通过搜索与抑郁症特别相关的术语，特别是 De Choudhury 等人在 unigram 中确定的词汇术语，来收集推文。艾尔。</a></p><p id="30b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/twintproject/twint" rel="noopener ugc nofollow" target="_blank"> TWINT </a>是一款非常简单易用的工具！</p><p id="51cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用以下命令从命令行下载它:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="dfdc" class="nr mp it nn b gy ns nt l nu nv">pip install twint</span></pre><p id="ef21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果您想要搜索 2019 年 7 月 20 日的术语“抑郁症”，并将数据存储为名为“抑郁症”的新 csv，您可以运行类似以下的命令:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="fea6" class="nr mp it nn b gy ns nt l nu nv">twint -s "depression" --since 2019-07-20 -o depression —csv</span></pre><p id="bde3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦你收集了推文，你就可以开始清理和预处理它们。您可能会得到大量不需要的信息，比如对话 id 等等。您可以决定创建多个想要合并的 CSV。我们会谈到这一切的！</p><h1 id="ae71" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated"><strong class="ak">模特表现如何？</strong></h1><p id="198d" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">一开始？没那么令人印象深刻。在对数据进行基本的清理和预处理之后，最好的结果(即使在花费时间对模型进行微调之后)徘徊在 80%左右。</p><p id="4b8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我检查了词频和二元模型后，这个原因真的很有意义。探索您的数据！当我看到单词本身时，我意识到以正确的方式清理和准备数据集需要大量的工作，并且这样做是绝对必要的。部分清洁过程必须手动完成，所以不要害怕进去弄脏你的手。这需要时间，但是值得！</p><p id="efe3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后呢？使用逻辑回归对该模型的准确性进行评估，并与二元分类基线模型进行比较。分析模型的准确性，并运行分类报告以确定精确度和召回分数。数据被分成训练集、测试集和验证集，模型的准确性是根据模型对测试数据的性能来确定的，测试数据是分开保存的。虽然使用相同的数据、学习率和时期，基准逻辑回归模型的性能为 64.32%，但 LSTM 模型的性能明显更好，为 97.21%。</p><p id="7c9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，我们是如何从抓取的推文得到结果的呢？</p><p id="248d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">练习，练习，练习！(还有一些正经工作。)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/96c4a9027a8d278167890a593d352c9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c85P3tbdIr3dmmsB0fAIZA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://www.pexels.com/@dsd-143941?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">DSD </a>from <a class="ae ky" href="https://www.pexels.com/photo/close-up-photo-of-monkey-on-tree-branch-1829979/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="48d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(如果你是数据科学、机器学习和人工智能的新手，你可能想看看 NumPy 的终极初学者指南！)</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/the-ultimate-beginners-guide-to-numpy-f5a2f99aef54"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">NumPy 初学者终极指南</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">开始使用 NumPy 需要知道的一切</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="nx l mk ml mm mi mn ks lz"/></div></div></a></div><h1 id="526c" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">基本清洁和预处理</h1><p id="c845" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">比方说，我们在 Twitter 上搜索“抑郁”、“沮丧”、“无望”、“孤独”、“自杀”和“抗抑郁”等搜索词，并将这些搜索到的推文文件保存为文件“tweets.csv”中的“抑郁”等。</p><p id="0d46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将从几个进口开始</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="be45" class="nr mp it nn b gy ns nt l nu nv"><strong class="nn iu">import</strong> <strong class="nn iu">pandas</strong> <strong class="nn iu">as</strong> <strong class="nn iu">pd</strong><br/><strong class="nn iu">import</strong> <strong class="nn iu">numpy</strong> <strong class="nn iu">as</strong> <strong class="nn iu">np</strong><br/><br/><strong class="nn iu">import</strong> <strong class="nn iu">pandas</strong> <strong class="nn iu">as</strong> <strong class="nn iu">pd</strong>  <br/><strong class="nn iu">import</strong> <strong class="nn iu">numpy</strong> <strong class="nn iu">as</strong> <strong class="nn iu">np</strong><br/><strong class="nn iu">import</strong> <strong class="nn iu">matplotlib.pyplot</strong> <strong class="nn iu">as</strong> <strong class="nn iu">plt</strong><br/>plt.style.use('fivethirtyeight')<br/><br/>%matplotlib inline<br/>%config InlineBackend.figure_format = 'retina'<br/><strong class="nn iu">import</strong> <strong class="nn iu">re</strong><br/><strong class="nn iu">from</strong> <strong class="nn iu">nltk.tokenize</strong> <strong class="nn iu">import</strong> WordPunctTokenizer<br/>tok = WordPunctTokenizer()</span></pre><p id="98f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将阅读我们的一个 CSV 文件，并看看头部。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="3853" class="nr mp it nn b gy ns nt l nu nv">hopeless_tweets_df = pd.read_csv('hopeless/tweets.csv')<br/>hopeless_tweets_df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/6cda9fa33846b20ae5c96069b3213464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rlQKPvFCmlzKDSb55QwnOw.png"/></div></div></figure><p id="1874" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们应该去掉数据集中存储的任何不必要的信息。对于这个项目，我们不需要名称、id、对话 id、地理位置等等。我们可以通过以下方式将它们取出:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="170c" class="nr mp it nn b gy ns nt l nu nv">hopeless_tweets_df.drop(['date', 'timezone', 'username', 'name', 'conversation_id', 'created_at', 'user_id', 'place', 'likes_count', 'link', 'retweet', 'quote_url', 'video', 'user_rt_id', 'near', 'geo', 'mentions', 'urls', 'photos', 'replies_count', 'retweets_count'], axis = 1, inplace = <strong class="nn iu">True</strong>)</span></pre><p id="f8fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在有了这个，就好处理多了！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/51d5f477c1c703cfbae61aeb9ea313b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fpTvVyI02_vjUdjCJnK33w.png"/></div></div></figure><p id="4f9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，只需对您使用搜索词创建的所有 CSV 进行上述操作，我们就可以将我们单独的数据集合并为一个！</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="58fa" class="nr mp it nn b gy ns nt l nu nv">df_row_reindex = pd.concat([depression_tweets_df, hopeless_tweets_df, lonely_tweets_df, antidepressant_tweets_df, antidepressants_tweets_df, suicide_tweets_df], ignore_index=<strong class="nn iu">True</strong>)<br/><br/>df_row_reindex</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/cba7397910c92c7b6509bf6017c5f40f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Lwgfpu7JJ5jRoW9JeydrQ.png"/></div></div></figure><p id="dc73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们继续之前，让我们放弃重复</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="4977" class="nr mp it nn b gy ns nt l nu nv">depressive_twint_tweets_df = df.drop_duplicates()</span></pre><p id="07eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并将我们的数据集保存为新的 CSV 格式！</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="c125" class="nr mp it nn b gy ns nt l nu nv">export_csv = depressive_twint_tweets_df.to_csv(r'depressive_unigram_tweets_final.csv')</span></pre><h1 id="01f7" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated"><strong class="ak">更高级的预处理</strong></h1><p id="1fb5" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">在模型中使用数据之前，有必要展开缩写、删除链接、标签、大写和标点符号。需要处理否定。这意味着创建一个否定字典，这样被否定的单词就可以被有效地处理。链接和网址需要删除连同空白。此外，需要删除标准 NLTK 停用词以外的停用词，以使模型更加健壮。这些单词包括一周中的日子及其缩写、月份名称，以及单词“Twitter”，令人惊讶的是，当单词 clouds 被创建时，它作为一个突出的单词出现。然后将推文标记化，并使用 PorterStemmer 来阻止推文。</p><p id="906b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们去掉所有对我们没有帮助的东西！</p><p id="9abe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然是进口</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="4f25" class="nr mp it nn b gy ns nt l nu nv"><strong class="nn iu">import</strong> <strong class="nn iu">pandas</strong> <strong class="nn iu">as</strong> <strong class="nn iu">pd</strong><br/><strong class="nn iu">import</strong> <strong class="nn iu">numpy</strong> <strong class="nn iu">as</strong> <strong class="nn iu">np</strong><br/><strong class="nn iu">import</strong> <strong class="nn iu">matplotlib.pyplot</strong> <strong class="nn iu">as</strong> <strong class="nn iu">plt</strong><br/><strong class="nn iu">import</strong> <strong class="nn iu">seaborn</strong> <strong class="nn iu">as</strong> <strong class="nn iu">sns</strong><br/><strong class="nn iu">import</strong> <strong class="nn iu">itertools</strong><br/><strong class="nn iu">import</strong> <strong class="nn iu">collections<br/>import</strong> <strong class="nn iu">re<br/>import</strong> <strong class="nn iu">networkx</strong> <strong class="nn iu">as</strong> <strong class="nn iu">nx</strong></span><span id="4398" class="nr mp it nn b gy nz nt l nu nv"><strong class="nn iu">import</strong> <strong class="nn iu">nltk<br/></strong>nltk.download(['punkt','stopwords'])<br/><strong class="nn iu">from</strong> <strong class="nn iu">nltk.corpus</strong> <strong class="nn iu">import</strong> stopwords<br/>stopwords = stopwords.words('english')<br/><strong class="nn iu">from</strong> <strong class="nn iu">nltk.corpus</strong> <strong class="nn iu">import</strong> stopwords<br/><strong class="nn iu">from</strong> <strong class="nn iu">nltk</strong> <strong class="nn iu">import</strong> bigrams<br/><br/><strong class="nn iu">import</strong> <strong class="nn iu">warnings</strong><br/>warnings.filterwarnings("ignore")<br/><br/>sns.set(font_scale=1.5)<br/>sns.set_style("whitegrid")<br/><strong class="nn iu">from</strong> <strong class="nn iu">vaderSentiment.vaderSentiment</strong> <strong class="nn iu">import</strong> SentimentIntensityAnalyzer<br/>analyzer = SentimentIntensityAnalyzer()%matplotlib inline<br/>%config InlineBackend.figure_format = 'retina'</span></pre><p id="6b71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">阅读您的新 CSV</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="c425" class="nr mp it nn b gy ns nt l nu nv">pd.read_csv('depressive_unigram_tweets_final.csv')</span></pre><p id="844e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">把它变成熊猫的数据框</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="e30c" class="nr mp it nn b gy ns nt l nu nv">df2 = pd.read_csv('depressive_unigram_tweets_final.csv')</span></pre><p id="5ce4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们看看是否有空值。我们来清理一下吧！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/4db9b667019981abedd60532f5c107fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SdKtF_D680jN8WMZWBKrUw.png"/></div></div></figure><p id="aadf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将快速删除推文中的停用词</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="4dec" class="nr mp it nn b gy ns nt l nu nv">df_new['clean_tweet'] = df_new['tweet'].apply(<strong class="nn iu">lambda</strong> x: ' '.join([item <strong class="nn iu">for</strong> item <strong class="nn iu">in</strong> x.split() <strong class="nn iu">if</strong> item <strong class="nn iu">not</strong> <strong class="nn iu">in</strong> stopwords]))</span></pre><p id="f457" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你愿意，你可以分析这些推文来获得 VADER 情绪分析分数！</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="2e63" class="nr mp it nn b gy ns nt l nu nv">df_new['vader_score'] = df_new['clean_tweet'].apply(<strong class="nn iu">lambda</strong> x: analyzer.polarity_scores(x)['compound'])</span></pre><p id="3559" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在那里，您还可以创建标签。对于二进制分类模型，您可能需要二进制标签系统。但是，要注意你的数据！情绪得分本身并不表明抑郁，假设负得分表明抑郁也太简单了。事实上，快感缺失是抑郁症极其常见的症状。中性或平淡的推文，如果不是更有可能，至少也是抑郁症的一个指标，不应被忽视。</p><p id="f6cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于实验的目的，您可能想要设置一个这样的情感分析标签。请随意使用它！</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="cbd0" class="nr mp it nn b gy ns nt l nu nv">positive_num = len(df_new[df_new['vader_score'] &gt;=0.05]) negative_num = len(df_new[df_new['vader_score']&lt;0.05])</span><span id="c192" class="nr mp it nn b gy nz nt l nu nv">df_new['vader_sentiment_label']= df_new['vader_score'].map(<strong class="nn iu">lambda</strong> x:int(1) <strong class="nn iu">if</strong> x&gt;=0.05 <strong class="nn iu">else</strong> int(0))</span></pre><p id="62ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你需要，扔掉你不需要的东西</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="e0b9" class="nr mp it nn b gy ns nt l nu nv">df_new = df_new[['Unnamed: 0', 'vader_sentiment_label', 'vader_score', 'clean_tweet']]</span><span id="8e00" class="nr mp it nn b gy nz nt l nu nv">df_new.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/c991bc57f081857c25df5bacc2fb4386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7UtBi8Q0qa3lJf-qgAaRQ.png"/></div></div></figure><p id="2f2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">继续保存一个 csv！</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="8e23" class="nr mp it nn b gy ns nt l nu nv">df_new.to_csv('vader_processed_final.csv')</span></pre><p id="e9a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们继续玩吧！</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="1170" class="nr mp it nn b gy ns nt l nu nv">df_new['text'] = df_new['clean_tweet']<br/>df_new['text']</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/486b62db60851fcbc28333ce24d82a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SNuIYjA-b8ptmwdlx9sWNg.png"/></div></div></figure><p id="850c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以删除网址</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="95bd" class="nr mp it nn b gy ns nt l nu nv"><strong class="nn iu">def</strong> remove_url(txt):<br/>    <strong class="nn iu">return</strong> " ".join(re.sub("([^0-9A-Za-z <strong class="nn iu">\t</strong>])|(\w+:\/\/\S+)", "", txt).split())</span><span id="57c8" class="nr mp it nn b gy nz nt l nu nv">all_tweets_no_urls = [remove_url(tweet) <strong class="nn iu">for</strong> tweet <strong class="nn iu">in</strong> df_new['text']]<br/>all_tweets_no_urls[:5]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/bc50fe9ce25f9bbbafbffc8727571160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Ny3bbtlQbogiX9jj_epJg.png"/></div></div></figure><p id="b08c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们把所有的东西都变成小写，把推文分开。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="680b" class="nr mp it nn b gy ns nt l nu nv"><em class="oa">#lower_case = [word.lower() for word in df_new['text']]</em><br/>sentences = df_new['text']</span><span id="bd6e" class="nr mp it nn b gy nz nt l nu nv">all_tweets_no_urls[0].split()</span><span id="b446" class="nr mp it nn b gy nz nt l nu nv">words_in_tweet = [tweet.lower().split() <strong class="nn iu">for</strong> tweet <strong class="nn iu">in</strong> all_tweets_no_urls]<br/>words_in_tweet[:2]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/01a1b4d1c605e342a63c1096de2c261b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFYb9KQ-ruNM4WKxx7doFQ.png"/></div></div></figure><h1 id="8daa" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">手工清洗</h1><p id="aa87" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">这不好玩，也不漂亮，但是手工清洁是至关重要的。这花了几个小时，但是去掉了对热带低气压和经济低气压的引用改进了模型。删除电影标题的推文改进了模型(你可以在下面的二元图中看到“X 特遣队”)。删除包含搜索术语的引用新闻标题改进了模型。感觉要花很长时间才能完成，但是这一步对模型的健壮性产生了巨大的影响。</p><h1 id="27c4" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">探索性可视化和分析</h1><p id="51d2" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">现在我们来看性格和词频！</p><p id="d816" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分析数据集中最常见的单词相当容易。去掉停用词后，很明显有些词比其他词出现得更频繁。</p><p id="1257" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来统计一下我们最常用的单词吧！</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="f873" class="nr mp it nn b gy ns nt l nu nv"><em class="oa"># List of all words</em><br/>all_words_no_urls = list(itertools.chain(*words_in_tweet))<br/><br/><em class="oa"># Create counter</em><br/>counts_no_urls = collections.Counter(all_words_no_urls)<br/><br/>counts_no_urls.most_common(15)</span></pre><p id="3d96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并把它们转换成数据帧。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="8189" class="nr mp it nn b gy ns nt l nu nv">clean_tweets_no_urls = pd.DataFrame(counts_no_urls.most_common(15),<br/>                             columns=['words', 'count'])<br/><br/>clean_tweets_no_urls.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/0ab9d9d3f85101b9df68f293a9404e5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByEy9b_gz852j1s9tcZ0rQ.png"/></div></div></figure><p id="232e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗯。停用词太多。让我们来处理这些。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="4309" class="nr mp it nn b gy ns nt l nu nv">stop_words = set(stopwords.words('english'))</span><span id="020d" class="nr mp it nn b gy nz nt l nu nv"><em class="oa"># Remove stop words from each tweet list of words</em><br/>tweets_nsw = [[word <strong class="nn iu">for</strong> word <strong class="nn iu">in</strong> tweet_words <strong class="nn iu">if</strong> <strong class="nn iu">not</strong> word <strong class="nn iu">in</strong> stop_words]<br/>              <strong class="nn iu">for</strong> tweet_words <strong class="nn iu">in</strong> words_in_tweet]<br/><br/>tweets_nsw[0]</span></pre><p id="eb02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们再看一看。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="2698" class="nr mp it nn b gy ns nt l nu nv">all_words_nsw = list(itertools.chain(*tweets_nsw))  counts_nsw = collections.Counter(all_words_nsw)  counts_nsw.most_common(15)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/9dba4ad2c6379929817d13fbc9b945f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNod3L09ZuPZpIX9911cRA.png"/></div></div></figure><p id="5491" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好些了，但还不够好。这些单词中的一些并没有告诉我们太多。再做几个调整吧。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="a8c4" class="nr mp it nn b gy ns nt l nu nv">collection_words = ['im', 'de', 'like', 'one']<br/>tweets_nsw_nc = [[w <strong class="nn iu">for</strong> w <strong class="nn iu">in</strong> word <strong class="nn iu">if</strong> <strong class="nn iu">not</strong> w <strong class="nn iu">in</strong> collection_words]<br/>                 <strong class="nn iu">for</strong> word <strong class="nn iu">in</strong> tweets_nsw]</span></pre><p id="506d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="6d05" class="nr mp it nn b gy ns nt l nu nv"><em class="oa"># Flatten list of words in clean tweets</em><br/>all_words_nsw_nc = list(itertools.chain(*tweets_nsw_nc))<br/><br/><em class="oa"># Create counter of words in clean tweets</em><br/>counts_nsw_nc = collections.Counter(all_words_nsw_nc)<br/><br/>counts_nsw_nc.most_common(15)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/170997f8702e9a6194fe07ec09d5fdc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yrqdj5Hdx3C8YRdwFpeqXA.png"/></div></div></figure><p id="bdbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好多了！让我们将它保存为数据帧。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="7dc8" class="nr mp it nn b gy ns nt l nu nv">clean_tweets_ncw = pd.DataFrame(counts_nsw_nc.most_common(15),<br/>                             columns=['words', 'count'])<br/>clean_tweets_ncw.head()</span></pre><p id="c4b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那看起来像什么？我们来形象化一下吧！</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="c7be" class="nr mp it nn b gy ns nt l nu nv">fig, ax = plt.subplots(figsize=(8, 8))<br/><br/><em class="oa"># Plot horizontal bar graph</em><br/>clean_tweets_no_urls.sort_values(by='count').plot.barh(x='words',<br/>                      y='count',<br/>                      ax=ax,<br/>                      color="purple")<br/><br/>ax.set_title("Common Words Found in Tweets (Including All Words)")<br/><br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/32b8b9ce5df03ce06b08fbf9bb8bea55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s3SnsYx4-rwPYvF6Uq7fYQ.png"/></div></div></figure><p id="1b16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看一些大人物！</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="df69" class="nr mp it nn b gy ns nt l nu nv"><strong class="nn iu">from</strong> <strong class="nn iu">nltk</strong> <strong class="nn iu">import</strong> bigrams<br/><br/><em class="oa"># Create list of lists containing bigrams in tweets</em><br/>terms_bigram = [list(bigrams(tweet)) <strong class="nn iu">for</strong> tweet <strong class="nn iu">in</strong> tweets_nsw_nc]<br/><br/><em class="oa"># View bigrams for the first tweet</em><br/>terms_bigram[0]</span><span id="fb37" class="nr mp it nn b gy nz nt l nu nv"><em class="oa"># Flatten list of bigrams in clean tweets</em><br/>bigrams = list(itertools.chain(*terms_bigram))<br/><br/><em class="oa"># Create counter of words in clean bigrams</em><br/>bigram_counts = collections.Counter(bigrams)<br/><br/>bigram_counts.most_common(20)</span><span id="e77a" class="nr mp it nn b gy nz nt l nu nv">bigram_df = pd.DataFrame(bigram_counts.most_common(20),                              columns=['bigram', 'count'])  bigram_df</span></pre><p id="8c22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">某些二元模型也非常常见，包括微笑和宽阔，出现 42，185 次，害怕和孤独，出现 4，641 次，感觉和孤独，出现 3，541 次。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/3dd3d004995003fcf014ba1d44b7706f.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*7liLG0ezzKMFtwa-AjrA3Q.png"/></div></figure><p id="e9d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这只是清理、预处理和可视化数据的开始。在构建我们的模型之前，我们还可以从这里做很多事情！</p><p id="d568" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦清理了推文，通过用清理过的推文创建单词云，很容易看出两个数据集之间的差异。仅通过 TWINT Twitter 的简短抓取，两个数据集之间的差异就很明显:</p><p id="aef9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">随机推文词云:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/7ed8e4df320b55b5c173cf041c9a48dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUsD6skX6B5c3qwweSHarw.png"/></div></div></figure><p id="5d0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">郁啾词云:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/eafc4eec8395d7b6b6b6fd60dd6a99af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esvfGDnq8eYhVJiaDm5VSg.png"/></div></div></figure><p id="e75a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个过程的早期，很明显，为了获得更准确的结果，优化模型的最重要部分是数据收集、清理和预处理阶段。在对推文进行适当的清理之前，该模型的准确性并不令人印象深刻。通过更加仔细地清理和处理推文，模型的鲁棒性提高到了 97%。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="d98b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您对了解数据清理和预处理的绝对基础感兴趣，可以看看这篇文章！</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/the-complete-beginners-guide-to-data-cleaning-and-preprocessing-2070b7d4c6d"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">数据清理和预处理完全初学者指南</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">如何在几分钟内为机器学习模型成功准备数据</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="om l mk ml mm mi mn ks lz"/></div></div></a></div><p id="8912" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！和往常一样，如果你用这些信息做了什么很酷的事情，请在下面的评论中让每个人都知道，或者随时联系我们！</p></div></div>    
</body>
</html>