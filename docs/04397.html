<html>
<head>
<title>The Problem of Vanishing Gradients</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">消失梯度的问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-problem-of-vanishing-gradients-68cea05e2625?source=collection_archive---------22-----------------------#2019-07-08">https://towardsdatascience.com/the-problem-of-vanishing-gradients-68cea05e2625?source=collection_archive---------22-----------------------#2019-07-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f053" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用基于梯度的优化方法训练深度神经网络时会出现消失梯度。这是由于用于训练神经网络的反向传播算法的性质造成的。</p><h2 id="6068" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">这篇博客将涵盖哪些内容？</h2><ol class=""><li id="b36b" class="le lf iq jp b jq lg ju lh jy li kc lj kg lk kk ll lm ln lo bi translated"><strong class="jp ir">解释消失渐变的问题:</strong>我们会明白为什么会存在消失渐变的问题。我们还将研究为什么这个问题在使用 sigmoid 激活函数时更容易观察到，以及 RELU 如何减少这个问题。</li><li id="b03c" class="le lf iq jp b jq lp ju lq jy lr kc ls kg lt kk ll lm ln lo bi translated"><strong class="jp ir">使用 TensorFlow 的比较研究:</strong>我们将使用 sigmoid 和 RELU 激活函数在时尚 MNIST 数据集上训练 MLP 分类器，并比较梯度。</li></ol><p id="3a26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们能够理解为什么消失梯度的问题存在之前，我们应该对反向传播算法的方程有一些理解。我将不推导反向传播方程，因为它超出了本文的范围。不过我附了一本著名的<a class="ae lu" href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener ugc nofollow" target="_blank">书</a>的截图。</p><h2 id="e8c3" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">反向传播算法</strong></h2><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/ae2187433b1c38f987a80ab07044821c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*f6FGytT7uF_o9wiLyiJOhA.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Source: <a class="ae lu" href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener ugc nofollow" target="_blank">http://neuralnetworksanddeeplearning.com/chap2.html</a></figcaption></figure><p id="b8d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">观察:</strong></p><ol class=""><li id="b3af" class="le lf iq jp b jq jr ju jv jy mh kc mi kg mj kk ll lm ln lo bi translated">从等式 1 和 2 可以看出，输出层中的误差取决于激活函数的导数。</li><li id="b33a" class="le lf iq jp b jq lp ju lq jy lr kc ls kg lt kk ll lm ln lo bi translated">从等式 2 中，我们可以观察到在前/隐藏层中的误差与权重、来自外层的误差以及激活函数的导数成比例。来自外层的误差又取决于其层的激活函数的导数。</li><li id="b16a" class="le lf iq jp b jq lp ju lq jy lr kc ls kg lt kk ll lm ln lo bi translated">从等式 1、2 和 4 中，我们可以得出结论，任何层的权重梯度都取决于激活层的导数的<em class="mk">乘积。</em></li></ol><h2 id="2356" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">渐变消失的问题是什么？</h2><p id="7037" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">考虑 sigmoid 函数及其导数的图形。观察到对于 sigmoid 函数的非常大的值，导数取非常低的值。如果神经网络有许多隐藏层，随着我们乘以每层的导数，早期层中的梯度将变得非常低。结果，早期层的学习变得非常慢。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/b9b2f5bcdf7464d9ffb25220f5b05114.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0yhJ7DbhOX-tRUseljjYoA.png"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Source: <a class="ae lu" href="https://codeodysseys.com/posts/gradient-calculation/" rel="noopener ugc nofollow" target="_blank">https://codeodysseys.com/posts/gradient-calculation/</a></figcaption></figure><p id="58e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，RELU 函数的导数要么为零，要么为 1。即使我们将许多层的导数相乘，也不会出现与 sigmoid 函数不同的退化(假设 RELU 不在死区中工作)。然而，如果仔细观察等式 2，误差信号也取决于网络的权重。如果网络的权重总是小于零，梯度会慢慢减小。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mt"><img src="../Images/4f5b0153ff8bc2d58d3739d88b3b0f22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MboIkD6Uc900_7BRPo28eA.png"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Source: <a class="ae lu" href="https://mc.ai/comparison-of-activation-functions-for-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://mc.ai/comparison-of-activation-functions-for-deep-neural-networks/</a></figcaption></figure><p id="1744" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经理解了消失梯度的问题，让我们训练两个不同的 MLP 分类器，一个使用 sigmoid，另一个使用 RELU 激活函数。</p><p id="d048" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们开始吧。</p><p id="42bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">导入库</strong></p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="2041" class="kl km iq mv b gy mz na l nb nc">import tensorflow as tf<br/>import numpy as np<br/>from tensorflow import keras<br/>import matplotlib.pyplot as plt <br/>from tqdm import tqdm</span></pre><p id="9159" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">下载数据集</strong></p><p id="3db8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用时尚 MNIST 数据集。它由 10 个不同产品类别的 70000 幅灰度图像组成。每个产品类别有 7000 个样本。你可以在这里阅读更多关于数据集<a class="ae lu" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank">的信息</a>。</p><p id="2794" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用 Keras 下载时尚 MNIST 数据集。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="645b" class="kl km iq mv b gy mz na l nb nc">fashion_mnist = keras.datasets.fashion_mnist<br/>(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()<br/>print(x_train.shape)<br/>print(y_train.shape)<br/>print(x_test.shape)<br/>print(y_test.shape)</span><span id="c643" class="kl km iq mv b gy nd na l nb nc">(60000,28,28)<br/>(60000,)<br/>(10000,28,28)<br/>(10000,)</span></pre><h2 id="8317" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">预处理</h2><p id="53ab" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">因为我们将使用 MLP 分类器，我们需要将 28*28 的灰度图像展平为 784 长的矢量。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="f18f" class="kl km iq mv b gy mz na l nb nc">x_train = x_train.reshape(60000,-1)<br/>x_test = x_test.reshape(10000,-1)</span></pre><p id="f615" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图像的像素值介于 0 和 1 之间。我们将定义一个函数，通过移除平均值并缩放到单位方差来标准化数据集。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="7ee6" class="kl km iq mv b gy mz na l nb nc">def normalize(x_train, x_test):<br/>  train_mean = np.mean(x_train)<br/>  train_std = np.mean(x_train)<br/>  x_train = (x_train - train_mean)/train_std<br/>  x_test = (x_test - train_mean)/train_std  <br/>  return x_train, x_test</span></pre><p id="e48d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">产品类别从 0 到 9 进行标记。我们将定义一个实用程序来将这些值转换成一个热编码。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="2686" class="kl km iq mv b gy mz na l nb nc">def convert_to_one_hot(labels):<br/>  no_samples = labels.shape[0]<br/>  n_classes = np.max(labels) + 1<br/>  one_hot = np.zeros((no_samples, n_classes))<br/>  one_hot[np.arange(no_samples),labels.ravel()] = 1<br/>  return one_hot</span></pre><p id="9db1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们通过将原始值传递给上面定义的效用函数来预处理数据集。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="3581" class="kl km iq mv b gy mz na l nb nc">x_train, x_test = normalize(x_train, x_test)<br/>y_train = convert_to_one_hot(y_train)<br/>y_test = convert_to_one_hot(y_test)</span></pre><h1 id="17f2" class="ne km iq bd kn nf ng nh kq ni nj nk kt nl nm nn kw no np nq kz nr ns nt lc nu bi translated">MLP 分类器</h1><p id="a2b9" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">我们将使用张量流训练一个 MLP 分类器。我们将创建一个包括输入层和输出层的 6 层网络。我们将训练两个不同的模型，一个使用 sigmoid 激活函数，另一个使用 RELU 激活函数。在反向传播算法的每次迭代中，我们将存储关于每层权重的损失梯度。最后，通过一些可视化，我们将比较</p><ol class=""><li id="3a80" class="le lf iq jp b jq jr ju jv jy mh kc mi kg mj kk ll lm ln lo bi translated">最终层相对于早期层的梯度</li><li id="1cdb" class="le lf iq jp b jq lp ju lq jy lr kc ls kg lt kk ll lm ln lo bi translated">sigmoid 激活函数与 RELU 激活函数的梯度</li></ol><p id="b377" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们建立模型。</p><h2 id="6ce9" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">定义占位符</strong></h2><p id="faa0" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">首先，我们定义输入和目标的占位符。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="e0f9" class="kl km iq mv b gy mz na l nb nc">def get_placeholders(input_size, output_size):<br/>  inputs = tf.placeholder(dtype=tf.float32, shape=[None, input_size], name="inputs")<br/>  targets = tf.placeholder(dtype=tf.float32, shape=[None, output_size], name="targets")<br/>  return inputs, targets</span></pre><h2 id="f762" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">致密层</strong></h2><p id="ce69" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">我们将定义一个函数来创建一个完全连接的层。每层的权重以格式<em class="mk">“kernel/&lt;layer _ num&gt;”</em>命名。这将有助于以后提取梯度。稍后将详细介绍。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="3807" class="kl km iq mv b gy mz na l nb nc">def dense_layer(input, hidden_units, layer_no, activation_fn= tf.nn.relu):<br/>  weights_name = "kernel/{}".format(layer_no)<br/>  bias_name = "biases/{}".format(layer_no)<br/>  weights = tf.get_variable(weights_name, shape=[input.shape[1], hidden_units], initializer = tf.contrib.layers.xavier_initializer(seed=0))<br/>  biases = tf.get_variable(bias_name, shape=[hidden_units], initializer = tf.zeros_initializer())<br/>  output = tf.add(tf.matmul(input, weights), biases)<br/>  if activation_fn:<br/>    return activation_fn(output)<br/>  else:<br/>    return output</span></pre><h2 id="4f01" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">前向网络</strong></h2><p id="d941" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">接下来，我们定义一个函数来创建网络的正向传递。我们将激活函数应用于每个隐藏层。输出图层不通过激活函数，因为输出图层的 softmax 激活将包含在损失定义中。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="d79d" class="kl km iq mv b gy mz na l nb nc">def build_network(features, labels, hidden_units, num_layers, activation_fn):<br/>  inputs = features<br/>  for layer in range(num_layers-1):<br/>    inputs = dense_layer(inputs, hidden_units[layer], layer+1, activation_fn)<br/>  logits = dense_layer(inputs, 10, num_layers, None) <br/>  return logits</span></pre><h2 id="10a9" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">损失和优化器</strong></h2><p id="dbca" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">因为我们想要在 10 个不同的产品类别之间进行分类，所以我们将 softmax 激活应用到输出层以获得最终的概率。我们将使用<em class="mk">交叉熵</em>损失作为衡量标准来确定我们的网络有多好。Tensorflow 提供了一个方便的实用程序来使用<em class="mk">TF . nn . soft max _ cross _ entropy _ with _ logits _ v2 一起计算这两者。</em></p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="c326" class="kl km iq mv b gy mz na l nb nc">def compute_loss(logits, labels):<br/>  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = labels , logits= logits))<br/>  train_op = tf.train.AdamOptimizer(0.001)<br/>  return loss, train_op</span></pre><p id="338b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你仔细观察，我们没有在优化器上添加<em class="mk">最小化</em>操作。<em class="mk">最小化</em>操作在内部执行两个操作。首先，它根据权重和偏差计算损失的梯度。其次，它使用上面计算的梯度更新权重和偏差。</p><p id="6c46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于我们希望每次迭代的梯度值来研究梯度消失的问题，我们将定义两个独立的操作，一个是计算梯度，另一个是应用梯度。当我们训练我们的模型时，我们将再次讨论这个问题。</p><h2 id="f597" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">精度</strong></h2><p id="5014" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">我们还将定义一个实用程序来计算模型的准确性。<em class="mk"> tf.argmax(logits，1) </em>将返回概率最大的索引。由于标签是一位热编码的，<em class="mk"> tf.argmax(labels，1) </em>将返回值为 1 的索引。我们比较这两个值来确定正确的预测和准确性。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="2ec6" class="kl km iq mv b gy mz na l nb nc">def compute_accuracy(logits, labels):<br/>  correct_predictions = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))<br/>  accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))<br/>  return accuracy</span></pre><h2 id="4895" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">网络中的权重</strong></h2><p id="ce60" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">我们将定义一个函数来返回网络中所有权重的名称。还记得我们将权重的名称定义为<em class="mk">内核/ &lt;层编号&gt;吗？</em> Tensorflow 在内部将“:0”附加到变量名称上。你可以在这里了解更多信息<a class="ae lu" href="https://stackoverflow.com/questions/40925652/in-tensorflow-whats-the-meaning-of-0-in-a-variables-name" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="6a0d" class="kl km iq mv b gy mz na l nb nc">def get_weights_name(num_layers):<br/>  return ["kernel/{}:0".format(layer+1) for layer in range(num_layers)]</span></pre><h2 id="85db" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">火车模型</h2><p id="7d6b" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">我们将定义一个函数来训练模型。我们将使用 sigmoid 和 RELU 激活来训练我们的模型。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="2c2d" class="kl km iq mv b gy mz na l nb nc">def train_model(features, labels, hidden_units, epochs, batch_size, learning_rate, num_layers, activation_fn):<br/>  tf.reset_default_graph()<br/>  input_size = features.shape[1]<br/>  output_size = labels.shape[1]<br/>  <br/>  <em class="mk"># get the placeholders </em><br/>  inputs, targets = get_placeholders(input_size,output_size)<br/>  <br/>  <em class="mk"># create a dataset</em><br/>  dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))<br/>  <br/>  <em class="mk"># make the required batches </em><br/>  dataset = dataset.batch(batch_size)<br/>  <br/>  <em class="mk"># initialize the iterator for the dataset </em><br/>  iterator = dataset.make_initializable_iterator()<br/>  <br/>  <em class="mk"># get the next batch</em><br/>  x_batch, y_batch = iterator.get_next()<br/>  <br/>  <em class="mk"># forward network</em><br/>  logits = build_network(x_batch, y_batch, hidden_units, num_layers, activation_fn)<br/> <br/>  <em class="mk"># compute the loss</em><br/>  loss, train_op = compute_loss(logits, y_batch)<br/>  <br/>  <em class="mk">''' instead of directly using the minimize function on the optimizer, </em><br/><em class="mk">   we break the operation into two parts</em><br/><em class="mk">   1. compute the gradients</em><br/><em class="mk">   2. apply the gradients</em><br/><em class="mk">  '''</em><br/>  grads_and_vars_tensor = train_op.compute_gradients(loss, tf.trainable_variables())<br/>  optimizer = train_op.apply_gradients(grads_and_vars_tensor)<br/>  <br/>  <em class="mk"># compute the accuracy of the model</em><br/>  accuracy = compute_accuracy(logits, y_batch)<br/>  <br/>  init_op = tf.global_variables_initializer()<br/>  saver = tf.train.Saver()<br/>  <br/>  with tf.Session() as sess:<br/>    sess.run(init_op)<br/>    def_graph = tf.get_default_graph()<br/>    gradient_means = []<br/>    losses = []<br/>    accuracies = []<br/>    train_samples = features.shape[0]<br/>    <br/>    iteration = 0<br/>    <em class="mk"># get the name of all the trainable variables</em><br/>    trainable_variables = [var.name for var in tf.trainable_variables()]<br/>    for epoch in range(epochs):<br/>      epoch_loss = 0<br/>      total_accuracy = 0<br/>      <em class="mk"># run the iterator's initializer</em><br/>      sess.run(iterator.initializer, feed_dict={inputs:features, targets:labels})<br/>      try:<br/>        <br/>        while True:<br/>          gradients = []<br/>          batch_loss, grads_and_vars,  _ , batch_accuracy = sess.run([loss, grads_and_vars_tensor, optimizer, accuracy])<br/>          if iteration % 100 == 0:<br/>            <br/>            <em class="mk"># create a dictionary of all the trianbale variables and it's gradients</em><br/>            var_grads = dict(zip(trainable_variables, [grad for grad,var in grads_and_vars]))<br/>            <br/>            <em class="mk"># get the gradients of all the weights for all the layers</em><br/>            weights_grads = [var_grads[var] for var in get_weights_name(num_layers)]<br/>            <br/>            <em class="mk"># take the mean of the gradients at each layer</em><br/>            mean_values = [np.mean(np.abs(val)) for val in weights_grads]<br/>            gradient_means.append(mean_values)<br/>            <br/>          epoch_loss += batch_loss<br/>          total_accuracy += batch_accuracy*batch_size<br/>          iteration += 1<br/>      except tf.errors.OutOfRangeError:<br/>        pass<br/>      print("Total Iterations {}".format(iteration))<br/>      acc = total_accuracy/train_samples<br/>      accuracies.append(acc)<br/>      losses.append(epoch_loss)<br/>      print("Epoch: {}/{} , Loss: {} , Accuracy: {}".format(epoch, epochs, epoch_loss, acc))<br/>      <br/>  return losses, accuracies, gradient_means</span></pre><p id="168e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于函数非常大，我们将把它分解成更小的部分来理解。</p><p id="e135" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">将数据输入模型</strong></p><p id="59a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们使用数据集和迭代器向网络提供数据。如果你想了解更多关于数据集和迭代器的细节，你可以参考<a class="ae lu" rel="noopener" target="_blank" href="/building-efficient-data-pipelines-using-tensorflow-8f647f03b4ce">这篇</a>博客。然后，我们使用之前定义的例程来定义逻辑、损失、优化和准确性操作。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="614c" class="kl km iq mv b gy mz na l nb nc">inputs, targets = get_placeholders(input_size,output_size)<br/>dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))<br/>dataset = dataset.batch(batch_size)<br/>iterator = dataset.make_initializable_iterator()<br/>x_batch, y_batch = iterator.get_next()<br/>  <br/>logits = build_network(x_batch, y_batch, num_layers, activation_fn)<br/> <br/>loss, train_op = compute_loss(logits, y_batch)<br/>accuracy = compute_accuracy(logits, y_batch)</span></pre><p id="d9a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">优化操作</strong></p><p id="071a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您还记得，我们没有将最小化操作添加到优化器中，因为我们需要梯度的值。我们可以使用<em class="mk"> tf.compute_gradients 计算 Tensorflow 中的梯度。</em>我们可以使用<em class="mk"> tf.apply_gradients 使用上述梯度更新权重和偏差。</em></p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="cc14" class="kl km iq mv b gy mz na l nb nc">grads_and_vars_tensor = train_op.compute_gradients(loss, tf.trainable_variables())</span><span id="0a0c" class="kl km iq mv b gy nd na l nb nc">optimizer = train_op.apply_gradients(grads_and_vars_tensor)</span></pre><p id="9c16" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">训练模型</strong></p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="7107" class="kl km iq mv b gy mz na l nb nc">with tf.Session() as sess:<br/>    sess.run(init_op)<br/>    def_graph = tf.get_default_graph()<br/>    gradient_means = []<br/>    losses = []<br/>    accuracies = []<br/>    train_samples = features.shape[0]<br/>    <br/>    iteration = 0<br/>    trainable_variables = [var.name for var in tf.trainable_variables()]<br/>    for epoch in range(epochs):<br/>      epoch_loss = 0<br/>      total_accuracy = 0<br/>      sess.run(iterator.initializer, feed_dict={inputs:features, targets:labels})<br/>      try:<br/>        <br/>        while True:<br/>          gradients = []<br/>          batch_loss, grads_and_vars,  _ , batch_accuracy = sess.run([loss, grads_and_vars_tensor, optimizer, accuracy])<br/>          var_grads = dict(zip(trainable_variables, [grad for grad,var in grads_and_vars]))<br/>          weights_grads = [var_grads[var] for var in get_weights_name(num_layers)]<br/>          mean_values = [np.mean(np.abs(val)) for val in weights_grads]<br/>          epoch_loss += batch_loss<br/>          total_accuracy += batch_accuracy*batch_size<br/>          iteration += 1<br/>          gradient_means.append(mean_values)<br/>      except tf.errors.OutOfRangeError:<br/>        pass<br/>      print("Total Iterations {}".format(iteration))<br/>      acc = total_accuracy/train_samples<br/>      accuracies.append(acc)<br/>      losses.append(epoch_loss)<br/>      print("Epoch: {}/{} , Loss: {} , Accuracy: {}".format(epoch, epochs, epoch_loss, acc))<br/>      <br/>  return losses, accuracies, gradient_means</span></pre><p id="52c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用下面的代码计算了每一次迭代中每一层的梯度平均值。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="c5d5" class="kl km iq mv b gy mz na l nb nc">var_grads = dict(zip(trainable_variables, [grad for grad,var in grads_and_vars]))</span><span id="10f3" class="kl km iq mv b gy nd na l nb nc">weights_grads = [var_grads[var] for var in get_weights_name(num_layers)]</span><span id="8dd9" class="kl km iq mv b gy nd na l nb nc">mean_values = [np.mean(np.abs(val)) for val in weights_grads</span></pre><p id="18fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们首先创建一个包含所有可训练变量和梯度的字典。接下来，我们只提取权重的梯度，并计算每层的平均值。</p><h2 id="f30d" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">运行模型</h2><p id="607b" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">我们已经定义了运行模型所需的所有例程。让我们定义模型的超参数，并运行 sigmoid 和 RELU 激活函数的模型。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="26da" class="kl km iq mv b gy mz na l nb nc">features = x_train<br/>labels = y_train<br/>epochs = 20<br/>batch_size = 256 <br/>learning_rate = 0.001</span><span id="3443" class="kl km iq mv b gy nd na l nb nc">num_layers = 5<br/>hidden_units = [30,30,30,30]<br/>input_units = x_train.shape[1]<br/>output_units = y_train.shape[1] </span><span id="0a9a" class="kl km iq mv b gy nd na l nb nc"><em class="mk"># run the model for the activation functions sigmoid and relu</em><br/>activation_fns = {"sigmoid":tf.nn.sigmoid,<br/>                 "relu":tf.nn.relu}</span><span id="6a12" class="kl km iq mv b gy nd na l nb nc">loss = {}<br/>acc= {}<br/>grad={} </span><span id="5608" class="kl km iq mv b gy nd na l nb nc">for name, activation_fn in activation_fns.items():<br/>  model_name = "Running model with activation function as {}".format(name)<br/>  print(model_name)<br/>  losses, accuracies, grad_means = train_model(features = features,<br/>              labels = labels, <br/>              hidden_units = hidden_units,                                 <br/>              epochs = epochs, <br/>              batch_size = batch_size, <br/>              learning_rate = learning_rate, <br/>              num_layers = num_layers, <br/>              activation_fn = activation_fn)<br/>  loss[name] = losses<br/>  acc[name] = accuracies<br/>  grad[name] = grad_means</span></pre><p id="c1cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们使用一些可视化工具来比较这两个模型的精确度和梯度。</p><h1 id="2c8e" class="ne km iq bd kn nf ng nh kq ni nj nk kt nl nm nn kw no np nq kz nr ns nt lc nu bi translated">形象化</h1><p id="8898" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated"><strong class="jp ir">精度</strong></p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="e5e9" class="kl km iq mv b gy mz na l nb nc">def plot_accuracy(accuracies, title):<br/>  for name, values in accuracies.items():<br/>    plt.plot(values, label = name)<br/>    plt.legend(title=title)</span><span id="5abc" class="kl km iq mv b gy nd na l nb nc">plot_accuracy(acc, "Accuracy")</span></pre><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nv"><img src="../Images/6caefdcf4f9edc3c48788af3f6955d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*F_GVpPam8_tSFcEsM0UWQw.png"/></div></div></figure><p id="746c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，用 RELU 函数训练的模型比用 sigmoid 训练的模型更准确。还可以观察到，sigmoid 模型需要更多的时间来实现高精度。<strong class="jp ir">为什么？</strong>由于渐变消失的问题，早期层的学习速度较慢。</p><h2 id="cc04" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">梯度</h2><p id="e451" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">让我们绘制每一层的梯度。</p><pre class="lw lx ly lz gt mu mv mw mx aw my bi"><span id="1185" class="kl km iq mv b gy mz na l nb nc">def plot_gradients(grads):<br/>  sigmoid = np.array(grads['sigmoid'])<br/>  relu = np.array(grads['relu'])<br/>  for layer_num in range(num_layers):<br/>    plt.figure(figsize=(20,20))<br/>    plt.subplot(5,1,layer_num+1)<br/>    plt.plot(sigmoid[:,layer_num], label='Sigmoid')<br/>    plt.plot(relu[:,layer_num], label='Relu')<br/>    plt.legend(title='Layer{}'.format(layer_num+1))</span><span id="448f" class="kl km iq mv b gy nd na l nb nc">plot_gradients(grad)</span></pre><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/749ad0f52fc327c263e14073262f65d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*cnzvpUn2aYjeY2cyBHeUvA.png"/></div></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/2502a9f1aeb42154a4a81b763c80a9d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*cGIwEYixIkgTy_n8BOgodg.png"/></div></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b4c8fd6dd04202e83c9f00b88a6f849f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*N3Te_wNt_IusSl3BVNKQtw.png"/></div></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/865bf64779b73b4980e7f2bf3dbc216c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*o5BTipWoWZEFl8No_NMrDQ.png"/></div></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/3adac53efc2dc4412eed4e7183c14856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*aaDZjHTzAefPmeZPpKqz0w.png"/></div></figure><p id="e7d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上面的图表中我们可以得出两个主要结论</p><ol class=""><li id="9938" class="le lf iq jp b jq jr ju jv jy mh kc mi kg mj kk ll lm ln lo bi translated">与最终层中的梯度相比，早期层中的梯度幅度非常低。这解释了渐变消失的问题。</li><li id="d2c2" class="le lf iq jp b jq lp ju lq jy lr kc ls kg lt kk ll lm ln lo bi translated">与 RELU 模型相比，sigmoid 模型的梯度值非常低。</li></ol><h1 id="38d1" class="ne km iq bd kn nf ng nh kq ni nj nk kt nl nm nn kw no np nq kz nr ns nt lc nu bi translated">摘要</h1><p id="b048" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">在这篇博客中，我们讨论了渐变消失的问题，以及当我们使用 sigmoid 函数时如何缓解这个问题。我们还讨论了 RELU 函数如何帮助大大减少这个问题。</p><p id="5138" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读博客。希望你喜欢这篇文章。请在下面的评论中留下你的建议。</p><p id="ac27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章的全部代码可以在<a class="ae lu" href="https://github.com/animesh-agarwal/deep-learning-hyperparameters-tuning/blob/master/Vanishing%20Gradients/Vanishing_Gradients_with_TensorFlow.ipynb" rel="noopener ugc nofollow" target="_blank">这个</a> Jupyter 笔记本中找到。</p><h1 id="4991" class="ne km iq bd kn nf ng nh kq ni nj nk kt nl nm nn kw no np nq kz nr ns nt lc nu bi translated">参考</h1><ol class=""><li id="78a3" class="le lf iq jp b jq lg ju lh jy li kc lj kg lk kk ll lm ln lo bi translated"><a class="ae lu" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Vanishing_gradient_problem</a></li><li id="2a5f" class="le lf iq jp b jq lp ju lq jy lr kc ls kg lt kk ll lm ln lo bi translated"><a class="ae lu" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a></li><li id="5690" class="le lf iq jp b jq lp ju lq jy lr kc ls kg lt kk ll lm ln lo bi translated"><a class="ae lu" href="https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/" rel="noopener ugc nofollow" target="_blank">https://adventuresinmachine learning . com/vanishing-gradient-problem-tensor flow/</a></li><li id="5f7b" class="le lf iq jp b jq lp ju lq jy lr kc ls kg lt kk ll lm ln lo bi translated"><a class="ae lu" href="https://stackoverflow.com/questions/40925652/in-tensorflow-whats-the-meaning-of-0-in-a-variables-name" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/40925652/in-tensor flow-0-in-a-variables-name 的含义是什么</a></li><li id="3004" class="le lf iq jp b jq lp ju lq jy lr kc ls kg lt kk ll lm ln lo bi translated"><a class="ae lu" href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener ugc nofollow" target="_blank">http://neuralnetworksanddeeplearning.com/chap2.html</a></li></ol></div></div>    
</body>
</html>