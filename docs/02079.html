<html>
<head>
<title>Review: GBD-Net / GBD-v1 &amp; GBD-v2 — Winner of ILSVRC 2016 (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:GBD-网络/ GBD-v1 和 GBD-v2——2016 年国际地球物理遥感中心(物体探测)获奖者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-gbd-net-gbd-v1-gbd-v2-winner-of-ilsvrc-2016-object-detection-d625fbeadeac?source=collection_archive---------24-----------------------#2019-04-05">https://towardsdatascience.com/review-gbd-net-gbd-v1-gbd-v2-winner-of-ilsvrc-2016-object-detection-d625fbeadeac?source=collection_archive---------24-----------------------#2019-04-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b7ea" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">门控双向网络，赢得 ILSVRC 2016 对象检测挑战赛</h2></div><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="ab gu cl kk"><img src="../Images/b11af17147dd46fd267f43e107882d0d.png" data-original-src="https://miro.medium.com/v2/format:webp/1*MXnHs6e52rYUEid0D5-lDA.png"/></div></figure><p id="abb9" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi lj translated"><span class="l lk ll lm bm ln lo lp lq lr di"> T </span>他的时代，<strong class="kp ir"> GBD 网(门控双向网络)</strong>，由<strong class="kp ir">香港中文大学(CUHK) </strong>和<strong class="kp ir"> SenseTime </strong>点评。GBD-Net 赢得了 ILSVRC 2016 目标检测挑战赛，在<strong class="kp ir"> 2016 ECCV </strong>中首次提出，引用超过<strong class="kp ir"> 30 次</strong>。然后延伸发表在<strong class="kp ir"> 2018 TPAMI </strong>，引用<strong class="kp ir"> 50 余次</strong>。(<a class="ls lt ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----d625fbeadeac--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="2bd1" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在这个故事中，主要是扩展，<strong class="kp ir"> 2018 TPAMI </strong>，因为它被描述得更加详细。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="37e7" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">概述</h1><ol class=""><li id="43da" class="mt mu iq kp b kq mv kt mw kw mx la my le mz li na nb nc nd bi translated"><strong class="kp ir">问题</strong></li><li id="386b" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kp ir"> GBD-v1 </strong></li><li id="f2cf" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kp ir"> GBD-v2 </strong></li><li id="bd7c" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kp ir">其他技术</strong></li><li id="eec0" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kp ir">消融研究</strong></li><li id="81fe" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kp ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="82d6" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">1.问题</h1><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/001be896981115aa846566ae4304a199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*og0MsZi4RFHXGcUr1xMFuA.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Potential Problems When We Classify the Object in a Candidate Box (Red) with Ground-Truth (Blue)</strong></figcaption></figure><ul class=""><li id="cdad" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated"><strong class="kp ir"> (a) </strong>:候选框可以是兔子，也可以是仓鼠。</li><li id="d079" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir"> (b) </strong> : b2 可能因 IoU 较小而被视为误报。</li><li id="573e" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir"> (c)和(d) </strong>:兔头不一定是兔子，可以是人。</li><li id="3546" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">因此，没有来自候选框的较大周围区域的信息，很难区分类别标签。</li><li id="a853" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">首先，候选框周围的上下文区域是一种自然的帮助。</li><li id="d39e" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">此外，周围区域还提供关于背景和其他附近物体的上下文信息，以帮助检测。</li><li id="8db1" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir">来自周围区域的信息用于改进候选框的分类。</strong></li></ul><h1 id="6e9a" class="mb mc iq bd md me nx mg mh mi ny mk ml jw nz jx mn jz oa ka mp kc ob kd mr ms bi translated">2.GBD-v1</h1><h2 id="5ecd" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">2.1.总体框架</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oo"><img src="../Images/0cb6928ba338c7261983063e16537bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QTeLNfLxW-HKQiDIZs-IqQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">GBD-v1 Overall Framework</strong></figcaption></figure><ul class=""><li id="ce5b" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">以上是 GBD-v1 的框架。</li><li id="5393" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><a class="ae op" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">使用快速 R-CNN </a>管道。</li><li id="9329" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">首先，诸如选择性搜索(SS)的区域提议方法将生成一组区域提议/候选框。</li><li id="26cd" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">在 ROI 合并之后，对于每个候选框，它都要经过建议的 GBD-v1。</li><li id="d95b" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">最终的特征图用于分类和包围盒回归，如在<a class="ae op" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>中使用的。</li></ul><h2 id="6168" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">2.2.毅力</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oq"><img src="../Images/414220d902c7b190fd3fde40a61bcb1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ITUL4ebTkDFivF9PXr2Lkg.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><a class="ae op" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"><strong class="bd ns">Inception-v2</strong></a><strong class="bd ns"> as Backbone</strong></figcaption></figure><ul class=""><li id="22a8" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated"><a class="ae op" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">BN-Inception/Inception-v2</a>用作特征提取的主干。</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi or"><img src="../Images/a2f17055b244ec3851a0dbd9ffb8827b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_W-wP0WPgf1u-bAg8QeotQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><a class="ae op" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="bd ns">ResNet-269</strong></a><strong class="bd ns"> as Backbone</strong></figcaption></figure><ul class=""><li id="0694" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">后来，<a class="ae op" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-269 </a>也被用作主干。更好的骨干，更好的准确性。</li></ul><h2 id="aa2f" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">2.3.<strong class="ak">不同分辨率和支持区域的投资回报池</strong></h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi os"><img src="../Images/4148c1b1ec260464ddec2e8acc275602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XG0f9whe9krmNLVlfdvNQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">ROI Pooling with Different Resolutions and Support Regions</strong></figcaption></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/74424fc1243285a93881f55986350a18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*CtZUaj9F8AiA-8jCNymY_A.png"/></div></figure><ul class=""><li id="039b" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">使用候选框(红色)，不同的分辨率和支持区域基于该框汇集在一起。</li><li id="cd05" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">用 p = {-0.2，0.2，0.8，1.7}生成不同的区域。</li></ul><h2 id="b3fb" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">2.4.使用门控双向结构的消息传递</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ou"><img src="../Images/59a4257c3a30faedfb5285b8a49ec0f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ZHQfvMY9GZemYrhWDGIMQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Naive Network Without Message Passing</strong></figcaption></figure><ul class=""><li id="4197" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">最简单的方法是使用不同的支持区域遍历网络进行分类，如上所示。</li><li id="b84d" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">但实际上，它们也应该是相互关联的，因为它们观察的是同一个物体。有些东西可以互相帮助。</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ov"><img src="../Images/9d0de9ae819468eadc9e903f85ec33ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xSS9iJG9HcC866vISbUaDg.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Network With Message Passing</strong></figcaption></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ow"><img src="../Images/ae7b13434c039384b957484b9d6e25cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VybGSFkgEZUMB0gaylU2pQ.png"/></div></div></figure><ul class=""><li id="4cf9" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">因此，这里提出了双向网络。</li><li id="a015" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">一个方向是从小尺寸区域连接到大尺寸区域。</li><li id="7a9f" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">另一种是从大尺寸区域连接到小尺寸区域。</li><li id="84f4" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">因此，来自不同地区的上下文可以使用双向结构相互帮助。</li><li id="1c56" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">⨂是卷积，σ是 ReLU(不是 Sigmoid)，cat()是串联。</li><li id="b72c" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">然而，有时一个上下文区域可能对另一个上下文区域没有帮助，就像第一张图中有兔子头的人一样。</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ox"><img src="../Images/ab9c474d2c7fe90032dd19bc551f031b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*stTkaq_tXCr8aAZEKR9npA.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Network With Message Passing Using Gate Function</strong></figcaption></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oy"><img src="../Images/6f8f9c40f2978625a1a3c6fa648b63c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-k8I3Hq4ZFx--5TmpLvGg.png"/></div></div></figure><ul class=""><li id="7628" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">在消息传递之前引入 Gate 函数。</li><li id="1993" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">因此，引入了上下文相关的门函数。开关将根据环境打开或关闭。</li><li id="4d43" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">门过滤器的尺寸是 3×3，而不是 1×1。</li><li id="8845" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">Sigm 是 sigmoid 函数，是元素级乘积，G 是基于 sigmoid 的门函数。</li><li id="1c5e" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">当 G = 0 时，消息不被传递。</li></ul></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="a7ad" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">3.GBD-v2</h1><h2 id="6040" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">3.1.GBD 的增强版</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oz"><img src="../Images/3098fbf3d1fea1b9aff97a3483721756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pih4VCYRMUZdjOQig-vmRg.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">GBD-v2</strong></figcaption></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/347dd387acff0f413d6c8f6fbd036e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*b83J1IV-OY2b55ODI_8K0g.png"/></div></figure><ul class=""><li id="6376" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">GBD 网络得到加强。</li><li id="1508" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">最大池用于合并来自<em class="pb"> h i </em>和<em class="pb"> h i </em>的信息。与 GBD-v1 相比，这可以节省内存和计算量。</li><li id="6e81" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">另外，从<em class="pb"> h⁰i </em>到<em class="pb"> h i </em>还增加了一个身份映射层。常数<em class="pb"> β </em>在相加前相乘。</li></ul></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="6b86" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated"><strong class="ak"> 4。其他技术</strong></h1><h2 id="670d" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">4.1.候选框生成</h2><ul class=""><li id="9154" class="mt mu iq kp b kq mv kt mw kw mx la my le mz li nw nb nc nd bi translated">CRAFT<a class="ae op" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">的改进版本</a>用于生成候选框。</li><li id="9a56" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">有 3 个版本。</li><li id="42bb" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir">Craft-v1</strong>:<a class="ae op" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">Craft</a>从 1000 级 ImageNet 预训。</li><li id="1b36" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir">Craft-v2</strong>:<a class="ae op" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">Craft</a>用于 GBD-v1，2016 ECCV 论文，但预训练自<a class="ae op" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快 R-CNN </a>使用的地区提案网(RPN)。</li><li id="1a83" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir">Craft-v3</strong>:GBD-v2 使用的改进型<a class="ae op" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858"> CRAFT </a>，训练时使用随机裁剪的 2018 TPAMI 论文，测试时使用多尺度金字塔。同样，阳性和阴性样本在 RPN 训练中是 1:1。使用 LocNet 添加了另一组建议。</li></ul><h2 id="17bd" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">4.2.其他人</h2><ul class=""><li id="c84c" class="mt mu iq kp b kq mv kt mw kw mx la my le mz li nw nb nc nd bi translated"><strong class="kp ir">多尺度测试</strong>:利用训练好的模型，在图像金字塔上计算特征图，图像的短边为{400，500，600，700，800}，长边不大于 1000。</li><li id="be51" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir">左右翻转</strong>:训练和测试都采用。</li><li id="5265" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir">包围盒投票</strong>:使用<a class="ae op" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde"> MR-CNN &amp; S-CNN </a>中的包围盒投票。</li><li id="7b2b" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir">非最大抑制(NMS)阈值</strong>:对于 ImageNet，NMS 阈值默认设置为 0.3。根据经验发现，0.4 是较好的阈值。</li><li id="9b21" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir">全局上下文</strong>:从预训练的网络来看，ImageNet 检测数据也是作为图像分类问题来处理的。这意味着 ROI 区域是整个图像。然后，这个 200 级图像分类分数被用于通过加权平均与 200 级对象检测分数相结合。</li><li id="f251" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated"><strong class="kp ir">模型组合</strong> : 6 个模型用于组合。</li></ul></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="2612" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">5.消融研究</h1><h2 id="1be5" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">5.1.多种分辨率的效果</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi pc"><img src="../Images/a66c25af23dc3255c1bf6cfa3d408840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mqE2dqIJxZcZEtPpduGU5g.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">The Effect of Multiple Resolutions Using </strong><a class="ae op" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"><strong class="bd ns">Inception-v2</strong></a><strong class="bd ns"> as Backbone</strong></figcaption></figure><ul class=""><li id="d7f5" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">使用四种分辨率获得 48.9%的最高 mAP。</li></ul><h2 id="68fa" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">5.2.<a class="ae op" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">工艺</a>版本</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi pd"><img src="../Images/fb838a1ee130ec765e5270844f1d7821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sap9SKTPc6z3715Ucj_6uQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Recall Rate on ImageNet val2</strong></figcaption></figure><ul class=""><li id="0baf" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">对 Craft-v2 的修改，即 Craft-v3，提高了召回率，如上图所示。</li></ul><h2 id="96e3" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">5.3.<strong class="ak">不同的比例因子<em class="pe">β</em>T23】</strong></h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi pf"><img src="../Images/4fbdbe480f462a5a8b7334ac58d45cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dRn8kh-t3X_gGw88B0_IZA.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Different Scaling Factor β in Controlling the Magnitude of Message on ImageNet val2 Using </strong><a class="ae op" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"><strong class="bd ns">Inception-v2</strong></a><strong class="bd ns"> as Backbone</strong></figcaption></figure><ul class=""><li id="0436" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">还测试了控制消息大小的不同比例因子<em class="pb"> β </em>。<em class="pb"> β </em> = 0.1 有 53.6%的最佳贴图。</li></ul><h2 id="0ea0" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">5.4.不同的深度模型作为主干</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi pg"><img src="../Images/6c1070f97caeeffee9a2a34fca0b5bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BW9sN4-ev75UJvut-ab6MQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Different Deep Models as Backbone (“+I” = </strong><a class="ae op" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"><strong class="bd ns">Pre-Activation ResNet</strong></a><strong class="bd ns"> with Identity Mapping, “+S” = </strong><a class="ae op" rel="noopener" target="_blank" href="/review-stochastic-depth-image-classification-a4e225807f4a"><strong class="bd ns">Stochastic Depth</strong></a><strong class="bd ns"> (SD))</strong></figcaption></figure><ul class=""><li id="9639" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated"><a class="ae op" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> Inception-v2 </a>、<a class="ae op" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>、<a class="ae op" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">预激活 ResNet </a> (+I)、<a class="ae op" rel="noopener" target="_blank" href="/review-stochastic-depth-image-classification-a4e225807f4a">随机深度</a> (+S)、<a class="ae op" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener"> Inception-v3 </a>(根据参考应该不是 v5)、<a class="ae op" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a>也尝试过。</li><li id="b310" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">更好的脊梁，更好的地图。</li></ul><h2 id="50b9" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">5.5.用于组装的 6 个深度模型</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ph"><img src="../Images/1eafc54c62c3cb5b60d9e427115826e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0FG9xscxAUpkDafi6cNOpA.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">6 Deep Models for Ensembling</strong></figcaption></figure><ul class=""><li id="a918" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">然而，不同的主干在不同的对象类别上具有不同的准确性。组装时，他们可以互相帮助。</li><li id="fe8f" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">最后选择以上 6 个模型，可以得到 66.9%的 mAP。</li></ul><h2 id="a0a6" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">5.6.包括其他技术</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi pi"><img src="../Images/f47132161c6b1b99a9d3f075b5e01658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Wow1o49RnlldDNldR9DaQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Including Other Techniques</strong></figcaption></figure><ul class=""><li id="1406" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">详情如上。通过以上技术，mAP 从 56.6%提高到 68%。</li><li id="f72b" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li nw nb nc nd bi translated">而 GBD 技术只能帮助将 mAP 从 56.6%提高到 58.8%，这实际上贡献了改进的一部分。</li></ul></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="7144" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated"><strong class="ak"> 6。与最先进方法的比较</strong></h1><h2 id="6fe0" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">6.1.ImageNet val2 上的对象检测</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi pj"><img src="../Images/4eb8ab6eb4055e92ab588f1844e47e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PzZtkVra8cRgweX4OPF10w.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Object Detection on ImageNet val2, sgl: Single Model, avg: Averaged Model (Ensembling)</strong></figcaption></figure><ul class=""><li id="bab9" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">GBD-v2 在所有技术方面都超过了美国有线电视新闻网(R-CNN)、<a class="ae op" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">谷歌网(Google Net)</a>、<a class="ae op" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6"> DeepID-Net </a>和<a class="ae op" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>。</li></ul><h2 id="68ba" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">6.2.不使用外部数据进行训练的 ImageNet 测试集上的对象检测</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi pk"><img src="../Images/c0e97d9ec481ed8d4aacf2cdd85e8c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CM3s2WXTXKJl0l1vZ964AQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Object Detection on ImageNet Test Set</strong></figcaption></figure><ul class=""><li id="2427" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">GBD-v2 优于许多最先进的方法，包括<a class="ae op" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">谷歌网</a>、<a class="ae op" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">雷斯网</a>、<a class="ae op" rel="noopener" target="_blank" href="/review-trimps-soushen-winner-in-ilsvrc-2016-image-classification-dfbc423111dd"> Trimps-Soushen </a>和海康威视(2016 年亚军)。(也许以后有时间我会回顾一下海康威视。)</li></ul><h2 id="9a99" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">6.3.COCO 上的对象检测</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/0e765ab39499a48d88132edfa33ae567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*eDVStcKCT7wnDYk9VWq5Sg.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Object Detection on MS COCO</strong></figcaption></figure><ul class=""><li id="1091" class="mt mu iq kp b kq kr kt ku kw nt la nu le nv li nw nb nc nd bi translated">同样，GBD-v2 优于最先进的方法，如<a class="ae op" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>、<a class="ae op" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> ION </a>和<a class="ae op" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>。</li></ul></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="bfe3" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了提高性能，除了深度神经网络管道上的新颖想法或更好的主干，其他技术也很重要，如 5.6 节中所述，如本文中的区域提议方法、数据扩充、多尺度训练、多尺度测试、包围盒投票、全局上下文和模型集成。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h2 id="28db" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">参考</h2><p id="2d95" class="pw-post-body-paragraph kn ko iq kp b kq mv jr ks kt mw ju kv kw pm ky kz la pn lc ld le po lg lh li ij bi translated">【2016 ECCV】【GBD 网/GBD v1】<br/><a class="ae op" href="https://www.semanticscholar.org/paper/Gated-Bi-directional-CNN-for-Object-Detection-Zeng-Ouyang/931282732f0be57f7fb895238e94bdda00a52cad" rel="noopener ugc nofollow" target="_blank">门控双向 CNN 进行物体检测</a></p><p id="ced9" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">【2018 TPAMI】【GBD-网/GBD-v1 &amp; GBD-v2】<br/><a class="ae op" href="https://arxiv.org/abs/1610.02579" rel="noopener ugc nofollow" target="_blank">制作用于物体检测的 GBD-网</a></p><h2 id="8a5c" class="oc mc iq bd md od oe dn mh of og dp ml kw oh oi mn la oj ok mp le ol om mr on bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kn ko iq kp b kq mv jr ks kt mw ju kv kw pm ky kz la pn lc ld le po lg lh li ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">物体检测<br/></strong><a class="ae op" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae op" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae op" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae op" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae op" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae op" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae op" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae op" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae op" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [ </a><a class="ae op" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a> ] [ <a class="ae op" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae op" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae op" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3 </a> ] [ <a class="ae op" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a> ] [ <a class="ae op" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a> ] [ <a class="ae op" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44"> DCN </a> ]</p><p id="6582" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">语义切分<br/></strong><a class="ae op" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae op" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae op" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae op" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae op" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae op" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae op" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae op" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a><a class="ae op" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a></p><p id="fc65" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">生物医学图像分割<br/></strong>[<a class="ae op" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae op" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae op" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae op" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>[<a class="ae op" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>]</p><p id="3134" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">实例分割<br/></strong>[<a class="ae op" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS</a>[<a class="ae op" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae op" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae op" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae op" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">Instance fcn</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS</a></p><p id="58de" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">超分辨率<br/></strong>[<a class="ae op" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">Sr CNN</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4">fsr CNN</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f">VDSR</a>][<a class="ae op" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener">ESPCN</a>][<a class="ae op" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>][<a class="ae op" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener">DRCN</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994">DRRN</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8">LapSRN&amp;MS-LapSRN</a>][<a class="ae op" rel="noopener" target="_blank" href="/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8">srdensenenet</a></p><p id="f29d" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">人体姿态估计</strong><br/><a class="ae op" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">深度姿态</a><a class="ae op" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">汤普逊·尼普斯 14 </a></p></div></div>    
</body>
</html>