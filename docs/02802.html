<html>
<head>
<title>Understanding PyTorch with an example: a step-by-step tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过示例了解 PyTorch:分步指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e?source=collection_archive---------0-----------------------#2019-05-07">https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e?source=collection_archive---------0-----------------------#2019-05-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/7a4bf333e4e328245f047f4e4f86627f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DG_jU1dr2RG2R1SF"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@aycai?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Allen Cai</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="kd ke kf"><p id="468b" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">更新(2021 年 5 月 18 日):今天我已经<strong class="kj ir">完成了</strong>我的书:<a class="ae kc" href="https://leanpub.com/pytorch/" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">深度学习用 PyTorch 循序渐进:入门指南</strong> </a> <strong class="kj ir">。</strong></p><p id="125d" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">更新(2022 年 2 月 23 日):<strong class="kj ir">平装版</strong>现已上市(三册)。更多详情请查看<a class="ae kc" href="https://pytorchstepbystep.com" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">pytorchstepbystep.com</strong></a>。</p><p id="65a2" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">更新(2022 年 7 月 19 日):第一卷《基础》的<strong class="kj ir">西班牙语版</strong>现已在<a class="ae kc" href="https://leanpub.com/pytorch_ES" rel="noopener ugc nofollow" target="_blank"> Leanpub </a>上发布。</p></blockquote><h1 id="a479" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">介绍</h1><p id="75de" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated"><strong class="kj ir"> PyTorch </strong>是<strong class="kj ir">发展最快的</strong>深度学习框架，<strong class="kj ir"> Fast.ai </strong>在其 MOOC、<a class="ae kc" href="https://course.fast.ai/" rel="noopener ugc nofollow" target="_blank">深度学习 for Coders </a>及其<a class="ae kc" href="https://docs.fast.ai/" rel="noopener ugc nofollow" target="_blank">库</a>中也使用了该框架。</p><p id="dc9c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">PyTorch 也非常 Python 化，也就是说，如果你已经是 Python 开发者，使用它会感觉更自然。</p><p id="e3b6" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">此外，根据安德烈·卡帕西的说法，使用 PyTorch 甚至可以改善你的健康状况</p><h1 id="3873" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">动机</h1><p id="ba15" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">有很多 PyTorch 教程，它的文档非常完整和广泛。所以，<strong class="kj ir">为什么</strong>要一直看这个循序渐进的教程？</p><p id="6a47" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">好吧，尽管人们可以找到关于 PyTorch 能做的几乎所有事情的信息，但我错过了从基本原理的结构化方法中得到结构化的方法。</p><p id="aa9e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在这篇文章中，我将引导你了解 PyTorch 使用 Python 构建深度学习模型变得更加简单和直观的主要原因，包括亲笔签名的模型、动态计算图、模型类和更多，我还将向你展示如何避免常见的陷阱</p><p id="4e1e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">此外，由于这是一篇很长的文章，我建立了一个 T2 目录 T3，如果你把它作为 T4 的迷你课程 T5，一次一个主题地浏览内容，会使浏览更容易。</p><h1 id="241f" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">目录</h1><ul class=""><li id="8829" class="ml mm iq kj b kk md ko me mf mn mh mo mj mp le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#40de" rel="noopener">一个简单的回归问题</a></li><li id="aeeb" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#dc96" rel="noopener">梯度下降</a></li><li id="f1ff" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#a657" rel="noopener">Numpy 中的线性回归</a></li><li id="ba43" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#3a3f" rel="noopener"> PyTorch </a></li><li id="e4e2" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#ea0d" rel="noopener">亲笔签名</a></li><li id="7e18" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#3806" rel="noopener">动态计算图</a></li><li id="6ae0" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#cf51" rel="noopener">优化器</a></li><li id="1e23" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#8877" rel="noopener">损失</a></li><li id="4ddd" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#6208" rel="noopener">型号</a></li><li id="90ab" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#2e24" rel="noopener">数据集</a></li><li id="b175" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#58f2" rel="noopener">数据加载器</a></li><li id="504e" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><a class="ae kc" href="https://medium.com/p/81fc5f8c4e8e#5017" rel="noopener">评估</a></li></ul><h1 id="40de" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">简单的回归问题</h1><p id="5259" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">大多数教程都是从一些好看的<em class="ki">图像分类问题</em>开始，来说明如何使用 PyTorch。这看起来很酷，但我相信它会分散你对主要目标的注意力。</p><p id="2fd3" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">为此，在本教程中，我将坚持使用一个<strong class="kj ir">简单的</strong>和<strong class="kj ir">熟悉的</strong>问题:一个<strong class="kj ir">线性回归</strong> <strong class="kj ir">与一个单一特征<em class="ki"> x </em> </strong>！没有比这更简单的了…</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/94866c028c1d52db4655bbd65eca38b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*a7_GUQQT5BjvAhh3qq0JwA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Simple Linear Regression model</figcaption></figure><h2 id="1aa0" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">数据生成</h2><p id="ff1f" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">让我们开始<strong class="kj ir">生成</strong>一些合成数据:我们从我们的<strong class="kj ir">特征<em class="ki"> x </em> </strong>的 100 个点的向量开始，并使用<strong class="kj ir"> <em class="ki"> a = 1 </em> </strong>、<strong class="kj ir"> <em class="ki"> b = 2 </em> </strong>和一些高斯噪声来创建我们的<strong class="kj ir">标签</strong>。</p><p id="53e0" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">接下来，让我们<strong class="kj ir">将我们的合成数据分割</strong>成<strong class="kj ir">训练</strong>和<strong class="kj ir">验证</strong>集合，洗牌指数数组并使用前 80 个洗牌点进行训练。</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Generating synthetic train and validation sets for a linear regression</figcaption></figure><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/6edb78c1729c3047dd1cad0f4cb762fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SsuTZ1y-pWikYJcaMnZgag.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1: Synthetic data — Train and Validation sets</figcaption></figure><p id="75c7" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们<strong class="kj ir">知道</strong>a = 1 和 b = 2，但是现在让我们看看通过使用<strong class="kj ir">梯度下降</strong>和<strong class="kj ir">训练</strong> <strong class="kj ir">集合</strong>中的 80 个点，我们能多接近真实值…</p><h1 id="dc96" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">梯度下降</h1><p id="7553" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">如果你对梯度下降的内部工作方式很熟悉，<em class="ki">可以跳过</em>这一部分。完全解释梯度下降是如何工作的已经超出了这篇文章的范围，但是我将涵盖计算梯度下降需要经历的<strong class="kj ir">四个基本步骤</strong>。</p><h2 id="8161" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">第一步:计算损失</h2><p id="a519" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">对于一个回归问题，<strong class="kj ir">损失</strong>由<strong class="kj ir">均方差(MSE) </strong>给出，即<strong class="kj ir">标签</strong> (y)和<strong class="kj ir">预测</strong> (a + bx)之间所有平方差的平均值。</p><blockquote class="kd ke kf"><p id="0102" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">值得一提的是，如果我们使用训练集中的<strong class="kj ir">所有点</strong>(<em class="iq">N</em>)来计算损失，我们正在执行<strong class="kj ir">批次</strong>梯度下降。如果我们每次都使用一个<strong class="kj ir">单点</strong>，这将是一个<strong class="kj ir">随机</strong>梯度下降。在 1 和 N 之间的任何其他(n) <strong class="kj ir">表示<strong class="kj ir">小批量</strong>梯度下降。</strong></p></blockquote><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d884dcdc4d9021a7ef1cb43721ce06eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*7fmJUcQT578OBfX7Q8hluQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Loss: Mean Squared Error (MSE)</figcaption></figure><h2 id="2759" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">第二步:计算梯度</h2><p id="9fae" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">一个<strong class="kj ir">梯度</strong>是一个<strong class="kj ir">偏导数</strong> — <em class="ki">为什么</em> <em class="ki">偏导数</em>？因为它是相对于(w.r.t .)一个<strong class="kj ir">单个</strong> <strong class="kj ir">参数</strong>来计算的。我们有两个参数，<strong class="kj ir"> <em class="ki"> a </em> </strong>和<strong class="kj ir"> <em class="ki"> b </em> </strong>，所以必须计算两个偏导数。</p><p id="6d5b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">一个<strong class="kj ir">导数</strong>告诉你<em class="ki">一个给定的 <strong class="kj ir">量变化多少</strong>当你<em class="ki">稍微</em> <em class="ki">变化</em>一些<strong class="kj ir">其他量</strong>。在我们的例子中，当我们改变两个参数中的每一个<strong class="kj ir">时，我们的<strong class="kj ir"> <em class="ki"> MSE </em> </strong> <strong class="kj ir">损耗</strong>变化了多少？</strong></em></p><p id="e39a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">下面等式的<em class="ki">最右边的</em>部分是你通常在简单线性回归的梯度下降实现中看到的。在<strong class="kj ir">中间步骤</strong>中，我向您展示<strong class="kj ir">应用<a class="ae kc" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">链式法则</a>弹出的所有元素</strong>，这样您就知道最终表达式是如何产生的。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/1a0973ed249806e314285b4ed7c75c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YvTj1B-h1gzSI5F24OgrrA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Computing gradients w.r.t coefficients a and b</figcaption></figure><h2 id="d000" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">步骤 3:更新参数</h2><p id="ed18" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">在最后一步，我们使用渐变来更新参数。由于我们试图<strong class="kj ir">最小化</strong>我们的<strong class="kj ir">损失</strong>，我们<strong class="kj ir">反转梯度的符号</strong>用于更新。</p><p id="408c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">还有另一个需要考虑的参数:学习率<strong class="kj ir"/>，用<em class="ki">希腊字母</em> <strong class="kj ir"> <em class="ki"> eta </em> </strong>(看起来像字母<strong class="kj ir"> <em class="ki"> n </em> </strong>)表示，这是我们需要应用于梯度的<strong class="kj ir">乘法因子</strong>，用于参数更新。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/6eefbb7e6e6c67cef6144e6ac341f9ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*eWnUloBYcSNPRBzVcaIr1g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Updating coefficients a and b using computed gradients and a learning rate</figcaption></figure><p id="e79d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">如何<strong class="kj ir">选择</strong>一个学习率？这是一个独立的话题，也超出了本文的范围。</p><h2 id="5e46" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">第四步:冲洗，重复！</h2><p id="4ffa" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">现在我们使用<strong class="kj ir">更新的</strong> <strong class="kj ir">参数</strong>返回到<strong class="kj ir">步骤 1 </strong>并重新开始该过程。</p><blockquote class="kd ke kf"><p id="8c44" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">每当每个点已经用于计算损失时，一个<strong class="kj ir">历元完成。对于<strong class="kj ir">批次</strong>梯度下降，这是微不足道的，因为它使用所有点来计算损失——一个历元</strong>与<strong class="kj ir">一个更新</strong>相同。对于<strong class="kj ir">随机</strong>梯度下降，<strong class="kj ir">一个历元</strong>表示<strong class="kj ir"> N 个</strong> <strong class="kj ir">更新</strong>，而对于<strong class="kj ir">小批量</strong>(尺寸 N)，<strong class="kj ir">一个历元</strong>有<strong class="kj ir"> N/n 个更新</strong>。</p></blockquote><p id="cf7e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">一遍又一遍地重复这个过程，对于<strong class="kj ir">许多时代</strong>，简而言之就是<strong class="kj ir">训练</strong>一个模型。</p><h1 id="a657" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">数字线性回归</h1><p id="c9ef" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">是时候使用<strong class="kj ir"> Numpy only </strong>使用梯度下降实现我们的线性回归模型了。</p><blockquote class="kd ke kf"><p id="b0bf" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">等一下…我以为这个教程是关于 PyTorch 的！</p></blockquote><p id="42f2" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">是的，但这有两个目的<strong class="kj ir"/>:<em class="ki">第一</em>，介绍我们任务的<strong class="kj ir">结构</strong>，它将基本保持不变，<em class="ki">第二</em>，向您展示主要的<strong class="kj ir">难点</strong>，这样您就可以充分体会 PyTorch 让您的生活变得多么轻松:-)</p><p id="4574" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">为了训练一个模型，有两个初始化步骤:</p><ul class=""><li id="63f6" class="ml mm iq kj b kk kl ko kp mf nw mh nx mj ny le mq mr ms mt bi translated">参数/权重的随机初始化(我们只有两个，<em class="ki"> a </em>和<em class="ki"> b </em> ) —第 3 行和第 4 行；</li><li id="45ae" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated">超参数的初始化(在我们的例子中，只有<em class="ki">学习率</em>和<em class="ki">周期数</em> ) —第 9 行和第 11 行；</li></ul><p id="502b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">确保<em class="ki">总是初始化你的随机种子</em>，以确保你的结果的<strong class="kj ir">再现性</strong>。像往常一样，随机种子是<a class="ae kc" href="https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42)" rel="noopener ugc nofollow" target="_blank"> 42 </a>，一个人可能选择的所有随机种子中最不随机的:)</p><p id="d073" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><strong class="kj ir">对于每个时期</strong>，有<strong class="kj ir">四个训练步骤</strong>:</p><ul class=""><li id="fd66" class="ml mm iq kj b kk kl ko kp mf nw mh nx mj ny le mq mr ms mt bi translated">计算模型的预测—这是<strong class="kj ir">正向传递</strong> —第 15 行；</li><li id="a2e4" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated">使用<em class="ki">预测</em>和<em class="ki">标签</em>和手头任务的适当<strong class="kj ir">损失函数</strong>计算损失——第 18 和 20 行；</li><li id="3b62" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated">计算每个参数的<strong class="kj ir">梯度</strong>——第 23 和 24 行；</li><li id="0749" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><strong class="kj ir">更新</strong>参数—第 27 行和第 28 行；</li></ul><p id="5ce0" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">只要记住，如果你<em class="ki">不</em>使用批量梯度下降(我们的例子使用了)，你将不得不写一个<strong class="kj ir">内循环</strong>来执行<strong class="kj ir">四个训练步骤</strong>用于每个<strong class="kj ir">个体点</strong> ( <strong class="kj ir">随机</strong>)或<strong class="kj ir"> <em class="ki"> n </em>点</strong> ( <strong class="kj ir">小批量</strong>)。稍后我们将看到一个小批量的例子。</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Implementing gradient descent for linear regression using Numpy</figcaption></figure><p id="ec64" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">只是为了确保我们没有在代码中犯任何错误，我们可以使用<em class="ki"> Scikit-Learn 的线性回归</em>来拟合模型并比较系数。</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="67d2" class="ne lg iq oa b gy oe of l og oh"># a and b after initialization<br/>[0.49671415] [-0.1382643]<br/># a and b after our gradient descent<br/>[1.02354094] [1.96896411]<br/># intercept and coef from Scikit-Learn<br/>[1.02354075] [1.96896447]</span></pre><p id="a548" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">它们<strong class="kj ir">匹配</strong>多达 6 位小数——我们有一个使用 Numpy 的<em class="ki">完全工作的线性回归</em>实现。</p><p id="a1a6" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">时间到了<strong class="kj ir">火炬</strong>它:-)</p><h1 id="3a3f" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">PyTorch</h1><p id="b702" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">首先，我们需要涵盖一些基本概念，如果你在全力建模之前没有很好地掌握它们，这些概念可能会让你失去平衡。</p><p id="c5bb" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在深度学习中，我们随处可见<strong class="kj ir">张量</strong>。嗯，Google 的框架叫<em class="ki"> TensorFlow </em>是有原因的！究竟什么是张量？</p><h2 id="2f02" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">张量</h2><p id="c0e6" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">在<em class="ki"> Numpy </em>中，你可能有一个<strong class="kj ir">数组</strong>有<strong class="kj ir">三个维度</strong>吧？也就是从技术上来说，一个<strong class="kj ir">张量</strong>。</p><p id="5858" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">一个<strong class="kj ir">标量</strong>(单个数)有<strong class="kj ir">个零</strong>维，一个<strong class="kj ir">向量</strong> <strong class="kj ir">有</strong> <strong class="kj ir">个</strong>维，一个<strong class="kj ir">矩阵有两个</strong>维，一个<strong class="kj ir">张量有三个或更多个</strong>维。就是这样！</p><p id="cd59" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">但是，为了简单起见，通常也称向量和矩阵为张量——所以，从现在开始，<strong class="kj ir">所有东西要么是标量，要么是张量</strong>。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/6a4649514f05971f70ee0257604c0166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GbwKkmA0NdndXRhOOwNclA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2: Tensors are just higher-dimensional matrices :-) <a class="ae kc" href="http://karlstratos.com/drawings/drawings.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="594d" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">加载数据、设备和 CUDA</h2><p id="115d" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">你会问，我们如何从 Numpy 的数组到 PyTorch 的张量？这就是<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/torch.html#torch.from_numpy" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">from_numpy</strong></a></code>的好处。不过，它返回一个 CPU 张量。</p><p id="e021" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">“<em class="ki">但是我想用我的花式 GPU… </em>”你说。别担心，这就是<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">to()</strong></a></code>的用处。它将你的张量发送到你指定的任何<strong class="kj ir">设备</strong>，包括你的<strong class="kj ir"> GPU </strong>(简称<code class="fe oj ok ol oa b">cuda</code>或<code class="fe oj ok ol oa b">cuda:0</code>)。</p><p id="e578" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">"<em class="ki">如果没有可用的 GPU，我希望我的代码回退到 CPU，该怎么办？你可能想知道……py torch 又一次支持了你——你可以使用<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/cuda.html?highlight=is_available#torch.cuda.is_available" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">cuda.is_available()</strong></a></code>来查找你是否有 GPU 供你使用，并相应地设置你的设备。</em></p><p id="4fbb" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">您还可以使用<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.float" rel="noopener ugc nofollow" target="_blank">float()</a></code>轻松地将<strong class="kj ir">转换为较低的精度(32 位浮点)。</strong></p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Loading data: turning Numpy arrays into PyTorch tensors</figcaption></figure><p id="1ef1" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">如果您比较两个变量的<strong class="kj ir">类型</strong>，您将得到您所期望的:第一个变量为<code class="fe oj ok ol oa b">numpy.ndarray</code>，第二个变量为<code class="fe oj ok ol oa b">torch.Tensor</code>。</p><p id="758e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">但是你的张量“住”在哪里呢？在你的 CPU 还是你的 GPU 里？你不能说……但是如果你用 PyTorch 的<code class="fe oj ok ol oa b"><strong class="kj ir">type()</strong></code>，它会揭示它的<strong class="kj ir">位置</strong> — <code class="fe oj ok ol oa b">torch.cuda.FloatTensor</code> —这里是一个 GPU 张量。</p><p id="5629" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们也可以反过来，使用<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/tensors.html?highlight=numpy#torch.Tensor.numpy" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">numpy()</strong></a></code>，将张量转换回 Numpy 数组。应该和<code class="fe oj ok ol oa b">x_train_tensor.numpy()</code> <strong class="kj ir">一样简单，但是</strong> …</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="b253" class="ne lg iq oa b gy oe of l og oh">TypeError: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.</span></pre><p id="1e31" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">不幸的是，Numpy <strong class="kj ir">不能</strong>处理 GPU 张量……你需要先用<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cpu" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">cpu()</strong></a></code>让它们成为 CPU 张量。</p><h2 id="d4c5" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">创建参数</h2><p id="f80a" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">用于<em class="ki">数据</em>的<em class="ki">张量</em>与用作(<em class="ki">可训练</em> ) <strong class="kj ir">参数/权重</strong>的<strong class="kj ir">张量</strong>有何区别？</p><p id="3a8c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">后面的张量需要<strong class="kj ir">计算其梯度</strong>，因此我们可以<strong class="kj ir">更新</strong>它们的值(即参数值)。这就是<code class="fe oj ok ol oa b"><strong class="kj ir">requires_grad=True</strong></code>论点的好处。它告诉 PyTorch 我们希望它为我们计算梯度。</p><p id="9bc5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">你可能想为一个参数创建一个简单的张量，然后发送到你选择的设备，就像我们对数据所做的那样，对吗？没那么快…</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Trying to create variables for the coefficients…</figcaption></figure><p id="0932" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">第一段代码为我们的参数、梯度和所有的东西创建了两个很好的张量。但他们是<strong class="kj ir"> CPU </strong>张量。</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="b453" class="ne lg iq oa b gy oe of l og oh"># FIRST<br/>tensor([-0.5531], requires_grad=True)<br/>tensor([-0.7314], requires_grad=True)</span></pre><p id="32d5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在第二段代码中，我们尝试了将它们发送到 GPU 的简单方法。我们成功地将它们发送到了另一个设备上，但是我们不知何故“丢失”了梯度</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="79be" class="ne lg iq oa b gy oe of l og oh"># SECOND<br/>tensor([0.5158], device='cuda:0', grad_fn=&lt;CopyBackwards&gt;) tensor([0.0246], device='cuda:0', grad_fn=&lt;CopyBackwards&gt;)</span></pre><p id="f7c0" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在第三块中，我们<strong class="kj ir">首先</strong>将我们的张量发送到<strong class="kj ir">设备</strong>和<strong class="kj ir">，然后</strong>使用<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.requires_grad_" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">requires_grad_()</strong></a></code>方法将其<code class="fe oj ok ol oa b">requires_grad</code>设置到<code class="fe oj ok ol oa b">True</code>位置。</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="632c" class="ne lg iq oa b gy oe of l og oh"># THIRD<br/>tensor([-0.8915], device='cuda:0', requires_grad=True) tensor([0.3616], device='cuda:0', requires_grad=True)</span></pre><blockquote class="kd ke kf"><p id="c979" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在 PyTorch 中，每一个<strong class="kj ir">以一个<strong class="kj ir">下划线</strong> ( <strong class="kj ir"> _ </strong>)结束</strong>的方法都会就地修改<strong class="kj ir">，也就是说，它们将<strong class="kj ir">修改</strong>底层变量。</strong></p></blockquote><p id="dd8a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">虽然最后一种方法工作得很好，但是最好是在<strong class="kj ir">创建</strong>时<strong class="kj ir">将</strong>张量分配给<strong class="kj ir">设备</strong>。</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Actually creating variables for the coefficients :-)</figcaption></figure><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="ed99" class="ne lg iq oa b gy oe of l og oh">tensor([0.6226], device='cuda:0', requires_grad=True) tensor([1.4505], device='cuda:0', requires_grad=True)</span></pre><p id="4ec6" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">简单多了，对吧？</p><p id="eb40" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">现在我们知道了如何创建需要梯度的张量，让我们看看 PyTorch 如何处理它们——这是……</p><h1 id="ea0d" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">亲笔签名</h1><p id="efda" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">亲笔签名的是 PyTorch 的<em class="ki">自动微分包</em>。多亏了它，我们<strong class="kj ir">不需要担心</strong>关于<em class="ki">偏导数，链式法则</em>或者任何类似的东西。</p><p id="139f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">那么，我们如何告诉 PyTorch 去做它的事情并且<strong class="kj ir">计算所有的梯度</strong>？这就是<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">backward()</strong></a></code>的好处。</p><p id="b734" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">你还记得<strong class="kj ir">计算梯度</strong>的<strong class="kj ir">起点</strong>吗？当我们计算它相对于我们的参数的偏导数时，它是损失。因此，我们需要从相应的 Python 变量中调用<code class="fe oj ok ol oa b">backward()</code>方法，比如<code class="fe oj ok ol oa b">loss.backward().</code></p><p id="c349" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><strong class="kj ir">坡度</strong>的<strong class="kj ir">实际值</strong>是多少？我们可以通过查看张量的<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/autograd.html#torch.Tensor.grad" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">grad</strong></a></code> <strong class="kj ir">属性</strong>来考察它们。</p><p id="e2b5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">如果你检查该方法的文档，它清楚地说明了<strong class="kj ir">梯度是累积的</strong>。所以，每次我们使用<strong class="kj ir">渐变</strong>到<strong class="kj ir">更新</strong>参数时，我们需要<strong class="kj ir">将渐变归零</strong>。这就是<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.zero_" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">zero_()</strong></a></code>的好处。</p><p id="4c1f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">方法名末尾的<strong class="kj ir">下划线</strong> ( <strong class="kj ir"> _ </strong>)是什么意思？你还记得吗？如果没有，请返回到上一节并找出答案。</p><p id="1afa" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">因此，让我们<strong class="kj ir">放弃</strong><strong class="kj ir">手动</strong> <strong class="kj ir">计算梯度</strong>并使用<code class="fe oj ok ol oa b">backward()</code>和<code class="fe oj ok ol oa b">zero_()</code>方法来代替。</p><p id="33fe" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">就这样？嗯，差不多吧…但是，总有一个<strong class="kj ir">抓手</strong>，这次和<strong class="kj ir">参数</strong>的<strong class="kj ir">更新</strong>有关…</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="005d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在第一次尝试中，如果我们使用与我们的<em class="ki"> Numpy </em>代码中相同的更新结构，我们将会得到下面奇怪的<strong class="kj ir">错误</strong>…但是我们可以通过查看张量本身得到一个关于正在发生什么的<em class="ki">提示</em>——我们再次<strong class="kj ir">“丢失”</strong><strong class="kj ir">梯度</strong>，同时将更新结果重新分配给我们的参数。因此,<code class="fe oj ok ol oa b"><strong class="kj ir">grad</strong></code>属性变成了<code class="fe oj ok ol oa b"><strong class="kj ir">None</strong></code>,并引发了错误…</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="295b" class="ne lg iq oa b gy oe of l og oh"># FIRST ATTEMPT<br/>tensor([0.7518], device='cuda:0', grad_fn=&lt;SubBackward0&gt;)<br/>AttributeError: 'NoneType' object has no attribute 'zero_'</span></pre><p id="7cee" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">然后我们稍微改变一下，在第二次尝试中使用熟悉的<strong class="kj ir">就地 Python 赋值</strong>。PyTorch 再一次对此进行了抱怨，并引发了一个<strong class="kj ir">错误</strong>。</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="aa62" class="ne lg iq oa b gy oe of l og oh"># SECOND ATTEMPT<br/>RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.</span></pre><blockquote class="kd ke kf"><p id="84c2" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">为什么？！原来是<strong class="kj ir">“过犹不及”</strong>的情况。罪魁祸首是 PyTorch 从每一个<strong class="kj ir"> Python 操作</strong>中构建一个<strong class="kj ir">动态计算图</strong>的能力，该操作涉及任何<strong class="kj ir">梯度计算张量</strong>或<strong class="kj ir">其依赖关系</strong>。</p><p id="2e8e" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">我们将在下一节更深入地研究动态计算图的内部工作原理。</p></blockquote><p id="cb86" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">那么，我们如何告诉 PyTorch 去<strong class="kj ir">“后退”</strong>并让我们<strong class="kj ir">更新我们的参数</strong>而不弄乱它的<em class="ki">花哨的动态计算图</em>？这就是<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">torch.no_grad()</strong></a></code>的好处。它允许我们<strong class="kj ir">独立于 PyTorch 的计算图</strong>对张量、<strong class="kj ir">执行常规的 Python 操作。</strong></p><p id="7d04" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">最后，我们成功地运行了我们的模型，并获得了<strong class="kj ir">结果参数</strong>。毫无疑问，它们<strong class="kj ir">与我们在<em class="ki"> Numpy </em>唯一实现中得到的</strong>相匹配。</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="f2c1" class="ne lg iq oa b gy oe of l og oh"># THIRD ATTEMPT<br/>tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)</span></pre><h1 id="3806" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">动态计算图</h1><blockquote class="om"><p id="3318" class="on oo iq bd op oq or os ot ou ov le dk translated">“不幸的是，没有人能被告知动态计算图是什么。你得自己去看。”摩耳甫斯</p></blockquote><p id="eaa5" class="pw-post-body-paragraph kg kh iq kj b kk ow km kn ko ox kq kr mf oy ku kv mh oz ky kz mj pa lc ld le ij bi translated">"<em class="ki">黑客帝国</em>"有多伟大？对吧。但是，玩笑归玩笑，我想让<strong class="kj ir">你</strong>到<strong class="kj ir">也自己看看图表</strong>！</p><blockquote class="kd ke kf"><p id="3d87" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><a class="ae kc" href="https://github.com/szagoruyko/pytorchviz" rel="noopener ugc nofollow" target="_blank"> PyTorchViz </a>包和它的<code class="fe oj ok ol oa b"><em class="iq">make_dot(variable)</em></code>方法允许我们轻松地可视化与给定 Python 变量相关的图形。</p></blockquote><p id="0756" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">因此，让我们坚持使用<strong class="kj ir">最小量</strong>:两个(<em class="ki">梯度计算</em> ) <strong class="kj ir">张量</strong>用于我们的参数、预测、误差和损失。</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Computing MSE in three steps</figcaption></figure><p id="9fa5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">如果我们调用<code class="fe oj ok ol oa b"><strong class="kj ir">make_dot(yhat)</strong></code>，我们将得到下面图 3 中最左边的<strong class="kj ir"/><strong class="kj ir">图</strong>:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pb"><img src="../Images/72178cee3f709a22fe41a6550e96b28e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K2QnR_TRF9XfqNgNGDRqng.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3: Computation graph for every step in computing MSE</figcaption></figure><p id="5da4" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">让我们仔细看看它的组件:</p><ul class=""><li id="a117" class="ml mm iq kj b kk kl ko kp mf nw mh nx mj ny le mq mr ms mt bi translated"><strong class="kj ir">蓝框</strong>:这些对应于我们用作<strong class="kj ir">参数</strong>的<strong class="kj ir">张量</strong>，我们要求 py torch<strong class="kj ir">计算梯度</strong>的那些张量；</li><li id="7cc9" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><strong class="kj ir">灰框</strong>:一个<strong class="kj ir"> Python 操作</strong>，涉及一个<strong class="kj ir">梯度计算张量</strong>或<strong class="kj ir">它的依赖</strong>；</li><li id="12e7" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><strong class="kj ir">绿框</strong>:与灰框相同，除了它是计算渐变的<strong class="kj ir">起点(假设<code class="fe oj ok ol oa b"><strong class="kj ir">backward()</strong></code>方法是从用于可视化</strong>图形的<strong class="kj ir">变量调用的)——它们是从图形中的<strong class="kj ir">自下而上</strong>开始计算的。</strong></li></ul><p id="05c0" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">如果我们为<code class="fe oj ok ol oa b"><strong class="kj ir">error</strong></code>(中间)和<code class="fe oj ok ol oa b"><strong class="kj ir">loss</strong></code>(右边)<strong class="kj ir">变量</strong>绘图，它们与第一个变量<strong class="kj ir">唯一的区别</strong>是<strong class="kj ir">中间步骤</strong> ( <strong class="kj ir">灰色方框</strong>)的数量。</p><p id="9f8f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">现在，仔细看看<strong class="kj ir">最左边</strong>图的<strong class="kj ir">绿框</strong>:有<strong class="kj ir">两个</strong> <strong class="kj ir">箭头</strong>指向它，既然是<strong class="kj ir">加</strong>上<strong class="kj ir">两个</strong> <strong class="kj ir">变量</strong>、<code class="fe oj ok ol oa b">a</code>和<code class="fe oj ok ol oa b">b*x</code>。似乎很明显，对吧？</p><p id="36fc" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">然后再看同图的<strong class="kj ir">灰色</strong> <strong class="kj ir">方框</strong>:它正在执行一个<strong class="kj ir">乘法</strong>，即<code class="fe oj ok ol oa b">b*x</code>。但是只有一个箭头指向它！箭头来自与我们的<strong class="kj ir">参数</strong> <strong class="kj ir"> b </strong>对应的<strong class="kj ir">蓝色</strong> <strong class="kj ir">框</strong>。</p><p id="0f2e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><strong class="kj ir">为什么</strong>没有一个盒子放我们的<strong class="kj ir">数据 x </strong>？答案是:我们<strong class="kj ir">不为它计算梯度</strong>！因此，尽管计算图执行的操作中涉及到更多的<em class="ki">张量</em>，但它<strong class="kj ir">仅</strong>显示了<strong class="kj ir">梯度计算张量</strong>和<strong class="kj ir">的依赖关系</strong>。</p><p id="916a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">如果我们为我们的<strong class="kj ir">参数 a </strong>设置<code class="fe oj ok ol oa b"><strong class="kj ir">requires_grad</strong></code>为<code class="fe oj ok ol oa b"><strong class="kj ir">False</strong></code>，计算图会发生什么？</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/93d9fe5ef5ed49a55d83a22e9c7bb81c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*HXZ6QxILteV3RlzaTaULcw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4: now variable a does NOT have its gradient computed anymore. But it is STILL used in computation</figcaption></figure><p id="bfb5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">不出所料，<strong class="kj ir">参数 a </strong>对应的<strong class="kj ir">蓝框</strong>没了！很简单:<strong class="kj ir">没有渐变，没有图形</strong>。</p><p id="2dc5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">关于<em class="ki">动态计算图</em>的<strong class="kj ir">最好的</strong>事情是，你可以让它<strong class="kj ir">如你所愿</strong>一样复杂。你甚至可以使用<em class="ki">控制流语句</em>(例如 if 语句)来<strong class="kj ir">控制渐变的流</strong>(显然！) :-)</p><p id="5529" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">下面的图 5 显示了一个例子。是的，我知道计算本身完全是无意义的</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/123e9f93ed2491fd7f1987130eda08d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*IxlhJ_iTKQh46ixxRK8BNg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5: Complex computation graph just to make a point :-)</figcaption></figure><h1 id="cf51" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">【计算机】优化程序</h1><p id="d74e" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">到目前为止，我们一直在使用计算出的梯度手动<strong class="kj ir">更新参数。这对两个参数的<em class="ki">来说可能没问题……但是如果我们有一大堆参数<strong class="kj ir">呢？！我们使用 PyTorch 的<strong class="kj ir">优化器</strong>之一，比如<a class="ae kc" href="https://pytorch.org/docs/stable/optim.html#torch.optim.SGD" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir"> SGD </strong> </a>或者<a class="ae kc" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir"> Adam </strong> </a>。</strong></em></strong></p><p id="756b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">一个优化器接受我们想要更新的<strong class="kj ir">参数</strong>，我们想要使用的<strong class="kj ir">学习率</strong>(可能还有许多其他超参数！)和<strong class="kj ir">执行</strong><strong class="kj ir"/><strong class="kj ir">通过其<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.step" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">step()</strong></a></code>方法更新</strong>。</p><p id="bd30" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">此外，我们也不再需要一个接一个地将梯度归零。我们只需调用优化器的<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.zero_grad" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">zero_grad()</strong></a></code>方法，仅此而已！</p><p id="8fbf" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在下面的代码中，我们创建一个<em class="ki">随机梯度下降</em> (SGD)优化器来更新我们的参数<strong class="kj ir"> a </strong>和<strong class="kj ir"> b </strong>。</p><blockquote class="kd ke kf"><p id="95bf" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">不要被<strong class="kj ir">优化器</strong>的名字所迷惑:如果我们一次使用<strong class="kj ir">所有训练数据</strong>进行更新——正如我们在代码中实际做的那样——优化器正在执行一个<strong class="kj ir">批处理</strong>梯度下降，尽管它的名字如此。</p></blockquote><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">PyTorch’s optimizer in action — no more manual update of parameters!</figcaption></figure><p id="d872" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">让我们检查我们的两个参数，之前和之后，只是为了确保一切仍然工作正常:</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="887e" class="ne lg iq oa b gy oe of l og oh"># BEFORE: a, b<br/>tensor([0.6226], device='cuda:0', requires_grad=True) tensor([1.4505], device='cuda:0', requires_grad=True)<br/># AFTER: a, b<br/>tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)</span></pre><p id="a328" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">酷！我们已经<em class="ki">优化了</em><strong class="kj ir">优化</strong>流程:-)还剩下什么？</p><h1 id="8877" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">失败</h1><p id="eaa7" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">我们现在处理<strong class="kj ir">损失计算</strong>。不出所料，PyTorch 又一次掩护了我们。根据手头的任务，有许多<a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#loss-functions" rel="noopener ugc nofollow" target="_blank">损失函数</a>可供选择。由于我们的是回归，我们使用的是<a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss" rel="noopener ugc nofollow" target="_blank">均方误差(MSE)损失</a>。</p><blockquote class="kd ke kf"><p id="64e3" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">注意<code class="fe oj ok ol oa b"><em class="iq">nn.MSELoss</em></code>实际上<strong class="kj ir">为我们创建了一个损失函数</strong>—<strong class="kj ir">它不是损失函数本身</strong>。此外，您可以指定一个要应用的<strong class="kj ir">归约方法</strong>，即<strong class="kj ir">您希望如何合计单个点的结果</strong> —您可以将它们平均(归约= '均值')或简单地将它们相加(归约= '总和')。</p></blockquote><p id="1e64" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">然后我们<strong class="kj ir">使用</strong>创建的损失函数，在第 20 行，计算给定我们的<strong class="kj ir">预测</strong>和我们的<strong class="kj ir">标签</strong>的损失。</p><p id="7a9e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们的代码现在看起来像这样:</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">PyTorch’s loss in action — no more manual loss computation!</figcaption></figure><p id="b72a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">此时，只剩下一段代码需要修改:预测<strong class="kj ir"/>。然后是时候介绍 PyTorch 实现一个…</p><h1 id="6208" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">模型</h1><p id="7845" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">在 PyTorch 中，一个<strong class="kj ir">模型</strong>由一个从<a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">模块</strong> </a>类继承的常规<strong class="kj ir"> Python 类</strong>表示。</p><p id="f8c5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">它需要实现的最基本的方法是:</p><ul class=""><li id="3250" class="ml mm iq kj b kk kl ko kp mf nw mh nx mj ny le mq mr ms mt bi translated"><code class="fe oj ok ol oa b"><strong class="kj ir">__init__(self)</strong></code> : <strong class="kj ir">它定义了组成模型</strong>的部件——在我们的例子中，是两个<em class="ki">参数</em>、<strong class="kj ir"> a </strong>和<strong class="kj ir"> b </strong>。</li></ul><blockquote class="kd ke kf"><p id="bd66" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">您不局限于定义<strong class="kj ir">参数</strong>，尽管……<strong class="kj ir">模型也可以包含其他模型(或层)作为其属性</strong>，因此您可以轻松地嵌套它们。我们很快也会看到这样的例子。</p></blockquote><ul class=""><li id="dce0" class="ml mm iq kj b kk kl ko kp mf nw mh nx mj ny le mq mr ms mt bi translated"><code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">forward(self, x)</strong></a></code>:在给定输入<strong class="kj ir"> x </strong>的情况下，执行<strong class="kj ir">实际计算</strong>，即<strong class="kj ir">输出预测</strong>。</li></ul><blockquote class="kd ke kf"><p id="8e3e" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">不过，你不应该把<strong class="kj ir">叫做</strong> <code class="fe oj ok ol oa b"><strong class="kj ir"><em class="iq">forward(x)</em></strong></code>方法。你应该<strong class="kj ir">调用整个模型本身</strong>，就像在<code class="fe oj ok ol oa b"><strong class="kj ir"><em class="iq">model(x)</em></strong></code>中执行正向传递和输出预测一样。</p></blockquote><p id="5ff0" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">让我们为我们的回归任务建立一个合适的(然而简单的)模型。它应该是这样的:</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Building our “Manual” model, creating parameter by parameter!</figcaption></figure><p id="9378" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在<code class="fe oj ok ol oa b">__init__</code>方法中，我们定义了我们的<strong class="kj ir">两个参数</strong>、<strong class="kj ir"> a </strong>和<strong class="kj ir"> b </strong>，使用<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">Parameter()</strong></a></code>类告诉 PyTorch 这些<strong class="kj ir">张量应该被认为是模型的参数，它们是</strong>的一个属性。</p><p id="de68" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们为什么要关心这个？通过这样做，我们可以使用我们模型的<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.parameters" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">parameters()</strong></a></code>方法来检索<strong class="kj ir">所有模型参数</strong>、<strong class="kj ir">甚至</strong>嵌套模型<strong class="kj ir">的那些参数的迭代器，我们可以使用这些参数来馈送我们的优化器(而不是我们自己构建一个参数列表！).</strong></p><p id="c644" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">此外，我们可以使用我们模型的<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">state_dict()</strong></a></code>方法获得所有参数的<strong class="kj ir">当前值。</strong></p><blockquote class="kd ke kf"><p id="846e" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ir">重要的</strong>:我们需要<strong class="kj ir">将我们的模型发送到数据所在的同一个设备</strong>。如果我们的数据是由 GPU 张量构成的，那么我们的模型也必须“生活”在 GPU 中。</p></blockquote><p id="87a5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们可以使用所有这些方便的方法来更改我们的代码，代码应该是这样的:</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">PyTorch’s model in action — no more manual prediction/forward step!</figcaption></figure><p id="b07d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">现在，打印出来的语句看起来像这样——参数<strong class="kj ir"> a </strong>和<strong class="kj ir"> b </strong>的最终值仍然相同，所以一切正常:-)</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="e1cc" class="ne lg iq oa b gy oe of l og oh">OrderedDict([('a', tensor([0.3367], device='cuda:0')), ('b', tensor([0.1288], device='cuda:0'))])<br/>OrderedDict([('a', tensor([1.0235], device='cuda:0')), ('b', tensor([1.9690], device='cuda:0'))])</span></pre><p id="e147" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我希望你注意到了代码中的一个特殊语句，我给它分配了一个注释<strong class="kj ir">“这是什么？！?"— </strong> <code class="fe oj ok ol oa b"><strong class="kj ir">model.train()</strong></code>。</p><blockquote class="kd ke kf"><p id="eb9c" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在 PyTorch 中，模型有一个<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir"><em class="iq">train()</em></strong></a></code>方法，有点令人失望的是，<strong class="kj ir">没有执行训练步骤</strong>。其唯一目的是<strong class="kj ir">将模型设置为训练模式</strong>。为什么这很重要？例如，一些模型可能使用类似<a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">退出</strong> </a>的机制，这些机制在培训和评估阶段具有<strong class="kj ir">不同的行为。</strong></p></blockquote><h2 id="54f8" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">嵌套模型</h2><p id="90b8" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">在我们的模型中，我们手动创建了两个参数来执行线性回归。让我们使用 PyTorch 的<a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Linear" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">线性</strong> </a>模型作为我们自己的属性，从而创建一个嵌套模型。</p><p id="d3f8" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">尽管这显然是一个虚构的例子，因为我们几乎包装了底层模型，而没有添加任何有用的东西(或者根本没有！)对它来说，它很好地说明了这个概念。</p><p id="8749" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在<code class="fe oj ok ol oa b"><strong class="kj ir">__init__</strong></code>方法中，我们创建了一个<strong class="kj ir">属性</strong>，它包含了我们的<strong class="kj ir">嵌套</strong> <code class="fe oj ok ol oa b"><strong class="kj ir">Linear</strong></code> <strong class="kj ir">模型</strong>。</p><p id="ac3f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在<code class="fe oj ok ol oa b"><strong class="kj ir">forward()</strong></code>方法中，我们<strong class="kj ir">调用嵌套模型本身</strong>来执行正向传递(<em class="ki">注意，我们是</em> <strong class="kj ir"> <em class="ki">而不是</em> </strong> <em class="ki">调用</em> <code class="fe oj ok ol oa b"><em class="ki">self.linear.forward(x)</em></code> <em class="ki">！</em>)。</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Building a model using PyTorch’s Linear layer</figcaption></figure><p id="532b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">现在，如果我们调用这个模型的<code class="fe oj ok ol oa b"><strong class="kj ir">parameters()</strong></code>方法，<strong class="kj ir"> PyTorch 将以递归的方式计算其属性的参数</strong>。您可以自己尝试使用类似于:<code class="fe oj ok ol oa b">[*LayerLinearRegression().parameters()]</code>的代码来获得所有参数的列表。你也可以添加新的<code class="fe oj ok ol oa b">Linear</code>属性，即使你在向前传球中根本不使用它们，它们也会<strong class="kj ir">仍然</strong>列在<code class="fe oj ok ol oa b">parameters()</code>下。</p><h2 id="0f2d" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">序列模型</h2><p id="8060" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">我们的模型足够简单…您可能会想:<em class="ki">“为什么还要费心为它构建一个类呢？!"嗯，你说得有道理…</em></p><p id="000c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">对于使用<strong class="kj ir">普通层</strong>的<strong class="kj ir">简单模型</strong>，其中一层的输出作为输入被顺序地馈送到下一层，我们可以使用一个，呃… <a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">顺序</strong> </a>模型:-)</p><p id="cc24" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">在我们的例子中，我们将建立一个带有单个参数的序列模型，也就是我们用来训练线性回归的<code class="fe oj ok ol oa b">Linear</code>层。该模型将如下所示:</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="e70f" class="ne lg iq oa b gy oe of l og oh"># Alternatively, you can use a Sequential model<br/>model = nn.Sequential(nn.Linear(1, 1)).to(device)</span></pre><p id="5b2b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">很简单，对吧？</p><h2 id="b1d0" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">训练步骤</h2><p id="fa5a" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">到目前为止，我们已经定义了一个<strong class="kj ir">优化器</strong>，一个<strong class="kj ir">损失函数</strong>和一个<strong class="kj ir">模型</strong>。向上滚动一点，快速查看循环中的代码<em class="ki">。如果我们使用<strong class="kj ir">不同的优化器</strong>，或者<strong class="kj ir">损失</strong>，甚至<strong class="kj ir">模型</strong>，会不会<strong class="kj ir">改变</strong>？如果不是，我们如何使<strong class="kj ir">更通用</strong>？</em></p><p id="36b8" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">嗯，我想我们可以说所有这些代码行<strong class="kj ir">执行一个训练步骤</strong>，给定那些<strong class="kj ir">三个元素</strong> ( <em class="ki">优化器、损耗和模型</em>)<strong class="kj ir">特性</strong>和<strong class="kj ir">标签</strong>。</p><p id="6e55" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">那么，<strong class="kj ir">编写一个函数，将这三个元素</strong>和<strong class="kj ir">作为参数，返回另一个执行训练步骤</strong>、<strong class="kj ir">、</strong>的函数，并返回相应的损失，这样如何？</p><p id="98bf" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">然后我们可以使用这个通用函数构建一个<code class="fe oj ok ol oa b"><strong class="kj ir">train_step()</strong> </code>函数，在我们的训练循环中调用。现在我们的代码应该看起来像这样…看看训练循环现在有多小？</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Building a function to perform one step of training!</figcaption></figure><p id="2094" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">让我们休息一下我们的训练循环，暂时关注一下我们的<strong class="kj ir">数据</strong>…到目前为止，我们只是简单地使用了我们的<em class="ki"> Numpy 数组</em>转<strong class="kj ir"> PyTorch 张量</strong>。但是我们可以做得更好，我们可以建立一个…</p><h1 id="2e24" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">资料组</h1><p id="63f4" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">在 PyTorch 中，<strong class="kj ir">数据集</strong>由从<a class="ae kc" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">数据集</strong> </a>类继承的常规<strong class="kj ir"> Python 类</strong>表示。你可以把它想象成一种 Python <strong class="kj ir">元组列表</strong>，每个元组对应<strong class="kj ir">一个点(特性，标签)</strong>。</p><p id="941e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">它需要实现的最基本的方法是:</p><ul class=""><li id="b9ac" class="ml mm iq kj b kk kl ko kp mf nw mh nx mj ny le mq mr ms mt bi translated"><code class="fe oj ok ol oa b"><strong class="kj ir">__init__(self)</strong></code> <strong class="kj ir"> </strong>:它采用<strong class="kj ir">构建一个<strong class="kj ir">元组列表</strong>所需的任何参数</strong>——它可能是将要加载和处理的 CSV 文件的名称；可能是<em class="ki">两个张量</em>，一个是特征，一个是标签；或者其他什么，取决于手头的任务。</li></ul><blockquote class="kd ke kf"><p id="e3a7" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在构造函数方法 ( <code class="fe oj ok ol oa b">__init__</code>)中<strong class="kj ir">不需要加载整个数据集。如果您的<strong class="kj ir">数据集很大</strong>(例如，成千上万的图像文件)，一次加载它将不会是内存高效的。建议<strong class="kj ir">按需加载</strong>(每当<code class="fe oj ok ol oa b">__get_item__</code>被调用时)。</strong></p></blockquote><ul class=""><li id="90ad" class="ml mm iq kj b kk kl ko kp mf nw mh nx mj ny le mq mr ms mt bi translated"><code class="fe oj ok ol oa b"><strong class="kj ir">__get_item__(self, index)</strong></code>:它允许数据集被<strong class="kj ir">索引</strong>，因此它可以像列表 ( <code class="fe oj ok ol oa b">dataset[i]</code>)一样工作<strong class="kj ir">——它必须<strong class="kj ir">返回一个对应于所请求数据点的元组(特征，标签)</strong>。我们可以返回我们的<strong class="kj ir">预加载的</strong>数据集或张量的<strong class="kj ir">对应切片</strong>，或者如上所述，<strong class="kj ir">按需加载</strong> <strong class="kj ir">它们</strong>(就像这个<a class="ae kc" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class" rel="noopener ugc nofollow" target="_blank">示例</a>)。</strong></li><li id="ed31" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><code class="fe oj ok ol oa b"><strong class="kj ir">__len__(self)</strong></code>:它应该简单地返回整个数据集的<strong class="kj ir">大小</strong>，这样，无论何时对它进行采样，它的索引都被限制为实际大小。</li></ul><p id="0373" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">让我们构建一个简单的自定义数据集，它采用两个张量作为参数:一个用于要素，一个用于标注。对于任何给定的索引，我们的数据集类将返回每个张量的相应切片。它应该是这样的:</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Creating datasets using train tensors</figcaption></figure><p id="772c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">再一次，你可能会想"<em class="ki">为什么要大费周章地在一个类中包含几个张量呢？</em>”。而且，再一次，你确实有一个观点…如果一个数据集只是一对张量的<strong class="kj ir">，我们可以使用 PyTorch 的<a class="ae kc" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir"> TensorDataset </strong> </a>类，它将做我们在上面的自定义数据集中所做的事情。</strong></p><blockquote class="kd ke kf"><p id="3825" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">你是否注意到我们用 Numpy 数组构建了我们的<strong class="kj ir">训练张量</strong>，但是我们<strong class="kj ir">没有将它们发送到设备</strong>？所以，他们现在是<strong class="kj ir"> CPU </strong>张量！<strong class="kj ir">为什么是</strong>？</p><p id="9b02" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">我们<strong class="kj ir">不希望我们的整个训练数据被加载到 GPU 张量</strong>中，就像我们到目前为止在我们的示例中所做的那样，因为<strong class="kj ir">会占用我们宝贵的<strong class="kj ir">显卡 RAM 中的空间</strong>。</strong></p></blockquote><p id="a322" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">好吧，好吧，但是话说回来，<strong class="kj ir">我们为什么要建立数据集呢？我们这样做是因为我们想用一种…</strong></p><h1 id="58f2" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">数据加载器</h1><p id="9ba5" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">到目前为止，我们在每个训练步骤中都使用了<strong class="kj ir">整体训练数据</strong>。一直是<strong class="kj ir">批次梯度下降</strong>。当然，这对我们<em class="ki">小得可笑的数据集</em>来说没问题，但是如果我们想要认真对待这一切，我们<strong class="kj ir">必须</strong>使用<strong class="kj ir">小批量</strong>梯度下降。因此，我们需要小批量。因此，我们需要<strong class="kj ir">相应地分割</strong>我们的数据集。要不要手动<em class="ki">做</em>？！我也没有！</p><p id="5cf0" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">所以我们使用 PyTorch 的<a class="ae kc" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir"> DataLoader </strong> </a>类来完成这项工作。我们告诉它使用哪个<strong class="kj ir">数据集</strong>(我们刚刚在上一节中构建的那个)，期望的<strong class="kj ir">小批量</strong>以及我们是否想要<strong class="kj ir">洗牌</strong>。就是这样！</p><p id="1eb1" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">我们的<strong class="kj ir">加载器</strong>将表现得像一个<strong class="kj ir">迭代器</strong>，所以我们可以<strong class="kj ir">循环遍历它</strong>并且<strong class="kj ir">每次获取一个不同的小批量</strong>。</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Building a data loader for our training data</figcaption></figure><p id="41d4" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">要检索小批量样本，只需运行下面的命令，它将返回包含两个张量的列表，一个用于要素，另一个用于标注。</p><pre class="na nb nc nd gt nz oa ob oc aw od bi"><span id="d37e" class="ne lg iq oa b gy oe of l og oh">next(iter(train_loader))</span></pre><p id="3c32" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这如何改变我们的训练循环？我们去看看吧！</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Using mini-batch gradient descent!</figcaption></figure><p id="037b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">现在有两件事情不同了:不仅我们有一个<strong class="kj ir"> </strong> <em class="ki">内循环</em>来从我们的<code class="fe oj ok ol oa b">DataLoader</code>加载每个<em class="ki">小批量</em>，更重要的是，我们现在<strong class="kj ir">只发送一个小批量到设备</strong>。</p><blockquote class="kd ke kf"><p id="9815" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">对于更大的数据集，<strong class="kj ir">使用<strong class="kj ir">数据集的</strong> <code class="fe oj ok ol oa b"><strong class="kj ir"> __get_item__</strong></code>逐个样本地加载数据</strong> <strong class="kj ir">(到<strong class="kj ir"> CPU </strong>张量中)，然后<strong class="kj ir">将属于同一<strong class="kj ir">小批量的所有样本</strong>一次发送到您的 GPU </strong>(设备)是为了让<strong class="kj ir">最好地利用您的显卡 RAM </strong>。</strong></p><p id="9000" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">此外，如果您有许多 GPU 来训练您的模型，最好保持您的数据集“不可知”，并在训练期间将批处理分配给不同的 GPU。</p></blockquote><p id="6578" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">到目前为止，我们只关注了<strong class="kj ir">训练</strong> <strong class="kj ir">数据</strong>。我们为它构建了一个<em class="ki">数据集</em>和一个<em class="ki">数据加载器</em>。我们可以对<strong class="kj ir">验证</strong>数据做同样的事情，使用我们在这篇文章开始时执行的<strong class="kj ir">分割</strong>…或者我们可以使用<code class="fe oj ok ol oa b">random_split</code>来代替。</p><h2 id="82af" class="ne lg iq bd lh nf ng dn ll nh ni dp lp mf nj nk lt mh nl nm lx mj nn no mb np bi translated">随机分裂</h2><p id="46f8" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">PyTorch 的<code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">random_split()</strong></a></code>方法是执行<strong class="kj ir">训练-验证分割</strong>的一种简单而熟悉的方式。请记住，在我们的示例中，我们需要将它应用到<strong class="kj ir">整个数据集</strong> ( <em class="ki">不是我们在前面两节中构建的训练数据集</em>)。</p><p id="e1a8" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">然后，对于每个数据子集，我们构建一个相应的<code class="fe oj ok ol oa b">DataLoader</code>，因此我们的代码看起来像这样:</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Splitting the dataset into training and validation sets, the PyTorch way!</figcaption></figure><p id="c182" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">现在我们有了一个<strong class="kj ir">数据加载器</strong>用于我们的<strong class="kj ir">验证集</strong>，所以，用它来…</p><h1 id="5017" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">估价</h1><p id="6d13" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">这是我们旅程的最后<strong class="kj ir">部分——我们需要改变训练循环，以包括对我们的模型</strong>的<strong class="kj ir">评估，即计算<strong class="kj ir">验证损失</strong>。第一步是包括另一个内部循环来处理来自<em class="ki">验证加载器</em>的<em class="ki">小批量</em>，将它们发送到与我们的模型相同的<em class="ki">设备</em>。接下来，我们使用我们的模型(第 23 行)进行<strong class="kj ir">预测</strong>，并计算相应的<strong class="kj ir">损失</strong>(第 24 行)。</strong></p><p id="fb1b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这就差不多了，但是还有<strong class="kj ir">两个小</strong>，<strong class="kj ir">一个大</strong>，要考虑的事情:</p><ul class=""><li id="c9f1" class="ml mm iq kj b kk kl ko kp mf nw mh nx mj ny le mq mr ms mt bi translated"><code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">torch.no_grad()</strong></a></code>:尽管在我们的简单模型中不会有什么不同，但是用这个<strong class="kj ir">上下文管理器<strong class="kj ir">包装验证</strong>内部循环来禁用您可能无意中触发的任何梯度计算</strong>是一个<strong class="kj ir">良好实践</strong>，因为<strong class="kj ir">梯度属于训练</strong>，而不属于验证步骤；</li><li id="2621" class="ml mm iq kj b kk mu ko mv mf mw mh mx mj my le mq mr ms mt bi translated"><code class="fe oj ok ol oa b"><a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">eval()</strong></a></code>:它唯一做的事情就是<strong class="kj ir">将模型设置为评估模式</strong>(就像它的<code class="fe oj ok ol oa b">train()</code>对应的那样)，这样模型就可以调整它关于一些操作的行为，比如<a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir"/></a>。</li></ul><p id="5345" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">现在，我们的训练循环应该是这样的:</p><figure class="na nb nc nd gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Computing validation loss</figcaption></figure><p id="1211" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">T21，我们还有什么可以改进或改变的吗？当然，总会有其他东西<strong class="kj ir">添加到你的模型中——例如，使用<a class="ae kc" href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">学习速率调度器</strong> </a>。但是这个帖子已经<em class="ki">太长了</em>，所以我就停在这里。</strong></p><p id="8dcf" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">包含所有附加功能的完整工作代码在哪里？“你问什么？你可以在这里 找到<a class="ae kc" href="https://gist.github.com/dvgodoy/1d818d86a6a0dc6e7c07610835b46fe4" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">。</strong></a></p><h1 id="36e3" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">最后的想法</h1><p id="b4ac" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">虽然这篇文章比我开始写它时预期的长得多，但我不会让它有任何不同——我相信它有大多数必要的步骤，以一种结构化的方式学习 T42，如何使用 PyTorch 开发深度学习模型。</p><p id="1dd3" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">希望在完成这篇文章中的所有代码后，你能够更好地理解 PyTorch 的官方<a class="ae kc" href="https://pytorch.org/tutorials/" rel="noopener ugc nofollow" target="_blank">教程</a>。</p><blockquote class="kd ke kf"><p id="72d5" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">更新(2021 年 5 月 18 日):今天我已经<strong class="kj ir">完成了</strong>我的书:<a class="ae kc" href="https://leanpub.com/pytorch/" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">深度学习用 PyTorch 循序渐进:入门指南</strong> </a> <strong class="kj ir">。</strong></p><p id="81cd" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">更新(2022 年 2 月 23 日):<strong class="kj ir">平装版</strong>现已上市(三册)。更多详情请查看<a class="ae kc" href="https://pytorchstepbystep.com" rel="noopener ugc nofollow" target="_blank"><strong class="kj ir">pytorchstepbystep.com</strong></a>。</p><p id="cfc2" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">更新(2022 年 7 月 19 日):第一卷《基础》的<strong class="kj ir">西班牙语版</strong>现已在<a class="ae kc" href="https://leanpub.com/pytorch_ES" rel="noopener ugc nofollow" target="_blank"> Leanpub </a>上发布。</p></blockquote><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pe"><img src="../Images/c714343a97ac3a5ed8d6517d9a338fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rhu3mqLSod3PRG1FJu7UyQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://pytorchstepbystep.com" rel="noopener ugc nofollow" target="_blank">Deep Learning with PyTorch Step-by-Step</a></figcaption></figure><p id="d580" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><em class="ki">如果您有任何想法、意见或问题，请在下方留言或联系我</em> <a class="ae kc" href="https://twitter.com/dvgodoy" rel="noopener ugc nofollow" target="_blank"> <em class="ki">推特</em> </a> <em class="ki">。</em></p></div></div>    
</body>
</html>