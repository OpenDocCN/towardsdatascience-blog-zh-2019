<html>
<head>
<title>Introduction to Actor Critic in Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习中的演员评论介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-actor-critic-7642bdb2b3d2?source=collection_archive---------10-----------------------#2019-06-17">https://towardsdatascience.com/introduction-to-actor-critic-7642bdb2b3d2?source=collection_archive---------10-----------------------#2019-06-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a812" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">直观的方法理解强化学习中最重要的方法之一。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b38e2a289bca2ad03a1838320d203d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zflYb1EO_mnpZs4n"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@fatihkilic?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Fatih Kılıç</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fed1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新</strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae ky" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="69ad" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">概观</h1><p id="43b4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在深入探讨演员评论家的细节之前，让我们提醒自己一下<a class="ae ky" rel="noopener" target="_blank" href="/policy-based-reinforcement-learning-the-easy-way-8de9a3356083">政策梯度</a>。</p><p id="5353" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于策略的强化学习意味着什么？简单地说，想象一个机器人发现自己处于某种状况，但这种状况似乎与它以前经历过的事情相似。<br/>因此，基于策略的方法说:既然我过去在这种特定情况下采取了行动<strong class="lb iu"> <em class="ms"> (a) </em> </strong>，那么让我们这次也尝试同样的行动。</p><p id="3be8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PS。不要把相似的情况和相同的状态混为一谈，在相似的情况下，机器人或智能体处于一些新的状态，类似于它以前经历过的，而不一定是完全相同的状态。</p><div class="kj kk kl km gt ab cb"><figure class="mt kn mu mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/9cb2b98fdf4472aff54c514075a41c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*kWmw1LMcwG8eptMrBm193A.png"/></div></figure><figure class="mt kn mz mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/ff749555f8b6f31fbb1eecb9bfd29e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*Jk73ry_ANKlx2jOVgXI6Vg.png"/></div></figure><figure class="mt kn na mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/7f61457c7ed537fca9bd7a536b040b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*GU8A1OvmVHj_vbYqfL1MOQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk nb di nc nd">Left: The robot faces a barrier, but he has seen something like that before and it knows what to do in this situation. Middle: The robot took the same action it took in such situation. Right: The robot is wondering hindsight if there were more efficient way to overcome the barrier.</figcaption></figure></div><p id="eef9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的图像中，一个机器人面对着一个壁垒，但它似乎以前见过这种情况，所以像以前一样，它用火箭跳过它。</p><p id="32e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这一次可能不是完美的行动！<br/>可能发生的情况是，有一个简单的触发器来打开门，从而节省宝贵的能量，这些能量可能在以后被使用。</p><p id="9644" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于这个概念，机器人必须做一些回顾性的回顾，以检查那个动作(使用他的火箭)有多有用。</p><p id="18b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们需要把它转化成一个数学方程。</p><h1 id="7ae2" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">目标</h1><p id="280d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">请记住，我们的目标是始终获得最高的回报。<br/>为了能够做到这一点，我们必须定义一个收集这些奖励的函数，并对其进行优化，以最大化这些奖励。<br/>另一个同样重要的点是，我们将使用神经网络来完成这项工作。<br/>所以我们要做的是找到神经网络的一组权重𝜽，帮助我们<strong class="lb iu">最大化</strong>目标函数。</p><p id="8f3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">重要！</strong>注意以下陷阱:如果你熟悉深度学习和神经网络，你更习惯于寻找使误差最小的权重。在这种情况下，我们不这样做，相反，我们希望最大化一个目标函数。意识不到这一点会让你陷入困惑。</p><p id="d869" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从基于政策的梯度中，我们找到了一个目标函数，并按如下方式计算其梯度(有关该等式如何形成的详细信息，请查看<a class="ae ky" rel="noopener" target="_blank" href="/policy-gradient-step-by-step-ac34b629fd55">政策梯度逐步</a>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/2fe96d0205f7800c0e05c1c6fe1dd424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7NhkED-5ZM-yfe2ocTsSLA.png"/></div></div></figure><p id="37c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">J(𝜽)是依赖于𝜽的目标函数<br/> m 是执行的情节(这里称为轨迹)的数量<br/> 𝛑是由𝜽参数化的政策，这意味着当𝜽变化时，政策将受到影响。请记住，策略给出了当代理处于特定状态时采取特定操作的概率。<br/> 𝞽ⁱ是执行的第 I 集或轨迹。<br/> R(𝞽ⁱ)是轨迹𝞽ⁱ.的回报(总报酬)<br/> T 是轨迹𝞽ⁱ.的步数</p><p id="288f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个方程告诉我们的是，J(𝜽的梯度是所有 m 个轨迹的平均值，其中每个轨迹是组成它的步骤的总和。在每一步，我们计算保单𝛑对数的导数，并将其乘以回报 R(𝞽ⁱ).</p><p id="232a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，我们正试图找出𝜽.之后政策的变化返回 R(𝞽ⁱ的使用)是为了放大(或减小)这些变化。这意味着如果 R(𝞽ⁱ)很高，它将提高结果，并使神经网络确信𝜽在正确的方向上前进。</p><p id="e291" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，返回 R(𝞽ⁱ).有一个问题</p><h1 id="0a5b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">退货问题</h1><p id="9b65" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">假设我们在一个轨迹中，早期的回报是负的，而接近轨迹末端的回报是正的，这样总 R = 0！<br/>在这种情况下，∇J(𝜽)将为 0，并且神经网络将不会从这种情况中学习𝜽的任何新值。<br/>为了解决这个问题，我们在每一步都使用了折扣奖励，从当前状态开始，直到轨迹结束。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/09cbc736eaa3b2f401900378dcad1212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mbJh0Jo_jVTUx4KGEEuNtA.png"/></div></div></figure><p id="2bfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将给我们一个新版本的∇J(𝜽):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/a13d7fbe79a716614f4fa47f0ad98c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iQ6sNGd8GR9GXkT6kWlv1g.png"/></div></div></figure><p id="37f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在神经网络将能够从轨迹中学习。还有更大的改进空间，但我们需要做更多的数学计算。</p><h1 id="5edd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数学来了</h1><p id="4ded" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如果我们深入思考呢？<br/>Rt(从步骤 t 开始返回)还不错，但是我们不确定 Rt 的值是多少，好到可以考虑？！</p><p id="b146" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以文章开头的机器人为例，使用火箭跳跃是一个好的解决方案吗？我们怎么知道？有多好？<br/>同样，如果你拿 100 米短跑运动员 10 秒的成绩举例，他做得好还是不好？如果它是好的，那么它有多好？<br/>赋予这个数字意义的一种方法是将其与一个参考值进行比较，或者我们称之为<strong class="lb iu">基线</strong>。</p><p id="34cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基线可以有几种形式，其中一种是预期绩效，或者说是平均绩效。如果短跑运动员得了 10 分，但平均分是 12 分，那么他做得很好，相反，如果平均分是 8 分，那么他做得很差。</p><p id="cdbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将基线表示为<strong class="lb iu"> b </strong> 𝑡，目标函数的梯度变为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/d548eb06b4d141743836ed8d4ae6f62e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X_GmK-UGN7lSXmLdezGu_Q.png"/></div></div></figure><p id="3ae9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们想一想，R𝑡是在步骤 t 采取行动<strong class="lb iu"><em class="ms"/></strong><br/>的回报，另一方面<strong class="lb iu"> b </strong> 𝑡代表所有行动结果的平均值。<br/>这看起来很奇怪地类似于 Q(s，a)是在状态<strong class="lb iu"><em class="ms"/></strong>采取的行动<strong class="lb iu"><em class="ms"/></strong>的值，以及 V(s)是状态的值，或者是在状态<strong class="lb iu"><em class="ms"/></strong>采取的所有行动(引起的)的平均值。</p><p id="049c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该等式可以改写为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/cacbd2f284bd08f3d356c56dae0cd292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4d8UiquTp64soEMBepenew.png"/></div></div></figure><p id="70bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们仔细观察上面的等式，我们会看到𝛑(a|s)是执行动作的(记住𝛑是动作的概率<strong class="lb iu"><em class="ms"/></strong>是在状态<strong class="lb iu"><em class="ms"/></strong>时获得的)，而 Q(s，a)-V(s)告诉我们它有多有价值。</p><p id="64ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，𝛑(a|s 是演员，Q(s，a)-V(s)是评论家。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/733815c6c229ac4a14c9df1747b9b122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*84w1XhFvFM8eckM3anR-2g.png"/></div></div></figure><p id="691d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">批评家的计算可以有不同的味道:</p><ul class=""><li id="fee2" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated">演员兼评论家</li><li id="0e0e" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">优势演员兼评论家</li><li id="10f7" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">TD 演员兼评论家</li><li id="f4f6" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">演员兼评论家</li><li id="7b17" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">天生的演员兼评论家</li></ul><h1 id="bf4d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">相关文章</h1><ul class=""><li id="775f" class="nj nk it lb b lc mn lf mo li nx lm ny lq nz lu no np nq nr bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/policy-gradient-step-by-step-ac34b629fd55">政策梯度循序渐进</a></li><li id="c4c1" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/function-approximation-in-reinforcement-learning-85a4864d566">强化学习中的函数逼近</a></li><li id="489b" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/policy-based-reinforcement-learning-the-easy-way-8de9a3356083">基于策略的强化学习</a></li></ul></div></div>    
</body>
</html>