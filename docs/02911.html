<html>
<head>
<title>Review: QSA+QNT — Quantization of Fully Convolutional Networks (Biomedical Image Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:QSA+qnt——全卷积网络的量子化(生物医学图像分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-qsa-qnt-neural-network-with-incremental-quantization-biomedical-image-segmentation-d9713daf9e0d?source=collection_archive---------12-----------------------#2019-05-11">https://towardsdatascience.com/review-qsa-qnt-neural-network-with-incremental-quantization-biomedical-image-segmentation-d9713daf9e0d?source=collection_archive---------12-----------------------#2019-05-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f183" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对神经网络进行增量量化，作为正则项，减少过拟合</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d1ad11ff9b7021dbb384d9598aa011cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TfSw4JgOpH2KHrWMmEVmdA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">A Photo Taken by Me in the Seminar Talk by Author Dr. Yiyu Shi</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kw"><img src="../Images/c9099316f37da7df9d3a585268f877f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*cuOEXJX2MIq6DzNI8Vs0qw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">More photos</strong></figcaption></figure><p id="542c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di">在</span>这个故事中，回顾了<strong class="kz ir">圣母大学</strong>和<strong class="kz ir">华中科技大学</strong>的一篇名为“<strong class="kz ir">量子化全卷积网络用于精确生物医学图像分割</strong>”的论文。我只是在故事标题中称之为<strong class="kz ir"> QSA+QNT </strong>自从<strong class="kz ir">量化应用于</strong><a class="ae mc" rel="noopener" target="_blank" href="/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6"><strong class="kz ir">【SA】</strong></a><strong class="kz ir">和网络训练(NT) </strong>。这是一篇<strong class="kz ir"> 2018 CVPR </strong>中<strong class="kz ir">超过 10 次引用</strong>的论文。(<a class="md me ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----d9713daf9e0d--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="2078" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这是我参加的研讨会演讲，让我开始阅读关于生物医学图像分割的深度学习论文。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="4cc7" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">概述</h1><ol class=""><li id="f844" class="ne nf iq kz b la ng ld nh lg ni lk nj lo nk ls nl nm nn no bi translated"><strong class="kz ir">简评</strong> <a class="ae mc" rel="noopener" target="_blank" href="/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6"> <strong class="kz ir">提示性注释【SA】</strong></a></li><li id="c066" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls nl nm nn no bi translated"><strong class="kz ir">对</strong> <a class="ae mc" rel="noopener" target="_blank" href="/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6"> <strong class="kz ir"> SA </strong> </a> <strong class="kz ir">架构</strong>的修改</li><li id="77b8" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls nl nm nn no bi translated"><strong class="kz ir">量化的选择</strong></li><li id="4611" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls nl nm nn no bi translated"><strong class="kz ir">消融研究</strong></li><li id="669a" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls nl nm nn no bi translated"><strong class="kz ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="8d73" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">1.简要回顾<a class="ae mc" rel="noopener" target="_blank" href="/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6"> <strong class="ak">暗示性注释【SA】</strong></a></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/3b2bb35b5b972f99abac251cff0b9b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*55hoaEMPAeKsJ3XDWxMLwQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Brief Review of SA</strong></figcaption></figure><ul class=""><li id="b3de" class="ne nf iq kz b la lb ld le lg nv lk nw lo nx ls ny nm nn no bi translated"><strong class="kz ir">步骤 A </strong>:首先，<strong class="kz ir"> 10%的样本经过专家</strong>标注，送入全卷积网络(<a class="ae mc" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>)进行训练。</li><li id="3778" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">步骤 B </strong>:然后，<strong class="kz ir">多个</strong><a class="ae mc" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"><strong class="kz ir">fcn</strong></a><strong class="kz ir">用自举训练</strong>。</li><li id="77a7" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">步骤 C </strong>:使用训练好的<a class="ae mc" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCNs </a>对<strong class="kz ir">未标注样本</strong>进行分割。</li><li id="56ce" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">步骤 D &amp; E </strong>:使用标准差的不确定性度量和使用余弦相似性的相似性估计用于<strong class="kz ir">在多个</strong><a class="ae mc" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"><strong class="kz ir">fcn</strong></a><strong class="kz ir">中选择不确定但对专家来说与数据集相似的样本。</strong></li><li id="98ab" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">步骤 F(步骤 A) </strong> : <strong class="kz ir">专家再次标注 10%的样本。</strong>但这次，这些样本是 <a class="ae mc" rel="noopener" target="_blank" href="/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6"> <strong class="kz ir"> SA </strong> </a>中的不确定性度量和相似性估计所建议的<strong class="kz ir">。</strong></li><li id="7775" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">只有 50%的训练数据，<a class="ae mc" rel="noopener" target="_blank" href="/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6"> SA </a>使用 100%的训练数据输出或接近最先进的方法。</li><li id="766d" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">因此，通过这种方式，专家可以从注释过多的样本中解放出来，即减少了注释工作，降低了成本，并且节省了时间成本。</li><li id="d222" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">更多详情，请阅读我关于<a class="ae mc" rel="noopener" target="_blank" href="/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6"> SA </a>的评论。</li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="6da1" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">2.<strong class="ak">对</strong> <a class="ae mc" rel="noopener" target="_blank" href="/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6"> SA </a> <strong class="ak">架构</strong>的修改</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/eef3189aea993b36ebb272af9ca9a8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*YN2QZMVxLs7LL5OcN5OXqA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">The New Part Comparing with </strong><a class="ae mc" rel="noopener" target="_blank" href="/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6"><strong class="bd kv">SA</strong></a></figcaption></figure><ul class=""><li id="07c8" class="ne nf iq kz b la lb ld le lg nv lk nw lo nx ls ny nm nn no bi translated">本文将《FCN》原著分为提示性的 FCN 和分割性的 FCN。</li><li id="bf3f" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">提示性 FCN </strong>:仅用于不确定性度量和相似性估计。</li><li id="cf64" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">分段 FCN(新)</strong>:仅用于分段。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/d32626d2dbc661c24dc0c31d560fce92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bQZjYQcoInUSEGmFqgRXFQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Quantization on FCNs</strong></figcaption></figure><ul class=""><li id="bb61" class="ne nf iq kz b la lb ld le lg nv lk nw lo nx ls ny nm nn no bi translated">最初，权重用 32 位表示。</li><li id="b6b6" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">现在，<strong class="kz ir">量化</strong>被引入到提示性 FCN 和分割 FCN 中(细节在后面的部分)。</li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="db38" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated"><strong class="ak"> 3。量化的选择</strong></h1><h2 id="9c49" class="ob mn iq bd mo oc od dn ms oe of dp mw lg og oh my lk oi oj na lo ok ol nc om bi translated"><strong class="ak"> 3.1。量化</strong></h2><ul class=""><li id="c60d" class="ne nf iq kz b la ng ld nh lg ni lk nj lo nk ls ny nm nn no bi translated"><strong class="kz ir">在保持可接受精度的同时，用较少的内存(精度)表示重量。</strong></li><li id="418d" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">在硬件上启用低位宽 NN 的训练加速</strong>。</li></ul><h2 id="4638" class="ob mn iq bd mo oc od dn ms oe of dp mw lg og oh my lk oi oj na lo ok ol nc om bi translated">3.2.DoReFa-Net(超过 300 次谷歌引用)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/7b8e1ec4173d303748049c1c3bd25b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*rrN5KHfyEwPEw7pnhvlKvw.png"/></div></figure><ul class=""><li id="af9d" class="ne nf iq kz b la lb ld le lg nv lk nw lo nx ls ny nm nn no bi translated"><strong class="kz ir"> (+1，-1) </strong>根据值是大于还是小于均值来赋值。</li><li id="5ffb" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">从 32 位量化到<strong class="kz ir"> 1 位</strong>。</li></ul><h2 id="e53d" class="ob mn iq bd mo oc od dn ms oe of dp mw lg og oh my lk oi oj na lo ok ol nc om bi translated">3.3.三元权重网络(TWN)(超过 200 次谷歌引用)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/4f877332e866c81aac1cf563ab751703.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*3Or0Uw0n0qoLlD60mOzgEA.png"/></div></figure><ul class=""><li id="0c0a" class="ne nf iq kz b la lb ld le lg nv lk nw lo nx ls ny nm nn no bi translated"><strong class="kz ir"> (+α，-α，0) </strong>根据数值是否接近、大于或小于均值来赋值。</li><li id="0066" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">从 32 位量化到<strong class="kz ir"> 2 位</strong>。</li></ul><h2 id="864b" class="ob mn iq bd mo oc od dn ms oe of dp mw lg og oh my lk oi oj na lo ok ol nc om bi translated">3.4.增量量化(INQ)(超过 200 次谷歌引用)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/a15f9a9c4aea10313b421cba1a02d5ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*Kw3eLD163o-5x2vjNFO27Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">An Illustrative Example Provided by Authors During the Seminar Talk</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/da17084419b4f631cb795e91450ecc7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*DIzLF3o2gMND_e6iOi2vMA.png"/></div></figure><ul class=""><li id="95c2" class="ne nf iq kz b la lb ld le lg nv lk nw lo nx ls ny nm nn no bi translated">量化到两个的<strong class="kz ir">次方即可。</strong></li><li id="a7d6" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">我们发现，有时量化可以提高精度</li><li id="8547" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">在[1]中，它将 ImageNet 分类的 Top-1 错误提高了 0.01%</li><li id="0557" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">在[2]中，它对 ImageNet 分类的 Top-1 和 Top-5 错误提高了 0.2%-1.47%。</li></ul><blockquote class="or"><p id="a9d1" class="os ot iq bd ou ov ow ox oy oz pa ls dk translated">最后，本文采用了<strong class="ak">增量量化(INQ)</strong>。</p></blockquote></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="bb86" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">4.消融研究</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/93a223a16f9dc72d896db112715f0d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yayyqReQj9QyrE-5t0jk6g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Fixed to 5 FCNs for suggestive FCN and 1 FCN for segmentation FCN (<em class="pc">F</em> is 32-bit)</strong></figcaption></figure><ul class=""><li id="54fb" class="ne nf iq kz b la lb ld le lg nv lk nw lo nx ls ny nm nn no bi translated">建议 FCN (QSA)的 n 位量化，分段 FCN (NT/QNT)的 7 位量化</li><li id="ef13" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">无论有无量化，使用 7 位量化性能最佳。</strong></li><li id="e0fc" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">但是对分割 FCN (QNT)的量化并不总是能提高精度。</strong></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/7e82e92351249f59552713643179b54c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ctCE_941ZFkTTGpv8YfVA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Fixed to 5 FCNs for suggestive FCN and 1 FCN for segmentation FCN (<em class="pc">F</em> is 32-bit)</strong></figcaption></figure><ul class=""><li id="4527" class="ne nf iq kz b la lb ld le lg nv lk nw lo nx ls ny nm nn no bi translated">分段 FCN (QNT)上的 n 位量化，带有/不带有暗示 FCN(南非/QSA)上的 7 位量化</li><li id="2251" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">无论有无量化，使用 7 位量化性能最佳。</strong></li><li id="f8cc" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">对暗示性 FCN (QSA)的量化总是比没有量化的(SA)具有更高的准确性。</strong></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi gj"><img src="../Images/9e1c208f689910c97951dbcda65ac220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dt1gSrnJxY2UKMAlLK2oxQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">When 5 FCNs are used at both sides</strong></figcaption></figure><ul class=""><li id="fcc0" class="ne nf iq kz b la lb ld le lg nv lk nw lo nx ls ny nm nn no bi translated">分段 FCN (QNT)上的 n 比特量化，带有/不带有建议 FCN (SA)上的 7 比特量化。</li><li id="b88c" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">无论有无量化，使用 7 位量化都具有最佳性能。</li><li id="9733" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">对分段 FCN (QNT)的 7 比特量化+对暗示 FCN (QNT)的 7 比特量化具有稍好的性能。</strong></li><li id="8aaa" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">(论文中有更多结果，有兴趣请看论文。)</li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="5ca3" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">5.<strong class="ak">与最先进方法的比较</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/ab7b5bc7ace931911af6bc05f93e4e34.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*k3jl__PDSAenUKd075oQyg.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/a95e5923e0e9bdded34376cb250cfc31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DS6wc-zFKHf3fRt5xKURoA.png"/></div></div></figure><ul class=""><li id="8edb" class="ne nf iq kz b la lb ld le lg nv lk nw lo nx ls ny nm nn no bi translated"><strong class="kz ir"> 7 位量化对 5 分段 FCN (QNT) </strong>和<strong class="kz ir"> 7 位量化对 5 暗示 FCN (QNT)的表现优于</strong> <a class="ae mc" rel="noopener" target="_blank" href="/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6"> <strong class="kz ir"> SA </strong> </a> <strong class="kz ir">、</strong> <a class="ae mc" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc"> <strong class="kz ir">多通道</strong> </a> <strong class="kz ir">(会议和事务两个版本)，以及</strong><a class="ae mc" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener"><strong class="kz ir">cumed vision 2/DCAN</strong></a><strong class="kz ir">。</strong></li><li id="f2e7" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated"><strong class="kz ir">7 位和 5 位量化的内存分别减少 4.6 倍和 6.4 倍</strong>。</li><li id="1399" class="ne nf iq kz b la np ld nq lg nr lk ns lo nt ls ny nm nn no bi translated">这就像一个正则项。</li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><p id="721d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">结论是，量化的建议性标注可以应用于其他数据有限或标注成本较高的问题。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h2 id="2d86" class="ob mn iq bd mo oc od dn ms oe of dp mw lg og oh my lk oi oj na lo ok ol nc om bi translated">参考</h2><p id="e5dd" class="pw-post-body-paragraph kx ky iq kz b la ng jr lc ld nh ju lf lg pg li lj lk ph lm ln lo pi lq lr ls ij bi translated">【2018 CVPR】【QSA+qnt】<br/><a class="ae mc" href="https://arxiv.org/abs/1803.04907" rel="noopener ugc nofollow" target="_blank">量子化全卷积网络精确生物医学图像分割</a></p><h2 id="df72" class="ob mn iq bd mo oc od dn ms oe of dp mw lg og oh my lk oi oj na lo ok ol nc om bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kx ky iq kz b la ng jr lc ld nh ju lf lg pg li lj lk ph lm ln lo pi lq lr ls ij bi translated">)(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(?)(我)(们)(都)(不)(在)(这)(些)(情)(况)(上)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(况)(。 [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]</p><p id="8b77" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">物体检测<br/></strong><a class="ae mc" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae mc" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae mc" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae mc" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae mc" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae mc" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae mc" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae mc" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae mc" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae mc" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4">G-RMI</a>][<a class="ae mc" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae mc" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae mc" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae mc" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae mc" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae mc" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3</a>[<a class="ae mc" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>[<a class="ae mc" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a>[<a class="ae mc" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a></p><p id="6582" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">语义切分<br/></strong><a class="ae mc" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae mc" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae mc" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae mc" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae mc" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae mc" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae mc" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae mc" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae mc" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae mc" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生物医学图像分割<br/></strong>[<a class="ae mc" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae mc" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae mc" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae mc" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae mc" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae mc" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae mc" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae mc" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>][<a class="ae mc" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1">M FCN</a></p><p id="3134" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> 实例分割 <br/> </strong> <a class="ae mc" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a> <a class="ae mc" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979"> Hypercolumn </a> <a class="ae mc" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a> <a class="ae mc" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a> <a class="ae mc" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413"> MultiPathNet </a> <a class="ae mc" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> <a class="ae mc" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> InstanceFCN </a> <a class="ae mc" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></p><p id="58de" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">人体姿态估计</strong><br/><a class="ae mc" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">深度姿态</a><a class="ae mc" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">汤普逊·尼普斯 14</a><a class="ae mc" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">汤普逊·CVPR 15</a></p></div></div>    
</body>
</html>