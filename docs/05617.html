<html>
<head>
<title>Ray Tune: a Python library for fast hyperparameter tuning at any scale</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Ray Tune:一个 Python 库，可以在任意比例下快速调整超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fast-hyperparameter-tuning-at-scale-d428223b081c?source=collection_archive---------2-----------------------#2019-08-18">https://towardsdatascience.com/fast-hyperparameter-tuning-at-scale-d428223b081c?source=collection_archive---------2-----------------------#2019-08-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ad6b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在不改变代码的情况下，将您的搜索从笔记本电脑扩展到数百台机器。 <a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune.html" rel="noopener ugc nofollow" target="_blank"> <em class="ki">验出雷调</em> </a> <em class="ki">。</em></h2></div><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi kk"><img src="../Images/9fcb97abf8d125930eab28c52c730150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bdnd8Y6fv8xeFqNuXvSeNA.png"/></div></div></figure><p id="2c64" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">如果你曾经试图调整机器学习模型的超参数，你就会知道这可能是一个非常痛苦的过程。简单的方法很快变得耗时。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ls"><img src="../Images/5b06b5e7805dc136cec3dfbb75a184be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*L3E67jSnLTCxy2ks"/></div></div></figure><p id="d0ee" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">现在，您比以往任何时候都更需要<strong class="ky iu">尖端的超参数调整工具来跟上最先进的技术。</strong></p><p id="cf1d" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">模型的进步越来越依赖于更新更好的超参数调整算法，如<strong class="ky iu">基于人口的训练</strong> ( <strong class="ky iu"> PBT </strong>)、HyperBand 和 ASHA。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi lt"><img src="../Images/feb2215581a29a95426d2621d43a20bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Cp7gPWoQQzIgc184"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Population-based Training improves DeepMind’s state-of-the-art algorithms on many domains by significant margins. Source: <a class="ae kj" href="https://deepmind.com/blog/population-based-training-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://deepmind.com/blog/population-based-training-neural-networks/</a></figcaption></figure><p id="91b0" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">这些算法提供了两个关键优势:</p><ul class=""><li id="d5a4" class="ly lz it ky b kz la lc ld lf ma lj mb ln mc lr md me mf mg bi translated"><strong class="ky iu">他们将模型性能</strong>最大化:例如，DeepMind 使用 PBT<a class="ae kj" href="https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/" rel="noopener ugc nofollow" target="_blank">在星际争霸</a>上实现超人性能；Waymo 使用<a class="ae kj" href="https://www.technologyreview.com/s/614004/deepmind-is-helping-waymo-evolve-better-self-driving-ai-algorithms/" rel="noopener ugc nofollow" target="_blank"> PBT 来启用自动驾驶汽车</a>。</li><li id="9924" class="ly lz it ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated"><strong class="ky iu">他们最大限度地降低培训成本</strong> : <a class="ae kj" href="https://determined.ai/blog/addressing-challenges-parallel-hyperparameter-optimization/" rel="noopener ugc nofollow" target="_blank"> HyperBand 和 ASHA 收敛到高质量配置</a>所需时间是以前方法的一半；<a class="ae kj" href="https://arxiv.org/abs/1905.05393" rel="noopener ugc nofollow" target="_blank">基于人口的数据增强算法</a>将成本削减了几个数量级。</li></ul><p id="a43d" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">事实是，绝大多数研究人员和团队没有利用这样的算法。大多数现有的超参数搜索框架没有这些更新的优化算法。一旦达到一定规模，大多数现有的并行超参数搜索解决方案可能会很难使用——您需要为每次运行配置每台机器，并且通常需要管理单独的数据库。</p><p id="777e" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">实际上，实现和维护这些算法需要大量的时间和工程。</p><p id="bdbf" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">但是不需要这样。没有理由为什么您不能轻松地将超参数调整集成到您的机器学习项目中，在您的集群中的 8 个 GPU 上无缝地运行并行异步网格搜索，并在云上大规模利用基于群体的训练或任何贝叶斯优化算法。</p><p id="5e64" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">在这篇博文中，我们将介绍<a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune.html" rel="noopener ugc nofollow" target="_blank"> RayTune，这是一个强大的超参数优化库</a>，旨在消除缩放实验执行和超参数搜索之间的摩擦。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi mn"><img src="../Images/174831636f18c63fb9174b1c0ca12a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EZKV8RTgDt0NfL49"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Tune scales your training from a single machine to a large distributed cluster without changing your code.</figcaption></figure><h2 id="f967" class="mo mp it bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">RayTune 是一个强大的库，可以加速超参数优化。以下是一些核心功能:</h2><ul class=""><li id="3510" class="ly lz it ky b kz nh lc ni lf nj lj nk ln nl lr md me mf mg bi translated">RayTune 提供了现成的分布式异步优化。</li><li id="b7d4" class="ly lz it ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated">RayTune 提供最先进的算法，包括(但不限于)<a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-schedulers.html#asynchronous-hyperband" rel="noopener ugc nofollow" target="_blank">【ASHA】</a><a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-searchalg.html#bohb" rel="noopener ugc nofollow" target="_blank">【BOHB】</a><a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-schedulers.html#population-based-training-pbt" rel="noopener ugc nofollow" target="_blank">基于群体的训练</a>。</li><li id="2084" class="ly lz it ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated">您可以将 RayTune 超参数搜索从单台机器扩展到大型分布式集群，而无需更改您的代码。</li><li id="fcc2" class="ly lz it ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated">RayTune 集成了许多优化库，如<a class="ae kj" href="http://ax.dev" rel="noopener ugc nofollow" target="_blank"> Ax/Botorch </a>、<a class="ae kj" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank"> HyperOpt </a>和<a class="ae kj" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a>，使您能够透明地扩展它们。</li><li id="a580" class="ly lz it ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated">RayTune 支持任何机器学习框架，包括 PyTorch、TensorFlow、XGBoost、LightGBM、scikit-learn 和 Keras。</li></ul><p id="0a9c" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">除了 RayTune 的核心特性之外，研究人员和开发人员更喜欢 RayTune 而不是其他现有超参数调优框架的两个主要原因是:<strong class="ky iu">规模</strong>和<strong class="ky iu">灵活性</strong>。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi mn"><img src="../Images/36aacb7d4921cd9d23ebbceb2c0caab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mxyRF_di4Bt7BX9-"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Note for Search Algorithms: as of 8/12/2019, HpBandSter supports HyperBand, Random Search, and BOHB. KerasTuner supports Random Search, HyperBand, and Bayesian Optimization. Optuna supports Median (Percentile) Stopping, ASHA, Random Search, and Bayesian Optimization (TPE). HyperOpt supports Bayesian Optimization and Random Search. Tune supports PBT, BOHB, ASHA, HyperBand, Median Stopping, Random Search, Bayesian Optimization (TPE, etc), and numerous others due to library integrations.</figcaption></figure><h2 id="2a34" class="mo mp it bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">调整简化了缩放。</h2><p id="37b4" class="pw-post-body-paragraph kw kx it ky b kz nh ju lb lc ni jx le lf nm lh li lj nn ll lm ln no lp lq lr im bi translated">通过添加不到 10 行 Python 代码，利用机器上的所有内核和 GPU 来执行并行异步超参数调优。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="np nq l"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">If you run into an ImportError, try installing from a snapshot wheel: <a class="ae kj" href="https://ray.readthedocs.io/en/latest/installation.html#trying-snapshots-from-master" rel="noopener ugc nofollow" target="_blank">https://ray.readthedocs.io/en/latest/installation.html#trying-snapshots-from-master</a></figcaption></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi nr"><img src="../Images/02d7e6dbcad443bb626fc7d36986ab0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*egs-KDkpylsuM0LNnDX36A.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk"><a class="ae kj" href="https://twitter.com/MarcCoru/status/1080596327006945281" rel="noopener ugc nofollow" target="_blank">https://twitter.com/MarcCoru/status/1080596327006945281</a></figcaption></figure><p id="e34a" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">使用另一个配置文件和 4 行代码，在云上启动大规模分布式超参数搜索，并自动关闭机器(我们将在下面向您展示如何做到这一点)。</p><p id="5113" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">借助 Tune 的内置容错、试验迁移和集群自动扩展，您可以安全地利用 spot(可抢占)实例，并将云成本降低<a class="ae kj" href="https://aws.amazon.com/ec2/spot/" rel="noopener ugc nofollow" target="_blank">高达 90%。</a></p><h2 id="1528" class="mo mp it bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">调谐是灵活的。</h2><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ns"><img src="../Images/7dbe2a38bd045b1db00b5daa98d86d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TIpM56EvejedgqkoLTaGEw.png"/></div></div></figure><p id="2e84" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">Tune 与实验管理工具无缝集成，如<a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-usage.html#mlflow" rel="noopener ugc nofollow" target="_blank"> MLFlow </a>和<a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-usage.html#visualizing-results" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a>。</p><p id="4676" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">Tune 为优化算法提供了一个<a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-searchalg.html#contributing-a-new-algorithm" rel="noopener ugc nofollow" target="_blank">灵活的接口，允许您轻松实现和扩展新的优化算法。</a></p><p id="543f" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">您可以使用 Tune 来利用和扩展许多最先进的搜索算法和库，如<a class="ae kj" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank"> HyperOpt </a>(如下)和<a class="ae kj" href="http://ax.dev" rel="noopener ugc nofollow" target="_blank"> Ax </a>，而无需修改任何模型训练代码。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="np nq l"/></div></figure></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="9b19" class="oa mp it bd mq ob oc od mt oe of og mw jz oh ka mz kc oi kd nc kf oj kg nf ok bi translated">使用 Tune 很简单！</h1><p id="3797" class="pw-post-body-paragraph kw kx it ky b kz nh ju lb lc ni jx le lf nm lh li lj nn ll lm ln no lp lq lr im bi translated">现在让我们深入一个具体的例子，展示如何利用最先进的早期停止算法(ASHA)。我们将首先在您工作站上的所有内核上运行 Tune。然后，我们将用大约 10 行代码在云上扩展相同的实验。</p><p id="0471" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">我们将在这个例子中使用 PyTorch，但是我们也有可用的<a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-examples.html" rel="noopener ugc nofollow" target="_blank"> Tensorflow 和 Keras 的例子。</a></p><p id="fba0" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">Tune 作为<a class="ae kj" href="https://ray.readthedocs.io/en/latest/installation.html" rel="noopener ugc nofollow" target="_blank">射线</a>的一部分安装。要运行这个示例，您需要安装以下软件:<code class="fe ol om on oo b">pip install ray torch torchvision.</code></p><p id="e3c2" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">你可以在这个博客这里下载<a class="ae kj" href="https://gist.github.com/richardliaw/c66357d057e24ca8c285a811d4a485d7" rel="noopener ugc nofollow" target="_blank">完整版的博客。</a></p><p id="9c6b" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">我们首先运行一些导入:</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="np nq l"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">header of `tune_script.py`</figcaption></figure><p id="2132" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">让我们用 PyTorch 写一个神经网络:</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="beb7" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">要开始使用 Tune，在 PyTorch training below 函数中添加一个简单的日志记录语句。</p><pre class="kl km kn ko gt op oo oq or aw os bi"><span id="ce16" class="mo mp it oo b gy ot ou l ov ow">def train_mnist(config):<br/>    model = ConvNet()<br/>    train_loader, test_loader = get_data_loaders()<br/>    optimizer = optim.SGD(<br/>        model.parameters(), <br/>        lr=config["lr"], <br/>        momentum=config["momentum"])<br/>    for i in range(10):<br/>        train(model, optimizer, train_loader, torch.device("cpu"))<br/>        acc = test(model, test_loader, torch.device("cpu"))<br/><strong class="oo iu">        tune.track.log(mean_accuracy=acc)</strong><br/>        if i % 5 == 0:<br/>            # This saves the model to the trial directory<br/>            torch.save(model, "./model.pth")</span></pre><p id="71a9" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">注意，在上面的训练脚本中有几个助手函数；你可以在这里看到它们的定义。</p><h2 id="cffa" class="mo mp it bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">运行曲调</h2><p id="93e2" class="pw-post-body-paragraph kw kx it ky b kz nh ju lb lc ni jx le lf nm lh li lj nn ll lm ln no lp lq lr im bi translated">让我们运行 1 次试验，从一个均匀分布中随机抽样，以获得学习率和动量。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="571a" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">现在，你已经完成了你的第一次跑调！您可以通过指定 GPU 资源来轻松启用 GPU 使用— <a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-usage.html#using-gpus-resource-allocation" rel="noopener ugc nofollow" target="_blank">参见文档</a>了解更多详细信息。然后我们可以画出这次试验的表现。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="np nq l"/></div></figure><h2 id="bb8b" class="mo mp it bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">并行执行和提前停止</h2><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/7f18f6519c2b8f8e07926c5a09864ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*ex13kSs6cKmDAIkp"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Early stopping with ASHA.</figcaption></figure><p id="84cb" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">让我们集成 ASHA，一个可扩展的提前停止算法(<a class="ae kj" href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" rel="noopener ugc nofollow" target="_blank">博文</a>和<a class="ae kj" href="https://arxiv.org/abs/1810.05934" rel="noopener ugc nofollow" target="_blank">论文</a>)。ASHA 终止了不太有希望的试验，并将更多的时间和资源分配给更有希望的试验。</p><p id="c67c" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">使用<code class="fe ol om on oo b">num_samples</code>在机器上的所有可用内核中并行搜索(额外的尝试将被排队)。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="a725" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">您可以使用与上例相同的数据帧绘图。运行后，如果安装了 Tensorboard，还可以使用 Tensorboard 可视化结果:<code class="fe ol om on oo b">tensorboard --logdir ~/ray_results</code></p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oy"><img src="../Images/f3f30a37bbbc72fac601eb3dcdcd8034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JMYx6TJEYFjETK0ELwhnMA.png"/></div></div></figure><h1 id="ebb4" class="oa mp it bd mq ob oz od mt oe pa og mw jz pb ka mz kc pc kd nc kf pd kg nf ok bi translated">走向分布式</h1><p id="ac6b" class="pw-post-body-paragraph kw kx it ky b kz nh ju lb lc ni jx le lf nm lh li lj nn ll lm ln no lp lq lr im bi pe translated">建立一个分布式超参数搜索通常工作量很大。Tune 和 Ray 让这一切变得天衣无缝。</p><h2 id="f1e1" class="mo mp it bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">使用简单的配置文件启动云</h2><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi pn"><img src="../Images/bf3fb5782547d57c6d35902aa24d49e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SxNdcQlY8M7eR-jT"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Launch a cluster and distribute hyperparameter search without changing your code</figcaption></figure><p id="64af" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">首先，我们将创建一个配置光线簇的 YAML 文件。作为<a class="ae kj" href="http://ray.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">射线</a>的一部分，Tune 与<a class="ae kj" href="https://ray.readthedocs.io/en/latest/autoscaling.html" rel="noopener ugc nofollow" target="_blank">射线簇发射器</a>之间的交互非常干净。下面显示的相同命令将在 GCP、AWS 和<a class="ae kj" href="https://ray.readthedocs.io/en/latest/autoscaling.html#quick-start-private-cluster" rel="noopener ugc nofollow" target="_blank">本地私有集群</a>上工作。除了一个头节点之外，我们还将使用 3 个工作节点，因此集群上总共应该有 32 个 vCPUs，这使我们能够并行评估 32 个超参数配置。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="np nq l"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">tune-default.yaml</figcaption></figure><h2 id="3ae2" class="mo mp it bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">把东西放在一起</h2><p id="d8f3" class="pw-post-body-paragraph kw kx it ky b kz nh ju lb lc ni jx le lf nm lh li lj nn ll lm ln no lp lq lr im bi translated">要在光线簇中分布超参数搜索，需要将它附加到脚本的顶部:</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="ddfa" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">考虑到计算的大幅增长，我们应该能够增加搜索空间和搜索空间中的样本数量:</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="4a85" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">你可以在这个博客(作为<code class="fe ol om on oo b">tune_script.py</code>)下载<a class="ae kj" href="https://gist.github.com/richardliaw/c66357d057e24ca8c285a811d4a485d7" rel="noopener ugc nofollow" target="_blank">完整版的剧本。</a></p><h2 id="4b1b" class="mo mp it bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">启动你的实验</h2><p id="239d" class="pw-post-body-paragraph kw kx it ky b kz nh ju lb lc ni jx le lf nm lh li lj nn ll lm ln no lp lq lr im bi translated">要启动您的实验，您可以运行(假设到目前为止您的代码在一个文件<code class="fe ol om on oo b">tune_script.py</code>中):</p><pre class="kl km kn ko gt op oo oq or aw os bi"><span id="312c" class="mo mp it oo b gy ot ou l ov ow">$ ray submit tune-default.yaml tune_script.py --start \<br/>     --args=”localhost:6379”</span></pre><p id="5710" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">这将在 AWS 上启动您的集群，将<code class="fe ol om on oo b">tune_script.py</code>上传到 head 节点上，并运行<code class="fe ol om on oo b">python tune_script localhost:6379</code>，这是 Ray 为支持分布式执行而打开的一个端口。</p><p id="fa03" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">脚本的所有输出都会显示在您的控制台上。请注意，集群将在设置任何工作节点之前首先设置头节点，因此最初您可能只看到 4 个可用的 CPU。一段时间后，您可以看到 24 个试验被并行执行，其他试验将排队等候，一旦一个试验空闲，就立即执行。</p><p id="1213" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">要关闭集群，您可以运行:</p><pre class="kl km kn ko gt op oo oq or aw os bi"><span id="14c2" class="mo mp it oo b gy ot ou l ov ow">$ ray down tune-default.yaml</span></pre><p id="74c3" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">你完了🎉！</p><h1 id="a805" class="oa mp it bd mq ob oz od mt oe pa og mw jz pb ka mz kc pc kd nc kf pd kg nf ok bi translated">了解更多信息:</h1><p id="7686" class="pw-post-body-paragraph kw kx it ky b kz nh ju lb lc ni jx le lf nm lh li lj nn ll lm ln no lp lq lr im bi translated">Tune 还有许多其他特性，使研究人员和从业人员能够加速他们的开发。这篇博客文章中没有涉及的其他调整功能包括:</p><ul class=""><li id="bfa1" class="ly lz it ky b kz la lc ld lf ma lj mb ln mc lr md me mf mg bi translated">运行分布式容错实验的简单 API<a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-distributed.html#pre-emptible-instances-cloud" rel="noopener ugc nofollow" target="_blank"/></li><li id="7a3f" class="ly lz it ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated">通过<a class="ae kj" href="https://ray.readthedocs.io/en/latest/distributed_training.html" rel="noopener ugc nofollow" target="_blank">的分布式超参数搜索为 PyTorch 进行分布式数据并行训练</a></li><li id="86c6" class="ly lz it ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated"><a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-schedulers.html#population-based-training-pbt" rel="noopener ugc nofollow" target="_blank">基于人口的培训</a></li></ul><p id="a6fb" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">对于能够访问云的用户，Tune 和 Ray 提供了许多实用程序，可以实现在笔记本电脑上开发和在云上执行之间的无缝过渡。<a class="ae kj" href="https://ray.readthedocs.io/en/latest/tune-distributed.html#common-commands" rel="noopener ugc nofollow" target="_blank">文档</a>包括:</p><ul class=""><li id="fa64" class="ly lz it ky b kz la lc ld lf ma lj mb ln mc lr md me mf mg bi translated">在后台会话中运行实验</li><li id="b98c" class="ly lz it ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated">向现有实验提交试验</li><li id="2b85" class="ly lz it ky b kz mh lc mi lf mj lj mk ln ml lr md me mf mg bi translated">在 TensorBoard 中可视化分布式实验的所有结果。</li></ul><blockquote class="po pp pq"><p id="3375" class="kw kx mm ky b kz la ju lb lc ld jx le pr lg lh li ps lk ll lm pt lo lp lq lr im bi translated">Tune 旨在轻松扩展实验执行和超参数搜索。如果您有任何意见或建议，或者对 Tune 有兴趣，您可以<a class="ae kj" href="https://github.com/richardliaw" rel="noopener ugc nofollow" target="_blank">联系我</a>或<a class="ae kj" href="https://groups.google.com/forum/#!forum/ray-dev" rel="noopener ugc nofollow" target="_blank"> ray-dev 邮件列表</a>。</p><p id="3888" class="kw kx mm ky b kz la ju lb lc ld jx le pr lg lh li ps lk ll lm pt lo lp lq lr im bi translated">代码:<a class="ae kj" href="https://github.com/ray-project/ray/tree/master/python/ray/tune" rel="noopener ugc nofollow" target="_blank">https://github . com/ray-project/ray/tree/master/python/ray/tune</a><br/>文档:<a class="ae kj" href="http://ray.readthedocs.io/en/latest/tune.html" rel="noopener ugc nofollow" target="_blank">http://ray.readthedocs.io/en/latest/tune.html</a></p></blockquote><p id="f462" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">关于超参数调整的其他阅读材料，请查看 Neptune.ai 关于 Optuna vs HyperOpt 的<a class="ae kj" href="https://neptune.ai/blog/optuna-vs-hyperopt" rel="noopener ugc nofollow" target="_blank">博文！</a></p><p id="78c7" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">感谢彭于晏、埃里克·梁、乔伊·冈萨雷斯、扬·斯托伊察、尤金·维尼斯基、丽莎·邓拉普、菲利普·莫里茨、、阿尔文·万、丹尼尔·罗斯柴尔德、布里詹·塔南杰扬、阿洛克·辛格(也许还有其他人？)来通读这篇博文的各种版本！</p></div></div>    
</body>
</html>