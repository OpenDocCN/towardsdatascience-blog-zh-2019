<html>
<head>
<title>Batch Normalization Tensorflow Keras Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批量标准化张量流 Keras 示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/backpropagation-and-batch-normalization-in-feedforward-neural-networks-explained-901fd6e5393e?source=collection_archive---------2-----------------------#2019-06-08">https://towardsdatascience.com/backpropagation-and-batch-normalization-in-feedforward-neural-networks-explained-901fd6e5393e?source=collection_archive---------2-----------------------#2019-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d0dca82731e52af49b75f11b48e202e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-LqMm00w19851SM_"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://www.pexels.com/photo/black-and-silver-laptop-computer-on-round-brown-wooden-table-1181243/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/black-and-silver-laptop-computer-on-round-brown-wooden-table-1181243/</a></figcaption></figure><div class=""/><p id="0889" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习是一个非常活跃的研究领域，你会经常在图书馆的文档中看到白皮书。在前面的文章中，我们将讨论 Loffe 和 Szegedy 所描述的批处理规范化。如果你是那种喜欢直接从源头获取信息的人，请查看他们白皮书的链接。</p><div class="is it gp gr iu le"><a href="https://arxiv.org/abs/1502.03167" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd jk gy z fp lj fr fs lk fu fw ji bi translated">批量标准化:通过减少内部协变量转移加速深度网络训练</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">训练深度神经网络是复杂的，因为每层输入的分布在训练过程中会发生变化</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">arxiv.org</p></div></div></div></a></div><p id="3cf8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">批量标准化用于稳定甚至加速学习过程。它通过应用保持平均激活接近 0 和激活标准偏差接近 1 的变换来做到这一点。</p><h1 id="9cf2" class="ln lo jj bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">反向传播</h1><p id="0633" class="pw-post-body-paragraph kg kh jj ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">在高层次上，反向传播修改权重以降低成本函数值。然而，在我们能够理解批处理规范化背后的推理之前，我们必须掌握反向传播背后的实际数学原理。</p><p id="f190" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使问题更简单，我们将假设我们有一个由两层组成的神经网络，每层有一个神经元。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/3f57cf8e0bc144b0735557dabd8e862b.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*rD3eQj0xzN-2p183YWHy-g.png"/></div></figure><p id="0517" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用以下公式来表示每个神经元的输出:</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mv"><img src="../Images/704601bf950efdb5c9f88d09cef33e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ZiRaWEVNDyF1SrvfSwJug.png"/></div></div></figure><p id="f4ed" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中:</p><ul class=""><li id="171e" class="mw mx jj ki b kj kk kn ko kr my kv mz kz na ld nb nc nd ne bi translated">L =神经网络中的层</li><li id="ea07" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">w =来自神经元的输出边缘所乘以的权重</li><li id="028c" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">a =前一层神经元的输出(输入边缘的值)</li><li id="edfc" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">σ =激活函数</li><li id="e736" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">b =偏置神经元的输出(输入边缘的值)</li></ul><p id="bbe4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">成本函数的典型例子是<strong class="ki jk">均方误差</strong>。对于单个样本，我们从预测值中减去实际值(即 y ),并对结果进行平方，以说明预测值大于或小于实际值的情况。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/2170b6287dc49a673dbb2c127a783db5.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*8KyWKLPut7LVCzR1mt3MIg.png"/></div></figure><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/42006694f7478d47f067743766284b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6lU5NyDu8WXcC8UGDBPYxQ.png"/></div></div></figure><p id="5862" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如前所述，我们修改权重以最小化成本函数。如果我们将成本与个体体重相关联，成本将在抛物线底部最低。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e498e640420bb29b318d872fe5672cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*QAw-37A_Z-zs3Esk903JrA.png"/></div></figure><p id="fa57" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用微积分中的链式法则，我们可以得到代价函数相对于权重的偏导数。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/e5bc4dba231a47e3a1531791fc5e4f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T9hKOuIAvhhXn9JJMDMtyg.png"/></div></div></figure><p id="e5c5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每一项的偏导数可以表示如下。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/29a87c579eee7df93a3b4284d7e7b0ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*IKqG1ESHPZK34XuXsh3i5g.png"/></div></figure><p id="ffc9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意我们如何使用激活函数的导数。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/2c273bc47bd6e926599885df82aeba9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACslQzW8AyCzxphYyF4wjA.png"/></div></div></figure><p id="3c03" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们使用 sigmoid 函数作为我们的激活函数，那么，如果<strong class="ki jk"> <em class="nq"> z </em> </strong>(激活函数之前的神经元输出)非常大或非常小，导数将近似为 0。因此，当我们计算梯度并更新权重时，变化会非常小，以至于模型不会改进。后者被称为<strong class="ki jk">消失梯度</strong> t 问题。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl nr"><img src="../Images/e83ec0c80e9c611e89008122501d9ab4.png" data-original-src="https://miro.medium.com/v2/format:webp/1*21FiWERy37p12RO1xCpGSg.png"/></div></figure><p id="6dad" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在神经元进入激活函数之前对其输出进行归一化，我们可以确保其保持接近于导数最高的 0。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl nr"><img src="../Images/d1cf3dcf8d43a512509b2997aa83cc43.png" data-original-src="https://miro.medium.com/v2/format:webp/0*1Ug-aNl7coz4lUds.png"/></div></figure><h1 id="115e" class="ln lo jj bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">正常化</h1><p id="44c3" class="pw-post-body-paragraph kg kh jj ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">自然界中的随机过程倾向于遵循一个钟形曲线，称为正态分布。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/85b84f3cfff1e2a4545d51800b3a7316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QxDPWojlGa-w7Bl_.png"/></div></div></figure><p id="a0a1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">平均值是所有数据点的总和除以总点数。增大平均值会将钟形曲线的中心向右移动，减小平均值会将钟形曲线的中心向左移动。另一方面，标准差(方差的平方根)描述了样本与平均值的差异程度。增加标准差会使曲线变宽。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/3d95d23ab29e0b639443f30a3bf479f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/0*UgRz9QCfRPTsOuP2.png"/></div></figure><p id="c2df" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使数据标准化，我们减去平均值并除以标准差。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl nr"><img src="../Images/a22feed7eccfe0877c8074c3b68c40ef.png" data-original-src="https://miro.medium.com/v2/1*13XKCXQc7eabfZbRzkvGvA.gif"/></div></figure><p id="fcd0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">无论我们正在处理的数据是什么，归一化后，平均值将等于 0，标准差将等于 1。</p><p id="f07d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nq">注意:这等同于说它确保方差等于 1，因为标准差等于方差的平方根。</em></p><h2 id="9e8c" class="nu lo jj bd lp nv nw dn lt nx ny dp lx kr nz oa mb kv ob oc mf kz od oe mj of bi translated">批量标准化</h2><p id="a080" class="pw-post-body-paragraph kg kh jj ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">假设我们建立了一个神经网络，目标是对灰度图像进行分类。灰度图像中每个像素的亮度从 0 到 255 不等。在进入神经网络之前，每个图像都将被转换成一维数组。然后，每个像素从输入层进入一个神经元。如果将每个神经元的输出传递给一个 sigmoid 函数，那么除 0 之外的每个值(即 1 到 255)都将减少到接近 1 的数字。因此，通常在训练之前对每个图像的像素值进行归一化。另一方面，批处理规范化用于将规范化应用于隐藏层的输出。</p><h1 id="193c" class="ln lo jj bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">密码</h1><p id="160a" class="pw-post-body-paragraph kg kh jj ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">让我们看看如何在 Python 中实现批处理规范化。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="88c9" class="nu lo jj oh b gy ol om l on oo">import matplotlib.pyplot as plt<br/>import matplotlib.image as mpimg<br/>plt.style.use('dark_background')</span><span id="1e81" class="nu lo jj oh b gy op om l on oo">from keras.models import Sequential<br/>from keras.preprocessing.image import ImageDataGenerator<br/>from keras.layers import BatchNormalization<br/>from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten<br/>from keras.datasets import cifar10<br/>from keras.utils import normalize, to_categorical</span></pre><p id="d3fd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> cifar10 </strong>数据集由 60，000 张 32×32 像素的图像组成，分为 10 类。下面列出了这些类及其相关的标准整数值。</p><ul class=""><li id="617a" class="mw mx jj ki b kj kk kn ko kr my kv mz kz na ld nb nc nd ne bi translated">0:飞机</li><li id="b2c1" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">1:汽车</li><li id="7804" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">2:鸟</li><li id="4614" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">3:猫</li><li id="2a30" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">4:鹿</li><li id="5b0b" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">5:狗</li><li id="fd17" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">6:青蛙</li><li id="6302" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">7:马</li><li id="f603" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">8:船</li><li id="6174" class="mw mx jj ki b kj nf kn ng kr nh kv ni kz nj ld nb nc nd ne bi translated">9:卡车</li></ul><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/23cbfc96912bd73f8dd655fb3580549b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*c8tEk4YZFFcKbJP0.png"/></div></div></figure><p id="34aa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在训练我们的模型之前，我们基于上面列出的相同原因对输入进行标准化，并对标签进行编码。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="1ebd" class="nu lo jj oh b gy ol om l on oo">(X_train, y_train), (X_test, y_test) = cifar10.load_data()</span><span id="ce59" class="nu lo jj oh b gy op om l on oo">X_train = normalize(X_train, axis=1)<br/>X_test = normalize(X_test, axis=1)<br/>y_train = to_categorical(y_train)<br/>y_test = to_categorical(y_test)</span></pre><p id="f135" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了提高我们的模型的概括能力，我们将随机移动，翻转和放大/缩小图像。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="4d9a" class="nu lo jj oh b gy ol om l on oo">train_datagen = ImageDataGenerator(<br/>    shear_range = 0.2,<br/>    zoom_range = 0.2,<br/>    horizontal_flip = True<br/>)</span><span id="014f" class="nu lo jj oh b gy op om l on oo">train_datagen.fit(X_train)</span><span id="62b6" class="nu lo jj oh b gy op om l on oo">train_generator = train_datagen.flow(<br/>    X_train,<br/>    y_train,<br/>    batch_size = 32<br/>)</span></pre><p id="dd27" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用下面的等式来设置步数，但是我们也可以使用任意值。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="b184" class="nu lo jj oh b gy ol om l on oo">steps = int(X_train.shape[0] / 64)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/13dd666358026187824c0aaa023197ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*IS4MH3GOw_9YlWVt8TQmFA.png"/></div></figure><p id="b017" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们定义一个函数来建立模型，使用和不使用批量标准化以及我们选择的激活函数。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="49b5" class="nu lo jj oh b gy ol om l on oo">def build_model(batch_normalization, activation):<br/>    model = Sequential()<br/>    model.add(Conv2D(32, 3, activation = activation, padding = 'same', input_shape = (32, 32, 3)))<br/>    if batch_normalization: model.add(BatchNormalization())<br/>    model.add(Conv2D(32, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))<br/>    if batch_normalization: model.add(BatchNormalization())<br/>    model.add(MaxPooling2D())<br/>    model.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))<br/>    if batch_normalization: model.add(BatchNormalization())<br/>    model.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))<br/>    if batch_normalization: model.add(BatchNormalization()) <br/>    model.add(MaxPooling2D())<br/>    model.add(Flatten())<br/>    model.add(Dense(128, activation = activation, kernel_initializer = 'he_uniform'))<br/>    model.add(Dense(10, activation = 'softmax'))</span><span id="c73c" class="nu lo jj oh b gy op om l on oo">return model</span></pre><p id="6b24" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了突出使用批处理规范化的好处，我们将训练并比较使用批处理规范化的模型和不使用批处理规范化的模型的性能。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="9d42" class="nu lo jj oh b gy ol om l on oo">sig_model = build_model(batch_normalization = False, activation = 'sigmoid')</span></pre><p id="8c26" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用 rmsprop 作为优化器，使用分类交叉熵作为损失函数，因为我们试图预测类。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="3bf7" class="nu lo jj oh b gy ol om l on oo">sig_model.compile(<br/>    optimizer = 'rmsprop',<br/>    loss = 'categorical_crossentropy',<br/>    metrics = ['accuracy']<br/>)</span></pre><p id="1afe" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们训练我们的模型。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="2548" class="nu lo jj oh b gy ol om l on oo">sig_history = sig_model.fit_generator(<br/>        train_generator,<br/>        steps_per_epoch = steps,<br/>        epochs = 10,<br/>        validation_data = (X_test, y_test)<br/>)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/b76516b2c8f05121d2540490733e0d7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9lesGV-SLi-frAC7mOXOVA.png"/></div></div></figure><p id="1e3a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过使用由<em class="nq">拟合</em>函数返回的历史变量来绘制每个时期的训练和验证准确度和损失。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="0155" class="nu lo jj oh b gy ol om l on oo">loss = sig_history.history['loss']<br/>val_loss = sig_history.history['val_loss']<br/>epochs = range(1, len(loss) + 1)<br/>plt.plot(epochs, loss, 'y', label='Training loss')<br/>plt.plot(epochs, val_loss, 'r', label='Validation loss')<br/>plt.title('Training and validation loss')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/a281628ba8915b16036e6d58432c58b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*0LweAnAupKyRzRWSIP4y4Q.png"/></div></figure><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="1229" class="nu lo jj oh b gy ol om l on oo">acc = sig_history.history['acc']<br/>val_acc = sig_history.history['val_acc']<br/>plt.plot(epochs, acc, 'y', label='Training acc')<br/>plt.plot(epochs, val_acc, 'r', label='Validation acc')<br/>plt.title('Training and validation accuracy')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Accuracy')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/61648effcff4ffd3ae44dfd185eacf90.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*BE9tTz5__UJSb1gICQucfw.png"/></div></figure><p id="c179" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们采取与之前相同的步骤，只是这次我们应用了批处理规范化。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="a700" class="nu lo jj oh b gy ol om l on oo">sig_norm_model = build_model(batch_normalization = True, activation = 'sigmoid')</span><span id="e1bb" class="nu lo jj oh b gy op om l on oo">sig_norm_model.compile(<br/>    optimizer = 'rmsprop',<br/>    loss = 'categorical_crossentropy',<br/>    metrics = ['accuracy']<br/>)</span><span id="7410" class="nu lo jj oh b gy op om l on oo">sig_norm_history = sig_norm_model.fit_generator(<br/>        train_generator,<br/>        steps_per_epoch = steps,<br/>        epochs = 10,<br/>        validation_data = (X_test, y_test)<br/>)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/460a0cdff4b03dd63fb4dcba552e68d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t1eKreR-ILrnDF8MQHEPkg.png"/></div></div></figure><p id="79e1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如您所见，训练损失和训练准确度图比没有批量标准化的模型平滑得多，并且获得了明显更好的结果。</p><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="6147" class="nu lo jj oh b gy ol om l on oo">loss = sig_norm_history.history['loss']<br/>val_loss = sig_norm_history.history['val_loss']<br/>epochs = range(1, len(loss) + 1)<br/>plt.plot(epochs, loss, 'y', label='Training loss')<br/>plt.plot(epochs, val_loss, 'r', label='Validation loss')<br/>plt.title('Training and validation loss')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/5ee65c4c1b7142751d62876eb6623b52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*fOXnI7SVrXdHesRjDwQDig.png"/></div></figure><pre class="mr ms mt mu gt og oh oi oj aw ok bi"><span id="30ec" class="nu lo jj oh b gy ol om l on oo">acc = sig_norm_history.history['acc']<br/>val_acc = sig_norm_history.history['val_acc']<br/>plt.plot(epochs, acc, 'y', label='Training acc')<br/>plt.plot(epochs, val_acc, 'r', label='Validation acc')<br/>plt.title('Training and validation accuracy')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Accuracy')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/619641a724d286a7d6ecb146e9097a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*Be_43XHtTxV1ChW4hylXPA.png"/></div></figure><h1 id="6357" class="ln lo jj bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">最后的想法</h1><p id="9957" class="pw-post-body-paragraph kg kh jj ki b kj ml kl km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">消失梯度问题指的是当我们向下传播到初始层时梯度如何指数下降。因此，初始层的权重和偏差将不能有效地更新。鉴于这些初始图层通常对识别输入数据的核心元素至关重要，因此可能会导致准确性较差。</p><p id="5a46" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最简单的解决方案是使用另一个激活函数，比如 ReLU。否则，我们可以使用批处理规范化来减轻问题，方法是规范化输入，使其保持在<em class="nq">金发区</em>。</p></div></div>    
</body>
</html>