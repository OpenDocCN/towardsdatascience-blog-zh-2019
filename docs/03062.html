<html>
<head>
<title>Gradient Boosting Decision Tree Algorithm Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度推进决策树算法讲解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4?source=collection_archive---------1-----------------------#2019-05-17">https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4?source=collection_archive---------1-----------------------#2019-05-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9b63f8b867f07240ff4795520be07ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iQYUeBdh9tECVbEu"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@neonbrand?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">NeONBRAND</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="11e2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在接下来的文章中，我们将看看如何在 Python 中实现渐变增强。梯度增强类似于 AdaBoost，因为它们都使用决策树的集合来预测目标标签。然而，与 AdaBoost 不同，梯度增强树的深度大于 1。在实践中，您通常会看到梯度增强的最大叶子数在 8 到 32 之间。</p><h1 id="a70f" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">算法</h1><p id="5cb3" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在我们深入研究代码之前，重要的是我们要理解梯度增强算法是如何实现的。假设，我们试图根据房子的年龄、面积和位置来预测房子的价格。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mh"><img src="../Images/6d742617052be3ab93a05f0128fc434a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dWlneZmWitSdsk9Gj7MsyA.png"/></div></div></figure><h2 id="4bdf" class="mm lf jj bd lg mn mo dn lk mp mq dp lo kr mr ms ls kv mt mu lw kz mv mw ma mx bi translated">步骤 1:计算目标标签的平均值</h2><p id="f490" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">当处理回归问题时，我们从一片叶子开始，它是我们想要预测的变量的平均值。在接下来的步骤中，这一页将用作接近正确解决方案的基线。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/903cee6022d5b3508f36a4c8a7a6094f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dw5-E1r15db4aVN7Hf6bfA.png"/></div></div></figure><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/dd2933bb766efe3caf4b87508e53a87a.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*mOSeYgXen_Jwz6Le2UEyaw.png"/></div></figure><h2 id="d5f0" class="mm lf jj bd lg mn mo dn lk mp mq dp lo kr mr ms ls kv mt mu lw kz mv mw ma mx bi translated">第二步:计算残差</h2><p id="e52b" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">对于每个样本，我们用下面的公式计算残差。</p><blockquote class="na"><p id="f611" class="nb nc jj bd nd ne nf ng nh ni nj ld dk translated">残差=实际值-预测值</p></blockquote><p id="4982" class="pw-post-body-paragraph kg kh jj ki b kj nk kl km kn nl kp kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated">在我们的示例中，预测值等于上一步中计算的平均值，实际值可以在每个样本的 price 列中找到。计算残差后，我们得到下表。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/b0542d56477341ad4db5d2c4f50ad0f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EuYMrGN2r9jo6VlVBigX3A.png"/></div></div></figure><h2 id="20c0" class="mm lf jj bd lg mn mo dn lk mp mq dp lo kr mr ms ls kv mt mu lw kz mv mw ma mx bi translated">步骤 3:构建决策树</h2><p id="bb3d" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">接下来，我们构建一棵树，目标是预测残差。换句话说，每片叶子将包含一个关于残差值的预测(不是期望的标签)。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/171993185a2ba7a0679ee5d058ccc5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjWwWylZuGNi6NgwhZO2fg.png"/></div></div></figure><p id="6a84" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果残差比叶子多，一些残差将在同一个叶子中结束。当这种情况发生时，我们计算它们的平均值，并将其放入叶子中。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/18310b25cc79c102adf8a86f40307134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*UVGofhps8VmK502tChePNQ.png"/></div></figure><p id="53f4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这样，树就变成了:</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/1b0e6362cea6557a6955d70c78ea8053.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2qzgrmK8GM7q28Qm4SdGOw.png"/></div></div></figure><h2 id="d758" class="mm lf jj bd lg mn mo dn lk mp mq dp lo kr mr ms ls kv mt mu lw kz mv mw ma mx bi translated">步骤 4:使用集合中的所有树预测目标标签</h2><p id="21db" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">每个样本通过新形成的树的决策节点，直到它到达给定的线索。所述叶中的残差用于预测房价。</p><p id="e326" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过实验表明，朝着解决方案采取小的增量步骤可以实现具有较低总体方差的可比偏差(较低的方差导致训练数据之外的样本具有更好的准确性)。因此，为了防止过度拟合，我们引入了一个叫做学习率的超参数。当我们做一个预测时，每个残差都要乘以学习率。这迫使我们使用更多的决策树，每一棵都向最终解决方案迈出一小步。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/d77ccc86654098f81f4414a292a9df8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*heTIEVLlsGhOjSoNOMXhsA.png"/></div></div></figure><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/2695ae07d24d6b05ce0292a67d9d8e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l18XAxwRY1Enmjx3SJ-VAQ.png"/></div></div></figure><h2 id="f130" class="mm lf jj bd lg mn mo dn lk mp mq dp lo kr mr ms ls kv mt mu lw kz mv mw ma mx bi translated">步骤 5:计算新的残差</h2><p id="923d" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们通过从上一步的预测中减去实际房价来计算一组新的残差。如步骤 3 所述，残差将用于下一个决策树的叶子。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/3c8a6501a4b30c4b443896583e0cb850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R3EOAzrqIQ2iuAF7E80f9A.png"/></div></div></figure><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/84b11f898360d8ecd71b6b9c16e0089c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0EV8Cm59XZVU2XfOjYFqQ.png"/></div></div></figure><h2 id="7ab4" class="mm lf jj bd lg mn mo dn lk mp mq dp lo kr mr ms ls kv mt mu lw kz mv mw ma mx bi translated">步骤 6:重复步骤 3 到 5，直到迭代次数与超参数指定的次数(即估计数)相匹配</h2><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/b0ada344d555c7a4ec5253140ab16d6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ihsdiQPQG3jlcCwkFFQCVg.png"/></div></div></figure><h2 id="6261" class="mm lf jj bd lg mn mo dn lk mp mq dp lo kr mr ms ls kv mt mu lw kz mv mw ma mx bi translated">步骤 7:一旦训练完毕，使用集合中的所有树对目标变量的值进行最终预测</h2><p id="ffe0" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">最终预测将等于我们在第一步中计算的平均值，加上组成森林的树木预测的所有残差乘以学习率。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/1599da980bd70c096caea0d92a58e127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c-Zo4QrliCyhio0iTp7z8g.png"/></div></div></figure><h1 id="5b3b" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">密码</h1><p id="ba16" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在本教程中，我们将使用来自<strong class="ki jk"> scikit-learn </strong>库中的<strong class="ki jk">GradientBoostingRegressor</strong>类。</p><pre class="mi mj mk ml gt nz oa ob oc aw od bi"><span id="7072" class="mm lf jj oa b gy oe of l og oh">from sklearn.ensemble import GradientBoostingRegressor<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import mean_squared_error<br/>from sklearn.datasets import load_boston<br/>from sklearn.metrics import mean_absolute_error</span></pre><p id="a09b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于前面的示例，我们将使用波士顿房价数据集。</p><pre class="mi mj mk ml gt nz oa ob oc aw od bi"><span id="49f5" class="mm lf jj oa b gy oe of l og oh">boston = load_boston()<br/>X = pd.DataFrame(boston.data, columns=boston.feature_names)<br/>y = pd.Series(boston.target)</span></pre><p id="a4d0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了评估我们模型的性能，我们将数据分为训练集和测试集。</p><pre class="mi mj mk ml gt nz oa ob oc aw od bi"><span id="051f" class="mm lf jj oa b gy oe of l og oh">X_train, X_test, y_train, y_test = train_test_split(X, y)</span></pre><p id="6030" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们构建并拟合我们的模型。<code class="fe oi oj ok oa b">max_depth</code>是指每棵树的叶子数量(即 4 片),而<code class="fe oi oj ok oa b">n_estimators</code>是指集合中树的总数。如前所述，<code class="fe oi oj ok oa b">learning_rate</code>超参数缩放每棵树的贡献。如果将其设置为较低的值，集合中将需要更多的树来适应训练集，但总体方差会更低。</p><pre class="mi mj mk ml gt nz oa ob oc aw od bi"><span id="07f0" class="mm lf jj oa b gy oe of l og oh">regressor = GradientBoostingRegressor(<br/>    max_depth=2,<br/>    n_estimators=3,<br/>    learning_rate=1.0<br/>)<br/>regressor.fit(X_train, y_train)</span></pre><p id="436c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe oi oj ok oa b">staged_predict()</code>方法测量每个训练阶段的验证误差(即一棵树、两棵树……)，以找到最佳的树数。</p><pre class="mi mj mk ml gt nz oa ob oc aw od bi"><span id="4b12" class="mm lf jj oa b gy oe of l og oh">errors = [mean_squared_error(y_test, y_pred) for y_pred in regressor.staged_predict(X_test)]</span><span id="fc76" class="mm lf jj oa b gy ol of l og oh">best_n_estimators = np.argmin(errors)</span></pre><p id="3024" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们可以使用最佳数量的树来构建和拟合我们的模型。</p><pre class="mi mj mk ml gt nz oa ob oc aw od bi"><span id="5891" class="mm lf jj oa b gy oe of l og oh">best_regressor = GradientBoostingRegressor(<br/>    max_depth=2,<br/>    n_estimators=best_n_estimators,<br/>    learning_rate=1.0<br/>)<br/>best_regressor.fit(X_train, y_train)</span></pre><p id="894d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Sklearn 提供了许多指标来评估我们的机器学习模型的性能。我发现特别有用的是，他们根据适用的问题领域对每个指标进行分类。例如，精度只在分类的上下文中有意义。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/6793412be56676e379e91fe4716de3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*OR_dzailAOI5FfBjD__V_g.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/model_evaluation.html</a></figcaption></figure><p id="f79d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用平均绝对误差，它可以解释为预测值和实际值之间的平均距离。</p><pre class="mi mj mk ml gt nz oa ob oc aw od bi"><span id="6354" class="mm lf jj oa b gy oe of l og oh">y_pred = best_regressor.predict(X_test)</span><span id="b44a" class="mm lf jj oa b gy ol of l og oh">mean_absolute_error(y_test, y_pred)</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a6009a189c30b4c28be33669b9b866ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*NgsVrmCwA_l2KVFMLmM8XQ.png"/></div></figure></div></div>    
</body>
</html>