<html>
<head>
<title>Batch, Mini Batch &amp; Stochastic Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批量、小型批量和随机梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a?source=collection_archive---------0-----------------------#2019-10-01">https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a?source=collection_archive---------0-----------------------#2019-10-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1047" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个深度学习的时代，机器已经超过了人类的智力，通过看例子来了解这些机器是如何学习的是很有趣的。当我们说我们正在<em class="kl">训练</em>模型时，是幕后的梯度下降在训练它。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/7ab91a38db0fb9ab713174ece2a84d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*MpLkcugbeMrJvFlz69LTNQ.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Machine Learning behind the scenes (Source: <a class="ae ky" href="https://me.me/i/machine-learning-gradient-descent-machine-learning-machine-learning-behind-the-ea8fe9fc64054eda89232d7ffc9ba60e" rel="noopener ugc nofollow" target="_blank">https://me.me/i/machine-learning-gradient-descent-machine-learning-machine-learning-behind-the-ea8fe9fc64054eda89232d7ffc9ba60e</a>)</figcaption></figure><p id="31a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，让我们更深入地研究深度学习模型，看看梯度下降及其兄弟姐妹。</p><h1 id="c3d6" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">梯度下降</h1><p id="8881" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">这是维基百科对梯度下降的说法</p><blockquote class="mc md me"><p id="d354" class="jn jo kl jp b jq jr js jt ju jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj kk ij bi translated"><strong class="jp ir">梯度下降</strong>是一种<a class="ae ky" href="https://en.wikipedia.org/wiki/Category:First_order_methods" rel="noopener ugc nofollow" target="_blank">一阶</a> <a class="ae ky" href="https://en.wikipedia.org/wiki/Iterative_algorithm" rel="noopener ugc nofollow" target="_blank">迭代</a> <a class="ae ky" href="https://en.wikipedia.org/wiki/Mathematical_optimization" rel="noopener ugc nofollow" target="_blank">优化</a> <a class="ae ky" href="https://en.wikipedia.org/wiki/Algorithm" rel="noopener ugc nofollow" target="_blank">算法</a>求函数的最小值</p></blockquote><p id="ab38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这似乎有点复杂，所以让我们分解一下。</p><p id="10cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降的目标是最小化给定的函数，在我们的情况下，是神经网络的损失函数。为了实现这个目标，它迭代地执行两个步骤。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/0bd9f6692b4f310376fac8f608c23b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*iRv8pCP7v8FzVJNe2vAjdw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" href="https://hackernoon.com/the-reason-behind-moving-in-the-direction-opposite-to-the-gradient-f9566b95370b" rel="noopener ugc nofollow" target="_blank">https://hackernoon.com/the-reason-behind-moving-in-the-direction-opposite-to-the-gradient-f9566b95370b</a></figcaption></figure><ol class=""><li id="38f8" class="mj mk iq jp b jq jr ju jv jy ml kc mm kg mn kk mo mp mq mr bi translated">计算斜率(梯度),即函数在当前点的一阶导数</li><li id="86bf" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">从当前点向坡度增加的相反方向移动计算出的量</li></ol><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/42798b3c2bddb22a750fdcdbd56e117d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P7z2BKhd0R-9uyn9ThDasA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Gradient descent (Source: <a class="ae ky" href="https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1" rel="noopener">https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1</a>)</figcaption></figure><p id="a1ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，想法是通过神经网络的隐藏层传递训练集，然后通过使用来自训练数据集的训练样本计算梯度来更新层的参数。</p><p id="ffc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这样想吧。假设一个人在山谷的顶端，他想到达谷底。所以他走下斜坡。他根据当前位置决定下一个位置，当他到达他的目标山谷底部时停下来。</p><p id="bccb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有不同的方法可以让那个人(重物)走下斜坡。让我们一个一个的来看看。</p><h1 id="f773" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">批量梯度下降</h1><p id="5c39" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">在批量梯度下降中，所有的训练数据都被考虑在内以采取一个单独的步骤。我们取所有训练样本梯度的平均值，然后使用该平均梯度来更新我们的参数。这只是一个时代中梯度下降的一步。</p><p id="c339" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">批量梯度下降对于凸的或相对平滑的误差流形是很好的。在这种情况下，我们有点直接走向最优解。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/233a886d76e3c0c115e7492309da4352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*44QbDJ9gJvw8tXtHNVLoCA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Cost vs Epochs (Source: <a class="ae ky" href="https://www.bogotobogo.com/python/scikit-learn/scikit-learn_batch-gradient-descent-versus-stochastic-gradient-descent.php" rel="noopener ugc nofollow" target="_blank">https://www.bogotobogo.com/python/scikit-learn/scikit-learn_batch-gradient-descent-versus-stochastic-gradient-descent.php</a>)</figcaption></figure><p id="731a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">成本与时期的关系图也非常平滑，因为我们对单个步骤的训练数据的所有梯度进行了平均。随着时间的推移，成本不断降低。</p><h1 id="b6f0" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">随机梯度下降</h1><p id="c78d" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">在批量梯度下降中，我们考虑梯度下降的每一步的所有例子。但是如果我们的数据集非常庞大呢？深度学习模型渴望数据。数据越多，模型越有可能是好的。假设我们的数据集有 500 万个示例，那么仅仅一步，模型就必须计算所有 500 万个示例的梯度。这似乎不是一个有效的方法。为了解决这个问题，我们有随机梯度下降。在随机梯度下降(SGD)中，我们一次只考虑一个例子来进行单步。我们在 SGD 的<strong class="jp ir">一个时期</strong>中执行以下步骤:</p><ol class=""><li id="804e" class="mj mk iq jp b jq jr ju jv jy ml kc mm kg mn kk mo mp mq mr bi translated">举个例子</li><li id="8037" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">把它输入神经网络</li><li id="8b07" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">计算它的梯度</li><li id="406e" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">使用我们在步骤 3 中计算的梯度来更新权重</li><li id="25c3" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">对训练数据集中的所有示例重复步骤 1-4</li></ol><p id="4f80" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为我们一次只考虑一个例子，成本会随着训练例子的变化而波动，而且<strong class="jp ir">不</strong>一定会降低。但是长期来看，你会看到成本随着波动而降低。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3a060f7c19e867a86bbe87bb72055354.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*yHmuv_SmZ0f78QTsTyTrsA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Cost vs Epochs in SGD (Source: <a class="ae ky" href="https://adventuresinmachinelearning.com/stochastic-gradient-descent/" rel="noopener ugc nofollow" target="_blank">https://adventuresinmachinelearning.com/stochastic-gradient-descent/</a>)</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/1337c048e9abce79b7bf4f784a8d7e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*JugKARhlrp9HLTF5_lN7EQ.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" rel="noopener" target="_blank" href="/optimizers-be-deeps-appetizers-511f3706aa67">https://towardsdatascience.com/optimizers-be-deeps-appetizers-511f3706aa67</a></figcaption></figure><p id="77f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还因为成本波动很大，它永远不会达到最小值，但会一直围绕着它波动。</p><p id="a1ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SGD 可以用于更大的数据集。当数据集很大时，它收敛得更快，因为它会更频繁地更新参数。</p><h1 id="bab6" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">小批量梯度下降</h1><p id="ad1c" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我们已经看到批次梯度下降。我们也看到了随机梯度下降。批量梯度下降可用于更平滑的曲线。当数据集很大时，可以使用 SGD。批量梯度下降直接收敛到最小值。对于较大的数据集，SGD 收敛更快。但是，因为在 SGD 中我们一次只使用一个例子，所以我们不能在它上面实现矢量化实现。这可能会降低计算速度。为了解决这个问题，使用了批量梯度下降和 SGD 的混合。</p><p id="c9bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们既不一次使用所有的数据集，也不一次使用一个例子。我们使用少于实际数据集的一批固定数量的训练样本，称之为小批。这样做有助于我们实现我们看到的前两种变体的优点。因此，在创建了固定大小的小批量后，我们在<strong class="jp ir">一个时期内执行以下步骤:</strong></p><ol class=""><li id="9ee7" class="mj mk iq jp b jq jr ju jv jy ml kc mm kg mn kk mo mp mq mr bi translated">挑选一个小批量</li><li id="4802" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">把它输入神经网络</li><li id="d9b6" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">计算小批量的平均梯度</li><li id="c38c" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">使用我们在步骤 3 中计算的平均梯度来更新权重</li><li id="8317" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">对我们创建的小批量重复步骤 1-4</li></ol><p id="6427" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就像 SGD 一样，小批量梯度下降中各时期的平均成本会波动，因为我们一次对少量样本进行平均。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/bcad7d0dd6c988b80b7dc76b0136472a.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*XYA1O4jrGN8-P6XhHL2NsQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Cost vs no of mini-batch (Source: <a class="ae ky" href="https://stats.stackexchange.com/questions/310734/why-is-the-mini-batch-gradient-descents-cost-function-graph-noisy" rel="noopener ugc nofollow" target="_blank">https://stats.stackexchange.com/questions/310734/why-is-the-mini-batch-gradient-descents-cost-function-graph-noisy</a>)</figcaption></figure><p id="1fbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，当我们使用小批量梯度下降时，我们会频繁更新我们的参数，并且我们可以使用矢量化实现来加快计算速度。</p><h1 id="4289" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">结论</h1><p id="4929" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">就像这个世界上的其他事物一样，我们看到的所有三种变体都有其优点和缺点。这并不是说一个变体经常被使用。根据情况和问题的背景，每个变体都被统一使用。</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><blockquote class="mc md me"><p id="a06e" class="jn jo kl jp b jq jr js jt ju jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj kk ij bi translated">有问题吗？需要帮助吗？联系我！</p></blockquote><p id="7f43" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ky" href="https://github.com/sushantPatrikar" rel="noopener ugc nofollow" target="_blank"> Github </a></p><p id="9eac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ky" href="https://sushantpatrikar.github.io/" rel="noopener ugc nofollow" target="_blank">个人网站</a></p><p id="838c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ky" href="https://www.linkedin.com/in/sushant-patrikar/" rel="noopener ugc nofollow" target="_blank">领英</a></p><p id="a2fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">电子邮件:sushantpatrikarml@gmail.com</p></div></div>    
</body>
</html>