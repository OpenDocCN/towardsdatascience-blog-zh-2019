<html>
<head>
<title>Dress Segmentation with Autoencoder in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras 中基于自动编码器的服装分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dress-segmentation-with-autoencoder-in-keras-497cf1fd169a?source=collection_archive---------12-----------------------#2019-06-01">https://towardsdatascience.com/dress-segmentation-with-autoencoder-in-keras-497cf1fd169a?source=collection_archive---------12-----------------------#2019-06-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a0b7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从照片中提取服装</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7fc32425bcfbc85d0be4c7904c354ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*N9mR6fEKVeyZHR1z"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@oladimeg?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Oladimeji Odunsi</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d9aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">时尚行业是人工智能非常赚钱的领域。数据科学家可以在很多领域开发有趣的用例并提供好处。我已经展示了我对这个领域的兴趣<a class="ae ky" rel="noopener" target="_blank" href="/zalando-dress-recomendation-and-tagging-f38e1cbfc4a9">在这里</a>，我开发了一个从 Zalando 在线商店推荐和标记服装的解决方案。</p><p id="4628" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我试图进一步开发一个系统，该系统接收原始图像(从网络上获取或用智能手机制作)作为输入，并尝试提取其中显示的服装。请记住，分割的挑战是臭名昭著的极端噪声出现在原始图像；我们试图用聪明的技巧(在预处理期间)开发一个强大的解决方案来处理这个方面。</p><p id="3f30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，你也可以尝试将这个解决方案与<a class="ae ky" rel="noopener" target="_blank" href="/zalando-dress-recomendation-and-tagging-f38e1cbfc4a9">之前引用的</a>合并。这允许你开发一个系统，通过你外出时拍摄的照片，实时推荐和标记服装。</p><h1 id="de25" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据集</h1><p id="a65b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">最近，一个关于服装视觉分析和分割的<a class="ae ky" href="https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6" rel="noopener ugc nofollow" target="_blank"> Kaggle 竞赛</a>也启动了。这是一个非常有趣的挑战，但这并不适合我们…我的目标是从照片中提取服装，因此由于其冗余和细粒度的属性，这个数据集是不够的。我们需要包含大部分服装的图像，所以最好的选择是我们自己构建数据。</p><p id="a466" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我从网上收集了一些图片，这些图片包含了在不同场景下穿着不同类型女装的人们。下一步需要创建遮罩:如果我们希望训练一个能够只关注真正感兴趣的点的模型，这对于每个对象分割任务都是必要的。</p><p id="46ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面我报告一个数据样本供我们使用。我从网上收集了<em class="ms">原始的</em>图片，然后我很享受地进一步剪辑它们，把<em class="ms">人</em>和<em class="ms">衣服分开。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/6388ac76594949f3704f98a013d1ce01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X2C8S7MYIPUH-zz6F1ZIGA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Example of image segmentation</figcaption></figure><p id="c144" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们进行这种区分，是因为我们想在背景、皮肤和着装之间进行区分。背景和皮肤是这类问题中最相关的噪声源，所以我们尽量抑制它们。</p><p id="36c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这些剪报，我们可以重新创建我们的面具如下所示，这是简单的图像二值化。皮肤是因人和服饰的不同而获得的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/a43fce1b2212be43d6292b5217a79605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7JyGF3KsOn0cbQ_Rzb9OA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Example of masks</figcaption></figure><p id="3fdf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一步，我们将所有的图像合并成一个三维的图像。这张图片解码了我们感兴趣的<em class="ms">原始</em>图像的相关特征。我们的目的是保持背景，皮肤和服装之间的分离:这个结果对我们的范围来说是完美的！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/2b9938bd1c187a302dea92bbb2fb2394.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*jcUusKuzOn80f0VyHlAzQA.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Final mask</figcaption></figure><p id="7742" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们对数据集中的每一幅图像重复这个过程，以便为每一幅原始图像建立一个相关的三维蒙版。</p><h1 id="9d5e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">模型</h1><p id="e687" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们拥有一切来创造我们的模型。我们心目中的工作流程非常简单:</p><p id="06c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们拟合一个模型，该模型接收原始图像作为输入，并输出三维掩模，即它能够从原始图像中重建皮肤/背景和服装之间的期望分离。这样，当一个新的 raw 图像进来时，我们可以将其分成三个不同的部分:背景、皮肤和服装。我们只考虑我们感兴趣的通道(服装)，用它从输入图像中创建一个蒙版，并剪切它以重新创建原始服装。</p><p id="f70a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于 UNet 的力量，所有这些魔法都是可能的。这种深度卷积自动编码器通常用于像这样的分割任务。它很容易在 Keras 中复制，我们训练它为我们想要的遮罩的每个通道重建像素。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/f5ad863849b7dd44e99f85363d4849de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9brkl_uiTACo99SYqH0Ikg.jpeg"/></div></div></figure><p id="ff85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在开始训练之前，我们决定用它们的 RGB 平均值来标准化我们所有的原始图像。</p><h1 id="109b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结果和预测</h1><p id="1523" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们注意到，在预测过程中，当我们遇到具有高噪声(就模糊背景或皮肤而言)的图像时，我们的模型开始挣扎。这种不便可以通过简单地增加训练图像的数量来克服。但是我们也发展了一个聪明的捷径来避免这些错误。</p><p id="d6da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用 OpenCV 提供的 GrubCut 算法<strong class="lb iu"> </strong>。该算法利用高斯混合模型实现前景和背景的分离。这对我们很有帮助，因为它有助于指出前景中的人周围去噪。</p><p id="be43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们实现了一个简单的函数来实现它。我们假设我们感兴趣的人站在图像的中间。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="0686" class="nb lw it mx b gy nc nd l ne nf">def cut(img):</span><span id="17e6" class="nb lw it mx b gy ng nd l ne nf">   img = cv.resize(img,(224,224))<br/>    <br/>    mask = np.zeros(img.shape[:2],np.uint8)<br/>    bgdModel = np.zeros((1,65),np.float64)<br/>    fgdModel = np.zeros((1,65),np.float64)<br/>    height, width = img.shape[:2]</span><span id="c516" class="nb lw it mx b gy ng nd l ne nf">    rect = (50,10,width-100,height-20)<br/>    cv.grabCut(img,mask,rect,bgdModel,fgdModel,5,<br/>               cv.GC_INIT_WITH_RECT)<br/>    mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')<br/>    img2 = img*mask2[:,:,np.newaxis]<br/>    img2[mask2 == 0] = (255, 255, 255)<br/>    <br/>    final = np.ones(img.shape,np.uint8)*0 + img2<br/>    <br/>    return mask, final</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/149e67d5214d7a4e3b9a738711e4692b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H0CZh0vN5KYebgj11EZuzQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">GrubCut in action</figcaption></figure><p id="e409" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们应用 UNet，并准备在新图像上看到一些结果！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/393458619150be6abc29ff8f62e59bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y8L0DHx7BwHUGptihcVeCw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Input — GrubCut + Prediction — Final Dress</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/37d8ff5643c1fea6775eb4b64951e3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0aHrqF4l4Zd3WuyC18kE-w.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Input — GrubCut + Prediction — Final Dress</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/3135f3577a1cdded1a572076b694cc5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*05aZ2ownFKNib6Q5Bl31xw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Input — GrubCut + Prediction — Final Dress</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/e88bb839ca1a094b0b8f3fafa2466b03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M-c572ooR_sY88t0p5KxQQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Input — GrubCut + Prediction — Final Dress</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/8101aa51b67abb9667b4c9deee60a791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sVS2lQRowmfmq5CFqEmtXA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Input — GrubCut + Prediction — Final Dress</figcaption></figure><p id="2978" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们预处理步骤与 UNet 功能相结合，能够获得更好的性能。</p><h1 id="1133" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">摘要</h1><p id="9036" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这篇文章中，我们开发了一个端到端的服装分割解决方案。为了达到这个目的，我们利用强大的自动编码器结合巧妙的预处理技术。我们计划这个解决方案是为了在真实照片的真实场景中使用它，并有可能在它的基础上建立一个视觉推荐系统。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><p id="d21c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank">T3】查看我的 GITHUB 回购 T5】</a></p><p id="7e0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div></div>    
</body>
</html>