<html>
<head>
<title>Why Gradient descent isn’t enough: A comprehensive introduction to optimization algorithms in neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么梯度下降是不够的:神经网络优化算法综合介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096?source=collection_archive---------10-----------------------#2019-05-07">https://towardsdatascience.com/why-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096?source=collection_archive---------10-----------------------#2019-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="2bf8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经网络的目标是最小化<strong class="js iu"> <em class="ko">、</em> </strong>的损失，以产生更好更准确的结果<strong class="js iu"> <em class="ko">。</em> </strong>为了尽量减少损失，我们需要更新内部学习参数(特别是<strong class="js iu"><em class="ko"/></strong><strong class="js iu"><em class="ko">偏差</em> </strong>)。这些参数根据某个<em class="ko">更新规则/功能</em>进行更新。通常，我们认为<strong class="js iu"> <em class="ko">渐变下降</em> </strong>是一种更新规则。现在出现了两种类型的问题。</p><ul class=""><li id="7830" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated"><strong class="js iu">更新应该使用多少/哪些数据？</strong></li><li id="10ff" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">应该使用什么更新规则？</li></ul><p id="5e55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本帖在更好优化的背景下，以最简单的方式围绕这两个问题和答案展开。在这篇文章中，我将展示优化算法的直观形象，它们的不同类型和变体。</p><blockquote class="ld le lf"><p id="c9df" class="jq jr ko js b jt ju jv jw jx jy jz ka lg kc kd ke lh kg kh ki li kk kl km kn im bi translated"><strong class="js iu">补充说明</strong></p><p id="af3c" class="jq jr ko js b jt ju jv jw jx jy jz ka lg kc kd ke lh kg kh ki li kk kl km kn im bi translated">本文假设读者对神经网络的概念、前向和后向传播、权重初始化、激活函数等有基本的了解。如果你不熟悉，我建议你关注我的其他关于这些话题的文章。</p><p id="a8b9" class="jq jr ko js b jt ju jv jw jx jy jz ka lg kc kd ke lh kg kh ki li kk kl km kn im bi translated"><a class="ae lj" rel="noopener" target="_blank" href="/forward-propagation-in-neural-networks-simplified-math-and-code-version-bbcfef6f9250">神经网络中的前向传播——简化的数学和代码版本</a></p><p id="fb1d" class="jq jr ko js b jt ju jv jw jx jy jz ka lg kc kd ke lh kg kh ki li kk kl km kn im bi translated"><a class="ae lj" rel="noopener" target="_blank" href="/why-better-weight-initialization-is-important-in-neural-networks-ff9acf01026d">为什么更好的权重初始化在神经网络中很重要？</a></p><p id="c8ea" class="jq jr ko js b jt ju jv jw jx jy jz ka lg kc kd ke lh kg kh ki li kk kl km kn im bi translated"><a class="ae lj" rel="noopener" target="_blank" href="/analyzing-different-types-of-activation-functions-in-neural-networks-which-one-to-prefer-e11649256209">分析神经网络中不同类型的激活函数——选择哪一种？</a></p></blockquote><h1 id="65e7" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">优化算法</h1><p id="1eac" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated"><strong class="js iu">优化算法</strong>试图通过遵循某种更新规则来最小化<strong class="js iu">损失(成本)</strong>。损失是一个<em class="ko">数学</em>函数，表示<em class="ko">预测值</em>和<em class="ko">实际值</em>之间的差异。损耗取决于实际值，该值是借助于<strong class="js iu"> <em class="ko">学习参数(权重</em> </strong>和<strong class="js iu"> <em class="ko">偏差)</em> </strong>和<em class="ko">输入</em>得出的。因此，学习<em class="ko">参数</em>对于更好的训练和产生准确的结果非常重要。为了找出这些参数的最佳值，我们需要不断地<em class="ko">更新</em>它们。为此，应该有一些更新规则。所以我们使用各种<strong class="js iu"> <em class="ko">优化算法</em> </strong>遵循一定的更新规则，每个<em class="ko">优化算法</em>都有不同的方法来计算、更新和找出模型参数的最优值。</p><h1 id="769a" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">优化算法的类型</h1><p id="db4b" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">基于我们的第一个问题<strong class="js iu">“一次更新应该使用多少数据”</strong>优化算法可以分为<strong class="js iu"><em class="ko"/></strong><strong class="js iu"><em class="ko">小批量梯度下降</em> </strong>和<strong class="js iu"> <em class="ko">随机梯度下降。</em> </strong></p><p id="ce16" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其实基本算法就是<em class="ko">梯度下降。小批量梯度下降</em>和<em class="ko">随机梯度下降</em> <strong class="js iu"> <em class="ko"> </em> </strong>是基于所取数据量的两种不同策略。这两个也被称为<em class="ko">梯度下降的变种。</em></p><h1 id="ca96" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><em class="mn">梯度下降</em></h1><p id="cbf8" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated"><strong class="js iu"> <em class="ko">梯度下降</em> </strong>是最常用和流行的<strong class="js iu"> <em class="ko">迭代</em> </strong>机器学习算法。它也是其他优化算法的基础。<em class="ko">梯度下降</em>有以下<em class="ko">更新规则</em>用于<em class="ko">重量</em>参数</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/207d7e62be28f71c7cd7c49734872698.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*faTF4_-a7qMAzw3y27wzOw.png"/></div></figure><p id="ad18" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于在用于更新参数的<em class="ko">反向传播</em>期间，计算损耗 w.r.t. a 参数的<em class="ko">导数。该<em class="ko">导数</em>可以依赖于多个变量，因此使用<strong class="js iu"> <em class="ko">乘法链规则</em> </strong>进行计算。为此，需要一个<strong class="js iu"> <em class="ko">梯度</em> </strong>。<strong class="js iu"> <em class="ko">梯度</em> </strong>是表示增加方向的向量。</em></p><p id="3929" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">对于梯度计算，我们需要计算损耗对<em class="ko">参数</em>的<em class="ko">导数，并在<em class="ko">梯度的反方向更新<em class="ko">参数</em>。</em>T9】</em></strong></p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/5a887d3f237adccbf0e8fe34fa8f1feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*jYQYuHpHdkZqNFQKJSuDTw.png"/></div></figure><p id="5893" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的理想凸曲线图像显示了与<em class="ko">梯度</em>方向相反的<em class="ko">权重更新</em>。因为我们可以注意到<em class="ko">权重</em>的值太大和太小，所以<em class="ko">损失</em>最大，我们的目标是<em class="ko">最小化</em>损失<em class="ko">T21，所以<em class="ko">权重</em>被更新。如果<em class="ko">坡度</em>为负，则<strong class="js iu"> <em class="ko">向正侧下降</em> </strong>(下潜)，如果<em class="ko">坡度</em>为正，则向负侧下降，直到找到<em class="ko">坡度</em>的最小值。</em></p><p id="a7bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Python 中使用具有<em class="ko"> sigmoid </em>激活函数的单个神经元进行<strong class="js iu"> <em class="ko">梯度下降</em> </strong>的算法</p><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="1b41" class="nc ll it my b gy nd ne l nf ng">def sigmoid(w,b,x):<br/>    return 1.0 / (1.0 + np.exp(-w*x + b))</span><span id="0efb" class="nc ll it my b gy nh ne l nf ng">def grad_w(w,b,x,y):<br/>    fx = sigmoid(w,b,x)<br/>    return (fx — y) * fx * (1-fx) * x</span><span id="4225" class="nc ll it my b gy nh ne l nf ng">def grad_b(w,b,x,y):<br/>    fx = sigmoid(w,b,x)<br/>    return (fx — y) * fx * (1-fx)</span><span id="12d8" class="nc ll it my b gy nh ne l nf ng">def do_gradient_descent():<br/>    w,b,eta = -2, -2, 1.0<br/>    max_epochs = 1000<br/>    for i in range(max_epochs):<br/>        dw,db = 0,0<br/>        for x,y in zip(X,Y):<br/>            dw += grad_w(w,b,x,y)<br/>            db += grad_b(w,b,x,y)<br/>        w = w — eta * dw<br/>        b = b — eta * db</span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/4f2c036eaf3728bff7c70a19f4d78097.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*nuoD-tQylhMj-SF8WJfUjg.gif"/></div></figure><p id="50e8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的<em class="ko">动画</em>表现了算法在 1000 个<em class="ko">纪元</em>后如何收敛。该<em class="ko">动画</em>中使用的误差面与输入一致。这个误差表面在 2D 空间中是动画的。对于 2D，使用<em class="ko">等高线图</em>，其中等高线表示第三维，即<strong class="js iu"> <em class="ko">误差</em> </strong>。红色区域代表高误差值，红色区域的强度越大，误差越大。类似地，蓝色区域表示误差的低值，蓝色区域的强度越小，误差越小。</p><p id="47d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">标准梯度下降</em> </strong>仅在每个历元后更新<em class="ko">参数</em>，即在计算所有观测值的<em class="ko">导数</em>后，它更新<em class="ko">参数</em>。这种现象可能会导致下面的<strong class="js iu"> <em class="ko">注意事项</em> </strong>。</p><ul class=""><li id="0e2c" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">对于非常大的数据集来说，这可能非常慢，因为每个<em class="ko">时期</em>只有一次<em class="ko">更新</em>，所以大量的<strong class="js iu"> <em class="ko">时期</em> </strong>需要有大量的<em class="ko">更新</em>。</li><li id="1bfa" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">对于大型数据集，数据的矢量化不适合<strong class="js iu"> <em class="ko">内存</em> </strong>。</li><li id="8b74" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">对于<em class="ko">非凸</em>曲面，可能只找到<strong class="js iu"> <em class="ko">局部最小值。</em>T85】</strong></li></ul><p id="62c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们看看<strong class="js iu"> <em class="ko">不同的渐变下降</em> </strong>如何应对这些挑战。</p><h1 id="05db" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">随机梯度下降</h1><p id="e260" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated"><strong class="js iu"> <em class="ko">随机梯度下降</em> </strong>为<em class="ko">每次观测</em>更新<em class="ko">参数</em>，这导致更多的更新次数<em class="ko">。因此，这是一种更快的方法，有助于更快地做出决策。</em></p><p id="1f5e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Python 中使用具有<em class="ko"> sigmoid </em>激活函数的单个神经元的<em class="ko">随机</em> <em class="ko">梯度下降</em>算法</p><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="a917" class="nc ll it my b gy nd ne l nf ng">def do_stochastic_gradient_descent():<br/>    w,b,eta = -2, -2, 1.0<br/>    max_epochs = 1000<br/>    for i in range(max_epochs):<br/>        dw,db = 0,0<br/>        for x,y in zip(X,Y):<br/>            dw += grad_w(w,b,x,y)<br/>            db += grad_b(w,b,x,y)<br/>            w = w — eta * dw<br/>            b = b — eta * db</span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/a23a20f7f0bbbc6e76bccf327683f4ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*eqbCPD7Yx_nRchmP6YtjYA.gif"/></div></figure><p id="4e44" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">在这个动画中可以注意到不同方向的</em> </strong>更快的更新。这里，大量振荡发生，导致<em class="ko">用<strong class="js iu"> <em class="ko">更高的方差</em> </strong>更新</em>，即<strong class="js iu"><em class="ko"/></strong><strong class="js iu"><em class="ko">更新</em> </strong>。这些嘈杂的更新帮助寻找<strong class="js iu"> <em class="ko">新的</em> </strong>和<strong class="js iu"> <em class="ko">更好的局部最小值</em> </strong>。</p><p id="414d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">新币的缺点</strong></p><ul class=""><li id="3f7a" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">因为<strong class="js iu"> <em class="ko">贪婪接近</em> </strong>，它只近似<strong class="js iu"><em class="ko"/></strong>的渐变。</li><li id="be4b" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">由于<strong class="js iu"> <em class="ko">的频繁波动</em> </strong>，它会将<strong class="js iu"><em class="ko"/></strong>保持在期望的<strong class="js iu"> <em class="ko">精确最小值</em> </strong>附近。</li></ul><p id="35ea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们看看梯度下降的另一种<strong class="js iu"> <em class="ko">变体</em> </strong>如何应对这些挑战。</p><h1 id="cce1" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">小批量梯度下降</h1><p id="66d9" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated"><strong class="js iu"> <em class="ko"> GD </em> </strong>的另一个变体解决了<strong class="js iu"><em class="ko"/></strong>SGD 的问题，它位于<strong class="js iu"> <em class="ko"> GD </em> </strong>和<strong class="js iu"> <em class="ko"> SGD 之间。</em> </strong> <strong class="js iu"> <em class="ko">小批量梯度下降</em> </strong>为有限数量的观察值更新参数。这些观察结果一起被称为具有固定大小的<strong class="js iu">批次</strong>。<strong class="js iu">批量大小</strong>选择为 64 的倍数，例如 64、128、256 等。通过<em class="ko">小批量 GD </em>，一个时期内会发生更多更新。</p><p id="9d93" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Python 中使用具有<em class="ko"> sigmoid </em>激活函数的单个神经元的<em class="ko">小批量</em> <em class="ko">梯度下降</em>算法</p><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="f7d9" class="nc ll it my b gy nd ne l nf ng">def do_mini_batch_gradient_descent():<br/>    w,b,eta = -2, -2, 1.0<br/>    max_epochs = 1000<br/>    mini_batch_size = 3<br/>    num_of_points_seen = 0<br/>    for i in range(max_epochs):<br/>        dw,db = 0,0<br/>        for x,y in zip(X,Y):<br/>            dw += grad_w(w,b,x,y)<br/>            db += grad_b(w,b,x,y)<br/>            num_of_points_seen += 1<br/>        if num_of_points_seen % mini_batch_size == 0:<br/>            w = w — eta * dw<br/>            b = b — eta * db</span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/48cac6b22de6f9816b635634bf9ffd24.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*b9hLP6yfXtlZzfAOAAe7-A.gif"/></div></figure><p id="de22" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到，与<strong class="js iu"> <em class="ko"> SGD </em> </strong>相比，<strong class="js iu"> <em class="ko">小批量</em> </strong>中的振荡更少。</p><p id="b797" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">基本符号</strong></p><p id="b3a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 1 个时期</strong> =整个数据的一次通过</p><p id="693b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 1 步</strong> =一次参数更新</p><p id="ba29" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> N </strong> =数据点数</p><p id="016e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">B  =小批量</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/df8825c1b4cdddba29b26abd6a10ca18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*XKVDn-pHsGpX8Jh1wMZloQ.png"/></div></figure><p id="453f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">小批量 GD 的优势</strong></p><ul class=""><li id="bc4e" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">与 SGD 相比，更新的噪音更小，从而导致更好的收敛。</li><li id="b7b1" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">与 GD 相比，单个历元中的更新次数较多，因此大型数据集所需的历元数量较少。</li><li id="d8e5" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">非常适合处理器内存，使计算速度更快。</li></ul><h1 id="307d" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">更好的优化 w.r.t .梯度下降</h1><p id="b319" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated"><strong class="js iu"> <em class="ko">误差面</em> </strong>包含<em class="ko">较不严谨</em>以及<em class="ko">较不严谨</em>的区域。在<em class="ko">反向传播</em>期间，斜率<em class="ko">较大的区域</em>的参数更新较多，而斜率<em class="ko">较小的区域</em>的参数更新较少。参数变化越大，损耗变化越大，同样，参数变化越小，损耗变化越小。</p><p id="5394" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果参数初始化位于<em class="ko">缓坡</em>区域，则需要大量的<em class="ko">时期</em>来浏览这些区域。这是因为<em class="ko">坡度</em>在<em class="ko">缓坡</em>区域非常小。于是它随着<strong class="js iu"> <em class="ko">小婴儿的脚步</em> </strong>在<em class="ko">平缓的</em>区域移动。</p><p id="5fa7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">考虑在如下所示的平坦表面中初始化的情况，其中使用了<strong class="js iu"><em class="ko"/></strong>并且当<strong class="js iu"><em class="ko"/></strong>在<strong class="js iu"> <em class="ko">平坦表面</em> </strong>中时<strong class="js iu"> <em class="ko">误差</em> </strong>不减小。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d941dbe389000b15df4a117205ea3382.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*5C6RvLtqXlOcK5lJboezWg.gif"/></div></figure><p id="8142" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">即使在大量的<em class="ko">历元</em>之后，例如 10000，算法也不会收敛<strong class="js iu"><em class="ko"/></strong>。</p><p id="8ad5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于这个问题，<em class="ko">收敛</em>不容易实现，学习需要太多时间。</p><p id="690f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了克服这个问题<strong class="js iu"> <em class="ko">使用了基于动量的梯度下降</em> </strong>。</p><h1 id="2e4f" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><strong class="ak"> <em class="mn">基于动量的梯度下降</em> </strong></h1><p id="f403" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">考虑这样一种情况，为了到达你想要的目的地，你不断地被要求沿着同一个方向前进，一旦你确信你正在沿着正确的方向前进，那么你就开始迈出<strong class="js iu"> <em class="ko">更大的步伐</em> </strong>并且你在同一个方向上不断获得<strong class="js iu"><em class="ko"/></strong>的动力。</p><p id="5801" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与此类似，如果<strong class="js iu"> <em class="ko">坡度</em> </strong>长期处于<strong class="js iu"> <em class="ko">平面</em> </strong>中，那么与其采取恒定的步数，不如采取更大的步数 并保持<strong class="js iu"> <em class="ko">动量</em> </strong>继续。这种方法被称为<strong class="js iu"> <em class="ko">基于动量的梯度下降</em> </strong>。</p><p id="37c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">权重参数基于动量的梯度下降更新规则</strong></p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/dc6fdd4ca6214d884a31dab983908d7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*w3A9ZNZowdjjUDzYpsqtGQ.png"/></div></figure><p id="d9b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">伽马参数(</em> γ) </strong>是动量项，表示你想要多大的加速度。这里随着<strong class="js iu"> <em class="ko">电流梯度(</em> η∇w(t)，</strong>移动也根据历史<strong class="js iu">(γv(t1))</strong>进行，因此<strong class="js iu"><em class="ko"/></strong>更新变大，这导致更快的移动和更快的<strong class="js iu"><em class="ko"/></strong>。</p><p id="f55a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> v(t) </strong>是<strong class="js iu"> <em class="ko">指数衰减加权和</em> </strong>，随着<strong class="js iu"> t </strong>的增加<strong class="js iu">γV(t1)</strong>变得越来越小，即该等式包含小幅度的更远更新和大幅度的最近更新。</p><p id="27bb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">基于动量的梯度下降</strong>在 Python 中用于 sigmoid 神经元</p><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="8833" class="nc ll it my b gy nd ne l nf ng">def do_momentum_based_gradient_descent():<br/>    w,b,eta,max_epochs = -2, -2, 1.0, 1000<br/>    v_w, v_b = 0, 0<br/>    for i in range(max_epochs):<br/>        dw,db = 0,0<br/>        for x,y in zip(X,Y):<br/>            dw += grad_w(w,b,x,y)<br/>            db += grad_b(w,b,x,y)<br/>        v_w = gamma * v_w + eta * dw<br/>        v_b = gamma * v_b + eta * db<br/>        w = w — v_w<br/>        b = b — v_b</span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/4d475c597396a0ffa4cdd177d1183ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*MxPOjaJDWZeE2zNUZxpdNQ.gif"/></div></figure><p id="74c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此算法<strong class="js iu"> <em class="ko">在<strong class="js iu"> <em class="ko">一致渐变</em> </strong>方向添加动量</em> </strong>，如果<strong class="js iu"><em class="ko"/></strong>在<strong class="js iu"> <em class="ko">不同方向</em> </strong>则<strong class="js iu"> <em class="ko">取消动量</em> </strong>。</p><p id="84c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">基于动量的梯度下降问题</strong></p><p id="445d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在导致精确的<strong class="js iu"> <em class="ko">期望最小值</em> </strong>的<strong class="js iu"> <em class="ko">谷</em> </strong>中，有大量的<em class="ko">振荡</em>使用<em class="ko">基于动量的 GD </em>。因为它<strong class="js iu"> <em class="ko">越过了</em></strong><strong class="js iu"><em class="ko">的最小值与</em> </strong>的较大步长，并且需要一个<strong class="js iu"> <em class="ko">的掉头</em> </strong>但是又一次越过，所以这个过程重复进行。这意味着用更大的步伐移动并不总是好的。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c0752b5959cda7895ed48fec1cbd1b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*nBI9KeSoepPGNNf25ANaJg.gif"/></div></figure><ul class=""><li id="7ba0" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated"><strong class="js iu"> <em class="ko">以动量为基础的 GD </em> </strong> <em class="ko">振荡</em>为一个<strong class="js iu"> <em class="ko">大次数</em> </strong>进出<strong class="js iu"> <em class="ko">小次数</em> </strong>。</li></ul><p id="bba2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了克服这个问题<strong class="js iu"> <em class="ko">内斯特罗夫加速梯度下降</em> </strong>被使用。</p><h1 id="bd35" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">内斯特罗夫加速梯度下降</h1><p id="094b" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">在<em class="ko">的动量基础上</em>的 GD 随着<em class="ko">的梯度</em>向山谷(极小区域)进发，这使得很多<strong class="js iu"> <em class="ko">【振荡】</em> </strong>在它之前<strong class="js iu"> <em class="ko"> </em> </strong> <em class="ko">汇聚。这个问题最初是由一位名叫尤里·内斯特罗夫的研究者发现并回应的。</em></p><p id="ea28" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">他建议，先通过<strong class="js iu"> <em class="ko">【先前动量】</em> </strong>使<em class="ko">运动</em>，然后计算此时的<strong class="js iu"><em class="ko"/></strong><em class="ko">更新</em><em class="ko">参数</em>。换句话说，在直接进行<em class="ko">更新</em>之前，它首先通过用<em class="ko">先前的动量</em>移动来向前看，然后它发现<em class="ko">梯度</em>应该是什么。</p><p id="7c69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种<em class="ko">向前看的</em>帮助<strong class="js iu"> <em class="ko">唠叨</em> </strong>比<strong class="js iu"> <em class="ko">基于动力的 GD 更快地完成它的工作(寻找最小值)。</em> </strong>因此<strong class="js iu"> <em class="ko">振荡</em> </strong>与基于<strong class="js iu"> <em class="ko">动量的【GD】</em></strong>相比<strong class="js iu"><em class="ko"/></strong>更少，并且错过<strong class="js iu"> <em class="ko">最小值的机会也更少。</em>T45】</strong></p><p id="1083" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">用于<em class="ko">重量</em>参数的 NAG 更新规则</strong></p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/bb47e375a5f913863214445d78eb21ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*aWTX8EOu10OkJzRzaR2P2A.png"/></div></figure><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/7b4aed0e481863e57dcc04a75c471ce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*SV3x2pD4lOGvaWmyMiSgPQ.png"/></div></figure><p id="086d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">Python 中针对 sigmoid 神经元的 NAG </strong>算法</p><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="1ec6" class="nc ll it my b gy nd ne l nf ng">def do_nag_gradient_descent():<br/>    w,b,eta,max_epochs = -2, -2, 1.0, 1000<br/>    v_w, v_b, gamma = 0, 0, 0.9<br/>    for i in range(max_epochs):<br/>        dw,db = 0,0<br/>        #compute the look ahead value<br/>        w = w — gamma * v_w<br/>        b = b — gamma * v_b<br/> <br/>       for x,y in zip(X,Y):<br/>           #compute the derivatives using look ahead value<br/>           dw += grad_w(w,b,x,y)<br/>           db += grad_b(w,b,x,y)<br/>       #Now move further in the opposite direction of that gradient<br/>       w = w — eta * dw<br/>       b = b — eta * db<br/> <br/>      #Now update the previous momentum<br/>      v_w = gamma * v_w + eta * dw<br/>      v_b = gamma * v_b + eta * db</span></pre><p id="0c52" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里 v_w 和 v_b 分别指<strong class="js iu"> v(t) </strong>和<strong class="js iu"> v(b) </strong>。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/b23994a6620f036163468a4a4625c4d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*5G4Adw4fy70IzVOuCjfu5w.gif"/></div></figure><h1 id="8fb2" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">适应性学习率的概念</h1><p id="dfa7" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">根据更新规则</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5966a836120d4f3e9d35b0d26472c691.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*jPYlnZJbprDsunEQX8D0vg.png"/></div></figure><p id="ab2d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">更新</em>正比于<strong class="js iu"> <em class="ko">渐变</em> </strong> ( <strong class="js iu"> ∇w).</strong>变小<em class="ko">渐变</em>变小<em class="ko">更新</em> <strong class="js iu"> </strong>而<em class="ko">渐变</em>与<strong class="js iu"> <em class="ko">输入</em> </strong>成正比。因此<em class="ko">更新</em>也依赖于<em class="ko">输入</em>。</p><p id="471d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">对自适应学习率的需求</strong></p><p id="d829" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于实时数据集，大多数特征是<strong class="js iu"> <em class="ko">稀疏</em> </strong>的，即具有零值。因此，在大多数情况下，相应的<em class="ko">梯度</em>为零，因此参数<em class="ko">更新</em>也为零。为了引起这个问题的共鸣，这些更新应该被提升也就是一个<strong class="js iu"> <em class="ko">高学习率</em> </strong>对于<em class="ko"/><strong class="js iu"><em class="ko"/></strong>的特性。因此<em class="ko">的学习率</em>应该是<strong class="js iu"><em class="ko"/></strong>对于相当<em class="ko">稀疏</em> <strong class="js iu"> <em class="ko"> </em> </strong>的数据。</p><p id="6008" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">换句话说，如果我们处理的是<strong class="js iu"> <em class="ko">稀疏特征</em> </strong>那么<em class="ko">的学习率</em>应该是<em class="ko">高</em>反之对于<em class="ko"> </em> <strong class="js iu"> <em class="ko">密集特征</em> </strong> <em class="ko">学习率</em>应该是<em class="ko">低</em>。</p><p id="2f41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko"> Adagrad、RMSProp、Adam </em> </strong>算法都是基于<em class="ko">自适应学习率</em>的概念。</p><h1 id="2c7b" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">阿达格拉德</h1><p id="dfe0" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">它采用了基于特征的<strong class="js iu"> <em class="ko">【稀疏度】</em> </strong>的<em class="ko">学习率(</em> <strong class="js iu"> η) </strong> <em class="ko"> </em>。因此具有<strong class="js iu">小<em class="ko">更新(稀疏特征)</em> </strong>的参数具有高<em class="ko">学习率</em>，而具有<strong class="js iu">大<em class="ko">更新(密集特征)</em> </strong>的参数具有低<em class="ko">学习率</em>。<em class="ko"> </em>因此<strong class="js iu"> adagrad </strong>对每个<em class="ko">参数使用不同的<em class="ko">学习率</em>。</em></p><p id="bc75" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> Adagrad </strong>对<em class="ko">重量</em>参数的更新规则</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c83e56eaaa13882146cb2d1a54780d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*iXwh6i5NLsyigdIwMQg10Q.png"/></div></figure><p id="4bb5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> v(t) </strong>累加梯度的平方和。<strong class="js iu"> ∇w(t) </strong>的平方忽略了梯度的符号。<strong class="js iu"> v(t) </strong>表示到时间<strong class="js iu"> t </strong>的累计坡度。<strong class="js iu"><em class="ko">ε</em></strong>中的分母避免了<strong class="js iu"> <em class="ko">被零除的几率</em> </strong>。</p><p id="659c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，如果对于一个<em class="ko">参数</em>来说<strong class="js iu"> v(t) </strong>为<strong class="js iu">低</strong>(由于更少的<em class="ko">更新</em>到时间<strong class="js iu"> t </strong>，那么有效的<em class="ko">学习率</em>将为<strong class="js iu">高</strong>，并且如果对于一个<em class="ko">参数<strong class="js iu">来说</strong>为高</em></p><p id="6be3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">用于 sigmoid 神经元的 Python 中的 Adagrad 算法</p><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="38e8" class="nc ll it my b gy nd ne l nf ng">def do_adagrad():<br/>    w,b,eta,max_epochs = -2, -2, 1.0, 1000<br/>    v_w, v_b = 0, 0<br/>    for i in range(max_epochs):<br/>        dw,db = 0,0<br/>        for x,y in zip(X,Y):<br/>            dw += grad_w(w,b,x,y)<br/>            db += grad_b(w,b,x,y)<br/>        v_w += dw**2<br/>        v_b += db**2<br/>        self.w -= (eta / np.sqrt(v_w) + eps) * dw<br/>        self.b -= (eta / np.sqrt(v_b) + eps) * db</span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/503c224d4a485e1dcfdcda39ac92fb72.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*b2AOmFFikbWDbr7ExMBgYQ.gif"/></div></figure><p id="e986" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">Adagrad 的缺点</strong></p><ul class=""><li id="107f" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">学习率急剧下降</li></ul><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/91f978e8f0490f58a978ef08caa4402c.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Ewo2vS0XfcbPYD3-UwalBw.gif"/></div></figure><p id="d8d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于<em class="ko">密集特征</em>对应的<em class="ko">参数</em>(尤其是<em class="ko">偏差</em>)，经过几次<em class="ko">更新</em>后，由于<em class="ko">平方梯度</em>的累积，分母快速增长，导致<em class="ko">学习速率</em> <strong class="js iu"> <em class="ko">快速衰减</em> </strong>。因此，在有限数量的<em class="ko">更新</em>之后，算法拒绝学习，并且<em class="ko">收敛缓慢</em>，即使我们运行它大量的<em class="ko">时期</em>。<em class="ko">梯度</em>达到不良最小值(接近期望的<em class="ko">最小值)</em>，但不在精确的<em class="ko">最小值</em>。因此<strong class="js iu"> adagrad </strong>导致<em class="ko">偏差</em>参数的学习率下降。</p><h1 id="2296" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">RMSProp</h1><p id="4aad" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated"><strong class="js iu"> <em class="ko"> RMSProp </em> </strong>克服了<strong class="js iu"><em class="ko"/></strong>的<strong class="js iu"> <em class="ko">衰减学习率</em> </strong>问题，防止了<strong class="js iu">【v(t)<em class="ko">中<strong class="js iu"> <em class="ko"> </em> </strong>的快速增长。</em>T51】</strong></p><p id="81b3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它不是从一开始就累积<em class="ko">平方梯度</em>，而是在某个部分(权重)累积<em class="ko">先前梯度</em>，这阻止了<strong class="js iu"> v(t) </strong>的快速增长，并且由于这个原因，算法保持学习并试图<em class="ko">收敛</em>。</p><p id="88f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> RMSProp </strong>更新<em class="ko">重量</em>参数的规则</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/442b12b10369af63788db834b03bde35.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*_2iZQroJGtJBNKmXxCk2Lw.png"/></div></div></figure><p id="a4fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里的<strong class="js iu"> v(t) </strong>是前面所有平方梯度的<strong class="js iu"> <em class="ko">指数衰减平均值</em> </strong>。<strong class="js iu"><em class="ko">β</em></strong>参数值被设置为与动量项相似的值。直到时间 t 的移动平均值<strong class="js iu"> v(t) </strong>取决于<strong class="js iu"> <em class="ko">加权的先前平均梯度</em> </strong>和<strong class="js iu"> <em class="ko">当前梯度</em> </strong>。<strong class="js iu"> v(t) </strong>保持(<strong class="js iu">)</strong>一个固定的窗口时间。</p><p id="56e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">用于 sigmoid 神经元的 Python 中的 Adagrad </strong>算法</p><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="2a3c" class="nc ll it my b gy nd ne l nf ng">def do_RMSProp():<br/>    w,b,eta,max_epochs = -2, -2, 1.0, 1000<br/>    v_w, v_b = 0, 0<br/>    for i in range(max_epochs):<br/>        dw,db = 0,0<br/>        for x,y in zip(X,Y):<br/>            dw += grad_w(w,b,x,y)<br/>            db += grad_b(w,b,x,y)<br/>        v_w = beta * v_w + (1 — beta) * dw**2<br/>        v_b = beta * v_b + (1 — beta) * db**2<br/>        self.w -= (eta / np.sqrt(v_w) + eps) * dw<br/>        self.b -= (eta / np.sqrt(v_b) + eps) * db</span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/7ba519ecbbbb0a34980d0ecb8141e239.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*g7Vi1mYHxa1Ai3bwZovDxw.gif"/></div></figure><p id="b058" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【RMSProp 的问题</p><ul class=""><li id="4c08" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">具有高学习率或大梯度的大量振荡</li></ul><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/361a999e3a5c6516b2aa9b60cc90f373.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Kmmt5FsBdZKsvWZeWC8Mnw.gif"/></div></figure><p id="3f8d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止在<strong class="js iu"> <em class="ko"> Adagrad，RMSProp </em> </strong>中我们在计算不同的<strong class="js iu"> <em class="ko">学习速率</em> </strong>对于不同的参数，我们可以有不同的<strong class="js iu"> <em class="ko">动量</em> </strong>对于不同的参数。<strong class="js iu"> <em class="ko">亚当</em> </strong>算法引入了<strong class="js iu"> <em class="ko">自适应动量</em> </strong>连同<strong class="js iu"> <em class="ko">自适应学习率</em> </strong>。</p><h1 id="5a96" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h1><p id="fb78" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated"><strong class="js iu">自适应矩估计(Adam) </strong>计算<strong class="js iu"/><strong class="js iu"/><em class="ko">先前梯度的指数衰减平均值 m </em> <strong class="js iu"> <em class="ko"> (t) </em> </strong>以及一个<strong class="js iu"> <em class="ko">自适应学习率。亚当</em> </strong>是基于<strong class="js iu"> <em class="ko">动量的 GD </em> </strong>和<strong class="js iu"> <em class="ko"> RMSProp 的组合形式。</em>T47】</strong></p><p id="4304" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<strong class="js iu"> <em class="ko">基于动量的 GD 中，</em> </strong> <em class="ko">先前梯度(历史)</em>用于计算当前梯度，而在 RMSProp <em class="ko">先前梯度(历史)</em>用于根据特征<em class="ko">调整<em class="ko">学习率</em>。因此</em> <strong class="js iu"> <em class="ko">亚当</em> </strong>处理<em class="ko">自适应学习速率</em>和<em class="ko">自适应动量</em>其中<strong class="js iu"> <em class="ko"> RMSProp </em> </strong>确保<strong class="js iu"> v(t) </strong>不会快速增长以避免学习速率和<strong class="js iu"> m(t) </strong>从<strong class="js iu"> <em class="ko">动量衰减的机会</em></strong></p><p id="5440" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">亚当</em> </strong>权重参数更新规则</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/9529f51beb903f0631825116e4b150c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*rVzUkpsTiIJl8fRfyjRH5Q.png"/></div></figure><p id="d7da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里的<strong class="js iu"> <em class="ko"> m(t) </em> </strong>和<strong class="js iu"><em class="ko">【v(t)</em></strong>都是从第一时刻得到的<strong class="js iu"> <em class="ko">的值。</em></strong></p><p id="d201" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> Adam </strong>使用<strong class="js iu">偏差修正值(<em class="ko">无中心方差)</em> </strong>梯度用于<em class="ko">更新规则</em>，这些值是通过二阶矩<strong class="js iu"> <em class="ko">获得的。</em>T115】</strong></p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/6df89215ccee0032be4b697440da1e19.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*zN5DtJoo605GZS0QS-1U3Q.png"/></div></figure><p id="98b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最终的<em class="ko">更新规则</em>如下所示</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a636f97b792c59b41befd5f169c7b476.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*ikZwULDSmQCEfGqJGnITYA.png"/></div></figure><p id="5d36" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">Python 中针对 sigmoid 神经元的 Adam </strong>算法</p><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="dadb" class="nc ll it my b gy nd ne l nf ng">def do_Adam():<br/>    w,b,eta,max_epochs = -2, -2, 1.0, 1000<br/>    v_w, v_b = 0, 0<br/>    m_w, m_b = 0, 0<br/>    num_updates = 0<br/>    for i in range(epochs):<br/>        dw, db = 0, 0<br/>        for x, y in zip(X, Y):<br/>            dw = self.grad_w(x, y)<br/>            db = self.grad_b(x, y)<br/>        num_updates += 1<br/>        m_w = beta1 * m_w + (1-beta1) * dw<br/>        m_b = beta1 * m_b + (1-beta1) * db<br/>        v_w = beta2 * v_w + (1-beta2) * dw**2<br/>        v_b = beta2 * v_b + (1-beta2) * db**2<br/>        #m_w_c, m_b_c, v_w_c and v_b_c for bias correction   <br/>        m_w_c = m_w / (1 — np.power(beta1, num_updates))<br/>        m_b_c = m_b / (1 — np.power(beta1, num_updates))<br/>        v_w_c = v_w / (1 — np.power(beta2, num_updates))<br/>        v_b_c = v_b / (1 — np.power(beta2, num_updates))<br/>        self.w -= (eta / np.sqrt(v_w_c) + eps) * m_w_c<br/>        self.b -= (eta / np.sqrt(v_b_c) + eps) * m_b_c</span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/fa49896254310c16dc4b3bb3e6f898af.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*df0uRBl949xJGv7jp8pGdA.gif"/></div></figure><p id="fc9a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以在<strong class="js iu"> <em class="ko">中亚当</em> </strong>不同于<strong class="js iu"> <em class="ko"> RMSProp </em> </strong>振荡<em class="ko">更少</em>并且它更确定地向正确的方向移动，这导致<strong class="js iu"> <em class="ko">更快</em> </strong> <strong class="js iu"> <em class="ko">收敛</em> </strong>和<strong class="js iu"> <em class="ko">更好的优化</em> </strong>。</p><h2 id="9157" class="nc ll it bd lm nx ny dn lq nz oa dp lu kb ob oc ly kf od oe mc kj of og mg oh bi translated">结束注释</h2><p id="cb09" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">在本文中，我讨论了不同类型的优化算法以及使用每种算法时可能遇到的常见问题。一般来说，<strong class="js iu"> Adam </strong>带<strong class="js iu">小批量</strong>是深度神经网络训练的首选。</p></div></div>    
</body>
</html>