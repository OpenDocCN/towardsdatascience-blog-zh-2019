<html>
<head>
<title>Building own Logistic Classifier in R [Logistic Trilogy, Part 2]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 R 中构建自己的逻辑分类器[逻辑三部曲，第 2 部分]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-own-logistic-classifier-in-r-logistic-trilogy-part-2-a36be209d2c?source=collection_archive---------25-----------------------#2019-09-12">https://towardsdatascience.com/building-own-logistic-classifier-in-r-logistic-trilogy-part-2-a36be209d2c?source=collection_archive---------25-----------------------#2019-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="3590" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果有人从为数据科学项目或统计项目开发自己的代码中获得了巨大的乐趣，那么这就是适合他/她的文章。</p><p id="0f3f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可能在 R，<br/> <strong class="js iu"> <em class="ko"> glm(y~x，data，family=binomial)中用过或者学过 glm 函数。</em> </strong>对提供的数据进行逻辑回归拟合，取 y 为响应变量，x 为预测变量。但你有没有想过背后到底发生了什么？如果我们能理解这一点，我们能在 R 中为逻辑构建我们自己的函数吗？让我们一起试试。只有一个条件，我们将限制自己只进行二元分类。</p><p id="0d65" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">开发一个模型首先需要什么？数据，分析中最重要的部分。我们需要一个可以是 1 或 0 的响应变量 Y 和 p 个预测变量 X1，X2，…，Xp。假设观察总数为 n。每个模型都有一些假设，</p><p id="5e8f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一般来说，假设是-</p><ol class=""><li id="4400" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">Yi 是独立的，并且每个 Yi 遵循具有参数<strong class="js iu"> pi </strong>的伯努利分布</li><li id="48c3" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">预测因子 X1，X2，…Xp 是不相关的</li></ol><p id="4232" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在逻辑框架下，假设是<strong class="js iu"> pi= </strong></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/073552dd584cf350e2f02ad3a2fb8ea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YRNQSjRI8ezwWzh0EsVEkw.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Logistic Inverse link function (CDF of standard Logistic Distribution)</figcaption></figure><p id="73d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">那么可能性函数是-</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/76c3b064fd278bf25139cd0a0f4edca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*nmQHzxp3RcfsjvdaYjb1Dw.png"/></div></figure><p id="49f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此对数可能性是-</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/53535d57430c6fa953e06ac06c3c6d54.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*VNG9oSSl4bghyUnjWhO1Gg.png"/></div></figure><p id="4fb7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">经过简化，它可以归结为以下内容(我不包括这些步骤，因为键入方程仍然是一项单调乏味的任务😅)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/c5bfb71440345b53d9135f33c1111de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*uqkkpo_CV78O1SzQk-F4_w.png"/></div></figure><p id="1b57" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中β是维数为(p+1)×1 的参数向量，xi 是 1×(p+1)阶的第 I 个观测向量。那就是<strong class="js iu"><em class="ko">Xi =【1x1i x2i x3i…xpi】。</em> </strong>现在我们需要训练的模型无非是根据手头的数据估计未知的参数。虽然有几种估计方法，但最大似然估计因其简单而被广泛使用。参数的最大似然是</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/e6999711faefe9bf053cf518c2424520.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*zAzuz_EO-ZrxxEIb0nz8EA.png"/></div></figure><p id="ab9b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们找到对数似然函数相对于参数向量的一阶导数，</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi lx"><img src="../Images/2c8606a12faa982e0932256fe0a2286f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*d3vJbw-j_d0UinVnYkhucg.png"/></div></div></figure><blockquote class="ly"><p id="9a92" class="lz ma it bd mb mc md me mf mg mh kn dk translated">β向量的极大似然估计必须满足条件 D=0。是不是很美？<br/>易-皮只不过是用皮预测易所犯的错误。它表明误差和预测值是正交的。在简单线性回归的情况下，这不是第一正规方程所说的吗？</p></blockquote><p id="976d" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">我们使用逻辑回归，因为线性回归模型的假设在分类响应变量的情况下不满足。但是我们又回到了线性模型。这就是回归理论的妙处。真是太神奇了。</p><p id="7f42" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但不幸的是，不那么神奇的是，上述方程没有封闭形式的解。😥😥😥😥😥</p><p id="320b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要一些优化技术。<strong class="js iu">梯度下降</strong>和<strong class="js iu">牛顿-拉夫森</strong>法可能是应用最广泛的。这里我坚持使用牛顿-拉夫森方法(这样我就不需要调整学习速度)。</p><p id="f7d5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我不打算深入研究牛顿-拉夫逊方法。粗略地说，它通过如下迭代过程找到函数 f(x)的根</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/ead2f7b2750091dfbbfdd6e3d965cb08.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*MAFliULcvlOknAbQ15hJ1Q.png"/></div></figure><p id="dc6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">并且当两个连续步骤的输出之间的差异变得太小时，我们停止。在我们的情况下，我们需要找到对数似然函数的一阶导数的根。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/9ad40a2c00be414035d347ba54cf193d.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*sdbELm2KM5kJ9PJSM6W1yA.png"/></div></figure><p id="4572" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一阶导数我们已经找到了，二阶导数是</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/d7ebdf93aaf87c230b21a18be0b5122d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*uB0XSduN_SnijV64YQIc6g.png"/></div></figure><p id="05b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它拥有另一种美。这一项总是负的，或者就矩阵而言，它总是负定矩阵。<strong class="js iu">Logit 模型的似然函数满足全局凹性。D=0 的解总会使可能性最大化。</strong></p><p id="ade4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于我们要处理不止一个预测值，让我们找出上述量的矩阵对应项，</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/5e862f6754ad96a2ebe6cc687942a44a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*HINTdBsFXpUPJl-UJxnAwg.png"/></div></figure><p id="92b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在哪里，</p><ol class=""><li id="1538" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">x 是 N 阶 x (p+1)预测值的矩阵，第一列全为 1。</li><li id="c610" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">y 是响应变量的列向量。</li><li id="1271" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">p-hat 是更新概率 P[Y=1| X=x]的列向量。</li><li id="4800" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">p 是 N×N 对角矩阵，第 I 个对角元素是 pi(1-pi)。</li><li id="28d4" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">n 表示第 n 次迭代。</li></ol></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="e029" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的理论背景已经准备好了，现在是编码的时候了。我在这里用 R。</p><pre class="le lf lg lh gt my mz na nb aw nc bi"><span id="b303" class="nd ne it mz b gy nf ng l nh ni">#at first lets write a function to find the pi values for given predictors and betas</span><span id="8bed" class="nd ne it mz b gy nj ng l nh ni">#x is the matrix of predictors, param is the vector of betas</span><span id="5bab" class="nd ne it mz b gy nj ng l nh ni"><strong class="mz iu">pi_finder=function(x,param){</strong><br/> <strong class="mz iu">pi=array(dim=1)</strong>               #initializing an array<br/> <strong class="mz iu">for(i in 1:nrow(x)){</strong><br/>  <strong class="mz iu">val=0</strong>                        #temporary variable<br/>  <strong class="mz iu">for(j in 1:ncol(x)){<br/>   val=val+x[i,j]*param[j]</strong>     #x[i,j]=ith value of jth predictor<br/>  <strong class="mz iu">}<br/>  pi[i]=1/(1+exp(-val))<br/> }<br/>return(pi)</strong>                     # it will return the updated pi<br/><strong class="mz iu">}</strong></span><span id="f6c6" class="nd ne it mz b gy nj ng l nh ni">#now lets write a function to construct the P matrix<br/>#as input we only need the pi values</span><span id="b7d6" class="nd ne it mz b gy nj ng l nh ni"><strong class="mz iu">P_matrix_finder=function(pi){ </strong><br/> <strong class="mz iu">P=diag(pi*(1-pi))</strong>   #diagonal matrix with ith diagonal=pi(1-pi)<br/> <strong class="mz iu">return(P)</strong>           #returning the matrix<br/><strong class="mz iu">}</strong></span><span id="3af0" class="nd ne it mz b gy nj ng l nh ni">#and finally the logistic function equivalent to glm.<br/># as input it will take the matrix of predictors, response variable and the precision for the convergence of Newton Raphson method.</span><span id="7cb0" class="nd ne it mz b gy nj ng l nh ni"><strong class="mz iu">logistic_own=function(predictor,response,precision){</strong><br/><strong class="mz iu"> predictor_new=as.matrix(predictor)</strong>   # just to be on the safe side<br/> <strong class="mz iu">distinct=sort(unique(as.numeric(response)),decreasing=FALSE)<br/> response_new=array(dim=1)<br/> for(i in 1:length(response)){<br/>  if(response[i]==distinct[1])<br/>   response_new[i]=0<br/>  else<br/>   response_new[i]=1</strong><br/> <strong class="mz iu">}</strong>                     #in case response is in factor (not numeric)<br/> <strong class="mz iu">constant=rep(1:length(response_new))</strong>  #the first column of X matrix<br/> <strong class="mz iu">X_matrix=cbind(constant,predictor_new)</strong><br/> <strong class="mz iu">beta=rep(0,ncol(X_matrix))</strong>          #initializing beta to 0 vector<br/> <strong class="mz iu">dif=100</strong>              #since do while loop does not exist in R  :(<br/> <strong class="mz iu">while(dif&gt;precision){<br/>  pi=pi_finder(X_matrix,beta)<br/>  P=P_matrix_finder(pi)<br/>  updated=solve(t(X_matrix)%*%P%*%X_matrix)%*%t(X_matrix)%*%(response_new-pi)   </strong><br/>  <strong class="mz iu">beta=beta+updated</strong>   #updating beta <br/>  <strong class="mz iu">dif=sum(updated^2)</strong>   #euclidean distance between old and new beta<br/> <strong class="mz iu">}<br/> return(beta)<br/>}</strong></span></pre><p id="9ab4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型培训框架完成。我一直保持简单。<em class="ko">多类分类会失败。</em></p><p id="a00a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们要不要在数据集上试一试，并与 glm 函数的结果进行比较？我正在使用著名的<em class="ko">虹膜数据集。</em></p><pre class="le lf lg lh gt my mz na nb aw nc bi"><span id="5f14" class="nd ne it mz b gy nf ng l nh ni"><strong class="mz iu">library(datasets)<br/>iris=iris<br/>iris_new=iris[which(iris$Species!="setosa"),]</strong> #taking only 2 outputs</span></pre><p id="b75c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据看起来像这样:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi gj"><img src="../Images/b481f263032dbc69a852004ef8c9bc3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_gQtB9f9h6u6BHYbMyljg.png"/></div></div></figure><pre class="le lf lg lh gt my mz na nb aw nc bi"><span id="9f4f" class="nd ne it mz b gy nf ng l nh ni">#train test split. I am considering 75:25 ratio<br/><strong class="mz iu">samples=sample(1:nrow(iris_new),size=nrow(iris_new)*0.75,replace=F)</strong><br/><strong class="mz iu">iris_new_train=iris_new[samples,]</strong>   #training data<br/><strong class="mz iu">iris_new_test=iris_new[-samples,]</strong>   #test data</span><span id="b791" class="nd ne it mz b gy nj ng l nh ni">#model training using glm function<br/><strong class="mz iu">inbuilt=glm(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=iris_new_train,family=binomial(link="logit"))<br/>inbuilt$coefficients</strong></span></pre><p id="c149" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出是</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nk"><img src="../Images/099e896ff677f28109a354503acc9b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CU-bcDcNzohS-sdDDCtRiw.png"/></div></div></figure><p id="408e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们试试我们的功能，</p><pre class="le lf lg lh gt my mz na nb aw nc bi"><span id="b216" class="nd ne it mz b gy nf ng l nh ni"><strong class="mz iu">model=own_logistic(iris_new_train[,1:4],iris_new_train[,5],0.000000001)<br/>model</strong></span></pre><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nl"><img src="../Images/19fbfba618011b742f5215b244a9bf64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SV7RApjCbypnSR4vBZ2zxw.png"/></div></div></figure><p id="1ce7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">答对了。！！！！！！！！！！😀😀😀</p><p id="6d72" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是工作还没有完成，我们仍然需要测试我们的模型。R 中有一个内置函数<strong class="js iu"> predict </strong>，它给出了测试数据点的预测概率。我们能开发类似的东西吗？</p><p id="b2ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们当然可以。我们已经有了训练好的系数。只需要测试数据集。</p><pre class="le lf lg lh gt my mz na nb aw nc bi"><span id="a568" class="nd ne it mz b gy nf ng l nh ni">#takes our model output and test data predictors as inputs<br/><strong class="mz iu">fitted=function(model,test_data_predictors){<br/>  predictors=as.matrix(test_data_predictors)<br/>  constant=rep(1,nrow(predictors))<br/>  X_matrix=cbind(constant,predictors)<br/>  pred=pi_finder(X_matrix,model)</strong>   # the predicted probabilities<br/>  <strong class="mz iu">return(pred) <br/>}</strong></span></pre><p id="2b51" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 R 中使用内置函数:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nm"><img src="../Images/63476237ba7dce18eb4d4b895ebdb339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iYSbSHTjgBWNH-7EUD4AnQ.png"/></div></div></figure><p id="a4bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看我们的函数给出了什么:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nn"><img src="../Images/f5db5b692a44a4dd4d4ac45ecbf34604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5AAbQJC19gLYPCBuPQYcBQ.png"/></div></div></figure><p id="dab7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这有点笨拙，让我们把它们放在一起:</p><pre class="le lf lg lh gt my mz na nb aw nc bi"><span id="391a" class="nd ne it mz b gy nf ng l nh ni"><strong class="mz iu">R_inbuilt_pred=predict(inbuilt,iris_new_test[,1:4],type="response")<br/>Our_own_pred=fitted(model,iris_new_test[,1:4])<br/>cbind(R_inbuilt_pred,Our_own_pred)</strong></span></pre><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi no"><img src="../Images/992804834adae31cdcf632acd57e5f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B36JyzmgPabiOgcRzWKgVw.png"/></div></div></figure><blockquote class="ly"><p id="457f" class="lz ma it bd mb mc md me mf mg mh kn dk translated">恭喜你！！！！！你已经学会了如何为二元分类创建自己的逻辑分类器。</p></blockquote></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="535e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">编写自己的代码是一个很好的实践。这样你会变得更有创造力，并且对不同的最大似然算法的背景理论有很强的把握。</p><p id="8f35" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你需要通过创造力来理解逻辑回归背后的思想，你可以去看看我之前的文章<a class="ae np" rel="noopener" target="_blank" href="/logistic-regression-derived-from-intuition-d1211fc09b10"> <strong class="js iu">逻辑回归——源自直觉【逻辑三部曲，第 1 部分】</strong> </a>。你自己将从纯粹的直觉和逻辑中推导出逻辑回归。</p><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/logistic-regression-derived-from-intuition-d1211fc09b10"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">逻辑回归——源自直觉</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">让我们通过一个故事从头开始推导逻辑回归。我希望这将是有趣和好玩的…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh ln nt"/></div></div></a></div></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="64e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">创造力和创新在任何领域都非常重要。它会把你和其他人分开。最重要的是，数据科学家需要有创造力。</p><p id="1af5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你不相信，或者有任何疑问或建议，请在评论区提问，或者通过我的 LinkedIn 个人资料联系我。</p><div class="nq nr gp gr ns nt"><a href="https://www.linkedin.com/in/soumalya-nandi-95176569/" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">SOUMALYA NANDI -联合健康组织(L2)助理数据科学家| LinkedIn</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">查看 SOUMALYA NANDI 在全球最大的职业社区 LinkedIn 上的个人资料。SOUMALYA 有 4 份工作列在…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">www.linkedin.com</p></div></div><div class="oc l"><div class="oi l oe of og oc oh ln nt"/></div></div></a></div></div></div>    
</body>
</html>