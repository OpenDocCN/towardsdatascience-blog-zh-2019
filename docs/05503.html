<html>
<head>
<title>Naive Bayes Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯解释道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/naive-bayes-explained-9d2b96f4a9c0?source=collection_archive---------1-----------------------#2019-08-14">https://towardsdatascience.com/naive-bayes-explained-9d2b96f4a9c0?source=collection_archive---------1-----------------------#2019-08-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8279" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">朴素贝叶斯是一种概率算法，通常用于分类问题。朴素贝叶斯简单、直观，但在许多情况下表现惊人地好。例如，电子邮件应用程序使用的垃圾邮件过滤器是建立在朴素贝叶斯基础上的。在本文中，我将解释朴素贝叶斯背后的基本原理，并用 Python 构建一个垃圾邮件过滤器。(为了简单起见，我将重点讨论二进制分类问题)</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/70340abb2f396a86ecb50cc3a55e47d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/0*Z8nRrQhBH508yOXP.gif"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Thomas Bayes, the ‘betting man’, from <a class="ae kx" href="https://www.bbc.com/news/uk-scotland-edinburgh-east-fife-14708583" rel="noopener ugc nofollow" target="_blank">BBC</a></figcaption></figure><h1 id="fc04" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated"><strong class="ak">理论</strong></h1><p id="50f1" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">在我们开始之前，请记住本文中使用的符号:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mb"><img src="../Images/97f765af55e4d53a90bdc136d6225f68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aFS9QOQpvC3DmD3qIuUKlQ.png"/></div></div></figure><h2 id="2188" class="mg kz iq bd la mh mi dn le mj mk dp li jy ml mm lm kc mn mo lq kg mp mq lu mr bi translated">基本想法</h2><p id="6870" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">要做分类，我们需要用 X 来预测 Y，换句话说，给定一个数据点 X=(x1，x2，…，xn)，Y 是 Y 的奇数是多少，这可以改写为下面的等式:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ms"><img src="../Images/5e94809a5f1315f7ffe2f29de10f7ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2WS6PIDrzuCfpasdLORaTg.png"/></div></div></figure><p id="d37a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是朴素贝叶斯的基本思想，算法的其余部分实际上更侧重于如何计算上面的条件概率。</p><h2 id="fac2" class="mg kz iq bd la mh mi dn le mj mk dp li jy ml mm lm kc mn mo lq kg mp mq lu mr bi translated">贝叶斯定理</h2><p id="a79c" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">到目前为止，贝叶斯先生对算法没有贡献。现在是他发光的时候了。根据贝叶斯定理:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mt"><img src="../Images/d239703fca4097280a11dff5e0a059fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wPZK-066neqKQyJXBVZbew.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk"><a class="ae kx" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">Bayes Theorem</a></figcaption></figure><p id="24d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一个相当简单的转变，但它弥合了我们想做的和我们能做的之间的差距。我们不能直接得到 P(Y|X)，但是可以从训练数据中得到 P(X|Y)和 P(Y)。这里有一个例子:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/7ad5fa8077e744e12d383049817246e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*dCHxmff8fbxaNDpEnPSZYA.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Weather dataset, from the <a class="ae kx" href="http://www.inf.ed.ac.uk/teaching/courses/inf2b/learnSlides/inf2b12-learnlec06.pdf" rel="noopener ugc nofollow" target="_blank">University of Edinburgh</a></figcaption></figure><p id="27f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种情况下，X =(展望，温度，湿度，有风)，Y =玩。P(X|Y)和 P(Y)可以通过下式计算:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mv"><img src="../Images/856b69583e3a996dbc7c048484c7278c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPpHEzzNPnyfxHj0ChcOJQ.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Example of finding P(Y) and P(X|Y)</figcaption></figure><h2 id="1f3d" class="mg kz iq bd la mh mi dn le mj mk dp li jy ml mm lm kc mn mo lq kg mp mq lu mr bi translated">朴素贝叶斯假设及其原因</h2><p id="d7af" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">理论上，求 P(X|Y)并不难。然而，随着特性数量的增加，这实际上要困难得多。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mw"><img src="../Images/d3e60cf1f0cb6e1d70bf23d248416056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQtba9U_jtsmxieMPE5Kyg.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">7 parameters are needed for a 2-feature binary dataset</figcaption></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mx"><img src="../Images/6bd4bc2290b12af0b3f084fadd9a17e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZzRC5jljaE3AjyFRkfp6Fg.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Estimate Join Distribution requires more data</figcaption></figure><p id="ae01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型中有这么多参数是不切实际的。为了解决这个问题，做了一个天真的假设。<strong class="jp ir">我们假设所有的特征都是独立的。</strong>这是什么意思？</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi my"><img src="../Images/6e4ba8d7c5a24e9d7f1d52f43bb25b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m6suNQra4uAKGjOguGgxgw.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk"><a class="ae kx" href="https://en.wikipedia.org/wiki/Conditional_independence" rel="noopener ugc nofollow" target="_blank">Conditional independence</a></figcaption></figure><p id="7ad3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在借助于这个天真的假设(天真是因为特征很少是独立的)，<strong class="jp ir">我们可以用少得多的参数进行分类</strong>:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mz"><img src="../Images/cb6220704ffcd6ea9e991d82901056e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kCBfBeP8CLJc2auJ-hEK9w.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Naive Bayes Classifier</figcaption></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi na"><img src="../Images/e394fce2ac19015936e08d474c875d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pD1G8XJBrKhk-QvHk6jF0w.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Naive Bayes need fewer parameters (4 in this case)</figcaption></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nb"><img src="../Images/bd1bb6b667568ae72e1c0f3dfaea8dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VYN1TZLm8saDijB0f2NwIA.png"/></div></div></figure><p id="e8ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一件大事。我们将参数数量从指数型改为线性型。这意味着朴素贝叶斯很好地处理了高维数据。</p><h2 id="893b" class="mg kz iq bd la mh mi dn le mj mk dp li jy ml mm lm kc mn mo lq kg mp mq lu mr bi translated">分类和连续特征</h2><p id="38c3" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated"><strong class="jp ir">分类数据</strong></p><p id="a3e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于分类特征，P(Xi|Y)的估计是容易的。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nc"><img src="../Images/8265d3b7e38a2466813dcea682294981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A1N7COgX2H8cHcIIu5wjCw.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Calculate the likelihood of categorical features</figcaption></figure><p id="9421" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，一个问题是，如果一些特征值从未出现(可能缺乏数据)，它们的可能性将为零，这使得整个后验概率为零。解决这个问题的一个简单方法叫做<a class="ae kx" href="https://en.wikipedia.org/wiki/Additive_smoothing" rel="noopener ugc nofollow" target="_blank">拉普拉斯估计器</a>:给每个类别添加假想样本(通常是一个)</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nd"><img src="../Images/b71adec424178ce8659e03bcecbd29db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEk9rUTDgY--OAkTVRGm8g.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Laplace Estimator</figcaption></figure><p id="848d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">连续数据</strong></p><p id="0bb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于连续特征，本质上有两种选择:离散化和连续朴素贝叶斯。</p><p id="53bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">离散化的工作原理是将数据分解成分类值。最简单的离散化是均匀宁滨，它创建具有固定范围的箱。当然，还有更智能和更复杂的方法，如递归最小熵分割或基于 SOM 的分割。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ne"><img src="../Images/85046e91637016efdb508b340e89d980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6sjqjEl1xhgBuNA7Vv6qQw.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk"><a class="ae kx" href="https://www.semanticscholar.org/paper/Discretizing-Continuous-Features-for-Naive-Bayes-C-Kaya/680de400a534028b548c2297b8e8ddd904ebbd56" rel="noopener ugc nofollow" target="_blank">Discretizing Continuous Feature for Naive Bayes</a></figcaption></figure><p id="f278" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二种选择是利用已知的分布。如果要素是连续的，朴素贝叶斯算法可以写成:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nf"><img src="../Images/c5b3eb98171e892a264096b996e5fa0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yZUx3bWdZIxlu-xJvaYc6Q.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">f is the probability density function</figcaption></figure><p id="6836" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，如果我们将数据可视化并看到一个钟形曲线状的分布，就可以假设该特征是正态分布的</p><p id="05c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一步是计算给定标签 y 的特征的均值和方差:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ng"><img src="../Images/247e9a27c539b3a26c1d32b483c77968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NYAJskbkcjSqYiuOEoCCjg.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk"><a class="ae kx" href="http://onlinestatbook.com/2/estimation/df.html" rel="noopener ugc nofollow" target="_blank">variance adjusted by the degree of freedom</a></figcaption></figure><p id="9bbb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们可以计算概率密度 f(x):</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nh"><img src="../Images/84e6754a76923c3d123d77c01a094af0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sGY7F_xp17twpqxIf7zcqQ.png"/></div></div></figure><p id="ae3d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当然，还有其他发行版:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/7da5154642916e500f9e31b0577b50e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*DAl8ul9skEwDlrdRCnTWkg.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk"><a class="ae kx" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank">Naive Bayes, from Scikit-Learn</a></figcaption></figure><p id="10d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然这些方法在形式上有所不同，但背后的核心思想是一致的:假设特征满足某种分布，估计该分布的参数，进而得到概率密度函数。</p><h1 id="8694" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated"><strong class="ak">优势和劣势</strong></h1><h2 id="5a07" class="mg kz iq bd la mh mi dn le mj mk dp li jy ml mm lm kc mn mo lq kg mp mq lu mr bi translated">力量</h2><ol class=""><li id="983f" class="nj nk iq jp b jq lw ju lx jy nl kc nm kg nn kk no np nq nr bi translated">尽管天真的假设很少是正确的，但该算法在许多情况下表现得出奇的好</li><li id="dcd0" class="nj nk iq jp b jq ns ju nt jy nu kc nv kg nw kk no np nq nr bi translated">很好地处理高维数据。易于并行化，能够很好地处理大数据</li><li id="6cc5" class="nj nk iq jp b jq ns ju nt jy nu kc nv kg nw kk no np nq nr bi translated">当数据集较小时，性能优于更复杂的模型</li></ol><h2 id="6c79" class="mg kz iq bd la mh mi dn le mj mk dp li jy ml mm lm kc mn mo lq kg mp mq lu mr bi translated">弱点</h2><ol class=""><li id="8fcf" class="nj nk iq jp b jq lw ju lx jy nl kc nm kg nn kk no np nq nr bi translated">由于天真的假设，估计的概率往往是不准确的。不适合回归使用或概率估计</li><li id="f430" class="nj nk iq jp b jq ns ju nt jy nu kc nv kg nw kk no np nq nr bi translated">当数据丰富时，其他更复杂的模型往往优于朴素贝叶斯</li></ol><h1 id="dc6d" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated"><strong class="ak">总结</strong></h1><p id="590c" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">朴素贝叶斯利用最基本的概率知识，并做出所有特征都是独立的天真假设。尽管简单(有些人可能会说过于简单)，朴素贝叶斯在许多应用程序中都有不错的表现。</p><p id="171f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在你明白了朴素贝叶斯的工作原理，是时候在实际项目中尝试一下了！</p></div></div>    
</body>
</html>