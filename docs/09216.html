<html>
<head>
<title>GPU-Accelerated Machine Learning on MacOS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MacOS 上 GPU 加速的机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gpu-accelerated-machine-learning-on-macos-48d53ef1b545?source=collection_archive---------4-----------------------#2019-12-06">https://towardsdatascience.com/gpu-accelerated-machine-learning-on-macos-48d53ef1b545?source=collection_archive---------4-----------------------#2019-12-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3eb0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">苹果可能不喜欢 NVIDIA 卡，解决方案叫 PlaidML+OpenCL</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5124c5ee655a8573ac8707eaca28ad56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pPRaKKKOoIRFp9x-W7vIGg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">PlaidML is a software framework that enables Keras to execute calculations on a GPU using OpenCL instead of CUDA. This is a good solution to do light ML development on a Mac without a NVIDIA eGPU card.</figcaption></figure><p id="5c7f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大规模并行编程对于在相似输入上多次应用相同操作的情况下加速计算非常有用。如果你的代码包含许多<code class="fe lu lv lw lx b">if</code>或<code class="fe lu lv lw lx b">case</code>语句，你可能想在使用<em class="ly">的 CPU 上运行，例如</em> <a class="ae lz" href="https://www.open-mpi.org/" rel="noopener ugc nofollow" target="_blank"> OpenMPI </a>。如果你的代码涉及到随机数的生成，并行编程可能不是最好的解决方案(不过，这里见<a class="ae lz" href="https://developer.nvidia.com/curand" rel="noopener ugc nofollow" target="_blank"/>)。否则，你很可能是在正确的地方，所以继续阅读！</p><blockquote class="ma mb mc"><p id="29f9" class="ky kz ly la b lb lc ju ld le lf jx lg md li lj lk me lm ln lo mf lq lr ls lt im bi translated"><em class="it">训练一个神经网络涉及</em>非常<em class="it">大量的矩阵乘法。这是典型的大规模并行操作，这也是 GPU 对机器学习至关重要的主要原因之一。需要记住的一条经验法则是，1K 个 CPUs = 16K 个内核= 3 个 GPU，尽管 CPU 可以执行的操作种类远远超过单个 GPU 内核。对于 GPU 来说，实力在数量！</em></p></blockquote><p id="0184" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">iMac 和 MacBook Pro 电脑配备了 AMD 镭龙 GPU 卡。不幸的是，这种硬件不能直接用于加速机器学习应用中典型的计算，例如训练 CNN。虽然没有适用于所有可能应用的解决方案，但是仍然有一个基于并行编程语言 OpenCL 的简单架构的解决方案。</p><p id="7eab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">两个最流行的 ML 框架<a class="ae lz" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>和<a class="ae lz" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>支持基于通用 GPU 库<a class="ae lz" href="https://developer.nvidia.com/cuda-zone" rel="noopener ugc nofollow" target="_blank"> NVIDIA CUDA </a>的 GPU 加速。CUDA 只适用于 NVIDIA GPU 卡。</p><blockquote class="ma mb mc"><p id="1458" class="ky kz ly la b lb lc ju ld le lf jx lg md li lj lk me lm ln lo mf lq lr ls lt im bi translated"><em class="it"> NVIDIA 外置 GPU 卡(eGPU)可用于带有雷电 3 端口的 MacOS 系统和 MacOS High Sierra 10.13.4 或更高版本。按照</em> <a class="ae lz" href="https://support.apple.com/en-us/HT208544" rel="noopener ugc nofollow" target="_blank"> <em class="it">本指南</em> </a> <em class="it">安装 eGPU。</em></p><p id="a5d9" class="ky kz ly la b lb lc ju ld le lf jx lg md li lj lk me lm ln lo mf lq lr ls lt im bi translated"><em class="it">在 MacOs 上安装 CUDA 遵循</em> <a class="ae lz" href="https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="it">官方文档</em> </a> <em class="it">。这里可以找到</em> <a class="ae lz" href="https://www.nvidia.com/download/driverResults.aspx/147830/" rel="noopener ugc nofollow" target="_blank"> <em class="it">驱动</em> </a> <em class="it">。</em></p></blockquote><h1 id="09f5" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">使用 Python 的 OpenCL</h1><p id="8c09" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">项目<a class="ae lz" href="https://documen.tician.de/pyopencl/" rel="noopener ugc nofollow" target="_blank"> PyOpenCL </a>可能是在 Mac 上开始使用 GP-GPU 的最简单的方法。另见其姊妹项目<a class="ae lz" href="https://documen.tician.de/pycuda/" rel="noopener ugc nofollow" target="_blank"> PyCUDA </a>。</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="1e09" class="nh mh it lx b gy ni nj l nk nl">$ pip install pyopencl</span></pre><p id="ac68" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">OpenCL 的核心是一个<em class="ly">内核</em>，这是一个可以应用于大量输入数据的函数(用类似于<em class="ly"> C </em>的语言编写)。</p><blockquote class="ma mb mc"><p id="4ff1" class="ky kz ly la b lb lc ju ld le lf jx lg md li lj lk me lm ln lo mf lq lr ls lt im bi translated"><em class="it">要了解并行编程，请阅读基于 CUDA 的在线课程</em>  <em class="it">，或者购买 Tim Mattson </em> et al <em class="it">撰写的本书</em> <a class="ae lz" href="https://www.amazon.ca/OpenCL-Programming-Guide-Aaftab-Munshi/dp/0321749642" rel="noopener ugc nofollow" target="_blank"> <em class="it">。</em></a></p></blockquote><p id="960b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">OpenCL 比 CUDA 更灵活，允许程序在不同的架构上执行。这是有代价的:需要编写一些“样板”代码来定义一个<em class="ly">上下文</em> ( <em class="ly">即</em>什么样的硬件可用)、一个<em class="ly">队列</em> ( <em class="ly">即</em>一个命令序列)和一组内存标志(<em class="ly">如</em> <code class="fe lu lv lw lx b">READ_ONLY</code>、<code class="fe lu lv lw lx b">WRITE_ONLY</code>等。).典型的工作流程如下:</p><ol class=""><li id="bec0" class="nm nn it la b lb lc le lf lh no ll np lp nq lt nr ns nt nu bi translated">输入数据被复制到 GPU 内存中</li><li id="cf23" class="nm nn it la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">GPU 缓冲存储器被保留用于存放计算结果</li><li id="b201" class="nm nn it la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">内核被执行</li><li id="c93c" class="nm nn it la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">结果从 GPU 内存复制到主机内存</li></ol><blockquote class="ma mb mc"><p id="c972" class="ky kz ly la b lb lc ju ld le lf jx lg md li lj lk me lm ln lo mf lq lr ls lt im bi translated"><em class="it"> NB: OpenCL </em>内核<em class="it">可以在 CPU 和 GPU 上执行，但是如果代码针对某个 GPU 架构进行了高度优化(</em>例如<em class="it">具有大内存)，它可能无法完全移植。</em></p><p id="0b00" class="ky kz ly la b lb lc ju ld le lf jx lg md li lj lk me lm ln lo mf lq lr ls lt im bi translated"><em class="it">虽然 GPU 通常更快，但根据处理器的架构，将大量数据从 CPU 传输到 GPU 所需的时间可能会导致更高的开销时间。</em></p></blockquote><p id="2bbb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是一个计算两个矩阵之和的全功能 OpenCL 程序示例:</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="bd39" class="nh mh it lx b gy ni nj l nk nl">#!/usr/bin/env python<br/># -*- coding: utf-8 -*-</span><span id="306b" class="nh mh it lx b gy oa nj l nk nl">from __future__ import absolute_import, print_function<br/>import numpy as np<br/>import pyopencl as cl</span><span id="906f" class="nh mh it lx b gy oa nj l nk nl">a_np = np.random.rand(50000).astype(np.float32)<br/>b_np = np.random.rand(50000).astype(np.float32)</span><span id="0a3e" class="nh mh it lx b gy oa nj l nk nl">ctx = cl.create_some_context()<br/>queue = cl.CommandQueue(ctx)</span><span id="49c0" class="nh mh it lx b gy oa nj l nk nl">mf = cl.mem_flags<br/>a_g = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=a_np)<br/>b_g = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b_np)</span><span id="212b" class="nh mh it lx b gy oa nj l nk nl">kernel = """<br/>__kernel void sum(<br/>    __global const float *a_g, __global const float *b_g, __global float *res_g)<br/>{<br/>  int gid = get_global_id(0);<br/>  res_g[gid] = a_g[gid] + b_g[gid];<br/>}<br/>"""</span><span id="e2a1" class="nh mh it lx b gy oa nj l nk nl">prg = cl.Program(ctx, kernel).build()</span><span id="0c8c" class="nh mh it lx b gy oa nj l nk nl">res_g = cl.Buffer(ctx, mf.WRITE_ONLY, a_np.nbytes)<br/>prg.sum(queue, a_np.shape, None, a_g, b_g, res_g)</span><span id="de13" class="nh mh it lx b gy oa nj l nk nl">res_np = np.empty_like(a_np)<br/>cl.enqueue_copy(queue, res_np, res_g)</span><span id="640d" class="nh mh it lx b gy oa nj l nk nl"># Check on CPU with Numpy:<br/>print(res_np - (a_np + b_np))<br/>print(np.linalg.norm(res_np - (a_np + b_np)))<br/>assert np.allclose(res_np, a_np + b_np)</span></pre><h1 id="df99" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">使用 OpenCL 作为 Keras 后端的机器学习(PlaidML)</h1><p id="2dee" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated"><a class="ae lz" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>是最流行的深度学习框架之一。使用 Keras' <a class="ae lz" href="https://keras.io/getting-started/functional-api-guide/" rel="noopener ugc nofollow" target="_blank">功能 API</a>定义网络架构、运行训练和执行推理非常容易。然而，Keras 本身并不执行实际的计算，而是部署其他软件库来定义、优化和评估涉及多维数组的数学表达式。最常见的有<a class="ae lz" href="https://github.com/Theano/Theano" rel="noopener ugc nofollow" target="_blank"> Theano </a>和<a class="ae lz" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>。反过来，这些库使用 CUDA 在 GPU 上执行并行计算。如前所述，这带来了非常强的硬件限制，即它只能在 NVIDIA 卡上工作。</p><p id="457d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过使用<a class="ae lz" href="https://vertexai-plaidml.readthedocs-hosted.com/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> PlaidML </a>库可以部分避免这个问题:</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="e675" class="nh mh it lx b gy ni nj l nk nl">$ pip install plaidml-keras</span></pre><p id="62b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">安装之后，执行安装脚本(选择默认，除非您知道自己在做什么):</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="e9d1" class="nh mh it lx b gy ni nj l nk nl">$ plaidml-setup</span></pre><p id="d679" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您应该会看到类似这样的内容:</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="d5a0" class="nh mh it lx b gy ni nj l nk nl">PlaidML Setup (0.6.4)</span><span id="cf45" class="nh mh it lx b gy oa nj l nk nl">Thanks for using PlaidML!</span><span id="1b98" class="nh mh it lx b gy oa nj l nk nl">Some Notes:<br/>  * Bugs and other issues: https://github.com/plaidml/plaidml<br/>  * Questions: https://stackoverflow.com/questions/tagged/plaidml<br/>  * Say hello: https://groups.google.com/forum/#!forum/plaidml-dev<br/>  * PlaidML is licensed under the Apache License 2.0<br/></span><span id="31cf" class="nh mh it lx b gy oa nj l nk nl">Default Config Devices:<br/>   metal_amd_radeon_r9_m380.0 : AMD Radeon R9 M380 (Metal)</span><span id="60e1" class="nh mh it lx b gy oa nj l nk nl">Experimental Config Devices:<br/>   llvm_cpu.0 : CPU (LLVM)<br/>   opencl_amd_radeon_r9_m380_compute_engine.0 : AMD AMD Radeon R9 M380 Compute Engine (OpenCL)<br/>   opencl_cpu.0 : Intel CPU (OpenCL)<br/>   metal_amd_radeon_r9_m380.0 : AMD Radeon R9 M380 (Metal)</span><span id="3d0f" class="nh mh it lx b gy oa nj l nk nl">Using experimental devices can cause poor performance, crashes, and other nastiness.</span><span id="8cc2" class="nh mh it lx b gy oa nj l nk nl">Enable experimental device support? (y,n)[n]:</span><span id="310d" class="nh mh it lx b gy oa nj l nk nl">Selected device:<br/>    metal_amd_radeon_r9_m380.0</span><span id="96d8" class="nh mh it lx b gy oa nj l nk nl">Almost done. Multiplying some matrices...<br/>Tile code:<br/>  function (B[X,Z], C[Z,Y]) -&gt; (A) { A[x,y : X,Y] = +(B[x,z] * C[z,y]); }<br/>Whew. That worked.</span><span id="101c" class="nh mh it lx b gy oa nj l nk nl">Save settings to /Users/user/.plaidml? (y,n)[y]:<br/>Success!</span></pre><blockquote class="ma mb mc"><p id="65b3" class="ky kz ly la b lb lc ju ld le lf jx lg md li lj lk me lm ln lo mf lq lr ls lt im bi translated"><em class="it">该库支持许多但不是所有的 Keras 层。如果您的架构只涉及密集层、LSTM 层、CNN 层和漏层，那么您当然很好，否则请查看文档。</em></p></blockquote><p id="3f98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">原则上，您所要做的就是在您的程序前添加以下代码行来激活 PlaidML 后端:</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="0a11" class="nh mh it lx b gy ni nj l nk nl">import os</span><span id="369f" class="nh mh it lx b gy oa nj l nk nl">os.environ["KERAS_BACKEND"] = "plaidml.keras.backend"</span></pre><p id="7933" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在执行过程中，您应该会看到如下打印输出:</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="f6a8" class="nh mh it lx b gy ni nj l nk nl">Using plaindml.keras.backend backend</span><span id="6c0e" class="nh mh it lx b gy oa nj l nk nl">INFO:plaidml:Opening device "metal_amd_radeon_r9_m380.0"</span></pre><p id="5196" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个示例，改编自 Keras 的<a class="ae lz" href="https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py" rel="noopener ugc nofollow" target="_blank">官方文档</a>，应该可以开箱即用:</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="bcc7" class="nh mh it lx b gy ni nj l nk nl">#!/usr/bin/env python</span><span id="19c8" class="nh mh it lx b gy oa nj l nk nl">import os</span><span id="d58a" class="nh mh it lx b gy oa nj l nk nl">os.environ["KERAS_BACKEND"] = "plaidml.keras.backend"</span><span id="f91a" class="nh mh it lx b gy oa nj l nk nl">import keras<br/>from keras.datasets import mnist<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Flatten<br/>from keras.layers import Conv2D, MaxPooling2D<br/>from keras import backend as K</span><span id="5a02" class="nh mh it lx b gy oa nj l nk nl">batch_size = 128<br/>num_classes = 10<br/>epochs = 12</span><span id="4c96" class="nh mh it lx b gy oa nj l nk nl"># input image dimensions<br/>img_rows, img_cols = 28, 28</span><span id="d3e4" class="nh mh it lx b gy oa nj l nk nl"># the data, split between train and test sets<br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><span id="6bee" class="nh mh it lx b gy oa nj l nk nl">if K.image_data_format() == 'channels_first':<br/>    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)<br/>    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)<br/>    input_shape = (1, img_rows, img_cols)<br/>else:<br/>    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)<br/>    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)<br/>    input_shape = (img_rows, img_cols, 1)</span><span id="aa56" class="nh mh it lx b gy oa nj l nk nl">x_train = x_train.astype('float32')<br/>x_test = x_test.astype('float32')<br/>x_train /= 255<br/>x_test /= 255<br/>print('x_train shape:', x_train.shape)<br/>print(x_train.shape[0], 'train samples')<br/>print(x_test.shape[0], 'test samples')</span><span id="c447" class="nh mh it lx b gy oa nj l nk nl"># convert class vectors to binary class matrices<br/>y_train = keras.utils.to_categorical(y_train, num_classes)<br/>y_test = keras.utils.to_categorical(y_test, num_classes)</span><span id="2653" class="nh mh it lx b gy oa nj l nk nl">model = Sequential()<br/>model.add(Conv2D(32, kernel_size=(3, 3),<br/>                 activation='relu',<br/>                 input_shape=input_shape))<br/>model.add(Conv2D(64, (3, 3), activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Dropout(0.25))<br/>model.add(Flatten())<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(num_classes, activation='softmax'))</span><span id="de8b" class="nh mh it lx b gy oa nj l nk nl">model.compile(loss=keras.losses.categorical_crossentropy,<br/>              optimizer=keras.optimizers.Adadelta(),<br/>              metrics=['accuracy'])</span><span id="9ae4" class="nh mh it lx b gy oa nj l nk nl">model.fit(x_train, y_train,<br/>          batch_size=batch_size,<br/>          epochs=epochs,<br/>          verbose=1,<br/>          validation_data=(x_test, y_test))<br/>score = model.evaluate(x_test, y_test, verbose=0)<br/>print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])</span></pre><p id="4eea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 2015 款 iMac (3.2 GHz 英特尔酷睿 i5，16 GB DDR RAM，AMD 镭龙 R9 M380 2 GB GPU)上，使用 PlaidML/OpenCL GPU 后端训练需要<strong class="la iu"> 1m50s </strong>，使用 TensorFlow-2.0/CPU 后端训练需要<strong class="la iu"> 5m06s </strong>。</p><p id="18b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，如果没有可用的后端，指令<code class="fe lu lv lw lx b">from keras import backend as K</code>将返回一个错误。例如，如果您没有安装<code class="fe lu lv lw lx b">TensorFlow</code>(默认)，您会看到:</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="92b5" class="nh mh it lx b gy ni nj l nk nl">Using TensorFlow backend.<br/>Traceback (most recent call last):<br/>  File "./test_keras.py", line 8, in &lt;module&gt;<br/>    import keras<br/>  File "/Users/Riccardo/development/venv_opencl/lib/python3.7/site-packages/keras/__init__.py", line 3, in &lt;module&gt;<br/>    from . import utils<br/>  File "/Users/Riccardo/development/venv_opencl/lib/python3.7/site-packages/keras/utils/__init__.py", line 6, in &lt;module&gt;<br/>    from . import conv_utils<br/>  File "/Users/Riccardo/development/venv_opencl/lib/python3.7/site-packages/keras/utils/conv_utils.py", line 9, in &lt;module&gt;<br/>    from .. import backend as K<br/>  File "/Users/Riccardo/development/venv_opencl/lib/python3.7/site-packages/keras/backend/__init__.py", line 89, in &lt;module&gt;<br/>    from .tensorflow_backend import *<br/>  File "/Users/Riccardo/development/venv_opencl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py", line 5, in &lt;module&gt;<br/>    import tensorflow as tf<br/>ModuleNotFoundError: No module named 'tensorflow'</span></pre><p id="cb57" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后一点，<code class="fe lu lv lw lx b">TensorFlow &gt;=2.0</code>包括 Keras API。如果你的程序是从 TF 定义层，而不是从 Keras 定义层，那么你不能仅仅改变 Keras 后端来运行在支持 OpenCL 的 GPU 上，因为 TF2 <em class="ly">不支持 OpenCL </em>。更具体地说:</p><p id="571d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这不会使用 GPU(假设你已经安装了<code class="fe lu lv lw lx b">TensorFlow &gt;=2.0</code>)</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="3b48" class="nh mh it lx b gy ni nj l nk nl">from tensorflow import keras<br/>from tensorflow.keras import layers</span></pre><p id="5c09" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将工作得很好(假设你已经安装了<code class="fe lu lv lw lx b">Keras</code>)</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="f782" class="nh mh it lx b gy ni nj l nk nl">import keras<br/>from keras import layers</span></pre><h2 id="2bab" class="nh mh it bd mi ob oc dn mm od oe dp mq lh of og ms ll oh oi mu lp oj ok mw ol bi translated">用 Tile 编写 OpenCL 内核</h2><p id="bd33" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">事实证明，PlaidML 不仅仅是一个 Keras 后端。事实上，它附带了一种叫做<a class="ae lz" href="https://www.intel.ai/automatic-kernel-generation-in-plaidml/#gs.lecnpy" rel="noopener ugc nofollow" target="_blank"> Tile </a>的编程语言，帮助用户编写优化的内核，而不需要深入了解<em class="ly"> C </em>和 OpenCL 的大部分古怪之处。Tile 指令看起来更像数学函数。例如，矩阵乘法</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/29dfa66498cbc5627fc751a12233e9c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*kv64mmHHrQPpNriyJI_uZA.png"/></div></figure><p id="9422" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在瓷砖上看起来像这样</p><pre class="kj kk kl km gt nd lx ne nf aw ng bi"><span id="a3e1" class="nh mh it lx b gy ni nj l nk nl">function (A[M, L], B[L, N]) -&gt; (C) {<br/>    C[i, j: M, N] = +(A[i, k] * B[k, j]);<br/>}</span></pre><p id="9b6a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中“+”运算符代表数学表达式中的“和”。</p><p id="3001" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更多示例可在<a class="ae lz" href="https://vertexai-plaidml.readthedocs-hosted.com/en/latest/writing_tile_code.html" rel="noopener ugc nofollow" target="_blank">文档</a>中找到，包括一些最常见的操作，如矩阵乘法、最小/最大、最大池、卷积、累积和。合成内核时，Tile 展平张量并将索引转换为指针偏移量。计算被分成适合内存的<em class="ly">块</em>，其大小根据可用硬件进行优化。然后在 GPU 内存中布置图块，以优化 SIMDs 指令，如乘法和累加。最后，内核被写出来，就好像它们是由人类用户创建的一样，并被传递给 OpenCL 进行实际执行。</p><h2 id="399d" class="nh mh it bd mi ob oc dn mm od oe dp mq lh of og ms ll oh oi mu lp oj ok mw ol bi translated">结论</h2><p id="4c36" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">PlaidML 现在是英特尔人工智能集团<a class="ae lz" href="https://www.intel.ai/plaidml/" rel="noopener ugc nofollow" target="_blank"> Vertex.ai </a>的一部分。它可能最终使 AMD 卡(可能会有更多的厂商出现)成为 ML 中一个可行的选择。事实上，英特尔也将在 2020 年进入 GPU 市场，为数据中心的 ML 设计硬件，这可能通过竞争进一步降低计算价格。英特尔首席执行官<a class="ae lz" href="https://www.forbes.com/sites/marcochiappetta/2019/10/26/intel-xe-dg1-discrete-gpu-silicon-is-alive-and-being-validated-in-intels-labs/#1caee6ba645f" rel="noopener ugc nofollow" target="_blank"> Bob Swan 表示</a><em class="ly">“2020 年，我们将继续扩大我们的 10 纳米产品组合，推出令人兴奋的新产品，包括人工智能推理加速器、5G 基站 SOC、用于服务器存储的至强 CPU、网络和独立 GPU。本季度，我们的首款独立 GPU DG1 实现了通电退出，这是一个重要的里程碑。”</em>。敬请关注！</p></div></div>    
</body>
</html>