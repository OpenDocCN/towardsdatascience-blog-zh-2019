<html>
<head>
<title>Getting Started with Deep Reinforcement Learning Can Be a Beast, Here’s a Way to Frame It</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">开始深度强化学习可能会很难，这里有一种方法来框定它</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-deep-reinforcement-learning-can-be-a-beast-heres-a-way-to-frame-it-7063c3e1c584?source=collection_archive---------24-----------------------#2019-08-15">https://towardsdatascience.com/getting-started-with-deep-reinforcement-learning-can-be-a-beast-heres-a-way-to-frame-it-7063c3e1c584?source=collection_archive---------24-----------------------#2019-08-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="9d12" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">很多关于 DRL 的初级读本可能会令人困惑，而且是不必要的高水平。有一种更简单的方式开始你的旅程。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/1b5f5475925810774f7e5dd3552a13c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i0-YrSOILuuugrqffEeATQ.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Q-learning at work. Source: <a class="ae le" href="https://unsplash.com/photos/NuE8Nu3otjo" rel="noopener ugc nofollow" target="_blank">NASA</a> at <a class="ae le" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c17a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">幻想小说中有一个很好的比喻，适用于学习计算机科学、人工智能和机器学习。通常，奇幻小说的“魔法使用者”会充当知识的守门人，使得每一个教程对那些想要学习魔法的人来说都变得不必要的复杂。这是打算为自己囤积这种知识和力量。我不是想把每个计算机系的教授都说成是邪恶的巫师，但是…开个玩笑，我只是觉得教计算机科学真的真的很难——尤其是如果你一开始就不想当老师的话。我不是专家，但我想让所有这些概念更容易理解。因此，下面是如何在你的脑海中构建深度强化学习的方法。</p><p id="041e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">理解和学习编程的最好方法是设想一个项目，并找出如何完成它。这意味着拼凑出你需要使用什么样的工具来完成它，以及如何使用这些工具。在本例中，我创建了一个名为 Marco 的虚构角色。Marco 非常擅长“魔法语言”(即 Python)，了解如何使用其他人为自己的目的(即包)而制作的咒语，并开始能够概括“魔法世界”(计算)的工作方式。和所有优秀的巫师一样，马可是懒 AF。他的目标是创造一个没有知觉的代理人，可以替他做所有的家务。但是他有一个主要的问题——创造咒语需要一段时间！他已经研究他的自动化代理有一段时间了，并且已经意识到拥有一个没有感情的管家意味着他必须亲自为每一个环境中的每一个动作创造一个独特的咒语。马尔科没有时间，他只是想要一个简单的答案，他认为魔法应该让事情变得更容易！马可希望他的仆人能够借鉴马可自己的人类经验，但他不想只是创造另一个人类。他希望代理能够在一个下午内掌握他所知道的一切，甚至更多。马可利用他的技能——他记得他可以创造迷你世界，在那里他可以控制一切！包括时间。尽管马可在现实世界中没有足够的力量来影响时间，但他可以在这个迷你世界中做到。但这对他没有帮助，对吗？他需要能够在现实世界中使用他的代理人，拥有一个仅限于他的虚拟世界的代理人是没有用的。Marco 花了一上午的时间思考，想出了一个解决方案！如果他把代理人的“思想”放入假世界以获得经验，然后把训练过的思想取出来，放回他的仆人体内，那会怎么样？他会得到他想要的！</p><p id="688a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Marco 刚刚确定了强化学习的基础，这是人工智能的一个子集。在这个虚构的世界中，魔法取代了编程，马可意识到，为了创造一个能完成高级任务的自动化生物，他需要它来获得经验。但是获得经验花了马可二十四年，他仍然不知道一切！他没时间等那个。他需要大规模地复制体验过程。因此，他找到了一个变通办法——制造一个他控制所有变量的虚拟世界，设置变量以模仿真实世界，但有一些变化(例如，一个虚拟的秒=一个虚拟的小时/天/年)，并将大脑放入虚拟世界。一旦它学得和马可一样多，或者比马可更多，他就把它拿出来，他就有了一个现成的代理愿意并且有能力为他做会计工作！现在，我们已经对 Marco 的工作有所了解，让我们放弃这个神奇的比喻，进入我刚才描述的一些更真实的应用。</p><p id="e69d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在接下来的三个部分中，我想涵盖深度强化学习的主要方面。首先是模型构建。有四个主要方面需要理解:</p><ul class=""><li id="b9be" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated">目标</li><li id="72f3" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">世界</li><li id="cc3b" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">行动</li><li id="d4b9" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">奖励</li></ul><p id="17df" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些因素几乎是你建立深度强化学习模型所需要的全部。我将很快逐一介绍，但在开始之前，我想交流一个关于数据的重要话题。</p><p id="708b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">马尔科很容易做到——有了魔法，他所要做的就是使用某种可用的神秘能量来创造他的虚拟世界，并认真思考他想要创造什么。对于我们这些真实世界的人来说，我们有不同的参数。数据最重要。有三种类型的数据:数值型，分类型，两者都有！数字数据是最容易处理的——计算机可以通过数学运算快速发现模式。分类数据有点复杂。你如何开始与计算机交流，两个不同的物体之间有质的区别？你怎么能抽象这个呢？数字。特征工程是深度强化学习的一个重要组成部分，许多人工智能“魔法”就是这样发生的。我们只是使用一些特定用途的方法将找到的所有东西转换成数字。特性工程非常重要，所以我会在后面的一整篇文章中讨论这个问题。需要理解的重要一点是，一切最终都将由数字来表示，这就是计算机如何识别模式的。</p><p id="2cbc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">考虑到这一点，我们来谈谈目标。这是可变的，可以变得非常复杂。看看自动驾驶汽车就知道了。你的目标可以是从 A 点到 B 点，但也需要不破坏汽车，不出马路，以一定速度行驶，不撞到行人，保证乘客安全等。等等。目标函数是任何深度学习模型中最重要的部分，也是它变得复杂的部分原因。所以让我们从简单的东西开始:房子里的温度计。目标很简单:当人们在家时，保持室内温度在华氏 70 度。然后，困难的部分变成了定义国家(即世界)。</p><p id="3286" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">状态空间就是虚拟世界。你可以创造和探索三种类型的世界。最简单和最容易使用的是基于模型的强化学习。MB-RL 适用于游戏，尤其是像国际象棋这种规则不变的游戏。“模型”就是游戏规则。它只能发生在一个特殊的环境中——(即棋盘)。棋盘上能发生的事情是有限的(对于人类来说，仍然有令人讨厌的大量可能性)，这也是为什么这是最容易使用的方法。<strong class="js iu">学习曲线提醒:</strong>教授们会提出一个叫做“样本效率”的概念，让你感到困惑，这涉及到很多数学知识。为了避免过于深入决策树和修剪，只需知道有一种数学方法可以模拟职业象棋或围棋选手如何“感觉”一步好棋，然后计算机会做得更好。计算机不必计划出可能发生的每一种排列，这一事实使它变得“高效”。下一步是基于价值的强化学习。VB-RL 试图通过称为“开关策略”的概念来估计状态的质量。基本上，VB-RL 猜测它的行为对世界和它的目标是好是坏。偏离策略意味着代理不断尝试重新评估其方法。这是“样本效率低下”，因为模型在真正能够做任何有用的事情之前需要大量的样本。他们不会直接学习行动的策略，他们会学习一个行动过程有多好，并尝试在行动中最大化它。偶尔，他们会抛硬币来尝试新事物。在策略上意味着代理人将不断改变他们的方法给定一个场景，以确定他们的行动是否是好的。确定什么是最佳回报的最著名的算法之一(我马上会谈到这个)被称为 Q-learning(以 Q 命名，是对动作的抽象)。现在，让我们继续行动:</p><p id="ee8b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">操作步骤可能会更熟悉—它只是一个 if 函数。一个动作表示，如果满足某些条件，那么程序将做一些事情。Q-learning 是一种状态-动作函数(即，它涉及状态和动作类的变量)，试图最大化其回报。奖励有助于产生输出(等一下，我将在这之后讨论奖励)。Q-learning 是一种算法，它基本上是这样说的:使用<strong class="js iu">任何策略</strong>来猜测一个使未来回报最大化的 Q(即一个动作)。这与你一开始给出的策略无关。Q 算法有点难以理解，但最好以编程方式来考虑。这基本上是一个 for-if-loop，即在你初始化世界后，<strong class="js iu">代表</strong>世界中的一个状态和动作，执行动作，测量回报，并更新 q。如果你还没有达到最佳状态，继续做这个<strong class="js iu">，否则，终止循环。</strong></p><p id="38c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，奖励功能:它可以得到超级 wonky。有时候，特别是如果你的数据都是数字，这真的很容易确定。你有一个变量，x，你只要试着让它尽可能的高。在现实世界中，经常会涉及更多的变量，并且它们都相互影响(也就是为什么它是一个函数)。许多人喜欢谈论眼镜蛇效应。《眼镜蛇效应》讲述了印度政府的一项政策，新德里试图鼓励人们为了金钱而引进毒蛇。人们立即开始饲养这些危险的蛇来利用这个系统。你不想训练你的系统采用比问题更糟糕的解决方案。这是一个边缘案例——你的系统的后果希望不是可怕的——但是这个确切的场景可以被抽象为简单地代表你不想从 DRL 模型中得到的所有负面结果。一个更好的例子可能是一个机器人手臂，它正在被训练将积木放在彼此的顶部。代码的作者认为让“奖励”成为距离的度量是一个好主意——尽可能远地放置立方体(作者认为机器人手臂会一直伸出来，并将立方体线性地放置在彼此之上)。机器人最终学会了如何尽可能用力地将东西扔向墙壁。奖励是一项复杂的任务，通常不像简单地最大化变量的单位那么容易。当我写这篇文章时，我意识到我有点跑题了，所以我想在以后的文章中更深入地讨论奖励。</p><p id="45dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是深度强化学习背后的基础——这一切都是关于创建一个世界，制作一个代理，给它一组它可以采取的行动，并教它最大化任意奖励，以获得经验和什么行动会导致什么结果的想法。这是一个迷人的想法，希望我能很好地传达它的基本内容。如果你有任何问题，让我知道！</p></div></div>    
</body>
</html>