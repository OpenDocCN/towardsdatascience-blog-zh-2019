<html>
<head>
<title>Decision Tree Classifier from Scratch: Classifying Student’s Knowledge Level</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的决策树分类器:对学生的知识水平进行分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-classifier-from-scratch-classifying-students-knowledge-level-c810876d6c8f?source=collection_archive---------13-----------------------#2019-10-16">https://towardsdatascience.com/decision-tree-classifier-from-scratch-classifying-students-knowledge-level-c810876d6c8f?source=collection_archive---------13-----------------------#2019-10-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1ea7" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">向数据科学迈出一小步</h2><div class=""/><div class=""><h2 id="78af" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用 python 编程语言从头开始编写决策树分类器</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fc78759ae7bd1645a9f200e3841a8480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwOKi-XR8QcBtiObH96K-A.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by on <a class="ae lh" href="https://unsplash.com/@craftedbygc" rel="noopener ugc nofollow" target="_blank"><strong class="bd li">craftedbygc</strong></a></figcaption></figure><h1 id="71d2" class="lj lk it bd ll lm ln lo lp lq lr ls lt ki lu kj lv kl lw km lx ko ly kp lz ma bi translated">先决条件</h1><p id="5d90" class="pw-post-body-paragraph mb mc it md b me mf kd mg mh mi kg mj mk ml mm mn mo mp mq mr ms mt mu mv mw im bi translated">您需要具备以下方面的基本知识:</p><ol class=""><li id="f0d9" class="mx my it md b me mz mh na mk nb mo nc ms nd mw ne nf ng nh bi translated">Python 编程语言</li><li id="28a2" class="mx my it md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated">科学计算图书馆</li><li id="e7d4" class="mx my it md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated">熊猫</li></ol><h1 id="f2ca" class="lj lk it bd ll lm ln lo lp lq lr ls lt ki lu kj lv kl lw km lx ko ly kp lz ma bi translated">数据集</h1><p id="1100" class="pw-post-body-paragraph mb mc it md b me mf kd mg mh mi kg mj mk ml mm mn mo mp mq mr ms mt mu mv mw im bi translated">我将在这个项目中使用的数据集来自加州大学欧文分校用户知识建模数据集(UC Irvine)的<a class="ae lh" href="https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling" rel="noopener ugc nofollow" target="_blank"> <strong class="md jd">。</strong></a><br/>《UCI》一页提到了以下出版物作为数据集的原始来源:</p><blockquote class="nn no np"><p id="a7ab" class="mb mc nq md b me mz kd mg mh na kg mj nr ns mm mn nt nu mq mr nv nw mu mv mw im bi translated">H.T. Kahraman，Sagiroglu，s .，Colak，I,“开发直观知识分类器和用户领域相关数据的建模”,基于知识的系统，第 37 卷，第 283–295 页，2013 年</p></blockquote></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="aed8" class="lj lk it bd ll lm oe lo lp lq of ls lt ki og kj lv kl oh km lx ko oi kp lz ma bi translated">介绍</h1><h2 id="fd05" class="oj lk it bd ll ok ol dn lp om on dp lt mk oo op lv mo oq or lx ms os ot lz iz bi translated">1.1 什么是决策树分类器？</h2><p id="517a" class="pw-post-body-paragraph mb mc it md b me mf kd mg mh mi kg mj mk ml mm mn mo mp mq mr ms mt mu mv mw im bi translated">简单来说，决策树分类器就是用于<strong class="md jd">监督+分类问题</strong>的<a class="ae lh" href="https://dataconomy.com/2015/01/whats-the-difference-between-supervised-and-unsupervised-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="md jd">监督机器学习算法</strong> </a> <strong class="md jd"> </strong>。在决策树中，每个节点都会询问一个关于某个特征的是非问题，并根据决策向左或向右移动。</p><p id="e22f" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">你可以从<a class="ae lh" href="https://medium.com/@chiragsehra42/decision-trees-explained-easily-28f23241248" rel="noopener"> <strong class="md jd">这里</strong> </a>了解更多关于决策树的知识。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/fcbfc043df4922268566734cef81bd51.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/0*s-FZAM_yChgM9bam.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><a class="ae lh" href="https://miro.medium.com/max/820/1*JAEY3KP7TU2Q6HN6LasMrw.png" rel="noopener">Example of decision tree</a></figcaption></figure><h2 id="8c4d" class="oj lk it bd ll ok ol dn lp om on dp lt mk oo op lv mo oq or lx ms os ot lz iz bi translated">1.2 我们在建造什么？</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/34b1731c20a2626929069afb489d95da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dAS65veWLPc3cQPw"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@andriklangfield?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Andrik Langfield</a> on <a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6e13" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">我们将使用机器学习算法来发现学生历史数据上的模式，并对他们的知识水平进行分类，为此，我们将使用 Python 编程语言从头开始编写我们自己的简单决策树分类器。</p><blockquote class="nn no np"><p id="2389" class="mb mc nq md b me mz kd mg mh na kg mj nr ns mm mn nt nu mq mr nv nw mu mv mw im bi translated">虽然我会一路解释每件事，但这不会是一个基本的解释。我将通过进一步的链接突出显示重要的概念，这样每个人都可以对该主题进行更多的探索。</p></blockquote></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="70e6" class="lj lk it bd ll lm oe lo lp lq of ls lt ki og kj lv kl oh km lx ko oi kp lz ma bi translated">代码部分</h1><h2 id="0482" class="oj lk it bd ll ok ol dn lp om on dp lt mk oo op lv mo oq or lx ms os ot lz iz bi translated">2.1 准备数据</h2><p id="6164" class="pw-post-body-paragraph mb mc it md b me mf kd mg mh mi kg mj mk ml mm mn mo mp mq mr ms mt mu mv mw im bi translated">我们将使用<strong class="md jd"> pandas </strong>进行数据处理和数据清理。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/42777f6f28081b91fcc50221f3ffc185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gFT5srvMUxcwFnzsuD0tEw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Data Dictionary</figcaption></figure><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="fb33" class="oj lk it oy b gy pc pd l pe pf">import <strong class="oy jd">pandas</strong> as <strong class="oy jd">pd</strong></span><span id="7a2c" class="oj lk it oy b gy pg pd l pe pf">df = <strong class="oy jd">pd</strong>.read_csv('data.csv')<br/>df.head()</span></pre><p id="fa31" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">首先，我们导入<em class="nq"> data.csv </em>文件，然后将其作为熊猫的数据帧进行处理，并查看数据。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/b0053fb50e676a67bb10ebba82501c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*nlwBs75MKPtG2-kO6XenHQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Data Preview</figcaption></figure><p id="af20" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">正如我们看到的，我们的数据有 6 列。它包含 5 个功能和 1 个标签。既然这个数据已经被清理了，就不需要再进行数据清理和<a class="ae lh" href="https://www.datawatch.com/what-is-data-wrangling/" rel="noopener ugc nofollow" target="_blank"> <strong class="md jd">扯皮</strong> </a>。但是，在处理其他真实世界的数据集时，检查数据集中的空值和异常值并从中设计出最佳要素非常重要。</p><h2 id="81a2" class="oj lk it bd ll ok ol dn lp om on dp lt mk oo op lv mo oq or lx ms os ot lz iz bi translated">2.2 列车测试分割</h2><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="91d1" class="oj lk it oy b gy pc pd l pe pf">train = df.values[-:20]<br/>test = df.values[-20:]</span></pre><p id="205d" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">在这里，我们将数据分为训练和测试，其中数据集的最后 20 个数据是测试数据，其余的是训练数据。</p><h2 id="68bd" class="oj lk it bd ll ok ol dn lp om on dp lt mk oo op lv mo oq or lx ms os ot lz iz bi translated">2.3 编写我们自己的机器学习模型</h2><p id="ffa6" class="pw-post-body-paragraph mb mc it md b me mf kd mg mh mi kg mj mk ml mm mn mo mp mq mr ms mt mu mv mw im bi translated">现在是时候编写决策树分类器了。</p><p id="dfa3" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">但是在深入研究代码之前，有一些东西需要学习:</p><ol class=""><li id="c05d" class="mx my it md b me mz mh na mk nb mo nc ms nd mw ne nf ng nh bi translated">为了构建这棵树，我们使用了一种叫做<strong class="md jd"> CART </strong>的决策树学习算法。还有其他学习算法，如<strong class="md jd"> ID3，C4.5，C5.0，</strong>等。你可以从<a class="ae lh" href="https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1" rel="noopener">这里</a>了解更多。</li><li id="d7b0" class="mx my it md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated"><strong class="md jd"> CART </strong>代表分类和回归树。<strong class="md jd"> CART </strong>使用<strong class="md jd"> Gini </strong> <strong class="md jd">杂质</strong>作为其度量，以量化一个问题在多大程度上有助于解混数据，或者简单地说 CARD 使用 Gini 作为其成本函数来评估误差。</li><li id="2aae" class="mx my it md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated">在引擎盖下，所有的学习算法都给了我们一个决定何时问哪个问题的程序。</li><li id="686f" class="mx my it md b me ni mh nj mk nk mo nl ms nm mw ne nf ng nh bi translated">为了检查这个问题帮助我们解混数据的正确程度，我们使用了<strong class="md jd">信息增益。</strong>这有助于我们减少不确定性，我们利用这一点来选择要问的最佳问题，对于给定的问题，我们递归地构建树节点。然后，我们进一步继续划分节点，直到没有问题要问，我们将最后一个节点表示为叶子。</li></ol><h2 id="73c6" class="oj lk it bd ll ok ol dn lp om on dp lt mk oo op lv mo oq or lx ms os ot lz iz bi translated"><strong class="ak"> 2.3.1 书写助手功能</strong></h2><p id="b45c" class="pw-post-body-paragraph mb mc it md b me mf kd mg mh mi kg mj mk ml mm mn mo mp mq mr ms mt mu mv mw im bi translated">为了实现决策树分类器，我们需要知道什么时候对数据提出什么问题。让我们为此编写一个代码:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="5c7f" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">class </strong>CreateQuestion:</span><span id="3908" class="oj lk it oy b gy pg pd l pe pf"><strong class="oy jd">    def </strong>__init__(self, column, value):<br/>        self.column = column<br/>        self.value = value</span><span id="b3ac" class="oj lk it oy b gy pg pd l pe pf"><strong class="oy jd">    def </strong>check(self, data):<br/>        val = data[self.column]<br/>        return val &gt;= self.value<br/>    <strong class="oy jd">def </strong>__repr__(self):<br/>      <br/>        return "Is %s %s %s?" % (<br/>            self.column[-1], "&gt;=", str(self.value))</span></pre><p id="cf3a" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">上面我们写了一个<strong class="md jd"> CreateQuestion </strong>类，它有两个输入:<em class="nq">列号</em>和<em class="nq">值</em>作为实例变量。它有一个用于比较特征值的<strong class="md jd">检查</strong>方法。</p><p id="e72d" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated"><em class="nq"> __repr__ </em>只是 python 的一个帮助显示问题的神奇函数。</p><p id="f116" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">让我们来看看它的实际应用:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="68b0" class="oj lk it oy b gy pc pd l pe pf">q = <strong class="oy jd">CreateQuestion</strong>(0, 0.08)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/6f27c4b685042751c23cde156366b93d.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*DKHtAhOUO3aTS_WyDpizDA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Creating question</figcaption></figure><p id="d518" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">现在让我们检查一下我们的<strong class="md jd">检查</strong>方法是否工作正常:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="590a" class="oj lk it oy b gy pc pd l pe pf">data = train[0]<br/>q.check(data)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/6a69931b477144440fb43eefa133b388.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*UzpGd_LQF2CET_nAAjT14A.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Testing check method</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/609d645f43737e46877d9e8e79bb41eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*Izj_XEUNy7h3gggCpCKEgg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Cross validating check method</figcaption></figure><p id="79bb" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">正如我们看到的，我们得到了一个错误的值。因为我们在训练集中的 0ᵗʰ值是 0.0，不大于或等于 0.08，所以我们的方法工作得很好。</p><p id="4237" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">现在是时候创建一个分区函数来帮助我们将数据分成两个子集:第一个集合包含所有为真的数据，第二个包含所有为假的数据。</p><p id="7c48" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">让我们为此编写一个代码:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="0446" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">def</strong> partition(rows, qsn):<br/>  <br/>    true_rows, false_rows = [], []<br/>    <strong class="oy jd">for</strong> row <strong class="oy jd">in</strong> rows:<br/>        <strong class="oy jd">if</strong> qsn.check(row):<br/>            true_rows.append(row)<br/>        <strong class="oy jd">else</strong>:<br/>            false_rows.append(row)<br/>    <strong class="oy jd">return</strong> true_rows, false_rows</span></pre><p id="1cb1" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">我们的分区函数接受两个输入:<em class="nq">行</em>和一个<em class="nq">问题</em>，然后返回一列<em class="nq">真行</em>和<em class="nq">假行</em>。</p><p id="9aff" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">让我们也来看看实际情况:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="8f2e" class="oj lk it oy b gy pc pd l pe pf">true_rows, false_rows = partition(train, CreateQuestion(0, 0.08))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/376cb592eaacf82124aefc9936697c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*ly1289pB2LQGVzd6nW8EWg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">True and False row preview</figcaption></figure><p id="981e" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">这里 t <em class="nq"> rue_rows </em>包含所有大于或等于 0.08 的数据，而<em class="nq"> false_rows </em>包含小于 0.08 的数据。</p><p id="fc9d" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">现在该写我们的<strong class="md jd">基尼杂质</strong>算法了。正如我们之前讨论的，它帮助我们量化节点中有多少不确定性，而<strong class="md jd">信息增益</strong>让我们量化一个问题减少了多少不确定性。</p><p id="ffd0" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">杂质指标范围在 0 到 1 之间，数值越低表示不确定性越小。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/5a7b96fa3ff017fc0d31d30e9841e0ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*8yLPGA2fsmzx0Y4zRW-juQ.png"/></div></figure><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="9c52" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">def</strong> gini(rows):</span><span id="45b6" class="oj lk it oy b gy pg pd l pe pf">    counts = class_count(rows)<br/>    impurity = 1<br/>    <strong class="oy jd">for</strong> label <strong class="oy jd">in</strong> counts:<br/>        probab_of_label = counts[label] / float(len(rows))<br/>        impurity -= probab_of_label**2<br/>    <strong class="oy jd">return</strong> impurity</span></pre><p id="b448" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">在我们的<strong class="md jd">基尼</strong>函数中，我们刚刚实现了<strong class="md jd">基尼的公式。</strong>返回给定行的杂质值。</p><p id="5f1d" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated"><em class="nq"> counts </em>变量保存数据集中给定值的总计数的字典。<em class="nq"> class_counts </em>是一个帮助函数，用于计算某个类的数据集中出现的数据总数。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="c112" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">def</strong> class_counts(rows):<br/>   <br/>    <strong class="oy jd">for</strong> row <strong class="oy jd">in</strong> rows:<br/>        <br/>        label = row[-1]<br/>        <strong class="oy jd">if</strong> label <strong class="oy jd">not</strong> <strong class="oy jd">in</strong> counts:<br/>            counts[label] = 0<br/>        counts[label] += 1<br/>    <strong class="oy jd">return</strong> counts</span></pre><p id="752f" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">让我们来看看<strong class="md jd">基尼</strong>的作用:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/73d6972dd95e3f9bc3825a5b44421445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*dhBwC3iFlsM1cBti5ictcA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Gini in action: 1</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi po"><img src="../Images/1ba602453a07ecde51a15fa8e6d502a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*rfWY0TD2Gi1P69nclAOX_w.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Gini in action: 2</figcaption></figure><p id="9eb1" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">正如您在图像<em class="nq"> 1 </em>中看到的，有一些杂质，所以它返回 0.5，而在图像<em class="nq"> 2 </em>中没有杂质，所以它返回 0。</p><p id="8c35" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">现在我们要编写计算信息增益的代码:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="d26f" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">def</strong> info_gain(left, right, current_uncertainty):<br/>    <br/>    p = float(len(left)) / (len(left) + len(right))<br/>    <strong class="oy jd">return</strong> current_uncertainty - p * gini(left) \<br/>            - (1 - p) * gini(right)</span></pre><p id="5d1d" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">通过减去具有两个子节点的加权杂质的不确定性起始节点来计算信息增益。</p><h2 id="d97d" class="oj lk it bd ll ok ol dn lp om on dp lt mk oo op lv mo oq or lx ms os ot lz iz bi translated">2.3.2 将所有内容放在一起</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pp"><img src="../Images/f8607e7a380d3b6f73e552c53e7f31ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MoTK-0M_QG9VaXRKWYkEnA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@ryoji__iwata" rel="noopener ugc nofollow" target="_blank">ryoji</a></figcaption></figure><p id="a319" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">现在是时候把所有东西放在一起了。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="8d6c" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">def</strong> find_best_split(rows):<br/>   <br/>    best_gain = 0<br/>    best_question = None<br/>    current_uncertainty = gini(rows)<br/>    n_features = len(rows[0]) - 1<br/><br/>    <strong class="oy jd">for</strong> col <strong class="oy jd">in</strong> range(n_features):<br/><br/>        values = set([row[col] <strong class="oy jd">for</strong> row <strong class="oy jd">in</strong> rows])  <br/><br/>        <strong class="oy jd">for</strong> val <strong class="oy jd">in</strong> values:<br/>            question = Question(col, val)<br/>            true_rows, false_rows = partition(rows, question)</span><span id="610b" class="oj lk it oy b gy pg pd l pe pf">            <strong class="oy jd">if</strong> len(true_rows) == 0 <strong class="oy jd">or</strong> len(false_rows) == 0:<br/>                <strong class="oy jd">continue</strong><br/>            <br/>            gain = info_gain(true_rows, false_rows,\<br/>                   current_uncertainty)</span><span id="8d81" class="oj lk it oy b gy pg pd l pe pf">            <strong class="oy jd">if</strong> gain &gt; best_gain:<br/>                best_gain, best_question = gain, question<br/><br/>    <strong class="oy jd">return</strong> best_gain, best_question</span></pre><p id="a7e2" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">我们编写了一个<strong class="md jd"> find_best_split </strong>函数，它通过迭代每个特征和标签找到最佳问题，然后计算信息增益。</p><p id="6943" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">让我们来看看这个函数的运行情况:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="97bc" class="oj lk it oy b gy pc pd l pe pf">best_gain, best_question = find_best_split(train)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/8ac7f194a995ab61daacd9922b0cdfd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*_PvCGxNli4t1iN_eO9esLA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Finding best question</figcaption></figure><p id="23fd" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">我们现在要编写我们的<strong class="md jd"> fit </strong>函数。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="73b5" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">def</strong> fit(features, labels):</span><span id="08e3" class="oj lk it oy b gy pg pd l pe pf">    data = features + labels</span><span id="b378" class="oj lk it oy b gy pg pd l pe pf">    gain, question = find_best_split(data)<br/><br/>    <strong class="oy jd">if</strong> gain == 0:<br/>        <strong class="oy jd">return</strong> Leaf(rows)<br/><br/>    true_rows, false_rows = partition(rows, question)<br/><br/>    <em class="nq"># Recursively build the true branch.</em><br/>    true_branch = build_tree(true_rows)<br/><br/>    <em class="nq"># Recursively build the false branch.</em><br/>    false_branch = build_tree(false_rows)<br/><br/>    <strong class="oy jd">return</strong> Decision_Node(question, true_branch, false_branch)</span></pre><p id="3ca2" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">我们的<strong class="md jd"> fit </strong>函数基本上为我们构建了一棵树。它从根节点开始，通过使用我们的<strong class="md jd"> find_best_split </strong>函数找到针对该节点的最佳问题。它迭代每个值，然后分割数据并计算信息增益。在这一过程中，它跟踪产生最大收益的问题。</p><p id="0235" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">之后，如果仍有有用的问题要问，增益将大于 0，因此行被分组为分支，并且它首先递归构建<strong class="md jd">真分支</strong>，直到没有进一步的问题要问并且增益为 0。</p><p id="a0e8" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">该节点随后成为一个<strong class="md jd">叶节点</strong>。</p><p id="0447" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">我们的<strong class="md jd"> Leaf </strong>类的代码如下所示:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="69d3" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">class</strong> <strong class="oy jd">Leaf</strong>:<br/><br/>    <strong class="oy jd">def</strong> __init__(self, rows):<br/>        self.predictions = class_counts(rows)</span></pre><p id="0c2c" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">它保存了一个类别为<em class="nq">(“高”、“低”)</em>的字典，以及它在到达当前叶子的数据的行中出现的次数。</p><p id="d735" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">对于<strong class="md jd">假分支</strong>也应用相同的过程。之后，它就变成了一个<strong class="md jd">决策 _ 节点</strong>。</p><p id="6f4d" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">我们的 Decision_Node 类的代码如下所示:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="af74" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">class</strong> <strong class="oy jd">Decision_Node</strong>:     <br/>   <strong class="oy jd">def</strong> __init__(self, question,\<br/>                true_branch,false_branch):<br/>       self.question = question<br/>       self.true_branch = true_branch<br/>       self.false_branch = false_branch</span></pre><p id="0587" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">这个类只保存我们已经问过的问题的引用和产生的两个子节点。</p><p id="91d6" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">现在我们回到根节点，建立<strong class="md jd">错误分支。</strong>因为没有任何问题要问，所以它成为<strong class="md jd">叶节点</strong>，并且根节点也成为<strong class="md jd">决策 _ 节点</strong>。</p><p id="5d67" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">让我们看看<strong class="md jd"> fit </strong>功能的实际应用:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="5545" class="oj lk it oy b gy pc pd l pe pf">_tree = fit(train)</span></pre><p id="0be1" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">为了打印树<em class="nq">_ 树</em>，我们必须编写一个特殊的函数。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="a6f3" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">def</strong> print_tree(node, spacing=""):<br/> <br/>    <em class="nq"># Base case: we've reached a leaf</em><br/>    <strong class="oy jd">if</strong> isinstance(node, Leaf):<br/>        <strong class="oy jd">print</strong> (spacing + "Predict", node.predictions)<br/>        <strong class="oy jd">return</strong><br/><br/>    <em class="nq"># Print the question at this node</em><br/>    <strong class="oy jd">print</strong> (spacing + str(node.question))<br/><br/>    <em class="nq"># Call this function recursively on the true branch</em><br/>    <strong class="oy jd">print</strong> (spacing + '--&gt; True:')<br/>    print_tree(node.true_branch, spacing + "  ")<br/><br/>    <em class="nq"># Call this function recursively on the false branch</em><br/>    <strong class="oy jd">print</strong> (spacing + '--&gt; False:')<br/>    print_tree(node.false_branch, spacing + "  ")</span></pre><p id="fa79" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">这个<strong class="md jd"> print_tree </strong>函数帮助我们以一种令人敬畏的方式可视化我们的树。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pr"><img src="../Images/97335211d2fe219287e1f74f903ff89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qPbfiF4BhwM6ECGDdp_ogA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Sample of our tree</figcaption></figure><p id="77e4" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">我们刚刚完成了决策树分类器的构建！</p><p id="b6eb" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">为了理解和查看我们构建的内容，让我们再编写一些帮助函数:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="5ccf" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">def</strong> classify(row, node):<br/>   <br/>    <strong class="oy jd">if</strong> isinstance(node, Leaf):<br/>        <strong class="oy jd">return</strong> node.predictions<br/><br/>    <strong class="oy jd">if</strong> node.question.check(row):<br/>        <strong class="oy jd">return</strong> classify(row, node.true_branch)<br/>    <strong class="oy jd">else</strong>:<br/>        <strong class="oy jd">return</strong> classify(row, node.false_branch)</span></pre><p id="de87" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated"><strong class="md jd">分类</strong>功能帮助我们检查给定行和树的可信度。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="b51b" class="oj lk it oy b gy pc pd l pe pf">classify(train[5], _tree)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/42b14279731f064635f102dcff8035f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*GhgpQX5TYTW7Pa0AJzBQpg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Classification sample</figcaption></figure><p id="eac1" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">正如你在上面看到的，我们的树把给定的值归类为 96 置信度的中间值。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="6ffb" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">def</strong> print_leaf(counts):<br/>   <br/>    total = sum(counts.values()) * 1.0<br/>    probs = {}<br/>    <strong class="oy jd">for</strong> lbl in counts.keys():<br/>        probs[lbl] = str(int(counts[lbl] / total * 100)) + "%"<br/>    <strong class="oy jd">return</strong> probs</span></pre><p id="fb17" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">函数帮助我们美化我们的预测。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/4b02b75a8f5b2e86d8a58906ff334d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*KG0SuRuFCvxuoVBWECCfmg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Pretty print</figcaption></figure><h2 id="3d85" class="oj lk it bd ll ok ol dn lp om on dp lt mk oo op lv mo oq or lx ms os ot lz iz bi translated">2.4 模型评估</h2><p id="bf9b" class="pw-post-body-paragraph mb mc it md b me mf kd mg mh mi kg mj mk ml mm mn mo mp mq mr ms mt mu mv mw im bi translated">我们已经成功地建立，可视化，并看到我们的树在行动。</p><p id="2dbb" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">现在让我们执行一个简单的模型评估:</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="8793" class="oj lk it oy b gy pc pd l pe pf"><strong class="oy jd">for</strong> row <strong class="oy jd">in</strong> testing_data:<br/>    <strong class="oy jd">print</strong> ("Actual level: <strong class="oy jd">%s</strong>. Predicted level: <strong class="oy jd">%s</strong>" %<br/>           (df['LABEL'], print_leaf(classify(row, _tree))))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/a2ff81e7b751fc9c4d7a0ece22c7f279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sdqgi9sgCzM67Fb68GHKkQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Predicted on test data</figcaption></figure><p id="df35" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">如你所见，我们的树很好地预测了从训练数据中分离出来的测试数据。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="b357" class="lj lk it bd ll lm oe lo lp lq of ls lt ki og kj lv kl oh km lx ko oi kp lz ma bi translated">结论</h1><p id="4f83" class="pw-post-body-paragraph mb mc it md b me mf kd mg mh mi kg mj mk ml mm mn mo mp mq mr ms mt mu mv mw im bi translated">决策树分类器是一个很棒的学习算法。它对初学者很友好，也很容易实现。我们从头开始构建了一个非常简单的决策树分类器，没有使用任何抽象库来预测学生的知识水平。通过交换数据集和调整一些函数，您也可以将该算法用于另一个分类目的。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="3ad4" class="lj lk it bd ll lm oe lo lp lq of ls lt ki og kj lv kl oh km lx ko oi kp lz ma bi translated">参考</h1><p id="95cd" class="pw-post-body-paragraph mb mc it md b me mf kd mg mh mi kg mj mk ml mm mn mo mp mq mr ms mt mu mv mw im bi translated">[1] <em class="nq"> H. T. Kahraman，Sagiroglu，s .，Colak，I. </em>，开发直观知识分类器并对 web 中用户的领域相关数据建模，基于知识的系统，第 37 卷，第 283–295 页，2013 年</p><p id="ab4e" class="pw-post-body-paragraph mb mc it md b me mz kd mg mh na kg mj mk ns mm mn mo nu mq mr ms nw mu mv mw im bi translated">[2] <em class="nq"> Aurelien Geron </em>，用 Scikit-Learn 进行动手机器学习，Keras &amp; TensorFlow，O'REILLY，第二版，2019</p></div></div>    
</body>
</html>