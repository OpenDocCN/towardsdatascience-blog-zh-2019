<html>
<head>
<title>Review: DeepPose — Cascade of CNN (Human Pose Estimation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:深度姿势 CNN 的级联(人体姿势估计)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=collection_archive---------7-----------------------#2019-03-30">https://towardsdatascience.com/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=collection_archive---------7-----------------------#2019-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e194" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在四个数据集上使用级联卷积神经网络进行优化，实现最先进的性能</h2></div><p id="e266" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事里，<strong class="kh ir">的<strong class="kh ir"> DeepPose，</strong>Google 的</strong>，<strong class="kh ir">用于人体姿态估计</strong>，进行了回顾。它被公式化为<strong class="kh ir">基于深度神经网络(DNN)的针对身体关节的回归问题</strong>。<strong class="kh ir">利用级联的 DNN，可以实现高精度的姿态估计</strong>。这是一篇<strong class="kh ir"> 2014 年 CVPR </strong>论文，引用超过<strong class="kh ir"> 900 次</strong>。(<a class="lk ll ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----cf3170103e36--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="948d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">概述</h1><ol class=""><li id="2fb6" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la ms mt mu mv bi translated"><strong class="kh ir">姿态向量</strong></li><li id="a955" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated"><strong class="kh ir">卷积神经网络(CNN)作为姿态回归器</strong></li><li id="ff63" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated"><strong class="kh ir">姿态回归器的级联</strong></li><li id="dcc3" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated"><strong class="kh ir">结果</strong></li></ol></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="d192" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.姿态向量</h1><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nb"><img src="../Images/6484b1429da0178683898bee03ffa1dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PVh7wwY2zDcKeO8v8OkSOw.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Pose Vector (Miss You Paul Walker!)</strong></figcaption></figure><ul class=""><li id="a31e" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">为了表达一个姿势，我们将所有 k 个身体关节的位置编码在姿势向量中，定义为<strong class="kh ir"> y </strong>:</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/2407425a52ca7dadec3cfce93cba1f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*XPda5cnmCFbT8PJy_j7GsA.png"/></div></figure><ul class=""><li id="e075" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">对于每一个<strong class="kh ir"><em class="nx"/></strong>，都有第 I 个关节的<em class="nx"> x </em>和<em class="nx"> y </em>坐标。这些是图像中的绝对坐标。</li><li id="38fa" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">被标记的图像由(<em class="nx"> x </em>，<strong class="kh ir"> <em class="nx"> y </em> </strong>)表示，其中<em class="nx"> x </em>是图像数据，<strong class="kh ir"> <em class="nx"> y </em> </strong>是地面真实姿态向量，如上式所示。(我遵循了论文中的指示，尽管对 y 来说可能有点混乱。)</li><li id="a007" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">并且我们可以<strong class="kh ir">归一化人体或其部分所围成的坐标<em class="nx">yi</em>w . r . t . a 框<em class="nx"> b </em>，其中<em class="nx"> b </em> =( <em class="nx"> bc </em>，<em class="nx"> bw </em>，<em class="nx"> bh </em>)以<em class="nx"> bc </em>为中心，<em class="nx"> bw </em>为</strong></li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/54df07ec6755c9939bdf7ea986579969.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*D9Z5AklgPZwFmNzH9x2w-g.png"/></div></figure><ul class=""><li id="c82a" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">如上图，<strong class="kh ir"> <em class="nx">易</em> </strong>按方框大小缩放，按方框中心平移。使用:</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/5f099fcf6933c68b587a64e62799f08b.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*XmFzxW-jMG441vPcdneedQ.png"/></div></figure><ul class=""><li id="6a63" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated"><strong class="kh ir"><em class="nx">N</em>(<em class="nx">y</em>；<em class="nx"> b </em> ) </strong>是归一化的姿态向量。以及<em class="nx">N</em>(<em class="nx">x</em>)；<em class="nx"> b </em>是由边界框<em class="nx"> b </em>对图像<em class="nx"> x </em>的裁剪。</li></ul></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="440a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.C <strong class="ak">选择神经网络(CNN)作为姿态回归器</strong></h1><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0331b2a93abb6d505673860f036e7717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*fxgKYTIUUHyDkj4JO8S6Fw.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">CNN As Pose Regressor</strong></figcaption></figure><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/64adfdfea315d7f0ab6e92ccb3a03a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*Ki11fYRN7_4SEY2MwqkDDw.png"/></div></figure><ul class=""><li id="dae6" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">利用训练好的参数<em class="nx"> θ </em>，基于 CNN 的<em class="nx"> ψ </em>输出关节的归一化预测。<strong class="kh ir"> <em class="nx"> y* </em> </strong>可以通过反规格化得到<em class="nx"> N </em> ^-1.</li><li id="7967" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">架构如上图是<a class="ae oc" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> AlexNet </a>。</li><li id="8995" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated"><strong class="kh ir">第一层</strong>将预定义尺寸的<strong class="kh ir">图像</strong>作为<strong class="kh ir">输入</strong>。</li><li id="f3b1" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated"><strong class="kh ir">最后一层输出<em class="nx"> 2k </em>关节坐标</strong>。</li><li id="0228" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">C(55×55×96)—LRN—P—C(27×27×256)—LRN—P—C(13×13×384)—C(13×13×384)—C(13×13×256)—P—F(4096)—F(4096)其中 C 为卷积，LRN 为局部响应归一化，P 为汇集，F 为全连通层。</li><li id="d000" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">参数总数为 40M。</li><li id="c713" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">该损失是通过<strong class="kh ir">最小化预测的和地面真实姿态向量</strong>之间的 L2 距离来预测姿态向量的线性回归损失。</li><li id="5952" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">利用归一化训练集<em class="nx"> D_N </em>，L2 损失为:</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d84acf6a80984d0701718a986886317a.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*B9Nht4e_Ah7uKgVyKFp1XQ.png"/></div></figure><ul class=""><li id="7c59" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">其中<em class="nx"> k </em>是该图像中关节的数量。</li><li id="0348" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">小批量是 128 个。通过随机平移和左/右翻转增加数据。</li></ul></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="367d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">3.<strong class="ak">姿态回归器的级联</strong></h1><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi oe"><img src="../Images/ab3c2d234c6b67ea8fff21a4ec407f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8u1-EDBtSts9K9Lzg2AVA.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Cascade of Pose Regressors: First Stage: (Left), Subsequent Stages (Right)</strong></figcaption></figure><ul class=""><li id="df27" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">不容易增加输入大小来获得更精细的姿态估计，因为这将增加已经很大数量的参数。因此，提出了级联的姿态回归器。</li><li id="4ea1" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">因此，随着阶段<em class="nx"> s </em>的参与，第一阶段:</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f0fc1f837540eb8c6e365dbe2c91a503.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*DAnDSUSLhzyoig4Wi498IA.png"/></div></figure><ul class=""><li id="1e27" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">其中<em class="nx"> b </em> ⁰是人探测器获得的完整图像或一个方框。</li><li id="c9ae" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">然后，后续阶段:</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi og"><img src="../Images/55f58ba5fa0a20e03410e4b6c0297882.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*GE1Y8oeME6TF1q42XZtSCQ.png"/></div></figure><ul class=""><li id="fe4d" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">其中 diam( <strong class="kh ir"> <em class="nx"> y </em> </strong>)是相对关节的距离，比如左肩和右髋，然后用σ缩放，σdiam( <strong class="kh ir"> <em class="nx"> y </em> </strong>)。</li><li id="7232" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">对于后续层，基于来自<em class="nx">ni</em>s<em class="nx">s</em>-1)的采样位移<em class="nx"> ẟ </em>进行增强以生成模拟预测:</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/417dac3ecf3e1f57bb33cafa6f691e3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*2yuOC05Hfj4QAEor1V3tkA.png"/></div></figure><ul class=""><li id="8ed6" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">并且训练基于这个扩充的训练集:</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/e056e57042d4663e5e51f28d82c0450b.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*nEDC7spbpEvMqlgjS4G-6Q.png"/></div></figure></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="ee2a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">4.结果</h1><h2 id="be2b" class="oj lu iq bd lv ok ol dn lz om on dp md ko oo op mf ks oq or mh kw os ot mj ou bi translated">4.1.数据集</h2><ul class=""><li id="800d" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la nv mt mu mv bi translated"><strong class="kh ir">电影中标记的帧(FLIC) </strong>:来自好莱坞电影的 4000 个训练和 1000 个测试图像，姿势多样，服装多样。对于每个被标记的人，标记 10 个上身关节。</li><li id="bc6f" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated"><strong class="kh ir">Leeds Sports Dataset(LSP)</strong>:11000 张训练和 1000 张测试图像，这些图像来自在外观和特别是发音方面具有挑战性的体育活动。大多数人有 150 像素高。每个人全身总共有 14 个关节。</li></ul><h2 id="f09f" class="oj lu iq bd lv ok ol dn lz om on dp md ko oo op mf ks oq or mh kw os ot mj ou bi translated">4.2.韵律学</h2><ul class=""><li id="594b" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la nv mt mu mv bi translated"><strong class="kh ir">正确部位百分比(PCP) </strong>:测量肢体检测率，其中如果两个预测关节位置和真实肢体关节位置之间的距离最多为肢体长度的一半，则认为检测到肢体。</li><li id="ede5" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated"><strong class="kh ir">检测到的关节百分比(PDJ) </strong>:如果预测关节和真实关节之间的距离在躯干直径的某个分数范围内，则认为检测到了一个关节。通过改变这个分数，可以获得不同程度定位精度的检测率。</li></ul><h2 id="7368" class="oj lu iq bd lv ok ol dn lz om on dp md ko oo op mf ks oq or mh kw os ot mj ou bi translated">4.3.消融研究</h2><ul class=""><li id="7666" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la nv mt mu mv bi translated">FLIC 和 LSP 数据集的 50 个图像的小集合。</li><li id="7d99" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">对于 FLIC，探索值{0.8，1.0，1.2}后σ = 1.0。</li><li id="6ebe" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">对于 LSP，探索值{1.5，1.7，2.0，2.3}后σ = 2.0。</li><li id="80a0" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">对于上述数据集，当<em class="nx"> S </em> = 3 时，停止改进。</li><li id="1312" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">对于从<em class="nx"> s </em> = 2 开始的每个级联阶段，增加 40 个随机平移的裁剪框。对于 14 节点的 LSP，训练样本数= 11000×40×2×14 = 12M。</li><li id="26d4" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">在 12 核 CPU 上，每个图像的运行时间约为 0.1 秒。</li><li id="fa07" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">初始阶段大约在 3 天内进行培训。100 名工人，但大部分最终表演是在 12 个小时后完成的。</li><li id="f820" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">每个细化阶段训练 7 天，因为由于数据扩充，数据量比初始阶段的数据量大 40。</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/132178e28ce1c9771b9299745df0c925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*ufSYttYqiHmy9qHA_YfcbA.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">PDJ on FLIC or the first three stages of the DNN cascade</figcaption></figure><ul class=""><li id="7050" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">级联 CNN 进行优化有助于改善结果。</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/87d099eecac311cdbe7795039d79de31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*kuKwwh1dEMYZR2W3Pu9MGQ.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Predicted Pose (Red) Ground Truth Poses (Green)</strong></figcaption></figure><ul class=""><li id="39a6" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">同样，细化有助于改善结果。</li></ul><h2 id="47c8" class="oj lu iq bd lv ok ol dn lz om on dp md ko oo op mf ks oq or mh kw os ot mj ou bi translated">4.4.与最先进方法的比较</h2><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ox"><img src="../Images/182739aa86e5050722487f3e355f00b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-BKI9AC_AEZfK_SIQ0EDAA.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">PDJ on FLIC for Two Joints: Elbows and Wrists</strong></figcaption></figure><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi oy"><img src="../Images/902216e8a73a00438e45ecf59f4b7ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8h9VZMZc4K5rk6bFKksPng.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">PDJ on LSP for Two Joints: Arms and Legs</strong></figcaption></figure><ul class=""><li id="ae2b" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">对于两个数据集，DeepPose 获得了与真实关节的不同归一化距离的最高检测率。</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi oz"><img src="../Images/b6e04cda9f967d5e5a27ddb96eb2f689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dkjYc5wC5SFdegXJUbWEZw.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">PCP at 0.5 on LSP</strong></figcaption></figure><ul class=""><li id="183b" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">DeepPose-st2 和 DeepPose-st3 获得了最先进的结果。</li></ul><h2 id="8008" class="oj lu iq bd lv ok ol dn lz om on dp md ko oo op mf ks oq or mh kw os ot mj ou bi translated">4.5.跨数据集综合</h2><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c286ee9908d4b0d7984b9552b8a47dfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*NuneK8kK1TDd16gBtUKHNg.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">PDJ on Buffy Dataset for To Joints: Elbow and Wrist</strong></figcaption></figure><ul class=""><li id="a5a5" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">此外，在 FLIC 上训练的上身模型应用于整个 Buffy 数据集。</li><li id="fb15" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la nv mt mu mv bi translated">DeepPose 获得可比较的结果。</li></ul><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pb"><img src="../Images/0cd1175ba769c2ce2a7e2cd40242e59f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PHMdrqQR91cJnzDHFlLMDA.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">PCP at 0.5 on Image Parse Dataset</strong></figcaption></figure><ul class=""><li id="a985" class="ml mm iq kh b ki kj kl km ko ns ks nt kw nu la nv mt mu mv bi translated">在 LSP 上训练的全身模型在图像解析数据集的测试部分上被测试。</li></ul><h2 id="a450" class="oj lu iq bd lv ok ol dn lz om on dp md ko oo op mf ks oq or mh kw os ot mj ou bi translated">4.6.示例姿势</h2><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pc"><img src="../Images/295167ee9bdbc6ee4d13ea65e5e63fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*00lhdTUCG3ZgbfBR6sFODg.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Visualization of LSP</strong></figcaption></figure><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pd"><img src="../Images/ed31fa6236225ed99d523de3b56de21e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppt72qoAoZcEgo7GZWuMoQ.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Visualization of FLIC</strong></figcaption></figure></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h2 id="b243" class="oj lu iq bd lv ok ol dn lz om on dp md ko oo op mf ks oq or mh kw os ot mj ou bi translated">参考</h2><p id="f011" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko pe kq kr ks pf ku kv kw pg ky kz la ij bi translated">【2014 CVPR】【Deep Pose】<br/><a class="ae oc" href="https://arxiv.org/abs/1312.4659" rel="noopener ugc nofollow" target="_blank">Deep Pose:通过深度神经网络进行人体姿态估计</a></p><h2 id="55eb" class="oj lu iq bd lv ok ol dn lz om on dp md ko oo op mf ks oq or mh kw os ot mj ou bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko pe kq kr ks pf ku kv kw pg ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(是)(这)(些)(人)(,)(还)(是)(这)(些)(人)(,)(还)(没)(有)(什)(么)(好)(的)(情)(情)(况)(,)(还)(是)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae oc" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae oc" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae oc" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae oc" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae oc" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae oc" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae oc" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae oc" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae oc" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [ </a><a class="ae oc" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a> ] [ <a class="ae oc" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae oc" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae oc" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3 </a> ] [ <a class="ae oc" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a> ] [ <a class="ae oc" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a> ] [ <a class="ae oc" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44"> DCN </a> ]</p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong><a class="ae oc" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae oc" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae oc" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae oc" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae oc" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae oc" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae oc" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae oc" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a><a class="ae oc" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a>]</p><p id="fc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong>[<a class="ae oc" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae oc" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae oc" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae oc" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae oc" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae oc" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>[<a class="ae oc" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>]</p><p id="3134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割<br/></strong>[<a class="ae oc" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS</a>[<a class="ae oc" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae oc" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae oc" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae oc" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a>][<a class="ae oc" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC</a>][<a class="ae oc" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">Instance fcn</a>][<a class="ae oc" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS</a></p><p id="58de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">超分辨率<br/></strong>[<a class="ae oc" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">Sr CNN</a>][<a class="ae oc" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4">fsr CNN</a>][<a class="ae oc" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f">VDSR</a>][<a class="ae oc" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener">ESPCN</a>][<a class="ae oc" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>][<a class="ae oc" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener">DRCN</a>][<a class="ae oc" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994">DRRN</a>][<a class="ae oc" rel="noopener" target="_blank" href="/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8">LapSRN&amp;MS-LapSRN</a>][<a class="ae oc" rel="noopener" target="_blank" href="/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8">srdensenenet</a></p><p id="f29d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">人体姿态估计</strong><br/><a class="ae oc" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">汤普逊 NIPS’14</a></p></div></div>    
</body>
</html>