<html>
<head>
<title>Lasso, ridge and dropout regularization — their effects on collinearity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">套索、脊和漏失正则化—它们对共线性的影响</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/different-forms-of-regularization-and-their-effects-6a714f156521?source=collection_archive---------11-----------------------#2019-09-21">https://towardsdatascience.com/different-forms-of-regularization-and-their-effects-6a714f156521?source=collection_archive---------11-----------------------#2019-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="d05f" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">介绍</h1><p id="f83f" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这是我上一篇业余的、晦涩的文章的续篇——<a class="ae lm" href="https://medium.com/@snaveenmathew/a-short-note-on-regularization-42ee07c65d90" rel="noopener">关于正规化的简短说明</a>。这篇文章提供了它所承诺的东西，但它不足以回答这些问题——正则化做了什么，为什么它可以工作，而相应的没有正则化的模型却不能？本文的目的是试图用线性代数(正规方程)和统计学(估计量的偏差-方差权衡)来回答这些问题。</p><p id="455d" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">让我们假设感兴趣的隐藏层的工程变量为 X 或 X，这是输入特征 Z 或 Z 的函数。在本文中，我们假设截距为 0，因为在一般情况下截距不会缩小。为简单起见，我们将导出线性回归的分析结果并将结果推广到逻辑回归，<em class="ls">假设</em> <strong class="kq iu"> <em class="ls">真实模型在(工程)特征</em>的空间中是线性的 </strong> <em class="ls">。</em></p><h1 id="872b" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">重要</h1><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/e31440eb6d412c96a4ec8e401baa06a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TDxGpa85BwFyXYnz.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Collinearity problem in linear regression. Image source: <a class="ae lm" href="https://medium.com/@rrfd/what-is-ridge-regression-applications-in-python-6ed3acbb2aaf" rel="noopener">https://medium.com/@rrfd/what-is-ridge-regression-applications-in-python-6ed3acbb2aaf</a></figcaption></figure><p id="d37c" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">我有意以机器学习中正则化的重要性开始这篇文章。这一部分可能含义模糊；本文的其余部分将从头开始构建思路，试图理解这些模糊的陈述。</p><ul class=""><li id="3d2e" class="mj mk it kq b kr ln kv lo kz ml ld mm lh mn ll mo mp mq mr bi translated">沿着具有 h 个神经元的全连接隐藏层对深度神经网络进行切片，得到具有 h 个特征的更小的下游神经网络</li><li id="6b2b" class="mj mk it kq b kr ms kv mt kz mu ld mv lh mw ll mo mp mq mr bi translated">如果上面选择的隐藏层是输出层之前的层，则生成的神经网络相当于逻辑回归</li><li id="c1f0" class="mj mk it kq b kr ms kv mt kz mu ld mv lh mw ll mo mp mq mr bi translated">可以应用于线性/逻辑回归的简单线性代数可以扩展到深度神经网络，该深度神经网络在完全连接的隐藏层被切片</li></ul><h1 id="4fd0" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">线性回归</h1><h2 id="4f9b" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">模型定义</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/138ff7147a4cce709fd33c3c1f3b3fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:152/format:webp/1*s5E37OPzOkZ2c5FBdG83qw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">True model, unknown</figcaption></figure><h2 id="d56c" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">估计的</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/44e70f4c55c6d957e66dac0cd2a606e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*zhcCqRkyBln1WRoJgKh6xQ.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Estimated</figcaption></figure><h2 id="921c" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">损失函数</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/77add32ac26e15e406b3f220b36e0485.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*pe3wCm7PP9r4YG94yOuihw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Square loss in terms of matrix product</figcaption></figure><h2 id="8dd0" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">解决办法</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/35251f6b8d410544aa2a20479b7d5292.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*H4SrkMZFVeUMQSjTALB6MQ.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">ML / OLS estimate for coefficients</figcaption></figure><p id="4011" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">阅读:<a class="ae lm" href="https://medium.com/@snaveenmathew/equivalence-of-mle-and-ols-in-linear-regression-d3e44e47df3c" rel="noopener">线性回归中极大似然估计和 OLS 的等价性</a></p><h2 id="5707" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">解析</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/86428aacdfe75bbe428d3d01a19030bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*sLM37nlcSqSBtu5eqm9B2A.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Normal equation — MLE by analytical method</figcaption></figure><h1 id="7670" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">L2 正则化线性回归</h1><h2 id="b616" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">模型定义</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/138ff7147a4cce709fd33c3c1f3b3fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:152/format:webp/1*s5E37OPzOkZ2c5FBdG83qw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">True model, unknown</figcaption></figure><h2 id="d651" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">估计的</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e5c5619c2840fc02d7ecd5658f5bdb5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*Vwhav1PhEhS_I-IIMJ70mA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Estimated</figcaption></figure><h2 id="be17" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">损失函数</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c9aeca90b032941eb87fb1713c7ed5df.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*-24OWHx3vQqov_s4jD6USg.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Loss in terms of matrix product</figcaption></figure><h2 id="7f79" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">解决办法</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/29723085b31902044371a153d7d5acd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*9r4GW8T3kOt-Gbwt5DzboQ.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Solution</figcaption></figure><h2 id="94bd" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">解析</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/dde8f92903a7d005d9bd0ab30bb136ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*kWpmc0Dfljg3qmuBkV2sow.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Ridge estimate using analytical method</figcaption></figure><h1 id="ee99" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">理解差异</h1><p id="ece4" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">考虑一种设计矩阵不是满秩的情况(我上一篇文章中定义的几种情况:<a class="ae lm" href="https://medium.com/@snaveenmathew/a-short-note-on-regularization-42ee07c65d90" rel="noopener">正则化的简短说明</a>)。因此协方差矩阵是不可逆的。因此，MLE 不存在。</p><p id="03be" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">在这种情况下，考虑两种极端情况:λ = 0 和λ = ∞</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/7b30f45085b6a65178e3e3d6fbbd2af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*7CcOfUoTDDpXvZJmhYoeKw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Case 1</figcaption></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/147be526c4fb40a55bf07c3d42fe9a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:140/format:webp/1*_6p1JsjYWqBXmKQFiLWV9Q.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Case 2</figcaption></figure><p id="8231" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">在这两种极端情况之间，修正的协方差矩阵由下式给出</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d8d380ab458be254ac1a4a167d8d67ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:130/format:webp/1*mTxoS8QbiujiKAg197K2qA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Modified covariance matrix for ridge regresion</figcaption></figure><p id="a251" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">将随着λ的增加而成为对角优势。所以保证是可逆的。这证明了即使设计矩阵不是满秩的，非零λ的岭估计总是存在的(在<a class="ae lm" href="https://stats.stackexchange.com/questions/282654/non-singularity-due-to-inclusion-of-non-zero-lambda-in-ridge-regression" rel="noopener ugc nofollow" target="_blank">这篇</a> StackExchange 文章中提供了严格的证明)。</p><p id="1d7a" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">因此，我们得出结论，使用 L2 正则化解决了共线性问题。与岭回归不同，Lasso (L1 正则化)回归没有解析解。在存在共线性的情况下，预计其行为类似于岭回归。Lasso 回归也是随着λ(小于∞)的增大，将变量的系数收缩到 0 来进行剪枝，这在 ridge 中是观察不到的(lasso 对剪枝的严谨分析可以在我的<a class="ae lm" href="https://www.quora.com/Why-do-L1-regularizations-causes-parameter-sparsity-whereas-L2-regularization-does-not/answer/Naveen-Mathew" rel="noopener ugc nofollow" target="_blank"> Quora 答案</a>上找到)。为方便起见，L1 正则化线性回归公式如下所示:</p><h1 id="5dec" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">L1 正则化线性回归</h1><h2 id="e943" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">模型定义</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/138ff7147a4cce709fd33c3c1f3b3fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:152/format:webp/1*s5E37OPzOkZ2c5FBdG83qw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">True model, unknown</figcaption></figure><h2 id="a7c0" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">估计的</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi no"><img src="../Images/df8c9c87a0ea8de56c8bb6a9f879c038.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*tTzKfv2W9i5IFktlm-OxkQ.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Estimated</figcaption></figure><h2 id="0f54" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">损失函数</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/b45623c327c80adfa18fcdfe1d5405b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*sO2V-AMK7qbpMv26L896NA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Loss in terms of independent variables</figcaption></figure><h2 id="8950" class="mx jr it bd js my mz dn jw na nb dp ka kz nc nd ke ld ne nf ki lh ng nh km ni bi translated">解决办法</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/738a9f25dc7f168bbce5420d4267198a.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*s0rVK9enPI8BwKD0jDWlkg.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Solution</figcaption></figure><p id="6a7d" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">我们假设 lasso 的行为在协方差矩阵的可逆性方面类似于 ridge 的行为(严格的分析可以在<a class="ae lm" href="https://web.stanford.edu/~hastie/Papers/graph.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文</a> —第 5 页中找到，也解释了使用坐标下降的原因)。lasso 的解析解不存在，除了一个简单的情况-当协方差矩阵是对角矩阵时。</p><p id="3393" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated"><strong class="kq iu">对角协方差注意:</strong>参数估计变得类似于轮廓似然性——在一次坐标下降迭代中所选β的变化不会影响其他β。因此，坐标下降在一次迭代中收敛。</p><p id="928d" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">注意:我将在两篇独立的文章中用几何解释以更严格的方式讨论 L1 和 L2 正则化。</p><h1 id="2bca" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">拒绝传统社会的人</h1><p id="3a1a" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">辍学通常被视为调整神经网络的一种实用方式。很难以完全分析的方式处理辍学问题，因为:</p><ol class=""><li id="17c0" class="mj mk it kq b kr ln kv lo kz ml ld mm lh mn ll nx mp mq mr bi translated">它涉及到一些随机化—只有期望值是已知的，实际上各个实现会根据种子而变化</li><li id="a757" class="mj mk it kq b kr ms kv mt kz mu ld mv lh mw ll nx mp mq mr bi translated">对(随机)梯度下降的每个样品/小批量/批量进行测试</li></ol><p id="3457" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">该模型可以被视为:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/70277cafab660329a83c4dc0139fad73.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*JqLhzL8adSHm3xP-LXAIKg.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Masking variables in X at random; excluding intercept/bias</figcaption></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/0d0944b5ee2cae2f0f3545d012072f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*vO6dGmYh7hL26b__rJk3SA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Linear regression on M vs y</figcaption></figure><p id="e420" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">在实践中，为了避免权重之间的相关性，已经使用了丢弃。实际上，这是通过随机化掩码来实现的，从而减少变量的同现。理论上，当相应的预测值相关时，权重是相关的。因此，使用压差进行掩蔽有助于减少过拟合。</p><h1 id="64a3" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">把东西放在一起</h1><p id="0ddd" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们在输出层之前选择隐藏层。对于 h &lt;&lt; n (sample size) we observe that the problem of overfitting occurs when variables are collinear. L2 regularization explicitly removes the effect of collinearity by modifying the covariance matrix; L1 regularization affects the covariance matrix indirectly. Dropout affects the covariance between the weights by sampling from the set of features and masking the features that are not chosen (similar to random forest) during each update based on gradients.</p><h1 id="5ee8" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">Conclusion</h1><p id="a35c" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Linear models and deep neural networks are related through linear algebra. Over determined systems (number of predictors &gt;个样本)和共线系统(秩&lt; number of predictors) lead to unstable solutions and overfitting that can be resolved using regularization. The 3 most common forms of regularization — ridge, lasso and droupout — reduce overfitting by reducing the collinearity among predictors (or hidden layer in deep neural networks). But it is important to note that collinearity is not the only cause of overfitting. There are other forms of regularization that penalize the curvature in each dimension (check smoothing splines).</p><p id="2c56" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">A more rigorous analysis with geometric interpretation of ridge and lasso will be published in the future.</p><h1 id="197e" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">Further reading</h1><p id="7941" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Research paper: <a class="ae lm" href="https://arxiv.org/pdf/1511.06068.pdf" rel="noopener ugc nofollow" target="_blank">通过去相关表示减少深度神经网络中的过拟合</a></p><p id="48af" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">研究论文:<a class="ae lm" href="https://www.ijcai.org/proceedings/2018/0301.pdf" rel="noopener ugc nofollow" target="_blank">用基于集成的去相关方法正则化深度神经网络</a></p><p id="480a" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">相关研究论文:<a class="ae lm" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.7991&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">神经网络解释和对称破缺的权重集去相关训练算法</a></p><p id="a016" class="pw-post-body-paragraph ko kp it kq b kr ln kt ku kv lo kx ky kz lp lb lc ld lq lf lg lh lr lj lk ll im bi translated">相关研究论文:<a class="ae lm" href="https://link.springer.com/chapter/10.1007/978-3-319-11656-3_2" rel="noopener ugc nofollow" target="_blank">多层感知器网络剪枝的去相关方法</a></p></div></div>    
</body>
</html>