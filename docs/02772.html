<html>
<head>
<title>Graph Embedding for Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于深度学习的图嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4?source=collection_archive---------0-----------------------#2019-05-06">https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4?source=collection_archive---------0-----------------------#2019-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7410" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">图形学习和几何深度学习—第 1 部分</h2></div><blockquote class="kf kg kh"><p id="638b" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">一定要看一篇<a class="ae lf" rel="noopener" target="_blank" href="/what-is-geometric-deep-learning-b2adb662d91d"> <strong class="kl ir">几何深度学习概述</strong> </a> <strong class="kl ir"> </strong>和<strong class="kl ir"> </strong> <a class="ae lf" rel="noopener" target="_blank" href="/graph-theory-and-deep-learning-know-hows-6556b0e9891b"> <strong class="kl ir">先决条件</strong> </a> <strong class="kl ir"> </strong>熟悉机器学习这个小众领域。</p><p id="5560" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">关注<a class="ae lf" href="https://twitter.com/FlawnsonTong" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir">我的推特</strong> </a>加入<a class="ae lf" href="https://www.reddit.com/r/GeometricDeepLearning/" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir">几何深度学习子编辑</strong> </a>获取空间最新更新。</p></blockquote><p id="d305" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi lj translated">这里有很多方法可以将机器学习应用于图表。最简单的方法之一是将图表转换成更容易理解的 ML 格式。</p><p id="679c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">图形嵌入是一种方法，用于将节点、边及其特征转换到向量空间(较低的维度)<strong class="kl ir">中，同时最大限度地保留图形结构和信息等属性。图表很复杂，因为它们在规模、特性和主题方面会有所不同。</strong></p><p id="e632" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">一个分子可以被表示为一个小的、稀疏的、静态的图，而一个社会网络可以被表示为一个大的、密集的、动态的图。最终，这使得很难找到一个银弹嵌入方法。每种方法在不同的数据集上的性能各不相同，但它们是深度学习中使用最广泛的方法。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/9ed12904de1cd6d46883252bb59233ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*m7555FzS-X2sHcqT.jpg"/></div></div></figure><p id="b2a0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">图形是这个系列的主要焦点，但是如果 3D 成像应用更适合你，那么我推荐<a class="ae lf" href="https://thegradient.pub/beyond-the-pixel-plane-sensing-and-learning-in-3d/" rel="noopener ugc nofollow" target="_blank">Gradient</a>的这篇精彩文章。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="43dc" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">嵌入图网络</h1><p id="5a06" class="pw-post-body-paragraph ki kj iq kl b km nd jr ko kp ne ju kr lg nf ku kv lh ng ky kz li nh lc ld le ij bi translated">如果我们将嵌入视为向低维的转换，嵌入方法本身就不是一种神经网络模型。相反，它们是一种在<strong class="kl ir">图形预处理中使用的算法，目的是将图形转换成可计算的格式。这是因为图形类型的数据本质上是离散的。</strong></p><blockquote class="kf kg kh"><p id="094b" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kl ir">机器学习算法针对连续数据进行调整，因此为什么嵌入总是针对连续向量空间。</strong></p></blockquote><p id="04f9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">正如最近的工作所显示的，有多种方法可以嵌入图，每种方法都有不同的粒度级别。嵌入可以在节点级、子图级执行，或者通过像图遍历这样的策略来执行。这些是一些最受欢迎的方法。</p><h2 id="fa46" class="ni mm iq bd mn nj nk dn mr nl nm dp mv lg nn no mx lh np nq mz li nr ns nb nt bi translated"><a class="ae lf" href="https://arxiv.org/pdf/1403.6652.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">深走— </strong>佩罗齐等人</a></h2><p id="cd45" class="pw-post-body-paragraph ki kj iq kl b km nd jr ko kp ne ju kr lg nf ku kv lh ng ky kz li nh lc ld le ij bi translated">Deepwalk 并不是这类方法中的第一个，但与其他图形学习方法相比，它是第一个被广泛用作基准的方法之一。Deepwalk 属于使用遍历的图形嵌入技术家族，遍历是图论中的一个概念，它通过从一个节点移动到另一个节点来实现图形的<strong class="kl ir">遍历，只要它们连接到一个公共边。</strong></p><p id="f880" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">如果用任意的表示向量来表示图中的每个节点，就可以遍历图。可以通过在矩阵中彼此相邻地排列节点表示向量来聚集该遍历的步骤。然后，你可以将表示图形的矩阵输入到一个递归神经网络中。基本上，您可以使用图遍历的截断步长作为 RNN 的输入。这类似于句子中单词向量的组合方式。</p><p id="1634" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">DeepWalk 采用的方法是使用以下等式完成一系列随机行走:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/527eab058d2dcd257050ff4451eacb68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*ptbEoweo9yn7_R_j3O4GQA.png"/></div></figure><p id="1ba3" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">目标是估计观察到节点<strong class="kl ir"> <em class="kk"> vi </em> </strong>的可能性，给定在随机行走中到目前为止访问的所有先前节点，其中<strong class="kl ir"> <em class="kk"> Pr() </em> </strong>是概率，φ是表示与图中每个节点<strong class="kl ir"> v </strong>相关联的潜在表示的映射函数。</p><p id="a169" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">潜在的表现是神经网络的输入。神经网络基于行走过程中遇到哪些节点以及遇到这些节点的频率，可以对节点特征或分类进行预测。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nv"><img src="../Images/ea09e932f82795c59918b7a457845cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CAkJLYcq1ilhdDn7tAZVYg.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">The original graph and it’s embedding (Courtesy of the DeepWalk research team)</figcaption></figure><p id="ef5d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">用于进行预测的方法是<strong class="kl ir"> skip-gram </strong>，就像 Word2vec 架构中的文本一样。DeepWalk 不是沿着文本语料库运行，而是沿着图运行来学习嵌入。该模型可以采用目标节点来预测它的“上下文”，在图的情况下，这意味着它的连通性、结构角色和节点特征。</p><p id="95e3" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">尽管 DeepWalk 的得分为<strong class="kl ir"> O(|V|) </strong> <em class="kk">，</em>相对高效，但这种方法是<strong class="kl ir">直推式</strong>，这意味着每当添加新节点时，模型都必须重新训练以嵌入新节点并从中学习。</p><h2 id="a223" class="ni mm iq bd mn nj nk dn mr nl nm dp mv lg nn no mx lh np nq mz li nr ns nb nt bi translated"><a class="ae lf" href="https://cs.stanford.edu/people/jure/pubs/node2vec-kdd16.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> Node2vec — Grover 等人</strong> </a></h2><p id="d28b" class="pw-post-body-paragraph ki kj iq kl b km nd jr ko kp ne ju kr lg nf ku kv lh ng ky kz li nh lc ld le ij bi translated">你听说过 Word2vec 现在准备… Node2vec</p><p id="b905" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">比较流行的图学习方法之一，Node2vec 是最早尝试从图结构化数据进行学习的深度学习方法之一。直觉类似于 DeepWalk 的直觉:</p><blockquote class="kf kg kh"><p id="6982" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">如果你把图中的每个节点像句子中的单词一样嵌入，神经网络可以学习每个节点的表示。</p></blockquote><p id="8b80" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">Node2vec 和 DeepWalk 之间的区别很微妙，但是很重要。Node2vec 具有一个行走偏差变量α，它由<em class="kk"> p </em>和<em class="kk"> q </em>参数化。参数<em class="kk"> p </em>优先考虑广度优先搜索(BFS)过程，而参数<em class="kk"> q </em>优先考虑深度优先搜索(DFS)过程。因此，下一步去哪里的决定受到概率 1/ <em class="kk"> p </em>或 1/<em class="kk">q</em>的影响</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/13ae933ea95e6d12ec5addf742bcf4ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/0*lFvvIeJW2rFH_xsV.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Both BFS and DFS are common algorithms in CS and graph theory (Courtesy of Semantic Scholar)</figcaption></figure><p id="8020" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">正如可视化所暗示的，<strong class="kl ir"> BFS 是学习局部邻居的理想选择，而 DFS 更适合学习全局变量。</strong> Node2vec 可以根据任务在两个优先级之间切换。这意味着给定一个图，Node2vec 可以根据参数值返回不同的结果。按照 DeepWalk，Node2vec 还获取 walks 的潜在嵌入，并将其作为神经网络的输入来对节点进行分类。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi od"><img src="../Images/7699dcc56a39f3c602100e334f99124d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*XoLCbRgeyDyxORWkyTLqkQ.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">BFS vs DFS (Courtesy of SNAP Stanford)</figcaption></figure><p id="b600" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">实验证明<strong class="kl ir"> BFS 更擅长根据结构角色(枢纽、桥梁、离群点等)进行分类。)而 DFS 返回一个更加社区驱动的分类方案。</strong></p><p id="022e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">Node2vec 是斯坦福大学 SNAP 研究小组致力于图形分析的众多图形学习项目之一。他们的许多作品是几何深度学习许多重大进展的起源。</p><h2 id="3da5" class="ni mm iq bd mn nj nk dn mr nl nm dp mv lg nn no mx lh np nq mz li nr ns nb nt bi translated"><a class="ae lf" href="https://arxiv.org/abs/1707.05005" rel="noopener ugc nofollow" target="_blank">graph 2 vec——纳拉亚南等人</a></h2><p id="e8d9" class="pw-post-body-paragraph ki kj iq kl b km nd jr ko kp ne ju kr lg nf ku kv lh ng ky kz li nh lc ld le ij bi translated">对 node2vec 变体的修改，graph2vec 本质上学习嵌入图的子图。doc2vec 中使用的一个等式证明了这一点，它是一个密切相关的变体，也是本文的灵感来源。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/53358e2d34818abb72de49b57bc16d23.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*lxfh3lCWg-Zdeq-B94KMsA.png"/></div></figure><p id="8624" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">用通俗的英语来说，这个等式可以写成:单词(<strong class="kl ir"> wj </strong>)出现在上下文给定文档(<strong class="kl ir"> d </strong>)中的概率等于文档嵌入矩阵(<strong class="kl ir"> <em class="kk"> d~ </em> </strong>)的指数乘以单词嵌入矩阵(<strong class="kl ir"> <em class="kk"> w~j </em> </strong>是从文档中采样的)，除以文档嵌入矩阵的所有指数之和乘以文档中每个单词的单词嵌入矩阵</p><p id="b74e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">用 word2vec 来类比，<strong class="kl ir">如果一个文档是由句子组成的(句子又是由单词组成的)，那么一个图是由子图组成的(子图又是由节点组成的)。</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi of"><img src="../Images/097654e1b593c497f781cf5a581dce5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RNA_0pXJJXB723tpMEsfKg.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Everything is made of smaller things</figcaption></figure><p id="92e4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">这些预定的子图具有由用户指定的设定数量的边。同样，是潜在的子图嵌入被传递到神经网络中用于分类。</p><h2 id="6593" class="ni mm iq bd mn nj nk dn mr nl nm dp mv lg nn no mx lh np nq mz li nr ns nb nt bi translated"><strong class="ak">【结构化深度网络嵌入】——王等</strong></h2><p id="a443" class="pw-post-body-paragraph ki kj iq kl b km nd jr ko kp ne ju kr lg nf ku kv lh ng ky kz li nh lc ld le ij bi translated">与以前的嵌入技术不同，SDNE 不使用随机行走。相反，它试图从两个不同的指标中学习:</p><ul class=""><li id="eaa0" class="og oh iq kl b km kn kp kq lg oi lh oj li ok le ol om on oo bi translated"><strong class="kl ir">一阶接近度:</strong>如果两个节点共享一条边，则认为它们是相似的(成对相似)</li><li id="aab6" class="og oh iq kl b km op kp oq lg or lh os li ot le ol om on oo bi translated"><strong class="kl ir">二阶接近度:</strong>如果两个节点共享许多相邻节点，则认为它们是相似的</li></ul><p id="ebc9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">最终目标是捕捉高度非线性的结构。这是通过使用<strong class="kl ir">深度自动编码器(半监督)来保持一阶(监督)和二阶(非监督)网络接近度来实现的。</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ou"><img src="../Images/57e37176309b5c3898830acee8406d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1vd-LSDa-GgGrfNF.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Dimensionality reduction with LE (Courtesy of MathWorld)</figcaption></figure><p id="71e6" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">为了保持一阶近似性，该模型也是<strong class="kl ir">拉普拉斯特征映射</strong>的变体，这是一种图形嵌入/维度缩减技术。拉普拉斯特征映射嵌入算法<strong class="kl ir">在相似节点在嵌入空间</strong>中彼此远离映射时应用惩罚，从而允许通过最小化相似节点之间的空间来进行优化。</p><p id="1240" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">通过将 te 图的邻接矩阵传递给<strong class="kl ir">无监督自动编码器来保持二阶接近度，该编码器具有内置的重建损失函数，必须最小化该函数。</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="ab gu cl ov"><img src="../Images/9546401d38d95501c7f208cf1f026b2e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">An autoencoder (Courtesy of Arden Dertat)</figcaption></figure><p id="80fe" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">一阶邻近损失函数和二阶重建损失函数一起被联合最小化以返回图嵌入。然后通过神经网络学习嵌入。</p><h2 id="0856" class="ni mm iq bd mn nj nk dn mr nl nm dp mv lg nn no mx lh np nq mz li nr ns nb nt bi translated"><a class="ae lf" href="https://arxiv.org/abs/1503.03578" rel="noopener ugc nofollow" target="_blank">大规模信息网络嵌入(线)——唐等</a></h2><p id="1d73" class="pw-post-body-paragraph ki kj iq kl b km nd jr ko kp ne ju kr lg nf ku kv lh ng ky kz li nh lc ld le ij bi translated">行(<a class="oa ob ep" href="https://medium.com/u/370a562c29b0?source=post_page-----4305c10ad4a4--------------------------------" rel="noopener" target="_blank">汤集安</a>等人)明确定义了两个功能；一个用于<strong class="kl ir">一阶接近度</strong>，另一个用于<strong class="kl ir">二阶接近度。</strong>在原始研究进行的实验中，二阶近似的表现明显优于一阶，这意味着包含更高阶的近似可能会抵消精度的提高。</p><p id="718d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><strong class="kl ir">LINE 的目标是最小化输入和嵌入分布之间的差异。</strong>这是通过使用 KL 散度实现的:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ow"><img src="../Images/d39f37e0f55ae831db9c480788e5d812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O-MXQk5ztaOYsGQH"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">A simple case of KL-divergence minimization</figcaption></figure><p id="1c78" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">可视化很简单，数学没那么简单。Aurélien Géron 有一个关于这个主题的很棒的<a class="ae lf" href="https://www.youtube.com/watch?v=ErfnhcEV1O8" rel="noopener ugc nofollow" target="_blank">视频。</a>另一方面，Géron 也是为数不多的图形学习研究人员之一，在 YouTube 工作期间，他将知识图形和深度学习结合起来，以改善视频推荐。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ox"><img src="../Images/cffa0de9e86e65460ca8f75eeb193f98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UhcvOUeSRFGs8tp6.gif"/></div></div></figure><p id="3f74" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><strong class="kl ir"> LINE 为每对节点定义两个联合概率分布，然后最小化分布的 KL 散度。</strong>这两种分布分别是邻接矩阵和节点嵌入的点积。KL 散度是信息论和熵中一个重要的相似性度量。该算法用在概率生成模型中，如变分自动编码器，其将自动编码器的输入嵌入到潜在空间中，该潜在空间成为分布。</p><p id="1b97" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">由于该算法必须为每个递增的接近度定义新的函数，如果应用程序需要理解节点社区结构，<strong class="kl ir"> LINE 的性能不是很好。</strong></p><p id="bbfb" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">然而，LINE 的简单性和有效性只是它成为 2015 年 WWW 上被引用最多的论文的几个原因。这项工作有助于激发人们对图形学习的兴趣，将其作为机器学习的一个利基，并最终成为特定的深度学习。</p><h2 id="fbec" class="ni mm iq bd mn nj nk dn mr nl nm dp mv lg nn no mx lh np nq mz li nr ns nb nt bi translated"><a class="ae lf" href="https://arxiv.org/abs/1706.07845" rel="noopener ugc nofollow" target="_blank">网络的分层表示学习—陈等</a></h2><p id="621c" class="pw-post-body-paragraph ki kj iq kl b km nd jr ko kp ne ju kr lg nf ku kv lh ng ky kz li nh lc ld le ij bi translated">HARP 是对前面提到的基于嵌入/行走的模型的改进。以前的模型有陷入局部最优的风险，因为它们的目标函数是非凸的。基本上这意味着，球不可能滚到绝对的山脚。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/5d8f1a7767b801e465af9713b1f42636.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/0*IoSNj7SCD2rZn95c"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Gradient decent isn’t perfect (Courtesy of Fatih Akturk)</figcaption></figure><p id="e1ad" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">因此，意图<strong class="kl ir">动机</strong>:</p><blockquote class="kf kg kh"><p id="b9e5" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">通过更好的权重初始化来改进解决方案并避免局部最优。</p></blockquote><p id="6a5c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">以及建议的<strong class="kl ir">方法</strong>:</p><blockquote class="kf kg kh"><p id="edbf" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">使用图粗化将相关节点聚集成“超级节点”</p></blockquote><p id="cae7" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">被制造出来。</p><p id="a2ca" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">HARP 本质上是一个<strong class="kl ir">图形预处理步骤，它简化了图形</strong>以利于更快的训练。</p><blockquote class="kf kg kh"><p id="dd5f" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kl ir">粗化图形后，它生成最粗“超节点”的嵌入，随后是整个图形的嵌入(图形本身由超节点组成)。</strong></p></blockquote><p id="b3bf" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">整个图中的每个“超级节点”都遵循这一策略。</p><p id="7e11" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">因为 HARP 可以<strong class="kl ir">与之前的嵌入算法</strong>结合使用，比如 LINE、Node2vec 和 DeepWalk。原始论文报道了当将 HARP 与各种图嵌入方法结合时，在分类任务中高达<strong class="kl ir"> <em class="kk"> 14% </em> </strong>的显著改进:显著的飞跃。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="7b49" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">本质上</h1><p id="d39d" class="pw-post-body-paragraph ki kj iq kl b km nd jr ko kp ne ju kr lg nf ku kv lh ng ky kz li nh lc ld le ij bi translated">我肯定错过了一堆算法和模型，尤其是最近对几何深度学习和图形学习的兴趣激增，导致几乎每天都有新的贡献出现在出版物上。</p><p id="8246" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">在任何情况下，图嵌入方法都是一种简单但非常有效的方法，可以将图转换为机器学习任务的最佳格式。由于它们的简单性，它们通常非常具有可伸缩性(至少与它们的卷积对应物相比)，并且易于实现。它们可以应用于大多数网络和图形，而不会牺牲性能或效率。<strong class="kl ir">但是我们能做得更好吗？</strong></p><p id="3a7d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">接下来是深入复杂而优雅的<strong class="kl ir">图形卷积</strong>世界！</p><h1 id="1db9" class="ml mm iq bd mn mo oz mq mr ms pa mu mv jw pb jx mx jz pc ka mz kc pd kd nb nc bi translated">关键要点</h1><ul class=""><li id="5780" class="og oh iq kl b km nd kp ne lg pe lh pf li pg le ol om on oo bi translated">图形嵌入技术<strong class="kl ir">在通过机器学习模型传递该表示之前，获取图形并将它们嵌入到较低维度的连续潜在空间</strong>。</li><li id="7e52" class="og oh iq kl b km op kp oq lg or lh os li ot le ol om on oo bi translated">行走嵌入方法<strong class="kl ir">执行图形遍历，目标是保持结构和特征</strong>并聚集这些遍历，然后可以通过递归神经网络传递。</li><li id="cb8b" class="og oh iq kl b km op kp oq lg or lh os li ot le ol om on oo bi translated">邻近嵌入方法使用<strong class="kl ir">深度学习方法和/或邻近损失函数来优化邻近</strong>，使得原始图中靠近在一起的节点同样在嵌入中。</li><li id="4193" class="og oh iq kl b km op kp oq lg or lh os li ot le ol om on oo bi translated">其他方法使用类似<strong class="kl ir">图粗化的方法来简化图，然后在图</strong>上应用嵌入技术，在保留结构和信息的同时降低复杂性。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="f405" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">需要看到更多这样的内容？</h1><p id="98f3" class="pw-post-body-paragraph ki kj iq kl b km nd jr ko kp ne ju kr lg nf ku kv lh ng ky kz li nh lc ld le ij bi translated"><em class="kk">跟我上</em><a class="ae lf" href="http://www.linkedin.com/in/flawnson" rel="noopener ugc nofollow" target="_blank"><strong class="kl ir"><em class="kk">LinkedIn</em></strong></a><strong class="kl ir"><em class="kk"/></strong><a class="ae lf" href="https://www.facebook.com/flawnson" rel="noopener ugc nofollow" target="_blank"><strong class="kl ir"><em class="kk">脸书</em></strong></a><strong class="kl ir"><em class="kk"/></strong><a class="ae lf" href="https://www.instagram.com/flaws.non/?hl=en" rel="noopener ugc nofollow" target="_blank"><strong class="kl ir"><em class="kk">insta gram</em></strong></a><em class="kk">，当然还有</em> <a class="ae lf" href="https://medium.com/@flawnsontong1" rel="noopener"> <strong class="kl ir"> <em class="kk">中</em></strong></a></p><p id="52d7" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><em class="kk">我所有的内容都在</em> <a class="ae lf" href="http://www.flawnson.com" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir"> <em class="kk">我的网站</em> </strong> </a> <em class="kk">我所有的项目都在</em><a class="ae lf" href="https://github.com/flawnson" rel="noopener ugc nofollow" target="_blank"><strong class="kl ir"><em class="kk">GitHub</em></strong></a></p><p id="3554" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><em class="kk">我总是希望结识新朋友、合作或学习新东西，所以请随时联系 flawnsontong1@gmail.com</em><a class="ae lf" href="http://mail.google.com" rel="noopener ugc nofollow" target="_blank"><strong class="kl ir"><em class="kk"/></strong></a></p><blockquote class="ph"><p id="8c53" class="pi pj iq bd pk pl pm pn po pp pq le dk translated">向上和向前，永远和唯一🚀</p></blockquote></div></div>    
</body>
</html>