<html>
<head>
<title>Ensemble Learning case study: Model Interpretability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习案例研究:模型可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ensemble-learning-and-model-interpretability-a-case-study-95141d75a96c?source=collection_archive---------23-----------------------#2019-10-15">https://towardsdatascience.com/ensemble-learning-and-model-interpretability-a-case-study-95141d75a96c?source=collection_archive---------23-----------------------#2019-10-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="2e70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一篇由两部分组成的文章的第一部分，在这篇文章中，我们将探索 1994 年人口普查收入数据集 ，它包含诸如<strong class="js iu">年龄</strong>、<strong class="js iu">受教育年限</strong>、<strong class="js iu">婚姻状况</strong>、<strong class="js iu">种族、</strong>以及许多其他信息。我们将使用该数据集将人们的潜在收入分为两类:年收入低于或等于 5 万美元的人(编码为 0)和年收入高于 5 万美元的人(编码为 1)。</p><p id="a824" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在第一部分中，我们将使用这个数据集来比较简单决策树和集成方法的性能。稍后，我们还将探索一些工具来帮助我们解释为什么模型遵循<strong class="js iu">可解释的人工智能(XAI) </strong>的一些原则来做出决策。</p><p id="5b06" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要做的第一件事是看一看选择的数据集，以便更好地了解它。所以，让我们开始吧！</p><h1 id="b8c0" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">1.准备数据</h1><p id="6d6f" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我们将使用没有空值的数据集的预处理版本。首先，我们加载基本库和数据集本身，并查看数据帧信息:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="2c56" class="mb kq it lx b gy mc md l me mf">import   as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>plt.style.use('ggplot')</span><span id="c479" class="mb kq it lx b gy mg md l me mf"># load the dataset<br/>income = pd.read_csv("income.csv")<br/>income.info()</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/bca314353895aa08d64dc7546f47ba6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*w-SIXKAf8jfOp-47cBp0xQ.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 1: </strong>DataFrame information</figcaption></figure><p id="66fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如我们所见，数据集有 32561 个观察值和 15 列，其中 14 列是特征，一列是目标变量(<strong class="js iu"> high_income </strong>)。有些特征是分类的(类型<strong class="js iu">对象</strong>)有些是数值的(类型<strong class="js iu"> int64 </strong>)，所以我们需要为它们执行不同的预处理步骤。</p><p id="3522" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了实现这一点，我们将创建两个<strong class="js iu">管道</strong>:一个对分类特征执行预处理步骤，另一个对数字特征执行预处理步骤。然后，我们将使用<strong class="js iu"> FeatureUnion </strong>将这两条管道连接在一起，形成最终的预处理管道。为此以及本文中的后续步骤，我们需要从 scikit-learn 库中导入必要的模块:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="443c" class="mb kq it lx b gy mc md l me mf">from sklearn.base import BaseEstimator<br/>from sklearn.base import TransformerMixin</span><span id="3cbb" class="mb kq it lx b gy mg md l me mf">from sklearn.preprocessing import StandardScaler<br/>from sklearn.preprocessing import FunctionTransformer</span><span id="04f3" class="mb kq it lx b gy mg md l me mf">from sklearn.pipeline import Pipeline<br/>from sklearn.pipeline import FeatureUnion</span><span id="85cd" class="mb kq it lx b gy mg md l me mf">from sklearn.model_selection import train_test_split<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.model_selection import KFold<br/>from sklearn.model_selection import StratifiedKFold<br/>from sklearn.model_selection import RandomizedSearchCV</span><span id="0af0" class="mb kq it lx b gy mg md l me mf">from sklearn.metrics import make_scorer<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix</span><span id="16ae" class="mb kq it lx b gy mg md l me mf">from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.ensemble import BaggingClassifier</span></pre><p id="22fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用<strong class="js iu"> BaseEstimator </strong>和<strong class="js iu">transformer mixin</strong>类，我们可以创建两个定制的转换程序放在我们的管道上:一个将数据分成分类和数字特征，另一个预处理分类特征。这两种变压器如下所示:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="3e71" class="mb kq it lx b gy mc md l me mf"># Custom Transformer that extracts columns passed as argument<br/>class FeatureSelector(BaseEstimator, TransformerMixin):<br/>    #Class Constructor <br/>    def __init__(self, feature_names):<br/>        self.feature_names = feature_names</span><span id="73eb" class="mb kq it lx b gy mg md l me mf">    #Return self nothing else to do here    <br/>    def fit(self, X, y = None):<br/>        return self</span><span id="25db" class="mb kq it lx b gy mg md l me mf">    #Method that describes what we need this transformer to do<br/>    def transform(self, X, y = None):<br/>        return X[self.feature_names]</span><span id="7e25" class="mb kq it lx b gy mg md l me mf"># converts certain features to categorical<br/>class CategoricalTransformer( BaseEstimator, TransformerMixin ):<br/>    #Class constructor method that takes a boolean as its argument<br/>    def __init__(self, new_features=True):<br/>        self.new_features = new_features</span><span id="e2ab" class="mb kq it lx b gy mg md l me mf">    #Return self nothing else to do here    <br/>    def fit( self, X, y = None ):<br/>        return self</span><span id="195a" class="mb kq it lx b gy mg md l me mf">    #Transformer method we wrote for this transformer <br/>    def transform(self, X , y = None ):<br/>        df = X.copy()<br/>        if self.new_features:<br/>            # Treat ? workclass as unknown<br/>            df['workclass']= df['workclass'].replace('?','Unknown') <br/>            # Two many category level, convert just US and Non-US<br/>            df.loc[df['native_country']!=' United-States','native_country'] = 'non_usa'<br/>            df.loc[df['native_country']==' United-States','native_country'] = 'usa'</span><span id="0deb" class="mb kq it lx b gy mg md l me mf">        # convert columns to categorical<br/>        for name in df.columns.to_list():<br/>            col = pd.Categorical(df[name])<br/>            df[name] = col.codes</span><span id="b54b" class="mb kq it lx b gy mg md l me mf">        # returns numpy array<br/>        return df</span></pre><p id="f436" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有了这些定制的转换器，我们就可以构建预处理管道。我们需要做的第一件事是创建<strong class="js iu"> X </strong>特征矩阵和<strong class="js iu"> y </strong>目标向量:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="bb13" class="mb kq it lx b gy mc md l me mf"># Create the X feature matrix and the y target vector<br/>X = income.drop(labels="high_income", axis=1)<br/>y = income["high_income"]</span><span id="b69d" class="mb kq it lx b gy mg md l me mf"># the only step necessary to be done outside of pipeline<br/># convert the target column to categorical<br/>col = pd.Categorical(y)<br/>y = pd.Series(col.codes)</span><span id="0578" class="mb kq it lx b gy mg md l me mf"># global variables<br/>seed = 108</span></pre><p id="40a4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">之后，我们提取分类和数字特征名称，并在定义了要在每个特征中使用的步骤之后创建 2 个管道。对于分类管道，我们使用<strong class="js iu"> FeatureSelector </strong>只选择分类列，然后使用<strong class="js iu"> CategoricalTransformer </strong>将数据转换成所需的格式；至于数字管道，我们也将使用<strong class="js iu"> FestureSelector，</strong>这次只选择数字特征，随后使用 as <strong class="js iu"> StandardScaler </strong>来标准化数据。代码如下所示:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="742a" class="mb kq it lx b gy mc md l me mf"># get the categorical feature names<br/>categorical_features = X.select_dtypes("object").columns.to_list()<br/># get the numerical feature names<br/>numerical_features = X.select_dtypes("int64").columns.to_list()</span><span id="9b52" class="mb kq it lx b gy mg md l me mf"># create the steps for the categorical pipeline<br/>categorical_steps = [<br/>    ('cat_selector', FeatureSelector(categorical_features)),<br/>    ('cat_transformer', CategoricalTransformer())<br/>]</span><span id="eaab" class="mb kq it lx b gy mg md l me mf"># create the steps for the numerical pipeline<br/>numerical_steps = [<br/>    ('num_selector', FeatureSelector(numerical_features)),<br/>    ('std_scaler', StandardScaler()),<br/>]</span><span id="a46c" class="mb kq it lx b gy mg md l me mf"># create the 2 pipelines with the respective steps<br/>categorical_pipeline = Pipeline(categorical_steps)<br/>numerical_pipeline = Pipeline(numerical_steps)</span></pre><p id="17ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以使用<strong class="js iu"> FeatureUnion </strong>类来水平连接这两个管道，这样我们最终只有一个最终预处理管道:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="98a5" class="mb kq it lx b gy mc md l me mf">pipeline_list = [<br/>    ('categorical_pipeline', categorical_pipeline),<br/>    ('numerical_pipeline', numerical_pipeline)<br/>]</span><span id="adf1" class="mb kq it lx b gy mg md l me mf"># Combining the 2 pieplines horizontally into one full pipeline <br/>preprocessing_pipeline =FeatureUnion(transformer_list=pipeline_list)</span></pre><p id="b285" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就这样，现在你所要做的就是对数据执行所有预处理步骤，调用<strong class="js iu">预处理 _ 管道</strong>对象的<strong class="js iu"> fit_transform </strong>方法，将<strong class="js iu"> X </strong>矩阵作为参数传递！简洁明了。</p><h1 id="2180" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">2.训练第一个模型</h1><p id="c292" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">在训练任何机器学习模型之前，我们必须将数据分成<strong class="js iu">训练</strong>和<strong class="js iu">测试</strong>组。为此，我们使用<strong class="js iu">训练 _ 测试 _ 分割</strong>功能:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="6217" class="mb kq it lx b gy mc md l me mf"># split-out train/validation and test dataset<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed, shuffle=True, stratify=y)</span></pre><p id="aab0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用的第一个模型是一个简单的<strong class="js iu">决策树分类器</strong>。为了充分利用<strong class="js iu">管道的全部功能，</strong>我们可以创建一个完整的管道，第一步通过预处理管道，第二步通过所需的分类模型:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="ebd2" class="mb kq it lx b gy mc md l me mf"># we pass the preprocessing pipeline as a step to the full pipeline<br/>full_pipeline_steps = [<br/>    ('preprocessing_pipeline', preprocessing_pipeline),<br/>    ('model', DecisionTreeClassifier(random_state=seed))<br/>]</span><span id="53c6" class="mb kq it lx b gy mg md l me mf"># create the full pipeline object<br/>full_pipeline = Pipeline(steps=full_pipeline_steps)</span></pre><p id="04de" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过这种方式，如果我们想要尝试不同的模型(正如我们稍后将要做的)，我们所要做的就是更新这个管道的<strong class="js iu">‘model’</strong>步骤！</p><p id="91c0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下一步是使用<strong class="js iu"> RandomizedSearchCV </strong>执行模型超参数调整。随机搜索不如常规网格搜索彻底，因为它不会测试超参数的每个可能组合。另一方面，它的计算成本更低，使我们能够在低端硬件中实现<strong class="js iu">“足够好”</strong>的结果，同时调整多个超参数。使用<strong class="js iu"> n_iter </strong>参数，我们将迭代次数限制为<strong class="js iu"> 50。</strong></p><p id="a190" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们还将使用<strong class="js iu">stratified fold</strong>来执行交叉验证。与常规的<strong class="js iu"> KFold </strong>不同，它保留了样品在褶皱间的分布，可能会产生更好的结果。</p><p id="dea4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后一步是用我们需要调优的超参数构建一个<strong class="js iu"> param_grid </strong>字典。完整的代码可以在下面看到。注意我们是如何将<strong class="js iu"> full_pipeline </strong>对象作为<strong class="js iu"> RandomizedSearchCV </strong>估计器传递的，然后我们在结果对象上调用<strong class="js iu"> fit </strong>方法，就像我们调用任何其他 sklearn 模型一样。这样，当我们想要测试其他模型时，我们所要做的就是改变管道上的模型，并创建一个新的参数网格来传递，就这么简单！</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="b545" class="mb kq it lx b gy mc md l me mf"># Create the grid search parameter grid and scoring funcitons<br/>param_grid = {<br/>    "model": [DecisionTreeClassifier(random_state=seed)],<br/>    "model__criterion": ["gini","entropy"],<br/>    "model__splitter": ["best","random"],<br/>    "model__max_leaf_nodes": [16, 64, 128, 256],<br/>    "model__max_depth": np.linspace(1, 32, 32)<br/>}</span><span id="4d1a" class="mb kq it lx b gy mg md l me mf">scoring = {<br/>    'AUC': 'roc_auc', <br/>    'Accuracy': make_scorer(accuracy_score)<br/>}</span><span id="3855" class="mb kq it lx b gy mg md l me mf"># create the Kfold object<br/>num_folds = 10<br/>kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)</span><span id="486e" class="mb kq it lx b gy mg md l me mf"># create the grid search object with the full pipeline as estimator<br/>n_iter=50</span><span id="2b9a" class="mb kq it lx b gy mg md l me mf">grid = RandomizedSearchCV(<br/>    estimator=full_pipeline, <br/>    param_distributions=param_grid,<br/>    cv=kfold,<br/>    scoring=scoring,<br/>    n_jobs=-1,<br/>    n_iter=n_iter,<br/>    refit="AUC"<br/>)</span><span id="b9d3" class="mb kq it lx b gy mg md l me mf"># fit grid search<br/>best_model = grid.fit(X_train,y_train)</span></pre><p id="859a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于<strong class="js iu">决策树分类器</strong>，我们正在调整以下参数:</p><ul class=""><li id="2f94" class="mq mr it js b jt ju jx jy kb ms kf mt kj mu kn mv mw mx my bi translated"><strong class="js iu">标准:</strong>定义了度量树节点上分割质量的函数；</li><li id="d6cd" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu">拆分器:</strong>定义在每个树节点选择拆分的策略；</li><li id="6490" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu"> max_leaf_nodes: </strong>限制树中叶子节点的最大数量；</li><li id="3c44" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu"> max_depth: </strong>限制树可以生长的最大深度；</li></ul><p id="0d23" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随机搜索完成且最佳模型符合我们的数据后，我们可以进行预测并衡量模型的性能:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="295e" class="mb kq it lx b gy mc md l me mf"># final Decision Tree model<br/>pred_test = best_model.predict(X_test)<br/>pred_train = best_model.predict(X_train)</span><span id="69bc" class="mb kq it lx b gy mg md l me mf">print('Train Accuracy: ', accuracy_score(y_train, pred_train))<br/>print('Test Accuraccy: ', accuracy_score(y_test, pred_test))</span><span id="5fa0" class="mb kq it lx b gy mg md l me mf">print('\nConfusion Matrix:')<br/>print(confusion_matrix(y_test,pred_test))<br/>print('\nClassification Report:')<br/>print(classification_report(y_test,pred_test))</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f9271be238c815b9a9ff1fcebd58cbe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*UnrDzTkc3NpCYz1fc_QX_g.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 2:</strong> Decision tree performance</figcaption></figure><h1 id="e6fe" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">3.集成方法</h1><p id="8fd8" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">集成方法是将几个基本模型结合起来以产生一个最佳预测模型的模型。RandomForestClassifier<strong class="js iu">是一个典型的例子，因为它结合了几个更简单的<strong class="js iu">决策树</strong>来生成输出。这样，就有可能克服决策树模型的几个局限性，比如它的过度拟合倾向。</strong></p><h2 id="6bb4" class="mb kq it bd kr nf ng dn kv nh ni dp kz kb nj nk ld kf nl nm lh kj nn no ll np bi translated">3.1.装袋分级机</h2><p id="c8ae" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我们要使用的第一个集成方法是<strong class="js iu"> BaggingClassifier </strong>。名称<em class="nq"> bagging </em>来自<em class="nq"> bootstrap aggregating </em>，它的工作原理是将数据分成几个随机的<em class="nq">子样本</em> <strong class="js iu"> </strong>，然后用于训练每个独立的基本估计器。该策略可以以两种方式之一执行:<strong class="js iu">替换</strong>，这意味着样本可以多次用于同一个估计器；<strong class="js iu">无需更换</strong>，意味着每个样品只能使用一次(这种方法称为<strong class="js iu">粘贴</strong>)。</p><p id="dd5d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">装袋通常比粘贴产生更好的结果，它还有一个巧妙的锦囊妙计:<strong class="js iu">袋外</strong> <strong class="js iu">评估。</strong>由于样本是替换抽取的，并且同一样本可以随机使用多次，因此训练集上的一些样本可能永远不会用于训练任何基础估计量！这意味着我们可以使用这些样本进行进一步的模型评估！</p><p id="0d52" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，考虑到所有这些，我们将更新我们之前的管道，以使用<strong class="js iu"> BaggingClassifier </strong>和<strong class="js iu">决策树</strong>作为基本估计器。为此，我们所要做的就是更改管道定义中的<strong class="js iu">‘模型’</strong>步骤，并重新定义<strong class="js iu"> RandomizedSearchCV </strong>参数搜索空间，如下面的代码片段所示:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="ccc7" class="mb kq it lx b gy mc md l me mf"># we pass the preprocessing pipeline as a step to the full pipeline<br/>full_pipeline_steps = [<br/>    ('preprocessing_pipeline', preprocessing_pipeline),<br/>    ('model', BaggingClassifier(<br/>        DecisionTreeClassifier(max_features="auto", splitter="random", max_leaf_nodes=128, random_state=seed),<br/>        random_state=seed<br/>    ))<br/>]</span><span id="0c55" class="mb kq it lx b gy mg md l me mf"># create the full pipeline object<br/>full_pipeline = Pipeline(steps=full_pipeline_steps)</span><span id="9981" class="mb kq it lx b gy mg md l me mf"># Create the grid search parameter grid<br/>param_grid = {<br/>    "model": [BaggingClassifier(<br/>        DecisionTreeClassifier(max_features="auto", splitter="random", max_leaf_nodes=128, random_state=seed),<br/>        random_state=seed<br/>    )],<br/>    "model__n_estimators":  np.arange(100, 1000, 100),<br/>    "model__max_samples":[0.8, 1.0], <br/>    "model__max_features": [0.8, 1.0],<br/>    "model__bootstrap": [True],<br/>    "model__oob_score": [True],<br/>}</span><span id="1b8f" class="mb kq it lx b gy mg md l me mf">scoring = {<br/>    'AUC': 'roc_auc', <br/>    'Accuracy': make_scorer(accuracy_score)<br/>}</span><span id="a249" class="mb kq it lx b gy mg md l me mf"># create the Kfold object<br/>num_folds = 10<br/>kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)</span><span id="ff93" class="mb kq it lx b gy mg md l me mf"># create the grid search object with the full pipeline as estimator<br/>n_iter=25</span><span id="c8eb" class="mb kq it lx b gy mg md l me mf">grid = RandomizedSearchCV(<br/>    estimator=full_pipeline, <br/>    param_distributions=param_grid,<br/>    cv=kfold,<br/>    scoring=scoring,<br/>    n_jobs=-1,<br/>    n_iter=n_iter,<br/>    refit="AUC"<br/>)<br/>                                      <br/># fit grid search<br/>best_bag = grid.fit(X_train,y_train)</span></pre><p id="acdf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于<strong class="js iu">打包分类器</strong>，我们正在调整以下参数:</p><ul class=""><li id="430e" class="mq mr it js b jt ju jx jy kb ms kf mt kj mu kn mv mw mx my bi translated"><strong class="js iu"> n_estimators: </strong>定义要使用的估计器(在这种情况下是决策树)的总数；</li><li id="5e6d" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu"> max_samples: </strong>从 X 中抽取的用于训练每个基本估计量的样本的最大百分比；</li><li id="f76f" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu"> max_features: </strong>从 X 中提取的特征的最大百分比，用于训练每个基本估计量；</li><li id="031c" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu"> bootstrap: </strong>设置为 False 时，抽取样本而不替换(粘贴)。当设置为真时，用替换(装袋)抽取样本。因为我们要使用开箱得分，所以我们必须将该参数设置为 True</li><li id="796e" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu"> oob_score: </strong>设置为 True 时，返回最佳模型的开箱得分；</li></ul><p id="a4f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随机搜索完成且最佳模型符合我们的数据后，我们可以检查调整后的模型超参数，进行预测并测量模型的性能:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="14bb" class="mb kq it lx b gy mc md l me mf">print(f'Best score: {best_bag.best_score_}')<br/>print(f'Best model: {best_bag.best_params_}')</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nr"><img src="../Images/8e156288a184469b32a3c684c5bccf58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rFRMqQ4GWN97YQcuMz2Y0Q.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 3: </strong>Best Bagging model</figcaption></figure><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="c9d1" class="mb kq it lx b gy mc md l me mf">pred_test = best_bag.predict(X_test)<br/>pred_train = best_bag.predict(X_train)</span><span id="cbf8" class="mb kq it lx b gy mg md l me mf">print('Train Accuracy: ', accuracy_score(y_train, pred_train))<br/>print('Test Accuraccy: ', accuracy_score(y_test, pred_test))<br/>print("Out-of-Bag Accuracy: ", best_bag.best_params_['model'].oob_score_)</span><span id="6f05" class="mb kq it lx b gy mg md l me mf">print('\nConfusion Matrix:')<br/>print(confusion_matrix(y_test,pred_test))<br/>print('\nClassification Report:')<br/>print(classification_report(y_test,pred_test))</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/777d11ec063abf811ab1a09b7cb559a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*fO1lmWE6AUd7V-LcpMoxPw.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 4:</strong> Bagging performance</figcaption></figure><p id="b54f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如我们所见，该模型达到的所有 3 个精确度彼此非常接近，表明该模型对新数据具有良好的泛化能力，因此可能不会<strong class="js iu">过度拟合</strong>。</p><h2 id="3374" class="mb kq it bd kr nf ng dn kv nh ni dp kz kb nj nk ld kf nl nm lh kj nn no ll np bi translated">3.2.随机森林分类器</h2><p id="6406" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我们将训练的最后一个模型是<strong class="js iu"> RandomForestClassifier </strong>。使用它的过程与上面给出的过程相同，可以在下面的代码片段中看到:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="3b70" class="mb kq it lx b gy mc md l me mf"># we pass the preprocessing pipeline as a step to the full pipeline<br/>full_pipeline_steps = [<br/>    ('preprocessing_pipeline', preprocessing_pipeline),<br/>    ('model', RandomForestClassifier(random_state=seed))<br/>]</span><span id="89be" class="mb kq it lx b gy mg md l me mf"># create the full pipeline object<br/>full_pipeline = Pipeline(steps=full_pipeline_steps)</span><span id="549b" class="mb kq it lx b gy mg md l me mf"># Create the grid search parameter grid and scoring funcitons<br/>param_grid = {<br/>    "model": [RandomForestClassifier(random_state=seed)],<br/>    "model__max_depth": np.linspace(1, 32, 32),<br/>    "model__n_estimators": np.arange(100, 1000, 100),<br/>    "model__criterion": ["gini","entropy"],<br/>    "model__max_leaf_nodes": [16, 64, 128, 256],<br/>    "model__oob_score": [True],<br/>}</span><span id="1fb8" class="mb kq it lx b gy mg md l me mf">scoring = {<br/>    'AUC': 'roc_auc', <br/>    'Accuracy': make_scorer(accuracy_score)<br/>}</span><span id="d7ce" class="mb kq it lx b gy mg md l me mf"># create the Kfold object<br/>num_folds = 10<br/>kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)</span><span id="7b12" class="mb kq it lx b gy mg md l me mf"># create the grid search object with the full pipeline as estimator<br/>n_iter=50</span><span id="fe34" class="mb kq it lx b gy mg md l me mf">grid = RandomizedSearchCV(<br/>    estimator=full_pipeline, <br/>    param_distributions=param_grid,<br/>    cv=kfold,<br/>    scoring=scoring,<br/>    n_jobs=-1,<br/>    n_iter=n_iter,<br/>    refit="AUC"<br/>)</span><span id="b684" class="mb kq it lx b gy mg md l me mf"># fit grid search<br/>best_rf = grid.fit(X_train,y_train)</span></pre><p id="47f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于<strong class="js iu"> RandomForestClassifier </strong>，我们正在调整以下参数:</p><ul class=""><li id="ead2" class="mq mr it js b jt ju jx jy kb ms kf mt kj mu kn mv mw mx my bi translated"><strong class="js iu">标准:</strong>定义了度量树节点上分割质量的函数；</li><li id="3ec5" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu"> max_leaf_nodes: </strong>限制树中的最大叶节点数；</li><li id="0f8e" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu"> max_depth: </strong>限制树木可以生长的最大深度；</li><li id="4f8f" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu"> n_estimators: </strong>定义森林中使用的树木总数</li><li id="3982" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated"><strong class="js iu"> oob_score: </strong>设置为 True 时，返回最佳模型的出袋分数；</li></ul><p id="3eea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们之前所做的，在随机搜索完成且最佳模型符合我们的数据后，我们可以进行预测并衡量模型的性能:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="2279" class="mb kq it lx b gy mc md l me mf">print(f'Best score: {best_rf.best_score_}')<br/>print(f'Best model: {best_rf.best_params_}')</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nx"><img src="../Images/c368d26944cecfe6e08591aaa5b5e696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xM0kuSsebNNTQytNZ1EpIg.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 5: </strong>Best Random Forest model</figcaption></figure><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="5b44" class="mb kq it lx b gy mc md l me mf">pred_test = best_rf.predict(X_test)<br/>pred_train = best_rf.predict(X_train)</span><span id="3c3c" class="mb kq it lx b gy mg md l me mf">print('Train Accuracy: ', accuracy_score(y_train, pred_train))<br/>print('Test Accuraccy: ', accuracy_score(y_test, pred_test))<br/>print("Out-of-Bag Accuracy: ", best_rf.best_params_['model'].oob_score_)</span><span id="79e7" class="mb kq it lx b gy mg md l me mf">print('\nConfusion Matrix:')<br/>print(confusion_matrix(y_test,pred_test))<br/>print('\nClassification Report:')<br/>print(classification_report(y_test,pred_test))</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/6a45bbc9812675a24b34ca4a2dfee78c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*6TKuf0DeA-uhpnctH0FqfA.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 6:</strong> Random Forest performance</figcaption></figure><p id="7dc8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与<strong class="js iu"> BaggingClassifier </strong>相似，精确度彼此非常接近，表明对看不见的数据具有良好的泛化能力，因此没有<strong class="js iu">过拟合</strong>。由于这是我们发现的性能最好的模型，我们将在本文的剩余部分中使用它作为例子。</p><h1 id="c2a0" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">4.模型可解释性</h1><p id="e116" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">在某些情况下，从你的模型中获得好的预测可以说和理解<em class="nq">为什么</em>给出答案一样重要。在这些情况下，我们可以利用<strong class="js iu">可解释的人工智能(XAI) </strong>的概念，并试图使模型更容易被人类理解。</p><p id="6eb9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一种方法是分析模型结果中每个特征的重要性。幸运的是，随机森林有一个内置属性可以告诉我们这一点，它叫做<strong class="js iu"> feature_importances_ </strong>。下面的代码显示了访问和绘制这些信息的步骤:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="ed48" class="mb kq it lx b gy mc md l me mf"># lets get the random forest model configuration and feature names<br/>rf_model = best_rf.best_params_['model']<br/>features = np.array(X_train.columns)</span><span id="1a81" class="mb kq it lx b gy mg md l me mf"># Transforming the test data.<br/>new_X_test = preprocessing_pipeline.fit_transform(X_test)<br/>new_X_test = pd.DataFrame(new_X_test, columns=X_test.columns)</span><span id="cc4c" class="mb kq it lx b gy mg md l me mf"># get the predicitons from the random forest object<br/>y_pred = rf_model.predict(new_X_test)</span><span id="1d0d" class="mb kq it lx b gy mg md l me mf"># get the feature importances<br/>importances = rf_model.feature_importances_</span><span id="0d72" class="mb kq it lx b gy mg md l me mf"># sort the indexes<br/>sorted_index = np.argsort(importances)<br/>sorted_importances = importances[sorted_index]<br/>sorted_features = features[sorted_index]</span><span id="f44b" class="mb kq it lx b gy mg md l me mf"># plot the explained variance using a barplot<br/>fig, ax = plt.subplots()<br/>ax.barh(sorted_features , sorted_importances)<br/>ax.set_xlabel('Importances')<br/>ax.set_ylabel('Features')</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/6af862c6f0c3dc0c1ec641afc6be1377.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*P_RVdnNvOaZ5XUHhVpKecw.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 7:</strong> Feature importances</figcaption></figure><p id="596d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个简单而有效的例子，可以让你从数据和模型推理中获得更多的洞察力。有了剧情，我们可以看到最重要的特征是<strong class="js iu"> education_num </strong>，<strong class="js iu"> capital_loss </strong>，<strong class="js iu"> fnlwgt </strong>，<strong class="js iu"> capital_gain </strong>，以及<strong class="js iu"> race </strong>。</p><p id="34c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了更进一步，我们将简要探索两个第三方库，它们使我们能够获得不同的可视化:<strong class="js iu"> ELI5 </strong>和<strong class="js iu"> SHAP。</strong></p><h2 id="d336" class="mb kq it bd kr nf ng dn kv nh ni dp kz kb nj nk ld kf nl nm lh kj nn no ll np bi translated">4.1.使用 ELI5 进行模型解释</h2><p id="23d8" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated"><a class="ae ko" href="https://github.com/TeamHG-Memex/eli5" rel="noopener ugc nofollow" target="_blank"> ELI5 </a>是一个 Python 包，帮助调试机器学习分类器，解释它们的预测。它为一些机器学习库提供支持，包括 scikit-learn 和 XGBoost。</p><p id="060f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有了它，我们能够以两种主要方式查看分类模型:第一种是检查模型参数并分析模型如何全局工作(类似于默认的特征重要性属性)；第二个是检查单个预测，以弄清楚为什么模型会做出这样的决定。</p><p id="0f14" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于第一个用例，我们使用<strong class="js iu"> show_weights() </strong>函数，如下面的代码片段所示:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="e440" class="mb kq it lx b gy mc md l me mf">import eli5</span><span id="e2f7" class="mb kq it lx b gy mg md l me mf"># lets get the random forest model configuration and feature names<br/>rf_model = best_rf.best_params_['model']<br/>features = np.array(X_train.columns)</span><span id="435f" class="mb kq it lx b gy mg md l me mf">eli5.show_weights(rf_model, feature_names=features)</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/68ef67a8cba99e74b878a84a81c56a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*ZLzCyCmkgj09GCSQWvs4jg.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 8:</strong> ELI5 feature weights</figcaption></figure><p id="2520" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们在上面的图像中看到的，结果与我们从树特征重要性中获得的结果非常相似。</p><p id="9b96" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">至于第二个用例，我们可以使用<strong class="js iu"> explain_prediction() </strong>函数来检查和分析模型的各个预测。为了测试这一点，我们检查了一个真正的负面预测(实际值为 0，预测值也为 0)和一个真正的正面预测(实际值为 1，预测值也为 1):</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="f5fc" class="mb kq it lx b gy mc md l me mf"># predicting a person earns less than 50k/year (true negative)<br/>index = 4<br/>print('Actual Label:', y_test.iloc[index])<br/>print('Predicted Label:', y_pred[index])<br/>eli5.explain_prediction(rf_model, new_X_test.iloc[index], feature_names=features)</span><span id="4bea" class="mb kq it lx b gy mg md l me mf"># predicting a person earns more than 50k/year (true positive)<br/>index = 7<br/>print('Actual Label:', y_test.iloc[index])<br/>print('Predicted Label:', y_pred[index])<br/>eli5.explain_prediction(rf_model, new_X_test.iloc[index], feature_names=features)</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f8222dd6de803a1d8d32b1df88beb275.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*tFUkkV06A5LTJ1Xp0DB7Zg.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 9: </strong>ELI5 true negative prediction example</figcaption></figure><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1eb97703ec850f1e02dbd248b37432ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*CoCTIygr9dQWsRgiweZJJw.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 10: </strong>ELI5 true positive prediction example</figcaption></figure><p id="af8f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，有助于模型预测这些特定观察结果的最有影响力的特征分别是<strong class="js iu">种族</strong>和<strong class="js iu">教育 _ 数量</strong>。</p><h2 id="673a" class="mb kq it bd kr nf ng dn kv nh ni dp kz kb nj nk ld kf nl nm lh kj nn no ll np bi translated">4.2.SHAP 的模型解释</h2><p id="69aa" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated"><strong class="js iu">SHAP(SHapley Additive exPlanations)</strong>库是解释任何机器学习模型输出的统一方法。与 ELI5 类似，它也支持几个机器学习库，包括 scikit-learn 和 XGBoost。</p><p id="a28e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了将它的功能用于我们的随机森林模型，我们首先需要创建一个<strong class="js iu"> TreeExplainer </strong>对象，并为模型获取所谓的<strong class="js iu"> shap_values </strong>。这个过程如下面的代码所示:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="28b2" class="mb kq it lx b gy mc md l me mf">import shap<br/>shap.initjs()</span><span id="2322" class="mb kq it lx b gy mg md l me mf"># Create the explainer object<br/>explainer = shap.TreeExplainer(rf_model)<br/>print('Expected Value:', explainer.expected_value)</span><span id="705f" class="mb kq it lx b gy mg md l me mf"># get the shap values from the explainer<br/>shap_values = explainer.shap_values(new_X_test)</span></pre><p id="249c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们对 ELI5 所做的那样，我们也可以使用 SHAP 库来解释单个模型预测，如下所示，这些数据点与我们之前处理的数据点相同:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="1b76" class="mb kq it lx b gy mc md l me mf"># predicting a person earns less than 50k/year (true negative)<br/>shap.force_plot(explainer.expected_value[0],<br/>                shap_values[0][4], X_test.iloc[4])</span><span id="3ffb" class="mb kq it lx b gy mg md l me mf"># predicting a person earns more than 50k/year (true positive)<br/>shap.force_plot(explainer.expected_value[1],<br/>                shap_values[1][7], X_test.iloc[7])</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi od"><img src="../Images/aeeb9c05d36ad85cc7aeaf2a25945f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N9s-bDB0AbSHdeb9rP-HSA.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 11: </strong>SHAP true negative prediction example</figcaption></figure><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nx"><img src="../Images/4e4a640be31de526b40def3e399856c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6WJERV-XN5qMWhpBVQC3lQ.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 12: </strong>SHAP true positive prediction example</figcaption></figure><p id="02e2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，还可以一次显示多个预测，如下图所示为数据集的前 1000 个样本:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="c529" class="mb kq it lx b gy mc md l me mf">shap.force_plot(explainer.expected_value[0],<br/>                shap_values[0][:1000,:], X_test.iloc[:1000,:])</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi od"><img src="../Images/0c980995e3e09de8c5117cea8c873b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fB4v4-uXi-nieDVKj2QhWA.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 12: </strong>SHAP prediction explanation for the first 1000 samples</figcaption></figure><p id="4668" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在同一个图中，还可以分析不同特征在最终模型预测中的影响。为了测试这一点，我们对图进行了配置，以显示<strong class="js iu"> education_num </strong>特性对相同样本的重要性:</p><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi oe"><img src="../Images/b0cb5dbf552b61fb660c075f94212188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6OSJ0UaQNemTXgd0nG63mw.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 13: </strong>SHAP education_num effect for the first 1000 samples</figcaption></figure><p id="9619" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们可以使用<strong class="js iu"> summary_plot </strong>函数来绘制按类划分的特性重要性:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="0740" class="mb kq it lx b gy mc md l me mf">shap.summary_plot(shap_values, X_test)</span></pre><figure class="ls lt lu lv gt mi gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8caf4c1ddbc034e731f100b7d9a3c2d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*Bjh7AyuOSgApBBKB3vg4kg.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><strong class="bd mp">Figure 14: </strong>SHAP feature importances by class</figcaption></figure><p id="7ebe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到，得到的结果与树内置特性重要性和 ELI5 库得到的结果非常相似。</p><h1 id="6228" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">5.结论</h1><p id="dd06" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我们几乎没有触及机器学习领域中几个重要主题的表面，如管道、超参数调整、集成方法和模型可解释性。还有更多的内容要介绍！</p><p id="0a41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个系列的下一部分，我们将看看新的集成方法的“酷小子”:<strong class="js iu">梯度增强</strong>。我们将看看由<strong class="js iu"> XGBoost </strong>库提供的实现，敬请关注！</p></div></div>    
</body>
</html>