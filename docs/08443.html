<html>
<head>
<title>Keras custom data generators example with MNIST Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MNIST 数据集的 Keras 自定义数据生成器示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/keras-custom-data-generators-example-with-mnist-dataset-2a7a2d2b0360?source=collection_archive---------14-----------------------#2019-11-16">https://towardsdatascience.com/keras-custom-data-generators-example-with-mnist-dataset-2a7a2d2b0360?source=collection_archive---------14-----------------------#2019-11-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="gh gi jx"><img src="../Images/27f18382048e56cae4074b29515cd98b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-K4bpH1IXEua6hcshfESw.jpeg"/></div></div><figcaption class="kj kk gj gh gi kl km bd b be z dk">Photo by <a class="ae kn" href="https://unsplash.com/@kmuza?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Carlos Muza</a> on <a class="ae kn" href="https://unsplash.com/s/photos/data-generator?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="bd50" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">通常，在现实世界的问题中，用于训练我们模型的数据集占用的内存比我们在 RAM 中的要多得多。问题是我们不能将整个数据集加载到内存中，并使用标准的 keras <em class="lm"> fit </em>方法来训练我们的模型。</p><p id="0d79" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">解决这个问题的一种方法是只把一批数据装入内存，然后把它输入网络。重复这个过程，直到我们用所有数据集训练了网络。然后我们打乱所有的数据集，重新开始。</p><p id="009a" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了制作自定义生成器，keras 为我们提供了一个序列类。这个类是抽象的，我们可以创建继承它的类。</p><p id="bc64" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们将编码一个自定义数据生成器，该生成器将用于生成 MNIST 数据集的批量样本。</p><p id="f661" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">首先，我们将导入 python 库:</p><pre class="jy jz ka kb gt ln lo lp lq aw lr bi"><span id="150c" class="ls lt it lo b gy lu lv l lw lx"><strong class="lo iu">import</strong> tensorflow <strong class="lo iu">as</strong> tf<br/><strong class="lo iu">import</strong> os<br/><strong class="lo iu">import</strong> tensorflow.keras <strong class="lo iu">as</strong> keras<br/><strong class="lo iu">from</strong> tensorflow.keras.models <strong class="lo iu">import</strong> Sequential<br/><strong class="lo iu">from</strong> tensorflow.keras.layers <strong class="lo iu">import</strong> Dense, Dropout, Flatten<br/><strong class="lo iu">from</strong> tensorflow.keras.layers <strong class="lo iu">import</strong> Conv2D, MaxPooling2D<br/><strong class="lo iu">import</strong> numpy <strong class="lo iu">as</strong> np<br/><strong class="lo iu">import</strong> math</span></pre><p id="1836" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">然后，我们将 MNIST 数据集加载到 RAM 内存中:</p><pre class="jy jz ka kb gt ln lo lp lq aw lr bi"><span id="65ad" class="ls lt it lo b gy lu lv l lw lx">mnist = tf.keras.datasets.mnist</span><span id="96c9" class="ls lt it lo b gy ly lv l lw lx">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span></pre><p id="c52f" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">MNIST 数据集由 60000 幅手写数字训练图像和 10000 幅测试图像组成。</p><p id="6a44" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">每个图像尺寸为 28×28 像素。你应该考虑到，为了训练模型，我们必须将<em class="lm"> uint8 </em>数据转换为<em class="lm"> float32。</em>float 32 中的每个像素需要 4 <em class="lm">字节</em>的内存。</p><p id="cbde" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因此，整个数据集需要:</p><p id="a6a5" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">每像素 4 字节* (28 * 28)每图像像素* 70000 个图像+ (70000*10)个标签。</p><p id="d1be" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">总共 220 Mb 的内存完全可以放在 RAM 内存中，但在现实世界的问题中，我们可能需要更多的内存。</p><p id="9786" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们的生成器模拟生成器将从 RAM 中加载图像，但在实际问题中，它们将从硬盘中加载。</p><pre class="jy jz ka kb gt ln lo lp lq aw lr bi"><span id="736a" class="ls lt it lo b gy lu lv l lw lx">class <strong class="lo iu">DataGenerator</strong>(tf.compat.v2.keras.utils.Sequence):<br/> <br/>    def <strong class="lo iu">__init__</strong>(self, X_data , y_data, batch_size, dim, n_classes,<br/>                 to_fit, shuffle = True):</span><span id="d893" class="ls lt it lo b gy ly lv l lw lx">        self.batch_size = batch_size<br/>        self.X_data = X_data<br/>        self.labels = y_data<br/>        self.y_data = y_data<br/>        self.to_fit = to_fit<br/>        self.n_classes = n_classes<br/>        self.dim = dim<br/>        self.shuffle = shuffle<br/>        self.n = 0<br/>        self.list_IDs = np.arange(len(self.X_data))<br/>        self.on_epoch_end()</span><span id="c0c8" class="ls lt it lo b gy ly lv l lw lx">    def <strong class="lo iu">__next__</strong>(self):<br/>        # Get one batch of data<br/>        data = self.__getitem__(self.n)<br/>        # Batch index<br/>        self.n += 1<br/>        <br/>        # If we have processed the entire dataset then<br/>        if self.n &gt;= self.__len__():<br/>            self.on_epoch_end<br/>            self.n = 0<br/>        <br/>        <strong class="lo iu">return</strong> data</span><span id="e68e" class="ls lt it lo b gy ly lv l lw lx">    def <strong class="lo iu">__len__</strong>(self):<br/>        # Return the number of batches of the dataset<br/>        <strong class="lo iu">return</strong> math.ceil(len(self.indexes)/self.batch_size)</span><span id="d385" class="ls lt it lo b gy ly lv l lw lx">    def <strong class="lo iu">__getitem__</strong>(self, index):<br/>        # Generate indexes of the batch<br/>        indexes = self.indexes[index*self.batch_size:<br/>            (index+1)*self.batch_size]</span><span id="017f" class="ls lt it lo b gy ly lv l lw lx">        # Find list of IDs<br/>        list_IDs_temp = [self.list_IDs[k] for k in indexes]<br/>        <br/>        X = self._generate_x(list_IDs_temp)<br/>        <br/>        if self.to_fit:<br/>            y = self._generate_y(list_IDs_temp)<br/>            <strong class="lo iu">return</strong> X, y<br/>        else:<br/>            <strong class="lo iu">return</strong> X</span><span id="a7aa" class="ls lt it lo b gy ly lv l lw lx">    def <strong class="lo iu">on_epoch_end</strong>(self):<br/>        <br/>        self.indexes = np.arange(len(self.X_data))<br/>        <br/>        if self.shuffle: <br/>            np.random.shuffle(self.indexes)</span><span id="85d3" class="ls lt it lo b gy ly lv l lw lx">    def <strong class="lo iu">_generate_x</strong>(self, list_IDs_temp):<br/>               <br/>        X = np.empty((self.batch_size, *self.dim))<br/>        <br/>        for i, ID in enumerate(list_IDs_temp):<br/>            <br/>            X[i,] = self.X_data[ID]<br/>            <br/>            # Normalize data<br/>            X = (X/255).astype('float32')<br/>            <br/>        <strong class="lo iu">return</strong> X[:,:,:, np.newaxis]</span><span id="d909" class="ls lt it lo b gy ly lv l lw lx">    def <strong class="lo iu">_generate_y</strong>(self, list_IDs_temp):<br/>        <br/>        y = np.empty(self.batch_size)<br/>        <br/>        for i, ID in enumerate(list_IDs_temp):<br/>            <br/>            y[i] = self.y_data[ID]<br/>            <br/>        <strong class="lo iu">return</strong> keras.utils.to_categorical(<br/>                y,num_classes=self.n_classes)</span></pre><p id="d6d7" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">然后我们要建立分类网络:</p><pre class="jy jz ka kb gt ln lo lp lq aw lr bi"><span id="2c2e" class="ls lt it lo b gy lu lv l lw lx">n_classes = 10<br/>input_shape = (28, 28)</span><span id="760a" class="ls lt it lo b gy ly lv l lw lx">model = Sequential()<br/>model.add(Conv2D(32, kernel_size=(3, 3),<br/>                 activation='relu',<br/>                 input_shape=(28, 28 , 1)))<br/>model.add(Conv2D(64, (3, 3), activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Dropout(0.25))<br/>model.add(Flatten())<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(n_classes, activation='softmax'))</span><span id="02a4" class="ls lt it lo b gy ly lv l lw lx">model.compile(loss=keras.losses.categorical_crossentropy,<br/>              optimizer=keras.optimizers.Adadelta(),<br/>              metrics=['accuracy'])</span></pre><p id="f619" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">下一步是制作我们的生成器的一个实例:</p><pre class="jy jz ka kb gt ln lo lp lq aw lr bi"><span id="4cdc" class="ls lt it lo b gy lu lv l lw lx">train_generator = DataGenerator(x_train, y_train, batch_size = 64,<br/>                                dim = input_shape,<br/>                                n_classes=10, <br/>                                to_fit=True, shuffle=True)</span><span id="5372" class="ls lt it lo b gy ly lv l lw lx">val_generator =  DataGenerator(x_test, y_test, batch_size=64, <br/>                               dim = input_shape, <br/>                               n_classes= n_classes, <br/>                               to_fit=True, shuffle=True)</span></pre><p id="051f" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果我们想检查生成器是否正常工作，我们可以调用产生一批样本和标签的<em class="lm"> next() </em>方法。然后检查图像和标签的数据类型是否正确，检查批次的尺寸等…</p><pre class="jy jz ka kb gt ln lo lp lq aw lr bi"><span id="b7ae" class="ls lt it lo b gy lu lv l lw lx">images, labels = next(train_generator)<br/>print(images.shape)<br/>print(labels.shape)</span></pre><p id="5ffc" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果我们希望在一个时期内将整个数据集输入网络:</p><pre class="jy jz ka kb gt ln lo lp lq aw lr bi"><span id="7526" class="ls lt it lo b gy lu lv l lw lx">steps_per_epoch = len(train_generator)<br/>validation_steps = len(val_generator)</span></pre><p id="bab7" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最后，我们将使用 keras 函数<em class="lm"> fit_generator()来训练网络。</em></p><pre class="jy jz ka kb gt ln lo lp lq aw lr bi"><span id="ce6e" class="ls lt it lo b gy lu lv l lw lx">model.fit_generator(<br/>        train_generator,<br/>        steps_per_epoch=steps_per_epoch,<br/>        epochs=10,<br/>        validation_data=val_generator,<br/>        validation_steps=validation_steps)</span></pre></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="c130" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">感谢阅读这篇文章。希望你觉得有用。</p></div></div>    
</body>
</html>