<html>
<head>
<title>My 10 recommendations after getting the Databricks Certification for Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">获得 Apache Spark 的 Databricks 认证后，我的 10 条建议</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-10-recommendations-after-getting-the-databricks-certification-for-apache-spark-53cd3690073?source=collection_archive---------2-----------------------#2019-08-02">https://towardsdatascience.com/my-10-recommendations-after-getting-the-databricks-certification-for-apache-spark-53cd3690073?source=collection_archive---------2-----------------------#2019-08-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jq jr js jt gh gi paragraph-image"><div class="ab gu cl ju"><img src="../Images/4ca1a3d56af6ee6740ae3646b4b86be1.png" data-original-src="https://miro.medium.com/v2/format:webp/1*LlZVZwoJnxl3Q7WhIk-7Dw.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Databricks Certified Developer Badget</figcaption></figure><p id="0147" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">几个月前，我开始准备获得 Apache Spark 的<a class="ae kz" href="https://academy.databricks.com/category/certifications" rel="noopener ugc nofollow" target="_blank"> Databricks 认证。这并不容易，因为没有太多关于它的信息，所以为了促进自我准备，我将分享 10 个有用的建议。</a></p><h1 id="776c" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">建议 1:在开始准备之前安排好考试</h1><p id="cd19" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">这是唯一的非技术性建议，但也是所有 9 个建议中有用的。当你有了参加考试的期限，你就有了更多学习的理由和压力。在这种情况下，对于考试来说，<strong class="kd iu">5-7 周的准备</strong>会让你为成功的结果做好准备，尤其是如果你有使用<a class="ae kz" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>的工作经验。在<a class="ae kz" href="https://www.webassessor.com/databricks" rel="noopener ugc nofollow" target="_blank">网页</a>上注册，你将花费<strong class="kd iu">300 美元</strong>，如果你第一次尝试失败，你将获得 1 次<strong class="kd iu">额外机会</strong>(我的建议是在接下来的 2 周内重新安排第二次尝试的最大次数)。你需要在<strong class="kd iu"> 3 小时</strong>内回答<strong class="kd iu"> 40 个多项选择问题</strong>的最低分数<strong class="kd iu"> 65% </strong>并且你可以<strong class="kd iu"> </strong>参加 Python 或 Scala 的<strong class="kd iu">考试。</strong></p><figure class="md me mf mg gt jt"><div class="bz fp l di"><div class="mh mi l"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Basic exam information</figcaption></figure><figure class="md me mf mg gt jt gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mj"><img src="../Images/35b89d716e5fa4a1e27458bf73289d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2yMUHUwkzMoltRRS"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Scheduling the exam makes you focus on practicing</figcaption></figure><h1 id="0d68" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">推荐 2:无论是 PySpark 还是 Spark Scala API 对于考试来说都差不多</h1><p id="4e24" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">如果你发现自己在 Spark 语言 API 使用 Python 还是 Scala 的问题上左右为难，我的建议是不要太担心，因为这个问题不需要对这些编程语言有深入的了解。例如，您可以找到这种类型的问题，其中代码片段(Python 或 Scala)提供给您，您需要确定哪一个是不正确的。</p><pre class="md me mf mg gt mo mp mq mr aw ms bi"><span id="5dbc" class="mt lb it mp b gy mu mv l mw mx"><em class="my">//Scala</em><br/><strong class="mp iu">val</strong> <!-- -->df<!-- --> <strong class="mp iu">=</strong> <!-- -->spark.read.format("parquet").load("/data/sales/june")<br/>df.createOrReplaceTempView("table")</span><span id="fbfd" class="mt lb it mp b gy mz mv l mw mx"><em class="my">#Python</em><br/>df<!-- --> <!-- -->=<!-- --> <!-- -->spark.read.orc().load("/data/bikes/june")<br/>df.<!-- -->createGlobalTempView<!-- -->("table")</span><span id="6c1e" class="mt lb it mp b gy mz mv l mw mx"><em class="my">#Python</em><strong class="mp iu"><br/>from</strong> <strong class="mp iu">pyspark.sql</strong> <strong class="mp iu">import</strong> <!-- -->Row<br/>myRow<!-- --> <!-- -->=<!-- --> <!-- -->Row(3.14156,<!-- --> "Chicago<!-- -->",<!-- --> 7<!-- -->)</span><span id="27bf" class="mt lb it mp b gy mz mv l mw mx"><strong class="mp iu">import</strong> <strong class="mp iu">org.apache.spark.sql.functions.lit</strong><br/>df.select(lit("7.5"),<!-- --> <!-- -->lit("11.47")).show(2)</span></pre><blockquote class="na nb nc"><p id="6717" class="kb kc my kd b ke kf kg kh ki kj kk kl nd kn ko kp ne kr ks kt nf kv kw kx ky im bi translated">你能找到不正确的代码吗？</p></blockquote><p id="cace" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以面对这种问题<strong class="kd iu">记住 Spark data frame(<strong class="kd iu"><em class="my">20%-25%的问题</em> </strong>)、RDDs、SQL、Streaming 和 Graphframes 中的结构和主要选项</strong>。例如，这些是 Spark 数据帧中的写和读核心结构。</p><pre class="md me mf mg gt mo mp mq mr aw ms bi"><span id="51c4" class="mt lb it mp b gy mu mv l mw mx">#Read<br/>DataFrameReader.format(...).option("key","value").schema(...).load()<br/>#Read modes: permissive (default), dropMalformed and failFast.</span><span id="d4f4" class="mt lb it mp b gy mz mv l mw mx">#Write<br/>DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()<br/>#Save modes: append, overwrite, errorIfExists (default) and ignore.</span></pre><h1 id="3aa1" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">建议 3:在头脑中用 Spark 数据帧和 SQL 练习“执行”代码</h1><p id="d712" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">如果你想得到上面代码的输出，你做得很好，因为在测试期间，你不允许检查任何文档，甚至没有纸来做笔记，所以你会发现另一种问题，你需要确定正确的选择(可能不止一个)，产生基于一个或多个表显示的输出。</p><pre class="md me mf mg gt mo mp mq mr aw ms bi"><span id="b568" class="mt lb it mp b gy mu mv l mw mx">#Table<br/>+---------+---------+<br/>|     Name|      Age| <br/>+---------+---------+<br/>|    David|       71|<br/>| Angelica|       22|<br/>|   Martin|        7|<br/>|      Sol|       12|<br/>+---------+---------+</span><span id="7a00" class="mt lb it mp b gy mz mv l mw mx">#Output needed<br/># Quantity of people greater than 21</span><span id="e7ac" class="mt lb it mp b gy mz mv l mw mx">df<!-- --> <!-- -->=<!-- --> <!-- -->spark.read.format("csv")<!-- -->\<br/>  <!-- -->.option("header",<!-- --> <!-- -->"true")<!-- -->\<br/>  <!-- -->.option("inferSchema",<!-- --> <!-- -->"true")<!-- -->\<br/>  <!-- -->.load("/names/*.csv")<br/>df.where(col("Age")&gt;21).count()</span><span id="ddd6" class="mt lb it mp b gy mz mv l mw mx">df<!-- --> <!-- -->=<!-- --> <!-- -->spark.read.parquet("/names/*.parquet")<!-- -->\<br/>  <!-- -->.option("inferSchema",<!-- --> <!-- -->"true")<!-- -->\<br/>df.where("Age &gt; 21").count()</span><span id="0363" class="mt lb it mp b gy mz mv l mw mx">logic = "Age &gt; 21"<br/>df<!-- --> <!-- -->=<!-- --> <!-- -->spark.read.("/names/*.parquet")<!-- -->\<br/>  <!-- -->.option("inferSchema",<!-- --> <!-- -->"true")<!-- -->\<br/>df.where(<!-- -->logic<!-- -->).count()</span><span id="12c6" class="mt lb it mp b gy mz mv l mw mx">df =spark.read.format("json").option("mode",<!-- --> <!-- -->"FAILFAST")<!-- -->\<br/>  <!-- -->.option("inferSchema",<!-- --> <!-- -->"true")<!-- -->\<br/>  <!-- -->.load("names.json")<br/>df.where("Age &gt; 21").count()</span></pre><blockquote class="na nb nc"><p id="e89c" class="kb kc my kd b ke kf kg kh ki kj kk kl nd kn ko kp ne kr ks kt nf kv kw kx ky im bi translated">你能找到正确的代码吗？提示:存在不止一个。</p></blockquote><p id="6154" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这种问题不仅是关于数据帧的，在 RDD 问题中也会用到，所以要仔细研究一些函数，如 map、reduce、flatmap、groupby 等。我的推荐是查看<a class="ae kz" href="https://amzn.to/376KhqF" rel="noopener ugc nofollow" target="_blank">书学习火花</a>尤其是第三章<a class="ae kz" href="https://learning.oreilly.com/library/view/learning-spark/9781449359034/ch04.html" rel="noopener ugc nofollow" target="_blank">和第四章</a>。</p><h1 id="b521" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">建议 4:理解基本的 Spark 架构</h1><figure class="md me mf mg gt jt gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d477abcaddf5e4f7022cfa2d56de489f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*sAusQ9yYp-FrBiMP.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Do you feel familiar with these components? [<a class="ae kz" href="https://spark.apache.org/docs/latest/cluster-overview.html" rel="noopener ugc nofollow" target="_blank">Spark documentation</a>]</figcaption></figure><p id="7a19" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">了解 Spark 架构<strong class="kd iu"> <em class="my">(占考试的 15%】</em></strong>意味着不仅要阅读<a class="ae kz" href="https://spark.apache.org/docs/latest/cluster-overview.html" rel="noopener ugc nofollow" target="_blank">官方文档</a>并了解模块，还要发现:</p><ul class=""><li id="29ad" class="nh ni it kd b ke kf ki kj km nj kq nk ku nl ky nm nn no np bi translated">Spark 中简单或复杂的查询是如何执行的？</li><li id="8ee0" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">Spark 的不同集群管理器</li><li id="38c8" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">“懒惰评估”、“动作”、“转换”是什么意思？</li><li id="8df1" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">Spark 应用程序的层次结构</li><li id="2dec" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">集群部署选择</li><li id="4309" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">Spark 工具集的基本知识</li></ul><figure class="md me mf mg gt jt gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi nv"><img src="../Images/b5da8f7e6c1f6c3b762abdded98307c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SuuaXEoN_ZnGKAgA.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Spark’s toolset [<a class="ae kz" href="https://amzn.to/33Q96VR" rel="noopener ugc nofollow" target="_blank">Spark: The Definitive Guide</a>]</figcaption></figure><p id="1e39" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Spark Architecture 尝试的问题是，检查一个概念或定义是否正确。</p><pre class="md me mf mg gt mo mp mq mr aw ms bi"><span id="8b23" class="mt lb it mp b gy mu mv l mw mx">What means RDDs? What part of Spark are?</span><span id="e9de" class="mt lb it mp b gy mz mv l mw mx">Resilent Distributed Dataframes. Streaming API<br/>Resilent Distributed Datasets. Structured APIs<br/>Resilent Distributed Datasets. Low lever APIs</span></pre><p id="c50b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了在建筑方面更进一步，我推荐阅读《Spark:权威指南》一书的第 2、3、15 和 16 章。</p><h1 id="3dc3" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">建议 5:识别 Spark 结构化流中的输入、接收和输出</h1><p id="bd9e" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">大约<strong class="kd iu"> <em class="my"> 10%的问题</em> </strong>是关于 Spark 结构化流*主要是尝试让您识别出不会产生错误的正确代码，以达到您需要清楚本模块中的基本组件和定义。</p><p id="0e18" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这种情况下，该代码从 Github 上的<a class="ae kz" href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/python/sql/streaming/structured_network_wordcount.py" rel="noopener ugc nofollow" target="_blank">官方 Spark 文档报告中获得，并显示了从套接字获取数据、应用一些基本逻辑并将结果写入控制台并完成<em class="my">输出模式的基本字数。</em></a></p><pre class="md me mf mg gt mo mp mq mr aw ms bi"><span id="7341" class="mt lb it mp b gy mu mv l mw mx"><em class="my"># Start running the query that prints the running counts to the console<br/></em><strong class="mp iu">from</strong> <strong class="mp iu">pyspark.sql</strong> <strong class="mp iu">import</strong> SparkSession<br/><strong class="mp iu">from</strong> <strong class="mp iu">pyspark.sql.functions</strong> <strong class="mp iu">import</strong> explode<br/><strong class="mp iu">from</strong> <strong class="mp iu">pyspark.sql.functions</strong> <strong class="mp iu">import</strong> split<br/><br/>spark = SparkSession \<br/>    .builder \<br/>    .appName("StructuredNetworkWordCount") \<br/>    .getOrCreate()</span><span id="ab3d" class="mt lb it mp b gy mz mv l mw mx">lines = spark \<br/>    .readStream \<br/>    .format("socket") \<br/>    .option("host", "localhost") \<br/>    .option("port", 9999) \<br/>    .load()<br/><br/><em class="my"># Split the lines into words</em><br/>words = lines.select(<br/>   explode(<br/>       split(lines.value, " ")<br/>   ).alias("word")<br/>)<br/><br/><em class="my"># Generate running word count</em><br/>wordCounts = words.groupBy("word").count()</span><span id="5be5" class="mt lb it mp b gy mz mv l mw mx">query = wordCounts \<br/>    .writeStream \<br/>    .outputMode("complete") \<br/>    .format("console") \<br/>    .start()<br/><br/>query.awaitTermination()</span></pre><figure class="md me mf mg gt jt gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi nw"><img src="../Images/a58c316d7ae278ada5327a62d5159f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lnx5uMxnR9V0OsnQ.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">[<a class="ae kz" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">Spark documentation</a>]</figcaption></figure><p id="46b2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">本模块的问题将要求您识别正确或不正确的代码。它将组合不同的输入源(Apache Kafka、文件、套接字等)和/或接收器(输出)，例如 Apache Kafka、任何文件格式、控制台、内存等。以及输出模式:追加、更新和完成。为了练习这个问题，请阅读《火花:权威指南》一书的第 21 章。</p><p id="0225" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">*我知道存在数据流，但它是低级 API，不太可能出现在考试中。</p><h1 id="574a" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">建议 6:练习火花图算法</h1><p id="49af" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">像建议 5 一样，你需要识别的图形算法很少，所以我的建议是首先检查图形和<a class="ae kz" href="https://github.com/graphframes/graphframes" rel="noopener ugc nofollow" target="_blank">图形框架</a>*(<strong class="kd iu">5%–10%的问题</strong>)的概念，然后练习这些算法:</p><ul class=""><li id="e7e8" class="nh ni it kd b ke kf ki kj km nj kq nk ku nl ky nm nn no np bi translated">PageRank</li><li id="f043" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">入度和出度度量</li><li id="4411" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">广度优先搜索</li><li id="8a03" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">连接的组件</li></ul><p id="cce5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">例如，在这里，我们基于两个数据帧创建一个 GraphFrame，如果您想进行更多的练习，您可以在 Databricks 上的<a class="ae kz" href="https://docs.databricks.com/_static/notebooks/graphframes-user-guide-py.html" rel="noopener ugc nofollow" target="_blank"> GraphFrame 用户指南中找到此代码和完整的笔记本</a></p><pre class="md me mf mg gt mo mp mq mr aw ms bi"><span id="54aa" class="mt lb it mp b gy mu mv l mw mx"><strong class="mp iu">from</strong> functools <strong class="mp iu">import</strong> reduce<br/><strong class="mp iu">from</strong> pyspark.sql.functions <strong class="mp iu">import</strong> col, lit, when<br/><strong class="mp iu">from</strong> graphframes <strong class="mp iu">import</strong> *</span><span id="48e6" class="mt lb it mp b gy mz mv l mw mx">vertices = sqlContext.createDataFrame([<br/>  ("a", "Alice", 34),<br/>  ("b", "Bob", 36),<br/>  ("c", "Charlie", 30),<br/>  ("d", "David", 29),<br/>  ("e", "Esther", 32),<br/>  ("f", "Fanny", 36),<br/>  ("g", "Gabby", 60)], ["id", "name", "age"])</span><span id="0836" class="mt lb it mp b gy mz mv l mw mx">edges = sqlContext.createDataFrame([<br/>  ("a", "b", "friend"),<br/>  ("b", "c", "follow"),<br/>  ("c", "b", "follow"),<br/>  ("f", "c", "follow"),<br/>  ("e", "f", "follow"),<br/>  ("e", "d", "friend"),<br/>  ("d", "a", "friend"),<br/>  ("a", "e", "friend")<br/>], ["src", "dst", "relationship"])</span><span id="0acd" class="mt lb it mp b gy mz mv l mw mx">g = GraphFrame(vertices, edges)<br/>print(g)</span></pre><p id="3fef" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">*在 Graphframes 出现之前也有 GraphX(现在仍然存在)，但对于第一个，现在使用得更多。</p><h1 id="cb0d" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">建议 7:理解构建 Spark ML 管道的步骤</h1><p id="b512" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">Spark ML* ( <strong class="kd iu"> <em class="my"> 10%考试</em> </strong>)是捆绑了很多机器学习算法的模块，用于分类、回归、聚类或者用于基础统计、调优、模型选择和流水线。</p><p id="7a28" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这里，您需要重点了解一些必须知道的概念，如构建、训练和应用训练模型的步骤。例如，强制所有算法只有数字变量，所以如果你有一个字符串列，你需要使用一个<code class="fe nx ny nz mp b"><strong class="kd iu">StringIndexer</strong></code>方法一个<code class="fe nx ny nz mp b"><strong class="kd iu">OneHotEncoder</strong></code>编码器，所有的变量都需要在一个向量中，所以我们需要使用类<code class="fe nx ny nz mp b"><strong class="kd iu">VectorAssembler</strong></code>来最终将所有的转换分组在一个<code class="fe nx ny nz mp b"><strong class="kd iu">Pipeline</strong></code>中</p><pre class="md me mf mg gt mo mp mq mr aw ms bi"><span id="e892" class="mt lb it mp b gy mu mv l mw mx"><strong class="mp iu">from</strong> <strong class="mp iu">pyspark.ml.feature</strong> <strong class="mp iu">import</strong> <!-- -->StringIndexer<br/>indexer<!-- --> <!-- -->=<!-- --> <!-- -->StringIndexer()<!-- -->\<br/>  <!-- -->.setInputCol("month")<!-- -->\<br/>  <!-- -->.setOutputCol("month_index")</span><span id="4e0f" class="mt lb it mp b gy mz mv l mw mx"><strong class="mp iu">from</strong> <strong class="mp iu">pyspark.ml.feature</strong> <strong class="mp iu">import</strong> <!-- -->OneHotEncoder<br/>encoder<!-- --> <!-- -->=<!-- --> <!-- -->OneHotEncoder()<!-- -->\<br/>  <!-- -->.setInputCol("month_index")<!-- -->\<br/>  <!-- -->.setOutputCol("month_encoded")</span><span id="683b" class="mt lb it mp b gy mz mv l mw mx"><strong class="mp iu">from</strong> <strong class="mp iu">pyspark.ml.feature</strong> <strong class="mp iu">import</strong> <!-- -->VectorAssembler<br/>vectorAssembler<!-- --> <!-- -->=<!-- --> <!-- -->VectorAssembler()<!-- -->\<br/>  <!-- -->.setInputCols(["Sales",<!-- --> <!-- -->"month_encoded"])<!-- -->\<br/>  <!-- -->.setOutputCol("features")</span><span id="9801" class="mt lb it mp b gy mz mv l mw mx"><strong class="mp iu">from</strong> <strong class="mp iu">pyspark.ml</strong> <strong class="mp iu">import</strong> <!-- -->Pipeline<br/>transfPipeline<!-- --> <!-- -->=<!-- --> <!-- -->Pipeline()<!-- -->\<br/>  <!-- -->.setStages([indexer,<!-- --> <!-- -->encoder,<!-- --> <!-- -->vectorAssembler])</span><span id="7e9f" class="mt lb it mp b gy mz mv l mw mx">fitPipeline<!-- --> <!-- -->=<!-- --> <!-- -->transfPipeline.fit(trainDataFrame)</span></pre><p id="efad" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">*Spark MLlib 是一个基于 RDD 的 API，从 Spark 2.0 开始进入维护模式，因此 Spark ML 是主要的 ML API，并且是基于数据帧的。</p><h1 id="f87c" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">建议 8:认可火花转化、行动等等</h1><p id="8576" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">回到 Spark RDDs ( <strong class="kd iu"> <em class="my"> 15%的问题</em> </strong>)一个问题可能类似于“选择所有变换(宽/窄)或动作的备选项”，所以你需要确保你认识到它们中的大多数。你在<a class="ae kz" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations" rel="noopener ugc nofollow" target="_blank"> Spark 文档</a>里有很好的解释。</p><p id="1679" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">另一个重要的主题是很好地理解这些主题:</p><ul class=""><li id="b5d1" class="nh ni it kd b ke kf ki kj km nj kq nk ku nl ky nm nn no np bi translated">广播变量</li></ul><pre class="md me mf mg gt mo mp mq mr aw ms bi"><span id="f3a6" class="mt lb it mp b gy mu mv l mw mx">broadcastVar = sc.broadcast([1, 2, 3])<br/>broadcastVar.value</span></pre><ul class=""><li id="126c" class="nh ni it kd b ke kf ki kj km nj kq nk ku nl ky nm nn no np bi translated">累加器</li><li id="f88e" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">RDD 持久性</li><li id="2f0b" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">将功能传递给 Spark</li><li id="a230" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">合并，重新分配</li></ul><pre class="md me mf mg gt mo mp mq mr aw ms bi"><span id="2932" class="mt lb it mp b gy mu mv l mw mx">What is the method of persistence cache()?</span><span id="e7bd" class="mt lb it mp b gy mz mv l mw mx">MEMORY_ONLY<br/>MEMORY_AND_DISK<br/>DISK_ONLY<br/>OFF_HEAP</span></pre><h1 id="3a83" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">建议 9:不要浪费时间构建 Spark 环境或获取培训数据</h1><p id="28e2" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">是啊！我知道你想学习 Medium 上的许多精彩的<a class="ae kz" href="https://medium.com/@GalarnykMichael/install-spark-on-windows-pyspark-4498a5d8d66c" rel="noopener">教程</a>,但是为了准备这次考试，我强烈建议你选择其中一个选项，让你专注于内容而不是配置。我更喜欢<strong class="kd iu"> Databricks </strong>，因为你得到一个配置好的小型 Spark 集群，可以免费开始练习。</p><div class="oa ob gp gr oc od"><a href="https://community.cloud.databricks.com/login.html" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">数据块-登录</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">编辑描述</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">community.cloud.databricks.com</p></div></div></div></a></div><div class="oa ob gp gr oc od"><a href="https://colab.research.google.com/notebooks/welcome.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">谷歌联合实验室</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">编辑描述</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">colab.research.google.com</p></div></div><div class="om l"><div class="on l oo op oq om or jv od"/></div></div></a></div><div class="oa ob gp gr oc od"><a href="https://notebooks.azure.com/" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">微软 Azure 笔记本电脑</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">提供对运行在微软 Azure 云上的 Jupyter 笔记本的免费在线访问。</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">notebooks.azure.com</p></div></div></div></a></div><div class="oa ob gp gr oc od"><a href="https://www.kaggle.com/kernels" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">内核| Kaggle</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">编辑描述</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">www.kaggle.com</p></div></div></div></a></div><p id="faf7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你需要一些数据来练习，我推荐这个<a class="ae kz" href="https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data" rel="noopener ugc nofollow" target="_blank"> Github 仓库</a>，在那里你可以有 CSV，JSONs，Parquet，ORC 文件。</p><h1 id="080f" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">建议 10:如果你是 Spark 的新手，可以考虑阅读这些书籍</h1><p id="f204" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">您在这条路上了解 Apache Spark 的朋友有:</p><ul class=""><li id="f53b" class="nh ni it kd b ke kf ki kj km nj kq nk ku nl ky nm nn no np bi translated"><a class="ae kz" href="https://amzn.to/33Q96VR" rel="noopener ugc nofollow" target="_blank">火花:权威指南</a></li><li id="9515" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated"><a class="ae kz" href="https://amzn.to/32RDBt0" rel="noopener ugc nofollow" target="_blank">学习火花</a></li><li id="af08" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated"><a class="ae kz" href="http://spark.apache.org/docs/latest/quick-start.html" rel="noopener ugc nofollow" target="_blank">火花文件</a></li><li id="3337" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated"><a class="ae kz" href="https://www.linkedin.com/pulse/5-tips-cracking-databricks-apache-spark-certification-vivek-bombatkar" rel="noopener ugc nofollow" target="_blank">破解 Databricks Apache Spark 认证的 5 个小技巧</a>。</li></ul><p id="d98e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，您已经准备好成为一名认证的 Apache Spark 开发人员:)</p><figure class="md me mf mg gt jt gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi os"><img src="../Images/7e7fd96d85f95820a7553435373a3973.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQn1UH8NoM6lgggn3qiZhQ.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Always keep learning</figcaption></figure><p id="c079" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">PS 如果你有任何问题，或者想要澄清一些事情，你可以在<a class="ae kz" href="https://twitter.com/thony_ac77" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae kz" href="https://www.linkedin.com/in/antoniocachuan/" rel="noopener ugc nofollow" target="_blank"> LinkedIn 上找到我。</a>另外，如果你想探索云认证，我最近发表了一篇关于<strong class="kd iu">谷歌云认证挑战</strong>的文章。</p><div class="oa ob gp gr oc od"><a rel="noopener follow" target="_blank" href="/how-i-could-achieve-the-google-cloud-certification-challenge-6f07a2993197"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">我如何完成谷歌云认证挑战？</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">在按照谷歌推荐的 12 周准备后，我通过了云工程师助理考试，在这里…</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">towardsdatascience.com</p></div></div><div class="om l"><div class="ot l oo op oq om or jv od"/></div></div></a></div></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><p id="8a20" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">更新:</strong>该认证将于 10 月 19 日截止，现在面向 Apache Spark 2.4 的 Databricks 认证助理开发人员提供相同的主题(重点关注 Spark 架构、SQL 和数据框架)</p><p id="73a6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">更新 2(2021 年初)</strong> : Databricks 现在还提供 Apache Spark 3.0 考试的 Databricks 认证助理开发人员。与 Spark 2.4 考试相比，Spark 3.0 考试还询问了 Spark 3.0 中的新功能，如自适应查询执行。我强烈推荐这些优秀的 Databricks 认证开发人员准备 Spark 3.0 实践考试[链接:【https://bit.ly/sparkpracticeexams】T2]。模拟考试包括解释和文档的静态 PDF，类似于真实考试中的内容。</p></div></div>    
</body>
</html>