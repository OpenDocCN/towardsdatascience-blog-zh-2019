<html>
<head>
<title>A Beginner’s Guide to Reinforcement Learning using Rock-Paper-Scissors and Tensorflow.js</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用石头剪刀布和 Tensorflow.js 进行强化学习的初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-reinforcement-learning-using-rock-paper-scissors-and-tensorflow-js-37d42b6197b5?source=collection_archive---------17-----------------------#2019-10-21">https://towardsdatascience.com/a-beginners-guide-to-reinforcement-learning-using-rock-paper-scissors-and-tensorflow-js-37d42b6197b5?source=collection_archive---------17-----------------------#2019-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2407" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Tensorflow.js 刚刚发布，我刚刚参加了一个关于它在浏览器中的用例的讲座。我有在 Python 中广泛使用 Tensorflow 的经验，但我很好奇在浏览器中构建一个小型强化学习示例有多困难，在这个示例中，您可以看到一个代理随着时间的推移而学习。</p><p id="3bc0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我选择了石头剪子布，出于简洁和懒惰的原因，从现在开始它将被称为 RPS，因为它的游戏规则很简单，并且因为我知道学习只需要很少的例子。</p><p id="683a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本教程面向谁:任何对强化学习(RL)和神经网络的直观非数学教程感兴趣的人。他们应该有 JavaScript 和 html 的基本知识来理解代码，但这不是理解概念部分所必需的。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="45cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目标是:构建一个能够使用强化学习和神经网络来学习 RPS 规则的代理。这意味着如果用户选择剪刀，我们希望代理能够选择石头。</p><p id="fe35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">强化学习直观上可以描述如下:</p><p id="45c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">循环:基于信念做某事→获得积极或消极的奖励→基于奖励更新信念</p><p id="d66a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，当构建强化学习问题时，你必须记住上面的循环。所以我们可以用同样的方式分解 RPS 游戏。</p><ol class=""><li id="b7c2" class="ks kt iq jp b jq jr ju jv jy ku kc kv kg kw kk kx ky kz la bi translated">当用户选择一步棋时，代理有一些关于它应该选择哪一步棋的信念。即，当用户选择纸张时，代理认为它应该选择石头。</li><li id="46d0" class="ks kt iq jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated">用户对代理选择的移动给予积极或消极的奖励。</li><li id="ecab" class="ks kt iq jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated">当用户选择纸张时，代理更新它关于是否应该选择石头的信念。</li></ol><p id="a395" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">记住这一点，本教程分为两个部分:</p><ol class=""><li id="e900" class="ks kt iq jp b jq jr ju jv jy ku kc kv kg kw kk kx ky kz la bi translated">script.js，它将包含执行强化学习周期的代码。</li><li id="8fbf" class="ks kt iq jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated">index.html 将允许用户与学习代理交互，并实时可视化学习。</li></ol><p id="96cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将从 script.js 开始。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Choose Move</figcaption></figure><p id="846b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">学习周期的开始是“根据代理的信念做一些事情”。在这个上下文中，这意味着根据用户的移动选择一个移动。</p><p id="c2ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 6–12 行:在评估阶段，我们想看看代理学得有多好。因此，选择的移动是具有最高值的移动。即代理最有信心的移动。这是借助神经网络完成的。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="ec8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">旁白:神经网络</p><p id="843e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络是一种可以调整的功能。它接受一些输入，对输入做一些事情，然后输出一些结果。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/ae4b03ffe9ba40fd6b67a57d034ffd6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bn0BO9Fn9NzmZTFswjZu8Q.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Function</figcaption></figure><p id="7cba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 RL 和 RPS 的上下文中，使用神经网络来表示代理的信念。网络的输入是用户的移动，输出是代理在选择石头、布、剪刀时的信心。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/836ab1b58cac1cb49dca4be58994661e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xAVG64jxGXnJaSWUnEjfmw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Neural Network for Agent’s Beliefs</figcaption></figure><p id="e978" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上面可以看出，用户的举动是摇滚。代理人 0.7 自信应该选剪刀，0.1 自信选石头，0.2 自信选纸。如果我们处于评估阶段，代理将选择价值最高/最有信心的行动。即剪刀。</p><p id="90b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 8–12 行:将移动转换为神经网络可以理解的格式，向网络请求其输出，然后选择具有最高值的移动。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="76f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 14–16 行:在训练阶段，不是基于信念选择移动，而是随机选择移动。这使得代理可以探索所有的移动，而不是总是选择相同的移动。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Update beliefs</figcaption></figure><p id="cc4c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">强化循环的下一阶段是从用户那里获得奖励，奖励将在 index.html 处理。收到奖励后，代理可以更新它的信念。</p><p id="93d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 5–7 行:处理获取代理对用户移动应该选择的移动的信心</p><p id="6417" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 10-12 行:用奖励更新代理的信念。这是使用神经网络完成的。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="379d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">旁白:更新神经网络</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lz"><img src="../Images/f328f81baaeb6eacb7dcbb6e8f9f8095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xdHdhvJyM2dVlVNPT5fDtw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Updating a Neural Network</figcaption></figure><p id="56f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个场景中，用户选择了石头，代理选择了剪刀。用户给出了-100 的奖励，因为这一步棋是错误的。然后将-100 加到 0.7 上，并送回网络。这将告诉网络 0.7 的值太高，应该降低一些。这个过程叫做反向传播。这是更新代理人信念的行为。在这种情况下，代理应该对选择剪刀不太有信心，而对选择石头或布更有信心。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="11c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 13 行:一旦网络被更新，新的信念就被绘制出来。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Plot the beliefs of the Agent</figcaption></figure><p id="9d9b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种方法遍历用户可以选择的所有移动，并为用户的每个移动绘制代理的信念。</p><p id="7219" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 10 行:应用一个函数使神经网络的输出总和为 1。这使得用户能够更容易地理解神经网络的输出。</p><p id="d4ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 25 行:使用 Plotly.js 绘制 index.html 每个 div 中的数据集</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/473a401dba60a0470df5162d2db9d5ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*ODN6vxEk5kgbg1Z0TSnNdw.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Example output of the Network</figcaption></figure><p id="5c2d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经完成了强化循环和可视化部分的所有部分。接下来我们将描述 index.html。这需要做几件事:</p><ol class=""><li id="80f5" class="ks kt iq jp b jq jr ju jv jy ku kc kv kg kw kk kx ky kz la bi translated">允许用户选择移动。</li><li id="d37c" class="ks kt iq jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated">用户应该能够说出代理的移动是好是坏。</li><li id="9cf6" class="ks kt iq jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated">在学习代理随机选择移动和选择它最有信心的移动之间切换的能力。</li><li id="4755" class="ks kt iq jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated">可视化代理的信念，这意味着当用户选择一个移动时，显示代理在选择一个特定移动时有多自信</li></ol><p id="e16d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下节选自 index.html，反映了上述观点。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">User interface</figcaption></figure><p id="4835" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 1–3 行:有三个按钮是用户移动的，单击它们将调用 chooseMove 函数并向它传递按钮的值。这将使代理选择并返回一个移动。</p><p id="096d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 5–6 行:当用户决定代理选择的移动是好是坏时，这些按钮将调用训练功能。这将告诉代理积极或消极地更新它的信念。</p><p id="2792" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 8–11 行:切换代理是正在学习还是正在评估其所学内容</p><p id="1891" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 13-17 行:包含 div，这些 div 将用于描绘代理对每个动作的信念。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="c627" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">至此，本教程到此结束。要查看所有代码，请访问这个<a class="ae mb" href="https://github.com/sachag678/freeCodeCamp" rel="noopener ugc nofollow" target="_blank">库</a>。你可以随意克隆它并使用代码。克隆完成后，在浏览器中打开 index.html。您将看到下图所示的设置。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mc"><img src="../Images/364acb7db58af1a37825bc0a6f6c5314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AtZHsTZhy8u9u9VEBKz_uw.png"/></div></div></figure><p id="689f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最上面的按钮是用户的移动。一旦您选择了移动，代理将随机选择一个移动。然后，用户可以点击正面奖励或负面奖励按钮，信念将实时更新。单击新的移动并重复该过程。一旦你对学习到的行为感到满意，将开关从训练切换到评估，然后代理选择的移动将是每个直方图中最高值的移动。</p><p id="f3cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望本教程对您有所帮助，并让您直观地了解如何构建强化学习问题，以及如何使用神经网络来帮助解决这个问题。</p><p id="a1d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请继续关注神经网络和强化学习基础的进一步教程。</p><p id="eaff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读。</p></div></div>    
</body>
</html>