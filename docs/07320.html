<html>
<head>
<title>Ensemble learning: A case study from the 1994 US Census database</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习:来自 1994 年美国人口普查数据库的案例研究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ensemble-learning-a-case-study-from-the-1994-us-census-database-8180342819fe?source=collection_archive---------24-----------------------#2019-10-14">https://towardsdatascience.com/ensemble-learning-a-case-study-from-the-1994-us-census-database-8180342819fe?source=collection_archive---------24-----------------------#2019-10-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/91bb06d92dac39fa5463760892550293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADwxprmz-Bq141nwhBHpzw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by 3dman_eu via <a class="ae kf" href="https://www.needpix.com/photo/489798/map-usa-flag-borders-country-states-of-america-land-borders-free-pictures-free-photos" rel="noopener ugc nofollow" target="_blank">Needpix</a>.</figcaption></figure><p id="07b9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated">在我们开始理解什么是<strong class="ki iu">集成学习</strong>以及它是如何工作的之前，我们需要知道在这个案例研究中使用的数据集。</p><p id="f8ab" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用的数据集是<strong class="ki iu"> 1994 年美国人口普查数据库</strong>的子集，该数据库由<em class="ln"> Barry Becker </em>提供，可在<a class="ae kf" href="http://archive.ics.uci.edu/ml/datasets/Adult" rel="noopener ugc nofollow" target="_blank"> UCI 机器学习库</a>获得。该库的预测任务是确定一个<strong class="ki iu"> <em class="ln">人一年的收入是否超过 50K</em></strong>，为此提供了以下属性和值:</p><ul class=""><li id="26f2" class="lo lp it ki b kj kk kn ko kr lq kv lr kz ls ld lt lu lv lw bi translated"><strong class="ki iu">高收入</strong>:目标阶层。</li><li id="284d" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">年龄</strong>:连续。</li><li id="2270" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">工作类别</strong>:私人，自我雇佣非公司，自我雇佣公司，联邦政府，地方政府，州政府，无薪，从未工作。</li><li id="d4d4" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated">fnlwgt :连续。</li><li id="f241" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">教育</strong>:学士、部分大学、11 年级、HS-grad、Prof-school、Assoc-acdm、Assoc-voc、9 年级、7-8 年级、12 年级、硕士、1-4 年级、10 年级、博士、5-6 年级、学前教育。</li><li id="2e9c" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">教育编号</strong>:连续。</li><li id="fe86" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">婚姻状况</strong>:已婚配偶、离婚、未婚、分居、丧偶、无配偶、已婚配偶。</li><li id="725c" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated">职业:技术支持、工艺修理、其他服务、销售、行政管理、专业、搬运工人、清洁工、机器操作员、检查员、行政文员、农业、渔业、运输、私人服务、保安服务、武装部队。</li><li id="3839" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">关系</strong>:妻子、亲生子女、丈夫、非家庭成员、其他亲属、未婚。</li><li id="3219" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated">种族:白人，亚洲太平洋岛民，美洲印第安爱斯基摩人，其他人，黑人。</li><li id="b68b" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">性别</strong>:女，男。</li><li id="8df0" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">资本收益</strong>:持续。</li><li id="aaae" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">资本损失</strong>:持续。</li><li id="e0b1" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">每周小时数</strong>:连续。</li><li id="28c3" class="lo lp it ki b kj lx kn ly kr lz kv ma kz mb ld lt lu lv lw bi translated"><strong class="ki iu">本土国家</strong>:美国、柬埔寨、英国、波多黎各、加拿大、德国、美国外围地区(关岛-USVI 等)、印度、日本、希腊、韩国、中国、古巴、伊朗、洪都拉斯、菲律宾、意大利、波兰、牙买加、越南、墨西哥、葡萄牙、爱尔兰、法国、多米尼加共和国、老挝、厄瓜多尔、台湾、海地、哥伦比亚、匈牙利、危地马拉、尼加拉瓜、苏格兰、泰国、南斯拉夫、萨尔瓦多、特立尼达岛&amp;多巴哥、秘鲁、香港、荷兰。</li></ul></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="a7c1" class="mj mk it bd ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bi translated">什么是集成学习？</h1><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a67a8b23cb2ae80943bea94530e2f7a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*eNm0qJKZVNjE4pJuttzvtQ.gif"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo via <a class="ae kf" href="http://www.verdevalleynursery.com/tree/" rel="noopener ugc nofollow" target="_blank">Verde Valley Nursery</a></figcaption></figure><blockquote class="nm"><p id="af0d" class="nn no it bd np nq nr ns nt nu nv ld dk translated"><em class="nw">集成学习</em>是一种结合其他机器学习模型来优化和创建更好模型的技术。</p></blockquote><p id="86a4" class="pw-post-body-paragraph kg kh it ki b kj nx kl km kn ny kp kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">有一些不同类型的集合方法，如:助推，堆叠，<strong class="ki iu">粘贴，装袋</strong>和<strong class="ki iu">随机森林</strong>。在本案例研究中，重点是最后两种方法。</p><h1 id="b0de" class="mj mk it bd ml mm oc mo mp mq od ms mt mu oe mw mx my of na nb nc og ne nf ng bi translated">装袋粘贴:是什么？它是如何工作的？</h1><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a24bd51601d022769a2afb4f4913f42b.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*OYUcalZ95e3A97tUdpOIuA.gif"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo via <a class="ae kf" href="https://giphy.com/gifs/season-12-the-simpsons-12x17-3orieTU2tBje4i5SzS" rel="noopener ugc nofollow" target="_blank">Giphy</a></figcaption></figure><p id="222b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="ln">B</em></strong><em class="ln">ootstrap</em><strong class="ki iu"><em class="ln">Agg</em></strong><em class="ln">regat</em><strong class="ki iu"><em class="ln">ing</em></strong>或<strong class="ki iu"> <em class="ln"> Bagging </em> </strong>是一种结合了自举和聚合方法的技术。第一种方法是用替换将数据集分成 n 个子集，第二种方法的思想是创建 n 个模型，每个子集一个模型，然后将它们聚集在一起以产生最终预测。</p><p id="2c8c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">粘贴</strong>方法的工作原理与装袋相似，但不同之处在于引导步骤，在该步骤中，分割是在没有替换的情况下进行的。</p><h1 id="f8d5" class="mj mk it bd ml mm oc mo mp mq od ms mt mu oe mw mx my of na nb nc og ne nf ng bi translated">那随机森林呢？</h1><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/f4037c54b97c2cb1633e151d66e2c1b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*3WpN3yJBrmLZ6kFX1IGNjw.gif"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo via <a class="ae kf" href="https://dribbble.com/shots/2408458-Mario-Blocks" rel="noopener ugc nofollow" target="_blank">Dribbble</a></figcaption></figure><p id="6209" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机森林是一种特殊类型的集成算法，它使用多个<strong class="ki iu">决策树</strong>来构建其模型。每棵树都用数据集的不同部分来训练。通常，这种分割与 bagging 技术类似，最终模型由几个决策树模型组成，这些模型组合起来产生模型的预测。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="88e8" class="mj mk it bd ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bi translated">嗷，该编码了！</h1><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/956ac656f3796cb4e55a49d8de0ae8dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*NWOXasFgnzlOVy6wh_mWtg.gif"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Via <a class="ae kf" href="https://camo.githubusercontent.com/d8ad845585368b53a7e79c7bd89c401df6c8cdd1/687474703a2f2f692e67697068792e636f6d2f4d50436763613145755a6c4c692e676966" rel="noopener ugc nofollow" target="_blank">Github</a></figcaption></figure><p id="5537" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这个项目，我们将使用<strong class="ki iu"> python </strong>作为编程语言，库:<strong class="ki iu"> pandas </strong>和<strong class="ki iu"> scikit-learn </strong>。首先，让我们了解一下我们数据的<em class="ln">面</em>:</p><pre class="ni nj nk nl gt oj ok ol om aw on bi"><span id="7d9c" class="oo mk it ok b gy op oq l or os"># load the dataset<br/>income = pd.read_csv("income.csv")<br/>income.head()</span></pre><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ot"><img src="../Images/470c0830bc5488b6c23dafb9b7c46dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmZDQV_GZPNTSLrnY95xQA.png"/></div></div></figure><p id="f4e0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在继续之前，我们需要对数据进行预处理，将分类变量转换为数值变量。在这个过程之后，这是一个数据样本:</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ou"><img src="../Images/487365935eb379dcfddbc08301e86001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qMTpbhFNs4icz7AfZZbqHA.png"/></div></div></figure><p id="ee95" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将使用两个伟大的 scikit-learn 特性，称为<em class="ln">管道</em>和<em class="ln"> GridSearchCV </em>，它们允许我们自动测试不同模型的几个超参数。</p><pre class="ni nj nk nl gt oj ok ol om aw on bi"><span id="633a" class="oo mk it ok b gy op oq l or os"># split-out train/validation and test dataset<br/>X_train, X_test, y_train, y_test = train_test_split(income.drop(labels="high_income",axis=1),                                                    income["high_income"],test_size=0.20,random_state=seed,shuffle=True,stratify=income["high_income"])</span></pre></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><pre class="oj ok ol om aw on bi"><span id="c168" class="oo mk it ok b gy ov ow ox oy oz oq l or os"># The full pipeline as a step in another pipeline with an estimator as the final step<br/>pipe = Pipeline(steps = [("clf",RandomForestClassifier())])</span><span id="36bd" class="oo mk it ok b gy pa oq l or os"># create a dictionary with the hyperparameters<br/>search_space = [{"clf":[DecisionTreeClassifier()],<br/>                 "clf__criterion": ["gini","entropy"],<br/>                 "clf__splitter": ["best","random"],<br/>                 "clf__random_state": [seed],<br/>                 "fs__score_func":[chi2],<br/>                 "fs__k":[4,6,8]},<br/>                {"clf":[RandomForestClassifier()],<br/>                 "clf__n_estimators": [200,300],<br/>                 "clf__criterion": ["gini","entropy"],<br/>                 "clf__max_leaf_nodes": [32,64,128],<br/>                 "clf__random_state": [seed],<br/>                 "fs__score_func":[chi2],<br/>                 "fs__k":[4,6,8]},<br/>                {'clf' [BaggingClassifier(DecisionTreeClassifier(random_state=42))],<br/>                 "clf__base_estimator__criterion": ['gini','entropy'],<br/>                 "clf__base_estimator__splitter": ['best','random'],<br/>                 "clf__oob_score": [True],<br/>                 "clf__n_estimators": [200,300],<br/>                 "clf__bootstrap":[True],<br/>                 "fs__score_func":[chi2],<br/>                 "fs__k":[4,6,8]},<br/>                {'clf': [BaggingClassifier(DecisionTreeClassifier(random_state=42))],<br/>                 "clf__base_estimator__criterion": ['gini','entropy'],<br/>                 "clf__base_estimator__splitter": ['best','random'],<br/>                 "clf__oob_score": [False],<br/>                 "clf__n_estimators": [200,300],<br/>                 "clf__bootstrap":[False],<br/>                 "fs__score_func":[chi2],<br/>                 "fs__k":[4,6,8]}]</span><span id="e745" class="oo mk it ok b gy pa oq l or os"># create grid search<br/>kfold = KFold(n_splits=num_folds,random_state=seed)</span><span id="f865" class="oo mk it ok b gy pa oq l or os">grid = GridSearchCV(estimator=pipe, <br/>                    param_grid=search_space,<br/>                    cv=kfold,<br/>                    scoring=scoring,<br/>                    return_train_score=True,<br/>                    n_jobs=-1,<br/>                    refit="AUC", verbose=10)</span></pre></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><pre class="oj ok ol om aw on bi"><span id="68a5" class="oo mk it ok b gy ov ow ox oy oz oq l or os"># fit grid search<br/>best_model = grid.fit(X_train,y_train)</span></pre><p id="a162" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种配置允许算法使用所有可用的核心，这将以并行方式测试 960 个不同配置的决策树、随机森林和 bagging 分类器(以决策树作为内部模型)。</p><p id="d71f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此过程之后，GridSearchCV 生成的最佳模型是一个随机森林模型，具有以下配置:</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pb"><img src="../Images/41f13f035bfcdc9678139be7a3d01ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9nQZtE0Ez-1NNlXZSEqt4A.png"/></div></div></figure><p id="ddac" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们定义这是否是最佳模型之前，让我们检查模型的准确性以训练和测试数据集。这种比较的目的是验证模型是<strong class="ki iu">欠配</strong>还是<strong class="ki iu">过配</strong>。</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/9bbcb64c6262cd378e93b7f4c8500578.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*h53Fkm0u12VGqucimAnTCg.png"/></div></figure><p id="0935" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们在上面看到的，两个集合的精度都很好，更重要的是，训练和测试精度的值非常接近。因此，这个结果表明 GridSearchCV 产生的最佳模型具有很好的通用性。</p><p id="f67e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在随机森林分类器中，我们可以设置一个名为<strong class="ki iu"> bootstrap </strong>的超参数，它定义天气样本是否将被替换训练。尽管选择的最佳模型的参数为<em class="ln">假</em>，这试图帮助模型最小化<strong class="ki iu">过拟合</strong>的机会，但当参数设置为<strong class="ki iu">真、</strong>时，其他几个模型也呈现出类似的结果，如下图所示。</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/f9d9e6fee22e2f95e954a24fddfea608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*QBDgEbegUCkZMRWpLlJ_Ow.png"/></div></figure><p id="c7d8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，对于这个数据集，不管 bootstrap 变量的值如何，我们都取得了很好的结果。然而，由于可能的过度拟合，bootstrap 等于 true 时会出现最坏的结果。</p><h1 id="74da" class="mj mk it bd ml mm oc mo mp mq od ms mt mu oe mw mx my of na nb nc og ne nf ng bi translated">了解特性的重要性</h1><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/78556537fc52b0e84ad72a998509c903.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*ChuD_QiFeqNFWE17sXaazQ.gif"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Via <a class="ae kf" href="https://giphy.com/gifs/season-12-the-simpsons-12x17-xT5LMF4nsy5F8lvxGU" rel="noopener ugc nofollow" target="_blank">Giphy</a></figcaption></figure><p id="ecd6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们检查数据集的每个特征对于我们的模型的重要性。对于这个任务，我们使用了两个工具:特征重要性，来自随机森林分类器和库<strong class="ki iu">SHAP</strong><em class="ln">(</em><strong class="ki iu"><em class="ln">SH</em></strong><em class="ln">apley</em><strong class="ki iu"><em class="ln">A</em></strong><em class="ln">加性 ex</em><strong class="ki iu"><em class="ln">P</em></strong><em class="ln">lanations)</em>其中<em class="ln"> </em>是解释任何机器输出的统一方法</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pe"><img src="../Images/65ffa3821c26b3393e157ad0655c08a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VjAPiJXM6IK6oXhttiqo1Q.png"/></div></div></figure><p id="16fb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">左图由 scikit-learn 的 feature_importances 制作，右图由 SHAP 制作。重要的是要看到两个结果呈现相似的结果，并且 4 个最重要的特征是相同的，仅改变了位置。有了这些结果，就很容易知道对模型影响最大的信息，这极大地有助于理解和解决问题。</p></div></div>    
</body>
</html>