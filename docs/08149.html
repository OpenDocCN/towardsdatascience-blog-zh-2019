<html>
<head>
<title>A Quick Primer on Databricks Koalas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">树袋熊数据快速入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-quick-primer-on-databricks-koalas-38c89369cf9f?source=collection_archive---------26-----------------------#2019-11-07">https://towardsdatascience.com/a-quick-primer-on-databricks-koalas-38c89369cf9f?source=collection_archive---------26-----------------------#2019-11-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="391a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与 Spark 数据框互动熊猫词汇</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/8854751c9f0007a6313ef1cfc276db38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZhCaZm6l_L4yhsU0Le0FEg.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@jwwhitt?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jordan Whitt</a> on <a class="ae le" href="https://unsplash.com/s/photos/koala?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="297c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我的一个项目中，我广泛使用 Spark 来管理一些大型数据文件。尽管它通常以使用大型分布式系统的诸多好处而闻名，但它在本地同样适用于处理大量信息的项目。</p><p id="95c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我经常使用熊猫进行数据处理，但有时 Spark 会让它相形见绌。你可以对 Pandas 使用 chunksize，但是根据我的经验，即使你找到了合适的大小来管理，它也比 Spark 数据帧要慢得多，而且要消耗更多的内存。从 Pandas 迁移到 PySpark(使用 Spark 的 Python API)的挑战是导航数据帧的词汇是不同的。我真的不介意摸索着与包交互，但是 PySpark 的错误消息通常很隐晦，文档也不是很好。在某些情况下，它就像编写一个快速的 SQL 查询一样简单，但是通常情况下，您需要进行更新和创建函数，并且这些部分所需的语言变得更加复杂。</p><p id="8779" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">考拉入场！<a class="ae le" href="https://databricks.com/blog/2019/04/24/koalas-easy-transition-from-pandas-to-apache-spark.html" rel="noopener ugc nofollow" target="_blank">今年早些时候由 Databricks 推出的</a>，考拉很容易将熊猫的相同知识应用到 Spark 数据框架中。从类似的导入开始，创建一个 Spark 会话:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="280d" class="lk ll it lg b gy lm ln l lo lp">import pandas as pd<br/>import numpy as np<br/>import databricks.koalas as ks<br/>from pyspark.sql import SparkSession</span></pre><p id="f8ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">树袋熊数据框可以通过多种不同方式创建:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="f2ed" class="lk ll it lg b gy lm ln l lo lp"># Dataframe from scratch<br/>koala_df = ks.DataFrame(<br/>    {'type': ['panda', 'koala', 'grizzly', 'brown'],<br/>     'color': ['black/white', 'grey', 'brown', 'brown'],<br/>      'score': 5, 1, 5, np.nan},<br/>    index=[1, 2, 3, 4])</span><span id="2a70" class="lk ll it lg b gy lq ln l lo lp"># From a file<br/>koala_df = <!-- -->ks.read_csv("all_the_bears.csv", header=0)</span><span id="6458" class="lk ll it lg b gy lq ln l lo lp"># From an existing Spark dataframe<br/>koala_df = spark_df.to_koalas()</span><span id="6e99" class="lk ll it lg b gy lq ln l lo lp"># From an existing Pandas dataframe<br/>koala_df = ks.from_pandas(pandas_df)</span></pre><p id="ba6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为一个树袋熊的数据框架，你可以接触到和熊猫一样的语言:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="9552" class="lk ll it lg b gy lm ln l lo lp"># Get first rows of dataframe<br/>koala_df.head()</span><span id="4527" class="lk ll it lg b gy lq ln l lo lp"># See the column names<br/>koala_df.columns</span><span id="60c9" class="lk ll it lg b gy lq ln l lo lp"># Quick summary stats<br/>koala_df.describe()</span><span id="0252" class="lk ll it lg b gy lq ln l lo lp"># Convert to Numpy<br/>koala_df.to_numpy()</span><span id="d036" class="lk ll it lg b gy lq ln l lo lp"># Group rows<br/>koala_df.groupby('score')</span></pre><p id="4bf2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它还提供了一些在 PySpark 中很麻烦的数据清理特性:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="3411" class="lk ll it lg b gy lm ln l lo lp"># Drop rows with missing values<br/>koala_df.dropna(how='any')</span><span id="d36e" class="lk ll it lg b gy lq ln l lo lp"># Fill missing values<br/>koala_df.fillna(value=5)</span></pre><p id="247c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我最喜欢的功能之一是轻松导出，这在 Spark 中肯定会很时髦:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="4a0a" class="lk ll it lg b gy lm ln l lo lp"># Export to csv<br/>koala_df.to_csv("bear_bears.csv")</span></pre><p id="1c50" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为一个不断学习和寻求提高技能的人，对我来说最重要的是，它使使用 Pandas 的本地项目更容易过渡到使用 Spark 的更具可扩展性的项目。我建议你用它弄脏你的手，看看你有什么想法。现在去下载一个 10GB 的 json 文件，尽情享受吧！</p></div></div>    
</body>
</html>