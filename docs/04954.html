<html>
<head>
<title>Image Panorama Stitching with OpenCV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 OpenCV 实现图像全景拼接</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-panorama-stitching-with-opencv-2402bde6b46c?source=collection_archive---------0-----------------------#2019-07-26">https://towardsdatascience.com/image-panorama-stitching-with-opencv-2402bde6b46c?source=collection_archive---------0-----------------------#2019-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/b1a0a8137a8e4af0df715b171cf2b4c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-gSGgtSXv-876HIdIywcg.jpeg"/></div></div></figure><p id="0fe7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">图像拼接是计算机视觉中最成功的应用之一。如今，很难找到不包含此功能的手机或图像处理 API。</p><p id="d910" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这篇文章中，我们将讨论如何使用 Python 和 OpenCV 执行图像拼接。给定一对共享一些公共区域的图像，我们的目标是“缝合”它们并创建一个全景图像场景。</p><p id="1972" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在整篇文章中，我们回顾了一些最著名的计算机视觉技术。其中包括:</p><ul class=""><li id="1835" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">关键点检测</li><li id="40ee" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">局部不变描述符(SIFT、SURF 等)</li><li id="28c7" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">特征匹配</li><li id="a215" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">使用 RANSAC 的单应性估计</li><li id="de6f" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">透视扭曲</li></ul><p id="193f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们探索了许多特征提取器，如 SIFT、SURF、BRISK 和 ORB。你可以使用这款<a class="ae ln" href="https://colab.research.google.com/drive/11Md7HWh2ZV6_g3iCYSUw76VNr4HzxcX5" rel="noopener ugc nofollow" target="_blank"> Colab 笔记本</a>进行跟踪，甚至用你的图片进行尝试。</p><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lo"><img src="../Images/29625ee2f641997b03a17a23e062b252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*metT9TUZ4TAu4lTvA5Vgqg.png"/></div></div></figure><h1 id="80fd" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">特征检测和提取</h1><p id="82c1" class="pw-post-body-paragraph kb kc it kd b ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku mv kw kx ky im bi translated">给定一对像上面这样的图像，我们想把它们拼接起来，创建一个全景场景。需要注意的是，两个图像需要共享一些公共区域。</p><p id="09e6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此外，我们的解决方案必须是健壮的，即使图片在以下一个或多个方面有差异<strong class="kd iu"/>:</p><ul class=""><li id="216e" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">缩放比例</li><li id="4d6d" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">角</li><li id="09cc" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">空间位置</li><li id="96bb" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">捕捉设备</li></ul><p id="d68d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个方向的第一步是提取一些关键点和感兴趣的特征。然而，这些特征需要具有一些特殊的属性。</p><p id="641e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们首先考虑一个简单的解决方案。</p><h1 id="1881" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">关键点检测</h1><p id="931c" class="pw-post-body-paragraph kb kc it kd b ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku mv kw kx ky im bi translated">一个最初的可能是幼稚的方法是使用算法提取关键点，比如 Harris Corners。然后，我们可以尝试基于某种相似性度量(如欧几里德距离)来匹配相应的关键点。我们知道，角点有一个很好的性质:<strong class="kd iu">它们对于旋转</strong>是不变的。这意味着，一旦我们检测到一个角点，如果我们旋转图像，这个角点将仍然存在。</p><p id="176b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，如果我们旋转然后缩放一幅图像呢？在这种情况下，我们将有一段艰难的时间，因为角落不是不变的比例。也就是说，如果我们放大一幅图像，之前检测到的角点可能会变成一条线！</p><p id="e74a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">总之，我们需要对旋转和缩放不变的特征。这就是 SIFT、SURF 和 ORB 等更健壮的方法的用武之地。</p><h1 id="80ce" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">要点和描述符。</h1><p id="16ef" class="pw-post-body-paragraph kb kc it kd b ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku mv kw kx ky im bi translated">像 SIFT 和 SURF 这样的方法试图解决角点检测算法的局限性。通常，角点检测算法使用固定大小的核来检测图像上的感兴趣区域(角点)。很容易看出，当我们缩放图像时，这个内核可能会变得太小或太大。</p><p id="a7c6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了解决这个限制，像 SIFT 这样的方法使用高斯差分(DoD)。这个想法是将 DoD 应用于同一幅图像的不同比例版本。它还使用相邻像素信息来寻找和改进关键点和相应的描述符。</p><p id="f47d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，我们需要加载两个图像，一个查询图像和一个训练图像。最初，我们从提取关键点和描述符开始。我们可以使用 OpenCV<em class="mw">detectAndCompute()</em>函数一步完成。注意，为了使用<em class="mw"> detectAndCompute() </em>，我们需要一个关键点检测器和描述符对象的实例。可以是 ORB，SIFT 或者 SURF 等。此外，在将图像输入<em class="mw"> detectAndCompute() </em>之前，我们将它们转换成灰度。</p><figure class="lp lq lr ls gt ju"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="e0c8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们在查询和火车图像上运行<em class="mw"> detectAndCompute() </em>。此时，我们有了两幅图像的一组关键点和描述符。如果我们使用 SIFT 作为特征提取器，它为每个关键点返回一个 128 维的特征向量。如果选择 SURF，我们得到一个 64 维的特征向量。下图显示了使用 SIFT、SURF、BRISK 和 ORB 提取的一些特征。</p><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/124fc70c00ec233973e8f6e0162e1754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BXftFIcXuXCkjjZoVmSMlQ.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Detection of key points and descriptors using SIFT</figcaption></figure><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/aecc57b16bbd46ee83211d9c5da98647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L6DB36_D16yY2wLu-3RDTQ.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Detection of key points and descriptors using SURF</figcaption></figure><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/559d7b2e0eb4176b9d0533c6bde07e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tClzsuYkA_Wlj7GYl5rn0A.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Detection of key points and descriptors using BRISK and Hamming distances.</figcaption></figure><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/c6baaf5e961784745201fc6d14daa8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h9YdDy6dvIrJGES-9cEeIA.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Detection of key points and descriptors using ORB and Hamming distances.</figcaption></figure><h1 id="03c0" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">特征匹配</h1><p id="9276" class="pw-post-body-paragraph kb kc it kd b ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku mv kw kx ky im bi translated">如我们所见，我们从两幅图像中获得了大量特征。现在，我们想比较两组特征，并坚持使用显示更多相似性的特征对。</p><p id="0d17" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在 OpenCV 中，特征匹配需要一个 Matcher 对象。在这里，我们探索两种口味:</p><ul class=""><li id="f9d3" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">强力匹配器</li><li id="e03c" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">KNN(k-最近邻)</li></ul><p id="1db7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">BruteForce (BF)匹配器正如其名所示。给定 2 组特征(来自图像 A 和图像 B)，将 A 组中的每个特征与 B 组中的所有特征进行比较。默认情况下，BF Matcher 计算两点之间的<strong class="kd iu">欧几里德距离。因此，对于集合 A 中的每个特征，它返回集合 b 中最接近的特征。对于 SIFT 和 SURF，OpenCV 建议使用欧几里德距离。对于其他特征提取器，如 ORB 和 BRISK，建议使用汉明距离。</strong></p><p id="c9b0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">要使用 OpenCV 创建一个 BruteForce 匹配器，我们只需要指定 2 个参数。第一个是距离度量。第二个是<em class="mw">交叉检查</em>布尔参数。</p><figure class="lp lq lr ls gt ju"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="e278" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="mw"> crossCheck </em> bool 参数指示两个特征是否必须相互匹配才能被视为有效。换句话说，对于被认为有效的一对特征(<em class="mw"> f1，F2</em>),<em class="mw">f1</em>需要匹配<em class="mw"> f2 </em>，并且<em class="mw"> f2 </em>也必须匹配<em class="mw"> f1 </em>作为最接近的匹配。该过程确保了一组更稳健的匹配特征，并且在原始 SIFT 论文中有所描述。</p><p id="897f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，对于我们想要考虑多个候选匹配的情况，我们可以使用基于 KNN 的匹配过程。</p><blockquote class="ne"><p id="2dd2" class="nf ng it bd nh ni nj nk nl nm nn ky dk translated">KNN 不会返回给定要素的单个最佳匹配，而是返回 k 个最佳匹配。</p></blockquote><p id="83e6" class="pw-post-body-paragraph kb kc it kd b ke no kg kh ki np kk kl km nq ko kp kq nr ks kt ku ns kw kx ky im bi translated">注意，k 值必须由用户预先定义。正如我们所料，KNN 提供了更多的候选特性。然而，我们需要确保所有这些匹配对在进一步之前都是健壮的。</p><h1 id="acd6" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">比率测试</h1><p id="33dd" class="pw-post-body-paragraph kb kc it kd b ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku mv kw kx ky im bi translated">为了确保 KNN 返回的特征具有很好的可比性，SIFT 论文的作者建议使用一种叫做<strong class="kd iu">比率测试</strong>的技术。基本上，我们迭代 KNN 返回的每一对，并执行距离测试。对于每一对特征(<em class="mw"> f1，f2 </em>)，如果<em class="mw"> f1 </em>和<em class="mw"> f2 </em>之间的距离在一定比例之内，我们保留它，否则，我们丢弃它。此外，比率值必须手动选择。</p><p id="8a90" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">本质上，比率测试的工作与来自 BruteForce Matcher 的交叉检查选项相同。两者都确保一对检测到的特征确实足够接近以被认为是相似的。下图显示了 BF 和 KNN 匹配器在 SIFT 特征上的结果。我们选择只显示 100 个匹配点，以便清晰可视化。</p><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/b0bcb85f402c075cdfe53cf4e5645354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qUOv6yPPwiVUFBwQQGYtIA.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Feature matching using KNN and Ration Testing on SIFT features</figcaption></figure><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/4ef5889c41782e861b0122a728a701ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-85uCT-yjL-CmcNN7fi52w.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Feature matching using Brute Force Matcher on SIFT features</figcaption></figure><p id="d677" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请注意，即使在 KNN 对暴力和比率测试进行了交叉检查之后，一些特性仍然不能正确匹配。</p><p id="0d70" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，匹配器算法将从两幅图像中给我们最好的(更相似的)特征集。现在，我们需要获得这些点，并找到转换矩阵，该矩阵将基于它们的匹配点将两幅图像缝合在一起。</p><p id="8139" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这种变换称为单应矩阵。简而言之，单应矩阵是一个 3×3 矩阵，可用于许多应用，如相机姿态估计、透视校正和图像拼接。单应是 2D 变换。它将点从一个平面(图像)映射到另一个平面。让我们看看我们如何得到它。</p><h1 id="3e30" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">估计单应性</h1><p id="443c" class="pw-post-body-paragraph kb kc it kd b ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku mv kw kx ky im bi translated">随机样本一致性或 RANSAC 是一种拟合线性模型的迭代算法。与其他线性回归不同，RANSAC 旨在对异常值保持稳健。</p><p id="e9c5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">线性回归等模型使用最小二乘估计来拟合数据的最佳模型。然而，普通最小二乘法对异常值非常敏感。因此，如果离群值的数量很大，它可能会失败。</p><p id="1349" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">RANSAC 通过仅使用数据中的<strong class="kd iu">内嵌器</strong>的子集来估计参数，从而解决了这个问题。下图显示了线性回归和 RANSAC 之间的比较。首先，注意数据集包含相当多的异常值。</p><p id="8c01" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们可以看到，线性回归模型很容易受到异常值的影响。这是因为它试图减少平均误差。因此，它倾向于支持最小化从所有数据点到模型本身的总距离的模型。这包括离群值。</p><p id="1c45" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">相反，RANSAC 仅在被识别为内点的点的子集上拟合模型。</p><p id="955b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个特征对我们的用例非常重要。这里，我们将使用 RANSAC 来估计单应矩阵。事实证明，单应性对我们传递给它的数据质量非常敏感。因此，重要的是要有一种算法(RANSAC ),能够将明显属于数据分布的点从不属于数据分布的点中过滤出来。</p><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d7f2896dc271e22a2c24115907627e44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*Tq8ui-bw80I8y3Qm.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Comparison between Least Squares and RANSAC model fitting. Note the substantial number of outliers in the data.</figcaption></figure><p id="28dc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一旦我们有了估计的单应性，我们需要将其中一个图像扭曲到一个公共平面上。</p><p id="3343" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里，我们将对其中一幅图像应用透视变换。基本上，透视变换可以组合一个或多个操作，如旋转、缩放、平移或剪切。想法是转换其中一个图像，使两个图像合并为一个。为此，我们可以使用 OpenCV <em class="mw"> warpPerspective() </em>函数。它接受一幅图像和单应性作为输入。然后，基于单应性将源图像扭曲到目的图像。</p><figure class="lp lq lr ls gt ju"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="42d3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">生成的全景图像如下所示。正如我们看到的，结果中有几个工件。更具体地说，我们可以看到一些与图像边界处的光照条件和边缘效应相关的问题。理想情况下，我们可以执行后处理技术来归一化亮度，如<strong class="kd iu">直方图匹配</strong>。这可能会使结果看起来更真实。</p><p id="59b1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">感谢阅读！</strong></p><div class="lp lq lr ls gt ab cb"><figure class="nv ju nw nx ny nz oa paragraph-image"><img src="../Images/46a14c541d53591b70779ae573a9f6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*57hxWhgcC3ipkFe8.jpg"/></figure><figure class="nv ju ob nx ny nz oa paragraph-image"><img src="../Images/a0ba060b2ddeeddbeb8acbc3bac2cc45.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*-JGkWeLlsGeiPxv0.jpg"/><figcaption class="na nb gj gh gi nc nd bd b be z dk oc di od oe">Input image pair</figcaption></figure></div><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/e35951329a444cddad7d21154b061922.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mXpVgpUFcPwI8TBEz1v7vA.jpeg"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Panoramic Image</figcaption></figure><div class="lp lq lr ls gt ab cb"><figure class="nv ju og nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/66ce4169d51fa79bd97424af6d94fa07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*uX7hbsfinsPfy70P.jpg"/></div></figure><figure class="nv ju oh nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/931e606741f5d0dff104bc91c86ad2f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/0*NTiVIXKkWp27EYYk.jpg"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk oi di oj oe">Input image pair</figcaption></figure></div><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/0b0d8355375bf09fdeef7d73cc0b8c73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WB1ggMCF55C5OqmPCVKcgw.jpeg"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Panoramic Image</figcaption></figure><div class="lp lq lr ls gt ab cb"><figure class="nv ju ol nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/466858993bc66b1d0e0ba745062b30f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*mMW2Edbt9cDRLsOA.jpg"/></div></figure><figure class="nv ju ol nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/117dcf21f7c0a9dbd1a9b0c2991d4c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*NvdWHPlFNigMD6NT.jpg"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk oi di oj oe">Input image pair</figcaption></figure></div><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div class="gh gi om"><img src="../Images/0e66f5fd79c2eab590ada65008219ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*oshRGkRgjMHWHJ62cDuk2w.jpeg"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Panoramic Image</figcaption></figure><div class="lp lq lr ls gt ab cb"><figure class="nv ju ol nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/4ee59b9d0049fc2f17eb755846174ed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*-jCFmt1ci3lxO-fq.jpg"/></div></figure><figure class="nv ju ol nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/76298e71be9bf03457211e7529e22238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*8PWi3CgOPwZIhtd7.jpg"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk oi di oj oe">Input image pair.</figcaption></figure></div><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi on"><img src="../Images/85948f581d9ecce92e8d6fb56c50d0b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*94l_fyJfL9HM-pgEZYh6dA.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Panoramic Image</figcaption></figure></div></div>    
</body>
</html>