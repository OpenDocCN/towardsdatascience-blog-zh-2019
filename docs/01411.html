<html>
<head>
<title>Unsupervised Machine Learning: Clustering Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督机器学习:聚类分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e?source=collection_archive---------0-----------------------#2019-03-06">https://towardsdatascience.com/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e?source=collection_archive---------0-----------------------#2019-03-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/0fa5888ca2cba575b11329ee80156dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*P3ixlaofL31Oycf4uoUL9Q.jpeg"/></div></figure><h1 id="f1f5" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">无监督学习简介</h1><p id="b866" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">到目前为止，我们只探索了<a class="ae lt" rel="noopener" target="_blank" href="/supervised-learning-basics-of-linear-regression-1cbab48d0eba">监督机器学习算法</a>和技术来开发模型，其中数据具有先前已知的标签。换句话说，我们的数据有一些带有特定值的目标变量，我们用它们来训练我们的模型。</p><p id="0d0b" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">然而，在处理现实世界的问题时，大多数时候，数据不会带有预定义的标签，因此我们希望开发能够正确分类这些数据的机器学习模型，通过自己找到特征中的一些共性，用于预测新数据的类别。</p><h2 id="f182" class="lz jy it bd jz ma mb dn kd mc md dp kh lg me mf kl lk mg mh kp lo mi mj kt mk bi translated"><strong class="ak">无监督学习分析过程</strong></h2><p id="b924" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在开发无监督学习模型时，我们将遵循的整个过程可以总结在下表中:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ml"><img src="../Images/9d1b4f0d1b9ad4e6559a3116fd49b0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MQjzeJweEb2Sf1o3xcpqoA.png"/></div></div></figure><p id="bffa" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">无监督学习的主要应用有:</p><ul class=""><li id="b07a" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">利用共享属性分割数据集。</li><li id="80b7" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">检测不适合任何组的异常。</li><li id="5254" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">通过聚合具有相似属性的变量来简化数据集。</li></ul><p id="83e9" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">总之，主要目标是研究数据的内在(通常是隐藏的)结构。</p><p id="697f" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">这种技术可以浓缩为无监督学习试图解决的两种主要类型的问题。这些问题是:</p><ul class=""><li id="50b9" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">使聚集</li><li id="e44d" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">降维</li></ul><p id="b6b1" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">在整篇文章中，我们将关注聚类问题，并将在以后的文章中讨论降维。</p><h1 id="7879" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">聚类分析</h1><p id="f0f7" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">基本上，聚类的目标是在数据的元素中找到不同的组。为此，聚类分析算法查找数据中的结构，以便同一个聚类(或组)的元素彼此之间比来自不同聚类的元素更相似。</p><p id="6b11" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">以一种可视化的方式:想象我们有一个电影的数据集，想要对它们进行分类。我们有以下电影评论:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ni"><img src="../Images/a44a3c58530ef143692ce9125471f33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UrTFgcUrxq5C-wOUFvxCkQ.png"/></div></div></figure><p id="e864" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">机器学习模型将能够推断出有两个不同的类别，而无需从数据中了解任何其他信息。</p><p id="0808" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">这些无监督学习算法具有令人难以置信的广泛应用，并且对于解决现实世界的问题非常有用，例如异常检测、推荐系统、文档分组或者基于购买发现具有共同兴趣的客户。</p><p id="7436" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">一些最常见的聚类算法，以及将在本文之外探讨的算法有:</p><ul class=""><li id="a5eb" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">k 均值</li><li id="f6e9" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">层次聚类</li><li id="5e1b" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">基于密度的扫描聚类(DBSCAN)</li><li id="495c" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">高斯聚类模型</li></ul><h1 id="43a2" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">k 均值聚类</h1><p id="24fc" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">K-Means 算法非常容易实现，而且计算效率非常高。这些是解释它们为什么如此受欢迎的主要原因。但是当处理不具有球形分布形状的组时，它们不太适合于识别类。</p><p id="4860" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">K-Means 算法的目标是找到彼此之间具有高度相似性的数据点并将其分组。就算法而言，这种相似性被理解为数据点之间距离的反义词。数据点越接近，它们就越相似，并且更有可能属于同一个聚类。</p><p id="086a" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">关键概念</strong></p><ul class=""><li id="9f59" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">平方欧几里德距离</li></ul><p id="d612" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">K-Means 中最常用的距离是平方欧几里德距离。在<em class="nj"> m 维</em>空间中的两点<em class="nj"> x 和 y </em>之间的距离的一个例子是:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/09cf81a7765c29ec7fce34c9a9a3bd50.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*svzWIVVO4k0tSu14pzSuFA.png"/></div></figure><p id="2d1f" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">这里，<em class="nj"> j </em>是样本点<em class="nj"> x 和 y</em>的第<em class="nj">j</em>维(或特征列)</p><ul class=""><li id="c7be" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">集群惯性</li></ul><p id="c5a7" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">聚类惯性是聚类上下文中误差平方和的名称，表示如下:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/0b21491f252dc86796d36056fc388afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*jO8AEM1Ttkc46ea7bIEl0Q.png"/></div></figure><p id="b763" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">其中<em class="nj"> μ(j) </em>是聚类<em class="nj"> j，</em>的质心，如果样本<em class="nj"> x(i) </em>在聚类<em class="nj"> j </em>中，则<em class="nj"> w(i，j) </em>为 1，否则为 0。</p><p id="3ad1" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">K-Means 可以理解为一种算法，它将尝试最小化聚类惯性因子。</p><p id="db0c" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">算法步骤</strong></p><ol class=""><li id="bf87" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls nm na nb nc bi translated">首先，我们需要选择 k，我们希望被发现的集群的数量。</li><li id="b41c" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls nm na nb nc bi translated">然后，该算法将随机选择每个聚类的质心。</li><li id="dff0" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls nm na nb nc bi translated">将每个数据点分配到最近的质心(使用欧几里德距离)。</li><li id="44c0" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls nm na nb nc bi translated">将计算群集惯性。</li><li id="f982" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls nm na nb nc bi translated">新的质心将被计算为属于上一步质心的点的平均值。换句话说，通过计算每个聚类中心的数据点的最小二次误差，将中心移向该点</li><li id="d6e4" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls nm na nb nc bi translated">回到步骤 3。</li></ol><p id="bd76" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">K-均值超参数</strong></p><ul class=""><li id="86ab" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">簇的数量:要生成的簇和质心的数量。</li><li id="c681" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">最大迭代次数:单次运行的算法。</li><li id="c523" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">Number initial:使用不同质心种子运行算法的次数。就惯性而言，最终结果将是所定义的连续运行次数的最佳输出。</li></ul><p id="e270" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">K 均值的挑战</strong></p><ul class=""><li id="a772" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">任何固定训练集的输出不会总是相同的，因为初始质心是随机设置的，这将影响整个算法过程。</li><li id="a887" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">如前所述，由于欧几里德距离的性质，当处理采用非球形形状的聚类时，它不是合适的算法。</li></ul><p id="43a1" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">应用 K 均值时需要考虑的要点</strong></p><ul class=""><li id="4604" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">必须在相同的尺度上测量要素，因此可能有必要执行 z 分数标准化或最大-最小缩放。</li><li id="bf1b" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">在处理分类数据时，我们将使用 get dummies 函数。</li><li id="44a2" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">探索性数据分析(EDA)非常有助于对数据进行概述，并确定 K-Means 是否是最合适的算法。</li><li id="1265" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">当有大量的列时，minibatch 方法非常有用，但是它不太准确。</li></ul><p id="81a7" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">如何选择合适的 K 数</strong></p><p id="35ef" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">选择正确的聚类数是 K-Means 算法的关键点之一。有一些方法可以找到这个数字:</p><ul class=""><li id="3f50" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">领域知识</li><li id="b55b" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">商业决策</li><li id="eaf4" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">肘法</li></ul><p id="515a" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">由于与数据科学的动机和性质一致，肘方法是首选方法，因为它依赖数据支持的分析方法来做出决策。</p><p id="5ae6" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">肘法</strong></p><p id="6793" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">肘方法用于确定数据集中聚类的正确数量。它的工作原理是绘制 K 的递增值与使用该 K 时获得的总误差的关系图。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/b6f7d2fdea84eace299ff792326d8464.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*yD_1yRUKD_0ed-eYw_I9wA.png"/></div></figure><p id="76bf" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">目标是找到对于每个聚类不会显著增加方差的 k</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi no"><img src="../Images/8b79f8a5f810d27ca6ab30c830f36814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*86R1OByRi6JoLq1JPAUnpQ.png"/></div></div></figure><p id="df0c" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">在这种情况下，我们将选择弯头所在的 k=3。</p><p id="c7b3" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu"> K-Means 局限性</strong></p><p id="d804" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">虽然 K-Means 是一个很好的聚类算法，但当我们预先知道准确的聚类数，并且处理球形分布时，它是最有用的。</p><p id="91a9" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">下图显示了如果我们在每个数据集中使用 K-means 聚类，即使我们事先知道准确的聚类数，我们会得到什么:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi np"><img src="../Images/61c4433f3f4b43ac8bbae99d5a0197c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykyaNxEi1QhICv8gbdI8aw.png"/></div></div></figure><p id="c368" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">将 K-Means 算法作为基准来评估其他聚类方法的性能是很常见的。</p><h1 id="c77c" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">层次聚类</h1><p id="85e2" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">层次聚类是基于原型的聚类算法的替代方法。层次聚类的主要优点是我们不需要指定聚类的数量，它会自己找到。此外，它还能够绘制树状结构。树形图是二进制层次聚类的可视化。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c2415481494860949e416a81640fe159.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*GDuQNu0Ioz0cuUgwnvCPGg.png"/></div></figure><p id="eb1b" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">融合在底部的观察结果是相似的，而融合在顶部的观察结果则完全不同。对于树形图，结论是根据纵轴的位置而不是横轴得出的。</p><p id="56c5" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">等级聚类的种类</strong></p><p id="d5f0" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">这种类型的聚类有两种方法:聚合和分裂。</p><ul class=""><li id="c470" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">Divisive:这种方法首先将所有数据点聚集在一个单独的集群中。然后，它会迭代地将聚类分成更小的聚类，直到每个聚类只包含一个样本。</li><li id="8da8" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">凝聚:这种方法从每个样本是一个不同的聚类开始，然后通过彼此更接近的样本来合并它们，直到只有一个聚类。</li></ul><p id="a28f" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">单联动&amp;全联动</strong></p><p id="b0c4" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">这些是用于凝聚层次聚类的最常用算法。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ee707dac44a0c6a870831bbf7bafa336.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*KJ3R_iyQmlNUxPfM5QBIow.png"/></div></figure><ul class=""><li id="81e3" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">单键</li></ul><p id="9a81" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">作为一种凝聚算法，单链从假设每个样本点是一个簇开始。然后，它计算每对聚类的最相似成员之间的距离，并合并最相似成员之间的距离最小的两个聚类。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ns"><img src="../Images/989a7db0a3d55e8c2e66fda434f3450c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HUOYokgnLlokcYvT2C4stg.png"/></div></div></figure><ul class=""><li id="68d9" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">完全连锁</li></ul><p id="3a22" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">虽然与它的兄弟(单链)相似，但它的原理完全相反，它比较一对集群中最不相似的数据点来执行合并。</p><p id="41b0" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">层次聚类的优势</strong></p><ul class=""><li id="9cfa" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">由此产生的层次结构表示可以提供很多信息。</li><li id="5010" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">树形图提供了一种有趣且信息丰富的可视化方式。</li><li id="6cb0" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">当数据集包含真正的层次关系时，它们特别有用。</li></ul><p id="ba3b" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">层次聚类的缺点</strong></p><ul class=""><li id="5b1e" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">它们对异常值非常敏感，当异常值出现时，模型性能会显著下降。</li><li id="4e0a" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">从计算角度来说，它们非常昂贵。</li></ul><h1 id="38ee" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">基于密度的噪声应用空间聚类(DBSCAN)</h1><p id="a5e4" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">带噪声应用程序的基于密度的空间聚类，或 DBSCAN，是另一种对正确识别数据中的噪声特别有用的聚类算法。</p><p id="419d" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu"> DBSCAN 分配标准</strong></p><p id="4b07" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">它基于多个具有指定半径ε的点，每个数据点都有一个特殊的标签。分配该标签的过程如下:</p><ul class=""><li id="ae98" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">它是指定数量(最小点数)的相邻点。如果有这个最小点数的点落在ε半径内，将指定一个核心点。</li><li id="3d95" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">边界点将落在核心点的ε半径内，但其邻域数将少于 MinPts 数。</li><li id="e126" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">每隔一个点将是噪声点。</li></ul><p id="e744" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu"> DBSCAN 算法</strong></p><p id="01cf" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">该算法遵循以下逻辑:</p><ol class=""><li id="92f9" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls nm na nb nc bi translated">确定一个核心点，并为每个核心点或每个连接的核心点组(如果它们符合核心点的标准)分组。</li><li id="a645" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls nm na nb nc bi translated">确定边界点并将其分配给各自的核心点。</li></ol><p id="bf7c" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">下图很好地总结了这个过程和注释符号。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/5c0a7ca73b9742023ea9431ec3b8d6bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*USv6WLj3A-9De9D7am2iZQ.png"/></div></figure><p id="90ee" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu"> DBSCAN 与 K 均值聚类</strong></p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nu"><img src="../Images/d8256479a65a4818f23da70cc60e24dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x48iVUvrWtYY31WEsVLdeQ.png"/></div></div></figure><p id="dd8c" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu"> DBDSCAN 的优势</strong></p><ul class=""><li id="555d" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">我们不需要指定集群的数量。</li><li id="03a7" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">簇可以采用的形状和大小具有高度的灵活性。</li><li id="2653" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">识别和处理噪声数据和异常值是非常有用的。</li></ul><p id="441b" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu"> DBSCAN 缺点</strong></p><ul class=""><li id="a6f7" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">它在处理两个集群可达的边界点时面临困难。</li><li id="a789" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">它找不到不同密度的井丛。</li></ul><h1 id="352f" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated"><strong class="ak">高斯混合模型(GMM) </strong></h1><p id="416c" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">高斯混合模型是一种概率模型，它假设所有样本都是由有限个高斯分布和未知参数混合而成的。</p><p id="a369" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">它属于软聚类算法组，其中每个数据点将属于数据集中存在的每个聚类，但是每个聚类具有不同的成员级别。这种隶属关系被指定为属于某一类的概率，范围从 0 到 1。</p><p id="3cff" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">例如，高亮显示的点将同时属于聚类 A 和聚类 B，但由于其与聚类 A 的接近性，因此属于聚类 A 的成员更高。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nv"><img src="../Images/c5bff302279281e634eb8e0b229b1635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jCGgXVlHGE3cXVncW3xdtw.png"/></div></div></figure><p id="6853" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">GMM 是我们将在本系列中学习的最高级的聚类方法之一，它假设每个聚类遵循一个概率分布，该分布可以是高斯分布或正态分布。它是 K-均值聚类的推广，包括关于数据的协方差结构以及潜在高斯中心的信息。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nw"><img src="../Images/df569c77cfc9ec3ca0c65e45ad6b8a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1QDCWZS0AUZ-51VKG8INiQ.png"/></div></div></figure><p id="ae30" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">一维 GMM 分布</strong></p><p id="1744" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">GMM 将在数据集中搜索高斯分布并混合它们。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nx"><img src="../Images/f55d53962bad6890b0db24ae71547c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uc63ZNYZaVcW75QOcJyAtQ.png"/></div></div></figure><p id="5b6e" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">二维中的 GMM</strong></p><p id="61e9" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">当多元分布如下所示时，对于 de 数据集分布的每个轴，平均中心应为+ σ。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d2e7c6f527e773de2f6e4cd51a447c7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*wkTgfCOdSS06ia6KJDAENw.png"/></div></figure><p id="ade8" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu"> GMM 算法</strong></p><p id="7f10" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">这是一种期望最大化算法，其过程可以概括如下:</p><ol class=""><li id="3ce1" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls nm na nb nc bi translated">初始化 K 高斯分布。它通过(平均值)和σ(标准差)值来实现这一点。它们可以取自数据集(朴素方法)或通过应用 K-Means。</li><li id="0158" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls nm na nb nc bi translated">对数据进行软聚类:这是“期望”阶段，在此阶段，所有的数据点将被分配给每个具有各自隶属级别的聚类。</li><li id="a664" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls nm na nb nc bi translated">重新估计高斯分布:这是“最大化”阶段，在此阶段检查期望值，并使用期望值计算高斯分布的新参数:new 和σ。</li><li id="2fb2" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls nm na nb nc bi translated">评估数据的对数似然性以检查收敛性。对数似然性越高，我们创建的模型的混合物就越有可能符合我们的数据集。这是最大化的函数。</li><li id="e928" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls nm na nb nc bi translated">从步骤 2 开始重复，直到收敛。</li></ol><p id="2feb" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu"> GMM 的优势</strong></p><ul class=""><li id="4ec9" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">这是一种软聚类方法，它将样本成员分配给多个聚类。这一特性使其成为学习混合模型的最快算法</li><li id="ea89" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">集群的数量和形状具有高度的灵活性。</li></ul><p id="094b" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu"> GMM 的缺点</strong></p><ul class=""><li id="6259" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">它对初始值非常敏感，这将极大地影响它的性能。</li><li id="9d52" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">GMM 可能收敛到局部最小值，这将是次优解。</li><li id="e45b" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">当每个混合物没有足够的点时，该算法发散并找到具有无限可能性的解，除非我们人工地调整数据点之间的协方差。</li></ul><h1 id="eb85" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">聚类验证</h1><p id="338c" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">聚类验证是对聚类结果进行客观定量评估的过程。我们将通过应用聚类验证指数来进行验证。有三个主要类别:</p><p id="7bbd" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">外部指数</strong></p><p id="beb8" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">这些是我们在原始数据被标记时使用的评分方法，这在这类问题中并不常见。我们将把一个聚类结构与预先知道的信息进行匹配。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/4c87684b0f057975b00cd2090d377fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*MMtZnLHmzEYmF5K7zfMbHA.png"/></div></figure><p id="d423" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">使用最多的指数是调整后的兰德指数。</p><ul class=""><li id="b6c4" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">调整后的€兰德指数[-1，1]</li></ul><p id="4126" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">为了理解它，我们应该首先定义它的组成部分:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d1e27c34c26170c42444981c4326746d.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*n5GidEL8cG-zzhyXQNWqCQ.png"/></div></figure><ul class=""><li id="f837" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">a:是在 C 和 K 中的同一个簇中的点数</li><li id="dfd4" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">b:是在 C 和 k 中不同簇中的点数。</li><li id="631e" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">n =样本总数</li></ul><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/fc4f6a3bdf952535fd29f2bc854f0e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*tSJ0NX7ZHwqpsRVhQrSebA.png"/></div></figure><p id="eb00" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">ARI 可以得到范围从-1 到 1 的值。该值越高，与原始数据的匹配程度越高。</p><p id="561d" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated"><strong class="kx iu">内部验证指标</strong></p><p id="e4cc" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">在无监督学习中，我们将处理未标记的数据，这是内部索引更有用的时候。</p><p id="ef99" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">最常见的指标之一是轮廓系数。</p><ul class=""><li id="a1ef" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">轮廓系数:</li></ul><p id="8f6b" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">每个数据点都有一个轮廓系数。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/7ce7e9f174264a8b9888c42c07cadc51.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*qydd2In0-dD7jo8yXfGVuQ.png"/></div></figure><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/bc2f1b9054a8c9e9ea40143a6fb36cdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*tN6h-Ief1VDgj0cNsDdAKg.png"/></div></figure><ul class=""><li id="4294" class="mu mv it kx b ky lu lc lv lg mw lk mx lo my ls mz na nb nc bi translated">a =到同一组中其他样品 I 的平均距离</li><li id="f0fe" class="mu mv it kx b ky nd lc ne lg nf lk ng lo nh ls mz na nb nc bi translated">b =到最近的相邻聚类中的其他样本 I 的平均距离</li></ul><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/127b26c3f5621d2229e450fbec58f12e.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*6E-zA1MfJJNBilIftRCX5w.png"/></div></figure><p id="cc28" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">轮廓系数(SC)的值可以从-1 到 1。值越高，选择的 K 越好。如果我们超过了理想的 K 值，就会比我们低于理想的 K 值受到更多的惩罚。</p><p id="0dde" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">它只适用于某些算法，如 K-Means 和层次聚类。它不适合与 DBSCAN 一起工作，我们将使用 DBCV。</p><h1 id="4696" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">结论</h1><p id="32e9" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们首先介绍了无监督学习和主要的聚类算法。</p><p id="0a7b" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">在下一篇文章中，我们将完成一个实现，作为构建 K-means 模型的示例，并回顾和实践所解释的概念。</p><p id="73c6" class="pw-post-body-paragraph kv kw it kx b ky lu la lb lc lv le lf lg lw li lj lk lx lm ln lo ly lq lr ls im bi translated">敬请期待！</p></div></div>    
</body>
</html>