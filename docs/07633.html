<html>
<head>
<title>PySpark &amp; AWS | Predicting Customer Churn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark &amp; AWS |预测客户流失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-customer-churn-with-pyspark-95cd352d393?source=collection_archive---------27-----------------------#2019-10-23">https://towardsdatascience.com/predicting-customer-churn-with-pyspark-95cd352d393?source=collection_archive---------27-----------------------#2019-10-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/e7c3601ba265233a556123de562c9cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H41AwhcZBbmZyVh4786skg.jpeg"/></div></div></figure><p id="cfca" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你怎么称呼一群分散的恐龙繁殖岛？</p><p id="d243" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">侏罗纪火花...*Ba Dum Tss* </em></p><p id="3bc5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi la translated"><span class="l lb lc ld bm le lf lg lh li di"> O </span>对于采用订阅式商业模式的公司来说，最重要的问题之一是客户流失。客户因为各种原因降级或停止服务，服务提供商通常直到客户离开才知道他们何时或为什么离开！</p><blockquote class="lj"><p id="e422" class="lk ll it bd lm ln lo lp lq lr ls ky dk translated">如果公司能够在客户离开之前预测到他们何时会流失，会怎么样？</p></blockquote><p id="9623" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">这将是强大的！如果我们能够可靠地预测客户是否会流失，我们就有机会通过促销、宣传新功能等方式留住这些客户。这是一种<strong class="kd iu">主动的</strong>方法来留住客户，而不是一种<strong class="kd iu">被动的</strong>方法来挽回失去的客户。</p><p id="e2ac" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Udacity 提供了两个类似格式的用户活动数据集(128MB 和 12GB)，来自一家虚构的音乐流媒体公司 Sparkify，我使用这些数据来更好地了解 Sparkify 的客户，然后预测客户是否会流失，准确率超过 80%。</p><p id="d402" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> <em class="kz">我无法在本地对我的机器上的 12GB 数据集进行分析和建模，所以我在 Jupyter 笔记本中使用<strong class="kd iu"> <em class="kz"> PySpark 的本地模式</em> </strong>在 128MB 数据集上探索并原型化了我的工作流，这让您<strong class="kd iu"> <em class="kz">模拟在一台机器上的分布式集群上工作的</em> </strong>。</em></strong></p><p id="2b30" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我把工作流变成了一个<strong class="kd iu"> <em class="kz"> Python 脚本</em> </strong>，旋出了一个<strong class="kd iu"> <em class="kz"> 4 节点 AWS EC2 集群</em> </strong>，在集群上通过<strong class="kd iu"> <em class="kz"> AWS EMR </em> </strong>执行脚本。</p><p id="9adc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以下是我在这篇文章中将要涉及的内容:</p><ol class=""><li id="60ca" class="ly lz it kd b ke kf ki kj km ma kq mb ku mc ky md me mf mg bi translated">特征工程&amp;在本地探索数据</li><li id="21aa" class="ly lz it kd b ke mh ki mi km mj kq mk ku ml ky md me mf mg bi translated">本地数据预处理</li><li id="5873" class="ly lz it kd b ke mh ki mi km mj kq mk ku ml ky md me mf mg bi translated">本地机器学习</li><li id="a78a" class="ly lz it kd b ke mh ki mi km mj kq mk ku ml ky md me mf mg bi translated">在 AWS 分布式集群上运行步骤 1–3</li></ol><p id="aa1e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">这篇文章是写给谁的？</strong></p><p id="0a22" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">本文旨在向您介绍我所采取的步骤和一些有用的代码，这些步骤和代码带我从一个小型 Jupyter 笔记本分析到一个 4 节点 AWS EMR 集群上的 12GB 数据集分析。所以，我假设你对<strong class="kd iu"><em class="kz"/></strong><strong class="kd iu"><em class="kz">Python</em></strong>&amp;<strong class="kd iu"><em class="kz">机器学习</em> </strong>。</p><p id="ca03" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此外，在这篇文章中，我展示了许多我使用<strong class="kd iu"><em class="kz">【py spark】</em></strong>的代码片段，这些代码片段与通过<strong class="kd iu"> <em class="kz"> Pandas </em> </strong>和<strong class="kd iu"> <em class="kz"> Scikit-Learn </em> </strong>库处理数据的<strong class="kd iu"><em class="kz"/></strong>非常不同。如果你不熟悉 PySpark，那就略读一下吧！没什么大不了的。</p><p id="c297" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后，如果你想对分析和机器学习如何通过我的代码进行更详细的分析和演练，请查看我的完整<a class="ae mm" href="http://patrickdeguzman.me/notebooks/SparkifyAnalysis.html" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>(托管在我的<a class="ae mm" href="http://patrickdeguzman.me/" rel="noopener ugc nofollow" target="_blank">网站</a>)或<a class="ae mm" href="https://github.com/pdeguzman96/sparkify" rel="noopener ugc nofollow" target="_blank"> GitHub Repo </a>。</p><p id="9232" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我希望你喜欢这个！</p><blockquote class="lj"><p id="0351" class="lk ll it bd lm ln lo lp lq lr ls ky dk translated">现在让我们开始吧…</p></blockquote><h1 id="dd1e" class="mn mo it bd mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk bi translated">一、特征工程&amp;探索数据</h1><p id="b5ad" class="pw-post-body-paragraph kb kc it kd b ke nl kg kh ki nm kk kl km nn ko kp kq no ks kt ku np kw kx ky im bi translated">首先，我将 128MB 的数据集从 JSON 格式转换成 Spark 数据帧。以下是模式，以便您了解数据的结构:</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/621f79446b7f4898c8870bff2bb7a6d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*Gu6rrdgKB4uvnuD_cYtW9g.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Original imported features.</figcaption></figure><p id="59b8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">日期特性工程| </strong>经过一些简单的探索，我使用“ts”(时间戳)属性通过 PySpark UDF(用户定义函数)创建了一些日期特性。</p><figure class="nr ns nt nu gt ju"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="5c0b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我为新的日期特性创建了一个计数图，以便更好地理解用户在整个数据集期间的行为。</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/8e378762334ebcaebd67a9bb5cab792a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RTTBOxzR0Hl_XKwT-D9oeg.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Note this smaller dataset (128MB) only contains data for about two months.</figcaption></figure><p id="0ab2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">事件特性工程| 我使用了“页面”特性来标记用户访问的特定页面或执行的特定操作。这些标志特性将在以后帮助我在用户级别而不是事件级别聚合用户活动。</p><figure class="nr ns nt nu gt ju"><div class="bz fp l di"><div class="nz oa l"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">This is identical to one-hot encoding based on this feature. In hindsight, this would have been faster with PySpark’s native OneHotEncoder. Oh well! It works.</figcaption></figure><p id="0c0c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">运行计数特征工程| </strong>我还利用了数据的事务性质(例如，某个用户在某个时间点执行的每个事件对应一行)来创建一些窗口计算特征(例如，一个月内的运行收听计数等)。)这里有一个例子，说明这样的功能是如何创建的。</p><figure class="nr ns nt nu gt ju"><div class="bz fp l di"><div class="nz oa l"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Creating a window for each user per month, counting rows whenever a user listens, and joining back to DF.</figcaption></figure><p id="60bb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这将创建一个类似于图中第四列的列:</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/58ee4f8f79c210c2a5b1a544a4f37204.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*L1_U2zmCDNcFrzvekOIY_w.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Last Column: Note the null values</figcaption></figure><p id="6dfb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">由于这是一个连续计数，空值<em class="kz">应该在前面填充</em>。然而，鉴于 Spark 的弹性分布式数据集(rdd)的分布式本质，没有前置填充空值的原生函数。为了解释，如果一个数据集在云中的几个机器之间随机分区，那么机器如何知道前一个值何时是数据集中的<em class="kz">真</em>直接前一个值？</p><p id="98bc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我最好的解决方法是创建一个 running listens 列的 lag，然后用这个 lag 迭代地替换空值。<em class="kz">连续的</em>空值需要多次执行这种练习，因为我们实际上是用先前的值替换当前的空值(对于连续的空值，先前的值为空)。我是这样实现的:</p><figure class="nr ns nt nu gt ju"><div class="bz fp l di"><div class="nz oa l"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Sharing this snippet in case anyone finds it useful. This is the only way I found to front-fill null values that was computationally cheaper than using a complicated cross-join.</figcaption></figure><p id="57ed" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">上面的代码片段用其先前的值填充空值 6 次，然后用 0 填充剩余的值以节省计算时间。这可能不是前置填充的最佳方式，但如果要用相对较少的空值填充大型数据集，这可能会很有用。这是可行的，因为该操作需要 Spark 连接分区来计算 lag(即紧接在前面的值)。</p><p id="ca2e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">定义流失| </strong>最后，我将客户流失定义为每当用户访问“取消确认”或“降级”页面时。</p><figure class="nr ns nt nu gt ju"><div class="bz fp l di"><div class="nz oa l"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">This labels the “Churn” column of the DataFrame as 1 if a user visits the aforementioned pages.</figcaption></figure><h1 id="64c9" class="mn mo it bd mp mq mr ms mt mu mv mw mx my od na nb nc oe ne nf ng of ni nj nk bi translated">二。数据预处理</h1><p id="d3cc" class="pw-post-body-paragraph kb kc it kd b ke nl kg kh ki nm kk kl km nn ko kp kq no ks kt ku np kw kx ky im bi translated">现在让我们退一步，想想我们拥有什么，我们想要实现什么。</p><p id="ec55" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> <em class="kz">我们所拥有的:</em> </strong>一个“事件”数据集，指定用户在给定的时间点执行什么活动。</p><p id="b4b2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> <em class="kz">我们想要的:</em> </strong>预测客户流失。</p><p id="58df" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">凭直觉，我不认为在我们现有的数据集中预测变动是一个好主意。基于用户事件进行训练和预测将是非常昂贵的<strong class="kd iu"><em class="kz"/></strong><strong class="kd iu"><em class="kz"/></strong>，这可能是非常非常昂贵的<strong class="kd iu"><em class="kz"/></strong>因为你每小时可以有数千个事件(或者更多！)</p><p id="3e97" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，我的方法是简单地汇总每个用户的数据。具体来说:</p><blockquote class="og oh oi"><p id="df15" class="kb kc kz kd b ke kf kg kh ki kj kk kl oj kn ko kp ok kr ks kt ol kv kw kx ky im bi translated"><em class="it">我使用了</em> <strong class="kd iu">、事件级数据</strong> <em class="it">到</em> <em class="it">来创建</em> <em class="it">一个基于</em> <strong class="kd iu">指标的用户级矩阵</strong> <em class="it">，它本质上是在一个矩阵中总结每个用户的活动，该矩阵的行数与唯一用户的行数一样多</em>。</p></blockquote><p id="867c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这可能无法从文本中很好地翻译出来，所以让我给你看看我是怎么做的。下面是一个代码聚合示例，用于查看每个用户执行的全部活动:</p><figure class="nr ns nt nu gt ju"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="b88b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这种类型的聚合帮助我创建一个整洁的数据框架和可视化，如下所示…</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi om"><img src="../Images/42e89274417fd4a9325f152d2ba8053e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*Ckob21wx-CQNzeBR1mfudw.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Truncated DF | One row per user with user-specific summary statistics.</figcaption></figure><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi on"><img src="../Images/e0acc48dfd134c547aab738d390bf1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j92BxBlZI-wXP-FI0L3oBg.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Using the summary statistics to compare our churned users vs. our non-churned users.</figcaption></figure><p id="0ac7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有趣的是，看起来我们被激怒的用户比我们没有被激怒的用户更活跃。</p><p id="9561" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我有了一个很好的、紧密的矩阵中的数据，我用它作为机器学习模型的基础。</p><h1 id="2441" class="mn mo it bd mp mq mr ms mt mu mv mw mx my od na nb nc oe ne nf ng of ni nj nk bi translated">三。机器学习</h1><p id="58c1" class="pw-post-body-paragraph kb kc it kd b ke nl kg kh ki nm kk kl km nn ko kp kq no ks kt ku np kw kx ky im bi translated">我的建模方法简单明了:尝试一些算法，选择一个看起来最有希望的，然后调整这个模型的超参数。</p><p id="7049" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里有一个用户矩阵的模式，这样您就知道正在建模什么了:</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/05fd12783abaab97a1c8bc35823468d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*falVrx6h3y66LSSDca-l8g.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">One user and several metrics/aggregations for this user per row.</figcaption></figure><p id="d4a9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">PySpark ML 要求数据采用非常特殊的数据帧格式。它需要它的<strong class="kd iu"> <em class="kz">特征</em> </strong>在一个<strong class="kd iu"> <em class="kz">列的向量</em> </strong>中，其中向量的每个元素代表它的每个特征的值。它还要求其<strong class="kd iu"> <em class="kz">标签</em> </strong>在其<strong class="kd iu"> <em class="kz">自有列</em> </strong>中。</p><p id="41de" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面的代码将原始矩阵转换成这种 ML 友好的格式，并标准化这些值，使它们具有相同的比例。</p><figure class="nr ns nt nu gt ju"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="3464" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们已经准备好了数据，下面的代码显示了我最初是如何评估模型的。我任意选择了逻辑回归、随机森林和梯度提升树作为最终流失模型的候选。</p><figure class="nr ns nt nu gt ju"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="19b5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在初始模型评估后，我发现 gbt 分类器(梯度增强树)表现最好，准确率为 79.1%，F-1 得分为 0.799。</p><p id="e96e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，我使用 PySpark 的原生 CrossValidator 和 ParamGridBuilder 将该模型调优为网格搜索，网格搜索使用 K-Fold 验证选择最佳超参数。这里，由于计算时间昂贵，我使用了超参数的小网格和 3 重验证。</p><figure class="nr ns nt nu gt ju"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="494e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">经过调优后，重新评估模型，准确率和 F-1 分别提高到 82.35%和 0.831！最佳模型的最大深度为 3，最大箱数为 16。如果在我的笔记本电脑上运行不需要这么长时间(大约需要 1 个小时)，我会尝试更广泛的网格搜索。</p><p id="5e2e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">PySpark 自动计算这些基于树的模型的特征重要性。</p><p id="3e73" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">注意:出于好奇，</em> <a class="ae mm" href="https://explained.ai/rf-importance/" rel="noopener ugc nofollow" target="_blank"> <em class="kz">下面是</em> </a> <em class="kz">一篇有趣的文章，解释了特征重要性是如何计算的，以及</em> <strong class="kd iu"> <em class="kz">为什么它实际上不是那么准确</em> </strong> <em class="kz">。这真的不在这个项目的范围内，所以我只是敷衍一下。</em></p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi op"><img src="../Images/524ce02a0fc1e986062abb08743bc51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9-aY6C4qbZy7y7TU3bDHRg.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Feature Importances calculated for the tuned GBTClassifier.</figcaption></figure><p id="1f8f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">看来前三个<strong class="kd iu"> <em class="kz">预测流失最重要的特征</em> </strong>是:</p><ul class=""><li id="2727" class="ly lz it kd b ke kf ki kj km ma kq mb ku mc ky oq me mf mg bi translated">每次会话的平均歌曲播放次数</li><li id="69a2" class="ly lz it kd b ke mh ki mi km mj kq mk ku ml ky oq me mf mg bi translated">总赞数</li><li id="3fef" class="ly lz it kd b ke mh ki mi km mj kq mk ku ml ky oq me mf mg bi translated">最连续几天不播放歌曲</li></ul><p id="1be6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">看起来，与错误和歌曲播放总量相关的指标与预测客户流失完全无关。</p><h1 id="b04c" class="mn mo it bd mp mq mr ms mt mu mv mw mx my od na nb nc oe ne nf ng of ni nj nk bi translated">四。AWS 集群部署</h1><p id="15ec" class="pw-post-body-paragraph kb kc it kd b ke nl kg kh ki nm kk kl km nn ko kp kq no ks kt ku np kw kx ky im bi translated">到目前为止，我一直在描述我在笔记本电脑上使用 Spark 的本地模式对小型 128MB 数据集执行的分析。</p><p id="92ae" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了对 12GB 的数据集进行同样的分析，我需要更多的处理能力。这就是 AWS 弹性地图简化(EMR)和弹性云计算(EC2)的用武之地。</p><p id="dc04" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">注意:EC2 可以让你在云中的机器上工作。EMR 用于轻松管理已经安装了 Spark 的 EC2 集群</em>。</p><p id="a2ef" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在设置好 EMR 并通过我的终端访问 AWS 之后，我将我的脚本上传到我的集群的 Hadoop 分布式文件系统(HDFS)，从 Udacity 的 S3 存储桶加载包含 12GB 数据集的数据，并从我的终端在主节点上运行该脚本。</p><p id="9165" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">注意:</em> <a class="ae mm" href="https://www.perfectlyrandom.org/2018/08/11/setup-spark-cluster-on-aws-emr/" rel="noopener ugc nofollow" target="_blank"> <em class="kz">这里是我用来设置 EMR 的</em> </a> <em class="kz">和</em> <a class="ae mm" href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html#install-bundle-other-os" rel="noopener ugc nofollow" target="_blank"> <em class="kz">这里是我用来设置我在终端上访问 AWS 的</em> </a> <em class="kz">。</em></p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi or"><img src="../Images/b91433a3032173193c2d0e9bd72b6370.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*9rI4FzyhELq1zsoynO6u4A.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Don’t even try to read this. Just FYI: this is what it looks like when you run a Spark app through EMR from your CLI/terminal.</figcaption></figure><p id="8ab0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个脚本需要很长时间才能完成(我在 4 台机器上花了大约 5 个小时)。为了不浪费所有这些辛苦的工作，我在脚本的最后保存了模型和预处理过的用户矩阵。</p><p id="efa6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果您感到好奇，在完整的 12GB 数据集上运行此分析和工作流会产生非常高的准确性和 F-1 分数！</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi os"><img src="../Images/9e95419537648b4f8885190655d3ee41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*IPpJ9ppOB6rURYtXTP3lKw.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">This was a HUGE improvement from the model trained on 128MB dataset.</figcaption></figure><h1 id="0169" class="mn mo it bd mp mq mr ms mt mu mv mw mx my od na nb nc oe ne nf ng of ni nj nk bi translated">动词 （verb 的缩写）总结</h1><p id="1b18" class="pw-post-body-paragraph kb kc it kd b ke nl kg kh ki nm kk kl km nn ko kp kq no ks kt ku np kw kx ky im bi translated">更多的功能可以从用户活动中设计出来，比如每天/每周/每月的竖起大拇指数，竖起大拇指与不竖起大拇指的比率，等等。特征工程可以比简单地优化一种算法更好地改善结果。</p><p id="1036" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，可以做进一步的工作，从我们的交易用户数据中提取更多的特征来改进我们的预测！</p><p id="1e7e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一旦一个模型被创建，也许它可以被部署到生产中，并且每隔 x 天或 x 小时运行一次。一旦我们预测到用户可能会流失，我们就有机会进行干预！</p><p id="0843" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了评估这个假设部署的模型做得有多好，我们可以运行一些概念验证分析，并且在给定的测试期间不干预它的预测。如果它预测的用户将以比普通用户更高的速度流失，这可以表明我们的模型工作正常！</p><h1 id="3f59" class="mn mo it bd mp mq mr ms mt mu mv mw mx my od na nb nc oe ne nf ng of ni nj nk bi translated">不及物动词总结想法</h1><p id="de7b" class="pw-post-body-paragraph kb kc it kd b ke nl kg kh ki nm kk kl km nn ko kp kq no ks kt ku np kw kx ky im bi translated">学习 PySpark 和 AWS 好像是一场噩梦(我从这次经历中知道)。然而，如果你已经熟悉 Python 和机器学习，你不需要知道太多的<em class="kz">来开始使用 PySpark 和 AWS。</em></p><p id="1cfe" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你知道如何处理数据。你知道机器学习是如何工作的。战斗的另一半是知道如何在 PySpark 和 AWS 集群上执行这些相同的任务，这可以通过 Google 搜索和教程找到！我建议慢慢浏览我的(或其他人的)<a class="ae mm" href="http://patrickdeguzman.me/notebooks/SparkifyAnalysis.html" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>以便你对 PySpark 有所了解，然后查找如何在 AWS EMR 上运行 Spark 应用程序。</p><p id="af37" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这看起来工作量很大，但是我根本不可能用笔记本电脑处理 12GB 的数据集。每天都有令人难以置信的大量数据被创建，如果你想处理这么多数据，Spark 是一个很好的学习工具！</p></div></div>    
</body>
</html>