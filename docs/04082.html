<html>
<head>
<title>Extending PyTorch with Custom Activation Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用自定义激活函数扩展 PyTorch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/extending-pytorch-with-custom-activation-functions-2d8b065ef2fa?source=collection_archive---------5-----------------------#2019-06-27">https://towardsdatascience.com/extending-pytorch-with-custom-activation-functions-2d8b065ef2fa?source=collection_archive---------5-----------------------#2019-06-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/eef73b0b349e33511b3aea0e1b7dfdd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zl4Isa-_9bzh476T-YwmLQ.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://www.pexels.com/@chivozol-43727?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">chivozol </a>from <a class="ae jg" href="https://www.pexels.com/photo/close-up-photography-of-spider-web-167259/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><div class=""/><div class=""><h2 id="a828" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">PyTorch 和深度学习初学者教程</h2></div><h1 id="5006" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">介绍</h1><p id="e700" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">今天，深度学习正在迅速传播，并被应用于各种机器学习问题，如图像识别、语音识别、机器翻译等。有各种高度可定制的神经网络架构，当给定足够的数据时，它们可以适合几乎任何问题。每个神经网络都应该被精心设计以足够好地适应给定的问题。您必须微调网络的超参数(学习率、下降系数、权重衰减和许多其他参数)以及隐藏层的数量和每层中的单元数量。<strong class="ls jk">为每一层选择正确的激活函数也至关重要，可能会对模型的度量分数和训练速度产生重大影响</strong>。</p><h1 id="6265" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">激活功能</h1><p id="9c4f" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">激活函数</a>是每个神经网络的基本构建模块。我们可以从流行的深度学习框架的大量流行激活函数中进行选择，如<a class="ae jg" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>、<a class="ae jg" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> Sigmoid </a>、<a class="ae jg" href="https://en.wikipedia.org/wiki/Hyperbolic_function" rel="noopener ugc nofollow" target="_blank"> Tanh </a>以及许多其他函数。</p><p id="9a57" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">然而，要创建一个特别为您的任务定制的最先进的模型，您可能需要使用自定义激活功能，这是您正在使用的深度学习框架中所没有的。激活功能可以根据复杂性大致分为以下几组:</p><ol class=""><li id="4f0b" class="mr ms jj ls b lt mm lw mn lz mt md mu mh mv ml mw mx my mz bi translated"><strong class="ls jk">简单的激活功能</strong>如<a class="ae jg" href="https://arxiv.org/pdf/1710.09967.pdf" rel="noopener ugc nofollow" target="_blank">路斯</a>、<a class="ae jg" href="https://arxiv.org/pdf/1710.09967.pdf" rel="noopener ugc nofollow" target="_blank">平方根倒数单元(ISRU) </a>。你可以使用任何深度学习框架快速实现这些功能。</li><li id="c16c" class="mr ms jj ls b lt na lw nb lz nc md nd mh ne ml mw mx my mz bi translated"><strong class="ls jk">具有可训练参数</strong>的激活功能，如<a class="ae jg" href="https://arxiv.org/pdf/1602.01321.pdf" rel="noopener ugc nofollow" target="_blank">软指数</a>激活或<a class="ae jg" href="https://arxiv.org/pdf/1512.07030.pdf" rel="noopener ugc nofollow" target="_blank"> S 形整流线性单元(SReLU) </a>。</li><li id="f797" class="mr ms jj ls b lt na lw nb lz nc md nd mh ne ml mw mx my mz bi translated"><strong class="ls jk">激活功能，在某些点</strong>不可微，需要自定义实现后退步骤，例如<a class="ae jg" href="https://arxiv.org/pdf/1709.04054.pdf" rel="noopener ugc nofollow" target="_blank">双极整流线性单元(BReLU) </a>。</li></ol><p id="636a" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在本教程中，我将介绍使用<a class="ae jg" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>框架实现和演示所有这些类型的函数。你可以在 GitHub 上找到这篇文章<a class="ae jg" href="https://github.com/Lexie88rus/Activation-functions-examples-pytorch/blob/master/custom_activations_example.py" rel="noopener ugc nofollow" target="_blank">的所有代码。</a></p><h1 id="dde2" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">安装</h1><p id="e12b" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">要浏览激活函数的实现示例，您需要:</p><ul class=""><li id="a170" class="mr ms jj ls b lt mm lw mn lz mt md mu mh mv ml nf mx my mz bi translated">安装<a class="ae jg" href="https://pytorch.org/get-started/locally/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>，</li><li id="c6aa" class="mr ms jj ls b lt na lw nb lz nc md nd mh ne ml nf mx my mz bi translated">要将必要的导入添加到脚本中，</li></ul><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nk nl l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">The necessary imports</figcaption></figure><ul class=""><li id="c743" class="mr ms jj ls b lt mm lw mn lz mt md mu mh mv ml nf mx my mz bi translated">为演示准备数据集。我们将使用众所周知的<a class="ae jg" href="https://www.kaggle.com/zalando-research/fashionmnist" rel="noopener ugc nofollow" target="_blank">时尚 MNIST 数据集</a>。</li></ul><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nk nl l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Prepare the dataset</figcaption></figure><p id="cf1e" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">最后一件事是建立一个示例函数，它运行模型训练过程并打印出每个时期的训练损失:</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nk nl l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">A sample model training function</figcaption></figure><p id="6163" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">现在已经为创建带有定制激活功能的模型做好了一切准备。</p><h1 id="a354" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">实现简单的激活功能</h1><p id="7023" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">最简单常见的激活功能</p><ul class=""><li id="cdd7" class="mr ms jj ls b lt mm lw mn lz mt md mu mh mv ml nf mx my mz bi translated"><strong class="ls jk">是可微的</strong>并且不需要手动执行后退步骤，</li><li id="e721" class="mr ms jj ls b lt na lw nb lz nc md nd mh ne ml nf mx my mz bi translated"><strong class="ls jk">没有任何可训练参数</strong>。它们的所有参数都应该预先设定。</li></ul><p id="a7f4" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这种简单功能的一个例子是<a class="ae jg" href="https://arxiv.org/pdf/1606.08415.pdf" rel="noopener ugc nofollow" target="_blank"> Sigmoid 线性单元或路斯</a>，也称为 Swish-1:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/150232b9fb1406d5fc3e598173d7e4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nnhq9rjk10wnZIVLlWYQqQ.png"/></div></div></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/792b57ae0deae87d43f275e1bc41fc44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BTyFlvabpQFqe-rASHc1Qg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">SiLU</figcaption></figure><p id="d420" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这样一个简单的激活函数可以像 Python 函数一样简单地实现:</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="6c4f" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">所以现在路斯可以用在用神经网络创建的模型中。顺序:</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="33de" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">或者在一个简单的模型中，它扩展了 nn。模块类别:</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="6fad" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">用可训练参数实现激活功能</h1><p id="6326" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">有很多带参数的激活函数，可以在训练模型的同时用梯度下降法进行训练。其中一个很好的例子是<a class="ae jg" href="https://arxiv.org/pdf/1602.01321.pdf" rel="noopener ugc nofollow" target="_blank">软指数</a>函数:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/ffaccef6d58cfca0cc3484d37fadad8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViIYhQB334EMHeUThAhhLA.png"/></div></div></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/1978665d957b1d8e5da7b5c016938cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBntY9vMO6NzGVtcqfsvnQ.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Soft Exponential</figcaption></figure><p id="4c8f" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">要使用可训练参数实现激活功能，我们必须:</p><ul class=""><li id="ca7b" class="mr ms jj ls b lt mm lw mn lz mt md mu mh mv ml nf mx my mz bi translated">从 nn 派生一个类。模块，并使该参数成为其成员之一，</li><li id="a10e" class="mr ms jj ls b lt na lw nb lz nc md nd mh ne ml nf mx my mz bi translated">将参数包装为 PyTorch 参数，并将 requiresGrad 属性设置为 True。</li></ul><p id="f41d" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">以下是软指数的一个例子:</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="863a" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">现在，我们可以在模型中使用软指数，如下所示:</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="f073" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">用自定义后退步骤实现激活功能</h1><p id="533f" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">激活功能的完美例子是<a class="ae jg" href="https://arxiv.org/pdf/1709.04054.pdf" rel="noopener ugc nofollow" target="_blank"> BReLU </a>(双极性整流线性单元)，它需要实现自定义的后退步骤:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/ab914bcb60a7e6584bea3efd82227af0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T3SrcvFCi_VjIz3nFahrFA.png"/></div></div></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/81eeb97916d95baede9a05c1502ad507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_7dBSNKS92XzwBBfFVztQ.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">BReLU</figcaption></figure><p id="7b6d" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">此函数在 0 处不可微，因此自动梯度计算可能会失败。这就是为什么我们应该提供一个自定义的后退步骤来确保稳定的计算。</p><p id="f317" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">要使用后退步骤实现自定义激活功能，我们应该:</p><ul class=""><li id="0a42" class="mr ms jj ls b lt mm lw mn lz mt md mu mh mv ml nf mx my mz bi translated">创建一个继承 torch.autograd 函数的类，</li><li id="8a50" class="mr ms jj ls b lt na lw nb lz nc md nd mh ne ml nf mx my mz bi translated">重写静态向前和向后方法。Forward 方法只是将函数应用于输入。向后方法是在给定损失函数相对于输出的梯度的情况下，计算损失函数相对于输入的梯度。</li></ul><p id="0b13" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们看一个 BReLU 的例子:</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="fbc6" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们现在可以在模型中使用 BReLU，如下所示:</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="0e92" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">结论</h1><p id="ec8d" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在本教程中，我介绍了:</p><ul class=""><li id="de95" class="mr ms jj ls b lt mm lw mn lz mt md mu mh mv ml nf mx my mz bi translated">如何用 PyTorch 创建一个<strong class="ls jk">简单的自定义激活函数</strong>，</li><li id="d0d7" class="mr ms jj ls b lt na lw nb lz nc md nd mh ne ml nf mx my mz bi translated">如何创建一个带有可训练参数的<strong class="ls jk">激活函数，该函数可以使用梯度下降进行训练，</strong></li><li id="b9a0" class="mr ms jj ls b lt na lw nb lz nc md nd mh ne ml nf mx my mz bi translated">如何创建一个自定义后退的<strong class="ls jk">激活函数？</strong></li></ul><p id="6681" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">本教程的所有代码都可以在<a class="ae jg" href="https://github.com/Lexie88rus/Activation-functions-examples-pytorch/blob/master/custom_activations_example.py" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。为<a class="ae jg" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>和<a class="ae jg" href="https://keras.io" rel="noopener ugc nofollow" target="_blank"> Keras </a>实现定制激活功能的其他例子可以在<a class="ae jg" href="https://github.com/digantamisra98/Echo/tree/Dev-adeis" rel="noopener ugc nofollow" target="_blank">这个 GitHub 库</a>中找到。</p><h1 id="9023" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">改进</h1><p id="3833" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在构建许多自定义激活函数时，我注意到它们通常会消耗更多的 GPU 内存。使用 PyTorch 就地方法创建定制激活的<strong class="ls jk">就地实现</strong>改善了这种情况。</p><h1 id="a45d" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">附加参考</h1><p id="4855" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">以下是额外资源和进一步阅读材料的链接:</p><ol class=""><li id="6a82" class="mr ms jj ls b lt mm lw mn lz mt md mu mh mv ml mw mx my mz bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">激活功能 wiki 页面</a></li><li id="5fc5" class="mr ms jj ls b lt na lw nb lz nc md nd mh ne ml mw mx my mz bi translated">【PyTorch 扩展教程</li></ol></div></div>    
</body>
</html>