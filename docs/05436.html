<html>
<head>
<title>t-SNE Python Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">t-SNE Python 示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/t-sne-python-example-1ded9953f26?source=collection_archive---------1-----------------------#2019-08-12">https://towardsdatascience.com/t-sne-python-example-1ded9953f26?source=collection_archive---------1-----------------------#2019-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6d207216b8431098bfdf5290d476f675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*chMM96Q89eAGCC8u"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@mattixdesign?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Matt Wildbore</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="7606" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">t 分布随机近邻嵌入(t-SNE)是一种降维技术，用于在二维或三维的低维空间中表示高维数据集，以便我们可以可视化它。与 PCA 等简单地最大化方差的其他降维算法相比，t-SNE 创建了一个降维的特征空间，其中相似的样本由附近的点建模，不相似的样本由远处的点以高概率建模。</p><p id="20a5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在高层次上，t-SNE 为高维样本构建概率分布，使得相似的样本被挑选的可能性很高，而不相似的点被挑选的可能性极小。然后，t-SNE 为低维嵌入中的点定义了类似的分布。最后，t-SNE 最小化关于嵌入中点的位置的两个分布之间的 kull back-lei bler 散度。</p><h1 id="e240" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">算法</h1><p id="9ce8" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">如前所述，t-SNE 采用高维数据集，并将其简化为保留大量原始信息的低维图。</p><p id="b866" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们有一个由 3 个不同的类组成的数据集。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/c7cff7e2749fc1c8fcbbf8b74837ba42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*R62g-kSX8zfEUzgwOibosQ.png"/></div></figure><p id="e98c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们希望将 2D 图简化为 1D 图，同时保持集群之间的清晰边界。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mm"><img src="../Images/92d60fe06ee6129c1ef77f9ba7e17e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qoozLr_hI_wf8yavTEXx1Q.png"/></div></div></figure><p id="4e15" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下，简单地将数据投影到轴上并不是降维的好方法，因为我们丢失了大量的信息。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mn"><img src="../Images/809ef939aaf0e1b0b994438184d9fcdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I2183keaw5ZJ8adAjVgUvg.png"/></div></div></figure><p id="a997" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相反，我们可以使用降维技术(提示:t-SNE)来实现我们想要的。t-SNE 算法的第一步是测量一个点到其他所有点的距离。我们没有直接处理距离，而是将它们映射成概率分布。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/4d45912672c7bac877a98f4d82886d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*BI0TvFpCZ31dmWQPoFiM6w.png"/></div></figure><p id="c5dc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在分布中，相对于当前点具有最小距离的点具有高可能性，而远离当前点的点具有非常低的可能性。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/8d53bd12271a1507254e150102375451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*qeMomw4p-57gBFiSkZ2vgg.png"/></div></figure><p id="d37e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再看一下 2D 的图，注意蓝色的星团比绿色的更分散。如果我们不解决这种规模上的差异，绿点的可能性将大于蓝点。为了说明这一事实，我们除以可能性的总和。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mq"><img src="../Images/967d0a733ed3e40da3deedf621f38ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yRzHskpDKk9Qq-SzOngmRQ.png"/></div></div></figure><p id="6b2c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，尽管两点之间的绝对距离不同，但它们被认为是相似的。</p><p id="1087" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们试着将这些概念与基本理论联系起来。从数学上讲，我们将正态分布的方程写成如下。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/52b4c22cf365557baaf7c40e04978e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*1X_gbBBSpK8_-i7yvOKmzg.png"/></div></figure><p id="19f7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们抛开指数之前的一切，用另一个点代替平均值，在解决之前讨论的比例问题的同时，我们得到了来自<a class="ae jg" href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的等式。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ms"><img src="../Images/b46d8afeaf1623b3258dcabb393c24d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*5hlTjH4asoV_fZlLoxBDng.png"/></div></div></figure><p id="3759" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们来解决我们如何得出减少的特征空间。首先，我们创建一个<code class="fe mt mu mv mw b">n_samples</code> x <code class="fe mt mu mv mw b">n_components</code>矩阵(在本例中为:9x1)并用随机值(即位置)填充它。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/d3cfdf80111ec22d2c1555a69ae7efcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YZc4E3kr2-nbh9iLyrWWDA.png"/></div></div></figure><p id="aaa9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们采用与上面类似的方法(测量点之间的距离并将它们映射到概率分布)，我们会得到下面的等式。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/dfc28299e7884e8662012519786f36dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*WMRiozYz-gtC632B-ZS_tw.png"/></div></figure><p id="ec4f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，像以前一样，我们采用正态分布的方程，去掉前面的所有内容，使用另一个点代替平均值，并通过除以所有其他点的可能性之和来说明规模(不要问我为什么我们去掉了标准偏差)。</p><p id="4060" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们能想出一些办法，使缩减的特征空间中的点的概率分布接近原始特征空间中的点的概率分布，我们就能得到定义良好的聚类。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mm"><img src="../Images/92d60fe06ee6129c1ef77f9ba7e17e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qoozLr_hI_wf8yavTEXx1Q.png"/></div></div></figure><p id="f821" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了做到这一点，我们利用了一种叫做库尔贝克-莱伯散度的东西。KL 散度是衡量一个概率分布与另一个概率分布的差异程度。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/0be5e3d40c70fbf121648c43aaaab9cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8oQvSPWQ7mKCvkQCK1tD0Q.png"/></div></div></figure><p id="6698" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">KL 散度值越低，两个分布就越接近。0 的 KL 散度意味着所讨论的两个分布是相同的。</p><p id="c0a2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这应该有望带来大量的想法。回想一下，在线性回归的情况下，我们如何通过使用梯度下降来最小化成本函数(即均方误差)来确定最佳拟合线。在 t-SNE 中，我们使用梯度下降来最小化所有数据点上的 Kullback-Leiber 散度之和。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/6565b36b99a5ba7bb6a7cf13ce97c33c.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*Xa60taQr6NcgirUTI1ZtEw.png"/></div></figure><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/0e0f832f12e4772eed8769b7cebb0d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sbbcZPPR-UBXlDMRhGjLAA.png"/></div></div></figure><p id="8f6f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了给出每次更新的方向，我们对每个点的成本函数取偏导数。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/8499836fd0604353cd1fbb0e38e4476b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*cNZHT7O-UJiwXfIDDYaUXw.png"/></div></figure><h1 id="0864" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Python 代码</h1><p id="b63f" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">很多时候，我们利用了一些库，但并没有真正理解库下发生了什么。在下一节中，我将尝试(尽管不成功)用 Python 代码实现算法和相关的数学方程。为了帮助这个过程，我从<code class="fe mt mu mv mw b">scikit-learn</code>库中的<code class="fe mt mu mv mw b">TSNE</code>类的<a class="ae jg" href="https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/manifold/t_sne.py#L481" rel="noopener ugc nofollow" target="_blank">源代码</a>中提取了一些片段。</p><p id="a06b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们将导入以下库并设置一些属性，这些属性将在我们绘制数据时发挥作用。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="ac4a" class="ng lf jj mw b gy nh ni l nj nk">import numpy as np<br/>from sklearn.datasets import load_digits<br/>from scipy.spatial.distance import pdist<br/>from sklearn.manifold.t_sne import _joint_probabilities<br/>from scipy import linalg<br/>from sklearn.metrics import pairwise_distances<br/>from scipy.spatial.distance import squareform<br/>from sklearn.manifold import TSNE<br/>from matplotlib import pyplot as plt<br/>import seaborn as sns<br/>sns.set(rc={'figure.figsize':(11.7,8.27)})<br/>palette = sns.color_palette("bright", 10)</span></pre><p id="1dd4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这个例子，我们将使用手绘数字。<code class="fe mt mu mv mw b">scikit-learn</code>库提供了将它们导入我们程序的方法。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="ac64" class="ng lf jj mw b gy nh ni l nj nk">X, y = load_digits(return_X_y=True)</span></pre><p id="4781" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">鉴于 t-SNE 严格用于可视化，我们只能看到最多 3 维的事物，我们将选择 2 或 3 作为组件的数量。另一方面，困惑与算法中使用的最近邻的数量有关。不同的困惑会导致最终结果的剧烈变化。在我们的例子中，我们将其设置为 t-SNE 的<code class="fe mt mu mv mw b">scitkit-learn</code>实现的默认值(30)。根据<code class="fe mt mu mv mw b">numpy</code>文档，机器ε是最小的可表示正数，因此<code class="fe mt mu mv mw b">1.0 + eps != 1.0</code>。换句话说，任何低于机器ε的数字都不能被计算机操纵，因为它缺少必要的位。正如我们将看到的，贡献者使用<code class="fe mt mu mv mw b">np.maximum</code>来检查矩阵中的值是否小于机器ε，如果它们是，就替换它们。我不明白这背后的原因，所以如果有人能留下评论解释原因，我将不胜感激。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="c09b" class="ng lf jj mw b gy nh ni l nj nk">MACHINE_EPSILON = np.finfo(np.double).eps<br/>n_components = 2<br/>perplexity = 30</span></pre><p id="1db9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们定义<code class="fe mt mu mv mw b">fit</code>函数。当我们转换数据时，我们将调用<code class="fe mt mu mv mw b">fit</code>函数。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="50ab" class="ng lf jj mw b gy nh ni l nj nk">def fit(X):<br/>    n_samples = X.shape[0]<br/>    <br/>    # Compute euclidean distance<br/>    distances = pairwise_distances(X, metric='euclidean', squared=True)<br/>    <br/>    # Compute joint probabilities p_ij from distances.<br/>    P = _joint_probabilities(distances=distances, desired_perplexity=perplexity, verbose=False)<br/>    <br/>    # The embedding is initialized with iid samples from Gaussians with standard deviation 1e-4.<br/>    X_embedded = 1e-4 * np.random.mtrand._rand.randn(n_samples, n_components).astype(np.float32)<br/>    <br/>    # degrees_of_freedom = n_components - 1 comes from<br/>    # "Learning a Parametric Embedding by Preserving Local Structure"<br/>    # Laurens van der Maaten, 2009.<br/>    degrees_of_freedom = max(n_components - 1, 1)<br/>    <br/>    return _tsne(P, degrees_of_freedom, n_samples, X_embedded=X_embedded)</span></pre><p id="1cbb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个函数中发生了很多事情，所以让我们一步一步地分解它。</p><p id="3df7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.我们将样本数存储在一个变量中，以备将来参考。</p><p id="d9ff" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.我们计算每个数据点之间的欧几里德距离。这对应于前面等式中的<code class="fe mt mu mv mw b">||xi — xj||^2</code>。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/d494bb31983d13c0567821dd21d71e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*OJfI8p9HYur963Z66X7CZQ.png"/></div></figure><p id="3229" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.我们将上一步中计算的欧几里德距离作为参数传递给<code class="fe mt mu mv mw b">_join_probabilities</code>函数，然后该函数计算并返回一个<code class="fe mt mu mv mw b">p_ji</code>值的矩阵(使用相同的等式)。</p><p id="2121" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.我们使用从标准偏差为 1e-4 的高斯分布中随机选择的值来创建缩减的特征空间。</p><p id="426c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.我们定义了<code class="fe mt mu mv mw b">degrees_of_freedom</code>。源代码中有一个注释，告诉你去看看这篇解释他们推理的论文。基本上，经验表明，当我们使用组件数减 1 时，我们会得到更好的结果(粗体)。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/d31720b7ef8657afb1fdce147b164559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*2lAZnNYs9GrzzLdAoeCqdA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Trustworthiness T(12) of low-dimensional representations of the MNIST dataset, the characters dataset, and the 20 newsgroups dataset.</figcaption></figure><p id="70b3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">6.最后，我们调用 tsne 函数，其实现如下。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="90ae" class="ng lf jj mw b gy nh ni l nj nk">def _tsne(P, degrees_of_freedom, n_samples, X_embedded):</span><span id="3a38" class="ng lf jj mw b gy nn ni l nj nk">params = X_embedded.ravel()<br/>    <br/>    obj_func = _kl_divergence<br/>    <br/>    params = _gradient_descent(obj_func, params, [P, degrees_of_freedom, n_samples, n_components])<br/>        <br/>    X_embedded = params.reshape(n_samples, n_components)</span><span id="eb79" class="ng lf jj mw b gy nn ni l nj nk">return X_embedded</span></pre><p id="2df1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个函数并没有太多的内容。首先，我们使用 np.ravel 将向量展平成一个一维数组。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="f68c" class="ng lf jj mw b gy nh ni l nj nk">&gt;&gt;&gt; x = np.array([[1, 2, 3], [4, 5, 6]])<br/>&gt;&gt;&gt; np.ravel(x)</span><span id="e71a" class="ng lf jj mw b gy nn ni l nj nk">array([1, 2, 3, 4, 5, 6])</span></pre><p id="c30c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们使用梯度下降来最小化 kl 散度。一旦完成，我们将嵌入改回 2D 数组并返回它。</p><p id="55f4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们来看看有更多肉的东西。下面的代码块负责计算 kl 散度和梯度形式的误差。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="9604" class="ng lf jj mw b gy nh ni l nj nk">def _kl_divergence(params, P, degrees_of_freedom, n_samples, n_components):<br/>    X_embedded = params.reshape(n_samples, n_components)<br/>    <br/>    dist = pdist(X_embedded, "sqeuclidean")<br/>    dist /= degrees_of_freedom<br/>    dist += 1.<br/>    dist **= (degrees_of_freedom + 1.0) / -2.0<br/>    Q = np.maximum(dist / (2.0 * np.sum(dist)), MACHINE_EPSILON)<br/>    <br/>    # Kullback-Leibler divergence of P and Q<br/>    kl_divergence = 2.0 * np.dot(P, np.log(np.maximum(P, MACHINE_EPSILON) / Q))<br/>    <br/>    # Gradient: dC/dY<br/>    grad = np.ndarray((n_samples, n_components), dtype=params.dtype)<br/>    PQd = squareform((P - Q) * dist)<br/>    for i in range(n_samples):<br/>        grad[i] = np.dot(np.ravel(PQd[i], order='K'),<br/>                         X_embedded[i] - X_embedded)<br/>    grad = grad.ravel()<br/>    c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom<br/>    grad *= c</span><span id="dc4a" class="ng lf jj mw b gy nn ni l nj nk">return kl_divergence, grad</span></pre><p id="a12c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，让我们一步一步地浏览代码。</p><p id="8d63" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.第一部分计算低维图中点的概率分布。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c897166d2b37bec0330580ef6bdf4082.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*j4wWHOXZ1ACJiW3D6Am9CQ.png"/></div></figure><p id="6c1b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者实际上使用了上述方程的一个变体，其中包括自由度。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/23520915a4933e279b955703a496cfc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*8f-bPPsXJaOxYeGEXeBFUw.png"/></div></figure><p id="2eda" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nq">其中α表示学生 t 分布的自由度数量</em></p><p id="a6a0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.我们计算 KL 散度(提示:每当你看到<code class="fe mt mu mv mw b">np.dot</code>就想 sum)。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5dfa267a2317eb1acd3cb1852f1d53e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*g-tc881WWs7rjZCdWM8jEA.png"/></div></figure><p id="d319" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.我们计算梯度(偏导数)。<code class="fe mt mu mv mw b">dist</code>实际上是<code class="fe mt mu mv mw b">yi — yj</code>在:</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2cb52eff7b2e2a4c5ce50d1741a8fe49.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*1yrVIk7902uiVULI2diwaA.png"/></div></figure><p id="cc65" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，他们使用上述方程的一个变型，带有自由度。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/bab3507b52a5068a01e3e9a9de927eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*EVWNPAgjCRl3TaB9Mnnq3A.png"/></div></figure><p id="1076" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nq">其中α代表学生 t 分布的自由度数量</em></p><p id="f7fd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">梯度下降函数通过最小化 KL 散度来更新嵌入中的值。当梯度范数低于阈值时，或者当我们达到最大迭代次数而没有取得任何进展时，我们会提前停止。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="801e" class="ng lf jj mw b gy nh ni l nj nk">def _gradient_descent(obj_func, p0, args, it=0, n_iter=1000,<br/>                      n_iter_check=1, n_iter_without_progress=300,<br/>                      momentum=0.8, learning_rate=200.0, min_gain=0.01,<br/>                      min_grad_norm=1e-7):<br/>    <br/>    p = p0.copy().ravel()<br/>    update = np.zeros_like(p)<br/>    gains = np.ones_like(p)<br/>    error = np.finfo(np.float).max<br/>    best_error = np.finfo(np.float).max<br/>    best_iter = i = it<br/>    <br/>    for i in range(it, n_iter):</span><span id="68c9" class="ng lf jj mw b gy nn ni l nj nk">error, grad = obj_func(p, *args)</span><span id="d68d" class="ng lf jj mw b gy nn ni l nj nk">grad_norm = linalg.norm(grad)</span><span id="4fe2" class="ng lf jj mw b gy nn ni l nj nk">inc = update * grad &lt; 0.0<br/>        dec = np.invert(inc)<br/>        gains[inc] += 0.2<br/>        gains[dec] *= 0.8<br/>        np.clip(gains, min_gain, np.inf, out=gains)<br/>        grad *= gains<br/>        update = momentum * update - learning_rate * grad<br/>        p += update</span><span id="fe9c" class="ng lf jj mw b gy nn ni l nj nk">print("[t-SNE] Iteration %d: error = %.7f,"<br/>                      " gradient norm = %.7f"<br/>                      % (i + 1, error, grad_norm))<br/>        <br/>        if error &lt; best_error:<br/>                best_error = error<br/>                best_iter = i<br/>        elif i - best_iter &gt; n_iter_without_progress:<br/>            break<br/>        <br/>        if grad_norm &lt;= min_grad_norm:<br/>            break</span><span id="eba7" class="ng lf jj mw b gy nn ni l nj nk">return p</span></pre><p id="c5fb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你已经走了这么远，给自己一点鼓励。我们准备用我们的数据调用<code class="fe mt mu mv mw b">fit</code>函数。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="579a" class="ng lf jj mw b gy nh ni l nj nk">X_embedded = fit(X)</span></pre><p id="8270" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所见，该模型在根据像素位置分离不同数字方面做得相当不错。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="45e8" class="ng lf jj mw b gy nh ni l nj nk">sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/edd377adbdb08e30b94d7168c7d1489c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ls3VakYQFb_UO_Ift9wmnw.png"/></div></div></figure><p id="76e5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用 t-SNE 的<code class="fe mt mu mv mw b">scikit-learn</code>实现做同样的事情。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="ae3a" class="ng lf jj mw b gy nh ni l nj nk">tsne = TSNE()</span><span id="d8f7" class="ng lf jj mw b gy nn ni l nj nk">X_embedded = tsne.fit_transform(X)</span></pre><p id="66b2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，该模型成功地获得了一个 64 维的数据集，并将其投影到一个 2 维空间中，使得相似的样本聚集在一起。</p><pre class="mi mj mk ml gt nc mw nd ne aw nf bi"><span id="b54f" class="ng lf jj mw b gy nh ni l nj nk">sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/ba61109d0cc702c322cbc46137380b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eq_EH_5gRNx0guAPZdu6_Q.png"/></div></div></figure></div></div>    
</body>
</html>