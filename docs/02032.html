<html>
<head>
<title>Graduating in GANs: Going from understanding generative adversarial networks to running your own</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">毕业于 GANs:从理解生成性对抗网络到经营自己的网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graduating-in-gans-going-from-understanding-generative-adversarial-networks-to-running-your-own-39804c283399?source=collection_archive---------4-----------------------#2019-04-04">https://towardsdatascience.com/graduating-in-gans-going-from-understanding-generative-adversarial-networks-to-running-your-own-39804c283399?source=collection_archive---------4-----------------------#2019-04-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7048" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">阅读如何生成对抗性网络(GANs)的研究和评估已经发展，然后实现自己的 GAN 生成手写数字</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/cd84f30d0142dcf4e169d52ce7e300d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R7A0qNVuOjgGh6pj4WUq5A.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Visualization of the latent space for the MNIST dataset — you can make your own GAN that generates MNIST-like handwritten digits later in the post!</figcaption></figure><p id="84e9" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">生殖对抗网络(GANs)已经接管了公众的想象力——通过人工智能生成的名人渗透流行文化，创造出在高端艺术拍卖会上以数千美元的价格在<a class="ae ly" href="https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx" rel="noopener ugc nofollow" target="_blank">出售的艺术。</a></p><p id="3ca8" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在本帖中，我们将探索:</p><ul class=""><li id="2b54" class="lz ma iq le b lf lg li lj ll mb lp mc lt md lx me mf mg mh bi translated"><strong class="le ir">甘斯简介</strong></li><li id="cd23" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated"><strong class="le ir">了解和评估 GANs </strong></li><li id="c577" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated"><strong class="le ir">经营自己的 GAN </strong></li></ul><p id="cc09" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">有大量的资源可以了解 GANs，所以本文的重点是了解如何评估 GANs。我们还将指导您运行自己的 GAN 来生成手写数字，如 MNIST。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mn"><img src="../Images/aff5aa5abfe607c4cf60b0986e1827c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ROOdzdUv2vYlK8VTzLpN3g.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Here’s one run of the GAN we’ll show you how to implement later on — see how the handwritten digits it generates become increasingly realistic as training progresses!</figcaption></figure></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="60b2" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated"><strong class="ak">关于 GANs 的简要介绍</strong></h1><p id="51b2" class="pw-post-body-paragraph lc ld iq le b lf ng jr lh li nh ju lk ll ni ln lo lp nj lr ls lt nk lv lw lx ij bi translated">自 2014 年 Ian Goodfellow 的'<a class="ae ly" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">Generative Adversarial Networks</a>'论文发表以来，GANs 取得了爆炸式的进展，并带来了越来越现实的成果。</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="1581" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">就在三年前，你可以在<a class="ae ly" href="https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/" rel="noopener ugc nofollow" target="_blank">这个 Reddit 帖子</a>上找到 Ian Goodfellow 对一个询问你是否可以用 GANs 发短信的用户的回复:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nn"><img src="../Images/35cb96b13bbb64e01a89f3fb08d4543f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hav0RPbTKB1lmLZ8eLllBQ.png"/></div></div></figure><blockquote class="no np nq"><p id="b192" class="lc ld nr le b lf lg jr lh li lj ju lk ns lm ln lo nt lq lr ls nu lu lv lw lx ij bi translated">“gan 尚未应用于 NLP，因为 gan 仅针对实值数据定义。</p><p id="8f8a" class="lc ld nr le b lf lg jr lh li lj ju lk ns lm ln lo nt lq lr ls nu lu lv lw lx ij bi translated">gan 的工作原理是训练一个输出合成数据的生成器网络，然后对合成数据运行一个鉴别器网络。鉴频器网络输出相对于合成数据的梯度告诉您如何稍微改变合成数据，使其更加真实。</p><p id="1dec" class="lc ld nr le b lf lg jr lh li lj ju lk ns lm ln lo nt lq lr ls nu lu lv lw lx ij bi translated">只有在基于连续数字的情况下，您才能对合成数据进行细微的更改。如果是基于离散数，就没办法稍作改动。</p><p id="6b9d" class="lc ld nr le b lf lg jr lh li lj ju lk ns lm ln lo nt lq lr ls nu lu lv lw lx ij bi translated">例如，如果输出像素值为 1.0 的图像，可以在下一步将该像素值更改为 1.0001。</p><p id="e5b4" class="lc ld nr le b lf lg jr lh li lj ju lk ns lm ln lo nt lq lr ls nu lu lv lw lx ij bi translated">如果输出“企鹅”这个词，下一步就不能把那个改成“企鹅+ .001”，因为没有“企鹅+ .001”这个词。你要从“企鹅”一路走到“鸵鸟”。</p><p id="b2db" class="lc ld nr le b lf lg jr lh li lj ju lk ns lm ln lo nt lq lr ls nu lu lv lw lx ij bi translated">因为所有的 NLP 都是基于离散值的，比如单词、字符或字节，所以还没有人真正知道如何将 GANs 应用于 NLP。"</p></blockquote><p id="f687" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">现在，GANs 正被用于创建各种内容，包括图像、视频、音频和文本。这些输出可以作为训练其他模型的合成数据，或者只是用来催生有趣的副业，比如【thispersondoesnotexist.com】<a class="ae ly" href="https://thisairbnbdoesnotexist.com/" rel="noopener ugc nofollow" target="_blank">【thisairbnbdoesnotexist.com/】</a><a class="ae ly" href="https://medium.com/comet-ml/this-machine-learning-medium-post-does-not-exist-c4705215b4a0" rel="noopener">这种机器学习媒介岗位不存在</a>。😎</p><h2 id="ae69" class="nv mp iq bd mq nw nx dn mu ny nz dp my ll oa ob na lp oc od nc lt oe of ne og bi translated">甘背后</h2><p id="5efc" class="pw-post-body-paragraph lc ld iq le b lf ng jr lh li nh ju lk ll ni ln lo lp nj lr ls lt nk lv lw lx ij bi translated">GAN 由两个神经网络组成，一个是从零开始合成新样本的<strong class="le ir">生成器</strong>，另一个是将训练样本与生成器生成的样本进行比较的<strong class="le ir">鉴别器</strong>。鉴别器的目标是区分“真实”和“虚假”输入(即分类样本是来自模型分布还是真实分布)。正如我们所描述的，这些样本可以是图像、视频、音频片段和文本。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="ab gu cl oh"><img src="../Images/f18351529a468e1f356973d7967ccb16.png" data-original-src="https://miro.medium.com/v2/format:webp/1*KF-XzsW2F44sCxlgdDy_9w.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Simple GAN overview from <a class="oi oj ep" href="https://medium.com/u/facae13d21a?source=post_page-----39804c283399--------------------------------" rel="noopener" target="_blank">Kiran Sudhir</a></figcaption></figure><p id="94ca" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">为了合成这些新样本，生成器被给予随机噪声，并试图从训练数据的学习分布中生成逼真的图像。</p><p id="9993" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">鉴别器网络(卷积神经网络)的输出相对于合成数据的梯度告知如何稍微改变合成数据以使其更真实。最终，生成器收敛于再现真实数据分布的参数，并且鉴别器无法检测到差异。</p><p id="563e" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">您可以在 GAN Lab 中看到并体验这些聚合数据分布:</strong></p><div class="ok ol gp gr om on"><a href="https://poloclub.github.io/ganlab/" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ir gy z fp os fr fs ot fu fw ip bi translated">甘实验室:在你的浏览器中玩生成性对抗网络！</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">GAN 实验室是由 Minsuk Kahng，Nikhil Thorat，Polo Chau，Fernanda Viégas 和 Martin Wattenberg 创建的，他们是…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">poloclub.github.io</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb kw on"/></div></div></a></div><p id="b241" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">这里有一些关于甘斯的最佳指南:</strong></p><ul class=""><li id="7c56" class="lz ma iq le b lf lg li lj ll mb lp mc lt md lx me mf mg mh bi translated"><a class="ae ly" href="https://www.youtube.com/watch?v=5WoItGTWV54" rel="noopener ugc nofollow" target="_blank">斯坦福 CS231 第 13 讲——生成模型</a></li><li id="54a9" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated"><a class="ae ly" href="https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank">基于风格的 GANs </a></li><li id="f7e8" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated"><a class="ae ly" rel="noopener" target="_blank" href="/understanding-generative-adversarial-networks-gans-cd6e4651a29">理解生成性对抗网络</a></li><li id="880a" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated"><a class="ae ly" href="https://heartbeat.fritz.ai/introduction-to-generative-adversarial-networks-gans-35ef44f21193" rel="noopener ugc nofollow" target="_blank">生成性对抗网络简介</a></li><li id="70cf" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated"><a class="ae ly" href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html" rel="noopener ugc nofollow" target="_blank">莉莲·翁:从甘到 WGAN </a></li><li id="d279" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated"><a class="ae ly" href="https://medium.freecodecamp.org/dive-head-first-into-advanced-gans-exploring-self-attention-and-spectral-norm-d2f7cdb55ede" rel="noopener ugc nofollow" target="_blank">一头扎进高级甘斯:探索自我关注和光谱标准</a></li><li id="d8ed" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">Guim Perarnau:奇异的甘以及在哪里可以找到他们</li></ul><h1 id="1be8" class="mo mp iq bd mq mr pc mt mu mv pd mx my jw pe jx na jz pf ka nc kc pg kd ne nf bi translated"><strong class="ak">了解和评估 GANs </strong></h1><p id="c652" class="pw-post-body-paragraph lc ld iq le b lf ng jr lh li nh ju lk ll ni ln lo lp nj lr ls lt nk lv lw lx ij bi translated">量化 GAN 的进展可能感觉非常主观— <em class="nr">“这个生成的人脸看起来足够逼真吗？”、“这些生成的图像足够多样吗？”—</em>GANs 可能感觉像是黑匣子，不清楚模型的哪些组成部分会影响学习或结果质量。</p><p id="3e4d" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">为此，麻省理工学院计算机科学和人工智能(CSAIL)实验室的一个小组最近发布了一篇论文，“<a class="ae ly" href="https://arxiv.org/abs/1811.10597" rel="noopener ugc nofollow" target="_blank"> GAN 解剖:可视化和理解生成性对抗网络</a>”,介绍了一种可视化 GAN 的方法，以及 GAN 单元如何与图像中的对象以及对象之间的关系相关联。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/19e709dc49b7e87b38e716ed05acc4dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*VlaaPHv0xm_xxyhcAW1u1g.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Figure 1 from<a class="ae ly" href="https://arxiv.org/pdf/1811.10597v2.pdf" rel="noopener ugc nofollow" target="_blank"> Bau et. al 2019 </a>showing image modification through intervention with certain GAN units.</figcaption></figure><p id="eddc" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">使用基于分段的网络剖析方法，本文的框架允许我们剖析和可视化发电机神经网络的内部工作。这是通过寻找一组 GAN 单元(称为神经元)与输出图像中的概念(如树、天空、云等)之间的一致来实现的。因此，我们能够识别负责某些物体(如建筑物或云)的神经元。</p><p id="f9cf" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">使神经元具有这种粒度级别允许通过强制激活和去激活(消融)那些对象的相应单元来编辑现有图像(例如，添加或移除图像中所示的树)。</p><p id="d6e0" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">然而，尚不清楚网络是否能够推理场景中的对象，或者它是否只是记忆这些对象。接近这个问题答案的一个方法是试图以不切实际的方式扭曲图像。也许麻省理工学院 CSAIL 的 GAN Paint 交互式网络演示最令人印象深刻的部分是该模型似乎能够将这些编辑限制在“照片级”更改。如果你试图将草添加到天空中，会发生以下情况:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi pi"><img src="../Images/b3d255c699c4a0e928142cbdbabc25e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*-Hjuuzf28CqzxkS1Pqj-Vw.gif"/></div></div></figure><p id="9b81" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">尽管我们激活了相应的神经元，但 GAN 似乎抑制了后面几层的信号。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi pj"><img src="../Images/9c077a2f75ec890fe5a9156ca0ad2d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3_OJl5dtZKTzf0VNen2qoA.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Figure 11 from <a class="ae ly" href="https://arxiv.org/pdf/1811.10597v2.pdf" rel="noopener ugc nofollow" target="_blank">Bau et. al. 2019</a> shows how the local context for an object impacts the likelihood of the object synthesis (in this case, the likelihood of a door being generated on a building versus on a tree or in the sky).</figcaption></figure><p id="095d" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">另一种可视化 GAN 的有趣方式是进行<strong class="le ir">潜在空间插值</strong>(记住，GAN 通过从已学习的潜在空间采样来生成新的实例)。这是查看生成的样本之间的过渡有多平滑的有用方法。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/10161b3c945ed5d8ed9a74b378184a64.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*cLM6c7EUf_IQYNj_yXBKYA.gif"/></div></figure><p id="35c5" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">这些可视化可以帮助我们理解 GAN 的内部表示，但找到量化的方法来理解 GAN 的进展和输出质量仍然是一个活跃的研究领域。</strong></p><p id="888e" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">图像质量和多样性的两个常用评估指标是:<strong class="le ir">初始得分</strong>和<strong class="le ir">弗雷歇初始距离(FID) </strong>。在 Shane Barratt 和 Rishi Sharma 发布了他们的论文“<a class="ae ly" href="https://arxiv.org/pdf/1801.01973.pdf" rel="noopener ugc nofollow" target="_blank">关于初始分数的注释</a>”之后，大多数从业者已经从初始分数转向 FID。</p><h2 id="d87c" class="nv mp iq bd mq nw nx dn mu ny nz dp my ll oa ob na lp oc od nc lt oe of ne og bi translated">初始得分</h2><p id="966d" class="pw-post-body-paragraph lc ld iq le b lf ng jr lh li nh ju lk ll ni ln lo lp nj lr ls lt nk lv lw lx ij bi translated">Salimans 等人于 2016 年在“<a class="ae ly" href="https://arxiv.org/abs/1606.03498" rel="noopener ugc nofollow" target="_blank">训练 GANs 的改进技术</a>”中发明了 Inception 评分，该评分基于一种启发式方法，即当现实样本通过预先训练的网络(如 ImageNet 上的 Inception)时，应该能够对其进行分类。从技术上讲，这意味着样本应该具有低熵 softmax 预测向量。</p><p id="9381" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">除了高预测性(低熵)，初始分数还基于所生成样本的差异程度(例如，所生成样本分布的高方差或熵)来评估 GAN。这意味着不应该有任何统治阶级。</p><p id="db8f" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">如果这两个特征都满足了，那么就应该有一个大的初始得分。结合这两个标准的方法是评估样本的条件标签分布和所有样本的边际分布之间的 Kullback-Leibler (KL)散度。</p><h2 id="569f" class="nv mp iq bd mq nw nx dn mu ny nz dp my ll oa ob na lp oc od nc lt oe of ne og bi translated"><strong class="ak">弗雷歇初始距离</strong></h2><p id="5549" class="pw-post-body-paragraph lc ld iq le b lf ng jr lh li nh ju lk ll ni ln lo lp nj lr ls lt nk lv lw lx ij bi translated">由<a class="ae ly" href="https://arxiv.org/abs/1706.08500" rel="noopener ugc nofollow" target="_blank"> Heusel et al. 2017 </a>推出，FID 通过测量生成的图像分布与真实分布之间的距离来估计真实感。FID 将一组生成的样本嵌入到由初始网的特定层给出的特征空间中。该嵌入层被视为连续的多元高斯，然后对生成的数据和真实数据估计均值和协方差。然后使用这两个高斯函数之间的弗雷歇距离(也称为 Wasserstein-2 距离)来量化生成样本的质量。较低的 FID 对应于更相似的真实样本和生成样本。</p><p id="9ba1" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">一个重要的注意事项是，FID 需要一个合适的样本量来给出好的结果(建议大小= 50k 样本)。如果使用的样本太少，最终会高估实际的 FID，估计值会有很大的差异。</p><blockquote class="no np nq"><p id="4f04" class="lc ld nr le b lf lg jr lh li lj ju lk ns lm ln lo nt lq lr ls nu lu lv lw lx ij bi translated">关于不同论文之间的 Inception 分数和 FID 分数的比较，见 Neal Jean 的帖子<a class="ae ly" href="https://nealjean.com/ml/frechet-inception-distance/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></blockquote><h2 id="13c3" class="nv mp iq bd mq nw nx dn mu ny nz dp my ll oa ob na lp oc od nc lt oe of ne og bi translated"><strong class="ak">想看更多吗？</strong></h2><p id="2c30" class="pw-post-body-paragraph lc ld iq le b lf ng jr lh li nh ju lk ll ni ln lo lp nj lr ls lt nk lv lw lx ij bi translated">Aji Borji 的论文“<a class="ae ly" href="http://Pros and Cons of GAN Evaluation Measures" rel="noopener ugc nofollow" target="_blank">GAN 评估方法的利弊</a>”包括一个非常好的表格，更详尽地涵盖了 GAN 评估指标:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi pl"><img src="../Images/c636eaf655416e20f39d939eb5e284f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QvlqEo6I3yJ7GIygDsfC7g.png"/></div></div></figure><p id="72ea" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">有趣的是，其他研究人员正在通过使用特定领域的评估指标来采取不同的方法。</strong>对于文本 GAN，Guy Tevet 和他的团队在他们的论文“<a class="ae ly" href="https://arxiv.org/abs/1810.12686" rel="noopener ugc nofollow" target="_blank">评估文本 GAN 作为语言模型</a>”中提出使用传统的基于概率的语言模型度量来评估 GAN 生成的文本的分布。</p><p id="f4e3" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我的甘有多好？’，Konstantin Shmelkov 和他的团队使用了两种基于图像分类的方法，GAN-train 和 GAN-test，这两种方法分别接近于 GAN 的召回率(多样性)和精确度(图像质量)。你可以在谷歌大脑研究论文中看到这些评估指标，“<a class="ae ly" href="https://arxiv.org/abs/1711.10337" rel="noopener ugc nofollow" target="_blank">GANS 创造了平等吗</a>”中，他们使用三角形数据集来测量不同 GAN 模型的精度和召回率。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="ab gu cl oh"><img src="../Images/e6b28d42d41b206f5ada2526b0882e69.png" data-original-src="https://miro.medium.com/v2/format:webp/1*0qc9oLuZxjeAqt4JBzPw2A.png"/></div></figure><h1 id="167e" class="mo mp iq bd mq mr pc mt mu mv pd mx my jw pe jx na jz pf ka nc kc pg kd ne nf bi translated">运行您自己的 GAN</h1><p id="900e" class="pw-post-body-paragraph lc ld iq le b lf ng jr lh li nh ju lk ll ni ln lo lp nj lr ls lt nk lv lw lx ij bi translated">为了说明 GANs，我们将改编来自 Wouter Bulten 的<a class="ae ly" href="https://www.wouterbulten.nl/blog/tech/getting-started-with-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank">这篇优秀教程</a>，它使用 Keras 和 MNIST 数据集来生成书面数字。</p><p id="80af" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">完整教程笔记本<a class="ae ly" href="https://gist.github.com/ceceshao1/935ea6000c8509a28130d4c55b32fcd6" rel="noopener ugc nofollow" target="_blank">见这里</a>。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mn"><img src="../Images/aff5aa5abfe607c4cf60b0986e1827c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ROOdzdUv2vYlK8VTzLpN3g.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">We’ll be tracking our GAN’s progress by visualizing our loss and accuracy curves but also by checking test outputs using <a class="ae ly" href="http://bit.ly/2WGduCM" rel="noopener ugc nofollow" target="_blank">Comet.ml</a></figcaption></figure></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><p id="ad2d" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">该 GAN 模型将 MNIST 训练数据和随机噪声作为输入(具体地，噪声的随机向量)来生成:</p><ul class=""><li id="40a1" class="lz ma iq le b lf lg li lj ll mb lp mc lt md lx me mf mg mh bi translated">图像(在这种情况下，是手写数字的图像)。<em class="nr">最终，这些生成的图像将类似于 MNIST 数据集的数据分布。</em></li><li id="95f5" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">鉴别器对生成图像的预测</li></ul><p id="24b7" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">生成器</strong>和<strong class="le ir">鉴别器</strong>模型一起形成对抗模型——对于这个例子，如果对抗模型的输出将生成的图像分类为所有输入的真实图像，那么生成器将表现良好。</p><p id="3341" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">看完整的代码<a class="ae ly" href="https://gist.github.com/ceceshao1/935ea6000c8509a28130d4c55b32fcd6" rel="noopener ugc nofollow" target="_blank">这里</a>和完整的彗星实验结果<a class="ae ly" href="https://www.comet.ml/ceceshao1/mnist-gan" rel="noopener ugc nofollow" target="_blank">这里</a></p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="pm nm l"/></div></figure><h2 id="ea34" class="nv mp iq bd mq nw nx dn mu ny nz dp my ll oa ob na lp oc od nc lt oe of ne og bi translated">跟踪模型的进度</h2><p id="81b5" class="pw-post-body-paragraph lc ld iq le b lf ng jr lh li nh ju lk ll ni ln lo lp nj lr ls lt nk lv lw lx ij bi translated">我们能够使用<a class="ae ly" href="http://bit.ly/2WGduCM" rel="noopener ugc nofollow" target="_blank"> Comet.ml </a>来跟踪我们的<strong class="le ir">生成器</strong>和<strong class="le ir">鉴别器</strong>模型的训练进度。</p><p id="5133" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我们正在绘制鉴别模型和对抗模型的准确性和损失图，这里要跟踪的最重要的指标是:</p><ul class=""><li id="5d38" class="lz ma iq le b lf lg li lj ll mb lp mc lt md lx me mf mg mh bi translated">鉴频器的损耗(见右图中的蓝线)——<em class="nr">dis _ loss</em></li><li id="5d6e" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">对抗模型的准确性(见左图中的蓝线)——<em class="nr">ACC _ adv</em></li></ul><p id="178a" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">此处</strong>  <strong class="le ir">见本实验</strong> <a class="ae ly" href="https://www.comet.ml/ceceshao1/mnist-gan/cf310adacd724bf280323e2eef92d1cd/chart" rel="noopener ugc nofollow" target="_blank"> <strong class="le ir">的训练进度。</strong></a></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi pn"><img src="../Images/37469f4ef23eb8af642a38f2629a5625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DlKgtd_dGVwJYcG5ryyvvA.png"/></div></div></figure><p id="6e8a" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">您还想确认您的培训过程确实使用了 GPU，这可以在<a class="ae ly" href="https://www.comet.ml/ceceshao1/mnist-gan/cf310adacd724bf280323e2eef92d1cd/systemMetrics" rel="noopener ugc nofollow" target="_blank">Comet 系统指标选项卡</a>中查看。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi po"><img src="../Images/3bc134396aecb0faae592940ab92a87b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wpQfiZBkohaolz2ll2EKaw.png"/></div></div></figure><p id="5f20" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">您会注意到我们的训练 for-loop 包含了从测试向量中报告图像的代码:</p><pre class="kn ko kp kq gt pp pq pr ps aw pt bi"><span id="65ad" class="nv mp iq pq b gy pu pv l pw px">if i % 500 == 0:<br/>        # Visualize the performance of the generator by producing images from the test vector<br/>        images = net_generator.predict(vis_noise)<br/>        # Map back to original range<br/>        #images = (images + 1 ) * 0.5<br/>        plt.figure(figsize=(10,10))<br/>        <br/>        for im in range(images.shape[0]):<br/>            plt.subplot(4, 4, im+1)<br/>            image = images[im, :, :, :]<br/>            image = np.reshape(image, [28, 28])<br/>            <br/>            plt.imshow(image, cmap='gray')<br/>            plt.axis('off')<br/>        <br/>        plt.tight_layout()<br/>        # plt.savefig('/home/ubuntu/cecelia/deeplearning-resources/output/mnist-normal/{}.png'.format(i))</span><span id="1794" class="nv mp iq pq b gy py pv l pw px">        plt.savefig(r'output/mnist-normal/{}.png'.format(i))</span><span id="2b2f" class="nv mp iq pq b gy py pv l pw px">        experiment.log_image(r'output/mnist-normal/{}.png'.format(i))</span><span id="cba1" class="nv mp iq pq b gy py pv l pw px">        plt.close('all')</span></pre><p id="567c" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我们希望每隔几个步骤就报告生成的输出的部分原因是，我们可以直观地分析我们的生成器和鉴别器模型在生成真实的手写数字和正确地将生成的数字分别分类为“真”或“假”方面的表现。</p><p id="9cc0" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">让我们来看看这些生成的输出！</p><blockquote class="no np nq"><p id="29b0" class="lc ld nr le b lf lg jr lh li lj ju lk ns lm ln lo nt lq lr ls nu lu lv lw lx ij bi translated">在<a class="ae ly" href="https://www.comet.ml/ceceshao1/mnist-gan" rel="noopener ugc nofollow" target="_blank">慧星实验</a>中查看自己生成的输出</p></blockquote><p id="9e40" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">你可以看到生成器模型是如何从这个模糊的灰色输出开始的(见下面的 0.png ),它看起来不像我们期望的手写数字。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi pz"><img src="../Images/90cc8ec48e0d915198129da0e446093f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rU0iXpsFNRqZEzxMUhNpQ.png"/></div></div></figure><p id="2df9" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">随着训练的进行，我们的模型损失下降，生成的数字变得越来越清晰。在以下位置查看生成的输出:</p><p id="f93f" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">步骤 500: </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi qa"><img src="../Images/eea892e6804b71ac52e2815c22ea6c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qWeR68xiUhxv2p0m4WPulA.png"/></div></div></figure><p id="5119" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">步骤 1000: </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi qb"><img src="../Images/1c5c868344291d5194141d1a46ee0277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pVUKT2BI8dDWc4ZttsRYNQ.png"/></div></div></figure><p id="943f" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">步骤 1500: </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi qc"><img src="../Images/867af3cc98eeec2db9903dabc61d6643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rPYZGgyUj1q7vUeeX1CjjQ.png"/></div></div></figure><p id="5ddd" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">最后，在<strong class="le ir">步骤 10，000 — </strong>中，您可以在下面的红色方框中看到一些 GAN 生成的数字样本</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi qd"><img src="../Images/5e58467b8f4a79b0a97edd49f037b9ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rSEigOpg3yz8ChS_gugLAQ.png"/></div></div></figure><p id="f99c" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">一旦我们的 GAN 模型完成训练，我们甚至可以在<a class="ae ly" href="https://www.comet.ml/ceceshao1/mnist-gan/cf310adacd724bf280323e2eef92d1cd/images" rel="noopener ugc nofollow" target="_blank"> Comet 的图形选项卡</a>中以电影形式查看我们报告的输出(只需按下播放按钮！).</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi qe"><img src="../Images/b15ea6907394df59dc8a6d5dc0c9e1b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*c0-2TFoBD8Vn51gU3gUDBQ.gif"/></div></div></figure><p id="e74a" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">为了完成实验，请务必运行<code class="fe qf qg qh pq b">experiment.end()</code>来查看模型和 GPU 使用情况的一些汇总统计数据。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi qi"><img src="../Images/1d98a2ee5cf2f03db47dad73111da7f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fslXUi-iP85ZGPBZecdRhw.png"/></div></div></figure><h2 id="ef4b" class="nv mp iq bd mq nw nx dn mu ny nz dp my ll oa ob na lp oc od nc lt oe of ne og bi translated">迭代你的模型</h2><p id="2d1d" class="pw-post-body-paragraph lc ld iq le b lf ng jr lh li nh ju lk ll ni ln lo lp nj lr ls lt nk lv lw lx ij bi translated">我们可以对模型进行更长时间的训练，看看这是如何影响性能的，但让我们尝试用几个不同的参数进行迭代。</p><p id="952a" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我们尝试的一些参数是:</p><ul class=""><li id="420c" class="lz ma iq le b lf lg li lj ll mb lp mc lt md lx me mf mg mh bi translated">鉴别器的优化器</li><li id="60d4" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">学习率</li><li id="df0b" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">退出概率</li><li id="1326" class="lz ma iq le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">批量</li></ul><p id="a276" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">从 Wouter 的<a class="ae ly" href="https://www.wouterbulten.nl/blog/tech/getting-started-with-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank">原始博文</a>中，他提到了自己在测试参数方面的努力:</p><blockquote class="no np nq"><p id="fc12" class="lc ld nr le b lf lg jr lh li lj ju lk ns lm ln lo nt lq lr ls nu lu lv lw lx ij bi translated">我测试了鉴别器的优化器的<code class="fe qf qg qh pq b">SGD</code>、<code class="fe qf qg qh pq b">RMSprop</code>和<code class="fe qf qg qh pq b">Adam</code>，但是<code class="fe qf qg qh pq b">RMSprop</code>表现最好。<code class="fe qf qg qh pq b">RMSprop</code>使用了低学习率，我将值修剪在-1 和 1 之间。学习速度的小幅下降有助于稳定</p></blockquote><p id="6d23" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我们将尝试将鉴别器的退出概率从 0.4 增加到 0.5，同时增加鉴别器的学习率(从 0.008 增加到 0.0009)和生成器的学习率(从 0.0004 增加到 0.0006)。很容易看出这些变化是如何失去控制的，并且很难跟踪…🤯</p><p id="a7da" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">要创建一个不同的实验，只需再次运行实验定义单元，Comet 将为您的新实验发布一个新的 url！记录你的实验很好，这样你就可以比较不同之处:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi qj"><img src="../Images/08d67884f511fb3c975e23282298ee7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*84UW2r-vio5MFasBAVKFJQ.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk"><a class="ae ly" href="https://www.comet.ml/ceceshao1/mnist-gan/cf310adacd724bf280323e2eef92d1cd/e7cdcbf789674be6af8d9c7cfade1922/compare?experiment-tab=params" rel="noopener ugc nofollow" target="_blank">See the difference </a>between the two experiments’ hyperparameters. Can you spot the differences in learning rate and dropout probability that we made?</figcaption></figure><p id="f31a" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">不幸的是，我们的调整并没有提高模型的性能！事实上，它产生了一些有趣的输出:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi qk"><img src="../Images/16894f89206b527864e84189d4fe2349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zBCfxippM64VSl7-IFOi2Q.png"/></div></div></figure><p id="979f" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">本教程到此为止！如果你喜欢这篇文章，请和你的朋友分享，他可能会觉得这篇文章很有用😎</p><h2 id="0a1f" class="nv mp iq bd mq nw nx dn mu ny nz dp my ll oa ob na lp oc od nc lt oe of ne og bi translated">👉🏼有问题或反馈吗？下面评论！</h2><h2 id="3b34" class="nv mp iq bd mq nw nx dn mu ny nz dp my ll oa ob na lp oc od nc lt oe of ne og bi translated">👉🏼想要更多牛逼的机器学习内容？F <a class="ae ly" href="https://medium.com/comet-ml" rel="noopener">在媒体上关注我们</a>！</h2></div></div>    
</body>
</html>