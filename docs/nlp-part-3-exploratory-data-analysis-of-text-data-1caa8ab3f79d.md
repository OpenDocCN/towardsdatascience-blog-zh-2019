# NLP ç¬¬ 3 éƒ¨åˆ†|æ–‡æœ¬æ•°æ®çš„æ¢ç´¢æ€§æ•°æ®åˆ†æ

> åŸæ–‡ï¼š<https://towardsdatascience.com/nlp-part-3-exploratory-data-analysis-of-text-data-1caa8ab3f79d?source=collection_archive---------3----------------------->

## è®©æˆ‘ä»¬ä»å‘˜å·¥è¯„ä¼°ä¸­æ”¶é›†ä¸€äº›è§è§£

![](img/6b77fb0b73151fa8307a42f0c56db480.png)

Photo by [Luke Chesser](https://unsplash.com/@lukechesser?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

è¿™æ˜¯å…³äºä½¿ç”¨ python çš„ NLP çš„ä¸‰éƒ¨åˆ†ç³»åˆ—çš„ç»§ç»­ã€‚è¯·éšæ„æŸ¥çœ‹æˆ‘çš„å…¶ä»–æ–‡ç« ã€‚([ç¬¬ä¸€éƒ¨åˆ†](/scraping-the-web-using-beautifulsoup-and-python-5df8e63d9de3)ï¼Œ[ç¬¬äºŒéƒ¨åˆ†](/preprocessing-text-data-using-python-576206753c28))

è®©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£æˆ‘ä»¬æ–°æ¸…ç†çš„æ•°æ®é›†ã€‚
æ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA)æ˜¯æ•°æ®åˆ†æå¸ˆç†Ÿæ‚‰ä»–ä»¬çš„æ•°æ®ä»¥æ¨åŠ¨ç›´è§‰å¹¶å¼€å§‹åˆ¶å®šå¯æµ‹è¯•å‡è®¾çš„è¿‡ç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸åˆ©ç”¨æè¿°æ€§ç»Ÿè®¡å’Œå¯è§†åŒ–ã€‚

åƒå¾€å¸¸ä¸€æ ·ï¼Œè®©æˆ‘ä»¬ä»å¯¼å…¥å¿…è¦çš„åº“å¹¶æ‰“å¼€æ•°æ®é›†å¼€å§‹ã€‚

```
import pandas as pd
import numpy as np
import nltk
import pickle
import pyLDAvis.sklearn
from collections import Counter
from textblob import TextBlob
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from wordcloud import WordCloud, ImageColorGenerator
import matplotlib.pyplot as plt
import seaborn as sns%matplotlib inline
pd.options.mode.chained_assignment = None
pd.set_option('display.max_colwidth', 100)with open('indeed_scrape_clean.pkl', 'rb') as pickle_file:
    df = pickle.load(pickle_file)
```

![](img/26a2732d520435617a1c4509f26e7bb0.png)

å¦‚æœæ‚¨è¿˜è®°å¾—æˆ‘ä»¬ä»¥å‰çš„æ•™ç¨‹ï¼Œæˆ‘ä»¬ç»å†äº†ä¸€ç³»åˆ—é¢„å¤„ç†æ­¥éª¤æ¥æ¸…ç†å’Œå‡†å¤‡æˆ‘ä»¬çš„æ•°æ®è¿›è¡Œåˆ†æã€‚æˆ‘ä»¬çš„æœ€ç»ˆæ•°æ®é›†åŒ…å«è®¸å¤šåˆ—ï¼Œä½†æœ€åä¸€åˆ—â€œlemmatizedâ€åŒ…å«æˆ‘ä»¬æœ€ç»ˆæ¸…ç†çš„å•è¯åˆ—è¡¨ã€‚æˆ‘ä»¬å°†è¦†ç›–ç°æœ‰çš„æ•°æ®æ¡†æ¶ï¼Œå› ä¸ºæˆ‘ä»¬åªå¯¹â€œratingâ€å’Œâ€œlemmatizedâ€åˆ—æ„Ÿå…´è¶£ã€‚

```
df = df[['rating', 'lemmatized']]
df.head()
```

![](img/8c3b78637b9e0de5827f01d89dd0709d.png)

# æƒ…æ„Ÿåˆ†æ

æƒ…æ„Ÿåˆ†ææ˜¯ç¡®å®šä½œè€…çš„æ€åº¦æˆ–è§‚ç‚¹çš„è¿‡ç¨‹ï¼Œå…¶èŒƒå›´ä»-1(æ¶ˆææ€åº¦)åˆ° 1(ç§¯ææ€åº¦)ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ TextBlob åº“æ¥åˆ†ææƒ…æ„Ÿã€‚TextBlob çš„æƒ…ç»ª()å‡½æ•°éœ€è¦ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä½†æˆ‘ä»¬çš„â€œlemmatizedâ€åˆ—å½“å‰æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚è®©æˆ‘ä»¬æŠŠåˆ—è¡¨è½¬æ¢æˆä¸€ä¸ªå­—ç¬¦ä¸²ã€‚

```
df['lemma_str'] = [' '.join(map(str,l)) for l in df['lemmatized']]
df.head()
```

![](img/017e858e99e8ee3377da3b10b409ee22.png)

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†â€œlemma_strâ€åˆ—ä¼ é€’åˆ°æƒ…æ„Ÿ()å‡½æ•°ä¸­æ¥è®¡ç®—æƒ…æ„Ÿã€‚å› ä¸ºæˆ‘ä»¬æœ‰â€œè¯„çº§â€æ ï¼Œæˆ‘ä»¬å¯ä»¥éªŒè¯æƒ…æ„Ÿåˆ†æèƒ½å¤Ÿå¤šå¥½åœ°ç¡®å®šä½œè€…çš„æ€åº¦ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ç¡®å®çœ‹åˆ°äº†æ˜æ˜¾çš„é”™è¯¯ï¼Œå› ä¸ºè¯„çº§#5 çš„è¯„çº§ä¸º 5ï¼Œä½†æƒ…ç»ªç›¸å½“ä½ã€‚

```
df['sentiment'] = df['lemma_str'].apply(lambda x: TextBlob(x).sentiment.polarity)
df.head()
```

![](img/0d11802cf8a39503d04440df135ee707.png)

å½“æ¯”è¾ƒæˆ‘ä»¬çš„æƒ…ç»ªç›´æ–¹å›¾æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬å¾—å‡ºçš„ç»å¤§å¤šæ•°æƒ…ç»ªè¯„çº§éƒ½æ˜¯éå¸¸ç§¯æçš„ã€‚å½“æˆ‘ä»¬å°†å…¶ä¸â€œè¯„çº§â€æ è¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç±»ä¼¼çš„æ¨¡å¼å‡ºç°ã€‚æˆ‘ä»¬ä¸ä»…å¯¹æƒ…ç»ªåˆ†æçš„å‡†ç¡®æ€§æ„Ÿåˆ°æ»¡æ„ï¼Œè€Œä¸”å¯ä»¥çœ‹åˆ°å‘˜å·¥å¯¹å…¬å¸çš„æ•´ä½“æ€åº¦éå¸¸ç§¯æã€‚éš¾æ€ªè°·æ­Œç»å¸¸è¢«åˆ—å…¥ç¦å¸ƒæ–¯æœ€ä½³å·¥ä½œåœºæ‰€åå•ã€‚

```
plt.figure(figsize=(50,30))
plt.margins(0.02)
plt.xlabel('Sentiment', fontsize=50)
plt.xticks(fontsize=40)
plt.ylabel('Frequency', fontsize=50)
plt.yticks(fontsize=40)
plt.hist(df['sentiment'], bins=50)
plt.title('Sentiment Distribution', fontsize=60)
plt.show()
```

![](img/f2ced8feffe4189d2d0813dd83bbbd63.png)

```
x_rating = df.rating.value_counts()
y_rating = x_rating.sort_index()
plt.figure(figsize=(50,30))
sns.barplot(x_rating.index, x_rating.values, alpha=0.8)
plt.title("Rating Distribution", fontsize=50)
plt.ylabel('Frequency', fontsize=50)
plt.yticks(fontsize=40)
plt.xlabel('Employee Ratings', fontsize=50)
plt.xticks(fontsize=40)
```

![](img/dea38db3de9ddf1a551a782fea13d298.png)

```
plt.figure(figsize=(30,10))
plt.title('Percentage of Ratings', fontsize=20)
df.rating.value_counts().plot(kind='pie', labels=['Rating5', 'Rating4', 'Rating3', 'Rating2', 'Rating1'],
                              wedgeprops=dict(width=.7), autopct='%1.0f%%', startangle= -20, 
                              textprops={'fontsize': 15})
```

![](img/f2ef534b94ccfe1799da967a5da0e676.png)

```
polarity_avg = df.groupby('rating')['sentiment'].mean().plot(kind='bar', figsize=(50,30))
plt.xlabel('Rating', fontsize=45)
plt.ylabel('Average Sentiment', fontsize=45)
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.title('Average Sentiment per Rating Distribution', fontsize=50)
plt.show()
```

![](img/c5e80cc7d4eb9959c0365e6220d08b88.png)

è®©æˆ‘ä»¬åˆ›å»ºä¸¤ä¸ªé¢å¤–çš„ç‰¹æ€§â€œword_countâ€æ¥ç¡®å®šæ¯æ¬¡è¯„è®ºçš„å­—æ•°ï¼Œåˆ›å»ºâ€œreview_lenâ€æ¥ç¡®å®šæ¯æ¬¡è¯„è®ºçš„å­—æ•°ã€‚

```
df['word_count'] = df['lemmatized'].apply(lambda x: len(str(x).split()))df['review_len'] = df['lemma_str'].astype(str).apply(len)
```

![](img/fff75814778f1f310fe1a7b061bab4e4.png)

è™½ç„¶å·®å¼‚å¹¶ä¸å¤§ï¼Œä½†åŸºäºå­—æ¯å’Œå•è¯è®¡æ•°çš„æœ€é•¿è¯„è®ºä¼¼ä¹æ˜¯è´Ÿé¢å’Œä¸­æ€§çš„ã€‚ä¼¼ä¹å¿ƒæ€€ä¸æ»¡çš„å‘˜å·¥é€šå¸¸ä¼šåœ¨ä»–ä»¬çš„è¯„ä¼°ä¸­æä¾›æ›´å¤šç»†èŠ‚ã€‚è¿™ç§ç»“æœå¹¶ä¸å°‘è§ï¼Œå› ä¸ºäººç±»å€¾å‘äºæŠ±æ€¨ç»†èŠ‚ï¼Œè€Œèµç¾ç®€çŸ­ã€‚è¿™å¯ä»¥é€šè¿‡æ£€æŸ¥ä¸‹é¢çš„ç›¸å…³çŸ©é˜µæ¥è¿›ä¸€æ­¥ç¡®è®¤ã€‚è¯„åˆ†å’Œæƒ…ç»ªéƒ½ä¸â€œreview_lenâ€å’Œâ€œword_countâ€è´Ÿç›¸å…³ã€‚è¿™å¯ä»¥è§£é‡Šç›¸åçš„å…³ç³»ï¼Œå› ä¸ºæ¯æ¬¡è¯„è®ºçš„å­—æ¯å’Œå•è¯æ•°å¢åŠ äº†æ€»ä½“è¯„åˆ†ï¼Œè€Œæƒ…ç»ªä¸‹é™äº†ã€‚ç„¶è€Œï¼Œç›¸å…³æ€§å†æ¬¡ç›¸å½“å°ï¼Œç„¶è€Œæ˜¯è´Ÿçš„ã€‚

```
letter_avg = df.groupby('rating')['review_len'].mean().plot(kind='bar', figsize=(50,30))
plt.xlabel('Rating', fontsize=35)
plt.ylabel('Count of Letters in Rating', fontsize=35)
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.title('Average Number of Letters per Rating Distribution', fontsize=40)
plt.show()
```

![](img/fd6411a50c18a288b69d68d5e9f6c271.png)

```
word_avg = df.groupby('rating')['word_count'].mean().plot(kind='bar', figsize=(50,30))
plt.xlabel('Rating', fontsize=35)
plt.ylabel('Count of Words in Rating', fontsize=35)
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.title('Average Number of Words per Rating Distribution', fontsize=40)
plt.show()
```

![](img/af898ab3ea9211b03e288c8ee03433aa.png)

```
correlation = df[['rating','sentiment', 'review_len', 'word_count']].corr()
mask = np.zeros_like(correlation, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
plt.figure(figsize=(50,30))
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
sns.heatmap(correlation, cmap='coolwarm', annot=True, annot_kws={"size": 40}, linewidths=10, vmin=-1.5, mask=mask)
```

![](img/feb9c5210d75b09acebfc09aa462d342.png)

# è¯é¢‘åˆ†æ

è®©æˆ‘ä»¬æ·±å…¥çœ‹çœ‹å®é™…çš„è¯„è®ºæœ¬èº«ã€‚æœ€å¸¸è§çš„è¯æ˜¯ä»€ä¹ˆï¼ŸæŒ‰è¯„åˆ†æœ€å¸¸è§çš„è¯æœ‰å“ªäº›ï¼Ÿå¯¹è¿™äº›é—®é¢˜çš„å›ç­”å°†ä¸ºè°·æ­Œå‘˜å·¥çš„è§‚ç‚¹æä¾›è¿›ä¸€æ­¥çš„è§è§£ã€‚

NLTK æœ‰ä¸€ä¸ªå¾ˆæ£’çš„åä¸ºâ€œFreqDistâ€çš„åº“ï¼Œå®ƒå…è®¸æˆ‘ä»¬ç¡®å®šè¯­æ–™åº“ä¸­æœ€å¸¸è§æœ¯è¯­çš„æ•°é‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬å•ç‹¬çš„æ ‡è®°åŒ–è¯„è®ºåˆ—è¡¨è½¬æ¢æˆä¸€ä¸ªç»¼åˆçš„å¯é‡å¤æ ‡è®°åˆ—è¡¨ï¼Œå°†æ‰€æœ‰è¯„è®ºå­˜å‚¨åœ¨ä¸€èµ·ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ FreqDist()ä¼ é€’â€œallwordsâ€å¯¹è±¡ï¼Œå¹¶åº”ç”¨â€œmost_common(100)â€å‡½æ•°æ¥è·å– 100 ä¸ªæœ€å¸¸è§çš„å•è¯ã€‚

```
words = df['lemmatized']
allwords = []
for wordlist in words:
    allwords += wordlistprint(allwords)
```

![](img/65b6bdf415de2e75b0623345ef7d9d3c.png)

```
mostcommon = FreqDist(allwords).most_common(100)wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(str(mostcommon))
fig = plt.figure(figsize=(30,10), facecolor='white')
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title('Top 100 Most Common Words', fontsize=100)plt.tight_layout(pad=0)
plt.show()
```

![](img/12963ea850cabf1568ed448ce93bd624.png)

```
mostcommon_small = FreqDist(allwords).most_common(25)
x, y = zip(*mostcommon_small)plt.figure(figsize=(50,30))
plt.margins(0.02)
plt.bar(x, y)
plt.xlabel('Words', fontsize=50)
plt.ylabel('Frequency of Words', fontsize=50)
plt.yticks(fontsize=40)
plt.xticks(rotation=60, fontsize=40)
plt.title('Frequency of 25 Most Common Words', fontsize=60)
plt.show()
```

è¯é¢‘åˆ†æçš„ç»“æœè‚¯å®šæ”¯æŒè¯„è®ºçš„æ€»ä½“ç§¯ææƒ…ç»ªã€‚è¯¸å¦‚â€œä¼Ÿå¤§â€ã€â€œå·¥ä½œâ€ã€â€œäººâ€ã€â€œå›¢é˜Ÿâ€ã€â€œå…¬å¸â€ç­‰æœ¯è¯­éƒ½æŒ‡å‘ä¸€ä¸ªç§¯æçš„å…¬å¸ç¯å¢ƒï¼Œå‘˜å·¥å–œæ¬¢ä¸€èµ·å·¥ä½œã€‚

> åŸºäºè¯¸å¦‚â€œå·¥ä½œâ€ã€â€œè°·æ­Œâ€ã€â€œå·¥ä½œâ€å’Œâ€œå…¬å¸â€ç­‰æœ¯è¯­åœ¨è¯­æ–™åº“ä¸­å‡ºç°é¢‘ç‡å¦‚æ­¤ä¹‹é«˜çš„äº‹å®ï¼Œç§»é™¤å®ƒä»¬å¯èƒ½æ˜¯ä¸ªå¥½ä¸»æ„(å³å°†å®ƒä»¬æ·»åŠ åˆ°æˆ‘ä»¬çš„åœç”¨è¯ä¸­)ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€ä¸ªå…¬å¸æ€»æ˜¯å¯ä»¥æ”¹è¿›çš„ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æ¯ä¸ªè¯„ä»·ä¸­æœ€å¸¸ç”¨çš„è¯ã€‚

![](img/9d3429d551a40cfd7fe753e933e12d9a.png)

è¿™ä¼¼ä¹æ˜¯è¯„åˆ†= 1 çš„è¯„è®ºä¸­æœ€å¸¸è§çš„è¯ï¼Œä¸â€œç®¡ç†â€ã€â€œç»ç†â€ã€â€œäººâ€æœ‰å…³ã€‚æˆ‘ä»¬åœ¨è§£é‡Šè¿™äº›ç»“æœæ—¶å¿…é¡»å°å¿ƒï¼Œå› ä¸ºæ ¹æ®ä¸Šé¢æ‰“å°çš„é¥¼çŠ¶å›¾ï¼Œåªæœ‰ 2%çš„è¯„è®ºè¯„çº§ä¸º 1ã€‚

```
group_by = df.groupby('rating')['lemma_str'].apply(lambda x: Counter(' '.join(x).split()).most_common(25))group_by_0 = group_by.iloc[0]
words0 = list(zip(*group_by_0))[0]
freq0 = list(zip(*group_by_0))[1]plt.figure(figsize=(50,30))
plt.bar(words0, freq0)
plt.xlabel('Words', fontsize=50)
plt.ylabel('Frequency of Words', fontsize=50)
plt.yticks(fontsize=40)
plt.xticks(rotation=60, fontsize=40)
plt.title('Frequency of 25 Most Common Words for Rating=1', fontsize=60)
plt.show()
```

![](img/e1774b6993363d5ff559a6671dddcf96.png)

è¯„åˆ†ä¸º 2 çš„è¯„è®ºæœ‰ä¸€ä¸ªå…±åŒçš„ä¸»é¢˜â€œç»ç†â€ã€â€œç®¡ç†â€ã€‚è¯„çº§åˆ†å¸ƒå†æ¬¡éå¸¸ä¸å‡è¡¡ï¼Œä½†è¿™ç¡®å®ç»™äº†æˆ‘ä»¬ä¸€äº›æ”¹è¿›ç»„ç»‡çš„çº¿ç´¢ã€‚

```
group_by_1 = group_by.iloc[1]
words1 = list(zip(*group_by_1))[0]
freq1 = list(zip(*group_by_1))[1]
plt.figure(figsize=(50,30))
plt.bar(words1, freq1)
plt.xlabel('Words', fontsize=50)
plt.ylabel('Frequency of Words', fontsize=50)
plt.yticks(fontsize=40)
plt.xticks(rotation=60, fontsize=40)
plt.title('Frequency of 25 Most Common Words for Rating=2', fontsize=60)
plt.show()
```

![](img/8664cd751e1a4814478616cbdcce7fd7.png)

å¾ˆéš¾ä»â€œä¸­æ€§â€è¯„çº§ä¸­è·å¾—å‡†ç¡®çš„è§è§£ï¼Œå› ä¸ºå‘˜å·¥å¯¹å…¬å¸æ²¡æœ‰ä»»ä½•è¿‡äºç§¯ææˆ–æ¶ˆæçš„çœ‹æ³•ã€‚è¯è™½å¦‚æ­¤ï¼Œæœ‰è¶£çš„æ˜¯,â€œç®¡ç†â€å†æ¬¡æˆä¸ºåå¤§çƒ­é—¨è¯æ±‡ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå¤§çº¦ 14%çš„å‘˜å·¥å¯¹è°·æ­Œçš„ç®¡ç†å±‚æŒè´Ÿé¢æˆ–ä¸­ç«‹çš„æ€åº¦(æ²¡ä»€ä¹ˆå¥½æˆ–ä¸å¥½çš„å¯è¯´)ã€‚åƒâ€œå·¥ä½œâ€å’Œâ€œè°·æ­Œâ€è¿™æ ·çš„è¯ä¼¼ä¹æ‰­æ›²äº†æ‰€æœ‰è¯„çº§çš„åˆ†å¸ƒï¼Œä»æœªæ¥çš„åˆ†æä¸­åˆ é™¤è¿™äº›è¯æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚

è¯·æ³¨æ„â€œâ€¦â€ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ‰§è¡Œä¸€äº›æ•°æ®å¤„ç†ã€‚ğŸ˜

```
group_by_2 = group_by.iloc[2]
words2 = list(zip(*group_by_2))[0]
freq2 = list(zip(*group_by_2))[1]
plt.figure(figsize=(50,30))
plt.bar(words2, freq2)
plt.xlabel('Words', fontsize=50)
plt.ylabel('Frequency of Words', fontsize=50)
plt.yticks(fontsize=40)
plt.xticks(rotation=60, fontsize=40)
plt.title('Frequency of 25 Most Common Words for Rating=3', fontsize=60)
plt.show()
```

![](img/48c7650b254eb002bec78aa20752fdcb.png)

è¯„åˆ†ä¸º 4 å’Œ 5 çš„æœ¯è¯­éå¸¸ç›¸ä¼¼ï¼Œå› ä¸ºå‘˜å·¥ä¼¼ä¹å–œæ¬¢ä»–ä»¬çš„å·¥ä½œï¼Œå–œæ¬¢ä¸ä»–ä»¬ä¸€èµ·å·¥ä½œçš„äººï¼Œå¹¶ä¸”é‡è§†è°·æ­Œçš„ç¯å¢ƒ/æ–‡åŒ–ã€‚æ¯”å¦‚â€œè®¾è®¡â€ã€â€œå­¦ä¹ æœºä¼šâ€ã€â€œäººâ€ã€â€œæ—¶é—´â€ã€â€œå›¢é˜Ÿâ€éƒ½æœ‰å‡ºç°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬åœ¨æ’åå‰ 10 ä½çš„å•è¯ä¸­æ²¡æœ‰çœ‹åˆ°â€œç®¡ç†â€æˆ–â€œç»ç†â€ã€‚è¿™æ˜¯éå¸¸æœ‰è§åœ°çš„ï¼Œå› ä¸ºå®ƒæœ‰åŠ©äºéªŒè¯è¯„çº§ 1ã€2 å’Œ 3 çš„ç»“æœã€‚æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œè¿™äº›è¯é¢‘(å³ã€‚ç­‰çº§ 4 å’Œ 5)æ˜¯ä»å¤§é‡è¯„è®ºä¸­å¾—å‡ºçš„ï¼Œè¿™åªä¼šå¢åŠ è¿™äº›ç»“æœçš„æœ‰æ•ˆæ€§ï¼›ç®¡ç†å½“ç„¶æ˜¯ä¸€ä¸ªéœ€è¦æ”¹è¿›çš„é¢†åŸŸã€‚

```
group_by_3 = group_by.iloc[3]
words3 = list(zip(*group_by_3))[0]
freq3 = list(zip(*group_by_3))[1]
plt.figure(figsize=(50,30))
plt.bar(words3, freq3)
plt.xlabel('Words', fontsize=50)
plt.ylabel('Frequency of Words', fontsize=50)
plt.yticks(fontsize=40)
plt.xticks(rotation=60, fontsize=40)
plt.title('Frequency of 25 Most Common Words for Rating=4', fontsize=60)
plt.show()
```

![](img/9048586fab68d7772f28e98944a10356.png)

```
group_by_4 = group_by.iloc[4]
words4 = list(zip(*group_by_4))[0]
freq4 = list(zip(*group_by_4))[1]
plt.figure(figsize=(50,30))
plt.bar(words4, freq4)
plt.xlabel('Words', fontsize=50)
plt.ylabel('Frequency of Words', fontsize=50)
plt.yticks(fontsize=40)
plt.xticks(rotation=60, fontsize=40)
plt.title('Frequency of 25 Most Common Words for Rating=5', fontsize=60)
plt.show()
```

![](img/4af23a5a2f3121ad70c17b2fd2574762.png)

# ä¸»é¢˜å»ºæ¨¡

æœ€åï¼Œè®©æˆ‘ä»¬åº”ç”¨ä¸€äº›ä¸»é¢˜å»ºæ¨¡ç®—æ³•æ¥å¸®åŠ©æˆ‘ä»¬çš„è¯„è®ºå¯¼å‡ºç‰¹å®šçš„ä¸»é¢˜ã€‚åœ¨æˆ‘ä»¬ç¡®å®šæ¯ä¸ªè¯„çº§çš„ä¸»é¢˜ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»æ‰§è¡Œä¸€ä¸ªé¢å¤–çš„å¤„ç†æ­¥éª¤ã€‚ç°åœ¨æˆ‘ä»¬çš„æ•°æ®/æ–‡å­—å¯¹æˆ‘ä»¬äººç±»æ¥è¯´ä»ç„¶æ˜¯å¯è¯»çš„ï¼Œè€Œè®¡ç®—æœºåªèƒ½ç†è§£æ•°å­—ã€‚æˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬è½¬æ¢æˆæ•°å­—æˆ–å‘é‡ã€‚

## è®¡æ•°çŸ¢é‡å™¨

å¯¹è®°å·è¿›è¡ŒçŸ¢é‡åŒ–çš„ CountVectorizer æ–¹æ³•å°†æ‰€æœ‰å•è¯/è®°å·è½¬ç½®ä¸ºç‰¹å¾ï¼Œç„¶åæä¾›æ¯ä¸ªå•è¯çš„å‡ºç°æ¬¡æ•°ã€‚ç»“æœç§°ä¸ºæ–‡æ¡£æœ¯è¯­çŸ©é˜µï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºçŸ¢é‡å™¨å¯¹è±¡ã€‚Max_df=0.9 å°†åˆ é™¤å‡ºç°åœ¨ 90%ä»¥ä¸Šè¯„è®ºä¸­çš„å•è¯ã€‚Min_df=25 å°†åˆ é™¤å‡ºç°åœ¨å°‘äº 25 æ¡è¯„è®ºä¸­çš„å•è¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºå¤‡ç”¨çŸ©é˜µä½œä¸º fit_transform()çš„ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ‰€æœ‰å•è¯/ç‰¹å¾çš„åˆ—è¡¨ã€‚ç»“æœå°±æ˜¯æˆ‘ä»¬çš„æ–‡æ¡£æœ¯è¯­çŸ©é˜µã€‚æ¯è¡Œä»£è¡¨å•ä¸ªå‘˜å·¥çš„è¯„ä»·ï¼Œå¹¶ç»Ÿè®¡æ¯ä¸ªè¯/ç‰¹å¾åœ¨æ¯æ¬¡è¯„ä»·ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚

```
tf_vectorizer = CountVectorizer(max_df=0.9, min_df=25, max_features=5000)tf = tf_vectorizer.fit_transform(df['lemma_str'].values.astype('U'))
tf_feature_names = tf_vectorizer.get_feature_names()doc_term_matrix = pd.DataFrame(tf.toarray(), columns=list(tf_feature_names))
doc_term_matrix
```

![](img/4015d074fed4318ccdba8e5ef89fc937.png)

## æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…(LDA)ä¸»é¢˜å»ºæ¨¡

æ—¢ç„¶æˆ‘ä»¬å·²ç»ä¸ºä¸»é¢˜å»ºæ¨¡å‡†å¤‡å¥½äº†æ•°æ®ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…(LDA)æ–¹æ³•æ¥ç¡®å®šè¯­æ–™åº“ä¸­çš„ä¸»é¢˜ã€‚åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å°†äº§ç”Ÿ 10 ä¸ªå•ç‹¬çš„ä¸»é¢˜(å³ n _ ç»„ä»¶)ã€‚ä¸€æ—¦åˆ›å»ºäº†æ¨¡å‹ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æ˜¾ç¤ºç¡®å®šçš„ä¸»é¢˜ã€‚æ¯ä¸ªé¢˜ç›®ç”± 10 ä¸ªå•è¯ç»„æˆã€‚è¯¥å‡½æ•°å°†æœ‰ä¸‰ä¸ªå¿…éœ€çš„å‚æ•°ï¼›LDA æ¨¡å‹ã€æ¥è‡ªæ–‡æ¡£æœ¯è¯­çŸ©é˜µçš„ç‰¹å¾åç§°ä»¥åŠæ¯ä¸ªä¸»é¢˜çš„å­—æ•°ã€‚

```
lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', max_iter=500, random_state=0).fit(tf)no_top_words = 10def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print("Topic %d:" % (topic_idx))
        print(" ".join([feature_names[i]
                          for i in topic.argsort()[:-no_top_words - 1:-1]]))

display_topics(lda_model, tf_feature_names, no_top_words)
```

![](img/b1f7e4e5c1ea18fc329d8b3153db05e8.png)

ç¡®å®š LDA æ¨¡å‹äº§ç”Ÿçš„ä¸»é¢˜å½“ç„¶éœ€è¦ä¸€ç‚¹æƒ³è±¡åŠ›ã€‚æ—¢ç„¶æˆ‘ä»¬çŸ¥é“â€œå·¥ä½œâ€ã€â€œè°·æ­Œâ€ã€â€œå·¥ä½œâ€æ˜¯å¾ˆå¸¸è§çš„ä½œå“æˆ‘ä»¬å‡ ä¹å¯ä»¥å¿½ç•¥å®ƒä»¬ã€‚

ä¸»é¢˜ 0:è‰¯å¥½çš„è®¾è®¡æµç¨‹

è¯é¢˜ 1:è‰¯å¥½çš„å·¥ä½œç¯å¢ƒ

è¯é¢˜ 2:å¼¹æ€§å·¥ä½œæ—¶é—´

ä¸»é¢˜ 3:æŠ€èƒ½åŸ¹å…»

è¯é¢˜ 4:å›°éš¾ä½†æ„‰å¿«çš„å·¥ä½œ

è¯é¢˜ 5:ä¼Ÿå¤§çš„å…¬å¸/å·¥ä½œ

è¯é¢˜ 6:å…³å¿ƒå‘˜å·¥

ä¸»é¢˜ 7:ä¼˜ç§€æ‰¿åŒ…å•†è–ªé…¬

ä¸»é¢˜ 8:å®¢æˆ·æœåŠ¡

è¯é¢˜ä¹:ï¼Ÿ

## çš®å°”æˆ´ç»´æ–¯

pyLDAvis æ˜¯ä¸€ä¸ªäº¤äº’å¼ LDA å¯è§†åŒ– python åº“ã€‚æ¯ä¸ªåœ†åœˆä»£è¡¨ä¸€ä¸ªç‹¬ç‰¹çš„ä¸»é¢˜ï¼Œåœ†åœˆçš„å¤§å°ä»£è¡¨ä¸»é¢˜çš„é‡è¦æ€§ï¼Œæœ€åï¼Œæ¯ä¸ªåœ†åœˆä¹‹é—´çš„è·ç¦»ä»£è¡¨ä¸»é¢˜ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦ã€‚é€‰æ‹©ä¸€ä¸ªä¸»é¢˜/åœ†åœˆå°†æ˜¾ç¤ºä¸€ä¸ªæ°´å¹³æ¡å½¢å›¾ï¼Œæ˜¾ç¤ºä¸è¯¥ä¸»é¢˜æœ€ç›¸å…³çš„ 30 ä¸ªå•è¯ï¼Œä»¥åŠæ¯ä¸ªå•è¯åœ¨è¯¥ä¸»é¢˜å’Œæ•´ä¸ªè¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡ã€‚

ç›¸å…³æ€§åº¦é‡æœ‰åŠ©äºåŒºåˆ†ä¸åŒäº/ä¸“ç”¨äºä¸»é¢˜çš„å•è¯(Î»Î»æ›´æ¥è¿‘ 0.0)å’Œå…·æœ‰è¢«åŒ…æ‹¬åœ¨æ‰€é€‰ä¸»é¢˜ä¸­çš„é«˜æ¦‚ç‡çš„å•è¯(Î»Î»æ›´æ¥è¿‘ 1.0)ã€‚

```
pyLDAvis.enable_notebook()
panel = pyLDAvis.sklearn.prepare(lda_model, tf, tf_vectorizer, mds='tsne')
panel
```

![](img/f84b045da4b4e641a38795d752756705.png)

## TF-IDF

LDA ä¸æ˜¯ä¸»é¢˜å»ºæ¨¡çš„å”¯ä¸€æ–¹æ³•ã€‚è®©æˆ‘ä»¬å°è¯•å¦ä¸€ç§æ–¹æ³•ï¼Œåä¸ºéè´ŸçŸ©é˜µåˆ†è§£(NMF)çš„æ–¹æ³•ï¼Œçœ‹çœ‹æˆ‘ä»¬çš„ä¸»é¢˜æ˜¯å¦å¯ä»¥ç¨å¾®æ›´åŠ æ˜ç¡®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ TF-IDF(æœ¯è¯­é¢‘ç‡-é€†æ–‡æ¡£é¢‘ç‡)æ–¹æ³•ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ç®€å•çš„ CountVectorizer æ–¹æ³•æ¥å¯¹æˆ‘ä»¬çš„å•è¯/æ ‡è®°è¿›è¡ŒçŸ¢é‡åŒ–ã€‚TF-IDF æ–¹æ³•æœ‰åŠ©äºé™ä½é«˜é¢‘è¯çš„æƒé‡/å½±å“(å³åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯â€œå·¥ä½œâ€ã€â€œè°·æ­Œâ€å’Œâ€œå·¥ä½œâ€)ã€‚

ä¸ CountVectorizer æ–¹æ³•éå¸¸ç›¸ä¼¼ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºçŸ¢é‡å™¨å¯¹è±¡ã€‚Max_df=0.9 å°†åˆ é™¤å‡ºç°åœ¨ 90%ä»¥ä¸Šè¯„è®ºä¸­çš„å•è¯ã€‚Min_df=25 å°†åˆ é™¤å‡ºç°åœ¨å°‘äº 25 æ¡è¯„è®ºä¸­çš„å•è¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºå¤‡ç”¨çŸ©é˜µä½œä¸º fit_transform()çš„ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ‰€æœ‰å•è¯/ç‰¹å¾çš„åˆ—è¡¨ã€‚

```
tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df =25, max_features=5000, use_idf=True)tfidf = tfidf_vectorizer.fit_transform(df['lemma_str'])
tfidf_feature_names = tfidf_vectorizer.get_feature_names()doc_term_matrix_tfidf = pd.DataFrame(tfidf.toarray(), columns=list(tfidf_feature_names))
doc_term_matrix_tfidf
```

![](img/58c61ac28143c2370c80a32cc1b6d099.png)

## éè´ŸçŸ©é˜µåˆ†è§£(NMF)

```
nmf = NMF(n_components=10, random_state=0, alpha=.1, init='nndsvd').fit(tfidf)display_topics(nmf, tfidf_feature_names, no_top_words)
```

![](img/94c3423441276812042010c7183e66a0.png)

ä¸ LDA ç›¸æ¯”ï¼Œé€šè¿‡ NMF åˆ¶ä½œçš„ä¸»é¢˜ä¼¼ä¹æ›´åŠ ç‹¬ç‰¹ã€‚

è¯é¢˜ 0:æœ‰è¶£çš„å·¥ä½œæ–‡åŒ–

ä¸»é¢˜ 1:è®¾è®¡è¿‡ç¨‹

è¯é¢˜ 2:æ„‰å¿«çš„å·¥ä½œ

ä¸»é¢˜ 3:

è¯é¢˜å››:ç»ä½³ä½“éªŒ

è¯é¢˜ 5:é¢å¤–æ´¥è´´

ä¸»é¢˜ 6:å­¦ä¹ æœºä¼š

è¯é¢˜ 7:ä¼Ÿå¤§çš„å…¬å¸/å·¥ä½œ

ä¸»é¢˜ 8:æ‰¿åŒ…å•†å‘˜å·¥ä½“éªŒ

ä¸»é¢˜ 9:ç®¡ç†

è®©æˆ‘ä»¬æŠŠ LDA å’Œ NMF çš„è¯é¢˜éƒ½æ·»åŠ åˆ°æˆ‘ä»¬çš„æ•°æ®æ¡†æ¶ä¸­ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥åˆ†æã€‚è®©æˆ‘ä»¬ä¹Ÿå°†æ•´æ•°ä¸»é¢˜é‡æ–°æ˜ å°„åˆ°æˆ‘ä»¬ä¸»è§‚å¯¼å‡ºçš„ä¸»é¢˜æ ‡ç­¾ä¸­ã€‚

```
nmf_topic_values = nmf.transform(tfidf)
df['nmf_topics'] = nmf_topic_values.argmax(axis=1)
lda_topic_values = lda_model.transform(tf)
df['lda_topics'] = lda_topic_values.argmax(axis=1)lda_remap = {0: 'Good Design Processes', 1: 'Great Work Environment', 2: 'Flexible Work Hours', 3: 'Skill Building', 4: 'Difficult but Enjoyable Work', 5: 'Great Company/Job', 6: 'Care about Employees', 7: 'Great Contractor Pay', 8: 'Customer Service', 9: 'Unknown1'}df['lda_topics'] = df['lda_topics'].map(lda_remap)nmf_remap = {0: 'Fun Work Culture', 1: 'Design Process', 2: 'Enjoyable Job', 3: 'Difficult but Enjoyable Work', 
             4: 'Great Experience', 5: 'Perks', 6: 'Learning Opportunities', 7: 'Great Company/Job', 
             8: 'Contractor Employee Experience', 9: 'Management'}df['nmf_topics'] = df['nmf_topics'].map(nmf_remap)
```

![](img/960709462c67770ee351f1df0355bcab.png)

æ£€æŸ¥ç”± NMF äº§ç”Ÿçš„ä¸»é¢˜çš„é¢‘ç‡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å‰ 5 ä¸ªä¸»é¢˜ä»¥ç›¸å¯¹ç›¸ä¼¼çš„é¢‘ç‡å‡ºç°ã€‚è¯·è®°ä½ï¼Œè¿™äº›æ˜¯æ‰€æœ‰è¯„è®º(æ­£é¢ã€ä¸­ç«‹å’Œè´Ÿé¢)çš„ä¸»é¢˜ï¼Œå¦‚æœæ‚¨è®°å¾—æˆ‘ä»¬çš„æ•°æ®é›†æ˜¯è´Ÿé¢å€¾æ–œçš„ï¼Œå› ä¸ºå¤§å¤šæ•°è¯„è®ºéƒ½æ˜¯æ­£é¢çš„ã€‚

```
nmf_x = df['nmf_topics'].value_counts()
nmf_y = nmf_x.sort_index()
plt.figure(figsize=(50,30))
sns.barplot(nmf_x, nmf_y.index)
plt.title("NMF Topic Distribution", fontsize=50)
plt.ylabel('Review Topics', fontsize=50)
plt.yticks(fontsize=40)
plt.xlabel('Frequency', fontsize=50)
plt.xticks(fontsize=40)
```

![](img/5c1ab1a6ef1066706e75da65386d2f43.png)

è®©æˆ‘ä»¬å°†æ•°æ®åˆ†å¼€ï¼Œæ ¹æ®è¯„åˆ† 1 å’Œ 2 æ£€æŸ¥è´Ÿé¢è¯„è®ºçš„ä¸»é¢˜ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå°½ç®¡æœ‰è´Ÿé¢è¯„ä»·ï¼Œå‘˜å·¥ä»¬ä»ç„¶éå¸¸å–œæ¬¢ä»–ä»¬çš„å·¥ä½œã€æ–‡åŒ–å’Œæ•´ä¸ªå…¬å¸ã€‚ç”±äºæˆ‘ä»¬æ•°æ®é›†çš„åæ–œï¼Œå¾ˆéš¾è·å¾—å…³äºè´Ÿé¢è¯„è®ºä¸»é¢˜çš„å‡†ç¡®è§‚ç‚¹(å³ç›¸å¯¹å°‘é‡çš„è´Ÿé¢è¯„è®º)ã€‚

```
df_low_ratings = df.loc[(df['rating']==1) | (df['rating']==2)]nmf_low_x = df_low_ratings['nmf_topics'].value_counts()
nmf_low_y = nmf_low_x.sort_index()
plt.figure(figsize=(50,30))
sns.barplot(nmf_low_x, nmf_low_y.index)
plt.title("NMF Topic Distribution for Low Ratings (1 & 2)", fontsize=50)
plt.ylabel('Frequency', fontsize=50)
plt.yticks(fontsize=40)
plt.xlabel('Review Topics', fontsize=50)
plt.xticks(fontsize=40)
```

![](img/3c0ea607bedcab7255bf8ec2610e3dab.png)

å› ä¸ºæˆ‘ä»¬æœ‰æ›´å¤šæ­£é¢çš„è¯„è®ºï¼Œæ‰€ä»¥é€šè¿‡ NMF è·å¾—çš„è¯é¢˜ä¼šæ›´å‡†ç¡®ã€‚ä¼¼ä¹æ‰¿åŒ…å•†é›‡å‘˜æ„æˆäº†è®¸å¤šè¯„è®ºã€‚å‘˜å·¥ä»¬å‘ç°äº†ä¸€ä¸ªé«˜æ•ˆçš„è®¾è®¡è¿‡ç¨‹ï¼Œå·¥ä½œè™½ç„¶è‰°éš¾ä½†å´ä»¤äººæ„‰å¿«ï¼Œå¹¶ä¸”å¯¹è°·æ­Œæœ‰ä¸€ç§æ€»ä½“ä¸Šæ„‰å¿«çš„æƒ…ç»ªã€‚

```
df_high_ratings = df.loc[(df['rating']==4) | (df['rating']==5)]nmf_high_x = df_high_ratings['nmf_topics'].value_counts()
nmf_high_y = nmf_high_x.sort_index()
plt.figure(figsize=(50,30))
sns.barplot(nmf_high_x, nmf_high_y.index)
plt.title("NMF Topic Distribution for High Ratings (3 & 4)", fontsize=50)
plt.ylabel('Frequency', fontsize=50)
plt.yticks(fontsize=40)
plt.xlabel('Review Topics', fontsize=50)
plt.xticks(fontsize=40)
```

![](img/a5e68bf628971d2c29ae3925d0ea2547.png)

# ç»“è®º

ä»è°ƒæŸ¥ç»“æœæ¥çœ‹ï¼Œè°·æ­Œçš„å‘˜å·¥ä¼¼ä¹éå¸¸ä¹äºåœ¨è°·æ­Œå·¥ä½œã€‚æˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªè´Ÿåæ€åˆ†å¸ƒï¼Œ84%çš„å‘˜å·¥ç»™è°·æ­Œæ‰“äº† 4 æˆ– 5 åˆ†(æ»¡åˆ†ä¸º 1-5 åˆ†çš„æå…‹ç‰¹é‡è¡¨)ã€‚ä¸€é¡¹æƒ…ç»ªåˆ†æè¯å®äº†è¿™äº›ç»“æœï¼Œå³ä½¿ç»™è°·æ­Œæ‰“ 2 åˆ° 3 åˆ†çš„å‘˜å·¥å¹³å‡æƒ…ç»ªå¾—åˆ†ä¸ºæ­£ã€‚

éšç€æˆ‘ä»¬å¯¹æ•°æ®çš„è¿›ä¸€æ­¥æŒ–æ˜ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼Œè¿™éœ€è¦ç”¨æ›´å¤šçš„æ•°æ®æ¥éªŒè¯ã€‚å½“æˆ‘ä»¬è§‚å¯Ÿæ¯ä¸ªç­‰çº§çš„æœ¯è¯­/è¯é¢‘æ—¶ï¼Œä¼¼ä¹â€œ*ç»ç†/ç®¡ç†å±‚â€*å‘¨å›´çš„æœ¯è¯­ä¼¼ä¹å‡ºç°åœ¨ç­‰çº§ 1ã€2 å’Œ 3 ä¸­ã€‚å¦‚å‰æ‰€è¿°ï¼Œæ•°æ®æ˜¯æœ‰åå·®çš„ï¼Œå› ä¸ºå¤§å¤šæ•°è¯„çº§éƒ½æ˜¯æ­£é¢çš„ï¼Œä½†æœ‰è¶£çš„æ˜¯ï¼Œæœ‰è´Ÿé¢æˆ–ä¸­æ€§è¯„çº§çš„å‘˜å·¥ä¼¼ä¹ç»å¸¸æåˆ°*ã€ç®¡ç†å±‚ã€‘*ã€‚

å¦ä¸€æ–¹é¢ï¼Œç»™è°·æ­Œæ‰“ 4 åˆ° 5 åˆ†çš„å‘˜å·¥ä¼¼ä¹åœ¨ä½¿ç”¨è¯¸å¦‚*ã€ã€ä¼Ÿå¤§ã€‘ã€ã€å·¥ä½œã€‘ã€ã€å·¥ä½œã€‘ã€ã€è®¾è®¡ã€‘ã€ã€å…¬å¸ã€‘ã€ã€å¥½ã€‘ã€ã€æ–‡åŒ–ã€‘ã€ã€äººã€‘ç­‰è¯æ±‡ã€‚è¿™äº›ç»“æœé€šè¿‡å¯¹æˆ‘ä»¬è¯­æ–™åº“ä¸­çš„è¯é¢˜/ä¸»é¢˜çš„æ£€æŸ¥å¾—åˆ°äº†è½»å¾®çš„è¯å®ã€‚NMF å¯¹ä¸»é¢˜çš„åˆ†æè¡¨æ˜ï¼Œç»™è°·æ­Œæ‰“ 4 æˆ– 5 åˆ†çš„å‘˜å·¥æ¸´æœ›è®¨è®ºå›°éš¾ä½†æ„‰å¿«çš„å·¥ä½œã€ä¼Ÿå¤§çš„æ–‡åŒ–å’Œè®¾è®¡è¿‡ç¨‹ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬è¿˜çœ‹åˆ°æ²¡æœ‰æåˆ°â€œç»ç†â€å’Œâ€œç®¡ç†â€è¿™æ ·çš„æœ¯è¯­ï¼Œè¿™ä¹Ÿè¯´æ˜å¹¶æœ‰åŠ©äºéªŒè¯æˆ‘ä»¬ä¹‹å‰çš„è§è§£ã€‚*

è°·æ­Œä»ç„¶æ˜¯è®¸å¤šäººé¦–é€‰çš„é›‡ä¸»ï¼Œ84%çš„è¯„è®ºæ˜¯æ­£é¢çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä»è°·æ­Œçš„ç®¡ç†è€…å’Œ/æˆ–ç®¡ç†æŠ€æœ¯ä¸­å‘ç°äº†ä¸€ä¸ªæ½œåœ¨çš„æ”¹è¿›é¢†åŸŸã€‚

ä½ èƒ½æƒ³åˆ°æˆ‘ä»¬å¯ä»¥æ¢ç´¢çš„ä»»ä½•å…¶ä»– EDA æ–¹æ³•å’Œ/æˆ–ç­–ç•¥å—ï¼ŸæŠŠå®ƒä»¬è´´åœ¨ä¸‹é¢çš„è¯„è®ºé‡Œã€‚æ­¤å¤–ï¼Œå¦‚æœä½ æœ‰ä»»ä½•å»ºè®¾æ€§çš„åé¦ˆæˆ–çœ‹åˆ°æˆ‘çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œè¯·å«æˆ‘å‡ºæ¥ğŸ˜ƒ

# **è°¢è°¢ï¼**