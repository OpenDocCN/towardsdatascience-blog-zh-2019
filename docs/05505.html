<html>
<head>
<title>BERT Text Classification in 3 Lines of Code Using Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Keras 在 3 行代码中实现 BERT 文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358?source=collection_archive---------3-----------------------#2019-08-14">https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358?source=collection_archive---------3-----------------------#2019-08-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="ea9b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">2019–08–17:</strong>文章<a class="ae ko" href="https://colab.research.google.com/drive/18SVeIFXWCiA9HL4WVCAFxlfH59ez6atc" rel="noopener ugc nofollow" target="_blank">中的代码演示已经分享到 Google Colab </a>上。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/f322b27e245e1c7ff858b953984b5fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ppiOjUg-UuKIjtPH.jpeg"/></div></figure><p id="7134" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">BERT ( <a class="ae ko" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">来自变形金刚</a>的双向编码器表示)是谷歌开发的深度学习模型。它代表了今年机器学习的重大突破之一，因为它在 11 个不同的自然语言处理(NLP)任务中取得了最先进的结果。此外，谷歌开源了代码，并提供了类似于 ImageNet 上预训练的计算机视觉模型的预训练模型供下载。由于这些原因，人们对 BERT 仍有很大的兴趣(尽管其他车型略微超过了它)。</p><p id="b827" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然 BERT 打破了许多不同任务的记录，从问答(SQuAD v1.1)到自然语言推理，<strong class="js iu">文本分类</strong>仍然是最实用和广泛适用的 NLP 任务之一。在本文中，我们将展示如何用少至<strong class="js iu"> 3 行代码</strong>将 BERT 应用于文本分类问题。为了实现这一点，我们将使用<a class="ae ko" href="https://github.com/amaiya/ktrain/tree/master/ktrain" rel="noopener ugc nofollow" target="_blank"> <em class="kx"> ktrain </em> </a>，一个类似<strong class="js iu"> fastai </strong>的接口到<strong class="js iu"> Keras。</strong> <em class="kx"> ktrain </em>是开源的，在这里<a class="ae ko" href="https://github.com/amaiya/ktrain" rel="noopener ugc nofollow" target="_blank">可以获得</a>。</p><p id="6078" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要安装<em class="kx"> ktrain </em>，只需输入以下命令:</p><pre class="kq kr ks kt gt ky kz la lb aw lc bi"><span id="7ab1" class="ld le it kz b gy lf lg l lh li">pip3 install ktrain</span></pre><p id="0669" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了在<em class="kx"> ktrain </em>和 Keras 中演示 BERT 文本分类，我们将使用在许多学术论文中使用的<a class="ae ko" href="https://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank"> IMDb 电影评论数据集</a>对电影评论进行情感分析。目标是正确地将验证集中的每个电影评论分类为正面或负面。你可以从<a class="ae ko" href="https://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据集，然后解压。首先，让我们导入<em class="kx"> ktrain </em>和<em class="kx"> ktrain.text </em>模块:</p><pre class="kq kr ks kt gt ky kz la lb aw lc bi"><span id="2b65" class="ld le it kz b gy lf lg l lh li">import ktrain<br/>from ktrain import text</span></pre><p id="3703" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">导入了<em class="kx"> ktrain </em>之后，我们开始吧。</p><h1 id="c09e" class="lj le it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">步骤 1:加载数据</h1><p id="ab5d" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">我们将首先使用<code class="fe ml mm mn kz b">texts_from_folder</code>函数从上面提取的文件夹中加载数据。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/191dea72c4627bd7d3e264af545cef43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*edasquwJ3ij3nP31HtjE9g.png"/></div></div></figure><p id="94c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一个参数应该是提取 Imdb 数据集的文件夹的路径。<code class="fe ml mm mn kz b">maxlen</code>参数指定每个电影评论中要考虑的最大字数(较长的评论被截断到这个长度)。BERT 可以处理的最大长度为 512，但是如果可以的话，您会希望使用更少的长度来减少内存并提高速度。必须以特定的方式对文本进行预处理，以便在 BERT 中使用。这通过将<code class="fe ml mm mn kz b">preprocess_mode</code>设置为‘Bert’来实现。如果需要的话，BERT 模型和词汇表将被自动下载。最后，<code class="fe ml mm mn kz b">texts_from_folder</code>函数需要以下目录结构，aclImdb 文件夹已经符合该结构:</p><pre class="kq kr ks kt gt ky kz la lb aw lc bi"><span id="244c" class="ld le it kz b gy lf lg l lh li">├── folder<br/>    │   ├── train<br/>    │   │   ├── class0       # folder for class 0 documents<br/>    │   │   ├── class1       # folder for class 1 documents<br/>    │   │   ├── class2       # folder for class 2 documents<br/>    │   │   └── classN       # folder for class N documents<br/>    │   └── test <br/>    │       ├── class0       # folder for class 0 documents<br/>    │       ├── class1       # folder for class 1 documents<br/>    │       ├── class2       # folder for class 2 documents<br/>    │       └── classN       # folder for class N documents</span></pre><h1 id="d24e" class="lj le it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">步骤 2:加载 BERT 并将其包装在一个学习者对象中</h1><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/3882124d24b2e19f2c799edfe2874a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*4LAJIsy8qGFB8CDXpUJ8Bw.png"/></div></figure><p id="43f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe ml mm mn kz b">get_learner</code>的第一个参数使用<em class="kx"> ktrain </em> <code class="fe ml mm mn kz b">text_classifier</code>函数来用随机初始化的最终密集层加载预训练的 BERT 模型。第二个和第三个参数分别是定型数据和验证数据。<code class="fe ml mm mn kz b">get_learner</code>的最后一个参数是批量大小。我们基于谷歌对<strong class="js iu">12GB</strong>GPU 的以下建议，使用 6 个小批量:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f95c1ac85f9591aaab5c6235ff3e9d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*NA-vtomjbStiBvcOiw_4AA.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk"><strong class="bd mz">Seq Length</strong> corresponds to the <strong class="bd mz">maxlen</strong> argument from <strong class="bd mz">STEP 1</strong>.</figcaption></figure><h1 id="c64c" class="lj le it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">第三步:训练模型</h1><p id="d804" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">为了训练模型，我们使用了<em class="kx"> ktrain </em>的<code class="fe ml mm mn kz b">fit_onecycle</code>方法，该方法采用了<a class="ae ko" href="https://arxiv.org/pdf/1803.09820.pdf" rel="noopener ugc nofollow" target="_blank">1 周期学习率策略</a>，该策略在训练的前半段线性增加学习率，然后在后半段降低学习率:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi na"><img src="../Images/78b589ec160c1b50e9e27ed613a66778.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*G5ZJxTrar9I7inNhkgXNnw.png"/></div></figure><p id="a80e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有关调整学习率的更多详情，请参见<em class="kx"> ktrain </em>上的<a class="ae ko" rel="noopener" target="_blank" href="/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c">这篇文章</a>。根据论文中的建议使用最大学习率<strong class="js iu"> 2e-5 </strong>(并通过执行<a class="ae ko" href="https://github.com/amaiya/ktrain/blob/master/tutorial-02-tuning-learning-rates.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="kx"> ktrain </em>学习率查找器</a>确认)。</p><p id="a5f8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从<strong class="js iu"> Keras </strong>输出可以看出，这在单个历元中实现了<strong class="js iu"> 93.71% </strong>的精度:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nb"><img src="../Images/4824384e9b00550c4abafe7307091310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qyXXEA-Q3IS_TO6aaRSqoA.png"/></div></div></figure><p id="4c37" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于我们似乎没有过度拟合，如果需要的话，该模型可以针对更多的时期进行训练，以产生甚至更高的精度。例如，在这个数据集上，三个时期的训练可以产生超过<strong class="js iu"> 94% </strong>的准确度。</p><h1 id="3d73" class="lj le it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">关于伯特实践的几点思考</h1><p id="ff92" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated"><strong class="js iu"> <em class="kx">速度:</em> </strong>虽然 BERT 的表现令人印象深刻，但在训练和推理(即对新数据的预测)方面都相对较慢。试图通过压缩来提高 BERT 的速度似乎并不成功。出于这些原因，如果训练超过一个历元，您可能希望省略<code class="fe ml mm mn kz b">get_learner</code>中的<code class="fe ml mm mn kz b">val_data</code>参数，并且仅在训练结束时进行验证。这可以在<em class="kx"> ktrain </em>中用<code class="fe ml mm mn kz b">learner.validate</code>的方法完成，如<a class="ae ko" href="https://colab.research.google.com/drive/1ixOZTKLz4aAa-MtC6dy_sAvc9HujQmHN" rel="noopener ugc nofollow" target="_blank">这个 Google Colab 笔记本</a>所示。鉴于 BERT 的缓慢，您还应该考虑更简单和更快的模型作为替代方案，以决定 BERT 提高的准确性是否值得。在某些情况下，你会惊讶地发现事实并非如此。</p><p id="bcd4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">更新 2020-01–14:【蒸馏】可用于</strong> <a class="ae ko" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">加速变压器型号</strong> </a> <strong class="js iu">。关于在<em class="kx"> ktrain </em>中使用 DistilBERT 模型的教程，请参见</strong> <a class="ae ko" href="https://medium.com/@asmaiya/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed" rel="noopener"> <strong class="js iu">我们的新媒体帖子</strong> </a> <strong class="js iu">。</strong></p><p id="217d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kx">内存:</em> </strong> BERT 可能相当占用内存。如果您遇到可能表明您超出 GPU 内存限制的错误(例如<code class="fe ml mm mn kz b">Blas GEMM launch failed</code>、<code class="fe ml mm mn kz b">CUDA_ERROR_OUT_OF_MEMORY</code>，您可以尝试减少<strong class="js iu">步骤 2 </strong>中使用的<code class="fe ml mm mn kz b">batch_size</code>参数或<strong class="js iu">步骤 1 </strong>中使用的<code class="fe ml mm mn kz b">maxlen</code>参数。</p><p id="c405" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kx">保存 BERT 模型:</em> </strong>在数据集上训练 BERT 后，可能需要将其保存到磁盘上，以便以后对新数据进行预测。如您所知，您可以分别使用<code class="fe ml mm mn kz b">model.save</code>方法和 Keras 内置的<code class="fe ml mm mn kz b">load_model</code>函数在 Keras 中保存和加载模型。然而，Keras <code class="fe ml mm mn kz b">load_model</code>函数在这里不会像预期的那样工作，因为 BERT 使用了自定义层。在重新执行上述步骤 1 和 2 后，您可以使用<em class="kx"> ktrain </em>中的<code class="fe ml mm mn kz b">learner.load_model</code>方法加载模型，而不是使用 Keras 的内置<code class="fe ml mm mn kz b">load_model</code>函数。这将正确工作，因为<em class="kx"> ktrain </em>将自定义 BERT 层传递给 Keras 的<code class="fe ml mm mn kz b">load_model</code>函数。或者，您可以使用对<code class="fe ml mm mn kz b">model.save_weights</code>和<code class="fe ml mm mn kz b">model.load_weights</code>的标准调用来保存和加载权重。(在这两种情况下，Keras 模型总是可以作为<code class="fe ml mm mn kz b">learner.model</code>直接访问。)</p><p id="51b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">文章源代码:</strong>这篇文章的<a class="ae ko" href="https://github.com/amaiya/ktrain/blob/master/examples/text/IMDb-BERT.ipynb" rel="noopener ugc nofollow" target="_blank">源代码</a>以下面这个 Jupyter 笔记本的形式提供:<a class="ae ko" href="https://github.com/amaiya/ktrain/blob/master/examples/text/IMDb-BERT.ipynb" rel="noopener ugc nofollow" target="_blank">IMDb-伯特. ipynb </a>。笔记本包括估计好的学习率和对新数据进行预测的例子。请随意在您自己的数据集上进行尝试。</p><p id="c326" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">更多信息:</strong>有关<em class="kx"> ktrain、</em>的更多信息，请参见关于 ktrain 的<a class="ae ko" href="https://github.com/amaiya/ktrain" rel="noopener ugc nofollow" target="_blank">教程笔记本和我们之前的 TDS 媒体出版物:</a></p><blockquote class="nc nd ne"><p id="1ed7" class="jq jr kx js b jt ju jv jw jx jy jz ka nf kc kd ke ng kg kh ki nh kk kl km kn im bi translated">k Train:Keras 的一个轻量级包装器，用于帮助训练神经网络</p></blockquote><p id="4c20" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<strong class="js iu"> Google Colab </strong>上使用<em class="kx"> ktrain </em>？另请参见<a class="ae ko" href="https://colab.research.google.com/drive/1AH3fkKiEqBpVpO5ua00scp7zcHs5IDLK" rel="noopener ugc nofollow" target="_blank">BERT 在多分类设置中的演示</a>。</p><h1 id="b12d" class="lj le it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">参考</h1><ul class=""><li id="d602" class="ni nj it js b jt mg jx mh kb nk kf nl kj nm kn nn no np nq bi translated"><em class="kx"> ktrain </em>通过<a class="ae ko" href="https://github.com/CyberZHG/keras-bert/tree/master/keras_bert" rel="noopener ugc nofollow" target="_blank"> keras_bert </a>包由<strong class="js iu">赵 HG </strong>支持 BERT 文本分类。</li></ul></div></div>    
</body>
</html>