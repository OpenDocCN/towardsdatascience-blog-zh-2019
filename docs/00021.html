<html>
<head>
<title>Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-3a7db7520711?source=collection_archive---------10-----------------------#2019-01-02">https://towardsdatascience.com/gradient-descent-3a7db7520711?source=collection_archive---------10-----------------------#2019-01-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/9e7a95991134b6b263fda92ba55a73a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGHMj4APQhNtBMB4YR2ptg.png"/></div></div></figure><div class=""/><div class=""><h2 id="87f8" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">这是一个滑坡，但是保证在底部会变得更好</h2></div><blockquote class="kq kr ks"><p id="1af4" class="kt ku kv kw b kx ky kc kz la lb kf lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jc">TL；dr </strong>梯度下降是一种优化技术，用于通过最小化成本函数来改善深度学习和基于神经网络的模型。</p></blockquote><p id="fd0d" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">在我们之前的帖子中，我们谈到了激活函数(链接<a class="ae lt" rel="noopener" target="_blank" href="/activation-functions-in-neural-networks-83ff7f46a6bd">这里</a>)以及它在机器学习模型中的应用。然而，我们也大量使用了术语“梯度下降”，这是深度学习模型中的一个关键元素，这将在本文中讨论。</p><h2 id="cc5e" class="lu lv jb bd lw lx ly dn lz ma mb dp mc lq md me mf lr mg mh mi ls mj mk ml mm bi translated">定义和术语</h2><p id="7435" class="pw-post-body-paragraph kt ku jb kw b kx mn kc kz la mo kf lc lq mp lf lg lr mq lj lk ls mr ln lo lp ij bi translated">梯度下降是发生在<strong class="kw jc">反向传播</strong>阶段的过程，其目标是基于权重<em class="kv"> w </em>在相反方向上连续地重新采样模型参数的梯度，不断地更新，直到我们达到函数<em class="kv"> J(w) </em>的<strong class="kw jc">全局最小值</strong>。</p><p id="b42a" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">简单来说，我们用梯度下降最小化代价函数，<em class="kv"> J(w) </em>。</p><figure class="mt mu mv mw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ms"><img src="../Images/2f1b7a94c33b7824809f9a7e6bd0101a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bP7EaVyPvBSRIxqgA9TbGQ.png"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk"><strong class="bd nb">Fig 1: </strong>How Gradient Descent works for one parameter, <em class="nc">w</em></figcaption></figure><p id="99df" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">可以用一座陡峭的山来做类比，这座山的底部与大海相接。我们假设一个人的目标是到达海平面。理想情况下，这个人必须一次迈出一步才能达到目标。每一步都有一个负向的梯度(注意:值可以是不同的量级)。这个人继续往下走，直到他到达底部或一个临界点，在那里没有空间再往下走了。</p><h2 id="8ae1" class="lu lv jb bd lw lx ly dn lz ma mb dp mc lq md me mf lr mg mh mi ls mj mk ml mm bi translated">数学</h2><p id="ba04" class="pw-post-body-paragraph kt ku jb kw b kx mn kc kz la mo kf lc lq mp lf lg lr mq lj lk ls mr ln lo lp ij bi translated">让我们将类比形式化为算法形式。我们计算输入参数的激活度，通过取激活度及其偏差的加权和来执行前馈。我们通过用实际“目标”值减去输出样本来提取误差项。</p><figure class="mt mu mv mw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/1f171db83cfc94cf8a3bb6da87b2ec69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ETudkFMzVEMsUrVD.png"/></div></div></figure><p id="2ea8" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">梯度下降过程以反向传播步骤的形式展示，其中我们从最后一层开始反向计算误差向量δ。根据激活函数，我们通过对函数相对于<strong class="kw jc"> w 的偏导数来确定需要多少变化。变化值乘以学习率。作为输出的一部分，我们从以前的输出中减去这个值，得到更新的值。我们继续这样，直到我们达到收敛。</strong></p><p id="93aa" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">在下面的代码中，我想强调如何编写一个简单的代码来可视化梯度下降是如何工作的。运行这段代码；使用 Tanh 激活函数，我们将观察到<strong class="kw jc"> 10 </strong>的当前值在第 10000 次迭代时下降到值<strong class="kw jc"> 8.407e-06 </strong>，这是我们的全局最小值。</p><figure class="mt mu mv mw gt is"><div class="bz fp l di"><div class="ne nf l"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk"><strong class="ak">Fig 2: </strong>A simple implementation of gradient descent, based on the Tanh activation function</figcaption></figure><p id="e63c" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">有很多梯度下降算法。下面我就举几个:</p><ul class=""><li id="be00" class="ng nh jb kw b kx ky la lb lq ni lr nj ls nk lp nl nm nn no bi translated">批量梯度下降</li><li id="e9fb" class="ng nh jb kw b kx np la nq lq nr lr ns ls nt lp nl nm nn no bi translated">随机梯度下降</li><li id="a037" class="ng nh jb kw b kx np la nq lq nr lr ns ls nt lp nl nm nn no bi translated">小批量梯度下降</li></ul><p id="3c7b" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">如果你想深入研究最近的一些技术细节，我强烈推荐你去看看 Sebastian Ruder 关于这个话题的文章。</p><h2 id="7c98" class="lu lv jb bd lw lx ly dn lz ma mb dp mc lq md me mf lr mg mh mi ls mj mk ml mm bi translated">爆炸和消失渐变</h2><p id="f550" class="pw-post-body-paragraph kt ku jb kw b kx mn kc kz la mo kf lc lq mp lf lg lr mq lj lk ls mr ln lo lp ij bi translated">在深度网络或递归神经网络中，Pascanu 等人(1994 年)在一篇论文中解释了两个已知问题——爆炸和消失梯度。当我们在代码中迭代进行反向传播时，会发生这种情况，权重矩阵的法线有可能超过 1。如果发生这种情况，梯度爆炸，但如果法线低于 1，梯度消失。</p><p id="8ce8" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">如果我们想可视化爆炸梯度，你会遇到至少一个问题:</p><ul class=""><li id="d580" class="ng nh jb kw b kx ky la lb lq ni lr nj ls nk lp nl nm nn no bi translated">该模型将输出“Nan”值</li><li id="acf2" class="ng nh jb kw b kx np la nq lq nr lr ns ls nt lp nl nm nn no bi translated">该模型在每一步都会显示非常大的变化</li><li id="c8b9" class="ng nh jb kw b kx np la nq lq nr lr ns ls nt lp nl nm nn no bi translated">对于训练层中的每个节点，误差梯度值始终高于 1.0。</li></ul><h2 id="9e6c" class="lu lv jb bd lw lx ly dn lz ma mb dp mc lq md me mf lr mg mh mi ls mj mk ml mm bi translated">解决方案:渐变裁剪</h2><p id="7859" class="pw-post-body-paragraph kt ku jb kw b kx mn kc kz la mo kf lc lq mp lf lg lr mq lj lk ls mr ln lo lp ij bi translated">为了解决渐变爆炸和消失的问题，我们引入了渐变裁剪，如果渐变超过了某个由最大绝对值表示的阈值，我们就对其进行“裁剪”。因此，我们保持神经网络稳定，因为权重值永远不会达到它返回“Nan”的点。在编码实现中，去除削波的梯度导致损失为“Nan”值或无穷大，并且不能进一步运行。</p><p id="b919" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">下面的代码展示了如何执行渐变裁剪。假设我们有一个损失向量和一个学习率，我们能够计算一个梯度向量，然后基于最大 L2 范数值对其进行剪切，在这种情况下，我写为 5。</p><figure class="mt mu mv mw gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="018e" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">因此，在一天结束时，当向数据科学家提出使用什么优化器来最大限度地减少损失的问题时，有几个因素需要考虑:</p><ul class=""><li id="725f" class="ng nh jb kw b kx ky la lb lq ni lr nj ls nk lp nl nm nn no bi translated">训练数据集的大小</li><li id="af27" class="ng nh jb kw b kx np la nq lq nr lr ns ls nt lp nl nm nn no bi translated">我们需要多快训练数据来实现收敛</li></ul><p id="bb24" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">参考论文链接:【http://proceedings.mlr.press/v28/pascanu13.pdf T2】</p><h2 id="eb0f" class="lu lv jb bd lw lx ly dn lz ma mb dp mc lq md me mf lr mg mh mi ls mj mk ml mm bi translated">结论</h2><p id="1d1a" class="pw-post-body-paragraph kt ku jb kw b kx mn kc kz la mo kf lc lq mp lf lg lr mq lj lk ls mr ln lo lp ij bi translated">在这篇文章中，我们讨论了很多事情:我们讨论了什么是梯度下降以及它在神经网络中是如何工作的。我们研究了相关的数学，并实现了它的编码版本。最后，我们以消失和爆炸梯度问题的形式讨论了涉及梯度下降的问题，并讨论了使用梯度裁剪的解决方案。在下一堂课中，我们将探索什么是激活函数，以及它们在深度学习模型中如何至关重要，请继续关注！</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="82bc" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated"><em class="kv">传播和分享知识。如果这篇文章激起了你的兴趣，请分享给你的朋友或专业人士。更多数据科技相关帖子关注我</em> <a class="ae lt" href="https://medium.com/@hamzamahmood" rel="noopener"> <em class="kv">这里</em> </a> <em class="kv">。<br/>我在</em><a class="ae lt" href="https://www.linkedin.com/in/hmahmood93/" rel="noopener ugc nofollow" target="_blank"><em class="kv">Linkedin</em></a><em class="kv">上也有空，偶尔</em><a class="ae lt" href="https://twitter.com/mahmooyo" rel="noopener ugc nofollow" target="_blank"><em class="kv">tweet</em></a><em class="kv">也有空。:)</em></p></div></div>    
</body>
</html>