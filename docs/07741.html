<html>
<head>
<title>NGBoost Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NGBoost 解释道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ngboost-explained-comparison-to-lightgbm-and-xgboost-fda510903e53?source=collection_archive---------4-----------------------#2019-10-27">https://towardsdatascience.com/ngboost-explained-comparison-to-lightgbm-and-xgboost-fda510903e53?source=collection_archive---------4-----------------------#2019-10-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1790" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" rel="noopener" target="_blank" href="https://towardsdatascience.com/data-science-in-the-real-world/home">现实世界中的数据科学</a></h2><div class=""/><div class=""><h2 id="f384" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">与 LightGBM 和 XGBoost 的比较</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/18ede8773c337e7736c85f9fffd5a747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FEzlTL1650XUQ99X"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@jamesponddotco?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">James Pond</a> on <a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="83e8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://stanfordmlgroup.github.io/projects/ngboost/" rel="noopener ugc nofollow" target="_blank"> Stanford ML Group </a>最近在他们的<a class="ae lh" href="https://arxiv.org/abs/1910.03225" rel="noopener ugc nofollow" target="_blank">论文</a>，【1】Duan et al .，2019 及其<a class="ae lh" href="https://github.com/stanfordmlgroup/ngboost" rel="noopener ugc nofollow" target="_blank">实现</a>中公布了一种新算法，名为 NGBoost。该算法通过使用<strong class="lk jd">自然梯度</strong>将不确定性估计包括到梯度增强中。本文试图理解这种新算法，并与其他流行的 boosting 算法 LightGBM 和 XGboost 进行比较，看看它在实践中是如何工作的。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="79ef" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">内容</h1><ol class=""><li id="62f3" class="nd ne it lk b ll nf lo ng lr nh lv ni lz nj md nk nl nm nn bi translated"><strong class="lk jd">什么是自然渐变增强？</strong></li><li id="4592" class="nd ne it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated"><strong class="lk jd">经验验证—与 LightGBM 和 XGBoost 的比较</strong></li><li id="276a" class="nd ne it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated"><strong class="lk jd">结论</strong></li></ol></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><ol class=""><li id="adc0" class="nd ne it lk b ll lm lo lp lr nt lv nu lz nv md nk nl nm nn bi translated"><strong class="lk jd">什么是自然渐变增强？</strong></li></ol><p id="f4b6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如我在简介中所写的，NGBoost 是一种新的提升算法，它使用自然梯度提升，这是一种用于概率预测的模块化提升算法。该算法由<strong class="lk jd">基本学习器</strong>、<strong class="lk jd">参数概率分布</strong>和<strong class="lk jd">评分规则</strong>组成。我将简要解释这些术语是什么。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/3f697e58319519e2e4d9411dcf761169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyApyqPusL8g8VrmuK6huQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">From the paper, Duan, et at., 2019</figcaption></figure><ul class=""><li id="84c5" class="nd ne it lk b ll lm lo lp lr nt lv nu lz nv md nx nl nm nn bi translated"><strong class="lk jd">基础学习者</strong></li></ul><p id="b65e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该算法使用基础(弱)学习器。它采用输入<em class="ny"> x </em>，输出用于形成条件概率。那些基础学习者对树学习者使用 scikit-learn 的决策树，对线性学习者使用岭回归。</p><ul class=""><li id="5b28" class="nd ne it lk b ll lm lo lp lr nt lv nu lz nv md nx nl nm nn bi translated"><strong class="lk jd">参数概率分布</strong></li></ul><p id="1223" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">参数概率分布是一种条件分布。这由基础学习者输出的附加组合形成。</p><ul class=""><li id="e683" class="nd ne it lk b ll lm lo lp lr nt lv nu lz nv md nx nl nm nn bi translated"><strong class="lk jd">评分规则</strong></li></ul><p id="bb28" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">评分规则采用预测的概率分布和对目标特征的一次观察来为预测评分，其中结果的真实分布在预期中获得最佳评分。该算法使用最大似然估计或 CRPS。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="c48b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们刚刚学习了 NGBoost 的基本概念。我绝对推荐你阅读原文进一步理解(有数学符号的算法更容易理解)。</p><p id="ab4f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2。经验验证—与 LightGBM 和 XGBoost 的比较</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/75efed57860a3d764af3d8a140938be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M4UlkECp78hIlyYW"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@billy2000?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">billy lee</a> on <a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="97ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们实现 NGBoost，看看它的性能如何。原论文也在各种数据集上做了一些实验。他们比较了 MC dropout、Deep Ensembles 和 NGBoost 在回归问题中的表现，NGBoost 显示了其极具竞争力的性能。在这篇博文中，我想展示一下模型在 Kaggle 上著名的<a class="ae lh" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener ugc nofollow" target="_blank">房价预测数据集</a>上的表现。该数据集由 81 个要素、1460 行组成，目标要素是销售价格。让我们看看 NGBoost 可以处理这些情况。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/bc4652e4b5d347fcbf1671a9db4f22ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1XlAsJ_B8iDMgmSc4X0Xg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Distribution of the target feature</figcaption></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="0d6d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于测试算法的性能是这篇文章的目的，我们将跳过整个特征工程部分，将使用那那西的<a class="ae lh" href="https://www.kaggle.com/jesucristo/1-house-prices-solution-top-1" rel="noopener ugc nofollow" target="_blank">解决方案</a>。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="71bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">导入包；</p><pre class="ks kt ku kv gt ob oc od oe aw of bi"><span id="87c2" class="og mm it oc b gy oh oi l oj ok"># import packages<br/>import pandas as pd</span><span id="7ebc" class="og mm it oc b gy ol oi l oj ok">from ngboost.ngboost import NGBoost<br/>from ngboost.learners import default_tree_learner<br/>from ngboost.distns import Normal<br/>from ngboost.scores import MLE</span><span id="3704" class="og mm it oc b gy ol oi l oj ok">import lightgbm as lgb</span><span id="b4de" class="og mm it oc b gy ol oi l oj ok">import xgboost as xgb</span><span id="a945" class="og mm it oc b gy ol oi l oj ok">from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import mean_squared_error<br/>from math import sqrt</span></pre><p id="dfbf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里我将使用上面的默认学习者、分布和评分规则。用这些东西来玩会很有趣，看看结果会有什么变化。</p><pre class="ks kt ku kv gt ob oc od oe aw of bi"><span id="df77" class="og mm it oc b gy oh oi l oj ok"># read the dataset<br/>df = pd.read_csv('~/train.csv')</span><span id="5c02" class="og mm it oc b gy ol oi l oj ok"># feature engineering<br/>tr, te = Nanashi_solution(df)</span></pre><p id="cfce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在用 NGBoost 算法预测。</p><pre class="ks kt ku kv gt ob oc od oe aw of bi"><span id="9e3a" class="og mm it oc b gy oh oi l oj ok"># NGBoost<br/>ngb = NGBoost(Base=default_tree_learner, Dist=Normal, Score=MLE(), natural_gradient=True,verbose=False)</span><span id="b261" class="og mm it oc b gy ol oi l oj ok">ngboost = ngb.fit(np.asarray(tr.drop(['SalePrice'],1)), np.asarray(tr.SalePrice))</span><span id="b27a" class="og mm it oc b gy ol oi l oj ok">y_pred_ngb = pd.DataFrame(ngb.predict(te.drop(['SalePrice'],1)))</span></pre><p id="7c9d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对 LightGBM 和 XGBoost 进行同样的操作。</p><pre class="ks kt ku kv gt ob oc od oe aw of bi"><span id="159f" class="og mm it oc b gy oh oi l oj ok"># LightGBM<br/>ltr = lgb.Dataset(tr.drop(['SalePrice'],1),label=tr['SalePrice'])</span><span id="bfc5" class="og mm it oc b gy ol oi l oj ok">param = {<br/>'bagging_freq': 5,<br/>'bagging_fraction': 0.6,<br/>'bagging_seed': 123,<br/>'boost_from_average':'false',<br/>'boost': 'gbdt',<br/>'feature_fraction': 0.3,<br/>'learning_rate': .01,<br/>'max_depth': 3,<br/>'metric':'rmse',<br/>'min_data_in_leaf': 128,<br/>'min_sum_hessian_in_leaf': 8,<br/>'num_leaves': 128,<br/>'num_threads': 8,<br/>'tree_learner': 'serial',<br/>'objective': 'regression',<br/>'verbosity': -1,<br/>'random_state':123,<br/>'max_bin': 8,<br/>'early_stopping_round':100<br/>}</span><span id="4bad" class="og mm it oc b gy ol oi l oj ok">lgbm = lgb.train(param,ltr,num_boost_round=10000,valid_sets=[(ltr)],verbose_eval=1000)</span><span id="afcf" class="og mm it oc b gy ol oi l oj ok">y_pred_lgb = lgbm.predict(te.drop(['SalePrice'],1))<br/>y_pred_lgb = np.where(y_pred&gt;=.25,1,0)</span><span id="0a35" class="og mm it oc b gy ol oi l oj ok"># XGBoost<br/>params = {'max_depth': 4, 'eta': 0.01, 'objective':'reg:squarederror', 'eval_metric':['rmse'],'booster':'gbtree', 'verbosity':0,'sample_type':'weighted','max_delta_step':4, 'subsample':.5, 'min_child_weight':100,'early_stopping_round':50}</span><span id="6e91" class="og mm it oc b gy ol oi l oj ok">dtr, dte = xgb.DMatrix(tr.drop(['SalePrice'],1),label=tr.SalePrice), xgb.DMatrix(te.drop(['SalePrice'],1),label=te.SalePrice)</span><span id="b7b0" class="og mm it oc b gy ol oi l oj ok">num_round = 5000<br/>xgbst = xgb.train(params,dtr,num_round,verbose_eval=500)</span><span id="1886" class="og mm it oc b gy ol oi l oj ok">y_pred_xgb = xgbst.predict(dte)</span></pre><p id="6fe7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们有了所有算法的预测。让我们检查一下准确性。我们将使用与这次卡格尔比赛相同的标准，RMSE。</p><pre class="ks kt ku kv gt ob oc od oe aw of bi"><span id="b67c" class="og mm it oc b gy oh oi l oj ok"># Check the results<br/>print('RMSE: NGBoost', round(sqrt(mean_squared_error(X_val.SalePrice,y_pred_ngb)),4))<br/>print('RMSE: LGBM', round(sqrt(mean_squared_error(X_val.SalePrice,y_pred_lgbm)),4))<br/>print('RMSE: XGBoost', round(sqrt(mean_squared_error(X_val.SalePrice,y_pred_xgb)),4))</span></pre><p id="ceed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下是预测结果汇总。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/19c0454b6bf6f71d7143c9c07a748fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*H6zlW6VbnCq3ym32mzcAdQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Summary of the results</figcaption></figure><p id="45fb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">看起来 NGBoost 胜过了其他著名的 boosting 算法。平心而论，我感觉如果调一下 BGBoost 的参数，会更好。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="4199" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">NGBoost 与其他 boosting 算法的最大区别之一是可以返回每个预测的概率分布。这可以通过使用<strong class="lk jd"> <em class="ny"> pred_dist </em> </strong>函数来可视化。该功能能够显示概率预测的结果。</p><pre class="ks kt ku kv gt ob oc od oe aw of bi"><span id="a0a5" class="og mm it oc b gy oh oi l oj ok"># see the probability distributions by visualising<br/>Y_dists = ngb.pred_dist(X_val.drop(['SalePrice'],1))<br/>y_range = np.linspace(min(X_val.SalePrice), max(X_val.SalePrice), 200)<br/>dist_values = Y_dists.pdf(y_range).transpose()</span><span id="c071" class="og mm it oc b gy ol oi l oj ok"># plot index 0 and 114<br/>idx = 114<br/>plt.plot(y_range,dist_values[idx])<br/>plt.title(f"idx: {idx}")<br/>plt.tight_layout()<br/>plt.show()</span></pre><div class="ks kt ku kv gt ab cb"><figure class="on kw oo op oq or os paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/7eae6fd3e9fc64009539f651f7c2b382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*P3ofObvPKsqDUjc23o2o0w.png"/></div></figure><figure class="on kw oo op oq or os paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/e5b2f283e4d4e373a423b3958a53d747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ks28Um_G4kzKZUNrf6b_Rw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk ot di ou ov">Probability distribution examples</figcaption></figure></div><p id="6a32" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的图是每个预测的概率分布。x 轴显示销售价格的对数值(目标特征)。我们可以观察到，索引 0 的概率分布比索引 114 更宽。</p><p id="15df" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 4。结论和想法</strong></p><p id="2b15" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从实验结果来看，NGBoost 和其他著名的 boosting 算法一样好。然而，计算时间比其他两种算法要长得多。这可以通过使用二次抽样方法来改善。此外，我有一个印象，NGBoost 包仍在进行中，例如没有提前停止选项，没有显示中间结果的选项，选择基础学习者的灵活性(到目前为止，我们只能在决策树和岭回归之间选择)，设置随机状态种子，等等。我相信这几点很快就会落实。或者你可以为项目做贡献:)</p><p id="e1a8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你也可以在我的<a class="ae lh" href="https://github.com/kyosek/NGBoost-experiments" rel="noopener ugc nofollow" target="_blank"> GitHub 页面</a>上找到我在这篇文章中使用的代码。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="74aa" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">包裹</h1><ul class=""><li id="f032" class="nd ne it lk b ll nf lo ng lr nh lv ni lz nj md nx nl nm nn bi translated">NGBoost 是一种新的 boosting 算法，它返回概率分布。</li><li id="02ca" class="nd ne it lk b ll no lo np lr nq lv nr lz ns md nx nl nm nn bi translated">自然梯度推进，一种用于<em class="ny">概率预测</em>的模块化推进算法。这由<strong class="lk jd">基本学习器</strong>、<strong class="lk jd">参数概率分布</strong>和 S <strong class="lk jd">取芯规则</strong>组成。</li><li id="45e2" class="nd ne it lk b ll no lo np lr nq lv nr lz ns md nx nl nm nn bi translated">NGBoost 预测与其他流行的 boosting 算法相比具有很强的竞争力。</li></ul></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="5f74" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你觉得这个故事有帮助，有趣或什么的，或者如果你有任何问题，反馈或字面上的任何东西，请随时在下面留下评论:)我真的很感激。还有，你可以在<a class="ae lh" href="https://www.linkedin.com/in/kyosuke-morita-58329286/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上找到我。</p><h1 id="eaac" class="ml mm it bd mn mo ow mq mr ms ox mu mv ki oy kj mx kl oz km mz ko pa kp nb nc bi translated">参考:</h1><p id="47d5" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr pb lt lu lv pc lx ly lz pd mb mc md im bi translated">[1] T. Duan 等，<a class="ae lh" href="https://www.semanticscholar.org/paper/NGBoost%3A-Natural-Gradient-Boosting-for-Prediction-Duan-Avati/3b432eea984904c926e2d6cc4dc2b70753499ca5" rel="noopener ugc nofollow" target="_blank"> NGBoost:概率预测的自然梯度推进</a> (2019)，ArXiv 1910.03225</p></div></div>    
</body>
</html>