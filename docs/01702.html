<html>
<head>
<title>Deep Deterministic Policy Gradients Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释了深层确定性策略梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b?source=collection_archive---------2-----------------------#2019-03-20">https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b?source=collection_archive---------2-----------------------#2019-03-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c1b7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">连续动作空间中的强化学习</strong></h2></div><p id="e127" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章是对 Deepmind 的出版物<em class="lb">“深度强化学习的连续控制”</em> (Lillicrap 等人，2015 年)的<strong class="kh ir">彻底的</strong>回顾，其中提出了深度确定性政策梯度(DDPG)，并且是为希望理解 DDPG 算法的人写的。如果你只对实现感兴趣，你可以跳到本文的最后一节。</p><blockquote class="lc ld le"><p id="80a6" class="kf kg lb kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">本文假设读者熟悉基本的强化学习概念、价值&amp;政策学习和行动者评论方法。如果你不完全熟悉这些概念，我还写过关于<a class="ae li" href="https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63" rel="noopener">政策梯度</a>和<a class="ae li" rel="noopener" target="_blank" href="/understanding-actor-critic-methods-931b97b6df3f">演员评论方法</a>的文章。</p><p id="5173" class="kf kg lb kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">熟悉 python 和 PyTorch 对阅读这篇文章也很有帮助。如果您不熟悉 PyTorch，请尝试按照代码片段进行操作，就像它们是伪代码一样。</p></blockquote></div><div class="ab cl lj lk hu ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="ij ik il im in"><h1 id="b762" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">浏览报纸</h1><h2 id="f6ec" class="mi lr iq bd ls mj mk dn lw ml mm dp ma ko mn mo mc ks mp mq me kw mr ms mg mt bi translated">网络示意图</h2><p id="d6a9" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">DDPG 使用四种神经网络:Q 网络、确定性策略网络、目标 Q 网络和目标策略网络。</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi mz"><img src="../Images/6ca24d84eef747b31b4baca6ed2ebd7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-87grz5iUZK4i7NCH1ldbw.png"/></div></div></figure><p id="7e9f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Q 网络和策略网络非常类似于简单的优势行动者-批评家，但是在 DDPG，行动者直接将状态映射到行动(网络的输出直接是输出)，而不是输出跨离散行动空间的概率分布</p><p id="8c05" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目标网络是其原始网络的延时副本，缓慢地跟踪已学习的网络。使用这些目标值网络大大提高了学习的稳定性。原因如下:在不使用目标网络的方法中，网络的更新方程依赖于网络本身计算的值，这使得它容易发散。例如:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nl"><img src="../Images/670955d3093dcbc01973f1a25d08c5e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rWEAu4HKQIzFTJw3i70Mkg.png"/></div></div></figure></div><div class="ab cl lj lk hu ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="ij ik il im in"><p id="7df4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们有了确定性策略网络和 Q 网络的标准参与者和批评家架构:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="607d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将网络和目标网络初始化为:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure></div><div class="ab cl lj lk hu ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="ij ik il im in"><h1 id="ca72" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">学问</h1><p id="f488" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">这是我们想要实现的算法的伪代码:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f3a319be5b1ee3372adcbe0245b361ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*BVST6rlxL2csw3vxpeBS8Q.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Taken from <em class="nt">“Continuous Control With Deep Reinforcement Learning”</em> (Lillicrap et al, 2015)</figcaption></figure><p id="c6fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将把它分解为:</p><ol class=""><li id="f31c" class="nu nv iq kh b ki kj kl km ko nw ks nx kw ny la nz oa ob oc bi translated">体验回放</li><li id="3946" class="nu nv iq kh b ki od kl oe ko of ks og kw oh la nz oa ob oc bi translated">演员和评论家网络更新</li><li id="ea4f" class="nu nv iq kh b ki od kl oe ko of ks og kw oh la nz oa ob oc bi translated">目标网络更新</li><li id="2ff1" class="nu nv iq kh b ki od kl oe ko of ks og kw oh la nz oa ob oc bi translated">探测</li></ol><h2 id="7e7b" class="mi lr iq bd ls mj mk dn lw ml mm dp ma ko mn mo mc ks mp mq me kw mr ms mg mt bi translated">重放缓冲器</h2><p id="8060" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">正如在深度 Q 学习(和许多其他 RL 算法)中使用的那样，DDPG 也使用重放缓冲区对经验进行采样，以更新神经网络参数。在每次轨迹推出期间，我们保存所有的体验元组(状态、动作、奖励、下一个状态)，并将它们存储在一个有限大小的缓存中——一个“重放缓冲区”然后，当我们更新值和策略网络时，我们从重放缓冲器中随机抽取小批量的经验。</p><p id="e359" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是重放缓冲区的样子:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="23e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们为什么要用经验回放？在优化任务中，我们希望数据独立分布。当我们以一种基于策略的方式优化一个连续的决策过程时，情况就不是这样了，因为那时的数据不会是相互独立的。当我们将它们存储在重放缓冲区中并随机进行批量训练时，我们克服了这个问题。</p><h2 id="795e" class="mi lr iq bd ls mj mk dn lw ml mm dp ma ko mn mo mc ks mp mq me kw mr ms mg mt bi translated">行动者(政策)和批评家(价值)网络更新</h2><p id="bf97" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">价值网络的更新类似于 Q-learning 中所做的。更新的 Q 值通过贝尔曼方程获得:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oi"><img src="../Images/0f70b31df3464fb7baef1d6bf984bbc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJ5fl6SemqEjoMX9Ns0jSA.png"/></div></div></figure><p id="3de1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在 DDPG，<strong class="kh ir">下一状态 Q 值是用目标值网络和目标策略网络</strong>计算的。然后，我们将更新后的 Q 值与原始 Q 值之间的均方损耗降至最低:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oj"><img src="../Images/c25e40eff2353f4ef93ed7de5a63a423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*96LU4dbopEncLQ9vc3pCzg.png"/></div></div></figure><p id="1486" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> *注意原 Q 值是用价值网计算的，不是目标价值网。</strong></p><p id="bc02" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在代码中，这看起来像:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="2947" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于保单功能，我们的目标是最大化预期收益:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ok"><img src="../Images/d06355e44a0d6fba31c7619b5e53d3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PF4odMpSZi1LWmcf7JhBZQ.png"/></div></div></figure><p id="2b5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了计算保单损失，我们取目标函数相对于保单参数的导数。请记住，参与者(策略)函数是可微的，因此我们必须应用链式法则。</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ol"><img src="../Images/0d37b0f6d43437f8b7c6976ca0294c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IFFNXK4CZMcmJ4g5nYstiw.png"/></div></div></figure><p id="53c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，由于我们是通过批量经验以非策略方式更新策略，因此我们采用从小批量计算的梯度总和的平均值:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi om"><img src="../Images/39914aad790fad15e2a1ead5e423631b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ta1Nn1sI6juWuPMJc3qepA.png"/></div></div></figure><p id="5ee0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在代码中，这看起来像:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="a1e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中优化器使用自适应矩估计(ADAM):</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="f77d" class="mi lr iq bd ls mj mk dn lw ml mm dp ma ko mn mo mc ks mp mq me kw mr ms mg mt bi translated">目标网络更新</h2><p id="a63c" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我们复制目标网络参数，并让它们通过“软更新”缓慢跟踪已学习网络的参数，如下所示:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi on"><img src="../Images/7866a81303ffb4eb056d915e83521e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LBlJpAQBLF95LsheOusmuA.png"/></div></div></figure><p id="7a47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这可以非常简单地实现:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="8b92" class="mi lr iq bd ls mj mk dn lw ml mm dp ma ko mn mo mc ks mp mq me kw mr ms mg mt bi translated">探测</h2><p id="4892" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">在离散动作空间的强化学习中，探索是通过概率性地选择随机动作来完成的(例如ε-贪婪或玻尔兹曼探索)。对于连续动作空间，探索是通过向动作本身添加噪声来完成的(也有参数空间噪声，但我们现在将跳过它)。在 DDPG 的论文中，作者使用<em class="lb">奥恩斯坦-乌伦贝克过程</em>将噪声<em class="lb"> </em>添加到动作输出中(乌伦贝克&amp;奥恩斯坦，1930):</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oo"><img src="../Images/4bf0f29ad833a48d71791f19e50f3100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LmL-P-_o10NWtQcF2PmkBg.png"/></div></div></figure><p id="772f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">奥恩斯坦-乌伦贝克过程</em>产生与先前噪声相关的噪声，以防止噪声抵消或“冻结”整体动态<strong class="kh ir">【1】</strong>。<a class="ae li" href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" rel="noopener ugc nofollow" target="_blank">维基百科对<em class="lb">奥恩斯坦-乌伦贝克过程</em>提供了详尽的解释。</a></p><p id="19ce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是 Pong 等人编写的 python 实现:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="7bad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">于是我们把演员网络产生的动作输入到<code class="fe op oq or os b">get_action()</code>函数中，得到一个新的动作，在这个动作中加入了时间相关噪声。</p><p id="0336" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在都准备好了！</p></div><div class="ab cl lj lk hu ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="ij ik il im in"><h1 id="dd17" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">把它们放在一起</h1><p id="dbe0" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我们在这里有重放缓冲区，奥恩斯坦-乌伦贝克过程，以及 OpenAI Gym 连续控制环境的规范化动作包装器，在<em class="lb"> utils.py </em>中:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="899e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以及<em class="lb"> models.py </em>中的演员&amp;评论家网络:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="c8c8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以及<em class="lb"> ddpg.py </em>中的 DDPG 代理:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6536" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以及<em class="lb"> main.py </em>中的测试:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="0549" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到 DDPG 代理是否学习到经典倒立摆任务的最优策略:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/d447f89c380589b14025daee2b5de479.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*miNYafAHXGXMyZx_-NKV0A.png"/></div></figure><p id="48d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是 DDPG！</p><p id="3ee2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">点击此处查看完整实施:</p><div class="ou ov gp gr ow ox"><a href="https://github.com/thechrisyoon08/Reinforcement-Learning" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd ir gy z fp pc fr fs pd fu fw ip bi translated">基督教 08/强化学习</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">用 Python 和 PyTorch 实现强化学习算法的模块化…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">github.com</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl nj ox"/></div></div></a></div></div><div class="ab cl lj lk hu ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="ij ik il im in"><h1 id="aaac" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">参考</h1><p id="474d" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated"><a class="ae li" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank"> Timothy P. Lillicrap，Jonathan J. Hunt，Alexander Pritzel，Nicolas Heess，Tom Erez，Yuval Tassa，David Silver 和金奎大·威斯特拉，<em class="lb">深度强化学习的持续控制</em>，CoRR abs/1509.02971 (2015)。</a></p><p id="e577" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae li" href="https://qr.ae/TW8NAa" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">【1】</strong>Edouard leu rent 对 Quora post 的回答<strong class="kh ir">“</strong><em class="lb">我们为什么要在 DDPG 的探索中使用奥恩斯坦乌伦贝克过程？”</em>T19】</a></p></div></div>    
</body>
</html>