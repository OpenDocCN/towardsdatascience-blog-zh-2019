<html>
<head>
<title>How To Breakout Data From Databricks-Spark-Hive</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从 Databricks-Spark-Hive 中获取数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-breakout-data-from-databricks-spark-hive-b9d94f656689?source=collection_archive---------28-----------------------#2019-10-08">https://towardsdatascience.com/how-to-breakout-data-from-databricks-spark-hive-b9d94f656689?source=collection_archive---------28-----------------------#2019-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/3e8e23cc93a6015afb1b6db75861bab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U__Wxm64tEad30ZF.jpg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Fingers Trying to break out of jail, <a class="ae jg" href="https://pixabay.com/photos/thieves-theft-robbery-swag-money-2012538/" rel="noopener ugc nofollow" target="_blank">Pixabay</a>.</figcaption></figure><div class=""/><div class=""><h2 id="d407" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">简单的方法。</h2></div><p id="d8d1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章是为那些使用 Databricks (DB)笔记本并希望通过使用 Pyspark 将基于 Hive 的数据集导出到外部机器的科学家而写的，以便使用 Pandas 获得更高效的工作流。</p><p id="30c9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有很多方法可以做到以下几点，但这个方法对我很有效。</p><p id="6910" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，在 DB 笔记本中创建一个 SQL 查询，并等待结果。以下查询是从 table_x 中选择所有列并将结果分配给 spark 数据框的简单示例。</p><blockquote class="lu lv lw"><p id="8a45" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">df = spark . SQL(" " SELECT * FROM table _ x " " " ")</p></blockquote><p id="80c5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Spark 不会参与，直到您要求它物化数据，因此，您必须执行以下命令。</p><blockquote class="lu lv lw"><p id="549d" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">df.show()</p></blockquote><p id="c590" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此时，您应该会看到一个非常难看的打印输出，其中包含一些数据。</p><p id="499b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在打印输出之后，您需要安装 DB 客户机，对其进行配置，并在 DB 文件存储上创建一个目录。</p><blockquote class="lu lv lw"><p id="edb2" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">pip3 安装数据块-cli</p><p id="3a0b" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">设置和配置 DB 客户端，我假设您可以遵循文档或者熟悉设置过程。<a class="ae jg" href="https://docs.databricks.com/dev-tools/databricks-cli.html#set-up-the-cli" rel="noopener ugc nofollow" target="_blank">点击此处</a>获取文件。</p><p id="8314" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">dbfs mkdirs dbfs:/file store/my/path/is/here</p></blockquote><p id="b2d9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">按照这些步骤，在 DB 笔记本中执行 write-to-JSON 命令，数据帧将保存在预定义路径的多个 JSON 文件中。请注意，路径应该根据您的配置进行更改。</p><blockquote class="lu lv lw"><p id="2c64" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">df . write . JSON(<br/>f " dbfs:/FileStore/my/path/is/here "，<br/> mode="overwrite "，<br/> compression="gzip" <br/>)</p></blockquote><p id="046b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">执行这些命令后，我们将在我们的机器上创建一个新目录，转到该目录，检查文件是否在我们的云目录中等待，最后使用 DB 客户端复制文件。</p><blockquote class="lu lv lw"><p id="3375" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">mkdir/data/our data<br/>CD/data/our data<br/>dbfs ls dbfs:/file store/my/path/is/here<br/>dbfs CP-r dbfs:/file store/my/path/is/here。</p></blockquote><p id="74ad" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">按照这些步骤，您可以将 location 指向您机器的数据目录并处理文件，将所有下载的 JSON.GZ 文件连接到一个 Pandas 数据框中。</p><blockquote class="lu lv lw"><p id="e839" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">进口熊猫作为 pd</p><p id="54b8" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">location = '/data/ourData '</p><p id="0c17" class="ky kz lx la b lb lc kk ld le lf kn lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">从 pathlib 导入路径<br/> files = list(路径(位置)。glob(" * . JSON . gz ")<br/>l =[]<br/>for f in files:<br/>l . append(PD . read _ JSON(f，lines = True))<br/>df = PD . concat(l)</p></blockquote><p id="549c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，您可以使用 Pandas，处理数据的速度比使用 DB 笔记本更快。唯一的条件是，你实际上可以把所有的数据都放进你的机器的内存里。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><p id="3ded" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我要感谢巴拉克·亚伊尔·赖夫的宝贵帮助，感谢阿黛尔·古尔审阅本文。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><p id="61e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Ori Cohen 博士拥有计算机科学博士学位，主要研究机器学习。他是 TLV 新遗迹公司的首席数据科学家，从事 AIOps 领域的机器和深度学习研究。</p></div></div>    
</body>
</html>