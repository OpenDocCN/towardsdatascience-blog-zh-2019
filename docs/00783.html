<html>
<head>
<title>Understanding Actor Critic Methods and A2C</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解演员评论方法和 A2C</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f?source=collection_archive---------0-----------------------#2019-02-06">https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f?source=collection_archive---------0-----------------------#2019-02-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="de94" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深度强化学习中的重要概念</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="5dac" class="km kn iq bd ko kp kq kr ks kt ku kv kw jw kx jx ky jz kz ka la kc lb kd lc ld bi translated">预赛</h1><p id="1941" class="pw-post-body-paragraph le lf iq lg b lh li jr lj lk ll ju lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在<a class="ae ma" href="https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63" rel="noopener">我之前的文章</a>中，我们导出了策略梯度并实现了加强算法(也称为蒙特卡罗策略梯度)。然而，普通的政策梯度有一些突出的问题:嘈杂的梯度和高方差。</p><p id="1e08" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated"><strong class="lg ir"> <em class="mg">但是为什么会这样呢？</em> </strong></p><p id="5f22" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">回想一下政策梯度:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/1774ee9b69f2de190b2953a2166702e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQqZyAJ1QehPXFW36TKwmw.png"/></div></div></figure><p id="0e51" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">如同在增强算法中一样，我们通过蒙特卡罗更新(即随机取样)来更新策略参数。这在对数概率(策略分布的对数)和累积奖励值中引入了固有的高可变性，因为训练期间的每个轨迹可以彼此偏离很大程度。</p><p id="c698" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">因此，对数概率和累积奖励值的高度可变性将产生有噪声的梯度，并导致不稳定的学习和/或向非最佳方向倾斜的策略分布。</p><p id="3484" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">除了梯度的高方差之外，策略梯度的另一个问题是轨迹的累积报酬为 0。政策梯度的实质是在政策分布中增加“好”行为的概率，减少“坏”行为的概率；如果累积奖励为 0，则不会学习“好”和“坏”行为。</p><p id="290f" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">总的来说，这些问题导致了普通政策梯度方法的不稳定性和缓慢收敛。</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="e298" class="mt kn iq bd ko mu mv dn ks mw mx dp kw ln my mz ky lr na nb la lv nc nd lc ne bi translated">改善政策梯度:用基线减少差异</h2><p id="a000" class="pw-post-body-paragraph le lf iq lg b lh li jr lj lk ll ju lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">减少方差和增加稳定性的一种方法是用基线减去累积奖励:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nf"><img src="../Images/5d6fd22156d756ba1b6f6a448fc2f659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45Wfsmp3KnzbJPKWt7GXpg.png"/></div></div></figure><p id="003e" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">直觉上，通过用基线减去累积奖励来使累积奖励变小，将产生更小的梯度，从而产生更小且更稳定的更新。</p><p id="a9af" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">下面是一个很有说明性的解释，摘自<a class="ae ma" href="https://www.quora.com/Why-does-the-policy-gradient-method-have-a-high-variance" rel="noopener ugc nofollow" target="_blank"> Jerry Liu 的帖子“为什么政策梯度法有很高的方差”</a>:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ng"><img src="../Images/7567c6afa61f9832634231580ecfc42f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3r6GvYe9Xm0xWrmNIoatzw.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">[Taken from Jerry Liu’s post “Why does the policy gradient method have high variance”]</figcaption></figure><h2 id="8f0b" class="mt kn iq bd ko mu mv dn ks mw mx dp kw ln my mz ky lr na nb la lv nc nd lc ne bi translated">常见基线功能概述</h2><p id="7bd0" class="pw-post-body-paragraph le lf iq lg b lh li jr lj lk ll ju lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">基线可以采用不同的值。下面的方程组说明了演员评论家方法的经典变体(关于加强)。在本帖中，我们将看看 Q 演员评论家和优势演员评论家。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nl"><img src="../Images/8d6f7e23571788ac05b215257e5f2187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T1zTYVLkMNngE09fOqTkSA.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Image taken from CMU CS10703 lecture slides</figcaption></figure></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="1cd3" class="km kn iq bd ko kp kq kr ks kt ku kv kw jw kx jx ky jz kz ka la kc lb kd lc ld bi translated">演员评论方法</h1><p id="be0e" class="pw-post-body-paragraph le lf iq lg b lh li jr lj lk ll ju lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在我们回到基线之前，让我们首先再次看一看普通的政策梯度，看看演员批评家架构是如何进来的(以及实际上是什么)。回想一下:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/1774ee9b69f2de190b2953a2166702e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQqZyAJ1QehPXFW36TKwmw.png"/></div></div></figure><p id="ff8d" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">然后，我们可以将期望值分解为:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nm"><img src="../Images/751b8687e2062a8a36bc77a77b58c1a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p0R0jWEaUAk2CEo-rk7MrQ.png"/></div></div></figure><p id="4b2e" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">第二个期望项应该很熟悉；就是 Q 值！(如果你还不知道这一点，我建议你阅读一下价值迭代和 Q 学习)。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nn"><img src="../Images/3003757b517fb91e246da980be277e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1CcRr_bsaebzGXyi6gC6yA.png"/></div></div></figure><p id="dd3d" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">将它代入，我们可以将更新等式改写如下:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi no"><img src="../Images/1fc432494610f6f7b0bdb22623d368cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JYp-uQrMJKEHadx4RBrR1A.png"/></div></div></figure><p id="b6eb" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">正如我们所知，Q 值可以通过用神经网络(上面用下标<code class="fe np nq nr ns b">w</code>表示)参数化 Q 函数来学习。</p><p id="446e" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">这就引出了<strong class="lg ir"> <em class="mg">演员的评论家方法，</em> </strong>的地方:</p><ol class=""><li id="48b0" class="nt nu iq lg b lh mb lk mc ln nv lr nw lv nx lz ny nz oa ob bi translated">“评论家”估计价值函数。这可以是动作值(<em class="mg"> Q 值</em>)或状态值(<em class="mg"> V 值</em>)。</li><li id="c9df" class="nt nu iq lg b lh oc lk od ln oe lr of lv og lz ny nz oa ob bi translated">“行动者”按照批评者建议的方向更新策略分布(例如使用策略梯度)。</li></ol><p id="9fd1" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">评论家和演员函数都用神经网络参数化。在上面的推导中，Critic 神经网络将 Q 值参数化—因此，它被称为<strong class="lg ir"> <em class="mg"> Q Actor Critic。</em>T13】</strong></p><p id="dc07" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">以下是 Q-Actor-Critic 的伪代码:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ng"><img src="../Images/79688c8a86ec52125f79c23d2d460759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVh9xq3VYEsgz6eNB3F6cA.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Adapted from Lilian Weng’s post “Policy Gradient algorithms”</figcaption></figure><p id="a557" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">如图所示，我们在每个更新步骤更新评论家网络和价值网络。</p><h2 id="dc83" class="mt kn iq bd ko mu mv dn ks mw mx dp kw ln my mz ky lr na nb la lv nc nd lc ne bi translated">回到基线</h2><p id="e998" class="pw-post-body-paragraph le lf iq lg b lh li jr lj lk ll ju lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">从权威的角度来说(因为我找不到原因)，状态值函数是一个最佳的基线函数。这在卡内基梅隆大学 CS10703 和 Berekely CS294 讲座幻灯片中有所陈述，但没有提供任何理由。</p><p id="847b" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">因此，使用 V 函数作为基线函数，我们用 V 值减去 Q 值项。直观地说，这意味着<em class="mg">在给定的状态下，采取特定的行动比一般的一般行动好多少</em>。我们将这个值称为<strong class="lg ir"> <em class="mg">优势值</em> </strong> <em class="mg"> : </em></p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oh"><img src="../Images/395ced59acdf0c9feab5467bf42ebdb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GjirmHTNdxHgo1Z8iQjDbg.png"/></div></div></figure><p id="4895" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">这是否意味着我们必须为 Q 值和 V 值构建两个神经网络(除了策略网络之外)？不行。那样效率会很低。相反，我们可以使用贝尔曼最优方程中 Q 和 V 之间的关系:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oh"><img src="../Images/df868a81807de27d63e2fca852d6ea7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ffw4Ri8eyB5FUArQq9wZeg.png"/></div></div></figure><p id="f6c0" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">所以，我们可以把优势改写为:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oi"><img src="../Images/1efc7ea75853fb5576a51c8539a0f499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pgGvGTyJ7gtlCICD5M-Z-A.png"/></div></div></figure><p id="2ce8" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">然后，我们只需要对 V 函数使用一个神经网络(由上面的<code class="fe np nq nr ns b">v</code>参数化)。因此，我们可以将更新等式改写为:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oj"><img src="../Images/4a631311277270a1292cfe2435c83180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjE0o_wWTdcjprdDQFnwog.png"/></div></div></figure><p id="ff8e" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">这是<strong class="lg ir"> <em class="mg">优势演员评论家。</em> </strong></p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="a699" class="mt kn iq bd ko mu mv dn ks mw mx dp kw ln my mz ky lr na nb la lv nc nd lc ne bi translated">优势行动者评论家(A2C)对异步优势行动者评论家(A3C)</h2><p id="f2f1" class="pw-post-body-paragraph le lf iq lg b lh li jr lj lk ll ju lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ir"> <em class="mg">优势行动者批评家</em> </strong>有两个主要变体:<strong class="lg ir">异步优势行动者批评家(A3C) </strong>和<strong class="lg ir">优势行动者批评家(A2C)。</strong></p><p id="dc9d" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated"><strong class="lg ir"> A3C </strong>是在<a class="ae ma" href="https://arxiv.org/abs/1602.01783" rel="noopener ugc nofollow" target="_blank"> Deepmind 的论文《深度强化学习的异步方法》(Mnih et al，2016) </a>中引入的。本质上，A3C 实现了<em class="mg">并行训练</em>，其中<em class="mg">并行环境</em><em class="mg"/>中的多个工人独立更新<em class="mg">全局</em>值函数——因此是“异步的”拥有异步参与者的一个主要好处是有效且高效地探索状态空间。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9d4017485aed5cd426f459b08eeef5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*EiFOXH3MZ0r_a_aKLd9r8A.jpeg"/></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">High Level Architecture of A3C (image taken from <a class="ae ma" href="https://www.groundai.com/project/a-brandom-ian-view-of-reinforcement-learning-towards-strong-ai/" rel="noopener ugc nofollow" target="_blank">GroundAI blog post</a>)</figcaption></figure><p id="bf30" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated"><strong class="lg ir"> A2C </strong>类似 A3C 但没有异步部分；这意味着 A3C 的单工人变体。经验表明，A2C 的性能与 A3C 相当，但效率更高。根据这篇 OpenAI 博客文章，研究人员并不完全确定异步是否或者如何有益于学习:</p><blockquote class="ol om on"><p id="3e74" class="le lf mg lg b lh mb jr lj lk mc ju lm oo md lp lq op me lt lu oq mf lx ly lz ij bi translated">在阅读了这篇论文之后，人工智能研究人员想知道异步是否会导致性能的提高(例如，“也许添加的噪声会提供一些正则化或探索？”)，或者它只是一个实现细节，允许使用基于 CPU 的实现进行更快的训练…</p><p id="a683" class="le lf mg lg b lh mb jr lj lk mc ju lm oo md lp lq op me lt lu oq mf lx ly lz ij bi translated">我们的同步 A2C 实现比异步实现性能更好——我们没有看到任何证据表明异步引入的噪声提供了任何性能优势。当使用单 GPU 机器时，这种 A2C 实现比 A3C 更具成本效益，并且当使用更大的策略时，比仅使用 CPU 的 A3C 实现更快。</p></blockquote><p id="aa66" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">无论如何，我们将在本帖中实现<strong class="lg ir"> A2C </strong>，因为它实现起来更简单。(这很容易扩展到 A3C)</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="95c3" class="km kn iq bd ko kp kq kr ks kt ku kv kw jw kx jx ky jz kz ka la kc lb kd lc ld bi translated">实施 A2C</h1><p id="bd84" class="pw-post-body-paragraph le lf iq lg b lh li jr lj lk ll ju lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，回想一下新的更新公式，用优势函数替换普通保单梯度中的折扣累积奖励:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi or"><img src="../Images/8c953d302cf579bb1df5d03fd5e3c50f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s_1ly9qZD8ob8n_tBq-dSw.png"/></div></div></figure><p id="1f7d" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">在每个学习步骤中，我们更新 Actor 参数(用策略梯度和优势值)和 Critic 参数(用 Bellman 更新方程最小化均方误差)。让我们看看这在代码中是什么样子的:</p><p id="0c3b" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">以下是包含和超参数:</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="e81a" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">首先，让我们从实现 Actor Critic 网络开始，配置如下:</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="74a9" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">主循环和更新循环，如上所述:</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="35dd" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">运行代码，我们可以看到 OpenAI Gym CartPole-v0 环境的性能如何提高:</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="os ot l"/></div></figure><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/fbdacf8144d21d9896f1437e1ab70516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*TMkif0BcjmgySoHSyyWQYQ.png"/></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Blue: Raw rewards; Orange: Smoothed rewards</figcaption></figure><p id="09ad" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">在此找到完整的实现:</p><p id="ef64" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated"><a class="ae ma" href="https://github.com/thechrisyoon08/Reinforcement-Learning" rel="noopener ugc nofollow" target="_blank">https://github.com/thechrisyoon08/Reinforcement-Learning</a>/</p><h1 id="170b" class="km kn iq bd ko kp ov kr ks kt ow kv kw jw ox jx ky jz oy ka la kc oz kd lc ld bi translated">参考资料:</h1><ol class=""><li id="b0b2" class="nt nu iq lg b lh li lk ll ln pa lr pb lv pc lz ny nz oa ob bi translated">加州大学柏克莱分校 CS294 讲座幻灯片</li><li id="a2a3" class="nt nu iq lg b lh oc lk od ln oe lr of lv og lz ny nz oa ob bi translated"><a class="ae ma" href="http://www.cs.cmu.edu/~rsalakhu/10703/Lecture_PG2.pdf" rel="noopener ugc nofollow" target="_blank">卡内基梅隆大学 CS10703 讲座幻灯片</a></li><li id="2cdc" class="nt nu iq lg b lh oc lk od ln oe lr of lv og lz ny nz oa ob bi translated"><a class="ae ma" href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" rel="noopener ugc nofollow" target="_blank"> Lilian Weng 关于政策梯度算法的帖子</a></li><li id="2950" class="nt nu iq lg b lh oc lk od ln oe lr of lv og lz ny nz oa ob bi translated"><a class="ae ma" href="https://www.quora.com/Why-does-the-policy-gradient-method-have-a-high-variance" rel="noopener ugc nofollow" target="_blank"> Jerry Liu 在 Quora Post 上回答“为什么政策梯度法有很高的方差”</a></li><li id="d933" class="nt nu iq lg b lh oc lk od ln oe lr of lv og lz ny nz oa ob bi translated"><a class="ae ma" href="https://www.youtube.com/watch?v=gINks-YCTBs&amp;t=2362s" rel="noopener ugc nofollow" target="_blank"> Naver D2 RLCode 讲座视频</a></li><li id="9d4d" class="nt nu iq lg b lh oc lk od ln oe lr of lv og lz ny nz oa ob bi translated"><a class="ae ma" href="https://blog.openai.com/baselines-acktr-a2c/" rel="noopener ugc nofollow" target="_blank"> OpenAI 关于 A2C 和 ACKTR 的博文</a></li><li id="44ab" class="nt nu iq lg b lh oc lk od ln oe lr of lv og lz ny nz oa ob bi translated">图表来自<a class="ae ma" href="http://Figure 4: Schematic representation of Asynchronous Advantage Actor Critic algorithm (A3C) algorithm." rel="noopener ugc nofollow" target="_blank"> GroundAI 的博客文章</a>“图 4:异步优势行动者评价算法(A3C)算法的示意图。”</li></ol><h2 id="3818" class="mt kn iq bd ko mu mv dn ks mw mx dp kw ln my mz ky lr na nb la lv nc nd lc ne bi translated">其他职位:</h2><p id="4bf6" class="pw-post-body-paragraph le lf iq lg b lh li jr lj lk ll ju lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">查看我关于强化学习的其他帖子:</p><ul class=""><li id="be68" class="nt nu iq lg b lh mb lk mc ln nv lr nw lv nx lz pd nz oa ob bi translated"><a class="ae ma" href="https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63" rel="noopener">导出政策梯度并实施强化</a></li><li id="a58b" class="nt nu iq lg b lh oc lk od ln oe lr of lv og lz pd nz oa ob bi translated"><a class="ae ma" rel="noopener" target="_blank" href="/understanding-actor-critic-methods-931b97b6df3f">了解演员评论家的方法</a></li><li id="8e5c" class="nt nu iq lg b lh oc lk od ln oe lr of lv og lz pd nz oa ob bi translated"><a class="ae ma" rel="noopener" target="_blank" href="/deep-deterministic-policy-gradients-explained-2d94655a9b7b">深度确定性政策梯度解释</a></li></ul><p id="fdad" class="pw-post-body-paragraph le lf iq lg b lh mb jr lj lk mc ju lm ln md lp lq lr me lt lu lv mf lx ly lz ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>