<html>
<head>
<title>Watch this Neural Network Learn to See</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">看这个神经网络学会看</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/watch-this-neural-network-learn-to-see-545492272440?source=collection_archive---------11-----------------------#2019-12-16">https://towardsdatascience.com/watch-this-neural-network-learn-to-see-545492272440?source=collection_archive---------11-----------------------#2019-12-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f74d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在整个模型训练中可视化卷积层激活</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e4b660e92b4211200ffa5e34b72a98b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*BBoSalYTRT7GBsbfCrewcw.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by <a class="ae ky" href="http://sianmolloy.com/" rel="noopener ugc nofollow" target="_blank">Sian Molloy</a></figcaption></figure><blockquote class="kz"><p id="10a4" class="la lb it bd lc ld le lf lg lh li lj dk translated">给手机用户的一个警告:这篇文章中有一些厚的 gif 图片。</p></blockquote><h1 id="1ba0" class="lk ll it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">动机</h1><p id="9561" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw lj im bi translated">深度学习相对于其他机器学习技术的主要优势是它能够自动学习输入数据的抽象表示。然而，事实并非总是如此。早在 1969 年，Minsky 和 Papert 出版了一本书,证明了单层感知器(人工神经网络的祖先)无法解决 XOR 问题。对于我们这些没有计算机科学背景或没有其他生活经历的人来说，异或问题就是接受两个二进制输入，<em class="mx"> A </em>和<em class="mx"> B </em>，当且仅当<em class="mx"> A </em>或<em class="mx"> B </em>中的一个为真时，返回<em class="mx">真</em>，因此得名“异或”，或 XOR。单层感知器无法解决这个问题的原因是它们只能解析线性可分的类。如果您要画出 XOR 问题的可能输入及其输出，空间看起来会像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/215d5d236a23e94d29681f842f56d40e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jNVK528rJya81OG-Uc9UwQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The world’s worst game of tic-tac-toe (source: wikimedia commons)</figcaption></figure><p id="99a6" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">现在，你能在这张图片上画一条直线，把圆圈和十字分开吗？剧透:不能，感知器也不能。幸运的是，勇敢的先驱们拥有将两个感知机粘在一起的洞察力，深度学习领域由此诞生(或多或少)。这样做的原因是，神经网络的每一层都可以被视为前一层的嵌入；虽然上图中的圆圈和十字在它们的原始形式下可能不是线性可分的，但是通过简单的编码它们是线性可分的。抓住图像的左上角和右下角，并在脑海中保持不动。然后，借助你的想象力，通过第三维度将图像对折<em class="mx">，将右上角拖出屏幕，然后向下按回到左下角。如果你做对了，它应该是这样的:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/bd9d4fb8bc766f613f47b9b0ae3fe293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*zr32nMcJ8J-GT7eNWVCAjA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">If you look closely, you may notice I’m not an artist</figcaption></figure><p id="2caa" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">现在，你能画一条穿过<em class="mx">这张</em>图片的直线，并把圆圈和十字分开吗？我真诚地希望如此。这种获取信息并将其编码为其他更有用的形式的行为，是神经网络擅长的主要任务。事实上，训练神经网络不是为了它们的预测能力，而是为了它们发现的学习表示，这一直是深度学习研究的支柱。</p><h1 id="112c" class="lk ll it bd lm ln lo lp lq lr ls lt lu jz nf ka lw kc ng kd ly kf nh kg ma mb bi translated">学习观察</h1><p id="3425" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw lj im bi translated">卷积神经网络(CNN)是将深度学习应用于图像数据的最流行的架构。简而言之，CNN 学习许多过滤器，它们应用于图像的每个像素及其层。通过将这些过滤器应用于图像，以及重复下采样，神经网络学习识别其第一层中的简单、低级特征以及其最后一层中的复杂、高级特征。或者至少，这是他们通常的解释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/3fe60d4ee841edcf5bdf8f9145f6ebfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*aRrWvkaLiCf_ryTkahsTeA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image Source: Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations, by Lee <em class="nj">et al.</em></figcaption></figure><p id="8f23" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">正如你所看到的，该模型学习识别各种边缘，然后面部特征，然后整个脸(许多中间层被删除)。如果你在谷歌上搜索“卷积神经网络层可视化”，你会发现大量的上述图片。然而，我从来没有在训练过程中见过 CNN 层的可视化<em class="mx">，所以我想我应该看看它们是什么样子的。在这次探索中，我使用了常见的<a class="ae ky" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST 数据集</a>，这是一组 60 000 个黑白手绘数字，每个数字的高度和宽度都是 28 像素。我使用了一个简单的卷积模型，如下所示:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/0df44dd04486e188f03d44f78847baf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3lcex0mW92u_TzMHyLLaA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The network architecture, including 6 convolutional layers and 3 dense layers</figcaption></figure><h2 id="7699" class="nl ll it bd lm nm nn dn lq no np dp lu ml nq nr lw mp ns nt ly mt nu nv ma nw bi translated">形象化</h2><p id="d6f2" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw lj im bi translated">神经网络被训练五个时期，具有 1024 个图像的小批量大小，总共 290 个训练步骤。在每一步之后，预先选择的一组十个样本图像(每个数字一个)被输入到模型中，并且保存每个卷积层的激活。尽管近年来它已经不再流行，取而代之的是更容易训练的<a class="ae ky" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>函数，我还是决定使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Hyperbolic_function#Tanh" rel="noopener ugc nofollow" target="_blank"> tanh </a>作为卷积层的激活函数。这是因为 tanh 介于-1 和 1 之间，这使得可视化变得简单。当第一层的激活被应用到红蓝颜色图时，结果如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ae7f9ec830531fd1199f26c390bc7a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*Wn3JaLL8DfFqYLpZeLrd-w.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Conv1: The input images (top row) and the activations of the four channels in convolutional layer 1. Activations range from +1 (blue) to 0 (white) to -1 (red). Frame (top left) is the number of training steps applied.</figcaption></figure><p id="4c85" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">Conv1 似乎已经学会识别第一和第二通道中的笔画宽度，因为每个数字的内部是暗红色，而外部是浅红色。在第三和第四个通道中，它似乎已经学会了边缘的概念，数字是蓝色的，背景是粉红色的，数字边缘是白色的。然而，这些激活从深度学习佳能所建议的来说是一个长镜头，即每个通道将学习一个清晰和独特的特征，例如垂直和水平边缘；Conv1 在很大程度上再现了原始输入，并稍加注释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/48e86b3e49738293c87e657a55070e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*cssvnJBooPYHAzjccUVDFg.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Conv2: The same setup as Conv1.</figcaption></figure><p id="ca5a" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">与 Conv1 类似，Conv2 似乎也在再现原始输入。通道 1、通道 2 和通道 4 彼此几乎相同，并且与 Conv1 中的边缘突出显示行为几乎相同，通道 3 只是输入的模糊再现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/aad2f703104ca3c23973bdc1be18e170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*06iBmkm9kwI3Mdy_HEd_kA.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Conv3: The same setup as Conv1, except with eight channels instead of four. This layer has half the resolution as the original image, so activations were upscaled without interpolation for visualization.</figcaption></figure><p id="5abc" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">在 Conv3 中，我们看到了可能是第一个真正习得的特性。在第六个通道中，接近训练结束时，我们看到手指是蓝色的，大部分背景是粉红色的，手指每个部分正下方的背景是红色的。这表明这个通道已经学会识别水平边缘的底部。类似地，第七个通道有红色数字，粉色背景，每个数字上方有白色水平边缘<em class="mx">。然而，其他通道似乎是原始图像的简单复制。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/7f1da227cda24e920f1f83b12f3f2038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*oV-AoGhVLAg_YnisKTPEmg.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Conv4: The same setup as Conv3.</figcaption></figure><p id="4e6a" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">在 Conv4 中，我们可以看到更清晰的特性。特别是，我们看到不同角度的边缘。第一、第二和第六通道标识水平边缘的顶部。第三、第七和第八通道标识对角边缘。其他两个通道是原始通道的粗略再现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/d2eedc035c5a6cbb087e18072c7a1b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*KBGm0XMsjk6nKf1jEn-NiQ.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Conv5: The same setup as Conv1, except with sixteen channels instead of four. This layer has one-quarter the resolution of the original image, so activations were upscaled without interpolation for visualization.</figcaption></figure><p id="6015" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">Conv5 进行了大量的下采样，分辨率仅为 7x7 像素，但似乎具有有意义的特征提取。在训练的最初阶段，每个频道都是粉红色的，基本上没有信息。到步骤 70，该层已经学会产生模糊地类似于输入的斑点。然而，在训练结束时，这些通道已经明显地彼此区分开来，并显示出激活的急剧变化。由于低分辨率和我们所谓的独立功能的纠缠，我们不清楚这里学到了什么功能，但很明显，这里的每个通道都有一些有用的东西。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/792637a5859ed5b686046e295243d8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*HQbt5SMvfM12Hc5cRJnpQA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Conv6: The <a class="ae ky" href="https://github.com/ConorLazarou/medium/blob/master/visualizing_mnist_12019_12/tanh/visualizations/conv6.gif" rel="noopener ugc nofollow" target="_blank">gif</a> was too large for Medium, so these are the activations after training has completed.</figcaption></figure><p id="e91e" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">不幸的是，Conv6 刚好超过 Medium 的文件大小限制，你必须<a class="ae ky" href="https://github.com/ConorLazarou/medium/blob/master/visualizing_mnist_12019_12/tanh/visualizations/conv6.gif" rel="noopener ugc nofollow" target="_blank">点击这个链接</a>才能观看它学习。与 Conv5 类似，学习到的功能清晰可见，但几乎不可能说出它们实际对应的是什么。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b5f583ea94f0bb7b01913565824e5d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Lk3qGf254-hrp2HS3wjNpQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Accuracy and loss (categorical_crossentropy) during training</figcaption></figure><h1 id="6687" class="lk ll it bd lm ln lo lp lq lr ls lt lu jz nf ka lw kc ng kd ly kf nh kg ma mb bi translated">经验教训</h1><p id="d5be" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw lj im bi translated">那么这个故事的寓意是什么呢？我建议有三个。首先，深度学习的结果很少像佳能所说的那样清晰。很多教材，包括<em class="mx">深度学习</em> (Goodfellow <em class="mx">等</em>。)，把低级卷积层比作<a class="ae ky" href="https://en.wikipedia.org/wiki/Gabor_filter#" rel="noopener ugc nofollow" target="_blank"> Gabor 滤波器</a>和其他手工制作的计算机视觉滤波器。尽管该模型在测试数据上达到了 95%以上的准确率，但前四个卷积层在特征提取方面做得很少。诚然，对于一个非常简单的任务来说，这是一个非常简单的模型，而且很可能一个为更困难的任务训练的更复杂的模型至少会学习一些有用的低级特征，但深度学习通常被教授的方式(以我的经验来看)表明，特征提炼和提取是不可避免的，即使对于简单的任务来说也是如此；事实显然并非如此。</p><p id="e378" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">第二个教训是，习得的特征不太可能是人类可能选择的直观、独立的品质。Conv5 和 Conv6 显然已经学会了<em class="mx">一些东西</em>，原始图像已经以这样一种方式进行编码，即网络的密集层可以根据数字类型对它们进行分类，但还不清楚它们学会了检测什么。这是深度学习中的一个常见问题，尤其是在生成模型中，模型可能会学习将两个或更多看似不相关的品质<a class="ae ky" href="https://openreview.net/references/pdf?id=Sy2fzU9gl" rel="noopener ugc nofollow" target="_blank">嵌入为单个特征</a>。</p><p id="3ddf" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">第三个教训是我在作为数据科学家的工作中每天都会想起的，那就是可视化一切是值得的。我进入这个项目，期望写一篇非常不同的文章。我很兴奋地展示了网络学习和提炼功能，从低级的边缘检测到高级的循环和旋转。相反，我发现了一个懒惰的懒汉，直到最后一刻才改进功能。最值得注意的是，我惊讶地发现，一旦这些层学会了输入的一些表示，它们在训练过程中几乎没有改变。将这一点可视化增强了我对卷积神经网络训练的理解。我希望你在这里也学到了一些东西。</p><p id="ec98" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw lj im bi translated">对于那些好奇的人来说，用来训练这个网络并产生这些可视化效果的代码可以在这个 Github repo 中找到:</p><div class="nz oa gp gr ob oc"><a href="https://github.com/ConorLazarou/medium/tree/master/12019/visualizing_mnist" rel="noopener  ugc nofollow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">ConorLazarou/培养基</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">卷积神经网络学习识别手写数字时的可视化</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">github.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq ks oc"/></div></div></a></div></div></div>    
</body>
</html>