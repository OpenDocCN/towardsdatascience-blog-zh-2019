<html>
<head>
<title>Data Augmentation in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的数据扩充</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28?source=collection_archive---------2-----------------------#2019-04-12">https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28?source=collection_archive---------2-----------------------#2019-04-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="315c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">文本增强简介</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/76739aa696e2e407c21de48d2bc81a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*etubCHFnSt_LNZOw"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edward Ma</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5d18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们拥有的数据越多，我们能够实现的性能就越好。然而，注释大量的训练数据是非常奢侈的。因此，适当的数据扩充有助于提高模型性能。增强在计算机视觉领域非常流行。通过图像增强库，如<a class="ae kv" href="https://github.com/aleju/imgaug" rel="noopener ugc nofollow" target="_blank"> imgaug </a>，可以通过翻转、添加盐等方式轻松增强图像。事实证明，增强是计算机视觉模型成功的支柱之一。</p><p id="c87d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在自然语言处理领域，由于语言的高度复杂性，很难对文本进行扩充。不是每一个词我们都可以用其他词代替，比如 a，an，the。另外，不是每个单词都有同义词。哪怕换一个词，语境也会完全不同。另一方面，在计算机视觉领域生成增强图像相对更容易。即使引入噪声或裁剪掉图像的一部分，该模型仍然可以对图像进行分类。</p><p id="daf2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">鉴于我们没有无限的资源来通过人类建立训练数据，作者尝试了不同的方法来实现相同的目标，即生成更多的数据用于模型训练。在这个故事中，我们探索了不同的作者如何通过生成更多的文本数据来增强模型，从而利用增强来刺激 NLP 任务。以下故事将涵盖:</p><ul class=""><li id="5137" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">宝库</li><li id="1d38" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">单词嵌入</li><li id="2ee9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">回译</li><li id="1d57" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">语境化的单词嵌入</li><li id="07ea" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">文本生成</li></ul><h1 id="3f41" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">宝库</h1><p id="919d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">张等人介绍了用于文本分类的同义词<a class="ae kv" href="https://arxiv.org/pdf/1509.01626.pdf" rel="noopener ugc nofollow" target="_blank">字符级卷积网络</a>。在实验过程中，他们发现一种有效的文本扩充方法是用同义词替换单词或短语。利用现有的词库有助于在短时间内生成大量数据。张等选取一个词，按几何分布用同义词替换。</p><h1 id="44b9" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">单词嵌入</h1><p id="c2f4" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">王和杨介绍的字相似的算计在<a class="ae kv" href="https://aclweb.org/anthology/D15-1306" rel="noopener ugc nofollow" target="_blank">真是让人讨厌！！！:基于词汇和框架语义嵌入的数据增强方法，使用#petpeeve Tweets </a>对恼人的行为进行自动分类。在论文中，王和杨提出利用 k 近邻法和余弦相似度来寻找相似词进行替换。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/281dcfcff74040c69d46fa9a756e9d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AEIhlRHRqn1M_R6vJEMgXw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Imp.: relative improvement to the baseline without data augmentation (Wang and Yang, 2015)</figcaption></figure><p id="76d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">或者，我们可以利用预先训练好的<a class="ae kv" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">经典单词嵌入</a>，比如 word2vec、GloVe 和 fasttext 来执行相似性搜索。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/e2f20ac4a5343d5761fbec6f47933586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*eatzgOd9njd6mv8pwXZ9LA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Most similar words of “fox” among classical word embeddings models</figcaption></figure><h1 id="3ef9" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">回译</h1><p id="4bfe" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">英语是具有大量翻译训练数据的语言之一，而一些语言可能没有足够的数据来训练机器翻译模型。Sennrich 等人使用反向翻译方法来生成更多的训练数据，以提高翻译模型的性能。</p><p id="cabe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们要训练一个翻译英语(源语言)→粤语(目标语言)的模型，而粤语没有足够的训练数据。回译是将目标语言翻译成源语言，并混合源语句和回译语句来训练模型。因此可以增加从源语言到目标语言的训练数据的数量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/4bc8b88160ee6dd6ee9df7637c822b3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*HWId-7cUjJllAE9lgm4eTg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Performance result with and without text augmentation (Sennrich et al., 2016)</figcaption></figure><h1 id="b146" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">语境化的单词嵌入</h1><p id="7623" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">Fadaee 等人没有使用静态单词嵌入，而是使用上下文化的单词嵌入来替换目标单词。他们使用这种文本增强来验证用于低资源神经机器翻译的<a class="ae kv" href="https://arxiv.org/pdf/1705.00440.pdf" rel="noopener ugc nofollow" target="_blank">数据增强中的机器翻译模型。</a></p><p id="48e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">提出的方法是 TDA，代表翻译数据增强。实验表明，通过利用文本增强，机器翻译模型得到了改进。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/8ba68d55fc01ecbc452a914548706d19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HI4QhZigMJr_vRc8ta3FrA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Performance result with and without text augmentation (Fadaee et al., 2017)</figcaption></figure><p id="8c59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Kobayashi 提出在<a class="ae kv" href="https://arxiv.org/pdf/1805.06201.pdf" rel="noopener ugc nofollow" target="_blank">语境扩充中使用双向语言模型:通过具有聚合关系的单词进行数据扩充</a>。在选择目标单词后，模型将通过给出周围的单词来预测可能的替换。由于目标存在于句子的任何位置，双向结构被用来学习左右语境。</p><p id="4179" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Kobayashi 用 CNN 和 RNN 在六个数据集上验证了语言模型方法，结果是肯定的。文本增强有助于进一步改进 NLP 模型结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c703a0c4e141d08737ea9466bb6edb9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*oSK5rngfMRKNtEWf529psA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Performance result with and without text augmentation (Kobayashi 2018)</figcaption></figure><h1 id="489a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">文本生成</h1><p id="19cf" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">Kafle 等人介绍了一种不同的方法，通过在<a class="ae kv" href="https://aclweb.org/anthology/W17-3529" rel="noopener ugc nofollow" target="_blank">视觉问题回答的数据增强</a>中生成增强数据来生成增强数据。与以前的方法不同，Kafle 等人的方法不是替换单个或几个单词，而是生成整个句子。</p><p id="64eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一种方法是使用模板扩充，即使用预定义的问题，这些问题可以使用基于规则的方法来生成与模板问题配对的答案。第二种方法利用 LSTM 通过提供图像特征来生成问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/38cc29ed9a00b2eba9c0dff5c99f0a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*9ppkImcPXGLFpQEysHXj9A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Performance result with and without text augmentation (Kafle et al. 2017)</figcaption></figure><h1 id="68a0" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">建议</h1><p id="74e7" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">上述方法旨在解决作者在他们的问题中所面临的问题。如果你了解你的数据，你应该量身定制增强方法。请记住，数据科学的黄金法则是垃圾进垃圾出。</p><p id="857b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一般来说，你可以在不完全理解你的数据的情况下尝试同义词库方法。由于上述同义词库方法的限制，它可能不会提高很多。</p><h1 id="94ec" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。欢迎在<a class="ae kv" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与<a class="ae kv" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>联系，或者在<a class="ae kv" href="http://medium.com/@makcedward/" rel="noopener"> Medium </a>或<a class="ae kv" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="be7f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">延伸阅读</h1><ul class=""><li id="8a14" class="ls lt iq ky b kz my lc mz lf nj lj nk ln nl lr lx ly lz ma bi translated">图像增强库(<a class="ae kv" href="https://github.com/aleju/imgaug" rel="noopener ugc nofollow" target="_blank"> imgaug </a>)</li><li id="19b1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">文本增强库(<a class="ae kv" rel="noopener" target="_blank" href="/data-augmentation-library-for-text-9661736b13ff"> nlpaug </a>)</li><li id="e4a8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/data-augmentation-library-for-text-9661736b13ff">文本的数据扩充</a></li><li id="4780" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/data-augmentation-for-audio-76912b01fdf6">音频数据增强</a></li><li id="d042" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/data-augmentation-for-speech-recognition-e7c607482e78">声谱图数据增强</a></li></ul><h1 id="1470" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><ul class=""><li id="4850" class="ls lt iq ky b kz my lc mz lf nj lj nk ln nl lr lx ly lz ma bi translated">X.张、赵军、李乐存。<a class="ae kv" href="https://arxiv.org/pdf/1509.01626.pdf" rel="noopener ugc nofollow" target="_blank">用于文本分类的字符级卷积网络</a>。2015</li><li id="fef8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">W.王燕和杨丹。<a class="ae kv" href="https://aclweb.org/anthology/D15-1306" rel="noopener ugc nofollow" target="_blank">真讨厌！！！:一种基于词汇和框架语义嵌入的数据增强方法，使用#petpeeve Tweets </a>对恼人的行为进行自动分类。2015</li><li id="3b01" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">R.森里奇，b .哈多和一棵桦树。<a class="ae kv" href="https://arxiv.org/pdf/1511.06709.pdf" rel="noopener ugc nofollow" target="_blank">用单语数据改进神经机器翻译模型</a>。2016</li><li id="5633" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">米（meter 的缩写））法达伊，a .比萨沙，c .蒙兹。<a class="ae kv" href="https://arxiv.org/pdf/1705.00440.pdf" rel="noopener ugc nofollow" target="_blank">低资源神经机器翻译的数据扩充</a>。2017</li><li id="6751" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">南小林。<a class="ae kv" href="https://arxiv.org/pdf/1805.06201.pdf" rel="noopener ugc nofollow" target="_blank">语境扩充:通过具有聚合关系的词语进行数据扩充</a>。2018</li><li id="321b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">K.卡弗尔、优素福·胡辛和卡南。<a class="ae kv" href="https://aclweb.org/anthology/W17-3529" rel="noopener ugc nofollow" target="_blank">视觉问答的数据扩充</a>。2017</li><li id="003e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">C.库伦贝。<a class="ae kv" href="https://arxiv.org/ftp/arxiv/papers/1812/1812.04718.pdf" rel="noopener ugc nofollow" target="_blank">利用 NLP 云 API 简化文本数据扩充</a>。2018</li></ul></div></div>    
</body>
</html>