<html>
<head>
<title>Growing your own RNN cell : Simplified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">培养你自己的 RNN 细胞:简化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/growing-your-own-rnn-cell-simplified-b68ba2c0f082?source=collection_archive---------12-----------------------#2019-07-10">https://towardsdatascience.com/growing-your-own-rnn-cell-simplified-b68ba2c0f082?source=collection_archive---------12-----------------------#2019-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b231" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一窥单个 RNN 细胞的“深层”世界</h2></div><h2 id="19f2" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">什么是 RNN 细胞？</h2><p id="03bc" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">根据 Tensorflow 文档，“在最抽象的设置中，<em class="lx">RNN 单元是指</em> <strong class="lg iu"> <em class="lx">具有状态</em> </strong> <em class="lx">并执行一些需要输入矩阵的操作的任何东西。”</em></p><p id="12d0" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">RNN 细胞在某种意义上区别于普通神经元，因为它们有一个状态，因此可以记住过去的信息。RNN 细胞构成了循环网络的主干。</p><p id="d7e4" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">在数学层面上，一系列输入通过 RNN 单元，一次一个。单元的状态有助于它记住过去的序列，并将该信息与当前输入相结合以提供输出。一个更简单的方法是展开序列中发生的事情，揭示一个更简单的深层网络。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/ce9ed6a2d5d360bb512d44d26069f785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png"/></div></div></figure><h2 id="59e8" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">常见的 RNN 单元架构</h2><p id="64af" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">两种最常用的 RNN 单元是 GRUs 和 LSTMs。这两个单元都有“门”,基本上是对应于每个输入的 0 和 1 之间的值。这些门背后的直觉是忘记和保留少数选定的输入，表明这些细胞既可以记住来自过去的信息，也可以在需要时让它离开。这使得他们能够更好地处理序列。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mp"><img src="../Images/aa2b1b14b4bf4f9fd08d1a6470066466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E18UBv1G7nAaq-soA-zBlw.png"/></div></div></figure><p id="a9f3" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">要详细了解这些细胞的工作，请参考这个包含动画的相当不错的博客，以便于解释。</p><div class="mq mr gp gr ms mt"><a rel="noopener follow" target="_blank" href="/animated-rnn-lstm-and-gru-ef124d06cf45"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd iu gy z fp my fr fs mz fu fw is bi translated">动画 RNN，LSTM 和 GRU</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">gif 中的递归神经网络细胞</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh mn mt"/></div></div></a></div><h2 id="c421" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">单个节点本身就是一个深度网络！！</h2><p id="6ab2" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">让我们看看 LSTM 工作的方程，它是最常用的 RNN 池之一。这里 x_t 表示单元的输入，而 h_t-1，h_t 和 c_t-1，c_t 表示隐藏状态和单元状态。所有其余的变量都是可训练的权重和偏差。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/7569f703f51314cd92566c0db9e6e55a.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*nngyYAuaI46JFbu9I4q0pA.png"/></div></figure><p id="eb26" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">它们看起来确实很吓人，想想这些都发生在 RNN 的一个牢房里。在不深入细节的情况下，这些等式是否让你想起了其他事情？让我强调一下，<em class="lx">矩阵与输入的权重相乘，然后是激活函数！！！</em> <strong class="lg iu"> <em class="lx">是的！！！人脉深厚！！！</em>T15】</strong></p><p id="fbe5" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">RNN 细胞本身包含一个小而深的网络。虽然 LSTMs 和 GRUs 中的架构是最常用的，但这并不意味着它们是手头每项任务的最佳选择。可能有比这些标准单元性能更好的更复杂、看起来不可思议的架构。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/4d4f29a8527af9e2b82d5e7345462058.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*gyQkfuIw3T4XCFvvMwHA7A.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">RNN cell created by WeNet, performing better than complex Recurrent networks containing LSTM cells, on the Penn Treebank dataset.</figcaption></figure><p id="0d43" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">就像<em class="lx">神经架构搜索(NAS) </em>和<em class="lx"> AutoML 的发展一样，</em>研究人员也一直致力于创建他们自己的 RNN 细胞架构。如果你不知道 NAS 或 AutoML 是什么，可以看看我之前在 AutoML 上的博客。</p><h2 id="29dc" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">常用方法</h2><ol class=""><li id="57ba" class="np nq it lg b lh li lk ll kr nr kv ns kz nt lw nu nv nw nx bi translated"><em class="lx">搜索整个参数空间</em>:从所有可能的连接开始，然后删除多余的连接，留下重要的连接。由于从真正“所有”可能的连接开始在计算上是一场噩梦，这些方法中的实际搜索空间通常是有限的。</li></ol><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c7715a51df08fcfc99e7a65c6dc27142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*zpeV8_2w56eCVlL_xmxp7w.png"/></div></figure><ol class=""><li id="3b76" class="np nq it lg b lh ly lk lz kr nz kv oa kz ob lw nu nv nw nx bi translated"><em class="lx">生长细胞，一次一个节点</em>:这些方法依赖于类似于生长决策树的策略。每次迭代后，都会在图的顶部添加一个新节点。树从 h_t(输出)开始生长，当我们在叶节点同时得到 x_t 和 h_t-1(输入)时结束。</li></ol><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi oc"><img src="../Images/1af8bcabd7db6b637f38a2198039783b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uhRsSRjgDrZi421SLH_eDw.jpeg"/></div></div></figure><ol class=""><li id="94d7" class="np nq it lg b lh ly lk lz kr nz kv oa kz ob lw nu nv nw nx bi translated"><em class="lx">遗传算法</em> : RNN 细胞架构是当前一代中的明星，杂交产生下一代更好的细胞架构。</li></ol><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi od"><img src="../Images/29b6e581b37d804b4b23539a24d9716b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*_-9pPdCZsqwC2E3iUTHUbA.png"/></div></figure><h2 id="f55c" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">下一步是什么？</h2><p id="b489" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">AutoML 和 NAS 领域的工作和开发是最近的事，而且每年都在快速增长。然而，不仅没有一种算法能够集成并真正创建任何深度学习网络(包括 CNN 和 RNNs ),而且现有算法所花费的计算时间也是巨大的。这些都是未来研究可以并且将会改进的方向。</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="effa" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">本博客是为机器学习领域创建简化介绍的努力的一部分。点击此处查看完整系列</p><div class="mq mr gp gr ms mt"><a rel="noopener follow" target="_blank" href="/machine-learning-simplified-1fe22fec0fac"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd iu gy z fp my fr fs mz fu fw is bi translated">机器学习:简化</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">在你一头扎进去之前就知道了</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="ol l ne nf ng nc nh mn mt"/></div></div></a></div><p id="aae7" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated"><em class="lx">或者干脆阅读本系列的下一篇博客</em></p><div class="mq mr gp gr ms mt"><a rel="noopener follow" target="_blank" href="/automl-and-autodl-simplified-b6786e5560ff"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd iu gy z fp my fr fs mz fu fw is bi translated">AutoML 和 AutoDL:简化</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">看看机器学习新阶段的开始</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="om l ne nf ng nc nh mn mt"/></div></div></a></div></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><h2 id="2c15" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">参考</h2><p id="b4fd" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated"><em class="lx"> [1]拉瓦尔、阿迪蒂亚、里斯托·米库拉宁。"从节点到网络:进化循环神经网络."arXiv 预印本 arXiv:1803.04439 (2018)。<br/>[2]Schrimpf，Martin 等人，“一种灵活的自动化 RNN 架构生成方法”arXiv 预印本 arXiv:1712.07316 (2017)。<br/> [3] Pham，Hieu 等，“通过参数共享进行有效的神经结构搜索”arXiv 预印本 arXiv:1802.03268 (2018)。<br/> [4]黄、智衡、秉祥。" WeNet:用于递归网络体系结构搜索的加权网络."arXiv 预印本 arXiv:1904.03819 (2019)。</em></p></div></div>    
</body>
</html>