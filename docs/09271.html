<html>
<head>
<title>Biomedical Image Segmentation: Attention U-Net</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生物医学图像分割:注意力 U 网</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/biomedical-image-segmentation-attention-u-net-29b6f0827405?source=collection_archive---------8-----------------------#2019-12-08">https://towardsdatascience.com/biomedical-image-segmentation-attention-u-net-29b6f0827405?source=collection_archive---------8-----------------------#2019-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="928f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过在标准 U-Net 上附加注意门来提高模型的灵敏度和准确性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/29df47edaf2c92c8c205b7fbd13257c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IcGCaQnX3A5LQszsaIP6wA.png"/></div></div></figure><p id="a8e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">医学图像分割已经被积极地研究以自动化临床分析。深度学习模型一般需要大量的数据，但获取医学图像既繁琐又容易出错。</p><p id="40a2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Attention U-Net 旨在自动学习关注不同形状和大小的目标结构；因此，Oktay 等人的论文名称<a class="ae lq" href="https://arxiv.org/abs/1804.03999" rel="noopener ugc nofollow" target="_blank">“学习在哪里寻找胰腺”</a></p><h1 id="1665" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">关注优图网之前的相关作品</h1><h2 id="4f18" class="mj ls it bd lt mk ml dn lx mm mn dp mb ld mo mp md lh mq mr mf ll ms mt mh mu bi translated">优信网</h2><p id="9a1f" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">U-Nets 通常用于图像分割任务，因为它的性能和对 GPU 内存的有效使用。它的目标是以较少的训练样本实现可靠的临床使用的高精度，因为获取带注释的医学图像可能是资源密集型的。<a class="ae lq" rel="noopener" target="_blank" href="/biomedical-image-segmentation-u-net-a787741837fa">阅读更多关于优信网</a>的信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/7d606b9e6a07c393a106d948e0390516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SAxlsyXAh4B76PhVjHlaBg.png"/></div></div></figure><p id="b3e7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管 U-Net 具有出色的表示能力，但它依赖于多级级联卷积神经网络来工作。这些级联框架提取感兴趣的区域并进行密集预测。这种方法导致计算资源的过度和冗余使用，因为它重复提取低级特征。</p><h2 id="ba4e" class="mj ls it bd lt mk ml dn lx mm mn dp mb ld mo mp md lh mq mr mf ll ms mt mh mu bi translated">注意模块</h2><p id="dd69" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated"><a class="ae lq" href="https://arxiv.org/abs/1804.02391" rel="noopener ugc nofollow" target="_blank">【需要注意】Jetley 等</a>推出端到端可训练的注意模块。注意门常用于自然图像分析和自然语言处理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/15d640a012bdbc3343aa895563e8de34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QtG2eWEBfnJ_IMo21gcaGg.png"/></div></div></figure><p id="df49" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意力被用于执行特定于类的池化，这导致更准确和鲁棒的图像分类性能。这些注意力地图可以放大相关区域，从而显示出优于几个基准数据集的泛化能力。</p><h2 id="ee7b" class="mj ls it bd lt mk ml dn lx mm mn dp mb ld mo mp md lh mq mr mf ll ms mt mh mu bi translated">软硬注意</h2><p id="d85a" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">注意力函数的工作原理是通过迭代区域提议和裁剪来使用图像区域。但这通常是不可微的，并且依赖于强化学习(一种基于采样的技术，称为强化)进行参数更新，这导致优化这些模型更加困难。</p><p id="b40a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">另一方面，软注意是概率性的，并且利用标准的反向传播，而不需要蒙特卡罗采样。Seo 等人的<a class="ae lq" href="https://arxiv.org/abs/1606.02393" rel="noopener ugc nofollow" target="_blank">的软注意方法通过实现非均匀、非刚性的注意图来展示改进，该注意图更适合于在真实图像中看到的自然物体形状。</a></p><h1 id="423c" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">关注优信网有什么新内容？</h1><h2 id="8645" class="mj ls it bd lt mk ml dn lx mm mn dp mb ld mo mp md lh mq mr mf ll ms mt mh mu bi translated">注意门</h2><p id="7e69" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">为了提高分割性能，<a class="ae lq" href="https://www.sciencedirect.com/science/article/abs/pii/S136184151830848X" rel="noopener ugc nofollow" target="_blank"> Khened 等人</a>和<a class="ae lq" href="https://www.ncbi.nlm.nih.gov/pubmed/29427897" rel="noopener ugc nofollow" target="_blank"> Roth 等人</a>依靠附加的在先对象定位模型来分离定位和后续分割步骤。这可以通过在<a class="ae lq" rel="noopener" target="_blank" href="/biomedical-image-segmentation-u-net-a787741837fa"> U-Net </a>架构之上集成注意力门来实现，而无需训练额外的模型。</p><p id="babd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，结合到 U-Net 中的注意门可以提高模型对前景像素的灵敏度和准确性，而不需要显著的计算开销。注意门可以逐渐抑制不相关背景区域的特征反应。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/61d38b0b0173a1b4efd4ea7535ab071c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kEuDd2mCqTZkyNmkTGdg9Q.png"/></div></div></figure><p id="46a3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意门在连接操作之前实现，以仅合并相关的激活。源自背景区域的梯度在反向传递期间被向下加权。这允许基于与给定任务相关的空间区域来更新先前层中的模型参数。</p><h2 id="4594" class="mj ls it bd lt mk ml dn lx mm mn dp mb ld mo mp md lh mq mr mf ll ms mt mh mu bi translated">基于网格的门控</h2><p id="2681" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">为了进一步改善注意机制，<a class="ae lq" href="https://arxiv.org/abs/1804.03999" rel="noopener ugc nofollow" target="_blank"> Oktay 等人</a>提出了网格注意机制。通过实现基于网格的选通，选通信号不是用于所有图像像素的单个全局向量，而是适应于图像空间信息的网格信号。每个跳过连接的选通信号聚集来自多个成像尺度的图像特征。</p><p id="9669" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过使用基于网格的选通，这允许关注系数更具体地针对局部区域，因为它增加了查询信号的网格分辨率。与基于全局特征向量的门控相比，这实现了更好的性能。</p><h2 id="a40f" class="mj ls it bd lt mk ml dn lx mm mn dp mb ld mo mp md lh mq mr mf ll ms mt mh mu bi translated">软注意技术</h2><p id="5538" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">附加软注意用于句子到句子的翻译(<a class="ae lq" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank"> Bahdanau 等人</a>和<a class="ae lq" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16126/16099" rel="noopener ugc nofollow" target="_blank"> Shen 等人</a>)和图像分类(<a class="ae lq" href="https://arxiv.org/abs/1804.02391" rel="noopener ugc nofollow" target="_blank"> Jetley 等人</a>和<a class="ae lq" href="https://arxiv.org/abs/1704.06904" rel="noopener ugc nofollow" target="_blank"> Wang 等人</a>)。虽然这在计算上更昂贵，Luong 等人已经表明，软注意可以比乘法注意实现更高的准确性。</p><h2 id="824e" class="mj ls it bd lt mk ml dn lx mm mn dp mb ld mo mp md lh mq mr mf ll ms mt mh mu bi translated">体系结构</h2><p id="1abc" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">下面是注意力 U-Net 的图解。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/6b8d2cf7844b4498850144b581766dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J3Rlq9K-ldxdJ5XWa4cD9g.png"/></div></div></figure><h1 id="82e0" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">我的注意力 U-Net 实验</h1><p id="0698" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">我将使用 Drishti-GS 数据集，其中包含 101 幅视网膜图像，以及光盘和光学杯的注释掩膜。50 幅图像用于训练，51 幅用于验证。</p><p id="cb86" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">实验设置和使用的指标将与<a class="ae lq" rel="noopener" target="_blank" href="/biomedical-image-segmentation-u-net-a787741837fa"> U-Net </a>相同。</p><p id="d67a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">模型在 13 分钟内完成训练；每个时期大约需要 15 秒。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/b289bd60243e2e7f43879fd079cdaa4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rN5TDqYBQpPWYVUGCyo-OQ.png"/></div></div></figure><p id="c003" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用于比较的几个 U-Net 模型之间的度量，如下所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="8a9b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">测试从模型处理一些看不见的样本开始，以预测光盘(红色)和光学杯(黄色)。下面是对注意力优网、<a class="ae lq" rel="noopener" target="_blank" href="/biomedical-image-segmentation-unet-a3f43342307b"> UNet++ </a>和<a class="ae lq" rel="noopener" target="_blank" href="/biomedical-image-segmentation-u-net-a787741837fa">优网</a>的测试结果，以供对比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/97f9222081a94f37a22f9ccca3747f70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fqqmov23zVQgT--scowAAA.png"/></div></div></figure><h1 id="4aaf" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">结论</h1><p id="4ab6" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">Attention U-Net 旨在通过在标准 U-Net 上附加注意门来进一步提高分割精度，并使用更少的训练样本。</p><p id="6c8b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Attention U-Net 消除了某些分割架构所需的外部对象定位模型的必要性，从而提高了模型对前景像素的灵敏度和准确性，而没有显著的计算开销。</p><p id="9fe8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Attention U-Net 还结合了基于网格的门控，这使得注意力系数更具体地针对局部区域。</p><p id="e663" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">阅读其他 U-Net 架构:</p><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/biomedical-image-segmentation-u-net-a787741837fa"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">生物医学图像分割:U-Net</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">使用非常少的训练图像，并产生更精确的分割。</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz ks nl"/></div></div></a></div><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/biomedical-image-segmentation-unet-991d075a3a4b"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">生物医学图像分割:UNet++</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">通过一系列嵌套、密集的跳过路径提高分段准确性</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="oa l nw nx ny nu nz ks nl"/></div></div></a></div><p id="4375" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里是 PyTorch 代码的注意 U-Net 架构:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><div class="kj kk kl km gt nl"><a rel="noopener follow" target="_blank" href="/data-scientist-the-dirtiest-job-of-the-21st-century-7f0c8215e845"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">数据科学家:21 世纪最肮脏的工作</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">40%的吸尘器，40%的看门人，20%的算命师。</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="oi l nw nx ny nu nz ks nl"/></div></div></a></div><div class="kj kk kl km gt ab cb"><figure class="oj kn ok ol om on oo paragraph-image"><a href="https://www.linkedin.com/in/jingles/"><img src="../Images/7820823f18c088b934fefc4fcbe5e6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*fPTPd_WxZ4Ey7iOVElxwJQ.png"/></a></figure><figure class="oj kn op ol om on oo paragraph-image"><a href="https://towardsdatascience.com/@jinglesnote"><img src="../Images/ed2857d42868ce52ed8f717376bc4cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*i2NzU4j49rZ36Mxz4gp4Sg.png"/></a></figure><figure class="oj kn op ol om on oo paragraph-image"><a href="https://jingles.substack.com/subscribe"><img src="../Images/c6faf13786230940c1756ff46938c471.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*oENDSDMTwXi2CJdO1gryug.png"/></a></figure></div></div></div>    
</body>
</html>