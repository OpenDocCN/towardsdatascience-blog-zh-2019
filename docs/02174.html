<html>
<head>
<title>Review: Residual Attention Network — Attention-Aware Features (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:剩余注意网络——注意感知特征(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-residual-attention-network-attention-aware-features-image-classification-7ae44c4f4b8?source=collection_archive---------12-----------------------#2019-04-10">https://towardsdatascience.com/review-residual-attention-network-attention-aware-features-image-classification-7ae44c4f4b8?source=collection_archive---------12-----------------------#2019-04-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d82b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">胜过<a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">预激活 ResNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004"> WRN </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">盗梦空间 ResNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a></h2></div><p id="95f5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi lc translated"><span class="l ld le lf bm lg lh li lj lk di">在</span>这个故事中，<strong class="ki ir">余额宝关注网</strong>，由<strong class="ki ir"> SenseTime </strong>、<strong class="ki ir">清华大学</strong>、<strong class="ki ir">香港中文大学(CUHK) </strong>、<strong class="ki ir">北京邮电大学</strong>，进行回顾。<strong class="ki ir">多个注意力模块叠加生成注意力感知特征</strong>。注意力剩余学习用于非常深的网络。最后，这是一篇<strong class="ki ir"> 2017 CVPR </strong>论文，引用超过<strong class="ki ir"> 200 次</strong>。(<a class="ll lm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----7ae44c4f4b8--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="f138" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">概述</h1><ol class=""><li id="8c74" class="mm mn iq ki b kj mo km mp kp mq kt mr kx ms lb mt mu mv mw bi translated"><strong class="ki ir">关注网络</strong></li><li id="fac8" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><strong class="ki ir">注意力剩余学习</strong></li><li id="051f" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><strong class="ki ir">软面膜分支</strong></li><li id="f64f" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><strong class="ki ir">整体架构</strong></li><li id="1870" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><strong class="ki ir">消融研究</strong></li><li id="5c49" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><strong class="ki ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="6d75" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 1。关注网络</strong></h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nc"><img src="../Images/79c58bb84338f177f2035f28a28ded30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhTbVMWeoxEFf5omAXk0ew.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Residual Attention Network</strong></figcaption></figure><ul class=""><li id="bc24" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">其中<strong class="ki ir"> <em class="nx"> p </em> </strong>为分割成主干分支和掩膜分支前预处理剩余单元的个数。</li><li id="895b" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir"> <em class="nx"> t </em> </strong>表示主干分支的剩余单元数<strong class="ki ir">。</strong></li><li id="3f3b" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir"> <em class="nx"> r </em> </strong>表示<strong class="ki ir">掩膜分支</strong>中相邻池层之间的剩余单元数。</li><li id="1535" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">在实验中，除非特别说明，<em class="nx"> p </em> =1，<em class="nx"> t </em> =2，<em class="nx"> r </em> =1。</li></ul><h2 id="2d7c" class="ny lv iq bd lw nz oa dn ma ob oc dp me kp od oe mg kt of og mi kx oh oi mk oj bi translated">1.1.掩模分支和主干分支</h2><ul class=""><li id="1f94" class="mm mn iq ki b kj mo km mp kp mq kt mr kx ms lb nw mu mv mw bi translated">剩余注意网络中有两个术语:掩蔽分支&amp;主干分支。</li><li id="287e" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir">主干分支</strong>:为<strong class="ki ir">特征提取</strong>的注意力模块中的上层分支。它们可以是<a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">预激活 ResNet </a>块或其他块。输入<em class="nx"> x </em>，输出<em class="nx"> T </em> ( <em class="nx"> x </em>)。</li><li id="0aec" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir"> Mask 分支</strong>:采用自下而上自上而下的结构学习同尺寸 mask <em class="nx"> M </em> ( <em class="nx"> x </em>)。这个<em class="nx"> M </em> ( <em class="nx"> x </em>)是作为类似<a class="ae kf" rel="noopener" target="_blank" href="/review-highway-networks-gating-function-to-highway-image-classification-5a33833797b5">高速公路网</a>的控制闸门。</li><li id="12e9" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">最后，注意模块<em class="nx"> H </em>的输出为:</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4bfeed3a201cbf38cb8c895d0328345d.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*ItwyFqSajGMA5C9YcgmAVQ.png"/></div></figure><ul class=""><li id="76c5" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">其中<em class="nx"> i </em>在空间位置范围内，并且<em class="nx"> c </em>是从 1 到<em class="nx"> C </em>的信道索引。</li><li id="508c" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir">注意力屏蔽可以在正向推理过程中充当特征选择器。</strong></li><li id="71b0" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">反向传播期间:</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/c703d6e256654ef0db52f51a1f6b5eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*EgYw_OGTy0lOgj5g8cH1_g.png"/></div></figure><ul class=""><li id="dac8" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">其中<em class="nx"> θ </em>为掩膜分支参数，<em class="nx"> φ </em>为主干分支参数。</li><li id="f5ef" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir">在反向传播期间，它还充当梯度更新过滤器。</strong></li><li id="ff06" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">因此，这使得注意力模块<strong class="ki ir">对噪声标签</strong>具有鲁棒性。屏蔽分支可以<strong class="ki ir">防止错误的梯度(来自噪声标签)来更新主干参数。</strong></li><li id="8a86" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">(这有点像<a class="ae kf" rel="noopener" target="_blank" href="/review-stn-spatial-transformer-network-image-classification-d3cbd98a70aa">【STN】</a>，但目标不同。STN 的目标是变形不变性，而注意网络的目标是生成注意感知特征。并且能够处理更具挑战性的数据集，如 ImageNet，其中图像包含需要建模的杂乱背景、复杂场景和大的外观变化。)</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi om"><img src="../Images/ebbb3a19a7c581241af5feaae9612c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWCR8SGuOBy_OouuXToM6w.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">An Example of Hot Air Balloon Images</strong></figcaption></figure><ul class=""><li id="0547" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">如上图所示，在热气球图像中，来自底层的<strong class="ki ir">蓝色特征有相应的天空遮罩来消除背景</strong>，而来自顶层的<strong class="ki ir">部分特征被气球实例遮罩</strong>细化。</li><li id="c23c" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">此外，<strong class="ki ir">堆叠网络结构的递增性质可以逐渐细化对复杂图像的注意力</strong>。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="aedf" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">2.<strong class="ak">注意力剩余学习</strong></h1><ul class=""><li id="f90c" class="mm mn iq ki b kj mo km mp kp mq kt mr kx ms lb nw mu mv mw bi translated">然而，<strong class="ki ir">幼稚的注意力学习(NAL) </strong>导致成绩下降。</li><li id="5466" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">这是因为掩模范围从 0 到 1 重复产生的点将<strong class="ki ir">降低深层特征的价值</strong>。</li><li id="0016" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">另外，软掩码<strong class="ki ir">可能会破坏主干分支</strong>的良好属性，例如来自<a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">预激活 ResNet </a>的剩余单元的相同映射。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/38e37891b9a0166d1cf7c562feeefdbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*SxPPFPwaBmBl3EJvmJdCGw.png"/></div></figure><ul class=""><li id="bbcb" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">如上构造一个更好的面具，叫做<strong class="ki ir">注意剩余学习(ARL) </strong>。</li><li id="5676" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><em class="nx"> F </em> ( <em class="nx"> x </em>)为原始特征，<em class="nx"> M </em> ( <em class="nx"> x </em>)范围为[0，1]。</li><li id="8eeb" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">因此，ARL 可以保持原有特色的良好属性。</li><li id="c9cf" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">堆叠的注意力模块可以如上图所示逐渐细化特征图。随着深度的增加，特征变得更加清晰。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="17b7" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">3.软掩膜分支</h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oo"><img src="../Images/e8b3766f303834c3782c395a40cf7d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MzHizZbaoXIXOn_OQPPRZw.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Soft Mask Branch</strong></figcaption></figure><ul class=""><li id="c8da" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">使用了<strong class="ki ir">自下而上自上而下的完全卷积结构</strong>。</li><li id="9721" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir">执行多次最大汇集，以在少量剩余单位后快速增加感受野</strong>。</li><li id="f796" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">然后，通过对称的自顶向下架构扩展全局信息，以引导每个位置的输入特征。</li><li id="317c" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">线性插值在一些剩余单元之后对输出进行上采样。</li><li id="fc21" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">然后，sigmoid 层在两次 1×1 卷积后对输出进行归一化。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="b8ee" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 4。整体架构</strong></h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ebc512a6b817d188ff2f2785ad830730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*-YdaeflivRsYIlEXXYfEgw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Overall Architecture</strong></figcaption></figure><ul class=""><li id="70ef" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">该网络由<strong class="ki ir"> 3 级</strong>组成，类似于<a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">预激活 ResNet </a>，每级堆叠相同数量的注意模块。</li><li id="ac7b" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">此外，在每个阶段添加两个剩余单元。</li><li id="1ff2" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">主干分支的加权层数为<strong class="ki ir"> 36 <em class="nx"> m </em> +20 </strong>其中<strong class="ki ir"> <em class="nx"> m </em>为一个阶段的关注模块数。</strong></li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="2293" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">5.<strong class="ak">消融研究</strong></h1><h2 id="8f41" class="ny lv iq bd lw nz oa dn ma ob oc dp me kp od oe mg kt of og mi kx oh oi mk oj bi translated">5.1.软遮罩分支中的激活功能</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/8c237a2685668cb856c7bab8b81798f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*Fwvmc-xetQfhX-GPiBuyrA.png"/></div></figure><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e08040a9e3db5820d0d59521be837ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*BTMBSfxd0tZFiMfv7859cw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Test Error (%) on CIFAR-10 of Attention-56</strong></figcaption></figure><ul class=""><li id="390d" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">除了 Sigmoid，其他类型的激活功能如上所述使用 CIFAR-10 和 56 重量层进行测试注意-56。</li><li id="cebf" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir">乙状结肠是上面三个中最好的</strong>。</li></ul><h2 id="04ae" class="ny lv iq bd lw nz oa dn ma ob oc dp me kp od oe mg kt of og mi kx oh oi mk oj bi translated">5.2.<strong class="ak">朴素注意学习(NAL) </strong> vs <strong class="ak">注意剩余学习(ARL) </strong></h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi os"><img src="../Images/30f258219f999bdb38057c03ceb61128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*bqbSvAyN8n-qajbLI4FeBw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Test Error on CIFAR-10</strong></figcaption></figure><ul class=""><li id="2414" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">用<em class="nx"> m </em> = {1，2，3，4}。分别导致关注度-56(以主干层深度命名)、关注度-92、关注度-128、关注度-164。</li><li id="c529" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">ARL 的表现一直优于 NAL。</li><li id="7841" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">随着注意模块数量的增加，NAL 出现了明显的退化。</li><li id="5170" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">在 RAL，当应用注意剩余学习时，性能随着注意模块的数量而增加。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/50fa1fa35c64e58b14c9ffb8bef5a134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*otgWDB-gFC--8HhMsY8aXA.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Mean Absolute Response Value Using Attention-164</strong></figcaption></figure><ul class=""><li id="c2e5" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">使用 Attention-164 测量每个阶段的输出层的平均绝对响应值。</li><li id="ae00" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir"> NAL 在第二阶段 4 个注意力模块后迅速消失。</strong></li><li id="9df1" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">ARL 可以在抑制噪声的同时保留有用的信息，使用相同的映射减轻信号衰减。它从降噪中获益，而没有显著的信息损失。</li></ul><h2 id="3596" class="ny lv iq bd lw nz oa dn ma ob oc dp me kp od oe mg kt of og mi kx oh oi mk oj bi translated">5.3.不同的掩模结构</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/7d1a63d19ccb000db29c959193aee4f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*f_ktbMER5XKaHEZ-bo1dJw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Test Error on CIFAR-10</strong></figcaption></figure><ul class=""><li id="3ce3" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated"><strong class="ki ir">局部卷积</strong>:没有编码器和解码器结构，只有卷积。</li><li id="8829" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir">编解码器</strong>:误差更小，得益于多尺度信息。</li></ul><h2 id="5c5e" class="ny lv iq bd lw nz oa dn ma ob oc dp me kp od oe mg kt of og mi kx oh oi mk oj bi translated">5.4.噪声标签鲁棒性</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/532416e43f50e3008a6257fb6ace2c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*6xgWRoJuRVOv7EMJo4Ajsg.png"/></div></figure><ul class=""><li id="43df" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">具有<em class="nx"> r </em>的混淆矩阵，干净标签比率，用于整个数据集。</li><li id="f87e" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">不同的<em class="nx"> r </em>，不同级别的标签噪声注入数据集。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/a2b10a8940e5167d5762dfe108db19b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*iYzOF7IMd-PKpdYaH8i5yQ.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Test Error on CIFAR-10 with Label Noises</strong></figcaption></figure><ul class=""><li id="0d5c" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">即使在高水平噪声数据下训练，ARL 也能表现良好。</li><li id="967a" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">当标签有噪声时，该掩模可以防止由标签误差引起的梯度，因为软掩模分支掩盖了错误的标签。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="5454" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 6。与最先进方法的比较</strong></h1><h2 id="fd0f" class="ny lv iq bd lw nz oa dn ma ob oc dp me kp od oe mg kt of og mi kx oh oi mk oj bi translated">6.1.西法尔-10 和西法尔-100</h2><ul class=""><li id="f735" class="mm mn iq ki b kj mo km mp kp mq kt mr kx ms lb nw mu mv mw bi translated">CIFAR-10 和 CIFAR-100 数据集分别由 10 类和 100 类的 60，000 幅 32×32 彩色图像组成，其中有 50，000 幅训练图像和 10，000 幅测试图像。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ox"><img src="../Images/9a9cea157698e5c2042b17d131cc32e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*zl_z9tuK2Skfo8epuDkKXg.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Comparisons with State-of-the-art Methods on CIFAR-10/100</strong></figcaption></figure><ul class=""><li id="cebc" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">Attention-452 由具有超参数设置的注意模块组成:{ <em class="nx"> p </em> = 2，<em class="nx"> t </em> = 4，<em class="nx"> r </em> = 3}和每阶段 6 个注意模块。</li><li id="ddf6" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">在注意模块方面，它优于<a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">预激活 ResNet </a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004"> WRN </a>。</li><li id="0baf" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir">注意-236 仅用一半的参数就胜过</strong><a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"><strong class="ki ir">ResNet-1001</strong></a><strong class="ki ir">。这意味着注意力模块和注意力剩余学习方案可以有效地减少网络中的参数数量，同时提高分类性能。</strong></li></ul><h2 id="b65f" class="ny lv iq bd lw nz oa dn ma ob oc dp me kp od oe mg kt of og mi kx oh oi mk oj bi translated">6.2.ImageNet</h2><ul class=""><li id="7eee" class="mm mn iq ki b kj mo km mp kp mq kt mr kx ms lb nw mu mv mw bi translated">ImageNet LSVRC 2012 数据集包含 1，000 个类，包含 120 万幅训练图像、50，000 幅验证图像和 100，000 幅测试图像。该评估是在 ImageNet LSVRC 2012 验证集的非黑名单图像上进行的。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oy"><img src="../Images/019a795952280ced57481dbd851b3bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tiJc1MIvWu2Mvk7-4LirDA.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Single Crop Validation Error on ImageNet</strong></figcaption></figure><ul class=""><li id="adc2" class="mm mn iq ki b kj kk km kn kp nt kt nu kx nv lb nw mu mv mw bi translated">与<a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"><strong class="ki ir">ResNet-152</strong></a>相比，<strong class="ki ir"> Attention-56 网络的 top-1 错误减少了 0.4%，top-5 错误减少了 0.26%，而<strong class="ki ir">只有 52%的参数和 56%的失败。</strong></strong></li><li id="f55d" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">并且<strong class="ki ir">剩余注意网络使用不同的基本单元</strong>可以很好的泛化。有了注意模块，它的表现优于没有注意模块的相应网络。</li><li id="1ae7" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir">注意 NeXt-56 网络性能与</strong> <a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> <strong class="ki ir"> ResNeXt-101 </strong> </a>相同，而<strong class="ki ir">参数和 FLOPs】明显少于<a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt-101 </a>。</strong></li><li id="3818" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir"> AttentionInception-56 的性能优于</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="ki ir">Inception-ResNet-v1</strong></a><strong class="ki ir">b</strong>y，前 1 个错误减少了 0.94%，前 5 个错误减少了 0.21%。</li><li id="7bee" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated"><strong class="ki ir">关注-92 大幅度胜过</strong><a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"><strong class="ki ir">ResNet-200</strong></a><strong class="ki ir"/>。top-1 误差减少 0.6%，而<a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"><strong class="ki ir">ResNet-200</strong></a><strong class="ki ir">网络比 Attention-92 </strong>多包含 32%的参数。</li><li id="c04f" class="mm mn iq ki b kj mx km my kp mz kt na kx nb lb nw mu mv mw bi translated">另外，<strong class="ki ir">注意力网络比</strong><a class="ae kf" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"><strong class="ki ir">ResNet-200</strong></a>减少了将近一半的训练时间。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h2 id="9b8b" class="ny lv iq bd lw nz oa dn ma ob oc dp me kp od oe mg kt of og mi kx oh oi mk oj bi translated">参考</h2><p id="2870" class="pw-post-body-paragraph kg kh iq ki b kj mo jr kl km mp ju ko kp oz kr ks kt pa kv kw kx pb kz la lb ij bi translated">【2017 CVPR】【剩余注意力网络】<br/> <a class="ae kf" href="https://arxiv.org/abs/1704.06904" rel="noopener ugc nofollow" target="_blank">用于图像分类的剩余注意力网络</a></p><h2 id="6914" class="ny lv iq bd lw nz oa dn ma ob oc dp me kp od oe mg kt of og mi kx oh oi mk oj bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kg kh iq ki b kj mo jr kl km mp ju ko kp oz kr ks kt pa kv kw kx pb kz la lb ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">物体检测<br/></strong><a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolo v1</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">yolo v2/yolo 9000</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">yolo v3</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a>]</p><p id="6582" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">语义切分<br/></strong><a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae kf" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">生物医学图像分割<br/> </strong> [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">累计视觉 1 </a> ] [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">累计视觉 2/DCAN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>]</p><p id="3134" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">实例分割<br/></strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">Hypercolumn</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">deep mask</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">sharp mask</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">multipath net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">Instance fcn</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS</a>]</p><p id="58de" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"/><br/><a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">【DeepPose】</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">【汤普森 NIPS'14】</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">【汤普森 CVPR'15】</a></p></div></div>    
</body>
</html>