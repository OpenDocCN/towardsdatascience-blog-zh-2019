<html>
<head>
<title>Double Deep Q Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">双深 Q 网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/double-deep-q-networks-905dd8325412?source=collection_archive---------1-----------------------#2019-07-17">https://towardsdatascience.com/double-deep-q-networks-905dd8325412?source=collection_archive---------1-----------------------#2019-07-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4653" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/Dqn-family" rel="noopener" target="_blank"> DQN 家族</a></h2><div class=""/><div class=""><h2 id="0ad0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">解决深度 Q 学习中的最大化偏差</h2></div><h1 id="92bb" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated"><strong class="ak">简介</strong></h1><p id="14b1" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在本帖中，我们将探究双 Q 学习网络背后的动机，并看看实现这一点的三种不同方式:</p><ul class=""><li id="0957" class="mc md iq li b lj me lm mf lp mg lt mh lx mi mb mj mk ml mm bi translated">原算法在<a class="ae mn" href="https://arxiv.org/pdf/1509.06461.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja">【双 Q 学习】(Hasselt，2010) </strong> </a></li><li id="e4d8" class="mc md iq li b lj mo lm mp lp mq lt mr lx ms mb mj mk ml mm bi translated"><a class="ae mn" href="https://arxiv.org/pdf/1509.06461.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja">《双 Q 学习的深度强化学习》(Hasselt et al .，2015) </strong> </a>中来自同一作者的更新算法，</li><li id="25d3" class="mc md iq li b lj mo lm mp lp mq lt mr lx ms mb mj mk ml mm bi translated">最近的方法，Clipped Double Q-learning，见于<a class="ae mn" href="https://arxiv.org/pdf/1802.09477.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="li ja">《Actor-Critic Methods 中的寻址函数逼近误差》(Fujimoto et al .，2018) </strong> </a>。</li></ul><p id="1349" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">如果你还不完全熟悉 Q-learning，我建议你快速看一下<a class="ae mn" rel="noopener" target="_blank" href="/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb">我关于 Q-learning 的帖子</a>！</p><h1 id="ee27" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">动机</h1><p id="68e3" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">考虑目标 Q 值:</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi mw"><img src="../Images/750c1b9bc3fcdf4c2779811d7c9a8563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAWamuglYGY6HRx9uCTC7Q.png"/></div></div></figure><p id="7b3c" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">具体来说，</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/92bf929192496c05f8e6102d9118921f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*pxg7T5Whro9LkUDvkQDMvg.png"/></div></figure><p id="61b0" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">照这样取最大高估值就是隐含地取最大值的估计值。这种系统性的高估在学习中引入了最大化偏差。由于 Q-learning 涉及 bootstrapping——从估计值中学习估计值——这种高估可能是有问题的。</p><blockquote class="nj nk nl"><p id="50d4" class="lg lh nm li b lj me ka ll lm mf kd lo nn mt lr ls no mu lv lw np mv lz ma mb ij bi translated">这里有一个例子:考虑一个单一状态<em class="iq"> s </em>，其中所有动作的真实 Q 值都等于 0，但是估计的 Q 值分布在零上下一些。取这些估计值的最大值(明显大于零)来更新 Q 函数会导致 Q 值的高估。</p></blockquote><p id="9538" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">Hasselt 等人(2015)在跨不同 Atari 游戏环境的实验中说明了这种高估偏差:</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/25365a21612c67194f1ec33f206dca3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*23FPxAUno0CcxAoFyXC8KQ.png"/></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk">Source: <a class="ae mn" href="https://arxiv.org/pdf/1509.06461.pdf" rel="noopener ugc nofollow" target="_blank">“Deep Reinforcement Learning with Double Q-learning” (Hasselt et al., 2015)</a>,</figcaption></figure><p id="e7d9" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">正如我们所看到的，传统的 DQN 倾向于大大高估行动价值，导致不稳定的培训和低质量的政策:</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a7bafc3a66ca70d7ce7070554921100b.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*kelIboHd-91jXkvSVLgBtQ.png"/></div></figure></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><h1 id="bd03" class="ko kp iq bd kq kr od kt ku kv oe kx ky kf of kg la ki og kj lc kl oh km le lf bi translated">解决方法:双 Q 学习</h1><p id="3d95" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">解决方案包括使用两个独立的 Q 值估计器，其中一个用于更新另一个。使用这些独立的估计，我们可以无偏 Q 值估计的行动选择使用相反的估计[3]。因此，我们可以通过从有偏差的估计中分离出我们的更新来避免最大化偏差。</p><p id="f07f" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">下面，我们将看看双 Q 学习的 3 种不同公式，并实现后两种。</p><p id="0b3d" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated"><strong class="li ja"> 1。“双 Q 学习”中的原始算法(Hasselt，2010) </strong></p><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/107490f9ff5938c7cc15627fc1f5100d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*NvvRn59pz-D1iSkBWpuIxA.png"/></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd oj">Pseudo-code Source: “Double Q-learning” (Hasselt, 2010)</strong></figcaption></figure><p id="cfa1" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">最初的双 Q 学习算法使用两个独立的估计值<code class="fe ok ol om on b">Q^{A}</code>和<code class="fe ok ol om on b">Q^{B}</code>。对于 0.5 的概率，我们使用估计值<code class="fe ok ol om on b">Q^{A}</code>来确定最大化动作，但是使用它来更新<code class="fe ok ol om on b">Q^{B}</code>。相反，我们使用<code class="fe ok ol om on b">Q^{B}</code>来确定最大化动作，但是使用它来更新<code class="fe ok ol om on b">Q^{A}</code>。通过这样做，我们获得了预期 Q 值的无偏估计量<code class="fe ok ol om on b">Q^{A}(state, argmaxQ^{next state, action)</code>,并抑制了偏差。</p><p id="b587" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated"><strong class="li ja"> 2。来自同一作者的“使用双 Q 学习的深度强化学习”(Hasselt 等人，2015)，</strong>的更新版本</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/986a3f8d843b06f73d20b4597110cfae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*4B46Bc9EDUdwrnqhAUp7hQ.png"/></div></figure><p id="6043" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">在第二个双 Q 学习算法中，我们有一个模型<code class="fe ok ol om on b">Q</code>和一个目标模型<code class="fe ok ol om on b">Q’</code>，而不是像(Hasselt，2010)中那样有两个独立的模型。我们使用<code class="fe ok ol om on b">Q’</code>进行动作选择，使用<code class="fe ok ol om on b">Q</code>进行动作评估。那就是:</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi op"><img src="../Images/ae11127265852f6b9d42b10872918eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8PMTWmT1XK1jdSK59QrYQ.png"/></div></div></figure><p id="2eee" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">我们最小化<code class="fe ok ol om on b">Q</code>和<code class="fe ok ol om on b">Q*</code>之间的均方误差，但是我们让<code class="fe ok ol om on b">Q'</code>慢慢复制<code class="fe ok ol om on b">Q</code>的参数。我们可以通过定期硬拷贝参数来实现，也可以通过 Polyak 平均来实现:</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/67d4b9219b013662a9f6778c7c9c1cf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*hfDpogX4Jbd0CWdMz4EDqg.png"/></div></figure><p id="9a7b" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">其中，θ'是目标网络参数，θ是主要网络参数，τ(平均速率)通常设置为 0.01。</p><p id="2af2" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated"><strong class="li ja"> 3。削波双 Q 学习，见于</strong> <a class="ae mn" href="https://arxiv.org/pdf/1802.09477.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja">《演员-评论家方法中的寻址函数逼近误差》(藤本等，2018) </strong> </a> <strong class="li ja">。</strong></p><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi or"><img src="../Images/67b30fb2b6989de9d57e82ecc19ec8db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*qHM7PWT-0AO7yRgifz8QKA.png"/></div></figure><p id="df56" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">在削波双 Q 学习中，我们遵循 Hasselt 2015 的原始公式。我们对真实的 Q 值有两个独立的估计。这里，为了计算更新目标，我们取由两个 Q 网络产生的两个下一状态动作值的最小值；当一方的 Q 估计值大于另一方时，我们将其降至最小，避免高估。</p><p id="48cf" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">Fujimoto 等人提出了这种设置的另一个好处:最小值算子应该为具有较低方差估计误差的状态提供较高的值。这意味着最小化将导致对具有低方差值估计的状态的偏好，从而导致具有稳定学习目标的更安全的策略更新。</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><h1 id="a95c" class="ko kp iq bd kq kr od kt ku kv oe kx ky kf of kg la ki og kj lc kl oh km le lf bi translated">实施指南</h1><p id="467b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们将从与本系列第 1 部分相同的 DQN 代理设置开始。如果你想看更完整的设置实现，请查看<a class="ae mn" rel="noopener" target="_blank" href="/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb">我的 Q-learning 帖子</a>或我的 Github 库(底部链接)。</p><p id="e668" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated"><em class="nm"> DQN 代理:</em></p><figure class="mx my mz na gt nb"><div class="bz fp l di"><div class="os ot l"/></div></figure><ol class=""><li id="cc2f" class="mc md iq li b lj me lm mf lp mg lt mh lx mi mb ou mk ml mm bi translated"><strong class="li ja">Hasselt 等人的双 Q 学习，2015: </strong></li></ol><p id="32b0" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">我们将初始化一个模型和一个目标模型:</p><figure class="mx my mz na gt nb"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="325c" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">为了计算损失，我们使用目标模型来计算下一个 Q 值:</p><figure class="mx my mz na gt nb"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="07ae" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">然后我们慢慢地将模型参数复制/平均到目标模型参数:</p><figure class="mx my mz na gt nb"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="fc43" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">2.<strong class="li ja">藤本等人 2018 年剪辑双 Q 学习:</strong></p><p id="e3e6" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">我们初始化两个 Q 网络:</p><figure class="mx my mz na gt nb"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="eb55" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">为了计算损耗，我们计算两个模型的当前状态 Q 值和下一状态 Q 值，但使用下一状态 Q 值的最小值来计算预期 Q 值。然后，我们使用预期的 Q 值更新两个模型。</p><figure class="mx my mz na gt nb"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="8840" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">最后是更新功能:</p><figure class="mx my mz na gt nb"><div class="bz fp l di"><div class="os ot l"/></div></figure></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="0734" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">这就结束了我们的双 Q 学习算法的实现。双 Q 学习经常在最新的 Q 学习变体和演员评论方法中使用。在我们以后的文章中，我们会一次又一次地看到这种技术。</p><p id="94ae" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated">感谢阅读！</p><p id="9ad2" class="pw-post-body-paragraph lg lh iq li b lj me ka ll lm mf kd lo lp mt lr ls lt mu lv lw lx mv lz ma mb ij bi translated"><strong class="li ja">在这里找到我的完整实现:</strong></p><div class="ov ow gp gr ox oy"><a href="https://github.com/cyoon1729/deep-Q-networks" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd ja gy z fp pd fr fs pe fu fw iz bi translated">cy oon 1729/深度 Q 网络</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">Q-learning 家族(PyTorch)算法的模块化实现。实现包括:DQN，DDQN，决斗…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">github.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm ng oy"/></div></div></a></div><h1 id="1540" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">参考</h1><ul class=""><li id="d3af" class="mc md iq li b lj lk lm ln lp pn lt po lx pp mb mj mk ml mm bi translated"><a class="ae mn" href="https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf" rel="noopener ugc nofollow" target="_blank">“使用函数逼近进行强化学习的问题”(Thrun 和 Schwartz，1993) </a></li><li id="40bf" class="mc md iq li b lj mo lm mp lp mq lt mr lx ms mb mj mk ml mm bi translated"><a class="ae mn" href="https://arxiv.org/pdf/1509.06461.pdf" rel="noopener ugc nofollow" target="_blank">“双 Q 学习”(Hasselt，2010) </a></li><li id="1e18" class="mc md iq li b lj mo lm mp lp mq lt mr lx ms mb mj mk ml mm bi translated"><a class="ae mn" href="https://arxiv.org/pdf/1509.06461.pdf" rel="noopener ugc nofollow" target="_blank">“双 Q 学习的深度强化学习”(Hasselt et al .，2015) </a>，</li><li id="65b2" class="mc md iq li b lj mo lm mp lp mq lt mr lx ms mb mj mk ml mm bi translated"><a class="ae mn" href="https://arxiv.org/pdf/1802.09477.pdf" rel="noopener ugc nofollow" target="_blank">“演员-评论家方法中的寻址函数近似误差”(藤本等人，2018) </a></li><li id="1301" class="mc md iq li b lj mo lm mp lp mq lt mr lx ms mb mj mk ml mm bi translated">强化学习:导论(萨顿和巴尔托)</li></ul></div></div>    
</body>
</html>