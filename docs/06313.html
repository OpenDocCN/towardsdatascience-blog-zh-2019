<html>
<head>
<title>Implementing Random Forest in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 R 中实现随机森林</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implement-random-forest-in-r-b00b69eb8501?source=collection_archive---------13-----------------------#2019-09-11">https://towardsdatascience.com/implement-random-forest-in-r-b00b69eb8501?source=collection_archive---------13-----------------------#2019-09-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="024a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">随机森林在乳腺癌患者分类中的实际应用</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6460f579fb641045ce4ae10e30b3054c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*S0EUat4BGC-Rqx8V"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@ruralexplorer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Rural Explorer</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="78e6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">什么是随机森林(RF)？</strong></h1><p id="6643" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了理解 RF，我们需要首先理解决策树。<a class="ae ky" href="https://medium.com/@rajesh_brid?source=post_page-----dc506a403aeb----------------------" rel="noopener"> Rajesh S. Brid </a>给<a class="ae ky" href="https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb" rel="noopener">写了一篇关于决策树的详细文章</a>。我们不会过多地讨论决策树的定义，因为这不是本文的目的。我只想快速总结几点。决策树是一系列是/否问题。对于树的每一级，如果你的答案是肯定的，你就属于一个类别，否则，你就属于另一个类别。你将回答这一系列是/否的问题，直到你到达最后一类。你将被归入那一组。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/30f96583b1534fd2077a80dbfb283274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/0*-R_7XJWuRwGsMDF8"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Taken from <a class="ae ky" href="https://www.aitimejournal.com/@akshay.chavan/a-comprehensive-guide-to-decision-tree-learning" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="4677" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">对于我们用来训练的数据，树工作得很好，但是当涉及到新的数据样本时，它们的表现并不好。幸运的是，我们有随机森林，它是许多具有灵活性的决策树的组合，因此导致了准确性的提高。</p><p id="d311" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在这里，我不会过多地讨论 RF 的细节，因为我们可以从外部的各种来源了解它背后的数学原理。<a class="ae ky" rel="noopener" target="_blank" href="/understanding-random-forest-58381e0602d2">这里的</a>就是其中之一。</p><p id="0a32" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">本文更多的是关于 RF 在癌症患者分类中的实际应用，所以我将直接进入编码部分。现在让我们打开 Rstudio，动手干一场:)</p><h1 id="6c55" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">在 R 中实现 RF</h1><p id="88e3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先，我们需要加载以下包。如果您不能加载它们，很可能您还没有安装它们。所以请在加载下面的包之前先这样做。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="8c4c" class="my la it mu b gy mz na l nb nc">library(ggplot2)<br/>library(corrplot)<br/>library(reshape2)<br/>library(ggthemes)<br/>library(dplyr)<br/>library(randomForest)<br/>Wisconsin = read.table(url(paste0("<a class="ae ky" href="https://archive.ics.uci.edu/ml/machine-learning-databases/" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/</a>",<br/>"breast-cancer-wisconsin/wdbc.data")),header=FALSE,sep=",",nrows=570)</span></pre><p id="fdc6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我直接从 web 链接中读取数据，并将数据集命名为 Wisconsin。让我们稍微检查一下数据</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="c0af" class="my la it mu b gy mz na l nb nc">head(Wisconsin)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/765cb18dad3c31eee885d31b25a6080b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GV-xv7a7Ad_rA4UlIIVgpw.png"/></div></div></figure><p id="ac0e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">V1 是 ID，所以它与我们这里的分析无关。V2 是分类结果，“M”代表“恶性”，“B”代表“良性”。剩下的只是关于癌症诊断信息的变量。</p><p id="d5d6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在我想把 M 和 B 改成真和假，以便于解释。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="1324" class="my la it mu b gy mz na l nb nc">Wisconsin$V2 &lt;- Wisconsin$V2 == “M”</span></pre><p id="f3d6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">预处理数据</strong></p><p id="0ac3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">首先，我们将数据混洗并分成训练和测试。我们决定七三分成。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="bbd7" class="my la it mu b gy mz na l nb nc">set.seed(2019)</span><span id="41a0" class="my la it mu b gy ne na l nb nc">test_size = floor(0.3 * nrow(Wisconsin))<br/>samp = sample(nrow(Wisconsin), test_size,replace = FALSE)</span><span id="c65e" class="my la it mu b gy ne na l nb nc">y_train = Wisconsin[-samp,2]<br/>x_train = Wisconsin[-samp,-c(1,2)] #since the first column is just ID<br/>y_test= Wisconsin[samp,2]<br/>x_test = Wisconsin[samp,-c(1,2)] #since the first column is just ID</span><span id="b226" class="my la it mu b gy ne na l nb nc"><strong class="mu iu">#convert labels to categorical</strong><br/>y_train = factor(y_train)<br/>y_test = factor(y_test)</span></pre><p id="363c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们应该注意，RF 只在响应变量是一个<strong class="lt iu"> <em class="nf">因子</em> </strong>时才起作用。<strong class="lt iu">刚才我们把‘M’和‘B’转换成 TRUE 和 FALSE 的时候，这个变量的类型是逻辑的。因此，我们需要使用 factor()函数</strong>将其转换为 factor。</p><p id="778a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在让我们将 x 和 y 组合起来，形成训练集和测试集。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="29cf" class="my la it mu b gy mz na l nb nc">#Create training set and testing set<br/>train = cbind(y_train,x_train)<br/>test = cbind(y_test,x_test)</span></pre><p id="cca0" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">训练集将用于训练 RF 模型，测试集将用于测试模型的性能。现在让我们给我们的响应变量命名。在这里，我把它命名为“标签”</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d79f" class="my la it mu b gy mz na l nb nc">colnames(train)[1] = ‘label’<br/>colnames(test)[1] = ‘label’</span></pre><p id="2907" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">它现在看起来像这样</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/d6732c22daa38b09db77f108e8bbe438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BF3dTpXvzGd314hCO9e5-g.png"/></div></div></figure><p id="7a3f" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">拟合随机森林模型</strong></p><p id="f690" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在一切准备就绪。我们可以开始拟合模型了。这一步很容易。</p><p id="0de8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">包中的“randomForest()”函数使随机森林模型适合数据。除了包括数据集和指定公式和标签之外，该函数的一些关键参数包括:</p><p id="17cd" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">1.<strong class="lt iu"> ntree </strong>:要种植的树木数量。默认值为 500。</p><p id="9b77" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">2.<strong class="lt iu"> mtry </strong>:每次分割随机选择的变量个数。在这个例子中，我们使用 p 的平方根(p 表示预测值的数量)。请注意，对于回归分析，一般规则是使用 mtry = p/3，这也是该参数在回归中的默认值。</p><p id="5cf4" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">3.<strong class="lt iu">重要性</strong>:如果为真，模型将计算特征的重要性，以供进一步分析。(默认值=假)</p><p id="6330" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">4.<strong class="lt iu">邻近度</strong>:如果为真，模型将包含一个 N*N 矩阵，代表邻近度。</p><p id="4f61" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">5.<strong class="lt iu"> maxnodes </strong>:树可以拥有的最大终端节点数。</p><p id="6a7e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">6.<strong class="lt iu">娜</strong>。<strong class="lt iu">动作</strong>:指定如何处理缺失数据的功能。</p><p id="91bc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">由于有 30 个独立变量，我们将<strong class="lt iu"> mtry </strong>设为 30 的平方根，然后拟合模型</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="f131" class="my la it mu b gy mz na l nb nc">mtry = sqrt(30)<br/>model_1 = randomForest(label~., data = train, importance = TRUE)</span></pre><p id="f3ed" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">就是这样。简单不是吗？现在我们已经有了一个射频模型</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="cb80" class="my la it mu b gy mz na l nb nc">print(model_1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/24a1b9dea8773acb5c69ce71d4370d33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-P-QE9_Y0cCeToj-sZ92A.png"/></div></div></figure><p id="40cb" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">出袋 OOB 误差估计率为 3%，这是非常好的，即 97%的准确度。如果我们看混淆矩阵，我们可以看到分类误差相当低。这表明我们的 RF 模型在分类训练集方面表现良好。</p><p id="ac07" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">让我们用测试集来测试这个模型。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="a679" class="my la it mu b gy mz na l nb nc">pred_1 = predict(model_1, x_test)<br/>table(y_test, pred_1)</span><span id="0543" class="my la it mu b gy ne na l nb nc">accuracy_m1 = mean(y_test == pred_1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/9f5d82b18b1aa0b6c685e772a8bc6d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*P9yMyvX0IzdRgVc_f_L2CA.png"/></div></figure><p id="3cce" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来我们的模型在测试集上表现也很好，准确率达到 95%。</p><p id="9916" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">可变重要性</strong></p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="1483" class="my la it mu b gy mz na l nb nc">varImpPlot(model_1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/a9254ab642326bdb67805f65d18a2833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PaBHWfaudOT5dWgogM8PYQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Importance of variables in the model. <strong class="bd nk">The higher the rank, the more important the variables</strong></figcaption></figure><p id="2a87" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们可视化绘图的另一种方法是使用<strong class="lt iu"> ggplot 包。</strong>请注意，下面的代码是为了可视化“平均降低准确度”。要得到“平均下降基尼系数”，只需将下面的粗体字改为“MeanDecreaseAccuracy”(无间距)。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="0bdf" class="my la it mu b gy mz na l nb nc">importance = importance(model_1)<br/>varImportance = data.frame(Variables = row.names(importance),<br/> Importance =round(importance[, “<strong class="mu iu">MeanDecreaseAccuracy</strong>”],2))</span><span id="c77b" class="my la it mu b gy ne na l nb nc">rankImportance=varImportance%&gt;%mutate(Rank=paste(‘#’,dense_rank(desc(Importance))))</span><span id="bdb8" class="my la it mu b gy ne na l nb nc">ggplot(rankImportance,aes(x=reorder(Variables,Importance),<br/> y=Importance,fill=Importance))+ <br/> geom_bar(stat=’identity’) + <br/> geom_text(aes(x = Variables, y = 0.5, label = Rank),<br/> hjust=0, vjust=0.55, size = 4, colour = ‘white’) +<br/> labs(x = ‘Variables’) +<br/> coord_flip() + <br/> theme_classic()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/53e8c5507d53f397b869e6ccb42b7406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4b4s9jdZc5kDas_xfGoXlQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Mean Decrease Accuracy</figcaption></figure><p id="f910" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">结果和我们之前得到的图类似。结果显示变量 V25、V30、V26 和 V23 是最重要的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/12a1952d53467518584f60a9f16b9d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IsDRlHeNE-tXu5tr5esNbw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Mean Decrease Gini</figcaption></figure><p id="a5a8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">使用平均下降基尼系数，我们得到 V25、V23 和 V26 作为最重要的变量。</p><h1 id="4441" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="7195" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本文展示了如何实现一个简单的随机森林模型来解决分类问题。我没有深入研究如何调整参数以优化模型，因为分类的准确度如此之高，我认为简单的模型就足够了。然而，在现实生活中，还有其他更复杂的分类问题需要我们调整参数以获得最佳模型，我将在下一次单独撰写一篇文章。但重要的是要记住总是从简单的模型开始，然后从那里建立模型以获得更好的预测。</p><p id="6bf9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">谢谢你的时间。我希望这篇文章能帮助你们，尤其是那些以前从未尝试过在 R 中实现 RF 的人，更好地了解如何实现。如果您有任何意见或问题，请告诉我。</p><p id="50e9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">祝您愉快，编程愉快:)</p></div></div>    
</body>
</html>