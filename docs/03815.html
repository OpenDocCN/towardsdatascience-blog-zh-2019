<html>
<head>
<title>Smart Prosthetics with Object Detection using Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用张量流进行物体检测的智能假肢</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/smart-prosthetics-with-object-detection-using-tensorflow-2bbb371c888d?source=collection_archive---------18-----------------------#2019-06-16">https://towardsdatascience.com/smart-prosthetics-with-object-detection-using-tensorflow-2bbb371c888d?source=collection_archive---------18-----------------------#2019-06-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="3c56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在北卡罗来纳州立大学主动机器人传感(ARoS)实验室工作期间，我有机会参与一个项目，利用计算机视觉技术对上肢假肢进行更智能的控制。一个假臂可以检测到它试图与之互动的是什么样的物体，并相应地调整它的动作。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/c04ca698a55925b4ceaaf1f1c9c2bfb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pww2ko5MjsloZsZc8LTYUQ.jpeg"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Source: Newcastle University</figcaption></figure><p id="1cf2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">类似的工作已经在<a class="ae ko" href="https://www.ncl.ac.uk/press/articles/archive/2017/05/handthatsees/" rel="noopener ugc nofollow" target="_blank">纽卡斯特大学</a>和<a class="ae ko" href="https://blogs.technet.microsoft.com/machinelearning/2018/09/10/why-would-prosthetic-arms-need-to-see-or-connect-to-cloud-ai/" rel="noopener ugc nofollow" target="_blank">微软创新杯的获胜者身上完成。</a>在微软项目的情况下，图像数据被发送到 Azure 云，使用<a class="ae ko" href="https://azure.microsoft.com/en-us/services/cognitive-services/custom-vision-service/" rel="noopener ugc nofollow" target="_blank"> Azure 自定义视觉服务</a>进行对象检测和分类。</p><p id="59c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们的方法中，我们希望演示如何在边缘完成对象检测和分类，嵌入在修复设备本身中。该系统由一个假肢、一个 NVIDIA GPU 和一个 USB 摄像头组成。相机会将图像帧发送到 GPU，GPU 会识别对象的类型，然后将这些信息发送到假肢。然后，手臂可以以允许其与被识别的物体最佳交互的方式移动。</p><p id="d449" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我使用 OpenCV 并实现了一个<a class="ae ko" rel="noopener" target="_blank" href="/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab">单次多盒检测器</a> (SSD)算法，使用<a class="ae ko" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>在上下文 (COCO)数据集的<a class="ae ko" href="http://cocodataset.org/" rel="noopener ugc nofollow" target="_blank">公共对象上进行训练。该程序部署在 NVIDIA Jetson TX2 GPU 上，用于处理来自连接到假肢的摄像头的图像。</a></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lf"><img src="../Images/1eba99b5197c3df213a251fcf667fa37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1xW2nRk8DlP_k_sixpbuQ.png"/></div></div></figure><p id="95a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为原型制作的一部分，<a class="ae ko" href="https://opencv.org/" rel="noopener ugc nofollow" target="_blank"> OpenCV </a>被用来实现计算机视觉技术，比如对样本图像文件进行 canny 边缘检测。该算法将获取一幅图像，绘制出它的边缘，并输出一幅只显示边缘的新图像。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/795545ceba16fe3b511c5a0bc3945318.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/0*8YkoltCWgoM8ck7C"/></div></figure><p id="16e2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可以通过定位图像颜色变化的位置来找到边缘。在 canny 边缘检测中，这是通过找到像素的强度梯度来完成的。然后，为了减少噪声，设置高和低阈值来确定哪些片段实际上是边缘。</p><p id="7beb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">梯度可以用以下公式计算:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/d5489f6a86d9863668f7e2ba6e928f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/0*tVmpJeXXvdZ1N_Xn"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">More details can be found at <a class="ae ko" href="https://docs.opencv.org/master/da/d22/tutorial_py_canny.html" rel="noopener ugc nofollow" target="_blank">https://docs.opencv.org/master/da/d22/tutorial_py_canny.html</a></figcaption></figure><p id="c62b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个程序随后得到了增强，可以从传入的摄像机流中读取帧。</p><pre class="kq kr ks kt gt li lj lk ll aw lm bi"><span id="ed79" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu">import</strong> <strong class="lj iu">numpy</strong> <strong class="lj iu">as</strong> <strong class="lj iu">np</strong><br/><strong class="lj iu">import</strong> <strong class="lj iu">cv2</strong><br/><br/>cap_stream = cv2.VideoCapture(1)<br/><strong class="lj iu">while</strong>(cap_stream.isOpened()):<br/>        <em class="lt"># Take each frame</em><br/>        ret, frame = cap_stream.read()<br/>        <strong class="lj iu">if</strong> ret == True:<br/>            edges = cv2.Canny(frame,100,200)<br/>            cv2.imshow('edges',edges)<br/>            <strong class="lj iu">if</strong> cv2.waitKey(1) &amp; 0xFF == ord('q'):<br/>                <strong class="lj iu">break</strong><br/>cap_stream.release()</span></pre><p id="b9e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">精明的边缘检测允许操作摄像机的输入，但是我们需要解释输入的图像。基于 Tensorflow 的深度学习框架用于对传入的相机馈送执行对象检测。SSD Inception COCO V2 <strong class="js iu">，</strong>使用在上下文中的公共对象(COCO)数据集上训练的预训练模型。</p><p id="9083" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">单镜头检测器使用一个网络来识别区域并对其进行分类，而不是将这两项任务分开。这种方法是嵌入式设备的首选，因为它的计算成本较低。</p><p id="98d4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对象检测的初始原型是在单个静止图像上完成的。这是通过加载预训练的模型来完成的，每当检测到对象时，使用 OpenCV 在对象周围绘制一个边界框，并用名称和置信度来标记它。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lu"><img src="../Images/eb99c75917aef70f43f3ac822107970c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Q5Asw37GjuOGG1PP"/></div></div></figure><p id="5d4c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了实时执行对象检测，步骤非常相似，除了接收输入，使用来自 OpenCV 的相机流，而不是读取文件。这类似于执行实时 canny 边缘检测的步骤。然后，相机流中的每一帧都被输入到 Tensorflow 会话中，这样就可以识别物体了。输出是一个视频源，其边界框类似于上面显示的静态图像输出。</p><p id="ed69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此过程的性能比预期的要慢，每帧大约需要 3 秒来识别对象，从而导致延迟馈送。为了使对象检测更有效，一个单独的线程专门用于处理相机的 I/O，从而产生更快的输出(不到 1 秒)。</p><p id="1037" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是张量流代码的一个片段:</p><pre class="kq kr ks kt gt li lj lk ll aw lm bi"><span id="0183" class="ln lo it lj b gy lp lq l lr ls">video_capture = WebcamVideoStream(src=1).start()     <br/>fps = FPS().start()<br/>config = tf.ConfigProto()<br/> config.gpu_options.allow_growth = True<br/> with detection_graph.as_default():<br/>     with tf.Session(config = config,graph=detection_graph) as sess:<br/>  while True:<br/>      image_np = video_capture.read()<br/>  #input and output tensors for detection_graph<br/>      image_tensor=detection_graph.get_tensor_by_name("image_tensor:0")<br/>      detection_boxes = detection_graph.get_tensor_by_name("detection_boxes:0")<br/>      detection_scores = detection_graph.get_tensor_by_name("detection_scores:0")<br/>      detection_classes = detection_graph.get_tensor_by_name("detection_classes:0")<br/>      num_detections = detection_graph.get_tensor_by_name("num_detections:0")<br/>  <br/>      <br/>      image_np_expanded = np.expand_dims(image_np,axis=0)<br/>      (boxes,scores,classes,num)= sess.run(<br/>              [detection_boxes, detection_scores, detection_classes, num_detections],<br/>      feed_dict={image_tensor: image_np_expanded})<br/>      vis_util.visualize_boxes_and_labels_on_image_array(<br/>            image_np,<br/>            np.squeeze(boxes),<br/>            np.squeeze(classes).astype(np.int32),<br/>            np.squeeze(scores),<br/>            category_index,<br/>          use_normalized_coordinates=True, line_thickness=8)<br/>      cv2.imshow('object detection', cv2.resize(image_np, (800,600)))<br/>      if cv2.waitKey(25) &amp; 0xFF == ord('q'):<br/>         cv2.destroyAllWindows()<br/>         break<br/></span></pre><p id="7b58" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后将这个 python 应用程序加载到运行 Linux 的 GPU 上并进行测试。GPU 的视频输入由通过 USB 连接的摄像头提供。</p><p id="f588" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个项目中遇到的大多数挑战都与安装和配置 GPU 以及运行对象检测应用程序所需的库有关。</p><p id="ebf0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该项目能够实现预期的结果，即能够在 GPU 上使用深度学习近实时演示对象检测。这为将其嵌入假肢以获得更好的适应性提供了可行性。</p><p id="8435" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过创建用于更准确的对象检测的自定义模型，而不是使用在 COCO 上训练的 SSD 模型，可以对该项目进行进一步的增强。可以通过调整曝光和亮度来消除输入视频帧中的异常值，从而对视频捕捉进行改进。</p><p id="6b2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">链接到我的 GitHub 项目:<a class="ae ko" href="https://github.com/rohinisharma/AROS-hsproject" rel="noopener ugc nofollow" target="_blank">https://github.com/rohinisharma/AROS-hsproject</a></p></div></div>    
</body>
</html>