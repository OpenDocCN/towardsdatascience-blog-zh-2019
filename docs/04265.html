<html>
<head>
<title>The Hitchhikers guide to handle Big Data using Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Spark 处理大数据的指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a?source=collection_archive---------4-----------------------#2019-07-03">https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a?source=collection_archive---------4-----------------------#2019-07-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ce8016ab2b18c7cfd443ab4aac53949e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Xb5ACP76W6d9jysr"/></div></div></figure><div class=""/><div class=""><h2 id="c21f" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">不仅仅是介绍</h2></div><p id="220f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">大数据已经成为数据工程的代名词。</p><p id="a075" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">但是数据工程和数据科学家之间的界限日益模糊。</p><p id="e402" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">此时此刻，我认为大数据必须是所有数据科学家的必备技能。</p><p id="56dd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">原因:<strong class="kv jf"> <em class="lp">每天生成太多数据</em> </strong></p><p id="671b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这就把我们带到了<a class="ae lq" href="https://amzn.to/2JZBgou" rel="noopener ugc nofollow" target="_blank"> Spark </a>。</p><p id="17fb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，大多数 Spark 文档虽然不错，但没有从数据科学家的角度进行解释。</p><p id="998c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">所以我想尝试一下。</p><p id="45b0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">这篇文章的主题是——“如何让 Spark 发挥作用？”</strong></p><p id="0fa6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这个帖子会很长。实际上我在媒体上最长的帖子，所以去买杯咖啡吧。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="cfc6" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">这一切是如何开始的？-MapReduce</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mq"><img src="../Images/b1cf32053bd2e82274d0beb83a409c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p3GQMEIPLX3TpTk0"/></div></div></figure><p id="2a34" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">假设你的任务是砍伐森林中的所有树木。也许在全球变暖的情况下，这不是一个好生意，但这符合我们的目的，我们只是在假设，所以我会继续。你有两个选择:</p><ul class=""><li id="6955" class="mv mw je kv b kw kx kz la lc mx lg my lk mz lo na nb nc nd bi translated"><em class="lp">让巴蒂斯塔带着电锯</em>去做你的工作，让他一棵一棵地砍下每棵树。</li><li id="6af1" class="mv mw je kv b kw ne kz nf lc ng lg nh lk ni lo na nb nc nd bi translated"><em class="lp">找 500 个有普通轴的普通人</em>让他们在不同的树上工作。</li></ul><p id="5dc1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf"> <em class="lp">你更喜欢哪个？</em>T19】</strong></p><p id="d59d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">尽管有些人仍然会选择选项 1，但是对选项 2 的需求导致了 MapReduce 的出现。</p><p id="d7e2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">用 Bigdata 的话来说，我们称 Batista 解决方案为纵向扩展<strong class="kv jf"><em class="lp">/纵向扩展</em> </strong>，即在单个 worker 中添加/填充大量 RAM 和硬盘。</p><p id="2605" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">第二种解决方案叫做水平缩放<strong class="kv jf"><em class="lp">/横向缩放</em> </strong>。就像你把许多普通的机器(内存较少)连接在一起，并行使用它们。</p><p id="5a6b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，垂直扩展比水平扩展有一定的优势:</p><ul class=""><li id="db64" class="mv mw je kv b kw kx kz la lc mx lg my lk mz lo na nb nc nd bi translated"><strong class="kv jf">问题规模小就快:</strong>想 2 棵树。巴蒂斯塔会用他的电锯干掉他们两个，而我们的两个家伙还在用斧子砍人。</li><li id="388a" class="mv mw je kv b kw ne kz nf lc ng lg nh lk ni lo na nb nc nd bi translated"><strong class="kv jf">很容易理解。这是我们一贯的做事方式。我们通常以顺序模式思考问题，这就是我们整个计算机体系结构和设计的演变。</strong></li></ul><p id="b497" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">但是，水平缩放是</p><ul class=""><li id="f9d0" class="mv mw je kv b kw kx kz la lc mx lg my lk mz lo na nb nc nd bi translated"><strong class="kv jf">更便宜:</strong>得到 50 个正常家伙本身就比得到巴蒂斯塔这样的单身家伙便宜多了。除此之外，巴蒂斯塔需要大量的照顾和维护，以保持冷静，他非常敏感，即使是小东西，就像高容量内存的机器。</li><li id="2267" class="mv mw je kv b kw ne kz nf lc ng lg nh lk ni lo na nb nc nd bi translated"><strong class="kv jf">问题规模大时速度更快:</strong>现在想象 1000 棵树和 1000 个工人 vs 一个 Batista。通过横向扩展，如果我们面临一个非常大的问题，我们只需多雇佣 100 或 1000 名廉价工人。巴蒂斯塔就不是这样了。你必须增加内存，这意味着更多的冷却基础设施和更多的维护成本。</li></ul><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/a4182a94202a02990285370d7e7bc197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*R3J6DVBDinxLU3h-.png"/></div></div></figure><p id="1b0f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf"> <em class="lp"> MapReduce </em> </strong>通过让我们使用<strong class="kv jf"> <em class="lp">计算机集群</em> </strong>进行并行化，使得第二种选择成为可能。</p><p id="8160" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，MapReduce 看起来像一个相当专业的术语。但是让我们打破它一点。MapReduce 由两个术语组成:</p><h2 id="b030" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">地图:</h2><p id="e7d0" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">它基本上是应用/映射功能。我们将数据分成 n 个数据块，并将每个数据块发送给不同的工作器(映射器)。如果我们希望对数据行应用任何函数，我们的工作人员会这样做。</p><h2 id="c1a0" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">减少:</h2><p id="a550" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">使用基于 groupby 键的函数来聚合数据。它基本上是一个团体。</p><p id="4825" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当然，有很多事情在后台进行，以使系统按预期工作。</p><p id="c309" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">不要担心，如果你还不明白的话。继续读下去。在我将要提供的例子中，当我们自己使用 MapReduce 时，也许你就会明白了。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="6df9" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">为什么是火花？</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl ob"><img src="../Images/0d588437d29b67e6f0c066d3414df63a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*sQGVLk43kXJTEw1mtJRoDw.png"/></div><figcaption class="oc od gj gh gi oe of bd b be z dk">Because Pyspark</figcaption></figure><p id="03fb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Hadoop 是第一个向我们介绍 MapReduce 编程范式的开源系统，Spark 是使它更快的系统，快得多(100 倍)。</p><p id="9905" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Hadoop 中曾经有大量的数据移动，因为它曾经将中间结果写入文件系统。</p><p id="5322" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这影响了你分析的速度。</p><p id="53a8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Spark 给我们提供了一个内存模型，所以 Spark 在工作的时候不会对磁盘写太多。</p><p id="651d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">简单地说，Spark 比 Hadoop 快，现在很多人都在用 Spark。</p><p id="ddc8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf"> <em class="lp">那么我们就不再多说，开始吧。</em>T13】</strong></p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="6026" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">Spark 入门</h1><p id="f6be" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">安装 Spark 其实本身就很头疼。</p><p id="cad6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">由于我们想了解它是如何工作的，并真正使用它，我建议您在社区版中使用 Sparks on Databricks<a class="ae lq" href="https://databricks.com/try-databricks?utm_source=databricks&amp;utm_medium=homev2tiletest" rel="noopener ugc nofollow" target="_blank">here</a>online。别担心，这是免费的。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi og"><img src="../Images/85a85d6948be5cc4238200225d112161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FlnBNKEiXOsvhmyuFouLeQ.png"/></div></div></figure><p id="3acc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">一旦您注册并登录，将出现以下屏幕。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/83ecebde6cde556a2e4b62d02da3fc6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3xVyJoqUqMwLL5sx6vQSgA.png"/></div></div></figure><p id="d68b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你可以在这里开始一个新的笔记本。</p><p id="383e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">选择 Python 笔记本，并为其命名。</p><p id="7858" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">一旦您启动一个新的笔记本并尝试执行任何命令，笔记本会询问您是否要启动一个新的集群。动手吧。</p><p id="0b00" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下一步将检查 sparkcontext 是否存在。要检查 sparkcontext 是否存在，只需运行以下命令:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="1fd4" class="nk lz je oj b gy on oo l op oq">sc</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/375aaf2b2cca6d579775a5ca10cb23ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mqbJfUss0Bn1amBWjD9klA.png"/></div></div></figure><p id="eec6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这意味着我们可以在笔记本上运行 Spark。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="b60d" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">加载一些数据</h1><p id="9af2" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">下一步是上传一些我们将用来学习 Spark 的数据。只需点击主页选项卡上的“导入和浏览数据”。</p><p id="fe25" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这篇文章的最后，我将使用多个数据集，但让我们从一些非常简单的开始。</p><p id="23d7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们添加文件<code class="fe os ot ou oj b">shakespeare.txt</code>，你可以从<a class="ae lq" href="https://github.com/MLWhiz/data_science_blogs/tree/master/spark_post" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/6e25eeaa6d8dd283ee8985542dfdadc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1u6-deQqFV24NpV8wQxrYQ.png"/></div></div></figure><p id="6b0f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">您可以看到文件被加载到了<code class="fe os ot ou oj b">/FileStore/tables/shakespeare.txt</code>位置。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="aa06" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">我们的第一个星火计划</h1><p id="f285" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">我喜欢通过例子来学习，所以让我们完成分布式计算的“Hello World”:<strong class="kv jf"><em class="lp">word count 程序。</em> </strong></p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="7881" class="nk lz je oj b gy on oo l op oq"># Distribute the data - Create a RDD <br/>lines = sc.textFile("/FileStore/tables/shakespeare.txt")</span><span id="bb2e" class="nk lz je oj b gy ow oo l op oq"># Create a list with all words, Create tuple (word,1), reduce by key i.e. the word<br/>counts = (lines.flatMap(lambda x: x.split(' '))          <br/>                  .map(lambda x: (x, 1))                 <br/>                  .reduceByKey(lambda x,y : x + y))</span><span id="8858" class="nk lz je oj b gy ow oo l op oq"># get the output on local<br/>output = counts.take(10)                                 <br/># print output<br/>for (word, count) in output:                             <br/>    print("%s: %i" % (word, count))</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/f5f19b3c4de83e268ac4abbc6c5627b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q4rljxTeBMZJzVsYBcL2CA.png"/></div></div></figure><p id="e737" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这是一个小例子，它计算文档中的字数，并打印出 10 个。</p><p id="0f77" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">大部分工作在第二个命令中完成。</p><p id="5e46" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果你还不能理解，请不要担心，因为我仍然需要告诉你让 Spark 工作的事情。</p><p id="8c7a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">但是在我们进入 Spark 基础知识之前，让我们刷新一些 Python 基础知识。如果你使用过 Python 的<a class="ae lq" href="https://amzn.to/2SuAtzL" rel="noopener ugc nofollow" target="_blank">函数式编程，理解 Spark 会变得容易得多。</a></p><p id="9ba8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">对于那些没有使用过它的人，下面是一个简短的介绍。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="29f2" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">Python 编程的函数式方法</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl ob"><img src="../Images/44a424b702166e6cf2bd13bdf6806284.png" data-original-src="https://miro.medium.com/v2/format:webp/1*nCX6bsSNUF_v2hFKgnaQIA.png"/></div></figure><h2 id="d03f" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">1.地图</h2><p id="7dbe" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated"><code class="fe os ot ou oj b">map</code>用于将一个函数映射到一个数组或一个列表。假设您想对列表中的每个元素应用一些函数。</p><p id="766f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你可以通过简单地使用 for 循环来实现这一点，但是 python lambda 函数允许你在 python 中用一行代码来实现这一点。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="1f02" class="nk lz je oj b gy on oo l op oq">my_list = [1,2,3,4,5,6,7,8,9,10]<br/># Lets say I want to square each term in my_list.<br/>squared_list = map(lambda x:x**2,my_list)<br/>print(list(squared_list))<br/>------------------------------------------------------------<br/>[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]</span></pre><p id="4911" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在上面的例子中，您可以将<code class="fe os ot ou oj b">map</code>看作一个带有两个参数的函数——一个函数和一个列表。</p><p id="4904" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然后，它将该函数应用于列表中的每个元素。</p><p id="ba5e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">lambda 允许你做的是写一个内联函数。在这里，<code class="fe os ot ou oj b"><strong class="kv jf">lambda x:x**2</strong></code>部分定义了一个以 x 为输入并返回 x 的函数。</p><p id="ceef" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你也可以提供一个合适的函数来代替 lambda。例如:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="e164" class="nk lz je oj b gy on oo l op oq">def squared(x):<br/>    return x**2</span><span id="8425" class="nk lz je oj b gy ow oo l op oq">my_list = [1,2,3,4,5,6,7,8,9,10]<br/># Lets say I want to square each term in my_list.<br/>squared_list = map(squared,my_list)<br/>print(list(squared_list))<br/>------------------------------------------------------------<br/>[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]</span></pre><p id="5c6f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">同样的结果，但是 lambda 表达式使得代码更紧凑，可读性更好。</p><h2 id="481f" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">2.过滤器</h2><p id="6acd" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">另一个广泛使用的功能是<code class="fe os ot ou oj b">filter</code>功能。这个函数有两个参数——一个条件和要过滤的列表。</p><p id="76a7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果你想使用某种条件过滤你的列表，你可以使用<code class="fe os ot ou oj b">filter</code>。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="6e24" class="nk lz je oj b gy on oo l op oq">my_list = [1,2,3,4,5,6,7,8,9,10]<br/># Lets say I want only the even numbers in my list.<br/>filtered_list = filter(lambda x:x%2==0,my_list)<br/>print(list(filtered_list))<br/>---------------------------------------------------------------<br/>[2, 4, 6, 8, 10]</span></pre><h2 id="5192" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">3.减少</h2><p id="1490" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">我要讲的下一个函数是 reduce 函数。这个功能将是 Spark 中的主力。</p><p id="c33e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这个函数有两个参数——一个 reduce 函数有两个参数，还有一个要应用 reduce 函数的列表。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="4021" class="nk lz je oj b gy on oo l op oq">import functools<br/>my_list = [1,2,3,4,5]<br/># Lets say I want to sum all elements in my list.<br/>sum_list = functools.reduce(lambda x,y:x+y,my_list)<br/>print(sum_list)</span></pre><p id="5fcc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在 python2 中 reduce 曾经是 python 的一部分，现在我们不得不用<code class="fe os ot ou oj b">reduce</code>作为<code class="fe os ot ou oj b">functools</code>的一部分。</p><p id="daea" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这里，lambda 函数接受两个值 x，y，并返回它们的和。直观上，您可以认为 reduce 函数的工作方式如下:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="4e9e" class="nk lz je oj b gy on oo l op oq">Reduce function first sends 1,2    ; the lambda function returns 3<br/>Reduce function then sends 3,3     ; the lambda function returns 6<br/>Reduce function then sends 6,4     ; the lambda function returns 10<br/>Reduce function finally sends 10,5 ; the lambda function returns 15</span></pre><p id="be80" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们在 reduce 中使用的 lambda 函数的一个条件是它必须是:</p><ul class=""><li id="570f" class="mv mw je kv b kw kx kz la lc mx lg my lk mz lo na nb nc nd bi translated">交换的，即 a + b = b + a 和</li><li id="66c2" class="mv mw je kv b kw ne kz nf lc ng lg nh lk ni lo na nb nc nd bi translated">结合律即(a + b) + c == a + (b + c)。</li></ul><p id="686b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在上面的例子中，我们使用了 sum，它既可交换又可结合。我们可以使用的其他函数:<code class="fe os ot ou oj b">max</code><strong class="kv jf"/><code class="fe os ot ou oj b">min</code><code class="fe os ot ou oj b">*</code>等。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="2183" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">再次走向火花</h1><p id="b8e7" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">现在我们已经了解了 Python 函数式编程的基础，让我们再一次回到 Spark。</p><p id="d8b8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">但首先，让我们深入了解一下 spark 的工作原理。火花实际上由两种东西组成，一个是司机，一个是工人。</p><p id="aa1a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">工人通常做所有的工作，司机让他们做这些工作。</p><h2 id="cdee" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">RDD</h2><p id="2c1a" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">RDD(弹性分布式数据集)是一种并行化的数据结构，分布在工作节点上。它们是 Spark 编程的基本单元。</p><p id="f930" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我们的字数统计示例中，第一行</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="ffcd" class="nk lz je oj b gy on oo l op oq">lines = sc.textFile("/FileStore/tables/shakespeare.txt")</span></pre><p id="bbc5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们取了一个文本文件，并把它分布在工作节点上，这样他们就可以并行地处理它。我们也可以使用函数<code class="fe os ot ou oj b">sc.parallelize</code>将列表并行化</p><p id="a127" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">例如:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="67f6" class="nk lz je oj b gy on oo l op oq">data = [1,2,3,4,5,6,7,8,9,10]<br/>new_rdd = sc.parallelize(data,4)<br/>new_rdd<br/>---------------------------------------------------------------<br/>ParallelCollectionRDD[22] at parallelize at PythonRDD.scala:267</span></pre><p id="fc14" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在 Spark 中，我们可以对 RDD 进行两种不同类型的操作:转换和操作。</p><ol class=""><li id="81e6" class="mv mw je kv b kw kx kz la lc mx lg my lk mz lo oy nb nc nd bi translated"><strong class="kv jf">转换:</strong>从现有的 rdd 创建新的数据集</li><li id="f5bc" class="mv mw je kv b kw ne kz nf lc ng lg nh lk ni lo oy nb nc nd bi translated"><strong class="kv jf">动作:</strong>从 Spark 获得结果的机制</li></ol></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="14c4" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">转型基础</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/3bec67d65f565a5d1d83f2397566eef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LP9yglc4UeUxDFBoTlfS9w.png"/></div></div></figure><p id="64d4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们假设你已经得到了 RDD 形式的数据。</p><p id="16ed" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">要重新报价，您的数据现在可供工作机访问。您现在想对数据进行一些转换。</p><p id="4598" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你可能想要过滤，应用一些功能，等等。</p><p id="2a95" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在 Spark 中，这是使用转换函数来完成的。</p><p id="9bf4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Spark 提供了许多转换功能。这里 可以看到<a class="ae lq" href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">的综合列表。我经常使用的一些主要工具有:</strong></a></p><h2 id="8a2a" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">1.地图:</h2><p id="fdcf" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">将给定函数应用于 RDD。</p><p id="b6f5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">请注意，语法与 Python 略有不同，但它必须做同样的事情。现在还不要担心<code class="fe os ot ou oj b">collect</code>。现在，就把它想象成一个将 squared_rdd 中的数据收集回一个列表的函数。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="a574" class="nk lz je oj b gy on oo l op oq">data = [1,2,3,4,5,6,7,8,9,10]<br/>rdd = sc.parallelize(data,4)<br/>squared_rdd = rdd.map(lambda x:x**2)<br/>squared_rdd.collect()<br/>------------------------------------------------------<br/>[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]</span></pre><h2 id="46f2" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">2.过滤器:</h2><p id="dc91" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">这里也不奇怪。接受一个条件作为输入，只保留那些满足该条件的元素。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="b1c9" class="nk lz je oj b gy on oo l op oq">data = [1,2,3,4,5,6,7,8,9,10]<br/>rdd = sc.parallelize(data,4)<br/>filtered_rdd = rdd.filter(lambda x:x%2==0)<br/>filtered_rdd.collect()<br/>------------------------------------------------------<br/>[2, 4, 6, 8, 10]</span></pre><h2 id="3069" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">3.独特:</h2><p id="7111" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">仅返回 RDD 中的不同元素。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="476b" class="nk lz je oj b gy on oo l op oq">data = [1,2,2,2,2,3,3,3,3,4,5,6,7,7,7,8,8,8,9,10]<br/>rdd = sc.parallelize(data,4)<br/>distinct_rdd = rdd.distinct()<br/>distinct_rdd.collect()<br/>------------------------------------------------------<br/>[8, 4, 1, 5, 9, 2, 10, 6, 3, 7]</span></pre><h2 id="275c" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">4.平面图:</h2><p id="52bd" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">类似于<code class="fe os ot ou oj b">map</code>，但是每个输入项可以映射到 0 个或多个输出项。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="1ad2" class="nk lz je oj b gy on oo l op oq">data = [1,2,3,4]<br/>rdd = sc.parallelize(data,4)<br/>flat_rdd = rdd.flatMap(lambda x:[x,x**3])<br/>flat_rdd.collect()<br/>------------------------------------------------------<br/>[1, 1, 2, 8, 3, 27, 4, 64]</span></pre><h2 id="705e" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">5.按键减少:</h2><p id="e1d0" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">与 Hadoop MapReduce 中的 reduce 并行。</p><p id="5872" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，如果 Spark 只处理列表，它就不能提供值。</p><p id="86b1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在 Spark 中，有一个对 rdd 的概念，这使得它更加灵活。假设我们有一个数据，其中有一个产品、它的类别和它的售价。我们仍然可以并行处理数据。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="8b1b" class="nk lz je oj b gy on oo l op oq">data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),('Carrot','Vegetable',34)]<br/>rdd = sc.parallelize(data,4)</span></pre><p id="16f3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们的 RDD<code class="fe os ot ou oj b">rdd</code>拥有元组。</p><p id="1152" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们想找出我们从每个类别中获得的总收入。</p><p id="ec50" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为此，我们必须将我们的<code class="fe os ot ou oj b">rdd</code>转换成一个对 rdd，这样它就只包含键值对/元组。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="4112" class="nk lz je oj b gy on oo l op oq">category_price_rdd = rdd.map(lambda x: (x[1],x[2]))<br/>category_price_rdd.collect()<br/>-----------------------------------------------------------------<br/>[(‘Fruit’, 200), (‘Fruit’, 24), (‘Fruit’, 56), (‘Vegetable’, 103), (‘Vegetable’, 34)]</span></pre><p id="d2cd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这里我们使用了 map 函数来得到我们想要的格式。当使用文本文件时，形成的 RDD 有很多字符串。我们用<code class="fe os ot ou oj b">map</code>把它转换成我们想要的格式。</p><p id="c1c3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">所以现在我们的<code class="fe os ot ou oj b">category_price_rdd</code>包含产品类别和产品销售价格。</p><p id="51c3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，我们希望减少关键类别并对价格求和。我们可以通过以下方式做到这一点:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="df7b" class="nk lz je oj b gy on oo l op oq">category_total_price_rdd = category_price_rdd.reduceByKey(lambda x,y:x+y)<br/>category_total_price_rdd.collect()<br/>---------------------------------------------------------<!-- -->[(‘Vegetable’, 137), (‘Fruit’, 280)]</span></pre><h2 id="8df2" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">6.按关键字分组:</h2><p id="a888" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">类似于<code class="fe os ot ou oj b">reduceByKey</code>,但是没有减少，只是把所有的元素放在一个迭代器中。例如，如果我们希望将所有产品的类别和值作为关键字，我们将使用该函数。</p><p id="9c3a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们再次使用<code class="fe os ot ou oj b">map</code>来获得所需形式的数据。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="df51" class="nk lz je oj b gy on oo l op oq">data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),('Carrot','Vegetable',34)]<br/>rdd = sc.parallelize(data,4)<br/>category_product_rdd = rdd.map(lambda x: (x[1],x[0]))<br/>category_product_rdd.collect()<br/>------------------------------------------------------------<br/>[('Fruit', 'Apple'),  ('Fruit', 'Banana'),  ('Fruit', 'Tomato'),  ('Vegetable', 'Potato'),  ('Vegetable', 'Carrot')]</span></pre><p id="3b23" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然后我们使用<code class="fe os ot ou oj b">groupByKey</code>作为:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="b07e" class="nk lz je oj b gy on oo l op oq">grouped_products_by_category_rdd = category_product_rdd.groupByKey()<br/>findata = grouped_products_by_category_rdd.collect()<br/>for data in findata:<br/>    print(data[0],list(data[1]))<br/>------------------------------------------------------------<br/>Vegetable ['Potato', 'Carrot'] <br/>Fruit ['Apple', 'Banana', 'Tomato']</span></pre><p id="f552" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这里,<code class="fe os ot ou oj b">groupByKey</code>函数起作用了，它返回类别和该类别中的产品列表。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="7d88" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">动作基础</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/4ceee55dea1dafd53270c22f4ba45940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-T8LTnsXH2AhzhbmajacXw.png"/></div></div></figure><p id="5bd2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你已经过滤了你的数据，映射了一些函数。完成你的计算。</p><p id="206f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，您希望在本地机器上获取数据，或者将数据保存到文件中，或者在 excel 或任何可视化工具中以一些图表的形式显示结果。</p><p id="73f6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你需要为此采取行动。此处  <strong class="kv jf">提供了一个全面的行动列表<a class="ae lq" href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">。</strong></a></strong></p><p id="f853" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我倾向于使用的一些最常见的操作是:</p><h2 id="b8c4" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">1.收集:</h2><p id="56d6" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">这个动作我们已经用过很多次了。它获取整个 RDD，并将其返回到驱动程序。</p><h2 id="07ed" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">2.减少:</h2><p id="b40d" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">使用 func 函数(接受两个参数并返回一个)聚合数据集的元素。该函数应该是可交换的和可结合的，这样它就可以被正确地并行计算。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="9df1" class="nk lz je oj b gy on oo l op oq">rdd = sc.parallelize([1,2,3,4,5])<br/>rdd.reduce(lambda x,y : x+y)<br/>---------------------------------<br/>15</span></pre><h2 id="8a29" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">3.拿走:</h2><p id="e520" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">有时你需要查看你的 RDD 包含了什么，而不是获取内存中的所有元素。<code class="fe os ot ou oj b">take</code>返回 RDD 的前 n 个元素的列表。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="ef61" class="nk lz je oj b gy on oo l op oq">rdd = sc.parallelize([1,2,3,4,5])<br/>rdd.take(3)<br/>---------------------------------<br/>[1, 2, 3]</span></pre><h2 id="7ba9" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">4.外卖:</h2><p id="6bd8" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated"><code class="fe os ot ou oj b">takeOrdered</code>使用自然顺序或自定义比较器返回 RDD 的前 n 个元素。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="5079" class="nk lz je oj b gy on oo l op oq">rdd = sc.parallelize([5,3,12,23])</span><span id="9aa0" class="nk lz je oj b gy ow oo l op oq"># descending order<br/>rdd.takeOrdered(3,lambda s:-1*s)<br/>----<br/>[23, 12, 5]</span><span id="04f0" class="nk lz je oj b gy ow oo l op oq">rdd = sc.parallelize([(5,23),(3,34),(12,344),(23,29)])</span><span id="a6d4" class="nk lz je oj b gy ow oo l op oq"># descending order<br/>rdd.takeOrdered(3,lambda s:-1*s[1])<br/>---<br/>[(12, 344), (3, 34), (23, 29)]</span></pre></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="9f6f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们终于学到了基础知识。让我们回到字数统计的例子</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="1dd2" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">理解字数示例</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/78e0ed67097f3b6aa5f060aa05544072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wcxaKwNMIiEsGmW2"/></div></div></figure><p id="0a46" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们有点理解 Spark 提供给我们的转换和动作。</p><p id="07b2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在理解 wordcount 程序应该不难。让我们一行一行地检查程序。</p><p id="4efa" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">第一行创建了一个 RDD，并将其分发给工人。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="ce40" class="nk lz je oj b gy on oo l op oq">lines = sc.textFile("<!-- -->/FileStore/tables/shakespeare.txt<!-- -->")</span></pre><p id="3b3a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这个 RDD <code class="fe os ot ou oj b">lines</code>包含文件中的句子列表。您可以使用<code class="fe os ot ou oj b">take</code>查看 rdd 内容</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="05a6" class="nk lz je oj b gy on oo l op oq">lines.take(5)<br/>--------------------------------------------<br/>['The Project Gutenberg EBook of The Complete Works of William Shakespeare, by ',  'William Shakespeare',  '',  'This eBook is for the use of anyone anywhere at no cost and with',  'almost no restrictions whatsoever.  You may copy it, give it away or']</span></pre><p id="3004" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这个 RDD 的形式是:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="cae0" class="nk lz je oj b gy on oo l op oq">['word1 word2 word3','word4 word3 word2']</span></pre><p id="b704" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下一行实际上是整个脚本中最重要的函数。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="4125" class="nk lz je oj b gy on oo l op oq">counts = (lines.flatMap(lambda x: x.split(' '))          <br/>                  .map(lambda x: (x, 1))                 <br/>                  .reduceByKey(lambda x,y : x + y))</span></pre><p id="5e04" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">它包含了我们对 RDD 线所做的一系列变换。首先，我们做一个<code class="fe os ot ou oj b">flatmap</code>转换。</p><p id="2208" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><code class="fe os ot ou oj b">flatmap</code>转换将行作为输入，将单词作为输出。所以在<code class="fe os ot ou oj b">flatmap</code>变换之后，RDD 的形式是:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="7993" class="nk lz je oj b gy on oo l op oq">['word1','word2','word3','word4','word3','word2']</span></pre><p id="485d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">接下来，我们对<code class="fe os ot ou oj b">flatmap</code>输出进行<code class="fe os ot ou oj b">map</code>转换，将 RDD 转换为:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="4226" class="nk lz je oj b gy on oo l op oq">[('word1',1),('word2',1),('word3',1),('word4',1),('word3',1),('word2',1)]</span></pre><p id="86e4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">最后，我们做一个<code class="fe os ot ou oj b">reduceByKey</code>转换，计算每个单词出现的次数。</p><p id="22a8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">之后 RDD 接近最终所需的形状。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="1121" class="nk lz je oj b gy on oo l op oq">[('word1',1),('word2',2),('word3',2),('word4',1)]</span></pre><p id="1669" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下一行是一个动作，它在本地获取生成的 RDD 的前 10 个元素。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="4ad9" class="nk lz je oj b gy on oo l op oq">output = counts.take(10)</span></pre><p id="e1bc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这一行只是打印输出</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="e416" class="nk lz je oj b gy on oo l op oq">for (word, count) in output:                 <br/>    print("%s: %i" % (word, count))</span></pre><p id="0ab0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这就是单词计数程序。希望你现在明白了。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="07b3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">到目前为止，我们讨论了 Wordcount 示例以及可以在 Spark 中使用的基本转换和操作。但我们在现实生活中不做字数统计。</p><p id="ec30" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们必须解决更大、更复杂的问题。不要担心！无论我们现在学到了什么，都将让我们做得更好。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="22d8" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">用实例点燃行动的火花</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/4b189ca49fe237b3630d020c5234be7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*e94sY_GitJyJz02J"/></div></div></figure><p id="241e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们用一个具体的例子来处理一些常见的转换。</p><p id="7853" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们将在 Movielens <a class="ae lq" href="https://github.com/MLWhiz/data_science_blogs/tree/master/spark_post" rel="noopener ugc nofollow" target="_blank"> ml-100k.zip </a>数据集上工作，这是一个稳定的基准数据集。1000 个用户对 1700 部电影的 100，000 次评分。1998 年 4 月发布。</p><p id="53ca" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Movielens 数据集包含许多文件，但我们将只处理 3 个文件:</p><p id="63ba" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">1) <strong class="kv jf">用户</strong>:该文件名保存为“u.user”，该文件中的列有:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="c5dc" class="nk lz je oj b gy on oo l op oq">['user_id', 'age', 'sex', 'occupation', 'zip_code']</span></pre><p id="587c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">2) <strong class="kv jf">评级</strong>:该文件名保存为“u.data”，该文件中的列有:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="1de0" class="nk lz je oj b gy on oo l op oq">['user_id', 'movie_id', 'rating', 'unix_timestamp']</span></pre><p id="e326" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">3) <strong class="kv jf">电影</strong>:该文件名保存为“u.item”，该文件中的栏目有:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="e05b" class="nk lz je oj b gy on oo l op oq">['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url', and 18 more columns.....]</span></pre><p id="7123" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们从使用 home 选项卡上的 Import and Explore Data 将这 3 个文件导入 spark 实例开始。</p><div class="mr ms mt mu gt ab cb"><figure class="pc iv pd pe pf pg ph paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/61bf07e7e3f4427682c15e05b8994cc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*dbAOlFRRRRJPByxq-bClzw.png"/></div></figure><figure class="pc iv pi pe pf pg ph paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/42f3a2c7d76dbc08144ad73612f33eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*nln4rJ01sulsOk1ZhbThgA.png"/></div></figure><figure class="pc iv pj pe pf pg ph paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/d38e47f5be4f47fef3fc146ce9cdc320.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*SgPCahKbLaAobCm--WJEAg.png"/></div></figure></div><p id="46bd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们的业务合作伙伴现在找到我们，要求我们从这些数据中找出<strong class="kv jf"> <em class="lp"> 25 个收视率最高的电影名称</em> </strong>。一部电影被评了多少次？</p><p id="a86a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们将数据加载到不同的 rdd 中，看看数据包含什么。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="98a4" class="nk lz je oj b gy on oo l op oq">userRDD = sc.textFile("/FileStore/tables/u.user") <br/>ratingRDD = sc.textFile("/FileStore/tables/u.data") <br/>movieRDD = sc.textFile("/FileStore/tables/u.item") <br/>print("userRDD:",userRDD.take(1))<br/>print("ratingRDD:",ratingRDD.take(1))<br/>print("movieRDD:",movieRDD.take(1))<br/>-----------------------------------------------------------<br/>userRDD: ['1|24|M|technician|85711'] <br/>ratingRDD: ['196\t242\t3\t881250949'] <br/>movieRDD: ['1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0']</span></pre><p id="2bc9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们注意到，要回答这个问题，我们需要使用<code class="fe os ot ou oj b">ratingRDD</code>。但是<code class="fe os ot ou oj b">ratingRDD</code>没有电影名。</p><p id="1604" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">所以我们必须用<code class="fe os ot ou oj b">movie_id</code>合并<code class="fe os ot ou oj b">movieRDD</code>和<code class="fe os ot ou oj b">ratingRDD</code>。</p><p id="c0e5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">我们如何在 Spark 中做到这一点？</strong></p><p id="6040" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下面是代码。我们还使用了新的转换<code class="fe os ot ou oj b">leftOuterJoin</code>。请务必阅读下面代码中的文档和注释。</p><figure class="mr ms mt mu gt iv"><div class="bz fp l di"><div class="pk pl l"/></div></figure><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="3438" class="nk lz je oj b gy on oo l op oq">OUTPUT:<br/>--------------------------------------------------------------------RDD_movid_rating: [('242', '3'), ('302', '3'), ('377', '1'), ('51', '2')] <br/>RDD_movid_title: [('1', 'Toy Story (1995)'), ('2', 'GoldenEye (1995)')] <br/>rdd_movid_title_rating: [('1440', ('3', 'Above the Rim (1994)'))] rdd_title_rating: [('Above the Rim (1994)', 1), ('Above the Rim (1994)', 1)] <br/>rdd_title_ratingcnt: [('Mallrats (1995)', 54), ('Michael Collins (1996)', 92)] </span><span id="1af3" class="nk lz je oj b gy ow oo l op oq">##################################### <br/>25 most rated movies: [('Star Wars (1977)', 583), ('Contact (1997)', 509), ('Fargo (1996)', 508), ('Return of the Jedi (1983)', 507), ('Liar Liar (1997)', 485), ('English Patient, The (1996)', 481), ('Scream (1996)', 478), ('Toy Story (1995)', 452), ('Air Force One (1997)', 431), ('Independence Day (ID4) (1996)', 429), ('Raiders of the Lost Ark (1981)', 420), ('Godfather, The (1972)', 413), ('Pulp Fiction (1994)', 394), ('Twelve Monkeys (1995)', 392), ('Silence of the Lambs, The (1991)', 390), ('Jerry Maguire (1996)', 384), ('Chasing Amy (1997)', 379), ('Rock, The (1996)', 378), ('Empire Strikes Back, The (1980)', 367), ('Star Trek: First Contact (1996)', 365), ('Back to the Future (1985)', 350), ('Titanic (1997)', 350), ('Mission: Impossible (1996)', 344), ('Fugitive, The (1993)', 336), ('Indiana Jones and the Last Crusade (1989)', 331)] #####################################</span></pre><p id="611e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">《星球大战》是 Movielens 数据集中评分最高的电影。</p><p id="6cd2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们可以使用下面的命令在一个命令中完成所有这些，但是代码现在有点乱。</p><p id="5a02" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我这样做是为了说明可以在 Spark 中使用链接函数，并且可以绕过变量创建过程。</p><figure class="mr ms mt mu gt iv"><div class="bz fp l di"><div class="pk pl l"/></div></figure><p id="d8c7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们再做一次。为了练习:</p><p id="2514" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，我们希望使用相同的数据集找到评分最高的 25 部电影。我们实际上只想要那些已经被评级至少 100 次的电影。</p><figure class="mr ms mt mu gt iv"><div class="bz fp l di"><div class="pk pl l"/></div></figure><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="0f67" class="nk lz je oj b gy on oo l op oq">OUTPUT:<br/>------------------------------------------------------------<br/>rdd_title_ratingsum: [('Mallrats (1995)', 186), ('Michael Collins (1996)', 318)] <br/>rdd_title_ratingmean_rating_count: [('Mallrats (1995)', (3.4444444444444446, 54))] <br/>rdd_title_rating_rating_count_gt_100: [('Butch Cassidy and the Sundance Kid (1969)', (3.949074074074074, 216))]</span><span id="86b4" class="nk lz je oj b gy ow oo l op oq">##################################### <br/>25 highly rated movies: [('Close Shave, A (1995)', (4.491071428571429, 112)), ("Schindler's List (1993)", (4.466442953020135, 298)), ('Wrong Trousers, The (1993)', (4.466101694915254, 118)), ('Casablanca (1942)', (4.45679012345679, 243)), ('Shawshank Redemption, The (1994)', (4.445229681978798, 283)), ('Rear Window (1954)', (4.3875598086124405, 209)), ('Usual Suspects, The (1995)', (4.385767790262173, 267)), ('Star Wars (1977)', (4.3584905660377355, 583)), ('12 Angry Men (1957)', (4.344, 125)), ('Citizen Kane (1941)', (4.292929292929293, 198)), ('To Kill a Mockingbird (1962)', (4.292237442922374, 219)), ("One Flew Over the Cuckoo's Nest (1975)", (4.291666666666667, 264)), ('Silence of the Lambs, The (1991)', (4.28974358974359, 390)), ('North by Northwest (1959)', (4.284916201117318, 179)), ('Godfather, The (1972)', (4.283292978208232, 413)), ('Secrets &amp; Lies (1996)', (4.265432098765432, 162)), ('Good Will Hunting (1997)', (4.262626262626263, 198)), ('Manchurian Candidate, The (1962)', (4.259541984732825, 131)), ('Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', (4.252577319587629, 194)), ('Raiders of the Lost Ark (1981)', (4.252380952380952, 420)), ('Vertigo (1958)', (4.251396648044692, 179)), ('Titanic (1997)', (4.2457142857142856, 350)), ('Lawrence of Arabia (1962)', (4.23121387283237, 173)), ('Maltese Falcon, The (1941)', (4.2101449275362315, 138)), ('Empire Strikes Back, The (1980)', (4.204359673024523, 367))] <br/>#####################################</span></pre></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="b4f5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">到目前为止，我们一直在谈论 rdd，因为它们非常强大。</p><p id="1f89" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">您也可以使用 rdd 来处理非关系数据库。</p><p id="0196" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">他们让你做很多用 SparkSQL 做不到的事情？</p><p id="df33" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf"> <em class="lp">是的，你也可以在 Spark 中使用 SQL，这就是我现在要说的。</em>T15】</strong></p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="d285" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">火花数据帧</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/3994fb785ce11f4f9f8665f695f65ee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/0*_Xne4_sz6lroaINt.png"/></div></figure><p id="a221" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Spark 为美国数据科学家提供了 DataFrame API 来处理关系数据。这是为喜欢冒险的人准备的文档。</p><p id="4700" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">请记住，在背景中，它仍然是所有的 rdd，这就是为什么这篇文章的开始部分侧重于 rdd。</p><p id="5f1c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我将从一些你使用 Spark 数据框需要的常用功能开始。会看起来很像熊猫，只是有一些语法上的变化。</p><h2 id="ad8b" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">1.读取文件</h2><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="769c" class="nk lz je oj b gy on oo l op oq">ratings = spark.read.load("/FileStore/tables/u.data",format="csv", sep="\t", inferSchema="true", header="false")</span></pre><h2 id="36fe" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">2.显示文件</h2><p id="5901" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">我们有两种方法使用 Spark 数据帧显示文件。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="33f2" class="nk lz je oj b gy on oo l op oq">ratings.show()</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pn"><img src="../Images/2d39989cb2a212600f01a8e5c1894acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rTy2s0F8DKVDfR4a5aw39Q.png"/></div></div></figure><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="fd76" class="nk lz je oj b gy on oo l op oq">display(ratings)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/2908abc52d57893c2ba79730836c450a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tdmL9lWCrvYYVDtqjLHa2g.png"/></div></div></figure><p id="18c4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我更喜欢<code class="fe os ot ou oj b">display</code>，因为它看起来更漂亮、更干净。</p><h2 id="b62b" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">3.更改列名</h2><p id="6df7" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">功能性好。总是需要。别忘了单子前面的<code class="fe os ot ou oj b">*</code>。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="f3b9" class="nk lz je oj b gy on oo l op oq">ratings = ratings.toDF(*['user_id', 'movie_id', 'rating', 'unix_timestamp'])</span><span id="687e" class="nk lz je oj b gy ow oo l op oq">display(ratings)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pp"><img src="../Images/b89db9f4526eceeba9a65969325ea622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DjRGPII7iHtjuBotlxfRgw.png"/></div></div></figure><h2 id="fcf2" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">4.一些基本数据</h2><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="21c3" class="nk lz je oj b gy on oo l op oq">print(ratings.count()) #Row Count<br/>print(len(ratings.columns)) #Column Count<br/>---------------------------------------------------------<br/>100000<br/>4</span></pre><p id="f807" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们还可以使用以下方式查看数据帧统计数据:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="4fc4" class="nk lz je oj b gy on oo l op oq">display(ratings.describe())</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pn"><img src="../Images/8cbb998cedf2bfc55dd30654eb7a61fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2ucRU02MofHF8G5_qdxTw.png"/></div></div></figure><h2 id="1cb3" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">5.选择几列</h2><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="5a71" class="nk lz je oj b gy on oo l op oq">display(ratings.select('user_id','movie_id'))</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/f9efa5de27da33f86c8a9ef54ec38482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h74EduHHrSsG5wgOMPo7NA.png"/></div></div></figure><h2 id="62e3" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">6.过滤器</h2><p id="1f2c" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">使用多个条件过滤数据帧:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="aa7c" class="nk lz je oj b gy on oo l op oq">display(ratings.filter((ratings.rating==5) &amp; (ratings.user_id==253)))</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/4dadd3df5d344c795ea732e6af2073b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpFeIR-W-vm2vwFZWDW1qQ.png"/></div></div></figure><h2 id="ae11" class="nk lz je bd ma nl nm dn me nn no dp mi lc np nq mk lg nr ns mm lk nt nu mo nv bi translated">7.分组依据</h2><p id="316c" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">我们也可以对 spark 数据帧使用 groupby 函数。除了你需要导入<code class="fe os ot ou oj b">pyspark.sql.functions</code>之外，和熊猫组基本相同</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="6c9a" class="nk lz je oj b gy on oo l op oq"><strong class="oj jf">from</strong> pyspark.sql <strong class="oj jf">import</strong> functions <strong class="oj jf">as</strong> F<br/>display(ratings.groupBy("user_id").agg(F.count("user_id"),F.mean("rating")))</span></pre><p id="58cf" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这里，我们发现了每个 user_id 的评分计数和平均评分</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/15cf2263d4c7c5cda4c97af123b33bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6PGa9_7ISy0CPt5hp1G-Eg.png"/></div></div></figure><h1 id="02c1" class="ly lz je bd ma mb pr md me mf ps mh mi kk pt kl mk kn pu ko mm kq pv kr mo mp bi translated">8.分类</h1><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="2ab1" class="nk lz je oj b gy on oo l op oq">display(ratings.sort("user_id"))</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/6db9a6d6d3cabee47e734e560784dc6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yMESTh-H1vGXlCIobeDYg.png"/></div></div></figure><p id="3ca0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们也可以使用下面的<code class="fe os ot ou oj b">F.desc</code>函数进行降序排序。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="d511" class="nk lz je oj b gy on oo l op oq"># descending Sort<br/><strong class="oj jf">from</strong> pyspark.sql <strong class="oj jf">import</strong> functions <strong class="oj jf">as</strong> F<br/>display(ratings.sort(F.desc("user_id")))</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pw"><img src="../Images/b73a1fadb429ca0e92ddc2eed9e5e88f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1tFAPP9sHFsybQLZDdwjAg.png"/></div></div></figure></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="92b7" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">与 Spark 数据帧连接/合并</h1><p id="a8ce" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">我找不到与 Spark 数据帧合并功能相当的 pandas，但是我们可以将 SQL 用于数据帧，因此我们可以使用 SQL 合并数据帧。</p><p id="a18c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们试着对评级运行一些 SQL。</p><p id="0a2e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们首先将评级 df 注册到一个临时表 ratings_table 中，我们可以在这个表中运行 sql 操作。</p><p id="9cd8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如您所见，SQL select 语句的结果又是一个 Spark 数据帧。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="fc58" class="nk lz je oj b gy on oo l op oq">ratings.registerTempTable('ratings_table')<br/>newDF = sqlContext.sql('select * from ratings_table where rating&gt;4')<br/>display(newDF)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi px"><img src="../Images/9ca400b3440db369d21c4a356a1ab349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FGsdH8GOwMCE7RF7II_x4g.png"/></div></div></figure><p id="a4f1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，让我们再添加一个 Spark 数据帧，看看是否可以通过 SQL 查询使用 join:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="c7bc" class="nk lz je oj b gy on oo l op oq">#get one more dataframe to join<br/>movies = spark.read.load("/FileStore/tables/u.item",format="csv", sep="|", inferSchema="true", header="false")</span><span id="af7d" class="nk lz je oj b gy ow oo l op oq"># change column names<br/>movies = movies.toDF(*["movie_id","movie_title","release_date","video_release_date","IMDb_URL","unknown","Action","Adventure","Animation ","Children","Comedy","Crime","Documentary","Drama","Fantasy","Film_Noir","Horror","Musical","Mystery","Romance","Sci_Fi","Thriller","War","Western"])</span><span id="35ab" class="nk lz je oj b gy ow oo l op oq">display(movies)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi px"><img src="../Images/84af165bc7a5225688699b7d1c37d1e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZKetrC8Q9EHgJBHcVWm-Zg.png"/></div></div></figure><p id="b432" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，让我们尝试连接 movie_id 上的表，以获得 ratings 表中的电影名称。</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="d6b1" class="nk lz je oj b gy on oo l op oq">movies.registerTempTable('movies_table')</span><span id="7c55" class="nk lz je oj b gy ow oo l op oq">display(sqlContext.sql('select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id'))</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/8c7fedcbb474a0ffdba75c3a5baf8d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xbFnY8R2GMU4VizLXSoPYw.png"/></div></div></figure><p id="c074" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们试着做我们之前在 RDDs 上做的事情。寻找收视率最高的 25 部电影:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="3367" class="nk lz je oj b gy on oo l op oq">mostrateddf = sqlContext.sql('select movie_id,movie_title, count(user_id) as num_ratings from (select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id)A group by movie_id,movie_title order by num_ratings desc ')</span><span id="271f" class="nk lz je oj b gy ow oo l op oq">display(mostrateddf)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi py"><img src="../Images/71d987357b7554db31dec81f5cd7aa29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kq_p7zOvdfBlI1xnSxKQlA.png"/></div></div></figure><p id="f60e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">并找到投票数超过 100 的最高评级的前 25 部电影:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="5b66" class="nk lz je oj b gy on oo l op oq">highrateddf = sqlContext.sql('select movie_id,movie_title, avg(rating) as avg_rating,count(movie_id) as num_ratings from (select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id)A group by movie_id,movie_title having num_ratings&gt;100 order by avg_rating desc ')</span><span id="38e2" class="nk lz je oj b gy ow oo l op oq">display(highrateddf)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pz"><img src="../Images/59a93802c0c636e5a5a940026bd5d8c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VN4eADfelg6njlAWlZ5mgw.png"/></div></div></figure><p id="0dfc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我在上面的查询中使用了 GROUP BY、HAVING 和 ORDER BY 子句以及别名。这表明你可以用<code class="fe os ot ou oj b">sqlContext.sql</code>做很多复杂的事情</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="76cc" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">关于显示的一个小注意事项</h1><p id="35e5" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">您也可以使用<code class="fe os ot ou oj b">display</code>命令显示笔记本中的图表。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi py"><img src="../Images/a02c8e24d33ff58f85e3cbb5c70ab98b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4LZXgPaneDmXZcl9zy3haw.png"/></div></div></figure><p id="fe93" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">选择<strong class="kv jf"> <em class="lp">剧情选项可以看到更多选项。</em> </strong></p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qa"><img src="../Images/199be7a7463bbadb50186f9898ce0501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nKoPRzASoH0rQ8Qof4b10g.png"/></div></div></figure></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="e4e5" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">从火花数据帧转换到 RDD 数据帧，反之亦然:</h1><p id="5d3d" class="pw-post-body-paragraph kt ku je kv b kw nw kf ky kz nx ki lb lc ny le lf lg nz li lj lk oa lm ln lo im bi translated">有时，您可能希望从 spark 数据框架转换到 RDD 数据框架，反之亦然，这样您就可以同时拥有两个世界的优势。</p><p id="1c7b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">要从 DF 转换到 RDD，您只需执行以下操作:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="e6d6" class="nk lz je oj b gy on oo l op oq">highratedrdd =highrateddf.rdd<br/>highratedrdd.take(2)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/25f1c1a78cfe4e729dc1fbbd68e8277e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0dt_8cE4i-_KDrTUdM5tPg.png"/></div></div></figure><p id="f796" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">要从 RDD 转到数据帧:</p><pre class="mr ms mt mu gt oi oj ok ol aw om bi"><span id="f8bd" class="nk lz je oj b gy on oo l op oq">from pyspark.sql import Row<br/># creating a RDD first<br/>data = [('A',1),('B',2),('C',3),('D',4)]<br/>rdd = sc.parallelize(data)</span><span id="1a43" class="nk lz je oj b gy ow oo l op oq"># map the schema using Row.<br/>rdd_new = rdd.map(lambda x: Row(key=x[0], value=int(x[1])))</span><span id="0b1b" class="nk lz je oj b gy ow oo l op oq"># Convert the rdd to Dataframe<br/>rdd_as_df = sqlContext.createDataFrame(rdd_new)<br/>display(rdd_as_df)</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qb"><img src="../Images/5eeb8dffa344e3f8572c07639d609099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kIY2mHEBo0SGfxi6tP_Jlg.png"/></div></div></figure><p id="66af" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">RDD 为您提供了<strong class="kv jf"> <em class="lp">更多的控制</em> </strong>以时间和编码工作为代价。而 Dataframes 为您提供了<strong class="kv jf"> <em class="lp">熟悉的编码</em> </strong>平台。现在你可以在这两者之间来回移动。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="39b0" class="ly lz je bd ma mb mc md me mf mg mh mi kk mj kl mk kn ml ko mm kq mn kr mo mp bi translated">结论</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qc"><img src="../Images/25093232b79f26042295f8db8544c8ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TK-uI698Vdxjh5kL"/></div></div></figure><p id="824f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这是一个很大的帖子，如果你完成了，恭喜你。</p><p id="32d3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Spark 为我们提供了一个接口，我们可以在这个接口上对数据进行转换和操作。Spark 还提供了 Dataframe API 来简化数据科学家向大数据的过渡。</p><p id="c730" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">希望我已经很好地介绍了基础知识，足以激起您的兴趣，并帮助您开始使用 Spark。</p><p id="0604" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf"> <em class="lp">你可以在</em></strong><a class="ae lq" href="https://github.com/MLWhiz/data_science_blogs/tree/master/spark_post" rel="noopener ugc nofollow" target="_blank"><strong class="kv jf"><em class="lp">GitHub</em></strong></a><strong class="kv jf"><em class="lp">库中找到所有的代码。</em>T29】</strong></p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="03a0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">此外，如果你想了解更多关于 Spark 和 Spark DataFrames 的知识，我想在 Coursera 上调出这些关于<a class="ae lq" href="https://coursera.pxf.io/4exq73" rel="noopener ugc nofollow" target="_blank">大数据基础的优秀课程:HDFS、MapReduce 和 Spark RDD </a>。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="58a0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我以后也会写更多这样的帖子。让我知道你对这个系列的看法。在<a class="ae lq" href="https://medium.com/@rahul_agarwal" rel="noopener"><strong class="kv jf"/></a>关注我或者订阅我的<a class="ae lq" href="http://eepurl.com/dbQnuX" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter <a class="ae lq" href="https://twitter.com/MLWhiz" rel="noopener ugc nofollow" target="_blank"> @mlwhiz </a>联系。</p></div></div>    
</body>
</html>