# 人工智能和集体行动

> 原文：<https://towardsdatascience.com/ai-and-collective-action-ce2c15632911?source=collection_archive---------26----------------------->

![](img/357db405eccd5bd0873ab9990b66e659.png)

Photo by [@pkmfaris](https://unsplash.com/@pkmfaris)

## 通过 OpenAI 的一篇研究论文迈向更负责任的人工智能发展

7 月 10 日 OpenAI 的团队成员发布了一篇关于 arXiv 的论文，名为 [*合作在负责任的 AI 开发中的作用*](https://arxiv.org/pdf/1907.04534.pdf) *作者* [*阿曼达·阿斯克尔*](https://medium.com/u/e05c3ea65eb5?source=post_page-----ce2c15632911--------------------------------) *，迈尔斯·布伦戴奇* [*吉莉安·哈德菲尔德*](https://medium.com/u/477e725799c9?source=post_page-----ce2c15632911--------------------------------) 。文章中的一个主要陈述如下:*“人工智能公司之间的竞争可能会通过增加它们更快发展的激励来减少每个公司负责任发展的激励。因此，如果人工智能公司更喜欢开发风险水平更接近社会最优的人工智能系统——我们相信许多人都这样做——负责任的人工智能开发可以被视为一个集体行动问题”*因此，建议我们如何处理这个问题？

## 负责任的 AI 开发？

人工智能在健康和福利领域的应用越来越多。

在短期内，有害的情况已经发生，例如:

*   从大数据集学到的偏见扭曲了信贷市场的决策。
*   扭曲刑事司法系统决策的历史数据和算法。
*   面部识别技术打破了人们对隐私和自主权的既定期望。
*   一些汽车中的自动驾驶功能导致了新类型的驾驶风险(同时降低了其他风险)。

更长期、更大规模的场景包括以下危险:

*   涉及自主武器系统的军事冲突意外升级。
*   大范围的失业。
*   对政治和社会机构的威胁。

负责任的人工智能开发被描述为降低风险，在这种情况下:安全，保障，以及与人工智能系统相关的结构性风险。

## 代价高昂的错误还是代价高昂的责任？

文章中说，如果优先考虑速度而不是安全的进度，人工智能投资的方向可能会出现混乱:“如果投入的额外资源可以用于更快地开发约束更少的人工智能系统，我们应该预计负责任的人工智能开发比轻率的人工智能开发需要更多的时间和金钱。”因此，他们提出了一个关于风险视角的重要问题<>而不是经常提出的竞争力问题。也许努力工作的态度对规模有害？

> “这意味着，如果率先开发和部署特定类型的人工智能系统的价值很高，负责任的开发对公司来说成本特别高。”

然而，这里讨论的成本与可能的财务收益有关，从某种意义上说是“失去”机会。

1.  先发优势的潜在损失。
2.  性能成本
3.  收入损失

这是因为没有以安全、安保或影响评估为由构建某些*‘有利可图的人工智能系统’*。关于种族的讨论假设他们有一个明确的终点，这被描述为一个“永久的 R & D 种族”。

## 责任的好处

令人欣慰的是，OpenAI 似乎确实概述了这些责任的负面影响，以继续向好的方面发展。如果消费者对更安全的产品有偏好，并对这种偏好做出理性反应，他们就不会购买安全性不足的产品。

这是一个平衡游戏，或者说是企业对负面影响的权衡。*“安全失败对非消费者造成的伤害是负外部性，而更安全的产品对非消费者产生的利益是正外部性。”*这种困境正在开发产品的公司中出现，如果没有消费者的惩罚或关注，可能很难让公司承担责任。

然而，并不是所有的工人或业主都忽视责任，据说最近技术工人的行动(抗议等)已经证明了这一点。).

## 这可能并不容易，原因如下

负责任必须有更强的激励。根据 OpenAI，市场力量、责任法和行业或政府监管可能还不够，他们用令人信服的论据概述了原因。

> “随着尖端人工智能系统变得越来越复杂，没有参与这些系统开发的消费者将很难获得关于这些系统安全性的准确信息。”

众所周知，要解释一个特定系统的决策是非常困难的。在这方面,“等着瞧”的策略可能是有害的，尤其是当这些算法被大规模实施时，对消费者而言。我以前写过关于“跟踪并告诉”的文章，这是科技公司中另一种常见的做法——首先跟踪一个数据点，然后告诉消费者，因此这两方面似乎都没有什么责任。

> 如果监管机构无法评估给定人工智能系统的风险，他们在使用监管控制时可能会过于严格或过于自由

伤害更有可能伤害:“…那些被指控犯罪的人，而不是那些购买工具的人”OpenAI 的研究人员列出了我想传达的关于伤害的几点:

*   减少对网上资源的信任。
*   对公司或保险公司来说太大的伤害以至于不能覆盖所有的损失。
*   人工智能系统可能会给未来几代人带来负面的外部性，而这些人无法惩罚企业或阻止它们发生

我对最后一点特别感兴趣，因为在开发人工智能解决方案/应用程序或研究人工智能领域的社区中，缺乏对气候变化或气候危机的讨论。作为一个例子，气候变化在这篇论文中一次也没有提到，这让我感到惊讶——尽管这篇论文写得多么透彻，但这应该是完全没有提到的一个明显的点。在为 OpenAI 辩护时,“可持续发展”一词被提到过一次，并且鼓励在人工智能方面进行合作以实现良好的倡议。

他们会问这样的问题:*“…为什么在缩短上市时间很有价值的其他行业(如制药业)中，竞相降低产品安全性的做法并不普遍”*然而，这种说法是完全错误的。喜剧演员约翰·奥利弗举的一个明显的例子是[阿片类](https://www.youtube.com/watch?v=-qCKR6wy94U&t=2s)。在这种情况下，某些首席执行官还吹嘘他们能多快获得药物批准。2018 年[FDA 批准的数量创下纪录](https://www.raps.org/news-and-articles/news-articles/2019/1/fdas-record-year-a-look-at-2018-new-drug-approva)，麦肯锡提到关于率先上市的讨论[已经进行了很长时间](https://www.mckinsey.com/industries/pharmaceuticals-and-medical-products/our-insights/pharmas-first-to-market-advantage)。制药行业也出现了逐底竞争，但问题是，在人工智能领域，或许需要更好的监管。

另外还提到:“公司之间的集体行动问题可以对消费者和公众产生积极影响。价格战是公司之间的集体行动问题，例如，它对消费者产生积极影响，因为它导致价格下降。”这否定了这样一个事实:价格战对生产商的影响要大得多，人们确实被解雇了——一些最穷的工厂工人或农场工人确实被解雇了。它有非常负面的影响，我不认为“大部分是正面的”会是这种意义上的特征，它忽略了对劳动力的关注。

本文中有一些尝试来玩我们的合作-缺陷游戏的场景，然而，正如他们所说的，他们的分析的一个缺点是，它吸引了过度简化的合作和缺陷的概念。他们认为:*“为了“解决”一个集体行动问题，我们可以尝试将其转化为一种相互合作是合理的情况。”*然而，这假定了经济理性或理性的人类行为者，而通常情况并非如此。

我可以探究非理性或缺乏理性的论点，诺贝尔奖获得者丹尼尔·卡内曼或其他人批评*经济人*或理性人的神话被夸大了。我推荐他的讲座[有限理性地图](https://www.nobelprize.org/prizes/economic-sciences/2002/kahneman/lecture/)。

## 如何改善合作？

概述了五个因素，以使人工智能公司在面临集体行动问题时更有可能进行合作:

1.  更加相信别人会合作。
2.  赋予相互合作更高的期望值。
3.  将较低的预期成本分配给未完成的合作。
4.  将较低的期望值赋予非往复式合作。
5.  给相互背叛赋予一个较低的期望值。

他们确实表示自己的观点不足，并强调:

> “……有必要将这些因素转化为各行为者能够实施的具体政策战略，以改善合作前景。”

作者反对在人工智能开发中使用敌对的修辞。另一种可能是加入研究，考虑到 DeepMind 最近发布的基于 OpenAI frontline 和谷歌多巴胺的套件，这似乎是 OpenAI 正在采取的一种方法。*“人工智能领域积极而明确的研究合作，尤其是跨机构和跨国界的合作，目前在数量、规模和范围上都相当有限。”*他们概述了一些合作的可能性:

需要考虑的领域可能包括

*   *共同研究人工智能系统能力的形式化验证以及广泛应用的人工智能安全保障的其他方面；*
*   *各种应用的“人工智能为善”项目，其结果可能有广泛和很大程度上积极的应用(例如在可持续性和健康等领域)；*
*   *协调特定基准的使用；*
*   *联合创建和共享有助于安全性研究的数据集；*
*   *共同制定应对全球人工智能相关威胁的对策，如在线合成媒体生成的误用。*

本文呼吁开放，但也要考虑影响，不要什么都开放。*“完全透明作为一个追求的理想是有问题的，因为对于在所有情况下实现问责制来说，它既不必要也不充分”*

## 我们可以激励良好的行为

OpenAI 论文中提到了一系列可能的激励措施，具体如下:

*   **社会激励**(例如，评价或批评与人工智能开发相关的某些行为)可以影响不同公司对风险和机遇的看法。
*   **经济激励**(由政府、慈善家、行业或消费者行为引发)可以增加高价值人工智能系统在特定市场或更普遍市场中的份额，并增加对特定规范的关注。
*   法律激励(即禁止某些形式的人工智能开发，并处以经济或更严厉的惩罚)可以大幅减少一些行为者以某些方式叛逃的诱惑。
*   **与人工智能特别相关的特定领域激励**(例如，早期获得最新一代的计算能力)可用于鼓励某些形式的行为。

## 本文引发的问题

作者概述了一系列需要进一步探讨的问题。逐字列出如下:

1.  *人工智能行业发展的竞争动态与政府主导或政府支持的人工智能发展有何不同？*
2.  *法律机构、政府和标准化机构在解决公司之间的集体诉讼问题时，特别是当这些集体诉讼问题可能在国际公司之间出现时，应该扮演什么样的角色？*
3.  *可以发现或构建哪些进一步的策略来帮助防止负责任的人工智能开发的集体行动问题的形成，并在这些问题出现时帮助解决这些问题？我们能从历史或当代工业中吸取什么教训？*
4.  *特定的技术发展或对此类发展的预期会如何影响竞争态势？*

## 最终反射

我很高兴 OpenAI 提出了这些问题，尽管一些关于某些理论思想的批判性想法正在进行中，但总体而言，这可以说是一个非常重要的举措，推动该领域朝着更负责任地使用技术的方向发展。这是 OpenAI 被要求承担的额外角色，以确保人类从这一技术变革中受益。我当然会鼓励你不要相信我的观点，通过阅读[原文](https://arxiv.org/pdf/1907.04534.pdf)来补充你自己的观点。否则，我希望这是有用的，并确保给我留下任何想法，你应该想到的东西。

这是#500daysofAI 每天写人工智能的第 79 天。