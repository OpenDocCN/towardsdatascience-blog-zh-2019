<html>
<head>
<title>Support Vector Machine: Digit Classification with Python; Including my Hand Written Digits</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机:用 Python 实现数字分类:包括我手写的数字</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machine-mnist-digit-classification-with-python-including-my-hand-written-digits-83d6eca7004a?source=collection_archive---------4-----------------------#2019-01-21">https://towardsdatascience.com/support-vector-machine-mnist-digit-classification-with-python-including-my-hand-written-digits-83d6eca7004a?source=collection_archive---------4-----------------------#2019-01-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ecf4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解 SVM 系列:第三部分</h2></div><p id="3a53" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在前面对 SVM 算法的详细讨论之后，我将用一个 SVM 对手写数字进行分类的应用程序来结束这个系列。这里我们将使用 MNIST <a class="ae le" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">数据库</a>来存储手写数字，并使用 SVM 对数字从 0 到 9 进行分类。原始数据集处理起来很复杂，所以我使用 Joseph <a class="ae le" href="https://pjreddie.com/projects/mnist-in-csv/" rel="noopener ugc nofollow" target="_blank"> Redmon </a>处理过的数据集。</p><p id="d605" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我已经遵循了 Kaggle 竞赛程序，你可以从 kaggle 本身下载数据集。数据集基于手写数字的灰度图像，每幅图像高 28 像素，宽 28 像素。每个像素都有一个与之关联的数字，其中 0 表示暗像素，255 表示白像素。训练和测试数据集都有 785 列，其中“标签”列代表手写数字，其余 784 列代表(28，28)像素值。训练和测试数据集分别包含 60，000 和 10，000 个样本。我将使用我在<a class="ae le" rel="noopener" target="_blank" href="/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976">上一篇</a>文章中介绍的几个技术，比如<code class="fe lf lg lh li b">GridSearchCV</code>和<code class="fe lf lg lh li b">Pipeline</code>，以及一些新概念，比如用<code class="fe lf lg lh li b">numpy</code>数组表示灰度图像。</p><p id="533c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我使用了来自训练和测试数据集的 12000 个样本和 5000 个样本，只是为了减少计算时间，建议使用完整的数据集来获得更好的分数，并避免选择偏差。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="5683" class="lr ls it li b gy lt lu l lv lw">import math, time <br/>import matplotlib.pyplot as plt<br/>import numpy as np <br/>import pandas as pd</span><span id="0beb" class="lr ls it li b gy lx lu l lv lw">start = time.time() </span><span id="1622" class="lr ls it li b gy lx lu l lv lw">MNIST_train_small_df = pd.read_csv('mnist_train_small.csv', sep=',', index_col=0)<br/>#print MNIST_train_small_df.head(3)<br/>print MNIST_train_small_df.shape</span><span id="0713" class="lr ls it li b gy lx lu l lv lw">&gt;&gt; (12000, 785)</span></pre><p id="7cc8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过打印出 <code class="fe lf lg lh li b"><strong class="kk iu">value_counts()</strong></code>和/或标签分布图<strong class="kk iu">来检查训练数据集是否偏向某些数字。</strong></p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="c5b6" class="lr ls it li b gy lt lu l lv lw">sns.countplot(MNIST_train_small_df['label'])<br/>plt.show()# looks kinda okay</span><span id="98e9" class="lr ls it li b gy lx lu l lv lw"># or we can just print</span><span id="9e73" class="lr ls it li b gy lx lu l lv lw">print MNIST_train_small_df['label'].value_counts()</span><span id="a6a3" class="lr ls it li b gy lx lu l lv lw">&gt;&gt;<br/>1    1351<br/>7    1279<br/>3    1228<br/>6    1208<br/>0    1206<br/>9    1193<br/>4    1184<br/>2    1176<br/>8    1127<br/>5    1048</span></pre><figure class="lj lk ll lm gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ly"><img src="../Images/a3ee4480cff73b7958cbd5357c6d3d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*suQgzc-Z_ZpXk16m9YDv-Q.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 1: Bar plots of sample distribution in training data set</figcaption></figure><p id="a4b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们看到选择稍微偏向数字 1，并且标签 1 的样本计数比样本 5 高大约 30%，即使我们使用完整的训练数据集(60，000 个样本)，这个问题仍然存在。继续，是时候分开标签和像素列了，标签是数据帧的第一列。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="66e0" class="lr ls it li b gy lt lu l lv lw">X_tr = MNIST_train_small_df.iloc[:,1:] # iloc ensures X_tr will be a dataframe<br/>y_tr = MNIST_train_small_df.iloc[:, 0]</span></pre><p id="eb7a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我将训练数据和测试数据分开，20%的样本用于测试数据。我使用了<strong class="kk iu"> </strong> <code class="fe lf lg lh li b"><strong class="kk iu">stratify=y</strong></code> <strong class="kk iu">来保存标签的分布</strong>(数字)—</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="e89e" class="lr ls it li b gy lt lu l lv lw">X_train, X_test, y_train, y_test = train_test_split(X_tr,y_tr,test_size=0.2, random_state=30, stratify=y_tr)</span></pre><p id="f312" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于像素值在 0-255 的范围内变化，是时候使用一些标准化了，我已经使用了<code class="fe lf lg lh li b"><a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">StandardScaler</a></code>，它通过移除平均值并将其缩放到单位方差来标准化特征。<strong class="kk iu"> <em class="mk">同样在尝试了所有的核之后，多项式核取得了最好的成绩和最少的时间。</em> </strong>要了解更多关于内核的技巧，你可以查看<a class="ae le" rel="noopener" target="_blank" href="/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d">以前的帖子</a>。</p><p id="d75c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我们将设置<code class="fe lf lg lh li b">Pipeline</code>对象，用<code class="fe lf lg lh li b">StandardScaler</code>和<code class="fe lf lg lh li b">SVC</code>分别作为转换器和估计器。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="e39c" class="lr ls it li b gy lt lu l lv lw">steps = [('scaler', StandardScaler()), ('SVM', SVC(kernel='poly'))]<br/>pipeline = Pipeline(steps) # define Pipeline object</span></pre><p id="e17f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了决定<code class="fe lf lg lh li b">C, gamma</code>的值，我们将使用具有 5 重交叉验证的<code class="fe lf lg lh li b">GridSearchCV</code>方法。如果你想了解更多关于管道和网格搜索的知识，请查看我之前的<a class="ae le" rel="noopener" target="_blank" href="/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976">帖子</a>。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="0204" class="lr ls it li b gy lt lu l lv lw">parameters = {'SVM__C':[0.001, 0.1, 100, 10e5], 'SVM__gamma':[10,1,0.1,0.01]}</span><span id="73ee" class="lr ls it li b gy lx lu l lv lw">grid = GridSearchCV(pipeline, param_grid=parameters, cv=5)</span></pre><p id="913f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们准备测试模型并找到最合适的参数。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="c6c5" class="lr ls it li b gy lt lu l lv lw">grid.fit(X_train, y_train)</span><span id="748f" class="lr ls it li b gy lx lu l lv lw">print "score = %3.2f" %(grid.score(X_test, y_test))</span><span id="1839" class="lr ls it li b gy lx lu l lv lw">print "best parameters from train data: ", grid.best_params_</span><span id="65eb" class="lr ls it li b gy lx lu l lv lw">&gt;&gt; <br/>score = 0.96</span><span id="7340" class="lr ls it li b gy lx lu l lv lw">best parameters from train data:  {'SVM__C': 0.001, 'SVM__gamma': 10}</span><span id="b169" class="lr ls it li b gy lx lu l lv lw">&gt;&gt;</span><span id="9bda" class="lr ls it li b gy lx lu l lv lw">y_pred = grid.predict(X_test)</span></pre><p id="03a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">使用 12000 个样本获得了 96%的准确率</strong>，我预计使用完整的 60000 个样本后，这个分数会有所提高。<code class="fe lf lg lh li b">GridSearchCV</code>部分比较耗时，如果你愿意，可以直接使用 C 和 gamma 参数。</p><p id="d47b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以检查一些预测</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="118f" class="lr ls it li b gy lt lu l lv lw">print y_pred[100:105]<br/>print y_test[100:105]</span><span id="0c3b" class="lr ls it li b gy lx lu l lv lw">&gt;&gt; <br/>[4 2 9 6 0]<br/>1765     4<br/>220      2<br/>932      9<br/>6201     6<br/>11636    0</span></pre><p id="d310" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在可以使用 python matplotlib pyplot <code class="fe lf lg lh li b">imshow</code>来绘制数字。我们使用预测列表和来自测试列表的像素值进行比较。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="6135" class="lr ls it li b gy lt lu l lv lw">for i in (np.random.randint(0,270,6)):<br/> two_d = (np.reshape(X_test.values[i], (28, 28)) * 255).astype(np.uint8)<br/> plt.title('predicted label: {0}'. format(y_pred[i]))<br/> plt.imshow(two_d, interpolation='nearest', cmap='gray')<br/> plt.show()</span></pre><p id="317c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我简单解释一下代码的第二行。由于像素值在数据集中排列成 784 列的一行，我们首先使用<code class="fe lf lg lh li b">numpy</code>‘reshape’模块将其排列成 28 X 28 的数组，然后乘以 255，因为像素值最初是标准化的。请注意<code class="fe lf lg lh li b">X_test.values</code>返回数据帧的‘numpy’表示。</p><figure class="lj lk ll lm gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ml"><img src="../Images/82c83ebcdd980f3c682408ce383db302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bv3Zqun47RI6h3slMmwcTw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 2: Examples of digit classification on training data-set.</figcaption></figure><p id="a83e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你在上面的图片中看到的，除了一个，所有的图片都被正确分类了(我认为图片(1，1)是数字 7 而不是 4)。要知道有多少数字被错误分类，我们可以打印出<em class="mk">混淆矩阵。</em>根据<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>中给出的定义</p><blockquote class="mm mn mo"><p id="8350" class="ki kj mk kk b kl km ju kn ko kp jx kq mp ks kt ku mq kw kx ky mr la lb lc ld im bi translated">混淆矩阵 C 使得 c(i，j)等于已知在组 I 中但预测在组 j 中的观察值的数量</p></blockquote><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="b461" class="lr ls it li b gy lt lu l lv lw">print "confusion matrix: \n ", confusion_matrix(y_test, y_pred)</span><span id="edf0" class="lr ls it li b gy lx lu l lv lw">&gt;&gt;</span><span id="0d57" class="lr ls it li b gy lx lu l lv lw">[[236   0   0   1   1   2   1   0   0   0]<br/> [  0 264   1   1   0   0   1   1   2   0]<br/> [  0   1 229   1   2   0   0   0   1   1]<br/> [  0   0   2 232   0   3   0   2   5   2]<br/> [  0   1   0   0 229   1   1   0   1   4]<br/> [  0   0   1   4   1 201   0   0   1   2]<br/> [  3   1   2   0   3   3 229   0   0   0]<br/> [  0   1   3   0   6   0   0 241   0   5]<br/> [  0   0   3   6   1   2   0   0 213   0]<br/> [  3   1   1   0   1   0   0   1   2 230]]</span></pre><p id="84f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，如果我们考虑第一行，我们可以理解，在 241 个零中，236 个被正确分类，等等..</p><p id="41d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将对测试数据集(mnist_test.csv)重复该过程，但我没有使用<code class="fe lf lg lh li b">GridSearchCV</code>来寻找 SVM (C，gamma)的最佳参数，而是使用了来自训练数据集的相同参数。</p><p id="3e9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如前所述，我使用了 5000 个样本而不是 10000 个测试样本来减少时间消耗。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="ef94" class="lr ls it li b gy lt lu l lv lw">MNIST_df = pd.read_csv('mnist_test.csv')<br/>MNIST_test_small = MNIST_df.iloc[0:5000]<br/>MNIST_test_small.to_csv('mnist_test_small.csv')<br/>MNIST_test_small_df = pd.read_csv('mnist_test_small.csv', sep=',', index_col=0)</span></pre><p id="d86f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下一步是选择特征和标签—</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="ede1" class="lr ls it li b gy lt lu l lv lw">X_small_test = MNIST_test_small_df.iloc[:,1:]<br/>Y_small_test = MNIST_test_small_df.iloc[:,0]</span></pre><p id="dafb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将特征和标签分为训练集和测试集</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="687e" class="lr ls it li b gy lt lu l lv lw">X_test_train, X_test_test, y_test_train, y_test_test = train_test_split(X_small_test,Y_small_test,test_size=0.2, random_state=30, stratify=Y_small_test)</span></pre><p id="5faa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">设置<code class="fe lf lg lh li b">Pipeline</code>对象</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="2143" class="lr ls it li b gy lt lu l lv lw">steps1 = [('scaler', StandardScaler()), ('SVM', SVC(kernel='poly'))]<br/>pipeline1 = Pipeline(steps1) # define</span></pre><p id="1a2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">设置<code class="fe lf lg lh li b">GridSearchCV</code>对象，但这次我们使用通过 mnist_train.csv 文件估算的参数。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="0ae9" class="lr ls it li b gy lt lu l lv lw">parameters1 = {'SVM__C':[grid.best_params_['SVM__C']], 'SVM__gamma':[grid.best_params_['SVM__gamma']]} </span><span id="cc9c" class="lr ls it li b gy lx lu l lv lw">grid1 = GridSearchCV(pipeline1, param_grid=parameters1, cv=5)<br/>grid1.fit(X_test_train, y_test_train)</span><span id="f136" class="lr ls it li b gy lx lu l lv lw">print "score on the test data set= %3.2f" %(grid1.score(X_test_test, y_test_test))</span><span id="23ef" class="lr ls it li b gy lx lu l lv lw">print "best parameters from train data: ", grid1.best_params_ # same as previous with training data set</span><span id="a73a" class="lr ls it li b gy lx lu l lv lw">&gt;&gt;<br/>score on the test data set= 0.93</span><span id="5252" class="lr ls it li b gy lx lu l lv lw">best parameters from train data:  {'SVM__C': 0.001, 'SVM__gamma': 10}</span><span id="3ba6" class="lr ls it li b gy lx lu l lv lw">&gt;&gt;</span><span id="c79d" class="lr ls it li b gy lx lu l lv lw">y_test_pred = grid1.predict(X_test_test)</span></pre><p id="381f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">测试数据集上的得分为 93%，而训练数据集上的得分为 96%。下面是测试数据集中的一些随机图像，与预测水平进行了比较。</p><figure class="lj lk ll lm gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ms"><img src="../Images/84fa0ea9fd089b6152f04f05a9ea0c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBF5BiH7ovW7f5vQO8IIFA.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 3: Examples of digit classification on test data-set.</figcaption></figure><p id="d84c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以检查测试数据集的混淆矩阵，以全面了解错误分类。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="c389" class="lr ls it li b gy lt lu l lv lw">print "confusion matrix: \n ", confusion_matrix(y_test_test, y_test_pred)</span><span id="f003" class="lr ls it li b gy lx lu l lv lw">&gt;&gt;</span><span id="7761" class="lr ls it li b gy lx lu l lv lw">[[ 91   0   0   0   0   0   0   0   1   0]<br/> [  0 111   2   0   1   0   0   0   0   0]<br/> [  0   0  98   1   0   0   1   2   4   0]<br/> [  0   0   1  91   0   2   0   0   4   2]<br/> [  0   0   0   1  95   0   0   1   0   3]<br/> [  0   0   1   3   1  77   4   0   3   2]<br/> [  1   1   1   0   2   0  85   0   2   0]<br/> [  0   0   0   1   0   0   0 100   0   2]<br/> [  0   0   1   1   0   2   0   1  93   0]<br/> [  0   0   0   0   4   1   0   3   3  93]]</span></pre><p id="4499" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，在 92 个标签中，只有 1 个数字 0 分类错误。现在我们将继续讨论对我自己的手写图像进行分类的可能性。</p><h2 id="b6b1" class="lr ls it bd mt mu mv dn mw mx my dp mz kr na nb nc kv nd ne nf kz ng nh ni nj bi translated">对自己的手写图像进行分类:</h2><p id="750e" class="pw-post-body-paragraph ki kj it kk b kl nk ju kn ko nl jx kq kr nm kt ku kv nn kx ky kz no lb lc ld im bi translated">下面是我准备数据集，然后对从 0 到 9 的数字进行分类的步骤</p><ol class=""><li id="7fa3" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated">我已经使用<a class="ae le" href="http://mypaint.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mk"> mypaint </em> </a> <em class="mk">首先编写(绘制)图像</em>，然后使用<em class="mk"> Imagemagick 来调整图像</em>的大小，使其高度和宽度为 28X28 像素。</li></ol><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="19d0" class="lr ls it li b gy lt lu l lv lw">convert -resize 28X28! sample_image0.png sample_image0_r.png</span></pre><figure class="lj lk ll lm gt lz gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7afb87d55176c99558543004fe01c45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*IaH1BNN-PWecdjsp5WKqCg.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 4: Resized (28X28) My Own Hand-written Images</figcaption></figure><p id="a3d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.<em class="mk">将图像转换为 numpy 数组</em>并检查像素值是如何分布的。你可以在我的<a class="ae le" href="https://github.com/suvoooo/MNIST_digit_classify/blob/description/codes/checkIMagePixel_stack.py" rel="noopener ugc nofollow" target="_blank"> github </a>上找到代码，下面是两个例子</p><figure class="lj lk ll lm gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi nz"><img src="../Images/5f50bbc1c4e4b9829b9e3706d930ab4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7wz1oVH_jrkA5_2tHxvgA.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 5: Representing images with pixels using Image and Numpy</figcaption></figure><p id="b9e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.<em class="mk">将数组(28X28)展平为(784，)</em>转换为列表。然后写在一个 csv 文件，包括标签，即像素代表的数字。所以现在总列数是 785，与我以前使用的训练和测试 csv 文件一致。这些代码可在<a class="ae le" href="https://github.com/suvoooo/MNIST_digit_classify/blob/description/codes/checkIMagePixel.py" rel="noopener ugc nofollow" target="_blank">github.com</a>获得。</p><p id="2f12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.把新的数据帧和测试数据帧连接起来，这样新文件就多了 10 行。</p><p id="ffad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.最后<em class="mk">用这个新文件</em>运行相同的分类过程，只有一个不同——训练和测试数据不是使用<code class="fe lf lg lh li b">train_test_split</code>方法准备的，因为我的主要目的是看看算法如何对新数据起作用。所以我选择了前 3500 行用于训练，剩下的行(包括新数据)作为测试样本。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="e3b7" class="lr ls it li b gy lt lu l lv lw">X_hand_train = new_file_df_hand.iloc[0:3500, 1:]<br/>X_hand_test  = new_file_df_hand.iloc[3500:5011, 1:]<br/>y_hand_test = new_file_df_hand.iloc[3500:5011, 0]<br/>y_hand_train = new_file_df_hand.iloc[0:3500, 0]</span></pre><p id="2b94" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">6.为了绘制手写图像以及它们与预测输出的匹配程度，我像以前一样使用了以下 for 循环——因为最后 1500 个样本(包括我自己的手写图像)被作为测试数据，所以循环在最后几行上进行。</p><pre class="lj lk ll lm gt ln li lo lp aw lq bi"><span id="e1f9" class="lr ls it li b gy lt lu l lv lw">for ik in range(1496, 1511, 1):<br/> three_d = (np.reshape(X_hand_test.values[ik], (28, 28)) * 255).astype(np.uint8)<br/> plt.title('predicted label: {0}'. format(y_hand_pred[ik]))<br/> plt.imshow(three_d, interpolation='nearest', cmap='gray')<br/> plt.show()</span></pre><p id="615a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">7.包括我自己手写数据在内的测试数据集上的得分是 93%。</p><p id="5f41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">8.让我们来看看分类器对我的笔迹从 0 到 9 的分类有多好</p><figure class="lj lk ll lm gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi oa"><img src="../Images/7c4e7ebd068b1647a7d4615a78fde276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jk_M2Cg05eN5Tt3XHByGhg.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 5: Predicted labels on my hand-written digits. 70% correct !!!</figcaption></figure><p id="db4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，10 个手写数字中有 7 个被正确分类，这很好，因为如果你与 MNIST 数据库的图像进行比较，我自己的图像是不同的，我认为一个原因是笔刷的选择。正如我意识到的，我使用的笔刷产生了更厚的图像。特别是在与 MNIST 图像比较时，我发现我的图像中边缘之间的像素比 MNIST 图像更亮(更高的像素值— &gt; 255)，这可能是 30%错误分类的原因。</p><p id="28c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我想你已经知道如何使用支持向量机来处理更现实的问题了。作为一个迷你项目，你可以使用类似的算法来分类 MNIST <a class="ae le" href="https://www.kaggle.com/zalando-research/fashionmnist/home" rel="noopener ugc nofollow" target="_blank">时尚数据</a>。</p><p id="97f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望你喜欢这篇文章，如果你想了解更多关于 SVM 的基础知识，请查看我以前在这个系列中的文章。</p></div></div>    
</body>
</html>