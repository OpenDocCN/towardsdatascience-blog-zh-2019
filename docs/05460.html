<html>
<head>
<title>Easily Query ORC Data in Python with PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PySpark 轻松查询 Python 中的 ORC 数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/easily-query-orc-data-in-python-with-pyspark-572749196828?source=collection_archive---------25-----------------------#2019-08-12">https://towardsdatascience.com/easily-query-orc-data-in-python-with-pyspark-572749196828?source=collection_archive---------25-----------------------#2019-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/032692317d31cc3ff4038b0dbc72e4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mwLnW6bjkNKQq-UN"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@madeyes?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Eric Han</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ecf7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">优化的行列(ORC)是一种面向列的数据存储格式，是 Apache Hadoop 家族的一部分。虽然 ORC 文件和处理它们通常不在数据科学家的工作范围内，但有时您需要提取这些文件，并使用您选择的数据管理库来处理它们。</p><p id="d46f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最近，我遇到了一种情况，我想处理一些以 ORC 格式存储的对象数据。当我把它读出来时，我不能直接把它写到数据帧中。我想分享一些以 ORC 格式获取数据并将其转换成更容易接受的格式的技巧，比如熊猫数据帧或 CSV。</p><p id="322b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我们将利用 Pyspark。如果你刚刚开始使用 Pyspark，这里有一个<a class="ae kf" rel="noopener" target="_blank" href="/a-brief-introduction-to-pyspark-ff4284701873">很棒的介绍</a>。在本教程中，我们将快速浏览 PySpark 库，并展示如何读取 ORC 文件，并将其读出到 Pandas 中。</p><p id="de1b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将从终端内部安装 PySpark 库</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="523b" class="ln lo it lj b gy lp lq l lr ls">Pip install pyspark</span></pre><p id="faf3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这里，我们将引入 PySpark 库的两个部分，SparkContext 和 SQLContext。如果你是 Spark 新手，那么<a class="ae kf" href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-SparkContext.html" rel="noopener ugc nofollow" target="_blank">我推荐这个教程。</a>您可以将 SparkContext 视为所有 Apache Spark 服务的入口点，也是我们 Spark 应用程序的核心。SQLContext 被认为是 Spark SQL 功能的<a class="ae kf" href="https://spark.apache.org/docs/1.6.1/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank">入口点</a>，使用 SQLContext 允许您以一种熟悉的、类似 SQL 的方式查询 Spark 数据。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="efa3" class="ln lo it lj b gy lp lq l lr ls">from pyspark import SparkContext, SQLContext<br/>sc = SparkContext(“local”, “SQL App”)<br/>sqlContext = SQLContext(sc)</span></pre><p id="a990" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在上面的代码中看到，我们还为 SparkContext 声明了一些细节。在这种情况下，我们说我们的代码在本地运行，我们给它一个 appName，在这种情况下我们称之为“SQL App”。</p><p id="951b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们创建了 SparkContext(这里称为 sc ),我们就将它传递给 SQLContext 类来初始化 SparkSQL。</p><p id="1b18" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">至此，我们已经安装了 PySpark 并创建了 Spark 和 SQL 上下文。现在到了重要的一点，读取和转换 ORC 数据！假设我们将数据存储在与 python 脚本相同的文件夹中，它被称为“objectHolder”。要将它读入 PySpark 数据帧，我们只需运行以下命令:</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="9249" class="ln lo it lj b gy lp lq l lr ls">df = sqlContext.read.format(‘orc’).load(‘objectHolder’)</span></pre><p id="f3ba" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们想把这个数据帧转换成熊猫数据帧，我们可以简单地做以下事情:</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="e4a1" class="ln lo it lj b gy lp lq l lr ls">pandas_df = df.toPandas()</span></pre><p id="34fb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">综上所述，我们的代码如下:</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="68cd" class="ln lo it lj b gy lp lq l lr ls">from pyspark import SparkContext, SQLContext<br/>sc = SparkContext(“local”, “SQL App”)<br/>sqlContext = SQLContext(sc)<br/>df = sqlContext.read.format(‘orc’).load(‘objectHolder’)<br/>pandas_df = df.toPandas()</span></pre><p id="3a09" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们有了。只用几行代码，我们就可以读取一个本地 orc 文件，并将其转换成我们更习惯的格式，在这个例子中，是一个熊猫数据帧。</p></div></div>    
</body>
</html>