<html>
<head>
<title>Neural Network tutorial with Devanagari Characters using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PyTorch 的梵文字符神经网络教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-network-tutorial-with-devanagari-characters-using-pytorch-6abceb78a020?source=collection_archive---------25-----------------------#2019-11-15">https://towardsdatascience.com/neural-network-tutorial-with-devanagari-characters-using-pytorch-6abceb78a020?source=collection_archive---------25-----------------------#2019-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4532" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">用 PyTorch 训练一个人工神经网络用于识别手写的梵文字符。</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi kg"><img src="../Images/8dcc088c03aa872e1e5449ea2d93807f.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/1*QQjF19obLmicrqwIC2XqSQ.gif"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk">Source: images.google.com</figcaption></figure><p id="4007" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">顾名思义，神经网络是由人脑神经元松散地激发出来的。但是我们不会在这里讨论大脑类比的任何细节。相反，我们将通过一些数学和编码来理解它。</p><h1 id="f136" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">感知器</h1><p id="a3c3" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">弗兰克·罗森布拉特(Frank Rosenblatt)在 20 世纪 50 年代提出了感知机(perceptron)，表明一种算法可以模仿人类大脑的决策能力(我们仍在尝试)。他在论文中写道，我们可以将输入视为由二进制数组成的神经元。如果这些二进制数可以生成一个二进制数作为输出<strong class="ku ir">，如果</strong>输出满足某个阈值，则只生成<strong class="ku ir">。</strong></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/f03f1f5c5772d8e3998412044a1a48f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*my4iy7SLDVIRUvHJuqDD8A.png"/></div></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk">Perceptron model</figcaption></figure><p id="5a03" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">你可能会问:</p><p id="9400" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">但是 Gopal，我们也可以写一个程序来做这个任务；为什么要写神经网络呢？我很高兴你问了。</p><p id="6beb" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">选择神经网络而不是任何程序的第一个原因是，它们是<strong class="ku ir">通用函数逼近器</strong>，可以推断出我们试图建立的模型，或者如果它太复杂，神经网络总是代表那个函数。</p><p id="854d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们可以假设将任何函数转化为数学术语，然后我们可以使用神经网络来表示该函数。</p><p id="07d5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">第二个原因是可伸缩性和灵活性。我们可以很容易地在神经网络中堆叠层，这将增加神经网络的复杂性。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="5a41" class="lo lp iq bd lq lr mx lt lu lv my lx ly jw mz jx ma jz na ka mc kc nb kd me mf bi translated">神经网络的基本架构</h1><p id="519b" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated"><strong class="ku ir">神经网络由以下组件组成:</strong></p><ul class=""><li id="ff11" class="nc nd iq ku b kv kw ky kz lb ne lf nf lj ng ln nh ni nj nk bi translated">输入层 x</li><li id="5f61" class="nc nd iq ku b kv nl ky nm lb nn lf no lj np ln nh ni nj nk bi translated">任意数量的隐藏层</li><li id="26a3" class="nc nd iq ku b kv nl ky nm lb nn lf no lj np ln nh ni nj nk bi translated">输出图层，𝑦</li><li id="a989" class="nc nd iq ku b kv nl ky nm lb nn lf no lj np ln nh ni nj nk bi translated">一套 parameters(𝑊)和 biases(𝑏)各 layer,𝑊和𝑏之间</li><li id="5438" class="nc nd iq ku b kv nl ky nm lb nn lf no lj np ln nh ni nj nk bi translated">每个隐藏层的激活函数的选择，𝜎</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/11ada75e3696caea3f0917480ee9519d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*NeRo40VwnRLl2UsPKecotA.jpeg"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk">Resource from <a class="ae nr" href="https://tex.stackexchange.com/questions/132444/diagram-of-an-artificial-neural-network" rel="noopener ugc nofollow" target="_blank">https://tex.stackexchange.com/questions/132444/diagram-of-an-artificial-neural-network</a></figcaption></figure><p id="13c0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们训练神经网络进行<em class="ns"> n </em>次迭代；每次迭代包括两个步骤:</p><ol class=""><li id="eeb9" class="nc nd iq ku b kv kw ky kz lb ne lf nf lj ng ln nt ni nj nk bi translated">前馈</li><li id="6b0d" class="nc nd iq ku b kv nl ky nm lb nn lf no lj np ln nt ni nj nk bi translated">反向传播</li></ol><h1 id="dfa9" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">前馈:</h1><p id="3243" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">简单来说，当第一层的输出成为下一层的输入。这种网络称为<strong class="ku ir">前馈网络。</strong></p><p id="9f56" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们的网络中没有反馈信息的回路；它将总是被前馈。</p><h1 id="0a80" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">反向传播:</h1><p id="71a4" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">这是我们的神经网络<em class="ns">实际上</em>从训练<strong class="ku ir">数据</strong>中学习的过程。</p><p id="68c2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">但问题仍然没有答案，我们的网络如何学习分类或预测？如果我们的模型已经预测到了什么，那么我们的模型如何确定它是正确的预测还是错误的呢？</p><p id="4e7f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">答案是<strong class="ku ir">损失函数；它</strong>有助于我们的网络预测与原始值相差多少。</p><p id="3cdb" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们的模型已经预测了一所房子的价格为 100K 美元，而原始价格为 101K 美元，那么原始价格和预测价格之间的差异将为 1K 美元；这就是损失函数帮助我们的网络决定的。</p><p id="93ae" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">回到反向传播，在我们借助损失函数计算模型预测和原始值的误差之后，<strong class="ku ir">我们发回这个误差以更新我们的输入神经元或</strong> <strong class="ku ir">权重和偏差。这被称为反向传播。</strong></p><p id="aecf" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">但是我们需要多少来更新我们的<strong class="ku ir">权重和偏差</strong>？</p><p id="5319" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">为了知道调整权重和偏差的适当量，我们必须推导出<em class="ns">我们的</em> <em class="ns">权重和偏差的损失函数</em>。</strong></p><p id="651d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我认为现在我们有足够的直觉来开始我们的编码部分。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="47af" class="lo lp iq bd lq lr mx lt lu lv my lx ly jw mz jx ma jz na ka mc kc nb kd me mf bi translated">实现人工神经网络对手绘天体文字的分类</h1><p id="0af9" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">我们将使用 PyTorch 库来构建我们的神经网络。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="cd9a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我写了一个小程序<em class="ns"> plot_images </em>用于显示字符及其标签。</p><p id="4062" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">让我们看看我们的 CSV 文件中有什么。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="a36b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><em class="ns"> df.head() </em>将给出数据帧的前 5 列。在我们的数据集中，我们有从 0 到 1023 的像素值。用于显示特定字符的<em class="ns">字符</em>列由特定像素值组成。</p><p id="6a86" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">让我们运行我们的<em class="ns"> plot_images </em>函数，看看这些图像。</p><pre class="kh ki kj kk gt nw nx ny nz aw oa bi"><span id="3ded" class="ob lp iq nx b gy oc od l oe of">&gt;&gt;plot_images(df, 4, "character")</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi og"><img src="../Images/03c93e311d7001ceb5d219f37474c073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RPSE6gLNhHex63JK-FOjNg.png"/></div></div></figure><p id="251e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们的数据集中有 46 个独特的手绘字符；因此，数字 46 将是我们神经网络的输出维度。</p><p id="7be1" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">但是在创建神经网络之前，我们需要准备数据加载器，以便将它提供给我们的模型进行训练和测试，因为 NumPy 数据不能与 PyTorch 的库一起工作。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="fa9a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">因此，在上面的代码示例中，我们读取数据并从中分离出要素和标签。<strong class="ku ir">如果你注意到了，我也在用 255.0 除<em class="ns">features _ numpy</em>；我这样做是为了规范我们的像素值。</strong></p><p id="3d30" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">然后，我将我们的分类标签转换成代码，因为我们只能用数字数据来创建张量值。</p><p id="ada3" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在我们的<em class="ns"> data_loader </em>函数中，我们获取特性和目标；如果它们在 NumPy 数据中，我们就把它们转换成张量；之后，我们用<strong class="ku ir"><em class="ns">torch . utils . Data . tensordataset、</em> </strong>和<strong class="ku ir">创建张量数据，最后将数据转换成数据加载器。</strong></p><p id="7982" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">就是这样；我们的数据现在可以输入到模型中了。让我们现在建立神经网络。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="8509" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在我们的<strong class="ku ir"> ANNModel 中，取 32*32，最后一层输入的输出维度为 46。但是我们这里有两个我还没有谈到的新术语。</strong></p><p id="88a6" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir"> Dropout:删除带有概率值的随机激活权重。假设我们已经将概率值设置为 0.2。那么对于每一个前馈或者反向传播过程，它都会忽略掉那 20%的神经元。它有助于防止过度合身。</strong></p><p id="4c4f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir"> Softmax:在数学中，Softmax 函数，也称为 softargmax 或归一化指数函数，是一个将 K 个实数的向量作为输入，并将其归一化为由 K 个与输入数的指数成比例的概率组成的概率分布的函数。</strong></p><p id="3bba" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">通过理解这两个术语，我们现在可以继续培训过程。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="1ddc" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">训练过程简单；我们正在迭代我们的训练数据加载器的图像和标签。然后清除初始梯度值并随后进行预测。运行这个程序后，您应该可以获得超过 94%的测试数据的准确率。</p><p id="9256" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">不可能解释每一步；请在评论区问我。</p><p id="b127" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">培训结束后，让我们看看我们的培训和验证曲线。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nu nv l"/></div></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/6f2845f2f9119641b68b2d0d7a581663.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*lwCyYoDZ5YBCN7QdwQslZQ.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk">Training vs. Validation loss</figcaption></figure><p id="3e9e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">看起来不错。:)</p><h2 id="f149" class="ob lp iq bd lq oi oj dn lu ok ol dp ly lb om on ma lf oo op mc lj oq or me os bi translated">推论:</h2><p id="c887" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">让我们检查一下我们的模型在测试数据上的表现。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nu nv l"/></div></figure><pre class="kh ki kj kk gt nw nx ny nz aw oa bi"><span id="4fe2" class="ob lp iq nx b gy oc od l oe of">&gt;&gt; make_predictions(test_loader, 44)</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/bb5db742937febd116a8a31e7b8688b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*pPuE20xcl1apwwoG1nwLMg.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk">Inference</figcaption></figure><p id="5ff3" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">它做得非常好。:)</p><p id="76ee" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">参考</p><div class="ou ov gp gr ow ox"><a href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd ir gy z fp pc fr fs pd fu fw ip bi translated">Softmax 函数</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">在数学中，softmax 函数，也称为 softargmax 或归一化指数函数，[2] :198 是一个…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">en.wikipedia.org</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl km ox"/></div></div></a></div><p id="816c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">数据集:</p><div class="ou ov gp gr ow ox"><a href="https://www.kaggle.com/rishianand/devanagari-character-set" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd ir gy z fp pc fr fs pd fu fw ip bi translated">梵文字符集</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">www.kaggle.com</p></div></div></div></a></div></div></div>    
</body>
</html>