# ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹é‡æ¸©ç¥ç»ç½‘ç»œåå‘ä¼ æ’­

> åŸæ–‡ï¼š<https://towardsdatascience.com/neural-network-back-propagation-revisited-892f42320d31?source=collection_archive---------30----------------------->

## é€šè¿‡ä½¿ç”¨å¾®åˆ†æ–¹ç¨‹çš„æ•°å€¼è§£æ¥ä¼˜åŒ–ç¥ç»ç½‘ç»œå‚æ•°è¢«è¯„è®ºä¸ºåœ¨åå‘ä¼ æ’­æœŸé—´æ”¶æ•›åˆ°æˆæœ¬å‡½æ•°çš„å…¨å±€æœ€å°å€¼çš„æ›¿ä»£æ–¹æ³•ã€‚

![](img/c6ec20850fae28750da79819213219e8.png)![](img/38e0221f3b359d29ba90fdf3b6028c92.png)

Different optimizers at work to find the global minimum of two different functions. The â€œODE Solverâ€ optimizer will be the subject of this post. If you focus on the white dot, you can see that the ODE solver reaches the minimum with fewer iterations in the two cases shown here. The animation on the left is the [six-hump camel function](http://www.sfu.ca/~ssurjano/camel6.html) with a global minimum at (0,0). The animation on the right is the [Goldstein-Price function](http://www.sfu.ca/~ssurjano/goldpr.html) where the global minimum at (0,-1) is only slightly deeper than the many other local minima. The animations have been created by adapting the code by Piotr Skalski. The code to reproduce the animation above can be found [here](https://gist.github.com/alessiot/aca064b64ff416f75a4d30e08b405c37).

è¿™ç¯‡æ–‡ç« çš„é‡ç‚¹æ˜¯ä»‹ç»ä¸€ç§ä¸åŒçš„æ–¹æ³•æ¥ä¼˜åŒ–ç¥ç»ç½‘ç»œçš„å‚æ•°(*åˆå*æƒé‡)ï¼ŒåŒæ—¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åå‘ä¼ æ’­æŸå¤±æ¢¯åº¦ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹(ODE)æ•°å€¼è§£ç®—å™¨æ¥å¯»æ‰¾æœ€å°åŒ–æ¢¯åº¦çš„æƒé‡ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ç”±[äºšä¼¦Â·æ•–æ–‡æ€ç­‰äºº](https://www.semanticscholar.org/paper/Efficient-training-of-the-backpropagation-network-a-Owens-Filkin/3ed4de93b828a2350489aaa40de382e3fec45e68?citingPapersSort=is-influential#citing-papers)åœ¨ 1989 å¹´é¦–æ¬¡æå‡ºçš„ã€‚è™½ç„¶ä½¿ç”¨æ•°å€¼è§£ç®—å™¨å¢åŠ äº†å¤§é‡çš„è®¡ç®—æ—¶é—´ï¼Œä½†æ˜¯è¿™ç§æ–¹æ³•å¯¹äºç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†å’Œ/æˆ–æ”¶æ•›åˆ°ç¨³å®šè§£éœ€è¦é¢å¤–åŠªåŠ›æ¥å¾®è°ƒæ›´â€œæ ‡å‡†â€ä¼˜åŒ–å™¨çš„è¶…å‚æ•°çš„æƒ…å†µæ˜¯æœ‰æ„ä¹‰çš„ã€‚äº‹å®ä¸Šï¼Œæ•°å€¼æ±‚è§£å™¨ä¸éœ€è¦ä»»ä½•è¶…å‚æ•°æ¥è°ƒæ•´ã€‚

æœ€è¿‘ï¼Œåœ¨ç¥ç»ç½‘ç»œå»ºæ¨¡ä¸­ä½¿ç”¨ ODE æ±‚è§£å™¨å·²ç»è¢«ç”¨æ¥é‡æ–°è®¾è®¡æ·±åº¦å­¦ä¹ å¯ä»¥å¯¹å¥åº·å˜åŒ–ç­‰è¿ç»­è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡çš„æ–¹å¼(è§[æ­¤å¤„](https://arxiv.org/abs/1806.07366))ã€‚

åœ¨ä»‹ç»äº†ç¥ç»ç½‘ç»œä¹‹åï¼Œæˆ‘å°†æè¿°å¦‚ä½•å¯¹åå‘ä¼ æ’­æ­¥éª¤è¿›è¡Œæ›´æ”¹ï¼Œä»¥ä¾¿ä½¿ç”¨ ODE è§£ç®—å™¨å’Œæ ‡å‡†ä¼˜åŒ–å™¨åœ¨ TensorFlow ä¸­è®­ç»ƒç¥ç»ç½‘ç»œã€‚

# ç¥ç»ç½‘ç»œåŸºç¡€ã€‚

ç¥ç»ç½‘ç»œæ˜¯ç”Ÿç‰©å¯å‘çš„æ•°å­¦æ¨¡å‹ï¼Œå¯ä»¥åœ¨è®¡ç®—æœºä¸Šç¼–ç¨‹ï¼Œä»¥å‘ç°å’Œå­¦ä¹ è§‚å¯Ÿæ•°æ®ä¸­çš„å¤æ‚å…³ç³»ã€‚å…³äºç¥ç»ç½‘ç»œèƒŒåçš„æ•°å­¦çš„è¯¦ç»†å›é¡¾ï¼Œè¯»è€…å¯ä»¥å‚è€ƒè¿ˆå…‹å°”Â·å°¼å°”æ£®çš„åœ¨çº¿ä¹¦ç±ã€‚èŠ‚ç‚¹çš„è¾“å…¥å±‚å’Œè¾“å‡ºå±‚åˆ†åˆ«è¡¨ç¤ºç‰¹å¾å’Œç»“æœã€‚ä¸€ä¸ªæˆ–å¤šä¸ªä¸­é—´å±‚(ç§°ä¸ºéšè—å±‚)çš„åºåˆ—é€šå¸¸ç”¨äºæ•è·åˆå§‹è¾“å…¥ç‰¹å¾çš„è¶Šæ¥è¶Šå¤æ‚çš„è¡¨ç¤ºã€‚

![](img/8cb1ecbb564b7b568a67b5522f1cee7b.png)

An example representation of neural network architecture with two input nodes and a single output node created with the [VisualizeNN](https://github.com/imranq/visualizeNN) module. Other online tools exist to create publication-ready images (see for example [NN-SVG](http://alexlenail.me/NN-SVG/index.html)).

åˆ©ç”¨è¿™ç§æ¶æ„è¿›è¡Œå­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µæ˜¯å¯è°ƒæƒé‡ï¼Œå…¶è¢«è¡¨ç¤ºä¸ºç½‘ç»œèŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥çº¿ã€‚ç¥ç»ç½‘ç»œä¹Ÿè¢«ç§°ä¸ºå¤šå±‚æ„ŸçŸ¥å™¨ç½‘ç»œ(MLP)ï¼Œå› ä¸ºä¸­é—´å±‚å……å½“*æ„ŸçŸ¥å™¨*ï¼Œå³ä¼ é€’æ¥è‡ªå‰ä¸€å±‚çš„ä¼ å…¥å€¼æˆ–é˜»æ­¢å®ƒçš„æœ¬åœ°åˆ†ç±»å™¨ã€‚

ç¥ç»ç½‘ç»œçš„è®­ç»ƒåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼Œå‰å‘å’Œåå‘ä¼ æ’­ï¼Œè¿™ä¸¤ä¸ªé˜¶æ®µé‡å¤å‡ æ¬¡ï¼Œç§°ä¸ºæ—¶æœŸã€‚åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè¾“å…¥æ•°æ®å‘ˆç°ç»™ç½‘ç»œï¼Œå¹¶é€šè¿‡å…¶å„å±‚è¿›è¡Œè½¬æ¢ã€‚åœ¨ä»»ä½•ç»™å®šçš„å±‚ *lï¼Œ*ä»å‰ä¸€å±‚ğ‘™âˆ’1 æ‰‡å‡ºçš„è¾“å…¥å€¼ğ´ä½¿ç”¨å°†å±‚ğ‘™âˆ’1 è¿æ¥åˆ°ğ‘™çš„æƒé‡( *W* )å’Œå±‚ğ‘™çš„åç½®( *b* )è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå¦‚ä¸‹

![](img/09a41549684c266aed01a8eafa5de4a0.png)

ğ‘æ˜¯ğ‘™.å›¾å±‚èŠ‚ç‚¹å¤„çš„å€¼çš„å‘é‡ä½¿ç”¨å¦‚ä¸‹æ¿€æ´»å‡½æ•°å°†éçº¿æ€§å˜æ¢åº”ç”¨äºğ‘

![](img/29e816d928d1891f84f330629b49f24e.png)

åœ¨åå‘ä¼ æ’­æœŸé—´ï¼Œä¸ºæ¯ä¸€å±‚ğ‘™.è®¡ç®—æˆæœ¬ğ¶ç›¸å¯¹äºç½‘ç»œæƒé‡çš„æ¢¯åº¦å¯¹äºğ‘™å±‚æ¥è¯´ï¼Œ

![](img/6b1a819c232db44e8b3e75659438b451.png)

åœ¨å“ªé‡Œ

![](img/2e482d1a7866a6eb7e1812629b841d6f.png)

çŸ¥é“è¿™äº›ç­‰å¼å°±è¶³ä»¥ä½¿ç”¨ *numpy ä»å¤´å¼€å§‹ç¼–å†™ç¥ç»ç½‘ç»œç¨‹åºã€‚*è¦æŸ¥çœ‹ï¼Œå¯ä»¥å‚è€ƒ Piotr Skalski å†™çš„[ä»£ç ](https://github.com/SkalskiP/ILearnDeepLearning.py)ï¼Œå¹¶åœ¨ä»–å…³äºä»‹è´¨çš„[æ–‡ç« ](/lets-code-a-neural-network-in-plain-numpy-ae7e74410795)ä¸­æŸ¥çœ‹ã€‚

# ç”¨ sklearn è®­ç»ƒ MLP ç½‘ç»œã€‚

åœ¨ *sklearn* ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•[ç›‘ç£æœºå™¨å­¦ä¹ æŠ€æœ¯](https://scikit-learn.org/stable/supervised_learning.html)ä¸­ä½¿ç”¨çš„å…¸å‹æ­¥éª¤æ¥è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘åœ¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µä¸­ä½¿ç”¨äº†å¼‚æˆ–(XOR)çœŸå€¼è¡¨ã€‚

# å¸¦ ODE è§£ç®—å™¨çš„å¼ é‡æµã€‚

å¸¸å¾®åˆ†æ–¹ç¨‹(ODE)å¯ä»¥ç”¨æ•°å€¼æ–¹æ³•æ±‚è§£ã€‚äº‹å®ä¸Šï¼Œå‰ä¸€èŠ‚ä»‹ç»çš„åå‘ä¼ æ’­æ–¹ç¨‹å¯ä»¥å†™æˆ

![](img/097d07c500a85dc55874c7fb91c7751b.png)

å¹¶ä¸”æ˜¯å¯ä»¥ç”¨æ•°å­—æ±‚è§£[çš„é¢‚æ­Œã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™ç§ ODE åœ¨æ•°å€¼ä¸Šæ˜¯ä¸¥æ ¼çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨ç§¯åˆ†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¸ç¨³å®šæ€§æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚](https://www.semanticscholar.org/paper/Efficient-training-of-the-backpropagation-network-a-Owens-Filkin/3ed4de93b828a2350489aaa40de382e3fec45e68?citingPapersSort=is-influential#citing-papers)

åˆšæ€§å¸¸å¾®åˆ†æ–¹ç¨‹çš„è§£è¦æ±‚ç§¯åˆ†å™¨çš„æ­¥é•¿éå¸¸å°ï¼Œå¹¶ä¸”å¯ä»¥éšæ—¶é—´å˜åŒ–ã€‚åœ¨ Python ä¸­ï¼Œ [ODE è§£ç®—å™¨](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.ode.html#scipy.integrate.ode)åœ¨ *scipy* åº“ä¸­å®ç°ã€‚ä¸ºäº†ä½¿ç”¨ ODE ç§¯åˆ†å™¨æ‰¾åˆ°æœ€ä½³æƒé‡ï¼Œæˆ‘ä»¬å°†å˜é‡ *t* å¼•å…¥åå‘ä¼ æ’­æ–¹ç¨‹ï¼Œè¿™å¯¹åº”äºæ”¹å˜ç§¯åˆ†å™¨çš„æ­¥é•¿ä»¥è¾¾åˆ°ç¨³å®šçš„æ•°å€¼è§£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„ç­‰å¼

![](img/a0ad84832c1b60ab1fae4da93212f812.png)

æœ¬èŠ‚æœ«å°¾çš„ç¬”è®°æœ¬æ›´è¯¦ç»†åœ°è§£é‡Šäº†ä¸ºäº†ç”¨ ODE æ±‚è§£å™¨ä¼˜åŒ–å¼ é‡æµç¥ç»ç½‘ç»œè€Œè¿›è¡Œçš„ä¿®æ”¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦åœ¨â€œæ¸´æœ›â€æ¨¡å¼ä¸‹è¿è¡Œ TensorFlowï¼Œå¦‚è¿™é‡Œçš„[æ‰€ç¤º](https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough)ã€‚å¯¹äºè¿™ä¸ªç»ƒä¹ ï¼Œæˆ‘ä» MNIST æ•°æ®é›†ä¸­æå–äº†ä¸¤ä¸ªéšæœºæ ·æœ¬ï¼Œå…¶ä¸­ä¸€ä¸ªç”¨äºè®­ç»ƒï¼Œå¦ä¸€ä¸ªç”¨äºéªŒè¯ï¼Œå³è°ƒæ•´ç½‘ç»œå‚æ•°ã€‚ä¸¤ä¸ªæ ·æœ¬éƒ½åŒ…å« 0 åˆ° 4 ä¹‹é—´çš„æ•°å­—çš„æ•°æ®ï¼Œæ¯ä¸ªæ•°å­—ç”¨ 100 ä¸ªæ ·æœ¬æ¥è¡¨ç¤ºã€‚å½“åœ¨ 50 æ¬¡è¿­ä»£ä¹‹åæ²¡æœ‰è§‚å¯Ÿåˆ°å¤§äº 5%çš„æ”¹è¿›æ—¶ï¼Œåœæ­¢è®­ç»ƒï¼Œå› ä¸ºåœ¨éªŒè¯é›†ä¸Šæœ‰æœ€ä½³æ€§èƒ½ã€‚è¯¥æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½æ˜¯æ ¹æ®æ•°å­— 0-4 çš„å…¨éƒ¨ MNIST æ•°æ®(35ï¼Œ000 ä¸ªæ ·æœ¬)è®¡ç®—çš„ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

![](img/1c55bcfbd3ed23a11c7bdf80a345cb4c.png)![](img/09aa58860dea5009f3fe65b8affdfc85.png)

Accuracy of the TensorFlow neural network model on the whole datasets of digits between 0 and 4\. (Left) Fewer iterations or presentations of the data to the network are needed to reach optimal performance. (Right) The use of a ODE solver requires significantly greater computation time.

ç”¨ ODE æ±‚è§£å™¨ä¼˜åŒ–ç¥ç»ç½‘ç»œæ¯”ç”¨æ ‡å‡†ä¼˜åŒ–å™¨ä¼˜åŒ–è¦å¤šèŠ± 8 å€çš„æ—¶é—´ã€‚ç„¶è€Œï¼ŒODE æ±‚è§£å™¨çš„ç²¾åº¦å¹¶ä¸æ¯”å…¶ä»–ä¼˜åŒ–å™¨çš„ç²¾åº¦å·®ã€‚

# ç»“è®ºã€‚

ä½¿ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹(ODE)æ•°å€¼è§£ç®—å™¨å·²è¢«è§†ä¸ºä¼˜åŒ–ç¥ç»ç½‘ç»œå‚æ•°çš„æ›¿ä»£æ–¹æ³•ã€‚è™½ç„¶è¿™æ˜¾è‘—å¢åŠ äº†è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹çš„è®¡ç®—æ—¶é—´ï¼Œä½†æ˜¯å®ƒä¸éœ€è¦ä»»ä½•è¶…å‚æ•°è°ƒæ•´ï¼Œå¹¶ä¸”å¯ä»¥æˆä¸ºæ”¶æ•›åˆ°æˆæœ¬å‡½æ•°å…¨å±€æœ€å°å€¼è€Œä¸èŠ±è´¹æ—¶é—´å¾®è°ƒè¶…å‚æ•°çš„æ›¿ä»£æ–¹å¼ã€‚