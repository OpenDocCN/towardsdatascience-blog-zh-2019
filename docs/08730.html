<html>
<head>
<title>How to run a Spark application from an EC2 Instance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从 EC2 实例运行 Spark 应用程序</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-run-a-spark-application-from-an-ec2-instance-a4584d4d490d?source=collection_archive---------12-----------------------#2019-11-23">https://towardsdatascience.com/how-to-run-a-spark-application-from-an-ec2-instance-a4584d4d490d?source=collection_archive---------12-----------------------#2019-11-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="cddc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么你会这样做，而不是使用电子病历？好吧，问得好。在某些情况下，使用 EC2 可能比使用 EMR 更便宜，但在其他情况下，EMR 可能是可取的。无论如何，下面是如何从 EC2 实例运行 Spark 应用程序:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/4e17b890d11dfb66dba552e5028424d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rh364YJasVkX_ra53HjE9A.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Photo by <a class="ae lb" href="https://unsplash.com/@steverichey?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Steve Richey</a> on <a class="ae lb" href="https://unsplash.com/s/photos/web?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f449" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用了一个深度学习 AMI (Ubuntu 16.04)版本 25.3，带有一个 p3 实例，<a class="ae lb" href="https://aws.amazon.com/ec2/instance-types/#instance-type-matrix" rel="noopener ugc nofollow" target="_blank">用于加速计算。</a></p><h2 id="f00b" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">SSH 到您的 EC2 实例。</h2><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="00a9" class="lc ld iq lw b gy ma mb l mc md">ssh -i pem_key.pem ubuntu@public_dns_key</span></pre><h2 id="a618" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">从一开始，你就安装了一些东西。</h2><p id="e800" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">您在 EC2 终端中键入:</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="e4ee" class="lc ld iq lw b gy ma mb l mc md">java -version</span></pre><p id="3cf3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它返回:</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="dd50" class="lc ld iq lw b gy ma mb l mc md">openjdk version “1.8.0_222”</span><span id="f2b2" class="lc ld iq lw b gy mj mb l mc md">OpenJDK Runtime Environment (build 1.8.0_222–8u222-b10–1ubuntu1~16.04.1-b10)</span><span id="2105" class="lc ld iq lw b gy mj mb l mc md">OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)</span></pre><p id="96e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Java 8 是我们希望 Spark 运行的，所以这很好。</p><p id="e024" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的应用程序是使用 python 编写的，所以我想检查它是否已安装。</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="aee0" class="lc ld iq lw b gy ma mb l mc md">python --version</span></pre><p id="311d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它返回:</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="67e8" class="lc ld iq lw b gy ma mb l mc md">Python 3.6.6 :: Anaconda, Inc.</span></pre><p id="5c95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太好了！</p><h2 id="d4ae" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">现在你需要安装 Hadoop。</h2><p id="d7d7" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">我使用了以下准则:</p><p id="d6c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="https://datawookie.netlify.com/blog/2017/07/installing-hadoop-on-ubuntu/" rel="noopener ugc nofollow" target="_blank">https://data wookie . netlify . com/blog/2017/07/installing-Hadoop-on-Ubuntu/</a></p><ol class=""><li id="d21b" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mp mq mr ms bi translated">去<a class="ae lb" href="http://Spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank"> Spark 下载网站</a>看看它用的是哪个版本的 Hadoop:</li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/7a41dd4b9f16ea7e63ceada1707da324.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*OOM1hrnnCd8OvQHSvZYeOg.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">It uses Hadoop 2.7, as of November 2019, this may be different for you.</figcaption></figure><p id="7943" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.它使用 Hadoop 2.7。好了，现在转到<a class="ae lb" href="http://apache.mirrors.ionfish.org/hadoop/common/hadoop-2.7.7/" rel="noopener ugc nofollow" target="_blank"> Hadoop 镜像站点</a>并使用 wget</p><ul class=""><li id="c389" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mu mq mr ms bi translated">右键单击，将链接复制到 Hadoop-2.7.7.tar.gz</li><li id="7d26" class="mk ml iq jp b jq mv ju mw jy mx kc my kg mz kk mu mq mr ms bi translated">输入你的 ubuntu 终端(粘贴你刚刚复制的，我加粗是为了让你知道你的可能不一样):</li></ul><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="cabc" class="lc ld iq lw b gy ma mb l mc md">wget <a class="ae lb" href="http://apache.mirrors.ionfish.org/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz" rel="noopener ugc nofollow" target="_blank"><strong class="lw ir">http://apache.mirrors.ionfish.org/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz</strong></a></span></pre><p id="47b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.打开压缩的内容</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="3a1f" class="lc ld iq lw b gy ma mb l mc md">tar -xvf hadoop-2.7.7.tar.gz</span></pre><p id="392c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.找到 java 的位置</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="bc6d" class="lc ld iq lw b gy ma mb l mc md">type -p javac|xargs readlink -f|xargs dirname|xargs dirname</span></pre><p id="6e7a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它返回:</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="0a9c" class="lc ld iq lw b gy ma mb l mc md">/usr/lib/jvm/java-8-openjdk-amd64</span></pre><p id="ac15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好的，复制你的输出^</p><p id="3b85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">5.好了，现在编辑 Hadoop 配置文件，这样它就可以与 java 交互了</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="e11e" class="lc ld iq lw b gy ma mb l mc md">vi hadoop-2.7.7/etc/hadoop/hadoop-env.sh</span></pre><p id="44a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">键入<code class="fe na nb nc lw b">i</code>通过粘贴您复制的输出来插入和更新 JAVA_HOME 变量</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nd"><img src="../Images/cb3e0755a0e50b711883b7e5a6d79df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H07na9wUzVoKAtAfXM7oiQ.png"/></div></div></figure><p id="0f44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要退出 vim，使用 ESC + :wq！(wq 代表 write 和 quit，解释点就是强制)</p><p id="33f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">6.设置 HADOOP_HOME 和 JAVA_HOME 环境变量。<a class="ae lb" href="https://stackoverflow.com/questions/5102022/what-is-the-reason-for-the-existence-of-the-java-home-environment-variable" rel="noopener ugc nofollow" target="_blank">JAVA _ HOME 环境变量指向计算机上安装 JAVA 运行时环境(JRE)的目录。目的是指向 Java 安装的位置。</a></p><p id="147a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以通过使用</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="2b85" class="lc ld iq lw b gy ma mb l mc md">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><span id="9a99" class="lc ld iq lw b gy mj mb l mc md">export HADOOP_HOME=/home/ubuntu/hadoop-2.7.7</span></pre><p id="abbc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并更新您的路径:</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="cb18" class="lc ld iq lw b gy ma mb l mc md">export PATH=$PATH:$HADOOP_HOME/bin/</span></pre><p id="957f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是如果关闭 EC2 实例，这可能无法保存。所以我使用 vim 并把这些导出添加到我的。bashrc 文件。</p><p id="ff58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不及物动词 bashrc，I 表示插入，然后:wq 表示完成。</p><p id="6f93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PS:这是你如何删除路径中的副本，你不需要这样做，它们不会伤害任何东西，但如果你想知道:</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="563f" class="lc ld iq lw b gy ma mb l mc md">if [ -n "$PATH" ]; then<br/>    old_PATH=$PATH:; PATH=<br/>    while [ -n "$old_PATH" ]; do<br/>        x=${old_PATH%%:*}       # the first remaining entry<br/>        case $PATH: in<br/>            *:"$x":*) ;;          # already there<br/>            *) PATH=$PATH:$x;;    # not there yet<br/>        esac<br/>        old_PATH=${old_PATH#*:}<br/>    done<br/>    PATH=${PATH#:}<br/>    unset old_PATH x<br/>fi</span></pre><p id="ab8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">来源:<a class="ae lb" href="https://unix.stackexchange.com/questions/40749/remove-duplicate-path-entries-with-awk-command" rel="noopener ugc nofollow" target="_blank">https://UNIX . stack exchange . com/questions/40749/remove-duplicate-path-entries-with-awk-command</a></p><p id="17c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">7.找到你的。bashrc 文件，在您的主目录中键入<code class="fe na nb nc lw b">source .bashrc</code></p><p id="1769" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">8.现在要检查版本，您可以回到您的主目录并键入<br/> hadoop 版本，它应该会告诉您</p><p id="1ced" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">9.您可以从您的主目录中删除压缩的 tar 文件:<code class="fe na nb nc lw b">rm hadoop-2.7.7.tar.gz</code></p><h2 id="d699" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">现在你需要安装 Spark。我遵循这些准则:</h2><p id="5be5" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated"><a class="ae lb" href="https://datawookie.netlify.com/blog/2017/07/installing-spark-on-ubuntu/" rel="noopener ugc nofollow" target="_blank">https://data wookie . netlify . com/blog/2017/07/installing-spark-on-Ubuntu/</a></p><p id="7730" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1.转到<a class="ae lb" href="https://www.apache.org/dyn/closer.lua/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">该站点</a>并复制第一个镜像站点的链接地址</p><p id="434a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.在你的 ubuntu 终端中输入 wget 并粘贴复制的链接</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="6618" class="lc ld iq lw b gy ma mb l mc md">wget <a class="ae lb" href="http://mirrors.sonic.net/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">http://mirrors.sonic.net/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</a></span></pre><p id="a3a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.现在解压压缩的 tar 文件</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="bbba" class="lc ld iq lw b gy ma mb l mc md">tar -xvf spark-2.4.4-bin-hadoop2.7.tgz</span></pre><p id="89d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.给你的环境变量添加火花<code class="fe na nb nc lw b">export SPARK_HOME=/home/ubuntu/spark-2.4.4-bin-hadoop2.7</code></p><p id="3838" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">5.安装 scala。为什么我们在这里用简单的方法？因为我们不需要查看 jar 文件或任何东西。</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="16ad" class="lc ld iq lw b gy ma mb l mc md">sudo apt install scala</span></pre><h2 id="626f" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">现在你必须确保 Spark、Hadoop 和 Java 能够协同工作。在我们的情况下，这也包括能够从 S3 读和写。</h2><p id="c22e" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">1.配置 Spark<br/><a class="ae lb" href="https://stackoverflow.com/questions/58415928/spark-s3-error-java-lang-classnotfoundexception-class-org-apache-hadoop-f" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/58415928/Spark-S3-error-Java-lang-classnotfoundexception-class-org-Apache-Hadoop-f</a></p><p id="5fda" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">a.导航到 EC2 终端中的文件夹<code class="fe na nb nc lw b">~/spark-2.4.4-bin-hadoop2.7/conf</code></p><p id="c69f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行代码:</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="82d7" class="lc ld iq lw b gy ma mb l mc md">touch spark_defaults.conf</span><span id="b211" class="lc ld iq lw b gy mj mb l mc md">vi spark_defaults.conf</span></pre><p id="63f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，您需要添加下面几行:<br/>如果 2FA 适用于您:确保您的访问密钥和秘密密钥是用于服务帐户的(不与用户名/密码相关联)</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="27ac" class="lc ld iq lw b gy ma mb l mc md">spark.hadoop.fs.s3a.access.key ***your_access_key***</span><span id="da69" class="lc ld iq lw b gy mj mb l mc md">spark.hadoop.fs.s3a.secret.key ***your_secret_key***</span><span id="f04a" class="lc ld iq lw b gy mj mb l mc md">spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem</span><span id="c013" class="lc ld iq lw b gy mj mb l mc md">spark.driver.extraClassPath /home/ubuntu/spark-2.4.4-bin-hadoop2.7/jars/<strong class="lw ir">hadoop-aws-2.7.3</strong>.<strong class="lw ir">jar</strong>:/home/ubuntu/spark-2.4.4-bin-hadoop2.7/jars/<strong class="lw ir">aws-java-sdk-1.7.4.jar</strong></span></pre><p id="36e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">b.如何确保这些 jar 文件是正确的呢？？我把它们加粗是因为它们对你来说可能不一样。所以要检查，去你的 Hadoop jars。</p><p id="62fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 cd 转到以下文件夹:<code class="fe na nb nc lw b">cd ~/hadoop-2.7.7/share/hadoop/tools/lib</code>并检查哪些罐子用于<code class="fe na nb nc lw b">aws-java-sdk</code>和<code class="fe na nb nc lw b">hadoop-aws</code>，确保这些。jar 文件与您刚刚放入<code class="fe na nb nc lw b">spark_defaults.conf</code>的内容相匹配。</p><p id="4285" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">c.将这些文件复制到 spark jars 文件夹:</p><p id="c245" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了让下面的代码工作，请放在<code class="fe na nb nc lw b">~/hadoop-2.7.7/share/hadoop/tools/lib</code>文件夹中</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="fad1" class="lc ld iq lw b gy ma mb l mc md">cp <strong class="lw ir">hadoop-aws-2.7.3.jar</strong> ~/spark-2.4.4-bin-hadoop2.7/jars/</span><span id="33b6" class="lc ld iq lw b gy mj mb l mc md">cp <strong class="lw ir">aws-java-sdk-1.7.4.jar</strong> ~/spark-2.4.4-bin-hadoop2.7/jars/</span></pre><h2 id="e30b" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">配置 Hadoop</h2><p id="a900" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">a.让你的每个工作节点都可以访问 S3(因为我们的代码从 S3 读写)<br/><a class="ae lb" href="https://github.com/CoorpAcademy/docker-pyspark/issues/13" rel="noopener ugc nofollow" target="_blank">https://github.com/CoorpAcademy/docker-pyspark/issues/13</a></p><p id="bb26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">编辑文件<code class="fe na nb nc lw b">hadoop-2.7.7/etc/hadoop/core-site.xml</code>以包含以下行:</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="3843" class="lc ld iq lw b gy ma mb l mc md">&lt;configuration&gt;<br/>&lt;property&gt;<br/>&lt;name&gt;fs.s3.awsAccessKeyId&lt;/name&gt;<br/>&lt;value&gt;*****&lt;/value&gt;<br/>&lt;/property&gt;</span><span id="1103" class="lc ld iq lw b gy mj mb l mc md">&lt;property&gt;<br/>&lt;name&gt;fs.s3.awsSecretAccessKey&lt;/name&gt;<br/>&lt;value&gt;*****&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="3a3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">b.将下面的 jar 文件复制到<code class="fe na nb nc lw b">hadoop-2.7.7/share/hadoop/common/lib</code>目录</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="24b9" class="lc ld iq lw b gy ma mb l mc md">sudo cp hadoop-2.7.7/share/hadoop/tools/lib/aws-java-sdk-1.7.4.jar hadoop-2.7.7/share/hadoop/common/lib/</span><span id="8b00" class="lc ld iq lw b gy mj mb l mc md">sudo cp hadoop-2.7.7/share/hadoop/tools/lib/hadoop-aws-2.7.5.jar Hadoop-2.7.7/share/hadoop/common/lib/</span></pre><h2 id="9965" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">好了，现在您已经准备好克隆您的存储库了</h2><p id="e421" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">git 克隆路径/到/your/repo.git</p><p id="ded0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">确保将下面一行添加到您的。bashrc 文件:</p><pre class="km kn ko kp gt lv lw lx ly aw lz bi"><span id="9856" class="lc ld iq lw b gy ma mb l mc md">export PYTHONPATH=$PYTHONPATH:/home/ubuntu/repo</span></pre><p id="325b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PYTHONPATH 是一个环境变量，您可以设置它来添加额外的目录，python 将在这些目录中查找模块和包。对于大多数安装，您不应该设置这些变量，因为 Python 运行不需要它们。Python 知道在哪里可以找到它的标准库。</p><p id="5de2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设置 PYTHONPATH 的唯一原因是维护您不想安装在全局默认位置(即 site-packages 目录)的自定义 Python 库的目录。</p><p id="5db2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">来源:<a class="ae lb" href="https://www.tutorialspoint.com/What-is-PYTHONPATH-environment-variable-in-Python" rel="noopener ugc nofollow" target="_blank">https://www . tutorialspoint . com/What-is-Python path-environment-variable-in-Python</a></p><h2 id="7bde" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">现在运行您的代码！</h2><p id="2ae5" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">快乐火花！</p><h2 id="df6f" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">有关在 EC2 中使用 Spark 的更多信息</h2><p id="0007" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">查看<a class="ae lb" href="https://medium.com/snipe-gg" rel="noopener"> Snipe.gg </a>的博客:</p><div class="ne nf gp gr ng nh"><a href="https://medium.com/snipe-gg/running-apache-spark-on-aws-without-busting-the-bank-5566dad18ea3" rel="noopener follow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd ir gy z fp nm fr fs nn fu fw ip bi translated">在 AWS 上运行 Apache Spark，而不会让银行破产</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">在 Snipe，我们处理大量的数据。考虑到有超过 1 亿的活跃联盟…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">medium.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv kv nh"/></div></div></a></div><p id="dfb3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在他们的案例中，他们发现</p><blockquote class="nw nx ny"><p id="0219" class="jn jo nz jp b jq jr js jt ju jv jw jx oa jz ka kb ob kd ke kf oc kh ki kj kk ij bi translated">[……]在检查了<a class="ae lb" href="https://aws.amazon.com/emr/pricing/" rel="noopener ugc nofollow" target="_blank"> EMR 定价</a>后，我们发现 EMR 在其使用的 EC2 实例的价格上增加了高达<strong class="jp ir"> 25% </strong>的定价开销</p></blockquote><p id="074b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个博客也很好地突出了工具和生产我刚刚概述的努力的考虑。</p></div></div>    
</body>
</html>