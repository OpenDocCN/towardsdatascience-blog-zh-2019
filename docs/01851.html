<html>
<head>
<title>Understand Data Normalization in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解机器学习中的数据规范化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0?source=collection_archive---------2-----------------------#2019-03-27">https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0?source=collection_archive---------2-----------------------#2019-03-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4601" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你是数据科学/机器学习的新手，你可能很想知道“特征规范化”这个时髦词的本质和效果。如果您阅读过任何 Kaggle 内核，很可能在数据预处理部分找到了特性规范化。那么，什么是数据规范化，为什么数据从业者如此重视它？</p><h1 id="a140" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">1.定义</h1><p id="2a87" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">有不同类型的数据规范化。假设您有一个数据集<em class="lo"> X </em>，它有<em class="lo"> N </em>行(条目)和<em class="lo"> D </em>列(特征)。<em class="lo"> X[:，i] </em>表示特征<em class="lo"> i </em>，X[j，:]表示条目<em class="lo"> j。我们有:</em></p><p id="826a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Z 规范化(标准化)</strong>:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/b33cff9b4d9d771bc4f2a4ad8b2979d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_xnna2P4MHKDtWHBpaWtfg.png"/></div></div></figure><p id="e484" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我曾经错误地认为这种方法会产生标准的高斯结果。事实上，标准化并没有改变分销的类型:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/3c42aa1d83b87d4d574f6c7bcd1eed74.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*D3xqess5HsJX_BDOica9yQ.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">pdf of standardized data</figcaption></figure><p id="bc48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此转换将数据的平均值设置为 0，标准差设置为 1。在大多数情况下，标准化是按功能使用的</p><p id="976c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">最小-最大归一化:</strong></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/7b8b29c50b029f246656e0c4b0e1df16.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*UtsLSRqJkfARSJ1rpWL5xA.png"/></div></figure><p id="7409" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该方法将数据的范围重新调整为[0，1]。在大多数情况下，标准化也是按功能使用的</p><p id="e765" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">单位矢量归一化:</strong></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/3825c80057b5e5609acf02330f439533.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*_KTacI955XGp0GtZt6AuGg.png"/></div></figure><p id="d08a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">缩放到单位长度将一个向量(一行数据可以看作一个<em class="lo"> D </em>维向量)收缩/拉伸到一个单位球体。当在整个数据集上使用时，转换后的数据可以被可视化为在<em class="lo"> D </em>维单位球上具有不同方向的一束向量。</p><p id="55bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">哇，正常化确实是一个宽泛的术语，它们各有利弊！在本文中，我将只关注标准化，否则这篇文章会太长。</p><h1 id="6e03" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">2.效果</h1><h2 id="94a3" class="mi km iq bd kn mj mk dn kr ml mm dp kv jy mn mo kz kc mp mq ld kg mr ms lh mt bi translated"><strong class="ak">回归</strong></h2><p id="e7ee" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">理论上，回归对标准化不敏感，因为输入数据的任何线性变换都可以通过调整模型参数来抵消。假设我们有一个带有参数<em class="lo"> W </em>和<em class="lo"> B </em>的线性模型:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/6ec7c8276a862f636309ff0bd2bd5ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*uxi4cdxNEhqTTHscFtBncA.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">W is D by 1, B is N by 1</figcaption></figure><p id="2080" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过减去矩阵<em class="lo"> M </em>并乘以对角矩阵<em class="lo"> T </em>可以实现列方式的改变</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a79ca192a079cb843f9121df11eb2c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*gMEzAAk2IpDm_yXx3Tv5jw.png"/></div></figure><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/dc7ee17184fa4d10c904b362f5497526.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*aUzNi9_KSydCDDZ5bCju4g.png"/></div></figure><p id="c7a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">很容易证明，我们可以通过调整<em class="lo"> W </em>和<em class="lo"> B </em>来抵消这种转换</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/bc8ad2d382dec5ea3619e3f7ab064c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*sVneq3kBe8-vpkb3aUUBhw.png"/></div></figure><p id="2464" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，输入数据的标准化不应影响输出或准确性。好吧，如果数学上说标准化在回归中起不了什么作用，为什么它仍然如此受欢迎？</p><p id="21f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">答。标准化提高了模型的数值稳定性</strong></p><p id="884e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们有一个简单的一维数据 X，并使用 MSE 作为损失函数，使用梯度下降的梯度更新是:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi my"><img src="../Images/7d367e8504eee76a47d111367c6a2290.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*x3kGmh3GAgwFai_T3kSW1Q.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Y’ is the prediction</figcaption></figure><p id="0658" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">X 在梯度下降公式中，意味着 X 的值决定了更新速率。所以更大的 X 会导致渐变景观更大的飞跃。同时，给定 Y，较大的 X 导致较小的 W:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/8425413bc748533b6508748b690a648d.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*avoowKUAS5wIiGERLHBuwQ.png"/></div></figure><p id="9495" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当 X 很大时，初始 W(随机选取的)和全局最小值之间的距离很可能很小。因此，当 X 较大时(学习率是固定的)，该算法更可能失败，因为该算法朝着非常接近的目标 W 进行了巨大的跳跃，而需要小步前进。这种超调会让你的亏损振荡或爆发。</p><p id="c660" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实验:</p><p id="f3fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用以下代码人工创建了一个数据集:</p><pre class="lq lr ls lt gt na nb nc nd aw ne bi"><span id="4093" class="mi km iq nb b gy nf ng l nh ni">X = np.arange(-5,5,0.1)<br/>w = 0.5<br/>b = 0.2<br/>Y = X*w+b+(np.random.rand(X.shape[0],)-0.5)#add some noise to Y<br/>X.reshape(100,1)<br/>Y.reshape(100,1)</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0883b5444bacc051ae213e3336b4897d.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*yeoZ9W_OvMvOkvdqtsanBQ.png"/></div></figure><p id="a4f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在实验中使用了一个简单的线性回归代码:</p><pre class="lq lr ls lt gt na nb nc nd aw ne bi"><span id="abd7" class="mi km iq nb b gy nf ng l nh ni">class linear_regression:<br/>    def fit(self, X,Y, lr=0.00001):<br/>        epoch = 0<br/>        loss_log = []<br/>        while (epoch&lt;8000):<br/>            pred = self.predict(X)<br/>            l = self.loss(pred,Y)# get loss<br/>            loss_log.append(l)<br/>            delta_w = 2*np.matmul((Y-pred).T, X) # gradient of w<br/>            delta_b = sum(2*(Y-pred))<br/>            self.w = self.w + lr*delta_w<br/>            self.b = self.b + lr*delta_b<br/>            epoch = epoch+1<br/>        plt.plot(loss_log)<br/>        plt.ylim([0.07,0.1])<br/>        print (loss_log[-5:-1])<br/>    def loss(self, pred,Y):<br/>        error = Y-pred<br/>        return sum(error**2)/error.shape[0] # MSE<br/>    def predict(self, X):<br/>        return X*self.w+self.b# n by 1<br/>    def __init__(self):# initial parameters<br/>        self.w=0.5# fixed initialization for comparison convenience<br/>        self.b=0.5</span></pre><p id="1dda" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下表显示了不同学习率和特征范围的最终 MSE。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nk"><img src="../Images/9bc109280c21c69a1e0dccd78419ca2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ESS31lvM5yzNxKajqopgWQ.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">final loss (MSE) after 8000 iterations</figcaption></figure><p id="de34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当 X 具有较大的系数(因此范围较大)时，模型需要较小的学习速率来保持稳定性。</p><p id="7a41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> b)。标准化可能会加快培训过程</strong></p><p id="a558" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一个“定理”的推论是，如果不同的特征具有完全不同的范围，则学习率由具有最大范围的特征决定。这带来了标准化的另一个好处:加快了培训过程。</p><p id="6da9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设我们有两个特征:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/83edf0e080462da1bed84c16b1263277.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*yt-duZXUpZJryWh2mhfipw.png"/></div></figure><p id="a3ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由<em class="lo"> X2 </em>确定的学习率对于<em class="lo"> X1 </em>来说并不“伟大”，因为<em class="lo"> X1 </em>对于大得多的学习率来说是可以的。我们正在慢慢地走，而大的跳跃本可以被用来代替，这导致了更长的训练时间。我们可以通过标准化这两个特性来减少训练时间。</p><p id="ada7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实验:</p><p id="f7b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用以下代码人工创建了一个包含两个要素的数据集:</p><pre class="lq lr ls lt gt na nb nc nd aw ne bi"><span id="8ab0" class="mi km iq nb b gy nf ng l nh ni">X1 = np.arange(-5,5,0.1)<br/>X2 = np.arange(-5,5,0.1)<br/>X = np.column_stack((X1,X2))<br/>W = np.array([[0.1],[0.2]])<br/>b = 0.1<br/>Y = np.matmul(X,W)+b+(np.random.rand(X.shape[0],1) -0.5)<br/># add some noise to labels</span></pre><p id="7ba1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用稍微修改的线性回归代码:</p><pre class="lq lr ls lt gt na nb nc nd aw ne bi"><span id="dfba" class="mi km iq nb b gy nf ng l nh ni">class linear_regression:<br/>    def fit(self, X,Y, lr=1e-4):<br/>        epoch = 0<br/>        loss_log = []<br/>        while (epoch&lt;80000):<br/>            pred = self.predict(X)<br/>            l = self.loss(pred,Y)# get loss<br/>            loss_log.append(l)<br/>            if l &lt; 0.0792:<br/>                print (epoch)<br/>                break<br/>            delta_w = 2*np.matmul((Y-pred).T, X) # gradient of w<br/>            delta_b = sum(2*(Y-pred))<br/>            self.w = self.w + lr*delta_w.T<br/>            self.b = self.b + lr*delta_b<br/>            epoch = epoch+1<br/>        plt.plot(loss_log)<br/>        plt.ylim([0.07,0.1])<br/>        print (loss_log[-5:-1])<br/>    def loss(self, pred,Y):<br/>        error = Y-pred<br/>        return sum(error**2)/error.shape[0] # MSE<br/>    def predict(self, X):<br/>        return np.matmul(X,self.w)+self.b# n by 1<br/>    def __init__(self):<br/>        self.w=np.array([[0.5],[0.5]])# for comparison convenience<br/>        self.b=0.3</span></pre><p id="3b9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下表显示了对于给定的系数和学习率，达到全局最小值所需的迭代次数:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nm"><img src="../Images/4ba70eead65e499b54c9995241aa787a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*68sNmDKAU6SZEQ2r3bpsQA.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">iterations to reach the global minimum</figcaption></figure><p id="c206" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，当数据没有标准化时，模型需要更多的迭代(训练时间)才能完成。</p><h2 id="d699" class="mi km iq bd kn mj mk dn kr ml mm dp kv jy mn mo kz kc mp mq ld kg mr ms lh mt bi translated"><strong class="ak">基于距离的算法</strong></h2><p id="f457" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">K-mean 和 K-NN 等基于距离的聚类算法很有可能受到标准化的影响。聚类算法需要计算条目之间的距离。最常用的距离是欧几里德距离:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/08d4a24d3f31d049e82bf6f61af3b9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*3SBtPlZOn5bv4VrTPfxsMw.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">distance between i and j</figcaption></figure><p id="ad72" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">很明显，特征缩放将改变节点之间的数值距离。</p><p id="fa6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实验:</p><p id="4599" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用以下代码创建数据集:</p><pre class="lq lr ls lt gt na nb nc nd aw ne bi"><span id="97b4" class="mi km iq nb b gy nf ng l nh ni">def random_2D_data(x,y,size):<br/>    x = (np.random.randn(size)/3.5)+x<br/>    y = (np.random.randn(size)*3.5)+y<br/>    return x,y<br/>x1,y1 = random_2D_data(2,20,50)<br/>x2,y2 = random_2D_data(2,-20,50)<br/>x3,y3 = random_2D_data(-2,20,50)<br/>x4,y4 = random_2D_data(-2,-20,50)<br/>x = np.concatenate((x1,x2,x3,x4))<br/>y = np.concatenate((y1,y2,y3,y4))</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi no"><img src="../Images/584e7af9b69a4a06e241abd8b288c49a.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*RJ6Ea0JRvprsC2lSsxOLlQ.png"/></div></figure><p id="1f1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 K 均值对数据进行聚类:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c584038e6135796b6381d4fc79bb5e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*DYz6wscRMcUs2mHpG-mfew.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">clustering without any feature scaling</figcaption></figure><p id="b5f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，对标准化数据做同样的事情会产生完全不同的结果:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/7f1ce47825097e518ef7bf07764623ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*5ZHLF5UK2ZcXnMjZf3CjRA.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">affected result</figcaption></figure><p id="c222" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太好了，我们有互相矛盾的结果。我应该选择哪个结果？</p><p id="ed7d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">标准化为每个功能提供了“平等”的考虑。</strong></p><p id="5767" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，X 具有两个特征 x1 和 x2</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/56cb36e68d32c1c6ce85c295300a4a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*xuREwFGQxZVgtYqMAWYBQw.png"/></div></figure><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a69ac59036cae2efdfdfb2fcb3d6f3fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*sEj1wwlMF5-9wgXUjX719g.png"/></div></figure><p id="76e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果直接计算欧氏距离，节点 1 和 2 会比节点 1 和 3 离得更远。然而，节点 3 完全不同于 1，而节点 2 和 1 仅在特征 1 (6%)上不同，并且共享相同的特征 2。这是因为特征 1 是“VIP”特征，以其大数值控制结果。</p><p id="5b68" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，如果我们不知道哪些特征是“钻石”特征，哪些是“珊瑚”特征，那么使用标准化来平等地考虑它们是一种很好的做法。</p><h2 id="e5dd" class="mi km iq bd kn mj mk dn kr ml mm dp kv jy mn mo kz kc mp mq ld kg mr ms lh mt bi translated"><strong class="ak">基于树的决策算法</strong></h2><p id="c2c7" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">决策树或随机森林等基于树的决策算法会在每个特征中寻找最佳分割点。分割点由使用特征正确分类的标注的百分比决定，该百分比对特征缩放具有弹性。因此，标准化对这种类型的 ML 模型没有任何重大影响。</p><h2 id="d1bc" class="mi km iq bd kn mj mk dn kr ml mm dp kv jy mn mo kz kc mp mq ld kg mr ms lh mt bi translated"><strong class="ak">神经网络</strong></h2><p id="d37c" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">神经网络可以像回归一样抵消标准化。因此，理论上，数据标准化不应该影响神经网络的性能。然而，经验证据表明，数据标准化在准确性方面是有益的[1]。目前，我还没有看到原因，但可能与梯度下降有关。</p><p id="863d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">很容易理解为什么标准化会提高训练时间。大输入值使激活函数饱和，如 sigmoid 或 ReLu(负输入)。这些类型的激活函数在饱和区域反馈很小的梯度或者根本没有梯度，因此减慢了训练。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nt"><img src="../Images/953922cf2d6343bdb377fa9f3a2e0e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PSF48-8XAw9FRYBtu3b8sg.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">large input kills backward gradient flow</figcaption></figure><h1 id="4408" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">3.摘要</h1><p id="6cdf" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">标准化在许多情况下是有益的。它提高了模型的数值稳定性，并且通常减少了训练时间。然而，标准化并不总是好的。假设特征的重要性相等会损害基于距离的聚类算法的性能。如果特性之间存在固有的重要性差异，那么进行标准化通常不是一个好主意。</p><p id="a16d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参考:</p><p id="fd63" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[1]: Shanker M，Hu MY，Hung MS，<em class="lo">数据标准化对神经网络训练的影响，</em><a class="ae nu" href="https://www.sciencedirect.com/science/article/pii/0305048396000102" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/science/article/pii/0305048396000102</a></p></div></div>    
</body>
</html>