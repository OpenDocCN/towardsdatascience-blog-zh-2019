<html>
<head>
<title>Representational Similarity Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">代表性相似性分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/representational-similarity-analysis-f2252291b393?source=collection_archive---------17-----------------------#2019-06-16">https://towardsdatascience.com/representational-similarity-analysis-f2252291b393?source=collection_archive---------17-----------------------#2019-06-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="560b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从神经科学到深度学习…然后再回来</h2></div><p id="0d21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">TL；博士:</em>在今天的博客文章中，我们讨论了表征相似性分析(RSA)，它可能如何改善我们对大脑的理解，以及 Samy Bengio 和 Geoffrey Hinton 小组最近系统研究深度学习架构中的表征的努力。所以让我们开始吧！</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="3a3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大脑以分布式和层次化的方式处理感觉信息。例如，视觉皮层(神经科学中研究最多的对象)顺序提取低到高水平的特征。光感受器通过双极+神经节细胞投射到外侧膝状体核(LGN)。从那里开始一连串的计算阶段。在腹侧(“什么”vs 背侧——“如何”/“在哪里”)视觉流的不同阶段(V1 → V2 → V4 → IT)，活动模式变得越来越趋向于物体识别的任务。虽然 V1 的神经元调谐主要与粗糙的边缘和线条有关，但它展示了更抽象的概念表征能力。这种调制层次对计算机视觉领域和卷积神经网络(CNN)的发展是一个很大的启发。</p><p id="9c4a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，在神经科学中，空间滤波器组模型已经有很长的历史(Sobel 等。)已经被用于研究视觉皮层的激活模式。直到最近，这些都是视觉感知的最先进的模型。这主要是因为<strong class="kk iu">计算模型必须以某种方式与大脑记录相比较。</strong>因此，研究的模型空间受到严重限制。<strong class="kk iu">回车:RSA </strong>。RSA 最初是由 Kriegeskorte 等人(2008) 引入的，目的是将认知和计算神经科学社区聚集在一起。它提供了一个简单的框架来比较不同的激活模式(不一定在视觉皮层；见下图)。更具体地，可以在不同条件(例如，猫和卡车的刺激呈现)之间比较基于 fMRI 体素的 GLM 估计或多单位记录。然后，这些激活测量被表示为向量，并且我们可以计算在不同条件下这些向量之间的距离测量。这可以对许多不同的刺激进行，每对刺激允许我们填充所谓的表征相异矩阵的一个条目。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/b7de8451f5cd5dd34530059bb6f6e10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ij3Bm6g723Z1EMHb.jpg"/></div></div></figure><p id="4b39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自从 RSA 的最初介绍以来，它已经得到了很多媒体的关注，许多受欢迎的神经科学家，如 James DiCarlo，David Yamins，Niko Kriegeskorte 和 Radek Cichy，一直在将 RSA 与卷积神经网络相结合，以研究腹侧视觉系统。这种方法的优点在于，特征向量的维度无关紧要，因为它被简化为单个距离值，然后在不同模态(即，大脑和模型)之间进行比较。<a class="ae lm" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003963" rel="noopener ugc nofollow" target="_blank"> Cadieu 等人(2014) </a>例如声称，中枢神经网络是腹侧流的最佳模型。为了做到这一点，他们从 ImageNet-pre-trained Alex net 的倒数第二层提取特征，并将这些特征与两只猕猴的多单位记录进行比较。在一次解码练习中，他们发现 AlexNet 的特征比同时记录的 V4 活动具有更强的预测能力。相当惊人的结果。(就预测而言。)另一项由<a class="ae lm" href="https://www.nature.com/articles/srep27755" rel="noopener ugc nofollow" target="_blank"> Cichy et al. (2016) </a>进行的强大研究，结合了 fMRI 和 MEG 来研究视觉处理穿越时间和空间。CNN 不知道时间和组织的概念。一层人工神经元很难被视为与新皮层中的一层神经元相类似。然而，作者发现提取的特征序列反映了测量的空间(fMRI)和时间(MEG)的神经激活模式。</p><p id="8a85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">这些结果之所以惊人，不是因为 CNN 与大脑“如此相似”，而是因为完全相反。通过 backprop 和 SGD 最小化标准成本函数来训练 CNN。卷积是生物学上难以置信的操作，CNN 在训练期间处理数百万个图像阵列。另一方面，大脑通过遗传操作和无监督学习来利用归纳偏差，以便检测自然图像中的模式。不过 backprop + SGD 和几千年的进化似乎也想出了类似的解决方案。但最终，作为研究人员，我们对理解大脑动态和深层结构背后的因果机制感兴趣。RSA 在这方面能帮我们多少？</strong></p><p id="4a12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RSA 中计算的所有度量都是相关的。RDM 条目基于相关距离。R 平方捕捉由模型 RDM 中的变化解释的神经 RDM 的变化。最终，很难解释任何因果关系。声称 CNN 中的信息是视觉皮层如何工作的最佳模型是廉价的。这真的没有太大帮助。CNN 通过反向传播进行训练，并封装了一个巨大的归纳偏差，其形式是在单个处理层中涉及的所有神经元之间共享核权重。但大脑无法实现这些精确的算法细节(可能已经找到了比莱布尼茨的链式法则更聪明的解决方案)。然而，已经有一堆最近的工作(例如，由 Blake Richards、Walter Senn、Tim Lillicrap、Richard Naud 和其他人)来探索神经回路逼近规范梯度驱动的成本函数优化的能力。所以最终，我们可能不会太远。</p><p id="2d27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此之前，我坚信必须将 RSA 与实验干预的科学方法结合起来。正如在经济学中一样，<strong class="kk iu">我们需要借助于受控操纵的准实验因果关系</strong>。这正是最近由<a class="ae lm" href="https://science.sciencemag.org/content/sci/364/6439/eaav9436.full.pdf?casa_token=7zJXfxcBvFUAAAAA:ny4nImzhBu3z8hUnzH-0lFyhw5cA37e_bVWFmTa4czuZkGHufYWKleOUhkCcoTY_MSnKRIApIk4cLxc" rel="noopener ugc nofollow" target="_blank"> Bashivan 等人(2019) </a>和<a class="ae lm" href="https://www.sciencedirect.com/science/article/pii/S0092867419303915" rel="noopener ugc nofollow" target="_blank"> Ponce 等人(2019) </a>所做的两项研究！更具体地说，他们使用基于深度学习的生成程序来生成一组刺激。最终目标是提供一种形式的神经控制(即驱动特定神经位点的放电率)。具体来说，<a class="ae lm" href="https://www.sciencedirect.com/science/article/pii/S0092867419303915" rel="noopener ugc nofollow" target="_blank"> Ponce 等人(2019) </a>展示了如何关闭从生成性对抗网络生成刺激、读出神经活动和改变 GAN 的输入噪声之间的环路，以驱动单个单元以及群体的活动。作者能够识别记录位置的可复制的抽象调谐行为。使用灵活的函数逼近法的最大优势在于它们能够清晰地表达我们作为实验者无法用语言表达的模式。</p><p id="bf14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于许多深度学习架构，权重初始化对于成功的学习至关重要。此外，我们仍然没有真正理解层间表现差异。RSA 提供了一个有效且易于计算的量，可以衡量对这些超参数的稳健性。在最近的 NeuRIPS 会议上，Sami Bengio 的小组(<a class="ae lm" href="http://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation.pdf" rel="noopener ugc nofollow" target="_blank"> Morcos 等人，2018 </a>)引入了投影加权典型相关分析(PWCCA)，以研究泛化以及窄网络和宽网络的差异。基于谷歌式的大型实证分析，得出了以下关键见解:</p><ol class=""><li id="8b7a" class="lz ma it kk b kl km ko kp kr mb kv mc kz md ld me mf mg mh bi translated"><strong class="kk iu">能够泛化的网络收敛到更相似的表示。</strong>直觉上，过拟合可以通过许多不同的方式实现。网络基本上不受训练数据的“约束”,可以在空间的那一部分之外为所欲为。泛化需要利用与真正的底层数据生成过程相关的模式。这只能通过一组更有限的架构配置来实现。</li><li id="8276" class="lz ma it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">网络的宽度与代表性收敛直接相关。<strong class="kk iu">更多的宽度=更多的相似表示</strong>(跨网络)。作者认为，这是所谓彩票假说的证据:经验表明，宽修剪网络比从一开始就很浅的网络表现更好。这可能是由于宽带网络的不同子网被不同地初始化。然后，修剪程序能够简单地识别具有最佳初始化的子配置，而浅网络从一开始就只有单个初始化。</li><li id="ed7d" class="lz ma it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">不同的初始化和学习率会导致不同的代表性解决方案集群。这些集群的泛化能力也很好。这可能表明损失面有多个定性不可区分的局部极小值。最终是什么推动了会员的加入还无法确定。</li></ol><p id="5ae1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Geoffrey Hinton 的谷歌大脑小组最近的一项扩展(<a class="ae lm" href="https://arxiv.org/pdf/1905.00414.pdf?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Kornblith 等人，2019 </a>)使用了中心核对齐(CKA)，以便将 CCA 扩展到更大的向量维度(人工神经元的数量)。就我个人而言，我真的很喜欢这项工作，因为计算模型给了我们科学家自由去打开所有的 nob。而且在 DL(架构、初始化、学习率、优化器、正则化器)也有不少。正如 Kriegeskorte 所说，网络是白盒。因此，如果我们不能成功地理解一个简单的多层感知器的动态，我们怎么能在大脑中成功呢？</p><p id="ff38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总而言之，我是每一项试图揭示大脑深度学习近似原理的科学发展的超级粉丝。然而，<strong class="kk iu">深度学习并不是大脑中计算的因果模型</strong>。认为大脑和中枢神经系统在时间和空间上执行相似的顺序操作是一个有限的结论。为了获得真正的洞察力，<strong class="kk iu">这个循环必须被关闭</strong>。因此，使用生成模型来设计刺激是神经科学中令人兴奋的新尝试。但是如果我们想要理解学习的动力，我们必须走得更远。损失函数和梯度是如何表示的？大脑如何克服将训练和预测阶段分开的必要性？表象只是回答这些基本问题的一个非常间接的窥视孔。展望未来，从跳过和循环连接以及通过漏失采样的贝叶斯 DL 中有很多收获(从建模者的角度来看)。但那是另一篇博文的故事。</p></div></div>    
</body>
</html>