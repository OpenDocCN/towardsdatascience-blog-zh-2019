<html>
<head>
<title>Predicting Customer Churn using PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PySpark 预测客户流失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-customer-churn-using-pyspark-6a78a78a8412?source=collection_archive---------25-----------------------#2019-12-23">https://towardsdatascience.com/predicting-customer-churn-using-pyspark-6a78a78a8412?source=collection_archive---------25-----------------------#2019-12-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b5d8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><a class="ae ki" href="https://www.denotedata.co.ke/" rel="noopener ugc nofollow" target="_blank">数据探索</a>，特征生成、建模和调整——使用 Spark。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/a2b0d0dfcd893c30f58e30f7c00850f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BDn4kjyXIK28ihU2lbvFGA.jpeg"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Photo by <a class="ae ki" href="https://www.pexels.com/@kaboompics?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Kaboompics</a> from <a class="ae ki" href="https://www.pexels.com/photo/customers-users-color-wheel-6231/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="065b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">客户流失是当今商业世界的一个主要商业问题，数据科学家正在快速采用工具和技术来有效预测客户流失并实时避免客户流失。</p><p id="1807" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Apache Spark，尤其是 PySpark，是执行探索性分析和机器学习来解决这一分类问题的最快工具之一。</p><p id="0b1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，主要目标是探索用户活动数据集，并使用 Spark 建立一个预测用户流失的机器学习模型。</p><p id="2a60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">喜欢你读的书吗？</strong>关注我上<strong class="lb iu"> </strong> <a class="ae ki" href="https://www.linkedin.com/in/ephraimwaithaka/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> LinkedIn </strong> </a>或<a class="ae ki" href="https://medium.com/@ephraim.mwai" rel="noopener"> <strong class="lb iu">中</strong> </a></p><blockquote class="lv lw lx"><p id="e9c7" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">点击获取数据服务产品<a class="ae ki" href="https://www.denotedata.co.ke/" rel="noopener ugc nofollow" target="_blank"/></p></blockquote><h1 id="7cf5" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">数据</strong></h1><p id="d553" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">数据已经出来了。json 格式由别名为 Sparkify 的音乐公司的日志组成。这是用户与音乐平台交互时捕获的日志。</p><p id="f2a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">加载数据</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="4665" class="ne md it na b gy nf ng l nh ni">path = “s3n://xxxxxxx-xxxx/sparkify/mini_sparkify_event_data.json”<br/>df = spark.read.json(path)</span></pre><p id="2447" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据架构:</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="8820" class="ne md it na b gy nf ng l nh ni">df.printSchema()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/ee8d0150cbc4803304e5448df4e55a0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*K37jtpW1C2Zrlpj6_58M2A.png"/></div></figure><h1 id="993c" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">探索性数据分析</strong></h1><p id="ec68" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本节中，我们将执行一些数据预处理，以回答一些业务问题。</p><p id="0786" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">删除任何空行或缺少 userId 的行</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="e0f3" class="ne md it na b gy nf ng l nh ni">#Drop null rows<br/>df = df.dropna(how = “any”, subset = [“userId”, “sessionId”,”ts”])</span><span id="7df2" class="ne md it na b gy nk ng l nh ni">#Drop rows with missing userId<br/>df = df.filter(df["userId"] != "")</span></pre><p id="cacf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">删除任何重复的行</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="3c3b" class="ne md it na b gy nf ng l nh ni">df.select(“userId”).dropDuplicates()</span></pre><p id="b843" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">添加流失标签</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="b4f3" class="ne md it na b gy nf ng l nh ni">churn_indicator = udf(lambda c: 1 if c == 'Cancellation Confirmation' else 0, IntegerType())<br/>df = df.withColumn('churn_indication', churn_indicator('page'))</span><span id="72cd" class="ne md it na b gy nk ng l nh ni">#Add churn columns to indicate users who have churned<br/>windowval = Window.partitionBy('userId')<br/>df = df.withColumn('churn', max('churn_indication').over(windowval))</span></pre><p id="7dc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么我们的客户流失数据框架将会是:</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="41d7" class="ne md it na b gy nf ng l nh ni">df_churn = df.filter('churn == 1')</span></pre><p id="32a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步，我们将回答以下业务问题:</p><p id="08f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">性别对流失有影响吗？</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="578e" class="ne md it na b gy nf ng l nh ni">#Group by users and gender to aggregate count of users</span><span id="5077" class="ne md it na b gy nk ng l nh ni">df_churn_by_gender = df.select(["userId", "gender","churn"]).groupby(["churn", "gender"]).count().sort("churn").toPandas()</span><span id="ea27" class="ne md it na b gy nk ng l nh ni">#Plot a barplot</span><span id="ba68" class="ne md it na b gy nk ng l nh ni">sb.barplot(x='churn', y='count', hue='gender', data=df_churn_by_gender)<br/>plt.title('What is the churn comparison by gender', fontsize= 16);<br/>plt.xlabel('Churn');<br/>plt.ylabel('Number of Users');</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/1c9ca7d7bbd4c6cd9adf06f09b5c8682.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*UfZ-n5hTaISItID3UiOtdw.png"/></div></figure><p id="9723" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图可以看出，男性比女性有更多的麻烦，因为女性是这项服务的主要用户。</p><p id="ec8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用户流失多久？</strong></p><p id="3ec9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了回答这个问题，我们添加了一个以天为单位的总时间列，并计算每个用户总天数的最大值。</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="51bc" class="ne md it na b gy nf ng l nh ni">df = df.withColumn('total_time_days', (df.ts-df.registration)/1000/3600/24)</span><span id="b1ca" class="ne md it na b gy nk ng l nh ni">total_time_df = df.select('UserId','churn','total_time_days').groupBy('userId','churn').agg(max('total_time_days').alias('total_time')).toPandas()</span><span id="d3b2" class="ne md it na b gy nk ng l nh ni">sb.boxplot(data=total_time_df, x='churn', y='total_time', orient='v');<br/>plt.ylabel('Total days');<br/>plt.xlabel('Churned');<br/>plt.title('After how long do Users churn?');</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e4286760b297ce5fc4e46cb74f116448.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*dn7h4BlvCXfwSzKcyRFAlg.png"/></div></figure><p id="2f7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数用户在使用音乐平台的第二个月就会流失</p><p id="a61e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">哪一天和哪一小时的客户流失率高？</p><p id="2005" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们从添加小时和工作日列开始。为了对上述问题有所了解，我们将汇总每天每小时的用户数量，然后以工作日为中心，如下图所示。</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="4f1e" class="ne md it na b gy nf ng l nh ni">#Create an hour and weekday column<br/>calc_hour = udf(lambda t: dt.datetime.fromtimestamp(t / 1000.0).hour)<br/>df_churn = df_churn.withColumn(“ts_hour”, calc_hour(df.ts))</span><span id="24ca" class="ne md it na b gy nk ng l nh ni">calc_weekday = udf(lambda t: dt.datetime.fromtimestamp(t / 1000.0).strftime(“%A”))<br/>df_churn = df_churn.withColumn(“ts_weekday”, calc_weekday(df.ts))</span><span id="fc67" class="ne md it na b gy nk ng l nh ni">df_churn_by_time = df_churn.select(['userId', 'ts_weekday','ts_hour','churn']).groupby(["userId","ts_weekday","ts_hour"]).agg(count(df.userId).alias('count'))#.toPandas()</span><span id="0ac0" class="ne md it na b gy nk ng l nh ni">df_churn_by_hr_week = df_churn_by_time.groupBy('ts_hour').pivot('ts_weekday').sum('count').sort('ts_hour')</span><span id="afd9" class="ne md it na b gy nk ng l nh ni">df_churn_by_hr_week = df_churn_by_hr_week.withColumn('ts_hour',df_churn_by_hr_week.ts_hour.cast('int'))<br/>df_churn_by_hr_week = df_churn_by_hr_week.toPandas().set_index('ts_hour').sort_index(axis=0,ascending=True)</span><span id="df9b" class="ne md it na b gy nk ng l nh ni">#Plot a Heat Map</span><span id="15fb" class="ne md it na b gy nk ng l nh ni">plt.figure(figsize=(16,10))<br/>sb.heatmap(df_churn_by_hr_week, fmt='d',  cmap='viridis_r', annot_kws={"size": 12},  cbar_kws={'label': 'Number of Churns'})<br/>plt.title("Which Hour and Day has High Customer Churn?", y=1, fontsize=18)<br/>plt.xlabel('Day of the week', labelpad=8)<br/>plt.ylabel('Hour (24hr)', labelpad=8)<br/>plt.yticks(rotation=360);</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nn"><img src="../Images/bf8094af93d54d4df436975c0897f9b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DYqXWtXT83LcgiUMEu7gRA.png"/></div></div></figure><h1 id="f6d8" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">特色工程</strong></h1><p id="95c1" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">让我们开始构建一些有希望的特性，我们将使用它们来训练我们的模型</p><ol class=""><li id="e21b" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">性别特征。男性或女性</li></ol><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="4532" class="ne md it na b gy nf ng l nh ni">gender_ft = df.select("userId", "gender").dropDuplicates().replace(['M', 'F'], ['0', '1'], 'gender').select('userId', col('gender').cast('int'))</span></pre><p id="a56c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.订阅级别—付费或免费</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="793f" class="ne md it na b gy nf ng l nh ni">level_ft = df.select("userId", "level").dropDuplicates().replace(['free', 'paid'], ['0', '1'], 'level').select('userId', col('level').cast('int'))</span></pre><p id="ff0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.用户在平台上的总时间(生命周期)</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="20ea" class="ne md it na b gy nf ng l nh ni">total_time_ft = df.select(‘UserId’,’total_time_days’).groupBy(‘userId’).agg(max(‘total_time_days’).alias(‘total_time’))</span></pre><p id="24ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.用户听过的歌曲总数</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="5562" class="ne md it na b gy nf ng l nh ni">total_songs_ft = df.select(‘UserId’,’song’).groupBy(‘userId’).agg(count(‘UserId’).alias(‘total_songs’))</span></pre><p id="387e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.平台上每个用户会话的歌曲数量</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="3333" class="ne md it na b gy nf ng l nh ni">songs_per_session_ft = df.filter(df.page=="NextSong").groupBy('UserId','sessionId').count().groupBy('userId').agg(avg('count').alias('songs_per_session'))</span></pre><p id="f016" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">6.艺术家总数</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="0cb4" class="ne md it na b gy nf ng l nh ni">total_artists_ft = df.filter(df.page==”NextSong”).select(‘UserId’,’artist’).dropDuplicates().groupBy(‘userId’).agg(count(‘UserId’).alias(‘total_artists’))</span></pre><p id="b181" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">7.总收听时间</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="1a79" class="ne md it na b gy nf ng l nh ni">total_listening_ft = df.select(‘UserId’,’length’).groupBy(‘userId’).sum().withColumnRenamed(‘sum(length)’, ‘total_listen_time’)</span></pre><p id="4183" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于页面导航日志的更多功能</p><p id="5702" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">8.添加的好友数量</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="721a" class="ne md it na b gy nf ng l nh ni">friends_added_ft = df.filter(df.page==”Add Friend”).groupBy(‘userId’).agg(count(‘UserId’).alias(‘friends_added’))</span></pre><p id="41f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">9.帮助页面访问</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="1642" class="ne md it na b gy nf ng l nh ni">help_ft = df.filter(df.page==”Help”).groupBy(‘UserId’).agg(count(‘UserId’).alias(‘help_access’))</span></pre><p id="606d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">10.添加到播放列表的歌曲数量</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="e9bb" class="ne md it na b gy nf ng l nh ni">songs_playlist_ft = df.filter(df.page==”Add to Playlist”).groupBy(‘UserId’).agg(count(‘UserId’).alias(‘songs_on_playlist’))</span></pre><p id="c158" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">11.赞数—竖起大拇指页面</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="cffa" class="ne md it na b gy nf ng l nh ni">likes_ft = df.filter(df.page==”Thumbs Up”).groupBy(‘UserId’).agg(count(‘UserId’).alias(‘likes’))</span></pre><p id="ed54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">12.不喜欢的数量—拇指向下页面</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="4b13" class="ne md it na b gy nf ng l nh ni">dislikes_ft = df.filter(df.page==”Thumbs Down”).groupBy(‘UserId’).agg(count(‘UserId’).alias(‘dislikes’))</span></pre><p id="584f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，</p><p id="11d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">13.流失标签</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="808d" class="ne md it na b gy nf ng l nh ni">churn_label_ft = df.select(‘UserId’, col(‘churn’).alias(‘label’)).dropDuplicates()</span></pre><p id="6c56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将使用外部连接将我们的特性连接到一个数据帧中，清除一些空值并删除不必要的 UserId 列</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="172d" class="ne md it na b gy nf ng l nh ni">#combine all datasources into a single data frame<br/>feature_df = gender_ft.join(level_ft,’userID’,’outer’) \<br/> .join(total_time_ft,’userID’,’outer’) \<br/> .join(total_songs_ft,’userID’,’outer’) \<br/> .join(total_artists_ft,’userID’,’outer’) \<br/> .join(songs_per_session_ft,’userID’,’outer’) \<br/> .join(total_listening_ft,’userID’,’outer’) \<br/> .join(friends_added_ft,’userID’,’outer’) \<br/> .join(help_ft,’userID’,’outer’) \<br/> .join(songs_playlist_ft,’userID’,’outer’) \<br/> .join(likes_ft,’userID’,’outer’) \<br/> .join(dislikes_ft,’userID’,’outer’) \<br/> .join(churn_label_ft,’userID’,’outer’)</span><span id="7d58" class="ne md it na b gy nk ng l nh ni">#Drop unnecessary userid column and fill null values</span><span id="bb72" class="ne md it na b gy nk ng l nh ni">feature_df = feature_df.drop(‘userID’).fillna(0)</span><span id="6729" class="ne md it na b gy nk ng l nh ni">feature_df.show(5)</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nx"><img src="../Images/0330ac22e241a5d6132268ea3ef79205.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rQIHiFnEsC-VfSU67Q0WqA.png"/></div></div></figure><h1 id="17ce" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">建模</strong></h1><p id="c1fd" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们将首先使用 VestorAssembler(一种将多个列合并为一个向量列的要素转换器)和 StandardScaler(通过移除平均值并缩放至单位方差来标准化要素)对要素数据集进行矢量化和标准化</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="b0b9" class="ne md it na b gy nf ng l nh ni">#Vectorize features<br/>cols = [‘gender’, ‘level’, ‘total_time’, ‘total_songs’, ‘total_artists’, ‘songs_per_session’, ‘total_listen_time’, ‘friends_added’, ‘help_access’, ‘songs_on_playlist’, ‘likes’, ‘dislikes’]<br/>assembler = VectorAssembler(inputCols=cols, outputCol=”vfeatures”)<br/>data = assembler.transform(feature_df)</span><span id="5af2" class="ne md it na b gy nk ng l nh ni"># standardize features<br/>scaler = StandardScaler(inputCol="vfeatures", outputCol="features", withStd=True)<br/>scalerModel = scaler.fit(data)<br/>data = scalerModel.transform(data)</span></pre><p id="8ea7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将数据集分为训练、测试、验证和测试，如下所示:</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="f5b4" class="ne md it na b gy nf ng l nh ni"># train test split<br/>train, rest = data.randomSplit([0.6, 0.4], seed=42)<br/>validation, test = rest.randomSplit([0.5, 0.5], seed=42)</span></pre><p id="ba1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步包括比较模型的性能并选择性能最佳的模型。我们将使用默认参数训练模型，并对最佳模型进行改进。最佳模型将用于扩展到更大的数据集。</p><ol class=""><li id="fc13" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">逻辑回归</li></ol><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="8110" class="ne md it na b gy nf ng l nh ni">#estimator<br/>lr = LogisticRegression(maxIter=10)<br/>#evaluator<br/>mce_f1_evaluator = MulticlassClassificationEvaluator(metricName=’f1')</span><span id="d6b5" class="ne md it na b gy nk ng l nh ni"># build an empty paramGrid for now<br/>paramGrid = ParamGridBuilder().build()</span><span id="57c7" class="ne md it na b gy nk ng l nh ni">lr_cv = CrossValidator(estimator=lr,<br/> evaluator=mce_f1_evaluator, <br/> estimatorParamMaps=paramGrid,<br/> numFolds=3)</span><span id="73e5" class="ne md it na b gy nk ng l nh ni">#Train the model<br/>lr_cv_model = lr_cv.fit(train)<br/>#fit the model<br/>svm_results = svm_model.transform(validation)</span><span id="1057" class="ne md it na b gy nk ng l nh ni">#Evaluate using f1 score<br/>evaluator = MulticlassClassificationEvaluator(predictionCol = "prediction")<br/>print('F1 Score:{}'.format(evaluator.evaluate(svm_results, {evaluator.metricName: "f1"})))</span></pre><p id="ac70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.支持向量机</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="0877" class="ne md it na b gy nf ng l nh ni">#estimator<br/>svm = LinearSVC(maxIter=10)<br/>#evluator<br/>mce_f1_evaluator = MulticlassClassificationEvaluator(metricName=’f1')<br/># build an empty paramGrid for now<br/>paramGrid = ParamGridBuilder().build()</span><span id="bb87" class="ne md it na b gy nk ng l nh ni">svm_cv = CrossValidator(estimator=svm,<br/> estimatorParamMaps=paramGrid,<br/> evaluator=mce_f1_evaluator,<br/> numFolds=3)</span><span id="9133" class="ne md it na b gy nk ng l nh ni">#train<br/>svm_model = svm_cv.fit(train)<br/>#fit<br/>svm_results = svm_model.transform(validation)<br/>#evaluate<br/>evaluator = MulticlassClassificationEvaluator(predictionCol = "prediction")<br/>print('F1 Score:{}'.format(evaluator.evaluate(svm_results, {evaluator.metricName: "f1"})))</span></pre><p id="58e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.随机森林分类器</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="97b1" class="ne md it na b gy nf ng l nh ni">#classifier<br/>rf = RandomForestClassifier()</span><span id="0637" class="ne md it na b gy nk ng l nh ni">#evaluator<br/>mce_f1_evaluator = MulticlassClassificationEvaluator(metricName=’f1')</span><span id="f27f" class="ne md it na b gy nk ng l nh ni"># build an empty paramGrid for now<br/>paramGrid = ParamGridBuilder().build()</span><span id="02ec" class="ne md it na b gy nk ng l nh ni">rf_cv = CrossValidator(estimator=rf,<br/> estimatorParamMaps=paramGrid,<br/> evaluator=mce_f1_evaluator,<br/> numFolds=3)</span><span id="07b6" class="ne md it na b gy nk ng l nh ni">#train<br/>rf_model = rf_cv.fit(train)</span><span id="3b32" class="ne md it na b gy nk ng l nh ni">#validate<br/>rf_results = rf_model.transform(validation)</span><span id="5130" class="ne md it na b gy nk ng l nh ni">#evaluate<br/>evaluator = MulticlassClassificationEvaluator(predictionCol = "prediction")<br/>print('F1 Score:{}'.format(evaluator.evaluate(rf_results, {evaluator.metricName: "f1"})))</span></pre><p id="7b35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.梯度推进树</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="2458" class="ne md it na b gy nf ng l nh ni">#classifier<br/>gbt = GBTClassifier(maxIter=10,seed=42)</span><span id="1215" class="ne md it na b gy nk ng l nh ni">#evaluator<br/>mce_f1_evaluator = MulticlassClassificationEvaluator(metricName=’f1')</span><span id="4c01" class="ne md it na b gy nk ng l nh ni"># build an empty paramGrid for now<br/>paramGrid = ParamGridBuilder().build()</span><span id="4bfe" class="ne md it na b gy nk ng l nh ni">gbt_cv = CrossValidator(estimator=gbt,<br/> estimatorParamMaps=paramGrid,<br/> evaluator=mce_f1_evaluator,<br/> numFolds=3)</span><span id="2696" class="ne md it na b gy nk ng l nh ni">#train<br/>gbt_model = gbt_cv.fit(train)</span><span id="f4aa" class="ne md it na b gy nk ng l nh ni">#validate<br/>gbt_results = gb.model.transform(validation)</span><span id="aa68" class="ne md it na b gy nk ng l nh ni">#evaluate<br/>evaluator = MulticlassClassificationEvaluator(predictionCol = "prediction")<br/>print('F1 Score:{}'.format(evaluator.evaluate(gbt_results, {evaluator.metricName: "f1"})))</span></pre><p id="ceb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4 个分类器的结果如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/dec2e41feaabcfdda4b7157a02f46be9.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*U_XnGXB9DOTIi6sWrUxjXw.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Classifier Results</figcaption></figure><p id="7516" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度提升树是迄今为止最好的评分模型，F1 得分为 0.9，准确度得分为 0.9，其次是随机森林分类器。我们还可以观察其他指标，比如模型训练所需的时间。可以说，我们希望提供最好的结果，因此基于准确性和 f1 分数，我们可以选择我们在这种情况下的最佳模型。</p><h1 id="ad21" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">超参数调谐</strong></h1><p id="8722" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在这最后一部分，我们希望通过将<strong class="lb iu">超参数</strong>添加到梯度推进树和随机森林分类器模型来调整我们的 2 个最佳模型</p><p id="6a26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">渐变提升树</strong></p><p id="51b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是默认参数</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="b5a3" class="ne md it na b gy nf ng l nh ni">{'seed': 42, 'predictionCol': 'prediction', 'labelCol': 'label', 'featuresCol': 'features', 'maxDepth': 5, 'maxBins': 32, 'minInstancesPerNode': 1, 'minInfoGain': 0.0, 'maxMemoryInMB': 256, 'cacheNodeIds': False, 'checkpointInterval': 10, 'lossType': 'logistic', 'maxIter': 10, 'stepSize': 0.1, 'subsamplingRate': 1.0, 'featureSubsetStrategy': 'all'}</span></pre><p id="3633" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用参数建立模型</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="9030" class="ne md it na b gy nf ng l nh ni">gbt = GBTClassifier(maxIter=10,seed=42)</span><span id="ff19" class="ne md it na b gy nk ng l nh ni"># build param Grid<br/>paramGrid_gbt = ParamGridBuilder() \<br/>    .addGrid(gbt.maxIter,[5, 10,15]) \<br/>    .addGrid(gbt.maxDepth,[2,3,4,5,6,7,8]) \<br/>    .build()<br/># set evaluator<br/>mce_f1_evaluator = MulticlassClassificationEvaluator(metricName='f1')</span><span id="e2a6" class="ne md it na b gy nk ng l nh ni">gbt_hpt_cv = CrossValidator(estimator=gbt,<br/>                          estimatorParamMaps=paramGrid_gbt,<br/>                          evaluator=mce_f1_evaluator,<br/>                          numFolds=3)</span><span id="8214" class="ne md it na b gy nk ng l nh ni">gbt_hpt_model = gbt_hpt_cv.fit(train)</span><span id="11e7" class="ne md it na b gy nk ng l nh ni">#Test the model using test data<br/>gbt_model_results = gbt_hpt_model.transform(test)</span></pre><p id="c6e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">随机森林分类器</strong></p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="6fe3" class="ne md it na b gy nf ng l nh ni">#Default parameters<br/>{'probabilityCol': 'probability', 'rawPredictionCol': 'rawPrediction', 'seed': -5387697053847413545, 'predictionCol': 'prediction', 'labelCol': 'label', 'featuresCol': 'features', 'maxDepth': 5, 'maxBins': 32, 'minInstancesPerNode': 1, 'minInfoGain': 0.0, 'maxMemoryInMB': 256, 'cacheNodeIds': False, 'checkpointInterval': 10, 'impurity': 'gini', 'numTrees': 20, 'featureSubsetStrategy': 'auto', 'subsamplingRate': 1.0}</span></pre><p id="f858" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用参数建立模型</p><pre class="kk kl km kn gt mz na nb nc aw nd bi"><span id="471a" class="ne md it na b gy nf ng l nh ni">#classifier<br/>rf = RandomForestClassifier()</span><span id="b9f7" class="ne md it na b gy nk ng l nh ni">#evaluator<br/>mce_f1_evaluator = MulticlassClassificationEvaluator(metricName=’f1')</span><span id="0f66" class="ne md it na b gy nk ng l nh ni"># build an empty paramGrid for now<br/>paramGrid = ParamGridBuilder() \<br/> .addGrid(rf.impurity,[‘entropy’, ‘gini’]) \<br/> .addGrid(rf.maxDepth,[2,3,4,5,6,7,8]) \<br/> .build()</span><span id="4f6b" class="ne md it na b gy nk ng l nh ni">rf_cv = CrossValidator(estimator=rf,<br/> estimatorParamMaps=paramGrid,<br/> evaluator=mce_f1_evaluator,<br/> numFolds=3)<br/>#train model<br/>rf_model = rf_cv.fit(train)<br/>#Test using test data set<br/>rf_model_results = rf_model.transform(test)</span></pre><p id="2e1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在测试数据上测试两个调整的模型之后，随机森林分类器具有体面的<strong class="lb iu"> 0.85 准确度分数</strong>和<strong class="lb iu"> 0.84 f1 分数。</strong></p><p id="a6af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度提升以 0.78 的准确度分数<strong class="lb iu">和 0.78 的 f1 分数</strong>位居第二</p><p id="c460" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">与梯度相比，随机森林分类器</strong>在参数调整后的测试数据集上得分最高。当把时间作为一个衡量标准时，训练也需要更少的时间。因此，在 4 种算法中，随机森林是最健壮的</p><h1 id="b914" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">结论</strong></h1><p id="7553" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我首先加载数据，删除丢失的值，删除重复的行，为数据探索创建额外的列，并使用可视化工具回答了一些业务问题。然后开始研究特征生成和建模。该项目的最有趣和最困难的部分是 spark ML 的实现，使用 PySpark 预处理和特征工程数据；考虑到这是第一次使用这个工具。能够在云上部署 ML 解决方案(在本例中是 AWS EMR 服务)是一个很好的技能补充。</p><h1 id="b328" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">改进</h1><p id="a298" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">另一个具有挑战性的部分是特征生成。虽然我只能想出 12 个功能，但他们有可能创造更多。因此，有了更大的数据集和更多的特征，该模型在预测客户流失方面会更加准确和稳健</p><p id="5bb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型也可以作为 API 部署，在业务环境中使用。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><p id="d10b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更多细节请查看我的<a class="ae ki" href="https://github.com/ephraimmwai/Capstone-Project-Predicting-Churn-Rates" rel="noopener ugc nofollow" target="_blank"> Github 库</a></p></div></div>    
</body>
</html>