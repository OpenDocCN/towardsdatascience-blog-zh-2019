<html>
<head>
<title>Voice Translation and Audio Style Transfer with GANs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 GANs 进行语音翻译和音频风格转换</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854?source=collection_archive---------3-----------------------#2019-11-06">https://towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854?source=collection_archive---------3-----------------------#2019-11-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6f5a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用 Spectrograms 和 GANs 将爵士乐转换为古典音乐</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0ea2edabd0607cfae36632c35bf94a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*g47Lg-GNCYP0g6ZK"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@mgmaasen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Michael Maasen</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="ac81" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="a7c2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们都听说过图像风格转移:从一幅名画中提取风格并将其应用到另一幅图像中，这是一项已经通过多种不同方法实现的任务。<strong class="lt iu">生成对抗网络</strong>(简称 GANs)也被用于图像生成、图像到图像的翻译等等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/b33d91154f693c99b2a2dc49d8d45f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5iiD4zYn6NyyQVaPO9lnw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Examples of image style transfer</figcaption></figure><p id="bbe9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">但是声音呢？从表面上看，你可能会认为<strong class="lt iu">音频与图像</strong>完全不同，所有为图像相关任务探索的不同技术都不能应用于声音。但是，如果我们能找到一种方法将音频信号转换成类似图像的二维表示会怎么样呢？</p><p id="a850" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">事实上，是的，我们可以！这种声音表示就是我们所说的“<strong class="lt iu">声谱图</strong>”，它是允许我们利用专门设计的算法来处理音频相关任务的关键。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a49153ca88814f226a37fcb93150207a.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*W2cxJev00kJJ2HIb1YGZFA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Spectrogram (<a class="ae ky" rel="noopener" target="_blank" href="/getting-to-know-the-mel-spectrogram-31bca3e2d9d0">source</a>)</figcaption></figure><h1 id="2a04" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">光谱图</h1><p id="5752" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果你是音频处理领域的新手，你可能不熟悉声谱图到底是什么。给定一个时域信号(一维)，我们希望获得一个<strong class="lt iu">时频二维表示</strong>。为了实现这一点，我们对音频信号应用具有特定长度窗口的短时<strong class="lt iu">傅立叶变换</strong> (STFT)，仅考虑结果的平方幅度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/7189211f35288b45e982dae159fdbd2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*D9opNS4IOTHcS8iMNE_uPA.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Incredible illustration of how Time and Frequency correlate from the <a class="ae ky" href="https://sjvasquez.github.io/blog/melnet/" rel="noopener ugc nofollow" target="_blank">MelNet paper page</a></figcaption></figure><p id="ef54" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">更简单地说，我们将原始<strong class="lt iu">波形</strong>信号分成相互重叠的块，提取块中<strong class="lt iu">频率</strong>的幅度(通过傅立叶变换)，每个结果向量将代表我们最终频谱图的一列。谱图的 x 轴代表<strong class="lt iu">时间</strong>，y 轴代表<strong class="lt iu">频率</strong>。</p><p id="4e7c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">为了让这些频谱图对我们的任务更有用，我们将每个“像素”(或量值)转换成分贝标度<strong class="lt iu"/>，取每个值的对数。最后，我们应用 mel 滤波器组，将光谱图转换为<strong class="lt iu"> mel 标度</strong>，得到所谓的<strong class="lt iu"> mel 光谱图</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/1f53064d368a3a9fc7530e7d1193994b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lentsUBDgplmmVttKYIoyw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Examples of mel-spectrograms with log-amplitude</figcaption></figure><p id="7d0c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这让我们能够让声谱图表现方式<strong class="lt iu">对我们人类对声音的理解</strong>更加敏感，突出我们人类更容易听到的振幅和频率。</p><p id="1cd6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">同样非常重要的是要注意，频谱图可以被<strong class="lt iu">转换回</strong>成“可听见的”<strong class="lt iu">波形</strong>数据:这不会是一个<strong class="lt iu">完美的重建</strong>(相位信息在我们的幅度频谱图中丢失了)，但是由于一种叫做<strong class="lt iu"> Griffin-Lim </strong>的算法，我们能够近似相位并且<strong class="lt iu">重现</strong>真实的声音音频。</p><h1 id="1240" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">我们的任务</h1><p id="d5a4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在我们知道了如何用图像来表示声音，让我们来玩一玩。</p><p id="e5e8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在这篇文章中，我将解释如何构建和训练一个能够执行<strong class="lt iu">语音转换</strong>和任何其他类型的<strong class="lt iu">音频风格转换</strong>(例如将一种音乐风格转换为另一种)的系统。该方法在很大程度上受到最近使用生成式对抗网络进行图像到图像翻译的研究的启发，主要区别在于将所有这些技术应用于音频数据。作为一个额外的功能，我们将能够翻译任意长度的样本，这在 GAN 系统中并不常见。</p><p id="de5a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">为了让你对将要学习的内容有一点兴奋，这里有一个<strong class="lt iu">演示视频</strong>，展示了我们用这种方法可以达到的效果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="97db" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在演示视频中，你可以听不同的<strong class="lt iu">语音翻译</strong>示例以及一些<strong class="lt iu">音乐流派转换</strong>，特别是从爵士乐到古典音乐的转换。听起来很不错，不是吗？</p><h1 id="eea7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">选择架构</h1><p id="e39e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">计算机视觉领域有许多不同的架构用于图像到图像的翻译，这是我们希望通过音频(比如说语音)的<strong class="lt iu">频谱图表示</strong>来实现的任务。</p><p id="9006" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">图像到图像的转换在于<strong class="lt iu">将图像</strong>从域 A(例如猫的图片)转换到不同的域 B(狗的图片)，同时<strong class="lt iu">保持来自原始图片(猫的表情和姿势)的内容信息</strong>。我们的任务实际上是<strong class="lt iu">相同的</strong>:我们希望从说话者 A 翻译到说话者 B，同时保持来自说话者 A 的相同的<strong class="lt iu">语言信息</strong>(生成的语音应该包含与来自说话者 A 的原始语音相同的<strong class="lt iu">单词</strong>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/e484e19f0f2ed764352c772dd07b804a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EWWm148aWAh7IX-9k2aS9g.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">CycleGAN architecture</figcaption></figure><p id="9f96" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">为这个目标打造的<strong class="lt iu">最著名的</strong> GAN 架构可能是<strong class="lt iu"> CycleGAN </strong>，于 2017 年推出，此后被广泛使用。虽然 CycleGAN 在<strong class="lt iu">相似领域</strong>(相似的形状和背景)之间的翻译非常成功，比如从马到斑马，或者从苹果到橙子，但当涉及非常不同的领域时，比如从鱼到鸟，或者从苹果到胡萝卜，它就显得力不从心了。</p><p id="0bff" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这个<strong class="lt iu">缺点</strong>的原因是 CycleGAN 严重依赖于<strong class="lt iu">像素损失</strong>的事实，或者换句话说，它的损失倾向于最小化真实和生成图像的像素值的差异:直觉上，当将一个对象(例如苹果)的图像转换到一个实质上不同的域(胡萝卜)时，我们需要改变原始对象的<strong class="lt iu">主要形状</strong>，在这种情况下<strong class="lt iu"> CycleGAN 不能帮助我们</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/4b0fc95dc677c5fb762171e3128dcf34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e1BweBQ8XeOTJWQoac1PpQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">CycleGAN translation example. (Zebra to Horse)</figcaption></figure><p id="f913" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">不同人的演讲的频谱图(或不同流派的音乐作品的频谱图)在视觉上可能彼此非常不同:因此我们需要找到一种更通用的方法来解决这个问题，一种不涉及被视觉上相似的领域之间的翻译所约束的方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/1f53064d368a3a9fc7530e7d1193994b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lentsUBDgplmmVttKYIoyw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Spectrograms of different speakers or different music genres can be very visually different</figcaption></figure><h1 id="1431" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">TraVeLGAN:我们的解决方案</h1><p id="553f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最初在这里介绍的<strong class="lt iu"> TraVeLGAN </strong>(变换向量学习 GAN)旨在解决我们的问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/f2724658552d5b60db03864296cde208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OBo-rvuda7bbHxprDv0lDg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Examples of TraVeLGAN image-to-image translations with very different domains</figcaption></figure><p id="25b5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">除了生成器和鉴别器(或评论家)，TraVeLGAN 还引入了一个<strong class="lt iu">暹罗网络</strong>(一个将图像编码为潜在向量的网络)以允许在<strong class="lt iu">实质上不同的域</strong>之间进行翻译，从而保持原始样本和转换样本之间的内容关系。</p><p id="843e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">让我们了解一下 TraVeLGAN 到底是如何工作的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/c9377c64078d1e8d3985e9038261f977.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LsF_UEbfB7e1b3raB3XTmw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">TraVeLGAN architecture</figcaption></figure><p id="b2a2" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们的目标是找到一种方法来保持原始样本和生成样本之间的关系，而不依赖于像素损失(如 CycleGAN 中的循环一致性约束)，这将限制视觉上相似的域之间的转换。因此，如果我们<strong class="lt iu">将</strong>图像(或光谱图)编码成<strong class="lt iu">向量</strong>，这些向量在<strong class="lt iu">组织的潜在空间</strong>中捕获它们的内容信息，我们就能够保持这些向量之间的关系，而不是整个图像。</p><p id="666b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这正是一个<strong class="lt iu">暹罗</strong>网络让我们实现的目标。最初用于人脸识别的任务，暹罗网络将一幅<strong class="lt iu">图像作为输入</strong>，而<strong class="lt iu">输出一个长度为<em class="nc"> vec_len 的单一向量</em></strong>。使用损失函数指定矢量空间中哪些图像<strong class="lt iu">编码应该靠近</strong>(例如同一张脸的图像)以及哪些应该远离<strong class="lt iu"/>(不同脸的图像)我们能够<strong class="lt iu">组织潜在空间</strong>并使其对我们的目标有用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/6c627446d925534f6591c4654cc9815e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bLRvkrcYgoQWKQfqXWbfZg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The Siamese network encodes images into vectors</figcaption></figure><p id="4437" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">更具体地说，我们的目标是保持编码对之间的<strong class="lt iu">变换向量</strong> <strong class="lt iu">相等:这似乎是一个非常难以理解的概念，但实际上是非常容易理解的<strong class="lt iu"/>。</strong></p><p id="d3aa" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">以<em class="nc"> G(X) </em>为平移图像<em class="nc"> X </em>(生成器的输出)<em class="nc"> S(X) </em>为矢量编码的<em class="nc"> X </em>和<em class="nc"> A1、A2 </em>两幅图像来自源域<em class="nc"> A </em>，网络必须对矢量进行编码，如:</p><p id="76be" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu"><em class="nc">(S(A1)-S(A2))=</em>(<em class="nc">S(G(A1)-S(G(A2)))</em></strong></p><p id="c617" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这样，连接一对源图像编码的变换向量<strong class="lt iu">必须等于</strong>由生成器转换的同一对之间的变换向量。</p><p id="f0bb" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这允许<strong class="lt iu">在翻译中保留语义信息</strong>(不同于使用其循环一致性约束保留更多几何内容信息的 CycleGAN)，允许约束不同域的图像之间的<strong class="lt iu">更多“抽象”关系</strong>。</p><p id="bcf1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">形式上，为了在翻译中保留内容信息，我们将<strong class="lt iu">最小化两个变换向量之间的欧几里德距离和余弦相似度</strong>，以便向量的<strong class="lt iu">角度</strong>和<strong class="lt iu">幅度</strong>都得到保留。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/f28c3af117c86d58cea26c57414b6a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LamxVKUATPaXkz_0mKH2rw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Formal TraVeL loss</figcaption></figure><p id="04f6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">此外，重要的是澄清<strong class="lt iu">发电机</strong>和<strong class="lt iu">连体</strong>网络<strong class="lt iu">必须配合</strong>来实现这一目标。更具体地说，行程损耗的梯度从到<strong class="lt iu">反向传播到两个网络</strong>，并且它们的权重相应地更新。因此，当鉴别者和发生者有一个<strong class="lt iu">对立的目标</strong>(他们互相挑战以达到他们的目标)时，暹罗人和发生者<strong class="lt iu">互相帮助</strong>，在相同的规则下合作。</p><p id="5109" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">除了这种“内容”损失之外，由于传统的<strong class="lt iu">对抗性损失</strong>(在我的实验中，我使用了铰链损失)，生成器将学习如何生成逼真的样本。</p><p id="ec01" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">如果你是 GANs 及其工作方式的新手，或者如果你想<strong class="lt iu">更深入一点</strong>如何用潜在空间保存内容信息，我推荐你<strong class="lt iu">查看</strong> <a class="ae ky" rel="noopener" target="_blank" href="/a-new-way-to-look-at-gans-7c6b6e6e9737"> <strong class="lt iu">我的文章</strong>这里</a>讲述如何在一个简单的图像到图像的翻译任务中应用相同的技术。</p><h1 id="0cef" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">翻译任意长度的音频信号</h1><p id="9a14" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">既然我们已经探索了一种允许我们在翻译中保留内容信息的方法，我们需要理解如何让生成器转换任意长的样本，而不增加额外的计算和训练时间。</p><p id="39fd" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">假设我们有一个音频信号:“提取”该信号的 mel 谱图，我们获得一个具有单通道的<strong class="lt iu">图像(不同于传统的 3 通道 RGB 图像)，该图像具有<strong class="lt iu">确定的高度</strong><em class="nc"/>(这取决于用于 STFT 的跳数大小)和<strong class="lt iu">宽度</strong><em class="nc">×25<strong class="lt iu">取决于音频样本的原始长度</strong>。</em></strong></p><p id="c32b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">然而，众所周知，处理具有<strong class="lt iu">可变尺寸</strong>的图像是一项具有<strong class="lt iu">挑战性的</strong>任务，尤其是如果我们事先没有决定这些尺寸的话。这就是为什么我们将<strong class="lt iu">分割</strong>所有光谱图(形状<em class="nc"> XxH </em>与<em class="nc"> X </em>不同)成<strong class="lt iu">块</strong>与<strong class="lt iu">确定宽度</strong>，让我们说<em class="nc"> L </em>。完美！我们的数据集现在由具有<strong class="lt iu">已知尺寸</strong> ( <em class="nc"> LxH </em>)的源和目标光谱图组成，我们已经准备好进行下一步。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/1f53064d368a3a9fc7530e7d1193994b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lentsUBDgplmmVttKYIoyw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Each spectrogram in the dataset has a fixed height H and width L</figcaption></figure><p id="ecac" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在创建我们的生成器<em class="nc"> G </em>之前，我们需要指定其输入的<strong class="lt iu">尺寸，在我们的例子中是<em class="nc"> (L/2)xH </em>。换句话说，G 将接受我们数据集中宽度为一半的光谱图。为什么？因为这样我们就能让 G 翻译我们之前拆分的<strong class="lt iu">整个</strong> <em class="nc"> XxH </em> <strong class="lt iu">光谱图</strong>。让我们来看看是如何做到的。</strong></p><p id="cac7" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们的<strong class="lt iu">培训管道</strong>将包括以下行动:</p><ol class=""><li id="87a4" class="nf ng it lt b lu mo lx mp ma nh me ni mi nj mm nk nl nm nn bi translated"><strong class="lt iu">将</strong>源<em class="nc"> LxH </em>谱图<strong class="lt iu">分成两半</strong>，得到<em class="nc"> (L/2)xH </em>谱图</li><li id="fc85" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><strong class="lt iu">将两半对</strong>送入发电机，并将<strong class="lt iu">平移的两对</strong>作为输出</li><li id="2959" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><strong class="lt iu">将平移的两半</strong>连接回它们的<strong class="lt iu">原始形状</strong> ( <em class="nc"> LxH </em>)</li><li id="7151" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><strong class="lt iu">将</strong>经<strong class="lt iu">翻译的</strong>和<strong class="lt iu">目标</strong>和<em class="nc"> LxH </em>光谱图送入鉴别器，使<strong class="lt iu">区分</strong>和<strong class="lt iu">对抗训练</strong>。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/63424596753678796fd49ecfc6c5f07d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxIk9RINFbJIia91ps5wTQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Illustration of the training pipeline: splitting, converting and concatenating.</figcaption></figure><p id="a7d5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">使鉴别器检查<strong class="lt iu">连接的“假”光谱图</strong>和<strong class="lt iu">，将</strong>与<strong class="lt iu">“真实”</strong>目标光谱图进行比较，迫使发生器生成样本，当<strong class="lt iu">连接在一起时，产生<strong class="lt iu">真实</strong>光谱图。</strong></p><p id="b745" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">换句话说，转换后的<em class="nc"> (L/2)xH </em>样本不得在边缘上出现任何<strong class="lt iu">不连续</strong>，这将使它们<strong class="lt iu">在鉴别器看来不真实</strong>。因此，发生器上这个重要的<strong class="lt iu">约束</strong>允许我们将任意长度的音频信号从一个域转换到另一个域。</p><p id="9580" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">训练后，当想要<strong class="lt iu">翻译</strong>一个任意形状的声谱图<em class="nc"> XxH </em>其中<em class="nc"> X </em> <strong class="lt iu">变化</strong>并由原始音频信号的长度给出时，这是我们需要做的:</p><ol class=""><li id="abc5" class="nf ng it lt b lu mo lx mp ma nh me ni mi nj mm nk nl nm nn bi translated"><strong class="lt iu">将<em class="nc"> XxH </em>谱图分割成<em class="nc"> (L/2)xH </em>块，如果<em class="nc"> X </em>不能被<em class="nc"> L/2 </em>整除，则使用填充</strong></li><li id="deba" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><strong class="lt iu">将</strong>每个<em class="nc"> (L/2)xH </em>样本送入发生器进行翻译</li><li id="36b2" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><strong class="lt iu">连接</strong>转换后的样本为原始<em class="nc"> XxH </em>形状，如果使用了填充，则剪切掉多余的部分。</li></ol><p id="b16e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在用 Griffin-Lim 算法将其变回<strong class="lt iu">波形</strong>之后，最终翻译的样本不应<strong class="lt iu">呈现</strong> <strong class="lt iu">不连续</strong>，而应呈现<strong class="lt iu">与目标域(特定的语音或音乐流派)相同的风格</strong>。很简单，不是吗？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/60cf6818974823171b144e27c179753e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UK4-kHx9vEwpPVuXkCysMg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Examples of source and converted spectrograms: the concatenated samples do not present discontinuities</figcaption></figure><h1 id="854a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">把所有东西放在一起</h1><p id="e3b4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们之前已经学习了如何<strong class="lt iu">保存来自源音频样本的内容</strong>(在语音的情况下，它将是一些<strong class="lt iu">言语信息</strong>，在音乐的情况下，它将是一首歌曲的特定<strong class="lt iu">旋律</strong>，而没有在<strong class="lt iu">视觉上相似的域</strong>之间进行转换的约束(不同声音或音乐流派的频谱图在视觉上可能极其<strong class="lt iu">不同</strong>)以及允许我们转换任意长度的样本的简单而有效的技术<strong class="lt iu"/></p><p id="7e0c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在终于是时候把所有的事情整合在一起了。</p><p id="05b1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这是我论文中的一段<strong class="lt iu">摘录</strong>，介绍了这项技术:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/63424596753678796fd49ecfc6c5f07d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxIk9RINFbJIia91ps5wTQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Putting everything together: the siamese network helps preserve content keeping vector arithmetic between source and converted samples</figcaption></figure><blockquote class="nv nw nx"><p id="7769" class="lr ls nc lt b lu mo ju lw lx mp jx lz ny mq mc md nz mr mg mh oa ms mk ml mm im bi translated">梅尔根-VC 训练程序。我们分离频谱样本，将它们馈送到发生器 G，将它们连接在一起，并将结果样本馈送到鉴别器 D，以允许无差异地翻译任意长度的样本。我们在传统的生成器-鉴别器 GAN 架构中添加了一个连体网络 S，以在潜在空间中保留矢量算法，从而在翻译中对低级内容进行约束。在还需要保存高级信息(在语音翻译的情况下是语言信息)的任务中添加了可选的身份映射约束。</p></blockquote><p id="a165" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">此外，我们必须为暹罗网络增加一个<strong class="lt iu">边际损失</strong>，以避免它从<strong class="lt iu">退化为学习一个<strong class="lt iu">平凡函数</strong>来满足它的目标。边际损失使得 S 产生的所有向量彼此远离，因此网络不能将相同的精确向量与每个输入相关联，并且必须<strong class="lt iu">学习有意义的关系</strong>创建有用的潜在空间。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/29d539f25112b317dba5a16047a98911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YMDlaxf_daCdyFy8w6swoQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">where delta is a fixed value and t is the transformation vector</figcaption></figure><p id="14dc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">最后，下面是用于训练三个网络的<strong class="lt iu">形式损失</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/b392dde1e144c014327f11b02beef0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KSTLykfvvqcVdJfB49SljQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Final losses for generator G, discriminator D, siamese network S</figcaption></figure><p id="c825" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">值得注意的是，添加的<strong class="lt iu">身份约束</strong>(来自目标域的样本和由生成器翻译的那些相同样本之间的均方误差)仅在<strong class="lt iu">语音翻译</strong>的情况下有用，其中语言信息必须被保留，并且其中我们的<strong class="lt iu">内容丢失</strong>(基于暹罗网络的向量输出)<strong class="lt iu">努力</strong>捕获那些<strong class="lt iu">高级别</strong>信息。</p><p id="d4a9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">如果你在寻找关于这种特殊技术的更多信息，或者如果你更喜欢一个更<strong class="lt iu">正式和系统的解释，我推荐并邀请你阅读我的全文。</strong></p><h1 id="ee9e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="bce9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">今天，我们已经学习了如何使用深度卷积神经网络架构和一些<strong class="lt iu">技巧和技术</strong>来执行<strong class="lt iu">语音翻译</strong>和<strong class="lt iu">音频风格转换</strong>(如音乐流派转换)，以实现对任意长度音频样本的<strong class="lt iu">逼真的</strong>翻译。</p><p id="7fe7" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们现在知道，我们能够<strong class="lt iu">利用</strong>大部分<strong class="lt iu">最近关于深度学习的研究</strong>用于<strong class="lt iu">计算机视觉</strong>应用，也解决与<strong class="lt iu">音频信号</strong>相关的任务，这要归功于图像等效频谱图表示。</p><p id="57c1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">最后，我想以承认这样一个事实来结束我的发言，即<strong class="lt iu">可能会为了<strong class="lt iu">恶意目的</strong>而误用</strong>这种和其他技术，尤其是在<strong class="lt iu">语音翻译</strong>的情况下。随着强大的机器学习方法<strong class="lt iu">的兴起</strong>来创建<strong class="lt iu">真实的虚假数据</strong>我们在探索和使用这种算法时都应该非常<strong class="lt iu">警觉和谨慎</strong>:虽然<strong class="lt iu">研究不会停止也不应该停止</strong>，我们也应该分配资源并研究如何<strong class="lt iu">检测我们帮助创建的虚假数据</strong>。</p><p id="b6e5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">非常感谢<strong class="lt iu">您宝贵的关注</strong>，玩得开心！</p><p id="3479" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">另外，如果您对 GAN 和 GAN 相关的<strong class="lt iu">现成想法</strong>和应用感兴趣，您也应该<strong class="lt iu">查看</strong>:</p><p id="fedc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628"> <strong class="lt iu"> <em class="nc">一年来我训练甘斯的 10 个教训</em></strong></a><strong class="lt iu"><em class="nc"/></strong><em class="nc">(如果你对帮助你完成超级挑战任务的技巧和窍门感兴趣，那就是训练甘斯)</em></p><p id="24e7" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/style-transfer-with-gans-on-hd-images-88e8efcf3716"> <strong class="lt iu"> <em class="nc">在高清图像上使用 GANs 进行风格转换</em></strong></a><strong class="lt iu"><em class="nc"/></strong><em class="nc">(我使用了类似的技术，允许使用很少的计算资源进行大图像的风格转换)</em></p><p id="acab" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/a-new-way-to-look-at-gans-7c6b6e6e9737"> <strong class="lt iu"> <em class="nc">以新的方式看待甘斯</em></strong></a><strong class="lt iu"><em class="nc"/></strong><em class="nc">(我在这里详细探讨了潜在空间是如何工作的，以及它如何被用于图像到图像的翻译任务)</em></p><p id="9d57" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/synthesizing-audio-with-generative-adversarial-networks-8e0308184edd"> <strong class="lt iu"> <em class="nc">用生成式对抗网络合成音频</em></strong></a><strong class="lt iu"><em class="nc"/></strong><em class="nc">(在这里我研究了一篇论文，该论文提出使用卷积 gan 来使用原始波形数据和一维卷积生成音频)</em></p></div></div>    
</body>
</html>