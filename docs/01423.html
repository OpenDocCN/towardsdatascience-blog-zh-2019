<html>
<head>
<title>NLP-based Data Preprocessing Method to Improve Prediction Model Accuracy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于自然语言处理的提高预测模型精度的数据预处理方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-based-data-preprocessing-method-to-improve-prediction-model-accuracy-30b408a1865f?source=collection_archive---------12-----------------------#2019-03-06">https://towardsdatascience.com/nlp-based-data-preprocessing-method-to-improve-prediction-model-accuracy-30b408a1865f?source=collection_archive---------12-----------------------#2019-03-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2e89" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">自然语言处理如何帮助均匀化异构数据集以优化机器学习回归任务的预测模型。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f2784deebc3d24eaf2e94a116d699b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ULy1Q-dIYnGC0Guf"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@fakurian?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Milad Fakurian</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="5676" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">任务概述</h1><p id="f78f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">如今，将<a class="ae kv" href="https://greenice.net/3-ways-use-machine-learning-p2p-marketplace/" rel="noopener ugc nofollow" target="_blank">机器学习用于点对点市场</a>非常流行，因为它可以提高 UX，增加客户忠诚度。在第一部分的<a class="ae kv" rel="noopener" target="_blank" href="/data-structure-evaluation-to-choose-the-optimal-machine-learning-method-eec66076f97a">中，我描述了众包平台【Arcbazar.com】基于 ML 的奖项推荐系统</a>的主要阶段，客户发起设计师竞赛并设立奖金。ML 系统在已完成的带有付费奖项的竞赛的数据集上进行训练，以帮助客户为某个建筑项目设置最佳奖项。经过数据分析，选择了最佳回归算法。</p><h1 id="d6ea" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">问题</h1><p id="1c1a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">正如在上一篇文章中提到的，我对数据集做了一些简化。我将训练数据集的三个文本描述字段替换为一个有数值的字段——字符总数。因此，我得到了 5 个字段的数据集，而不是 7 个字段。</p><p id="38ac" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，测试表明，模型的准确性可以提高。</p><p id="8cca" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这篇文章中，我将告诉您如何使用自然语言处理来升级数据集预处理的预测模型。</p><h1 id="cda0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">假设</h1><p id="af88" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">古希腊哲学家苏格拉底说:“说话，让我能看见你”。这句格言的意思是，我们的言语比我们想说的更多地揭示了我们的个性。此外，它还提示了一个假设，即项目的文本描述可能与奖励金额相关联，并且这些关系比大量字符更强。</p><p id="1c41" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我决定使用自然语言处理方法将文本描述的字符串类型数据转换为数值，目的是丰富和均匀化数据集。</p><p id="e169" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">ML 系统升级分为三个主要步骤:</p><ol class=""><li id="078e" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">文本清理。</li><li id="b37f" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">文本到数字的转换。</li><li id="3800" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">选择最佳回归方法。</li></ol><h1 id="7664" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">文本清理</h1><p id="2795" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">对于数据库的每个文本描述字段，我使用 Python 语言的<a class="ae kv" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">自然语言工具包</a>和<a class="ae kv" href="https://radimrehurek.com/gensim/index.html" rel="noopener ugc nofollow" target="_blank"> gensim </a>库应用了文本清理算法。但是，您可以使用您喜欢的任何 NLP 库。我将文本转换成小写，去掉标点符号和英语停用词。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="dd8c" class="ni kx iq ne b gy nj nk l nl nm">#Transform to lower case<br/>import string</span><span id="793f" class="ni kx iq ne b gy nn nk l nl nm">features['description'] = features['description'].str.lower()</span><span id="b74d" class="ni kx iq ne b gy nn nk l nl nm">#Remove punctuation<br/>table = str.maketrans('', '', string.punctuation)<br/>features['description'] = [features['description'][row].translate(table) for row in range(len(features['description']))]</span><span id="2ded" class="ni kx iq ne b gy nn nk l nl nm">#Remove stopwords<br/>import nltk<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords<br/>stop = stopwords.words('english')</span><span id="4abb" class="ni kx iq ne b gy nn nk l nl nm">features['description'] = features['description'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))</span></pre><h1 id="9332" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">文本到数字的转换</h1><p id="95d5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">然后，每个字段中最常用和最不常用的单词都被删除。语义分析中常用(最频繁)词的含义非常接近停用词。他们给文本添加了一种类似噪音的模式。此外，最不常用的词的意义可以忽略不计，它们可以被过滤掉。这一步的目的是筛选出最有价值的单词。我使用了图形分析，绘制单词和它们的频率。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="a34e" class="ni kx iq ne b gy nj nk l nl nm">#Find words spreading (each word frequency)<br/>freq_d = pd.Series(‘ ‘.join(features[‘description’]).split()).value_counts()</span><span id="a8b0" class="ni kx iq ne b gy nn nk l nl nm">#Plot the words distribution</span><span id="865d" class="ni kx iq ne b gy nn nk l nl nm">freq_d.plot(kind=’line’, ax=None, figsize=None, use_index=True,<br/>            title=None, grid=None, legend=False, style=None,<br/>            logx=False, logy=False, loglog=False, xticks=None,<br/>            yticks=None, xlim=None, ylim=None, rot=None,<br/>            fontsize=None, colormap=None, table=False, yerr=None,<br/>            xerr=None, label=None, secondary_y=False)</span></pre><p id="9668" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">词频可视化帮助我筛选出频率大约在 5 到 200 之间的词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/20bc701112117562bc30b644a2629807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mr8MA4K6qnsi7VNb3-PZtg.png"/></div></div></figure><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="14d0" class="ni kx iq ne b gy nj nk l nl nm">#Remove the least frequent words<br/>rare_d = pd.Series(' '.join(features['description']).split()).value_counts()[-17528:]</span><span id="3b46" class="ni kx iq ne b gy nn nk l nl nm">rare_d = list(rare_d.index)</span><span id="fcfe" class="ni kx iq ne b gy nn nk l nl nm">features['description'] = features['description'].apply(lambda x: " ".join(x for x in x.split() if x not in rare_d))</span><span id="5b76" class="ni kx iq ne b gy nn nk l nl nm">#Remove the most frequent words<br/>freq_d = pd.Series(' '.join(features['description']).split()).value_counts()[:30]</span><span id="84e7" class="ni kx iq ne b gy nn nk l nl nm">freq_d = list(freq_d.index)</span><span id="7c49" class="ni kx iq ne b gy nn nk l nl nm">features['description'] = features['description'].apply(lambda x: " ".join(x for x in x.split() if x not in freq_d))</span></pre><p id="07da" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后每个文本记录被标记化——一个文本被分割成一个单词数组。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="72fc" class="ni kx iq ne b gy nj nk l nl nm">features['description'] = [text.split() for text in features['description']]</span></pre><p id="534a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">单词及其频率的组合给出了一个文本向量，其中每个单词都用它的索引代替。相似的向量表示相似的文本。这就是语义搜索引擎的工作方式。与现代搜索引擎不同，这里我只关注可能的相似性的一个方面——它们的文本(单词)的明显语义相关性。没有超链接，没有随机游走的静态排名，只是对布尔关键字匹配的语义扩展。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="20d2" class="ni kx iq ne b gy nj nk l nl nm">#Create a set of text records for each field<br/>from gensim import corpora</span><span id="7874" class="ni kx iq ne b gy nn nk l nl nm">dict_d = corpora.Dictionary(features['description'])</span></pre><p id="3483" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">将文本转换成单词包(BoW)格式，即(token_id，token_count)元组的列表，是由类<code class="fe np nq nr ne b">gensim.corpora.dictionary.Dictionary()</code> — <code class="fe np nq nr ne b"> .doc2bow()</code>的属性完成的。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="8f38" class="ni kx iq ne b gy nj nk l nl nm">#Convert tokenized text (corpora) to vectors</span><span id="779a" class="ni kx iq ne b gy nn nk l nl nm">corpus_d = [dict_d.doc2bow(line) for line in features['description']]</span></pre><p id="ab4d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了得到一个实数而不是一个向量，我使用了一个范数，它是一个函数，为向量空间中的每个向量指定一个严格为正的长度或大小。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="dcae" class="ni kx iq ne b gy nj nk l nl nm">#Transform vectors of texts to scalar values (calculating norms of vectors)<br/>from numpy import linalg as LA</span><span id="15c4" class="ni kx iq ne b gy nn nk l nl nm">corpus_d_vec_norm = [LA.norm(vec) for vec in corpus_d]<br/>#Replace text descriptions in the database with norms of vectors<br/>features[‘description’] = corpus_d_vec_norm</span></pre><p id="7a6d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，我得到了一个同质的 7 场数据集。</p><h1 id="a1f4" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">选择最佳回归方法</h1><p id="b13a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我使用了之前测试中评级最高的机器学习方法——随机森林回归器——来计算模型如何适应我们的新数据集。决定系数 R 的平方是 5 字段数据集(0.37)的两倍(约 0.75)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/7d004549b85ff1cbc4d66b0d8c472b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*_pHYxBkGurKA88lbfixCrA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Random Forest Regressor results distribution on 7-field data</figcaption></figure><p id="56bf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最初，我有混合数据集:前三个字段(下拉菜单)显然是相互依赖的，接下来的三个字段(描述)更具波动性。</p><p id="74a8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">由于我的假设，描述域在哲学上(甚至心理学上)更接近于奖励值。所以，我把 ML 方法改成了人工神经网络。顺便说一下，该算法在之前的 5 字段数据集测试中被拒绝，因为它的 R 平方非常低，为 0.05。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/0d441ff1a3909afdcfc2b28cce60b8ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*bVkvTM_KWWMPVhheGs5w2w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image source: <a class="ae kv" href="http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html" rel="noopener ugc nofollow" target="_blank">scikit-learn.org</a></figcaption></figure><p id="179d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，<a class="ae kv" href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html" rel="noopener ugc nofollow" target="_blank">多层感知器</a>只有三个隐藏层的回归器给出了 R 平方为 0.999962 的惊人结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/609da5b6f0093814f5d1a41174542bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*ON65O5Z0lL8KXPYe7TJF5A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Multi-layer Perceptron Regression on 7-fields dataset.</figcaption></figure><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="62eb" class="ni kx iq ne b gy nj nk l nl nm">import sklearn<br/>from sklearn.neural_network import MLPRegressor</span><span id="3faa" class="ni kx iq ne b gy nn nk l nl nm">mlpreg = MLPRegressor(hidden_layer_sizes=(3,), activation=’relu’,<br/>         solver=’adam’, alpha=0.001, batch_size=’auto’,<br/>         learning_rate=’adaptive’, learning_rate_init=0.01,<br/>         power_t=0.5, max_iter=1000, shuffle=True, random_state=9,<br/>         tol=0.0001, verbose=False, warm_start=False, momentum=0.9,<br/>         nesterovs_momentum=True, early_stopping=False,<br/>         validation_fraction=0.1, beta_1=0.9, beta_2=0.999,<br/>         epsilon=1e-08)</span></pre><h1 id="d32a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="726d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">1.所获得的结果显示了向量的范数对于替换文本向量的适用性，而没有显著的数据丢失。</p><p id="1769" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">2.客户对他/她的需求的口头描述比下拉菜单更接近于奖金数额，因为这些文字可能表达了人类的本质或共同的动机。</p><p id="db12" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">3.基于人工神经网络的算法在“理解”人性方面比随机森林更好，首先是因为它在结构上与人脑相似。</p></div></div>    
</body>
</html>