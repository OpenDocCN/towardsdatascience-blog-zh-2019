<html>
<head>
<title>Meta Learning — AI Generalised.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">元学习——人工智能一般化。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/meta-learning-ai-generalised-1007b9695fe1?source=collection_archive---------10-----------------------#2019-06-08">https://towardsdatascience.com/meta-learning-ai-generalised-1007b9695fe1?source=collection_archive---------10-----------------------#2019-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1beb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">人工智能学会学习，帮助从几个“镜头”中学习。</h2></div><p id="cfce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">深度学习已经在各个领域取得了巨大的成功，并且正在继续展开它的翅膀。但是，训练任何传统神经网络模型的一个主要问题是需要大量的数据，并使用这些数据对许多标记的例子进行多次迭代更新。</p><p id="bf3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来看一个猫 vs 狗分类的经典例子。虽然在过去的二十年里，我们已经使我们的模型越来越好，以增加准确性，但上述基本问题仍然存在。我们仍然需要大量贴有标签的狗和猫来获得相当的准确性。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/9cd1e5c1c919bad1ba177e217f13ecb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*R-ItxBW2SWarITBKe7HZuA.gif"/></div></div></figure><p id="c046" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">人类如何用少得多的例子对它们进行分类。比方说，突然向你展示了两种新的动物，它们在视觉上就像猫和狗一样容易区分。我很确定任何正常人都可以在不到 100 个例子中获得相当的准确性。怎么会？？多年来，我们已经了解了动物的基本结构。我们知道如何提取特征，如脸型、头发、尾巴、身体结构等..简而言之，我们已经掌握了<strong class="kk iu">学会学习的艺术。</strong></p><p id="b9d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">元学习的目标基本上是<strong class="kk iu">学会学习，用最少的数据将人工智能推广到许多不同的场景。</strong>你可以说，迁移学习不也是这样吗？嗯，是的，这是朝着正确的方向发展，但还不够远。已经观察到，随着网络被训练的任务偏离目标任务，预训练网络的益处大大降低。元学习建议在两个层次上构建学习问题。第一个是在每个单独的任务中快速获取知识。这个过程是由第二个引导的，第二个过程是从所有的任务中慢慢提取信息。</p><p id="558b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">元学习算法可以大致分为三类—</p><h1 id="6c2b" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">经典的基于梯度下降的方法</h1><p id="f02a" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">这类方法背后的直觉是再次使用标准梯度下降更新来使神经网络一般化到各种各样的数据集。</p><p id="77fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种方法中，我们使用一组数据集，每个数据集有几个例子，假设每个数据集有 k 个例子，用于“<em class="mn"> k-shot 学习</em>”。设数据集的集合为 p(T)。假设我们的模型是一个参数化的函数ₜₕₑₜₐ.如果我们从参数<em class="mn"> θ、</em>开始，我们知道模型随着每个单独数据集的标准梯度下降更新而更新。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mo"><img src="../Images/690546067b4b7d36f9e5524f19182f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SqpstUtV_PO03mR-GM1dgg.png"/></div></div></figure><p id="acf3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们希望我们的模型能够适用于大范围的数据集。因此，我们需要 p(T)中所有数据集的误差总和以及更新后的参数。这可以用数学方法表达如下—</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/ce3884dc9353cf95943a57d4041bd62d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mlXqe1ISsWRzQADP0IXZmw.png"/></div></div></figure><p id="91eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于 p(T)中的一批数据集，我们用一个<strong class="kk iu">标准 SGD 更新</strong>来更新<em class="mn"> θ w.r. </em> t .上面提到的元目标</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mo"><img src="../Images/db453a8bb3a477f3172174a0b70c591c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wyf90JVgarel99952gIQ8Q.png"/></div></div></figure><p id="41a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，通过模型的梯度反向传播元损失包括计算导数的导数。这可以使用 TensorFlow 支持的<strong class="kk iu"> Hessian-vector products，</strong>来完成。</p><h1 id="ab79" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">最近邻方法</h1><p id="4761" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">这组方法的指导思想是最近邻算法不需要任何训练，但是性能取决于所选择的度量。</p><p id="b707" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它们包括一个将输入域映射到特征空间的嵌入模型和一个将特征空间映射到任务变量的基本学习器。元学习的目标是学习一个嵌入模型，使得基础学习者能够很好地跨任务进行概括。这里是嵌入的基于距离的预测规则。我们将看一个具体的例子，<strong class="kk iu">匹配网络</strong>来理解工作原理。</p><p id="38a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">匹配网络支持一组 k 个图像标签对的例子 S={(xᵢ ,yᵢ)}到分类器 cₛ(x'，该分类器在给定测试例子 x’的情况下定义了输出 y’上的概率分布。S → cₛ(x')定义为 P(y'|x '，s)，其中 p 由神经网络参数化。因此，给定一个新的示例 S’的支持集，我们简单地使用由 P 定义的参数神经网络来为每个测试示例 x’:P(y ' | x '，S’)预测适当的标签 y’。简单地说，我们可以说—</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/214c7567ef30d19599bada0db76ff552.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*psqwRLzMWc-PIjOFeQAVIA.png"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">y cap and x cap — new example</figcaption></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/4a157deedb5556b9a72c54e37905587f.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*p4uXZ8rqz3GbfxWAAH11Dw.png"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">the attention mechanism</figcaption></figure><p id="1353" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述方法让人想起 KDE 和 kNN 算法。</p><p id="25f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mn"> f </em>和<em class="mn"> g </em>是适合 x cap 和 xᵢ的嵌入式神经网络，这取决于任务。它们是用于图像任务(如 Inception)的深度 CNN 或用于语言任务的简单形式单词嵌入。</p><h1 id="4d4a" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">使用辅助空间的基于模型的方法</h1><p id="b3e5" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">我们人类在处理东西的同时，也储存了表示以备将来使用。所以这些算法试图通过一些辅助内存块来模拟它。基本的策略是学习它应该放入记忆中的表示类型，以及它以后应该如何使用这些表示进行预测。</p><p id="aad4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这些方法中，输入序列和输出标签是按顺序给出的。一个数据集 d yₜ)}={dₜ}={(xₜ，其中<em class="mn"> t </em>表示时间步长。xₜ.之后没有给出输出标签 yₜ对于每个数据集，输入和输入理想标签是混乱的。期望模型在给定的时间步输出 yₜxₜ(i.e.的适当标签。因此，它被迫将数据样本存储在内存中，直到找到合适的标签，之后可以绑定并存储样本类信息以备后用。</p><p id="f2b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们要讲的这个具体实现中的记忆模块是<strong class="kk iu">神经图灵机(NTM) </strong>。它基本上是一个图灵机(<em class="mn">读写头在一个内存块上</em>)和一个<strong class="kk iu"> LSTM </strong> ( <em class="mn">)或者有时是简单的基于神经网络</em> ) <strong class="kk iu">的控制器</strong>。NTM 外部存储器模块中的存储器编码和检索是快速的，每个时间步都有可能将向量表示放入或取出存储器。这种能力使得 NTM 成为元学习和低射预测的完美候选，因为它既能够通过缓慢更新其权重进行长期存储，又能够通过其外部存储模块进行短期存储。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mv"><img src="../Images/6e349520286b543bf2b4be06f87da0e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uMblNkgSSN9fsVMxp6IU6A.png"/></div></div></figure><p id="f723" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在某个时间步骤 t，给定<strong class="kk iu"> xₜ </strong>，LSTM 控制器给出一个密钥<strong class="kk iu"> kₜ </strong>用于访问存储器矩阵<strong class="kk iu"> Mₜ </strong>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/7436c488a5b5bafe16a601c651f82c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*ah8nAYqlaaOW4Eg7KuQz0w.png"/></div></figure><p id="6745" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Softmax 用于产生读写向量。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mx"><img src="../Images/61e8ba42c59c4ae343b43cf0188b1466.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*wCB8eE4geE7_EdrlufaIVw.png"/></div></div></figure><p id="843d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是 rₜ.用来获取记忆的</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi my"><img src="../Images/2e9d1abc3fa9a261386c248ab99812d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*VLzoU30GzDu2Rhak9p5HZA.png"/></div></div></figure><p id="0ba1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这被用作下一个控制器状态的输入以及基于软最大值的分类器的输入。</p></div></div>    
</body>
</html>