<html>
<head>
<title>Deep Learning for Natural Language Processing Using word2vec-keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 word2vec-keras 进行自然语言处理的深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-for-natural-language-processing-using-word2vec-keras-d9a240c7bb9d?source=collection_archive---------16-----------------------#2019-11-04">https://towardsdatascience.com/deep-learning-for-natural-language-processing-using-word2vec-keras-d9a240c7bb9d?source=collection_archive---------16-----------------------#2019-11-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b546" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">结合 Word2Vec 和 Keras LSTM 的自然语言处理深度学习方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4311e11d552ceb6471eff8997a8065a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fEJypry4nBiZDEN3ojjdVg.jpeg"/></div></div></figure><p id="584f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP) </a>是语言学、计算机科学、信息工程、人工智能等多个研究领域共有的研究子领域。NLP 通常关注计算机和人类自然语言之间的交互，特别是如何使用计算机来处理和分析自然语言数据(例如，文本、语音等)。).NLP 中的一些主要挑战包括语音识别、自然语言理解和自然语言生成。</p><p id="6850" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">文本是自然语言处理数据最广泛的形式之一。它可以被视为字符序列或单词序列，但随着深度学习的进步，趋势是在单词级别上工作。给定一个单词序列，在它能够被机器学习或深度学习算法/模型(如 LSTM)理解之前，它必须以某种方式转换成数字。一种直接的方法是使用<a class="ae lq" href="https://en.wikipedia.org/wiki/One-hot" rel="noopener ugc nofollow" target="_blank">一键编码</a>将每个单词映射到词汇长度的稀疏向量。另一种方法(例如，<a class="ae lq" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> Word2vec </a>)使用<a class="ae lq" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">单词嵌入</a>将单词转换成可配置长度的紧凑向量。</p><p id="2596" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在传统机器学习的 NLP 中[1]，文本数据预处理和特征工程都是必需的。最近发布了一个新的深度学习模型<a class="ae lq" href="https://pypi.org/project/word2vec-keras/" rel="noopener ugc nofollow" target="_blank"> Word2Vec-Keras 文本分类器</a>【2】，用于无特征工程的文本分类。它通过一个嵌入层将<a class="ae lq" href="https://pypi.org/project/gensim/" rel="noopener ugc nofollow" target="_blank">Gensim</a>【3】的<em class="lr"> Word2Vec </em>模型(一个用于大型语料库的主题建模、文档索引和相似性检索的 Python 库)与 Keras LSTM 结合起来作为输入。</p><p id="d6f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本文中，与[1]类似，我使用公开的<a class="ae lq" href="https://www.kaggle.com/uciml/sms-spam-collection-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle 垃圾短信收集数据集</a> [4]来评估<em class="lr"> Word2VecKeras </em>模型在没有特征工程的情况下在垃圾短信分类中的性能。涵盖了以下两种情况:</p><ul class=""><li id="edd5" class="ls lt it kw b kx ky la lb ld lu lh lv ll lw lp lx ly lz ma bi translated">基于数据预处理的垃圾短信分类</li><li id="c585" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">无需数据预处理的垃圾短信分类</li></ul><p id="f8bb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下代码用于导入所有必需的 Python 库:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="7de6" class="ml mm it mh b gy mn mo l mp mq">from word2vec_keras import Word2VecKeras<br/>from pprint import pprint<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import itertools<br/>import numpy as np<br/>import nltk<br/>import string<br/>import re<br/>import ast # abstract syntax tree: <a class="ae lq" href="https://docs.python.org/3/library/ast.html" rel="noopener ugc nofollow" target="_blank">https://docs.python.org/3/library/ast.html</a><br/>from sklearn.model_selection import train_test_split<br/>import mlflow<br/>import mlflow.sklearn</span><span id="3a50" class="ml mm it mh b gy mr mo l mp mq">%matplotlib inline</span></pre><p id="8bf0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦 SMS 数据集文件<em class="lr"> spam.csv </em>被下载到计算机上，下面的代码可以将本地数据集文件加载到 Pandas DataFrame 中，如下所示:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="b09e" class="ml mm it mh b gy mn mo l mp mq">column_names = ['label', 'body_text', 'missing_1', 'missing_2', 'missing_3']<br/>        raw_data = pd.read_csv('./data/spam.csv', encoding = "ISO-8859-1")<br/>        raw_data.columns = column_names<br/>        raw_data.drop(['missing_1', 'missing_2', 'missing_3'], axis=1, inplace=True)<br/>        raw_data = raw_data.sample(frac=1.0)<br/>        raw_data.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/118db8978e823af6500f9b3c93cb8465.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*qUkvBWAmvEAIJM_sJ6n4ZQ.jpeg"/></div></figure><p id="4b52" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请注意，加载此数据集需要使用编码格式 ISO-8859–1，而不是默认的编码格式 UTF-8。</p><h1 id="8e0d" class="mt mm it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">1.带有数据预处理的垃圾邮件分类</h1><p id="910d" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">在本节中，首先，应用类似于[1]的数据预处理程序来清理 SMS 数据集。然后，生成的干净数据集被输入到<em class="lr"> Word2VecKeras </em>模型中，用于垃圾短信的模型训练和预测。mlflow [5][6]用于追踪模型执行的历史。</p><h2 id="9d8d" class="ml mm it bd mu np nq dn my nr ns dp nc ld nt nu ne lh nv nw ng ll nx ny ni nz bi translated">1.1 数据预处理</h2><p id="3473" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated"><em class="lr">预处理</em>类的<em class="lr">预处理</em>()方法对短信原始数据进行如下预处理:</p><ul class=""><li id="4ddc" class="ls lt it kw b kx ky la lb ld lu lh lv ll lw lp lx ly lz ma bi translated">删除标点符号</li><li id="8a0f" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">标记化</li><li id="57b2" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">删除停用词</li><li id="cb8c" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">应用词干</li><li id="abc2" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">应用词汇化</li><li id="c7d7" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">将标记加入句子</li><li id="bdf7" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">删除中间数据列</li></ul><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="3264" class="ml mm it mh b gy mn mo l mp mq">class Preprocessing(object):<br/>    def __init__(self, data, target_column_name='body_text_clean'):<br/>        self.data = data<br/>        self.feature_name = target_column_name<br/>        <br/>    def remove_punctuation(self, text):<br/>        text_nopunct = "".join([char for char in text if char not in string.punctuation])# It will discard all punctuations<br/>        return text_nopunct<br/>    <br/>    def tokenize(self, text):<br/>        # Match one or more characters which are not word character<br/>        tokens = re.split('\W+', text) <br/>        return tokens<br/>    <br/>    def remove_stopwords(self, tokenized_list):<br/>        # Remove all English Stopwords<br/>        stopword = nltk.corpus.stopwords.words('english')<br/>        text = [word for word in tokenized_list if word not in stopword]<br/>        return text</span><span id="4527" class="ml mm it mh b gy mr mo l mp mq">    def stemming(self, tokenized_text):<br/>        ps = nltk.PorterStemmer()<br/>        text = [ps.stem(word) for word in tokenized_text]<br/>        return text<br/>    <br/>    def lemmatizing(self, tokenized_text):<br/>        wn = nltk.WordNetLemmatizer()<br/>        text = [wn.lemmatize(word) for word in tokenized_text]<br/>        return text<br/>    <br/>    def tokens_to_string(self, tokens_string):<br/>        try:<br/>            list_obj = ast.literal_eval(tokens_string)<br/>            text = " ".join(list_obj)<br/>        except:<br/>            text = None<br/>        return text<br/>    <br/>    def dropna(self):<br/>        feature_name = self.feature_name<br/>        if self.data[feature_name].isnull().sum() &gt; 0:<br/>            column_list=[feature_name]<br/>            self.data = self.data.dropna(subset=column_list)<br/>            return self.data<br/>        <br/>    def preprocessing(self):<br/>        self.data['body_text_nopunc'] = self.data['body_text'].apply(lambda x: self.remove_punctuation(x))<br/>        self.data['body_text_tokenized'] = self.data['body_text_nopunc'].apply(lambda x: self.tokenize(x.lower())) <br/>        self.data['body_text_nostop'] = self.data['body_text_tokenized'].apply(lambda x: self.remove_stopwords(x))<br/>        self.data['body_text_stemmed'] = self.data['body_text_nostop'].apply(lambda x: self.stemming(x))<br/>        self.data['body_text_lemmatized'] = self.data['body_text_nostop'].apply(lambda x: self.lemmatizing(x))<br/>        <br/>        # save cleaned dataset into csv file and load back<br/>        self.save()<br/>        self.load()<br/>        <br/>        self.data[self.feature_name] = self.data['body_text_lemmatized'].apply(lambda x: self.tokens_to_string(x))<br/>        <br/>        self.dropna()<br/>        <br/>        drop_columns = ['body_text_nopunc', 'body_text_tokenized', 'body_text_nostop', 'body_text_stemmed', 'body_text_lemmatized'] <br/>        self.data.drop(drop_columns, axis=1, inplace=True)<br/>        return self.data<br/>    <br/>    def save(self, filepath="./data/spam_cleaned.csv"):<br/>        self.data.to_csv(filepath, index=False, sep=',')  <br/>        <br/>    def load(self, filepath="./data/spam_cleaned.csv"):<br/>        self.data = pd.read_csv(filepath)<br/>        return self.data</span></pre><p id="c51e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">结果数据保存在新列<em class="lr"> body_text_clean </em>中，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/34354136775323b97d389a49012dfaf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NZ83WAoCs9ROe0kWysBgDA.png"/></div></div></figure><p id="de04" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在上述数据预处理中，<a class="ae lq" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">自然语言工具包(NLTK) </a>的<em class="lr">停用词</em>和<em class="lr"> wordnet </em>数据文件都是必需的，需要在 Mac 上手动下载(此处<a class="ae lq" href="http://www.nltk.org/nltk_data/" rel="noopener ugc nofollow" target="_blank">可用</a>)。<em class="lr"> nltk.download() </em>方法无法正常工作。</p><h2 id="20f1" class="ml mm it bd mu np nq dn my nr ns dp nc ld nt nu ne lh nv nw ng ll nx ny ni nz bi translated">1.2 建模</h2><p id="6e31" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated"><em class="lr"> SpamClassifier </em>类的<em class="lr"> prepare_data </em>()方法是获取准备建模的短信数据，如下所示:</p><ul class=""><li id="9f35" class="ls lt it kw b kx ky la lb ld lu lh lv ll lw lp lx ly lz ma bi translated">将数据集文件<em class="lr"> spam.csv </em>加载到 Pandas DataFrame</li><li id="af34" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">使用<em class="lr">预处理</em>类预处理原始数据(参见<em class="lr"> body_text_clean </em>列)</li><li id="5398" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">将数据预处理后的干净数据分成训练和测试数据集</li><li id="29b9" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">将训练和测试数据集重新格式化为 Python 列表，以与模型<em class="lr"> Word2VecKeras </em> API [2]保持一致</li></ul><p id="cf4b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦数据为建模做好准备，就可以调用<em class="lr"> train_model </em>()方法来训练<em class="lr"> Word2VecKeras </em>模型。然后可以调用方法<em class="lr"> evaluate </em>()和<em class="lr"> predict </em>()来获得模型性能度量(例如，准确性)并分别执行预测。</p><p id="8aa0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lr"> mlFlow </em>()方法将上述方法调用、模型执行结果跟踪、将训练好的模型记录到文件中组合成一个工作流。</p><p id="20a0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意，超参数<em class="lr"> w2v_min_count </em>的值是忽略总频率低于该值的所有单词。因此，需要根据特定的数据集进行调整。如果它设置得太高(例如，本文中使用的 SMS spam 数据集的值为 5)，则会由于空句而出现词汇错误。</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="8213" class="ml mm it mh b gy mn mo l mp mq">class SpamClassifier(object):<br/>    def __init__(self):<br/>        self.model = Word2VecKeras()<br/>        <br/>    def load_data(self):<br/>        column_names = ['label', 'body_text', 'missing_1', 'missing_2', 'missing_3']<br/>        data = pd.read_csv('./data/spam.csv', encoding = "ISO-8859-1")<br/>        data.columns = column_names<br/>        data.drop(['missing_1', 'missing_2', 'missing_3'], axis=1, inplace=True)<br/>        self.raw_data = data.sample(frac=1.0) <br/>        <br/>        return self.raw_data<br/>    <br/>    def split_data(self):<br/>        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x, self.y, test_size=0.25, random_state=42)<br/>        <br/>    def numpy_to_list(self):<br/>        self.x_train = self.x_train.tolist()<br/>        self.y_train = self.y_train.tolist()<br/>        self.x_test  = self.x_test.tolist()<br/>        self.y_test  = self.y_test.tolist()<br/>    <br/>    def prepare_data(self, feature, label='label'):<br/>        self.load_data()<br/>        pp = Preprocessing(self.raw_data)<br/>        self.data = pp.preprocessing()<br/>        self.x = self.data[feature].values<br/>        self.y = self.data[label].values<br/>        self.split_data()<br/>        self.numpy_to_list()<br/>        <br/>        return self.data<br/>        <br/>    def train_model(self):<br/>        self.w2v_size = 300<br/>        self.w2v_min_count = 1 # 5<br/>        self.w2v_epochs = 100<br/>        self.k_epochs = 5 # 32<br/>        self.k_lstm_neurons = 512<br/>        self.k_max_sequence_len = 1000<br/>        <br/>        self.model.train(self.x_train, self.y_train, <br/>            w2v_size=self.w2v_size, <br/>            w2v_min_count=self.w2v_min_count, <br/>            w2v_epochs=self.w2v_epochs, <br/>            k_epochs=self.k_epochs, <br/>            k_lstm_neurons=self.k_lstm_neurons, <br/>            k_max_sequence_len=self.k_max_sequence_len, <br/>            k_hidden_layer_neurons=[])<br/>        <br/>    def evaluate(self):<br/>        self.result = self.model.evaluate(self.x_test, self.y_test)<br/>        self.accuracy = self.result["ACCURACY"]<br/>        self.clf_report_df = pd.DataFrame(self.result["CLASSIFICATION_REPORT"])<br/>        self.cnf_matrix = self.result["CONFUSION_MATRIX"]<br/>        return self.result<br/>    <br/>    def predict(self, idx=1):<br/>        print("LABEL:", self.y_test[idx])<br/>        print("TEXT :", self.x_test[idx])<br/>        print("/n============================================")<br/>        print("PREDICTION:", self.model.predict(self.x_test[idx]))<br/>        <br/>    def mlFlow(self, feature='body_text_clean'):<br/>        np.random.seed(40)  <br/>        with mlflow.start_run():<br/>            self.prepare_data(feature=feature) # feature should be 'body_text' if no need to preprocessing<br/>            self.train_model()<br/>            self.evaluate()<br/>            self.predict()<br/>            mlflow.log_param("feature", feature) <br/>            mlflow.log_param("w2v_size", self.w2v_size)  <br/>            mlflow.log_param("w2v_min_count", self.w2v_min_count)<br/>            mlflow.log_param("w2v_epochs", self.w2v_epochs)<br/>            mlflow.log_param("k_lstm_neurons", self.k_lstm_neurons)<br/>            mlflow.log_param("k_max_sequence_len", self.k_max_sequence_len)<br/>            mlflow.log_metric("accuracy", self.accuracy)<br/>            mlflow.sklearn.log_model(self.model, "Word2Vec-Keras")</span></pre><p id="262a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的代码展示了如何实例化一个<em class="lr"> SpamClassifier </em>对象，并调用<em class="lr"> mlFlow </em>()方法进行数据预处理建模和预测:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="32d9" class="ml mm it mh b gy mn mo l mp mq">spam_clf = SpamClassifier()<br/>spam_clf.mlFlow(feature='body_text_clean')</span></pre><h2 id="7077" class="ml mm it bd mu np nq dn my nr ns dp nc ld nt nu ne lh nv nw ng ll nx ny ni nz bi translated">1.3 比较</h2><p id="3342" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">一种简单的基线分类算法是预测类别(垃圾邮件或 ham)的大多数(即 ham)。任何有用的监督机器学习分类模型都必须在性能上打败它。在 Kaggle 垃圾短信收集数据集中，共有 5572 个样本，其中 747 个是垃圾短信，4825 个是火腿短信。因此，基准算法性能的准确率约为 86.6%。</p><p id="e6bb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在[1]中，首先将类似的数据预处理过程应用于相同的 Kaggle 垃圾短信数据集。然后对预处理后的数据集进行特征工程，以获得建模特征，如<em class="lr">文本消息长度</em>和<em class="lr">文本中标点符号的百分比。</em>然后 sci kit-learn<em class="lr">RandomForestClassifier</em>模型被训练用于预测。获得的准确度约为 97.7%。</p><p id="e00b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本文中，数据预处理后，<em class="lr"> Word2VecKeras </em>模型直接在预处理后的数据集上训练进行预测，不需要任何特征工程。5 个历元达到的精度约为 98.5%。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/57f351bd96387d8ae5ba52e9ef8c9412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EgFNjdw0_2PaG3k_c1WPyw.png"/></div></div></figure><p id="8275" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">结果表明，无论是传统的机器学习方法[1]还是本文提出的新的深度学习方法，其准确率都明显优于基线算法。</p><h1 id="bf54" class="mt mm it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">2.无需数据预处理的垃圾邮件分类</h1><p id="22b2" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">在这一部分中，一旦 Kaggle 垃圾短信收集数据集被加载，原始数据(参见<em class="lr"> body_text </em>列)就被直接馈送到<em class="lr"> Word2Vec-Keras </em>模型中，用于垃圾短信的模型训练和预测。既不使用数据预处理也不使用特征工程。与上一节类似，<em class="lr"> mlflow </em> [5][6】用于跟踪模型执行的历史。这是通过以下方式实现的:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="c848" class="ml mm it mh b gy mn mo l mp mq">spam_clf = SpamClassifier()<br/>spam_clf.mlFlow(feature='body_text')</span></pre><p id="1c54" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如下图所示，获得的 5 个历元的准确率约为 99.0%，与前一节中数据预处理的垃圾邮件分类准确率的<em class="lr"> Word2VecKeras </em>模型性能相当。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/2ababa88bbc7a4286ed0c6d38c2454bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CtTVhQsf7RDXZ19WyDdD5A.png"/></div></div></figure><p id="0100" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">mlflow UI 的以下快照显示了模型执行的历史:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/0df530976458a6f923c5330c4311c821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cbki04BD4a8Lt-aCpjB1MA.png"/></div></div></figure><h1 id="b28d" class="mt mm it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">摘要</h1><p id="6cef" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">在本文中，使用公开的<a class="ae lq" href="https://www.kaggle.com/uciml/sms-spam-collection-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle 垃圾短信收集数据集</a> [4]来评估新的<em class="lr"> Word2VecKeras </em>模型在没有特征工程的情况下在垃圾短信分类中的性能。</p><p id="3f51" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">涵盖了两个场景。一种是应用通用的文本数据预处理来清理原始数据集，然后使用清理后的数据集来训练预测模型。另一种方法直接使用未经任何数据预处理的原始数据集进行模型训练和预测。</p><p id="56ce" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">模型在准确性方面的性能结果表明,<em class="lr"> Word2VecKeras </em>模型的性能优于传统的 NLP 方法[1],并且在上述两种情况下的性能相似。这表明新的<em class="lr"> Word2VecKeras </em>模型具有直接应用于原始文本数据进行文本分类的潜力，而无需文本数据预处理或特征工程。</p><p id="c8c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本文中的所有源代码都可以在 Github [7]中找到。</p><h1 id="ef51" class="mt mm it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">参考</h1><p id="c063" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">[1].b .谢蒂，<a class="ae lq" rel="noopener" target="_blank" href="/natural-language-processing-nlp-for-machine-learning-d44498845d5b">机器学习的自然语言处理</a></p><p id="9c17" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[2].<a class="ae lq" href="https://pypi.org/project/word2vec-keras/" rel="noopener ugc nofollow" target="_blank"> Word2Vec-Keras 文本分类器</a></p><p id="9e15" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[3].<a class="ae lq" href="https://pypi.org/project/gensim/" rel="noopener ugc nofollow" target="_blank"> Gensim </a></p><p id="f03c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[4].<a class="ae lq" href="https://www.kaggle.com/uciml/sms-spam-collection-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle 垃圾短信收集数据集</a></p><p id="b0ee" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[5].<a class="ae lq" href="https://www.mlflow.org/" rel="noopener ugc nofollow" target="_blank"> mlflow </a></p><p id="71e4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[6].张，<a class="ae lq" rel="noopener" target="_blank" href="/object-oriented-machine-learning-pipeline-with-mlflow-for-pandas-and-koalas-dataframes-ef8517d39a12">面向对象的机器学习流水线与 mlflow 熊猫和考拉数据框架</a></p><p id="a8bb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[7].<a class="ae lq" href="https://github.com/yzzhang/machine-learning/tree/master/deep_learning/nlp/spam-classification-with-word2vec-keras" rel="noopener ugc nofollow" target="_blank">Github</a>Jupyter 笔记本 Y. Zhang</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="39f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">披露声明:2019 首创一。观点是作者个人的观点。除非本帖中另有说明，否则 Capital One 不隶属于所提及的任何公司，也不被这些公司认可。使用或展示的所有商标和其他知识产权是其各自所有者的财产。</p></div></div>    
</body>
</html>