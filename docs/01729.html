<html>
<head>
<title>Implementing the XOR Gate using Backpropagation in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用神经网络反向传播实现异或门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d?source=collection_archive---------1-----------------------#2019-03-21">https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d?source=collection_archive---------1-----------------------#2019-03-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="ea54" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi kv translated">使用神经网络实现逻辑门有助于理解神经网络处理其输入以达到特定输出的数学计算。这个神经网络将处理 XOR 逻辑问题。XOR(异或门)是一种数字逻辑门，只有当它的两个输入彼此不同时，它才给出真输出。XOR 门的真值表如下所示:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/5cc282e7515c2fd4f8124fcbbb2e9adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*01idVj7sVw2ZnGZFapvW4A.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Truth Table for XOR</figcaption></figure><p id="71f0" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">神经网络的目标是根据上述真值表对输入模式进行分类。如果输入模式是根据它们的输出绘制的，可以看出这些点不是线性可分的。因此，必须对神经网络进行建模，以使用<em class="lq">决策平面来分离这些输入模式。</em></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/425638b0f47d02488419ae23808f423f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*_HLG8KlGJFZxtWoB8J1kFA.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">XOR, Graphically</figcaption></figure></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="e5df" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">神经网络模型</h1><p id="be0a" class="pw-post-body-paragraph jx jy it jz b ka mq kc kd ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku im bi translated">如前所述，神经网络需要产生两个不同的决策平面，以根据输出模式线性分离输入数据。这是通过使用<em class="lq">隐藏层</em>的概念实现的。神经网络将由一个具有两个节点(X1，X2)的输入层组成；一个具有两个节点的隐藏层(因为需要两个决策平面)；和一个具有一个节点(Y)的输出层。因此，神经网络看起来像这样:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/16af1c7f074731c87717cf4ce72b70f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*qXt_iBvWods-FOvTldxYFw.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">The Neural Network Model to solve the XOR Logic (from: <a class="ae mw" href="https://stopsmokingaids.me/" rel="noopener ugc nofollow" target="_blank">https://stopsmokingaids.me/</a>)</figcaption></figure><h1 id="da87" class="ls lt it bd lu lv mx lx ly lz my mb mc md mz mf mg mh na mj mk ml nb mn mo mp bi translated">乙状结肠神经元</h1><p id="7f65" class="pw-post-body-paragraph jx jy it jz b ka mq kc kd ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku im bi translated">为了实现 XOR 门，我将使用一个 Sigmoid 神经元作为神经网络中的节点。乙状结肠神经元的特征是:</p><p id="0e1d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">1.可以接受实值作为输入。</p><p id="b278" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">2.激活值等于其输入的加权和<br/>，即∑wi xi</p><p id="cb3f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">3.乙状结肠神经元的输出是乙状结肠函数的函数，也称为逻辑回归函数。sigmoid 函数是一个连续函数，它输出 0 到 1 之间的值:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/baaa859595038034a48477d434f39456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*R4twuYNUKXVzsvgOGkPZsA.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">The Sigmoidal Curve</figcaption></figure></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="c2bd" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">学习算法</h1><p id="fa45" class="pw-post-body-paragraph jx jy it jz b ka mq kc kd ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku im bi translated">神经网络的信息存储在神经元之间的互连中，即权重中。神经网络通过根据帮助其收敛到预期输出的学习算法更新其权重来进行学习。学习算法是基于损失函数改变权重和偏差的原理方法。</p><ol class=""><li id="0d90" class="nc nd it jz b ka kb ke kf ki ne km nf kq ng ku nh ni nj nk bi translated">随机初始化权重和偏差。</li><li id="3e89" class="nc nd it jz b ka nl ke nm ki nn km no kq np ku nh ni nj nk bi translated">迭代数据<br/> i。使用 sigmoid 函数<br/> ii 计算预测输出。使用平方误差损失函数<br/> iii 计算损失。W(新)= W(旧)—α∏W<br/>iv。B(新)= B(旧)—αB</li><li id="5235" class="nc nd it jz b ka nl ke nm ki nn km no kq np ku nh ni nj nk bi translated">重复直到误差最小</li></ol><p id="7d06" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这是一个相当简单的学习算法，仅包含算术运算来更新权重和偏差。该算法可分为两部分:前向传递<em class="lq">和后向传递<em class="lq">也称为<em class="lq">“反向传播”</em></em></em></p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="28ba" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">让我们实现算法的第一部分。我们将根据 XOR 的真值表初始化我们的权重和预期输出。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="da14" class="nv lt it nr b gy nw nx l ny nz">inputs = np.array([[0,0],[0,1],[1,0],[1,1]])<br/>expected_output = np.array([[0],[1],[1],[0]])</span></pre><p id="fa93" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">步骤 1:用随机值初始化权重和偏差</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="8f2e" class="nv lt it nr b gy nw nx l ny nz">import numpy as np</span><span id="123f" class="nv lt it nr b gy oa nx l ny nz">inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,2,1</span><span id="7ce2" class="nv lt it nr b gy oa nx l ny nz">hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))<br/>hidden_bias =np.random.uniform(size=(1,hiddenLayerNeurons))</span><span id="a670" class="nv lt it nr b gy oa nx l ny nz">output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))<br/>output_bias = np.random.uniform(size=(1,outputLayerNeurons))</span></pre><p id="8853" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">正向传递包括计算预测输出，该输出是给予神经元的输入的加权和的函数:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ob"><img src="../Images/00a54104b165597502d5feb2e6f2d065.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*m6Bs9A8FvWbqCa7nqn_GWg.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">The Sigmoid Function</figcaption></figure><p id="9412" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">其中σwx+b 称为激活。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="1033" class="nv lt it nr b gy nw nx l ny nz">def sigmoid (x):<br/>    return 1/(1 + np.exp(-x))</span><span id="14a0" class="nv lt it nr b gy oa nx l ny nz">hidden_layer_activation = np.dot(inputs,hidden_weights)<br/>hidden_layer_activation += hidden_bias<br/>hidden_layer_output = sigmoid(hidden_layer_activation)</span><span id="6a5c" class="nv lt it nr b gy oa nx l ny nz">output_layer_activation = np.dot(hidden_layer_output,output_weights)<br/>output_layer_activation += output_bias<br/>predicted_output = sigmoid(output_layer_activation)</span></pre><p id="6271" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这就完成了一次正向传递，其中需要将我们的预测输出与预期输出进行比较。基于这种比较，使用反向传播来改变隐藏层和输出层的权重。反向传播是使用梯度下降算法完成的。</p><h1 id="9e21" class="ls lt it bd lu lv mx lx ly lz my mb mc md mz mf mg mh na mj mk ml nb mn mo mp bi translated">梯度下降</h1><p id="82b5" class="pw-post-body-paragraph jx jy it jz b ka mq kc kd ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku im bi translated">乙状结肠神经元的损失函数是平方误差损失。如果我们绘制损失/误差与重量的关系图，我们会得到如下结果:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/641410d1d2244f3a01c9a68d5bf8c25d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rNDygHX0Ds1In2mBE1ZC4g.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Error/Loss vs Weights Graph</figcaption></figure><p id="22e6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们的目标是找到对应于误差最小的点的权重向量，即误差梯度的最小值。这就是微积分发挥作用的地方。</p><h1 id="001f" class="ls lt it bd lu lv mx lx ly lz my mb mc md mz mf mg mh na mj mk ml nb mn mo mp bi translated">梯度下降背后的数学</h1><p id="6b65" class="pw-post-body-paragraph jx jy it jz b ka mq kc kd ke mr kg kh ki ms kk kl km mt ko kp kq mu ks kt ku im bi translated">误差可以简单地写成预测结果和实际结果之间的差异。数学上:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi og"><img src="../Images/87037ed9c0353bc4ae3d3449c176d933.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*bSE-d1xTHdPMc_woav7m-w.png"/></div></figure><p id="b966" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">其中<em class="lq"> t </em>是目标/预期输出&amp; <em class="lq"> y </em>是预测输出</p><p id="a33b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然而，为相同的误差量分配不同的误差值公平吗？例如，1 和 0 以及 1 和 0 之间的绝对差值是相同的，但是上面的公式会对预测-1 的结果产生负面影响。为了解决这个问题，我们使用平方误差损失。(注意没有使用模数，因为它使区分变得更加困难)。此外，这个误差被除以 2，以便更容易区分，我们将在下面的步骤中看到。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1ac51c60aea18e7ce831d58d0d1bfde3.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/format:webp/1*FawfyTR5ga85aFd8Jhbbjg.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Squared Error Loss</figcaption></figure><p id="d9a8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">由于可能有许多重量会导致这一误差，我们每次对每个重量进行偏导数，以找出最小误差。对于输出层权重(W31 和 W32)和隐藏层权重(W11，W12，W21，W22)，权重的变化是不同的。<br/>设外层权重为 wo，隐含层权重为 wh。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a8a6a49f4ab46310d964c62c8fa884d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/format:webp/1*pZ8p9GnRNp7pI9_LtD755A.png"/></div></figure><p id="dad2" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们将首先找到外层权重的 W。由于结果是激活的函数，并且进一步的激活是权重的函数，因此根据链式法则:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/38ec0f85324e8812ed1316cb387f4264.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*lQmnCwjEwo0MdVjvRswCHg.png"/></div></figure><p id="21cf" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在解决问题时，</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9f90cea1ce3028cf01186dafe4766744.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*BRGgm_r1yfh0_Zwd6QMfiQ.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Change in the outer layer weights</figcaption></figure><p id="9e0d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">注意，对于 Xo 来说，它只不过是隐藏层节点的输出。<br/>隐藏层节点的输出也是激活的函数，并且相应地是权重的函数。因此，链式法则针对隐藏层权重进行扩展:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/afad0eafa470816a3bac59f4a6cdf6f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*ytRh9SKcH5beWceMUNyLFw.png"/></div></figure><p id="408d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">也就是说，</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi om"><img src="../Images/935284dca54c07202eaf4422f4be2c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*l7rYNFhoaD5GDU_oNo3XwQ.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Change in the hidden layer weights</figcaption></figure><blockquote class="on oo op"><p id="02c1" class="jx jy lq jz b ka kb kc kd ke kf kg kh oq kj kk kl or kn ko kp os kr ks kt ku im bi translated">注意:Xo 也可以被认为是 Yh，即隐藏层的输出是输出层的输入。Xh 是隐藏层的输入，它是真值表中的实际输入模式。</p></blockquote></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="7797" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然后让我们实现向后传递。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="4b76" class="nv lt it nr b gy nw nx l ny nz">def sigmoid_derivative(x):<br/>    return x * (1 - x)</span><span id="1532" class="nv lt it nr b gy oa nx l ny nz">#Backpropagation<br/>error = expected_output - predicted_output<br/>d_predicted_output = error * sigmoid_derivative(predicted_output)<br/> <br/>error_hidden_layer = d_predicted_output.dot(output_weights.T)<br/>d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)</span><span id="c7e2" class="nv lt it nr b gy oa nx l ny nz">#Updating Weights and Biases<br/>output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr<br/>output_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr<br/>hidden_weights += inputs.T.dot(d_hidden_layer) * lr<br/>hidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr</span></pre><p id="b6ec" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">重复这个过程，直到预测输出收敛到预期输出。将该过程重复一定次数(迭代次数/时期)比设置预期收敛程度的阈值更容易。</p><h1 id="0e64" class="ls lt it bd lu lv mx lx ly lz my mb mc md mz mf mg mh na mj mk ml nb mn mo mp bi translated">PYTHON 实现</h1><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="39bb" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">选择历元的数量和学习率的值决定了两件事:模型的精确度，以及模型计算最终输出的速度。<em class="lq">超参数调优</em>的概念本身就是一个完整的课题。<br/>历元= 10000，学习率= 0.1 的输出为:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="5af1" class="nv lt it nr b gy nw nx l ny nz">Output from neural network after 10,000 epochs: <br/>[0.05770383] [0.9470198] [0.9469948] [0.05712647]</span></pre><p id="c504" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">因此，神经网络已经收敛到预期输出:<br/>【0】【1】【1】【0】。历元与误差图显示了误差是如何最小化的。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/ce37c1ce3648dd57a71a8b8ad2432648.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*lZ0aYuPWcDaGUgbgSFE90A.png"/></div></figure><h1 id="8488" class="ls lt it bd lu lv mx lx ly lz my mb mc md mz mf mg mh na mj mk ml nb mn mo mp bi translated">资源</h1><ol class=""><li id="ddf5" class="nc nd it jz b ka mq ke mr ki ow km ox kq oy ku nh ni nj nk bi translated">凯文·格尼的神经网络导论</li><li id="53e4" class="nc nd it jz b ka nl ke nm ki nn km no kq np ku nh ni nj nk bi translated"><a class="ae mw" href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2017/05/neural-network-from-scratch-in-python-and-r/</a></li><li id="5a31" class="nc nd it jz b ka nl ke nm ki nn km no kq np ku nh ni nj nk bi translated">感谢<a class="ae mw" href="https://www.codecogs.com/latex/eqneditor.php" rel="noopener ugc nofollow" target="_blank">https://www.codecogs.com/latex/eqneditor.php</a>将 LaTeX 方程转换成本文使用的 png。</li></ol></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="ae12" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我希望听到任何反馈/建议！这里和我<a class="ae mw" href="https://www.linkedin.com/in/siddharthapratimdutta/" rel="noopener ugc nofollow" target="_blank">连线。</a></p></div></div>    
</body>
</html>