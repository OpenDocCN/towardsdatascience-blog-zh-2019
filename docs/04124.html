<html>
<head>
<title>A General-Purpose Architecture for Text Mining</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一种通用的文本挖掘体系结构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-unified-model-for-text-mining-2085054f6072?source=collection_archive---------15-----------------------#2019-06-28">https://towardsdatascience.com/a-unified-model-for-text-mining-2085054f6072?source=collection_archive---------15-----------------------#2019-06-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a777" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过自然语言处理使独立于领域的信息抽取成为可能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b02d158c4cdc5954fad82b9199e961a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*McI__fIPgVWw2ArQ6q8RNw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" href="https://www.renemagritte.org/golconda.jsp" rel="noopener ugc nofollow" target="_blank">Rene Magritte</a></figcaption></figure><p id="8b26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自然语言处理是<strong class="lb iu">人工智能</strong>领域中一个令人兴奋的新兴领域。语言学和机器学习算法的重大发展使得计算机处理和分析大量自然语言数据所需的子任务变得可行。研究表明，计算机能够完成被认为只有人类才能完成的任务。</p><p id="4d6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更广泛的 NLP 任务，如<strong class="lb iu">信息提取</strong> (IE)，需要在考虑灵活性的基础上安排子任务。与大多数 IE 系统相比，所讨论的体系结构具有两个重要的优点，即它是通用的和独立于领域的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/afb433b8349b2c52ef7aea194f75a556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hPlpo-albRtKF_J81n3yCQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">General Architecture of the Information Extraction System</figcaption></figure><h1 id="bb0c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">初始自然语言处理模型</strong></h1><p id="1fe9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">文本数据的初始处理需要一系列步骤来为我们的模型提供经过处理的输入。文本首先被<strong class="lb iu">标记化</strong>或分割成单词、标点符号等。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="a5c1" class="my lx it mu b gy mz na l nb nc">Example text:<br/>Ukraine formed a limited military alliance with Russia while also establishing a partnership with NATO in 1994.</span><span id="402e" class="my lx it mu b gy nd na l nb nc">Tokenized Text:<br/>[Ukraine, formed, a, limited, military, alliance, with, Russia, while, also, establishing, a, partnership, with, NATO, in, 1994, .]</span></pre><p id="47c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个单词都标记有形态、语法和字母数字特征。诸如词干、语法依赖标签、开始和结束索引等附加功能可能适用于您的用例。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/bb7c8ec57899ff4f3b2eedbe95577716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tXl08YP03OIvhr1A2WSeDg.png"/></div></div></figure><p id="ec87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习模型使用统计推断，通过分析典型现实世界文档的大型语料库来自动确定形态和句法特征。这些统计推断与手工编码的试探法一起使用，以获得最佳结果。</p><p id="6a5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">命名实体识别(NER)模型将选择术语放入预定义的桶中:个人、组织、事件、地点等。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="db26" class="my lx it mu b gy mz na l nb nc">+------+-----------+--------------+<br/>| Date | Geography | Organization |<br/>+------+-----------+--------------+<br/>| 1994 | Russia    | NATO         |<br/>|      | Ukraine   |              |<br/>+------+-----------+--------------+</span></pre><p id="9ce5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些术语不与每个集合的元素列表进行比较，ML 技术如此成功的部分原因是它们发现了缺乏可解释性但在实践中有效的统计模式。</p><p id="c4c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">依赖于领域的字典查找方法需要日常维护，因为随着时间的推移，集合中的元素增加/减少，并且不考虑上下文。</p><h2 id="6449" class="my lx it bd ly nf ng dn mc nh ni dp mg li nj nk mi lm nl nm mk lq nn no mm np bi translated">关键词提取</h2><p id="ccf6" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">使用<a class="ae ky" href="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> TextRank </strong> </a>提取关键词；基于图的排名模型。</p><p id="80da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，我们构建了一个图，其中文本中不同的名词、动词和形容词充当一个顶点。如果词汇单元共现(在一个固定的单词窗口内一起出现)，则创建边。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/76760fd615b6dc5b1a9d1ce5cfc63893.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*3lgbEE0jJHpkmX2axnWEbg.png"/></div></figure><p id="d74c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对单词进行排序:将所有顶点初始化为 1，收集其每个入站连接的影响，迭代直到收敛，并归一化得分。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="f221" class="my lx it mu b gy mz na l nb nc">+-------------+----------+<br/>|   Keyterm   | TextRank |<br/>+-------------+----------+<br/>| limited     |     0.17 |<br/>| partnership |     0.17 |<br/>| military    |     0.16 |<br/>| Russia      |     0.16 |<br/>| alliance    |     0.16 |<br/>| Ukraine     |     0.09 |<br/>| NATO        |     0.09 |<br/>+-------------+----------+</span></pre><h1 id="8e12" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">词向量</h1><p id="38c1" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">通过使用预训练的<strong class="lb iu">单词向量</strong>，系统变得与领域无关；其中感兴趣的单词被映射到实数的向量。<a class="ae ky" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">手套单词嵌入</strong> </a>通过从大型语料库聚集全局单词-单词共现矩阵来创建。手套向量之间的余弦距离方便地指示语义相似性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/f0925d1e3521a6026e12146acddfe5b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MePOwYe06quj-jUfpTc1yg.png"/></div></div></figure><h1 id="5cda" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">使聚集</h1><p id="69ca" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在许多可用的聚类方法中，重叠划分聚类是优选的，因为它允许一个词同时适合多个类别。使用这种方法的直觉是，自然语言中的类别之间经常存在模糊性。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="3131" class="my lx it mu b gy mz na l nb nc">cluster_1 = [partnership, alliance]</span></pre><h2 id="87b7" class="my lx it bd ly nf ng dn mc nh ni dp mg li nj nk mi lm nl nm mk lq nn no mm np bi translated">标记集群</h2><p id="5fb5" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">将从受控的单词和短语词汇表中选择聚类标签。从关键术语上下文向量的贝叶斯推断比朴素余弦相似性更有可能产生准确的标签。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="9b80" class="my lx it mu b gy mz na l nb nc">+----------------------------+<br/>| International_Organisation |<br/>+----------------------------+<br/>| partnership                |<br/>| alliance                   |<br/>+----------------------------+</span></pre><h1 id="320d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">下游任务</h1><p id="f207" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">提取的信息可以用于许多下游任务，包括文本摘要、事件提取、概念映射和关系提取。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/fe097fac7a992c9dc2078406405904f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tMz4-df7jqWvSE2LJOdoaQ.png"/></div></div></figure><p id="65f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣学习更多关于 NLP 的知识，我强烈推荐你阅读<a class="ae ky" href="https://www.nltk.org/book/" rel="noopener ugc nofollow" target="_blank">自然语言工具包(NLTK)书籍</a>。</p></div></div>    
</body>
</html>