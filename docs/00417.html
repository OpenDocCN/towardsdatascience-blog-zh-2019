<html>
<head>
<title>Review: DeepLabv3 — Atrous Convolution (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:DeepLabv3 —阿特鲁卷积(语义分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74?source=collection_archive---------0-----------------------#2019-01-19">https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74?source=collection_archive---------0-----------------------#2019-01-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0dcb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Rethink DeepLab，优于 PSP net(2016 ils vrc 场景解析挑战赛冠军)</h2></div><p id="e52f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事中，<strong class="kh ir">谷歌</strong>的<strong class="kh ir"> DeepLabv3 </strong>被呈现。在<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv1 和 DeepLabv2 </a>被发明出来后，<strong class="kh ir">作者试图重新思考或重组 DeepLab 架构，并最终提出了一个更加增强的 DeepLabv3。</strong> DeepLabv3 的性能优于<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv1 和 DeepLabv2 </a>，即使去掉了最初用于<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv1 和 DeepLabv2 </a>的后处理步骤条件随机字段(CRF)。</p><p id="c7c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，这篇论文的名字叫做“<strong class="kh ir">重新思考用于语义图像分割的阿特鲁卷积</strong>”。与<a class="ae lk" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener"> Inception-v3 </a>的论文名称相伴的是“重新思考……”，名为“重新思考计算机视觉的 Inception 架构”，其中<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">Inception-v1(Google net)</a>和<a class="ae lk" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">Inception-v2(Batch Norm)</a>被重组为<a class="ae lk" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener"> Inception-v3 </a>。但是现在，<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv2 </a>在这里重组为 DeepLabv3。而且是一篇<strong class="kh ir"> 2017 arXiv </strong>科技报告，引用<strong class="kh ir"> 200 多次</strong>。(<a class="ll lm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----6d818bfd1d74--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="1b48" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">概述</h1><ol class=""><li id="795c" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la mt mu mv mw bi translated"><strong class="kh ir">阿特鲁卷积</strong></li><li id="3079" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">使用多重网格深入阿特鲁卷积</strong></li><li id="7bd1" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">阿特鲁空间金字塔汇集(ASPP) </strong></li><li id="7cd0" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">PASCAL VOC 2012 年烧蚀研究</strong></li><li id="81c7" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">与 PASCAL VOC 2012 年最新方法的比较</strong></li><li id="687e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">与最新城市景观方法的比较</strong></li></ol></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="c0f7" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 1。阿特鲁卷积</strong></h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/18b31cccfee1b44a4f6f9d8d3f570512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*-r7CL0AkeO72MIDpjRxfog.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Atrous Convolution with Different Rates r</strong></figcaption></figure><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi np"><img src="../Images/6b0bf2cbe413a0ce07f9fa38e5f7ecf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*Gm3S_I_8A4QWIgqmNSDjQQ.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Atrous Convolution</strong></figcaption></figure><ul class=""><li id="4cb1" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">对于输出<em class="nu"> y </em>和滤波器<em class="nu"> w </em>上的每个位置<em class="nu"> i </em>，atrous 卷积应用于输入特征图<em class="nu"> x </em>上，其中 atrous 速率 r 对应于我们对输入信号进行采样的步幅。</li><li id="a991" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">这相当于将输入<em class="nu"> x </em>与通过沿每个空间维度在两个连续滤波器值之间插入<em class="nu"> r </em> -1 个零而产生的上采样滤波器进行卷积。(trous 在英语中是洞的意思。)</li><li id="8838" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">当<em class="nu"> r </em> =1 时，为标准卷积。</li><li id="ce73" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">通过调整 r，我们可以自适应地修改过滤器的视野。</li><li id="e1b4" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">也称<strong class="kh ir">扩张卷积</strong> ( <a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">扩张网</a>)或<strong class="kh ir">孔洞算法</strong>。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="ab gu cl nv"><img src="../Images/f0b59fdb5b51091c8e668310a86faaa5.png" data-original-src="https://miro.medium.com/v2/format:webp/1*O5B0IRWewitfivGklGDJjA.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Standard Convolution (Top) Atrous Convolution (Bottom)</strong></figcaption></figure><ul class=""><li id="457f" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated"><strong class="kh ir">顶</strong>:标准卷积。</li><li id="cd46" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated"><strong class="kh ir">底部</strong>:阿特鲁卷积。我们可以看到，当 rate = 2 时，输入信号是交替采样的。首先，pad=2 意味着我们在左右两边都填充 2 个零。然后，当 rate=2 时，我们每隔 2 个输入对输入信号进行采样以进行卷积。阿特鲁卷积<strong class="kh ir">允许我们扩大过滤器的视野，以纳入更大的背景。</strong>因此，它提供了一种有效的机制来<strong class="kh ir">控制视野</strong>和 f <strong class="kh ir">找到精确定位(小视野)和上下文同化(大视野)之间的最佳折衷。</strong></li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="4024" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 2。使用多重网格深入研究阿特鲁卷积</strong></h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi nw"><img src="../Images/5b8f9caf8939a0ede99adbb21b76627e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nFJ_GqK1D3zKCRgtnRfrcw.png"/></div></div></figure><ul class=""><li id="6526" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated"><strong class="kh ir"> (a)无阿特鲁 Conv </strong>:执行标准 Conv 和池化，使输出步幅增加，即输出特征图变小，越深入。然而，连续跨越对于语义分割是有害的，因为位置/空间信息在更深的层丢失了。</li><li id="1564" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated"><strong class="kh ir"> (b)与阿特鲁·conv</strong>:与阿特鲁斯·conv 一起，我们可以保持步幅不变，但视野更大，而不增加参数数量或计算量。最后，我们可以有更大的输出特征图，这有利于语义分割。</li><li id="92c6" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">例如，当 output stride = 16 且 Multi Grid = (1，2，4)时，块 4 中的三个卷积将分别具有 rates = 2×(1，2，4) = (2，4，8)。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="d96f" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 3。阿特鲁空间金字塔池(ASPP) </strong></h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi ob"><img src="../Images/3d460299c1698b7ce081e478acd71f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Lg66z7e7ijuLmSkOzhYvA.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Atrous Spatial Pyramid Pooling (ASPP)</strong></figcaption></figure><ul class=""><li id="3775" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">ASPP 已经在<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv2 </a>中推出。这次，来自<a class="ae lk" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> Inception-v2 </a>的批量标准化(BN)被包含到 ASPP。</li><li id="85a5" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">使用 ASPP 的原因是，随着采样率变大，有效滤波器权重(即，应用于有效特征区域而不是填充零的权重)的数量变小。</li><li id="a522" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated"><strong class="kh ir">一个 1×1 卷积和三个 3×3 卷积，码率= (6，12，18) </strong>当输出步长= 16 时。</li><li id="b8b5" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">此外，<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener"> ParseNet </a>的<strong class="kh ir">图像池</strong>或<strong class="kh ir">图像级特征</strong>也包含在<strong class="kh ir">全局上下文</strong>中。(有兴趣请阅读我的<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener"> ParseNet </a>评测。)</li><li id="7999" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">全部用<strong class="kh ir"> 256 过滤器</strong>和<strong class="kh ir">批量归一化</strong>。</li><li id="a702" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">当输出步幅= 8 时，速率加倍。</li><li id="7a41" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">来自所有分支的结果特征然后被<strong class="kh ir">连接</strong>并通过<strong class="kh ir">另一个 1×1 卷积</strong>(也有 256 个过滤器和批量标准化)，然后通过最终 1×1 卷积生成最终逻辑。</li></ul><h1 id="2163" class="lu lv iq bd lw lx oc lz ma mb od md me jw oe jx mg jz of ka mi kc og kd mk ml bi translated">其他人</h1><h2 id="4349" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">上采样逻辑</h2><ul class=""><li id="64ce" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nt mu mv mw bi translated">在<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv2 </a>中，目标地面实况在训练期间被向下采样 8 倍。</li><li id="2b6d" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">在 DeepLabv3 中，发现保持基本事实的完整性，而不是对最终逻辑进行上采样是非常重要的。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="e778" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">4.PASCAL VOC 2012 烧蚀研究</h1><h2 id="fb42" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">4.1.输出步幅</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b4e3727034233fa7c1bfde4688807f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*c4T-IylFMqLgQP7QXpOWVw.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Going deeper with atrous convolution when employing ResNet-50 with block7 and different output stride.</strong></figcaption></figure><ul class=""><li id="f3c9" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">将 ResNet-50 与 block7(即额外的 block5、block6 和 block7)一起使用时。如表所示，在输出跨距= 256 的情况下(即完全没有 atrous 卷积)，性能要差得多。</li><li id="3f41" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">当输出步幅变大并相应地应用 atrous 卷积时，性能从 20.29%提高到 75.18%，表明 atrous 卷积在为语义分割级联构建更多块时是必不可少的。</li></ul><h2 id="c167" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">4.2.ResNet-101</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/8b4a8b3738907571f00415608f003304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*66C8FrcAvsJw1k1QqBQzbw.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">ResNet-50 vs ResNet-101</strong></figcaption></figure><ul class=""><li id="92bf" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">毫无疑问，ResNet-101 始终比 ResNet-50 好。</li><li id="e9a4" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">值得注意的是，将 block7 用于 ResNet-50 会略微降低性能，但仍会提高 ResNet-101 的性能。</li></ul><h2 id="4150" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">4.3.多重网格</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/64e5294ebf293b5b1d0c7f744c47a449.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*ruCm9zPba6KCcs5wk38IcQ.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Employing multi-grid method for ResNet-101 with different number of cascaded blocks at output stride = 16.</strong></figcaption></figure><ul class=""><li id="148b" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">应用多重网格方法通常比(r1，r2，r3) = (1，1，1)的普通版本更好。</li><li id="9660" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">简单地将单位速率加倍(即(r1，r2，r3) = (2，2，2))是无效的。</li><li id="a5a1" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">使用多重网格可以提高性能。</li><li id="4ee9" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">最佳模型是采用 block7 和(r1，r2，r3) = (1，2，1)的情况。</li></ul><h2 id="3b59" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">4.4.推理策略</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/e349539a460b17e82308ceef9ea811c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*Aq_QI2enbyDzjGpt5x5rPQ.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Inference strategy on the val set. MG</strong>: Multi-grid. <strong class="bd no">OS</strong>: output stride. <strong class="bd no">MS</strong>: Multi-scale inputs during test. <strong class="bd no">Flip</strong>: Adding left-right flipped inputs.</figcaption></figure><ul class=""><li id="f1e5" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">用输出步幅= 16 训练该模型。</li><li id="e160" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">当在推理过程中使用 output stride = 8 (OS=8)来获得更详细的特征图时，性能提高了 1.39%。</li><li id="c6f1" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">当使用尺度= {0.5，0.75，1.0，1.25，1.5，1.75}的多尺度(MS)输入以及使用左右翻转图像并平均概率时，性能进一步提高到 79.35%。</li></ul><h2 id="16f1" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">4.5.ASPP</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/cd72a8745630d580b5ece3dfb1832ccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*7i5Iy3SL7bskwVl0dNwEfg.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">ASPP with MG method and image-level features at output stride = 16.</strong></figcaption></figure><ul class=""><li id="a0fa" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">由<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener"> ParseNet </a>提供的图像池或图像级功能也包含在全局上下文中。(有兴趣请阅读我的<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener"> ParseNet </a>评测。)</li><li id="e14e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">在 ASPP = (6，12，18)的背景下采用多重网格= (1，2，4)优于多重网格= (1，1，1)和(1，2，1)。</li><li id="48cc" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">用 ASPP = (6，12，18)比 ASPP = (6，12，18，24)好。</li><li id="a2cf" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">采用图像级特征后，性能进一步提高到 77.21%。</li></ul><h2 id="399e" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">4.6.作物大小、上采样逻辑、批量标准、批量大小、训练和测试输出步幅</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi oy"><img src="../Images/d400432b139fb2ed68196edadb1ae709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2okzGvHYXgRQDJ10D48AAg.png"/></div></div></figure><ul class=""><li id="b3c0" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">使用更大的作物尺寸 513 比 321 好。</li><li id="3e4b" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">用上采样对数和批量常模，77.21%。</li><li id="bee4" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">在 4、8、12 和 16 中，使用批量 16 是最好的。</li><li id="d0d3" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">使用训练和测试输出步幅= (8，8)具有 77.21%的结果，而使用训练和测试输出步幅= (16，8)具有 78.51%的更好结果。</li></ul><h2 id="de12" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">4.7.培训期间的副本数量</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/352b47367c622912e876ade61766fdcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*8rULrOXJClQBoK1_C6sV0w.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Number of Replicas During Training</strong></figcaption></figure><ul class=""><li id="21a9" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">TensorFlow 用于训练。</li><li id="f9d7" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">使用<strong class="kh ir">只需要 1 个副本，3.65 天</strong>的训练时间。</li><li id="7aff" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">使用<strong class="kh ir"> 32 个副本，只需要 2.74 小时</strong>的训练时间。</li></ul><h2 id="1729" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">4.7.一起</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/33d3189e0f4cf878d29672001479a240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*pjI5jGo1umorgSa8zqq5oA.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Inference strategy on the val set</strong></figcaption></figure><ul class=""><li id="8732" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated"><strong class="kh ir"> MG(1，2，4) + ASPP(6，12，18) +图像池</strong> : <strong class="kh ir">得到 77.21% </strong>，与 4.5 的结果相同。</li><li id="b901" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated"><strong class="kh ir">推理输出步幅= 8 </strong>，<strong class="kh ir"> 78.51% </strong>。</li><li id="2bb7" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated"><strong class="kh ir">多尺度(MS)测试</strong> : <strong class="kh ir"> 79.45% </strong>。</li><li id="1ef5" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated"><strong class="kh ir">水平翻转(翻转)</strong> : <strong class="kh ir"> 79.77% </strong>。</li><li id="95dd" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">用<strong class="kh ir">椰子</strong>进行预处理后:<strong class="kh ir"> 82.70% </strong>。</li><li id="8c3d" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">值得注意的是，经过重新思考和重组，在不使用后处理 CRF(用于<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv2 </a>)的情况下，已经比使用 CRF 和使用 COCO 进行预训练的<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv2 </a>好了 77.69%。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="d520" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 5。与最先进方法的比较</strong></h1><h2 id="2db3" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">5.1.PASCAL VOC 2012 测试集</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/be69136eb1dea26883eb3400670a4d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*sa8XjbJOs0o7pExlWbeXqA.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">PASCAL VOC 2012 Test Set</strong></figcaption></figure><ul class=""><li id="056a" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated"><strong class="kh ir"> DeepLabv3 </strong>:对 PASCAL VOC 2012 trainval set 进行进一步微调，使用输出步幅= 8 进行训练，<strong class="kh ir">在硬映像上引导</strong>。特别地，包含硬分类的图像被复制，<strong class="kh ir"> 85.7% </strong>。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi pc"><img src="../Images/2971256353f9a08e4cea40a982857369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Qt-kcofKUFAvVXs9HhRKw.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Effect of Bootstrapping</strong></figcaption></figure><ul class=""><li id="c239" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">上面显示的硬图像上自举的改进提高了稀有和精细注释类(如自行车)的分割精度。</li><li id="6366" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">并且<strong class="kh ir"> DeepLabv3 胜过</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d"> <strong class="kh ir"> PSPNet </strong> </a>，在 ILSVRC 2016 场景解析挑战赛中获得第一名。</li><li id="c880" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated"><strong class="kh ir"> DeepLabv3-JFT </strong>:采用已经在 ImageNet 和 JFT-300M 数据集上<strong class="kh ir">预处理过的 ResNet-101，<strong class="kh ir"> 86.9% </strong>。</strong></li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi pd"><img src="../Images/d8ec80fc8513fb01cde24f50b86cb61d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Q5zbNSFruHPvhAjlMrN_Q.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Qualitative Results (Last Row, Failure Case) on PASCAL VOC 2012</strong></figcaption></figure></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="d04c" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 6。与最新城市景观方法的比较</strong></h1><h2 id="1018" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">6.1.不同的设置</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/9252c1b58b453ad140cb0b97bca2b7cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*UBZxwPMeekefNoLQOpSQFQ.png"/></div></figure><ul class=""><li id="850e" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">类似于 PASCAL VOC 2012，使用 8 的输出跨距进行测试，多标度和水平翻转也是如此，性能得到了提高。</li></ul><h2 id="4d81" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">6.2.城市景观测试集</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/ef7e48561d1ef85d8a6d8678faa67206.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*xyr5RWmFyNFwQ4RSQEx8lA.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Cityscape Test Set</strong></figcaption></figure><ul class=""><li id="f2ac" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nt mu mv mw bi translated">为了获得更好的比较性能，DeepLabv3 在 trainval 粗集上<strong class="kh ir">进一步训练(即 3475 个精细注释图像和额外的 20000 个粗注释图像)。</strong></li><li id="fd8a" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated"><strong class="kh ir">推理时使用更多的刻度</strong>和<strong class="kh ir">更精细的输出步幅</strong>。特别地，在<strong class="kh ir">标度= {0.75，1，1.25，1.5，1.75，2} </strong>和求值<strong class="kh ir">输出步距= 4 </strong>的情况下，这分别为<strong class="kh ir">在验证集</strong>上贡献了额外的 0.8%和 0.1%。</li><li id="815d" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nt mu mv mw bi translated">最终在测试集上取得了<strong class="kh ir"> 81.3% </strong> mIOU，比<a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d"><strong class="kh ir">PSP net</strong></a><strong class="kh ir">略好<strong class="kh ir">。</strong></strong></li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi pg"><img src="../Images/c38e27d2e8bbb165cca3a77bfa0faa26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WEfbqyn2oBhS56rKqD4C4A.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Qualitative Results on Cityscape</strong></figcaption></figure></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><p id="f36f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DeepLabv3 仅在很小的差距上胜过 PSPNet，也许这也是为什么它只是 arXiv 中的一份技术报告。但是后来，发明了比 DeepLabv3 好得多的 DeepLabv3+。希望以后可以复习 DeepLabv3+。:)</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h2 id="f4dd" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">参考</h2><p id="80ca" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko ph kq kr ks pi ku kv kw pj ky kz la ij bi translated">【2017 arXiv】【DeepLabv3】<br/><a class="ae lk" href="https://arxiv.org/abs/1706.05587" rel="noopener ugc nofollow" target="_blank">反思阿特鲁卷积用于语义图像分割</a></p><h2 id="e0e8" class="oh lv iq bd lw oi oj dn ma ok ol dp me ko om on mg ks oo op mi kw oq or mk os bi translated">我的相关评论</h2><p id="4da9" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko ph kq kr ks pi ku kv kw pj ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a></p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a>】<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a>]</p><p id="fc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong></p><p id="3134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割<br/>T28】[<a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></strong></p><p id="58de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">超分辨率<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">Sr CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4">fsr CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f">VDSR</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener">ESPCN</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>】</p></div></div>    
</body>
</html>