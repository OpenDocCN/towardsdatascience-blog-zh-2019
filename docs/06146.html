<html>
<head>
<title>Is overfitting a bad thing?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">过度合身是一件坏事吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-overfitting-really-bad-ddaabfc2f6b8?source=collection_archive---------32-----------------------#2019-09-05">https://towardsdatascience.com/is-overfitting-really-bad-ddaabfc2f6b8?source=collection_archive---------32-----------------------#2019-09-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/c3175e7b34c62f4dd8c431c81ee8635f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iCcZnpPNyCQk1XZC-aZJyw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">source &amp; credit — <a class="ae kf" href="https://www.pexels.com/@brunoscramgnon" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/@brunoscramgnon</a></figcaption></figure><p id="8fd2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">经典统计学和机器学习中教授的一个基本原则是<strong class="ki iu">“你不应该有一个过度拟合的模型”</strong>！</p><p id="f2ff" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“过度拟合”的意思是，你的神经网络已经学习了一个在训练数据上表现非常好的函数，但是当向它显示新数据(也称为测试数据)时，它不能提供正确的推断/预测。</p><p id="d0df" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，为了避免过度拟合的模型(或者换句话说，你渴望有一个通用的模型)，你应该利用一个叫做<strong class="ki iu">偏差-方差权衡</strong>的概念。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/f68b2979a445dee91221c49ee81b6db6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7ZKM93QVdpeSCGbF5TjIg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">bias-variance tradeoff</figcaption></figure><blockquote class="lj lk ll"><p id="f88b" class="kg kh lm ki b kj kk kl km kn ko kp kq ln ks kt ku lo kw kx ky lp la lb lc ld im bi translated"><strong class="ki iu">偏差</strong>这里实际上是<strong class="ki iu">偏差-误差</strong>反映了事实和预测之间的差异。预测越接近事实，误差越低！</p></blockquote><p id="f201" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个概念的要点是</p><ul class=""><li id="c78e" class="lq lr it ki b kj kk kn ko kr ls kv lt kz lu ld lv lw lx ly bi translated">如果你有一个<strong class="ki iu">高偏差</strong>，这意味着即使在训练数据上，它也没有学习预期的功能</li><li id="5136" class="lq lr it ki b kj lz kn ma kr mb kv mc kz md ld lv lw lx ly bi translated">如果您有一个<strong class="ki iu">高方差</strong>，这意味着您学习的函数已经尝试考虑几乎所有的训练数据点，几十年的使用表明，当这种情况发生时，您的神经网络在测试数据上表现不佳</li></ul><p id="d22a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一个链接，非常详细地介绍了<strong class="ki iu">偏差-方差权衡</strong>—<a class="ae kf" rel="noopener" target="_blank" href="/understanding-the-bias-variance-tradeoff-165e6942b229">https://towards data science . com/understanding-the-bias-variance-trade-165 e 6942 b229</a></p><p id="6c53" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">很长一段时间以来，我一直把这个基本原则视为既定事实，直到我开始看到网络在训练数据上表现得非常好(几乎零训练误差)，但在没有任何正则化的情况下在测试数据上表现得也很好<strong class="ki iu"><em class="lm"/>。</strong></p><blockquote class="lj lk ll"><p id="298b" class="kg kh lm ki b kj kk kl km kn ko kp kq ln ks kt ku lo kw kx ky lp la lb lc ld im bi translated">非常有趣的是，一个人在一次练习(在这里是训练)中获得零错误，却感到失望！</p></blockquote><p id="f55c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有另一个不同的原理，叫做<strong class="ki iu">奥卡姆剃刀。</strong>奥卡姆的威廉用通俗的话给出的哲学是:一个人应该喜欢简单胜过复杂。我不敢反驳！</p><p id="6277" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在<strong class="ki iu">奥卡姆剃刀</strong>也被认为是统计学家应用的&amp;，他们说——<strong class="ki iu">最简单的假设(函数)应该永远是首选。同样，原则上我不能反对它，但我真的看不到的是，机器学习想要解决的大多数问题本质上都很复杂，因此这条原则真的适用于这里吗！。</strong></p><blockquote class="lj lk ll"><p id="8192" class="kg kh lm ki b kj kk kl km kn ko kp kq ln ks kt ku lo kw kx ky lp la lb lc ld im bi translated">统计学家所提倡的奥卡姆剃刀有没有可能最终固定我们的思维，而我们没有考虑到最简单的解释并不总是最好的选择？</p></blockquote></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="86bc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">幸运的是，我在去年发表的一篇论文中找到了一些安慰——<a class="ae kf" href="https://arxiv.org/abs/1812.11118" rel="noopener ugc nofollow" target="_blank">调和现代机器学习和偏差-方差权衡</a>。</p><p id="de08" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者表明，现代神经网络往往在训练数据上实现零损失，但在测试数据上表现良好。他们认为，非常像经典统计学中的<strong class="ki iu">【甜蜜点】</strong>，其中有<strong class="ki iu">低偏差，但假设(或学习函数)并不过分复杂，</strong>还有另一个被称为<strong class="ki iu">插值阈值</strong>的“甜蜜点”，在此之后，即使<strong class="ki iu"> <em class="lm">假设复杂性(以参数数量衡量)</em> </strong>增加，仍然可以有<strong class="ki iu">低偏差和低方差。</strong>他们阐释了下图所示的概念，并称之为<strong class="ki iu">“双体面风险曲线”。</strong></p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ml"><img src="../Images/c2cb27690a0808db704a01609f634b56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cK3ESTYui5_Aq8FSL_VbCg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">source — <a class="ae kf" href="https://arxiv.org/pdf/1812.11118.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1812.11118.pdf</a></figcaption></figure><p id="0358" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">他们不仅展示了在神经网络环境中达到<strong class="ki iu">插值阈值</strong>的现象，还研究了其他机器学习算法，如<strong class="ki iu">决策树&amp;集成方法。</strong>他们提供了一个经验证据，即通过使用决策树和随机森林进行 boosting 探索的函数族也显示出与神经网络相似的泛化行为，无论是在插值阈值之前还是之后。</p><p id="56b8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么为什么这个<strong class="ki iu">插值阈值</strong>在过去没有被观测到呢？该论文的作者提供了对它的见解，并引用了传统上特征具有固定大小的观点。我引用报纸上的话</p><blockquote class="lj lk ll"><p id="f103" class="kg kh lm ki b kj kk kl km kn ko kp kq ln ks kt ku lo kw kx ky lp la lb lc ld im bi translated">经典统计学中广泛研究的常用线性设置通常假设一组固定的特征，因此具有固定的拟合能力。各种形式的正则化既可以防止插值，也可以改变函数类的有效容量，从而衰减或掩盖插值峰值。</p></blockquote><p id="bc37" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你训练了深层网络，你就能理解上面的观点；大多数情况下，如果我们开始发现测试误差没有减少，我们会提前停止执行<strong class="ki iu">。在定义我们的网络层时，我们还系统地添加了<strong class="ki iu">权重衰减&amp; </strong>其他<strong class="ki iu">正则化子</strong>。作者建议(我也同意)的是，由于过早地关注正则化，我们经常无法实现<strong class="ki iu">双体面风险</strong>曲线中所示的<strong class="ki iu">插值阈值/峰值</strong>。</strong></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mm"><img src="../Images/7b672e28baecfa5f53d7e8efae9e3f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*947lqwFk6r1ru-EMcLjKqA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">source — <a class="ae kf" href="https://www.pexels.com/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com</a></figcaption></figure><p id="e224" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相对于奥卡姆剃刀，我找到安慰的另一个地方是在<a class="ae kf" href="https://www.cs.toronto.edu/~radford/homepage.html" rel="noopener ugc nofollow" target="_blank">的拉德福德·尼尔博士</a>的精彩论文中，题目是<a class="ae kf" href="https://www.cs.toronto.edu/~radford/ftp/thesis.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> <em class="lm">神经网络的贝叶斯学习</em> </strong> </a> <strong class="ki iu"> <em class="lm">。</em></strong>Neal 博士是贝叶斯学习应用于工程应用领域的先驱，他介绍了马尔可夫链蒙特卡罗方法如何用于贝叶斯推理。</p><p id="6c6d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在他的论文(第 1.1.4 节)中，他提到奥卡姆剃刀原理通常被认为是归纳推理的一个基本组成部分，虽然它在科学背景下是没问题的，但它在精密、复杂且通常混乱的工程应用中的效用产生了合理的怀疑。</p><p id="b0a4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">他进一步谈到，频繁统计学家所认为的过度拟合在贝叶斯观点中并没有找到同样的相关性。我很乐意分享这个观点，但这是以后的事了！</p><h1 id="fcb1" class="mn mo it bd mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk bi translated"><strong class="ak">结束语</strong></h1><p id="5d10" class="pw-post-body-paragraph kg kh it ki b kj nl kl km kn nm kp kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated"><a class="ae kf" href="https://arxiv.org/pdf/1812.11118.pdf" rel="noopener ugc nofollow" target="_blank">对我来说，协调现代机器学习和 Belkin 等人的偏差-方差权衡</a>非常有见地。首先，它帮助我消除了对零训练误差的担忧，其次，它表明挑战既定原则是可以的。</p><blockquote class="lj lk ll"><p id="d09a" class="kg kh lm ki b kj kk kl km kn ko kp kq ln ks kt ku lo kw kx ky lp la lb lc ld im bi translated">在 stackexchange 上看到了下面这个帖子的同名主题—<a class="ae kf" href="https://ai.stackexchange.com/questions/4136/is-overfitting-always-a-bad-thing" rel="noopener ugc nofollow" target="_blank">https://ai . stack exchange . com/questions/4136/is-over fitting-always-a-bad-thing</a>……..当我读到作者说“响亮的是”的答案时，我笑了……这对你仍然适用吗？</p></blockquote><p id="8ee2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不要害怕你的网络超载；我没有！</p></div></div>    
</body>
</html>