# 如果你懂 SQL，你可能会理解 Transformer，BERT 和 GPT。

> 原文：<https://towardsdatascience.com/if-you-know-sql-you-probably-understand-transformer-bert-and-gpt-7b197cb48d24?source=collection_archive---------21----------------------->

## 都是关于查询和检索的。

![](img/aaf72ed1e7090acf060ca4a952eee26a.png)

Paris, France

对于那些自 2017/2018 年以来一直关注自然语言处理如何发展的人来说， [Transformer](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) / [BERT](https://arxiv.org/pdf/1810.04805.pdf) 对你来说并不陌生。由于 [Alexander Rush](http://nlp.seas.harvard.edu/2018/04/03/attention.html) 、 [Jay Alammar](http://jalammar.github.io/illustrated-transformer/) 和最近 [Peter Bloem](http://www.peterbloem.nl/blog/transformers) 写了许多非常详细的介绍，我不打算涵盖引擎盖下的内容；相反，我在寻找第一个原则，并试图用更容易理解的东西——数据库和内存——来做一个类比。

如果你能理解 SQL 查询，也许你能理解 Transformer 架构的本质。即使你不是，这也应该不难理解——我只是从我的“纸库”中提取所有 NLP 论文的作者和标题。

> 从纸质数据库中选择作者、标题，其中 Category = "NLP "

对数据库的查询必须灵活地适应各种键(Category = "NLP ")和值(作者和标题)。同时，键和值必须灵活地服务于用户进行的任何查询。在 SQL 背后，它由一个跨查询、键和值共享的数据模式管理。有效记录包含所有项目，以便它们可以互换使用。

**本质上都是查询**

当查询被传递到计算机时，它是一个两步过程:

*   首先，计算机必须搜索并匹配你的查询和关键。
*   第二，计算机返回键的相同记录的值。

这与注意力机制惊人地相似，它是变形金刚的支柱:

*   QKt 正在匹配查询和键。从数学上来说，它是测量 Q 和 k 之间的余弦相似性。相似性越高，记录越相关。
*   Softmax(。)V 是键的相关记录的返回值。数学上讲，softmax 函数会返回一个概率。这意味着，每条记录都是概率加权的。

![](img/2e2ae19cf05c27042c57bc5f90cdc019.png)

Formula 1 in Transformer (Vaswani et al., 2017)

我想强调两点:

*   查询关键字匹配和数据检索步骤从 IF/ELSE/AND/OR 逻辑跳到基于概率的逻辑。这呼应了经典/基于规则的人工智能是如何进化成概率人工智能的。毕竟，不可能根据任何条件写下所有的规则，最好的策略是用概率来编码。
*   存储的值是一个向量(有理数数组)，而不是整数。整数和有理数之间有很大的区别。毕竟从 0 到 1 (2 个整数)，里面实际上有无限个数字(0.1，0.11，0.111，0.1111，…等等。).这意味着，如果我们将每个有理数与一个记录相关联，潜在地，我们可以存储 0 到 1 之间的大量记录。

我在 SQL 和注意力机制之间做了一个类比。

SQL vs Attention Mechanism

**查询增强理解**

注意力机制的另一个好处是，我们隐式地要求机器编写查询，使用返回的结果，并整合不同的返回结果以形成另一个有意义的查询和结果等。

![](img/fa87de418f8992fb5e5a47dc214aabc1.png)

Queries enhance understanding

一开始，如果我不明白 CV 是什么，我会查询“CV”，所有的计算机视觉论文都会被返回，所以我知道 CV 代表计算机视觉。反过来，如果我们不明白一篇论文讲的是什么，通过查询标题，返回类别，我知道这是一篇关于 NLP 的论文。通过迭代的，如果不是穷尽的，查询，我们通过在记录之间建立关联来更好地理解记录**。**

同样，在 Transformer 中，我们查询每条记录，以便返回每条记录(单词)与所有其他记录(单词)的关联。例如，如果我以概率方式查询“paper ”,它可能会返回一个概率估计，其中“read”是最有可能的。通过对所有单词的详尽查询，机器可以跨所有单词开发理解和关系。

一个 Transformer 有多个关注点，并将关注点叠加在关注点上，所以你可以想象 Transformer 就像一群聪明的分析师，他们合作地使用高级语义 SQL 迭代地从一个超大型数据库中挖掘洞察力；当多个中层经理从他们的直接下属那里获得洞察力时，他们会将发现提交给他们的经理(比双重报告更难)，他们最终会进行提炼，然后提交给首席执行官。

**从变形金刚到伯特和 GPT**

著名的伯特和 GPT 正在使用 Transformer 架构来从非常大的语料库中捕获和存储单词之间的语义关系。基本上，大多数其他最先进的语言模型都是基于 Transformer 的，因为这种类似数据库的结构允许更好的检索，因此可以存储像语言这样的大量数据中更复杂的关系。

而且这个数据库是可以转移的，你可以把任何一个预训练的模型想象成压缩的训练数据。事实上，探索这些预先训练好的模型几乎成了大多数从业者事实上的练习。

**都是关于查询和检索的**

良好的记忆力(又名数据库)对智力至关重要。你的每一个想法实际上都是在向你的大脑数据库查询，然后返回相关的语言、图像和知识。没有有效的查询和检索是不可能的。因此，找到一种有效的数据结构来更好地查询和检索一直是人工智能研究的重点，这也是为什么我们有越来越多新颖的深度学习架构可以玩的原因！