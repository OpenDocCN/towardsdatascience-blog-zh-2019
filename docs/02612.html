<html>
<head>
<title>A step towards general NLP with Dynamic Memory Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用动态记忆网络实现通用自然语言处理的一步</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-step-towards-general-nlp-with-dynamic-memory-networks-2a888376ce8f?source=collection_archive---------9-----------------------#2019-04-29">https://towardsdatascience.com/a-step-towards-general-nlp-with-dynamic-memory-networks-2a888376ce8f?source=collection_archive---------9-----------------------#2019-04-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2d46" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用动态记忆网络和问答格式解决不同的自然语言处理任务</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d1e2762a1ab57f2eddabdb60e33bdd9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lkoxqtfNaPG7qMf0.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by <a class="ae ky" href="https://pixabay.com/users/TheDigitalArtist-202249/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3308188" rel="noopener ugc nofollow" target="_blank">Pete Linforth</a> from <a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3308188" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi kz"><img src="../Images/e003677be316b52c9c1f7c671a3b690f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VSQoTqcbY0WHr_3kadNxIw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: [1]</figcaption></figure><p id="7eb0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="lw">我假设你已经熟悉 LSTMs 和 GRUs 等递归神经网络(包括 seq2seq 编码器-解码器架构)。</em></p><p id="723c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">通用自然语言处理的一个障碍是不同的任务(如文本分类、序列标记和文本生成)需要不同的顺序架构。处理这个问题的一个方法是将这些不同的任务视为问答问题。因此，例如，可以向模型询问对一段文本的情感是什么(传统上是文本分类问题)，答案可以是“正面”、“负面”或“中性”之一。</p><p id="6db3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">论文“问我任何问题:自然语言处理的动态记忆网络”介绍了一种新的、模块化的问答体系结构。</p><p id="3657" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于复杂的问答问题，LSTMs 和 GRUs 的内存组件可能会成为瓶颈。很难一次就在内存组件中积累所有相关信息，因此，本文背后的关键思想是允许模型根据需要多次访问数据。</p><p id="3102" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">尽管该架构乍看起来极其复杂，但它可以被分解成许多简单的组件。</p><h1 id="cbd6" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">模块</h1><h2 id="0df6" class="mp ly it bd lz mq mr dn md ms mt dp mh lj mu mv mj ln mw mx ml lr my mz mn na bi translated">语义记忆模块</h2><p id="d5d5" class="pw-post-body-paragraph la lb it lc b ld nb ju lf lg nc jx li lj nd ll lm ln ne lp lq lr nf lt lu lv im bi translated">语义记忆模块简单地引用单词嵌入，例如手套向量，输入文本在被传递到输入模块之前被转换成该单词嵌入。</p><h2 id="33d1" class="mp ly it bd lz mq mr dn md ms mt dp mh lj mu mv mj ln mw mx ml lr my mz mn na bi translated">输入模块</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/31d2e0b4fe7175216a2b0ee10174c279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A-tBLD04g5IGLZnvcH094g.png"/></div></div></figure><p id="fa48" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">输入模块是一个标准的 GRU(或 BiGRU)，其中每个句子的最后一个隐藏状态是显式可访问的。</p><h2 id="c47a" class="mp ly it bd lz mq mr dn md ms mt dp mh lj mu mv mj ln mw mx ml lr my mz mn na bi translated">问题模块</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/83bbb689b46d72df0c9b916b86514992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2rU8qGaRMM3ry3Myy2ynJA.png"/></div></div></figure><p id="daee" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">问题模块也是一个标准的 GRU，要回答的问题作为输入，最后一个状态隐藏状态是可访问的。</p><h2 id="7c5a" class="mp ly it bd lz mq mr dn md ms mt dp mh lj mu mv mj ln mw mx ml lr my mz mn na bi translated">情节记忆模块</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/23b8d4f8001e3be23a30000645452b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbqcaZbeQtcnqOn7DuJLFw.png"/></div></div></figure><p id="511f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这是对输入数据进行多次传递的模块。在每一遍中，来自输入模块的句子嵌入作为输入被馈送到情节记忆模块中的 GRU。这里，每个嵌入的句子被分配一个权重，该权重对应于其与所问问题的相关性。</p><p id="0b4b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">不同的权重可以被分配给不同遍的句子嵌入。例如，在下面的例子中；</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/ee416fd9cac6d4d730f9ef819974eb1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*h44Cqokk_moFHKAnNdzK3w.png"/></div></figure><p id="35e0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">由于句子(1)与问题没有直接关系，因此可能不会在第一遍中给予很高的权重。然而，在第一次传递中，模型发现足球与约翰相关联，因此在第二次传递中，给予句子(1)更高的权重。</p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><p id="b7b1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于第一遍(或第一“情节”)，嵌入“q”的问题用于计算来自输入模块的句子嵌入的注意力分数。</p><p id="e11d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">句子<em class="lw"> sᵢ </em>的注意力分数然后可以通过 softmax(这样注意力分数总和为 1)或单个 sigmoid 来获得<em class="lw"> gᵢ。<br/> gᵢ </em>是赋予语句<em class="lw">sᵢ</em>的权重，并在时间步长<em class="lw"> i. </em>充当 GRU 输出的全局门</p><p id="ca19" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">时间步<em class="lw"> i </em>和情节<em class="lw"> t </em>的隐藏状态计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/b04f2c43d812a65d2b21a996cb24d4f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*U-V065gAB3IpOnFsl5b-oA.png"/></div></figure><p id="b23d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当<em class="lw"> g </em> = 0 时，隐藏状态被简单地向前复制。也就是说，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/9246f1cb859c3710b5f10726b75613b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/format:webp/1*4W5jMOXcWd7UV4o-e_fS1Q.png"/></div></figure><p id="cbf6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">第<em class="lw"> t 集的 GRU 的最后隐藏状态，</em>被称为<em class="lw"> m </em> ᵗ，可以被视为在第<em class="lw"> t. </em>集期间发现的事实的聚集。从第二集开始，<em class="lw"> m </em> ᵗ被用于计算第<em class="lw"> t+1 </em>集的句子嵌入以及问题嵌入<em class="lw"> q </em>的注意力分数。</p><p id="3d5f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/6fdb163c511656856d318088555c4528.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*0G1v18CNeFa0TQqPwZoqTQ.png"/></div></figure><p id="150e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在<em class="lw"> sᵢ </em>和<em class="lw"> q、</em>和<em class="lw"> sᵢ </em>和<em class="lw"> m </em> ᵗ⁻之间计算多个简单的相似性度量，即逐元素乘法和绝对差。连接的结果然后通过 2 层神经网络来计算<em class="lw"> sᵢ的注意力分数。</em>第一集，<em class="lw"> m⁰ </em>换成了<em class="lw"> </em> q</p><p id="10a6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">剧集的数量可以是固定的、预定义的数量，或者由网络本身确定。在后一种情况下，一个特殊的刀路结束表示被附加到输入中。如果这个向量被选通函数选择，那么迭代停止。</p><h2 id="433b" class="mp ly it bd lz mq mr dn md ms mt dp mh lj mu mv mj ln mw mx ml lr my mz mn na bi translated">回答模块</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/230e229968774552e5bc6cb7aa9320e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7t0309IXP56QsqfsvDvrw.png"/></div></div></figure><p id="094c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">答案模块由一个解码器 GRU 组成。在每个时间步，与问题嵌入连接的前一个输出作为输入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/cbb4680546630409e6b69b8e08dbeb8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JrYIbk3MxFkk3oOOS6QKoA.png"/></div></div></figure><p id="1438" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">使用词汇表上的标准 softmax 生成输出。</p><p id="1ddd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">解码器通过<em class="lw"> m </em>向量(来自情节存储模块的 GRU 计算的最后隐藏状态)上的函数初始化。</p><h1 id="fbd3" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">应用于情感分析</h1><p id="1b5f" class="pw-post-body-paragraph la lb it lc b ld nb ju lf lg nc jx li lj nd ll lm ln ne lp lq lr nf lt lu lv im bi translated">该模型在发表时达到了情感分析的艺术水平。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/fc2ec8ab9fce0506678539b74b27e787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*z0KRTijrsx2rWwalnSj5Uw.png"/></div></figure><p id="e857" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于下面的例子，该模型关注所有的形容词，并且当仅允许 1 次通过时，最终产生不正确的预测。然而，当允许两次通过时，该模型对第二次通过时的正面形容词给予显著更高的关注，并产生正确的预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d7802b32783090e19658f12237f4c5d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*QsTpsKpfbV0dbmodeD87WQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Analysis of Attention for Sentiment: [1]</figcaption></figure><h1 id="cbf1" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">在其他数据集上的性能</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/3ee656f8658efad3d6c92b2780c04c05.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*lQsbRQWRG83J5B88-QtFNw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: [1]</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b4cb4084597c977abf763d3269f3be1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*Bp9-uAR4KS-gjLR-f8BD9w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: [1]</figcaption></figure><h1 id="9e4b" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">开关模块</h1><p id="3d67" class="pw-post-body-paragraph la lb it lc b ld nb ju lf lg nc jx li lj nd ll lm ln ne lp lq lr nf lt lu lv im bi translated">模块化的一个重要好处是，只要替换模块具有正确的接口，就可以用另一个模块替换另一个模块，而无需修改任何其他模块。</p><p id="4373" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">论文“用于视觉和文本问答的动态记忆网络”展示了使用动态记忆网络来回答基于图像的问题。</p><p id="6317" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">输入模块被另一个使用基于 CNN 的网络从图像中提取特征向量的模块代替。如前所述，提取的特征向量然后被馈送到情节记忆模块。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/419e8c07c13e37f011e1432db2cac941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*jTeBLM-nsjTBBR6Q7N-qlg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: [2]</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ee0e9f8bc039b460ed575edb5e0057cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*CGDE4hdvrkKvCdDPzWM8og.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: [2]</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/0b53887895de9d2fe37040ffc96a7001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2mBgsds92EzIRlp-asqxSg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Visual Question Answering Accuracy: [2]</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/708e3dfa674ed665bdb912679d496c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JoP-DExXTrsGiJGdaFYavw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Attention visualisations of answers to some questions: [2]</figcaption></figure><h1 id="d1be" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">额外资源</h1><ul class=""><li id="d87d" class="od oe it lc b ld nb lg nc lj of ln og lr oh lv oi oj ok ol bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=T3octNTE7Is&amp;list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&amp;index=17" rel="noopener ugc nofollow" target="_blank">斯坦福大学，cs224n，第 16 讲:问答的动态神经网络</a></li><li id="479d" class="od oe it lc b ld om lg on lj oo ln op lr oq lv oi oj ok ol bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/attention-and-its-different-forms-7fc3674d14dc">注意力及其不同形式</a></li></ul><h1 id="8d57" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">参考</h1><p id="9db0" class="pw-post-body-paragraph la lb it lc b ld nb ju lf lg nc jx li lj nd ll lm ln ne lp lq lr nf lt lu lv im bi translated">[1] K. Ankit，O. Irsoy，J. Su，J. Bradbury，R. English，B. Pierce，P. Ondruska，I. Gulrajani 和 R. Socher，随便问我:自然语言处理的动态记忆网络，<em class="lw"/>(2016)。</p><p id="776a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[2] C .熊，S. Merity 和 R. Socher，视觉和文本问答的动态记忆网络，<em class="lw"/>(2016)。</p></div></div>    
</body>
</html>