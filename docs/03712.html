<html>
<head>
<title>A new Tool to your Toolkit, Intro to KL Divergence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的工具箱中的一个新工具，KL 散度介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e?source=collection_archive---------14-----------------------#2019-06-12">https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e?source=collection_archive---------14-----------------------#2019-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f15f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">以有趣且非常直观的方式揭开熵、交叉熵和 KL 散度的神秘面纱</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/948a843d9843b4007428d54b0ee40755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WuZzabeqlOeCOqYc.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://www.shoezone.com/Boys/Size-Guide" rel="noopener ugc nofollow" target="_blank">Image Source</a></figcaption></figure><p id="44b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一名数据科学从业者，您在工作中多久使用一次<a class="ae ky" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> KL 散度</a>的概念？你对<a class="ae ky" href="https://en.wikipedia.org/wiki/Entropy" rel="noopener ugc nofollow" target="_blank">熵</a>、<a class="ae ky" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">交叉熵</a>、或<a class="ae ky" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> KL 散度</a>这些概念有多少清晰和把握？一点点，只是理论上的或者看过一次就忘了的类型。</p><p id="27c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不管是什么，都没关系。你一定在网上看过一些关于这个话题的文章，甚至我也看过，但是它太理论化和无聊了，以至于我们会随着时间而忘记。但是，等等，如果我提出一个完全不同的观点，这个观点使我很好地理解了这个概念，并使它成为我武器库中的一个强有力的武器。相信我，一旦你完成了这些，你就会知道如何在每个小的分类、聚类或其他日常机器学习问题中利用这些概念。</p><p id="2ce7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章之后会有另一篇文章。在这一部分，我们将从了解熵开始，然后是交叉熵，最后是 KL 散度。然后，我们将使用学到的概念，并将它们应用到一个数据集中，这将在下一篇文章中使事情变得非常清楚。我会尽我最大的努力让事情简单和直观，但请随时跳转到参考资料部分，并研究更多的主题。让我们开始旅程吧。</p><p id="0b7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你也可以找到我在 Youtube 上的相同主题的视频。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">My Youtube video on the same topic</figcaption></figure><h1 id="5e30" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">1.理解熵</h1><p id="89d5" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">互联网上说熵是信息增益的数量。让我简单明了地说，熵就是与事件相关的无序、不确定性、惊奇或不可预测性的数量。这是什么意思？</p><p id="f64f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举个例子，看到我预测的今天的天气，</p><ol class=""><li id="0aaa" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">明天下雨的可能性为 50%。</li><li id="e612" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">明天有 75%的可能性会下雨。</li></ol><p id="a639" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">哪个案例给了我们更多的信息增益或知识？</strong></p><p id="c091" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当说到 50-50 的机会时，它意味着更多的不确定性，任何事情都有可能发生，因此有更多的随机性。而在第二种情况下，75%的下雨几率意味着下雨的可能性更大，随机性更小，或者获得新信息。因此，第一种情况比第二种情况具有更高的熵。</p><p id="0abc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们再举一个例子，抛硬币的可能结果是什么，给定</p><ol class=""><li id="2395" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">头部和尾部概率相等的无偏硬币，0.5</li><li id="d0e3" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">正面概率为 0.75 的有偏硬币</li></ol><p id="1324" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和以前一样，在第一种情况下，有 50–50 的机会正面或反面。这意味着更多的不确定性，任何事情都有可能发生，因此有更多的随机性。而在第二种情况下，75%的正面机会意味着硬币的结果更有可能是正面，随机性更小，或新信息增益。因此，第一种情况比第二种情况具有更高的熵。</p><h2 id="2c5d" class="ni ly it bd lz nj nk dn md nl nm dp mh li nn no mj lm np nq ml lq nr ns mn nt bi translated">计算熵</h2><p id="72d9" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">系统“H(p)”的熵计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bf29b57e725573e235f34ac5a981c6c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*rIIiiIGfbSrH0Xuv5h6VNQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The entropy of the system</figcaption></figure><p id="4395" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="nv"> pᵢ </em>是第 I 个事件的概率。</p><p id="6b6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们计算上面例子的熵，并表明无偏硬币比有偏硬币具有更高的熵。</p><p id="b990" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">情况一:头部和尾部概率相等的无偏硬币的熵，0.5</p><p id="b9f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">=-0.5 * log(0.5)-0.5 * log(0.5)= 1.0</p><p id="1cfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">案例二:正面概率为 0.75 的有偏硬币的熵</p><p id="42cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">=-0.75 * log(0.75)-0.25 * log(0.25)= 0.81</p><h1 id="0ee2" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">2.理解交叉熵</h1><p id="db9c" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">按照我的说法，交叉熵是拟合度，即预测分布与实际分布的接近程度。让我简化一下，</p><p id="db29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们得到了一枚正面概率为 0.75 的有偏硬币。我们没有意识到硬币的偏见，所以</p><p id="68af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">案例一:我们投掷硬币 100 次，得到 70 个正面和 30 个反面。我们的估计是硬币有偏差，正面概率为 0.70。</p><p id="fab7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">案例二:我们投掷硬币 100 次，得到 72 个正面和 28 个反面。我们的估计是硬币有偏差，正面概率为 0.72。</p><p id="b4bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们更接近情况 II 中的实际分布，因此情况 II 中的交叉熵比情况 I 中的更低。如果碰巧我们能够无任何误差地模拟真实分布，则交叉熵将与熵相同。因此，交叉熵总是大于或等于熵。</p><h2 id="2759" class="ni ly it bd lz nj nk dn md nl nm dp mh li nn no mj lm np nq ml lq nr ns mn nt bi translated">计算交叉熵</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/211cdbdfdd3319ba56eb678447896790.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*-FCvK_3r0USmSQyGzxsPfA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The Cross-Entropy of the system</figcaption></figure><p id="be61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="nv"> pᵢ </em>是第<em class="nv">个</em>事件的实际概率，q <em class="nv"> ᵢ </em>是第 I 个事件的估计概率。所用对数以 2 为底。</p><p id="cb69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们计算上述示例的交叉熵，并显示情况 II 比情况 I 具有更低的交叉熵，因为它更接近实际分布。</p><p id="e5da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">情况一:发现硬币有偏差，头部概率为 0.70。</p><p id="d5e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">=-0.75 * log(0.70)-0.25 * log(0.30)= 0.820</p><p id="07a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">情况二:发现硬币有偏差，正面概率为 0.72。</p><p id="51e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">=-0.75 * log(0.72)-0.25 * log(0.28)= 0.814</p><p id="767f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，情况 II 的交叉熵小于情况 I，因为它更接近实际分布。</p><p id="9eec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果估计是，硬币偏向正面的概率是 0.75，</p><p id="5b9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">=-0.75 * log(0.75)-0.25 * log(0.25)= 0.811(最小，这里交叉熵与熵相同)</p><h1 id="9acf" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">3.了解 KL 差异</h1><p id="7594" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">KL 散度只是拟合分布和实际分布之间的差异，即交叉熵和熵之间的差异。还可以查看这两种分布有多大差异。</p><h2 id="803a" class="ni ly it bd lz nj nk dn md nl nm dp mh li nn no mj lm np nq ml lq nr ns mn nt bi translated">计算 KL 散度</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/83d5824830ba7ce2c0da39527563e7e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wzgJUCDsBgtleCIGmCMo5Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">KL Divergence</figcaption></figure><p id="3bd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们举一个同样的例子，我们得到了一个偏向硬币，正面的概率是 0.75。我们没有意识到硬币的偏见，所以</p><p id="533f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">案例一:我们掷硬币一百次，得到 70 个正面和 30 个反面。我们的估计是硬币有偏差，正面概率为 0.70。</p><p id="56e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">KL = 0.75 * log(0.75/0.70)+0.25 * log(0.25/0.30)= 0.0088</p><p id="645b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">案例二:我们投掷硬币一百次，得到 72 个正面和 28 个反面。我们的估计是硬币有偏差，正面概率为 0.72。</p><p id="6db5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nv">KL-散度</em>= 0.75 * log(0.75/0.72)+0.25 * log(0.25/0.28)= 0.0032</p><p id="dca8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，第二种情况下的距离比第一种情况下的距离小，因为第二种分布更接近实际分布。</p><p id="7a99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果估计是，硬币偏向正面的概率是 0.75，</p><p id="92c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nv">KL-Divergence</em>= 0.75 * log(0.75/0.75)+0.25 * log(0.25/0.25)= 0(熵与交叉熵相同)，所以两个分布没有区别。</p><h1 id="7035" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">4.结论</h1><p id="5024" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">这是第一篇让我们彻底理解熵、交叉熵和 KL-散度的文章。我们也通过直观的方式和通过计算理解了这些术语，并通过例子看到了它们的价值。我希望这澄清了这个话题，给了它一个不同于传统互联网上的观点。在下一篇文章中，我打算在真实的数据集中使用这些概念。它将为每个人提供如何在每个小的分类、聚类或其他日常机器学习问题中利用这些学习到的概念的想法/直觉。不要错过它伙计们，记住我的话，它只会变得更好。</p><p id="f24a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nv">我的 Youtube 频道更多内容:</em> </strong></p><div class="ny nz gp gr oa ob"><a href="https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd iu gy z fp og fr fs oh fu fw is bi translated">阿布舍克·蒙戈利</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">嗨，伙计们，欢迎来到频道。该频道旨在涵盖各种主题，从机器学习，数据科学…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">www.youtube.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op ks ob"/></div></div></a></div><p id="4956" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">写一篇清晰易懂的好文章需要很多努力。我会继续努力做好我的工作。在<a class="ae ky" href="https://medium.com/@mungoliabhishek81" rel="noopener"> <strong class="lb iu">中</strong> </a>关注我，查看我以前的帖子。我欢迎反馈和建设性的批评。</p><h1 id="775a" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">5.参考</h1><ol class=""><li id="fb25" class="mu mv it lb b lc mp lf mq li oq lm or lq os lu mz na nb nc bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Entropy" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Entropy</a></li><li id="0e97" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Cross_entropy</a></li><li id="947e" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/kull back % E2 % 80% 93 lei bler _ divergence</a></li><li id="a939" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/demystifying-entropy-f2c3221e2550">https://towards data science . com/demystifying-entropy-f2c 3221 e 2550</a></li><li id="5255" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/demystifying-cross-entropy-e80e3ad54a8">https://towards data science . com/demystifying-cross-entropy-e 80 e 3 ad 54 a 8</a></li><li id="94d4" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/demystifying-kl-divergence-7ebe4317ee68">https://towards data science . com/demystifying-KL-divergence-7 ebe 4317 ee 68</a></li></ol></div></div>    
</body>
</html>