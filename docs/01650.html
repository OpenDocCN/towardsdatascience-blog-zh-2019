<html>
<head>
<title>Correlation between score and comments on the front page of Reddit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Reddit 首页评分与评论的相关性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/correlation-between-score-and-comments-on-the-front-page-of-reddit-a9e7b4f23b64?source=collection_archive---------18-----------------------#2019-03-17">https://towardsdatascience.com/correlation-between-score-and-comments-on-the-front-page-of-reddit-a9e7b4f23b64?source=collection_archive---------18-----------------------#2019-03-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="88bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我不是 Reddit 的大用户，但当我在那里时，我通常只在几个子网站上闲逛(r/dataisbeautiful，r/geography，r/philadelphia，r/emo 和其他几个网站)。除了广受欢迎的数据 viz subreddit，这些数据都不会产生你在 reddit 首页看到的分数。</p><p id="2b59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我说这些只是为了指出<em class="ko">我没有</em>挖掘出 Reddit 文化的所有细微差别。<em class="ko">然而</em>有一天，在仔细阅读了互联网的首页后，我确实有一个问题——一个热门帖子的分数和围绕它的对话量之间有关联吗？</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/109d210cb04aba7cee25bf93dabcff56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*20rxptI01ksWY4zwzad_HQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">It looks like r/worldnews really struck up some conversations!</figcaption></figure><p id="3512" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，<strong class="js iu">获取数据</strong>。我看了一下 Reddit 主页的 api，但没有看到在给定的日期和时间内拉顶部帖子的方法。所以我求助于<a class="ae lf" href="https://web.archive.org/" rel="noopener ugc nofollow" target="_blank"> Wayback 机器的</a> API，它可以使用特定的日期和时间作为端点，并将返回最近的网页截图的 url。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lg"><img src="../Images/8156a2014d461cb7124c5739cdec3a7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UdYwXteJHHVeOqABAe-7vQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">The front page is pretty well archived it appears.</figcaption></figure><p id="2b7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我非常自信地认为我可以为 2018 年收集大量数据，于是我跳到 R 并生成了一个完整的 URL 列表来调用 API。</p><pre class="kq kr ks kt gt lh li lj lk aw ll bi"><span id="d02b" class="lm ln it li b gy lo lp l lq lr">library(lubridate)<br/>dateRange &lt;- gsub(“-”,””,seq(ymd(‘20180101’),ymd(‘20181231’), by = ‘1 day’, truncated=2))<br/>base_url &lt;- "<a class="ae lf" href="https://archive.org/wayback/available?url=reddit.com" rel="noopener ugc nofollow" target="_blank">https://archive.org/wayback/available?url=reddit.com</a>"</span><span id="0e26" class="lm ln it li b gy ls lp l lq lr">#create list of api urls<br/>url_list &lt;- c()<br/>for (date in dateRange) {<br/>  full_url &lt;- paste(base_url, "&amp;timestamp=",date, "120000", sep="")<br/>  url_list &lt;- c(url_list, full_url)<br/>}</span></pre><p id="af05" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以调用 Wayback 机器来获取网页截图列表。</p><pre class="kq kr ks kt gt lh li lj lk aw ll bi"><span id="f349" class="lm ln it li b gy lo lp l lq lr">#create list of archive links<br/>archive_list &lt;- c()<br/>archive_times &lt;- c()</span><span id="8c36" class="lm ln it li b gy ls lp l lq lr">for (url in url_list) {<br/>  #get raw result from api call<br/>  raw.result &lt;- GET(url = url)<br/>  raw.result$status_code<br/>  #get raw content from results<br/>  this.raw.content &lt;- rawToChar(raw.result$content)<br/>  #put content into list<br/>  this.content &lt;- fromJSON(this.raw.content)<br/>  #extract archive url from list and add to archive_list<br/>  archive_times &lt;- c(archive_times, this.content$archived_snapshots$closest$timestamp)<br/>  archive_list &lt;- c(archive_list, this.content$archived_snapshots$closest$url)<br/>}</span></pre><p id="2606" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这给了我们一个 365 个 URL 的列表，从每天中午开始捕获。现在开始真正的网络抓取。可能有更快的方法，但是我用一个老式的<em class="ko">来循环</em>，并使用<strong class="js iu"> rvest </strong>包来抓取该页面 25 个帖子中每个帖子的分数、评论数和 r/subreddit。</p><p id="bca9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在将 r/subreddit 值添加到 datalist 变量之前，我通过检查来确保 r/subreddit 值的长度大于 0(即任何帖子实际上都被提取了),从而包含了一些简单的错误处理。</p><p id="4b3c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">循环完成后，我使用 rbind 填充数据帧并过滤掉任何有问题的数据。</p><pre class="kq kr ks kt gt lh li lj lk aw ll bi"><span id="5c9f" class="lm ln it li b gy lo lp l lq lr">#create empty list<br/>datalist = list()</span><span id="0df1" class="lm ln it li b gy ls lp l lq lr">#loop through archive urls<br/>for (i in 1:length(archive_list)) {<br/>  #get all the html from the webpage<br/>  webpage &lt;- read_html(archive_list[i])<br/>  #filter all the .things<br/>  things &lt;- webpage %&gt;%<br/>    html_node("#siteTable") %&gt;%<br/>    html_nodes(".thing")<br/>  #get votes<br/>  score &lt;- things %&gt;%<br/>    html_node(".score") %&gt;%<br/>    html_text()<br/>  #get number of comments<br/>  comments &lt;- things %&gt;%<br/>    html_node(".comments") %&gt;%<br/>    html_text()<br/>  #remove " comments" and convert to number<br/>  comments &lt;- as.numeric(gsub(" comments","", comments))<br/>  # get post subreddit<br/>  subreddit &lt;- things %&gt;%<br/>    html_node(".subreddit") %&gt;%<br/>    html_text()<br/>  #get date of page<br/>  date &lt;- gsub("<a class="ae lf" href="http://web.archive.org/web/|/https://www.reddit.com/" rel="noopener ugc nofollow" target="_blank">http://web.archive.org/web/|/https://www.reddit.com/</a>", "", archive_list[i])</span><span id="c6c7" class="lm ln it li b gy ls lp l lq lr">if (length(subreddit) &gt; 0) {<br/>    print(paste(unique(date),length(subreddit),sep=" "))<br/>    #create temp df<br/>    temp &lt;- data.frame(date = date, score = score, comments = comments, subreddit = subreddit)<br/>    #add it to the list<br/>    datalist[[i]] &lt;- temp<br/>  }<br/>}</span><span id="b20b" class="lm ln it li b gy ls lp l lq lr">#make a df from the datalist<br/>main_data = do.call(rbind, datalist)<br/>#remove incomplete posts<br/>reddit_posts &lt;- main_data %&gt;% <br/>  filter(score != "•",<br/>         !is.na(score),<br/>         !is.na(comments)<br/>         ) %&gt;% <br/>  mutate(score = as.numeric(sub("k", "e3", score, fixed = TRUE)),<br/>         subreddit = gsub(".*r/","r/",subreddit))</span></pre><p id="04aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">擦伤怎么样了？不算太坏。该网站在一年的 75%的时间里成功地发布了每日帖子。我没有对此进行彻底的调查，因为我有足够的数据来处理，但我认为 Wayback 机器在 Reddit 网站重新设计方面存在一些问题。</p><p id="7c2d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们有了一个新生成的数据集，但是为了产生我想要的可视化效果，它需要一些争论。</p><ol class=""><li id="0d81" class="lt lu it js b jt ju jx jy kb lv kf lw kj lx kn ly lz ma mb bi translated">找出将最多帖子发送到首页的八个子编辑</li><li id="e4eb" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated">将来自非顶级订阅者的帖子的 subreddit 值更改为“其他”</li><li id="9036" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated">对子编辑因子级别进行重新分类，使其以降序排列，末尾是“其他”。</li></ol><pre class="kq kr ks kt gt lh li lj lk aw ll bi"><span id="097f" class="lm ln it li b gy lo lp l lq lr">#get top 8 subreddits<br/>top_subs &lt;- reddit_posts %&gt;% <br/>  group_by(subreddit) %&gt;% <br/>  summarise(count=n()) %&gt;% <br/>  top_n(8, count) %&gt;% <br/>  ungroup()</span><span id="6db4" class="lm ln it li b gy ls lp l lq lr">#create vector of top_subs<br/>top_subs &lt;- as.character(top_subs$subreddit)</span><span id="ec6e" class="lm ln it li b gy ls lp l lq lr">#make notin operator<br/>'%!in%' &lt;- function(x,y)!('%in%'(x,y))</span><span id="d884" class="lm ln it li b gy ls lp l lq lr">reddit_posts_reduc &lt;- reddit_posts %&gt;% <br/>  mutate(subreddit = case_when(<br/>    subreddit %!in% top_subs ~ 'other',<br/>    TRUE ~ as.character(.$subreddit)<br/>  ))</span><span id="cd00" class="lm ln it li b gy ls lp l lq lr">#get list of factors in descending order<br/>factor_order &lt;- reddit_posts_reduc %&gt;% <br/>  group_by(subreddit) %&gt;% <br/>  summarise(count=n()) %&gt;% <br/>  arrange(desc(count)) %&gt;% <br/>  select(subreddit)</span><span id="2575" class="lm ln it li b gy ls lp l lq lr">#overwrite with list<br/>factor_order &lt;- as.vector(factor_order$subreddit) <br/>#remove "other" from first position<br/>factor_order &lt;- factor_order[-1]<br/>#create new factor level list<br/>factor_order2 &lt;- factor_order<br/>#update new factor list with ordering info<br/>for (i in 1:length(factor_order)) {<br/>  factor_order2[[i]] &lt;- paste("#",i," ",factor_order[[i]], sep = "")<br/>}<br/>#append other to both factor lists<br/>factor_order &lt;- append(factor_order, "other")<br/>factor_order2 &lt;- append(factor_order2, "other")</span><span id="a86c" class="lm ln it li b gy ls lp l lq lr">#update dataframe levels with update factor levels<br/>reddit_posts_reduc$subreddit_f &lt;- mapvalues(reddit_posts_reduc$subreddit, from = factor_order, to = factor_order2)<br/>levels(reddit_posts_reduc$subreddit_f)</span></pre><p id="c42a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，是时候策划了。我把评论的数量标在 x 轴上，分数标在 y 轴上。我使用轴限制来说明异常值，最终结果是一个按子网格分组并标有相关系数的小多次波图。</p><pre class="kq kr ks kt gt lh li lj lk aw ll bi"><span id="64f0" class="lm ln it li b gy lo lp l lq lr">#plot data<br/>reddit_posts_reduc %&gt;% <br/>ggplot(aes(<br/>  x=score,<br/>  y=comments,<br/>  color=subreddit_f)<br/>       ) +<br/>  geom_point(size=3, alpha=0.4) +<br/>  facet_wrap(~subreddit_f, ncol = 3) +<br/>  geom_smooth(se=F) +<br/>  theme_fivethirtyeight() +<br/>  theme(axis.title=element_text()) +<br/>  # labs(title = "Correlation between score and comments on front page",<br/>  #      subtitle = "Posts from the front page of Reddit in 2018 plotted to show correlation between score and the number of comments. Posts are grouped by the eight subreddits that sent the most posts to the front page with all other posts grouped in other.",<br/>  #      caption = "Data from Reddit via Archive.org\nChart by @jared_whalen"<br/>  #      ) +<br/>  theme(legend.position="none") +<br/>  stat_cor(method = "pearson", label.x = 110000, label.y = 9000) +<br/>  scale_y_continuous(label=unit_format(unit = "K", scale = 1e-3, sep=""),<br/>                     limits=c(0,10000)) +<br/>  scale_x_continuous(label=unit_format(unit = "K", scale = 1e-3, sep=""),<br/>                     limits=c(0,150000)) +<br/>  xlab("Score") +<br/>  ylab("Number of comments"<strong class="li iu">)</strong></span></pre><p id="7fad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">我从这个项目中得到的东西</strong></p><ul class=""><li id="a651" class="lt lu it js b jt ju jx jy kb lv kf lw kj lx kn mh lz ma mb bi translated">如何使用 Wayback 机器的 API 来抓取存档的页面</li><li id="880b" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn mh lz ma mb bi translated">更好地理解重新指定因子级别，以便在打印时自定义排序</li></ul><p id="d955" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里是整个源代码的要点。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="mi mj l"/></div></figure></div></div>    
</body>
</html>