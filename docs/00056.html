<html>
<head>
<title>Predicting Invasive Ductal Carcinoma using Convolutional Neural Network (CNN) in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Keras 中使用卷积神经网络(CNN)预测浸润性导管癌</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-invasive-ductal-carcinoma-using-convolutional-neural-network-cnn-in-keras-debb429de9a6?source=collection_archive---------12-----------------------#2019-01-03">https://towardsdatascience.com/predicting-invasive-ductal-carcinoma-using-convolutional-neural-network-cnn-in-keras-debb429de9a6?source=collection_archive---------12-----------------------#2019-01-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8db1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 CNN 将组织病理学切片分类为恶性或良性</h2></div><p id="ffc3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博客中，我们将学习如何在真实世界的组织病理学数据集中使用 CNN。真实世界的数据比 MNIST 等标准数据集需要更多的预处理，我们将经历使数据为分类做好准备的过程，然后使用 CNN 对图像进行分类。我还将讨论所使用的 CNN 架构以及在构建模型时调整的一些超参数。</p><p id="81d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个博客的不同部分是:</p><ul class=""><li id="9c15" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">介绍</li><li id="8855" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">了解数据集</li><li id="ffa8" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">加载数据集</li><li id="3cc0" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">数据预处理</li><li id="b48f" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">通过随机欠采样处理数据不平衡</li><li id="55ee" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">模型架构</li><li id="4475" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">编译模型</li><li id="ee19" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">数据扩充</li><li id="66d2" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">训练模型</li><li id="ccfc" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">做预测</li><li id="7a54" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">评估模型性能</li></ul><p id="e3b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我假设读者知道什么是卷积神经网络及其基本工作机制。</p><p id="0e35" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博客中，我们将只讨论在深度学习的背景下重要的代码，同时避免重复解释不止一次使用的代码。</p><p id="03dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章的 Github 库可以在这里找到。我建议你在阅读本教程的时候跟着 Jupyter 笔记本走。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h2 id="47f2" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated">介绍</h2><p id="b0ab" class="pw-post-body-paragraph kf kg iq kh b ki mq jr kk kl mr ju kn ko ms kq kr ks mt ku kv kw mu ky kz la ij bi translated">在过去的几年里，使用深度学习进行医学图像分析的情况越来越多，并且越来越成功。医疗保健领域的深度学习用于识别模式、分类和分割医学图像。与大多数图像相关的任务一样，卷积神经网络用于完成这一任务。</p><p id="f894" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里解决的分类问题是将浸润性导管癌(IDC)的组织病理学切片分类为恶性或良性。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi mv"><img src="../Images/74aa299b4077724b0341ec4960e302bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dD7oWfCLnS8kHPZLqHo5yg.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Histopathology slide of a malignant tumour. Image credits: Department of Pathology at Johns Hopkins University</figcaption></figure><p id="1840" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">IDC 是一种乳腺癌，其中癌症已经扩散到周围的乳腺组织。</p><p id="d118" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">癌症肿瘤可以分为两种类型:恶性和良性。良性肿瘤是一种不侵犯其周围组织的肿瘤，而恶性肿瘤是一种可以扩散到其周围组织或身体其他部位的肿瘤。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h2 id="3dd6" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated">了解数据集</h2><blockquote class="nl nm nn"><p id="dc2f" class="kf kg no kh b ki kj jr kk kl km ju kn np kp kq kr nq kt ku kv nr kx ky kz la ij bi translated">我们将使用的数据集可以从<a class="ae lp" href="http://www.andrewjanowczyk.com/use-case-6-invasive-ductal-carcinoma-idc-segmentation/" rel="noopener ugc nofollow" target="_blank">这里</a>下载。向下滚动到页面的数据集描述部分，并下载 1.6 GB 的 zip 文件。</p></blockquote><p id="695b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该数据集由 162 幅以 40 倍扫描的乳腺癌标本的完整载片图像组成。从中提取了 277，524 个大小为 50×50 的斑块，其中 198，738 个为 IDC 阴性(良性)，78，786 个为 IDC 阳性(恶性)。</p><p id="4c7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个修补程序的文件名的格式如下:</p><p id="3aa7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">u_xX_yY_classC.png →示例 10253 _ idx 5 _ x 1351 _ y 1101 _ class 0 . png</p><p id="6ace" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中 u 是患者 ID (10253_idx5)，X 是裁剪此补丁的 X 坐标，Y 是裁剪此补丁的 Y 坐标，C 表示类别，其中 0 表示非 IDC(良性)，1 表示 IDC(恶性)</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h2 id="4bfc" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated"><strong class="ak">加载数据集:</strong></h2><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="fa3e" class="lx ly iq nt b gy nx ny l nz oa">imagePatches = glob('C:/Users/asus/Documents/Breast cancer classification/**/*.png', recursive=True)</span><span id="0d6c" class="lx ly iq nt b gy ob ny l nz oa">patternZero = '*class0.png'<br/>patternOne = '*class1.png'</span><span id="cd6d" class="lx ly iq nt b gy ob ny l nz oa">#saves the image file location of all images with file name 'class0' classZero = fnmatch.filter(imagePatches, patternZero) </span><span id="896b" class="lx ly iq nt b gy ob ny l nz oa">#saves the image file location of all images with file name 'class1'<br/>classOne = fnmatch.filter(imagePatches, patternOne)</span></pre><p id="b922" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据集由 279 个文件夹组成，279 个文件夹中的每个文件夹都有子文件夹 0 和 1。我们首先创建两个变量 classZero 和 classOne，分别保存所有 class 0 和 class 1 图像的位置</p><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="bc24" class="lx ly iq nt b gy nx ny l nz oa">def process_images(lowerIndex,upperIndex):<br/>    """<br/>    Returns two arrays: <br/>        x is an array of resized images<br/>        y is an array of labels<br/>    """ <br/>    height = 50<br/>    width = 50<br/>    channels = 3<br/>    x = [] #list to store image data<br/>    y = [] #list to store corresponding class<br/>    for img in imagePatches[lowerIndex:upperIndex]:<br/>        full_size_image = cv2.imread(img)<br/>        image = (cv2.resize(full_size_image, (width,height), interpolation=cv2.INTER_CUBIC))<br/>        x.append(image)<br/>        if img in classZero:<br/>            y.append(0)<br/>        elif img in classOne:<br/>            y.append(1)<br/>        else:<br/>            return<br/>    return x,y</span></pre><p id="5dd3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们创建一个函数<em class="no"> process_images </em>，它将图像的起始和结束索引作为输入。这个函数首先使用 OpenCV 的 cv2.imread()读取图像，并调整图像的大小。因为数据集中的图像很少不是 50x50x3，所以完成了调整大小。该函数返回两个数组:X 是调整大小后的图像数据的数组，Y 是相应标签的数组。</p><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="1632" class="lx ly iq nt b gy nx ny l nz oa">X, Y = process_images(0,100000)</span></pre><p id="b580" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于本教程，我们将只分析从索引 0 到 60，000 的图像。图像数据(像素值)现在存储在列表 X 中，它们相应的类存储在列表 y 中</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h2 id="67b9" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated"><strong class="ak">数据预处理:</strong></h2><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="3f52" class="lx ly iq nt b gy nx ny l nz oa">X = np.array(X)<br/>X = X.astype(np.float32)<br/>X /= 255.</span></pre><p id="8022" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">列表 X 首先被转换为 numpy 数组，然后被转换为 float32 类型以节省空间。</p><p id="5756" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图像首先通过除以 255 进行归一化。这确保了所有值都在 0 和 1 之间。这有助于我们更快地训练模型，也防止我们陷入消失和爆炸梯度的问题。</p><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="8ea6" class="lx ly iq nt b gy nx ny l nz oa">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.15)</span></pre><p id="e39e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据集分为训练集和测试集，整个数据集的 15%保留用于测试。对于 60，000 的数据集，这意味着 51000 个图像被保留用于训练，9000 个用于测试。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h2 id="515e" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated">通过随机欠采样处理数据不平衡</h2><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="e79d" class="lx ly iq nt b gy nx ny l nz oa">y_train.count(1)  #counting the number of 1<br/>y_train.count(0)  #counting the number of 0</span></pre><p id="4d37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计算数组 Y 中 1 和 0 的数量，我们发现有 44478 个 0 类图像和 15522 个 1 类图像。</p><p id="76c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个问题被称为数据不平衡，会导致我们的模型更偏向于某个特定的类，通常是拥有更多样本的类。特别是在医疗保健等领域，将少数群体(在这种情况下是恶性的)归类为多数群体(在这种情况下是良性的)可能非常危险。我们将通过随机欠采样多数类来处理数据不平衡，即移除多数类的样本以使多数类和少数类的样本数量相等。</p><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="9e64" class="lx ly iq nt b gy nx ny l nz oa">y_train = to_categorical(y_train)<br/>y_test = to_categorical(y_test)</span></pre><p id="2fd4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在此之前，我们需要对输出变量 y_train 和 y_test 进行一次热编码。</p><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="fc6d" class="lx ly iq nt b gy nx ny l nz oa">X_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]<br/>X_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]<br/>X_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)<br/>X_testFlat = X_test.reshape(X_test.shape[0], X_testShape)</span></pre><p id="e2c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还需要改变 X_train 和 X_test 的形状，以使用随机欠采样。</p><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="17a5" class="lx ly iq nt b gy nx ny l nz oa">from imblearn.under_sampling import RandomUnderSampler<br/>random_under_sampler = RandomUnderSampler(ratio='majority')<br/>X_trainRos, Y_trainRos = random_under_sampler.fit_sample(X_trainFlat, y_train)<br/>X_testRos, Y_testRos = random_under_sampler.fit_sample(X_testFlat, y_test)</span></pre><p id="d8f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参数“ratio=majority”表示随机欠采样到欠采样多数类。</p><p id="446e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在执行随机欠采样后，再次检查每个类别的样本数量，我们发现两个类别的样本数量相等。然后，图像数据被转换回其 50 x 50 x 3 的原始形状。</p><h2 id="e824" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated">模型架构</h2><p id="e688" class="pw-post-body-paragraph kf kg iq kh b ki mq jr kk kl mr ju kn ko ms kq kr ks mt ku kv kw mu ky kz la ij bi translated">我们使用一个与这篇<a class="ae lp" href="http://www.apsipa.org/proceedings/2017/CONTENTS/papers2017/15DecFriday/FP-02/FP-02.3.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中讨论的<em class="no">类似的</em>架构。</p><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="1d35" class="lx ly iq nt b gy nx ny l nz oa">batch_size = 256<br/>num_classes = 2<br/>epochs = 80</span><span id="44d8" class="lx ly iq nt b gy ob ny l nz oa">model = Sequential()<br/>model.add(Conv2D(32, kernel_size=(3,3),<br/>                 activation='relu',<br/>                 input_shape=(50,50,3)))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Conv2D(64, (3,3), activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2,2)))<br/>model.add(Conv2D(128, (3, 3), activation='relu'))<br/>model.add(Conv2D(256, (3, 3), activation='relu'))<br/>model.add(Flatten()) #this converts our 3D feature maps to 1D feature vectors for the dense layer below<br/>model.add(Dropout(0.5))<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dense(num_classes, activation='sigmoid'))</span></pre><p id="9b01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型是连续的，允许我们逐层创建模型。该架构由卷积层、最大池层、丢弃层和全连接层组成。</p><p id="9fb7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一层是卷积层，具有 32 个大小为 3×3 的滤波器。我们还需要在第一层中指定输入形状，在我们的例子中是 50 x 50 x 3。</p><p id="a68b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将对除最终输出层之外的所有层使用整流线性单元(ReLU)激活函数。ReLU 是隐藏层中激活函数的最常见选择，并且已经显示出相当好的效果。</p><p id="119f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二层是汇集层。汇集层用于减少维度。2x2 窗口的最大池仅考虑 2x2 窗口中的最大值。</p><p id="6c62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第三层也是 64 个滤波器的卷积层，每个滤波器的大小为 3×3，后面是另一个 2×2 窗口的最大池层。通常，卷积层中的滤波器数量在每一层之后都会增长。具有较低数量滤波器的第一层学习图像的简单特征，而较深的层学习更复杂的特征。</p><p id="538e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来的两层也是卷积层，具有相同的滤波器大小，但是滤波器数量增加；128 和 256。</p><p id="5069" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在添加完全连接的层之前，我们需要将从卷积层输出的 3D 特征图展平为 1D 特征向量。这就是扁平化层的用武之地。</p><p id="3d1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一层是辍学率为 0.5 的辍学层。脱落率为 0.5 的脱落层意味着 50%的神经元将被随机关闭。这有助于防止过度拟合，方法是让所有的神经元学习一些关于数据的知识，而不仅仅依赖于少数几个神经元。在训练过程中随机丢弃神经元意味着其他神经元将不得不做关闭的神经元的工作，从而更好地泛化并防止过度拟合。</p><p id="8f25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">值 0.5 取自<a class="ae lp" href="https://arxiv.org/pdf/1207.0580.pdf" rel="noopener ugc nofollow" target="_blank"> Hinton (2012) </a>的原始论文，经证明非常有效。这些脱落层被添加到输出之前的每个完全连接的层之后。辍学也减少了每个历元的训练时间。</p><p id="a82d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的密集层(全连接层)有 128 个神经元。其后是另一个退出层，退出率为 0.5</p><p id="3915" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一层是另一个密集层，有 128 个神经元。</p><p id="25a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最终输出层是另一个密集层，其神经元的数量等于类别的数量。这一层中的激活函数是 sigmoid，因为手头的问题是二元分类问题。对于多类分类问题，激活函数应设置为 softmax。</p><h2 id="7446" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated">编译模型</h2><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="a5b7" class="lx ly iq nt b gy nx ny l nz oa">model.compile(loss=keras.losses.binary_crossentropy,<br/>              optimizer=keras.optimizers.Adam(lr=0.00001),<br/>              metrics=['accuracy'])</span></pre><p id="5780" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型采用二元交叉熵损失函数进行编译，并使用 Adam 优化器。“准确性”指标用于评估模型。</p><p id="c204" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Adam 是一种优化算法，它以迭代的方式更新网络权重。</p><p id="96df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然可以设置 Adam 的初始学习速率(在我们的例子中，我们将其设置为 0.00001)，但这是初始学习速率，每个参数的学习速率随着训练的开始而调整。这就是 Adam(自适应矩估计的缩写)与随机梯度下降的不同之处，随机梯度下降对所有权重更新保持单一的学习速率。亚当优化算法的详细解释可以在<a class="ae lp" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><p id="38c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">学习率决定了我们将网络的权重向局部最小值调整的速度。过高的学习率会导致如此高的权重变化，以至于可能导致超过局部最小值。这导致训练或验证误差在连续的时期之间剧烈波动。学习率太低会导致花更长的时间来训练我们的网络。因此，学习率是建立模型时需要调整的最重要的超参数之一。</p><h2 id="853a" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated">数据扩充</h2><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="250d" class="lx ly iq nt b gy nx ny l nz oa">datagen = ImageDataGenerator(<br/>    featurewise_center=True,<br/>    featurewise_std_normalization=True,<br/>    rotation_range=180,<br/>    horizontal_flip=True,vertical_flip = True)</span></pre><p id="a481" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一般来说，我们拥有的数据越多，深度学习往往就越有效。Keras ImageDataGenerator 使用数据扩充在训练期间生成实时图像。转换是在运行中对小批量执行的。数据扩充通过减少网络过度拟合训练数据的能力来帮助概括模型。旋转、垂直和水平翻转是一些常用的数据扩充技术。</p><p id="9219" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Keras ImageDataGenerator 提供了多种数据扩充技术。但是，我们只会使用其中的少数几个。</p><blockquote class="nl nm nn"><p id="1cf7" class="kf kg no kh b ki kj jr kk kl km ju kn np kp kq kr nq kt ku kv nr kx ky kz la ij bi translated">如果将标记为恶性的组织病理学载玻片旋转 20 度并垂直翻转，它仍然是恶性的。</p></blockquote></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h2 id="1850" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated">训练模型</h2><blockquote class="nl nm nn"><p id="3ed6" class="kf kg no kh b ki kj jr kk kl km ju kn np kp kq kr nq kt ku kv nr kx ky kz la ij bi translated">使用 GPU 训练模型可以加快训练过程。你将需要一个 NVIDIA 的 GPU 来做到这一点。我按照这个<a class="ae lp" href="https://medium.com/@ab9.bhatia/set-up-gpu-accelerated-tensorflow-keras-on-windows-10-with-anaconda-e71bfa9506d1" rel="noopener">教程</a>来启用 GPU 训练。</p></blockquote><p id="baae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将历元的数量设置为一个较大的数字，在我们的例子中为 80，并使用一种称为早期停止的正则化方法</p><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="c876" class="lx ly iq nt b gy nx ny l nz oa">early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=3, mode='min')</span></pre><p id="81ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">早期停止是一种用于避免过度拟合的方法，当要观察的参数集在一定数量的时期内没有改善时，通过停止训练过程来避免过度拟合。</p><blockquote class="nl nm nn"><p id="cf4e" class="kf kg no kh b ki kj jr kk kl km ju kn np kp kq kr nq kt ku kv nr kx ky kz la ij bi translated">在我们的例子中，我们告诉 EarlyStopping 监控 val_loss，如果连续 3 个时期没有改善，就停止训练过程。"</p></blockquote><p id="e053" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">批量大小通常设置为 2 的幂，因为这样计算效率更高。我们将其设置为 256。</p><p id="17e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用另一个称为 ModelCheckpoint 的 Keras 回调</p><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="4115" class="lx ly iq nt b gy nx ny l nz oa">model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)</span></pre><p id="759c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型检查点用于保存模型。monitor 参数允许我们设置一个我们想要关注指标。在我们的例子中，我们只在验证损失最小的时候保存模型。我们保存最佳模型，以便稍后用于进行预测，从而评估模型的性能。</p><blockquote class="nl nm nn"><p id="ff31" class="kf kg no kh b ki kj jr kk kl km ju kn np kp kq kr nq kt ku kv nr kx ky kz la ij bi translated">结合这两个回调，保存最佳模型(其具有最小的验证损失),然后，如果验证损失在接下来的 3 个时期(由 EarlyStopping 设置)没有改善(减少),则模型训练停止。</p></blockquote><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="06a4" class="lx ly iq nt b gy nx ny l nz oa">training = model.fit_generator(datagen.flow(X_trainRosReshaped,Y_trainRosHot,batch_size=batch_size),steps_per_epoch=len(X_trainRosReshaped) / batch_size, epochs=epochs,validation_data=(X_testRosReshaped, Y_testRosHot), verbose=1, callbacks=[early_stopping_monitor, model_checkpoint])</span></pre><p id="8058" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为我们正在动态使用 ImageDataGenerator，所以我们使用 model.fit_generator 来训练模型。我们将其设置为变量“training ”,因为我们稍后将绘制训练损失和验证损失。这有助于我们了解方差，即训练误差和验证集误差之间的差异。</p><p id="2072" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了验证，我们将使用 x_testRosReshaped 和 y_test shot，它们是在对 X _ test 和 Y _ test 集进行欠采样后获得的。</p><p id="a318" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于提前停止，训练在 37 个周期后停止。因此，保存的最佳模型是在时期 34 期间的模型，验证精度为 79.10%</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oc"><img src="../Images/abcc464f1b1eb7aa234fd38bfcabecec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2C8z5CyZwdsy4qo03NvWCw.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">val_loss stopped improving after Epoch 34</figcaption></figure><p id="13ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">绘制训练集和验证集损失，我们发现方差非常低。该图确保我们的模型不会过度拟合。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi od"><img src="../Images/10d71245ced36ea716dec1ee0700bc76.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*5i7znHkADNL6_40jl2R2Pw.png"/></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Training and testing set loss</figcaption></figure><h2 id="d532" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated">做预测</h2><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="1873" class="lx ly iq nt b gy nx ny l nz oa">from keras.models import load_model<br/>from sklearn import metrics</span><span id="3557" class="lx ly iq nt b gy ob ny l nz oa">model = load_model('best_model.h5')</span><span id="4f80" class="lx ly iq nt b gy ob ny l nz oa">y_pred_one_hot = model.predict(X_testRosReshaped)</span><span id="f01f" class="lx ly iq nt b gy ob ny l nz oa">y_pred_labels = np.argmax(y_pred_one_hot, axis = 1)<br/>y_true_labels = np.argmax(Y_testRosHot,axis=1)</span></pre><p id="4663" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们加载由 ModelCheckpoint 保存的最佳模型，并使用 predict 函数来预测 X_testRosReshaped 数组中图像的类别。预测现在存储在列表 y_pred_labels 中。</p><h2 id="a0fa" class="lx ly iq bd lz ma mb dn mc md me dp mf ko mg mh mi ks mj mk ml kw mm mn mo mp bi translated">评估模型的性能</h2><pre class="mw mx my mz gt ns nt nu nv aw nw bi"><span id="59eb" class="lx ly iq nt b gy nx ny l nz oa">confusion_matrix = metrics.confusion_matrix(y_true=y_true_labels, y_pred=y_pred_labels)</span></pre><p id="315b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用混淆矩阵来评估模型的性能。二进制分类矩阵中的混淆矩阵有四个象限；假阳性，假阴性，真阳性和真阴性。</p><p id="18bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于我们的情况，混淆矩阵的四个象限可以简化如下:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7ca45c62b756cba3eb0e394223034ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*GQCxnaPXqHAQhiaVKgJ5kw.png"/></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Confusion matrix with description of the 4 quadrants for our case</figcaption></figure><p id="e3f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们得到的混淆矩阵是:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi of"><img src="../Images/2c8bc096da8277032ec6fbaaa7a23c1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*B2gT86xCNPgrQYtyJCuvcg.png"/></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Resultant confusion matrix</figcaption></figure><p id="79d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，较低的假阴性比较低的假阳性要好。这是因为将恶性肿瘤识别为良性比将良性肿瘤识别为恶性更危险，因为前者将导致患者因误诊而接受不同的治疗，而后者无论如何都可能要接受进一步的检查。</p><p id="0e82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到，我们的模型在测试集上表现良好，准确率为 79.10%。只有混淆矩阵也对我们有利，我们有一个低方差的模型。</p><p id="63fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">应用深度学习是一个迭代的过程。您可以通过调整优化算法的学习速率、更改批量大小、更改卷积层的滤波器、添加更多层或使用更多数据等超参数来尝试和进一步改进该模型。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><p id="a512" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="no">感谢阅读。如果你有任何问题，请在下面评论。我将定期撰写深度学习等主题的文章，所以请在 Medium 上关注我。我也可以上</em> <a class="ae lp" href="https://www.linkedin.com/in/bikramb/" rel="noopener ugc nofollow" target="_blank"> <em class="no"> LinkedIn </em> </a> <em class="no">！:)快乐编码。</em></p></div></div>    
</body>
</html>