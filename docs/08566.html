<html>
<head>
<title>Comparing Transformer Tokenizers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">比较变压器标记器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955?source=collection_archive---------17-----------------------#2019-11-19">https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955?source=collection_archive---------17-----------------------#2019-11-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fbf3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">比较最先进的变形金刚的分词器词汇(伯特，GPT-2，罗伯塔，XLM)</h2></div><p id="48cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果有人使用 Word2vec 或 GloVe 之类的单词嵌入，那么适应 BERT 之类的新的上下文嵌入可能会很困难。在这个故事中，我们将研究其中的一个区别:<em class="lb">子词</em>标记。这个故事的灵感来自于<a class="ae lc" href="http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html" rel="noopener ugc nofollow" target="_blank">探索多语言 BERT 词汇</a>的一个类似帖子。</p><p id="e78f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这个实验，我们将使用 Huggingface transformer 库[1]。他们实施了多种最先进的架构。这个实验的代码<a class="ae lc" href="https://colab.research.google.com/drive/1cPxYzWFZ99e64WaWPCJumdZfkxDjGiEI" rel="noopener ugc nofollow" target="_blank">可以在 Google Colab </a>上获得。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/1aaa4bf56b2ba1a8a941ea31d1bf3880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rOY7YwVpoF71vN1Nms7q0w.jpeg"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Image by <a class="ae lc" href="https://pixabay.com/users/PDPics-44804/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=390055" rel="noopener ugc nofollow" target="_blank">PDPics</a> from <a class="ae lc" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=390055" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><h1 id="0ab3" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">子词标记</h1><p id="9ee5" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">子单词标记(或<em class="lb">单词片段</em>)可用于将单词分割成多个片段，从而减少涵盖每个单词的词汇量[2]。单词片段背后的思想和书面语言一样古老。<em class="lb">字符</em>是最知名的词块，英文单词可以用 26 个字符书写。</p><p id="f6fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，寻找单词块的正确大小还没有被规范化。字符可以用 26 个左右的键来表示每个单词，而原始单词嵌入对每个单词使用不同的键(3M 键用于<a class="ae lc" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">这个 word2vec 字典</a>)【3】。小说《变形金刚》中的记号赋予者使用的词汇量介于两者之间。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mq mr l"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Vocabulary size of the Transformers used in this experiment</figcaption></figure><h2 id="8ca2" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">伯特</h2><p id="d711" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">BERT [4]使用单词片段[2]标记，其中非单词起始片段以<code class="fe ne nf ng nh b">##</code>开始。不同的 BERT 模型有不同的词汇表。例如，无外壳的基本模型有 994 个令牌保留用于可能的微调(<code class="fe ne nf ng nh b">[unused0] </code>到<code class="fe ne nf ng nh b">[unused993]</code>)。cased 模型只有 101 个未使用的标记，因为它需要更多的标记来覆盖大写字母和小写字母。多语言模型只有 100 个未使用的标记，然而，它的总词汇量是无案例的四倍。多语言模型存储更多的特殊字符，以涵盖 104 种语言的单词。</p><p id="eabf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">单词<em class="lb">标记化</em>用<code class="fe ne nf ng nh b">bert-base-cased</code> : <br/> <code class="fe ne nf ng nh b"> [‘token’, ‘##ization’]</code>标记化</p><h2 id="de67" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">罗伯塔 GPT2</h2><p id="5f5c" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">Huggingface 的 GPT2 [5]和 RoBERTa [6]实现使用相同的词汇表，包含 50000 个单词。他们使用 BPE ( <em class="lb">字节对编码</em>【7】)字段和<code class="fe ne nf ng nh b">\u0120</code>作为特殊的信令字符，然而，Huggingface 实现对用户隐藏了它。</p><p id="f645" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> BPE </strong>是一种基于频率的字符拼接算法:它以两字节字符作为标记开始，并基于 n 元语法标记对的频率，包括额外的更长的标记。例如，如果字母<code class="fe ne nf ng nh b">e</code>和<code class="fe ne nf ng nh b">r</code>在语言中频繁出现，一个新的标记<code class="fe ne nf ng nh b">er</code>将被添加到词汇表中。接下来，如果<code class="fe ne nf ng nh b">h</code>和<code class="fe ne nf ng nh b">er</code>经常在一起，<code class="fe ne nf ng nh b">her</code>被添加到词汇表中。该算法继续下去，直到它达到所需的大小。</p><p id="3b3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">单词<em class="lb">标记化</em>用模型<code class="fe ne nf ng nh b">gpt</code> : <br/> <code class="fe ne nf ng nh b"> [‘token’, ‘ization’]</code>标记化</p><h2 id="0caa" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">XLM</h2><p id="969e" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">与之前的词汇相比，XLM [8]在词尾使用了一个后缀:<code class="fe ne nf ng nh b">&lt;/w&gt;</code>表示这是一个单词的结尾。XLM 使用多种语言和模型共享的基于 BPE 的词汇表。</p><p id="0ce3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用模型<code class="fe ne nf ng nh b">xlm-mlm-en-2048</code> : <br/> <code class="fe ne nf ng nh b">[‘to’, ‘ken’, ‘ization&lt;/w&gt;’]</code>将单词标记化</p><h1 id="1290" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">词汇相似度</h1><p id="b7b1" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">下面的实验通过计算不同词汇的交集来度量它们之间的相似性。唯一的预处理步骤是消除上一节提到的特殊字符。第 I 行第 j 列单元格的计算公式:<code class="fe ne nf ng nh b">size(intersection(vocab(i),vocab(j)))/size(vocab(j))</code>。</p><p id="2541" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了正确分析该表，应该注意不同的模型具有不同的词汇量。<code class="fe ne nf ng nh b">bert-base-cased</code> (28996wps)和<code class="fe ne nf ng nh b">bert-base-multilingual-cased </code> (119547wps)的交集只能覆盖多语言词汇的四分之一，即使这两个词汇完全匹配。</p><p id="cd37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Word2Vec [3]和 GloVe [9]是使用单词作为关键字的静态单词嵌入。对于其他信息，该表包括这些嵌入的 30000 个最常见的单词以及总词汇表。我们对 Word2Vec 使用 3M GoogleNews 模型，对<a class="ae lc" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>使用<a class="ae lc" href="http://dumps.wikimedia.org/enwiki/20140102/" rel="noopener ugc nofollow" target="_blank">维基百科 2014 </a> + <a class="ae lc" href="https://catalog.ldc.upenn.edu/LDC2011T07" rel="noopener ugc nofollow" target="_blank"> Gigaword 5 </a>模型。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ni"><img src="../Images/793e776c109c7251c466520f7b888d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2-UaCWQC3xV5Q_aQFmJhBA.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Comparison of the tokens in the different vocabularies</figcaption></figure><p id="38f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这个表中，我们可以看到在 Word2Vec 模型中，<code class="fe ne nf ng nh b">bert-base-cased</code>中 87%的所有标记都被表示为单词。此外，<code class="fe ne nf ng nh b">bert-base-uncased</code>和<code class="fe ne nf ng nh b">xlm-mlm-en-2048</code>与其他相比彼此相对相似，因为它们具有超过 70%的相似性。</p><h1 id="0285" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">标记化差异</h1><p id="775b" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">不同的单词片段带来不同的标记化。以下是一些不同单词分词的例子:</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mq mr l"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">The different tokenization of the words “Marion”, “baptist” and “nuggets”</figcaption></figure><p id="f8ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图显示了 Word2vec 最常见单词的无大小写修改中同等标记化单词的增加。正如我们所看到的，在前 1000 个单词中，有 763 个单词以同样的方式被标记，然而，这个数字在前 4000 个单词中只增加到 1986 个，如果我们看前 10000 个最常见的单词，则增加到 3055 个。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nj"><img src="../Images/cc2a3ae66d88b7a10b605492fdf636a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pI8vGwv-2mVtWOvWpNejw.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Words tokenized the same way in every model</figcaption></figure><h1 id="ebfa" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">摘要</h1><p id="77ce" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在这个故事中，我们看到不同的 Transformer 模型使用不同的标记器和不同的子词标记。正因为如此，在令牌级对模型进行比较是困难的。全球标准化子词模型的可能性是一个公开的问题。即使对于英语来说，标记化的词汇也有很大差异。</p><h1 id="6f21" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><p id="057a" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">[1]t . Wolf，处女作，l .，Sanh，v .，Chaumond，j .，Delangue，c .，Moi，a .，… &amp; Brew，J. (2019)。<a class="ae lc" href="https://arxiv.org/abs/1910.03771" rel="noopener ugc nofollow" target="_blank">拥抱脸的变形金刚:最先进的自然语言处理。</a> <em class="lb"> ArXiv，abs </em>。<em class="lb"> arXiv:1910.03771 </em></p><p id="9e57" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]吴，m .舒斯特，陈，z .乐，Q. V .，m .马切里，w .，… &amp;克林纳，J. (2016)。<a class="ae lc" href="https://arxiv.org/abs/1609.08144" rel="noopener ugc nofollow" target="_blank">谷歌的神经机器翻译系统:弥合人类和机器翻译之间的鸿沟。</a>T11【arXiv 预印本 arXiv:1609.08144 。</p><p id="3ed6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]t .米科洛夫，陈，k .，科拉多，g .，&amp;迪安，J. (2013 年)。<a class="ae lc" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的有效估计。</a>T15】arXiv 预印本 arXiv:1301.3781 。</p><p id="b72a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] Devlin，j .，Chang，M. W .，Lee，k .，&amp; Toutanova，K. (2018 年)。<a class="ae lc" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Bert:用于语言理解的深度双向转换器的预训练。</a>T19】arXiv 预印本 arXiv:1810.04805 。</p><p id="5ce9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5]a .、吴 j .、蔡尔德 r .、栾 d .、阿莫代伊 d .、&amp;苏茨基弗 I. (2019)。<a class="ae lc" href="https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是无人监督的多任务学习器。</a> <em class="lb"> OpenAI 博客</em>，<em class="lb"> 1 </em> (8)。</p><p id="7e01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[6]刘，y .，奥特，m .，戈亚尔，n .，杜，j .，乔希，m .，陈，d .，… &amp;斯托扬诺夫，V. (2019)。Roberta: <a class="ae lc" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">一种稳健优化的 bert 预训练方法。</a> <em class="lb"> arXiv 预印本 arXiv:1907.11692 </em>。</p><p id="47ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[7]森里奇(r .)、哈多(b .)、伯奇(a .)(2015 年)。<a class="ae lc" href="https://arxiv.org/abs/1508.07909" rel="noopener ugc nofollow" target="_blank">带子词单元的生僻字神经机器翻译</a>。<em class="lb"> arXiv 预印本 arXiv:1508.07909 </em>。</p><p id="ff43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[8]康奈尔大学和兰普尔大学(2019 年)。<a class="ae lc" href="http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining" rel="noopener ugc nofollow" target="_blank">跨语言语言模型预训练。<em class="lb">神经信息处理系统的进展</em>(第 7057–7067 页)。</a></p><p id="3444" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[9]j .潘宁顿、r .索彻和 c .曼宁(2014 年 10 月)。<a class="ae lc" href="https://www.aclweb.org/anthology/D14-1162" rel="noopener ugc nofollow" target="_blank"> Glove:单词表示的全局向量。</a>见<em class="lb">2014 年自然语言处理经验方法会议论文集</em>(第 1532-1543 页)。</p></div></div>    
</body>
</html>