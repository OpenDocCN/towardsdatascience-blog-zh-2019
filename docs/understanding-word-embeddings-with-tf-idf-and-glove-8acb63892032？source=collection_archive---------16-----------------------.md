# 用 TF-IDF 和 GloVe 理解单词嵌入

> 原文：<https://towardsdatascience.com/understanding-word-embeddings-with-tf-idf-and-glove-8acb63892032?source=collection_archive---------16----------------------->

## 它们是如何工作的，你能用它们做什么？

![](img/75d4c2de3885c2735954df265bd1d4d7.png)

单词嵌入是自然语言处理中最流行的技术之一。因此，理解它们是如何工作的以及它们可以用来做什么，对于任何愿意投身自然语言处理领域的数据爱好者来说都是至关重要的。

为了向你展示它们有多重要，请考虑以下情况:我们都使用过谷歌搜索来找到数以千计或更多的与我们的查询相匹配的结果。但是你有没有想过计算机是如何理解你指的是苹果公司而不是水果？同样，如果你输入“巴拉克·奥巴马的妻子”，它如何返回“米歇尔·奥巴马”？

**上面的答案是:单词嵌入**。这种技术允许以一种捕捉单词的含义、语义关系和使用它们的上下文的方式来表示单词。

> “从一个人和什么样的人交往，你就可以知道他是什么样的人。”——弗斯。

# **词语嵌入清晰解释**

**单词嵌入是将单词转换成实数向量的过程。**

我们为什么需要它？嗯，机器学习中的大多数算法都无法处理原始形式的字符串或纯文本。相反，它们需要数字作为输入才能发挥作用。通过将单词转换为向量，单词嵌入因此允许我们处理大量的文本数据，并使它们适合机器学习算法。

*它是如何工作的？*将单词转换成数字的最基本方法之一是通过一键编码法。考虑这个句子:“我正在学习单词嵌入是如何工作的”。这个句子中的词是“学习”、“嵌入”等。由此，我们可以创建一个字典，它是句子中所有独特单词的列表。在这种情况下:["我"，"我"，"学习"，"如何"，"词"，"嵌入"，"工作"]。一个字的独热编码矢量表示可以以这样的方式编码，1 代表该字存在的位置，而 0 代表其它任何位置。例如，“学习”的向量表示如下:[0，0，1，0，0，0，0]。因此，每个单词都有其独特的维度。

此外，它们被映射到一个**向量空间** *。为什么？*其背后的思想是，上下文相近的词占据相近的空间位置。换句话说，在向量空间中，在相似上下文中使用的单词彼此靠近，而不靠近的单词彼此远离。这就是这一切的美妙之处！

在本文中，我将展示将单词转换成数字的两种不同方法:使用 TF-IDF 转换和 GloVe。虽然 TF-IDF 依赖于**稀疏向量**表示，但 GloVe 属于**密集向量**表示。

# 稀疏向量:TF-IDF

TF-IDF 遵循与上述独热码编码向量相似的逻辑。然而，它不是只计算一个单词在单个文档中的出现次数，而是计算整个语料库的出现次数。这允许我们检测一个单词对语料库中的文档有多重要。

*这是什么意思？*嗯，像“the”或“a”这样的常用词在几乎每个文档中都会非常频繁地出现。然而，其他单词可能只在一个或两个文档中频繁出现，而这些单词可能更能代表它们所在的文档。

TF-IDF 的目标是揭示这一点:降低在几乎每个文档中出现的常用词(例如“the”或“a”)的权重，并赋予那些只在少数文档中出现的词更多的重要性。

详细地说，TF IDF 由两部分组成: *TF* ，它是单词的词频，即该单词在文档中出现的次数；以及 *IDF* ，它是逆文档频率，即对只在少数文档中出现的单词给予较高权重的权重分量。

# 密集向量:手套

密集向量分为两类:矩阵分解和神经嵌入。GloVe 属于后一类，旁边是另一种流行的神经方法 Word2vec。

简而言之，GloVe 是一种无监督的学习算法，它强调单词-单词共现对提取意义的重要性，而不是其他技术，如 skip-gram 或单词包。其背后的想法是，某个词通常比另一个词更频繁地与一个词同时出现。例如，单词*冰*更有可能与单词*水*一起出现。

手套在 R 中非常容易使用，带有 [text2vec](http://text2vec.org/glove.html) 包。

# 应用中的单词嵌入:我能用它们展示什么？

理解单词嵌入是关键，但掌握如何使用它们也同样重要。单词嵌入如此重要的原因不仅在于它们在数据科学中的应用，还延伸到了政治、通信、营销或决策领域。

**1/寻找两个单词之间的相似度**

一旦将单词转换成数字，就可以使用相似性度量来查找单词之间的相似程度。

一个有用的度量是**余弦相似度**，它测量两个向量之间角度的余弦。重要的是要理解，它测量的是方向而不是幅度，即相似的矢量将具有相似的矢量方向。更详细地说，这意味着具有相同方向的两个向量将具有 1 的余弦相似度，相对于彼此成 90°方向的两个向量将具有 0 的相似度，并且直径上相对的两个向量具有-1 的相似度。这完全与它们的大小无关。

找到单词之间的相似性可以带来强大的洞察力。**在市场营销中，这可以帮助营销人员了解消费者将特定产品与哪些词语联系在一起。**因此，这有助于通过有针对性的搜索查询开展更高效的广告活动。以类似的方式，这也可以通知传播专家或政治家在制定针对受众的活动。

**2/比较一个词在不同语料库中的使用**

与上述相关，人们还可以关注特定单词的使用，并使用余弦相似性的度量来理解该单词在不同的语料库中是如何不同地使用的。例如，你可以比较“气候”一词在各种联合国演讲中的用法，或者“移民”一词在共和党和民主党宣言中的上下文。你可以从中提取政党甚至政策的差异。

根据你想要分析的文本，由单词嵌入给出的研究可能性是无限的。

[](https://medium.com/data-social/natural-language-processing-word-embeddings-e0b2edc773d2) [## 自然语言处理如何帮助你通过词语理解党派差异

### 用词汇嵌入法分析民主党和共和党政纲的异同

medium.com](https://medium.com/data-social/natural-language-processing-word-embeddings-e0b2edc773d2) 

**3/分析像女人+国王-男人=王后**这样令人惊奇的事情

单词类比是单词嵌入真正有趣的部分！他们允许你以“a 对 b”和“x 对 y”的形式进行扣除。

举个例子:国王——男人+女人=王后。换句话说，将向量 king 和 woman 相加，同时减去 man，得到与 queen 相关的向量。简单地说，这意味着:国王对于男人就像女王对于女人一样。

作为人类，我们知道国王指的是男性，而女王指的是女性。单词嵌入让计算机也能理解这一点，它利用了 king 和 man 的向量表示之间的差异。如果一个人试图把女性向量投射到同一个方向，他就会得到 queen 这个词。女王是国王的女性的信息从未被直接提供给模型，但是模型能够通过单词嵌入来捕捉这种关系。

再比如:巴黎—法国+德国=柏林。换句话说，巴黎对于法国就像柏林对于德国一样！

在这里， [text2vec](http://text2vec.org/glove.html) 是 R 中一个易于使用的包，用于使用上述余弦相似性度量从 GloVe 算法中执行这些单词类比。

# 始终小心道德方面的考虑！

尽管单词嵌入非常有用，并且应用广泛，但是应该仔细考虑它们的实现。

不幸的是，语言是一种强有力的手段，通过它，种族、性别歧视和刻板偏见得以复制。因此，考虑这些偏见对基于处理人类语言的自动化任务的影响是至关重要的。

*这怎么会成为问题，为什么会成为问题？*

考虑在工作招聘流程或翻译服务等环境中实现基于单词嵌入的机器学习算法。在这些情况下，陈规定型观念会影响并实际上隐含对某一群体的歧视。在这种情况下给出的一个流行的例子是男人:程序员=女人之间的关系，这将产生结果家庭主妇。嵌入模式将看到程序员与男性比女性更亲密，因为我们自己对这份工作的社会认知反映在我们使用的语言中。

另一个例子是 cv 扫描的自动化。让我们假设公司决定在一个大数据集上训练单词嵌入，比如维基百科。在一封激励信中找到积极的形容词，如狡猾、聪明和聪明的机会很高，但我们也发现，在预先训练的嵌入空间中，这些术语更接近男性而不是女性。因此，这种性别偏见将在自动化任务中重现。

性别不平等、种族歧视和其他刻板偏见在我们的社会和语言使用中根深蒂固。因此，在这种语言上应用机器学习算法有传播和放大所有这些偏见的风险。因此，算法从来都不是“中性”的，因为我们的语言本身就不是中性的。

因此:永远要小心你的模型的伦理含义！

# **结论**

在我们与计算机的日常互动中，词语嵌入无处不在。他们可以提供超越数据科学应用的见解，触及营销、通信、政治和政策制定领域。识别单词的相似性和相似性是其强大应用的例子。尽管如此，它们的应用从来都不是中立的，就像任何算法一样，一定要考虑它们的伦理含义！

我定期撰写关于数据科学和自然语言处理的文章。关注我的 [*Twitter*](https://twitter.com/celine_vdr) *或*[*Medium*](https://medium.com/@celine.vdr)*查看更多类似的文章或简单地更新下一篇文章！*