<html>
<head>
<title>Build and Compare 3 Models — NLP Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建并比较 3 个模型— NLP 预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-and-compare-3-models-nlp-sentiment-prediction-67320979de61?source=collection_archive---------8-----------------------#2019-11-12">https://towardsdatascience.com/build-and-compare-3-models-nlp-sentiment-prediction-67320979de61?source=collection_archive---------8-----------------------#2019-11-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6f966b695c24853c86163412c6eddd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*396DIwqAkvEgYHb-WBggCQ.png"/></div></div></figure><h2 id="02a6" class="jc jd je bd b dl jf jg jh ji jj jk dk jl translated" aria-label="kicker paragraph">从零开始用 3 种算法预测情绪-初学者友好。</h2><div class=""/><div class=""><h2 id="690d" class="pw-subtitle-paragraph kk jn je bd b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dk translated">Python 上的自然语言处理(Jupyter)！</h2></div><p id="d8fd" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">创建这个项目是为了学习和理解各种分类算法在自然语言处理模型中是如何工作的。自然语言处理，我现在称之为 NLP，是机器学习的一个分支，专注于使计算机能够解释和处理语音和文本形式的人类语言。</p><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ly"><img src="../Images/34f35ba91ad7de9b22d0832837829f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FdrbOcM7AtWD00g9"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Photo by <a class="ae mh" href="https://unsplash.com/@impatrickt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Patrick Tomasso</a> on <a class="ae mh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e33f" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">在这条流水线上，我经历了以下几个步骤:</strong></p><ol class=""><li id="297c" class="mi mj je le b lf lg li lj ll mk lp ml lt mm lx mn mo mp mq bi translated">导入所需的包和库</li><li id="096f" class="mi mj je le b lf mr li ms ll mt lp mu lt mv lx mn mo mp mq bi translated">导入数据集</li><li id="c701" class="mi mj je le b lf mr li ms ll mt lp mu lt mv lx mn mo mp mq bi translated">在数据集中的文本可以被计算机分析之前对其进行处理</li><li id="4338" class="mi mj je le b lf mr li ms ll mt lp mu lt mv lx mn mo mp mq bi translated">创建一个单词袋模型</li><li id="eb4e" class="mi mj je le b lf mr li ms ll mt lp mu lt mv lx mn mo mp mq bi translated">将数据集分成训练集和测试集</li><li id="f481" class="mi mj je le b lf mr li ms ll mt lp mu lt mv lx mn mo mp mq bi translated">朴素贝叶斯算法</li><li id="4c7a" class="mi mj je le b lf mr li ms ll mt lp mu lt mv lx mn mo mp mq bi translated">决策树算法</li><li id="8595" class="mi mj je le b lf mr li ms ll mt lp mu lt mv lx mn mo mp mq bi translated">随机森林算法</li><li id="213a" class="mi mj je le b lf mr li ms ll mt lp mu lt mv lx mn mo mp mq bi translated">比较准确度、精确度、召回率和 F1 分数</li></ol><h1 id="fce8" class="mw mx je bd my mz na nb nc nd ne nf ng kt nh ku ni kw nj kx nk kz nl la nm nn bi translated">||二||问题</h1><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/d9be6f3d9030ba47c8e31c833ae38a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4JsQdRT4U-fLrsiu"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Photo by <a class="ae mh" href="https://unsplash.com/@zulmaury?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Zulmaury Saavedra</a> on <a class="ae mh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ed5c" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">对于这个项目，我将使用来自 Kaggle 的数据集，其中包含不同用户对一家披萨店的 1000 条评论。<a class="ae mh" href="https://www.kaggle.com/yelp-dataset/yelp-dataset/version/6" rel="noopener ugc nofollow" target="_blank"><strong class="le jo">|链接到数据集| </strong> </a></p><p id="cb61" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">人类可以阅读评论，并判断它是积极的还是消极的。如果我们可以创建一个模型来将他们分为积极的或消极的呢？做这件事的最好方法是什么？</p><p id="53ce" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">首先说一下流程。我们首先对数据进行预处理，删除对我们的预测没有帮助的不必要的词。然后，我们采用词干形式的重要单词<em class="np">(例如 lov 是 loved、loving 或 lovely 的词干)</em>。然后，我们训练机器根据词干来学习哪些评论是正面的。之后，我们使用类似的信息测试数据，看看我们的机器能够多准确地预测评论是正面还是负面(1 或 0)。</p><h1 id="eaef" class="mw mx je bd my mz na nb nc nd ne nf ng kt nh ku ni kw nj kx nk kz nl la nm nn bi translated">|| III || <strong class="ak">导入基本库</strong></h1><p id="64e1" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">在这里，我们导入这个模型工作所需的所有库。在开始之前，请确保安装了所有的依赖项。我们将主要与<code class="fe nv nw nx ny b">pandas, numpy, re, nltk, matplotlib, and sci-kit learn.</code>合作</p><p id="f4b4" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">确保在命令行上运行上面提到的所有库。</p><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="beb1" class="od mx je ny b gy oe of l og oh">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import re<br/>import nltk<br/>from nltk.corpus import stopwords<br/>from nltk.stem.porter import PorterStemmer<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import RandomForestClassifier</span></pre></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="870f" class="mw mx je bd my mz op nb nc nd oq nf ng kt or ku ni kw os kx nk kz ot la nm nn bi translated"><strong class="ak"> || IV ||导入数据集</strong></h1><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="d763" class="od mx je ny b gy oe of l og oh">dataset = pd.read_csv(‘Restaurant_Reviews.tsv’, delimiter = ‘\t’, quoting = 3)</span></pre><p id="d6b7" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在上面的代码中，我使用的是. tsv 文件，而不是. csv 文件。当我们分解缩略词时，区别就更容易分辨了。. tsv(制表符分隔的值)在文本文件中由空格分隔，而. csv(逗号分隔的值)使用逗号分隔不同的值。</p><p id="fe10" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><code class="fe nv nw nx ny b">pd.read_csv</code>可用于两者，但为了指定 tsv 文件，我在分隔符中添加了' \t ',以告诉机器值由制表符而不是逗号分隔。引用被设置为 3，因为我们的评论包含一些双引号，如果我们不添加这一行，机器将无法将它们解释为常规文本。<br/>下面我们可以看到我们数据集的前 10 个评论和结果:正面(1)或负面(0)</p><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="a901" class="od mx je ny b gy oe of l og oh">dataset.head(10)</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/b3e10769c3dfef38f620204f57747420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KH9OoISdwKdJi64lv7V90g.png"/></div></div></figure></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="da57" class="mw mx je bd my mz op nb nc nd oq nf ng kt or ku ni kw os kx nk kz ot la nm nn bi translated">|| V || <strong class="ak">文本预处理</strong></h1><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="30a3" class="od mx je ny b gy oe of l og oh">nltk.download(‘stopwords’)<br/>corpus = []</span><span id="4edb" class="od mx je ny b gy ov of l og oh">for i in range(0, 1000):<br/>review = re.sub(‘[^a-zA-Z]’, ‘ ‘, dataset[‘Review’][i])<br/>review = review.lower()<br/>review = review.split()<br/>ps = PorterStemmer()<br/>review = [ps.stem(word) for word in review if not word in set(stopwords.words(‘english’))]<br/>review = ‘ ‘.join(review)<br/>corpus.append(review)</span></pre><p id="c1e0" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">上面的代码显示了这个模型中最重要的一步。我从导入 Regex 作为<code class="fe nv nw nx ny b">re</code>开始。这个类允许我们匹配和搜索字符串对象。我使用 re 的子功能来允许机器包含我们需要的数据元素，即大写和小写的字母 A-Z。</p><p id="5c6c" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我还导入了代表自然语言工具包的<code class="fe nv nw nx ny b">nltk</code>。从<code class="fe nv nw nx ny b">nltk</code>中，我导入了两个类:<code class="fe nv nw nx ny b">stopwords</code>类和<code class="fe nv nw nx ny b">PorterStemmer</code>类。</p><p id="344b" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><code class="fe nv nw nx ny b">stopwords</code>允许我们删除对我们的模型没有帮助的单词(如“the”、“an”、“this”等)。stopwords 类已经包含了这些单词，所以我不必手动输入它们。我使用了一个 for 循环来告诉机器，如果这个单词不在 stopwords 类中，我们需要把它取出来。</p><p id="b8be" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><code class="fe nv nw nx ny b">PorterStemmer</code>允许我们提取一个单词的词干，并将其归类为相似单词的常见预测值。例如，在第一个评论中，“Lov”是单词“loving”或“loved”的词干，这两个单词本质上都转化为积极的评论，因此允许我们的模型有更少的单词。</p><p id="101d" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">最后，为了将预处理步骤应用于我们数据集中的所有 1000 条评论，我在文本处理步骤之前添加了另一个 for 循环。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="0b9c" class="mw mx je bd my mz op nb nc nd oq nf ng kt or ku ni kw os kx nk kz ot la nm nn bi translated">|| VI || <strong class="ak">创建单词袋模型</strong></h1><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="6233" class="od mx je ny b gy oe of l og oh">cv = CountVectorizer(max_features = 1500)</span><span id="ae8e" class="od mx je ny b gy ov of l og oh">X = cv.fit_transform(corpus).toarray()<br/>y = dataset.iloc[:, 1].values</span></pre><p id="6805" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">单词袋模型允许我们从文本数据中提取特征，在本例中，我们从每个观察结果中提取所有单词，并将它们集中在一个“袋”中，通过不计算重复项来减少冗余。我是通过从<code class="fe nv nw nx ny b">sklearn</code>导入<code class="fe nv nw nx ny b">CountVectorizer</code>类来做到这一点的。</p><p id="5233" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">每个单词以某种方式形成自己的列，因为有这么多单词，我们可以有大量的列。然而，我使用<code class="fe nv nw nx ny b">CountVectorizer</code>类的<code class="fe nv nw nx ny b">max_features</code>参数指定了最大列数。</p><p id="811a" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">然后，我所要做的就是将单词列与 X(输入)变量相匹配，并将 y(输出)变量指定为数据集中的第二列，这将根据评论是正面还是负面分别给出 1 或 0。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="5c5f" class="mw mx je bd my mz op nb nc nd oq nf ng kt or ku ni kw os kx nk kz ot la nm nn bi translated">|| VII || <strong class="ak">分成训练集和测试集</strong></h1><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="0ddc" class="od mx je ny b gy oe of l og oh">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span></pre><p id="a445" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">然后，我必须将数据集分为训练集和测试集，并使用 0.2 的测试大小，这样我们就有 800 个值来训练数据集，200 个值来测试数据集。</p><p id="bfbb" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">下面的代码向我们展示了训练和测试集的样子</p><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="2b48" class="od mx je ny b gy oe of l og oh">X_train[0, 0:10] <em class="np">#First 10 rows of the first column of X_train.</em></span><span id="a2d8" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)</span><span id="bdaa" class="od mx je ny b gy ov of l og oh">X_test[0, 0:10] <em class="np">#First 10 rows of the first column of X_test.</em></span><span id="8770" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)</span><span id="6053" class="od mx je ny b gy ov of l og oh">y_train[:10] <em class="np">#First 10 values of the first column of y_train.</em></span><span id="59cb" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: array([1, 1, 1, 0, 1, 0, 1, 0, 0, 0])</span><span id="01ae" class="od mx je ny b gy ov of l og oh">y_test[:10] <em class="np">#First 10 values of the first column of y_test.</em></span><span id="7d7f" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: array([0, 0, 0, 0, 0, 0, 1, 0, 0, 1])</span></pre><p id="89f5" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">现在我已经完成了所有的预处理步骤，我将开始对 model 应用一些分类算法，以帮助它预测评论。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="9536" class="mw mx je bd my mz op nb nc nd oq nf ng kt or ku ni kw os kx nk kz ot la nm nn bi translated">|| VIII || <strong class="ak">朴素贝叶斯模型</strong></h1><p id="ce4d" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">我使用的第一个模型是朴素贝叶斯模型。在机器学习中，朴素贝叶斯分类器是一系列简单的“概率分类器”，基于应用贝叶斯定理，在特征之间具有强(朴素)独立性假设。</p><p id="79e5" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">给定类变量，所有朴素贝叶斯分类器都假设特定特征的值独立于任何其他特征的值。在我们的模型中，朴素贝叶斯算法根据输出集，查看评论的特定关键字来描述它是正面还是负面的。</p><p id="9fbd" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在下面的代码中，我导入了 GaussianNB 类，它假设我们的数据是正态分布的(具有高斯钟形曲线)。</p><h2 id="5323" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> VIII~i ||将朴素贝叶斯拟合到训练集</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="d5d2" class="od mx je ny b gy oe of l og oh">classifier = GaussianNB()<br/>classifier.fit(X_train, y_train)</span><span id="42f7" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: GaussianNB(priors=None, var_smoothing=1e-09)</span></pre><h2 id="9612" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> VIII~ii ||预测测试集结果</strong></h2><p id="3dbd" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">我创建了<code class="fe nv nw nx ny b">y_pred_NB</code>，这是我们的模型的预测将被存储的地方。下面的输出向我们展示了预测矩阵的样子。它是一堆 1 和 0，就像我们的<code class="fe nv nw nx ny b">y_train</code>数据集。这些是我们的模型做出的预测。<code class="fe nv nw nx ny b">y_pred_NB</code>可以与<code class="fe nv nw nx ny b">y_test</code>进行比较，并确定我们的模型有多精确。</p><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="5593" class="od mx je ny b gy oe of l og oh">y_pred_NB = classifier.predict(X_test)<br/>y_pred_NB</span><span id="0c7a" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1])</span></pre><h2 id="9d82" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> VIII~iii ||制作混淆矩阵</strong></h2><p id="7db0" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">朴素贝叶斯的混淆矩阵在左上角显示我们的真阴性，在右上角显示假阴性，在右下角显示真阳性，在左下角显示假阳性。</p><p id="ed0d" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">下面，我用缩写对每个数字进行了编码，T 代表真，F 代表假，N 代表负，P 代表正。</p><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="259c" class="od mx je ny b gy oe of l og oh">cm_NB = confusion_matrix(y_test, y_pred_NB) <br/>cm_NB</span><span id="0989" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: array([[55, 42],[12, 91]])</span><span id="5895" class="od mx je ny b gy ov of l og oh">TP_NB = 91 <em class="np">#True Positives (Naive Bayes)</em></span><span id="f91d" class="od mx je ny b gy ov of l og oh">TN_NB = 55 <em class="np">#True Negatives (Naive Bayes)</em></span><span id="7fee" class="od mx je ny b gy ov of l og oh">FP_NB = 12 <em class="np">#False Positives (Naive Bayes)</em></span><span id="de3d" class="od mx je ny b gy ov of l og oh">FN_NB = 42 <em class="np">#False Negatives (Naive Bayes)</em></span></pre><p id="f1e9" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">下面，我将使用真/假阳性和阴性来计算准确度、精确度、召回率和 F1 分数。</p><h2 id="8cba" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> VIII~iv ||朴素贝叶斯算法的精度</strong></h2><p id="247a" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">准确性顾名思义。它通过将真实预测相加并除以预测总数来衡量准确性。</p><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="2326" class="od mx je ny b gy oe of l og oh">Accuracy_NB = (TP_NB + TN_NB) / (TP_NB + TN_NB + FP_NB + FN_NB) Accuracy_NB  </span><span id="7d4f" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.73</span></pre><h2 id="c523" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated">VIII~v || <strong class="ak">朴素贝叶斯算法的精度</strong></h2><p id="ba7b" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">精度是指两个或多个测量值之间的接近程度。它是通过将真阳性除以总阳性来计算的</p><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="7261" class="od mx je ny b gy oe of l og oh">Precision_NB = TP_NB / (TP_NB + FP_NB)</span><span id="c923" class="od mx je ny b gy ov of l og oh">Precision_NB</span><span id="5872" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.883495145631068</span></pre><h2 id="7516" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> VIII~vi ||回忆朴素贝叶斯算法</strong></h2><p id="ddaa" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">回忆是正确预测的正面观察与实际类中所有观察的比率。将真阳性除以真阳性和假阴性之和。</p><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="3e0a" class="od mx je ny b gy oe of l og oh">Recall_NB = TP_NB / (TP_NB + FN_NB)</span><span id="88de" class="od mx je ny b gy ov of l og oh">Recall_NB</span><span id="6477" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.6842105263157895</span></pre><h2 id="dfb7" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> VIII~vii ||朴素贝叶斯算法的 F1 得分</strong></h2><p id="d1b3" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">F1 分数是精确度和召回率的加权平均值。如果我们需要在精确度和召回率之间寻求平衡，并且存在不均匀的类别分布，F1 分数可能是一个更好的衡量标准。它的计算方法是将精度和召回率相乘，将结果除以精度和召回率之和，然后将最终结果乘以 2。</p><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="9d32" class="od mx je ny b gy oe of l og oh">F1_Score_NB = 2 * Precision_NB * Recall_NB / (Precision_NB + Recall_NB) F1_Score_NB</span><span id="21c2" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.7711864406779663</span></pre></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="9bca" class="mw mx je bd my mz op nb nc nd oq nf ng kt or ku ni kw os kx nk kz ot la nm nn bi translated">|| IX || <strong class="ak">决策树</strong></h1><p id="04d4" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">我使用的下一个算法是决策树。决策树允许您开发分类系统，该系统基于一组决策规则对未来的观察结果进行预测或分类。</p><h2 id="97d3" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated">IX~i || <strong class="ak">将决策树分类拟合到训练集</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="546a" class="od mx je ny b gy oe of l og oh">classifier = DecisionTreeClassifier(criterion = ‘entropy’, random_state = 0)</span><span id="4051" class="od mx je ny b gy ov of l og oh">classifier.fit(X_train, y_train)</span><span id="28d9" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output:</strong> DecisionTreeClassifier(class_weight=None, criterion=’entropy’,max_depth=None,max_features=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0, presort=False, random_state=0,splitter=’best’)</span></pre><h2 id="b91e" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> IX~ii ||预测测试集结果</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="1176" class="od mx je ny b gy oe of l og oh">In [59]: y_pred_DT = classifier.predict(X_test)</span><span id="a067" class="od mx je ny b gy ov of l og oh">y_pred_DT</span><span id="8632" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: array([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,0, 0])</span></pre><h2 id="4577" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> IX~iii ||制作混淆矩阵</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="1f42" class="od mx je ny b gy oe of l og oh">cm_DT = confusion_matrix(y_test, y_pred_DT) cm_DT</span><span id="78c7" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: array([[74, 23],[35, 68]])</span><span id="86f1" class="od mx je ny b gy ov of l og oh">TP_DT = 68 <em class="np">#True Positives (Decision Tree)</em></span><span id="23fd" class="od mx je ny b gy ov of l og oh">TN_DT = 74 <em class="np">#True Negatives (Decision Tree)</em></span><span id="59e0" class="od mx je ny b gy ov of l og oh">FP_DT = 35 <em class="np">#False Positives (Decision Tree)</em></span><span id="267c" class="od mx je ny b gy ov of l og oh">FN_DT = 23 <em class="np">#False Negatives (Decision Tree)</em></span></pre><h2 id="d14c" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> IX~iv ||决策树算法的准确性</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="b693" class="od mx je ny b gy oe of l og oh">Accuracy_DT = (TP_DT + TN_DT) / (TP_DT + TN_DT + FP_DT + FN_DT)</span><span id="71a1" class="od mx je ny b gy ov of l og oh">Accuracy_DT</span><span id="ff02" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.71</span></pre><h2 id="ad49" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> IX~v ||决策树算法的精度</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="a441" class="od mx je ny b gy oe of l og oh">Precision_DT = TP_DT / (TP_DT + FP_DT)</span><span id="2c35" class="od mx je ny b gy ov of l og oh">Precision_DT</span><span id="83b9" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.6601941747572816</span></pre><h2 id="9dd2" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> IX~vi ||召回决策树算法</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="1dca" class="od mx je ny b gy oe of l og oh">Recall_DT = TP_DT / (TP_DT + FN_DT)</span><span id="49f3" class="od mx je ny b gy ov of l og oh">Recall_DT</span><span id="c226" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.7472527472527473</span></pre><h2 id="a4ed" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> IX~vii ||决策树算法的 F1 得分</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="c23d" class="od mx je ny b gy oe of l og oh">F1_Score_DT = 2 * Precision_DT * Recall_DT / (Precision_DT + Recall_DT)</span><span id="1948" class="od mx je ny b gy ov of l og oh">F1_Score_DT</span><span id="1ec2" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.7010309278350515</span></pre></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="5962" class="mw mx je bd my mz op nb nc nd oq nf ng kt or ku ni kw os kx nk kz ot la nm nn bi translated"><strong class="ak"> || X ||随机森林</strong></h1><p id="5294" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">最后，我使用了随机森林算法，这只是一些决策树的组合。在我的例子中，我选择使用 300 棵树，但是我可以根据我想要的模型精度来改变这个数字。</p><h2 id="c688" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak">X～I | |将随机森林分类拟合到训练集</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="e34e" class="od mx je ny b gy oe of l og oh">classifier = RandomForestClassifier(n_estimators = 300, criterion = ‘entropy’, random_state = 0)</span><span id="7e8c" class="od mx je ny b gy ov of l og oh">classifier.fit(X_train, y_train)</span><span id="a30b" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: RandomForestClassifier(bootstrap=True, class_weight=None, criterion=’entropy’,max_depth=None,max_features=’auto’,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,oob_score=False, random_state=0, verbose=0, warm_start=False)</span></pre><h2 id="51d6" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> X~ii ||预测测试集结果</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="ae28" class="od mx je ny b gy oe of l og oh">y_pred_RF = classifier.predict(X_test)</span><span id="6a37" class="od mx je ny b gy ov of l og oh">y_pred_RF</span><span id="072c" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: array([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0])</span></pre><h2 id="7ac3" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated"><strong class="ak"> X~iii ||制作混淆矩阵</strong></h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="09fc" class="od mx je ny b gy oe of l og oh">cm_RF = confusion_matrix(y_test, y_pred_RF) cm_RF</span><span id="3f49" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: array([[87, 10],[47, 56]])</span><span id="f0b3" class="od mx je ny b gy ov of l og oh"><em class="np"># Calculating True/False Positives/Negatives<br/></em>TP_RF = 56 <em class="np">#True Positives (Random Forest)<br/></em>TN_RF = 87 <em class="np">#True Negatives (Random Forest)<br/></em>FP_RF = 47 <em class="np">#False Positives (Random Forest)<br/></em>FN_RF = 10 <em class="np">#False Negatives (Random Forest)</em></span></pre><h2 id="6ea6" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated">X~iv ||随机森林的精度</h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="e90a" class="od mx je ny b gy oe of l og oh">Accuracy_RF = (TP_RF + TN_RF) / (TP_RF + TN_RF + FP_RF + FN_RF)</span><span id="cad2" class="od mx je ny b gy ov of l og oh">Accuracy_RF</span><span id="b7e4" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.715</span></pre><h2 id="172a" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated">x～v | |随机森林的精度</h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="f0a8" class="od mx je ny b gy oe of l og oh">Precision_RF = TP_RF / (TP_RF + FP_RF)</span><span id="b8bc" class="od mx je ny b gy ov of l og oh">Precision_RF</span><span id="b844" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.5436893203883495</span></pre><h2 id="4063" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated">X~vi ||随机森林的回忆</h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="d706" class="od mx je ny b gy oe of l og oh">Recall_RF = TP_RF / (TP_RF + FN_RF)</span><span id="24f2" class="od mx je ny b gy ov of l og oh">Recall_RF</span><span id="58fa" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.8484848484848485</span></pre><h2 id="b759" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated">X~vii ||随机森林 F1 得分</h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="0d8d" class="od mx je ny b gy oe of l og oh">F1_Score_RF = 2 * Precision_RF * Recall_RF / (Precision_RF + Recall_RF)</span><span id="1fb1" class="od mx je ny b gy ov of l og oh">F1_Score_RF</span><span id="9d0f" class="od mx je ny b gy ov of l og oh"><strong class="ny jo">Output</strong>: 0.6627218934911243</span></pre><h1 id="c230" class="mw mx je bd my mz na nb nc nd ne nf ng kt nh ku ni kw nj kx nk kz nl la nm nn bi translated">XI ||对比模型</h1><p id="9028" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">在最后一部分，我将比较我使用的每种算法的准确度、精确度、召回率和 F1 值。我将把它们绘制在条形图上，以图形方式展示不同型号之间的比较。</p><h2 id="9ee6" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated">Xi～I | |比较模型精度</h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="33d4" class="od mx je ny b gy oe of l og oh">Accuracy = [Accuracy_RF, Accuracy_DT, Accuracy_NB]<br/>Methods = [‘Random_Forest’, ‘Decision_Trees’, ‘Naive_Bayes’]<br/>Accuracy_pos = np.arange(len(Methods))</span><span id="6391" class="od mx je ny b gy ov of l og oh">plt.bar(Accuracy_pos, Accuracy)<br/>plt.xticks(Accuracy_pos, Methods)<br/>plt.title(‘comparing the accuracy of each model’)</span><span id="354d" class="od mx je ny b gy ov of l og oh">plt.show()</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/3f3c8c1a47c7dc0d2f665146ec008ba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*A_ycKgS4KaSx9kQtfM_QcQ.png"/></div></figure><p id="7dd9" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">正如我们在上面的柱状图中看到的，朴素贝叶斯在所有算法中具有最高的准确性，有 73%的正确预测。决策树和随机森林算法也很接近，准确率分别为 71%和 71.5%。</p><h2 id="89b7" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated">XI~ii ||比较模型精度</h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="f03f" class="od mx je ny b gy oe of l og oh">Precision = [Precision_RF, Precision_DT, Precision_NB]<br/>Precision_pos = np.arange(len(Methods))</span><span id="be9e" class="od mx je ny b gy ov of l og oh">plt.bar(Precision_pos, Precision)<br/>plt.xticks(Precision_pos, Methods)<br/>plt.title(‘comparing the precision of each model’)</span><span id="e810" class="od mx je ny b gy ov of l og oh">plt.show()</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/cda3c73dcbe9fbca32dbe73e5bcf70ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*s_Ws_FIlI37QNmLLmgEP-Q.png"/></div></figure><p id="a1d4" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">朴素贝叶斯是最精确的模型，精度为 88.35%，而决策树的精度为 66%。随机森林的准确率最低，约为 54.4%。</p><h2 id="982b" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated">XI~iii ||对比车型召回</h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="bf18" class="od mx je ny b gy oe of l og oh">Recall = [Recall_RF, Recall_DT, Recall_NB]<br/>Recall_pos = np.arange(len(Methods))</span><span id="ae4d" class="od mx je ny b gy ov of l og oh">plt.bar(Recall_pos, Recall)<br/>plt.xticks(Recall_pos, Methods)<br/>plt.title(‘comparing the recall of each model’)</span><span id="35ca" class="od mx je ny b gy ov of l og oh">plt.show()</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/9ba3058176587aa48c794c8a12e3ce85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*4O7IkZh5kYg5nOEc0WXTjA.png"/></div></figure><p id="dceb" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">随机森林的召回率最高，约为 84.8%，而决策树的召回率为 74.7%。朴素贝叶斯的召回率最低，为 68.4%</p><h2 id="0a41" class="od mx je bd my ow ox dn nc oy oz dp ng ll pa pb ni lp pc pd nk lt pe pf nm jk bi translated">XI~iv ||对比 F1 车型分数</h2><pre class="lz ma mb mc gt nz ny oa ob aw oc bi"><span id="0141" class="od mx je ny b gy oe of l og oh">F1_Score = [F1_Score_RF, F1_Score_DT, F1_Score_NB]</span><span id="0e71" class="od mx je ny b gy ov of l og oh">F1_Score_pos = np.arange(len(Methods))</span><span id="c06f" class="od mx je ny b gy ov of l og oh">plt.bar(F1_Score_pos, F1_Score)</span><span id="c7af" class="od mx je ny b gy ov of l og oh">plt.xticks(F1_Score_pos, Methods)</span><span id="e241" class="od mx je ny b gy ov of l og oh">plt.title(‘comparing the F1 Score of each model’)</span><span id="3044" class="od mx je ny b gy ov of l og oh">plt.show()</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pj"><img src="../Images/19898d00cb3265910e5b4a82d6d9e818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*j5L48T0nKosRj-aAdsYLEg.png"/></div></div></figure><ul class=""><li id="5b81" class="mi mj je le b lf lg li lj ll mk lp ml lt mm lx pk mo mp mq bi translated">朴素贝叶斯的 F1 值最高，为 77.1%</li><li id="1e68" class="mi mj je le b lf mr li ms ll mt lp mu lt mv lx pk mo mp mq bi translated">决策树的 F1 值为 70.1%。</li><li id="c04e" class="mi mj je le b lf mr li ms ll mt lp mu lt mv lx pk mo mp mq bi translated">随机森林的 F1 得分最低，为 66.2%</li></ul></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="ad69" class="mw mx je bd my mz op nb nc nd oq nf ng kt or ku ni kw os kx nk kz ot la nm nn bi translated">十二||结论</h1><p id="f367" class="pw-post-body-paragraph lc ld je le b lf nq ko lh li nr kr lk ll ns ln lo lp nt lr ls lt nu lv lw lx im bi translated">平均而言，我们的模型大约有 71.8%的准确率。虽然这可能意味着机器无法准确预测每一篇评论，但它也向我们展示了我们的模型没有过度拟合数据的证据。过度拟合是一种建模错误，发生在函数与有限的一组数据点过于接近的时候。过度拟合模型通常发生在使用过于复杂的模型来解释数据中的特质时。然而，由于我们的模型正在被训练成像人脑一样思考，因此可以公平地假设，即使是人类也无法 100%地预测评论是正面还是负面的。这实际上取决于数据及其处理方式。</p><p id="7a92" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">那么这个模型的最佳算法是什么呢？在本项目使用的 3 种算法中，最准确和精确的是<strong class="le jo">朴素贝叶斯</strong>算法。</p><p id="e029" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">然而，我们的随机森林算法的召回率是最高的。这意味着随机森林算法实际上通过将它标记为阳性(真阳性)来计算我们的模型捕获了多少实际阳性。当存在与假阴性相关联的<strong class="le jo">高成本时，这将是我们用来选择最佳模型的良好指标。</strong></p><p id="75f8" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">事实存在于我们的 F1 分数中，这实际上可能是在所选的三个模型中哪个模型最好的最佳预测器。<em class="np">朴素贝叶斯算法具有最高的 F1 分数</em>，这意味着它定义了特定模型的召回率和精确度之间的关系。如果我们需要在精确度和召回率之间寻求平衡，并且如果存在不均匀的类别分布(大量实际否定)，F1 分数可能是更好的衡量标准。</p><p id="b546" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">感谢阅读！希望你学到了有用的东西。</strong></p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><blockquote class="pl"><p id="cc02" class="pm pn je bd po pp pq pr ps pt pu lx dk translated">关注 Rohan Gupta，了解更多与数据科学和机器学习相关的内容</p></blockquote></div></div>    
</body>
</html>