<html>
<head>
<title>Multi-Class Metrics Made Simple, Part III: the Kappa Score (aka Cohen’s Kappa Coefficient)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多类度量变得简单，第三部分:Kappa 分数(又名科恩的 Kappa 系数)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c?source=collection_archive---------2-----------------------#2019-12-18">https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c?source=collection_archive---------2-----------------------#2019-12-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="c192" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">韵律学</h2><div class=""/><div class=""><h2 id="1750" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">衡量预测值和真实值之间的一致性</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c777629740c9f3f619a5c717f0fcda8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mfIgJmdNwoafE9WY.png"/></div></div></figure><p id="c668" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在我之前的<strong class="lf jd">多类度量简单的</strong>帖子中，我写了关于<a class="ae lz" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiGvcyA3bvmAhWSZt4KHZhFDf4QFjAAegQIBRAB&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2&amp;usg=AOvVaw3IZlPlxiAv7zI8t1IUB5tS" rel="noopener ugc nofollow" target="_blank">精度和召回</a>，以及<a class="ae lz" rel="noopener" target="_blank" href="/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1"> F1 分数</a>。我从许多读者那里收到了令人鼓舞的反馈。所以首先谢谢大家！在这篇文章中，我写了另一个流行的衡量标准:kappa 评分。你可能会发现 kappa 分数对你的申请很有用。</p><p id="c6d0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">kappa 分数是一个有趣的指标。它起源于心理学领域:在对<em class="ma">受试者</em>(患者)进行评分时，用于测量两个人<em class="ma">评价者</em>或<em class="ma">评分者</em>(如心理学家)之间的<em class="ma">一致度</em>。它后来被机器学习社区“挪用”来衡量分类性能。也被称为<em class="ma">科恩的卡帕系数</em>，卡帕分数是以<a class="ae lz" href="https://en.wikipedia.org/wiki/Jacob_Cohen_(statistician)" rel="noopener ugc nofollow" target="_blank">雅各布·科恩</a>的名字命名的，雅各布·科恩是一位美国统计学家和心理学家，他写了关于这个主题的开创性论文。这一指标的其他名称包括<em class="ma">科恩的 kappa </em>和<em class="ma"> kappa 统计</em>。在这个帖子里，我简单的用<em class="ma"> kappa 评分</em>。</p><p id="f050" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我首先解释什么是<em class="ma">一致性</em>以及 kappa 评分如何用于衡量一致性，以提供这一指标背后的直觉。然后我继续分类。</p><h1 id="7f51" class="mb mc it bd md me mf mg mh mi mj mk ml ki mm kj mn kl mo km mp ko mq kp mr ms bi translated">什么是协议？</h1><p id="de7c" class="pw-post-body-paragraph ld le it lf b lg mt kd li lj mu kg ll lm mv lo lp lq mw ls lt lu mx lw lx ly im bi translated">想象一下，我们在一所著名的音乐学校(出于某种原因，我想到了茱莉亚音乐学院)管理钢琴系。现在是招生季节，25 名紧张的候选人准备在漂亮的斯坦威三角钢琴上展示他们的技能。我们请两位经验丰富的教授——被称为 A 教授和 B 教授——给每位候选人打分，看他们是<strong class="lf jd">接受</strong>、<strong class="lf jd">等待名单(WL)、</strong>还是<strong class="lf jd">拒绝</strong>。</p><p id="3e85" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">以下是 25 位候选人的试镜结果:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="0f25" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">每个候选人现在有两个评级，每个教授一个。自然，教授们对一些候选人意见一致，对另一些人意见不一。教授之间的高度一致增加了我们对评级可靠性的信心。低水平的一致意味着我们不能相信评级。kappa 分数衡量两个评估者之间的<em class="ma">一致程度</em>，也称为<em class="ma">评分者间可靠性</em>。</p><p id="c0a2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了计算 kappa 分数，首先在矩阵中总结评级是方便的:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi na"><img src="../Images/85eb84673fd347470e45258b18eadb08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*JatpOHZmk1zmc5Nt.png"/></div></figure><p id="f4cd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">列显示教授 a 的评分。行显示教授 b 的评分。每个单元格中的值是两位教授相应评分的候选人数量。</p><p id="fb02" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">衡量一致程度的一个简单方法是看两位教授之间的一致评级比例。姑且称这个数字为<strong class="lf jd"> <em class="ma">同意</em> </strong>。计算<strong class="lf jd"> <em class="ma">同意</em> </strong>很简单:我们只需要查看绿色单元格(对角线)中值的总和，然后除以学生总数:在我们的例子中，25 个评级中有 4+2+6=12 个是一致的，所以:</p><blockquote class="nb nc nd"><p id="a18c" class="ld le ma lf b lg lh kd li lj lk kg ll ne ln lo lp nf lr ls lt ng lv lw lx ly im bi translated"><strong class="lf jd"> <em class="it">同意</em> </strong> <em class="it"> =12/25 =0.48 </em></p></blockquote><p id="4e64" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">就是这样！我们刚刚找到了达成一致的完美标准。或者我们有吗？嗯，我们的衡量标准并不复杂，因为它没有考虑到协议也可能是偶然发生的。这就是 kappa 分数的由来；<em class="ma">kappa 评分考虑协议比偶然协议好多少</em>。由此可见，除了<strong class="lf jd"> <em class="ma">同意</em> </strong>之外，卡帕公式还使用了<em class="ma">机会协议的预期比例；</em>姑且称这个数字为<strong class="lf jd"><em class="ma">chance agree</em></strong><em class="ma">。</em></p><p id="310f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们已经知道如何计算<strong class="lf jd"> <em class="ma">同意</em> </strong>。目前，让我们假设我们知道<strong class="lf jd"> <em class="ma"> ChanceAgree </em> </strong>的值。我们将这两个数字结合起来计算 kappa 分数，如下所示</p><blockquote class="nb nc nd"><p id="5550" class="ld le ma lf b lg lh kd li lj lk kg ll ne ln lo lp nf lr ls lt ng lv lw lx ly im bi translated"><strong class="lf jd"><em class="it">KappaScore =(Agree-chance Agree)/(1-chance Agree)</em></strong></p></blockquote><p id="ba83" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">注意，分子计算的是<strong class="lf jd"> <em class="ma">同意</em>和<em class="ma">机会同意之间的差值。</em> </strong>如果<strong class="lf jd"> <em class="ma">一致</em> </strong> <em class="ma"> = </em> 1，我们有完美一致，对应一个矩阵，其中所有非对角线(粉色)的单元格都是 0。在这种情况下，kappa 值为 1，不考虑<strong class="lf jd"> <em class="ma"> ChanceAgree </em> </strong>。相比之下，如果<strong class="lf jd"><em class="ma">Agree = chance Agree</em></strong>，kappa 为 0，表示教授们的约定是偶然的。如果<strong class="lf jd"> <em class="ma">同意</em> </strong>小于<strong class="lf jd"> <em class="ma">偶然同意</em> </strong>，则 kappa 值为负，表示同意程度低于偶然同意<em class="ma">。</em></p><p id="4248" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回到我们的例子:在我们的例子中(如我们不久前计算的那样)，<strong class="lf jd"> <em class="ma"> ChanceAgree </em> </strong>是 0.3024。换句话说，我们预计约 30%的协议是偶然发生的，也就是说约 7-8 名学生(25 名学生中的 30%)。教授们同意 25 名学生中的 12 名，因此 kappa 分数为正:</p><blockquote class="nb nc nd"><p id="4b25" class="ld le ma lf b lg lh kd li lj lk kg ll ne ln lo lp nf lr ls lt ng lv lw lx ly im bi translated"><em class="it"> KappaScore =(同意-机会同意)/(1-机会同意)<br/>=(0.48–0.3024)/(1–0.3024)<br/>= 0.2546</em></p></blockquote><p id="fd5c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在下一节中，我将解释<strong class="lf jd"> <em class="ma"> ChanceAgree </em> </strong>是如何计算的。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><p id="d4de" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了计算<strong class="lf jd"> <em class="ma"> ChanceAgree </em> </strong>，我们首先来看看每位教授将学生评为<strong class="lf jd"> Accept </strong>的概率。我们再来看看评级矩阵；为了方便起见，我们添加了行和列的总计:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ab4cf008209989b6adc57fd48ccd92b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/0*xoNLU_pV4uLzpAWp.png"/></div></figure><p id="1157" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于 A 教授来说，25 个评分中 4+1+1=6 被<strong class="lf jd">接受</strong>；对于 B 教授来说，这个数字是 4+6+3=13。因此，对于教授 A 和 B，将学生评定为<strong class="lf jd">接受</strong>的概率为:</p><blockquote class="nb nc nd"><p id="f562" class="ld le ma lf b lg lh kd li lj lk kg ll ne ln lo lp nf lr ls lt ng lv lw lx ly im bi translated"><em class="it"> ProbA(接受)= 6/25=0.24 <br/> ProbB(接受)=13/25=0.52 </em></p></blockquote><p id="37e4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">两位教授偶然同意一个<strong class="lf jd">接受</strong> <em class="ma">的概率</em>等于 ProbA 和 ProbB 的乘积:</p><blockquote class="nb nc nd"><p id="bfb0" class="ld le ma lf b lg lh kd li lj lk kg ll ne ln lo lp nf lr ls lt ng lv lw lx ly im bi translated"><em class="it">chance agree(Accept)= ProbA(Accept)×ProbB(Accept)= 0.24×0.52 =</em><strong class="lf jd"><em class="it">0.1248</em></strong></p></blockquote><p id="0f84" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">同样，我们计算<strong class="lf jd"> WL </strong>和<strong class="lf jd">拒绝</strong>的同意概率:</p><blockquote class="nb nc nd"><p id="4a5c" class="ld le ma lf b lg lh kd li lj lk kg ll ne ln lo lp nf lr ls lt ng lv lw lx ly im bi translated"><em class="it">普罗巴(WL)=10/25=0.4 <br/>普罗布(WL)=3/25=0.12 <br/>钱斯·阿科雷(WL)=普罗巴(WL)×普罗布(WL)= 0.4×0.12 =</em><strong class="lf jd"><em class="it">0.048</em></strong></p><p id="bcd9" class="ld le ma lf b lg lh kd li lj lk kg ll ne ln lo lp nf lr ls lt ng lv lw lx ly im bi translated"><em class="it">ProbA(Reject)= 9/25 = 0.36<br/>ProbB(Reject)= 9/25 = 0.36<br/>chance agree(Reject)= ProbA(Reject)×ProbB(Reject)= 0.36×0.36 =</em><strong class="lf jd"><em class="it">0.1188</em></strong></p></blockquote><p id="4983" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">将上述三种概率相加，我们得到协议中任何一个事件——<strong class="lf jd">接受</strong>、<strong class="lf jd">、<strong class="lf jd">拒绝</strong>——偶然发生的概率:</strong></p><blockquote class="nb nc nd"><p id="555c" class="ld le ma lf b lg lh kd li lj lk kg ll ne ln lo lp nf lr ls lt ng lv lw lx ly im bi translated"><em class="it">机会同意= <br/> =机会同意(接受)+机会同意(WL)+机会同意(拒绝)<br/>= 0.1248+0.0480+0.1188 =</em><strong class="lf jd"><em class="it">0.3024</em></strong></p></blockquote><p id="adc7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">就是这样！我们学习了如何计算<strong class="lf jd"> <em class="ma">机会一致</em> </strong>。结合<strong class="lf jd"> <em class="ma">同意</em> </strong>，我们现在可以使用 kappa 评分来衡量同意程度。但是我们如何用它来衡量分类性能呢？</p><h1 id="3ce4" class="mb mc it bd md me mf mg mh mi mj mk ml ki mm kj mn kl mo km mp ko mq kp mr ms bi translated">使用 Kappa 评分进行分类</h1><p id="d783" class="pw-post-body-paragraph ld le it lf b lg mt kd li lj mu kg ll lm mv lo lp lq mw ls lt lu mx lw lx ly im bi translated">要了解 kappa 评分如何用作分类器性能的衡量标准，让我们再次查看评级矩阵:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi na"><img src="../Images/76ec668e42db5c660c3c2ffe574c1e47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*Dox3BxITAQPyUSAY.png"/></div></figure><p id="28ce" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">嗯，你可能已经猜到了:它非常类似于一个混淆矩阵！将其与我在其他帖子中使用的混淆矩阵进行比较(<a class="ae lz" href="https://medium.com/@shmueli/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2" rel="noopener"> Precision and Recall </a>，<a class="ae lz" rel="noopener" target="_blank" href="/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1"> F1-score </a>)，在那里我们构建了一个分类器，可以检测照片中的:“猫”、“母鸡”或“鱼”。注意到“深刻”的相似性了吗？单元格值完全相同。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8b0202518b215de9f74da77be2dea44a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*Da-PGWOSN09fPJeN.png"/></div></figure><p id="67f2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">要用 kappa 进行分类，我们可以简单的把每个评级看成一个类；另外，A 教授的评分是真实/实际值；B 教授的评分就是预测值(或者反过来——无所谓)。在这个实例中，<em class="ma">kappa 分数测量真实值和预测值之间的一致程度</em>，我们将其用作分类器的性能。在我们的例子中，我们可以使用之前计算的 kappa 值，因为单元值是相同的:</p><blockquote class="nb nc nd"><p id="a16c" class="ld le ma lf b lg lh kd li lj lk kg ll ne ln lo lp nf lr ls lt ng lv lw lx ly im bi translated">KappaScore = 0.2546 </p></blockquote><p id="dd55" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因为 1 表示完全一致，0 表示偶然一致，所以我们通常会得到一个介于两者之间的值。如果值小于 0，我们做的<em class="ma">比机会一致(不一致)</em>差，所以分类器中的某些东西被严重破坏了。</p><p id="b47b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">kappa 分数可以使用 Python 的 scikit-learn 库来计算(R 用户可以使用<a class="ae lz" href="https://www.rdocumentation.org/packages/psych/versions/1.8.12/topics/cohen.kappa" rel="noopener ugc nofollow" target="_blank"> cohen.kappa()函数</a>，它是 psych 库的一部分)。</p><p id="9082" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">以下是我如何证实我的计算:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="fd06" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">帖子到此结束。希望你觉得有用！</p></div></div>    
</body>
</html>