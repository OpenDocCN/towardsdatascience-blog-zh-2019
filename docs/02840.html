<html>
<head>
<title>Common Classification Model Evaluation metrics.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通用分类模型评估指标。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/common-classification-model-evaluation-metrics-2ba0a7a7436e?source=collection_archive---------7-----------------------#2019-05-08">https://towardsdatascience.com/common-classification-model-evaluation-metrics-2ba0a7a7436e?source=collection_archive---------7-----------------------#2019-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn"><p id="59ea" class="jo jp iq bd jq jr js jt ju jv jw jx dk translated"><em class="jy">所有的模型都是错的，但有些是有用的，</em>乔治 E. P. Box。</p></blockquote><h1 id="5a14" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">引言。</h1><p id="dd2b" class="pw-post-body-paragraph kx ky iq kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt jx ij bi translated">分类模型有多准确？模型靠谱吗？</p><p id="15b1" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">这两个问题很容易通过评估一个模型在受到看不见的观察时的表现来回答。这篇文章展示了评估模型的一些最佳方法。</p><p id="15c8" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">你将从这篇文章中学到什么:</p><ol class=""><li id="20b4" class="lz ma iq kz b la lu le lv li mb lm mc lq md jx me mf mg mh bi translated">Jaccard 索引。</li><li id="5cd3" class="lz ma iq kz b la mi le mj li mk lm ml lq mm jx me mf mg mh bi translated">混淆矩阵</li><li id="2fba" class="lz ma iq kz b la mi le mj li mk lm ml lq mm jx me mf mg mh bi translated">F-1 分数</li><li id="df99" class="lz ma iq kz b la mi le mj li mk lm ml lq mm jx me mf mg mh bi translated">原木损失</li></ol><h1 id="7e56" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk mn km kn ko mo kq kr ks mp ku kv kw bi translated">样本模型。</h1><p id="afda" class="pw-post-body-paragraph kx ky iq kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt jx ij bi translated">首先我将拟合一个简单的模型，并用它来说明这些方法在模型性能评估中的应用。该模型预测癌细胞是否是恶性的。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="0eff" class="mz ka iq mv b gy na nb l nc nd">#quick model fit<br/>import numpy as np<br/>import warnings<br/>import pandas<br/>warnings.filterwarnings("ignore")#not recomended but i have included this for my own convenience.<br/>from sklearn.datasets import load_breast_cancer<br/>data = load_breast_cancer()<br/>X = pandas.DataFrame(data = data.data,columns=data.feature_names)<br/>y = data.target<br/>#train test split<br/>from sklearn import model_selection<br/>np.random.seed(2) #to enable you replicate the same thing i am doing here.<br/>X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=0.30)<br/># I will use logistic reg<br/>from sklearn.linear_model import LogisticRegression<br/>reg = LogisticRegression()<br/>reg.fit(X_train,y_train)<br/>preds = reg.predict(X_test)<br/>predsprob = reg.predict_proba(X_test)</span></pre><h1 id="620c" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk mn km kn ko mo kq kr ks mp ku kv kw bi translated">Jaccard 索引</h1><p id="2c7d" class="pw-post-body-paragraph kx ky iq kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt jx ij bi translated">假设预测值为(y hat ),实际值为 y，Jaccard 指数可定义为:</p><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/18236f6cc3ae2b7b38e1b3378ddab8f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:252/0*_PVWxMRXNFsGdDcu"/></div></figure><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c94543657baf52e78220c6566c64d04c.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/0*jNo18NfEIvS2VHXg"/></div></figure><p id="2f5f" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">假设你有下面一组预测值和实际值。</p><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/adf69dbf222b1d7bfa6f7dbcc0d87674.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/0*DctK84lI7vXKcQqg"/></div></figure><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/fd55c2ed519428d3f66afac1118a6327.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/0*QkgbCuYH2qINO1rC"/></div></figure><p id="dd4a" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">Jaccard 指数将为:</p><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/9db39acc13b70140801cc21adbdb8be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/0*F5CgJ-GEl-iUiubj"/></div></figure><p id="ca9d" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">该指数背后的思想是这两个组的相似性越高，指数越高。</p><h1 id="dafa" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk mn km kn ko mo kq kr ks mp ku kv kw bi translated">将此应用于上面的模型。</h1><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="8fc5" class="mz ka iq mv b gy na nb l nc nd">from sklearn.metrics import jaccard_similarity_score<br/>j_index = jaccard_similarity_score(y_true=y_test,y_pred=preds)<br/>round(j_index,2)</span><span id="2d9a" class="mz ka iq mv b gy nm nb l nc nd">0.94</span></pre><h1 id="5493" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk mn km kn ko mo kq kr ks mp ku kv kw bi translated">混淆矩阵</h1><p id="38c2" class="pw-post-body-paragraph kx ky iq kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt jx ij bi translated">混淆矩阵用于描述分类模型对一组真实值已知的测试数据的性能。</p><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/8a74b52a766f3ebb18a1de9fffb2470b.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*eciFR5FjijDtddpBm4dbeQ.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk">confusion matrix</figcaption></figure><p id="eba2" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">从混淆矩阵中可以提取以下信息:</p><ol class=""><li id="1e36" class="lz ma iq kz b la lu le lv li mb lm mc lq md jx me mf mg mh bi translated"><strong class="kz ir">真阳性</strong>。这表明一个模型<em class="ns">正确地</em>预测到<em class="ns">阳性</em>病例为<em class="ns">阳性</em>。疾病被诊断为存在，并且确实存在。</li><li id="5172" class="lz ma iq kz b la mi le mj li mk lm ml lq mm jx me mf mg mh bi translated"><strong class="kz ir">假阳性(FP) </strong>:这表明一个模型<em class="ns">错误地将<em class="ns">阴性</em>病例预测为<em class="ns">阳性</em>。一种疾病被诊断为存在，但并不存在。(第一类错误)</em></li><li id="b8bd" class="lz ma iq kz b la mi le mj li mk lm ml lq mm jx me mf mg mh bi translated"><strong class="kz ir">假阴性:(FN) </strong>这表明一个<em class="ns">模型错误地将<em class="ns">阳性</em>病例预测为<em class="ns">阴性</em>。一种疾病被诊断为不存在，但却存在。(第二类错误)</em></li><li id="fb58" class="lz ma iq kz b la mi le mj li mk lm ml lq mm jx me mf mg mh bi translated"><strong class="kz ir">真阴性(TN): </strong>这表明一个模型<em class="ns">正确地预测了<em class="ns">阴性</em>病例为<em class="ns">阳性</em>。一种疾病被诊断为不存在，并且确实不存在。</em></li></ol><h1 id="e05e" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk mn km kn ko mo kq kr ks mp ku kv kw bi translated">将此应用于上面的模型。</h1><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="2939" class="mz ka iq mv b gy na nb l nc nd">from sklearn.metrics import confusion_matrix<br/>print(confusion_matrix(y_test,preds,labels=[1,0]))<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>sns.heatmap(confusion_matrix(y_test,preds),annot=True,lw =2,cbar=False)<br/>plt.ylabel("True Values")<br/>plt.xlabel("Predicted Values")<br/>plt.title("CONFUSSION MATRIX VISUALIZATION")<br/>plt.show()</span></pre><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/fe0608dbb8af44e6f8ad0ab5a44f7986.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*BShyqK0lN7GDjj4wVO3BAA.png"/></div></figure><p id="4ecc" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">在这种情况下，对于乳腺癌数据，模型正确预测 62 例为良性，98 例为恶性。相比之下，它总共错误预测了 11 个案例。</p><h1 id="34d7" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk mn km kn ko mo kq kr ks mp ku kv kw bi translated">f1-得分。</h1><p id="bf1c" class="pw-post-body-paragraph kx ky iq kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt jx ij bi translated">这来自于混淆矩阵。基于上面的混淆矩阵，我们可以计算精确度和召回分数。</p><p id="7d7a" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">精度分数:这是精度的度量，前提是已经预测了类别标签。简单地说，它回答了下面的问题，在所有的类中，有多少是正确预测的？这个问题的答案应该是越高越好。</p><p id="58bd" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">它可以计算如下:</p><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/74595e206c2271f457013e766146eb84.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/0*x4NPH56nfofGE73N"/></div></figure><p id="fb41" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">回忆分数(敏感度):这是真正的阳性率，如果它预测阳性，那么它发生的频率是多少？</p><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/3f3b5010ac014203ed6d63a27131e8de.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/0*7unLfKOEMe8JZao2"/></div></figure><p id="c16f" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">F1 分数是根据每个类的精确度和召回率计算的。它是精确度和召回分数的加权平均值。F1 分数在 1 时达到完美值，在 0 时最差。这是一个很好的方法来表明一个分类器有很好的召回率和精确度值。</p><p id="3e0f" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">我们可以用这个公式来计算:</p><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/324e871ea510193c1a281c4f7f4622d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/0*oZTSoTgtQm0sw14T"/></div></figure><h1 id="f6c8" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk mn km kn ko mo kq kr ks mp ku kv kw bi translated">应用于上面的模型。</h1><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="0a5d" class="mz ka iq mv b gy na nb l nc nd">from sklearn.metrics import f1_score<br/>f1_score(y_test,preds)</span><span id="9dfe" class="mz ka iq mv b gy nm nb l nc nd">0.9468599033816425</span></pre><p id="0bf1" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">可以计算所有类别的 F1 分数，因此可以使用实际分数的平均值，如下面的分类报告所示。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="bba8" class="mz ka iq mv b gy na nb l nc nd">from sklearn.metrics import classification_report<br/>print(classification_report(y_test,preds))</span><span id="6734" class="mz ka iq mv b gy nm nb l nc nd">precision    recall  f1-score   support</span><span id="5df3" class="mz ka iq mv b gy nm nb l nc nd">           0       0.91      0.93      0.92        67<br/>           1       0.95      0.94      0.95       104</span><span id="4e44" class="mz ka iq mv b gy nm nb l nc nd">   micro avg       0.94      0.94      0.94       171<br/>   macro avg       0.93      0.93      0.93       171<br/>weighted avg       0.94      0.94      0.94       171</span></pre><h1 id="4c3d" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk mn km kn ko mo kq kr ks mp ku kv kw bi translated">日志损失。</h1><p id="478b" class="pw-post-body-paragraph kx ky iq kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt jx ij bi translated">在分类器的结果是类别概率而不是类别标签的情况下，我们可以使用对数损失，就像逻辑回归模型的情况一样。</p><p id="9410" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">对数损失衡量模型的性能，其中预测结果是介于 0 和 1 之间的概率值。</p><p id="6ff2" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">在现实生活中，当预测 0.101 的概率时，真实标签应该是 1，这将导致高对数损失。可以使用对数损失公式计算数据集中每一行的对数损失。</p><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/4ff5b11c9e02753939f4fd1df604e359.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/0*MEYsqUHouvgfR5OE"/></div></figure><p id="570c" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">该方程简单地测量了每个预测概率与实际标签的距离。所有行的日志损失的平均值给出了日志损失的理想值。</p><figure class="mq mr ms mt gt nf gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/81fb7b2fca2c58876a637fd497371034.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/0*tSxNOVxHQObQPby3"/></div></figure><p id="0845" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">好的和模型应该具有较小的对数损失值。</p><h1 id="cfad" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk mn km kn ko mo kq kr ks mp ku kv kw bi translated">在上述模型中应用。</h1><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="eedd" class="mz ka iq mv b gy na nb l nc nd">from sklearn.metrics import log_loss<br/>log_loss(y_test,predsprob)</span><span id="a1a8" class="mz ka iq mv b gy nm nb l nc nd">0.13710589473837184</span></pre><p id="9f79" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">这里我们有一个 0.14 的对数损失，相当不错！</p><h1 id="e94d" class="jz ka iq bd kb kc kd ke kf kg kh ki kj kk mn km kn ko mo kq kr ks mp ku kv kw bi translated">结论。</h1><ol class=""><li id="2a64" class="lz ma iq kz b la lb le lf li nz lm oa lq ob jx me mf mg mh bi translated">基于所应用的模型，应该很好地理解评估指标的选择。</li><li id="6521" class="lz ma iq kz b la mi le mj li mk lm ml lq mm jx me mf mg mh bi translated">为了从评估指标中获得优异的结果，建议对模型进行特征工程和参数调整。</li></ol><p id="5421" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">感谢阅读，欢迎任何评论和/或补充。</p><p id="3927" class="pw-post-body-paragraph kx ky iq kz b la lu lc ld le lv lg lh li lw lk ll lm lx lo lp lq ly ls lt jx ij bi translated">干杯！</p></div></div>    
</body>
</html>