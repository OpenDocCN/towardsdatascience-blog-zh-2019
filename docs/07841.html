<html>
<head>
<title>Decision Tree from Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中从头开始的决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775?source=collection_archive---------1-----------------------#2019-10-30">https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775?source=collection_archive---------1-----------------------#2019-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e217" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">决策树</strong>是当今可用的最强大的机器学习工具之一，被广泛用于各种现实世界的应用中，从脸书的<a class="ae ko" href="https://quinonero.net/Publications/predicting-clicks-facebook.pdf" rel="noopener ugc nofollow" target="_blank">广告点击预测</a>到 Airbnb 体验的<a class="ae ko" href="https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789" rel="noopener">排名</a>。然而，它们是直观的、易于解释的——并且易于实现。在本文<strong class="js iu">中，我们将用 66 行 Python 代码</strong>训练我们自己的决策树分类器。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/e02f0bb1352f3adf8cb9ce7f50552937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*peEdca4b2aYgpX3ANITmww.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Let’s build this!</figcaption></figure><h1 id="6e36" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">什么是决策树？</h1><p id="6d24" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">决策树可用于<strong class="js iu">回归</strong>(连续实值输出，<br/> <em class="mi">例如</em>预测房屋价格)或<strong class="js iu">分类</strong>(分类输出，<br/> <em class="mi">例如</em>预测垃圾邮件与无垃圾邮件)，但这里我们将重点讨论分类。决策树分类器是一个<strong class="js iu">二叉树</strong>，其中通过从根到叶遍历树来进行预测——在每个节点，<strong class="js iu">如果特征小于阈值，我们向左，否则向右</strong>。最后，每个叶子与一个<strong class="js iu">类</strong>相关联，这是预测器的输出。</p><p id="a46b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，考虑这个<a class="ae ko" href="https://archive.ics.uci.edu/ml/datasets/Wireless+Indoor+Localization" rel="noopener ugc nofollow" target="_blank">无线室内定位数据集</a>。它给出了 7 个特征，代表公寓中手机感知的 7 个 Wi-Fi 信号的强度，以及手机的室内位置，可能是 1、2、3 或 4 号房间。</p><pre class="kq kr ks kt gt mj mk ml mm aw mn bi"><span id="8f57" class="mo lg it mk b gy mp mq l mr ms">+-------+-------+-------+-------+-------+-------+-------+------+<br/>| Wifi1 | Wifi2 | Wifi3 | Wifi4 | Wifi5 | Wifi6 | Wifi7 | Room |<br/>+-------+-------+-------+-------+-------+-------+-------+------+<br/>|  -64  |  -55  |  -63  |  -66  |  -76  |  -88  |  -83  |   1  |<br/>|  -49  |  -52  |  -57  |  -54  |  -59  |  -85  |  -88  |   3  |<br/>|  -36  |  -60  |  -53  |  -36  |  -63  |  -70  |  -77  |   2  |<br/>|  -61  |  -56  |  -55  |  -63  |  -52  |  -84  |  -87  |   4  |<br/>|  -36  |  -61  |  -57  |  -27  |  -71  |  -73  |  -70  |   2  |<br/>                               ...</span></pre><p id="a296" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">目标是根据 Wi-Fi 信号 1 到 7 的强度来预测手机位于哪个房间。深度为<strong class="js iu">2 的经过训练的决策树可能如下所示:</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mt"><img src="../Images/7e5ce206ab72b1e5705edec77437ad07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXRpvrts6yEWmA294GQccA.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Trained decision tree. Predictions are performed by traversing the tree from root to leaf and going left when the condition is true. For example, if Wifi 1 strength is -60 and Wifi 5 strength is -50, we would predict the phone is located in room 4.</figcaption></figure><h1 id="e990" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">基尼杂质</h1><p id="8371" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">在我们深入研究代码之前，让我们定义算法中使用的度量。决策树使用<strong class="js iu">基尼杂质</strong>的概念来描述一个节点的<strong class="js iu">同质性</strong>或“纯”程度。如果一个节点的所有样本都属于同一个类，则该节点是纯的(<em class="mi"> G = 0 </em>),而具有来自许多不同类的许多样本的节点将具有更接近 1 的基尼系数。</p><p id="4b05" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更正式的说法是，跨<em class="mi"> k </em>个类别划分的<em class="mi"> n </em>个训练样本的基尼系数被定义为</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/0c543161cd03000b9dd7ab8320eeb0ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*UNszwSYfUJFHtfC0jvBKsw@2x.png"/></div></figure><p id="ed27" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="mi"> p[k] </em>是属于类别<em class="mi"> k </em>的样本分数。</p><p id="3eb5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，如果一个节点包含五个样本，其中两个是教室 1，两个是教室 2，一个是教室 3，而没有教室 4，那么</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mv"><img src="../Images/f9056d5ffdd06c23f9768cdc6b60f569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DKF4UezsNpe3y0z3xCqVWg@2x.png"/></div></div></figure><h1 id="9b63" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">CART 算法</h1><p id="ba37" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">训练算法是一种叫做 CART 的<strong class="js iu">递归</strong>算法，是<em class="mi">分类和回归树</em>的简称。每个节点都被分割，以使子节点的基尼系数杂质(更具体地说，是按大小加权的子节点的基尼系数平均值)最小化。</p><p id="59e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当达到<strong class="js iu">最大深度</strong>、超参数<strong class="js iu"/>、时，或者当没有分裂可以导致两个孩子比他们的父母更纯时，递归停止。<a class="ae ko" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">其他超参数</a>可以控制这个停止标准(在实践中对避免过度拟合至关重要)，但我们不会在这里讨论它们。</p><p id="750b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，如果<code class="fe mw mx my mk b">X = [[1.5], [1.7], [2.3], [2.7], [2.7]]</code>和<code class="fe mw mx my mk b">y = [1, 1, 2, 2, 3]</code>，那么最优分割是<code class="fe mw mx my mk b">feature_0 &lt; 2</code>，因为如上计算，父母的基尼系数是 0.64，分割后子女的基尼系数是</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mz"><img src="../Images/f45b5ca14fe9038b543c6fcfea197686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Q8zV8T_adwI622Q7NGK-w@2x.png"/></div></div></figure><p id="7ae9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以说服自己，没有任何其他分割方式能产生更低的基尼系数。</p><h2 id="a013" class="mo lg it bd lh na nb dn ll nc nd dp lp kb ne nf lt kf ng nh lx kj ni nj mb nk bi translated">寻找最佳特征和阈值</h2><p id="6265" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">CART 算法的关键是找到最优特征和阈值，使得基尼系数杂质最小。为此，我们尝试了所有可能的分裂，并计算了由此产生的基尼系数。</p><p id="9e58" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是我们如何尝试连续值的所有可能的阈值呢？有一个简单的技巧-对给定特性的值进行排序，并考虑两个相邻值之间的所有中点。排序是昂贵的，但我们很快就会看到，无论如何它是需要的。</p><p id="11b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们如何计算所有可能分裂的基尼系数呢？</p><p id="0cc3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一种解决方案是实际执行每次分割，并计算得出的基尼系数。不幸的是，这很慢，因为我们需要查看所有的样本，将它们分成左和右。更准确地说，这将是<em class="mi"> n </em>个拆分，每个拆分有<em class="mi">个 O(n) </em>个操作，使得整个操作为<em class="mi"> O(n ) </em>。</p><p id="32a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更快的方法是<strong class="js iu"> 1。</strong>遍历排序后的特征值作为可能的阈值，<strong class="js iu"> 2。</strong>记录左侧和右侧每类样品的数量，以及<strong class="js iu"> 3。</strong>在每个阈值后，将它们递增/递减 1。从它们我们可以很容易地计算出常数时间内的基尼系数。</p><p id="0c55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">实际上，如果<em class="mi"> m </em>是节点的大小，并且<em class="mi">m【k】</em>是节点中类别<em class="mi"> k </em>的样本数，那么</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nl"><img src="../Images/9a51dbeced0f02223b3a128bdcbea494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*jp_Xlzk2kw3_I0y5CFVXqA@2x.png"/></div></div></figure><p id="0107" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于在看到第<em class="mi"> i </em>个阈值后，左边有<em class="mi"> i </em>元素，右边有<em class="mi">m–I</em>，</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/506b1da20ad0351dbf2fd5f3953943ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*t9T4whJ85lqs7sVW2s4bUQ@2x.png"/></div></figure><p id="290a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">和</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/67bf0c98fab5af9175e38f47a2892e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*4NXwGKZ7fgI_QkdngVwddw@2x.png"/></div></figure><p id="e778" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由此得出的基尼系数是一个简单的加权平均数:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c92dfdabe98ace4fa394a3acf9aa18cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*MhDFmZB81LgahP-AbCms9w@2x.png"/></div></figure><p id="28a2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是完整的<code class="fe mw mx my mk b">_best_split</code>方法。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="f72e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第 61 行的条件是最后一个微妙之处。通过遍历所有特征值，我们允许对具有相同值的样本进行分割。实际上，我们只能在它们对于该特性有独特的值的情况下对它们进行分割，因此需要额外的检查。</p><h2 id="b727" class="mo lg it bd lh na nb dn ll nc nd dp lp kb ne nf lt kf ng nh lx kj ni nj mb nk bi translated">递归</h2><p id="ac7f" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">难的部分完成了！现在我们要做的就是递归地分割每个节点，直到达到最大深度。</p><p id="f53a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是首先让我们定义一个<code class="fe mw mx my mk b">Node</code>类:</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="01cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将决策树拟合到数据<code class="fe mw mx my mk b">X</code>和目标<code class="fe mw mx my mk b">y</code>是通过调用递归方法<code class="fe mw mx my mk b">_grow_tree()</code>的<code class="fe mw mx my mk b">fit()</code>方法完成的:</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="np nq l"/></div></figure><h2 id="a2bc" class="mo lg it bd lh na nb dn ll nc nd dp lp kb ne nf lt kf ng nh lx kj ni nj mb nk bi translated">预言</h2><p id="fe84" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">我们已经看到了如何<strong class="js iu">拟合</strong>决策树，现在我们如何用它来<strong class="js iu">预测</strong>看不见的数据的类？再简单不过了——如果特征值低于阈值，则<em class="mi">向左</em>，否则<em class="mi">向右</em>。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="np nq l"/></div></figure><h1 id="fd98" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">训练模型</h1><p id="3481" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">我们的<code class="fe mw mx my mk b">DecisionTreeClassifier</code>准备好了！让我们在无线室内定位数据集上训练一个模型:</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="np nq l"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nr"><img src="../Images/298490e82d0723d92ae18e7ff6e8df88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kPQp91pPxDR42wxXOUWFpw.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Our trained decision tree. For the ASCII visualization — not in the scope of this article — check out <a class="ae ko" href="https://github.com/joachimvalente/decision-tree-cart/blob/master/tree.py" rel="noopener ugc nofollow" target="_blank">the full code</a> for the Node class.</figcaption></figure><p id="2c4e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为健全性检查，下面是<a class="ae ko" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>实现的输出:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ns"><img src="../Images/6ec1226036ec3edca6b55f95aa605dde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ayeaXGGHaPc0rh-elmjJUw.png"/></div></div></figure><h1 id="2005" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">复杂性</h1><p id="e601" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">很容易看出，预测在<strong class="js iu"> <em class="mi"> O(log m) </em> </strong>，其中<em class="mi"> m </em>是树的深度。</p><p id="4c55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是训练呢？<a class="ae ko" href="https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">主定理</strong> </a>在这里会很有帮助。在具有<em class="mi"> n </em>个样本的数据集上拟合树的时间复杂度可以用下面的<strong class="js iu">递归</strong>关系来表示:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/15e75509d8e54b749b4583164e61d92a.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*43hc7El-eZr0suAl-J0iTg@2x.png"/></div></figure><p id="3287" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中，假设左右孩子大小相同的最佳情况，<em class="mi"> a </em> = 2，<em class="mi">b</em>= 2；而<em class="mi"> f(n) </em>是将节点拆分成两个子节点的复杂度，换句话说就是<code class="fe mw mx my mk b">_best_split</code>的复杂度。第一个<code class="fe mw mx my mk b">for</code>循环对特性进行迭代，对于每次迭代，都有一个复杂度为<em class="mi"> O(n log n) </em>的<strong class="js iu">排序</strong> <em class="mi"> </em>和<em class="mi"> O(n) </em>中的另一个<code class="fe mw mx my mk b">for</code>循环。因此<em class="mi"> f(n) </em>是<em class="mi"> O(k n log n) </em>其中<em class="mi"> k </em>是特征的数量。</p><p id="6ca7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这些假设下，主定理告诉我们总时间复杂度是</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ab6b9a96ef082781fde82ef208a7bd46.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*rODwB5lbrPA-sxa_YAAxOg@2x.png"/></div></figure><p id="92bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这与<a class="ae ko" href="https://scikit-learn.org/stable/modules/tree.html#complexity" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn 实现</a>的复杂性差不太远，但仍然比它差，显然是在<em class="mi"> O(k n log n)中。</em>如果有人知道这怎么可能，请在评论里告诉我！</p><h1 id="b4ea" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">完全码</h1><p id="469e" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">完整的代码可以在这个<a class="ae ko" href="https://github.com/joachimvalente/decision-tree-cart" rel="noopener ugc nofollow" target="_blank"> Github repo </a>上找到。正如承诺的那样，为了好玩，这里有一个精简到 66 行的版本<a class="ae ko" href="https://github.com/joachimvalente/decision-tree-cart/blob/master/minimal_cart.py#L14-L79" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><blockquote class="oc od oe"><p id="6713" class="jq jr mi js b jt ju jv jw jx jy jz ka of kc kd ke og kg kh ki oh kk kl km kn im bi translated">何、、欧瑾、、徐、、、史、、安托万·阿塔拉、拉尔夫·赫布里希、斯图尔特·鲍尔斯和华金·基诺内罗·坎德拉。2014.预测脸书广告点击率的实践经验。《第八届在线广告数据挖掘国际研讨会论文集》(ADKDD'14)。美国纽约州纽约市 ACM，第 5 条，9 页。DOI = http://dx . DOI . org/10.1145/26484848364</p><p id="cf98" class="jq jr mi js b jt ju jv jw jx jy jz ka of kc kd ke og kg kh ki oh kk kl km kn im bi translated">Jayant G Rohra、Boominathan Perumal、Swathi Jamjala Narayanan、Priya Thakur 和 Rajen B Bhatt，“使用粒子群优化和重力搜索算法与神经网络的模糊混合在室内环境中的用户定位”，载于第六届软计算解决问题国际会议论文集，2017 年，第 286-295 页。</p><p id="66c4" class="jq jr mi js b jt ju jv jw jx jy jz ka of kc kd ke og kg kh ki oh kk kl km kn im bi translated">布雷曼，利奥；J. H .弗里德曼；奥尔申。斯通，C. J. (1984)。<em class="it">分类和回归树</em>。加利福尼亚州蒙特雷:沃兹沃斯&amp;布鲁克斯/科尔高级图书&amp;软件。</p></blockquote></div></div>    
</body>
</html>