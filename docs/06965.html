<html>
<head>
<title>Applying Artifical Intelligence to ESports (PUBG)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将人工智能应用于电子竞技(PUBG)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/applying-data-science-to-esports-pubg-b7950330423a?source=collection_archive---------21-----------------------#2019-10-02">https://towardsdatascience.com/applying-data-science-to-esports-pubg-b7950330423a?source=collection_archive---------21-----------------------#2019-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="f66f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们都是玩电子游戏长大的，对吗？那么，我们如何将数据科学和数据分析应用于视频游戏呢？PlayerUnknown's Battleground 也被称为 PUBG，可以说已经彻底改变了皇室战争电子游戏领域！关于 PUBG 如何开启《皇室战争》竞技游戏模式有一个争论，其他视频游戏设计者、创造者&amp;制造商，如<em class="ko"/>&amp;<em class="ko">【Apex 传奇】</em>，仅举几个例子，都复制了重新设计的&amp;，使自己更加独特！我决定从 Kaggle 举办比赛的网站上获取数据，看看我们能否根据单个玩家的特征统计数据，找出一个人在比赛中的排名。</p><p id="bea1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 PUBG 游戏中，每场比赛最多有 100 名玩家开始(matchId)。玩家可以加入在游戏结束时(winPlacePerc)排名的队伍(groupId ),排名是基于当他们被淘汰时有多少其他队伍还活着。在游戏中，玩家可以捡起不同的弹药，救活倒下但没有被击倒的队友，驾驶车辆，游泳，跑步，射击&amp;体验所有的后果——例如摔得太远或撞倒自己&amp;消灭自己。</p><p id="b9dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们得到了大量匿名的 PUBG 游戏统计数据，格式化后每行包含一个玩家的赛后统计数据。数据来自各种类型的比赛:单人赛、双人赛、团队赛和定制赛；不能保证每场比赛有 100 名球员，也不能保证每组最多有 4 名球员。我们必须创建一个模型，根据球员的最终统计数据预测他们的最终排名，范围从 1(第一名)到 0(最后一名)。我创建了一个数据字典来获取更多关于数据的信息&amp;让我们的生活更简单。</p><p id="6f78" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在整篇文章中，我打算提供允许我创建这样一个模型的代码——其他模型可能对其他人来说表现得更好。我的下一个目标是创造视觉效果&amp;提供任何进一步的见解！这个项目和文章类似于我的关于爱荷华州房价的项目和文章！感兴趣的话，点击<a class="ae kp" href="https://medium.com/datadriveninvestor/regression-modeling-with-python-66780f118097" rel="noopener">这里</a>！</p><p id="deac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">理解这篇文章需要什么:</p><ul class=""><li id="0943" class="kq kr it js b jt ju jx jy kb ks kf kt kj ku kn kv kw kx ky bi translated">丰富的 Python 经验和知识</li><li id="4d9c" class="kq kr it js b jt kz jx la kb lb kf lc kj ld kn kv kw kx ky bi translated">Python 统计数据包的中级知识</li><li id="214c" class="kq kr it js b jt kz jx la kb lb kf lc kj ld kn kv kw kx ky bi translated">IDE</li><li id="bb33" class="kq kr it js b jt kz jx la kb lb kf lc kj ld kn kv kw kx ky bi translated">熟悉数据科学、分析和统计</li></ul><p id="16fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们从进口开始吧！</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="a7de" class="ln lo it lj b gy lp lq l lr ls">import pandas as pd <br/>import numpy as np  <br/>import seaborn as sns <br/>import matplotlib.pyplot as plt <br/>from sklearn.model_selection import train_test_split, GridSearchCV <br/>from sklearn.preprocessing import PolynomialFeatures, StandardScaler, LabelEncoder</span><span id="84ad" class="ln lo it lj b gy lt lq l lr ls">from sklearn.pipeline import Pipeline<br/>from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RidgeCV</span><span id="92c2" class="ln lo it lj b gy lt lq l lr ls">from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier</span><span id="8145" class="ln lo it lj b gy lt lq l lr ls">%matplotlib inline</span></pre><p id="9e89" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们需要导入我们的文件。在这个项目中，我们使用了两个文件。一组训练数据和一组测试数据。训练集和测试集非常相似，除了我们在训练集中缺少了一列，球员的最后落点！请记住，无论我们对训练数据集做什么，我们都必须对测试数据做什么，以保持我们的数据无偏、精确和相似。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="0f5e" class="ln lo it lj b gy lp lq l lr ls"># Imports the csv files from Kaggle<br/># Files are stored locally<br/>pubg_train = pd.read_csv('./data/pubg_train.csv')<br/>pubg_train.head()</span><span id="8833" class="ln lo it lj b gy lt lq l lr ls">pubg_test = pd.read_csv('./data/pubg_test.csv')<br/>pubg_test.head()</span></pre><p id="adab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们希望对数据显示的任何内容应用一些数据清理、探索性数据分析和常识。下面我写了一个简单的数据清理和探索性数据分析的函数，我将它应用于两个 csv 文件。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="cce1" class="ln lo it lj b gy lp lq l lr ls"># Here is a function for basic exploratory data analysis:<br/>def eda(dataframe):<br/>    # Replace any blank spaces w/ a underscore.<br/>    dataframe.columns = dataframe.columns.str.replace(" ", "_")<br/>    # Checks for the null values.<br/>    print("missing values: {}".format(dataframe.isnull().sum().sum()))<br/>    # Checks the data frame range size.<br/>    print("dataframe index: {}".format(dataframe.index))<br/>    # Checks for data types of the columns within the data frame.<br/>    print("dataframe types: {}".format(dataframe.dtypes))<br/>    # Checks the shape of the data frame.<br/>    print("dataframe shape: {}".format(dataframe.shape))<br/>    # Gives us any statistical information of the data frame.<br/>    print("dataframe describe: {}".format(dataframe.describe()))<br/>    # Gives us the duplicated data of the data frame. <br/>    print("dataframe duplicates: {}".format(dataframe[dataframe.duplicated()].sum()))<br/>    <br/>    # A for loop that does this for every single column &amp; their values within our data frame giving us all <br/>        # unique values.<br/>    for item in dataframe:<br/>        print(item)<br/>        print(dataframe[item].nunique())</span><span id="7ea3" class="ln lo it lj b gy lt lq l lr ls"># Let's apply this function to our entire data frame.<br/>eda(pubg_train)<br/>eda(pubg_test)</span></pre><p id="b93d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">经过一些探索性分析，我们意识到我们有一些“NaN”或空值，由于不准确的数据、创建者的错误输入或不适用于训练数据集中的某个球员而留空。记住，我们必须对训练集和测试集做同样的事情。我们的数据非常“脏”</p><p id="2548" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面，我创建了一个函数来解决这个问题。我们可以通过删除行来删除这些值，看看这些值是否扭曲了数据，这些值是否是某种异常值，或者我们可以手动编辑和替换它们。我决定将“NaN”值转换为 0.0 浮点类型。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="b110" class="ln lo it lj b gy lp lq l lr ls"># Here's a function to convert NaN's in a specific column in the data # set to 0.0 for floats.<br/># Just pass in the entire data frame &amp; specify a specific column w/ a # float NaN.<br/>def convert_float_nan(data):<br/>    return data.replace(np.nan, 0.0, inplace = True)<br/>convert_float_nan(pubg_train["winPlacePerc"])</span></pre><p id="05e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">清理完数据后，我们已经清除了训练集的所有空值数据，因此它将与测试数据集相似！两者都没有空值&amp;都有八行，但是记住训练数据集有 24 列，而测试数据集有 25 列。但是，我们还没有定下来！让我们改变位于两个数据集中的列的类型！转换类型使分析更容易，有效和正确！</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="1e62" class="ln lo it lj b gy lp lq l lr ls">pubg_train['damageDealt'] = pubg_train['damageDealt'].astype(int)<br/>pubg_train['longestKill'] = pubg_train['longestKill'].astype(int)<br/>pubg_train['rideDistance'] = pubg_train['rideDistance'].astype(int)<br/>pubg_train['swimDistance'] = pubg_train['swimDistance'].astype(int)<br/>pubg_train['walkDistance'] = pubg_train['walkDistance'].astype(int)<br/>pubg_train['winPlacePerc'] = pubg_train['winPlacePerc'].astype(int)</span><span id="4a96" class="ln lo it lj b gy lt lq l lr ls">pubg_test['damageDealt'] = pubg_test['damageDealt'].astype(int)<br/>pubg_test['longestKill'] = pubg_test['longestKill'].astype(int)<br/>pubg_test['rideDistance'] = pubg_test['rideDistance'].astype(int)<br/>pubg_test['swimDistance'] = pubg_test['swimDistance'].astype(int)<br/>pubg_test['walkDistance'] = pubg_test['walkDistance'].astype(int)</span></pre><figure class="le lf lg lh gt lv gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/fa860b28ff50d998b750ae7f8dbdf741.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*cXAHftTCPrztD0G96gl2fg.png"/></div></figure><p id="0dc7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">既然两个集合的数据都是干净的&amp; set，我们是否可以可视化所有列与“winPlacePerc”列的关系？这是我们希望预测的仅位于定型数据集中而不在测试数据集中的列。以下代码将完成这项工作:</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="cea3" class="ln lo it lj b gy lp lq l lr ls"># Let's make a simple heat map on what column we're trying to <br/># predict to see correlations.<br/>plt.figure(figsize = (5,5))<br/>sns.heatmap(np.round(pubg_train.corr()[['winPlacePerc']].sort_values('winPlacePerc'), 30), annot = True, cmap = 'viridis')</span></pre><p id="db14" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上图显示了我们想要预测的内容&amp;每列如何与该列正相关和负相关。某一列的接近程度&amp;该列中的数据量如何准确地与排名第一或最后相关？这是我们的目标！让我们看看哪些列为我们提供了最强的相关性:</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="ef63" class="ln lo it lj b gy lp lq l lr ls"># Let's view the largest negative correlated columns to our <br/># "winPlacePerc" column.<br/>largest_neg_corr_list = pubg_train.corr()[['winPlacePerc']].sort_values('winPlacePerc').head(5).T.columns<br/>largest_neg_corr_list</span></pre><figure class="le lf lg lh gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/be72c0c6d1dd7cb173ac64b928d4e167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-oAD-faEUKU_GKUpzXFH2A.png"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Largest Negatively Correlated Columns</figcaption></figure><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="5c9a" class="ln lo it lj b gy lp lq l lr ls"># Let's view the largest positive correlated columns to our <br/># "winPlacePerc" column.<br/>largest_pos_corr_list = pubg_train.corr()[['winPlacePerc']].sort_values('winPlacePerc').tail(5).T.columns.drop('winPlacePerc')<br/>largest_pos_corr_list</span></pre><figure class="le lf lg lh gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mh"><img src="../Images/cae95844a4e2e9ffd382ee94f4211ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*zWLL45DCV4mxjsZCqWpapQ.png"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Largest Positively Correlated Columns</figcaption></figure><p id="6670" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">出于特征工程的目的，我们需要这样做！特征工程是机器学习算法中使用的特征(变量)的创建和操作。(集思广益或测试特征，决定要创建的特征，检查特征如何与模型配合工作，改进特征(如果需要)。我们的特征，我们指定的相关列&amp;它们的数据有助于预测和显示关系，在这种情况下是我们试图预测的关系！</p><p id="0801" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们已经得到了我们的特性，让我们从训练/测试分割数据开始&amp;然后扩展它。我们可以通过将样本数据分割成子集来执行模型验证，这也称为训练/测试分割。这是我们可以用来训练模型和测试模型的代表性数据。这可以帮助表示未来的数据，避免某些调整的过度拟合，并提高我们预测的质量。</p><p id="3f26" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我们缩放数据时，我们实质上是将我们的列转换成 Z 分数。这意味着您正在转换我们的数据，使其符合特定的范围，如 0-100 或 0-1。当您使用基于测量数据点之间距离的方法时，您希望缩放数据。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="2698" class="ln lo it lj b gy lp lq l lr ls"># Here we can feature load our best correlated variables.<br/># Our y-variable is what we want to predict.<br/>X = pubg_train[['killPlace', 'matchDuration', 'winPoints', 'killPoints', 'rankPoints', 'damageDealt', 'weaponsAcquired', 'boosts', 'walkDistance']]<br/>y = pubg_train['winPlacePerc'] # What's being predicted</span><span id="cf5b" class="ln lo it lj b gy lt lq l lr ls"># Let's train test split our data.<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)</span><span id="209b" class="ln lo it lj b gy lt lq l lr ls"># Scales our data.<br/># Feature scaling is a method used to normalize the range of <br/># independent variables or features of data. <br/># In data processing, it is also known as data normalization.<br/>ss = StandardScaler()<br/>ss.fit(X_train)<br/>X_train = ss.transform(X_train)<br/>X_test = ss.transform(X_test)</span></pre><p id="e128" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们已经完成了所有的预处理步骤，让我们开始创建我们的模型，该模型将帮助我们根据我们已经训练/测试的特性来预测胜率。对于这个项目，我决定使用逻辑回归函数，也称为 Logit 函数，因为我们能够处理带有分类标签数据的数字数据。分类标签数据是我选择的特征！</p><p id="1c05" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为什么是 Logit 模型？除了简单的线性回归，逻辑回归是一种替代方法。逻辑回归使用比值比的概念来计算概率。这被定义为一个事件发生的几率与它不发生的几率之比。我决定通过网格搜索来运行我的 Logit 模型，以找到最佳参数&amp;在这个范围内，有一条优化的管道！</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="89ca" class="ln lo it lj b gy lp lq l lr ls"># A pipeline that instantiates a tuned Logistic Regression model.<br/>pipe_cv = Pipeline([<br/>    ('lr', LogisticRegression())<br/>])<br/># Here are our parameters we've tuned.<br/>params = {<br/>    'lr__C':[1.0],<br/>    'lr__penalty':["l1"]<br/>}</span><span id="fb10" class="ln lo it lj b gy lt lq l lr ls"># Our Logistic Regression Model ran through a Grid Search Object.<br/>gs_lr = GridSearchCV(pipe_cv, param_grid = params, cv = 3)<br/>gs_lr.fit(X_train, y_train)<br/>gs_lr.score(X_train, y_train)</span></pre><p id="eb6b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我们基于跑步或 Logit 模型对训练/测试分割功能中的训练数据得出的分数。请记住，分割允许我们使用部分训练数据集作为测试数据集，因为整个训练数据集代表整个测试数据集！</p><figure class="le lf lg lh gt lv gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/c7652bc83e26f7bb9fe1947cbdad037b.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*Majys2qCdKj3uXXEAm9jPw.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Pipeline Gridsearch Training Score</figcaption></figure><p id="de32" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意我们的训练分数有多高！那太好了，如果我们的测试分数相似，那么我们就不是过度健康或不健康！更多了解过度拟合和欠拟合模型，查看我的数据科学家指南<a class="ae kp" rel="noopener" target="_blank" href="/a-guide-for-data-scientists-concepts-statistics-prep-more-911298d8ee70">这里</a>！这将展示如何避免过度装配&amp;装配不足。</p><p id="7bf2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们给测试数据打分吧！</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="446e" class="ln lo it lj b gy lp lq l lr ls"># Let's run our model on the test data.<br/>gs_lr.score(X_test, y_test)</span></pre><figure class="le lf lg lh gt lv gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/32bf83c4d579a943a25318127b82ab92.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/format:webp/1*i-jKmNyYhjJdpcZiLeURWg.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Pipeline Gridsearch Testing Score</figcaption></figure><p id="0d09" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的模型既不是过合身也不是欠合身！干得好！现在，我们对管道的最佳调整参数是什么？</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="d1cc" class="ln lo it lj b gy lp lq l lr ls"># Give's us our best parameters.<br/>gs_lr.best_params_</span></pre><figure class="le lf lg lh gt lv gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/04b565a36cda8b90180626d4d37631f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*-sblh-vdfpt5mqqu6bh92g.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Best Parameters Based off of a Logit Model Pipeline</figcaption></figure><p id="a03f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的 Logit 模型像一个套索逻辑回归特征，在调整参数中用“l1”表示！现在我们已经有了模型集，清理了数据，并发现基于这些特征预测成功率的可能性很大，我们的下一步是将它们可视化！这将在本文后面出现，所以请继续关注！</p></div></div>    
</body>
</html>