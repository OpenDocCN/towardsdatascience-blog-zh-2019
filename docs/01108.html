<html>
<head>
<title>Sentiment Preserving Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">情感保持词嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-preserving-word-embeddings-9bb5a45b2a5?source=collection_archive---------19-----------------------#2019-02-20">https://towardsdatascience.com/sentiment-preserving-word-embeddings-9bb5a45b2a5?source=collection_archive---------19-----------------------#2019-02-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/5be9a09a6f21f1aa0c0825348fafb100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GOj36I9RGBDKxzeU3wIHWw.png"/></div></div></figure><div class=""/><p id="cc96" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">单词嵌入是一种使用连续值向量将单词映射到空间中的技术，使得具有相似上下文的单词看起来彼此更接近。通常，这是通过获取大量数据，然后使用 Word2Vec 或 GloVE 或其他算法从中提取单词嵌入来完成的。这些算法有助于捕捉不同单词的语义和句法上下文，但在涉及情感时会受到很大影响。</p><p id="f685" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们看一个例子。这里我们来看单词嵌入空间中单词<strong class="ka jc">“good”</strong>的 10 个最近邻。我们使用在谷歌新闻上训练的 Word2Vec 模型，有 300 个维度。下面是查看前 10 个邻居的代码。</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="7bca" class="lf lg jb lb b gy lh li l lj lk"><em class="ll">import </em>gensim.models.keyedvectors <em class="ll">as </em>word2vec<br/><br/><br/>wordVectorModel = word2vec.KeyedVectors.load_word2vec_format("GoogleNews-vectors-negative300.bin",<br/>                                                                 binary=<em class="ll">True</em>)<br/>neighbors = list(wordVectorModel.similar_by_word("good", topn=10))<br/>print(neighbors)</span></pre><p id="faa0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这为我们提供了以下输出。</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="e4ae" class="lf lg jb lb b gy lh li l lj lk">[('great', 0.7291510105133057), ('bad', 0.7190051078796387), ('terrific', 0.6889115571975708), ('decent', 0.6837348937988281), ('nice', 0.6836092472076416), ('excellent', 0.644292950630188), ('fantastic', 0.6407778263092041), ('better', 0.6120728254318237), ('solid', 0.5806034803390503), ('lousy', 0.5764201879501343)]</span></pre><p id="e774" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从这个结果可以明显看出，当前的 Word2Vec 模型并没有真正捕获情感信息。第二个最接近“好”的词是<br/>“坏”，它们在情感上是相反的。</p><h1 id="5258" class="lm lg jb bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">解决方案</strong></h1><p id="e555" class="pw-post-body-paragraph jy jz jb ka b kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr mn kt ku kv ij bi translated">[1]为这个问题提供了一个非常新颖的解决方案。在这篇博客中，我们将讨论他们的方法，然后展示“好”的最近邻居，看看这种方法是否有效。</p><p id="a888" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种方法背后的主要思想是更新当前可用的单词嵌入，使得在单词的最近邻居中，情感上相似的单词看起来更接近。这是按照两步程序完成的。</p><h1 id="1990" class="lm lg jb bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">最近邻排序</strong></h1><p id="0830" class="pw-post-body-paragraph jy jz jb ka b kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr mn kt ku kv ij bi translated">在这一步骤中，他们使用了一种叫做 E-ANEW 的情感词典，这种词典用于研究英语单词情感规范的扩展版本。它包含大约 13000 个单词，每个单词有 1-9 分，其中 1 代表非常消极，5 代表中性，9 代表非常积极。</p><p id="3783" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，他们基于当前可用的单词嵌入选择前 K 个最近邻，并根据它们的余弦相似性对它们进行降序排列。然后根据参考词与该列表中词的情感得分的绝对差值，对该列表进行重新排序。这种重新排序促进情感上相似的单词更接近参考单词。</p><h1 id="fef7" class="lm lg jb bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">细化模型</strong></h1><p id="aa8c" class="pw-post-body-paragraph jy jz jb ka b kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr mn kt ku kv ij bi translated">在为情感词典中的所有单词创建排序列表之后，他们更新单词嵌入。本次更新是在牢记以下几点的情况下完成的。</p><ol class=""><li id="6fb8" class="mo mp jb ka b kb kc kf kg kj mq kn mr kr ms kv mt mu mv mw bi translated">更接近情感上相似的邻居</li><li id="7f3a" class="mo mp jb ka b kb mx kf my kj mz kn na kr nb kv mt mu mv mw bi translated">远离不同的邻居，和</li><li id="e7d8" class="mo mp jb ka b kb mx kf my kj mz kn na kr nb kv mt mu mv mw bi translated">离原始向量不太远。</li></ol><p id="9f29" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在更新单词 embeddings 之后，我们得到单词<strong class="ka jc">“good”的如下结果。</strong>如果您更改训练参数，这些结果可能会有所不同。</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="c1f7" class="lf lg jb lb b gy lh li l lj lk">[('astonishing', 0.9998810291290283), ('unbelievable', 0.9998570680618286), ('respectable', 0.9998552203178406), ('solid', 0.9998213648796082), ('commendable', 0.9998155236244202), ('decent', 0.9998145699501038), ('excellent', 0.9998089075088501), ('incredible', 0.9997923374176025), ('amazing', 0.9997886419296265), ('exciting', 0.9997859001159668)]</span></pre><p id="8dc1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个方法的代码可以在[2]中找到。值得注意的是，它们的词嵌入也保留了情感，可以在情感分析性能方面有很大帮助。</p><h1 id="b880" class="lm lg jb bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">参考文献</strong></h1><p id="c345" class="pw-post-body-paragraph jy jz jb ka b kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr mn kt ku kv ij bi translated">[1]<a class="ae nc" href="https://www.aclweb.org/anthology/D17-1056" rel="noopener ugc nofollow" target="_blank">https://www.aclweb.org/anthology/D17-1056</a></p><p id="d1c4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[2]https://github.com/wangjin0818/word_embedding_refine<a class="ae nc" href="https://github.com/wangjin0818/word_embedding_refine" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>