<html>
<head>
<title>Review: Tompson CVPR’15 — Spatial Dropout (Human Pose Estimation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:汤普逊·CVPR 15—空间丢失(人体姿态估计)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=collection_archive---------21-----------------------#2019-04-08">https://towardsdatascience.com/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=collection_archive---------21-----------------------#2019-04-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e293" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在空间丢失的情况下，级联的粗略和精细热图回归优于<a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">Tompson NIPS’14</a></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/2c788eda017b1480dbebe35f6b189fde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R-JraF9inc70lexJV8c8xA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">MPII-Human-Pose Dataset</strong></figcaption></figure><p id="263a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这个故事中，<strong class="kz ir"> NYU </strong>的《<strong class="kz ir">利用卷积网络</strong>进行高效的物体定位》被简要回顾。本文中的方法似乎没有简写形式。由于在 2015 年 CVPR<strong class="kz ir">的论文中，第一作者的名字是汤普逊，所以我就在标题中称之为<strong class="kz ir">汤普逊 CVPR 的 15 </strong>。你可能会注意到，这是 NYU 写的，也是 LeCun 教授的论文之一。就在上个月<strong class="kz ir">2019 年 3 月，LeCun 获得了图灵奖</strong>，与 Yoshua Bengio 和 Geoffrey Hinton 分享，其中图灵奖是<strong class="kz ir">“计算的诺贝尔奖</strong>”。而这是一篇超过<strong class="kz ir"> 300 次引用</strong>的论文。(<a class="lt lu ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----c7d6a5cecd8c--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</strong></p><p id="1da1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">人体姿态估计的目标是定位人体关节。有很多困难，比如关节遮挡，体型、服装、灯光、视角等的变化。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="69ce" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">概述</h1><ol class=""><li id="73e9" class="mu mv iq kz b la mw ld mx lg my lk mz lo na ls nb nc nd ne bi translated"><strong class="kz ir">粗略热图回归</strong></li><li id="09bc" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls nb nc nd ne bi translated"><strong class="kz ir">空间脱落</strong></li><li id="5860" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls nb nc nd ne bi translated"><strong class="kz ir">使用级联架构的精细热图回归</strong></li><li id="bc2e" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls nb nc nd ne bi translated"><strong class="kz ir">消融研究</strong></li><li id="f513" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls nb nc nd ne bi translated"><strong class="kz ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="e814" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated"><strong class="ak"> 1。粗略热图回归</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nk"><img src="../Images/6fc07dc3fcc6cd60fd93955375d468b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*78ss_2fQLzSjt5YXFyXBVA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Coarse Heat-Map Regression (Only 2 scales are shown)</strong></figcaption></figure><ul class=""><li id="2d18" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">网络是一个<strong class="kz ir">全卷积网络</strong>。</li><li id="ed9d" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">使用 3 种不同比例的输入</strong>，并输出每个关节的热图。</li><li id="a646" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">在第一层，对输入图像应用局部对比度归一化(LCN)。</li><li id="0b99" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">有 2 个数据集进行测试:FLIC 和 MPII。</li><li id="87de" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">对于 FLIC，使用 7 级卷积神经网络(CNN ),而对于 MPII，使用 11 级 CNN。</li><li id="f4f6" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">由于共用，输出图像的分辨率低于输入图像。</strong></li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a98618082cb59139d3df3e0907842901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*IVFP4pg9RRmcxzpNOFIwJg.png"/></div></figure><ul class=""><li id="c80c" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated"><strong class="kz ir">均方误差(MSE) </strong>用作损失函数，其中<em class="nq"> H'j </em>和<em class="nq"> Hj </em>是第<em class="nq"> j </em>个接头的预测和地面实况热图。</li><li id="22ea" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">训练时，随机旋转度<em class="nq">r</em>[-20，+20]，<em class="nq">s</em>[0.5，1.5]和<em class="nq"> p </em> = 0.5 的翻转版本用于数据扩充。</li></ul></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="1b2b" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">2.<strong class="ak">空间脱落</strong></h1><h2 id="4422" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">2.1.标准辍学</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c0e0557f7f3f813ad5dd9a70ecd8b33c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*XtsnKQHfHP6XrPS5k9G5bw.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Standard Dropout</strong></figcaption></figure><ul class=""><li id="8c20" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">上面两行像素表示特征图 1 和 2 的卷积核，下面一行表示前一层的输出特征。</li><li id="3fc5" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">对于标准的辍学，比如说上图右边的，虽然 f2b 被放弃了，但是强相关的 f2a 仍然存在。</li><li id="afd7" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">由于网络是一个<strong class="kz ir">全卷积网络</strong>，这使得丢失无效。</li></ul><h2 id="bd91" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">2.2.空间辍学</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/84776969ce6635129dfe73e256ef53e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*3IeJrukkpLz5NKNkWa5s4g.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">SpatialDropout</strong></figcaption></figure><ul class=""><li id="4f06" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">提出了一种新的辍学生，空间辍学生。</li><li id="0e42" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">假设有尺寸为<em class="nq"> n </em> _ <em class="nq">功勋</em> × <em class="nq">高</em> × <em class="nq">宽</em>，<strong class="kz ir">的特征地图，只进行<em class="nq">n _ 功勋</em>脱靶试验</strong>。</li><li id="d40f" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">差值跨越整个特征地图。</strong></li><li id="f6c9" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">因此，漏失特征图中的<strong class="kz ir">相邻像素要么全为 0(漏失)，要么全为活动</strong>，如上图右侧所示。</li><li id="2a30" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">如第一幅图所示，这个附加的下降层被添加在第一个 1×1 卷积层</strong>之前。</li></ul></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="c565" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated"><strong class="ak"> 3。使用级联架构的精细热图回归</strong></h1><h2 id="71f5" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">3.1.级联架构</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi of"><img src="../Images/53ae70dd17b93fc73a9f18ac9cdce51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3c0zQYSrvaxKyL51RXcqBg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Cascaded Architecture</strong></figcaption></figure><ul class=""><li id="3db9" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">这里的目标是<strong class="kz ir">恢复由于粗略热图回归模型的汇集</strong>而损失的空间精度。</li><li id="4103" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">不像<a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">深度图</a>那样单独使用<strong class="kz ir">输出热图</strong>作为精细热图回归模型的输入，来自粗糙热图回归模型的<strong class="kz ir">中间特征图也被用作输入<strong class="kz ir">。</strong></strong></li><li id="78b7" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">这可以帮助<strong class="kz ir">减少参数</strong>的数量，并作为粗糙热图模型的<strong class="kz ir">正则化器。</strong></li><li id="aba7" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">精细热图回归模型为接头生成(⇼x，⇼y)。通过在粗糙模型中添加(x，y ),我们可以获得最终结果。</li></ul><h2 id="2b44" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">3.2.裁剪模块</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2ffb6eb95178d63b0148db6191ce02e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*r_pVCiMS8bflMPAXc7O9tA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Crop Module</strong></figcaption></figure><ul class=""><li id="ffda" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">在通过精细的热图回归模型之前，热图和特征图需要通过裁剪模块。</li><li id="a85f" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">为了使<strong class="kz ir">保持窗口的上下文大小不变</strong>，在每个更高的分辨率级别执行裁剪区域的缩放。</li></ul><h2 id="f082" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">3.3.暹罗网络</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/332cbe0eaa6d40329168f4a5a5b2d8a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*FrMDs0CtjiiJhscMsSr2Ow.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">14 Joints, 14 Siamese Network in Fine Heat-Map Regression Model</strong></figcaption></figure><ul class=""><li id="c30d" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">假设有 14 个接头，我们将得到 14 个网络，有许多参数。</li><li id="dea6" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">作者采用<strong class="kz ir">连体网络</strong>，其中<strong class="kz ir">权重和偏差对于所有实例都是共享的</strong>。</li><li id="7f69" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">这可以<strong class="kz ir">减少参数</strong>的数量并<strong class="kz ir">防止过度训练</strong>。</li><li id="98f9" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">最后，使用 1×1 卷积(无任何权重分配)输出每个关节的详细分辨率热图。</strong>最后一层的目的是对每个关节进行最终检测。</li></ul><h2 id="ed6f" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">3.4.单个接头的精细热图网络</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oi"><img src="../Images/3c502d51b84bbe855eea841dd8c0fb39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6XxICkLlwpyfs_vxd3yBsg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Fine Heat-Map Network for a Single Joint</strong></figcaption></figure><ul class=""><li id="9377" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">4 条路径的 CNN 如上所示，这是单个关节的网络。</li><li id="8bc2" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">在需要的地方应用升级。</li><li id="238a" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">并且来自所有 4 条路径的所有特征图被加在一起，然后经过 1×1 卷积以获得输出。</li></ul><h2 id="f5dd" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">3.5.联合训练</h2><ul class=""><li id="cf07" class="mu mv iq kz b la mw ld mx lg my lk mz lo na ls no nc nd ne bi translated">首先，对粗略的热图模型进行预训练。</li><li id="39ae" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">然后固定粗略热图模型，并使用以下损失函数训练精细热图模型</strong>:</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c2e194a33c23c4afe91006ed5a21b40b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*lhXJb99rzqJH5tuWSH_7KQ.png"/></div></figure><ul class=""><li id="c8b4" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">其中<em class="nq"> G'j </em>和<em class="nq"> Gj </em>是第<em class="nq"> j </em>个接头的预测和地面实况热图。</li><li id="d34d" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">最后，通过最小化 E3 =<em class="nq">E1</em>+<em class="nq">λ</em>×<em class="nq">E2</em>，对粗、细模型进行联合训练</strong>，其中<em class="nq"> λ </em> = 0.1。</li></ul></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="5bd8" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">4.消融研究</h1><h2 id="6063" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">4.1.数据集</h2><ul class=""><li id="b2e2" class="mu mv iq kz b la mw ld mx lg my lk mz lo na ls no nc nd ne bi translated"><strong class="kz ir"> FLIC </strong>数据集由 3987 个训练样本和 1016 个来自好莱坞电影的静态场景测试样本组成，并标注了上身关节标签。姿势主要是面向前方和直立。</li><li id="ee41" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir"> MPII </strong>数据集包括 28，821 个训练和 11，701 个测试示例中的各种全身姿势注释。</li></ul><h2 id="4c5d" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">4.2.汇集效应</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/c015a9181a8f2404cc40780209c6da63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*rShvTuqUb18NaBJEFQMIBw.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Pooling impact on FLIC test-set Average Joint Accuracy for the coarse heat-map model</strong></figcaption></figure><ul class=""><li id="8517" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">使用的池越多，性能越差。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ol"><img src="../Images/a076f8941107e4c5ec770c991413b657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*Xyn22ueS1gM6ML8HZbp4EA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">σ of (x; y) pixel annotations on FLIC test-set images (at 360×240 resolution)</strong></figcaption></figure><ul class=""><li id="f7b3" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">测量地面实际节理的标准偏差<em class="nq"> σ </em> <strong class="kz ir"/>。</li><li id="6199" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">标签噪声(10 幅图像)</strong> : 13 个用户从 FLIC 测试集中为面部、左手腕、左肩和左肘关节标注了 10 幅随机图像。这可以当作<strong class="kz ir">人的表现</strong>。</li><li id="20ab" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated">同样，使用的池越多，性能越差。</li></ul><h2 id="6644" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">4.3.仅粗略或级联模型</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi om"><img src="../Images/a6e9a6c497375ba558f53b2653be4967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3irdq0U9dT_L3fd1PyADrw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Coarse and Fine Models</strong></figcaption></figure><ul class=""><li id="1651" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">仅使用粗略模型，预测误差(像素)分布很广。</li><li id="9427" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">使用级联模型，预测误差(像素)更窄。</strong></li></ul><h2 id="3a4b" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">4.4.级联模型的效果</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi on"><img src="../Images/490dc6b681610ec4299a8b9249fbc64c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Duqn6CrbY8eu9y89vwvyqg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Performance improvement from cascaded model</strong></figcaption></figure><ul class=""><li id="e48f" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">使用较少的池(4 倍)和级联模式，性能最佳。</li><li id="7e68" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">使用 8 倍和 16 倍池时，级联效应显著。</strong></li></ul><h2 id="3085" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">4.5.测试时间</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/19bd5c34a286b6bf5e157a642d753a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*9133xtIQKjrvJciamKFLmg.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Testing Time in Seconds</strong></figcaption></figure><ul class=""><li id="3f09" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">用的是 Nvidia-K40 GPU。</li><li id="4245" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">汇集越少(4 倍)，测试时间越长。</strong></li></ul><h2 id="02ab" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">4.6.<strong class="ak">共享特征&amp; </strong>空间缺失</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi op"><img src="../Images/3f702967b799e154c59e172616abfc95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3DArvi3FUuAlICXzhYZjg.png"/></div></div></figure><ul class=""><li id="467c" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated"><strong class="kz ir">(左)基线/标准级联</strong>:精细模型只以图像为输入，粗精模型独立训练。</li><li id="f261" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">(左)共享特征</strong>:粗模型中的特征图也作为细模型的输入。当然，<strong class="kz ir">共享特性有更好的效果。</strong></li><li id="ee2d" class="mu mv iq kz b la nf ld ng lg nh lk ni lo nj ls no nc nd ne bi translated"><strong class="kz ir">(右)有空间遗漏:</strong>检出率更高。</li></ul></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="d4a1" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">5.<strong class="ak">与最先进方法的比较</strong></h1><h2 id="4a67" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">5.1.警察</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oq"><img src="../Images/feee78f34b6227f3fda2f2b2ceea8967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUt6pv_KDi_15ZkxVy5fyw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">FLIC — FCK Performance, Average (Left) Individual Joints (Right)</strong></figcaption></figure><ul class=""><li id="2b00" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">提议的级联网络优于所有最先进的方法，包括<a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">Tompson NIPS’14</a>。</li></ul><h2 id="4a23" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">5.2.MPII</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi or"><img src="../Images/370106e0fe667a8ec4903156c5d763b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sqkv4Ior3AXbvU5L_FeA0A.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">MPII— FCK Performance, Average (Left) Individual Joints (Right)</strong></figcaption></figure><ul class=""><li id="9cdf" class="mu mv iq kz b la lb ld le lg nl lk nm lo nn ls no nc nd ne bi translated">同样，所提出的级联网络比所有最先进的方法具有更大的优势。</li></ul></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><p id="3960" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">利用级联网络对预测的关节位置进行微调，可以获得更高的检测率。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h2 id="782b" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">参考</h2><p id="4490" class="pw-post-body-paragraph kx ky iq kz b la mw jr lc ld mx ju lf lg os li lj lk ot lm ln lo ou lq lr ls ij bi translated">【2015 CVPR】【汤普森·CVPR 15】<br/><a class="ae kf" href="https://arxiv.org/abs/1411.4280" rel="noopener ugc nofollow" target="_blank">利用卷积网络的高效物体定位</a></p><h2 id="8f51" class="nr md iq bd me ns nt dn mi nu nv dp mm lg nw nx mo lk ny nz mq lo oa ob ms oc bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kx ky iq kz b la mw jr lc ld mx ju lf lg os li lj lk ot lm ln lo ou lq lr ls ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">物体检测<br/></strong><a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolo v1</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">yolo v2/yolo 9000</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">yolo v3</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a>]</p><p id="6582" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">语义切分<br/></strong><a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae kf" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生物医学图像分割<br/> </strong> [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">累计视觉 1 </a> ] [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">累计视觉 2/DCAN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>]</p><p id="3134" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">实例分割<br/></strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">Hypercolumn</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">deep mask</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">sharp mask</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">multipath net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">Instance fcn</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS</a>]</p><p id="58de" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">超分辨率<br/>T2<a class="ae kf" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">Sr CNN</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4">fsr CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f">VDSR</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener">ESPCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener">DRCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994">DRRN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8">LapSRN&amp;MS-LapSRN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8">SRDenseNet</a>]</strong></p><p id="f29d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">人体姿态估计</strong><br/><a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">深度姿态</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">汤普逊 NIPS’14</a></p></div></div>    
</body>
</html>