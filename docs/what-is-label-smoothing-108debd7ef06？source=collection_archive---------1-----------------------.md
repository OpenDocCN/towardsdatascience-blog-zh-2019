# 什么是标签平滑？

> 原文：<https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06?source=collection_archive---------1----------------------->

## 一种让你的模型不那么过于自信的技巧

![](img/46e21392720936ed385754232cba8874.png)

Photo by [Levi XU](https://unsplash.com/@xusanfeng?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

当使用深度学习模型进行分类任务时，我们通常会遇到以下问题:过度拟合和过度自信。过拟合得到了很好的研究，可以通过早期停止、放弃、权重调整等来解决。另一方面，我们对付过度自信的工具更少了。*标签平滑*是一种解决这两个问题的正则化技术。

## 过度自信和校准

如果一个分类模型的预测结果概率反映了它们的准确性，那么它就是*校准的*。例如，考虑我们数据集中的 100 个例子，每个例子的模型预测概率为 0.9。如果我们的模型被校准，那么 90 个例子应该被正确分类。类似地，在另外 100 个预测概率为 0.6 的例子中，我们预计只有 60 个例子被正确分类。

模型校准对于以下方面非常重要

*   模型的可解释性和可靠性
*   决定下游应用的决策阈值
*   将我们的模型集成到集成或机器学习管道中

过度自信的模型没有被校准，它的预测概率总是高于准确性。例如，对于精度仅为 0.6 的输入，它可能会预测 0.9。请注意，测试误差较小的模型仍然可能过于自信，因此可以从标签平滑中受益。

## 标签平滑公式

标签平滑用 *y_hot* 和均匀分布的混合代替一个热编码的标签向量 *y_hot* :

```
*y_ls* = (1 - *α*) * *y_hot* + *α* / *K*
```

其中 *K* 是标注类的数量， *α* 是确定平滑量的超参数。如果 *α* = 0，我们获得原始的一个热码编码的 *y_hot* 。如果 *α* = 1，我们得到均匀分布。

## 标签平滑的动机

当损失函数是交叉熵时，使用标签平滑，并且模型将 softmax 函数应用于倒数第二层的 logit 向量 *z* 以计算其输出概率 *p* 。在这种设置中，交叉熵损失函数相对于 logits 的梯度简单地为

```
∇CE = *p* - *y =* softmax(*z*) *- y*
```

其中 *y* 为标签分布。特别是，我们可以看到

1.  梯度下降会尽量使 *p* 接近 *y* 。
2.  梯度限制在-1 和 1 之间。

独热编码标签鼓励将最大可能的 logit 间隙输入 softmax 函数。直觉上，较大的 logit 差距与有界梯度相结合会使模型的适应性降低，并对其预测过于自信。

相比之下，平滑的标注鼓励小的 logit 间隙，如下例所示。[3]显示，这导致更好的模型校准，并防止过度自信的预测。

## 具体的例子

假设我们有 *K* = 3 个类，我们的标签属于第 1 类。设[ *a* ， *b* ， *c* 为我们的 logit 向量。

如果我们不使用标签平滑，标签向量就是独热编码向量[1，0，0]。我们的模型将使 *a* ≫ *b* 和 *a* ≫ *c* 。例如，将 softmax 应用于 logit 向量[10，0，0]会得到四舍五入到 4 位小数的[0.9999，0，0]。

如果我们使用 *α* = 0.1 的标签平滑，平滑后的标签向量≈ [0.9333，0.0333，0.0333]。logit 向量[3.3322，0，0]将平滑后的标签向量近似到 softmax 之后的 4 位小数，并且它具有更小的间隙。这就是为什么我们称标签平滑为正则化技术，因为它限制了最大的 logit 变得比其余的大得多。

## 履行

*   Tensorflow:标签平滑已经在 Tensorflow 的交叉熵损失函数中实现。参见[二元交叉熵](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)和[分类交叉熵](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy)。
*   PyTorch:参见 OpenNMT 中的[示例](https://github.com/OpenNMT/OpenNMT-py/blob/e8622eb5c6117269bb3accd8eb6f66282b5e67d9/onmt/utils/loss.py#L186)。

## 常见问题

问:我们什么时候使用标签平滑？

答:每当分类神经网络遭受过度拟合和/或过度自信时，我们可以尝试标签平滑。

问:我们如何选择 *α* ？

答:就像其他正则化超参数一样，没有选择 *α* 的公式。通常通过试错来完成，而 *α =* 0.1 是一个很好的起点。

问:我们可以在标签平滑中使用均匀分布以外的分布吗？

答:技术上来说是的。在[4]中，理论基础是为任意分布开发的。也就是说，绝大多数关于标签平滑的实证研究都使用均匀分布。

问:标签平滑在深度学习之外使用吗？

答:不尽然。大多数流行的非深度学习方法不使用 softmax 函数。因此，标签平滑通常不适用。

## 进一步阅读

1.  [3]研究了标签平滑的工作方式和原因，提供了一种新的可视化方案，并分析了标签平滑对于不同任务的性能。知识蒸馏的部分尤其有趣。
2.  [5]和[4]讨论了标签平滑如何影响损失函数及其与 KL 散度的关系。
3.  [1]第 7.5.1 章介绍了标签平滑如何帮助处理有噪声的标签。
4.  [2]介绍了*温度缩放*，这是一种简单而有效的校准神经网络的方法。

## 参考

1.  古德菲勒、本吉奥和库维尔。[深度学习](http://www.deeplearningbook.org/) (2016)，麻省理工学院出版社。
2.  C.郭，g .普莱斯，y .孙，k .温伯格。[关于现代神经网络的校准](http://proceedings.mlr.press/v70/guo17a/guo17a.pdf) (2017)，ICML 2017。
3.  R.米勒、s .科恩布利斯和 g .辛顿。[标签平滑何时有帮助？](https://papers.nips.cc/paper/8717-when-does-label-smoothing-help) (2019)，NeurIPS 2019。
4.  G.佩雷拉，g .塔克，j .乔洛夫斯基，凯泽和 g .辛顿。[通过惩罚置信输出分布来正则化神经网络](https://arxiv.org/abs/1701.06548) (2017)，arXiv。
5.  C.Szegedy、V. Vanhoucke、S. Ioffe、J. Shlens 和 Z. Wojna。[重新思考计算机视觉的初始架构](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf) (2016)，CVPR，2016。