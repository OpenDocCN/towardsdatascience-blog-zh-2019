<html>
<head>
<title>Boosting Algorithms Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">增压算法解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30?source=collection_archive---------2-----------------------#2019-06-26">https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30?source=collection_archive---------2-----------------------#2019-06-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="96ba" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理论、实现和可视化</h2></div><p id="c0a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与许多关注单个模型完成的高质量预测的 ML 模型不同，boosting 算法试图通过训练一系列弱模型来提高预测能力，每个弱模型补偿其前任的弱点。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/b92b49cfabefcc409a8756d8df18c1ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jbncjeM4CfpobEnDO0ZTjw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">One is weak, together is strong, learning from past is the best</figcaption></figure><p id="0d96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要理解 Boosting，关键是要认识到 boosting <strong class="kh ir">是一个通用算法，而不是一个特定的模型</strong>。Boosting 需要您指定一个弱模型(例如回归、浅层决策树等)，然后对其进行改进。</p><p id="3723" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解决了这个问题，是时候探索弱点的不同定义和相应的算法了。我将介绍两种主要算法:自适应增强(AdaBoost)和梯度增强。</p><h1 id="3de6" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">1.adaboost 算法</h1><h2 id="d0a1" class="mj ls iq bd lt mk ml dn lx mm mn dp mb ko mo mp md ks mq mr mf kw ms mt mh mu bi translated">1.1 弱点的定义</h2><p id="b54d" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">AdaBoost 是一种专门为分类问题开发的提升算法(也称为离散 AdaBoost)。弱点由弱估计量的误差率来识别:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi na"><img src="../Images/9dcdb764fb06a98880d7b755f5af28ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kJhONGjwCgApB5CZ2XHvIA.png"/></div></div></figure><p id="1589" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每次迭代中，AdaBoost 识别错误分类的数据点，增加它们的权重(在某种意义上，减少正确点的权重)，以便下一个分类器将额外注意使它们正确。下图说明了权重如何影响简单决策树桩(深度为 1 的树)的性能</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nb"><img src="../Images/a0fdf24731260c88fe53f2054e0d5d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CUM0izDeCIVg-qr4pAQuew.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">How sample weights affect the decision boundary</figcaption></figure><p id="7bd4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在确定了弱点，下一步是找出如何组合模型序列，使整体随着时间的推移变得更强。</p><h2 id="d844" class="mj ls iq bd lt mk ml dn lx mm mn dp mb ko mo mp md ks mq mr mf kw ms mt mh mu bi translated">1.2 伪代码</h2><p id="494e" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">研究人员提出了几种不同的算法。这里我将介绍最流行的方法 SAMME，这是一种处理多分类问题的特定方法。(【朱，】邹，S. Rosset，T. Hastie，“多级 AdaBoost”，2009 )。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nd"><img src="../Images/babdc3cb7ffa643cc3e1e614b93b305d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-j0_UzLDyEuF187-cZtyoA.png"/></div></div></figure><p id="6595" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">AdaBoost 使用增强的样本权重训练一系列模型，根据误差为各个分类器生成“置信度”系数α。低误差导致大α，这意味着在投票中更高的重要性。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ne"><img src="../Images/67948bbe7482d520a22ebfac65cee64d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TE7EZ8jQn13pxy-5Wec0yg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">the size of dots indicates their weights</figcaption></figure><h2 id="6704" class="mj ls iq bd lt mk ml dn lx mm mn dp mb ko mo mp md ks mq mr mf kw ms mt mh mu bi translated">1.3 Python 中的实现</h2><p id="eb49" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">Scikit-Learn 通过<a class="ae nc" href="https://web.stanford.edu/~hastie/Papers/samme.pdf" rel="noopener ugc nofollow" target="_blank"> SAMME </a>(多分类的特定算法)提供了 AdaBoost 的一个很好的实现。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nf ng l"/></div></figure><blockquote class="nh ni nj"><p id="b044" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">参数:<strong class="kh ir"> base_estimator </strong>:对象，可选(默认=无)</p><p id="d77f" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">构建增强系综的基本估计量。如果<code class="fe no np nq nr b">None</code>，则基本估计量为<code class="fe no np nq nr b">DecisionTreeClassifier(max_depth=1)</code></p><p id="dc86" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated"><strong class="kh ir"> n_estimators </strong>:整数，可选(默认值=50)</p><p id="512e" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">增强终止时估计器的最大数量。在完美匹配的情况下，学习过程会提前停止。</p><p id="579a" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated"><strong class="kh ir"> learning_rate </strong> : float，可选(默认=1。)</p><p id="cca2" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">学习率将每个分类器的贡献缩小<code class="fe no np nq nr b">learning_rate</code>。</p><p id="129b" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated"><strong class="kh ir">算法</strong> : {'SAMME '，' SAMME。R'}，可选(default='SAMME。r’)</p><p id="918c" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">如果是萨姆。那就用 SAMME。r 实升压算法。<code class="fe no np nq nr b">base_estimator</code>必须支持类别概率的计算。如果是“SAMME ”,则使用 SAMME 离散增强算法。</p><p id="7903" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated"><strong class="kh ir"> random_state </strong> : int，RandomState instance 或 None，可选(默认为 None)</p></blockquote><h1 id="6f68" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">2.梯度推进</h1><h2 id="7cfe" class="mj ls iq bd lt mk ml dn lx mm mn dp mb ko mo mp md ks mq mr mf kw ms mt mh mu bi translated">2.1 弱点的定义</h2><p id="b976" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">梯度推进解决问题的方式有所不同。梯度增强不是调整数据点的权重，而是关注预测和实际情况之间的差异。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ns"><img src="../Images/977f97b77863eae1e5527e33b35d7b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ob2kDwzHKrcdqbYOqHxVEA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">weakness is defined by gradients</figcaption></figure><h2 id="82e7" class="mj ls iq bd lt mk ml dn lx mm mn dp mb ko mo mp md ks mq mr mf kw ms mt mh mu bi translated">2.2 伪代码</h2><p id="203e" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">梯度增强需要微分损失函数，并且适用于回归和分类。我将使用一个简单的最小二乘法作为损失函数(用于回归)。<a class="ae nc" href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf" rel="noopener ugc nofollow" target="_blank">分类</a>的算法也有同样的想法，但是数学稍微复杂一些。(<a class="ae nc" href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank"> J. Friedman，贪婪函数逼近:梯度推进机</a>)</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nt"><img src="../Images/68bfdb8710c457000e6cbcbb08daac64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e1YXzz1b-zdD0YjTtaLdoQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Gradient Boosting with Least Square</figcaption></figure><p id="4d45" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是弱估计量 H 如何随时间推移而建立的可视化。每次我们将新的估计量(在这种情况下，max_depth =3 的回归树)拟合到损失梯度(在这种情况下，LS)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nu"><img src="../Images/e5d3bdfa1e62994dfe31e375a4f80bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZpO0FjszEdygXQW-b3s7Q.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">gradient is scaled down for visualization purpose</figcaption></figure><h2 id="abd5" class="mj ls iq bd lt mk ml dn lx mm mn dp mb ko mo mp md ks mq mr mf kw ms mt mh mu bi translated">2.3 Python 中的实现</h2><p id="af94" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">同样，你可以在 Scikit-Learn 的<a class="ae nc" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank">库</a>中找到渐变提升函数。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nf ng l"/></div></figure><blockquote class="nh ni nj"><p id="0b70" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">回归:</p><p id="02d9" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated"><strong class="kh ir">损失</strong> : {'ls '，' lad '，' huber '，'分位数' }，可选(默认='ls ')</p><p id="ee70" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">分类:</p><p id="cc4f" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated"><strong class="kh ir">损失</strong> : { '偏差'，'指数' }，可选(默认= '偏差')</p><p id="ea72" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">其余的都一样</p><p id="9094" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated"><strong class="kh ir"> learning_rate </strong> : float，可选(默认值=0.1)</p><p id="dbf7" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated"><strong class="kh ir"> n_estimators </strong> : int(默认值=100)</p><p id="8c4c" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">梯度增强对过拟合相当稳健，因此较大的数量通常会产生更好的性能。</p><p id="9159" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated"><strong class="kh ir">子样本</strong>:浮点型，可选(默认值=1.0)</p><p id="0f87" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">用于拟合单个基础学习者的样本分数。如果小于 1.0，这将导致随机梯度增强。<code class="fe no np nq nr b">subsample</code>与参数<code class="fe no np nq nr b">n_estimators</code>交互。选择<code class="fe no np nq nr b">subsample &lt; 1.0</code>会导致方差减少，偏差增加。</p><p id="d8e9" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated"><strong class="kh ir">标准</strong>:字符串，可选(default="friedman_mse ")</p><p id="e6de" class="kf kg nk kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">衡量分割质量的函数。</p></blockquote><h1 id="34e3" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">优势和劣势</h1><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nv"><img src="../Images/2a08ab92092f8beae7dc415406af1065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FMA5hpCrZW3hjgq28QTWOw.png"/></div></div></figure><ol class=""><li id="1884" class="nw nx iq kh b ki kj kl km ko ny ks nz kw oa la ob oc od oe bi translated">易于解释:boosting 本质上是一个集合模型，因此很容易解释它的预测</li><li id="6222" class="nw nx iq kh b ki of kl og ko oh ks oi kw oj la ob oc od oe bi translated">预测能力强:通常 boosting &gt; bagging (random forrest)&gt;决策树</li><li id="26ab" class="nw nx iq kh b ki of kl og ko oh ks oi kw oj la ob oc od oe bi translated">适应过度拟合:<a class="ae nc" href="https://jeremykun.com/2015/09/21/the-boosting-margin-or-why-boosting-doesnt-overfit/" rel="noopener ugc nofollow" target="_blank">见本文</a></li><li id="d1cc" class="nw nx iq kh b ki of kl og ko oh ks oi kw oj la ob oc od oe bi translated">对异常值敏感:由于每个弱分类器都致力于修复其前任的缺点，因此该模型可能会过于关注异常值</li><li id="23df" class="nw nx iq kh b ki of kl og ko oh ks oi kw oj la ob oc od oe bi translated">难以扩展:因为每个评估器都是建立在它的前辈之上的，所以这个过程很难并行化。</li></ol><h1 id="9e39" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated"><strong class="ak">总结</strong></h1><p id="6f1d" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">助推算法代表了一种不同的机器学习视角:将一个弱模型变成一个更强的模型，以修复其弱点。现在您已经了解了 boosting 的工作原理，是时候在实际项目中尝试一下了！</p></div></div>    
</body>
</html>