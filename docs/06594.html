<html>
<head>
<title>MLE, MAP and Bayesian Inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大似然估计、映射和贝叶斯推理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mle-map-and-bayesian-inference-3407b2d6d4d9?source=collection_archive---------0-----------------------#2019-09-21">https://towardsdatascience.com/mle-map-and-bayesian-inference-3407b2d6d4d9?source=collection_archive---------0-----------------------#2019-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d26a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过关注与最大似然法和映射的区别，掌握贝叶斯推理的思想</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e19e8514a77d262f5fc8b93394b37009.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-RNgyg8IC1BH8XiA"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@aysha_be?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Aysha Begum</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d06a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最大似然法、映射和贝叶斯推断是推断观测数据背后的概率分布特性的方法。话虽如此，MLE/MAP 和贝叶斯推断还是有很大区别的。</p><p id="9277" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将通过关注 MLE/MAP 和贝叶斯推理之间的差异来介绍贝叶斯推理。</p><p id="6d1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>本文假设了 MLE 和 MAP 的初步知识。如果你不熟悉这些方法，请参考下面的文章。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/a-gentle-introduction-to-maximum-likelihood-estimation-and-maximum-a-posteriori-estimation-d7c318f9d22d"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">最大似然估计和最大后验估计简介</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">以足球为例获得最大似然法和地图的直觉</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="e15d" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated">最大似然估计和贝叶斯推理的区别</h2><p id="0003" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">让我们从 MLE 和 MAP 的重述开始。<br/>给定观测数据<em class="ns"> D </em>，MLE 和 MAP 对概率模型参数<em class="ns"> θ </em>的估计如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/a027d6f2209122d03137457d16958b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1yjWNRh0IYBvqOL611Cwg.png"/></div></div></figure><p id="65aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MLE 给出了使可能性最大的值<em class="ns"> P(D|θ) </em>。MAP 给出了使后验概率最大化的值<em class="ns"> P(θ|D) </em>。由于两种方法都给你一个固定值，它们被认为是<strong class="lb iu">点估计量</strong>。</p><p id="afeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，贝叶斯推理完全计算后验概率分布，如下式。因此，输出不是单个值，而是概率密度函数(当<em class="ns"> θ </em>是连续变量时)或概率质量函数(当<em class="ns"> θ </em>是离散变量时)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ad6ff82b6cda180add2c12e3cb37bdea.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*i8C2LVtU7OxpEY0nXsHSwQ.png"/></div></figure><p id="0952" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是 MLE/MAP 和贝叶斯推断的区别。MLE 和 MAP 返回单个固定值，但贝叶斯推断返回概率密度(或质量)函数。</p><p id="9b66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是为什么我们甚至需要完全计算分布，当我们有 MLE 和 MAP 来确定<em class="ns"> θ </em>的值时？<em class="ns"> </em>要回答这个问题，让我们看看 MAP(和其他点估计器)不好用的情况。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="8320" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated">地图(或一般意义上的点估计器)不能很好工作的情况</h2><p id="035b" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">假设你在一个充满老虎机的赌场，赢的概率为<em class="ns"> 50% </em>。玩了一会儿后，你听到谣言说有一个特殊的吃角子老虎机有 67% 的获胜概率。现在，你正在观察人们玩两个可疑的吃角子老虎机(你确定其中一个是特殊的吃角子老虎机！)并得到了以下数据。</p><p id="6c70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">A 机:4 局 3 胜</em><br/><em class="ns">B 机:121 局 81 胜</em></p><p id="180e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">凭直觉，你会认为<em class="ns">机 B </em>是特别的那一个。因为在<em class="ns">机器上 4 次游戏中有 3 次获胜，所以</em>可能只是偶然发生。但是<em class="ns">机 B </em>的数据看起来不像是偶然发生的。</p><p id="1994" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是为了以防万一，您决定通过使用超参数<em class="ns"> α=β=2 </em>的 MAP 来估计这两台机器的获胜概率。(假设结果(<em class="ns"> k </em>在<em class="ns"> n </em>次游戏中获胜)遵循二项式分布，吃角子老虎机的获胜概率<em class="ns"> θ </em>作为其参数。)</p><p id="615e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">公式和结果如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5d231e471076be017778fb58996c7254.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*6oSNjIcAYzuJgHg2L89E7g.png"/></div></figure><p id="04be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">甲机:(3+2–1)/(4+2+2–2)= 4/6 =</em><strong class="lb iu"><em class="ns">66.7%</em></strong><em class="ns"><br/>乙机:(81+2–1)/(121+2+2–2)= 82/123 =</em><strong class="lb iu"><em class="ns">66.7%</em></strong></p><p id="4d3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与你的直觉不同，通过地图估算的两台机器的获胜概率<em class="ns"> θ </em>完全相同。因此，通过地图，你不能确定哪一个是特殊的老虎机。</p><p id="46a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是真的吗？难道看起来不明显的是<em class="ns">机器 B </em>更有可能是特殊的一个吗？</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="51bb" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated">贝叶斯推理</h2><p id="2075" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">为了看看<em class="ns">机器 A </em>和<em class="ns">机器 B </em>之间是否真的没有区别，让我们全面计算后验概率分布，而不仅仅是地图估计。</p><p id="a397" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上述情况下，后验概率分布<em class="ns"> P(θ|D) </em>计算如下。(详细的计算将在下一节介绍。)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/510a41645826d0de22dcf2c69ed3b087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYOIZME1GOBB92ngo91HFQ.png"/></div></div></figure><p id="d11b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">A 机</em>和<em class="ns">B 机</em>的<em class="ns"> P(θ|D) </em>绘制如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/06486afc9166aab85431a64d467171b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8zncCAahVKwAW0af7nl1BA.png"/></div></div></figure><p id="c129" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然两种分布在<em class="ns"> θ=0.666… </em>上都有其<strong class="lb iu"> <em class="ns">模态</em> </strong>(这就是为什么它们的 MAP 估计值相同)，但是分布的形状却大不相同。在<em class="ns">机 B </em>的分布中模式周围的密度远远高于<em class="ns">机 A </em>的分布。这就是为什么你想计算完整的分布，而不仅仅是地图估计。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="2fa3" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated">贝叶斯推理的计算</h2><p id="1ddc" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">由于我们在上一节中跳过了对<em class="ns"> P(θ|D) </em>的计算，所以我们在本节中来看看详细的计算过程。</p><p id="1644" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">地图和贝叶斯推理都是基于贝叶斯定理。贝叶斯推理和 MAP 的计算区别在于，在贝叶斯推理中，我们需要计算<em class="ns"> P(D) </em>称为<strong class="lb iu">边际似然</strong>或<strong class="lb iu">证据</strong>。这是贝叶斯定理的分母，它确保<em class="ns"> P(θ|D) </em>在所有可能的<em class="ns"> θ </em>上的积分值*变为 1。(*若<em class="ns"> θ </em>为离散变量，则为<em class="ns"> P(θ|D) </em>之和。)</p><p id="2929" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns"> P(D) </em>是通过联合概率的边缘化得到的。当<em class="ns"> θ </em>为连续变量时，公式如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/38f336904a7a8b782a9ab1b9e3388176.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*tPtb9ZC-WVQrkssaatOQqw.png"/></div></figure><p id="f083" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到<a class="ae ky" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)" rel="noopener ugc nofollow" target="_blank">链式法则</a>，我们得到以下公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/b8e86c86cafffc444630b5accbff2134.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*sIg-EvCyaT_-wIUK4Cf2IQ.png"/></div></div></figure><p id="d197" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，把这个放到后验概率分布的原始公式中。下面计算是贝叶斯推理的目标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/9ced81e797de8ce4d95d4cb1c322c2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*LR-Nb8jAmdVPEORf2rkCdg.png"/></div></figure></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="e2da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们为上面的情况计算一下<em class="ns"> P(θ|D) </em>。</p><p id="1e63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从<em class="ns"> P(D|θ) </em> — <strong class="lb iu">似然</strong> —即给定参数<em class="ns"> θ </em>时，观察到数据<em class="ns"> D </em>的概率。上例中，<em class="ns"> D </em>为“<em class="ns">4 局 3 胜”</em>，参数<em class="ns"> θ </em>为<em class="ns">机 A </em>的获胜概率。由于我们假设胜数服从二项式分布，公式如下，其中<em class="ns"> n </em>为匹配数，<em class="ns"> k </em>为胜数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/6b2d9f8512d8fb7c71bcc7881b926ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*8ouhYclbly_NBnRREZ_d8A.png"/></div></figure><p id="0467" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么<em class="ns">P(θ)</em>—<em class="ns">θ—</em>的<strong class="lb iu">先验概率分布</strong>就是表达我们对<em class="ns"> θ </em>的先验知识的概率分布。这里，对应于可能性<em class="ns"> P(D|θ) </em>的概率分布，使用特定的概率分布。叫做<a class="ae ky" rel="noopener" target="_blank" href="/a-gentle-introduction-to-maximum-likelihood-estimation-and-maximum-a-posteriori-estimation-d7c318f9d22d">共轭先验分布</a>。</p><p id="20c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于二项分布的共轭先验是贝塔分布，所以我们这里用贝塔分布来表示<em class="ns"> P(θ) </em>。贝塔分布描述如下，其中<em class="ns"> α </em>和<em class="ns"> β </em>为超参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/301b9f6f57d1bc737814d27e2a7ba072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5QkPiTyCK6Ns6nMfrV4OEQ.png"/></div></div></figure><p id="c3cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们得到了<em class="ns">P(D |θ)P(θ)——</em>公式的分子——如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/01288ef69e31ab456ad691dcf1d9e0da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*apbMK3YGujmqFsbe0WmE_Q.png"/></div></div></figure><p id="d548" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，<em class="ns">P(D)</em>——公式的分母——计算如下。注意<em class="ns"> θ </em>的可能范围是 0 ≤ <em class="ns"> θ ≤ 1。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/753db27e18f09e0694d09a38a5297974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nm1nuOHEKstalnaUQar3Yw.png"/></div></div></figure><p id="47a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了第一类<a class="ae ky" href="https://en.wikipedia.org/wiki/Euler_integral" rel="noopener ugc nofollow" target="_blank">欧拉积分</a>，上面的公式可以变形为下面。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/765c4e8a5e8b0f8baa86ea3604c43739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*JReB_7TGCTnkOoJHu2ALyg.png"/></div></figure><p id="3a3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后我们可以得到<em class="ns"> P(θ|D) </em>如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/dce3f4be082c504ee02df3638e018fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*9yAUTXbCzILvggoQQHgRPA.png"/></div></div></figure></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="8a98" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated">预期后验概率</h2><p id="54d5" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">你可能已经注意到了，MAP 估计的是后验分布的<strong class="lb iu"> <em class="ns">模</em> </strong>。但是我们也可以使用其他统计量进行点估计，比如θ|D 的<strong class="lb iu"> <em class="ns">期望值</em> </strong> <em class="ns">。使用<em class="ns"> θ|D </em>的期望值的估计称为<strong class="lb iu">期望后验</strong>。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/2ade53330eb3bc5c0b0fd7c1b0e1fcfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*ugXCxn4rNhzWG6P6rNzbyQ.png"/></div></figure></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="7a5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用 EAP 估计一下 2 台机器的中奖概率。从上面的讨论来看，这种情况下的<em class="ns"> P(θ|D) </em>低于<em class="ns">。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/1ed9019b69f69d4f2989e33fceb0822a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GlMPmkg2n_auQ3UIbPXVTQ.png"/></div></div></figure><p id="fd9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，估计如下所述。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/5689b41126763a6c08b53a20ae97ca17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WF-YMY2sNdwyIx31tkaavA.png"/></div></div></figure><p id="6133" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用第一类欧拉积分<a class="ae ky" href="https://en.wikipedia.org/wiki/Euler_integral" rel="noopener ugc nofollow" target="_blank">和</a><a class="ae ky" href="https://en.wikipedia.org/wiki/Gamma_function" rel="noopener ugc nofollow" target="_blank">伽玛函数</a>的定义，上述公式可以变形为以下公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/dc2956e721286200fc07abec0ab34554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WgzVwM4B33zYKm0oIme2Qw.png"/></div></div></figure><p id="0503" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在超参数<em class="ns"> α=β=2 </em>下，EAP 对 2 台机器获胜概率的估计低于<em class="ns">。</em></p><p id="2542" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">甲机:(3+2)/(4+2+2)= 5/8 =</em><strong class="lb iu"><em class="ns">62.5%</em></strong><em class="ns"><br/>乙机:(81+2)/(121+2+2)= 83/125 =</em><strong class="lb iu"><em class="ns">66.4%</em></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/c4e191e85c2d9fadaf4f35df1db8e1b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4fqCVVUkYA2eWjcwl1t7WA.png"/></div></div></figure></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="cabc" class="mu mv it bd mw mx my dn mz na nb dp nc li nd ne nf lm ng nh ni lq nj nk nl nm bi translated">结论</h2><p id="53c9" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">如上所述，贝叶斯推断比点估计(如 MLE 和 MAP)提供了更多的信息。然而，它也有一个缺点——积分计算的复杂性。本文中的情况非常简单，并且通过分析得到了解决，但是在现实世界的应用程序中并不总是这样。然后，我们需要使用 MCMC 或其他算法来代替直接积分计算。<br/>希望这篇文章能帮助你理解贝叶斯推断。</p></div></div>    
</body>
</html>