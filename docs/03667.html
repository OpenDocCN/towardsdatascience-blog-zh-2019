<html>
<head>
<title>Deep Neural Networks from scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中从零开始的深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-neural-networks-from-scratch-in-python-451f07999373?source=collection_archive---------4-----------------------#2019-06-11">https://towardsdatascience.com/deep-neural-networks-from-scratch-in-python-451f07999373?source=collection_archive---------4-----------------------#2019-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="1c06" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本指南中，我们将构建一个深度神经网络，你想要多少层就有多少层！该网络可应用于二分类的监督学习问题。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/4720d1fe6c63521064ee891bcd77f8f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zTBAO0amYqZs2DMhLCrBnA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Figure 1.</strong> Example of neural network architecture</figcaption></figure><h2 id="936e" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">注释</h2><p id="e215" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">上标[ <em class="md"> l </em>表示与<em class="md"> l </em> ᵗʰ层相关的量。</p><p id="1070" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上标(<em class="md"> i </em>)表示与<em class="md"> i </em> ᵗʰ示例相关的量。</p><p id="544d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Lowerscript <em class="md"> i </em>表示向量的<em class="md"> i </em> ᵗʰ条目。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="963b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="md">本文假设读者已经熟悉神经网络的概念。不然我推荐看这个好看的介绍</em><a class="ae ml" rel="noopener" target="_blank" href="/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6"><em class="md">https://towardsdatascience . com/how-to-build-your-own-neural-network-from-scratch-in-python-68998 a08e 4 F6</em></a></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="fa77" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">单个神经元</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mm"><img src="../Images/8611b9ea6ac917977dfc0d83144e98a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VjDk47JJvkmac9nogfXQ3w.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Figure 2.</strong> Example of single neuron representation</figcaption></figure><p id="9c7a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经元计算一个线性函数(z = Wx + b ),然后是一个激活函数。我们一般说一个神经元的输出是 a = g(Wx + b)其中 g 是激活函数(sigmoid，tanh，ReLU，…)。</p><h2 id="e5ba" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">资料组</h2><p id="84e9" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">让我们假设我们有一个非常大的数据集，其中包含温度、湿度、大气压力和降雨概率等天气数据。</p><p id="ffc5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">问题陈述:</p><ul class=""><li id="8315" class="mn mo it js b jt ju jx jy kb mp kf mq kj mr kn ms mt mu mv bi translated">标记为下雨(1)或不下雨(0)的 m_train 天气数据的训练集</li><li id="cd07" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">标记为下雨或不下雨的 m_test 天气数据的测试集</li><li id="1e66" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">每个天气数据包括 x1 =温度，x2 =湿度，x3 =大气压力</li></ul><p id="a3e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">机器学习中一个常见的预处理步骤是对数据集进行居中和标准化，这意味着您从每个示例中减去整个 numpy 数组的平均值，然后用整个 numpy 数组的标准偏差除每个示例。</p><div class="nb nc gp gr nd ne"><a href="https://en.wikipedia.org/wiki/Standard_deviation" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">标准偏差-维基百科</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">随机变量、统计总体、数据集或概率分布的标准偏差是…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">en.wikipedia.org</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ky ne"/></div></div></a></div><h1 id="b118" class="nt lg it bd lh nu nv nw lk nx ny nz ln oa ob oc lq od oe of lt og oh oi lw oj bi translated">一般方法(构建我们算法的各个部分)</h1><p id="112a" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">我们将遵循深度学习方法来构建模型:</p><ol class=""><li id="4b2f" class="mn mo it js b jt ju jx jy kb mp kf mq kj mr kn ok mt mu mv bi translated">定义模型结构(如输入要素的数量)</li><li id="9886" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ok mt mu mv bi translated">初始化参数并定义超参数:</li></ol><ul class=""><li id="d095" class="mn mo it js b jt ju jx jy kb mp kf mq kj mr kn ms mt mu mv bi translated">迭代次数</li><li id="b1a6" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">神经网络的层数 L</li><li id="f679" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">隐藏层的大小</li><li id="0177" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">学习率α</li></ul><p id="93b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.num_iterations 的循环:</p><ul class=""><li id="d647" class="mn mo it js b jt ju jx jy kb mp kf mq kj mr kn ms mt mu mv bi translated">正向传播(计算电流损耗)</li><li id="11d5" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">计算成本函数</li><li id="af0f" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">反向传播(计算电流梯度)</li><li id="c31b" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">更新参数(使用参数和来自反向投影的梯度)</li></ul><p id="1197" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.使用训练好的参数来预测标签</p><h2 id="77dd" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">初始化</h2><p id="3a44" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">更深的 L 层神经网络的初始化更复杂，因为有更多的权重矩阵和偏置向量。我提供下面的表格是为了帮助你保持正确的结构尺寸。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/23c864610ee691c3c1a04f0decaffa99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MVw7SgoHpVhheJUDltXKeQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Table 1.</strong> Dimensions of weight matrix <strong class="bd le">W, </strong>bias vector <strong class="bd le">b </strong>and<strong class="bd le"> </strong>activation<strong class="bd le"> Z </strong>for layer l</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/3a3019d0ce1ad06206100c423c2b510d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l7mq3P3WLcF0jhYuFTxMOQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Table 2.</strong> Dimensions of weight matrix <strong class="bd le">W, </strong>bias vector <strong class="bd le">b </strong>and<strong class="bd le"> </strong>activation<strong class="bd le"> Z </strong>for the neural network for our example architecture</figcaption></figure><p id="9703" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">表 2 帮助我们为图 1 中示例神经网络架构的矩阵准备正确的维度。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="ak">Snippet 1.</strong> Initialization of the parameters</figcaption></figure><p id="6c56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用小随机数进行参数初始化是一种简单的方法，但它保证了我们的算法有足够好的起点。</p><p id="498a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请记住:</p><ul class=""><li id="7030" class="mn mo it js b jt ju jx jy kb mp kf mq kj mr kn ms mt mu mv bi translated">不同的初始化技术，如零，随机，他或泽维尔导致不同的结果</li><li id="6098" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">随机初始化确保不同的隐藏单元可以学习不同的东西(将所有的权重初始化为零，这样每一层中的每个神经元都将学习相同的东西)</li><li id="7bc3" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">不要初始化太大的值</li></ul><h2 id="4617" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">激活功能</h2><p id="b893" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">激活函数使神经网络具有非线性。在我们的例子中，我们将使用 sigmoid 和 ReLU。</p><p id="399e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Sigmoid 输出一个介于 0 和 1 之间的值，这使它成为二进制分类的一个非常好的选择。如果小于 0.5，可以将输出分类为 0，如果大于 0.5，可以将输出分类为 1。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="ak">Snippet 2.</strong> Sigmoid and ReLU activation functions and their derivatives</figcaption></figure><p id="eccc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在代码片段 2 中，你可以看到激活函数及其导数的矢量化实现(<a class="ae ml" href="https://en.wikipedia.org/wiki/Derivative" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Derivative</a>)。该代码将用于进一步的计算。</p><h2 id="ac99" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">正向传播</h2><p id="3b7f" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">在正向传播过程中，在层的正向函数中<em class="md"> l </em>你需要知道层中的激活函数是什么(Sigmoid，tanh，ReLU 等。).给定来自前一层的输入信号，我们计算 Z，然后应用选定的激活函数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/9218de28708e2479396d41f926c0ec5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0Dh541brmx0lmi1QuEBdA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Figure 3.</strong> Forward propagation for our example neural network</figcaption></figure><p id="0536" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">线性正向模块(对所有示例进行矢量化)计算以下等式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi op"><img src="../Images/4b4615d47bf11bf668bfcc77b2684a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OyXH8CRyJpAjBVScbTO-jw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Equation 1.</strong> Linear forward function</figcaption></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="ak">Snippet 3.</strong> Forward propagation module</figcaption></figure><p id="ee5f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们使用“缓存”(Python 字典，其中包含为特定层计算的<strong class="js iu"> A </strong>和<strong class="js iu"> Z </strong>值)将向前传播期间计算的变量传递给相应的向后传播步骤。它包含向后传播计算导数的有用值。</p><h2 id="6bca" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">损失函数</h2><p id="c8b9" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">为了监控学习过程，我们需要计算成本函数的值。我们将使用下面的公式来计算成本。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/de5cbfd381970ff652594dee51d63ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kGpmkl6kDlO2RFYJArYYCg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Equation 2.</strong> Cross-entropy cost</figcaption></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="ak">Snippet 4.</strong> Computation of the cost function</figcaption></figure><h2 id="42d7" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">反向传播</h2><p id="b391" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">反向传播用于计算损失函数相对于参数的梯度。这个算法是微积分中已知的“链式法则”的递归使用。</p><p id="6c22" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">反向传播计算中使用的方程:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi or"><img src="../Images/b7048b15e56965005a9a80e8c26e25b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*niJ4X6MEAOu6ifkHqoeBgA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Equation 3.</strong> Formulas for backward propagation calculation</figcaption></figure><p id="4e7a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">链式法则是计算复合函数导数的公式。复合函数是由其他函数内部的函数组成的函数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/5c50a95beee8e6fe8480f61f2349f266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dsbuSoKc1MnDZNe4TGrxyg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Equation 4.</strong> Chain rule examples</figcaption></figure><p id="2cc4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">没有“链式法则”(以方程 5 为例)，很难计算损耗。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ot"><img src="../Images/65435bb2e96ebb1693b4b358af7f6ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQuoms-2J3t5XQ1Mp6KsYQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Equation 5.</strong> Loss function (with substituted data) and its derivative with respect to the first weight.</figcaption></figure><p id="4114" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的神经网络模型的反向传播的第一步是从最后一层计算我们的损失函数相对于 Z 的导数。等式 6 包括两个分量，来自等式 2 的损失函数的导数(相对于激活函数)和来自最后一层的激活函数“sigmoid”相对于 Z 的导数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/05a67f8477e81a8827df1945d1fe75cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRgG196cZCi2MmF3K9cqhA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Equation 6.</strong> The derivative of the loss function with respect to Z from 4ᵗʰ layer</figcaption></figure><p id="38be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">等式 6 的结果可用于计算等式 3 的导数:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/240ed6d2a164e3336828602207bfcc15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ix9yyAacEQkyBRt3JFVknw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Equation 7.</strong> The derivative of the loss function with respect to A from 3ᵗʰ layer</figcaption></figure><p id="1de2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">损失函数相对于来自第三层的激活函数的导数(等式 7)用于进一步的计算。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/d4f0ee0736ea18094383a6a609564d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9g-S6OPSIC5oq-POYEhHtg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Equation 8. </strong>The derivatives for the third layer</figcaption></figure><p id="c8f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">等式 7 的结果和来自第三层的激活函数“ReLU”的导数用于计算等式 8 的导数(损失函数相对于 Z 的导数)。接下来，我们对等式 3 进行计算。</p><p id="9f78" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们对等式 9 和 10 进行类似的计算。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/55a47b379772fbf7d460657633eb4b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jf7HGxH7OngkaxQcvsuWkg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Equation 9. </strong>The derivatives for the second layer</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/cbc23fe2bfe203cd1bc55b3ae0a4961d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60oYEXOzLYi9dScE-8dp5Q.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Equation 10. </strong>The derivatives for the first layer</figcaption></figure><p id="eba2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总的想法是:</p><p id="f7a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">来自 lᵗʰ层的损失函数相对于 z 的导数有助于计算来自 l-1)ᵗʰ层(前一层)的损失函数相对于 a 的导数。然后将结果与激活函数的导数一起使用。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oz"><img src="../Images/fe360fb22eea4e807a4d832faff442f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pd9Ot-HxjFo1N-Ol2oGJmg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le">Figure 4.</strong> Backward propagation for our example neural network</figcaption></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="ak">Snippet 5.</strong> Backward propagation module</figcaption></figure><h2 id="dd68" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">更新参数</h2><p id="1904" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">该函数的目标是使用梯度优化来更新模型的参数。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="ak">Snippet 6.</strong> Updating parameters values using gradient descent</figcaption></figure><h2 id="f188" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">全模型</h2><p id="e5be" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">神经网络模型的完整实现由片段中提供的方法组成。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="ak">Snippet 7.</strong> The full model of the neural network</figcaption></figure><p id="2877" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了进行预测，您只需要使用接收到的权重矩阵和一组测试数据运行一个完整的前向传播。</p><p id="5961" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以修改<strong class="js iu">片段 1 </strong>中的<em class="md"> nn_architecture </em>来构建一个具有不同层数和隐藏层大小的神经网络。此外，准备激活函数及其派生函数的正确实现(<strong class="js iu">片段 2 </strong>)。实现的函数可以用来修改<strong class="js iu">片段 3 </strong>中的<em class="md">linear _ activation _ forward</em>方法和<strong class="js iu">片段 5 </strong>中的<em class="md">linear _ activation _ backward</em>方法。</p><h1 id="1d11" class="nt lg it bd lh nu nv nw lk nx ny nz ln oa ob oc lq od oe of lt og oh oi lw oj bi translated">进一步的改进</h1><p id="79d6" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">如果训练数据集不够大，您可能会面临“过拟合”问题。这意味着学习过的网络不会归纳出它从未见过的新例子。你可以使用<strong class="js iu">正则化</strong>方法，比如<strong class="js iu"> L2 正则化</strong>(它包括适当修改你的<br/>代价函数)<strong class="js iu"> </strong>或者<strong class="js iu">丢弃</strong>(它在每次迭代中随机关闭一些神经元)。</p><p id="2f01" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们使用<strong class="js iu">梯度下降</strong>来更新参数并最小化成本。您可以学习更高级的优化方法，这些方法可以加快学习速度，甚至让您获得更好的成本函数最终值，例如:</p><ul class=""><li id="bd7b" class="mn mo it js b jt ju jx jy kb mp kf mq kj mr kn ms mt mu mv bi translated">小批量梯度下降</li><li id="74ca" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">动力</li><li id="daad" class="mn mo it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">Adam 优化器</li></ul></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="68e4" class="nt lg it bd lh nu pa nw lk nx pb nz ln oa pc oc lq od pd of lt og pe oi lw oj bi translated">参考资料:</h1><p id="e8db" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">[1]<a class="ae ml" href="https://www.coursera.org/learn/neural-networks-deep-learning" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/neural-networks-deep-learning</a></p><p id="3020" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ml" href="https://www.coursera.org/learn/deep-neural-network" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/learn/deep-neural-network</a></p><p id="2b06" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3]<a class="ae ml" href="https://ml-cheatsheet.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">https://ml-cheatsheet.readthedocs.io/en/latest/index.html</a></p><p id="8132" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[4]<a class="ae ml" href="https://medium.com/towards-artificial-intelligence/one-lego-at-a-time-explaining-the-math-of-how-neural-networks-learn-with-implementation-from-scratch-39144a1cf80" rel="noopener">https://medium . com/forward-artificial-intelligence/one-Lego-at-a-time-explain-the-math-of-how-neural-networks-learn-implementation-from scratch-39144 a1cf 80</a></p><p id="b31c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[5]<a class="ae ml" rel="noopener" target="_blank" href="/gradient-descent-in-a-nutshell-eaf8c18212f0">https://towards data science . com/gradient-descent-in-a-shell-EAF 8c 18212 f 0</a></p><p id="d163" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[6]<a class="ae ml" href="https://medium.com/datadriveninvestor/math-neural-network-from-scratch-in-python-d6da9f29ce65" rel="noopener">https://medium . com/datadriveninvestor/math-neural-network-from-scratch-in-python-d6da 9 f 29 ce 65</a></p><p id="1769" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[7]<a class="ae ml" rel="noopener" target="_blank" href="/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6">https://towards data science . com/how-to-build-your-own-your-own-neural-network-from scratch-in-python-68998 a08e 4 F6</a></p></div></div>    
</body>
</html>