<html>
<head>
<title>Everything you need to know about Graph Theory for Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于深度学习的图论，你需要知道的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-theory-and-deep-learning-know-hows-6556b0e9891b?source=collection_archive---------2-----------------------#2019-04-23">https://towardsdatascience.com/graph-theory-and-deep-learning-know-hows-6556b0e9891b?source=collection_archive---------2-----------------------#2019-04-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="74a4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">图形学习和几何深度学习—第 0 部分</h2></div><blockquote class="kf kg kh"><p id="6ad3" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">预测未来的最好方法是创造未来——亚伯拉罕·林肯</p><p id="6dc0" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">关注<a class="ae lf" href="https://twitter.com/FlawnsonTong" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir">我的推特</strong> </a>加入<a class="ae lf" href="https://www.reddit.com/r/GeometricDeepLearning/" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir">几何深度学习子编辑</strong> </a>获取空间最新更新。</p></blockquote><p id="599b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi lj translated">今天，机器学习风靡一时，一旦科学赶上宣传，它将很可能成为我们生活中的一种常态。我们达到下一步的方法之一是一种新形式的深度学习；几何深度学习。点击阅读灵感和创意<a class="ae lf" href="https://medium.com/@flawnsontong1/what-is-geometric-deep-learning-b2adb662d91d" rel="noopener">。这个系列的重点是我们如何在图上使用深度学习</a></p><p id="d703" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">理解图学习所需的两个先决条件就在名称本身；<strong class="kl ir">图论和深度学习</strong>。这就是你所需要知道的，来理解的本质，并为这两个想法建立一个高层次的直觉。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/8f9204390b1c2ae9aea8f84a94b72c7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*SujGujiFHbCz11e4dIS1_g.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Progress in A.I means more cute robots (Courtesy of Techgyd)</figcaption></figure><h1 id="a6a1" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">图论—速成班</h1><h2 id="3741" class="mw mf iq bd mg mx my dn mk mz na dp mo lg nb nc mq lh nd ne ms li nf ng mu nh bi translated">什么是图？</h2><p id="af21" class="pw-post-body-paragraph ki kj iq kl b km ni jr ko kp nj ju kr lg nk ku kv lh nl ky kz li nm lc ld le ij bi translated">在图论的上下文中，<strong class="kl ir">图</strong>是一种结构化的数据类型，具有<strong class="kl ir">节点</strong>(保存信息的实体)和<strong class="kl ir">边</strong>(也可以保存信息的节点之间的连接)。图表是一种组织数据的方式，但它本身也可以是一个数据点。图表是一种<strong class="kl ir">非欧几里得数据</strong>，这意味着它们以 3D 形式存在，不像图像、文本和音频等其他数据类型。图表可以具有某些属性，这些属性限制了可以对其执行的可能操作和分析。这些属性是可以定义的。</p><h2 id="da03" class="mw mf iq bd mg mx my dn mk mz na dp mo lg nb nc mq lh nd ne ms li nf ng mu nh bi translated">图形定义</h2><p id="cb2b" class="pw-post-body-paragraph ki kj iq kl b km ni jr ko kp nj ju kr lg nk ku kv lh nl ky kz li nm lc ld le ij bi translated">首先让我们来看一些定义，这是我一生中最简单的 Photoshop 工作带给你的。</p><p id="e73b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">在计算机科学中，我们经常谈论一种叫做<strong class="kl ir">图的数据结构:</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/27ccdedcdeb1eb5042628e85b65f77c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aGVjUth3UyBzlvin_ZZtwA.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">He’s cute, let’s call him Graham</figcaption></figure><p id="3281" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">图可以在它们的边和/或节点上有<strong class="kl ir">标签</strong>，让我们给 Graham 一些边和节点标签。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/e989a0b9bc29d4bb2a601b601f3c76c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_-vbnIxHY_zHsGgYivm-w.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Graham looks rather dashing in his new attire</figcaption></figure><p id="a53a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">标签也可以被认为是权重，但这取决于图表的设计者。</p><p id="ace9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><strong class="kl ir">标签不一定是数字</strong>，也可以是文字。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/b81999be1ed0239dca637b85befa2284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RDLJmU0RSFDv5RsS_eFfQA.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Graham also likes memes</figcaption></figure><p id="8112" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><strong class="kl ir">标签不必唯一</strong>；给多个节点相同的标签是完全可能的，有时也是有用的。以氢分子为例:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/24be562d542057458d0cf95c7d997bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j4NJBZ_MVnsH3SfKa76HRQ.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Notice the mix of numerical and textual datatypes</figcaption></figure><p id="02ba" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">图形可以有<strong class="kl ir">特征</strong>(也称为属性)<strong class="kl ir">。</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/0115b139fea8dc686e99842e9d8e4f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NpJshvW7HlJ_6nJ69mtHAw.png"/></div></div></figure><p id="bf6d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">注意不要混淆特征和标签。一种简单的思考方式是用名字、人物和人来类比:</p><blockquote class="ns"><p id="c2d8" class="nt nu iq bd nv nw nx ny nz oa ob le dk translated">节点是一个人，节点的标签是一个人的名字，节点的特征是这个人的特征。</p></blockquote><p id="6695" class="pw-post-body-paragraph ki kj iq kl b km oc jr ko kp od ju kr lg oe ku kv lh of ky kz li og lc ld le ij bi translated">图形可以是<strong class="kl ir">有向的或无向的:</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/278d11ece13cff307ebbc9bce75a8a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*01whFGyfYCasLLqZAVUwYg.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Note that directed graphs can have undirected edges too</figcaption></figure><p id="4ebd" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">图中的节点甚至可以有一条指向/连接到自身的边。这就是所谓的<strong class="kl ir">自循环。</strong></p><p id="4430" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">图表可以是:</p><ul class=""><li id="ffc3" class="oh oi iq kl b km kn kp kq lg oj lh ok li ol le om on oo op bi translated"><strong class="kl ir">异构</strong>——由不同类型的节点组成</li><li id="f2b5" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated"><strong class="kl ir">同类</strong> —由相同类型的节点组成</li></ul><p id="ce8a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">并且是:</p><ul class=""><li id="af87" class="oh oi iq kl b km kn kp kq lg oj lh ok li ol le om on oo op bi translated"><strong class="kl ir">静态</strong> —节点和边不变，不添加或删除任何东西</li><li id="4e28" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated"><strong class="kl ir">动态</strong> —节点和边的改变、添加、删除、移动等。</li></ul><p id="0276" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">粗略地说，图可以模糊地描述为</p><ul class=""><li id="d6ad" class="oh oi iq kl b km kn kp kq lg oj lh ok li ol le om on oo op bi translated"><strong class="kl ir">密集</strong>——由许多节点和边组成</li><li id="9b27" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated"><strong class="kl ir">稀疏</strong> —由较少的节点和边组成</li></ul><p id="ae76" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">通过将图表转换成平面形式，可以使它们看起来更整洁，这基本上意味着重新排列节点，使边不相交</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/48e343e64b3c8deea9015ef30114325f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*EFTtnCgA_ESQ86LW.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">(Courtesy of The Geography of Transport Systems<strong class="bd ow">)</strong></figcaption></figure><p id="90fb" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">这些概念和术语在我们探索当前在各种 GNN 架构中使用的许多不同方法时会派上用场。这些基本方法中的一些在:</p><h2 id="0317" class="mw mf iq bd mg mx my dn mk mz na dp mo lg nb nc mq lh nd ne ms li nf ng mu nh bi translated">图形分析</h2><p id="bb06" class="pw-post-body-paragraph ki kj iq kl b km ni jr ko kp nj ju kr lg nk ku kv lh nl ky kz li nm lc ld le ij bi translated"><strong class="kl ir">有许多不同的图形结构</strong>可供 ML 模型学习(轮子、周期、星形、网格、棒棒糖、密集、稀疏等)。)</p><p id="7bb6" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">你可以遍历一个图</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/44ec5733e78aa04b0292e6c600de6682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvqvx89SQUd5lgR_rjse4A.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Jon went from Bob’s to Bic’s in 4 timesteps; he better hope it doesn’t snow!</figcaption></figure><p id="22e5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">在这种情况下，我们正在遍历一个无向图。显然，如果图是<strong class="kl ir">有向</strong>的，人们可以简单地沿着边的方向走。有几种不同类型的遍历，所以要小心措辞。以下是一些最常见的图形遍历术语及其含义:</p><ul class=""><li id="f5c8" class="oh oi iq kl b km kn kp kq lg oj lh ok li ol le om on oo op bi translated"><strong class="kl ir">走查:</strong>一个图的遍历——一个<strong class="kl ir">封闭走查</strong>是当目的节点与源节点相同时</li><li id="e94b" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated"><strong class="kl ir">步道:</strong>没有重复边缘的步道——一条<strong class="kl ir">环路</strong>是一条封闭的步道</li><li id="6a81" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated"><strong class="kl ir">路径:</strong>没有重复节点的行走——c<strong class="kl ir">循环</strong>是封闭路径</li></ul><p id="cacc" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">基于遍历的概念，我们也可以通过图发送消息。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/4dcde34c14fcb93695919d8e1e852808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x0IrgVBdBlOeECl30Jehbg.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Sam? More like S-p-am…</figcaption></figure><p id="9503" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">山姆的所有邻居都在给他发信息，其中<em class="kk"> t </em>代表时间步长。Sam 可以选择打开自己的邮箱，更新自己的信息。通过网络传播信息的概念对于具有<strong class="kl ir">注意力机制的模型</strong>来说非常重要。在图中，消息传递是我们推广卷积的一种方式。稍后将详细介绍。</p><h2 id="dcb8" class="mw mf iq bd mg mx my dn mk mz na dp mo lg nb nc mq lh nd ne ms li nf ng mu nh bi translated">电子图形——计算机上的图形</h2><p id="8c76" class="pw-post-body-paragraph ki kj iq kl b km ni jr ko kp nj ju kr lg nk ku kv lh nl ky kz li nm lc ld le ij bi translated">学完这些，你现在对图论有了一个基本的了解！对于 GNNs 来说，任何其他重要的概念都会在它们出现的时候被解释，但是同时，还有最后一个关于图的主题我们需要讨论。我们必须学会如何通过计算来表达图形。</p><p id="fd1c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">有几种方法可以把一张图表转换成计算机可以理解的格式；它们都是不同类型的矩阵。</p><p id="f7dd" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><strong class="kl ir">关联矩阵(<em class="kk"> I </em> ): </strong></p><p id="8dea" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">关联矩阵，在研究论文中通常用大写字母<strong class="kl ir"> <em class="kk"> I </em> </strong>表示，由 1、0 和-1 组成，关联矩阵可以通过以下简单模式制作:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi ox"><img src="../Images/b273c76185dd7bab55142fe68a388e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Ux80uESvaUdtkEyRMYajw.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">From Graph to Incidence Matrix</figcaption></figure><p id="eaed" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><strong class="kl ir">(加权)邻接矩阵(<em class="kk"> A </em> ): </strong></p><p id="c704" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">图的邻接矩阵是由 1 和 0 组成的<strong class="kl ir">，除非</strong>它被另外加权或标记。在任何情况下<strong class="kl ir"> <em class="kk">一个</em> </strong>都可以按照这个规则来构建:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi oy"><img src="../Images/ded0cc99e99438a79465fdeca22260d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkv1OmkA3YsJnUevwPpd_g.png"/></div></div></figure><p id="1982" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><strong class="kl ir">因此，无向图的邻接矩阵沿其对角线对称</strong>，从左上到右下:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/294751ab9b1dbe4d38f8668d22352bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/1*NZyJI_-uHKZ4PUj_LX6SFQ.gif"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Adjacency Matrices (Courtesy of Wolfram Mathworld)</figcaption></figure><p id="931f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">有向图的邻接矩阵只覆盖对角线的一边，因为有向图的边只有一个方向。</p><p id="7a71" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><strong class="kl ir">邻接矩阵可以被“加权”</strong>，这基本上意味着每条边都有一个与之相关联的值，因此该值被放在各自的矩阵坐标中，而不是 1。这些权重可以代表你想要的任何东西。以分子为例，它们可以代表两个节点(原子)之间的键的类型。在 LinkedIn 这样的社交网络中，它们可以代表两个节点(人)之间的第一、第二或第三顺序连接。</p><p id="19b9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">边缘权重的概念是 GNNs 如此强大的一个原因；它们允许我们同时考虑结构(相关)和奇异(独立)信息。对于真实世界的应用程序，这意味着我们既可以考虑内部信息，也可以考虑外部信息。</p><p id="4d33" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><strong class="kl ir">度矩阵(<em class="kk"> D </em> ): </strong></p><p id="dd90" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">图的度矩阵可以用前面讲过的度的概念来求。<strong class="kl ir"> <em class="kk"> D </em> </strong>本质上是一个对角矩阵，其中<strong class="kl ir">对角线的每一个值都是其对应节点的度数。</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi pa"><img src="../Images/d75a40983ac59820c9d12f24de33042b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*q162kpWGY8o4GeL8.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">The different types of graphs and Matrices (Courtesy of the EU Bioinformatics Institute)</figcaption></figure><p id="882b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">注意，度只是邻接矩阵中每一行的和。然后将这些度放在矩阵的对角线上(邻接矩阵的对称线)。这很好地引出了最终的矩阵:</p><p id="a46c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><strong class="kl ir">拉普拉斯矩阵(L): </strong></p><p id="4dab" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">图的拉普拉斯矩阵是从度矩阵中减去邻接矩阵的结果:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi ox"><img src="../Images/2978f8d013ecf360ae5a78440ba07b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pq4JbL13x6vgvSm5ZWrXHQ.png"/></div></div></figure><p id="03a0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">度矩阵中的每个值都减去其在邻接矩阵中的相应值，如下所示:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi ox"><img src="../Images/bf41cafe0493d9c8083c5e67e7badea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*art_foSYIB5jNuQtBuM3sg.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">The graph matrix trinity (Courtesy of Wikipedia)</figcaption></figure><p id="ef07" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">还存在其他图形矩阵表示，如<strong class="kl ir">关联矩阵、</strong>，但是绝大多数图形类型数据的 GNN 应用程序使用一个、两个或所有三个矩阵。这是因为它们，尤其是拉普拉斯矩阵，提供了关于<strong class="kl ir">实体</strong>(具有属性的元素)和<strong class="kl ir">关系</strong>(实体之间的连接)的大量信息。</p><p id="f8db" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">唯一缺少的是一个<strong class="kl ir">规则</strong>(一个通过关系将实体映射到其他实体的函数)。这就是神经网络派上用场的地方。</p><blockquote class="kf kg kh"><p id="e816" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kl ir">如果你需要更多关于图形及其表示的见解，我强烈推荐你看看</strong> <a class="ae lf" href="https://medium.com/basecs/from-theory-to-practice-representing-graphs-cfd782c5be38" rel="noopener"> <strong class="kl ir">这篇深度中等的文章。</strong>T13】</a></p></blockquote><h1 id="1f7c" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">深度学习—速成班</h1><p id="d7c4" class="pw-post-body-paragraph ki kj iq kl b km ni jr ko kp nj ju kr lg nk ku kv lh nl ky kz li nm lc ld le ij bi translated">现在让我们快速运行另一半“图形<strong class="kl ir">神经网络</strong>”。当有人说“<strong class="kl ir">深度学习</strong>”时，神经网络就是我们谈论的架构。神经网络架构建立在<strong class="kl ir">感知器</strong>的概念之上，感知器受到人类大脑中神经元相互作用的启发。</p><p id="8dc9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">人工神经网络(或简称 NN)及其扩展家族，包括卷积神经网络、递归神经网络，当然还有图神经网络，都是深度学习算法的类型。</p><blockquote class="ns"><p id="0c85" class="nt nu iq bd nv nw nx ny nz oa ob le dk translated">深度学习是一种机器学习算法，而机器学习算法又是人工智能的一个子集。</p></blockquote><p id="375a" class="pw-post-body-paragraph ki kj iq kl b km oc jr ko kp od ju kr lg oe ku kv lh of ky kz li og lc ld le ij bi translated">这一切都始于不起眼的线性方程。</p><blockquote class="ns"><p id="6ed8" class="nt nu iq bd nv nw nx ny nz oa ob le dk translated">y = mx + b</p></blockquote><p id="e193" class="pw-post-body-paragraph ki kj iq kl b km oc jr ko kp od ju kr lg oe ku kv lh of ky kz li og lc ld le ij bi translated">如果我们把这个等式构造成一个感知器，我们会看到:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi pb"><img src="../Images/e362f903249fe9d30284caa5f7bc09a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9ruBeAtmrA6vFyJr"/></div></div></figure><p id="a609" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">其中<strong class="kl ir">输出(y)是偏差(b)和输入(x)之和(E)乘以权重(m)。</strong></p><p id="27cb" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">神经网络通常有一个<strong class="kl ir">激活函数</strong>，它基本上决定一个给定的神经元输出(即<em class="kk"> y </em>)是否应该被认为是“激活的”，并将一个感知器的输出值保持在一个合理的、可计算的范围内。(sigmoid 表示 0–1，tanh 表示-1–1，ReLU 表示 0 或 1，以此类推。).这就是为什么我们在感知器的末端附加一个激活函数。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi pb"><img src="../Images/dfd45afd90c3a443d49c82e709d5151e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uR4mNZ6pYT1L7COX"/></div></div></figure><p id="3911" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">当我们把一堆感知器放在一起时，我们得到了类似于神经网络的开端的东西！这些感知器将数值从一层传递到另一层，每次传递都使该数值更接近网络训练所针对的目标/标签。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi pb"><img src="../Images/2eecf3d8abe409ffadea9bcb4691a3a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DCsw_xQU0VQrIZP4"/></div></div></figure><p id="fc25" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">当你把一堆感知器放在一起，你会得到:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi pc"><img src="../Images/4bce3dddacf2b24c5805817bae2aa456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JwcquDlAfzhE-y1X"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">A vanilla NN (courtesy of Digital Trends)</figcaption></figure><p id="59d5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">为了训练神经网络，我们需要首先计算我们需要调整模型的权重多少。我们用一个<strong class="kl ir">损失函数</strong>来计算<strong class="kl ir">误差</strong>。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/c3d274ef5eac5d853d6208c73481ce51.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*f3lH6IbFF9C4aPfc6I6CJg.png"/></div></figure><p id="3445" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">其中<strong class="kl ir"> <em class="kk"> e </em> </strong>为误差，<strong class="kl ir"> <em class="kk"> Y </em> </strong>为预期输出，<strong class="kl ir"><em class="kk"/></strong>为实际输出。在高层次上，误差被计算为实际输出(神经网络的预测)减去预期输出(目标)。目标是<strong class="kl ir">最小化误差</strong>。通过使用称为<strong class="kl ir">反向传播的过程来调整每个层的权重，误差被最小化。</strong></p><p id="76dc" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">本质上，反向传播将调整分布在从输出层到输入层的整个网络中。调整量由接收误差作为输入的<strong class="kl ir">优化功能</strong>决定。优化函数可以想象成一个球滚下山坡，球的位置就是误差。因此，当球滚到山脚时，误差最小。</p><p id="190a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">此外，还有一些必须定义的<strong class="kl ir">超参数</strong>，其中最重要的一个是<strong class="kl ir">学习率</strong>。学习速率调整应用优化函数的速率。学习速度就像重力设置；重力越大(学习率越高)球滚下山的速度越快，反过来也是一样。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/6ad7998b120c3f4d84616b948462c1e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*zjTuE91Skw7y2Vx-"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">The propagation of a network (Courtesy of 3Blue1Brown)</figcaption></figure><p id="b4a4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">神经网络有许多不同的宏观和微观定制，使每个模型都是独特的，具有不同的性能水平，但所有这些都是基于这个<strong class="kl ir">普通</strong>模型。稍后我们将会看到这是如何实现的，特别是对于图形学习。像卷积和递归这样的运算将根据需要引入。</p><h1 id="38b5" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">深度学习是图论</h1><p id="0d46" class="pw-post-body-paragraph ki kj iq kl b km ni jr ko kp nj ju kr lg nk ku kv lh nl ky kz li nm lc ld le ij bi translated">为了把我们所研究的一切联系起来，并检验我们的知识，我们将解决房间里的大象。如果你一直在关注，你可能会注意到一个微妙但明显的事实:</p><blockquote class="ns"><p id="2bee" class="nt nu iq bd nv nw nx ny nz oa ob le dk translated">人工神经网络其实只是图！</p></blockquote><p id="135f" class="pw-post-body-paragraph ki kj iq kl b km oc jr ko kp od ju kr lg oe ku kv lh of ky kz li og lc ld le ij bi translated">NNs 是一种特殊的图形，但是它们具有相同的结构，因此具有相同的术语、概念和规则。</p><p id="22ba" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">回想一下，感知器的结构本质上是:</p><blockquote class="kf kg kh"><p id="1ff6" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">感知器图片(修改)</p></blockquote><p id="681d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">我们可以将输入值(x)、偏差值(b)和求和运算(E)看作是图中的 3 个节点。我们可以将权重(m)视为连接输入值(x)和求和运算(E)的边。</p><p id="75bd" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">NNs 最相似的特定类型的图是<strong class="kl ir">多部分图。多部图是可以分成不同节点集的图。每个集合中的节点可以共享集合之间的边，但不能共享每个集合内的边。</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/c390fcf2b4c122aa85e91be40b53101e.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/0*QPG71z_WHVIMqqcm.gif"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Isomorphic bipartite graphs (courtesy of Wolfram MathWorld)</figcaption></figure><p id="9e80" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">一些神经网络甚至具有全连接节点、条件节点和其他疯狂的架构，这些架构赋予了神经网络其标志性的多功能性和能力；以下是一些最受欢迎的架构:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi pb"><img src="../Images/628252ef9ddfcdb55dc176df5deb4a37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UH0CeoRi3-qKHJhb"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Neural Network Zoo (Courtesy of Asimov Institute)</figcaption></figure><p id="ab9d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">每种颜色对应一种不同类型的节点，节点可以用多种不同的方式排列。通过网络向前或向后传播数据类似于在图中传递<strong class="kl ir">消息。图中的<strong class="kl ir">边或节点特征</strong>类似于神经网络中的权重。注意一些节点甚至有我们之前提到的<strong class="kl ir">自循环</strong>(RNNs 固有的——递归神经网络)。</strong></p><p id="65e8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">神经网络不是唯一具有类似图形结构的机器学习模型。</p><ul class=""><li id="900f" class="oh oi iq kl b km kn kp kq lg oj lh ok li ol le om on oo op bi translated">k 均值</li><li id="21a3" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated">k-最近邻</li><li id="069b" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated">决策树</li><li id="1f01" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated">随机森林</li><li id="6818" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated">马尔可夫链</li></ul><p id="9142" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">都像图形本身一样构造，或者在图形结构中输出数据。</p><p id="c2b7" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">因此，这意味着图形学习模型可以用来学习这些机器学习算法本身。在<strong class="kl ir">超参数优化</strong>中有潜在的应用。这正是这篇令人惊叹的论文的作者们所做的。</p><p id="1ea8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">随着我们更多地了解在几何数据上推广深度学习，可能性才刚刚开始浮出水面。</p></div><div class="ab cl pf pg hu ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="ij ik il im in"><h1 id="0abe" class="me mf iq bd mg mh pm mj mk ml pn mn mo jw po jx mq jz pp ka ms kc pq kd mu mv bi translated">本质上</h1><p id="4c49" class="pw-post-body-paragraph ki kj iq kl b km ni jr ko kp nj ju kr lg nk ku kv lh nl ky kz li nm lc ld le ij bi translated">我们讨论了很多，但概括来说，我们深入探讨了 3 个概念</p><ol class=""><li id="0492" class="oh oi iq kl b km kn kp kq lg oj lh ok li ol le pr on oo op bi translated">图论</li><li id="e4fa" class="oh oi iq kl b km oq kp or lg os lh ot li ou le pr on oo op bi translated">深度学习</li><li id="ad02" class="oh oi iq kl b km oq kp or lg os lh ot li ou le pr on oo op bi translated">基于图论的机器学习</li></ol><p id="2dcc" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">记住先决条件，你就能完全理解和欣赏图形学习。在高层次上，图学习使用一系列旨在处理非欧几里得数据的神经网络，进一步探索和利用深度学习和图论之间的关系。</p><h1 id="9823" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">关键要点</h1><p id="f64e" class="pw-post-body-paragraph ki kj iq kl b km ni jr ko kp nj ju kr lg nk ku kv lh nl ky kz li nm lc ld le ij bi translated">有许多关键要点，但重点是:</p><ul class=""><li id="ec47" class="oh oi iq kl b km kn kp kq lg oj lh ok li ol le om on oo op bi translated">所有图形都有<strong class="kl ir">属性，这些属性定义了可以使用或分析的可能动作和限制</strong>。</li><li id="9270" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated"><strong class="kl ir">使用各种矩阵通过计算表示图形</strong>。每个矩阵提供不同数量或类型的信息。</li><li id="5f81" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated"><strong class="kl ir">深度学习是机器学习的一个子集</strong>它大致模仿了人类大脑使用神经元的工作方式。</li><li id="2364" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated"><strong class="kl ir">深度学习通过迭代学习</strong>通过网络向前传递信息，向后传播神经元调整。</li><li id="e5dd" class="oh oi iq kl b km oq kp or lg os lh ot li ou le om on oo op bi translated">神经网络(以及其他机器学习算法)与图论有着密切的联系；<strong class="kl ir">有的本身就是图，或者输出。</strong></li></ul><p id="aa16" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">现在，您已经具备了进入图形学习奇妙世界所需的所有先决条件。一个好的起点是研究到目前为止已经开发的各种图形神经网络。</p></div><div class="ab cl pf pg hu ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="ij ik il im in"><h1 id="79f0" class="me mf iq bd mg mh pm mj mk ml pn mn mo jw po jx mq jz pp ka ms kc pq kd mu mv bi translated">需要看到更多这样的内容？</h1><p id="98f3" class="pw-post-body-paragraph ki kj iq kl b km ni jr ko kp nj ju kr lg nk ku kv lh nl ky kz li nm lc ld le ij bi translated"><em class="kk">跟我上</em><a class="ae lf" href="http://www.linkedin.com/in/flawnson" rel="noopener ugc nofollow" target="_blank"><strong class="kl ir"><em class="kk">LinkedIn</em></strong></a><strong class="kl ir"><em class="kk">，</em> </strong> <a class="ae lf" href="https://www.facebook.com/flawnson" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir"> <em class="kk">脸书</em> </strong> </a> <strong class="kl ir"> <em class="kk">，</em></strong><a class="ae lf" href="https://www.instagram.com/flaws.non/?hl=en" rel="noopener ugc nofollow" target="_blank"><strong class="kl ir"><em class="kk">insta gram</em></strong></a><em class="kk">，当然还有</em> <a class="ae lf" href="https://medium.com/@flawnsontong1" rel="noopener"> <strong class="kl ir"> <em class="kk">中</em> </strong> </a> <em class="kk"/></p><p id="52d7" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><em class="kk">我所有的内容都在</em> <a class="ae lf" href="http://www.flawnson.com" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir"> <em class="kk">我的网站</em> </strong> </a> <em class="kk">我所有的项目都在</em><a class="ae lf" href="https://github.com/flawnson" rel="noopener ugc nofollow" target="_blank"><strong class="kl ir"><em class="kk">GitHub</em></strong></a></p><p id="3554" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated"><em class="kk">我总是希望结识新朋友、合作或学习新东西，所以请随时联系</em><a class="ae lf" href="http://mail.google.com" rel="noopener ugc nofollow" target="_blank"><strong class="kl ir"><em class="kk">flawnsontong1@gmail.com</em></strong></a></p><blockquote class="ns"><p id="8c53" class="nt nu iq bd nv nw nx ny nz oa ob le dk translated">向上和向前，永远和唯一🚀</p></blockquote></div></div>    
</body>
</html>