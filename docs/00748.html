<html>
<head>
<title>Creating a Custom Classifier for Text Cleaning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为文本清理创建自定义分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-a-custom-classifier-for-text-cleaning-a2a1fc818935?source=collection_archive---------15-----------------------#2019-02-04">https://towardsdatascience.com/creating-a-custom-classifier-for-text-cleaning-a2a1fc818935?source=collection_archive---------15-----------------------#2019-02-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b370" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">句子分类的机器学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9d62741a981dbcd5622321b10bb3b727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6CwZ9aWj5pd3zXib_ZmFdQ.jpeg"/></div></div></figure><p id="8ef2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最近，我对 NLP 的研究超过了其他数据科学领域，我经常面临的一个挑战是该过程的清理部分。构建 NLP 模型需要许多预处理步骤，如果数据处理不当，可能会导致模型质量差，而这正是我们想要避免的。</p><p id="ad75" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本文中，我们将重点关注 PDF 文档。这里的目标是打开一个 PDF 文件，将其转换为纯文本，了解数据清理的需要，并为此建立一个机器学习模型。</p><p id="d3d0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本帖中，我们将:</p><ul class=""><li id="5e96" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">打开 PDF 文件并将其转换为文本字符串</li><li id="993f" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">将文本分割成句子并建立数据集</li><li id="fa73" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">通过用户交互手动标记数据</li><li id="f5e6" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">制作一个分类器来删除不需要的句子</li></ul><p id="3b98" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将要使用的一些库:</p><ul class=""><li id="7072" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><em class="mb"> pdfminer </em> →阅读 PDF 文件</li><li id="6f81" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><em class="mb">文本斑点</em> →文本处理</li><li id="6ccd" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><em class="mb">熊猫</em> →数据分析</li></ul><h1 id="287e" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">PDF 阅读器</h1><p id="0c88" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">和往常一样，我会试着解释文本中使用的代码，所以如果你愿意，可以跳过这些代码片段。让我们从导入一些模块开始:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="d3c5" class="ne md iq na b gy nf ng l nh ni">from collections import Counter<br/>from IPython.display import clear_output<br/>from pdfminer.converter import TextConverter<br/>from pdfminer.layout import LAParams<br/>from pdfminer.pdfinterp import PDFResourceManager<br/>from pdfminer.pdfinterp import PDFPageInterpreter<br/>from pdfminer.pdfpage import PDFPage<br/>from textblob import TextBlob<br/>import io<br/>import math<br/>import numpy as np<br/>import pandas as pd<br/>import string</span></pre><p id="cc59" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将使用<em class="mb"> pdfminer </em>来构建我们的 PDF 阅读器:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="975e" class="ne md iq na b gy nf ng l nh ni">def read_pdf(path):<br/>    rsrcmgr = PDFResourceManager()<br/>    retstr = io.StringIO()<br/>    codec = 'utf-8'<br/>    laparams = LAParams()<br/>    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)<br/>    fp = open(path, 'rb')<br/>    interpreter = PDFPageInterpreter(rsrcmgr, device)<br/>    password = ""<br/>    maxpages = 0<br/>    caching = True<br/>    pagenos=set()<br/>    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password, caching=caching, check_extractable=True): <br/>        interpreter.process_page(page)<br/>    text = retstr.getvalue()<br/>    text = " ".join(text.replace(u"\xa0", " ").strip().split())  <br/>    fp.close()<br/>    device.close()<br/>    retstr.close()<br/>    return text</span></pre><p id="09d1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然这个函数看起来很长，但它只是读取一个 PDF 文件并以字符串形式返回它的文本。我们将把它应用到一篇名为“谷歌数据实用指南”的论文中:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="925c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">只要看第一页，我们很快就会发现，一篇文章包含的不仅仅是简单的句子，还包括像<strong class="kt ir">日期</strong>、<strong class="kt ir">行数</strong>、<strong class="kt ir">页码</strong>、<strong class="kt ir">数字</strong>、<strong class="kt ir">标题和副标题</strong>、<strong class="kt ir">节分隔符</strong>、<strong class="kt ir">等式、</strong>等等。让我们来看看当论文被转换成纯文本时，这些属性将如何显示(<em class="mb">primer.pdf</em>是文件的名称，存储在我的本地计算机中):</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="5ec5" class="ne md iq na b gy nf ng l nh ni">read_pdf('primer.pdf')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/da548f4b86b0b23db4a76a237636391a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ocwS3wxuShyYHNVlW3NQGw.png"/></div></div></figure><p id="aebe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">很明显，我们丢失了所有的文本结构。行数和页码分散开来，因为它们是句子的一部分，而标题和参考文献无法与正文明确区分。可能有很多方法可以让你在阅读 PDF 时保留文本结构，但是为了便于解释，让我们保持混乱状态(因为这通常就是原始文本数据的样子)。</p><h1 id="3bb3" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">文本清理</h1><p id="0d39" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">一个完整的清洗管道有许多步骤，为了熟悉它们，我建议遵循一些教程(<a class="ae nm" href="https://machinelearningmastery.com/clean-text-machine-learning-python/" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae nm" href="https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/" rel="noopener ugc nofollow" target="_blank">这个</a>是很好的起点)。一般来说，清洗流程链包括:</p><ul class=""><li id="da8b" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">标记化</li><li id="62eb" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">正常化</li><li id="1601" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">实体提取</li><li id="1243" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">拼写和语法纠正</li><li id="6a16" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">删除标点符号</li><li id="8aa3" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">删除特殊字符</li><li id="be22" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">词干</li></ul><p id="3896" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们在这里的目标不是取代任何一个阶段，而是建立一个更通用的工具来删除我们不需要的东西。把它作为辅助步骤，在中间帮忙。</p><p id="c37b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">假设我们想要删除任何看起来不像人类写的句子。这个想法是将那些句子归类为“不想要的”或“怪异的”，而将其余的句子视为“正常的”。例如:</p><blockquote class="nn no np"><p id="2a9b" class="kr ks mb kt b ku kv jr kw kx ky ju kz nq lb lc ld nr lf lg lh ns lj lk ll lm ij bi translated">32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 相关。</p></blockquote><p id="5a7a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">或者</p><blockquote class="nn no np"><p id="64c3" class="kr ks mb kt b ku kv jr kw kx ky ju kz nq lb lc ld nr lf lg lh ns lj lk ll lm ij bi translated">51 52 53 54 55 #从关联中读取数据，并使其成为动物园时间序列 dat </p></blockquote><p id="e1d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Those sentences are clearly messed up because of the text transformation and in case we're making, let's say, a PDF summarizer, they shouldn't be included.</p><p id="1a48" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">To remove them, we could manually analyze the text, figure out some patterns and apply <a class="ae nm" href="https://docs.python.org/2/library/re.html" rel="noopener ugc nofollow" target="_blank">正则表达式</a>。但是，在某些情况下，为我们建立一个发现这些模式的模型可能会更好。这就是我们在这里做的。我们将创建一个分类器来识别奇怪的句子，这样我们就可以轻松地将它们从正文中删除。</p><h1 id="dc90" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">构建数据集</h1><p id="80fc" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">让我们构建一个函数来打开 PDF 文件，将文本拆分成句子并保存到一个数据框中，该数据框包含列<em class="mb">标签</em>和<em class="mb">句子</em>:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="cba5" class="ne md iq na b gy nf ng l nh ni">def pdf_to_df(path):<br/>    content = read_pdf(path)<br/>    blob = TextBlob(content)<br/>    sentences = blob.sentences<br/>    df = pd.DataFrame({'sentence': sentences, 'label': np.nan})<br/>    df['sentence'] = df.sentence.apply(''.join)<br/>    return df</span><span id="b6fb" class="ne md iq na b gy nt ng l nh ni">df = pdf_to_df('primer.pdf')<br/>df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/c7c6a24904c473fefef0015992581685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tSHkiPQgtGNmSrWY7NbJww.png"/></div></div></figure><p id="148e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由于我们没有标记数据(在“怪异”或“正常”中)，我们将手动填充我们的<em class="mb">标签</em>列。这个数据集将是可更新的，以便我们可以附加新的文件，并标记他们的句子。</p><p id="0e04" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们首先将未标记的数据集保存到一个<em class="mb">中。泡菜</em>文件:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="7c38" class="ne md iq na b gy nf ng l nh ni">df.to_pickle('weird_sentences.pickle')</span></pre><p id="4552" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，我们将创建一个用户交互功能来手动分类数据点。对于数据集中的每个句子，我们将显示一个文本框，供用户键入“1”或不键入任何内容。如果用户键入‘1’，该句子将被分类为‘怪异’。</p><p id="5688" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我使用的是 Jupyter 笔记本，所以我从 IPython.display 调用了 clear_output()函数来改善交互。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="e216" class="ne md iq na b gy nf ng l nh ni">def manually_label(pickle_file):<br/>    print('Is this sentence weird? Type 1 if yes. \n')<br/>    df = pd.read_pickle(pickle_file)<br/>    for index, row in df.iterrows():<br/>        if pd.isnull(row.label):<br/>            print(row.sentence)<br/>            label = input()<br/>            if label == '1':<br/>                df.loc[index, 'label'] = 1<br/>            if label == '':<br/>                df.loc[index, 'label'] = 0<br/>            clear_output()<br/>            df.to_pickle('weird_sentences.pickle')<br/>            <br/>    print('No more labels to classify!')</span><span id="0078" class="ne md iq na b gy nt ng l nh ni">manually_label('weird_sentences.pickle')</span></pre><p id="aa40" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">每个句子的输出如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/c4a304c8a16c998d391d781cd3ed1e8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*du1pPB-Z-xl0TrlLUAZuuQ.png"/></div></div></figure><p id="39f6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由于这句话看起来挺正常的，我就不打' 1 '了，直接按<em class="mb">回车</em>进入下一句。这个过程将一直重复，直到数据集被完全标记或者当您中断时。每一个用户输入都被保存到<em class="mb"> pickle </em>文件中，因此数据集在每一个句子中都被更新。这种简单的交互使得标记数据变得相对较快。我花了 20 分钟标记了大约 500 个数据点。</p><p id="4e5a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了简单起见，还编写了另外两个函数。一个用于将另一个 PDF 文件附加到我们的数据集，另一个用于重置所有标签(将<em class="mb">标签</em>列值设置为<em class="mb"> np.nan </em>)。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="1a92" class="ne md iq na b gy nf ng l nh ni">def append_pdf(pdf_path, df_pickle):<br/>    new_data = pdf_to_df(pdf_path)<br/>    df = pd.read_pickle(df_pickle)<br/>    df = df.append(new_data)<br/>    df = df.reset_index(drop=True)<br/>    df.to_pickle(df_pickle)</span><span id="a075" class="ne md iq na b gy nt ng l nh ni">def reset_labels(df_pickle):<br/>    df = pd.read_pickle(df_pickle)<br/>    df['label'] = np.nan<br/>    df.to_pickle(df_pickle)</span></pre><p id="dff5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由于我们最终得到了更多“正常”而非“怪异”的句子，我构建了一个函数来对数据集进行欠采样，否则，一些机器学习算法将无法很好地执行:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="ab27" class="ne md iq na b gy nf ng l nh ni">def undersample(df, target_col, r=1):<br/>    falses = df[target_col].value_counts()[0]<br/>    trues = df[target_col].value_counts()[1]<br/>    relation = float(trues)/float(falses)</span><span id="882f" class="ne md iq na b gy nt ng l nh ni">    if trues &gt;= r*falses:<br/>        df_drop = df[df[target_col] == True]<br/>        drop_size = int(math.fabs(int((relation - r) * (falses))))<br/>    else: <br/>        df_drop = df[df[target_col] == False]<br/>        drop_size = int(math.fabs(int((r-relation) * (falses))))</span><span id="b758" class="ne md iq na b gy nt ng l nh ni">    df_drop = df_drop.sample(drop_size)<br/>    df = df.drop(labels=df_drop.index, axis=0)<br/>    return df</span><span id="f765" class="ne md iq na b gy nt ng l nh ni">df = pd.read_pickle('weird_sentences.pickle').dropna()<br/>df = undersample(df, 'label')<br/>df.label.value_counts()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d0b2193d7432801867d539283be60017.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*6N6PiGhLZcN64kjm_OAssw.png"/></div></figure><p id="ed79" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">645 个标记的数据点。不足以制作一个像样的模型，但我们会用它作为一个操场的例子。</p><h1 id="d453" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">文本转换</h1><p id="58f4" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">现在，我们需要以算法可以理解的方式转换句子。一种方法是计算每个字符在句子中的出现次数。这有点像是一种文字袋技术，但是是在角色层面上。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="65cc" class="ne md iq na b gy nf ng l nh ni">def bag_of_chars(df, text_col):<br/>    chars = []<br/>    df['char_list'] = df[text_col].apply(list)<br/>    df['char_counts'] = df.char_list.apply(Counter)<br/>    for index, row in df.iterrows():<br/>        for c in row.char_counts:<br/>            df.loc[index, c] = row.char_counts[c]<br/>    chars = list(set(chars))<br/>    df = df.fillna(0).drop(['sentence', 'char_list', 'char_counts'], 1)<br/>    return df</span><span id="f358" class="ne md iq na b gy nt ng l nh ni">data = bag_of_chars(df, 'sentence')<br/>data.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/b278a20003e117b616b4b146a1aa39cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I2ZnQncPfCmSt7i1AHt3dA.png"/></div></div></figure><h1 id="b4da" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated"><strong class="ak">机器学习模型</strong></h1><p id="a413" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">完美！现在我们只剩下一个常见的机器学习挑战。一个分类问题中的多个特征和一个目标。让我们将数据分成训练集和测试集:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="3c5b" class="ne md iq na b gy nf ng l nh ni">data = data.sample(len(data)).reset_index(drop=True)<br/>train_data = data.iloc[:400]<br/>test_data = data.iloc[400:]</span><span id="26f9" class="ne md iq na b gy nt ng l nh ni">x_train = train_data.drop('label', 1)<br/>y_train = train_data['label']<br/>x_test = test_data.drop('label', 1)<br/>y_test = test_data['label']</span></pre><p id="c487" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们准备选择一个算法并检查它的性能。在这里，我使用逻辑回归来看看我们能实现什么:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="4167" class="ne md iq na b gy nf ng l nh ni">from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score</span><span id="e77a" class="ne md iq na b gy nt ng l nh ni">lr = LogisticRegression()<br/>lr.fit(x_train, y_train)</span><span id="072d" class="ne md iq na b gy nt ng l nh ni">accuracy_score(y_test, lr.predict(x_test))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/542ed0eb912070cc67d3145dccbc5dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*w-r2UjIb2Z4R6DIW4o5bRQ.png"/></div></figure><p id="4470" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">86 %的准确率。对于小型数据集、浅层模型和字符包方法来说，这已经很不错了。唯一的问题是，尽管我们分成了训练和测试，但是我们是用我们训练的同一个文档来评估模型的。更合适的方法是使用新文档作为测试集。</p><p id="4d20" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们创建一个函数，使我们能够预测任何自定义句子:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="b8df" class="ne md iq na b gy nf ng l nh ni">def predict_sentence(sentence):<br/>    sample_test = pd.DataFrame({'label': np.nan, 'sentence': sentence}, [0])<br/>    for col in x_train.columns:<br/>        sample_test[str(col)] = 0<br/>    sample_test = bag_of_chars(sample_test, 'sentence')<br/>    sample_test = sample_test.drop('label', 1)<br/>    pred = lr.predict(sample_test)[0]<br/>    if pred == 1:<br/>        return 'WEIRD'<br/>    else:<br/>        return 'NORMAL'</span><span id="fd51" class="ne md iq na b gy nt ng l nh ni">weird_sentence = 'jdaij oadao //// fiajoaa32 32 5555'<br/></span></pre><p id="baec" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正常句子:</p><blockquote class="nn no np"><p id="754e" class="kr ks mb kt b ku kv jr kw kx ky ju kz nq lb lc ld nr lf lg lh ns lj lk ll lm ij bi translated">我们刚刚建立了一个很酷的机器学习模型</p></blockquote><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="0124" class="ne md iq na b gy nf ng l nh ni">normal_sentence = 'We just built a cool machine learning model'<br/>predict_sentence(normal_sentence)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/6dda6e1e0cc3bf98792a5ed5b74cffc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*xlJ8duLty5YW-Fd9i9i0iw.png"/></div></figure><p id="d77f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">怪句:</p><blockquote class="nn no np"><p id="e6a0" class="kr ks mb kt b ku kv jr kw kx ky ju kz nq lb lc ld nr lf lg lh ns lj lk ll lm ij bi translated">jdaij oadao //// fiajoaa32 32 5555</p></blockquote><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="23b8" class="ne md iq na b gy nf ng l nh ni">weird_sentence = 'jdaij oadao //// fiajoaa32 32 5555'<br/>predict_sentence(weird_sentence)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/02f87bf32c5d1b1882f28b0dd5b897d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*RMbcRkFdsqutFW63lKqOwg.png"/></div></figure><p id="d706" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们的模特得分了！不幸的是，当我尝试更多的句子时，它显示出对其中一些句子进行分类的糟糕表现。单词袋(在这种情况下是字符)方法可能不是最好的选择，算法本身可以大大改进，我们应该标记更多的数据点，使模型变得可靠。这里的要点是，您可以使用相同的方法来执行许多不同的任务，例如识别特定的元素(例如链接、日期、名称、主题、标题、等式、引用等等)。使用正确的方式，文本分类可以是一个强大的工具，以帮助在清理过程中，不应该被视为理所当然。好清洁！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob nk l"/></div></figure></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><p id="d7c8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">谢谢你一直读到最后。这是一篇关注文本分类以解决清理问题的文章。请关注我的个人资料，了解更多关于数据科学的信息，并随时向我提出任何意见或问题。下一篇帖子再见！</p></div></div>    
</body>
</html>