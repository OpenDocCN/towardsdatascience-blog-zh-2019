<html>
<head>
<title>Types of Neural Networks (and what each one does!) Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的类型(以及每种类型的作用！)解释道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/types-of-neural-network-and-what-each-one-does-explained-d9b4c0ed63a1?source=collection_archive---------2-----------------------#2019-12-28">https://towardsdatascience.com/types-of-neural-network-and-what-each-one-does-explained-d9b4c0ed63a1?source=collection_archive---------2-----------------------#2019-12-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bcaa" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">有大量的神经网络具有吸引人的特性。以下是最著名的几个。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4879d08438eeb3a33cbc433bd775ae2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FyWf3y_y2UJhWfuAq-3PtA.png"/></div></div></figure><p id="a60f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">机器学习是人工智能的一个子集，它结合神经网络来创建一些我们日常使用的令人惊叹的软件。</p><p id="ce58" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你用谷歌找到这篇中型文章，你用的是谷歌的神经网络，它会根据你给的关键词对最相关的页面进行排序。如果你最近去了 Amazon.com，该网站向你推荐的所有产品都是由神经网络策划的。即使在今天，如果你使用手机，你可能会遇到一个神经网络，让你的生活变得更容易。它就在我们身边，它们都做着不同的事情，以不同的方式工作。</p><h1 id="fcd5" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated"><em class="mi">那么……什么是“神经网络”</em></h1><p id="1366" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">“神经”这个词只是大脑的另一种说法。<em class="mo">“那么这是一个</em> <strong class="kw iu"> <em class="mo">大脑</em> </strong> <em class="mo">网络？”</em>本质上，完全！神经网络是我们最强大的工具——大脑的简化。它使用的<strong class="kw iu">神经元</strong>都是通过<strong class="kw iu">权重</strong>相互连接的(下图中的线)。给神经元一些数字输入，并乘以权重。权重是神经网络的核心，通过将它们更改为特定的数值，我们可以处理任何输入并获得所需的输出。神经网络只是处理数据的一种方式。数据是🔑在这里，通过各种神经网络操纵数据…我们可以建立非常强大的工具，做一些疯狂的事情！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/cdb1540c277b79b24816c19718bad65d.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*fwh8O_gdlrx_ORR8.png"/></div></figure><h1 id="20cb" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated"><em class="mi">感知器——最古老的&amp;最简单的神经网络</em></h1><p id="a36b" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">感知器是最古老的神经网络，创建于 1958 年。也是最简单的神经网络。由 Frank Rosenblatt 开发的感知机为神经网络的基础奠定了基础。</p><p id="b222" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个神经网络只有一个神经元，因此非常简单。它取 n 个数量的输入并乘以相应的权重。它只计算一个输出。它的缺点是缺乏复杂性，因为它只能处理一种复杂程度的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/4c135a64344c0d6db91207446995c4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/0*nk1wqcRZIjHd1mPt"/></div></figure><h2 id="3604" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated">使用案例:</h2><ul class=""><li id="d723" class="nd ne it kw b kx mj la mk ld nf lh ng ll nh lp ni nj nk nl bi translated">了解人类大脑</li><li id="cf46" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">针对更高级的神经网络进行扩展</li></ul><h1 id="49b8" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated"><em class="mi">多层感知器——什么是层？</em></h1><p id="cc9f" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">多层感知器(MLP)仍然是一个感知器，但是通过<strong class="kw iu">层</strong>的出现增加了复杂性。MLP 中有三种类型的图层:</p><h2 id="2be9" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated"><em class="mi">输入图层</em>:</h2><p id="7e91" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">输入层就是它听起来的样子，你输入到神经网络的数据。输入数据必须是数字。这意味着你可能需要一些非数字的东西，并找到一种方法把它变成数字。在将数据输入到神经网络之前处理数据的过程被称为<strong class="kw iu">数据</strong> <strong class="kw iu">处理</strong>，通常情况下，这将是制作机器学习模型最耗时的部分。</p><h2 id="85d5" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated"><em class="mi">隐藏图层</em>:</h2><p id="cf78" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">隐藏层由神经网络中的大多数神经元组成，是操纵数据以获得所需输出的核心。数据将通过隐藏层，并受到许多权重和偏见的操纵。它被称为“隐藏”层，因为与输入和输出层相反，神经网络的开发人员不会直接使用这些层。</p><h2 id="d310" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated"><em class="mi">输出层:</em></h2><p id="98a3" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">输出层是处理神经网络中的数据的最终产品，可以表示不同的事物。通常，输出层由神经元组成，每个神经元代表一个对象，附加的数值是它是该特定对象的概率。其他时候，它可能是一个神经元输出，当给定某些输入时，它就是某个东西的值。主要思想是输出层是数据通过神经网络时的结果，也是我们试图达到的目标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/831f18a78e414f4fc2b3640d4f6d8c95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*qcOTPXaSB_rt66CW"/></div></figure><h2 id="094b" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated">前馈本金:</h2><p id="5659" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">这个想法是，我们将数字数据传递到网络中，它继续向前进行许多操作。我们向前输送数据。为了获得正确的操作，使得任何给定的输入总是产生期望的输出，需要<strong class="kw iu">训练</strong>。训练本质上是找到产生最佳结果的东西，并将它们应用到网络中。</p><h2 id="6d28" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated">使用案例:</h2><ul class=""><li id="cc05" class="nd ne it kw b kx mj la mk ld nf lh ng ll nh lp ni nj nk nl bi translated">计算机视觉</li><li id="ee12" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">自然语言处理</li><li id="3a6a" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">其他神经网络的基础</li></ul><h1 id="bdd9" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated"><em class="mi">卷积神经网络—卷积层？</em></h1><p id="3da2" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">卷积神经网络仍然使用 MLP 使用的相同原理，但是该神经网络实现卷积层。值得注意的是，卷积神经网络通常用于图像和视频。</p><p id="29ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">重要的是要认识到，图像只是一个数字网格，每个数字都告诉你某个像素有多强烈。知道了这是一个数字网格，我们就可以操纵这些数字来寻找图像的模式和特征。卷积层通过使用滤波器来实现这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/800bae55f5300ae9c2c6a2e8de279f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ulfFYH5HbWpLTIfuebj5mQ.gif"/></div></div></figure><h2 id="acba" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated"><em class="mi">过滤器</em></h2><p id="008d" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">滤镜是一个定义的 N x M (N &amp; M 代表网格的大小)的数字网格，它与原始图像相乘多次。要了解实际发生的情况，请参考动画。</p><p id="2a0d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">过滤器在网格中移动并产生新值。这些新值可以表示图像中的边缘或线条。例如，以下面的过滤器为例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/fb16dd4e40f0c78adaf05d4dfb4bd8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/0*2lGVsuaLXPY072h7"/></div></figure><p id="266e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">水平过滤器试图消除垂直中心以外的值。它通过使用负值来去除边缘，使用 0 表示中心来使像素保持中性。如果过滤成功，您将能够从新值中看到一条水平线。对于刚刚反转的垂直滤波器也是如此。</p><p id="b9ea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在整个图像中应用过滤器后，我们可以使用合并层轻松提取过滤器发现的主要特征。在训练模型时，确定哪些数字应该出现在过滤器中。弄清楚什么是最好的数字将为整个任务带来最好的结果。</p><h2 id="4e26" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated"><em class="mi">汇集层</em></h2><p id="25de" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">池层做他们听起来像的事情。它们将过滤器发现的最重要的特征“汇集”在一起。这是通过使用多种方法完成的。一种流行的方法是 Max Pooling，对于图像的每个过滤部分，取最大的数字并存储到新的网格中。这基本上是把最重要的特征压缩成一幅图像，以便处理成 MLP。这个过程也可以称为数据采样，使用这个过程会产生非常有希望的结果。</p><h2 id="3166" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated"><em class="mi">用例</em></h2><ul class=""><li id="30ce" class="nd ne it kw b kx mj la mk ld nf lh ng ll nh lp ni nj nk nl bi translated">图像分类</li><li id="5804" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">计算机视觉</li><li id="d267" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">在图像中寻找特征/模式</li></ul><h1 id="72f2" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated"><em class="mi">递归神经网络——时态数据？</em></h1><p id="d652" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">我们可以用神经网络分析的数据并不完全局限于静态数据。像图像、数字、帧这些东西，都是可以自己分析的数据。然而，依赖于自身的过去实例来预测未来的数据是时态数据的例子。像股票市场数据、时间序列数据、脑波数据等总是通过使用因变量的过去实例来分析。到目前为止，提到的神经网络没有处理数据的其他状态，但是 RNNs 是解决方案。</p><h2 id="4902" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated"><em class="mi">状态矩阵</em></h2><p id="3e00" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">rnn 通过将最后一次输出存储在自己的存储器中来记住以前的数据状态。这些被称为<strong class="kw iu">状态矩阵</strong>。它的工作方式类似于 MLP 中的普通图层，但它使用状态矩阵来计算新的输出。使用以前的输出和数据状态实质上是考虑最终输出中的数据。这对于股票市场预测和时间序列预测等应用至关重要。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/0049a26532da325811040e5ef8c1c941.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*5EcqQO-2xmNn-7fVsYol1g.png"/></div></figure><h2 id="4a0c" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated"><em class="mi"> LSTMs </em></h2><p id="d0de" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">长短期记忆网络进一步扩展了将状态矩阵<strong class="kw iu">保存为两种状态的思想。有长期状态和短期状态。如果一个状态持续存在于模型输出中，它将成为一个长期的状态矩阵，并且在考虑新数据时会更有分量。</strong></p><p id="67d4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">LSTM 系统在发现连续数据的模式时非常有效，是股市预测的先锋。</p><h2 id="71ac" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated"><em class="mi">用例</em></h2><ul class=""><li id="aa89" class="nd ne it kw b kx mj la mk ld nf lh ng ll nh lp ni nj nk nl bi translated">自然语言处理</li><li id="2a7e" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">股票市场预测</li><li id="495a" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">基于时间的数据预测</li></ul><h1 id="1915" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated"><em class="mi">自动编码器——以压缩方式表示数据</em></h1><p id="2a68" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">大多数神经网络接收数据并做出某些类型的决策。自动编码器有一个不同的任务，那就是找出一种方法来压缩数据，但保持相同的质量。</p><p id="187e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">传统上，在机器学习中，附加到我们数据上的标签是不同的，并且是神经网络要产生的目标。在自动编码器中，标签与输入相同。</p><p id="dfa3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，在这种架构中，您有一个相同的输入和输出层。隐藏层比输入和输出层小(就节点而言)，被称为<strong class="kw iu">【瓶颈】</strong>。由于瓶颈变小了，被迫想办法压缩原始数据，放回输出层。这种压缩通常比传统方法更好，因为它仍然可以保持高质量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/e190ef300e516de94c0f7581ec0a95ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GscoY2U2QxZ04MX-"/></div></div></figure><h2 id="4362" class="mr lr it bd ls ms mt dn lw mu mv dp ma ld mw mx mc lh my mz me ll na nb mg nc bi translated"><em class="mi">用例:</em></h2><ul class=""><li id="7a5a" class="nd ne it kw b kx mj la mk ld nf lh ng ll nh lp ni nj nk nl bi translated">主要用于以较小的压缩方式表示大量数据。</li></ul><h1 id="5270" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated"><em class="mi">关键要点</em></h1><blockquote class="nw nx ny"><p id="16e0" class="ku kv mo kw b kx ky ju kz la lb jx lc nz le lf lg oa li lj lk ob lm ln lo lp im bi translated">多层感知器</p></blockquote><ul class=""><li id="daf8" class="nd ne it kw b kx ky la lb ld oc lh od ll oe lp ni nj nk nl bi translated">基本神经网络</li><li id="0f6d" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">用于简单的任务</li></ul><blockquote class="nw nx ny"><p id="e0a8" class="ku kv mo kw b kx ky ju kz la lb jx lc nz le lf lg oa li lj lk ob lm ln lo lp im bi translated">卷积神经网络</p></blockquote><ul class=""><li id="77fd" class="nd ne it kw b kx ky la lb ld oc lh od ll oe lp ni nj nk nl bi translated">使用过滤器和池来查找数据中的特征</li><li id="55dd" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">主要用于图像任务</li></ul><blockquote class="nw nx ny"><p id="48ef" class="ku kv mo kw b kx ky ju kz la lb jx lc nz le lf lg oa li lj lk ob lm ln lo lp im bi translated">递归神经网络</p></blockquote><ul class=""><li id="6a38" class="nd ne it kw b kx ky la lb ld oc lh od ll oe lp ni nj nk nl bi translated">使用以前的数据结果来计算新的输出</li><li id="7d56" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">用于时态数据</li></ul><blockquote class="nw nx ny"><p id="faa3" class="ku kv mo kw b kx ky ju kz la lb jx lc nz le lf lg oa li lj lk ob lm ln lo lp im bi translated">自动编码器</p></blockquote><ul class=""><li id="fc03" class="nd ne it kw b kx ky la lb ld oc lh od ll oe lp ni nj nk nl bi translated">一种无损质量的压缩数据的新方法</li></ul></div></div>    
</body>
</html>