<html>
<head>
<title>Fit a Linear Regression Model with Gradient Descent from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始用梯度下降拟合线性回归模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fit-a-linear-regression-model-with-gradient-descent-from-scratch-d9bb41bc821e?source=collection_archive---------12-----------------------#2019-12-28">https://towardsdatascience.com/fit-a-linear-regression-model-with-gradient-descent-from-scratch-d9bb41bc821e?source=collection_archive---------12-----------------------#2019-12-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eee3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">实施梯度下降以找到简单线性回归的最佳权重。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/22da73274bb298713b1f9d37fde5fb4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*cCk_YFaUAuqQYsyT5dA8OA.png"/></div></figure><p id="dd9b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们都知道 sklearn 可以为我们试衣模特。但是当我们调用<code class="fe lm ln lo lp b">.fit()</code>时，我们知道它实际上在做什么吗？请继续阅读，寻找答案。</p><p id="2ccf" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">今天我们将编写一组实现梯度下降以拟合线性回归模型的函数。然后，我们将我们的模型的重量与一个合适的 sklearn 模型的重量进行比较。</p><h1 id="1012" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated"><strong class="ak">所需背景</strong></h1><p id="3e26" class="pw-post-body-paragraph kq kr it ks b kt mi ju kv kw mj jx ky kz mk lb lc ld ml lf lg lh mm lj lk ll im bi translated"><strong class="ks iu">拟合</strong> =找出模型的偏差和系数，使误差最小化。</p><p id="10e7" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">误差</strong> =均方误差(MSE)。数据集内实际值和预测值之间的平方差的平均值。</p><p id="e6a7" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">简单线性回归</strong> =基于一条线的方程的模型，“y=mx+b”。它将单个特征作为输入，应用偏差和系数，并预测 y。</p><p id="e8db" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">此外，系数和偏差有时也统称为“权重”。</p><h1 id="4c8c" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">准备数据集</h1><p id="105f" class="pw-post-body-paragraph kq kr it ks b kt mi ju kv kw mj jx ky kz mk lb lc ld ml lf lg lh mm lj lk ll im bi translated">从<a class="ae mn" href="https://www.kaggle.com/camnugent/california-housing-prices" rel="noopener ugc nofollow" target="_blank"> kaggle </a>下载加州住房数据集，并将其加载到数据框架中。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="a119" class="ms lr it lp b gy mt mu l mv mw">import pandas as pd<br/>df = pd.read_csv('california-housing-dataset.csv')<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/2892fc9228d1fcd1f2eace58b16f2ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgqDklrx9aheLtMPbWalFA.png"/></div></div></figure><p id="c760" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">对于这个数据集，我们通常尝试使用所有其他特征来预测<code class="fe lm ln lo lp b">median_house_value</code>。</p><p id="fe36" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">但是在我们的例子中，我们关心的是拟合一个简单的线性回归(它只接受一个单一的输入特征)，所以我们将选择<code class="fe lm ln lo lp b">median_income</code>作为那个特征，忽略其他的。这不会创建可能的最佳模型，但会使梯度下降的实现更简单。</p><p id="dca5" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">对数据集中的示例进行计数。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="ced0" class="ms lr it lp b gy mt mu l mv mw">len(df)<br/>#=&gt; 20640</span></pre><p id="7973" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这是大量的数据。让我们把尺寸缩小 75%。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="55b5" class="ms lr it lp b gy mt mu l mv mw">df = df.sample(frac=0.25)<br/>len(df)<br/>#=&gt; 5160</span></pre><p id="5f58" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">那更容易管理。</p><p id="3cea" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">收集我们的输入要素和标注。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="e878" class="ms lr it lp b gy mt mu l mv mw">X = df['median_income'].tolist()<br/>y = df['median_house_value'].tolist()</span></pre><h1 id="e259" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">用梯度下降拟合线性回归</h1><p id="b9be" class="pw-post-body-paragraph kq kr it ks b kt mi ju kv kw mj jx ky kz mk lb lc ld ml lf lg lh mm lj lk ll im bi translated">根据<a class="ae mn" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">维基百科</a>，</p><blockquote class="nc nd ne"><p id="1702" class="kq kr nf ks b kt ku ju kv kw kx jx ky ng la lb lc nh le lf lg ni li lj lk ll im bi translated"><strong class="ks iu">梯度下降</strong>是一种寻找函数局部极小值的一阶迭代优化算法。为了使用梯度下降找到函数的<a class="ae mn" href="https://en.wikipedia.org/wiki/Local_minimum" rel="noopener ugc nofollow" target="_blank">局部最小值</a>，在当前点采取与函数的<a class="ae mn" href="https://en.wikipedia.org/wiki/Gradient" rel="noopener ugc nofollow" target="_blank">梯度</a>(或近似梯度)的<em class="it">负</em>成比例的步骤。</p></blockquote><p id="a51a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">我的翻译:</strong></p><p id="48b1" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">梯度下降使用误差函数的梯度来预测系数<code class="fe lm ln lo lp b">m</code>和偏差<code class="fe lm ln lo lp b">b</code>应该在哪个方向更新，以减少给定数据集的误差。</p><p id="0ace" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">单独计算每个权重的梯度<code class="fe lm ln lo lp b">m</code>和<code class="fe lm ln lo lp b">b</code>。梯度是通过对所有例子的重量偏导数取平均值来计算的。</p><p id="58ee" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">梯度的方向和陡度决定了权重的更新方向和更新量。后者也受一个超参数，即学习率的影响。</p><p id="e702" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">该算法反复迭代训练集并更新权重，直到代价函数最小。</p><h2 id="65df" class="ms lr it bd ls nj nk dn lw nl nm dp ma kz nn no mc ld np nq me lh nr ns mg nt bi translated">履行</h2><p id="fe78" class="pw-post-body-paragraph kq kr it ks b kt mi ju kv kw mj jx ky kz mk lb lc ld ml lf lg lh mm lj lk ll im bi translated">你可以在互联网上找到 MSE 函数的偏导数(如下)，所以我们在这里不推导它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nu"><img src="../Images/d3f4b2d99fc0c4be66a05f2a2070368f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3B1QN9qA4UFdT5Dmtzj5Iw.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><a class="ae mn" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="be3f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们将用下面的代码实现上面的代码，同时遍历数据集。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="0a30" class="ms lr it lp b gy mt mu l mv mw">m_gradient += -(2/N) * x * (y - y_hat)<br/>b_gradient += -(2/N) * (y - y_hat)</span></pre><p id="ba15" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们写一个函数，它接受当前的权重、特征、标签和学习率，并输出更新后的权重。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="3dd9" class="ms lr it lp b gy mt mu l mv mw">def bias_coef_update(m, b, X, Y, learning_rate):<br/>    m_gradient = 0<br/>    b_gradient = 0<br/>    <br/>    N = len(Y)<br/>    <br/>    # iterate over examples<br/>    for idx in range(len(Y)):<br/>        x = X[idx]<br/>        y = Y[idx]<br/>                      <br/>        # predict y with current bias and coefficient<br/>        y_hat = (m * x) + b</span><span id="0523" class="ms lr it lp b gy nz mu l mv mw">        m_gradient += -(2/N) * x * (y - y_hat)<br/>        b_gradient += -(2/N) * (y - y_hat)<br/>        <br/>    # use gradient with learning_rate to nudge bias and coefficient<br/>    new_coef = m - (m_gradient * learning_rate)<br/>    new_bias = b - (b_gradient * learning_rate)<br/>    <br/>    return new_coef, new_bias</span></pre><p id="569d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">该函数的后半部分将梯度乘以学习率，并使用结果来更新当前权重。学习率越高，模型拟合得越快，代价是找到精确的局部最小值(注意:它实际上永远不会达到真正的最小值)。</p><p id="fd28" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">编写另一个函数，迭代应用上述函数一个设定的历元数。这是一种不太复杂的方法(为了简单起见),比在某个预定的梯度陡度返回拟合的权重要简单。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="b50c" class="ms lr it lp b gy mt mu l mv mw">def run(epoch_count=1000):<br/>    # store output to plot later<br/>    epochs = []<br/>    costs = []<br/>        <br/>    m = 0 <br/>    b = 0 <br/>    learning_rate = 0.01</span><span id="6914" class="ms lr it lp b gy nz mu l mv mw">    for i in range(epoch_count):<br/>        m, b = bias_coef_update(m, b, X, y, learning_rate)<br/>        print(m,b)<br/>        <br/>        C = cost(b, m, x_y_pairs)<br/>        <br/>        epochs.append(i)<br/>        costs.append(C)<br/>    <br/>    return epochs, costs, m, b</span><span id="82aa" class="ms lr it lp b gy nz mu l mv mw">epochs, costs, m, b = run()</span></pre><p id="bf6f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们输出拟合模型的最终成本和权重。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="1835" class="ms lr it lp b gy mt mu l mv mw">print(m)<br/>print(b)<br/>print(costs[-1])</span><span id="7125" class="ms lr it lp b gy nz mu l mv mw"># I've rounded these myself so they're nicer to look at<br/>#=&gt; 46,804<br/>#=&gt; 19,963<br/>#=&gt; 7,261,908,362</span></pre><p id="27a4" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">并绘制出它是如何随着时代而改进的。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="f7b2" class="ms lr it lp b gy mt mu l mv mw">import matplotlib.pyplot as plt</span><span id="6ff1" class="ms lr it lp b gy nz mu l mv mw">plt.xlabel('Epoch')<br/>plt.ylabel('Error')<br/>plt.suptitle('Cost by epoch')</span><span id="eacc" class="ms lr it lp b gy nz mu l mv mw">plt.plot(epochs,costs, linewidth=1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d2addffc3fa53e2baab554b228cd9902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*fCo1_sL6Vj1DjTGVG0iiSw.png"/></div></figure><p id="3c9f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">酷毙了。我们可以看到它的大部分进步都是在前 100 个纪元内取得的。</p><h1 id="e310" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">用 Sklearn 拟合线性回归</h1><p id="574d" class="pw-post-body-paragraph kq kr it ks b kt mi ju kv kw mj jx ky kz mk lb lc ld ml lf lg lh mm lj lk ll im bi translated">现在，我们将在装有 sklearn 的模型中检查相同的值。</p><p id="3a0a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">重塑要素。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="1ddd" class="ms lr it lp b gy mt mu l mv mw">import numpy as np<br/>X_array = np.array(X).reshape(5160,1)</span></pre><p id="f2fe" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">符合模型。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="74a4" class="ms lr it lp b gy mt mu l mv mw">from sklearn.linear_model import LinearRegression<br/>model = LinearRegression()<br/>model.fit(X_array,y)</span></pre><p id="593b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">检查重量和误差。</p><pre class="kj kk kl km gt mo lp mp mq aw mr bi"><span id="a367" class="ms lr it lp b gy mt mu l mv mw">from sklearn.metrics import mean_squared_error</span><span id="3aa6" class="ms lr it lp b gy nz mu l mv mw">m = model.coef_[0]<br/>b = model.intercept_</span><span id="1aca" class="ms lr it lp b gy nz mu l mv mw">mse = mean_squared_error(y_test, y_pred, sample_weight=None, multioutput='uniform_average')</span><span id="31bb" class="ms lr it lp b gy nz mu l mv mw">print(m)<br/>print(b)<br/>print(mse)</span><span id="347d" class="ms lr it lp b gy nz mu l mv mw"># rounded<br/>#=&gt; 42,324<br/>#=&gt; 41,356<br/>#=&gt; 7,134,555,443</span></pre><h1 id="4c4d" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">我们表现如何？</h1><p id="d299" class="pw-post-body-paragraph kq kr it ks b kt mi ju kv kw mj jx ky kz mk lb lc ld ml lf lg lh mm lj lk ll im bi translated">Sklearn 的调音以微弱优势超过我们，<code class="fe lm ln lo lp b">7,134,555,443</code>对<code class="fe lm ln lo lp b">7,261,908,362</code>，但我们非常接近。</p><p id="c1d6" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们的偏见也与 sklearn 发现的偏见大相径庭，<code class="fe lm ln lo lp b">41,356</code> VS <code class="fe lm ln lo lp b">19,963</code>。</p><p id="f376" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">不用深入研究每个时期后值是如何变化的(这很容易做到)，我想我们可以通过降低学习率和增加时期的数量来改进我们的模型。</p></div></div>    
</body>
</html>