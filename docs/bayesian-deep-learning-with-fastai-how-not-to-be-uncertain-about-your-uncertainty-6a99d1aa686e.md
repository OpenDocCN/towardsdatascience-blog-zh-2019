# 使用 Fastai 的贝叶斯深度学习:如何不确定自己的不确定性！

> 原文：<https://towardsdatascience.com/bayesian-deep-learning-with-fastai-how-not-to-be-uncertain-about-your-uncertainty-6a99d1aa686e?source=collection_archive---------1----------------------->

![](img/d1f06ea9434978a739dea3575f8474f9.png)

Bobby Axelrod speaks the words!

今天，神经网络已经成为许多领域的头条新闻，如癌症组织的图像分类，文本生成，甚至信用评分。然而，这些模型很少解决的一个大问题是预测的不确定性。

当我们人类学习时，我们最大的优势之一是知道我们的弱点，并且在有太多不确定性时不采取行动。然而，对于大多数机器学习模型来说，情况并非如此，在这些模型中，决策是在没有考虑不确定性的情况下做出的。

例如，如果你训练一个关于猫和狗的分类器，它将只能输出猫或狗。但如果你给它一张卡车的图片，模型仍然会做出预测，有时概率很大，即猫的概率接近 1，狗的概率接近 0，而你的模型应该简单地说“好吧，这是我无法识别的东西”。当我们着眼于医疗应用时，这一点尤其重要，因为医疗应用必须处理不确定性。

因此，理想情况下，我们希望有一种方法在我们的模型中添加不确定性评估，以便知道模型何时不应做出决定，并将其传递给能够做出正确决定的人，以帮助模型学习。这被称为**主动学习**，我们将在另一篇文章中讨论这个话题。

最后，获得不确定性的度量是非常可取的，因为这是我们人类所拥有的，而大多数模型都缺乏。在本文中，我们将看到如何在我们的模型中用几行代码实现这一点，并看到流行任务的结果，如情感分析、图像分类或信用评分。

**一.快速浏览贝叶斯神经网络**

![](img/defde87506d63d38f255245a6349a24e.png)

Bayesian Neural Networks seen as an ensemble of learners

贝叶斯神经网络(BNNs)是一种在我们的模型中增加不确定性处理的方法。这个想法很简单，我们不是学习确定性权重，而是学习随机变量的参数，我们将使用这些参数在前向传播过程中对权重进行采样。然后，为了学习参数，我们将使用反向传播，有时用一点小技巧使我们的参数可微。

![](img/c0d4ee6c9b4d49abb783dc361992caf4.png)

Comparison of a standard NN with a Bayesian NN

这张图可以帮助你理解其中的区别。虽然常规 NN 通过将预测损失反向传播到权重来学习权重，但这里我们使用分布的参数(这里是高斯分布)对 W 进行采样，并照常进行，但这次我们一直反向传播到高斯分布的参数(这里是均值和方差)。

尽管乍一看，使用随机变量而不是确定性权重来对权重进行采样似乎很奇怪，但如果您认为这不是一个单一的网络，而是一个潜在的无限网络集合，这实际上是有意义的。

事实上，每次你对你的网络的权重进行采样，就像从一个群体中采样一个批评家，并且每次向前传递可以有不同的输出，因为对于你采样的每个批评家。即使你可能有大量(甚至无限)的批评者，每个批评者都将帮助整个群体变得更好，因为我们将对这个给定批评者的表现使用反向传播，以适应整个群体的参数。在上面的例子中，我们不是只学习 **W** ，而是学习 **W** 的均值和方差。

所以最后，使用贝叶斯神经网络时的训练过程如下:

*   从 BNN 分布的随机参数开始，即使用一些参数初始化您的总体，例如，如果您想要高斯权重，从 0 均值和单位方差开始。
*   对于训练循环中的每一批:
    -根据与每一层相关的分布对你的体重进行抽样。
    -使用这些权重进行正向传递，就像使用常规网络一样
    -将损失反向传递给生成权重的分布参数

数学细节将在另一篇文章中解释，这里我们将只关注 bnn 如何工作的全局思想，所以如果你记得 bnn 的行为就像一种神经网络的集合，这已经是很好的一步了！

那么最后，为什么我们要学习权重的概率分布，而不是直接学习权重呢？有两个主要原因:

*   首先，这防止了过度拟合
*   第二，这允许我们得到不确定性估计

我们将更详细地讨论第二点。事实上，因为我们的模型本质上是随机的，所以对于相同的输入，我们的模型可以有不同的输出，因为对于每个前向传递，我们将有不同的权重被采样，因此可能有不同的输出。

然后，您可以选择采样 **T** 次，这样您将拥有 **T** 不同的权重，以及 **T** 不同的输出。因此，**您现在可以通过对这些结果进行平均来进行投票，但是您也可以计算方差来查看您的模型之间的一致程度！**

通过相同输入的预测方差，贝叶斯神经网络能够输出关于其预测的不确定性。让我们来看看下面这个简单的回归问题:

![](img/f78ad7ced0d191007c09dca39f2c8590.png)

Source : [http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html)

这里，我们使用蓝色虚线之前的训练数据点，并要求模型预测虚线之前和之后的点。我们训练一个贝叶斯神经网络，输出几个我们平均的估计值，并基于方差用置信区间来绘制它。

有趣的是，我们注意到模型在左侧输出的点没有任何差异，正如在训练期间所看到的那样，模型对其预测很有信心，但之后，随着我们看到的点离我们的训练集越来越远，差异也在增加。

这是意料之中的，因为我们不知道虚线之外是什么，所以我们越往前走，不确定性越大的模型是合理的。

现在我们已经了解了贝叶斯神经网络的原理，以及如何使用它们来输出模型的不确定性，我们将看到一种简单的方法，用几行代码就可以使您的模型贝叶斯化！

**二。辍学作为一种方式，使你的 NNs 贝叶斯**

Dropout 是一种流行的正则化技术，其目标是通过在给定层中随机将激活设置为零来帮助减少过拟合。

![](img/271f29855451434f956c71d7870a3a4c.png)

Illustration of Dropout from the paper “Dropout: A Simple Way to Prevent Neural Networks fromOverfitting”

这个想法很简单，我们不希望让一些神经元变得过于专业，只依赖这些神经元进行预测，因此我们在我们的层中随机置零神经元，每层都有不同的丢失概率 **p** 。这样，没有神经元变得过度专业化，这是一种确保所有神经元都对预测做出贡献的方法，而不会过度拟合。

Dropout 通常以下列方式使用:

*   在训练过程中，我们激活 Dropout，这意味着我们每一层都有不同的概率丢弃神经元。一旦神经元被丢弃，我们通常会除以保留神经元的概率，以保持相同的数量级。
*   在推理过程中，我们停用丢弃，因此我们使用所有的神经元，并且我们的输出是完全确定的，因为随机丢弃神经元的随机部分被关闭。

那么辍学和贝叶斯神经网络有什么关系呢？嗯，使用辍学可以被视为使用伯努利随机变量来采样你的权重，因此**简单地添加辍学，你使你的神经网络贝叶斯！**

数学在论文[“作为贝叶斯近似的辍学:表示深度学习中的模型不确定性”](https://arxiv.org/pdf/1506.02142.pdf)中进行了探索。我强烈推荐你阅读，尽管乍一看并不容易，但它解释了作为贝叶斯神经网络的辍学背后的许多理论，尤其是它们涵盖了什么是先验、后验和相关的 KL 项，但我们将在另一篇文章中讨论这一点。

因此，通过简单地添加 Dropout，我们现在了解了一个贝叶斯模型，其中权重是从行(或列，取决于矩阵乘法是如何完成的)伯努利分布中采样的。需要注意的一点是，训练将与之前完全相同，唯一的区别是在推断过程中，我们不关闭随机性，而是采样几个权重，将其传递给前向传递，并获得输出的分布。我们在推断过程中使用多个权重样本的事实被称为**蒙特卡洛遗漏(或 MC 遗漏)。**

![](img/7a383bb8438c5ff33a0c57b7afecb819.png)

How Dropout creates an ensemble of NNs which can output a distribution of results

在这里，我们可以看到压差如何为我们提供不同的网络，每个网络提供不同的输出，这允许我们对网络的输出进行分布。这样，我们就可以知道有不确定性度量，比如方差用于回归，或者如我们将看到的，熵用于分类。

因此，在我们用具体的例子探究这些结果之前，让我总结一下我们目前所看到的一切:

*   贝叶斯神经网络使用权重的概率分布，而不是具有确定性的权重。每次向前传递将采样不同的权重，因此具有不同的输出。人们可以将贝叶斯神经网络视为不具有单一模型，而是模型的集合，这为每个样本提供了不同的预测器。
*   假设相同的输入在向前传递后可以有不同的输出，我们可以通过查看不同预测的方差来了解我们的模型有多确定，因为如果模型不确定，我们预计方差会很高。
*   辍学是一种让你的神经网络贝叶斯几乎免费的方式，在推理过程中使用它，你只需要保持辍学，并对几个模型进行采样，这被称为 **MC 辍学。**

**三世。探索 MC 辍学的 Fastai 示例**

现在我们已经看到了 bnn 是如何工作的，以及如何用 Dropout 轻松地实现它们，我们现在将看到一些使用 Fastai 的例子。

代码可以在这个 Github 的资源库中找到:[https://github.com/DanyWind/fastai_bayesian](https://github.com/DanyWind/fastai_bayesian)，如果你想直接在 Colab 上运行，这里有链接:[https://Colab . research . Google . com/drive/1 bifawl _ o _ _ zkfsvve 9994 wj2 CFD 9976 I](https://colab.research.google.com/drive/1bIFAWl_o__ZKFSVvE9994WJ2cfD9976i)

无论任务是什么，流程都是一样的:

*   首先用我们都知道的 Fastai API 训练你的模型:lr_find，fit_one_cycle 等…
*   最后，要在 MC Dropout 推断期间保持 Dropout，只需添加以下代码行:

定义函数和类的代码行就在这里，它们并不复杂。我选择用一个模块函数来替换另一个模块，因为我们将在以后的文章中使用它来计算自动退学率。

Colab 中也定义了度量标准，但我们不会涉及它们，因为它非常标准。我们将特别使用一个只对分类有效的不确定性度量:**熵。**

如果有 **C** 类要预测，并且我们采样 **T** 倍我们的模型进行推断，那么熵被定义为:

![](img/6d76c80bdddd49e3ddb7db414f1eee43.png)

Definition of Entropy

这个度量可以看作是对无序度的度量，无序度越高，我们模型的不确定性就越高。这比简单地查看分类概率的方差更有意义，因为 softmax 有压缩一切的趋势。

**A .猫狗分类器**

![](img/d3345218e684bbcb6352da19e4cc5100.png)

Cats and Dogs dataset

在这里，我们将重点放在猫和狗的数据集。从这张图片上可以看出，我们将尝试对图片上的狗或猫的品种进行分类。

我不会详细介绍如何训练该模型，这是使用 Resnet34 的经典迁移学习，您可以直接查看代码以了解更多细节。

更有趣的是，用我们用辍学训练的模型做推断，看看猫和狗的常规图像，也看看分布图像，看看模型有多确定。

![](img/07a40fa7c79edf96e589b82e57c0a95c.png)

Entropy of regular and out of distribution images

正如您在这里看到的，已知模型的图像具有低熵，而完全不在分布范围内的图像具有高熵。有趣的是，我们注意到 softmax 的概率并没有给出不确定性的信息，因为在 Colab 笔记本中，我们可以看到分配给夜王类的最高概率是 90%，甚至超过了猫的 78%！

您可以看到，在某些应用中，测量不确定性是多么重要，尤其是在医疗领域，**例如，如果我们想对一个人是否应该接受化疗进行分类，我们希望知道我们的模型有多确定，如果不确定，就让一个人参与进来。**

![](img/cf5433c90f3ccc76f0689eb27b228b7a.png)

Distribution of entropy based on classification results on Cats and Dogs

每个分类结果的熵分布直方图也可以帮助我们诊断我们的模型。正如我们在上面可以看到的，当我们的模型正确执行时，即当存在真阳性或真阴性时，我们观察到熵往往比它错误时(即假阳性和假阴性)低得多。

有趣的是，无论我们的模型正确与否，熵的表现都是不同的，高熵似乎与错误分类高度相关！如果我们回头看看夜王的例子，我们会发现 2.2 的熵确实很高，并且通常出现在错误分类的样本中，而普通猫的熵通常只有 0.7。

**B .使用 IMDB 评论进行情感分析**

现在我们将快速回顾另一个著名的数据集:IMDB 评论。这个数据集只是收集正面和负面的评论。这里我们将跳过绘制直方图，直接显示数据集最确定和最不确定的样本:

下面的评论拥有最高的熵，这是可以理解的，因为评论很难定义是好是坏。即使我们人类可以感觉到整体情绪是好的，但这也不是微不足道的，我们可以预计机器会更加挣扎。

```
(Text xxbos xxmaj just got my copy of this xxup dvd two disc set and while not perfect , i found the overall experience to be a fun way to waste some time . i have to say right up front that i am a huge fan of xxmaj zombie movies , and i truly think that the fine people who made these films must be too . i also have a soft spot for people who are trying , sometimes against all odds , to live a dream . xxmaj and again , these people are doing it . xxmaj is this some award - winning collection of amazing film ? xxmaj no . xxmaj not even close . xxmaj but for what they do on their xxunk xxunk , these films should be recommended . xxmaj for me , the bottom line is always , was i entertained ? xxmaj did i have a good time with this movie ? xxmaj and here the answer to both was " xxmaj yes ... Category positive)
```

**C .成年人收入分类表**

最后，我们将在这里探索成人收入数据集，我们试图根据社会经济变量来预测某人的年收入是否超过 5 万英镑。我们也将看看最确定和不确定的预测。

![](img/9a1c0aa57b5c76170b297dd93013ef91.png)

我认为这个例子很有趣，因为它可能会对没有考虑不确定性的模型提出一些伦理问题。

事实上，该模型对预测某人是否有高工资非常有信心，这些人通常是白人、男性和受过高等教育的人。因此，该模型很容易区分这些情况。

但另一方面，当处理第二种更困难的情况时，这个模型就更不确定了，比如一个白人男性，受过一些大学教育，工作时间比平均水平长得多。这里的模型是相当不确定的，因为这个人可能在 50K 以上或以下。相反，如果目标是决定是否给予信用，一个好的模型应该让人类在存在巨大不确定性时决定是否做出决定，我们可能会重现模型中可能存在的偏差。

**结论**

在这篇文章中，我们已经看到了贝叶斯神经网络是什么，我们如何在已经训练好的架构上快速使用它们的属性，通过使用简单的 MC Dropout 在推断过程中简单地利用 Dropout，最后我们如何通过这些模型获得不确定性估计。

这些不确定性估计是宝贵的，因为它们在人工智能的大多数应用中是至关重要的，不仅可以为可信任的人工智能提供保障，而且正如我们将在未来的文章中看到的那样，还可以帮助我们通过主动学习将人类置于循环中。

所以，我希望你喜欢读这篇文章，如果能得到你的反馈，那就太好了，我会在未来试着发布更多的文章。

如果您有任何问题，请不要犹豫，通过 [Linkedin](https://www.linkedin.com/in/dhuynh95/) 联系我，您也可以通过 [Twitter 找到我！](https://twitter.com/dhuynh95)