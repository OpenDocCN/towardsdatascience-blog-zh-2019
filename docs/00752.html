<html>
<head>
<title>A Journey Into Big Data with Apache Spark — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark 大数据之旅—第 2 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-2-4511aa19a900?source=collection_archive---------19-----------------------#2019-02-04">https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-2-4511aa19a900?source=collection_archive---------19-----------------------#2019-02-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7b7f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">介绍如何用 Scala 构建第一个 Apache Spark 应用程序来读取 CSV 文件。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ae94835ce6517997c070444db1479829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mL25ylEXNDTwc3Rzvy7tww.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">A tabular view of a DataFrame from a CSV file in Apache Spark</figcaption></figure><p id="591c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">欢迎回到(我希望成为的)关于了解<a class="ae lr" href="https://spark.apache.org" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>的系列文章的第二部分。</p><p id="0013" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在第一集<a class="ae lr" rel="noopener" target="_blank" href="/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2">中，我们学习了如何使用 Docker 创建和运行 Apache Spark 集群。如果你还没有阅读，你可以在这里阅读。我将使用该集群作为运行我的 Spark 应用程序的集群，因此启动并运行它对您来说非常有用。</a></p><p id="1ceb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我将使用 Scala 构建应用程序，因为我也想了解这一点。Scala 并不是我经常使用的东西，所以当我弄清楚一些事情的时候，请一定要忍耐一下:)。如果有更好的方法来做我做的任何事情，请让我知道——我随时欢迎反馈！</p><h2 id="2d67" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">开始之前</h2><p id="5ec6" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">本文不会深入探讨如何设置和配置 Scala 应用程序，因为我们的目标是快速启动和运行，并深入 Spark 的世界。Scala world 确实让这变得简单了，它提供了一个叫做 SBT 的工具，Scala Build Tool，有一些值得注意的东西只会让这变得更简单。</p><h2 id="b140" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">设置我们的环境</h2><p id="ef53" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">如果你读过我之前的帖子，我暗示我是一个 Docker 的超级粉丝，以真正的粉丝风格，我将使用已经包含 Scala 和 SBT 的 Docker 图像来帮助我的开发。我将要使用的<em class="mq"> Dockerfile </em>可以在这里找到<a class="ae lr" href="https://github.com/ls12styler/scala-sbt/blob/master/Dockerfile" rel="noopener ugc nofollow" target="_blank">。这个<em class="mq"> Dockerfile </em>很棒，因为它在构建时接受环境参数，安装由这些参数指定的特定版本的 Scala 和 SBT(非常感谢原作者&amp;贡献者！).我在<em class="mq"> Dockerfile </em>中添加了我自己的小改动，主要是将<code class="fe mr ms mt mu b">WORKDIR</code>设置为<code class="fe mr ms mt mu b">/project</code>(因为那是我挂载项目目录的地方，就像我的代码一样)，并添加了一个<code class="fe mr ms mt mu b">CMD</code>，以便在我们启动容器时启动到<code class="fe mr ms mt mu b">sbt</code>控制台中。我们可以通过运行以下命令来构建映像:</a></p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="94c1" class="ls lt iq mu b gy mz na l nb nc">docker build -t ls12styler/scala-sbt:latest \<br/>    --build-arg SCALA_VERSION=2.12.8 \<br/>    --build-arg SBT_VERSION=1.2.7 \<br/>    github.com/ls12styler/scala-sbt</span></pre><p id="3969" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您可以看到我们将构建的图像标记为<code class="fe mr ms mt mu b">ls12styler/scala-sbt:latest</code>，因此我们可以通过运行以下命令来简单地运行图像，让我们进入 bash shell，在我们之前配置的<code class="fe mr ms mt mu b">WORKDIR</code>中:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="8c37" class="ls lt iq mu b gy mz na l nb nc">docker run -it --rm <!-- -->ls12styler/scala-sbt:latest /bin/bash</span></pre><p id="9931" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以通过运行<code class="fe mr ms mt mu b">scala -version</code>和<code class="fe mr ms mt mu b">sbt sbtVersion</code>来验证安装，得到以下输出:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="6aca" class="ls lt iq mu b gy mz na l nb nc">bash-4.4# scala -version<br/>Scala code runner version 2.12.8 -- Copyright 2002-2018, LAMP/EPFL and Lightbend, Inc.<br/>bash-4.4# sbt sbtVersion<br/>[warn] No sbt.version set in project/build.properties, base directory: /<br/>[info] Set current project to root (in build file:/)<br/>[info] 1.2.7</span></pre><p id="14ce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了能够访问我们的本地文件，我们需要将一个卷从我们的工作目录挂载到正在运行的容器中的某个位置。我们可以通过简单地在我们的<code class="fe mr ms mt mu b">run</code>命令中添加<code class="fe mr ms mt mu b">-v</code>选项来做到这一点。我们将移除<code class="fe mr ms mt mu b">/bin/bash</code>，这样我们就可以直接进入<code class="fe mr ms mt mu b">sbt</code>控制台:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="8f27" class="ls lt iq mu b gy mz na l nb nc">docker run -it --rm -v `pwd`:/project ls12styler/scala-sbt:latest</span></pre><p id="222a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们在这里所做的就是将<code class="fe mr ms mt mu b">pwd</code>(当前工作目录)挂载到容器上的<code class="fe mr ms mt mu b">/project</code>目录下。当运行上述内容时，我们将在 SBT 控制台的那个路径上结束:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="6300" class="ls lt iq mu b gy mz na l nb nc">[warn] No sbt.version set in project/build.properties, base directory: /project<br/>[info] Set current project to project (in build file:/project/)<br/>[info] sbt server started at local:///root/.sbt/1.0/server/07b05e14c4489ea8d2f7/sock<br/>sbt:project&gt;</span></pre><h2 id="a204" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">设置我们的项目</h2><p id="32d2" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">在你的项目目录中创建一个名为<code class="fe mr ms mt mu b">build.sbt</code>的新文件，用你最喜欢的编辑器打开它。用以下内容填充它，这些内容实际上是从<a class="ae lr" href="https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications" rel="noopener ugc nofollow" target="_blank">官方 Spark 文档</a>中借用的，尽管略有改动:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="fb37" class="ls lt iq mu b gy mz na l nb nc">name := "MyFirstScalaSpark"<br/>version := "0.1.0"<br/>scalaVersion := "2.11.12"<br/>libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.0"</span></pre><p id="00d9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这给了我们一个最小的项目定义来开始。<em class="mq">注意:我们已经将 Scala 版本指定为 2.11.12，因为 Spark 是针对 Scala 2.11 编译的，但是容器上的 Scala 版本是 2.12。</em>在 SBT 控制台中，运行<code class="fe mr ms mt mu b">reload</code>命令，用新的构建设置刷新 SBT 项目:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="bea1" class="ls lt iq mu b gy mz na l nb nc">sbt:local&gt; reload<br/>[info] Loading project definition from /project/project<br/>[info] Loading settings for project project from build.sbt ...<br/>[info] Set current project to MyFirstScalaSpark (in build file:/project/)<br/>sbt:MyFirstScalaSpark&gt;</span></pre><p id="2a55" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您应该注意到控制台采用了我们项目的名称:<code class="fe mr ms mt mu b">MyFirstScalaSpark</code>。现在我们有了一个构建项目的环境。我们写点代码吧！</p><h2 id="9db0" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">我们的第一个应用</h2><p id="08af" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">我们将进一步跟踪 Spark 文档，只是为了测试我们到目前为止的进展。</p><p id="0b8a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">SBT 应用程序采用 Java 应用程序的标准目录结构，所以让我们在项目目录中创建一些新目录(使用<code class="fe mr ms mt mu b">-p</code>标志将递归地创建目录):<code class="fe mr ms mt mu b">mkdir -p ./src/main/scala/</code></p><p id="5360" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在新创建的目录中创建一个名为<code class="fe mr ms mt mu b">MyFirstScalaSpark.scala</code>的新文件，并在您喜欢的编辑器中打开它。添加以下内容(同样，从原始内容略作调整):</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="3132" class="ls lt iq mu b gy mz na l nb nc">package com.example<br/>import org.apache.spark.sql.SparkSession</span><span id="b1ba" class="ls lt iq mu b gy nd na l nb nc">object MyFirstScalaSpark {<br/>  def main(args: Array[String]) {<br/>    val SPARK_HOME = sys.env("SPARK_HOME")<br/>    val logFile = s"${SPARK_HOME}/README.md"<br/>    val spark = SparkSession.builder<br/>      .appName("MyFirstScalaSpark")<br/>      .getOrCreate()<br/>    val logData = spark.read.textFile(logFile).cache()<br/>    val numAs = logData.filter(line =&gt; line.contains("a")).count()<br/>    val numBs = logData.filter(line =&gt; line.contains("b")).count()<br/>    println(s"Lines with a: $numAs, Lines with b: $numBs")<br/>    spark.stop()<br/>  }<br/>}</span></pre><p id="b96e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于我们将在第 1 部分创建的集群上运行这个应用程序，我们知道将设置环境变量<code class="fe mr ms mt mu b">$SPARK_HOME</code>并指向 Spark Workers 上的正确目录。在上面的代码中，我们简单地检索了<code class="fe mr ms mt mu b">$SPARK_HOME</code>(应该是<code class="fe mr ms mt mu b">/spark</code>)环境变量的内容，将其插入到我们正在使用的 Spark 发行版附带的<code class="fe mr ms mt mu b">README.md</code>的文件路径中，创建我们的 Spark 会话，然后执行几个 MapReduce 过滤器来计算包含字母<code class="fe mr ms mt mu b">a</code>或<code class="fe mr ms mt mu b">b</code>的各种行数。然后，我们将这些计数输出到控制台。</p><p id="41b8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们已经有了一些需要实际编译的代码，我们可以创建一个 jar 并提交给 Spark 集群。在 SBT 控制台中，只需运行<code class="fe mr ms mt mu b">package</code>来生成 jar。您应该会看到类似于以下内容的输出:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="1292" class="ls lt iq mu b gy mz na l nb nc">sbt:MyFirstScalaSpark&gt; package<br/>[info] Updating ...<br/>[info] Done updating.<br/>[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.<br/>[info] Compiling 1 Scala source to /project/target/scala-2.11/classes ...<br/>[info] Done compiling.<br/>[info] Packaging /project/target/scala-2.11/myfirstscalaspark_2.11-1.0.jar ...<br/>[info] Done packaging.<br/>[success] Total time: 11 s, completed Dec 27, 2018 3:22:16 PM</span></pre><p id="449e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如您所见，jar 已经在容器内的<code class="fe mr ms mt mu b">/project/target/scala-2.11/myfirstscalaspark_2.11-1.0.jar</code>输出，这意味着在本地，我们可以在<code class="fe mr ms mt mu b">`pwd`/target/scala-2.11/</code>中找到 jar。</p><h2 id="ecb7" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">提交我们的申请</h2><p id="4346" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">现在是时候让我们在第 1 部分中创建的集群起死回生了！找到包含<code class="fe mr ms mt mu b">docker-compose.yml</code>的目录并运行:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="19b3" class="ls lt iq mu b gy mz na l nb nc">docker-compose up --scale spark-worker=2</span></pre><p id="f128" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这将产生一个 Spark Master 和两个 Spark Workers，这足以证明我们的第一个应用程序实际上是可行的。</p><p id="331b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的项目目录中，我们可以使用我们在集群中创建的相同 Docker 映像作为我们的 Spark 驱动程序。Spark 驱动程序是我们向 Spark 集群提交应用程序的地方的名称，我们可以使用以下命令启动它:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="edca" class="ls lt iq mu b gy mz na l nb nc">docker run --rm -it -e SPARK_MASTER="spark://spark-master:7077" \<br/>  -v `pwd`:/project --network docker-spark_spark-network \<br/>  ls12styler/spark:latest /bin/bash</span></pre><p id="c353" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个命令中，我们设置了环境变量<code class="fe mr ms mt mu b">$SPARK_MASTER</code>的内容，在容器上的<code class="fe mr ms mt mu b">/project</code>下安装了<code class="fe mr ms mt mu b">pwd</code>，将它附加到我们创建的 Docker 网络，并放入一个 bash shell 中。要提交我们的申请，只需将其提交给 spark driver:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="e29e" class="ls lt iq mu b gy mz na l nb nc">bash-4.4# spark-submit --master $SPARK_MASTER \<br/>  --class com.example.MyFirstScalaSpark \<br/>  /project/target/scala-2.11/myfirstscalaspark_2.11-0.0.1.jar</span></pre><p id="5914" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我们提交应用程序时，我们指定 Spark Master 的 URI、要运行的类名以及该类所在的 jar。当我们在项目目录中启动容器时，Spark Driver 容器可以访问我们构建的 jar，而不需要我们在底层文件系统中复制它。在提交给 Spark 的日志中，您会看到我们在代码输出中构建的行:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="d706" class="ls lt iq mu b gy mz na l nb nc">...<br/>Lines with a: 62, Lines with b: 31<br/>...</span></pre><p id="1191" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们检查群集的日志，我们会看到如下所示的内容:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="12d7" class="ls lt iq mu b gy mz na l nb nc">spark-master    | 2018-12-27 16:24:10 INFO  Master:54 - Registering app MyFirstScalaSpark<br/>spark-master    | 2018-12-27 16:24:10 INFO  Master:54 - Registered app MyFirstScalaSpark with ID app-20181227162410-0005<br/>spark-master    | 2018-12-27 16:24:10 INFO  Master:54 - Launching executor app-20181227162410-0005/0 on worker worker-20181227134310-172.21.0.4-39747<br/>spark-master    | 2018-12-27 16:24:10 INFO  Master:54 - Launching executor app-20181227162410-0005/1 on worker worker-20181227134310-172.21.0.3-42931<br/>spark-worker_1  | 2018-12-27 16:24:10 INFO  Worker:54 - Asked to launch executor app-20181227162410-0005/0 for MyFirstScalaSpark<br/>spark-worker_2  | 2018-12-27 16:24:10 INFO  Worker:54 - Asked to launch executor app-20181227162410-0005/1 for MyFirstScalaSpark</span></pre><p id="b9ce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这显示了我们的应用程序被注册到 Master 并被赋予一个 ID。然后在每个 Worker 上启动 Executors，然后在我们的应用程序运行时发生一系列其他事情。</p><p id="9621" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们已经成功构建了第一个基于 Scala 的 Spark 应用程序，并在第 1 部分构建的集群上运行。恭喜你！</p><h2 id="581f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">还有一件事…</h2><p id="fef0" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">现在有一个小问题:这只在目前有效，因为我们很幸运地选择了一个文件，该文件可用于所有使用我们在第 1 部分中构建的相同映像的容器(Master、Worker &amp; Driver)。如果我们希望能够访问没有捆绑在映像中的文件，例如主机文件系统上的某个文件，我们需要与 Spark Workers 共享该文件系统。这可以通过在我们启动集群时在<code class="fe mr ms mt mu b">docker-compose</code>中挂载卷来轻松实现。在您的编辑器中打开<code class="fe mr ms mt mu b">docker-compose.yml</code>,并在工作者服务声明的末尾添加以下 YAML:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="3de6" class="ls lt iq mu b gy mz na l nb nc">    volumes:<br/>      - "./:/local"</span></pre><p id="9f08" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">保存文件并重新启动集群。现在，我们的 Spark 员工之间有了一个共享目录。接下来，我们需要与驱动程序共享同一个目录(我们提交应用程序的目录)。这只是为了方便起见，所以我们可以使用 bash 自动完成来构建文件路径，并将其作为参数传递给应用程序。我们可以通过更新我们的<code class="fe mr ms mt mu b">run</code>命令来包含新的卷(假设您运行<code class="fe mr ms mt mu b">docker-compose up</code>的目录与您的项目目录在同一级别):</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="4459" class="ls lt iq mu b gy mz na l nb nc">docker run --rm -it -e SPARK_MASTER="spark://spark-master:7077" \<br/>  -v `pwd`:/project -v `pwd`/../docker-spark:/local \<br/>  --network docker-spark_spark-network -w /project \<br/>  ls12styler/spark:latest /bin/bash</span></pre><p id="aa36" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个容器现在有了我们的项目目录和共享数据目录，可以分别在<code class="fe mr ms mt mu b">/project</code>和<code class="fe mr ms mt mu b">/local</code>访问。</p><h2 id="a74a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">在 Spark 中读取 CSV 文件</h2><p id="920e" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">在我们应用程序的第一次迭代中，我们使用了 Spark 中的<code class="fe mr ms mt mu b">read.textFile</code>函数来加载一个 README，这个 README 已经对工作人员可用了。接下来，我们将使用<code class="fe mr ms mt mu b">read.csv</code>，它将以一种我们可以执行操作的方式加载一个 CSV 文件。我将使用<a class="ae lr" href="https://www.gov.uk/government/publications/uk-space-agency-spending-report-october-2018" rel="noopener ugc nofollow" target="_blank">英国航天局支出报告:2018 年 10 月</a>数据来完成这篇文章的其余部分，我将把这些数据放入我在容器上的<code class="fe mr ms mt mu b">/local</code>下安装的目录中。首先，我们将简单地使用<code class="fe mr ms mt mu b">count</code>方法来查看文件中有多少行。我们还将通过传递给 jar 的命令行参数将 CSV 文件的文件路径传递给应用程序。</p><p id="05e9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在编辑器中，打开<code class="fe mr ms mt mu b">MyFirstScalaSpark.scala</code>文件并添加以下代码:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="cb2e" class="ls lt iq mu b gy mz na l nb nc">package com.example<br/>import org.apache.spark.sql.SparkSession</span><span id="3e44" class="ls lt iq mu b gy nd na l nb nc">object MyFirstScalaSpark {<br/>  def main(args: Array[String]) {<br/>    val spark = SparkSession.builder<br/>      .appName("MyFirstScalaSpark")<br/>      .getOrCreate()</span><span id="3db3" class="ls lt iq mu b gy nd na l nb nc">    val filePath = args(0)<br/>    val data = spark.read<br/>      .csv(filePath)</span><span id="6e5f" class="ls lt iq mu b gy nd na l nb nc">    println(data.count)<br/>    spark.stop()<br/>  }<br/>}</span></pre><p id="515d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们实际上只是添加了使用参数来指定文件路径，并使用它作为用 Spark 打开的文件。Spark 会将该文件加载到一个数据帧中。然后，我们将数据帧中的行数打印到控制台。在 SBT 控制台中再次运行<code class="fe mr ms mt mu b">package</code>来构建我们应用程序的新版本。提交构建的应用程序，这次传入我们使用的数据集的文件路径:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="44ee" class="ls lt iq mu b gy mz na l nb nc">spark-submit --master $SPARK_MASTER \<br/>  --class com.example.MyFirstScalaSpark \<br/>  target/scala-2.11/myfirstscalaspark_2.11-1.0.jar \<br/>  /local/UKSA_Oct_18_-_Transparency_Data.csv</span></pre><p id="af67" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在 Spark 的日志输出中，您应该看到数字<code class="fe mr ms mt mu b">689</code>。命令行上的快速检查显示了相同的内容:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="054f" class="ls lt iq mu b gy mz na l nb nc">bash-4.4# wc -l /local/UKSA_Oct_18_-_Transparency_Data.csv<br/>689 /local/UKSA_Oct_18_-_Transparency_Data.csv</span></pre><h2 id="6278" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">获得正确的结构</h2><p id="0cc2" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">然而，如果我们想要某种类似于“表格”的数据表示，行数可能不是这样。检查 CSV 文件的第一行显示该文件包含列标题，因此我们实际上只有 688 行数据。让我们来看看我们加载的数据的实际结构。在我们的代码中，我们可以添加<code class="fe mr ms mt mu b">data.printSchema</code>来做到这一点:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="a59e" class="ls lt iq mu b gy mz na l nb nc">    ...<br/>    val data = spark.read<br/>      .csv(filePath)</span><span id="a324" class="ls lt iq mu b gy nd na l nb nc">    <strong class="mu ir">data.printSchema</strong><br/>    ...</span></pre><p id="125d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe mr ms mt mu b">package</code>并提交以查看类似下面的输出:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="32df" class="ls lt iq mu b gy mz na l nb nc">root<br/> |-- _c0: string (nullable = true)<br/> |-- _c1: string (nullable = true)<br/> |-- _c2: string (nullable = true)<br/> |-- _c3: string (nullable = true)<br/> |-- _c4: string (nullable = true)<br/> |-- _c5: string (nullable = true)<br/> |-- _c6: string (nullable = true)<br/> |-- _c7: string (nullable = true)<br/> |-- _c8: string (nullable = true)<br/> |-- _c9: string (nullable = true)<br/> |-- _c10: string (nullable = true)</span></pre><p id="2498" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这并没有告诉我们太多。共有 11 列，都是<code class="fe mr ms mt mu b">string</code>类型，而且这些列都没有很好地命名。当使用 Spark 读取 CSV 文件时，我们可以指定一个选项，使用第一行作为列标题，然后使用剩余的行作为“表”的行。我们通过添加到<code class="fe mr ms mt mu b">read</code>行将它添加到我们的应用程序中:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="7e35" class="ls lt iq mu b gy mz na l nb nc">    val data = spark.read<br/>      <strong class="mu ir">.option("header", true)</strong><br/>      .csv(filePath)</span></pre><p id="a6d7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">运行<code class="fe mr ms mt mu b">package</code>构建一个新的 jar，然后再次提交给集群。我们现在应该可以在日志和下面的模式中看到数字<code class="fe mr ms mt mu b">688</code>。您可以看到，我们现在有了已命名的列，而不是按位置命名的列:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="2852" class="ls lt iq mu b gy mz na l nb nc">root<br/> |-- Department: string (nullable = true)<br/> |-- Entity: string (nullable = true)<br/> |-- Date of Payment: string (nullable = true)<br/> |-- Expense Type: string (nullable = true)<br/> |-- Expense Area: string (nullable = true)<br/> |-- Supplier: string (nullable = true)<br/> |-- Transaction Number: string (nullable = true)<br/> |-- Amount: string (nullable = true)<br/> |-- Description: string (nullable = true)<br/> |-- Supplier Post Code: string (nullable = true)<br/> |-- Supplier Type: string (nullable = true)</span></pre><p id="ff2f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以我们现在已经有了正确的行数和一个已命名列的结构——太好了！仔细看看这个模式——我们所有的值都是类型<code class="fe mr ms mt mu b">string</code>。同样，这不是很有帮助。我们应该和合适的类型一起工作。幸运的是，Spark 提供了另一个选项来尝试并最好地猜测文件的模式:<code class="fe mr ms mt mu b">inferSchema</code>。我们可以像对待第一个选项一样，将它添加到我们的代码中:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="8167" class="ls lt iq mu b gy mz na l nb nc">    val data = spark.read<br/>      .option("header", true)<br/>      <strong class="mu ir">.option("inferSchema", true)</strong><br/>      .csv(filePath)</span></pre><p id="4aa7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe mr ms mt mu b">package</code>然后提交，您将得到一个稍微改进的模式版本:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="c9ff" class="ls lt iq mu b gy mz na l nb nc">root<br/> |-- Department: string (nullable = true)<br/> |-- Entity: string (nullable = true)<br/> |-- Date of Payment: string (nullable = true)<br/> |-- Expense Type: string (nullable = true)<br/> |-- Expense Area: string (nullable = true)<br/> |-- Supplier: string (nullable = true)<br/> |-- Transaction Number: integer (nullable = true)<br/> |-- Amount: double (nullable = true)<br/> |-- Description: string (nullable = true)<br/> |-- Supplier Post Code: string (nullable = true)<br/> |-- Supplier Type: string (nullable = true)</span></pre><p id="4ee5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">更值得注意的是，<code class="fe mr ms mt mu b">Transaction Number</code>和<code class="fe mr ms mt mu b">Amount</code>字段现在分别是<code class="fe mr ms mt mu b">integer</code>和<code class="fe mr ms mt mu b">double</code>类型。其他的都保持为字符串，甚至是<code class="fe mr ms mt mu b">Date of Payment</code>列。让我们变得迂腐些，让它成为<code class="fe mr ms mt mu b">timestamp</code>类型。再次，火花来拯救！我们可以添加另一个选项来详细说明包含日期的列的格式:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="7f69" class="ls lt iq mu b gy mz na l nb nc"> val data = spark.read<br/>      .option("header", true)<br/>      .option("inferSchema", true)<br/>      <strong class="mu ir">.option("timestampFormat", "dd/MM/yyyy")</strong><br/>      .csv(filePath)</span></pre><p id="96b7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果模式应该如下所示:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="5c4b" class="ls lt iq mu b gy mz na l nb nc">root<br/> |-- Department: string (nullable = true)<br/> |-- Entity: string (nullable = true)<br/> |-- Date of Payment: timestamp (nullable = true)<br/> |-- Expense Type: string (nullable = true)<br/> |-- Expense Area: string (nullable = true)<br/> |-- Supplier: string (nullable = true)<br/> |-- Transaction Number: integer (nullable = true)<br/> |-- Amount: double (nullable = true)<br/> |-- Description: string (nullable = true)<br/> |-- Supplier Post Code: string (nullable = true)<br/> |-- Supplier Type: string (nullable = true)</span></pre><p id="84b6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意<code class="fe mr ms mt mu b">Date of Payment</code>列现在是<code class="fe mr ms mt mu b">timestamp</code>类型！我们现在有了一个类似表格的数据表示，其中包含了正确类型的列。来看看我们的数据吧！在<code class="fe mr ms mt mu b">data.printSchema</code>行之后，插入以下内容:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="567a" class="ls lt iq mu b gy mz na l nb nc">    ...<br/>    data.printSchema<br/>    <strong class="mu ir">data.show<br/>    ...</strong></span></pre><p id="d9c2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">打包并提交应用程序，在输出日志中，我们将看到显示的前 20 行:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="0105" class="ls lt iq mu b gy mz na l nb nc">+--------------------+---------------+-------------------+--------------------+--------------------+--------------------+------------------+----------+-----------+------------------+-------------+<br/>|          Department|         Entity|    Date of Payment|        Expense Type|        Expense Area|            Supplier|Transaction Number|    Amount|Description|Supplier Post Code|Supplier Type|<br/>+--------------------+---------------+-------------------+--------------------+--------------------+--------------------+------------------+----------+-----------+------------------+-------------+<br/>|Department for Bu...|UK Space Agency|2018-10-11 00:00:00|R &amp; D Current Gra...|UK Space Agency -...|SCIENCE AND TECHN...|            241835|   38745.0|       null|           SN2 1SZ|     WGA ONLY|<br/>|Department for Bu...|UK Space Agency|2018-10-17 00:00:00|R &amp; D Printing an...|UK Space Agency -...|ENGINEERING AND P...|            250485|   2256.94|       null|           SN2 1FF|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-23 00:00:00|R &amp; D Other Staff...|UK Space Agency -...|A.M. HEALTH &amp; SAF...|            253816|      37.5|       null|           SN5 6BD|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-17 00:00:00|R &amp; D Other Profe...|UK Space Agency -...|         QINETIQ LTD|            254217|   33320.0|       null|          GU14 0LX|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-17 00:00:00|R &amp; D Other Profe...|UK Space Agency -...|         QINETIQ LTD|            254318|   -5000.0|       null|          GU14 0LX|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-17 00:00:00|R &amp; D Other Profe...|UK Space Agency -...|         QINETIQ LTD|            254318|   10000.0|       null|          GU14 0LX|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-01 00:00:00|R &amp; D Other Profe...|         UKSA - UKSA|Border Consulting...|            255630|    571.06|       null|          GU51 3BE|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-01 00:00:00|R &amp; D Other Profe...|         UKSA - UKSA|Border Consulting...|            255630|  16289.65|       null|          GU51 3BE|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-01 00:00:00|R &amp; D Other Profe...|         UKSA - UKSA|Border Consulting...|            255630|  16289.65|       null|          GU51 3BE|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-02 00:00:00|R &amp; D Subsistence...|         UKSA - UKSA|Personal Expense,...|            256572|      2.42|       null|              null|     EMPLOYEE|<br/>|Department for Bu...|UK Space Agency|2018-10-02 00:00:00|R &amp; D Subsistence...|         UKSA - UKSA|Personal Expense,...|            256572|       5.1|       null|              null|     EMPLOYEE|<br/>|Department for Bu...|UK Space Agency|2018-10-02 00:00:00|R &amp; D Rail Travel...|         UKSA - UKSA|Personal Expense,...|            256572|      35.4|       null|              null|     EMPLOYEE|<br/>|Department for Bu...|UK Space Agency|2018-10-02 00:00:00|R &amp; D Subsistence...|         UKSA - UKSA|Personal Expense,...|            256572|     39.97|       null|              null|     EMPLOYEE|<br/>|Department for Bu...|UK Space Agency|2018-10-26 00:00:00|R &amp; D Other Profe...|UK Space Agency -...|      Cabinet Office|            256868|   -3570.0|       null|           FY5 3FW|     WGA ONLY|<br/>|Department for Bu...|UK Space Agency|2018-10-26 00:00:00|R &amp; D Other Profe...|UK Space Agency -...|      Cabinet Office|            256868|    1230.0|       null|           FY5 3FW|     WGA ONLY|<br/>|Department for Bu...|UK Space Agency|2018-10-26 00:00:00|R &amp; D Other Profe...|UK Space Agency -...|      Cabinet Office|            256868|    5910.0|       null|           FY5 3FW|     WGA ONLY|<br/>|Department for Bu...|UK Space Agency|2018-10-01 00:00:00|R &amp; D Current Gra...|UK Space Agency -...|CENTRE NATIONAL D...|            256966|  127900.0|       null|             31401|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-01 00:00:00|   R &amp; D Contractors|         UKSA - UKSA|Alexander Mann So...|            257048|   3435.24|       null|          EC2N 3AQ|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-01 00:00:00|   R &amp; D Contractors|UK Space Agency -...|Alexander Mann So...|            257065|   2532.19|       null|          EC2N 3AQ|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-02 00:00:00|R &amp; D Current Gra...|UK Space Agency -...|SCIENCE AND TECHN...|            257213|2498072.42|       null|           SN2 1SZ|     WGA ONLY|<br/>+--------------------+---------------+-------------------+--------------------+--------------------+--------------------+------------------+----------+-----------+------------------+-------------+<br/>only showing top 20 rows</span></pre><p id="a98b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们有一个有点宽的表(抱歉的格式！)，但你有望看到我们将要使用的一些价值观。</p><h2 id="cb3e" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">用我们的数据做些什么</h2><p id="b7ee" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">现在我们已经有了一个真正的表状数据结构，我们可以开始进行分析了。首先，让我们简单地按<code class="fe mr ms mt mu b">Date of Payment</code>列降序排列数据。当我们在数据帧上操作时，Spark 提供了许多函数来执行这样的操作。我们可以简单地使用<code class="fe mr ms mt mu b">orderBy</code>函数来做我们想做的事情。我们还将使用<code class="fe mr ms mt mu b">desc</code>函数，向它传递我们想要排序的列的名称。Spark 中对数据帧的任何操作都会返回一个新的数据帧，因此我们将把返回的数据帧分配给<code class="fe mr ms mt mu b">orderedData</code>并显示它。我们还将输出的行数限制为 5，只是为了保持输出最小化。</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="61e4" class="ls lt iq mu b gy mz na l nb nc">    ...<br/>    data.show<br/><br/>    <strong class="mu ir">val orderedData = data.orderBy(desc("Date of Payment"))<br/>    orderedData.show(5)</strong><br/>    ...</span></pre><p id="a995" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe mr ms mt mu b">package</code>并提交，我们应该看到以下输出:</p><pre class="kg kh ki kj gt mv mu mw mx aw my bi"><span id="8575" class="ls lt iq mu b gy mz na l nb nc">+--------------------+---------------+-------------------+--------------------+--------------------+--------------------+------------------+--------+-----------+------------------+-------------+<br/>|          Department|         Entity|    Date of Payment|        Expense Type|        Expense Area|            Supplier|Transaction Number|  Amount|Description|Supplier Post Code|Supplier Type|<br/>+--------------------+---------------+-------------------+--------------------+--------------------+--------------------+------------------+--------+-----------+------------------+-------------+<br/>|Department for Bu...|UK Space Agency|2018-10-31 00:00:00|R &amp; D Office Supp...|UK Space Agency -...| OFFICE DEPOT UK LTD|            261459|  145.57|       null|          SP10 4JZ|       VENDOR|<br/>|Department for Bu...|UK Space Agency|2018-10-31 00:00:00|  R &amp; D Other Travel|UK Space Agency -...|Personal Expense,...|            261508|    6.14|       null|              null|     EMPLOYEE|<br/>|Department for Bu...|UK Space Agency|2018-10-31 00:00:00|R &amp; D Other Profe...|UK Space Agency -...|Geospatial Insigh...|            261474| 13475.0|       null|           B46 3AD|        GRANT|<br/>|Department for Bu...|UK Space Agency|2018-10-31 00:00:00|R &amp; D Current Gra...|UK Space Agency -...|REACTION ENGINES LTD|            261327|167000.0|       null|          OX14 3DB|        GRANT|<br/>|Department for Bu...|UK Space Agency|2018-10-31 00:00:00|R &amp; D Hotel &amp; Acc...|         UKSA - UKSA|Personal Expense,...|            261505|   114.0|       null|              null|     EMPLOYEE|<br/>+--------------------+---------------+-------------------+--------------------+--------------------+--------------------+------------------+--------+-----------+------------------+-------------+<br/>only showing top 5 rows</span></pre><p id="a1b4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">恭喜你！您已经能够使用 Apache Spark 加载 CSV 文件，并学习了如何使用带有<code class="fe mr ms mt mu b">desc</code>的<code class="fe mr ms mt mu b">orderBy</code>方法执行基本排序。</p><p id="7d70" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这一版就完成了！下次回来时，我们将研究如何使用 Spark 来回答一些基于我们已有数据的商业问题。</p></div></div>    
</body>
</html>