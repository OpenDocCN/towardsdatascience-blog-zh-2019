<html>
<head>
<title>Linear Classifiers: An Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性分类器:综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-classifiers-an-overview-e121135bd3bb?source=collection_archive---------6-----------------------#2019-05-20">https://towardsdatascience.com/linear-classifiers-an-overview-e121135bd3bb?source=collection_archive---------6-----------------------#2019-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="792a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">本文讨论了四种流行的线性分类方法的数学属性和实际 Python 应用。</h2></div><p id="7687" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文基于优秀的 Hastie，t .，Tibshirani，r .，&amp; Friedman，J. H. (2009)的一章。统计学习的要素:数据挖掘、推理和预测。第二版。纽约:斯普林格。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/4fa79d8e1efffb34c1c49ca6a5ee5eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Hnd_TV9wu8KY5sWnN_sTw.png"/></div></div></figure><p id="9dec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用于解决分类任务的一类流行程序是基于线性模型的。这意味着，他们旨在将特征空间划分为根据目标可以采用的值标记的区域集合，其中这些区域之间的决策边界是线性的:它们是 2D 中的线、3D 中的平面和具有更多特征的超平面。<br/>本文回顾了流行的线性分类模型，提供了对所讨论的方法以及 Python 实现的描述。我们将介绍以下方法:</p><ul class=""><li id="f9f1" class="lq lr it kk b kl km ko kp kr ls kv lt kz lu ld lv lw lx ly bi translated">线性判别分析，</li><li id="73bc" class="lq lr it kk b kl lz ko ma kr mb kv mc kz md ld lv lw lx ly bi translated">二次判别分析，</li><li id="f516" class="lq lr it kk b kl lz ko ma kr mb kv mc kz md ld lv lw lx ly bi translated">正则化判别分析，</li><li id="87c9" class="lq lr it kk b kl lz ko ma kr mb kv mc kz md ld lv lw lx ly bi translated">逻辑回归。</li></ul><p id="adee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">出于演示的目的，我们将把每个讨论的方法应用于垃圾邮件数据集，其中的任务是基于描述电子邮件中使用的词频的一组特征来将电子邮件分类为垃圾邮件或非垃圾邮件。数据集以及一些变量的描述可以在哈斯蒂等人的网站<a class="ae me" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">“统计学习的要素”教科书</a>的数据部分找到。</p><p id="5755" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从导入本教程中使用的所有包并加载数据开始。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mf mg l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/4fa79d8e1efffb34c1c49ca6a5ee5eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Hnd_TV9wu8KY5sWnN_sTw.png"/></div></div></figure><h1 id="436b" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">线性判别分析</h1><p id="5eb7" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">要讨论的第一种方法是线性判别分析(LDA)。它假设所有特征的联合密度(取决于目标的类别)是一个多元高斯分布。这意味着，假设目标<em class="ne"> y </em>在类别<em class="ne"> k </em>中，特征<em class="ne"> X </em>的密度<em class="ne"> P </em>假定由下式给出</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7bc0a1c58be2908925e022e0c2b23e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*MAqoEp7oe8_R9yXXWAnHDw.png"/></div></figure><p id="e7de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="ne"> d </em>是特征的数量，μ是平均向量，σ_<em class="ne">k</em>是类别<em class="ne"> k </em>的高斯密度的协方差矩阵。</p><p id="8ccb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两个类之间的判定边界，比如说<em class="ne"> k </em>和<em class="ne"> l </em>，是属于任一类的概率相同的超平面。这意味着，在这个超平面上，两个密度之间的差异(以及它们之间的对数优势比)应该为零。</p><p id="71bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">LDA 中的一个重要假设是不同类别的高斯分布共享同一个协方差矩阵</strong>:上式中σ_<em class="ne">k</em>的下标<em class="ne"> k </em>可以去掉。这个假设对于计算对数优势比很方便:它使标准化因子和指数中的一些二次部分相互抵消。这产生了在<em class="ne"> k </em>和<em class="ne"> l </em>之间的判定边界，其在<em class="ne"> X </em>中是线性的:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/e10b6bb1df30cbb99a3c3dac7c338980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*-p-V1LhN9k2uoRyoUarEeA.png"/></div></figure><p id="1f3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了计算特征的密度，<em class="ne">P</em>(<em class="ne">X</em>|<em class="ne">y</em>=<em class="ne">k</em>)，只需估计高斯参数:均值μ <em class="ne"> _k </em>作为样本均值，协方差矩阵σ作为经验样本协方差矩阵。计算后，目标属于类别<em class="ne"> k </em>的概率可以从贝叶斯规则中获得:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/15c382842c736e3f4e16fd4e1df4a209.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*dFM6n3xYKpYY0AUaxprn9A.png"/></div></figure><p id="ba02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="ne">P</em>(<em class="ne">y</em>=<em class="ne">k</em>)是属于<em class="ne"> k </em>类的先验概率，可以通过样本中<em class="ne"> k </em>类观测值的比例来估计。</p><p id="ce45" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，LDA 没有要优化的超参数。只需几行代码就可以将其应用于垃圾邮件数据。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mf mg l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/4fa79d8e1efffb34c1c49ca6a5ee5eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Hnd_TV9wu8KY5sWnN_sTw.png"/></div></div></figure><h1 id="5fa2" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">二次判别分析</h1><p id="73ac" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">LDA 假设不同类的高斯分布共享相同的协方差矩阵，这很方便，但对于特定数据可能不正确。下图的左栏显示了 LDA 如何处理来自具有共同协方差矩阵的多元高斯分布的数据(上图)，以及不同类别的数据具有不同协方差时的情况(下图)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ni"><img src="../Images/8739ba2b6a1a8e623e79cf92ac7b7fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0Jf0jSEAJOMMTeNZCapzg.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Source: <a class="ae me" href="https://scikit-learn.org/stable/modules/lda_qda.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/lda_qda.html</a></figcaption></figure><p id="89da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，人们可能希望放松公共协方差假设。在这种情况下，要估计的协方差矩阵不是一个，而是<em class="ne"> k </em>个。如果有许多特征，这可能导致模型中的参数数量急剧增加。另一方面，高斯指数中的二次项不再相互抵消，决策边界在<em class="ne"> X </em>中也是二次的，这给了模型更多的灵活性:见上图。这种方法被称为二次判别分析(QDA)。</p><p id="0c2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">多亏了 scikit-learn，QDA 的 Python 实现和 LDA 一样简单。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mf mg l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/4fa79d8e1efffb34c1c49ca6a5ee5eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Hnd_TV9wu8KY5sWnN_sTw.png"/></div></div></figure><h1 id="3a63" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">正则判别分析</h1><p id="bb6f" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">正如<a class="ae me" rel="noopener" target="_blank" href="/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16">用于回归的线性模型可以被正则化</a>以提高准确性，线性分类器也可以。可以引入收缩参数α，其将 QDA 的独立协方差矩阵收缩到公共 LDA 矩阵:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/782015905b7f1a91ab2931a4e093fee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*WmaZjrkWW3zeemSz1D3N8w.png"/></div></figure><p id="9d86" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">收缩参数可以取从 0 (LDA)到 1 (QDA)的值，任何介于两者之间的值都是两种方法的折衷。α的最佳值可以基于交叉验证来选择。要在 Python 中做到这一点，我们需要将收缩参数传递给 LDA 函数，并将计算算法指定为最小二乘法，因为其他计算方法不支持收缩。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mf mg l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/4fa79d8e1efffb34c1c49ca6a5ee5eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Hnd_TV9wu8KY5sWnN_sTw.png"/></div></div></figure><h1 id="6839" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">逻辑回归</h1><p id="56c2" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">线性分类的另一种方法是逻辑回归模型，尽管它的名字，它是一种分类而不是回归方法。<strong class="kk iu">逻辑回归通过线性函数对属于每个<em class="ne"> K </em>类的观察值的概率进行建模，确保这些概率总和为 1，并保持在(0，1)范围内。</strong>根据<em class="ne"> K </em> -1 对数优势比指定模型，选择任意类别作为参考类别(在本例中是最后一个类别，<em class="ne"> K </em>)。因此，属于给定类别和属于参考类别的对数概率之间的差异被线性建模为</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi no"><img src="../Images/3b615f8712d48217a21b55dcafe9b0bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*uys5W0f69vanYyxCbDDJSA.png"/></div></figure><p id="fbd2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="ne"> G </em>代表真实的、被观察的类。从这里，属于每个类别的观察值的概率可以计算如下</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi np"><img src="../Images/6ac0afe5421f569f790b46db0bc5417a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGRb-uYXIQzJw6LSyu9Fhg.png"/></div></div></figure><p id="433d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这清楚地表明所有类别的概率总和为 1。</p><p id="d36a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">逻辑回归模型通常由 scikit-learn 负责的最大似然估计。正如<a class="ae me" rel="noopener" target="_blank" href="/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16">回归的线性模型可以被正则化</a>以提高准确性，逻辑回归也是如此。事实上，<em class="ne"> L2 </em>惩罚是 scikit-learn 中的默认设置。它还支持<em class="ne"> L1 </em>和<em class="ne">弹性网</em>惩罚(要了解更多信息，请查看上面的链接)，但并非所有解算器都支持它们。<a class="ae me" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn 的逻辑回归文档</a>有详细描述。</p><p id="f057" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然逻辑回归主要用作任务中的推理工具，目标是理解输入变量在解释结果中的作用(它产生易于解释的系数，就像线性回归一样)，但它也可以证明具有重要的预测能力，如下例所示。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mf mg l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/4fa79d8e1efffb34c1c49ca6a5ee5eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Hnd_TV9wu8KY5sWnN_sTw.png"/></div></div></figure><h1 id="f83c" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">总结和结论</h1><p id="2513" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">本文讨论了几个线性分类器:</p><ul class=""><li id="5d98" class="lq lr it kk b kl km ko kp kr ls kv lt kz lu ld lv lw lx ly bi translated"><strong class="kk iu">线性判别分析(LDA) </strong>假设给定目标类别的所有特征的联合密度是多元高斯分布，每个类别具有相同的协方差。公共协方差的假设是一个很强的假设，但如果正确，允许更有效的参数估计(较低的方差)。另一方面，该公共协方差矩阵是基于所有点来估计的，也包括那些远离决策边界的点。这使得 LDA 容易出现异常值。</li><li id="1ee9" class="lq lr it kk b kl lz ko ma kr mb kv mc kz md ld lv lw lx ly bi translated"><strong class="kk iu">二次判别分析(QDA) </strong>通过为每个类别估计单独的协方差矩阵，放松了 LDA 的公共协方差假设。这给了模型更多的灵活性，但是在许多特征的情况下会导致模型中参数数量的急剧增加。</li><li id="1c3d" class="lq lr it kk b kl lz ko ma kr mb kv mc kz md ld lv lw lx ly bi translated"><strong class="kk iu">正则化判别分析</strong>是 LDA 和 QDA 之间的一种折衷:正则化参数可以调整为将协方差矩阵设置在所有类别的 1(LDA)和每个类别的完全分离(QDA)之间的任何位置。</li><li id="4dc0" class="lq lr it kk b kl lz ko ma kr mb kv mc kz md ld lv lw lx ly bi translated"><strong class="kk iu">逻辑回归</strong>通过线性函数模拟属于每个类别的观察值的概率。一般认为它比判别分析方法更安全、更稳健，因为它依赖的假设更少。对于我们的示例垃圾邮件数据来说，这也是最准确的。</li></ul><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/4fa79d8e1efffb34c1c49ca6a5ee5eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Hnd_TV9wu8KY5sWnN_sTw.png"/></div></div></figure><p id="18f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！我希望你已经学到了对你的项目有益的东西🚀</p><p id="f8b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你喜欢这篇文章，试试我的另一篇文章。不能选择？从这些中选择一个:</p><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">线性回归中收缩法和选择法的比较</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">详细介绍 7 种流行的收缩和选择方法。</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh lo nt"/></div></div></a></div><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/boost-your-grasp-on-boosting-acf239694b1"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">增强你对助推的把握</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">揭秘著名的竞赛获奖算法。</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="oi l oe of og oc oh lo nt"/></div></div></a></div><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/model-selection-assessment-bb2d74229172"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">模型选择和评估</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">超越火车价值测试分裂</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="oj l oe of og oc oh lo nt"/></div></div></a></div><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/4fa79d8e1efffb34c1c49ca6a5ee5eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Hnd_TV9wu8KY5sWnN_sTw.png"/></div></div></figure><h1 id="9d78" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">来源</h1><ol class=""><li id="e855" class="lq lr it kk b kl mz ko na kr ok kv ol kz om ld on lw lx ly bi translated">Hastie，Tibshirani，r .，，j . h . Friedman(2009 年)。统计学习的要素:数据挖掘、推理和预测。第二版。纽约:斯普林格。</li><li id="fc63" class="lq lr it kk b kl lz ko ma kr mb kv mc kz md ld on lw lx ly bi translated"><a class="ae me" href="https://scikit-learn.org/stable/modules/lda_qda.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/lda_qda.html</a></li><li id="b80c" class="lq lr it kk b kl lz ko ma kr mb kv mc kz md ld on lw lx ly bi translated"><a class="ae me" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . linear _ model。LogisticRegression.html</a></li></ol></div></div>    
</body>
</html>