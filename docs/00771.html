<html>
<head>
<title>Review: LapSRN &amp; MS-LapSRN — Laplacian Pyramid Super-Resolution Network (Super Resolution)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:LapSRN 和 MS-lap srn-拉普拉斯金字塔超分辨率网络(超分辨率)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8?source=collection_archive---------10-----------------------#2019-02-05">https://towardsdatascience.com/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8?source=collection_archive---------10-----------------------#2019-02-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4d74" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">渐进重构残差、Charbonnier 损失、参数共享、局部残差学习，优于 SRCNN、VDSR、DRCN、DRRN</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4ae3e53020290ed5430c92d704da5495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cUNPPG12JvHIJCcg.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">32×, 16×, 8×, 4× and 2× SR</strong></figcaption></figure><p id="f9e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>这个故事中，回顾了<strong class="ky ir"> LapSRN(拉普拉斯金字塔超分辨率网络)</strong>和<strong class="ky ir"> MS-LapSRN </strong> <strong class="ky ir">(多尺度拉普拉斯金字塔超分辨率网络)</strong>。通过<strong class="ky ir">逐步重构子带残差</strong>，利用<strong class="ky ir">夏邦尼尔损失函数</strong>，LapSRN 优于<a class="ae mb" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"> SRCNN </a>、<a class="ae mb" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4"> FSRCNN </a>、<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>和<a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"> DRCN </a>。有了<strong class="ky ir">参数共享</strong>、<strong class="ky ir">局部残差学习</strong>、<strong class="ky ir">、</strong>、<strong class="ky ir">多尺度训练</strong>，MS-LapSRN 甚至胜过<a class="ae mb" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994"> DRRN </a>。<strong class="ky ir"> LapSRN </strong>和<strong class="ky ir"> MS-LapSRN </strong>分别发表在<strong class="ky ir"> 2017 CVPR </strong>超过<strong class="ky ir"> 200 篇引用</strong>和<strong class="ky ir"> 2018 TPAMI </strong>数十篇引用<strong class="ky ir">。(<a class="mc md ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----c5fe2b65f5e8--------------------------------" rel="noopener" target="_blank">曾植和</a> @中)</strong></p><p id="5a8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于 MS-LapSRN 是 LapSRN 的扩展，我将只介绍 MS-LapSRN 论文中的内容，但它也涉及 LapSRN 和 MS-LapSRN 的方法和结果。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="b634" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">概述</h1><ol class=""><li id="7d65" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nk nl nm nn bi translated"><strong class="ky ir">以前方法中的问题</strong></li><li id="9ccb" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky ir"> LapSRN:架构</strong></li><li id="2ea3" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky ir"> LapSRN: Charbonnier 损失函数</strong></li><li id="0314" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky ir"> MS-LapSRN:参数共享</strong></li><li id="1690" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky ir"> MS-LapSRN:局部剩余学习(LRL) </strong></li><li id="c349" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky ir"> MS-LapSRN:多尺度训练</strong></li><li id="11ca" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky ir">消融研究</strong></li><li id="a579" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky ir">与最先进结果的比较</strong></li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="3acf" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 1。先前方法中的问题</strong></h1><ul class=""><li id="72fd" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nt nl nm nn bi translated">在以前的方法中有三个问题</li></ul><h2 id="2b4a" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">1.1.双三次插值</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/d670533e0277ff551e1a33a70c46d040.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*GZGvhiwqcVNSndMhOvJtyg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Bicubic Interpolation</strong></figcaption></figure><ul class=""><li id="65b9" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">双三次插值用于在进入网络之前放大输入 LR 图像。然而，该预上采样步骤<strong class="ky ir">增加了不必要的计算成本</strong>并且<strong class="ky ir">没有为重构 HR 图像</strong>提供额外的高频信息。</li></ul><h2 id="00e5" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">1.2.L2 损失</h2><ul class=""><li id="5086" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nt nl nm nn bi translated">现有方法利用 L2 损失(即，均方误差损失)来优化网络。</li><li id="6490" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">由于<strong class="ky ir">同一个 LR 面片可能有多个对应的 HR 面片</strong>而<strong class="ky ir"> L2 损失未能捕捉到 HR 面片</strong>的基本多模态分布，重建的 HR 图像往往<strong class="ky ir">过平滑</strong>和<strong class="ky ir">与自然图像上的人类视觉感知</strong>不一致。</li></ul><h2 id="30c7" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">1.3.单步上采样</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/60bcd5c9eba666b4cb67b6d80754baa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*aukrDSS2hyXrgSG7VQSxIA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">One-Step Upsampling</strong></figcaption></figure><ul class=""><li id="43d1" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">这种一步上采样不能很好地超分辨精细结构，这使得学习大比例因子(例如 8 倍)的映射函数更加困难。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="e316" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 2。LapSRN:建筑</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a3001a52ae1473942d1cd259e99c21ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*-0Ln6HW2U0C8dbhFhHr-5w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">LapSRN / MS-LapSRN Architecture</strong></figcaption></figure><ul class=""><li id="ca6f" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">与一步上采样相反，<strong class="ky ir">网络在多个金字塔等级，特别是在 log2( <em class="om"> S </em>等级，其中<em class="om"> S </em>是比例因子(即 2，4，8)，渐进地重建高分辨率图像的子带残差。</strong></li><li id="5ec3" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir"/><strong class="ky ir">不使用双三次，直接从低分辨率输入图像中提取特征，从而降低计算负荷。</strong></li><li id="f44d" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">(拉普拉斯金字塔已经用了几十年。它被称为拉普拉斯金字塔，因为在特征提取分支，在每一级都有一个残余图像输出。如果感兴趣，请访问<a class="ae mb" href="https://en.wikipedia.org/wiki/Pyramid_(image_processing)" rel="noopener ugc nofollow" target="_blank"> wiki 关于金字塔(图像处理)</a>，尤其是高斯金字塔&amp;拉普拉斯金字塔。)</li><li id="9908" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">有<strong class="ky ir">两个分支:</strong> <strong class="ky ir">特征提取&amp;图像重建</strong>。</li></ul><h2 id="028c" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">2.1.<strong class="ak">特征提取</strong></h2><ul class=""><li id="47e1" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nt nl nm nn bi translated"><strong class="ky ir">在级别<em class="om"> s </em>，有<em class="om"> d </em>卷积层和一个转置卷积层</strong>(或去卷积层)以 2 倍的比例对提取的特征进行上采样。</li><li id="22ca" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">每个转置卷积层的输出<strong class="ky ir">连接到两个不同的层</strong> : <strong class="ky ir"> (1)用于在级别<em class="om"> s </em> </strong>重构残差图像的卷积层，以及<strong class="ky ir"> (2)用于在更精细级别<em class="om"> s </em> +1 </strong>提取特征的卷积层。</li><li id="c1ab" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir">较低级别的特征表示与较高级别</strong>共享，因此<strong class="ky ir">可以增加网络的非线性</strong>以使<strong class="ky ir">在更精细的级别学习复杂映射</strong>。</li></ul><h2 id="c1d0" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">2.2.图像重建</h2><ul class=""><li id="ad6a" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nt nl nm nn bi translated"><strong class="ky ir">在级别<em class="om"> s </em>，输入图像通过转置卷积(上采样)层以 2 的比例上采样。</strong>这一层用双线性内核初始化。</li><li id="4a27" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir">上采样图像然后与来自特征提取分支的预测残差图像</strong>组合(使用逐元素求和)以产生高分辨率输出图像。</li><li id="4b5a" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">级别<em class="om"> s </em>的输出 HR 图像然后被馈送到级别<em class="om"> s </em> +1 的图像重建分支。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="7cb6" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">3.<strong class="ak"> LapSRN: Charbonnier 损失函数</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/776ea601cd68084d555585993b5455b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*kW1n149HH6ceynh679zbCA.png"/></div></figure><p id="f923" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/2aa4258570a6c652b132cc7078d11c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*LoxK37HQAqyKFE3w6bJ7nw.png"/></div></figure><ul class=""><li id="9c26" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">LapSRN 不使用标准的 MSE 损失函数，而是使用上述损失函数。这个ρ函数是 Charbonnier 罚函数(l1 范数的可微变体)，它对于处理异常值是稳健的。</li><li id="ac8c" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><em class="om"> N </em>:一批样品数，<em class="om"> L </em>，金字塔级数，<em class="om"> ε </em>设为 1e-3。</li><li id="e0b2" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">这种深度监督<strong class="ky ir">指导网络训练预测不同级别的子带残差图像</strong>和<strong class="ky ir">产生多尺度输出图像</strong>。</li><li id="df83" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">8 倍模型可以在一个前馈通道中产生 2 倍、4 倍和 8 倍的超分辨率结果。</li><li id="3937" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">上述拉普拉斯金字塔网络体系结构和 Charbonnier 损失函数被用在 LapSRN 和 MS-LapSRN 中。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="118d" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">4.<strong class="ak"> MS-LapSRN:参数共享</strong></h1><h2 id="9657" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">4.1.跨金字塔等级的参数共享</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/fe95a12ecac9c1b52783cf537e8a1a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t63ivMf6BoNfpWVkayii5A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Parameter Sharing ACROSS Pyramid Levels</strong></figcaption></figure><ul class=""><li id="7fa5" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">正如我们在图中看到的，特征嵌入子网络、上采样层和残差预测层的参数在所有金字塔等级上共享。</li><li id="5553" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">因此，网络参数的数量与上采样尺度无关。</li></ul><h2 id="c87d" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">4.2.金字塔等级内的参数共享</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/78ec8cf3f72ac0506fc79c082e38f7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*6NzJBBfDN0BVOYVS93lJAg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Parameter Sharing WITHIN Pyramid Levels Using Recursive Blocks for the Feature Embedding Subnetwork</strong></figcaption></figure><ul class=""><li id="d6f4" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated"><strong class="ky ir">特征嵌入子网具有<em class="om"> R </em>递归块</strong>。<strong class="ky ir">每个递归块都有<em class="om"> D </em>个不同的卷积层</strong>，控制整个模型中参数的数量。</li><li id="b55f" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir">预激活结构(</strong> <a class="ae mb" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"> <strong class="ky ir">预激活 ResNet </strong> </a> <strong class="ky ir">)但没有批处理规范化层被用于</strong>递归块中。</li><li id="80f2" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">因此，总深度变为:</li></ul><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="158c" class="nu mm iq os b gy ow ox l oy oz">depth = (D × R + 1) × L + 2;<br/>L = log2(S)</span></pre><ul class=""><li id="d28f" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated"><em class="om"> S </em>是上采样比例因子。</li><li id="7cb2" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">括号中的 1 表示转置的卷积层。</li><li id="d612" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">末尾的 2 表示应用于输入图像的第一个卷积层和应用于残差的最后一个卷积层。</li><li id="353c" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">因此，<strong class="ky ir">使用深度递归层来扩展特征嵌入子网络，以在不增加参数数量的情况下有效地增加网络深度。</strong></li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="eb29" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 5。MS-LapSRN:本地剩余学习(LRL) </strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/0ae66558080edf8405d456a9624c931f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*YmLHbFini1yIPZilXDqTSw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Different Kinds of Local Residual Learning (LRL)</strong></figcaption></figure><ul class=""><li id="875b" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">由于 LRL 是一种有效的成分，MS-LapSRN 还测试了如上所示的不同变体。</li><li id="16ba" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir"> (a) LapSRN_NS </strong>:无跳跃连接。</li><li id="d884" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir"> (b) LapSRN_DS </strong>:使用与前一输出的跳过连接作为源输入，即不同源跳过连接。</li><li id="c5c2" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir"> (c) LapSRN_SS </strong>:使用跳过连接，将最开始的输出作为源，即共享源跳过连接。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="c523" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 6。MS-LapSRN:多尺度训练</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/188a13c10ad5d484bbb748445eda18bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*GK9xifPEdFC7Ui96PKVCnw.png"/></div></figure><ul class=""><li id="0c39" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated"><strong class="ky ir">使用多尺度夏邦尼损失</strong>。</li><li id="07e2" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">比如说 3 级 LapSRN，所有 3 个音阶的 Charbonnier 损失加起来就是总损失。</li><li id="4e9d" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">要注意的是，这里的比例增加被限制在老 2^n×。不支持任意的上采样率。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="6051" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 7。消融研究</strong></h1><h2 id="38d3" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">7.1.一些细节</h2><ul class=""><li id="f203" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nt nl nm nn bi translated">除了应用于输入图像的第一层、用于预测残差的层和图像上采样层之外的所有卷积层中的 64 个滤波器。</li><li id="4110" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">卷积和转置卷积层的滤波器大小分别为 3×3 和 4×4。</li><li id="72d1" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">使用斜率为 0.2 的泄漏 ReLUs。</li><li id="9c46" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">训练集:291 幅图像，其中 91 幅来自杨，200 幅来自 Berkeley 分割数据集。</li><li id="1737" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">批次大小为 64，HR 面片的大小裁剪为 128×128。</li><li id="5e2f" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">一个时期有 1000 次迭代。</li><li id="c911" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">数据扩充:(1)通过在[0.5，1.0]之间随机缩小图像进行缩放。(2)任意旋转 90 度、180 度或 270 度。(3)概率为 0.5 的随机水平翻转。</li><li id="b6d3" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">LR 训练面片由双三次下采样生成。</li><li id="7ae3" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">使用 MatConvNet 工具箱。</li></ul><h2 id="efc6" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">7.2.金字塔结构</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/4d37c019003c9ceffe76dc5dd9bcc9b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*uJaiWeC1XZyWtYcgo_DQyQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">LapSRN with Different Components (Here, residual learning is talking about GRL or the image reconstruction branch, not LRL)</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/b5f4571d04f4c9d2d70323abd6084643.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*8KD5zYO8mJLdSSsMrKNs0g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">LapSRN with Different Components (Here, GRL is the image reconstruction branch, not LRL)</strong></figcaption></figure><ul class=""><li id="ea8c" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">这里，网络使用了 10 个卷积层，测量了 4× SR 的 Set14 上的 PSNR。</li><li id="2d0b" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">与没有金字塔结构的网络(即类似于<a class="ae mb" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4"> FSRCNN </a>)(棕色)的网络)相比，金字塔结构导致了相当大的性能提高，在 SET5 上提高了 0.7 dB，在 SET14 上提高了 0.4 dB。</li></ul><h2 id="0b85" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">7.3.全球剩余学习(GRL)</h2><ul class=""><li id="30ef" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nt nl nm nn bi translated"><strong class="ky ir">去除图像重建分支</strong>，直接预测各级 HR 图像(蓝色)。</li><li id="831c" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir">蓝色曲线收敛缓慢</strong>，训练时波动较大。</li><li id="6b9e" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">另一方面，<strong class="ky ir">全 LapSRN(红色)在 10 个 epoches 内胜过</strong> <a class="ae mb" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"> <strong class="ky ir"> SRCNN </strong> </a> <strong class="ky ir">。</strong></li></ul><h2 id="2824" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">7.4.Charbonnier 损失函数</h2><ul class=""><li id="9863" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nt nl nm nn bi translated"><strong class="ky ir">使用常规 L2 损失函数(绿色)的 LapSRN 比使用夏邦尼尔损失函数(红色)的 LapSRN 差</strong>。</li></ul><h2 id="4b95" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">7.5.跨金字塔等级的参数共享</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/caa6357f6d16c9f38bc903ec90eb03b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*25IIYqzGHFQnaO6jeVvv0g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Parameter Sharing Across Pyramid Levels for 4× SR model</strong></figcaption></figure><ul class=""><li id="0cff" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated"><strong class="ky ir"> LapSRN 4×模型有 812k 参数。</strong></li><li id="61df" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">通过跨金字塔等级共享参数，<strong class="ky ir">参数的数量减少到 407k </strong>。</li><li id="fcca" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">在没有 LRL 的情况下，这个模型有<strong class="ky ir"> 10 个卷积层(<em class="om"> D </em> ) </strong>)和<strong class="ky ir"> 1 个递归块(<em class="om"> R </em> ) </strong>，称为<strong class="ky ir"> LapSRN_NS-D10R1 </strong>。</li><li id="d91c" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir"> LapSRN_NS-D10R1 在使用一半网络参数的情况下，取得了与 LapSRN 不相上下的性能。</strong></li></ul><h2 id="0510" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">7.6.金字塔等级内的参数共享</h2><ul class=""><li id="1ae0" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nt nl nm nn bi translated">与<strong class="ky ir"> LapSRN_NS-D5R2 </strong>和<strong class="ky ir"> LapSRN_NS-D2R5 </strong>一起，模型也共享金字塔内的参数，分别具有 222k 和 112k 参数。但是，<strong class="ky ir">性能下降。</strong></li><li id="2c10" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">这是<strong class="ky ir">，因为在金字塔等级中没有使用 LRL </strong>。</li></ul><h2 id="6067" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">7.7.本地剩余学习(LRL)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/173aef7b9d81306a1aa2c21f362f5275.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*u1832NX_1dwXQvy3ZbH0Hw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Different Kinds of Local Residual Learning (LRL) Using LapSRN-D5R5 on Set5 for 4× SR</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/8abeeff627c754408f759c0d85ead62f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*BzSDqCP-q-Z3HqfkTS5p4w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Different Kinds of Local Residual Learning (LRL) Using Different Models on URBAN100 for 4× SR</strong></figcaption></figure><ul class=""><li id="f656" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">如图和表所示，使用共享源的<strong class="ky ir"> LapSRN_SS 具有最高的 PSNR。</strong></li></ul><h2 id="1afb" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">7.8.D 和 R 研究</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/68deff3f00b370c8531a73d9c88641ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*sE2n63TXRZDtLiqWatgFQw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Study of D and R</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/f36bbe4a7021ef5995444004d7e78222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*EzCWiknFthYXt3LUxqLJXQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Study of D and R</strong></figcaption></figure><ul class=""><li id="e1c8" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">测试不同的<em class="om"> D </em>和<em class="om"> R </em>值。</li><li id="acc4" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">D2R5、D5R2 和 D10R1 表现不相上下。</li><li id="2702" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir"> D4R8 在网络深度大于 80 的情况下，重建精度最好。</strong></li></ul><h2 id="7245" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">7.9.多尺度训练</h2><ul class=""><li id="2110" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nt nl nm nn bi translated">MS-LapSRN 支持多尺度训练，训练样本尺度组合为{2×} 、{4×} 、{8×} 、{2×、4×、{2×、8×、{4×、8×}和{2×、4×、8×}。</li><li id="e767" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir">用多尺度{2×，4×，8×}训练的 MS-LapSRN 产生最好的结果。</strong></li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="0a5a" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">8.<strong class="ak">与最先进结果的比较</strong></h1><h2 id="d528" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">8.1.SSIM PSNR 国际金融公司</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/ae1915290bf76569d22d0a426b915802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WGaLWhnUAUrjX8L2.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Quantitative Results</strong></figcaption></figure><ul class=""><li id="a868" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">测试了五个数据集:Set5，Set14，BSDS100，URBAN100，MANGA190。</li><li id="65e7" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">LapSRN_SS-D5R2 的深度与<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>、<a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"> DRCN </a>和 LapSRN 相似。</li><li id="8ac9" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">LapSRN_SS-D5R5 与<a class="ae mb" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994"> DRRN </a>深度相同。</li><li id="bf42" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">LapSRN_SS-D5R8 对于 4× SR 有 84 层。</li><li id="c7ad" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">MS 表示使用{2×，4×，8×}的等级进行多等级训练。</li><li id="8823" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">LapSRN 尤其在 4 倍和 8 倍 SR 上表现良好。</li><li id="10e2" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">LapSRN 不使用任何 3 个 SR 样本进行训练，但仍然生成与<a class="ae mb" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994"> DRRN </a>相当的结果。</li></ul><h2 id="00d4" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">8.2.定性结果</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pk"><img src="../Images/7b6f0f2752c723534f939688788cebb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V6TQRrOzyc6D6xyB34NjMQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">4× SR on BSDS100, URBAN100 and MANGA109</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pl"><img src="../Images/b5cd481d2e7db8324f606b36dba33853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SpLqls_q3fcFTYwIVQrcTA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">8× SR on BSDS100, URBAN100 and MANGA109</strong></figcaption></figure><ul class=""><li id="41b7" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">如上图，<strong class="ky ir"> MS-LapSRN 精确重建平行直线、网格图案、文字。</strong></li><li id="cae4" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">在 8× SR 的情况下，那些使用双三次预上采样或使用一步上采样的现有技术不能很好地超分辨精细结构。</li></ul><h2 id="7807" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">8.3.执行时间</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pm"><img src="../Images/86f35232f8109d79d0a4fb3921d99169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*UKq2n0f_aACJCU1kjPCqmw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">4× SR on URBAN100</strong></figcaption></figure><ul class=""><li id="5684" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">使用了 3.4 GHz 英特尔 i7 CPU (32G RAM)和 NVidia Titan Xp GPU (12G 内存)。</li><li id="2144" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><a class="ae mb" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"> SRCNN </a>和<a class="ae mb" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4"> FSRCNN </a>原本在 CPU 上，但被作者为 GPU 重建。</li><li id="9777" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">对于 URBAN100 上的 4× SR，<strong class="ky ir"> MS-LapSRN-D5R2 比除了</strong><a class="ae mb" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4"><strong class="ky ir">fsr CNN</strong></a><strong class="ky ir">之外的所有现有方法都要快。</strong></li><li id="fb4b" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated"><strong class="ky ir"> MS-LapSRN-D5R8 胜过</strong><a class="ae mb" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994"><strong class="ky ir">DRRN</strong></a><strong class="ky ir">。</strong></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/a111f89f2914f99f29022d72417f0481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*wxo9Z-u2RDw_CyjchwcedQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">2×, 4× and 8× SR</strong></figcaption></figure><ul class=""><li id="550a" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated"><a class="ae mb" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"> SRCNN </a>和<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>复杂度取决于输出图像大小。对于 4×和 8× SR，它们的运行时间比 MS-LapSRN 增加得更多。</li><li id="d1b7" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">虽然 FSRCNN 速度最快，但 MS-LapSRN 的 SR 质量要高得多。</li></ul><h2 id="5b5f" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">8.6.模型参数</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi po"><img src="../Images/b7a43669985b82d0df1ab6402d679896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*XoFw67XTQ_TGk_AJjVxxzw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">4× SR on URBAN100</strong></figcaption></figure><ul class=""><li id="47cd" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated"><strong class="ky ir"> MS-LapSRN 的参数比 LapSRN 少约 73%，比</strong><a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"><strong class="ky ir">VDSR</strong></a><strong class="ky ir">少约 66%，比</strong><a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"><strong class="ky ir">【DRCN</strong></a><strong class="ky ir">少约 87%，比</strong><a class="ae mb" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994"><strong class="ky ir">DRRN</strong></a><strong class="ky ir">少约 25%。</strong></li></ul><h2 id="917e" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">8.7.真实世界的照片(JPEG 压缩伪像)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/3b319539c395a4f1fc0eba2ea9d47b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*F_zA1Mad2oYekJRzmiUOxQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">4× SR</strong></figcaption></figure><ul class=""><li id="5843" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">MS-LapSRN 重建出<strong class="ky ir">更清晰</strong>和<strong class="ky ir">更精确</strong>的图像。</li></ul><h2 id="2ed1" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">8.8.与 LAPGAN 的比较</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/08e5890ad3625b8c1c5c227ab022ea5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*OayXlOwupRv9i4LM0qYg_g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">LAPGAN ARchitecture</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/d122b886246bc6a9b843367ba4fdd295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*rKQDfh14abYmTCIl7cR6aA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Comparison with LAPGAN</strong></figcaption></figure><ul class=""><li id="12a8" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">作者还与拉普根进行了比较。</li><li id="0571" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">LAPGAN 最初的目的不是为了超分辨率。它是一个用于图像纹理合成的生成式图像模型。在这里，作者使用与 LapSRN 相同的训练数据和设置来训练 LAPGAN。</li><li id="47de" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">最后，LapSRN 表现更好。</li></ul><h2 id="19ba" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">8.9.对抗训练</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/05bc31f4b3cddcc1e594ef5933aef091.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*NJqVyca1rmvueUppnI52yA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">4× SR</strong></figcaption></figure><ul class=""><li id="317c" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">LapSRN 被扩展为一个生成网络，并使用 DCGAN 的鉴别器建立了一个鉴别网络。</li><li id="d99e" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nt nl nm nn bi translated">具有对抗训练的网络生成关于不规则结构区域的更可信的细节，例如草和羽毛。</li></ul><h2 id="8c22" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">8.10 限制</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/8be9c7fc5f261809faf5ac7ceac8fe3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*36LNqD7SVNqVW_uOhsCoxA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">8× SR Failure Case</strong></figcaption></figure><ul class=""><li id="161f" class="nd ne iq ky b kz la lc ld lf oh lj oi ln oj lr nt nl nm nn bi translated">LapSRN 可为较大的上采样比例(例如 8 倍)生成清晰锐利的 HR 图像，它不会“幻觉”精细的细节。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="ca54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">渐进上采样让我想到了对象检测的渐进去卷积或上采样方法(如<a class="ae mb" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a>、<a class="ae mb" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a>、<a class="ae mb" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4"> RetinaNet </a>)或语义分割(如<a class="ae mb" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">去卷积</a>)。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="6e37" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">参考</h2><p id="9e16" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf pu lh li lj pv ll lm ln pw lp lq lr ij bi translated">【2017 CVPR】【LapSRN】<br/><a class="ae mb" href="https://arxiv.org/abs/1704.03915" rel="noopener ugc nofollow" target="_blank">深度拉普拉斯金字塔网络快速精确超分辨率</a> <br/> &amp; <a class="ae mb" href="http://vllab.ucmerced.edu/wlai24/LapSRN/papers/cvpr17_LapSRN_supp.pdf" rel="noopener ugc nofollow" target="_blank">补充材料</a></p><p id="cfa9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">【2018 TPAMI】【MS-LapSRN】<br/><a class="ae mb" href="https://arxiv.org/abs/1704.03915" rel="noopener ugc nofollow" target="_blank">深度拉普拉斯金字塔网络快速准确的图像超分辨率</a></p><h2 id="99f5" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lf nz oa mx lj ob oc mz ln od oe nb of bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf pu lh li lj pv ll lm ln pw lp lq lr ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(们)(还)(不)(想)(到)(这)(些)(人)(们)(,)(我)(们)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(们)(还)(没)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(里)(来)(。</p><p id="8b77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">物体检测<br/></strong><a class="ae mb" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae mb" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae mb" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae mb" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae mb" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae mb" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae mb" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae mb" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae mb" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a></p><p id="6582" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">语义切分<br/></strong><a class="ae mb" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae mb" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae mb" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae mb" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>】<a class="ae mb" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae mb" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a><a class="ae mb" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a></p><p id="fc65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">生物医学图像分割<br/></strong>[<a class="ae mb" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae mb" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae mb" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae mb" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae mb" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a></p><p id="3134" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实例分割<br/>T32】[<a class="ae mb" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae mb" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae mb" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae mb" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae mb" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae mb" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></strong></p><p id="58de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度】: 【解析度: 解析度: 【解析度】: 【解析度】: 【解析度: 解析度: 【解析度】: 【解析度】: 【解析度: 解析度: 解</p></div></div>    
</body>
</html>