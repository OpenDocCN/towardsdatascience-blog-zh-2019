<html>
<head>
<title>Principal Component Analysis for Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析降维</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad?source=collection_archive---------0-----------------------#2019-05-24">https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad?source=collection_archive---------0-----------------------#2019-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a085" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过学习算法背后的数学知识并使用 Python 一步一步地执行它，了解如何执行 PCA！</h2></div><p id="6d15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在现代技术时代，越来越多的数据被产生和收集。然而，在机器学习中，太多的数据可能是一件坏事。在某一点上，更多的特征或维度会降低模型的准确性，因为有更多的数据需要归纳——这被称为<strong class="kk iu">维度诅咒</strong>。</p><p id="8e9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">降维</strong>是降低模型复杂性和避免过度拟合的方法。降维主要有两类:特征选择和特征提取。通过特征选择，我们选择原始特征的子集，而在特征提取中，我们从特征集中提取信息来构建新的特征子空间。</p><p id="6178" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本教程中，我们将探讨特征提取。在实践中，特征提取不仅用于提高存储空间或学习算法的计算效率，还可以通过减少维数灾难来提高预测性能，尤其是在我们使用非正则化模型的情况下。</p><p id="153b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具体来说，我们将讨论用于将数据集压缩到低维特征子空间的<strong class="kk iu">主成分分析</strong> ( <strong class="kk iu"> PCA </strong>)算法，目标是保持大部分相关信息。我们将探索:</p><ul class=""><li id="d255" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">PCA 背后的概念和数学</li><li id="9943" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">如何使用 Python 一步一步从头开始执行 PCA</li><li id="60d3" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">如何使用 Python 库执行 PCA<code class="fe ls lt lu lv b">scikit-learn</code></li></ul><p id="9b0b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们开始吧！</p><blockquote class="lw lx ly"><p id="8fdd" class="ki kj lz kk b kl km ju kn ko kp jx kq ma ks kt ku mb kw kx ky mc la lb lc ld im bi translated"><em class="it">本教程改编自 Next Tech 的</em> <strong class="kk iu"> <em class="it"> Python 机器学习</em> </strong> <em class="it">系列的</em> Part 2 <em class="it">，带你从 0 到 100 用 Python 进行机器学习和深度学习算法。它包括一个浏览器内沙盒环境，预装了所有必要的软件和库，以及使用公共数据集的项目。这里</em>  <em class="it">可以免费上手</em> <a class="ae md" href="https://c.next.tech/2vZveOh" rel="noopener ugc nofollow" target="_blank"> <em class="it">！</em></a></p></blockquote></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="f6ae" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">主成分分析导论</h1><p id="8f69" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated"><strong class="kk iu">主成分分析</strong> ( <strong class="kk iu"> PCA </strong>)是一种无监督的线性变换技术，广泛应用于不同领域，最突出的是用于特征提取和降维。PCA 的其他流行应用包括股票市场交易中的探索性数据分析和信号去噪，以及生物信息学领域中的基因组数据和基因表达水平的分析。</p><p id="55d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PCA 帮助我们基于特征之间的相关性来识别数据中的模式。简而言之，PCA 旨在找到高维数据中最大方差的方向，并将其投影到一个新的子空间，该子空间的维数等于或小于原始子空间的维数。</p><p id="2802" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定新特征轴彼此正交的约束，新子空间的正交轴(<strong class="kk iu">主分量</strong>)可以被解释为最大方差的方向，如下图所示:</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/8269e28f57466b914f8c4fd408c0d2d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*zh1GxZ4BPCOPFmTxreVFdw.jpeg"/></div></figure><p id="d2be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上图中，<em class="lz"> x1 </em>和<em class="lz"> x2 </em>为原始特征轴，<strong class="kk iu"> PC1 </strong>和<strong class="kk iu"> PC2 </strong>为主要部件。</p><p id="aab2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们使用 PCA 进行降维，我们将构建一个<em class="lz">d</em>x<em class="lz">k</em>–维度变换矩阵<strong class="kk iu"> <em class="lz"> W </em> </strong>，它允许我们将样本向量<strong class="kk iu"> <em class="lz"> x </em> </strong>映射到一个新的<em class="lz">k</em>–维度特征子空间上，该子空间的维度少于原始的<em class="lz">d</em>–维度特征空间:</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/9d713635d26f96a920b034ec7e392f7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*ytEHO-Wjos_ugY2LffXEyg.png"/></div></figure><p id="94d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为将原始<em class="lz"> d </em>维数据转换到这个新的<em class="lz"> k </em>维子空间(通常为<em class="lz"> k </em> ≪ <em class="lz"> d </em>)的结果，第一个主分量将具有最大可能的方差，并且所有后续主分量将具有最大方差，假设这些分量与其他主分量不相关(正交)——即使输入特征相关，得到的主分量也将相互正交(不相关)。</p><p id="32a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，PCA 方向对数据缩放高度敏感，如果在不同的缩放比例上测量特征，我们需要在 PCA 之前标准化特征<em class="lz"/>，并且我们希望对所有特征赋予同等的重要性。</p><p id="4b2d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在更详细地查看用于降维的 PCA 算法之前，让我们用几个简单的步骤来总结该方法:</p><ol class=""><li id="cfa0" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld nr lk ll lm bi translated">标准化<em class="lz"> d </em>维数据集。</li><li id="20ea" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nr lk ll lm bi translated">构建协方差矩阵。</li><li id="93e5" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nr lk ll lm bi translated">将协方差矩阵分解为其特征向量和特征值。</li><li id="64f3" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nr lk ll lm bi translated">按降序对特征值进行排序，对对应的特征向量进行排序。</li><li id="2220" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nr lk ll lm bi translated">选择<em class="lz"> k 个</em>最大特征值对应的<em class="lz"> k 个</em>特征向量，其中<em class="lz"> k </em>为新特征子空间的维数(<em class="lz"> k </em> ≤ <em class="lz"> d </em>)。</li><li id="8668" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nr lk ll lm bi translated">从“顶”<em class="lz"> k </em>特征向量构造一个投影矩阵<strong class="kk iu"> <em class="lz"> W </em> </strong>。</li><li id="acdb" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nr lk ll lm bi translated">利用投影矩阵<strong class="kk iu"> <em class="lz"> W </em> </strong>对<em class="lz"> d </em>维输入数据集<strong class="kk iu"> <em class="lz"> X </em> </strong>进行变换，得到新的<em class="lz"> k </em>维特征子空间。</li></ol><p id="705d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用 Python 作为学习练习，一步一步地执行 PCA。然后，我们将看到如何使用<code class="fe ls lt lu lv b">scikit-learn</code>更方便地执行 PCA。</p><h1 id="cb65" class="ml mm it bd mn mo ns mq mr ms nt mu mv jz nu ka mx kc nv kd mz kf nw kg nb nc bi translated">逐步提取主成分</h1><p id="84d7" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">在我们的例子中，我们将使用来自 UCI 机器学习知识库的<em class="lz"> Wine </em>数据集。该数据集由 178 个葡萄酒样本组成，其中 13 个特征描述了它们不同的化学性质。你可以在这里找到更多<a class="ae md" href="https://archive.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="1287" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本节中，我们将处理 PCA 的前四个步骤；稍后我们将复习最后三个。您可以通过使用 Next Tech <a class="ae md" href="https://c.next.tech/30EzSiV" rel="noopener ugc nofollow" target="_blank">沙箱</a>来遵循本教程中的代码，沙箱已经预安装了所有必要的库，或者如果您愿意，您可以在自己的本地环境中运行代码片段。</p><p id="1df7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦您的沙盒加载完毕，我们将从直接从存储库加载<em class="lz"> Wine </em>数据集开始:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/eca34d7ae5d5ae7f75ce3c5a91f74e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*4qDPuAo1N_SCV44YATKDFA.png"/></div></figure><p id="880f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们将把<em class="lz">葡萄酒</em>的数据处理成单独的训练和测试集——使用 70:30 的分割——并将其标准化为单位方差:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="5c73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">完成强制预处理后，我们进入第二步:构造协方差矩阵。对称的<em class="lz"> d </em> x <em class="lz"> d </em>维协方差矩阵存储不同特征之间的成对协方差，其中<em class="lz"> d </em>是数据集中的维数。例如，在群体水平上的两个特征<em class="lz"> x_j </em>和<em class="lz"> x_k </em>之间的协方差可以通过以下等式来计算:</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/bcb6552098976d347d0ec0bacdc32b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*zsJQIL5ZnHRxxqxz0OxntQ.png"/></div></figure><p id="68e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<em class="lz"> μ_j </em>和<em class="lz"> μ_k </em>分别是特征<em class="lz"> j </em>和<em class="lz"> k </em>的样本均值。</p><p id="d3ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，如果我们标准化数据集，样本均值为零。两个要素之间的正协方差表示要素一起增加或减少，而负协方差表示要素以相反的方向变化。例如，三个特征的协方差矩阵可以写成如下形式(注意，<strong class="kk iu">σ</strong>代表希腊大写字母<strong class="kk iu">σ</strong>，不要与<strong class="kk iu">总和</strong>符号混淆):</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e1821a50e060fdae7c1a2bf81a3d741e.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*vyx57OmdL9HXraN-AkBSEw.png"/></div></figure><p id="f728" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">协方差矩阵的特征向量代表主要分量(最大方差的方向)，而相应的特征值将定义它们的大小。在<em class="lz"> Wine </em>数据集的情况下，我们将从 13×13 维协方差矩阵中获得 13 个特征向量和特征值。</p><p id="6ce0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，对于我们的第三步，让我们获得协方差矩阵的特征对。一个特征向量<strong class="kk iu"> <em class="lz"> v </em> </strong>满足以下条件:</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/0d5a73316ba83c961b53eaf3f7e0a34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:136/format:webp/1*noDBksvur0Y3mi0zeyy9dg.png"/></div></figure><p id="7514" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<strong class="kk iu"> λ </strong>是标量:特征值。由于手动计算特征向量和特征值是一项有些繁琐和复杂的任务，我们将使用来自<code class="fe ls lt lu lv b">NumPy</code>的<code class="fe ls lt lu lv b">linalg.eig</code>函数来获得<em class="lz">和</em>协方差矩阵的特征对:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="292a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<code class="fe ls lt lu lv b">numpy.cov</code>函数，我们计算了标准化训练数据集的协方差矩阵。使用<code class="fe ls lt lu lv b">linalg.eig</code>函数，我们执行了特征分解，产生了一个向量(<code class="fe ls lt lu lv b">eigen_vals</code>)，该向量由 13 个特征值和相应的特征向量组成，以列的形式存储在 13×13 维矩阵中(<code class="fe ls lt lu lv b">eigen_vecs</code>)。</p><h1 id="e0aa" class="ml mm it bd mn mo ns mq mr ms nt mu mv jz nu ka mx kc nv kd mz kf nw kg nb nc bi translated">总差异和解释差异</h1><p id="8097" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">由于我们希望通过将数据集压缩到一个新的特征子空间来降低数据集的维数，因此我们只选择包含大部分信息(方差)的特征向量(主成分)的子集。特征值定义了特征向量的大小，所以我们要把特征值按大小递减排序；我们感兴趣的是基于它们相应特征值的值的前 k 个特征向量。</p><p id="68f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是在我们收集那些最有信息的特征向量之前，让我们先画出特征值的方差解释比率。特征值<em class="lz"> λ_j </em>的方差解释比就是特征值<em class="lz"> λ_j </em>和特征值总和的分数:</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/679c36383db34c87e85c37d0b731a0ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:126/format:webp/1*e3Ud73T1QZaock4MBb-lMw.png"/></div></figure><p id="d4b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<code class="fe ls lt lu lv b">NumPy</code> <code class="fe ls lt lu lv b">cumsum</code>函数，我们可以计算解释方差的累积和，然后通过<code class="fe ls lt lu lv b">matplotlib</code>的<code class="fe ls lt lu lv b">step</code>函数绘制:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/b1cab8833faca65067a0839257eba3e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*buXqasMp7vA6aPqdMkp2XA.png"/></div></figure><p id="34ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">得到的图表明，仅第一主成分就占了方差的大约 40%。此外，我们可以看到，前两个主成分合起来解释了数据集中近 60%的方差。</p><h1 id="92da" class="ml mm it bd mn mo ns mq mr ms nt mu mv jz nu ka mx kc nv kd mz kf nw kg nb nc bi translated">特征转换</h1><p id="d99b" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">在我们成功地将协方差矩阵分解成特征对之后，现在让我们继续进行 PCA 的最后三个步骤，将<em class="lz"> Wine </em>数据集变换到新的主成分轴上。</p><p id="d690" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将按照特征值的降序对特征对进行排序，从选定的特征向量中构造一个投影矩阵，并使用投影矩阵将数据转换到低维子空间上。</p><p id="faab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先按照特征值的降序对特征对进行排序:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="f62b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们收集对应于两个最大特征值的两个特征向量，以捕获该数据集中约 60%的方差。请注意，出于说明的目的，我们只选择了两个特征向量，因为我们将在本小节稍后通过二维散点图绘制数据。在实践中，主成分的数量必须通过计算效率和分类器性能之间的权衡来确定:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><pre class="nj nk nl nm gt of lv og oh aw oi bi"><span id="3882" class="oj mm it lv b gy ok ol l om on">[Out:]<br/>Matrix W:<br/> [[-0.13724218  0.50303478]<br/> [ 0.24724326  0.16487119]<br/> [-0.02545159  0.24456476]<br/> [ 0.20694508 -0.11352904]<br/> [-0.15436582  0.28974518]<br/> [-0.39376952  0.05080104]<br/> [-0.41735106 -0.02287338]<br/> [ 0.30572896  0.09048885]<br/> [-0.30668347  0.00835233]<br/> [ 0.07554066  0.54977581]<br/> [-0.32613263 -0.20716433]<br/> [-0.36861022 -0.24902536]<br/> [-0.29669651  0.38022942]]</span></pre><p id="7a19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过执行前面的代码，我们已经从顶部的两个特征向量创建了一个 13 x 2 维的投影矩阵<strong class="kk iu"> <em class="lz"> W </em> </strong>。</p><p id="c1e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用投影矩阵，我们现在可以将样本<strong class="kk iu"><em class="lz"/></strong>(表示为 1×13 维行向量)变换到 PCA 子空间(主分量 1 和 2)上，获得<strong class="kk iu"><em class="lz">【x’</em></strong>，现在是由两个新特征组成的二维样本向量:</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/a82664b098133d20847421bcba2a059d.png" data-original-src="https://miro.medium.com/v2/resize:fit:152/format:webp/1*nuYZrTF8trabAfJq-3wVcA.png"/></div></figure><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="8705" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，我们可以通过计算矩阵点积将整个 124 x 13 维训练数据集转换到两个主分量上:</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/e7a2e632094f021028e692d3c5c000cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/format:webp/1*Lo1e06UOCLsKPjJaeiNcDQ.png"/></div></figure><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="f105" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，让我们在一个二维散点图中可视化转换后的<em class="lz"> Wine </em>训练集，现在存储为一个 124 x 2 维矩阵:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi oq"><img src="../Images/ab7d80955f57a3e28d4f22298edd0ee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3mAQhYWFt5vcsa_bE9qYqw.png"/></div></div></figure><p id="0f3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们在结果图中看到的，数据更多地分布在第一主成分的<em class="lz">x</em>-轴上，而不是第二主成分(<em class="lz">y</em>-轴)上，这与我们之前创建的方差比图一致。然而，我们可以直观地看到，线性分类器可能能够很好地分离这些类别。</p><p id="44f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管在前面的散点图中，我们对类别标签信息进行了编码，但我们必须记住，PCA 是一种不使用任何类别标签信息的无监督技术。</p><h1 id="0b53" class="ml mm it bd mn mo ns mq mr ms nt mu mv jz nu ka mx kc nv kd mz kf nw kg nb nc bi translated">scikit 中的 PCA 学习</h1><p id="771d" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">尽管前一小节中的详细方法帮助我们了解了 PCA 的内部工作方式，但我们现在将讨论如何使用在<code class="fe ls lt lu lv b">scikit-learn</code>中实现的<code class="fe ls lt lu lv b">PCA</code>类。<code class="fe ls lt lu lv b">PCA</code>类是<code class="fe ls lt lu lv b">scikit-learn</code>的另一个 transformer 类，在使用相同的模型参数转换训练数据和测试数据集之前，我们首先使用训练数据拟合模型。</p><p id="3dfb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用<em class="lz">葡萄酒</em>训练数据集上的<code class="fe ls lt lu lv b">PCA</code>类，通过逻辑回归对转换后的样本进行分类:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="9bd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，使用一个定制的<code class="fe ls lt lu lv b">plot_decision_regions</code>函数，我们将可视化决策区域:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/973d753eb9cb005e813b952e67f56765.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*lIFV-5PzduL3aoqCb85gcw.png"/></div></figure><p id="4ed3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过执行前面的代码，我们现在应该看到训练数据的决策区域减少到两个主分量轴。</p><p id="dec9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了完整起见，让我们在转换后的测试数据集上绘制逻辑回归的决策区域，看看它是否能很好地分离类:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/543ee3fe034d8038ff93a92ef3d20edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*WotGez5Twhp0SP1B8c4OAg.png"/></div></figure><p id="62a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们通过执行前面的代码为测试集绘制了决策区域之后，我们可以看到，逻辑回归在这个小的二维特征子空间上表现得相当好，并且仅错误分类了测试数据集中非常少的样本。</p><p id="ceac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们对不同主成分的解释的方差比率感兴趣，我们可以简单地初始化<code class="fe ls lt lu lv b">PCA</code>类，将<code class="fe ls lt lu lv b">n_components</code>参数设置为<code class="fe ls lt lu lv b">None</code>，这样所有主成分都被保留，然后可以通过<code class="fe ls lt lu lv b">explained_variance_ratio_</code>属性访问解释的方差比率:</p><figure class="nj nk nl nm gt nn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="0add" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我们在初始化<code class="fe ls lt lu lv b">PCA</code>类时设置了<code class="fe ls lt lu lv b">n_components=None</code>，这样它将按照排序的顺序返回所有主成分，而不是执行维度缩减。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="da14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇关于主成分分析降维的教程！我们讨论了 PCA 算法背后的数学，如何用 Python 一步一步地执行 PCA，以及如何使用<code class="fe ls lt lu lv b">scikit-learn</code>实现 PCA。其他降维技术有<strong class="kk iu">线性判别分析</strong> (LDA)和<strong class="kk iu">核 PCA </strong>(用于非线性可分数据)。</p><p id="36f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lz">Next Tech 的</em> <strong class="kk iu"> <em class="lz"> Python 机器学习(第二部分)</em> </strong> <em class="lz">课程涵盖了这些其他技术和更多提高模型性能的主题，如数据预处理、模型评估、超参数调整和集成学习技术。</em></p><p id="3fad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lz">这里</em>  <em class="lz">可以免费上手</em> <a class="ae md" href="https://c.next.tech/2vZveOh" rel="noopener ugc nofollow" target="_blank"> <em class="lz">！</em></a></p></div></div>    
</body>
</html>