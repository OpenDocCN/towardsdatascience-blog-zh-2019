# OpenAI、欺骗性技术和模型风险管理

> 原文：<https://towardsdatascience.com/openai-deceptive-technology-and-model-risk-management-57286070c06c?source=collection_archive---------15----------------------->

## GPT-2 和数字防御战术手册发布后的点点滴滴

![](img/3ebfd685e468311ab07730ddc415d85f.png)

Example of a prompt and the synthetic text generated by OpenAI’s GPT-2\. (Source: [OpenAI](https://blog.openai.com/better-language-models/#sample1))

在我整理自己对发布可能有负面影响的技术和信息的想法时，这篇文章将一些点联系起来。它涉及 OpenAI 发布的 GPT-2 和我们的数据机构发布的数字防御剧本*，破坏性与欺骗性技术，恶意意图，深度伪造，验证和监视，以及模型风险管理。尽情享受吧！*

# OpenAI 的 GPT-2

OpenAI 最近在[发布的 GPT-2](https://blog.openai.com/better-language-models/) 引起了轰动，这是一种语言模型，可以接受人类写的提示，并生成不止一页连贯的“人类质量”的文本。我看到人们对他们的发布策略比对模型本身更感兴趣。他们选择不发布完整的模型、数据集、训练代码或模型权重，理由是担心如何使用它们。他们解释说:

> 这个决定，以及我们对它的讨论，是一个实验:虽然我们不确定这是今天的正确决定，但我们相信人工智能社区最终需要在某些研究领域以一种深思熟虑的方式解决出版规范的问题。生物技术和网络安全等其他学科长期以来一直在积极讨论在明显可能被滥用的情况下负责任的发布，我们希望我们的实验可以作为一个案例研究，在人工智能社区中对模型和代码发布决策进行更细致的讨论。

反应很快，许多人对 OpenAI 的动机感到好奇。其他人担心开源的未来。

# 破坏性与欺骗性技术

Gradient 发表了一封由 Hugh Zhang 撰写的公开信，呼吁 OpenAI 提供完整的模型，区分破坏性和欺骗性技术:

*   破坏性技术主要在物理领域运作。想想化学武器、实验室制造的超级病毒、致命的自主武器或原子弹。
*   另一方面，*欺骗性*技术主要在我们的意识领域运作，有可能被大范围滥用来操纵和控制人们。

张接着说，对付欺骗性技术的方法是“尽可能公开它的威力”，而“绝对不能让足够危险的破坏性技术轻易获得”

他确实承认欺骗性技术可能是危险的，并建议使用“论文发表和代码发布之间的一小段延迟，以防止快速反应的恶意行为者在公众有时间完全处理新结果之前突袭。”

虽然我能理解这种观点，但公开知识和公众有知识是有区别的。公开信息并不能保证公众最终会看到它，或者有时间或兴趣去完全处理它。

# 恶意

在一篇对 GPT-2 的发布持不同立场的文章中，瑞恩·洛威问了机器学习(ML)研究人员一个问题:

> 我们正在建造影响人们的东西。迟早，我们会越过一条线，我们的研究可以被恶意利用来做坏事。我们是否应该等到这种情况发生时再决定如何处理可能有负面副作用的研究？

这与张所说的“快速反应的恶意演员”相呼应。当我读到这几行时，我的胃翻了个底朝天。我们必须努力解决的一个问题是，这项研究正被用来做坏事——人们甚至没有恶意。Lowe 在文章的后面确实提到了这个问题:

> ML research 已经在现实世界中产生了影响(例如，对[贷款申请](https://www.smartdatacollective.com/how-ai-is-transforming-lending-and-loan-management/)、[法院判决](https://www.wired.com/2017/04/courts-using-ai-sentence-criminals-must-stop-now/)、[人员招聘](https://beamery.com/blog/ai-recruiting)等做出决策。).这些算法中的偏见现在正在被暴露出来，但这是在它们首次实施多年之后，同时它们对人们的生活产生了严重影响。

在对创新的关注、[人工智能(AI)领域缺乏多样性](https://www.technologyreview.com/s/610192/were-in-a-diversity-crisis-black-in-ais-founder-on-whats-poisoning-the-algorithms-in-our/)以及可能的大量其他因素之间，新技术的引入往往没有认真考虑潜在的不利影响或意想不到的后果。

人工智能研究员兼 fast.ai 联合创始人雷切尔·托马斯解释了让她害怕人工智能的 5 件事:

1.  *算法的实现往往没有解决错误的方法。*
2.  人工智能更容易让人没有责任感。
3.  *AI 编码&放大偏见。*
4.  *优化指标高于一切会导致负面结果。*
5.  *大型科技公司没有问责制。*

这里没有一点是关于恶意行为者的，而是关于人工智能的日常实现。似乎解决这些问题表明了透明度和公开性的重要性。

# Deepfakes

实际的恶意行为者呢？我已经写了关于 deepfakes 引起的担忧和损害。大量的金钱和资源被投入到检测它们的工作中，一些人表达了对 deepfakes 和类似技术如何被用来扰乱 2020 年选举(以及更广泛的影响)的担忧。

![](img/e40b498749223fce81fb679a9ed5a3e1.png)

Still from video by Deeptrace. (Source: [IEEE Spectrum](https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/will-deepfakes-detection-be-ready-for-2020))

事实上，OpenAI 在他们不提供 GPT-2 完整版本的理由中引用了这些技术:

*我们还可以想象这些模型的应用出于*[](https://blog.openai.com/preparing-for-malicious-uses-of-ai/)**的恶意目的，包括以下几种(或者其他我们还无法预料的应用):**

*   **产生误导性新闻文章**
*   *在网上冒充他人*
*   **自动制作发布在社交媒体上的辱骂或伪造内容**

*回到张关于欺骗性技术的“论文发表和代码发布之间的一个小延迟”的建议，许多致力于检测 deepfakes 的研究人员已经谈到了分享他们的发现的问题。*

*计算机科学家 Siwei Lyu 在 2018 年 8 月发表了一篇[文章](https://theconversation.com/detecting-deepfake-videos-in-the-blink-of-an-eye-101072)，解释了他的团队如何基于眨眼实现了超过 95%的深度假货检测率。但是随后的一篇文章透露,“就在他的团队将论文草稿放到网上的几个星期后，他们收到了匿名邮件，这些邮件链接到伪造程度很高的 YouTube 视频，这些视频中的明星更正常地睁开和闭上眼睛。虚假内容的创造者已经进化了。”*

*加州大学计算机科学教授哈尼·法里德使用法医技术来检测深度假货。他[解释了](https://www.theguardian.com/technology/2018/nov/12/deep-fakes-fake-news-truth)由于机器学习，对抗它们变得更加困难，以及为什么他不分享新的突破:*

> *程序员所要做的就是更新算法来寻找，比如说，与心跳相对应的面部颜色变化，然后突然之间，假货就融入了这种曾经难以察觉的迹象。一旦我泄露了这项研究，只需要一个混蛋把它加入他们的系统。*

*虽然有些人呼吁将可以生成假媒体的技术开源，但那些致力于检测假货的人通过保密他们的创新，在所谓的人工智能军备竞赛中受益。*

# *核查和监督*

*![](img/842940bc083c9d07e2de9d9086bd4123.png)*

*(Photo by [Chris Ried](https://unsplash.com/@cdr6934?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral).)*

*一些人不再专注于检测假媒体，而是着眼于验证真媒体。技术社会学家泽内普·图费克奇[建议](https://www.wired.com/story/zeynep-tufekci-facts-fake-news-verification/)验证可以通过摄像头或区块链数据库中的防欺骗元数据来实现。这些解决方案可能有助于验证图像或视频，但验证文本可能会更加困难。图费克奇还提醒我们:*

> *然而，有效的身份识别系统也带来了一个令人担忧的事实:每一种验证方法都有被监控的危险。*

*虽然她继续说有办法减轻这种担忧，但这不是一个可以掉以轻心的问题。*

# *数字防御战术手册*

*在 OpenAI 发布 GPT-2 的一周前，还有一个我更关心的版本，但它没有像 GPT-2 那样受到关注。我们的数据机构(ODB) [发布了](https://www.alliedmedia.org/news/2019/02/07/our-data-bodies-playbook-out)*数字防御手册:回收数据的社区动力工具*。该工作簿被描述为“一套久经考验的工具，用于诊断、处理和修复普遍存在的惩罚性数据收集和数据驱动系统的不公正。”*

*该新闻稿继续说，“ODB 希望剧本将激励社区参与解决源于社会不公正的监控、侧写和隐私问题。”*

*ODB 正在努力处理前面提到的一些问题的后果，包括编码到 AI 中的偏见(有意和无意)以及意外后果的影响。*

*![](img/d849baf7934153665746f83d3184981b.png)*

*Tawana Petty during the Data for Black Lives II closing panel. (Source: [Data for Black Lives Facebook](https://www.facebook.com/data4blacklives/videos/300185840850038/))*

*在麻省理工学院媒体实验室举行的“黑人生活数据 II”会议的[闭幕式上，组织者和 ODB 团队成员 Tawana Petty 告诉观众:](https://www.facebook.com/data4blacklives/videos/300185840850038/)*

> *你们都拿到了*数字防御战术手册*，但是我们没有告诉你们他们所有的策略，我们永远也不会告诉你们。因为我们希望我们的社区成员继续生存和发展。所以你会得到一些东西，但是让他们活下去的东西，我们自己留着。*

*虽然团队创建这个资源是为了共享知识，但是限制他们共享的内容对他们的项目同样重要。它还回顾了法里德的策略，即解释他研究的某些方面，但对其他方面保密以保持优势。*

# *模型风险管理*

*上周，在我已经开始尝试将这篇文章拼凑起来之后，我参加了 QuantUniversity.com 创始人 [Sri Krishnamurthy](https://www.linkedin.com/in/srikrishnamurthy/) 的演讲，题目是“数据科学和人工智能时代的模型治理”他谈到了围绕代码可再现性的挑战，以及代码库的可解释性和透明性的重要性。*

*我特别有兴趣了解更多关于美联储[sr11–7](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)定义的模型风险管理。该文件将“模型风险”定义为“基于不正确或误用的模型输出和报告的决策的潜在不利后果。”尽管该文件在列举不利后果(财务损失或“对银行组织声誉的损害”)的例子时明显是针对金融机构的，但它确实提供了减轻风险的指导原则，可以帮助指导其他地方的做法。*

*如果我们从对人的影响而不是对业务的影响的角度来考虑不良后果，我们可能会开始朝着正确的方向前进。(这句台词我已经能听到笑声了。这篇文章可能会转向另一个方向，但我会继续前进。)*

# *那么这给我们留下了什么？*

*在另一篇报道 OpenAI 发布 GPT-2 的文章中，技术作家 Aaron Mak 重申了我在上面思考的问题:*

> *机器学习从业者尚未建立许多被广泛接受的框架，来考虑创造和发布人工智能技术的伦理影响。*

*然而，他继续说道:*

> *如果最近的历史有任何迹象，试图抑制或控制人工智能工具的扩散也可能是一场失败的战斗。即使在传播某些算法的道德问题上存在共识，也不足以阻止持不同意见的人。*

*虽然协议或广泛采用的一套准则可以帮助解决意外后果和这里讨论的其他问题，但 Mak 是正确的，因为它不会阻止实际的恶意行为者。*

*就个人而言，我很高兴 OpenAI 选择将这一对话带入公众，并发现这比他们发布或不发布的模型更重要。虽然我也倾向于让事情开源和共享信息——为了技术的民主化、透明度和问责制——但我也理解为什么人们会对发布可能以他们不希望的方式使用的技术和信息持谨慎态度。(我担心开源的其他方面，但那是以后的事了。)*

*谢谢你陪我踏上这段旅程。我仍在努力发展我对这些问题的理解，并澄清我的立场。我很乐意听到您对此的想法、问题或反馈。*