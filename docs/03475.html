<html>
<head>
<title>Randomly wired neural networks and state-of-the-art accuracy? Yes it works.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机连线神经网络和最先进的精确度？是的，它有效。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/randomly-wired-neural-networks-and-state-of-the-art-accuracy-yes-it-works-9fb3cedc8059?source=collection_archive---------7-----------------------#2019-06-03">https://towardsdatascience.com/randomly-wired-neural-networks-and-state-of-the-art-accuracy-yes-it-works-9fb3cedc8059?source=collection_archive---------7-----------------------#2019-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8804" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何设计出<em class="ki">最好的</em>卷积神经网络(CNN)？</h2></div><blockquote class="kj kk kl"><p id="51c2" class="km kn ko kp b kq kr ju ks kt ku jx kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">想获得灵感？快来加入我的<a class="ae lj" href="https://www.superquotes.co/?utm_source=mediumtech&amp;utm_medium=web&amp;utm_campaign=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="kp iu">超级行情快讯</strong> </a>。😎</p></blockquote><p id="76d0" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">尽管深度学习已经存在好几年了，但这仍然是一个没有答案的问题。</p><p id="6c3b" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">设计一个好的神经网络的大部分困难源于它们仍然是黑盒的事实。我们对它们是如何工作的有一些高层次的想法，但是我们并不真正了解它们是如何实现它们所做的结果的。</p><h1 id="6878" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">CNN 的一些线索</h1><p id="c674" class="pw-post-body-paragraph km kn it kp b kq mf ju ks kt mg jx kv lk mh ky kz ll mi lc ld lm mj lg lh li im bi translated">我们可以从一些线索入手。最近的研究为我们提供了一些 CNN 准确性提高的具体证据:</p><ul class=""><li id="a6ac" class="mk ml it kp b kq kr kt ku lk mm ll mn lm mo li mp mq mr ms bi translated">提高图像分辨率</li><li id="e4ca" class="mk ml it kp b kq mt kt mu lk mv ll mw lm mx li mp mq mr ms bi translated">增加网络深度</li><li id="7763" class="mk ml it kp b kq mt kt mu lk mv ll mw lm mx li mp mq mr ms bi translated">增加网络宽度</li><li id="bb63" class="mk ml it kp b kq mt kt mu lk mv ll mw lm mx li mp mq mr ms bi translated">添加跳过连接(无论是密集连接还是剩余连接)</li></ul><p id="f6d6" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">从直观的角度来看，深度学习的所有这些补充都是有意义的。提高图像分辨率可为网络提供更多数据。增加宽度和深度给了网络更多的参数。并且增加跳跃连接会增加网络的复杂性，从而增加表示能力。</p><div class="my mz na nb gt ab cb"><figure class="nc nd ne nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><img src="../Images/7c1655b6ed508bd73dd99f7a1c14c323.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*_YB5kL2KYBWYeVryKMCmgw.png"/></div></figure><figure class="nc nd np nf ng nh ni paragraph-image"><img src="../Images/14b9cf4040ebdc90cb2cc76be8be1775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*6DsE7zeluwc2sy55WCPf5Q.png"/><figcaption class="nq nr gj gh gi ns nt bd b be z dk nu di nv nw">Dense connections (left, <a class="ae lj" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener ugc nofollow" target="_blank">DenseNet</a>) and skip connections with a wide network (right, <a class="ae lj" href="https://arxiv.org/pdf/1611.05431.pdf" rel="noopener ugc nofollow" target="_blank">ResNeXt</a>). Sources are linked.</figcaption></figure></div><p id="5d22" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">为了发现更强大的网络，谷歌人工智能的研究团队开发了一种技术，让算法<em class="ko">搜索</em>好的网络。这被称为<a class="ae lj" rel="noopener" target="_blank" href="/everything-you-need-to-know-about-automl-and-neural-architecture-search-8db1863682bf">神经架构搜索</a> (NAS)。</p><h1 id="7f56" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">神经结构搜索</h1><p id="f4d7" class="pw-post-body-paragraph km kn it kp b kq mf ju ks kt mg jx kv lk mh ky kz ll mi lc ld lm mj lg lh li im bi translated">NAS 是<em class="ko">搜索</em>寻找<em class="ko">最佳</em> <em class="ko">神经网络架构</em>的算法。大多数 NAS 算法的工作原理非常相似。</p><p id="4a1f" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">首先定义一组可能用于我们网络的“构建块”。然后尝试通过将这些块的不同组合放在一起来训练网络。通过这种反复试验，NAS 算法将最终找出哪些数据块和哪些配置工作得最好。</p><p id="c984" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">事实证明，这种方法效果很好，达到了最先进的精度。它还发现了一些看起来非常奇怪的架构，你可以从下面的官方文件中看到:</p><figure class="my mz na nb gt nd gh gi paragraph-image"><div class="ab gu cl nx"><img src="../Images/b96188857d4cd477ea58ad46aef742c4.png" data-original-src="https://miro.medium.com/v2/format:webp/1*ChLWM5j4bUnC2HvpL24Pqw.png"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">NASNet blocks discovered by the NAS algorithm. <a class="ae lj" href="https://arxiv.org/pdf/1707.07012.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="fef7" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">NAS 的伟大之处在于，我们可以看到一些神经网络体系结构，否则我们可能根本看不到。</p><p id="56e4" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">然而，它也有自己的局限性。由于我们是从一组固定的数据块中进行采样，因此我们不会发现任何新的数据块。跳过连接的情况也是如此:NASNet 只允许跳过 1 个下采样级别的连接，但是如果其他类型的连接也可以工作呢？</p><p id="9b68" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated"><a class="ae lj" href="https://ai.facebook.com/" rel="noopener ugc nofollow" target="_blank">脸书的人工智能研究团队</a>通过尝试<a class="ae lj" href="https://arxiv.org/pdf/1904.01569.pdf" rel="noopener ugc nofollow" target="_blank"><em class="ko"/></a>随机连线神经网络，在 NAS 上进行了新的尝试。直觉是，如果体系结构搜索的一般思想对固定的块和连接工作良好，也许用更宽的搜索空间(即随机连接)运行它将导致一些未探索的配置。</p><h1 id="1edf" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">随机连线神经网络</h1><p id="3529" class="pw-post-body-paragraph km kn it kp b kq mf ju ks kt mg jx kv lk mh ky kz ll mi lc ld lm mj lg lh li im bi translated">由于该论文的作者希望特别关注<em class="ko">线路</em>即神经网络的连接，他们对网络结构做了一些限制:</p><ul class=""><li id="d20c" class="mk ml it kp b kq kr kt ku lk mm ll mn lm mo li mp mq mr ms bi translated">输入总是大小为 224x224 的图像</li><li id="620f" class="mk ml it kp b kq mt kt mu lk mv ll mw lm mx li mp mq mr ms bi translated">这些区块总是一个雷鲁-conv-巴特诺姆三联体。三元组中的卷积始终是一个来自<a class="ae lj" href="https://arxiv.org/pdf/1610.02357.pdf" rel="noopener ugc nofollow" target="_blank">的 3×3 可分卷积，但</a>除外。</li><li id="c1cb" class="mk ml it kp b kq mt kt mu lk mv ll mw lm mx li mp mq mr ms bi translated">多个张量的聚合(例如当融合与其他张量的跳过连接时)总是以相同的方式完成——通过加权求和。这些权重是可以学习的，并且总是正的。</li><li id="95be" class="mk ml it kp b kq mt kt mu lk mv ll mw lm mx li mp mq mr ms bi translated">网络的<em class="ko">级</em>总是保持不变。多重卷积块的一般 CNN 结构，随后是下采样，反复重复直到最终的 softmax 分类器，已经成为最先进的网络设计中的标准。在这些研究实验中也采用了这种范式。</li></ul><figure class="my mz na nb gt nd gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ddc027c485cb003a0d7040320626af75.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*re0CVqZdEny4EZRT-SWgZA.png"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">General structure of state-of-the-art CNNs, also adapted for this research. <a class="ae lj" href="https://arxiv.org/pdf/1904.01569.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="024b" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">224x224 的输入大小在 ImageNet 竞赛中是非常标准的，ImageNet 数据集用于对其他手工制作的网络以及 NAS 生成的网络进行基准测试。ReLU-Conv-BatchNorm 的三联体阻滞也很常见，并已被广泛证明在整个深层 CNN 中重复时可提供最先进的结果。</p><p id="a697" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">张量的聚合方式是一个小小的转变。大多数最先进的网络都是直接相加或无权重连接，但这是一个不应该对性能产生太大影响的小加法。上表所示的 CNN 分级设计也已成为 ResNets、DenseNets 和 NASNets 中使用的首选网络结构。</p><p id="a4f1" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">注意这并不完全是一个“随机神经网络”。没有完全从零开始的随机化。相反，CNN 设计的单个组件，即布线，在其他组件保持固定的情况下进行探索。</p><p id="d3dd" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">这是作者试图向读者传达的一个重要观点。我们还没有做完全随机的神经网络。但是我们开始对组件搜索空间做一些深入的探索——一步一个脚印。</p><p id="e587" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">有了这些约束，各种经典随机图模型<em class="ko">被用来生成网络的随机布线。</em></p><p id="ad1a" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">我说的随机是指<strong class="kp iu">随机</strong>。</p><figure class="my mz na nb gt nd gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/bfcfa354570616b9816698b4713b8e6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*CbucZKoUm5hMxVhQRYGVUg.png"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Some of the randomly wired networks. <a class="ae lj" href="https://arxiv.org/pdf/1904.01569.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="1c7e" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">随机网络打开了深度学习探索的大门</h1><p id="8954" class="pw-post-body-paragraph km kn it kp b kq mf ju ks kt mg jx kv lk mh ky kz ll mi lc ld lm mj lg lh li im bi translated">本研究的意义在于其主要的探索性思想:扩展 NAS 算法的搜索空间，以发现新的、更好的网络设计。虽然研究人员自己已经发现了一些伟大的设计，但手动搜索整个搜索空间实际上是不可行的。</p><p id="c4c1" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">这也延伸到另一点:如果我们正在扩大我们的搜索空间，我们将需要一个算法，在这种情况下，我们的网络生成器，要真正擅长搜索。它必须知道要寻找什么，或者至少有一些通过设计向它移动的趋势，就像梯度下降工作这样的优化算法一样。</p><p id="2635" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">架构搜索是深度学习研究的下一个前沿。它允许我们使用算法，而不是试错法来发现最佳架构。</p><p id="eaa0" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">目前，修复一些网络组件同时搜索其他组件(在本例中为布线)是有意义的。这就把问题简化成了一件小事，更容易处理。NAS 算法应该有一定程度的随机性，因为这是我们发现真正新颖的架构的唯一方法。</p><p id="a9e2" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">下一步当然是进一步扩展搜索空间和我们搜索算法的随机性。这意味着扩展到越来越多的网络组件，直到整个事情都是用算法设计的。</p><p id="59fb" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">当我们放开 NAS 时会发生什么？它会选择像 2x4 这样的非正方形卷积吗？它提议使用反馈回路吗？网络变得更简单还是更复杂？</p><p id="b80a" class="pw-post-body-paragraph km kn it kp b kq kr ju ks kt ku jx kv lk kx ky kz ll lb lc ld lm lf lg lh li im bi translated">神经结构搜索提出了一个令人兴奋的新的研究领域。希望让搜索算法变得宽松，成为一种利用随机化的方式，为我们的优势，发现创造性的和以前从未想过的架构。</p></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><h1 id="f042" class="ln lo it bd lp lq oh ls lt lu oi lw lx jz oj ka lz kc ok kd mb kf ol kg md me bi translated">喜欢学习？</h1><p id="ec46" class="pw-post-body-paragraph km kn it kp b kq mf ju ks kt mg jx kv lk mh ky kz ll mi lc ld lm mj lg lh li im bi translated">在<a class="ae lj" href="https://twitter.com/GeorgeSeif94" rel="noopener ugc nofollow" target="_blank">推特</a>上关注我，我会在那里发布所有最新最棒的人工智能、技术和科学！也在<a class="ae lj" href="https://www.linkedin.com/in/georgeseif/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上和我联系吧！</p></div></div>    
</body>
</html>