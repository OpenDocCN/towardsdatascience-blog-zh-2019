<html>
<head>
<title>Analyzing my weight loss with machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用机器学习分析我的减肥</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/analyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2?source=collection_archive---------8-----------------------#2019-02-19">https://towardsdatascience.com/analyzing-my-weight-loss-journey-with-machine-learning-b74aa2e170f2?source=collection_archive---------8-----------------------#2019-02-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="677e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">个人健康</h2><div class=""/><div class=""><h2 id="4ec8" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">我如何用 Python 从头开始构建一个逻辑回归分类器来预测我的体重减轻</h2></div><p id="4aaa" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">要看我为这个项目写的代码，可以看看它的 Github </em> <a class="ae ll" href="https://github.com/seismatica/logweight" rel="noopener ugc nofollow" target="_blank"> <em class="lk">回购</em> </a></p><h1 id="a260" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">背景</h1><p id="d6a9" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi mj translated">我在 2018 年初开始了我的减肥之旅，遵循着人们常说的“减肥=饮食+运动”的建议。在饮食方面，我开始跟踪我每天的食物消耗量(使用食物秤并通过<a class="ae ll" href="https://www.loseit.com/" rel="noopener ugc nofollow" target="_blank"> Loseit </a>应用程序记录卡路里)。在锻炼方面，我开始遵循“沙发到 5 公里”计划，到目前为止已经完成了四个 5 公里，一个 10K，还有几周前的一个半程马拉松。最后，每天早上，我醒来后马上称体重，并在同一个 Loseit 应用程序中记录我的体重。</p><h1 id="e71c" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">问题</h1><p id="a1c8" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">任何试图减肥的人都不可避免地会遇到减肥平台期，最初快速的减肥开始放缓。就我个人而言，我在五月初的假期后达到了一个主要的平台期。在那之后，我放弃了追踪我的进度，持续了近三个月。只有在我用一个 Amazfit Bip 取代了快要报废的 Pebble 智能手表后，我才重新获得了一些按下恢复按钮的动力，部分原因是我可以用新手表开始记录我的步数。然而，我的体重持续稳定，在经历了两个月令人沮丧的体重波动后(见下面的虚线区域)，我完全停止了追踪我的体重和卡路里。那是去年的 11 月。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ms"><img src="../Images/130f7d40ab275959afc1b0bb4253d527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7mhRSD1ZNE0jBvCUblWuGw.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><strong class="bd ni">Left:</strong> Progress report of my weight in 2018. <strong class="bd ni">Right:</strong> time window for my data (when steps were tracked)</figcaption></figure><p id="741e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在是 2019 年，随着 Tet(越南新年)最近接近尾声，我决定更仔细地研究在这两个月期间收集的数据，希望发现我的减肥和跟踪的卡路里/步数之间的有趣关系，以便我可以制定比以前更有效的减肥计划。</p><h1 id="7ccd" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">数据</h1><h2 id="82f2" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">数据收集</h2><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/37b27fa6911ed89441b642dc831b74ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lFFMu7OJ7UFTLr1fhz4FEw.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Calories (budget &amp; consumed) and steps for September 6th, 2018</figcaption></figure><p id="6e7f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">卡路里:</strong>从我的 Loseit 账户导出为 CSV 文件。出于某种奇怪的原因，Loseit 只允许一次导出一周的卡路里数据，但是将它们连接在一起是快速 Python 脚本所不能做到的。每个日期都有我当天记录的所有食物的相应卡路里计数，以及应用程序根据我当天的体重和我最初指定的减肥目标(每周减肥 0.75 公斤)为我计算的卡路里“预算”。</p><p id="b5db" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">步骤:</strong>我的 Amazfit Bip 智能手表的 Android 应用程序不允许数据导出，除非有人使用第三方工具使用一些令人毛骨悚然的<a class="ae ll" href="https://forum.xda-developers.com/general/accessories/xiaomi-mi-band-data-extraction-t3019156" rel="noopener ugc nofollow" target="_blank">变通方法</a>。因此，获得我的步骤数据的最快方法是手动滚动手机上的几十个日期(在这两个月内)，并将每个日期的步骤输入到一个 CSV 文件中。一点也不优雅，但是，嘿，这很有效！</p><p id="0dff" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">体重:</strong>谢天谢地，Loseit 网站允许我将我所有记录的体重——以及它们的拍摄日期——导出为一个 CSV 文件。</p><p id="5fd1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在按日期将这 3 个数据源连接在一起后，我最终得到了这两个月中只有 46 个日期的卡路里+步数+体重数据。显然，我在相当多的日子里(出于显而易见的原因，很多日子是周末)忽略了记录这三个数据中的至少一个。</p><h2 id="69b7" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">数据转换</h2><p id="a883" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">根据这三个原始数据源，计算三个附加数据字段:</p><p id="daba" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">剩余=消耗的卡路里-卡路里预算</strong>。正剩余意味着我吃了比那天允许的预算更多的卡路里，反之亦然。我选择使用卡路里盈余而不是消耗的原始卡路里，因为应用程序的卡路里预算会随着我体重的上升和下降而自然变化，所以卡路里盈余(考虑了上述预算)比卡路里本身更能准确地衡量我的饮食习惯。</p><p id="fd75" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">增重=明天体重-今天体重</strong>。给定日期的正体重增加意味着我在那一天增加了体重(咄！)，反之亦然。</p><p id="75c3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">体重增加状态:</strong>正的体重增加标记为 1，负的或零体重增加标记为 0。我决定使用二元体重增加状态——无论我是否增加了体重——而不是更具体的体重增加量，因为纠结于增加 0.5 公斤还是 0.3 公斤是非常适得其反的，尤其是因为体重增加量会受到除卡路里和步骤之外的许多因素的影响(如饮水量、进食时间等)。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nv"><img src="../Images/4af5b239cae4b9081e2d915c6be85dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GCgq_bdnyssIhYitUUHwQQ.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">First few rows of my final data table</figcaption></figure><h2 id="6259" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">数据可视化</h2><p id="881b" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">当绘制卡路里剩余量和每天的步数与我当天体重增加状况的关系图时(见下面的前两个图)，这两个据说很重要的因素如何预测我是否会增加体重似乎没有明显的模式。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nw"><img src="../Images/4e64a542abc9625cfa1d8b9234e40707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pD9VqzlPeD2UvRcDPmu2uA.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><strong class="bd ni">Left &amp; middle:</strong> plot of weight gain status against calories surplus and steps separately. <strong class="bd ni">Right:</strong> plot of weight gain status against surplus and steps together</figcaption></figure><p id="9b7f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，当绘制卡路里剩余量和步数时(上图右侧)，非常有趣的模式出现了！例如，我马上就能看出，根据我的步数，有两组截然不同的日期:那些低于 5000 步的日期(我的“基线”懒惰日)整齐地围绕着一条水平线，而那些高于 5000 步的日期(我的活跃日)，这主要归功于我的跑步。就卡路里过剩而言，有三个主要观察结果:</p><p id="378f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> 1。</span>在我懒惰的日子里，如果我吃的超过了我的卡路里预算限制，第二天当我走上体重秤时，这将是个坏消息。有一些奇迹，比如有一天我吃了超过限量的 1500 卡路里，但第二天体重并没有增加，但这种情况很少。</p><p id="044d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> 2。</span>另一方面，如果我在懒惰的日子里吃得低于我的卡路里预算限制，我也不完全清楚:有好几天我是个好孩子，吃我的沙拉(打个比方)，但仍然增加了体重。这表明我应该在懒惰的日子里更加保守地饮食。</p><p id="468b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> 3。</span>然而，在我活跃的日子里，我似乎可以吃得比我的卡路里限量还多，因为有几天我吃得比限量还多，但体重却没有增加。</p><p id="42ce" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这些观察表明，我应该考虑我的日常活动(以步数的形式)，而不是使用 Loseit 应用程序的默认限制。</p><ul class=""><li id="f67a" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">例如，在上面的剩余步骤图上，我可以画一条直线，将我体重增加的天数(红色)和体重减少的天数(绿色)很大程度上分开。</li><li id="939a" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">然后，我可以使用该线性边界来告知我的减肥策略，即我如何保持在边界的减肥一侧，而不是另一侧。</li><li id="fe3b" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">用机器学习的说法，这相当于建立一个<a class="ae ll" href="https://en.wikipedia.org/wiki/Linear_classifier" rel="noopener ugc nofollow" target="_blank">线性分类器</a>对我的数据进行二进制分类(对体重增加和体重减少进行分类)。</li></ul><h2 id="9ae8" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">使用哪个线性分类器？</h2><p id="2eb6" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">从数据构建线性分类器有几种常见的方法，如<a class="ae ll" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>、<a class="ae ll" href="http://linear discriminant analysis" rel="noopener ugc nofollow" target="_blank">线性判别分析</a>或<a class="ae ll" href="https://en.wikipedia.org/wiki/Support-vector_machine#Linear_SVM" rel="noopener ugc nofollow" target="_blank">支持向量机</a>。但是，对于这个项目，我将使用<strong class="kq ja">逻辑回归</strong>，因为:</p><ul class=""><li id="441f" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">这很容易解释:例如，分类边界的方程可以很容易地从逻辑回归系数中获得。</li><li id="3968" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">易于实现:这是一个非常重要的原因，因为这个项目的另一个目标是让我实现一个机器学习项目，而不使用预先存在的库(如 scikit-learn)。正如我将在后面展示的，逻辑回归学习算法的核心只需要 3 行代码就可以完成！</li></ul><p id="1cc2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">选择模型后，让我们看看如何使用我的数据实现和解释逻辑回归。然而，这需要我们回顾一些关于逻辑回归分类器如何工作以及如何从数据中学习的数学知识。</p><h1 id="7489" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">数学评论</h1><p id="bb45" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">请注意，这个数学复习主要是为了建立一个通用的符号，以便在从数学方程实现算法之前，我们在同一页上。如果你想了解这些方程是如何推导出来的，以及它们背后的直觉，我在这篇博文的末尾链接了更多的资源，这些资源可以比我做得更好。</p><h2 id="20a9" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">根据特征预测体重增加的概率</h2><p id="05b1" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">在逻辑回归中，我在某一天体重增加的预测概率是应用于我的特征(卡路里剩余量和步数)的加权线性组合的<a class="ae ll" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid 函数</a>加上常数截距项。这种关系在数学上表示为:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ol"><img src="../Images/b8272a938a33675632c8d653f75be393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v8_8MNLrI9YeAUZKHiPiNw.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Superscript (i) denotes a a single data point/date</figcaption></figure><p id="dcef" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在哪里</p><ul class=""><li id="0ed5" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated"><strong class="kq ja"> y(i): </strong>日期 I 的体重再次状态(1 =体重增加，0 =体重减少)</li><li id="926f" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><strong class="kq ja"> P(y(i)=1) </strong>:我在第一天体重增加的预测概率</li><li id="addf" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><strong class="kq ja"> x(i): </strong>在日期 i*时各个特征(卡路里剩余量和步数)的观察值</li><li id="4166" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><strong class="kq ja"> θ </strong>:各个特征的回归系数/权重(从数据中学习)*</li></ul><p id="6b2b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">*请注意，我添加了一个额外的特征(<strong class="kq ja"> x_intercept </strong>)，该特征将始终等于 1，以便可以学习截距项(<strong class="kq ja"> θ_intercept </strong>)以及两个现有特征的系数。</p><p id="7d6f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，一旦从数据中学习到θ，就可以使用逻辑回归来分类我是否会在任何给定的一天增加体重——给定我的卡路里剩余量和步数——通过检查我在那一天增加体重的概率是否高于某个阈值(通常为 50%)。</p><h2 id="ef81" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">如何学习<strong class="ak"> θ值</strong></h2><p id="0cdf" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">回归系数(θ)是通过最大化观察我的训练数据的概率的对数(也称为对数似然)来学习的。对数似然的公式是:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi om"><img src="../Images/9f65b422e1a101523bf1be0e01fa7668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUr4deBs9fcqK8leooAuiw.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Log-likelihood of m training data points/dates</figcaption></figure><p id="50e7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在哪里</p><ul class=""><li id="8575" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated"><strong class="kq ja"> L </strong>:训练数据的对数似然(m 个数据点)</li><li id="6c5b" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><strong class="kq ja"> y(i): </strong>日期 I 的真实体重增加状态(1 =体重增加，0 =体重减少)</li><li id="3649" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><strong class="kq ja"> P(y(i)=1) </strong>:我在第一天体重增加的预测概率(从之前的 sigmoid 函数中获得)</li></ul><p id="02a9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">根据 sigmoid 函数，不同的θ值将产生不同的体重增加预测概率(P(y(i)=1)，从而产生不同的对数似然性。因此，我们的目标是找到最大化我的训练数据的对数似然性的一组θ，即最好地“解释”我的训练数据的θ。</p><h2 id="0e74" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">使用批量梯度上升最大化对数似然</h2><p id="58b7" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">找到使我的训练数据的对数似然最大化的θ的一个简单算法是<strong class="kq ja">批量梯度上升</strong>，如下所述:</p><p id="0176" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">第 0 步:</strong>初始化θ的一些值(θ_ 截距，θ_ 剩余，θ_ 步长)</p><p id="aaf5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">步骤 1: </strong>对于每个训练数据点 I，使用该数据点的特征值(x_intercept*，x_surplus，x_step)和步骤 0 中初始化的θ的值，计算体重增加的概率。这是使用熟悉的 sigmoid 函数完成的:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi on"><img src="../Images/0db027c39706591d6b6a33e433c69a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RKPJu8a4KUyE-48x-gODog.png"/></div></div></figure><p id="189b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">*回忆 x_intercept = 1</p><p id="6780" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">步骤 2: </strong>对于每个特征 j——截距/剩余/步长——使用下面的等式找到对数似然性相对于该特征θ的偏导数:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi oo"><img src="../Images/56dc7b0334dddb8912f888c2aa7c868f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rvB_uyoKUV8NjSO5ILDFw.png"/></div></div></figure><p id="4d68" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在哪里</p><ul class=""><li id="22a9" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated"><strong class="kq ja"> ∂L/∂θⱼ: </strong>对数似然相对于特征 j 的θ的偏导数</li><li id="46cb" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><strong class="kq ja"> y(i): </strong>日期 I 的真实体重再次状态(1 =体重增加，0 =体重减少)</li><li id="9b8b" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><strong class="kq ja"> P(y(i)=1): </strong>第一天体重增加的预测概率(来自步骤 1)</li><li id="ede4" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><strong class="kq ja"> xⱼ(i): </strong>特性 j(截距/剩余/步长)在日期 I 的观测值，其中 x _ 截距(i) = 1，对于任何 I</li></ul><p id="91f3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">步骤 3: </strong>对于每个特征 j，通过对数似然性相对于θ的偏导数(来自步骤 2)乘以一个小常数(也称为学习率<em class="lk"> α </em>)来更新其θ:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi op"><img src="../Images/162eae19edfda7d28d550119ac04c128.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*NLedi7bUZsgftCL11SnWTw.png"/></div></figure><p id="2c68" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这种学习速率控制算法收敛到对数似然最大值的速度，甚至控制算法是否收敛(有关更多详细信息，请参见后面的可视化算法收敛一节)。</p><p id="e35a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">步骤 4: </strong>用这些更新的θ，重复步骤 1 到步骤 3，直到收敛。测试收敛的一种方法是查看对数似然是否已经收敛到稳定值，即它是否已经达到可能的最大值。</p><h2 id="99e4" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">批量与随机梯度上升</h2><p id="a6af" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">步骤 2 中的求和符号——对所有数据点 I 求和<strong class="kq ja"> (y(i) - P(y(i)=1)) * xⱼ(i) </strong>以计算偏导数——是该梯度上升算法属于<strong class="kq ja">批次</strong>种类的原因，因为每个偏导数是使用训练数据中的<em class="lk">所有</em>数据点计算的。没有这个求和符号，即偏导数仅使用<em class="lk">一个</em>数据点计算；假设数据点是随机选择的，梯度上升算法称为<strong class="kq ja">随机</strong>。</p><p id="dc54" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于这个项目，我选择实现批量梯度上升，因为我的训练数据非常小(只有 46 个数据点)，所以一次使用整个训练数据集来计算偏导数没有问题。另一个原因是使用 numpy 的矢量化运算可以更容易地实现批量梯度上升(请参考下面的实现来了解如何实现)。</p><h1 id="8c75" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">模型实现</h1><h2 id="fd6c" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">数据预处理</h2><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/e4b5e2f61e82486704fb880b92d317cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*LzmESOKIpMLNqODbu5M3gg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Features &amp; labels for 6 dates out of 46 in my training data</figcaption></figure><p id="be31" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">根据我之前的数据表，我使用剩余卡路里和步数列作为我的特征，体重增加状态列作为标签(见左)来训练我的逻辑回归分类器。</p><p id="0e2b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是，在对该数据实施批量梯度上升算法之前，我首先需要:</p><ol class=""><li id="1176" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj or od oe of bi translated"><strong class="kq ja">在我的训练数据中添加一列 1 的</strong>来表示 x_intercept 特性的值。</li><li id="d52c" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj or od oe of bi translated"><strong class="kq ja">通过(a)减去每个特征列的平均值和(b)除以其标准偏差，重新调整我的卡路里剩余量和步数特征</strong>。这样做有两个原因:</li></ol><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ol"><img src="../Images/b8272a938a33675632c8d653f75be393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v8_8MNLrI9YeAUZKHiPiNw.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Sigmoid function</figcaption></figure><ul class=""><li id="33f7" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">回想一下，sigmoid 函数(左)涉及 x 的线性组合的指数，因此 x 的非常小/大的值将使该指数内爆/爆炸。事实上，在我重新调整我的特征之前，我有可怕的趋同性，我不知道为什么。只有当我偶然看到 Jupyter 终端(而不是 Jupyter 笔记本输出)时，我才看到 numpy 默默产生的下溢/上溢警告页面！在我重新调整我的特征后，这些警告消失了，我的算法能够收敛了。</li><li id="d5d2" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">通过将我的特征缩小到相同的比例，我可以使用这些特征的回归系数(θ)来衡量它们在我体重增加/减少中的相对重要性。这将在我的模型的解释部分详细阐述。</li></ul><p id="c445" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在以上两个步骤之后，我的特征矩阵(X)被转换成维数为(46，3)的二维 numpy 数组，我的标签向量(y)被转换成维数为(46)的一维 numpy 数组。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi os"><img src="../Images/074b8e56c724d1eaa69d47e4e50b5a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vP80IZ3MceA7r-Sv97E-Cg.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><strong class="bd ni">Left: </strong>feature matrix X (46, 3). Columns (L to R): x_intercept, x_surplus, x_step. <strong class="bd ni">Right: </strong>label array y (46)</figcaption></figure><h2 id="4b2c" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">实施批量梯度上升</h2><p id="b244" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated"><strong class="kq ja">步骤 0:初始化θ的</strong></p><pre class="mt mu mv mw gt ot ou ov ow aw ox bi"><span id="421e" class="nj ln iq ou b gy oy oz l pa pb">theta = np.array([0.5, 0.5, 0.5])</span></pre><p id="980b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我将所有θ初始化为 0.5。结果是一个维度为(3)的一维 numpy 数组<code class="fe pc pd pe ou b">theta</code></p><p id="6689" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">步骤 1:对于每个数据点，使用步骤 0 中的θ和 sigmoid 函数计算体重增加的概率</strong></p><pre class="mt mu mv mw gt ot ou ov ow aw ox bi"><span id="6e3b" class="nj ln iq ou b gy oy oz l pa pb">prob = 1 / (1 + np.exp(-X @ theta))</span></pre><p id="20c8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这就是 numpy 的矢量化运算派上用场的地方，因为 numpy 不是计算每个数据点的体重增加概率，而是一次计算所有数据点的体重增加概率:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pf"><img src="../Images/a599e6646f9cdc1d7663903f5e75e180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X6cKtHcVA50o191EgTtRiA.png"/></div></div></figure><ul class=""><li id="d548" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated"><code class="fe pc pd pe ou b">X @ theta</code>:通过将矩阵<code class="fe pc pd pe ou b">X</code>与向量<code class="fe pc pd pe ou b">theta</code>相乘，numpy 实质上是通过将<code class="fe pc pd pe ou b">X</code>的每一行与<code class="fe pc pd pe ou b">theta</code>列进行点积来计算每个数据点的 x 和θ的线性组合(见图中加粗的单元格)。</li><li id="546b" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><code class="fe pc pd pe ou b">1 / (1 + np.exp(-X @ theta))</code>:在计算了所有数据点的 x 和θ的线性组合后，将 sigmoid 函数应用于它们中的每一个，以获得所有数据点的体重增加的最终概率。注意，这个 sigmoid 函数中的操作(<code class="fe pc pd pe ou b">/</code>、<code class="fe pc pd pe ou b">+</code>、<code class="fe pc pd pe ou b">-</code>、<code class="fe pc pd pe ou b">np.exp</code>)都表示 numpy 在后台运行的矢量化函数。这最终输出了概率向量<code class="fe pc pd pe ou b">prob</code>，一个维数为(46)的一维 numpy 数组。</li></ul><p id="9295" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">步骤 2:对于每个特征，使用步骤 1 </strong>中计算的概率，计算对数似然对相应θ的偏导数</p><pre class="mt mu mv mw gt ot ou ov ow aw ox bi"><span id="d0d7" class="nj ln iq ou b gy oy oz l pa pb">gradient = (y - prob) @ X</span></pre><p id="eef8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这也是 numpy 的矢量化运算的亮点:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pg"><img src="../Images/39574935d3bb9cdad62f41dbad35b297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EM56cRbkqEjBbfgBB3IsEw.png"/></div></div></figure><ul class=""><li id="b1e5" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated"><code class="fe pc pd pe ou b">y - prob</code>:对所有数据点的真实标签和预测概率进行直接的逐个元素的减法，得到表示差异的一维 numpy 数组(46 维)。</li><li id="c1ed" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><code class="fe pc pd pe ou b">(y - prob) @ X</code>:在<code class="fe pc pd pe ou b">y - prob</code>差向量(46)与特征矩阵<code class="fe pc pd pe ou b">X</code>相乘之前，由 numpy 在幕后转置成一个维数为(1，46)的行向量，使其维数与<code class="fe pc pd pe ou b">X</code> (46，3)的维数对齐。这种转置在 numpy 中也称为列向量的“向维度添加 1”。</li><li id="f967" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">一旦尺寸对齐，<code class="fe pc pd pe ou b">y - prob</code>和<code class="fe pc pd pe ou b">X</code>之间的向量矩阵乘法可以发生:点积在<code class="fe pc pd pe ou b">y - prob</code>的行向量和<code class="fe pc pd pe ou b">X</code>的每个特征列之间进行(见图中加粗的单元)。这实际上是对所有数据点 I 的<strong class="kq ja">(y(I)-p(y(I)= 1))* xⱼ(i)</strong>求和，以获得特征 j 的对数似然的偏导数(等式 2)。更令人印象深刻的是，在这种向量矩阵乘法下，可以同时计算所有三个特征的偏导数，从而产生一个称为<code class="fe pc pd pe ou b">gradient</code>的偏导数向量。</li><li id="6697" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">从技术上讲，这个向量应该是(1，46)向量乘以(46，3)矩阵所得的维数(1，3)。但在幕后，numpy 在乘法后“去掉了(乘法前的)前置 1”，最后的<code class="fe pc pd pe ou b">gradient</code>向量是一个维数为(3)的一维数组。numpy 在乘法前后应用于其数组的这些幕后“扭曲”可以参考<code class="fe pc pd pe ou b"><a class="ae ll" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html" rel="noopener ugc nofollow" target="_blank">numpy.matmul</a></code>的文档，它实现了矩阵乘法运算符<code class="fe pc pd pe ou b">@</code>。</li></ul><blockquote class="ph pi pj"><p id="02bb" class="ko kp lk kq b kr ks ka kt ku kv kd kw pk ky kz la pl lc ld le pm lg lh li lj ij bi translated">如果第一个参数是一维的，则通过在它的维数前加上 1，将它提升为矩阵。在矩阵乘法之后，前置的 1 被移除。</p><p id="1ef4" class="ko kp lk kq b kr ks ka kt ku kv kd kw pk ky kz la pl lc ld le pm lg lh li lj ij bi translated">— numpy.matmul 文档</p></blockquote><p id="31de" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">步骤 3:对于每个特征，用步骤 2 中的偏导数乘以学习率<em class="lk">α</em>T12】来更新其θ</strong></p><pre class="mt mu mv mw gt ot ou ov ow aw ox bi"><span id="27db" class="nj ln iq ou b gy oy oz l pa pb">alpha = 0.01<br/>theta = theta + alpha * gradient</span></pre><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pn"><img src="../Images/257b8940e69568675b51ff3c5e991313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Im0-fgS3n3mQNsIDA14UHw.png"/></div></div></figure><p id="d40b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这再简单不过了:我们可以通过将偏导数的<code class="fe pc pd pe ou b">gradient</code>向量乘以某个预定义的学习速率<code class="fe pc pd pe ou b">alpha</code>，并将乘积加到<code class="fe pc pd pe ou b">theta</code>向量，来一次更新所有 3 个θ，而不是更新每个θ。</p><p id="ed6d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">步骤 4:重复步骤 1 至 3，直到收敛</strong></p><p id="28d8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这可以通过将前面的 3 行代码嵌套到一个多次迭代的<code class="fe pc pd pe ou b">for</code>循环中来轻松实现——每一步一行(见下面加粗的代码块)。下面展示的是运行 100 次迭代的批量梯度上升的完整算法(实现如此简单，难道不令人惊讶吗？！):</p><pre class="mt mu mv mw gt ot ou ov ow aw ox bi"><span id="4f1d" class="nj ln iq ou b gy oy oz l pa pb">theta = np.array([0.5, 0.5, 0.5])<br/>alpha = 0.01</span><span id="1f3d" class="nj ln iq ou b gy po oz l pa pb">for _ in range(100):<br/><strong class="ou ja">    prob = 1 / (1 + np.exp(-X @ theta))<br/>    gradient = (y - prob) @ X<br/>    theta = theta + alpha * gradient</strong></span></pre><h2 id="90da" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">检查收敛</h2><p id="717a" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">检查算法收敛性的一种方法是查看对数似然性的差异在循环的过去几次迭代中是否保持在某个小容限水平以下，这表明对数似然性可能已经达到其最大值。使用<code class="fe pc pd pe ou b">y</code>和<code class="fe pc pd pe ou b">prob</code>(及其各自补码)的点积，对数似然可以简单地计算为:</p><pre class="mt mu mv mw gt ot ou ov ow aw ox bi"><span id="13d9" class="nj ln iq ou b gy oy oz l pa pb">log_likelihood = y @ np.log(prob) + (1 - y) @ np.log(1 - prob)</span></pre><p id="8981" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一种(也许更有趣)方法是运行算法一定次数的迭代，并观察对数似然性是否达到了可能的最大值。下面是学习率α = 0.01 的批量梯度上升算法的 60 次迭代的可视化:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pp"><img src="../Images/6a07d66c1e817f3077d27e1837d8339e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1B7BxNWV-KoJy5jlImMkVw.gif"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><strong class="bd ni">Left:</strong> convergence of θ’s. <strong class="bd ni">Middle: </strong>convergence of average log-likelihood. <strong class="bd ni">Right:</strong> convergence of decision boundary</figcaption></figure><ul class=""><li id="9398" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">从上面的动画中可以看出，平均对数似然性(对数似然性除以训练数据的数量，见中间的面板)在前 10 次迭代中快速上升，但之后开始趋于平稳。在第 60 次迭代时，迭代之间的平均对数似然的差异在 10^-5 的数量级，这表明足够好的收敛。这也对应于左图中回归系数(θ)的收敛。</li><li id="b7b5" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">另一种可视化这种收敛的方法是通过分类边界(见右图)。分类边界，也称为<strong class="kq ja">决策边界</strong>，代表体重增加的预测概率为 50%的线:其上的每个点的预测概率低于 50%，因此被分类为体重下降，其下的每个点的预测概率高于 50%，因此被分类为体重增加。在这 60 次迭代中，决策边界似乎稳定在一个合理的边界上，该边界将大多数体重减轻的日子(绿色)与体重增加的日子(红色)分开。</li></ul><p id="8c19" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在我们有了一个融合的逻辑回归模型，可以很好地对我的训练数据进行分类(至少通过目测决策边界)，让我们看看如何改进它。</p><h1 id="16c1" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">模型改进</h1><h2 id="ba61" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated"><strong class="ak">选择合适的学习速度</strong></h2><p id="7c11" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">给定固定的迭代次数，学习率(α)的值可以确定算法在这些迭代之后是否会收敛，或者是否会收敛:</p><ul class=""><li id="cb98" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">当α减少到 0.001(从最初的 0.01)时，θ的学习发生得更慢，并且在 60 次迭代之后，平均对数似然性仍然显示出增加的迹象。因此，在这个小的学习速率下，迭代次数应该增加，或者学习速率本身应该提高。</li></ul><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pp"><img src="../Images/b1295ee5268dfdf64a18bf50ce6a665d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*iCuSTx5wjFDBKqwAMnlRcg.gif"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Dotted line represents the respective values at α = 0.01 for comparison</figcaption></figure><ul class=""><li id="31b1" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">然而，如果学习率太高，θ可能会被“过度修正”，并在每次更新后在最佳点附近反弹。这一点从下面的α = 1 时的收敛动画可以看出，60 次迭代后似乎并没有收敛。因此，在这些情况下，应该降低学习率。</li></ul><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pp"><img src="../Images/29075d1903ff7980ea7bdbd9234db597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*kQ_qwjfC9KrEOEDmHYBuYQ.gif"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Dotted line represents the respective values at α = 0.01 for comparison</figcaption></figure><p id="c7d5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当α = 0.01 作为我们学习率的最佳点时，我们当然可以增加迭代的次数，以确保我们的模型很好地收敛到最大对数似然。事实上，在 1000 次迭代时，迭代之间的平均对数似然的差异实际上为零。</p><h2 id="3bd8" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">使用岭回归减少过度拟合</h2><p id="b86e" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">尽管我的逻辑回归模型已经收敛到我的训练数据的最大对数似然，但它可能会<strong class="kq ja">过度拟合</strong>训练数据，即它从数据中学习得有点太好了。因此，该模型可能会对我在 2018 年收集的过去数据进行很好的预测，但如果我用它来预测我在 2019 年的体重增加，可能会很糟糕。</p><p id="cf6d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">减少逻辑回归过度拟合的一个解决方案是使用回归的<a class="ae ll" href="https://en.wikipedia.org/wiki/Tikhonov_regularization" rel="noopener ugc nofollow" target="_blank"> L2 正则化</a>版本(也称为岭回归)，它通过一个由θ的平方组成的项减去原始对数似然:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pq"><img src="../Images/e1c5649918c6c26517186c893bbdcf68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_L7MKhwrQlha-8QxJdv3xQ.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><strong class="bd ni">m:</strong> number of training data points, <strong class="bd ni">n:</strong> number of features</figcaption></figure><p id="63bb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，最大化上述函数相当于尽可能最大化训练数据的对数似然，同时保持θ较低(因为较高的θ将降低 L)。λ符号(λ)表示θ保持较低的程度(通常称为模型的<strong class="kq ja">正则化超参数</strong>)。当λ = 0 时，岭回归精确地返回到原始的、非正则化的逻辑回归。</p><p id="15db" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在岭回归的实现方面，与原始逻辑回归的唯一区别在于批量梯度上升的偏导数(等式 2)的计算，其中从每个特征 j 的偏导数中减去<strong class="kq ja"> λ*θⱼ </strong>正则化项:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/efb235f485b8f104078bb06f988fbaa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*IzhcSGM-ykUq1KvATzq2eQ.png"/></div></figure><p id="021a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">需要注意的是，这种正则化并不经常用于截距θ，因此θ_intercept 的偏导数的计算与非正则化版本相同，即不减去λ*θ项。</p><p id="c0b8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该实现可以通过以下方式轻松集成到我们现有的 Python 代码中:</p><ol class=""><li id="3f10" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj or od oe of bi translated">将<code class="fe pc pd pe ou b">theta</code>乘以正则化超参数<code class="fe pc pd pe ou b">lambda_reg</code>得到<code class="fe pc pd pe ou b">reg_term</code>——维数(3)的λ*θ正则化项。</li><li id="0d5c" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj or od oe of bi translated">将<code class="fe pc pd pe ou b">reg_term</code>的第一个元素设置为零，表示θ_intercept 没有被正则化</li><li id="bc11" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj or od oe of bi translated">从<code class="fe pc pd pe ou b">(y - prob) @ X</code>中减去<code class="fe pc pd pe ou b">reg_term</code>得到<code class="fe pc pd pe ou b">gradient</code>。</li></ol><p id="a945" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">以下是λ = 10 的岭回归代码，对原始算法的修改以粗体显示:</p><pre class="mt mu mv mw gt ot ou ov ow aw ox bi"><span id="3a4d" class="nj ln iq ou b gy oy oz l pa pb">theta = np.array([0.5, 0.5, 0.5])<br/>alpha = 0.01<br/>lambda_reg = 10</span><span id="c415" class="nj ln iq ou b gy po oz l pa pb">for _ in range(100):<br/>    prob = 1 / (1 + np.exp(-X @ theta))    <br/><strong class="ou ja">    reg_term = lambda_reg * theta<br/>    reg_term[0] = 0</strong><br/>    gradient = (y - prob) @ X <strong class="ou ja">- reg_term</strong><br/>    theta = theta + alpha * gradient</span></pre><p id="934a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当在 60 次迭代中监测该岭回归的收敛时，我们可以看到:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pp"><img src="../Images/da927b092069447ebc729ccbca05ca0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*2FGHHbAJb8bXXADSVn7Z6Q.gif"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Dotted line represents the respective values at λ = 0 (non-regularized logistic regression) for comparison</figcaption></figure><ul class=""><li id="de97" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">在λ = 10 时，我的两个主要特征(卡路里剩余量和步数，见左图)的θ收敛到更接近于零的值，即与非正则化版本(λ = 0)相比幅度更低。然而，截距θ的收敛基本不受影响。</li><li id="0dbb" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">岭回归的平均对数似然比非正则化版本收敛到更低的值(见中间面板)，这表明岭回归对我的训练数据提供了不太完美的拟合，但这也可能意味着它对我的训练数据的过度拟合更少。</li><li id="587e" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">岭回归的决策边界稍微偏离非正则化边界(见右图)。然而，它似乎仍然能够很好地区分我的训练数据点(红色和绿色)。</li></ul><p id="c170" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">随着λ的增加，学习回归系数(θ)进一步向零压缩，截距除外(见下图)。此外，从决策边界可以看出，岭回归在对我的训练数据进行分类时变得越来越不有效:λ = 1 时的决策边界非常接近非正则化边界，而在λ = 100 时，边界实际上不可用(见右图)。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nw"><img src="../Images/42f0e4220e94835f24c19d4d23543bc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3wluz2cdybMmyHmmNTl-FQ.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">How θ’s and decision boundary changes at different λ values</figcaption></figure><p id="a73d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，岭回归的目的不是改善对训练数据的拟合(因为如果是这样，它的表现将总是比非正则化版本差，如上所述)。更确切地说，它是为了<strong class="kq ja">改进对新数据</strong>的预测，即没有训练过岭回归的数据。</p><p id="b712" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了比较岭回归和它的非正则化对应物，我使用了<a class="ae ll" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation" rel="noopener ugc nofollow" target="_blank">双重交叉验证</a>，如下所示:</p><p id="867c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> 1。</span> <strong class="kq ja">将</strong>46 个数据点随机分成 2 等份:A &amp; B(各 23 个数据点)</p><p id="3413" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> 2。</span> <strong class="kq ja">在部分 A——训练集——上训练</strong>岭回归，并在训练集上记录回忆(正确预测体重增加的天数/部分 A 中真实体重增加的天数)</p><p id="cfca" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> 3。</span>使用在 A 部分训练的θ来<strong class="kq ja">预测</strong>我是否会在 B 部分(验证集)增加体重，并在验证集上记录回忆(正确预测体重增加的天数/B 部分中真实体重增加的天数)</p><p id="a6ab" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> 4。</span> <strong class="kq ja">交换零件，重复</strong>步骤 2 和 3，即零件 B 现在是训练集，零件 A 是验证集</p><p id="930a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> 5。</span> <strong class="kq ja">对两次试验中的训练集召回进行平均</strong>，验证集召回也是如此</p><h2 id="4e02" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated"><strong class="ak">为什么要用回忆？</strong></h2><p id="f2f4" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">有两个基本的度量标准来衡量分类器工作的好坏:精确度和召回率。在这方面:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ps"><img src="../Images/de655dbeda01671660d1fc4930b4df9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*18U3QlzHSOC3-QBtqRILwg.png"/></div></div></figure><ul class=""><li id="942c" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">就我个人而言，如果分类器预测我会增加体重，而事实证明我不会，我不会在意；事实上，这甚至会是一个受欢迎的惊喜！所以在预测的增重天数中，我并不是太在意<strong class="kq ja">假阳性</strong>。换句话说，精度对我来说没那么重要。</li><li id="d286" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">另一方面，我更可能纠结于分类器是否会标记出我真正增重的所有日子，以免它们被误报为<strong class="kq ja">假阴性</strong>(分类器预测我会减肥，而事实恰恰相反)。换句话说，我会尽可能提高召回率。</li></ul><p id="c6f1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">以下是不同正则化水平下的平均训练和验证召回，λ范围从 0(非正则化)到 10:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/a45ef2343b8808d40df2ff44ddc405da.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*fh1LV-rGcwsjlxxPI5jGPA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Average train &amp; validation set recall (across 2 folds) at different λ values</figcaption></figure><ul class=""><li id="94f9" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">从左图来看，随着λ从 0 开始增加，平均训练集召回率保持在恒定水平，并且仅在λ接近 10 时开始下降。这与早期的观察结果一致，即岭回归对训练数据的性能随着λ的增加而变差。</li><li id="3b93" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">另一方面，对于λ在 0.1 和 1 之间，平均验证集召回具有明显的隆起，这表明在这些λ处的岭回归在新的验证数据上比其非正则化对应物表现得更好，即使两者在训练数据上表现得一样好。</li><li id="49f2" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">通过进一步的检查，发现在一个验证组中，真实的体重增加日(红色带黑边，见下图)被非正则化回归错误地归类为体重减少:它停留在(实)决策边界之上。另一方面，当λ = 0.5(介于 0.1 和 1 之间)时，决策边界会略微扭曲。因此，该点保持在该(虚线)决策边界之下，并且被正确分类。这是岭回归在λ = 0.5 时比其非正则化对应物在验证集上具有更好召回率的唯一原因。</li></ul><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nw"><img src="../Images/61299ddd125f2ee6f36f44a7ed4d6281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*frjvNcStNQOhDHsETVZKxQ.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><strong class="bd ni">Left:</strong> Decision boundary on one validation set (with training set faded in background). <strong class="bd ni">Right:</strong> dotted region on left panel enlarged</figcaption></figure><p id="c5c7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">尽管这些结果表明我应该选择具有最高平均验证集召回率(λ = 0.5)的λ值，但验证集(23)中的少量数据点(其中甚至有更少数量的重量增加点(红色))表明这种性能的提高可能只是由于运气。这也解释了为什么对于一些λ，在相同的λ下，验证集的性能高于训练集的性能，即使通常发生相反的情况；我可以有一个“幸运的”验证集。</p><p id="e3c1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">也就是说，选择λ = 0.5 没有害处，因为当在我的 46 个点的整个数据上训练时，它的决策边界实际上与λ = 0 的决策边界没有什么区别，正如前面不同λ(从 0 到 100)的决策边界图所示。因此，对于我的最终模型，我选择保持λ = 0.5。</p><h2 id="251e" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">选择决策边界的阈值</h2><p id="0d49" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">在调整我的模型时，选择λ并不是最有影响的决定。相反，选择我的决策边界的<strong class="kq ja">阈值是:</strong></p><ul class=""><li id="3983" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">对于我到目前为止建立的所有回归模型，分类阈值，也就是决策边界，被设置为 50%(或 0.5)。这是一个合理的阈值，因为预测体重增加概率高于 50%的日子自然应被归类为体重增加的日子，反之亦然。</li><li id="ffc9" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">然而，当这个体重增加阈值降低时，越来越多的数据点将被归类为体重增加(正确与否)。回忆，也被称为<a class="ae ll" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Definitions" rel="noopener ugc nofollow" target="_blank">真阳性率</a>，当然会增加，但是假阳性率——在真正体重减轻的日子里被错误地预测为体重增加的日子——会同步增加(见左图，以及下面中间图中的 ROC 曲线)。然而，如前所述，由于我不太担心假阳性，我可以容忍许多体重减轻日被错误地归类为体重增加，即高假阳性率，如果这意味着我的真实体重增加日被更好地检测，即高真阳性率/回忆。</li></ul><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nw"><img src="../Images/fc21b18d08cd1df1b07344102aa41426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tg9KwgXE2ITdHo0nBwsNuQ.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><strong class="bd ni">Left:</strong> True/False Positive Rate at different classification threshold. <strong class="bd ni">Middle:</strong> ROC curve. <strong class="bd ni">Right:</strong> decision boundary at different thresholds</figcaption></figure><p id="66b4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果是这样的话，门槛是不是应该定得越低越好，甚至到 0%？不，因为:</p><blockquote class="pu"><p id="b1ad" class="pv pw iq bd px py pz qa qb qc qd lj dk translated">当阈值降低时，决策边界上移。</p></blockquote><ul class=""><li id="202d" class="nx ny iq kq b kr qe ku qf kx qg lb qh lf qi lj oc od oe of bi translated">例如，为了使回忆增加到最近的较高水平，阈值必须从 50%减少到 44%(在左侧面板中从黑色减少到棕色)。结果，决策边界上移，以捕捉更多的体重增加点(右侧面板中带有棕色边框的红点)。这相当于向左移动了 107 千卡。换句话说，如果我以前在 50%的界限，我必须在相同的步数下少吃 107 千卡，才能保持在 44%界限的良好减肥侧。</li><li id="3235" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">在阈值接近 0%的极端情况下(左侧面板中的橙色点)，边界向上移动，以便捕捉所有体重增加点，包括带有橙色边框的最高红色点(右侧面板)。这个决策界限决定了在我懒惰的一天，我平均走了将近 2500 步，我应该比我的预算少吃 1740 千卡来保持健康。鉴于我的数据中的平均预算约为 1715 千卡，这转化为在那些懒惰的日子里的上限<strong class="kq ja"> -25 卡路里</strong>(是的，你没看错)。当然，在这个物理极限下，我的回忆会是惊人的，但是我会死的！</li><li id="3deb" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">那么我到底该不该降低自己的门槛呢？鉴于从我的数据中学到的 50%决策界限已经相当保守了——在懒惰的日子里，这表明我应该比通常的 1700+千卡预算少吃大约 140 千卡——我决定坚持默认的 50%阈值。将这个阈值降低到下一个可能的水平 44%会让我在回忆中多得几分，但额外的 107 千卡限制不值得我失去理智。</li></ul><h1 id="1290" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">模型解释</h1><p id="1281" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">概括地说，我们通过梯度上升算法训练了一个逻辑回归分类器，使用我的每日卡路里盈余和步数作为特征，我的体重增加状态作为标签。我们模型的参数是:</p><ul class=""><li id="17ce" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated"><strong class="kq ja">学习率:</strong> α = 0.01</li><li id="e05c" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><strong class="kq ja">正则化参数:</strong> λ = 0.5</li><li id="45c2" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated"><strong class="kq ja">阈值:</strong> 50%</li></ul><p id="ed44" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在整个数据集上训练分类器后，学习的回归系数(θ)得出:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/5fd0a96e7a991814520b235be6296e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*M-JJjLP2J1lSiW8N0MUzzg.png"/></div></figure><h2 id="0bf9" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">优势比分析</h2><ul class=""><li id="b3da" class="nx ny iq kq b kr me ku mf kx qk lb ql lf qm lj oc od oe of bi translated">逻辑回归系数的一个常见解释是通过<a class="ae ll" href="https://en.wikipedia.org/wiki/Odds_ratio#Role_in_logistic_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">优势比</strong> </a>:特征的一个单位变化，结果变化的几率是多少倍。对于一个特征 j，那个特征的比值比正好是它的θ的指数。</li></ul><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/b3f7df76a9657cf722d44b462d05b297.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*jAONATF9vxwKjQyUjMhrmA.png"/></div></figure><ul class=""><li id="defb" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">回想一下，逻辑回归是对卡路里盈余的标准化值进行的(通过减去盈余平均值并除以盈余标准偏差得到)。因此，θ_surplus = 0.9 意味着在任何步数下，剩余卡路里的标准差减少(约 420 千卡)对应于 e^0.9，或者我体重增加的几率减少 2.5 倍。</li><li id="eab6" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">另一方面，在θ_step = -1.2 的情况下，我步数的标准差增加——大约 5980 步——对应于 e^1.2，或者我体重增加的几率减少 3.3 倍(在任何卡路里剩余量的情况下)。在我最近的 10 次锻炼中，我的平均步频是 1366 步/公里。因此，这 5890 步相当于大约 4.4 公里。</li><li id="42f8" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">换句话说:</li></ul><blockquote class="pu"><p id="6e9b" class="pv pw iq bd px py qo qp qq qr qs lj dk translated">步数的标准差增加(5980 步)比卡路里消耗量的标准差减少(420 千卡)更能有效减少我体重增加的几率。</p></blockquote><p id="38f1" class="pw-post-body-paragraph ko kp iq kq b kr qe ka kt ku qf kd kw kx qt kz la lb qu ld le lf qv lh li lj ij bi translated">当然，我愿意跑 4.4 公里而不是不吃那碗 420 千卡的河粉完全是另一回事！</p><p id="28f9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">尽管比值比为我应该如何制定减肥策略提供了一些见解，但从我的逻辑回归分类器的决策边界中可以得出一个更可行的计划。</p><h2 id="e440" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">决策边界分析</h2><p id="2c01" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">回想一下我们之前可靠的 sigmoid 函数:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi qw"><img src="../Images/58b73d144a149c3f43bde0a13353fda5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JFZAM029EUSHS_phH8KmfQ.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Superscript (i) removed for simplicity</figcaption></figure><p id="cda9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">很容易看出，体重减轻的概率为 50%(其中分类阈值为)，θ和 x 的线性组合必须为零*(我还用 1 代替了 x_intercept):</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi qx"><img src="../Images/41b292cd2d0d14c592d46f6346a4ad3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Q-oflOOix9abNxcRIgj_w.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">50% decision boundary formula for normalized features</figcaption></figure><p id="88f4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">*对于其他概率阈值，θ和 x 的线性组合可以通过取阈值的<a class="ae ll" href="https://en.wikipedia.org/wiki/Logistic_regression#Logistic_function,_odds,_odds_ratio,_and_logit" rel="noopener ugc nofollow" target="_blank"> logit </a>得到:ln(p/(1-p))</p><p id="4f97" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，在我们的逻辑回归中使用的 x 是我们原始特征的标准化值。因此，我们可以将上面的等式改写为:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi qy"><img src="../Images/89f4c665f89054f5279c953fb80c222c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rV3TZPjwo1LhuIL5RyP28w.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><strong class="bd ni">*</strong> denotes the original feature value. <strong class="bd ni">μ:</strong> feature mean, and <strong class="bd ni">σ:</strong> feature standard deviation</figcaption></figure><p id="7d3c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">重新排列，我们有:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi qz"><img src="../Images/9de26f4c18552c79ad546154668152c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SS6m2b6d5vO4KNurq1jmMw.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">50% decision boundary formula for original features</figcaption></figure><p id="b8da" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">将θ和特征均值(μ)和标准偏差(σ)代入上述方程，得到原始卡路里剩余量和步数特征之间的线性方程:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ra"><img src="../Images/795850a7a7bb57648cd660487f43bbb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6AhF7AsBgz3xWaM0PuOuXA.png"/></div></div></figure><p id="e6f9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从图形上看，这个方程代表了原始剩余步骤图中的决策边界(见下面的左图)。根据这个决策界限，我应该记住 3 个重要数字(见右图):</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi rb"><img src="../Images/a363ebc3e6d1ad0b98344d6574ebc376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzvAigVFzC7HJgJuk2qfvw.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><strong class="bd ni">Left:</strong> surplus-step plot with decision boundary. <strong class="bd ni">Right: </strong>dotted region from left panel enlarged</figcaption></figure><h2 id="fc31" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">1) -140 千卡</h2><p id="8351" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">这是我在懒惰的日子里(平均 2480 步)应该吃的低于 Loseit 应用程序卡路里预算 <strong class="kq ja">的量<strong class="kq ja">。平均预算刚刚超过 1700 千卡，这意味着在那些日子里，我应该平均摄入低于 1560 千卡的热量。这个数字听起来确实很严格。</strong></strong></p><h2 id="8372" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">130 千卡</h2><p id="64eb" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">然而，一个可取之处是，根据决策边界，<strong class="kq ja">对于我超出正常活动范围的任何 1 公里，我都可以将这个限制增加 130 千卡</strong>。例如，如果我计划在某一天跑 5000 米，我可以负担得起比应用程序当天的卡路里预算多吃 140 + 130 * 5 = 510 千卡。希望这能鼓励我坚持我的跑步计划。</p><h2 id="bbe7" class="nj ln iq bd lo nk nl dn ls nm nn dp lw kx no np ly lb nq nr ma lf ns nt mc iw bi translated">3) 1070 步</h2><p id="1417" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">另一方面，当我试图吃得超过卡路里限制时(如前两条规则所规定的)，我吃的任何超过限制的 100 千卡必须通过至少 1070 步来获得。这可以通过以下方式实现:</p><ul class=""><li id="7b6b" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">绕着街区走 1070 步，然后去吃那家<a class="ae ll" href="https://en.wikipedia.org/wiki/B%C3%A1nh_bao" rel="noopener ugc nofollow" target="_blank"> banh bao </a>，或者</li><li id="3d02" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">意识到我太懒了，把食物从我愚蠢的嘴里拿出来(嘿，我刚刚做了我自己的<a class="ae ll" href="https://www.youtube.com/watch?v=CfsAi95ghuU" rel="noopener ugc nofollow" target="_blank">掌掴厨师</a>！)</li></ul><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi rc"><img src="../Images/7886b4cdf31050e8195387cb04b7df96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*zUrPRtC1YsC3hqMxspWVOQ.gif"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Hit me with that number baby!</figcaption></figure><h1 id="a2d9" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">结论</h1><p id="8e0f" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">我希望用以上的指导方针，让我 2019 年的减肥之旅比 2018 年更成功。当然，即使有数字支持，我也不会总是成功，但我从这个项目中学到的最重要的一课是:</p><blockquote class="pu"><p id="db6b" class="pv pw iq bd px py pz qa qb qc qd lj dk translated">我应该善待自己。</p></blockquote><p id="7b9d" class="pw-post-body-paragraph ko kp iq kq b kr qe ka kt ku qf kd kw kx qt kz la lb qu ld le lf qv lh li lj ij bi translated">例如，我<em class="lk">减肥，即使我在活跃的日子里吃得比预算多，所以我不应该为这样做感到内疚。希望有了这种结合饮食和锻炼的新模式，在我持续的减肥旅程中，我可以感觉到更少的罪恶感和羞耻感(就像我以前一样)。</em></p><h1 id="fc64" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">其他课程</h1><p id="ece2" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">我着手这个项目的另一个重要原因是实现一个机器项目，而不使用预先存在的库，如 scikit-learn。以下是我从这个过程中学到的一些经验:</p><ul class=""><li id="7d83" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">一年多前，当我第一次学习 Python(或者任何严肃的编程)时，我不确定为什么会有人使用类。嗯，在这个项目中(a)使用全局变量<code class="fe pc pd pe ou b">theta</code> , (b)得到一些奇怪的结果，比如不收敛，以及(c)意识到所述的<code class="fe pc pd pe ou b">theta</code>属于我许多个月前运行的其他模型，我现在意识到为什么类中的封装是如此重要:一个模型对象可以有它自己的属性(<code class="fe pc pd pe ou b">theta</code>、<code class="fe pc pd pe ou b">alpha</code>、<code class="fe pc pd pe ou b">lambda_reg</code>)和方法(<code class="fe pc pd pe ou b">fit</code>、<code class="fe pc pd pe ou b">predict</code>)，这些属性和方法不会与其他模型对象的属性和方法冲突，我可以在下次需要它的时候愉快地把它完整地捡起来。</li></ul><blockquote class="ph pi pj"><p id="b2b4" class="ko kp lk kq b kr ks ka kt ku kv kd kw pk ky kz la pl lc ld le pm lg lh li lj ij bi translated">我真的对如何使用类来建模数据科学问题很感兴趣，并且我认为通过更多相关的例子，我可能能够更好地理解面向对象编程的强大功能(现在我仍然不太确定我应该何时或者如何使用它们)。</p><p id="b8b0" class="ko kp lk kq b kr ks ka kt ku kv kd kw pk ky kz la pl lc ld le pm lg lh li lj ij bi translated">—初学 Python 时的我</p></blockquote><ul class=""><li id="7e71" class="nx ny iq kq b kr ks ku kv kx nz lb oa lf ob lj oc od oe of bi translated">这个项目还允许我实现和理解一些我从未有机会使用的简洁编程概念的实际好处，例如使用<a class="ae ll" href="https://wiki.python.org/moin/Generators" rel="noopener ugc nofollow" target="_blank">生成器</a>即<code class="fe pc pd pe ou b">yield</code>语句一次返回一个训练和验证文件夹。这允许我仅在这些折叠上评估我的模型，然后返回并生成更多的折叠，而不是一次将所有折叠存储在内存中(尽管对于我的数据大小来说，这几乎没有区别)。甚至像编写清晰的伪代码这样的事情也被证明是非常重要的，尤其是在实现像逻辑回归这样的数学算法时；令人尴尬的是，我不得不承认我花了很长时间进行调试，因为我没有事先编写明确的伪代码，并且错误地交换了<code class="fe pc pd pe ou b">alpha</code>和<code class="fe pc pd pe ou b">theta</code>。</li><li id="f670" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">最后，这个项目帮助我更详细地研究了我经常使用的一些机器学习库的实现，例如偶尔警告我在训练 scikit-learn 模型时必须指定<code class="fe pc pd pe ou b">max_iter</code>或<code class="fe pc pd pe ou b">tol</code>:前者指定让模型收敛的迭代次数，后者指定迭代停止的容差水平——这正是我在为自己的模型检查收敛性时面临的两个选择。</li><li id="d355" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">另一个例子是，我现在了解到一些 scikit-learn 模型的行为不像我期望的那样:<code class="fe pc pd pe ou b">SGDClassifier(loss=’log’, penalty=’l2', learning_rate=’constant’)</code>似乎没有缩小截距的θ，并给了我的模型类似的θ，而<code class="fe pc pd pe ou b">LogisticRegression(penalty=’l2')</code>在默认设置下缩小截距的θ，除非有人篡改了<code class="fe pc pd pe ou b">intercept_scaling</code>参数。因此，我在将来使用第三方库分析我的数据时会更加小心，并在必要时验证结果。</li></ul><h1 id="1ba5" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">更多资源</h1><ul class=""><li id="8ca7" class="nx ny iq kq b kr me ku mf kx qk lb ql lf qm lj oc od oe of bi translated">我从<a class="ae ll" href="https://see.stanford.edu/Course/CS229/" rel="noopener ugc nofollow" target="_blank"> CS229 </a>的<a class="ae ll" href="https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf" rel="noopener ugc nofollow" target="_blank">讲义</a>和<a class="ae ll" href="https://see.stanford.edu/Course/CS229/42" rel="noopener ugc nofollow" target="_blank">视频</a> <a class="ae ll" href="https://see.stanford.edu/Course/CS229/49" rel="noopener ugc nofollow" target="_blank">讲座</a>(我极力推荐的吴恩达教授讲授的机器学习课程)中获取了逻辑回归及其梯度上升法的大部分推导。注释提供了随机梯度上升的公式，但是批量版本可以很容易地修改(如前面的数学回顾部分所示)。</li><li id="0735" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">一个更容易理解的解释可以在他在 Coursera 上的第三周机器课程中找到。本课程也涵盖岭回归在逻辑回归上的应用，这是 CS229 课程所没有的。一个小细节:Coursera 课程将逻辑回归称为<em class="lk">使用随机梯度<em class="lk">下降</em>最小化</em>对数损失。然而，这与我实现的使用随机梯度<em class="lk">上升</em>最大化对数似然性是一样的，对数损失只不过是对数似然性的负值(有一些小的修改)。</li><li id="2c9e" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">对逻辑回归和岭回归的另一个很好的解释来自华盛顿大学<a class="ae ll" href="https://www.coursera.org/learn/ml-classification/" rel="noopener ugc nofollow" target="_blank"> Coursera </a>关于分类方法的课程。幸运的是，本课程使用对数似然最大化来解释逻辑回归，所以它应该与 CS229 和我的符号一致。</li><li id="4ce4" class="nx ny iq kq b kr og ku oh kx oi lb oj lf ok lj oc od oe of bi translated">最后但并非最不重要的是，我在网上找到的两个减肥分析项目真正启发了我自己的项目:一个来自<a class="ae ll" rel="noopener" target="_blank" href="/data-science-a-practical-application-7056ec22d004">威尔·科尔森</a>，他只使用他过去的体重来预测未来的减肥，另一个来自<a class="ae ll" href="https://github.com/arielf/weight-loss" rel="noopener ugc nofollow" target="_blank">阿里尔·费根</a>，他使用几十个因素来预测他每天增加或减少多少体重，并得出非常有趣的结果:显然睡眠是他减肥的最重要因素！我的方法介于两者之间，仅使用两个特征来预测我是否会减肥，这使得我可以在我的减肥旅程中带走简单的可视化和简单可行的见解</li></ul><p id="f84a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我希望我的项目可以启发其他人使用机器学习和数据科学来帮助他们更多地了解自己，完成减肥等个人目标。如果您有任何问题或反馈，请不要犹豫通过媒体联系我！</p></div></div>    
</body>
</html>