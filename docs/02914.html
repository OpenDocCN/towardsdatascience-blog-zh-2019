<html>
<head>
<title>GANs vs. Autoencoders: Comparison of Deep Generative Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GANs 与自动编码器:深度生成模型的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gans-vs-autoencoders-comparison-of-deep-generative-models-985cf15936ea?source=collection_archive---------0-----------------------#2019-05-12">https://towardsdatascience.com/gans-vs-autoencoders-comparison-of-deep-generative-models-985cf15936ea?source=collection_archive---------0-----------------------#2019-05-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fa2c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">想把马变成斑马？制作 DIY 动漫人物或名人？生成敌对网络是你新的最好的朋友。</h2></div><p id="b714" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">生成对抗网络是过去 10 年机器学习中最有趣的想法——<strong class="kh ir">扬·勒村，脸书人工智能研究中心主任</strong></p><p id="1893" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本教程的第 1 部分可以在这里找到:</p><div class="lc ld gp gr le lf"><a rel="noopener follow" target="_blank" href="/comprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">图灵学习和 GANs 简介</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">想把马变成斑马？制作 DIY 动漫人物或名人？生成敌对网络是…</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">towardsdatascience.com</p></div></div><div class="lo l"><div class="lp l lq lr ls lo lt lu lf"/></div></div></a></div><p id="c18d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本教程的第 2 部分可以在这里找到:</p><div class="lc ld gp gr le lf"><a rel="noopener follow" target="_blank" href="/comprehensive-introduction-to-turing-learning-and-gans-part-2-fd8e4a70775"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">GANs 中的高级主题</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">想把马变成斑马？制作 DIY 动漫人物或名人？生成敌对网络是…</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">towardsdatascience.com</p></div></div><div class="lo l"><div class="lv l lq lr ls lo lt lu lf"/></div></div></a></div><p id="70ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些文章基于哈佛大学关于<a class="ae lw" href="https://harvard-iacs.github.io/2019-CS109B/" rel="noopener ugc nofollow" target="_blank"> AC209b </a>的讲座，主要归功于哈佛大学 IACS 系的讲师<a class="ae lw" href="https://iacs.seas.harvard.edu/people/pavlos-protopapas" rel="noopener ugc nofollow" target="_blank"> Pavlos Protopapas </a>。</p><p id="9490" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是专门使用生成性对抗网络创建深度生成模型的三部分教程的第三部分。这是上一个关于变型自动编码器主题的自然延伸(在这里找到<a class="ae lw" rel="noopener" target="_blank" href="/generating-images-with-autoencoders-77fd3a8dd368"/>)。我们将看到，与可变自动编码器相比，GANs 作为深度生成模型通常更优越。然而，众所周知，它们很难使用，并且需要大量的数据和调整。我们还将研究一种称为 VAE-GAN 的混合 GAN 模型。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/d56e801da7bc4f16a949aee0e9631ebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7kpjaMU631RD-CTKG6DyZw.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Taxonomy of deep generative models. This article’s focus is on GANs.</figcaption></figure><p id="20ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">教程的这一部分将主要是变分自动编码器(VAEs)的编码实现，也将向读者展示如何制作 VAEs 甘。</p><ul class=""><li id="8134" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated">CelebA 数据集的 VAE</li><li id="ffd3" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">西里巴数据集的 DC-甘</li><li id="be70" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">动漫数据集 DC-甘</li><li id="c7f0" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">动漫数据集 VAE-甘</li></ul><p id="1482" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我强烈建议读者在进一步阅读之前，至少先阅读一下 GAN 教程的第 1 部分，以及我的自动编码器变化演练，否则，读者可能对实现没有太多的了解。</p><p id="28e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有相关代码现在都可以在我的 GitHub 存储库中找到:</p><div class="lc ld gp gr le lf"><a href="https://github.com/mrdragonbear/GAN-Tutorial" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">龙熊先生/甘-教程</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码、管理项目和构建…</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">github.com</p></div></div><div class="lo l"><div class="na l lq lr ls lo lt lu lf"/></div></div></a></div><p id="7edb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们开始吧！</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="a22e" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">CelebA 数据集的 VAE</strong></h1><p id="2d97" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">CelebFaces 属性数据集(CelebA)是一个大规模的人脸属性数据集，拥有超过 20 万张名人图像，每张图像都有 40 个属性注释。该数据集中的图像覆盖了较大的姿态变化和背景混乱。西里巴有很大的多样性，数量大，注释丰富，包括</p><ul class=""><li id="ca23" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated">10177 个身份，</li><li id="be4a" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">202，599 个面部图像，以及</li><li id="fbe7" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">5 个标志位置，每幅图像 40 个二元属性注释。</li></ul><p id="4c38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以从 Kaggle 这里下载数据集:</p><div class="lc ld gp gr le lf"><a href="https://www.kaggle.com/jessicali9530/celeba-dataset" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">名人面孔属性(CelebA)数据集</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">超过 200，000 张名人图片，带有 40 个二元属性注释</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">www.kaggle.com</p></div></div><div class="lo l"><div class="of l lq lr ls lo lt lu lf"/></div></div></a></div><p id="f513" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一步是导入所有必要的函数并提取数据。</p><p id="7bfd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">进口</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="b22f" class="ol nj iq oh b gy om on l oo op">import shutil<br/>import errno<br/>import zipfile<br/>import os<br/>import matplotlib.pyplot as plt</span></pre><p id="b297" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">提取数据</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="aa0c" class="ol nj iq oh b gy om on l oo op"># Only run once to unzip images<br/>zip_ref = zipfile.ZipFile('img_align_celeba.zip','r')<br/>zip_ref.extractall()<br/>zip_ref.close()</span></pre><p id="3cea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">自定义图像生成器</strong></p><p id="7219" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一步可能是大多数读者以前没有用过的。由于我们的数据非常庞大，可能无法将数据集加载到 Jupyter 笔记本的内存中。在处理大型数据集时，这是一个很正常的问题。</p><p id="13fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解决这一问题的方法是使用流生成器，它将成批的数据(本例中是图像)按顺序流入内存，从而限制该函数所需的内存量。需要注意的是，理解和编写它们有点复杂，因为它们需要对计算机内存、GPU 架构等有合理的理解。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="1930" class="ol nj iq oh b gy om on l oo op"># data generator<br/># source from https://medium.com/@ensembledme/writing-custom-keras-generators-fe815d992c5a<br/>from skimage.io import imread<br/><br/>def get_input(path):<br/>    """get specific image from path"""<br/>    img = imread(path)<br/>    return img<br/><br/>def get_output(path, label_file = None):<br/>    """get all the labels relative to the image of path"""<br/>    img_id = path.split('/')[-1]<br/>    labels = label_file.loc[img_id].values<br/>    return labels<br/><br/>def preprocess_input(img):<br/>    # convert between 0 and 1<br/>    return img.astype('float32') / 127.5 -1<br/><br/>def image_generator(files, label_file, batch_size = 32):<br/>    while True:<br/><br/>        batch_paths = np.random.choice(a = files, size = batch_size)<br/>        batch_input = []<br/>        batch_output = []<br/><br/>        for input_path in batch_paths:<br/><br/>            input = get_input(input_path)<br/>            input = preprocess_input(input)<br/>            output = get_output(input_path, label_file = label_file)<br/>            batch_input += [input]<br/>            batch_output += [output]<br/>        batch_x = np.array(batch_input)<br/>        batch_y = np.array(batch_output)<br/><br/>        yield batch_x, batch_y<br/><br/>def auto_encoder_generator(files, batch_size = 32):<br/>    while True:<br/>        batch_paths = np.random.choice(a = files, size = batch_size)<br/>        batch_input = []<br/>        batch_output = []<br/><br/>        for input_path in batch_paths:<br/>            input = get_input(input_path)<br/>            input = preprocess_input(input)<br/>            output = input<br/>            batch_input += [input]<br/>            batch_output += [output]<br/>        batch_x = np.array(batch_input)<br/>        batch_y = np.array(batch_output)<br/><br/>        yield batch_x, batch_y</span></pre><p id="1d5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于用 Keras 编写定制生成器的更多信息，我在上面的代码中引用了一篇很好的文章:</p><div class="lc ld gp gr le lf"><a href="https://medium.com/@ensembledme/writing-custom-keras-generators-fe815d992c5a" rel="noopener follow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">编写定制的 Keras 生成器</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">使用 Keras 生成器背后的想法是在训练期间动态地获得成批的输入和相应的输出…</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">medium.com</p></div></div></div></a></div><p id="c062" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">加载属性数据</strong></p><p id="e185" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们不仅有这个数据集的图像，而且每个图像还有一个与名人的各个方面相对应的属性列表。例如，有描述名人是否涂口红或戴帽子、他们是否年轻、他们是否有黑头发等的属性。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="ea36" class="ol nj iq oh b gy om on l oo op"># now load attribute<br/><br/># 1.A.2<br/>import pandas as pd<br/>attr = pd.read_csv('list_attr_celeba.csv')<br/>attr = attr.set_index('image_id')<br/><br/># check if attribute successful loaded<br/>attr.describe()</span></pre><p id="bbe5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">完成发电机的制作</strong></p><p id="de7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们完成了发电机的制作。我们将图像名称长度设置为 6，因为我们的数据集中有 6 位数的图像。阅读定制 Keras 生成器文章后，这部分代码应该有意义。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="a96b" class="ol nj iq oh b gy om on l oo op">import numpy as np<br/>from sklearn.model_selection import train_test_split</span><span id="2be5" class="ol nj iq oh b gy oq on l oo op">IMG_NAME_LENGTH = 6</span><span id="461a" class="ol nj iq oh b gy oq on l oo op">file_path = "img_align_celeba/"<br/>img_id = np.arange(1,len(attr.index)+1)<br/>img_path = []<br/>for i in range(len(img_id)):<br/>    img_path.append(file_path + (IMG_NAME_LENGTH - len(str(img_id[i])))*'0' + str(img_id[i]) + '.jpg')</span><span id="e029" class="ol nj iq oh b gy oq on l oo op"># pick 80% as training set and 20% as validation set<br/>train_path = img_path[:int((0.8)*len(img_path))]<br/>val_path = img_path[int((0.8)*len(img_path)):]</span><span id="b903" class="ol nj iq oh b gy oq on l oo op">train_generator = auto_encoder_generator(train_path,32)<br/>val_generator = auto_encoder_generator(val_path,32)</span></pre><p id="88fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在可以选择三个图像，并检查属性是否有意义。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="3b25" class="ol nj iq oh b gy om on l oo op">fig, ax = plt.subplots(1, 3, figsize=(12, 4))<br/>for i in range(3):    <br/>    ax[i].imshow(get_input(img_path[i]))<br/>    ax[i].axis('off')<br/>    ax[i].set_title(img_path[i][-10:])<br/>plt.show()<br/>    <br/>attr.iloc[:3]</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi or"><img src="../Images/0ff784df069b971d1ee23e65aff9b6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SgO5O96CtDc5t3yjDqEPMw.png"/></div></div></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi os"><img src="../Images/ae2752fa4f82046cf95dd17c2cce0c2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UFRaWY7cuyF-yXpuwkO2Iw.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Three random images along with some of their attributes.</figcaption></figure><h2 id="fe07" class="ol nj iq bd nk ot ou dn no ov ow dp ns ko ox oy nu ks oz pa nw kw pb pc ny pd bi translated">建立和训练一个 VAE 模型</h2><p id="e705" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">首先，我们将为名人面孔数据集创建并编译一个卷积 VAE 模型(包括编码器和解码器)。</p><p id="83d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">更多进口商品</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="bc50" class="ol nj iq oh b gy om on l oo op">from keras.models import Sequential, Model<br/>from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D, Input, Reshape, UpSampling2D, InputLayer, Lambda, ZeroPadding2D, Cropping2D, Conv2DTranspose, BatchNormalization<br/>from keras.utils import np_utils, to_categorical<br/>from keras.losses import binary_crossentropy<br/>from keras import backend as K,objectives<br/>from keras.losses import mse, binary_crossentropy</span></pre><p id="4b10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">模型架构</strong></p><p id="6678" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们可以创建并总结该模型。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="2fb8" class="ol nj iq oh b gy om on l oo op">b_size = 128<br/>n_size = 512<br/>def sampling(args):<br/>    z_mean, z_log_sigma = args<br/>    epsilon = K.random_normal(shape = (n_size,) , mean = 0, stddev = 1)<br/>    return z_mean + K.exp(z_log_sigma/2) * epsilon<br/>  <br/>def build_conv_vae(input_shape, bottleneck_size, sampling, batch_size = 32):<br/>    <br/>    # ENCODER<br/>    input = Input(shape=(input_shape[0],input_shape[1],input_shape[2]))<br/>    x = Conv2D(32,(3,3),activation = 'relu', padding = 'same')(input)    <br/>    x = BatchNormalization()(x)<br/>    x = MaxPooling2D((2,2), padding ='same')(x)<br/>    x = Conv2D(64,(3,3),activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = MaxPooling2D((2,2), padding ='same')(x)<br/>    x = Conv2D(128,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = MaxPooling2D((2,2), padding ='same')(x)<br/>    x = Conv2D(256,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = MaxPooling2D((2,2), padding ='same')(x)<br/>    <br/>    # Latent Variable Calculation<br/>    shape = K.int_shape(x)<br/>    flatten_1 = Flatten()(x)<br/>    dense_1 = Dense(bottleneck_size, name='z_mean')(flatten_1)<br/>    z_mean = BatchNormalization()(dense_1)<br/>    flatten_2 = Flatten()(x)<br/>    dense_2 = Dense(bottleneck_size, name ='z_log_sigma')(flatten_2)<br/>    z_log_sigma = BatchNormalization()(dense_2)<br/>    z = Lambda(sampling)([z_mean, z_log_sigma])<br/>    encoder = Model(input, [z_mean, z_log_sigma, z], name = 'encoder')<br/>    <br/>    # DECODER<br/>    latent_input = Input(shape=(bottleneck_size,), name = 'decoder_input')<br/>    x = Dense(shape[1]*shape[2]*shape[3])(latent_input)<br/>    x = Reshape((shape[1],shape[2],shape[3]))(x)<br/>    x = UpSampling2D((2,2))(x)<br/>    x = Cropping2D([[0,0],[0,1]])(x)<br/>    x = Conv2DTranspose(256,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = UpSampling2D((2,2))(x)<br/>    x = Cropping2D([[0,1],[0,1]])(x)<br/>    x = Conv2DTranspose(128,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = UpSampling2D((2,2))(x)<br/>    x = Cropping2D([[0,1],[0,1]])(x)<br/>    x = Conv2DTranspose(64,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = UpSampling2D((2,2))(x)<br/>    x = Conv2DTranspose(32,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    output = Conv2DTranspose(3,(3,3), activation = 'tanh', padding ='same')(x)<br/>    decoder = Model(latent_input, output, name = 'decoder')<br/><br/>    output_2 = decoder(encoder(input)[2])<br/>    vae = Model(input, output_2, name ='vae')<br/>    return vae, encoder, decoder, z_mean, z_log_sigma<br/><br/>vae_2, encoder, decoder, z_mean, z_log_sigma = build_conv_vae(img_sample.shape, n_size, sampling, batch_size = b_size)<br/>print("encoder summary:")<br/>encoder.summary()<br/>print("decoder summary:")<br/>decoder.summary()<br/>print("vae summary:")<br/>vae_2.summary()</span></pre><p id="d1de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">定义 VAE 损失</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="92c0" class="ol nj iq oh b gy om on l oo op">def vae_loss(input_img, output):<br/>    # Compute error in reconstruction<br/>    reconstruction_loss = mse(K.flatten(input_img) , K.flatten(output))<br/>    <br/>    # Compute the KL Divergence regularization term<br/>    kl_loss = - 0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis = -1)<br/>    <br/>    # Return the average loss over all images in batch<br/>    total_loss = (reconstruction_loss + 0.0001 * kl_loss)    <br/>    return total_loss</span></pre><p id="bb47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">编译模型</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="1759" class="ol nj iq oh b gy om on l oo op">vae_2.compile(optimizer='rmsprop', loss= vae_loss)<br/>encoder.compile(optimizer = 'rmsprop', loss = vae_loss)<br/>decoder.compile(optimizer = 'rmsprop', loss = vae_loss)</span></pre><p id="040d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">训练模型</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="14ad" class="ol nj iq oh b gy om on l oo op">vae_2.fit_generator(train_generator, steps_per_epoch = 4000, validation_data = val_generator, epochs=7, validation_steps= 500)</span></pre><p id="197b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们随机选择训练集的一些图像，通过编码器运行它们以参数化潜在代码，然后用解码器重建图像。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="e7cd" class="ol nj iq oh b gy om on l oo op">import random<br/>x_test = []<br/>for i in range(64):<br/>    x_test.append(get_input(img_path[random.randint(0,len(img_id))]))<br/>x_test = np.array(x_test)<br/>figure_Decoded = vae_2.predict(x_test.astype('float32')/127.5 -1, batch_size = b_size)<br/>figure_original = x_test[0]<br/>figure_decoded = (figure_Decoded[0]+1)/2<br/>for i in range(4):<br/>    plt.axis('off')<br/>    plt.subplot(2,4,1+i*2)<br/>    plt.imshow(x_test[i])<br/>    plt.axis('off')<br/>    plt.subplot(2,4,2 + i*2)<br/>    plt.imshow((figure_Decoded[i]+1)/2)<br/>    plt.axis('off')<br/>plt.show()</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pe"><img src="../Images/ca91e0117abe9411a4f86b1c5fc1149d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iCYWlSr4t5OeddcDX4XYjg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Random samples from training set compared to their VAE reconstruction.</figcaption></figure><p id="26d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，重建的图像与原始版本有相似之处。然而，新的图像有点模糊，这是一个已知的 VAEs 现象。这被假设是由于变分推理优化了似然性的下限，而不是实际的似然性本身。</p><p id="4913" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">潜在空间表征</strong></p><p id="e399" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以选择两个不同属性的图像，并绘制它们的潜在空间表示。请注意，我们可以看到潜在代码之间的一些差异，我们可以假设解释原始图像之间的差异。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="7bf8" class="ol nj iq oh b gy om on l oo op"># Choose two images of different attributes, and plot the original and latent space of it<br/><br/>x_test1 = []<br/>for i in range(64):<br/>    x_test1.append(get_input(img_path[np.random.randint(0,len(img_id))]))<br/>x_test1 = np.array(x_test)<br/>x_test_encoded = np.array(encoder.predict(x_test1/127.5-1, batch_size = b_size))<br/>figure_original_1 = x_test[0]<br/>figure_original_2 = x_test[1]<br/>Encoded1 = (x_test_encoded[0,0,:].reshape(32, 16,)+1)/2 <br/>Encoded2 = (x_test_encoded[0,1,:].reshape(32, 16)+1)/2<br/><br/>plt.figure(figsize=(8, 8))<br/>plt.subplot(2,2,1)<br/>plt.imshow(figure_original_1)<br/>plt.subplot(2,2,2)<br/>plt.imshow(Encoded1)<br/>plt.subplot(2,2,3)<br/>plt.imshow(figure_original_2)<br/>plt.subplot(2,2,4)<br/>plt.imshow(Encoded2)<br/>plt.show()</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pf"><img src="../Images/b674b3cf02713a90011374658f302925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eUiTW9oV9R9-Alt9P0-0A.png"/></div></div></figure><p id="f36e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">从潜空间取样</strong></p><p id="3585" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以随机抽取 15 个潜在代码，解码后生成新的名人面孔。我们可以从这个表示中看到，由我们的模型生成的图像与我们训练集中的那些图像具有非常相似的风格，并且它也具有良好的真实性和变化性。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="5f24" class="ol nj iq oh b gy om on l oo op"># We randomly generated 15 images from 15 series of noise information</span><span id="9d7f" class="ol nj iq oh b gy oq on l oo op">n = 3<br/>m = 5<br/>digit_size1 = 218<br/>digit_size2 = 178<br/>figure = np.zeros((digit_size1 * n, digit_size2 * m,3))<br/> <br/>for i in range(3):<br/>    for j in range(5):<br/>        z_sample = np.random.rand(1,512)<br/>        x_decoded = decoder.predict([z_sample])<br/>        figure[i * digit_size1: (i + 1) * digit_size1,<br/>               j * digit_size2: (j + 1) * digit_size2,:] = (x_decoded[0]+1)/2 </span><span id="296f" class="ol nj iq oh b gy oq on l oo op">plt.figure(figsize=(10, 10))<br/>plt.imshow(figure)<br/>plt.show()</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pg"><img src="../Images/5437be16ebd1db85f35a0228ed23b262.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0cLHCr5-sTbCn-xzKxhTA.png"/></div></div></figure><p id="5069" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以看起来我们的 VAE 模式并不是特别好。如果有更多的时间和更好地选择超参数等等，我们可能会取得比这更好的结果。</p><p id="4ece" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们将这个结果与相同数据集上的 DC-甘进行比较。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="c4cc" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">CelebA 数据集上的 DC-甘</strong></h1><p id="9206" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">因为我们已经设置了流生成器，所以没有太多的工作要做来启动和运行 DC-甘模型。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="fd78" class="ol nj iq oh b gy om on l oo op"># Create and compile a DC-GAN model, and print the summary<br/><br/>from keras.utils import np_utils<br/>from keras.models import Sequential, Model<br/>from keras.layers import Input, Dense, Dropout, Activation, Flatten, LeakyReLU,\<br/>      BatchNormalization, Conv2DTranspose, Conv2D, Reshape<br/>from keras.layers.advanced_activations import LeakyReLU<br/>from keras.optimizers import Adam, RMSprop<br/>from keras.initializers import RandomNormal<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import random<br/>from tqdm import tqdm_notebook<br/>from scipy.misc import imresize<br/><br/>def generator_model(latent_dim=100, leaky_alpha=0.2, init_stddev=0.02):<br/><br/>    g = Sequential()<br/>    g.add(Dense(4*4*512, input_shape=(latent_dim,),<br/>                kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    g.add(Reshape(target_shape=(4, 4, 512)))<br/>    g.add(BatchNormalization())<br/>    g.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    g.add(Conv2DTranspose(256, kernel_size=5, strides=2, padding='same',<br/>                kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    g.add(BatchNormalization())<br/>    g.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    g.add(Conv2DTranspose(128, kernel_size=5, strides=2, padding='same', <br/>                kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    g.add(BatchNormalization())<br/>    g.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    g.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', <br/>                kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    g.add(Activation('tanh'))<br/>    g.summary()<br/>    #g.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])<br/>    return g<br/><br/>  <br/>def discriminator_model(leaky_alpha=0.2, init_stddev=0.02):<br/>    <br/>    d = Sequential()<br/>    d.add(Conv2D(64, kernel_size=5, strides=2, padding='same', <br/>               kernel_initializer=RandomNormal(stddev=init_stddev),<br/>               input_shape=(32, 32, 3)))<br/>    d.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    d.add(Conv2D(128, kernel_size=5, strides=2, padding='same', <br/>               kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    d.add(BatchNormalization())<br/>    d.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    d.add(Conv2D(256, kernel_size=5, strides=2, padding='same', <br/>               kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    d.add(BatchNormalization())<br/>    d.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    d.add(Flatten())<br/>    d.add(Dense(1, kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    d.add(Activation('sigmoid'))<br/>    d.summary()<br/>    return d<br/><br/>def DCGAN(sample_size=100):<br/>    # Generator<br/>    g = generator_model(sample_size, 0.2, 0.02)<br/><br/>    # Discriminator<br/>    d = discriminator_model(0.2, 0.02)<br/>    d.compile(optimizer=Adam(lr=0.001, beta_1=0.5), loss='binary_crossentropy')<br/>    d.trainable = False<br/>    # GAN<br/>    gan = Sequential([g, d])<br/>    gan.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')<br/>    <br/>    return gan, g, d</span></pre><p id="5d60" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以上代码只是针对发生器和鉴别器网络的架构。将这种编码 GAN 的方法与我在第 2 部分中使用的方法进行比较是一个好主意，您可以看到这种方法不太清晰，并且我们没有定义全局参数，因此有许多地方我们可能会有潜在的错误。</p><p id="267d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们定义了一些函数来简化我们的工作，这些函数主要用于图像的预处理和绘图，以帮助我们分析网络输出。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="39c5" class="ol nj iq oh b gy om on l oo op">def load_image(filename, size=(32, 32)):<br/>    img = plt.imread(filename)<br/>    # crop<br/>    rows, cols = img.shape[:2]<br/>    crop_r, crop_c = 150, 150<br/>    start_row, start_col = (rows - crop_r) // 2, (cols - crop_c) // 2<br/>    end_row, end_col = rows - start_row, cols - start_row<br/>    img = img[start_row:end_row, start_col:end_col, :]<br/>    # resize<br/>    img = imresize(img, size)<br/>    return img<br/><br/>def preprocess(x):<br/>    return (x/255)*2-1<br/><br/>def deprocess(x):<br/>    return np.uint8((x+1)/2*255)<br/><br/>def make_labels(size):<br/>    return np.ones([size, 1]), np.zeros([size, 1])  <br/><br/>def show_losses(losses):<br/>    losses = np.array(losses)<br/>    <br/>    fig, ax = plt.subplots()<br/>    plt.plot(losses.T[0], label='Discriminator')<br/>    plt.plot(losses.T[1], label='Generator')<br/>    plt.title("Validation Losses")<br/>    plt.legend()<br/>    plt.show()<br/><br/>def show_images(generated_images):<br/>    n_images = len(generated_images)<br/>    cols = 5<br/>    rows = n_images//cols<br/>    <br/>    plt.figure(figsize=(8, 6))<br/>    for i in range(n_images):<br/>        img = deprocess(generated_images[i])<br/>        ax = plt.subplot(rows, cols, i+1)<br/>        plt.imshow(img)<br/>        plt.xticks([])<br/>        plt.yticks([])<br/>    plt.tight_layout()<br/>    plt.show()</span></pre><p id="65ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">训练模型</strong></p><p id="d80e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在定义训练函数。正如我们之前所做的，请注意，我们在将鉴别器设置为可训练和不可训练之间进行了切换(我们在第 2 部分中隐式地这样做了)。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="d1a7" class="ol nj iq oh b gy om on l oo op">def train(sample_size=100, epochs=3, batch_size=128, eval_size=16, smooth=0.1):</span><span id="8155" class="ol nj iq oh b gy oq on l oo op">    batchCount=len(train_path)//batch_size<br/>    y_train_real, y_train_fake = make_labels(batch_size)<br/>    y_eval_real,  y_eval_fake  = make_labels(eval_size)<br/>    <br/>    # create a GAN, a generator and a discriminator<br/>    gan, g, d = DCGAN(sample_size)<br/>    <br/>    losses = []</span><span id="8b7b" class="ol nj iq oh b gy oq on l oo op">    for e in range(epochs):<br/>        print('-'*15, 'Epoch %d' % (e+1), '-'*15)<br/>        for i in tqdm_notebook(range(batchCount)):<br/>            <br/>            path_batch = train_path[i*batch_size:(i+1)*batch_size]<br/>            image_batch = np.array([preprocess(load_image(filename)) for filename in path_batch])<br/>            <br/>            noise = np.random.normal(0, 1, size=(batch_size, noise_dim))<br/>            generated_images = g.predict_on_batch(noise)</span><span id="1c31" class="ol nj iq oh b gy oq on l oo op">            # Train discriminator on generated images<br/>            d.trainable = True<br/>            d.train_on_batch(image_batch, y_train_real*(1-smooth))<br/>            d.train_on_batch(generated_images, y_train_fake)</span><span id="e84d" class="ol nj iq oh b gy oq on l oo op">            # Train generator<br/>            d.trainable = False<br/>            g_loss=gan.train_on_batch(noise, y_train_real)<br/>        <br/>        # evaluate<br/>        test_path = np.array(val_path)[np.random.choice(len(val_path), eval_size, replace=False)]<br/>        x_eval_real = np.array([preprocess(load_image(filename)) for filename in test_path])</span><span id="b372" class="ol nj iq oh b gy oq on l oo op">        noise = np.random.normal(loc=0, scale=1, size=(eval_size, sample_size))<br/>        x_eval_fake = g.predict_on_batch(noise)<br/>        <br/>        d_loss  = d.test_on_batch(x_eval_real, y_eval_real)<br/>        d_loss += d.test_on_batch(x_eval_fake, y_eval_fake)<br/>        g_loss  = gan.test_on_batch(noise, y_eval_real)<br/>        <br/>        losses.append((d_loss/2, g_loss))<br/>  <br/>        print("Epoch: {:&gt;3}/{} Discriminator Loss: {:&gt;6.4f} Generator Loss: {:&gt;6.4f}".format(<br/>            e+1, epochs, d_loss, g_loss))  <br/>        <br/>        show_images(x_eval_fake[:10])<br/>    <br/>    # show the result<br/>    show_losses(losses)<br/>    show_images(g.predict(np.random.normal(loc=0, scale=1, size=(15, sample_size))))    <br/>    return g</span><span id="d493" class="ol nj iq oh b gy oq on l oo op">noise_dim=100<br/>train()</span></pre><p id="56b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该函数的输出将为我们提供每个时期的以下输出:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ph"><img src="../Images/f9d52f41c44602678485d737545c40ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gYznU_5Po1WXjqvRT8T1eQ.png"/></div></div></figure><p id="f992" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它还将绘制鉴别器和发生器的验证损失。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pi"><img src="../Images/be7461d47103c59924a5ebbe26a9eba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_UVLHNQilVdcYUJh48Z7A.png"/></div></div></figure><p id="3f04" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">生成的图像看起来很合理。在这里，我们可以看到我们的模型表现得足够好，尽管图像质量不如训练集中的图像质量好(因为我们对图像进行了整形，使其变得更小，并使它们比原始图像更模糊)。但是，它们足够生动，可以创建有效的人脸，并且这些人脸足够接近现实。此外，与 VAE 制作的图像相比，这些图像更有创意，看起来更真实。</p><p id="e3f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以看起来 GAN 在这种情况下表现更好。现在，让我们尝试一个新的数据集，看看 GAN 与混合变体 VAE-GAN 相比表现如何。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="4824" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">动漫数据集</strong></h1><p id="b694" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">在这一节中，我们将使用 GAN 以及另一种特殊形式的 GAN，即 VAE-GAN，来生成与动画数据集风格相同的人脸。术语 VAE-甘首先由 Larsen 等人使用。al 在他们的论文<a class="ae lw" href="https://arxiv.org/abs/1512.09300" rel="noopener ugc nofollow" target="_blank">“使用学习的相似性度量进行像素以外的自动编码”</a>。VAE-甘模型与甘模型的区别在于它们的<strong class="kh ir">生成器是变异自动编码器</strong>。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pj"><img src="../Images/f45b973004a186700c45f332f71c45e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePINXrbAYFtORd8ZFPOCpg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">VAE-GAN architecture. Source: <a class="ae lw" href="https://arxiv.org/abs/1512.09300" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1512.09300</a></figcaption></figure><p id="cd4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们将重点放在 DC-甘。动漫数据集由超过 20K 张 64x64 图像形式的动漫头像组成。我们还需要创建另一个<a class="ae lw" href="https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43" rel="noopener ugc nofollow" target="_blank"> Keras 定制数据生成器</a>。该数据集的链接可在此处找到:</p><div class="lc ld gp gr le lf"><a href="https://github.com/Mckinsey666/Anime-Face-Dataset" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">McKinsey 666/动漫人脸数据集</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">🖼收集了高质量的动漫面孔。为 Mckinsey666/Anime-Face-Dataset 开发做出贡献，创建一个…</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">github.com</p></div></div><div class="lo l"><div class="pk l lq lr ls lo lt lu lf"/></div></div></a></div></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="a4cd" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">动漫数据集上的甘</strong></h1><p id="3b8f" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">我们需要做的第一件事是创建动漫目录并下载数据。这可以通过上面的链接来完成。在继续之前检查数据总是好的做法，所以我们现在就这样做。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="94b5" class="ol nj iq oh b gy om on l oo op">from skimage import io<br/>import matplotlib.pyplot as plt<br/><br/>filePath='anime-faces/data/'<br/>imgSets=[]<br/><br/>for i in range(1,20001):<br/>    imgName=filePath+str(i)+'.png'<br/>    imgSets.append(io.imread(imgName))<br/><br/>plt.imshow(imgSets[1234])<br/>plt.axis('off')<br/>plt.show()</span></pre><p id="3d74" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在创建并编译我们的 DC-甘模型。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="9f3c" class="ol nj iq oh b gy om on l oo op"># Create and compile a DC-GAN model</span><span id="7805" class="ol nj iq oh b gy oq on l oo op">from keras.models import Sequential, Model<br/>from keras.layers import Input, Dense, Dropout, Activation, \<br/>    Flatten, LeakyReLU, BatchNormalization, Conv2DTranspose, Conv2D, Reshape<br/>from keras.layers.advanced_activations import LeakyReLU<br/>from keras.layers.convolutional import UpSampling2D<br/>from keras.optimizers import Adam, RMSprop,SGD<br/>from keras.initializers import RandomNormal</span><span id="58f2" class="ol nj iq oh b gy oq on l oo op">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import os, glob<br/>from PIL import Image<br/>from tqdm import tqdm_notebook<br/></span><span id="c032" class="ol nj iq oh b gy oq on l oo op">image_shape = (64, 64, 3)<br/>#noise_shape = (100,)<br/>Noise_dim = 128<br/>img_rows = 64<br/>img_cols = 64<br/>channels = 3</span><span id="3fa0" class="ol nj iq oh b gy oq on l oo op">def generator_model(latent_dim=100, leaky_alpha=0.2):<br/>    model = Sequential()<br/>    <br/>    # layer1 (None,500)&gt;&gt;(None,128*16*16)<br/>    model.add(Dense(128 * 16 * 16, activation="relu", input_shape=(Noise_dim,)))<br/>    <br/>    # (None,16*16*128)&gt;&gt;(None,16,16,128)<br/>    model.add(Reshape((16, 16, 128)))<br/>    <br/>   # (None,16,16,128)&gt;&gt;(None,32,32,128)<br/>    model.add(UpSampling2D())<br/>    model.add(Conv2D(256, kernel_size=3, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(Activation("relu"))</span><span id="bb15" class="ol nj iq oh b gy oq on l oo op">    #(None,32,32,128)&gt;&gt;(None,64,64,128)<br/>    model.add(UpSampling2D())<br/>    <br/>    # (None,64,64,128)&gt;&gt;(None,64,64,64)<br/>    model.add(Conv2D(128, kernel_size=3, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(Activation("relu"))</span><span id="3260" class="ol nj iq oh b gy oq on l oo op">    # (None,64,64,128)&gt;&gt;(None,64,64,32)</span><span id="170d" class="ol nj iq oh b gy oq on l oo op">    model.add(Conv2D(32, kernel_size=3, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(Activation("relu"))<br/>    <br/>    # (None,64,64,32)&gt;&gt;(None,64,64,3)<br/>    model.add(Conv2D(channels, kernel_size=3, padding="same"))<br/>    model.add(Activation("tanh"))</span><span id="68b6" class="ol nj iq oh b gy oq on l oo op">    model.summary()<br/>    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])<br/>    return model<br/></span><span id="7834" class="ol nj iq oh b gy oq on l oo op">def discriminator_model(leaky_alpha=0.2, dropRate=0.3):<br/>    model = Sequential()<br/>    <br/>    # layer1 (None,64,64,3)&gt;&gt;(None,32,32,32)<br/>    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, padding="same"))<br/>    model.add(LeakyReLU(alpha=leaky_alpha))<br/>    model.add(Dropout(dropRate))</span><span id="6b7c" class="ol nj iq oh b gy oq on l oo op">    # layer2 (None,32,32,32)&gt;&gt;(None,16,16,64)<br/>    model.add(Conv2D(64, kernel_size=3, strides=2, padding="same"))</span><span id="cedd" class="ol nj iq oh b gy oq on l oo op">    # model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(LeakyReLU(alpha=leaky_alpha))<br/>    model.add(Dropout(dropRate))</span><span id="1905" class="ol nj iq oh b gy oq on l oo op">    # (None,16,16,64)&gt;&gt;(None,8,8,128)<br/>    model.add(Conv2D(128, kernel_size=3, strides=2, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(LeakyReLU(alpha=0.2))<br/>    model.add(Dropout(dropRate))</span><span id="cc4b" class="ol nj iq oh b gy oq on l oo op">    # (None,8,8,128)&gt;&gt;(None,8,8,256)<br/>    model.add(Conv2D(256, kernel_size=3, strides=1, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(LeakyReLU(alpha=0.2))<br/>    model.add(Dropout(dropRate))</span><span id="dc45" class="ol nj iq oh b gy oq on l oo op">     # (None,8,8,256)&gt;&gt;(None,8,8,64)<br/>    model.add(Conv2D(64, kernel_size=3, strides=1, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(LeakyReLU(alpha=0.2))<br/>    model.add(Dropout(dropRate))<br/>    <br/>    # (None,8,8,64)<br/>    model.add(Flatten())<br/>    model.add(Dense(1, activation='sigmoid'))</span><span id="0146" class="ol nj iq oh b gy oq on l oo op">    model.summary()</span><span id="80b3" class="ol nj iq oh b gy oq on l oo op">    sgd=SGD(lr=0.0002)<br/>    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])<br/>    return model<br/></span><span id="8d56" class="ol nj iq oh b gy oq on l oo op">def DCGAN(sample_size=Noise_dim):<br/>    # generator<br/>    g = generator_model(sample_size, 0.2)</span><span id="262a" class="ol nj iq oh b gy oq on l oo op">    # discriminator<br/>    d = discriminator_model(0.2)<br/>    d.trainable = False<br/>    # GAN<br/>    gan = Sequential([g, d])<br/>    <br/>    sgd=SGD()<br/>    gan.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')<br/>    return gan, g, d<br/></span><span id="0a3f" class="ol nj iq oh b gy oq on l oo op">def get_image(image_path, width, height, mode):<br/>    image = Image.open(image_path)<br/>    #print(image.size)</span><span id="36ff" class="ol nj iq oh b gy oq on l oo op">    return np.array(image.convert(mode))<br/></span><span id="65e2" class="ol nj iq oh b gy oq on l oo op">def get_batch(image_files, width, height, mode):<br/>    data_batch = np.array([get_image(sample_file, width, height, mode) \<br/>                           for sample_file in image_files])<br/>    return data_batch<br/></span><span id="78f4" class="ol nj iq oh b gy oq on l oo op">def show_imgs(generator,epoch):<br/>    row=3<br/>    col = 5<br/>    noise = np.random.normal(0, 1, (row * col, Noise_dim))<br/>    gen_imgs = generator.predict(noise)</span><span id="1cf4" class="ol nj iq oh b gy oq on l oo op">    # Rescale images 0 - 1<br/>    gen_imgs = 0.5 * gen_imgs + 0.5</span><span id="0596" class="ol nj iq oh b gy oq on l oo op">    fig, axs = plt.subplots(row, col)<br/>    #fig.suptitle("DCGAN: Generated digits", fontsize=12)<br/>    cnt = 0</span><span id="a48c" class="ol nj iq oh b gy oq on l oo op">    for i in range(row):<br/>        for j in range(col):<br/>            axs[i, j].imshow(gen_imgs[cnt, :, :, :])<br/>            axs[i, j].axis('off')<br/>            cnt += 1</span><span id="6192" class="ol nj iq oh b gy oq on l oo op">    #plt.close()<br/>    plt.show()</span></pre><p id="73e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在可以在动画数据集上训练模型。我们将以两种不同的方式来实现这一点，第一种方式将涉及以 1:1 的训练时间比例来训练鉴别器和生成器。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="0159" class="ol nj iq oh b gy om on l oo op"># Training the discriminator and generator with the 1:1 proportion of training times</span><span id="f273" class="ol nj iq oh b gy oq on l oo op">def train(epochs=30, batchSize=128):<br/>    filePath = r'anime-faces/data/'</span><span id="b9a3" class="ol nj iq oh b gy oq on l oo op">    X_train = get_batch(glob.glob(os.path.join(filePath, '*.png'))[:20000], 64, 64, 'RGB')<br/>    X_train = (X_train.astype(np.float32) - 127.5) / 127.5</span><span id="3225" class="ol nj iq oh b gy oq on l oo op">    halfSize = int(batchSize / 2)<br/>    batchCount=int(len(X_train)/batchSize)</span><span id="e241" class="ol nj iq oh b gy oq on l oo op">    dLossReal = []<br/>    dLossFake = []<br/>    gLossLogs = []</span><span id="1b15" class="ol nj iq oh b gy oq on l oo op">    gan, generator, discriminator = DCGAN(Noise_dim)</span><span id="540a" class="ol nj iq oh b gy oq on l oo op">    for e in range(epochs):<br/>        for i in tqdm_notebook(range(batchCount)):<br/>            index = np.random.randint(0, X_train.shape[0], halfSize)<br/>            images = X_train[index]</span><span id="d9fe" class="ol nj iq oh b gy oq on l oo op">            noise = np.random.normal(0, 1, (halfSize, Noise_dim))<br/>            genImages = generator.predict(noise)</span><span id="0ff3" class="ol nj iq oh b gy oq on l oo op">            # one-sided labels<br/>            discriminator.trainable = True<br/>            dLossR = discriminator.train_on_batch(images, np.ones([halfSize, 1]))<br/>            dLossF = discriminator.train_on_batch(genImages, np.zeros([halfSize, 1]))<br/>            dLoss = np.add(dLossF, dLossR) * 0.5<br/>            discriminator.trainable = False</span><span id="62dd" class="ol nj iq oh b gy oq on l oo op">            noise = np.random.normal(0, 1, (batchSize, Noise_dim))<br/>            gLoss = gan.train_on_batch(noise, np.ones([batchSize, 1]))</span><span id="63ca" class="ol nj iq oh b gy oq on l oo op">        dLossReal.append([e, dLoss[0]])<br/>        dLossFake.append([e, dLoss[1]])<br/>        gLossLogs.append([e, gLoss])</span><span id="c714" class="ol nj iq oh b gy oq on l oo op">        dLossRealArr = np.array(dLossReal)<br/>        dLossFakeArr = np.array(dLossFake)<br/>        gLossLogsArr = np.array(gLossLogs)<br/>            </span><span id="9dc0" class="ol nj iq oh b gy oq on l oo op">        # At the end of training plot the losses vs epochs<br/>        show_imgs(generator, e)</span><span id="b435" class="ol nj iq oh b gy oq on l oo op">    plt.plot(dLossRealArr[:, 0], dLossRealArr[:, 1], label="Discriminator Loss - Real")<br/>    plt.plot(dLossFakeArr[:, 0], dLossFakeArr[:, 1], label="Discriminator Loss - Fake")<br/>    plt.plot(gLossLogsArr[:, 0], gLossLogsArr[:, 1], label="Generator Loss")<br/>    plt.xlabel('Epochs')<br/>    plt.ylabel('Loss')<br/>    plt.legend()<br/>    plt.title('GAN')<br/>    plt.grid(True)<br/>    plt.show()<br/>    <br/>    <br/>    return gan, generator, discriminator<br/></span><span id="beaa" class="ol nj iq oh b gy oq on l oo op">GAN,Generator,Discriminator=train(epochs=20, batchSize=128)  <br/>train(epochs=1000, batchSize=128, plotInternal=200)</span></pre><p id="2d3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出现在将开始打印一系列的动漫人物。它们起初非常粗糙，随着时间的推移逐渐变得越来越明显。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pl"><img src="../Images/735ed6f8251508b90e13ea4aef36f4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDfCiFhLqbM76RGKv79chA.png"/></div></div></figure><p id="4131" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还将得到发电机和鉴频器损耗函数的曲线图。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pm"><img src="../Images/4c7bd2ea43fa1c085a898f9f8fb186fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fwLeI_9ndMFsvtV3_rh73w.png"/></div></div></figure><p id="1568" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们将做同样的事情，但是用不同的训练时间来训练鉴别器和生成器，看看效果如何。</p><p id="9b3b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在继续之前，最好将模型的权重保存在某个地方，这样您就不需要再次运行整个训练，而是可以将权重加载到网络中。</p><p id="64bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了节省重量:</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="8611" class="ol nj iq oh b gy om on l oo op">discriminator.save_weights('/content/gdrive/My Drive/discriminator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')<br/>gan.save_weights('/content/gdrive/My Drive/gan_DCGAN_lr0.0001_deepgenerator+proportion2.h5')<br/>generator.save_weights('/content/gdrive/My Drive/generator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')</span></pre><p id="54fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要加载砝码:</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="e958" class="ol nj iq oh b gy om on l oo op">discriminator.load_weights('/content/gdrive/My Drive/discriminator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')<br/>gan.load_weights('/content/gdrive/My Drive/gan_DCGAN_lr0.0001_deepgenerator+proportion2.h5')<br/>generator.load_weights('/content/gdrive/My Drive/generator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')</span></pre><p id="4ab3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们转到第二个网络实施，而不用担心比之前的网络节省成本。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="6e46" class="ol nj iq oh b gy om on l oo op"># Train the discriminator and generator separately and with different training times</span><span id="501c" class="ol nj iq oh b gy oq on l oo op">def train(epochs=300, batchSize=128, plotInternal=50):<br/>    gLoss = 1<br/>    filePath = r'anime-faces/data/'<br/>    <br/>    X_train = get_batch(glob.glob(os.path.join(filePath,'*.png'))[:20000],64,64,'RGB')<br/>    X_train=(X_train.astype(np.float32)-127.5)/127.5<br/>    halfSize= int (batchSize/2)</span><span id="62b4" class="ol nj iq oh b gy oq on l oo op">    dLossReal=[]<br/>    dLossFake=[]<br/>    gLossLogs=[]<br/>    </span><span id="ec63" class="ol nj iq oh b gy oq on l oo op">    for e in range(epochs):<br/>        index=np.random.randint(0,X_train.shape[0],halfSize)<br/>        images=X_train[index]</span><span id="e9c8" class="ol nj iq oh b gy oq on l oo op">        noise=np.random.normal(0,1,(halfSize,Noise_dim))<br/>        genImages=generator.predict(noise)<br/>        <br/>        if e &lt; int(epochs*0.5):    <br/>            #one-sided labels<br/>            discriminator.trainable=True<br/>            dLossR=discriminator.train_on_batch(images,np.ones([halfSize,1]))<br/>            dLossF=discriminator.train_on_batch(genImages,np.zeros([halfSize,1]))<br/>            dLoss=np.add(dLossF,dLossR)*0.5<br/>            discriminator.trainable=False</span><span id="08cb" class="ol nj iq oh b gy oq on l oo op">            cnt = e</span><span id="9a5a" class="ol nj iq oh b gy oq on l oo op">            while cnt &gt; 3:<br/>                cnt = cnt - 4</span><span id="2c2f" class="ol nj iq oh b gy oq on l oo op">            if cnt == 0:<br/>                noise=np.random.normal(0,1,(batchSize,Noise_dim))<br/>                gLoss=gan.train_on_batch(noise,np.ones([batchSize,1]))<br/>                <br/>        elif e&gt;= int(epochs*0.5) :<br/>            cnt = e</span><span id="58fc" class="ol nj iq oh b gy oq on l oo op">            while cnt &gt; 3:<br/>                cnt = cnt - 4</span><span id="31ea" class="ol nj iq oh b gy oq on l oo op">            if cnt == 0:<br/>                #one-sided labels<br/>                discriminator.trainable=True<br/>                dLossR=discriminator.train_on_batch(images,np.ones([halfSize,1]))<br/>                dLossF=discriminator.train_on_batch(genImages,np.zeros([halfSize,1]))<br/>                dLoss=np.add(dLossF,dLossR)*0.5<br/>                discriminator.trainable=False</span><span id="9d1c" class="ol nj iq oh b gy oq on l oo op">            <br/>            noise=np.random.normal(0,1,(batchSize,Noise_dim))<br/>            gLoss=gan.train_on_batch(noise,np.ones([batchSize,1]))</span><span id="fcf5" class="ol nj iq oh b gy oq on l oo op">        if e % 20 == 0:<br/>           print("epoch： %d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (e, dLoss[0], 100 * dLoss[1], gLoss))</span><span id="e8ad" class="ol nj iq oh b gy oq on l oo op">        dLossReal.append([e,dLoss[0]])<br/>        dLossFake.append([e,dLoss[1]])<br/>        gLossLogs.append([e,gLoss])</span><span id="5fb3" class="ol nj iq oh b gy oq on l oo op">        if e % plotInternal == 0 and e!=0:<br/>            show_imgs(generator, e)<br/>            <br/>            <br/>        dLossRealArr= np.array(dLossReal)<br/>        dLossFakeArr = np.array(dLossFake)<br/>        gLossLogsArr = np.array(gLossLogs)<br/>        <br/>        chk = e</span><span id="b416" class="ol nj iq oh b gy oq on l oo op">        while chk &gt; 50:<br/>            chk = chk - 51</span><span id="74f1" class="ol nj iq oh b gy oq on l oo op">        if chk == 0:<br/>            discriminator.save_weights('/content/gdrive/My Drive/discriminator_DCGAN_lr=0.0001,proportion2,deepgenerator_Fake.h5')<br/>            gan.save_weights('/content/gdrive/My Drive/gan_DCGAN_lr=0.0001,proportion2,deepgenerator_Fake.h5')<br/>            generator.save_weights('/content/gdrive/My Drive/generator_DCGAN_lr=0.0001,proportion2,deepgenerator_Fake.h5')<br/>        # At the end of training plot the losses vs epochs<br/>    plt.plot(dLossRealArr[:, 0], dLossRealArr[:, 1], label="Discriminator Loss - Real")<br/>    plt.plot(dLossFakeArr[:, 0], dLossFakeArr[:, 1], label="Discriminator Loss - Fake")<br/>    plt.plot(gLossLogsArr[:, 0], gLossLogsArr[:, 1], label="Generator Loss")<br/>    plt.xlabel('Epochs')<br/>    plt.ylabel('Loss')<br/>    plt.legend()<br/>    plt.title('GAN')<br/>    plt.grid(True)<br/>    plt.show()<br/>    <br/>    <br/>    return gan, generator, discriminator</span><span id="ac30" class="ol nj iq oh b gy oq on l oo op">gan, generator, discriminator = DCGAN(Noise_dim)<br/>train(epochs=4000, batchSize=128, plotInternal=200)</span></pre><p id="9192" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们比较一下这两个网络的输出。通过运行该行:</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="53e2" class="ol nj iq oh b gy om on l oo op">show_imgs(Generator)</span></pre><p id="cfa5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">网络将从生成器输出一些图像(这是我们之前定义的函数之一)。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pn"><img src="../Images/6cc4033c4a44d571e8c5b34b431292a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*41NT55eQwng0pOMovzsX8Q.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Generated images from 1:1 training of discriminator vs. generator.</figcaption></figure><p id="dee8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们检查第二个模型。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi po"><img src="../Images/5353ccde8122af5660467cb96f754037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FtrkpRNJAd4oMAmIm3ldeQ.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Generated images from the second network with different training times for the discriminator and generator.</figcaption></figure><p id="889c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到，生成的图像的细节得到了改善，它们的纹理稍微更加详细。然而，与训练图像相比，它们仍然是不合格的。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi po"><img src="../Images/a4209a9809e24ec6a6e1fb5217fcf38c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x5fUEObLu1RgpqJjPeKI2Q.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Training images from Anime dataset.</figcaption></figure><p id="8ca6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也许 VAE-甘会表现得更好？</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="42e6" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">动漫数据集上的甘</strong></h1><p id="3849" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">重申一下我之前说过的关于 VAE-甘的话，术语 VAE-甘首先是由 Larsen 等人使用的。al 在他们的论文<a class="ae lw" href="https://arxiv.org/abs/1512.09300" rel="noopener ugc nofollow" target="_blank">“使用学习的相似性度量进行像素以外的自动编码”</a>。VAE-甘模型与甘模型的区别在于它们的<strong class="kh ir">生成器是变异自动编码器</strong>。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pj"><img src="../Images/f45b973004a186700c45f332f71c45e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePINXrbAYFtORd8ZFPOCpg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">VAE-GAN architecture. Source: <a class="ae lw" href="https://arxiv.org/abs/1512.09300" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1512.09300</a></figcaption></figure><p id="7e3e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们需要创建并编译 VAE-GAN，并对每个网络进行总结(这是简单检查架构的好方法)。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="4d50" class="ol nj iq oh b gy om on l oo op"># Create and compile a VAE-GAN, and make a summary for them</span><span id="7244" class="ol nj iq oh b gy oq on l oo op">from keras.models import Sequential, Model<br/>from keras.layers import Input, Dense, Dropout, Activation, \<br/>    Flatten, LeakyReLU, BatchNormalization, Conv2DTranspose, Conv2D, Reshape,MaxPooling2D,UpSampling2D,InputLayer, Lambda<br/>from keras.layers.advanced_activations import LeakyReLU<br/>from keras.layers.convolutional import UpSampling2D<br/>from keras.optimizers import Adam, RMSprop,SGD<br/>from keras.initializers import RandomNormal<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import os, glob<br/>from PIL import Image<br/>import pandas as pd<br/>from scipy.stats import norm<br/>import keras<br/>from keras.utils import np_utils, to_categorical<br/>from keras import backend as K<br/>import random<br/>from keras import metrics<br/>from tqdm import tqdm<br/></span><span id="3fe7" class="ol nj iq oh b gy oq on l oo op"># plotInternal<br/>plotInternal = 50</span><span id="d426" class="ol nj iq oh b gy oq on l oo op">#######<br/>latent_dim = 256<br/>batch_size = 256<br/>rows = 64<br/>columns = 64<br/>channel = 3<br/>epochs = 4000<br/># datasize = len(dataset)</span><span id="7b80" class="ol nj iq oh b gy oq on l oo op"># optimizers<br/>SGDop = SGD(lr=0.0003)<br/>ADAMop = Adam(lr=0.0002)<br/># filters<br/>filter_of_dis = 16<br/>filter_of_decgen = 16<br/>filter_of_encoder = 16<br/></span><span id="ccc2" class="ol nj iq oh b gy oq on l oo op">def sampling(args):<br/>    mean, logsigma = args<br/>    epsilon = K.random_normal(shape=(K.shape(mean)[0], latent_dim), mean=0., stddev=1.0)<br/>    return mean + K.exp(logsigma / 2) * epsilon</span><span id="cded" class="ol nj iq oh b gy oq on l oo op">def vae_loss(X , output , E_mean, E_logsigma):<br/>	# compute the average MSE error, then scale it up, ie. simply sum on all axes<br/>  reconstruction_loss = 2 * metrics.mse(K.flatten(X), K.flatten(output))<br/>  <br/>	# compute the KL loss<br/>  kl_loss = - 0.5 * K.sum(1 + E_logsigma - K.square(E_mean) - K.exp(E_logsigma), axis=-1) </span><span id="a003" class="ol nj iq oh b gy oq on l oo op">  total_loss = K.mean(reconstruction_loss + kl_loss)    <br/>  <br/>  return total_loss<br/>  </span><span id="4999" class="ol nj iq oh b gy oq on l oo op">def encoder(kernel, filter, rows, columns, channel):<br/>    X = Input(shape=(rows, columns, channel))<br/>    model = Conv2D(filters=filter, kernel_size=kernel, strides=2, padding='same')(X)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="1ae6" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="dccb" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="436c" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="c7df" class="ol nj iq oh b gy oq on l oo op">    model = Flatten()(model)</span><span id="a835" class="ol nj iq oh b gy oq on l oo op">    mean = Dense(latent_dim)(model)<br/>    logsigma = Dense(latent_dim, activation='tanh')(model)<br/>    latent = Lambda(sampling, output_shape=(latent_dim,))([mean, logsigma])<br/>    meansigma = Model([X], [mean, logsigma, latent])<br/>    meansigma.compile(optimizer=SGDop, loss='mse')<br/>    return meansigma<br/></span><span id="cd64" class="ol nj iq oh b gy oq on l oo op">def decgen(kernel, filter, rows, columns, channel):<br/>    X = Input(shape=(latent_dim,))</span><span id="6250" class="ol nj iq oh b gy oq on l oo op">    model = Dense(2*2*256)(X)<br/>    model = Reshape((2, 2, 256))(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = Activation('relu')(model)</span><span id="7a64" class="ol nj iq oh b gy oq on l oo op">    model = Conv2DTranspose(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = Activation('relu')(model)<br/>    <br/>    model = Conv2DTranspose(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = Activation('relu')(model)</span><span id="41cb" class="ol nj iq oh b gy oq on l oo op">    model = Conv2DTranspose(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = Activation('relu')(model)</span><span id="1565" class="ol nj iq oh b gy oq on l oo op">    model = Conv2DTranspose(filters=filter, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = Activation('relu')(model)</span><span id="ab88" class="ol nj iq oh b gy oq on l oo op">    model = Conv2DTranspose(filters=channel, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = Activation('tanh')(model)</span><span id="14ba" class="ol nj iq oh b gy oq on l oo op">    model = Model(X, model)<br/>    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])<br/>    return model<br/></span><span id="a4aa" class="ol nj iq oh b gy oq on l oo op">def discriminator(kernel, filter, rows, columns, channel):<br/>    X = Input(shape=(rows, columns, channel))</span><span id="e33f" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(X)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="be96" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="de76" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="2d4b" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)<br/></span><span id="3581" class="ol nj iq oh b gy oq on l oo op">    dec = BatchNormalization(epsilon=1e-5)(model)<br/>    dec = LeakyReLU(alpha=0.2)(dec)<br/>    dec = Flatten()(dec)<br/>    dec = Dense(1, activation='sigmoid')(dec)</span><span id="3acc" class="ol nj iq oh b gy oq on l oo op">    output = Model(X, dec)<br/>    output.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])<br/>                   <br/>    return output</span><span id="f866" class="ol nj iq oh b gy oq on l oo op">  <br/>def VAEGAN(decgen,discriminator):<br/>    # generator<br/>    g = decgen</span><span id="e1e6" class="ol nj iq oh b gy oq on l oo op">    # discriminator<br/>    d = discriminator<br/>    d.trainable = False<br/>    # GAN<br/>    gan = Sequential([g, d])<br/>    <br/>#     sgd=SGD()<br/>    gan.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')<br/>    return g, d, gan</span></pre><p id="439a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们再次定义了一些函数，这样我们就可以打印来自生成器的图像。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="1db8" class="ol nj iq oh b gy om on l oo op">def get_image(image_path, width, height, mode):<br/>    image = Image.open(image_path)<br/>    #print(image.size)<br/><br/>    return np.array(image.convert(mode))<br/><br/>def show_imgs(generator):<br/>    row=3<br/>    col = 5<br/>    noise = np.random.normal(0, 1, (row*col, latent_dim))<br/>    gen_imgs = generator.predict(noise)<br/><br/>    # Rescale images 0 - 1<br/>    gen_imgs = 0.5 * gen_imgs + 0.5<br/><br/>    fig, axs = plt.subplots(row, col)<br/>    #fig.suptitle("DCGAN: Generated digits", fontsize=12)<br/>    cnt = 0<br/><br/>    for i in range(row):<br/>        for j in range(col):<br/>            axs[i, j].imshow(gen_imgs[cnt, :, :, :])<br/>            axs[i, j].axis('off')<br/>            cnt += 1<br/><br/>    #plt.close()<br/>    plt.show()</span></pre><p id="b764" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">发生器的参数将受到 GAN 和 VAE 训练的影响。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="015f" class="ol nj iq oh b gy om on l oo op"># note: <!-- -->The parameters of the generator will be affected by both the GAN and VAE training<br/><br/><br/>G, D, GAN = VAEGAN(decgen(5, filter_of_decgen, rows, columns, channel),discriminator(5, filter_of_dis, rows, columns, channel))<br/><br/># encoder<br/>E = encoder(5, filter_of_encoder, rows, columns, channel)<br/>print("This is the summary for encoder:")<br/>E.summary()<br/><br/><br/># generator/decoder<br/># G = decgen(5, filter_of_decgen, rows, columns, channel)<br/>print("This is the summary for dencoder/generator:")<br/>G.summary()<br/><br/><br/># discriminator<br/># D = discriminator(5, filter_of_dis, rows, columns, channel)<br/>print("This is the summary for discriminator:")<br/>D.summary()<br/><br/><br/>D_fixed = discriminator(5, filter_of_dis, rows, columns, channel)<br/>D_fixed.compile(optimizer=SGDop, loss='mse')<br/><br/># gan<br/>print("This is the summary for GAN:")<br/>GAN.summary()<br/><br/># VAE<br/>X = Input(shape=(rows, columns, channel))<br/><br/>E_mean, E_logsigma, Z = E(X)<br/><br/>output = G(Z)<br/># G_dec = G(E_mean + E_logsigma)<br/># D_fake, F_fake = D(output)<br/># D_fromGen, F_fromGen = D(G_dec)<br/># D_true, F_true = D(X)<br/><br/># print("type(E)",type(E))<br/># print("type(output)",type(output))<br/># print("type(D_fake)",type(D_fake))<br/><br/>VAE = Model(X, output)<br/>VAE.add_loss(vae_loss(X, output, E_mean, E_logsigma))<br/>VAE.compile(optimizer=SGDop)<br/><br/>print("This is the summary for vae:")<br/>VAE.summary()</span></pre><p id="5121" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下面的单元格中，我们开始训练我们的模型。注意，我们使用前面的方法来训练鉴频器以及 GAN 和 VAE 不同的时间长度。我们在训练过程的前半部分强调鉴别器的训练，在后半部分我们更多地训练发生器，因为我们想提高输出图像的质量。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="611c" class="ol nj iq oh b gy om on l oo op"># We train our model in this cell<br/><br/>dLoss=[]<br/>gLoss=[]<br/>GLoss = 1<br/>GlossEnc = 1<br/>GlossGen = 1<br/>Eloss = 1<br/><br/>halfbatch_size = int(batch_size*0.5)<br/><br/>for epoch in tqdm(range(epochs)):<br/>    if epoch &lt; int(epochs*0.5):<br/>        noise = np.random.normal(0, 1, (halfbatch_size, latent_dim))<br/>        index = np.random.randint(0,dataset.shape[0], halfbatch_size)<br/>        images = dataset[index]  <br/><br/>        latent_vect = E.predict(images)[0]<br/>        encImg = G.predict(latent_vect)<br/>        fakeImg = G.predict(noise)<br/><br/>        D.Trainable = True<br/>        DlossTrue = D.train_on_batch(images, np.ones((halfbatch_size, 1)))<br/>        DlossEnc = D.train_on_batch(encImg, np.ones((halfbatch_size, 1)))       <br/>        DlossFake = D.train_on_batch(fakeImg, np.zeros((halfbatch_size, 1)))<br/><br/>#         DLoss=np.add(DlossTrue,DlossFake)*0.5<br/>        <br/>        DLoss=np.add(DlossTrue,DlossEnc)<br/>        DLoss=np.add(DLoss,DlossFake)*0.33<br/>        D.Trainable = False<br/><br/>        cnt = epoch<br/><br/>        while cnt &gt; 3:<br/>            cnt = cnt - 4<br/><br/>        if cnt == 0:<br/>            noise = np.random.normal(0, 1, (batch_size, latent_dim))<br/>            index = np.random.randint(0,dataset.shape[0], batch_size)<br/>            images = dataset[index]  <br/>            latent_vect = E.predict(images)[0]     <br/>            <br/>            GlossEnc = GAN.train_on_batch(latent_vect, np.ones((batch_size, 1)))<br/>            GlossGen = GAN.train_on_batch(noise, np.ones((batch_size, 1)))<br/>            Eloss = VAE.train_on_batch(images, None)   <br/>            GLoss=np.add(GlossEnc,GlossGen)<br/>            GLoss=np.add(GLoss,Eloss)*0.33<br/>        dLoss.append([epoch,DLoss[0]]) <br/>        gLoss.append([epoch,GLoss])<br/>    <br/>    elif epoch &gt;= int(epochs*0.5):<br/>        cnt = epoch<br/>        while cnt &gt; 3:<br/>            cnt = cnt - 4<br/><br/>        if cnt == 0:<br/>            noise = np.random.normal(0, 1, (halfbatch_size, latent_dim))<br/>            index = np.random.randint(0,dataset.shape[0], halfbatch_size)<br/>            images = dataset[index]  <br/><br/>            latent_vect = E.predict(images)[0]<br/>            encImg = G.predict(latent_vect)<br/>            fakeImg = G.predict(noise)<br/><br/>            D.Trainable = True<br/>            DlossTrue = D.train_on_batch(images, np.ones((halfbatch_size, 1)))<br/>        #     DlossEnc = D.train_on_batch(encImg, np.ones((halfbatch_size, 1)))       <br/>            DlossFake = D.train_on_batch(fakeImg, np.zeros((halfbatch_size, 1)))<br/><br/>            DLoss=np.add(DlossTrue,DlossFake)*0.5<br/>        <br/>#             DLoss=np.add(DlossTrue,DlossEnc)<br/>#             DLoss=np.add(DLoss,DlossFake)*0.33<br/>            D.Trainable = False<br/><br/>        noise = np.random.normal(0, 1, (batch_size, latent_dim))<br/>        index = np.random.randint(0,dataset.shape[0], batch_size)<br/>        images = dataset[index]  <br/>        latent_vect = E.predict(images)[0]<br/>        <br/>        GlossEnc = GAN.train_on_batch(latent_vect, np.ones((batch_size, 1)))<br/>        GlossGen = GAN.train_on_batch(noise, np.ones((batch_size, 1)))<br/>        Eloss = VAE.train_on_batch(images, None)   <br/>        GLoss=np.add(GlossEnc,GlossGen)<br/>        GLoss=np.add(GLoss,Eloss)*0.33<br/>    <br/>        dLoss.append([epoch,DLoss[0]]) <br/>        gLoss.append([epoch,GLoss])<br/><br/>    if epoch % plotInternal == 0 and epoch!=0:<br/>        show_imgs(G)<br/><br/><br/>    dLossArr= np.array(dLoss)<br/>    gLossArr = np.array(gLoss)<br/>    <br/>#     print("dLossArr.shape:",dLossArr.shape)<br/>#     print("gLossArr.shape:",gLossArr.shape)<br/>    <br/>    chk = epoch<br/><br/>    while chk &gt; 50:<br/>        chk = chk - 51<br/><br/>    if chk == 0:<br/>        D.save_weights('/content/gdrive/My Drive/VAE discriminator_kernalsize5_proportion_32.h5')<br/>        G.save_weights('/content/gdrive/My Drive/VAE generator_kernalsize5_proportion_32.h5')<br/>        E.save_weights('/content/gdrive/My Drive/VAE encoder_kernalsize5_proportion_32.h5')<br/><br/>        <br/>    if epoch%20 == 0:    <br/>        print("epoch:", epoch + 1,"  ", "DislossTrue loss:",DlossTrue[0],"D accuracy：",100* DlossTrue[1], "DlossFake loss:", DlossFake[0],"GlossEnc loss:",<br/>          GlossEnc, "GlossGen loss:",GlossGen, "Eloss loss:",Eloss)<br/>#     print("loss:")<br/>#     print("D:", DlossTrue, DlossEnc, DlossFake)<br/>#     print("G:", GlossEnc, GlossGen)<br/>#     print("VAE:", Eloss)<br/><br/>print('Training done,saving weights')<br/>D.save_weights('/content/gdrive/My Drive/VAE discriminator_kernalsize5_proportion_32.h5')<br/>G.save_weights('/content/gdrive/My Drive/VAE generator_kernalsize5_proportion_32.h5')<br/>E.save_weights('/content/gdrive/My Drive/VAE encoder_kernalsize5_proportion_32.h5')<br/><br/><br/>print('painting losses')<br/># At the end of training plot the losses vs epochs<br/>plt.plot(dLossArr[:, 0], dLossArr[:, 1], label="Discriminator Loss")<br/>plt.plot(gLossArr[:, 0], gLossArr[:, 1], label="Generator Loss")<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.title('GAN')<br/>plt.grid(True)<br/>plt.show()<br/>print('end')</span></pre><p id="3748" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你计划运行这个网络，请注意培训过程需要很长时间。我不会尝试这样做，除非你有一些强大的图形处理器，或者愿意运行一整天的模型。</p><p id="4fd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们的 VAE-GAN 训练已经完成，我们可以检查看看我们的输出图像看起来如何，并与我们以前的 GAN 进行比较。</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="246e" class="ol nj iq oh b gy om on l oo op"># In this cell, we generate and visualize 15 images. <br/><br/>show_imgs(G)</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi po"><img src="../Images/f11fcca1614b9ce1255c0040905a542c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7K0-so0qXvUJ9wN6YY4-lQ.png"/></div></div></figure><p id="9dfb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到，在 VAE-甘的实现中，我们得到了一个很好的模型，它可以生成清晰的图像，并且具有与原始图像相似的风格。我们的 VAE-甘可以创建更健壮的图像，这可以在没有额外的动画脸噪声的情况下完成。然而，我们模型的概括能力不是很好，它很少改变角色的方式或性别，所以这是我们可以尝试改进的一点。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="c5c7" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">最终点评</strong></h1><p id="fdb3" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">不一定清楚哪个模型比其他模型更好，而且这些方法都没有经过适当的优化，因此很难进行比较。</p><p id="1656" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这仍然是一个活跃的研究领域，所以如果你感兴趣，我建议你投入进去，在你自己的工作中尝试使用 GANs，看看你能想出什么。</p><p id="50ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你喜欢这个关于 GANs 的文章三部曲，并且现在对它们是什么、它们能做什么以及如何制作你自己的有了更好的了解。</p><p id="1a6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您的阅读！</p><h2 id="ef59" class="ol nj iq bd nk ot ou dn no ov ow dp ns ko ox oy nu ks oz pa nw kw pb pc ny pd bi translated">时事通讯</h2><p id="0742" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">关于新博客文章和额外内容的更新，请注册我的时事通讯。</p><div class="lc ld gp gr le lf"><a href="https://mailchi.mp/6304809e49e7/matthew-stewart" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">时事通讯订阅</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">丰富您的学术之旅，加入一个由科学家，研究人员和行业专业人士组成的社区，以获得…</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">mailchi.mp</p></div></div><div class="lo l"><div class="pp l lq lr ls lo lt lu lf"/></div></div></a></div><h1 id="385e" class="ni nj iq bd nk nl pq nn no np pr nr ns jw ps jx nu jz pt ka nw kc pu kd ny nz bi translated">进一步阅读</h1><p id="8c00" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated"><strong class="kh ir">在 COLAB 中运行 BigGAN:</strong></p><ul class=""><li id="04e0" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated"><a class="ae lw" href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/github/tensor flow/hub/blob/master/examples/colab/biggan _ generation _ with _ TF _ hub . ipynb</a></li></ul><p id="1d01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">更多代码帮助+示例:</strong></p><ul class=""><li id="11c2" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated"><a class="ae lw" href="https://www.jessicayung.com/explaining-tensorflow-code-for-a-convolutional-neural-network/" rel="noopener ugc nofollow" target="_blank">https://www . jessicayung . com/explaining-tensor flow-code-for-a-卷积神经网络/ </a></li><li id="9eda" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae lw" href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2017/08/20/from-GAN-to-wgan . html</a></li><li id="2a1c" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae lw" href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/初学者/dcgan_faces_tutorial.html </a></li><li id="d205" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae lw" href="https://github.com/tensorlayer/srgan" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorlayer/srgan</a></li><li id="c925" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae lw" href="https://junyanz.github.io/CycleGAN/" rel="noopener ugc nofollow" target="_blank">https://junyanz.github.io/CycleGAN/</a>T14】https://affinelayer.com/pixsrv/</li><li id="674f" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae lw" href="https://tcwang0509.github.io/pix2pixHD/" rel="noopener ugc nofollow" target="_blank">https://tcwang0509.github.io/pix2pixHD/</a></li></ul><p id="4b52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">有影响力的论文:</strong></p><ul class=""><li id="9dab" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated">https://arxiv.org/pdf/1511.06434v2.pdf</li><li id="4ea2" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">瓦瑟斯坦甘<a class="ae lw" href="https://arxiv.org/pdf/1701.07875.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1701.07875.pdf</a></li><li id="9c3f" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">条件生成对抗网(CGAN)<a class="ae lw" href="https://arxiv.org/pdf/1411.1784v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1411.1784v1.pdf</a></li><li id="1477" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">使用对抗网络的拉普拉斯金字塔的深度生成图像模型(拉普根)<a class="ae lw" href="https://arxiv.org/pdf/1506.05751.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.05751.pdf</a></li><li id="ac13" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">使用生成式对抗网络(SRGAN)的照片级单幅图像超分辨率<a class="ae lw" href="https://arxiv.org/pdf/1609.04802.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1609.04802.pdf</a></li><li id="7549" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">使用循环一致对抗网络的不成对图像到图像翻译<a class="ae lw" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1703.10593.pdf</a></li><li id="ec51" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">InfoGAN:通过信息最大化生成对抗网络的可解释表示学习<a class="ae lw" href="https://arxiv.org/pdf/1606.03657" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1606.03657</a></li><li id="0ea1" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">https://arxiv.org/pdf/1704.00028.pdf 的 DCGAN <a class="ae lw" href="https://arxiv.org/pdf/1704.00028.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="2710" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">瓦瑟斯坦·甘斯的强化训练(WGAN-GP)【https://arxiv.org/pdf/1701.07875.pdf T4</li><li id="4fa6" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">基于能量的生成对抗网络(EBGAN)<a class="ae lw" href="https://arxiv.org/pdf/1609.03126.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1609.03126.pdf</a></li><li id="cd63" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">使用学习的相似性度量(VAE-甘)对像素之外的内容进行自动编码<a class="ae lw" href="https://arxiv.org/pdf/1512.09300.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1512.09300.pdf</a></li><li id="795b" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">对抗性特征学习(https://arxiv.org/pdf/1605.09782v6.pdf<a class="ae lw" href="https://arxiv.org/pdf/1605.09782v6.pdf" rel="noopener ugc nofollow" target="_blank">甘比</a></li><li id="8c3f" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">堆叠生成敌对网络(SGAN)<a class="ae lw" href="https://arxiv.org/pdf/1612.04357.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.04357.pdf</a></li><li id="705a" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">StackGAN++使用堆叠式生成对抗网络进行现实图像合成<a class="ae lw" href="https://arxiv.org/pdf/1710.10916.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1710.10916.pdf</a></li><li id="f2d1" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">通过对抗训练(SimGAN)从模拟和无监督图像中学习<a class="ae lw" href="https://arxiv.org/pdf/1612.07828v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.07828v1.pdf</a></li></ul></div></div>    
</body>
</html>