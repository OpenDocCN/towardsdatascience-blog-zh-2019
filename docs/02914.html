<html>
<head>
<title>GANs vs. Autoencoders: Comparison of Deep Generative Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GANs ä¸è‡ªåŠ¨ç¼–ç å™¨:æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„æ¯”è¾ƒ</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/gans-vs-autoencoders-comparison-of-deep-generative-models-985cf15936ea?source=collection_archive---------0-----------------------#2019-05-12">https://towardsdatascience.com/gans-vs-autoencoders-comparison-of-deep-generative-models-985cf15936ea?source=collection_archive---------0-----------------------#2019-05-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fa2c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">æƒ³æŠŠé©¬å˜æˆæ–‘é©¬ï¼Ÿåˆ¶ä½œ DIY åŠ¨æ¼«äººç‰©æˆ–åäººï¼Ÿç”Ÿæˆæ•Œå¯¹ç½‘ç»œæ˜¯ä½ æ–°çš„æœ€å¥½çš„æœ‹å‹ã€‚</h2></div><p id="b714" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ˜¯è¿‡å» 10 å¹´æœºå™¨å­¦ä¹ ä¸­æœ€æœ‰è¶£çš„æƒ³æ³•â€”â€”<strong class="kh ir">æ‰¬Â·å‹’æ‘ï¼Œè„¸ä¹¦äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­å¿ƒä¸»ä»»</strong></p><p id="1893" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æœ¬æ•™ç¨‹çš„ç¬¬ 1 éƒ¨åˆ†å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°:</p><div class="lc ld gp gr le lf"><a rel="noopener follow" target="_blank" href="/comprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">å›¾çµå­¦ä¹ å’Œ GANs ç®€ä»‹</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">æƒ³æŠŠé©¬å˜æˆæ–‘é©¬ï¼Ÿåˆ¶ä½œ DIY åŠ¨æ¼«äººç‰©æˆ–åäººï¼Ÿç”Ÿæˆæ•Œå¯¹ç½‘ç»œæ˜¯â€¦</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">towardsdatascience.com</p></div></div><div class="lo l"><div class="lp l lq lr ls lo lt lu lf"/></div></div></a></div><p id="c18d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æœ¬æ•™ç¨‹çš„ç¬¬ 2 éƒ¨åˆ†å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°:</p><div class="lc ld gp gr le lf"><a rel="noopener follow" target="_blank" href="/comprehensive-introduction-to-turing-learning-and-gans-part-2-fd8e4a70775"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">GANs ä¸­çš„é«˜çº§ä¸»é¢˜</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">æƒ³æŠŠé©¬å˜æˆæ–‘é©¬ï¼Ÿåˆ¶ä½œ DIY åŠ¨æ¼«äººç‰©æˆ–åäººï¼Ÿç”Ÿæˆæ•Œå¯¹ç½‘ç»œæ˜¯â€¦</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">towardsdatascience.com</p></div></div><div class="lo l"><div class="lv l lq lr ls lo lt lu lf"/></div></div></a></div><p id="70ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è¿™äº›æ–‡ç« åŸºäºå“ˆä½›å¤§å­¦å…³äº<a class="ae lw" href="https://harvard-iacs.github.io/2019-CS109B/" rel="noopener ugc nofollow" target="_blank"> AC209b </a>çš„è®²åº§ï¼Œä¸»è¦å½’åŠŸäºå“ˆä½›å¤§å­¦ IACS ç³»çš„è®²å¸ˆ<a class="ae lw" href="https://iacs.seas.harvard.edu/people/pavlos-protopapas" rel="noopener ugc nofollow" target="_blank"> Pavlos Protopapas </a>ã€‚</p><p id="9490" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è¿™æ˜¯ä¸“é—¨ä½¿ç”¨ç”Ÿæˆæ€§å¯¹æŠ—ç½‘ç»œåˆ›å»ºæ·±åº¦ç”Ÿæˆæ¨¡å‹çš„ä¸‰éƒ¨åˆ†æ•™ç¨‹çš„ç¬¬ä¸‰éƒ¨åˆ†ã€‚è¿™æ˜¯ä¸Šä¸€ä¸ªå…³äºå˜å‹è‡ªåŠ¨ç¼–ç å™¨ä¸»é¢˜çš„è‡ªç„¶å»¶ä¼¸(åœ¨è¿™é‡Œæ‰¾åˆ°<a class="ae lw" rel="noopener" target="_blank" href="/generating-images-with-autoencoders-77fd3a8dd368"/>)ã€‚æˆ‘ä»¬å°†çœ‹åˆ°ï¼Œä¸å¯å˜è‡ªåŠ¨ç¼–ç å™¨ç›¸æ¯”ï¼ŒGANs ä½œä¸ºæ·±åº¦ç”Ÿæˆæ¨¡å‹é€šå¸¸æ›´ä¼˜è¶Šã€‚ç„¶è€Œï¼Œä¼—æ‰€å‘¨çŸ¥ï¼Œå®ƒä»¬å¾ˆéš¾ä½¿ç”¨ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„æ•°æ®å’Œè°ƒæ•´ã€‚æˆ‘ä»¬è¿˜å°†ç ”ç©¶ä¸€ç§ç§°ä¸º VAE-GAN çš„æ··åˆ GAN æ¨¡å‹ã€‚</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/d56e801da7bc4f16a949aee0e9631ebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7kpjaMU631RD-CTKG6DyZw.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Taxonomy of deep generative models. This articleâ€™s focus is on GANs.</figcaption></figure><p id="20ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æ•™ç¨‹çš„è¿™ä¸€éƒ¨åˆ†å°†ä¸»è¦æ˜¯å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨(VAEs)çš„ç¼–ç å®ç°ï¼Œä¹Ÿå°†å‘è¯»è€…å±•ç¤ºå¦‚ä½•åˆ¶ä½œ VAEs ç”˜ã€‚</p><ul class=""><li id="8134" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated">CelebA æ•°æ®é›†çš„ VAE</li><li id="ffd3" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">è¥¿é‡Œå·´æ•°æ®é›†çš„ DC-ç”˜</li><li id="be70" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">åŠ¨æ¼«æ•°æ®é›† DC-ç”˜</li><li id="c7f0" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">åŠ¨æ¼«æ•°æ®é›† VAE-ç”˜</li></ul><p id="1482" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘å¼ºçƒˆå»ºè®®è¯»è€…åœ¨è¿›ä¸€æ­¥é˜…è¯»ä¹‹å‰ï¼Œè‡³å°‘å…ˆé˜…è¯»ä¸€ä¸‹ GAN æ•™ç¨‹çš„ç¬¬ 1 éƒ¨åˆ†ï¼Œä»¥åŠæˆ‘çš„è‡ªåŠ¨ç¼–ç å™¨å˜åŒ–æ¼”ç»ƒï¼Œå¦åˆ™ï¼Œè¯»è€…å¯èƒ½å¯¹å®ç°æ²¡æœ‰å¤ªå¤šçš„äº†è§£ã€‚</p><p id="28e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æ‰€æœ‰ç›¸å…³ä»£ç ç°åœ¨éƒ½å¯ä»¥åœ¨æˆ‘çš„ GitHub å­˜å‚¨åº“ä¸­æ‰¾åˆ°:</p><div class="lc ld gp gr le lf"><a href="https://github.com/mrdragonbear/GAN-Tutorial" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">é¾™ç†Šå…ˆç”Ÿ/ç”˜-æ•™ç¨‹</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">GitHub æ˜¯è¶…è¿‡ 5000 ä¸‡å¼€å‘äººå‘˜çš„å®¶å›­ï¼Œä»–ä»¬ä¸€èµ·å·¥ä½œæ¥æ‰˜ç®¡å’Œå®¡æŸ¥ä»£ç ã€ç®¡ç†é¡¹ç›®å’Œæ„å»ºâ€¦</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">github.com</p></div></div><div class="lo l"><div class="na l lq lr ls lo lt lu lf"/></div></div></a></div><p id="7edb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬å¼€å§‹å§ï¼</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="a22e" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">CelebA æ•°æ®é›†çš„ VAE</strong></h1><p id="2d97" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">CelebFaces å±æ€§æ•°æ®é›†(CelebA)æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„äººè„¸å±æ€§æ•°æ®é›†ï¼Œæ‹¥æœ‰è¶…è¿‡ 20 ä¸‡å¼ åäººå›¾åƒï¼Œæ¯å¼ å›¾åƒéƒ½æœ‰ 40 ä¸ªå±æ€§æ³¨é‡Šã€‚è¯¥æ•°æ®é›†ä¸­çš„å›¾åƒè¦†ç›–äº†è¾ƒå¤§çš„å§¿æ€å˜åŒ–å’ŒèƒŒæ™¯æ··ä¹±ã€‚è¥¿é‡Œå·´æœ‰å¾ˆå¤§çš„å¤šæ ·æ€§ï¼Œæ•°é‡å¤§ï¼Œæ³¨é‡Šä¸°å¯Œï¼ŒåŒ…æ‹¬</p><ul class=""><li id="ca23" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated">10177 ä¸ªèº«ä»½ï¼Œ</li><li id="be4a" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">202ï¼Œ599 ä¸ªé¢éƒ¨å›¾åƒï¼Œä»¥åŠ</li><li id="fbe7" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">5 ä¸ªæ ‡å¿—ä½ç½®ï¼Œæ¯å¹…å›¾åƒ 40 ä¸ªäºŒå…ƒå±æ€§æ³¨é‡Šã€‚</li></ul><p id="4c38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ä½ å¯ä»¥ä» Kaggle è¿™é‡Œä¸‹è½½æ•°æ®é›†:</p><div class="lc ld gp gr le lf"><a href="https://www.kaggle.com/jessicali9530/celeba-dataset" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">åäººé¢å­”å±æ€§(CelebA)æ•°æ®é›†</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">è¶…è¿‡ 200ï¼Œ000 å¼ åäººå›¾ç‰‡ï¼Œå¸¦æœ‰ 40 ä¸ªäºŒå…ƒå±æ€§æ³¨é‡Š</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">www.kaggle.com</p></div></div><div class="lo l"><div class="of l lq lr ls lo lt lu lf"/></div></div></a></div><p id="f513" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç¬¬ä¸€æ­¥æ˜¯å¯¼å…¥æ‰€æœ‰å¿…è¦çš„å‡½æ•°å¹¶æå–æ•°æ®ã€‚</p><p id="7bfd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">è¿›å£</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="b22f" class="ol nj iq oh b gy om on l oo op">import shutil<br/>import errno<br/>import zipfile<br/>import os<br/>import matplotlib.pyplot as plt</span></pre><p id="b297" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">æå–æ•°æ®</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="aa0c" class="ol nj iq oh b gy om on l oo op"># Only run once to unzip images<br/>zip_ref = zipfile.ZipFile('img_align_celeba.zip','r')<br/>zip_ref.extractall()<br/>zip_ref.close()</span></pre><p id="3cea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">è‡ªå®šä¹‰å›¾åƒç”Ÿæˆå™¨</strong></p><p id="7219" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è¿™ä¸€æ­¥å¯èƒ½æ˜¯å¤§å¤šæ•°è¯»è€…ä»¥å‰æ²¡æœ‰ç”¨è¿‡çš„ã€‚ç”±äºæˆ‘ä»¬çš„æ•°æ®éå¸¸åºå¤§ï¼Œå¯èƒ½æ— æ³•å°†æ•°æ®é›†åŠ è½½åˆ° Jupyter ç¬”è®°æœ¬çš„å†…å­˜ä¸­ã€‚åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆæ­£å¸¸çš„é—®é¢˜ã€‚</p><p id="13fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è§£å†³è¿™ä¸€é—®é¢˜çš„æ–¹æ³•æ˜¯ä½¿ç”¨æµç”Ÿæˆå™¨ï¼Œå®ƒå°†æˆæ‰¹çš„æ•°æ®(æœ¬ä¾‹ä¸­æ˜¯å›¾åƒ)æŒ‰é¡ºåºæµå…¥å†…å­˜ï¼Œä»è€Œé™åˆ¶è¯¥å‡½æ•°æ‰€éœ€çš„å†…å­˜é‡ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç†è§£å’Œç¼–å†™å®ƒä»¬æœ‰ç‚¹å¤æ‚ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¯¹è®¡ç®—æœºå†…å­˜ã€GPU æ¶æ„ç­‰æœ‰åˆç†çš„ç†è§£ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="1930" class="ol nj iq oh b gy om on l oo op"># data generator<br/># source from https://medium.com/@ensembledme/writing-custom-keras-generators-fe815d992c5a<br/>from skimage.io import imread<br/><br/>def get_input(path):<br/>    """get specific image from path"""<br/>    img = imread(path)<br/>    return img<br/><br/>def get_output(path, label_file = None):<br/>    """get all the labels relative to the image of path"""<br/>    img_id = path.split('/')[-1]<br/>    labels = label_file.loc[img_id].values<br/>    return labels<br/><br/>def preprocess_input(img):<br/>    # convert between 0 and 1<br/>    return img.astype('float32') / 127.5 -1<br/><br/>def image_generator(files, label_file, batch_size = 32):<br/>    while True:<br/><br/>        batch_paths = np.random.choice(a = files, size = batch_size)<br/>        batch_input = []<br/>        batch_output = []<br/><br/>        for input_path in batch_paths:<br/><br/>            input = get_input(input_path)<br/>            input = preprocess_input(input)<br/>            output = get_output(input_path, label_file = label_file)<br/>            batch_input += [input]<br/>            batch_output += [output]<br/>        batch_x = np.array(batch_input)<br/>        batch_y = np.array(batch_output)<br/><br/>        yield batch_x, batch_y<br/><br/>def auto_encoder_generator(files, batch_size = 32):<br/>    while True:<br/>        batch_paths = np.random.choice(a = files, size = batch_size)<br/>        batch_input = []<br/>        batch_output = []<br/><br/>        for input_path in batch_paths:<br/>            input = get_input(input_path)<br/>            input = preprocess_input(input)<br/>            output = input<br/>            batch_input += [input]<br/>            batch_output += [output]<br/>        batch_x = np.array(batch_input)<br/>        batch_y = np.array(batch_output)<br/><br/>        yield batch_x, batch_y</span></pre><p id="1d5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">å…³äºç”¨ Keras ç¼–å†™å®šåˆ¶ç”Ÿæˆå™¨çš„æ›´å¤šä¿¡æ¯ï¼Œæˆ‘åœ¨ä¸Šé¢çš„ä»£ç ä¸­å¼•ç”¨äº†ä¸€ç¯‡å¾ˆå¥½çš„æ–‡ç« :</p><div class="lc ld gp gr le lf"><a href="https://medium.com/@ensembledme/writing-custom-keras-generators-fe815d992c5a" rel="noopener follow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">ç¼–å†™å®šåˆ¶çš„ Keras ç”Ÿæˆå™¨</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">ä½¿ç”¨ Keras ç”Ÿæˆå™¨èƒŒåçš„æƒ³æ³•æ˜¯åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€åœ°è·å¾—æˆæ‰¹çš„è¾“å…¥å’Œç›¸åº”çš„è¾“å‡ºâ€¦</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">medium.com</p></div></div></div></a></div><p id="c062" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">åŠ è½½å±æ€§æ•°æ®</strong></p><p id="e185" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬ä¸ä»…æœ‰è¿™ä¸ªæ•°æ®é›†çš„å›¾åƒï¼Œè€Œä¸”æ¯ä¸ªå›¾åƒè¿˜æœ‰ä¸€ä¸ªä¸åäººçš„å„ä¸ªæ–¹é¢ç›¸å¯¹åº”çš„å±æ€§åˆ—è¡¨ã€‚ä¾‹å¦‚ï¼Œæœ‰æè¿°åäººæ˜¯å¦æ¶‚å£çº¢æˆ–æˆ´å¸½å­ã€ä»–ä»¬æ˜¯å¦å¹´è½»ã€ä»–ä»¬æ˜¯å¦æœ‰é»‘å¤´å‘ç­‰çš„å±æ€§ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="ea36" class="ol nj iq oh b gy om on l oo op"># now load attribute<br/><br/># 1.A.2<br/>import pandas as pd<br/>attr = pd.read_csv('list_attr_celeba.csv')<br/>attr = attr.set_index('image_id')<br/><br/># check if attribute successful loaded<br/>attr.describe()</span></pre><p id="bbe5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">å®Œæˆå‘ç”µæœºçš„åˆ¶ä½œ</strong></p><p id="de7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç°åœ¨æˆ‘ä»¬å®Œæˆäº†å‘ç”µæœºçš„åˆ¶ä½œã€‚æˆ‘ä»¬å°†å›¾åƒåç§°é•¿åº¦è®¾ç½®ä¸º 6ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ä¸­æœ‰ 6 ä½æ•°çš„å›¾åƒã€‚é˜…è¯»å®šåˆ¶ Keras ç”Ÿæˆå™¨æ–‡ç« åï¼Œè¿™éƒ¨åˆ†ä»£ç åº”è¯¥æœ‰æ„ä¹‰ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="a96b" class="ol nj iq oh b gy om on l oo op">import numpy as np<br/>from sklearn.model_selection import train_test_split</span><span id="2be5" class="ol nj iq oh b gy oq on l oo op">IMG_NAME_LENGTH = 6</span><span id="461a" class="ol nj iq oh b gy oq on l oo op">file_path = "img_align_celeba/"<br/>img_id = np.arange(1,len(attr.index)+1)<br/>img_path = []<br/>for i in range(len(img_id)):<br/>    img_path.append(file_path + (IMG_NAME_LENGTH - len(str(img_id[i])))*'0' + str(img_id[i]) + '.jpg')</span><span id="e029" class="ol nj iq oh b gy oq on l oo op"># pick 80% as training set and 20% as validation set<br/>train_path = img_path[:int((0.8)*len(img_path))]<br/>val_path = img_path[int((0.8)*len(img_path)):]</span><span id="b903" class="ol nj iq oh b gy oq on l oo op">train_generator = auto_encoder_generator(train_path,32)<br/>val_generator = auto_encoder_generator(val_path,32)</span></pre><p id="88fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬ç°åœ¨å¯ä»¥é€‰æ‹©ä¸‰ä¸ªå›¾åƒï¼Œå¹¶æ£€æŸ¥å±æ€§æ˜¯å¦æœ‰æ„ä¹‰ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="3b25" class="ol nj iq oh b gy om on l oo op">fig, ax = plt.subplots(1, 3, figsize=(12, 4))<br/>for i in range(3):    <br/>    ax[i].imshow(get_input(img_path[i]))<br/>    ax[i].axis('off')<br/>    ax[i].set_title(img_path[i][-10:])<br/>plt.show()<br/>    <br/>attr.iloc[:3]</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi or"><img src="../Images/0ff784df069b971d1ee23e65aff9b6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SgO5O96CtDc5t3yjDqEPMw.png"/></div></div></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi os"><img src="../Images/ae2752fa4f82046cf95dd17c2cce0c2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UFRaWY7cuyF-yXpuwkO2Iw.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Three random images along with some of their attributes.</figcaption></figure><h2 id="fe07" class="ol nj iq bd nk ot ou dn no ov ow dp ns ko ox oy nu ks oz pa nw kw pb pc ny pd bi translated">å»ºç«‹å’Œè®­ç»ƒä¸€ä¸ª VAE æ¨¡å‹</h2><p id="e705" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¸ºåäººé¢å­”æ•°æ®é›†åˆ›å»ºå¹¶ç¼–è¯‘ä¸€ä¸ªå·ç§¯ VAE æ¨¡å‹(åŒ…æ‹¬ç¼–ç å™¨å’Œè§£ç å™¨)ã€‚</p><p id="83d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">æ›´å¤šè¿›å£å•†å“</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="bc50" class="ol nj iq oh b gy om on l oo op">from keras.models import Sequential, Model<br/>from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D, Input, Reshape, UpSampling2D, InputLayer, Lambda, ZeroPadding2D, Cropping2D, Conv2DTranspose, BatchNormalization<br/>from keras.utils import np_utils, to_categorical<br/>from keras.losses import binary_crossentropy<br/>from keras import backend as K,objectives<br/>from keras.losses import mse, binary_crossentropy</span></pre><p id="4b10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">æ¨¡å‹æ¶æ„</strong></p><p id="6678" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºå¹¶æ€»ç»“è¯¥æ¨¡å‹ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="2fb8" class="ol nj iq oh b gy om on l oo op">b_size = 128<br/>n_size = 512<br/>def sampling(args):<br/>    z_mean, z_log_sigma = args<br/>    epsilon = K.random_normal(shape = (n_size,) , mean = 0, stddev = 1)<br/>    return z_mean + K.exp(z_log_sigma/2) * epsilon<br/>  <br/>def build_conv_vae(input_shape, bottleneck_size, sampling, batch_size = 32):<br/>    <br/>    # ENCODER<br/>    input = Input(shape=(input_shape[0],input_shape[1],input_shape[2]))<br/>    x = Conv2D(32,(3,3),activation = 'relu', padding = 'same')(input)    <br/>    x = BatchNormalization()(x)<br/>    x = MaxPooling2D((2,2), padding ='same')(x)<br/>    x = Conv2D(64,(3,3),activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = MaxPooling2D((2,2), padding ='same')(x)<br/>    x = Conv2D(128,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = MaxPooling2D((2,2), padding ='same')(x)<br/>    x = Conv2D(256,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = MaxPooling2D((2,2), padding ='same')(x)<br/>    <br/>    # Latent Variable Calculation<br/>    shape = K.int_shape(x)<br/>    flatten_1 = Flatten()(x)<br/>    dense_1 = Dense(bottleneck_size, name='z_mean')(flatten_1)<br/>    z_mean = BatchNormalization()(dense_1)<br/>    flatten_2 = Flatten()(x)<br/>    dense_2 = Dense(bottleneck_size, name ='z_log_sigma')(flatten_2)<br/>    z_log_sigma = BatchNormalization()(dense_2)<br/>    z = Lambda(sampling)([z_mean, z_log_sigma])<br/>    encoder = Model(input, [z_mean, z_log_sigma, z], name = 'encoder')<br/>    <br/>    # DECODER<br/>    latent_input = Input(shape=(bottleneck_size,), name = 'decoder_input')<br/>    x = Dense(shape[1]*shape[2]*shape[3])(latent_input)<br/>    x = Reshape((shape[1],shape[2],shape[3]))(x)<br/>    x = UpSampling2D((2,2))(x)<br/>    x = Cropping2D([[0,0],[0,1]])(x)<br/>    x = Conv2DTranspose(256,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = UpSampling2D((2,2))(x)<br/>    x = Cropping2D([[0,1],[0,1]])(x)<br/>    x = Conv2DTranspose(128,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = UpSampling2D((2,2))(x)<br/>    x = Cropping2D([[0,1],[0,1]])(x)<br/>    x = Conv2DTranspose(64,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    x = UpSampling2D((2,2))(x)<br/>    x = Conv2DTranspose(32,(3,3), activation = 'relu', padding = 'same')(x)<br/>    x = BatchNormalization()(x)<br/>    output = Conv2DTranspose(3,(3,3), activation = 'tanh', padding ='same')(x)<br/>    decoder = Model(latent_input, output, name = 'decoder')<br/><br/>    output_2 = decoder(encoder(input)[2])<br/>    vae = Model(input, output_2, name ='vae')<br/>    return vae, encoder, decoder, z_mean, z_log_sigma<br/><br/>vae_2, encoder, decoder, z_mean, z_log_sigma = build_conv_vae(img_sample.shape, n_size, sampling, batch_size = b_size)<br/>print("encoder summary:")<br/>encoder.summary()<br/>print("decoder summary:")<br/>decoder.summary()<br/>print("vae summary:")<br/>vae_2.summary()</span></pre><p id="d1de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">å®šä¹‰ VAE æŸå¤±</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="92c0" class="ol nj iq oh b gy om on l oo op">def vae_loss(input_img, output):<br/>    # Compute error in reconstruction<br/>    reconstruction_loss = mse(K.flatten(input_img) , K.flatten(output))<br/>    <br/>    # Compute the KL Divergence regularization term<br/>    kl_loss = - 0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis = -1)<br/>    <br/>    # Return the average loss over all images in batch<br/>    total_loss = (reconstruction_loss + 0.0001 * kl_loss)    <br/>    return total_loss</span></pre><p id="bb47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">ç¼–è¯‘æ¨¡å‹</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="1759" class="ol nj iq oh b gy om on l oo op">vae_2.compile(optimizer='rmsprop', loss= vae_loss)<br/>encoder.compile(optimizer = 'rmsprop', loss = vae_loss)<br/>decoder.compile(optimizer = 'rmsprop', loss = vae_loss)</span></pre><p id="040d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">è®­ç»ƒæ¨¡å‹</strong></p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="14ad" class="ol nj iq oh b gy om on l oo op">vae_2.fit_generator(train_generator, steps_per_epoch = 4000, validation_data = val_generator, epochs=7, validation_steps= 500)</span></pre><p id="197b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬éšæœºé€‰æ‹©è®­ç»ƒé›†çš„ä¸€äº›å›¾åƒï¼Œé€šè¿‡ç¼–ç å™¨è¿è¡Œå®ƒä»¬ä»¥å‚æ•°åŒ–æ½œåœ¨ä»£ç ï¼Œç„¶åç”¨è§£ç å™¨é‡å»ºå›¾åƒã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="e7cd" class="ol nj iq oh b gy om on l oo op">import random<br/>x_test = []<br/>for i in range(64):<br/>    x_test.append(get_input(img_path[random.randint(0,len(img_id))]))<br/>x_test = np.array(x_test)<br/>figure_Decoded = vae_2.predict(x_test.astype('float32')/127.5 -1, batch_size = b_size)<br/>figure_original = x_test[0]<br/>figure_decoded = (figure_Decoded[0]+1)/2<br/>for i in range(4):<br/>    plt.axis('off')<br/>    plt.subplot(2,4,1+i*2)<br/>    plt.imshow(x_test[i])<br/>    plt.axis('off')<br/>    plt.subplot(2,4,2 + i*2)<br/>    plt.imshow((figure_Decoded[i]+1)/2)<br/>    plt.axis('off')<br/>plt.show()</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pe"><img src="../Images/ca91e0117abe9411a4f86b1c5fc1149d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iCYWlSr4t5OeddcDX4XYjg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Random samples from training set compared to their VAE reconstruction.</figcaption></figure><p id="26d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è¯·æ³¨æ„ï¼Œé‡å»ºçš„å›¾åƒä¸åŸå§‹ç‰ˆæœ¬æœ‰ç›¸ä¼¼ä¹‹å¤„ã€‚ç„¶è€Œï¼Œæ–°çš„å›¾åƒæœ‰ç‚¹æ¨¡ç³Šï¼Œè¿™æ˜¯ä¸€ä¸ªå·²çŸ¥çš„ VAEs ç°è±¡ã€‚è¿™è¢«å‡è®¾æ˜¯ç”±äºå˜åˆ†æ¨ç†ä¼˜åŒ–äº†ä¼¼ç„¶æ€§çš„ä¸‹é™ï¼Œè€Œä¸æ˜¯å®é™…çš„ä¼¼ç„¶æ€§æœ¬èº«ã€‚</p><p id="4913" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">æ½œåœ¨ç©ºé—´è¡¨å¾</strong></p><p id="e399" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸¤ä¸ªä¸åŒå±æ€§çš„å›¾åƒï¼Œå¹¶ç»˜åˆ¶å®ƒä»¬çš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ½œåœ¨ä»£ç ä¹‹é—´çš„ä¸€äº›å·®å¼‚ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾è§£é‡ŠåŸå§‹å›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="7bf8" class="ol nj iq oh b gy om on l oo op"># Choose two images of different attributes, and plot the original and latent space of it<br/><br/>x_test1 = []<br/>for i in range(64):<br/>    x_test1.append(get_input(img_path[np.random.randint(0,len(img_id))]))<br/>x_test1 = np.array(x_test)<br/>x_test_encoded = np.array(encoder.predict(x_test1/127.5-1, batch_size = b_size))<br/>figure_original_1 = x_test[0]<br/>figure_original_2 = x_test[1]<br/>Encoded1 = (x_test_encoded[0,0,:].reshape(32, 16,)+1)/2 <br/>Encoded2 = (x_test_encoded[0,1,:].reshape(32, 16)+1)/2<br/><br/>plt.figure(figsize=(8, 8))<br/>plt.subplot(2,2,1)<br/>plt.imshow(figure_original_1)<br/>plt.subplot(2,2,2)<br/>plt.imshow(Encoded1)<br/>plt.subplot(2,2,3)<br/>plt.imshow(figure_original_2)<br/>plt.subplot(2,2,4)<br/>plt.imshow(Encoded2)<br/>plt.show()</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pf"><img src="../Images/b674b3cf02713a90011374658f302925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eUiTW9oV9R9-Alt9P0-0A.png"/></div></div></figure><p id="f36e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">ä»æ½œç©ºé—´å–æ ·</strong></p><p id="3585" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬å¯ä»¥éšæœºæŠ½å– 15 ä¸ªæ½œåœ¨ä»£ç ï¼Œè§£ç åç”Ÿæˆæ–°çš„åäººé¢å­”ã€‚æˆ‘ä»¬å¯ä»¥ä»è¿™ä¸ªè¡¨ç¤ºä¸­çœ‹åˆ°ï¼Œç”±æˆ‘ä»¬çš„æ¨¡å‹ç”Ÿæˆçš„å›¾åƒä¸æˆ‘ä»¬è®­ç»ƒé›†ä¸­çš„é‚£äº›å›¾åƒå…·æœ‰éå¸¸ç›¸ä¼¼çš„é£æ ¼ï¼Œå¹¶ä¸”å®ƒä¹Ÿå…·æœ‰è‰¯å¥½çš„çœŸå®æ€§å’Œå˜åŒ–æ€§ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="5f24" class="ol nj iq oh b gy om on l oo op"># We randomly generated 15 images from 15 series of noise information</span><span id="9d7f" class="ol nj iq oh b gy oq on l oo op">n = 3<br/>m = 5<br/>digit_size1 = 218<br/>digit_size2 = 178<br/>figure = np.zeros((digit_size1 * n, digit_size2 * m,3))<br/> <br/>for i in range(3):<br/>    for j in range(5):<br/>        z_sample = np.random.rand(1,512)<br/>        x_decoded = decoder.predict([z_sample])<br/>        figure[i * digit_size1: (i + 1) * digit_size1,<br/>               j * digit_size2: (j + 1) * digit_size2,:] = (x_decoded[0]+1)/2 </span><span id="296f" class="ol nj iq oh b gy oq on l oo op">plt.figure(figsize=(10, 10))<br/>plt.imshow(figure)<br/>plt.show()</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pg"><img src="../Images/5437be16ebd1db85f35a0228ed23b262.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0cLHCr5-sTbCn-xzKxhTA.png"/></div></div></figure><p id="5069" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æ‰€ä»¥çœ‹èµ·æ¥æˆ‘ä»¬çš„ VAE æ¨¡å¼å¹¶ä¸æ˜¯ç‰¹åˆ«å¥½ã€‚å¦‚æœæœ‰æ›´å¤šçš„æ—¶é—´å’Œæ›´å¥½åœ°é€‰æ‹©è¶…å‚æ•°ç­‰ç­‰ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå–å¾—æ¯”è¿™æ›´å¥½çš„ç»“æœã€‚</p><p id="4ece" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç°åœ¨è®©æˆ‘ä»¬å°†è¿™ä¸ªç»“æœä¸ç›¸åŒæ•°æ®é›†ä¸Šçš„ DC-ç”˜è¿›è¡Œæ¯”è¾ƒã€‚</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="c4cc" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">CelebA æ•°æ®é›†ä¸Šçš„ DC-ç”˜</strong></h1><p id="9206" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">å› ä¸ºæˆ‘ä»¬å·²ç»è®¾ç½®äº†æµç”Ÿæˆå™¨ï¼Œæ‰€ä»¥æ²¡æœ‰å¤ªå¤šçš„å·¥ä½œè¦åšæ¥å¯åŠ¨å’Œè¿è¡Œ DC-ç”˜æ¨¡å‹ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="fd78" class="ol nj iq oh b gy om on l oo op"># Create and compile a DC-GAN model, and print the summary<br/><br/>from keras.utils import np_utils<br/>from keras.models import Sequential, Model<br/>from keras.layers import Input, Dense, Dropout, Activation, Flatten, LeakyReLU,\<br/>      BatchNormalization, Conv2DTranspose, Conv2D, Reshape<br/>from keras.layers.advanced_activations import LeakyReLU<br/>from keras.optimizers import Adam, RMSprop<br/>from keras.initializers import RandomNormal<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import random<br/>from tqdm import tqdm_notebook<br/>from scipy.misc import imresize<br/><br/>def generator_model(latent_dim=100, leaky_alpha=0.2, init_stddev=0.02):<br/><br/>    g = Sequential()<br/>    g.add(Dense(4*4*512, input_shape=(latent_dim,),<br/>                kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    g.add(Reshape(target_shape=(4, 4, 512)))<br/>    g.add(BatchNormalization())<br/>    g.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    g.add(Conv2DTranspose(256, kernel_size=5, strides=2, padding='same',<br/>                kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    g.add(BatchNormalization())<br/>    g.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    g.add(Conv2DTranspose(128, kernel_size=5, strides=2, padding='same', <br/>                kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    g.add(BatchNormalization())<br/>    g.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    g.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', <br/>                kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    g.add(Activation('tanh'))<br/>    g.summary()<br/>    #g.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])<br/>    return g<br/><br/>  <br/>def discriminator_model(leaky_alpha=0.2, init_stddev=0.02):<br/>    <br/>    d = Sequential()<br/>    d.add(Conv2D(64, kernel_size=5, strides=2, padding='same', <br/>               kernel_initializer=RandomNormal(stddev=init_stddev),<br/>               input_shape=(32, 32, 3)))<br/>    d.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    d.add(Conv2D(128, kernel_size=5, strides=2, padding='same', <br/>               kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    d.add(BatchNormalization())<br/>    d.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    d.add(Conv2D(256, kernel_size=5, strides=2, padding='same', <br/>               kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    d.add(BatchNormalization())<br/>    d.add(Activation(LeakyReLU(alpha=leaky_alpha)))<br/>    d.add(Flatten())<br/>    d.add(Dense(1, kernel_initializer=RandomNormal(stddev=init_stddev)))<br/>    d.add(Activation('sigmoid'))<br/>    d.summary()<br/>    return d<br/><br/>def DCGAN(sample_size=100):<br/>    # Generator<br/>    g = generator_model(sample_size, 0.2, 0.02)<br/><br/>    # Discriminator<br/>    d = discriminator_model(0.2, 0.02)<br/>    d.compile(optimizer=Adam(lr=0.001, beta_1=0.5), loss='binary_crossentropy')<br/>    d.trainable = False<br/>    # GAN<br/>    gan = Sequential([g, d])<br/>    gan.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')<br/>    <br/>    return gan, g, d</span></pre><p id="5d60" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ä»¥ä¸Šä»£ç åªæ˜¯é’ˆå¯¹å‘ç”Ÿå™¨å’Œé‰´åˆ«å™¨ç½‘ç»œçš„æ¶æ„ã€‚å°†è¿™ç§ç¼–ç  GAN çš„æ–¹æ³•ä¸æˆ‘åœ¨ç¬¬ 2 éƒ¨åˆ†ä¸­ä½¿ç”¨çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒæ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°è¿™ç§æ–¹æ³•ä¸å¤ªæ¸…æ™°ï¼Œå¹¶ä¸”æˆ‘ä»¬æ²¡æœ‰å®šä¹‰å…¨å±€å‚æ•°ï¼Œå› æ­¤æœ‰è®¸å¤šåœ°æ–¹æˆ‘ä»¬å¯èƒ½ä¼šæœ‰æ½œåœ¨çš„é”™è¯¯ã€‚</p><p id="267d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€äº›å‡½æ•°æ¥ç®€åŒ–æˆ‘ä»¬çš„å·¥ä½œï¼Œè¿™äº›å‡½æ•°ä¸»è¦ç”¨äºå›¾åƒçš„é¢„å¤„ç†å’Œç»˜å›¾ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬åˆ†æç½‘ç»œè¾“å‡ºã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="39c5" class="ol nj iq oh b gy om on l oo op">def load_image(filename, size=(32, 32)):<br/>    img = plt.imread(filename)<br/>    # crop<br/>    rows, cols = img.shape[:2]<br/>    crop_r, crop_c = 150, 150<br/>    start_row, start_col = (rows - crop_r) // 2, (cols - crop_c) // 2<br/>    end_row, end_col = rows - start_row, cols - start_row<br/>    img = img[start_row:end_row, start_col:end_col, :]<br/>    # resize<br/>    img = imresize(img, size)<br/>    return img<br/><br/>def preprocess(x):<br/>    return (x/255)*2-1<br/><br/>def deprocess(x):<br/>    return np.uint8((x+1)/2*255)<br/><br/>def make_labels(size):<br/>    return np.ones([size, 1]), np.zeros([size, 1])  <br/><br/>def show_losses(losses):<br/>    losses = np.array(losses)<br/>    <br/>    fig, ax = plt.subplots()<br/>    plt.plot(losses.T[0], label='Discriminator')<br/>    plt.plot(losses.T[1], label='Generator')<br/>    plt.title("Validation Losses")<br/>    plt.legend()<br/>    plt.show()<br/><br/>def show_images(generated_images):<br/>    n_images = len(generated_images)<br/>    cols = 5<br/>    rows = n_images//cols<br/>    <br/>    plt.figure(figsize=(8, 6))<br/>    for i in range(n_images):<br/>        img = deprocess(generated_images[i])<br/>        ax = plt.subplot(rows, cols, i+1)<br/>        plt.imshow(img)<br/>        plt.xticks([])<br/>        plt.yticks([])<br/>    plt.tight_layout()<br/>    plt.show()</span></pre><p id="65ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">è®­ç»ƒæ¨¡å‹</strong></p><p id="d80e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬ç°åœ¨å®šä¹‰è®­ç»ƒå‡½æ•°ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„ï¼Œè¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨å°†é‰´åˆ«å™¨è®¾ç½®ä¸ºå¯è®­ç»ƒå’Œä¸å¯è®­ç»ƒä¹‹é—´è¿›è¡Œäº†åˆ‡æ¢(æˆ‘ä»¬åœ¨ç¬¬ 2 éƒ¨åˆ†ä¸­éšå¼åœ°è¿™æ ·åšäº†)ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="d1a7" class="ol nj iq oh b gy om on l oo op">def train(sample_size=100, epochs=3, batch_size=128, eval_size=16, smooth=0.1):</span><span id="8155" class="ol nj iq oh b gy oq on l oo op">    batchCount=len(train_path)//batch_size<br/>    y_train_real, y_train_fake = make_labels(batch_size)<br/>    y_eval_real,  y_eval_fake  = make_labels(eval_size)<br/>    <br/>    # create a GAN, a generator and a discriminator<br/>    gan, g, d = DCGAN(sample_size)<br/>    <br/>    losses = []</span><span id="8b7b" class="ol nj iq oh b gy oq on l oo op">    for e in range(epochs):<br/>        print('-'*15, 'Epoch %d' % (e+1), '-'*15)<br/>        for i in tqdm_notebook(range(batchCount)):<br/>            <br/>            path_batch = train_path[i*batch_size:(i+1)*batch_size]<br/>            image_batch = np.array([preprocess(load_image(filename)) for filename in path_batch])<br/>            <br/>            noise = np.random.normal(0, 1, size=(batch_size, noise_dim))<br/>            generated_images = g.predict_on_batch(noise)</span><span id="1c31" class="ol nj iq oh b gy oq on l oo op">            # Train discriminator on generated images<br/>            d.trainable = True<br/>            d.train_on_batch(image_batch, y_train_real*(1-smooth))<br/>            d.train_on_batch(generated_images, y_train_fake)</span><span id="e84d" class="ol nj iq oh b gy oq on l oo op">            # Train generator<br/>            d.trainable = False<br/>            g_loss=gan.train_on_batch(noise, y_train_real)<br/>        <br/>        # evaluate<br/>        test_path = np.array(val_path)[np.random.choice(len(val_path), eval_size, replace=False)]<br/>        x_eval_real = np.array([preprocess(load_image(filename)) for filename in test_path])</span><span id="b372" class="ol nj iq oh b gy oq on l oo op">        noise = np.random.normal(loc=0, scale=1, size=(eval_size, sample_size))<br/>        x_eval_fake = g.predict_on_batch(noise)<br/>        <br/>        d_loss  = d.test_on_batch(x_eval_real, y_eval_real)<br/>        d_loss += d.test_on_batch(x_eval_fake, y_eval_fake)<br/>        g_loss  = gan.test_on_batch(noise, y_eval_real)<br/>        <br/>        losses.append((d_loss/2, g_loss))<br/>  <br/>        print("Epoch: {:&gt;3}/{} Discriminator Loss: {:&gt;6.4f} Generator Loss: {:&gt;6.4f}".format(<br/>            e+1, epochs, d_loss, g_loss))  <br/>        <br/>        show_images(x_eval_fake[:10])<br/>    <br/>    # show the result<br/>    show_losses(losses)<br/>    show_images(g.predict(np.random.normal(loc=0, scale=1, size=(15, sample_size))))    <br/>    return g</span><span id="d493" class="ol nj iq oh b gy oq on l oo op">noise_dim=100<br/>train()</span></pre><p id="56b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è¯¥å‡½æ•°çš„è¾“å‡ºå°†ä¸ºæˆ‘ä»¬æä¾›æ¯ä¸ªæ—¶æœŸçš„ä»¥ä¸‹è¾“å‡º:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ph"><img src="../Images/f9d52f41c44602678485d737545c40ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gYznU_5Po1WXjqvRT8T1eQ.png"/></div></div></figure><p id="f992" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">å®ƒè¿˜å°†ç»˜åˆ¶é‰´åˆ«å™¨å’Œå‘ç”Ÿå™¨çš„éªŒè¯æŸå¤±ã€‚</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pi"><img src="../Images/be7461d47103c59924a5ebbe26a9eba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_UVLHNQilVdcYUJh48Z7A.png"/></div></div></figure><p id="3f04" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç”Ÿæˆçš„å›¾åƒçœ‹èµ·æ¥å¾ˆåˆç†ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å¾—è¶³å¤Ÿå¥½ï¼Œå°½ç®¡å›¾åƒè´¨é‡ä¸å¦‚è®­ç»ƒé›†ä¸­çš„å›¾åƒè´¨é‡å¥½(å› ä¸ºæˆ‘ä»¬å¯¹å›¾åƒè¿›è¡Œäº†æ•´å½¢ï¼Œä½¿å…¶å˜å¾—æ›´å°ï¼Œå¹¶ä½¿å®ƒä»¬æ¯”åŸå§‹å›¾åƒæ›´æ¨¡ç³Š)ã€‚ä½†æ˜¯ï¼Œå®ƒä»¬è¶³å¤Ÿç”ŸåŠ¨ï¼Œå¯ä»¥åˆ›å»ºæœ‰æ•ˆçš„äººè„¸ï¼Œå¹¶ä¸”è¿™äº›äººè„¸è¶³å¤Ÿæ¥è¿‘ç°å®ã€‚æ­¤å¤–ï¼Œä¸ VAE åˆ¶ä½œçš„å›¾åƒç›¸æ¯”ï¼Œè¿™äº›å›¾åƒæ›´æœ‰åˆ›æ„ï¼Œçœ‹èµ·æ¥æ›´çœŸå®ã€‚</p><p id="e3f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æ‰€ä»¥çœ‹èµ·æ¥ GAN åœ¨è¿™ç§æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œçœ‹çœ‹ GAN ä¸æ··åˆå˜ä½“ VAE-GAN ç›¸æ¯”è¡¨ç°å¦‚ä½•ã€‚</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="4824" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">åŠ¨æ¼«æ•°æ®é›†</strong></h1><p id="b694" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ GAN ä»¥åŠå¦ä¸€ç§ç‰¹æ®Šå½¢å¼çš„ GANï¼Œå³ VAE-GANï¼Œæ¥ç”Ÿæˆä¸åŠ¨ç”»æ•°æ®é›†é£æ ¼ç›¸åŒçš„äººè„¸ã€‚æœ¯è¯­ VAE-ç”˜é¦–å…ˆç”± Larsen ç­‰äººä½¿ç”¨ã€‚al åœ¨ä»–ä»¬çš„è®ºæ–‡<a class="ae lw" href="https://arxiv.org/abs/1512.09300" rel="noopener ugc nofollow" target="_blank">â€œä½¿ç”¨å­¦ä¹ çš„ç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œåƒç´ ä»¥å¤–çš„è‡ªåŠ¨ç¼–ç â€</a>ã€‚VAE-ç”˜æ¨¡å‹ä¸ç”˜æ¨¡å‹çš„åŒºåˆ«åœ¨äºå®ƒä»¬çš„<strong class="kh ir">ç”Ÿæˆå™¨æ˜¯å˜å¼‚è‡ªåŠ¨ç¼–ç å™¨</strong>ã€‚</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pj"><img src="../Images/f45b973004a186700c45f332f71c45e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePINXrbAYFtORd8ZFPOCpg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">VAE-GAN architecture. Source: <a class="ae lw" href="https://arxiv.org/abs/1512.09300" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1512.09300</a></figcaption></figure><p id="cd4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">é¦–å…ˆï¼Œæˆ‘ä»¬å°†é‡ç‚¹æ”¾åœ¨ DC-ç”˜ã€‚åŠ¨æ¼«æ•°æ®é›†ç”±è¶…è¿‡ 20K å¼  64x64 å›¾åƒå½¢å¼çš„åŠ¨æ¼«å¤´åƒç»„æˆã€‚æˆ‘ä»¬è¿˜éœ€è¦åˆ›å»ºå¦ä¸€ä¸ª<a class="ae lw" href="https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43" rel="noopener ugc nofollow" target="_blank"> Keras å®šåˆ¶æ•°æ®ç”Ÿæˆå™¨</a>ã€‚è¯¥æ•°æ®é›†çš„é“¾æ¥å¯åœ¨æ­¤å¤„æ‰¾åˆ°:</p><div class="lc ld gp gr le lf"><a href="https://github.com/Mckinsey666/Anime-Face-Dataset" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">McKinsey 666/åŠ¨æ¼«äººè„¸æ•°æ®é›†</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">ğŸ–¼æ”¶é›†äº†é«˜è´¨é‡çš„åŠ¨æ¼«é¢å­”ã€‚ä¸º Mckinsey666/Anime-Face-Dataset å¼€å‘åšå‡ºè´¡çŒ®ï¼Œåˆ›å»ºä¸€ä¸ªâ€¦</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">github.com</p></div></div><div class="lo l"><div class="pk l lq lr ls lo lt lu lf"/></div></div></a></div></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="a4cd" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">åŠ¨æ¼«æ•°æ®é›†ä¸Šçš„ç”˜</strong></h1><p id="3b8f" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">æˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯åˆ›å»ºåŠ¨æ¼«ç›®å½•å¹¶ä¸‹è½½æ•°æ®ã€‚è¿™å¯ä»¥é€šè¿‡ä¸Šé¢çš„é“¾æ¥æ¥å®Œæˆã€‚åœ¨ç»§ç»­ä¹‹å‰æ£€æŸ¥æ•°æ®æ€»æ˜¯å¥½çš„åšæ³•ï¼Œæ‰€ä»¥æˆ‘ä»¬ç°åœ¨å°±è¿™æ ·åšã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="94b5" class="ol nj iq oh b gy om on l oo op">from skimage import io<br/>import matplotlib.pyplot as plt<br/><br/>filePath='anime-faces/data/'<br/>imgSets=[]<br/><br/>for i in range(1,20001):<br/>    imgName=filePath+str(i)+'.png'<br/>    imgSets.append(io.imread(imgName))<br/><br/>plt.imshow(imgSets[1234])<br/>plt.axis('off')<br/>plt.show()</span></pre><p id="3d74" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬ç°åœ¨åˆ›å»ºå¹¶ç¼–è¯‘æˆ‘ä»¬çš„ DC-ç”˜æ¨¡å‹ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="9f3c" class="ol nj iq oh b gy om on l oo op"># Create and compile a DC-GAN model</span><span id="7805" class="ol nj iq oh b gy oq on l oo op">from keras.models import Sequential, Model<br/>from keras.layers import Input, Dense, Dropout, Activation, \<br/>    Flatten, LeakyReLU, BatchNormalization, Conv2DTranspose, Conv2D, Reshape<br/>from keras.layers.advanced_activations import LeakyReLU<br/>from keras.layers.convolutional import UpSampling2D<br/>from keras.optimizers import Adam, RMSprop,SGD<br/>from keras.initializers import RandomNormal</span><span id="58f2" class="ol nj iq oh b gy oq on l oo op">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import os, glob<br/>from PIL import Image<br/>from tqdm import tqdm_notebook<br/></span><span id="c032" class="ol nj iq oh b gy oq on l oo op">image_shape = (64, 64, 3)<br/>#noise_shape = (100,)<br/>Noise_dim = 128<br/>img_rows = 64<br/>img_cols = 64<br/>channels = 3</span><span id="3fa0" class="ol nj iq oh b gy oq on l oo op">def generator_model(latent_dim=100, leaky_alpha=0.2):<br/>    model = Sequential()<br/>    <br/>    # layer1 (None,500)&gt;&gt;(None,128*16*16)<br/>    model.add(Dense(128 * 16 * 16, activation="relu", input_shape=(Noise_dim,)))<br/>    <br/>    # (None,16*16*128)&gt;&gt;(None,16,16,128)<br/>    model.add(Reshape((16, 16, 128)))<br/>    <br/>   # (None,16,16,128)&gt;&gt;(None,32,32,128)<br/>    model.add(UpSampling2D())<br/>    model.add(Conv2D(256, kernel_size=3, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(Activation("relu"))</span><span id="bb15" class="ol nj iq oh b gy oq on l oo op">    #(None,32,32,128)&gt;&gt;(None,64,64,128)<br/>    model.add(UpSampling2D())<br/>    <br/>    # (None,64,64,128)&gt;&gt;(None,64,64,64)<br/>    model.add(Conv2D(128, kernel_size=3, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(Activation("relu"))</span><span id="3260" class="ol nj iq oh b gy oq on l oo op">    # (None,64,64,128)&gt;&gt;(None,64,64,32)</span><span id="170d" class="ol nj iq oh b gy oq on l oo op">    model.add(Conv2D(32, kernel_size=3, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(Activation("relu"))<br/>    <br/>    # (None,64,64,32)&gt;&gt;(None,64,64,3)<br/>    model.add(Conv2D(channels, kernel_size=3, padding="same"))<br/>    model.add(Activation("tanh"))</span><span id="68b6" class="ol nj iq oh b gy oq on l oo op">    model.summary()<br/>    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])<br/>    return model<br/></span><span id="7834" class="ol nj iq oh b gy oq on l oo op">def discriminator_model(leaky_alpha=0.2, dropRate=0.3):<br/>    model = Sequential()<br/>    <br/>    # layer1 (None,64,64,3)&gt;&gt;(None,32,32,32)<br/>    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, padding="same"))<br/>    model.add(LeakyReLU(alpha=leaky_alpha))<br/>    model.add(Dropout(dropRate))</span><span id="6b7c" class="ol nj iq oh b gy oq on l oo op">    # layer2 (None,32,32,32)&gt;&gt;(None,16,16,64)<br/>    model.add(Conv2D(64, kernel_size=3, strides=2, padding="same"))</span><span id="cedd" class="ol nj iq oh b gy oq on l oo op">    # model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(LeakyReLU(alpha=leaky_alpha))<br/>    model.add(Dropout(dropRate))</span><span id="1905" class="ol nj iq oh b gy oq on l oo op">    # (None,16,16,64)&gt;&gt;(None,8,8,128)<br/>    model.add(Conv2D(128, kernel_size=3, strides=2, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(LeakyReLU(alpha=0.2))<br/>    model.add(Dropout(dropRate))</span><span id="cc4b" class="ol nj iq oh b gy oq on l oo op">    # (None,8,8,128)&gt;&gt;(None,8,8,256)<br/>    model.add(Conv2D(256, kernel_size=3, strides=1, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(LeakyReLU(alpha=0.2))<br/>    model.add(Dropout(dropRate))</span><span id="dc45" class="ol nj iq oh b gy oq on l oo op">     # (None,8,8,256)&gt;&gt;(None,8,8,64)<br/>    model.add(Conv2D(64, kernel_size=3, strides=1, padding="same"))<br/>    model.add(BatchNormalization(momentum=0.8))<br/>    model.add(LeakyReLU(alpha=0.2))<br/>    model.add(Dropout(dropRate))<br/>    <br/>    # (None,8,8,64)<br/>    model.add(Flatten())<br/>    model.add(Dense(1, activation='sigmoid'))</span><span id="0146" class="ol nj iq oh b gy oq on l oo op">    model.summary()</span><span id="80b3" class="ol nj iq oh b gy oq on l oo op">    sgd=SGD(lr=0.0002)<br/>    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])<br/>    return model<br/></span><span id="8d56" class="ol nj iq oh b gy oq on l oo op">def DCGAN(sample_size=Noise_dim):<br/>    # generator<br/>    g = generator_model(sample_size, 0.2)</span><span id="262a" class="ol nj iq oh b gy oq on l oo op">    # discriminator<br/>    d = discriminator_model(0.2)<br/>    d.trainable = False<br/>    # GAN<br/>    gan = Sequential([g, d])<br/>    <br/>    sgd=SGD()<br/>    gan.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')<br/>    return gan, g, d<br/></span><span id="0a3f" class="ol nj iq oh b gy oq on l oo op">def get_image(image_path, width, height, mode):<br/>    image = Image.open(image_path)<br/>    #print(image.size)</span><span id="36ff" class="ol nj iq oh b gy oq on l oo op">    return np.array(image.convert(mode))<br/></span><span id="65e2" class="ol nj iq oh b gy oq on l oo op">def get_batch(image_files, width, height, mode):<br/>    data_batch = np.array([get_image(sample_file, width, height, mode) \<br/>                           for sample_file in image_files])<br/>    return data_batch<br/></span><span id="78f4" class="ol nj iq oh b gy oq on l oo op">def show_imgs(generator,epoch):<br/>    row=3<br/>    col = 5<br/>    noise = np.random.normal(0, 1, (row * col, Noise_dim))<br/>    gen_imgs = generator.predict(noise)</span><span id="1cf4" class="ol nj iq oh b gy oq on l oo op">    # Rescale images 0 - 1<br/>    gen_imgs = 0.5 * gen_imgs + 0.5</span><span id="0596" class="ol nj iq oh b gy oq on l oo op">    fig, axs = plt.subplots(row, col)<br/>    #fig.suptitle("DCGAN: Generated digits", fontsize=12)<br/>    cnt = 0</span><span id="a48c" class="ol nj iq oh b gy oq on l oo op">    for i in range(row):<br/>        for j in range(col):<br/>            axs[i, j].imshow(gen_imgs[cnt, :, :, :])<br/>            axs[i, j].axis('off')<br/>            cnt += 1</span><span id="6192" class="ol nj iq oh b gy oq on l oo op">    #plt.close()<br/>    plt.show()</span></pre><p id="73e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨åŠ¨ç”»æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬å°†ä»¥ä¸¤ç§ä¸åŒçš„æ–¹å¼æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œç¬¬ä¸€ç§æ–¹å¼å°†æ¶‰åŠä»¥ 1:1 çš„è®­ç»ƒæ—¶é—´æ¯”ä¾‹æ¥è®­ç»ƒé‰´åˆ«å™¨å’Œç”Ÿæˆå™¨ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="0159" class="ol nj iq oh b gy om on l oo op"># Training the discriminator and generator with the 1:1 proportion of training times</span><span id="f273" class="ol nj iq oh b gy oq on l oo op">def train(epochs=30, batchSize=128):<br/>    filePath = r'anime-faces/data/'</span><span id="b9a3" class="ol nj iq oh b gy oq on l oo op">    X_train = get_batch(glob.glob(os.path.join(filePath, '*.png'))[:20000], 64, 64, 'RGB')<br/>    X_train = (X_train.astype(np.float32) - 127.5) / 127.5</span><span id="3225" class="ol nj iq oh b gy oq on l oo op">    halfSize = int(batchSize / 2)<br/>    batchCount=int(len(X_train)/batchSize)</span><span id="e241" class="ol nj iq oh b gy oq on l oo op">    dLossReal = []<br/>    dLossFake = []<br/>    gLossLogs = []</span><span id="1b15" class="ol nj iq oh b gy oq on l oo op">    gan, generator, discriminator = DCGAN(Noise_dim)</span><span id="540a" class="ol nj iq oh b gy oq on l oo op">    for e in range(epochs):<br/>        for i in tqdm_notebook(range(batchCount)):<br/>            index = np.random.randint(0, X_train.shape[0], halfSize)<br/>            images = X_train[index]</span><span id="d9fe" class="ol nj iq oh b gy oq on l oo op">            noise = np.random.normal(0, 1, (halfSize, Noise_dim))<br/>            genImages = generator.predict(noise)</span><span id="0ff3" class="ol nj iq oh b gy oq on l oo op">            # one-sided labels<br/>            discriminator.trainable = True<br/>            dLossR = discriminator.train_on_batch(images, np.ones([halfSize, 1]))<br/>            dLossF = discriminator.train_on_batch(genImages, np.zeros([halfSize, 1]))<br/>            dLoss = np.add(dLossF, dLossR) * 0.5<br/>            discriminator.trainable = False</span><span id="62dd" class="ol nj iq oh b gy oq on l oo op">            noise = np.random.normal(0, 1, (batchSize, Noise_dim))<br/>            gLoss = gan.train_on_batch(noise, np.ones([batchSize, 1]))</span><span id="63ca" class="ol nj iq oh b gy oq on l oo op">        dLossReal.append([e, dLoss[0]])<br/>        dLossFake.append([e, dLoss[1]])<br/>        gLossLogs.append([e, gLoss])</span><span id="c714" class="ol nj iq oh b gy oq on l oo op">        dLossRealArr = np.array(dLossReal)<br/>        dLossFakeArr = np.array(dLossFake)<br/>        gLossLogsArr = np.array(gLossLogs)<br/>            </span><span id="9dc0" class="ol nj iq oh b gy oq on l oo op">        # At the end of training plot the losses vs epochs<br/>        show_imgs(generator, e)</span><span id="b435" class="ol nj iq oh b gy oq on l oo op">    plt.plot(dLossRealArr[:, 0], dLossRealArr[:, 1], label="Discriminator Loss - Real")<br/>    plt.plot(dLossFakeArr[:, 0], dLossFakeArr[:, 1], label="Discriminator Loss - Fake")<br/>    plt.plot(gLossLogsArr[:, 0], gLossLogsArr[:, 1], label="Generator Loss")<br/>    plt.xlabel('Epochs')<br/>    plt.ylabel('Loss')<br/>    plt.legend()<br/>    plt.title('GAN')<br/>    plt.grid(True)<br/>    plt.show()<br/>    <br/>    <br/>    return gan, generator, discriminator<br/></span><span id="beaa" class="ol nj iq oh b gy oq on l oo op">GAN,Generator,Discriminator=train(epochs=20, batchSize=128)  <br/>train(epochs=1000, batchSize=128, plotInternal=200)</span></pre><p id="2d3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è¾“å‡ºç°åœ¨å°†å¼€å§‹æ‰“å°ä¸€ç³»åˆ—çš„åŠ¨æ¼«äººç‰©ã€‚å®ƒä»¬èµ·åˆéå¸¸ç²—ç³™ï¼Œéšç€æ—¶é—´çš„æ¨ç§»é€æ¸å˜å¾—è¶Šæ¥è¶Šæ˜æ˜¾ã€‚</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pl"><img src="../Images/735ed6f8251508b90e13ea4aef36f4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDfCiFhLqbM76RGKv79chA.png"/></div></div></figure><p id="4131" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬è¿˜å°†å¾—åˆ°å‘ç”µæœºå’Œé‰´é¢‘å™¨æŸè€—å‡½æ•°çš„æ›²çº¿å›¾ã€‚</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pm"><img src="../Images/4c7bd2ea43fa1c085a898f9f8fb186fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fwLeI_9ndMFsvtV3_rh73w.png"/></div></div></figure><p id="1568" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç°åœ¨æˆ‘ä»¬å°†åšåŒæ ·çš„äº‹æƒ…ï¼Œä½†æ˜¯ç”¨ä¸åŒçš„è®­ç»ƒæ—¶é—´æ¥è®­ç»ƒé‰´åˆ«å™¨å’Œç”Ÿæˆå™¨ï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚</p><p id="9b3b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">åœ¨ç»§ç»­ä¹‹å‰ï¼Œæœ€å¥½å°†æ¨¡å‹çš„æƒé‡ä¿å­˜åœ¨æŸä¸ªåœ°æ–¹ï¼Œè¿™æ ·æ‚¨å°±ä¸éœ€è¦å†æ¬¡è¿è¡Œæ•´ä¸ªè®­ç»ƒï¼Œè€Œæ˜¯å¯ä»¥å°†æƒé‡åŠ è½½åˆ°ç½‘ç»œä¸­ã€‚</p><p id="64bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ä¸ºäº†èŠ‚çœé‡é‡:</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="8611" class="ol nj iq oh b gy om on l oo op">discriminator.save_weights('/content/gdrive/My Drive/discriminator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')<br/>gan.save_weights('/content/gdrive/My Drive/gan_DCGAN_lr0.0001_deepgenerator+proportion2.h5')<br/>generator.save_weights('/content/gdrive/My Drive/generator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')</span></pre><p id="54fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è¦åŠ è½½ç ç :</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="e958" class="ol nj iq oh b gy om on l oo op">discriminator.load_weights('/content/gdrive/My Drive/discriminator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')<br/>gan.load_weights('/content/gdrive/My Drive/gan_DCGAN_lr0.0001_deepgenerator+proportion2.h5')<br/>generator.load_weights('/content/gdrive/My Drive/generator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')</span></pre><p id="4ab3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬è½¬åˆ°ç¬¬äºŒä¸ªç½‘ç»œå®æ–½ï¼Œè€Œä¸ç”¨æ‹…å¿ƒæ¯”ä¹‹å‰çš„ç½‘ç»œèŠ‚çœæˆæœ¬ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="6e46" class="ol nj iq oh b gy om on l oo op"># Train the discriminator and generator separately and with different training times</span><span id="501c" class="ol nj iq oh b gy oq on l oo op">def train(epochs=300, batchSize=128, plotInternal=50):<br/>    gLoss = 1<br/>    filePath = r'anime-faces/data/'<br/>    <br/>    X_train = get_batch(glob.glob(os.path.join(filePath,'*.png'))[:20000],64,64,'RGB')<br/>    X_train=(X_train.astype(np.float32)-127.5)/127.5<br/>    halfSize= int (batchSize/2)</span><span id="62b4" class="ol nj iq oh b gy oq on l oo op">    dLossReal=[]<br/>    dLossFake=[]<br/>    gLossLogs=[]<br/>    </span><span id="ec63" class="ol nj iq oh b gy oq on l oo op">    for e in range(epochs):<br/>        index=np.random.randint(0,X_train.shape[0],halfSize)<br/>        images=X_train[index]</span><span id="e9c8" class="ol nj iq oh b gy oq on l oo op">        noise=np.random.normal(0,1,(halfSize,Noise_dim))<br/>        genImages=generator.predict(noise)<br/>        <br/>        if e &lt; int(epochs*0.5):    <br/>            #one-sided labels<br/>            discriminator.trainable=True<br/>            dLossR=discriminator.train_on_batch(images,np.ones([halfSize,1]))<br/>            dLossF=discriminator.train_on_batch(genImages,np.zeros([halfSize,1]))<br/>            dLoss=np.add(dLossF,dLossR)*0.5<br/>            discriminator.trainable=False</span><span id="08cb" class="ol nj iq oh b gy oq on l oo op">            cnt = e</span><span id="9a5a" class="ol nj iq oh b gy oq on l oo op">            while cnt &gt; 3:<br/>                cnt = cnt - 4</span><span id="2c2f" class="ol nj iq oh b gy oq on l oo op">            if cnt == 0:<br/>                noise=np.random.normal(0,1,(batchSize,Noise_dim))<br/>                gLoss=gan.train_on_batch(noise,np.ones([batchSize,1]))<br/>                <br/>        elif e&gt;= int(epochs*0.5) :<br/>            cnt = e</span><span id="58fc" class="ol nj iq oh b gy oq on l oo op">            while cnt &gt; 3:<br/>                cnt = cnt - 4</span><span id="31ea" class="ol nj iq oh b gy oq on l oo op">            if cnt == 0:<br/>                #one-sided labels<br/>                discriminator.trainable=True<br/>                dLossR=discriminator.train_on_batch(images,np.ones([halfSize,1]))<br/>                dLossF=discriminator.train_on_batch(genImages,np.zeros([halfSize,1]))<br/>                dLoss=np.add(dLossF,dLossR)*0.5<br/>                discriminator.trainable=False</span><span id="9d1c" class="ol nj iq oh b gy oq on l oo op">            <br/>            noise=np.random.normal(0,1,(batchSize,Noise_dim))<br/>            gLoss=gan.train_on_batch(noise,np.ones([batchSize,1]))</span><span id="fcf5" class="ol nj iq oh b gy oq on l oo op">        if e % 20 == 0:<br/>           print("epochï¼š %d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (e, dLoss[0], 100 * dLoss[1], gLoss))</span><span id="e8ad" class="ol nj iq oh b gy oq on l oo op">        dLossReal.append([e,dLoss[0]])<br/>        dLossFake.append([e,dLoss[1]])<br/>        gLossLogs.append([e,gLoss])</span><span id="5fb3" class="ol nj iq oh b gy oq on l oo op">        if e % plotInternal == 0 and e!=0:<br/>            show_imgs(generator, e)<br/>            <br/>            <br/>        dLossRealArr= np.array(dLossReal)<br/>        dLossFakeArr = np.array(dLossFake)<br/>        gLossLogsArr = np.array(gLossLogs)<br/>        <br/>        chk = e</span><span id="b416" class="ol nj iq oh b gy oq on l oo op">        while chk &gt; 50:<br/>            chk = chk - 51</span><span id="74f1" class="ol nj iq oh b gy oq on l oo op">        if chk == 0:<br/>            discriminator.save_weights('/content/gdrive/My Drive/discriminator_DCGAN_lr=0.0001,proportion2,deepgenerator_Fake.h5')<br/>            gan.save_weights('/content/gdrive/My Drive/gan_DCGAN_lr=0.0001,proportion2,deepgenerator_Fake.h5')<br/>            generator.save_weights('/content/gdrive/My Drive/generator_DCGAN_lr=0.0001,proportion2,deepgenerator_Fake.h5')<br/>        # At the end of training plot the losses vs epochs<br/>    plt.plot(dLossRealArr[:, 0], dLossRealArr[:, 1], label="Discriminator Loss - Real")<br/>    plt.plot(dLossFakeArr[:, 0], dLossFakeArr[:, 1], label="Discriminator Loss - Fake")<br/>    plt.plot(gLossLogsArr[:, 0], gLossLogsArr[:, 1], label="Generator Loss")<br/>    plt.xlabel('Epochs')<br/>    plt.ylabel('Loss')<br/>    plt.legend()<br/>    plt.title('GAN')<br/>    plt.grid(True)<br/>    plt.show()<br/>    <br/>    <br/>    return gan, generator, discriminator</span><span id="ac30" class="ol nj iq oh b gy oq on l oo op">gan, generator, discriminator = DCGAN(Noise_dim)<br/>train(epochs=4000, batchSize=128, plotInternal=200)</span></pre><p id="9192" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è®©æˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹è¿™ä¸¤ä¸ªç½‘ç»œçš„è¾“å‡ºã€‚é€šè¿‡è¿è¡Œè¯¥è¡Œ:</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="53e2" class="ol nj iq oh b gy om on l oo op">show_imgs(Generator)</span></pre><p id="cfa5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç½‘ç»œå°†ä»ç”Ÿæˆå™¨è¾“å‡ºä¸€äº›å›¾åƒ(è¿™æ˜¯æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„å‡½æ•°ä¹‹ä¸€)ã€‚</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pn"><img src="../Images/6cc4033c4a44d571e8c5b34b431292a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*41NT55eQwng0pOMovzsX8Q.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Generated images from 1:1 training of discriminator vs. generator.</figcaption></figure><p id="dee8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç°åœ¨è®©æˆ‘ä»¬æ£€æŸ¥ç¬¬äºŒä¸ªæ¨¡å‹ã€‚</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi po"><img src="../Images/5353ccde8122af5660467cb96f754037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FtrkpRNJAd4oMAmIm3ldeQ.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Generated images from the second network with different training times for the discriminator and generator.</figcaption></figure><p id="889c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç”Ÿæˆçš„å›¾åƒçš„ç»†èŠ‚å¾—åˆ°äº†æ”¹å–„ï¼Œå®ƒä»¬çš„çº¹ç†ç¨å¾®æ›´åŠ è¯¦ç»†ã€‚ç„¶è€Œï¼Œä¸è®­ç»ƒå›¾åƒç›¸æ¯”ï¼Œå®ƒä»¬ä»ç„¶æ˜¯ä¸åˆæ ¼çš„ã€‚</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi po"><img src="../Images/a4209a9809e24ec6a6e1fb5217fcf38c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x5fUEObLu1RgpqJjPeKI2Q.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Training images from Anime dataset.</figcaption></figure><p id="8ca6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ä¹Ÿè®¸ VAE-ç”˜ä¼šè¡¨ç°å¾—æ›´å¥½ï¼Ÿ</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="42e6" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">åŠ¨æ¼«æ•°æ®é›†ä¸Šçš„ç”˜</strong></h1><p id="3849" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">é‡ç”³ä¸€ä¸‹æˆ‘ä¹‹å‰è¯´è¿‡çš„å…³äº VAE-ç”˜çš„è¯ï¼Œæœ¯è¯­ VAE-ç”˜é¦–å…ˆæ˜¯ç”± Larsen ç­‰äººä½¿ç”¨çš„ã€‚al åœ¨ä»–ä»¬çš„è®ºæ–‡<a class="ae lw" href="https://arxiv.org/abs/1512.09300" rel="noopener ugc nofollow" target="_blank">â€œä½¿ç”¨å­¦ä¹ çš„ç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œåƒç´ ä»¥å¤–çš„è‡ªåŠ¨ç¼–ç â€</a>ã€‚VAE-ç”˜æ¨¡å‹ä¸ç”˜æ¨¡å‹çš„åŒºåˆ«åœ¨äºå®ƒä»¬çš„<strong class="kh ir">ç”Ÿæˆå™¨æ˜¯å˜å¼‚è‡ªåŠ¨ç¼–ç å™¨</strong>ã€‚</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pj"><img src="../Images/f45b973004a186700c45f332f71c45e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePINXrbAYFtORd8ZFPOCpg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">VAE-GAN architecture. Source: <a class="ae lw" href="https://arxiv.org/abs/1512.09300" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1512.09300</a></figcaption></figure><p id="7e3e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºå¹¶ç¼–è¯‘ VAE-GANï¼Œå¹¶å¯¹æ¯ä¸ªç½‘ç»œè¿›è¡Œæ€»ç»“(è¿™æ˜¯ç®€å•æ£€æŸ¥æ¶æ„çš„å¥½æ–¹æ³•)ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="4d50" class="ol nj iq oh b gy om on l oo op"># Create and compile a VAE-GAN, and make a summary for them</span><span id="7244" class="ol nj iq oh b gy oq on l oo op">from keras.models import Sequential, Model<br/>from keras.layers import Input, Dense, Dropout, Activation, \<br/>    Flatten, LeakyReLU, BatchNormalization, Conv2DTranspose, Conv2D, Reshape,MaxPooling2D,UpSampling2D,InputLayer, Lambda<br/>from keras.layers.advanced_activations import LeakyReLU<br/>from keras.layers.convolutional import UpSampling2D<br/>from keras.optimizers import Adam, RMSprop,SGD<br/>from keras.initializers import RandomNormal<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import os, glob<br/>from PIL import Image<br/>import pandas as pd<br/>from scipy.stats import norm<br/>import keras<br/>from keras.utils import np_utils, to_categorical<br/>from keras import backend as K<br/>import random<br/>from keras import metrics<br/>from tqdm import tqdm<br/></span><span id="3fe7" class="ol nj iq oh b gy oq on l oo op"># plotInternal<br/>plotInternal = 50</span><span id="d426" class="ol nj iq oh b gy oq on l oo op">#######<br/>latent_dim = 256<br/>batch_size = 256<br/>rows = 64<br/>columns = 64<br/>channel = 3<br/>epochs = 4000<br/># datasize = len(dataset)</span><span id="7b80" class="ol nj iq oh b gy oq on l oo op"># optimizers<br/>SGDop = SGD(lr=0.0003)<br/>ADAMop = Adam(lr=0.0002)<br/># filters<br/>filter_of_dis = 16<br/>filter_of_decgen = 16<br/>filter_of_encoder = 16<br/></span><span id="ccc2" class="ol nj iq oh b gy oq on l oo op">def sampling(args):<br/>    mean, logsigma = args<br/>    epsilon = K.random_normal(shape=(K.shape(mean)[0], latent_dim), mean=0., stddev=1.0)<br/>    return mean + K.exp(logsigma / 2) * epsilon</span><span id="cded" class="ol nj iq oh b gy oq on l oo op">def vae_loss(X , output , E_mean, E_logsigma):<br/>	# compute the average MSE error, then scale it up, ie. simply sum on all axes<br/>  reconstruction_loss = 2 * metrics.mse(K.flatten(X), K.flatten(output))<br/>  <br/>	# compute the KL loss<br/>  kl_loss = - 0.5 * K.sum(1 + E_logsigma - K.square(E_mean) - K.exp(E_logsigma), axis=-1) </span><span id="a003" class="ol nj iq oh b gy oq on l oo op">  total_loss = K.mean(reconstruction_loss + kl_loss)    <br/>  <br/>  return total_loss<br/>  </span><span id="4999" class="ol nj iq oh b gy oq on l oo op">def encoder(kernel, filter, rows, columns, channel):<br/>    X = Input(shape=(rows, columns, channel))<br/>    model = Conv2D(filters=filter, kernel_size=kernel, strides=2, padding='same')(X)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="1ae6" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="dccb" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="436c" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="c7df" class="ol nj iq oh b gy oq on l oo op">    model = Flatten()(model)</span><span id="a835" class="ol nj iq oh b gy oq on l oo op">    mean = Dense(latent_dim)(model)<br/>    logsigma = Dense(latent_dim, activation='tanh')(model)<br/>    latent = Lambda(sampling, output_shape=(latent_dim,))([mean, logsigma])<br/>    meansigma = Model([X], [mean, logsigma, latent])<br/>    meansigma.compile(optimizer=SGDop, loss='mse')<br/>    return meansigma<br/></span><span id="cd64" class="ol nj iq oh b gy oq on l oo op">def decgen(kernel, filter, rows, columns, channel):<br/>    X = Input(shape=(latent_dim,))</span><span id="6250" class="ol nj iq oh b gy oq on l oo op">    model = Dense(2*2*256)(X)<br/>    model = Reshape((2, 2, 256))(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = Activation('relu')(model)</span><span id="7a64" class="ol nj iq oh b gy oq on l oo op">    model = Conv2DTranspose(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = Activation('relu')(model)<br/>    <br/>    model = Conv2DTranspose(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = Activation('relu')(model)</span><span id="41cb" class="ol nj iq oh b gy oq on l oo op">    model = Conv2DTranspose(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = Activation('relu')(model)</span><span id="1565" class="ol nj iq oh b gy oq on l oo op">    model = Conv2DTranspose(filters=filter, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = Activation('relu')(model)</span><span id="ab88" class="ol nj iq oh b gy oq on l oo op">    model = Conv2DTranspose(filters=channel, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = Activation('tanh')(model)</span><span id="14ba" class="ol nj iq oh b gy oq on l oo op">    model = Model(X, model)<br/>    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])<br/>    return model<br/></span><span id="a4aa" class="ol nj iq oh b gy oq on l oo op">def discriminator(kernel, filter, rows, columns, channel):<br/>    X = Input(shape=(rows, columns, channel))</span><span id="e33f" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(X)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="be96" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="de76" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)<br/>    model = BatchNormalization(epsilon=1e-5)(model)<br/>    model = LeakyReLU(alpha=0.2)(model)</span><span id="2d4b" class="ol nj iq oh b gy oq on l oo op">    model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)<br/></span><span id="3581" class="ol nj iq oh b gy oq on l oo op">    dec = BatchNormalization(epsilon=1e-5)(model)<br/>    dec = LeakyReLU(alpha=0.2)(dec)<br/>    dec = Flatten()(dec)<br/>    dec = Dense(1, activation='sigmoid')(dec)</span><span id="3acc" class="ol nj iq oh b gy oq on l oo op">    output = Model(X, dec)<br/>    output.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])<br/>                   <br/>    return output</span><span id="f866" class="ol nj iq oh b gy oq on l oo op">  <br/>def VAEGAN(decgen,discriminator):<br/>    # generator<br/>    g = decgen</span><span id="e1e6" class="ol nj iq oh b gy oq on l oo op">    # discriminator<br/>    d = discriminator<br/>    d.trainable = False<br/>    # GAN<br/>    gan = Sequential([g, d])<br/>    <br/>#     sgd=SGD()<br/>    gan.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')<br/>    return g, d, gan</span></pre><p id="439a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬å†æ¬¡å®šä¹‰äº†ä¸€äº›å‡½æ•°ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ‰“å°æ¥è‡ªç”Ÿæˆå™¨çš„å›¾åƒã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="1db8" class="ol nj iq oh b gy om on l oo op">def get_image(image_path, width, height, mode):<br/>    image = Image.open(image_path)<br/>    #print(image.size)<br/><br/>    return np.array(image.convert(mode))<br/><br/>def show_imgs(generator):<br/>    row=3<br/>    col = 5<br/>    noise = np.random.normal(0, 1, (row*col, latent_dim))<br/>    gen_imgs = generator.predict(noise)<br/><br/>    # Rescale images 0 - 1<br/>    gen_imgs = 0.5 * gen_imgs + 0.5<br/><br/>    fig, axs = plt.subplots(row, col)<br/>    #fig.suptitle("DCGAN: Generated digits", fontsize=12)<br/>    cnt = 0<br/><br/>    for i in range(row):<br/>        for j in range(col):<br/>            axs[i, j].imshow(gen_imgs[cnt, :, :, :])<br/>            axs[i, j].axis('off')<br/>            cnt += 1<br/><br/>    #plt.close()<br/>    plt.show()</span></pre><p id="b764" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">å‘ç”Ÿå™¨çš„å‚æ•°å°†å—åˆ° GAN å’Œ VAE è®­ç»ƒçš„å½±å“ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="015f" class="ol nj iq oh b gy om on l oo op"># note: <!-- -->The parameters of the generator will be affected by both the GAN and VAE training<br/><br/><br/>G, D, GAN = VAEGAN(decgen(5, filter_of_decgen, rows, columns, channel),discriminator(5, filter_of_dis, rows, columns, channel))<br/><br/># encoder<br/>E = encoder(5, filter_of_encoder, rows, columns, channel)<br/>print("This is the summary for encoder:")<br/>E.summary()<br/><br/><br/># generator/decoder<br/># G = decgen(5, filter_of_decgen, rows, columns, channel)<br/>print("This is the summary for dencoder/generator:")<br/>G.summary()<br/><br/><br/># discriminator<br/># D = discriminator(5, filter_of_dis, rows, columns, channel)<br/>print("This is the summary for discriminator:")<br/>D.summary()<br/><br/><br/>D_fixed = discriminator(5, filter_of_dis, rows, columns, channel)<br/>D_fixed.compile(optimizer=SGDop, loss='mse')<br/><br/># gan<br/>print("This is the summary for GAN:")<br/>GAN.summary()<br/><br/># VAE<br/>X = Input(shape=(rows, columns, channel))<br/><br/>E_mean, E_logsigma, Z = E(X)<br/><br/>output = G(Z)<br/># G_dec = G(E_mean + E_logsigma)<br/># D_fake, F_fake = D(output)<br/># D_fromGen, F_fromGen = D(G_dec)<br/># D_true, F_true = D(X)<br/><br/># print("type(E)",type(E))<br/># print("type(output)",type(output))<br/># print("type(D_fake)",type(D_fake))<br/><br/>VAE = Model(X, output)<br/>VAE.add_loss(vae_loss(X, output, E_mean, E_logsigma))<br/>VAE.compile(optimizer=SGDop)<br/><br/>print("This is the summary for vae:")<br/>VAE.summary()</span></pre><p id="5121" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">åœ¨ä¸‹é¢çš„å•å…ƒæ ¼ä¸­ï¼Œæˆ‘ä»¬å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨å‰é¢çš„æ–¹æ³•æ¥è®­ç»ƒé‰´é¢‘å™¨ä»¥åŠ GAN å’Œ VAE ä¸åŒçš„æ—¶é—´é•¿åº¦ã€‚æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹çš„å‰åŠéƒ¨åˆ†å¼ºè°ƒé‰´åˆ«å™¨çš„è®­ç»ƒï¼Œåœ¨ååŠéƒ¨åˆ†æˆ‘ä»¬æ›´å¤šåœ°è®­ç»ƒå‘ç”Ÿå™¨ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³æé«˜è¾“å‡ºå›¾åƒçš„è´¨é‡ã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="611c" class="ol nj iq oh b gy om on l oo op"># We train our model in this cell<br/><br/>dLoss=[]<br/>gLoss=[]<br/>GLoss = 1<br/>GlossEnc = 1<br/>GlossGen = 1<br/>Eloss = 1<br/><br/>halfbatch_size = int(batch_size*0.5)<br/><br/>for epoch in tqdm(range(epochs)):<br/>    if epoch &lt; int(epochs*0.5):<br/>        noise = np.random.normal(0, 1, (halfbatch_size, latent_dim))<br/>        index = np.random.randint(0,dataset.shape[0], halfbatch_size)<br/>        images = dataset[index]  <br/><br/>        latent_vect = E.predict(images)[0]<br/>        encImg = G.predict(latent_vect)<br/>        fakeImg = G.predict(noise)<br/><br/>        D.Trainable = True<br/>        DlossTrue = D.train_on_batch(images, np.ones((halfbatch_size, 1)))<br/>        DlossEnc = D.train_on_batch(encImg, np.ones((halfbatch_size, 1)))       <br/>        DlossFake = D.train_on_batch(fakeImg, np.zeros((halfbatch_size, 1)))<br/><br/>#         DLoss=np.add(DlossTrue,DlossFake)*0.5<br/>        <br/>        DLoss=np.add(DlossTrue,DlossEnc)<br/>        DLoss=np.add(DLoss,DlossFake)*0.33<br/>        D.Trainable = False<br/><br/>        cnt = epoch<br/><br/>        while cnt &gt; 3:<br/>            cnt = cnt - 4<br/><br/>        if cnt == 0:<br/>            noise = np.random.normal(0, 1, (batch_size, latent_dim))<br/>            index = np.random.randint(0,dataset.shape[0], batch_size)<br/>            images = dataset[index]  <br/>            latent_vect = E.predict(images)[0]     <br/>            <br/>            GlossEnc = GAN.train_on_batch(latent_vect, np.ones((batch_size, 1)))<br/>            GlossGen = GAN.train_on_batch(noise, np.ones((batch_size, 1)))<br/>            Eloss = VAE.train_on_batch(images, None)   <br/>            GLoss=np.add(GlossEnc,GlossGen)<br/>            GLoss=np.add(GLoss,Eloss)*0.33<br/>        dLoss.append([epoch,DLoss[0]]) <br/>        gLoss.append([epoch,GLoss])<br/>    <br/>    elif epoch &gt;= int(epochs*0.5):<br/>        cnt = epoch<br/>        while cnt &gt; 3:<br/>            cnt = cnt - 4<br/><br/>        if cnt == 0:<br/>            noise = np.random.normal(0, 1, (halfbatch_size, latent_dim))<br/>            index = np.random.randint(0,dataset.shape[0], halfbatch_size)<br/>            images = dataset[index]  <br/><br/>            latent_vect = E.predict(images)[0]<br/>            encImg = G.predict(latent_vect)<br/>            fakeImg = G.predict(noise)<br/><br/>            D.Trainable = True<br/>            DlossTrue = D.train_on_batch(images, np.ones((halfbatch_size, 1)))<br/>        #     DlossEnc = D.train_on_batch(encImg, np.ones((halfbatch_size, 1)))       <br/>            DlossFake = D.train_on_batch(fakeImg, np.zeros((halfbatch_size, 1)))<br/><br/>            DLoss=np.add(DlossTrue,DlossFake)*0.5<br/>        <br/>#             DLoss=np.add(DlossTrue,DlossEnc)<br/>#             DLoss=np.add(DLoss,DlossFake)*0.33<br/>            D.Trainable = False<br/><br/>        noise = np.random.normal(0, 1, (batch_size, latent_dim))<br/>        index = np.random.randint(0,dataset.shape[0], batch_size)<br/>        images = dataset[index]  <br/>        latent_vect = E.predict(images)[0]<br/>        <br/>        GlossEnc = GAN.train_on_batch(latent_vect, np.ones((batch_size, 1)))<br/>        GlossGen = GAN.train_on_batch(noise, np.ones((batch_size, 1)))<br/>        Eloss = VAE.train_on_batch(images, None)   <br/>        GLoss=np.add(GlossEnc,GlossGen)<br/>        GLoss=np.add(GLoss,Eloss)*0.33<br/>    <br/>        dLoss.append([epoch,DLoss[0]]) <br/>        gLoss.append([epoch,GLoss])<br/><br/>    if epoch % plotInternal == 0 and epoch!=0:<br/>        show_imgs(G)<br/><br/><br/>    dLossArr= np.array(dLoss)<br/>    gLossArr = np.array(gLoss)<br/>    <br/>#     print("dLossArr.shape:",dLossArr.shape)<br/>#     print("gLossArr.shape:",gLossArr.shape)<br/>    <br/>    chk = epoch<br/><br/>    while chk &gt; 50:<br/>        chk = chk - 51<br/><br/>    if chk == 0:<br/>        D.save_weights('/content/gdrive/My Drive/VAE discriminator_kernalsize5_proportion_32.h5')<br/>        G.save_weights('/content/gdrive/My Drive/VAE generator_kernalsize5_proportion_32.h5')<br/>        E.save_weights('/content/gdrive/My Drive/VAE encoder_kernalsize5_proportion_32.h5')<br/><br/>        <br/>    if epoch%20 == 0:    <br/>        print("epoch:", epoch + 1,"  ", "DislossTrue loss:",DlossTrue[0],"D accuracyï¼š",100* DlossTrue[1], "DlossFake loss:", DlossFake[0],"GlossEnc loss:",<br/>          GlossEnc, "GlossGen loss:",GlossGen, "Eloss loss:",Eloss)<br/>#     print("loss:")<br/>#     print("D:", DlossTrue, DlossEnc, DlossFake)<br/>#     print("G:", GlossEnc, GlossGen)<br/>#     print("VAE:", Eloss)<br/><br/>print('Training done,saving weights')<br/>D.save_weights('/content/gdrive/My Drive/VAE discriminator_kernalsize5_proportion_32.h5')<br/>G.save_weights('/content/gdrive/My Drive/VAE generator_kernalsize5_proportion_32.h5')<br/>E.save_weights('/content/gdrive/My Drive/VAE encoder_kernalsize5_proportion_32.h5')<br/><br/><br/>print('painting losses')<br/># At the end of training plot the losses vs epochs<br/>plt.plot(dLossArr[:, 0], dLossArr[:, 1], label="Discriminator Loss")<br/>plt.plot(gLossArr[:, 0], gLossArr[:, 1], label="Generator Loss")<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.title('GAN')<br/>plt.grid(True)<br/>plt.show()<br/>print('end')</span></pre><p id="3748" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">å¦‚æœä½ è®¡åˆ’è¿è¡Œè¿™ä¸ªç½‘ç»œï¼Œè¯·æ³¨æ„åŸ¹è®­è¿‡ç¨‹éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚æˆ‘ä¸ä¼šå°è¯•è¿™æ ·åšï¼Œé™¤éä½ æœ‰ä¸€äº›å¼ºå¤§çš„å›¾å½¢å¤„ç†å™¨ï¼Œæˆ–è€…æ„¿æ„è¿è¡Œä¸€æ•´å¤©çš„æ¨¡å‹ã€‚</p><p id="4fd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç°åœ¨æˆ‘ä»¬çš„ VAE-GAN è®­ç»ƒå·²ç»å®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥çœ‹çœ‹æˆ‘ä»¬çš„è¾“å‡ºå›¾åƒçœ‹èµ·æ¥å¦‚ä½•ï¼Œå¹¶ä¸æˆ‘ä»¬ä»¥å‰çš„ GAN è¿›è¡Œæ¯”è¾ƒã€‚</p><pre class="ly lz ma mb gt og oh oi oj aw ok bi"><span id="246e" class="ol nj iq oh b gy om on l oo op"># In this cell, we generate and visualize 15 images. <br/><br/>show_imgs(G)</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi po"><img src="../Images/f11fcca1614b9ce1255c0040905a542c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7K0-so0qXvUJ9wN6YY4-lQ.png"/></div></div></figure><p id="9dfb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ VAE-ç”˜çš„å®ç°ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªå¾ˆå¥½çš„æ¨¡å‹ï¼Œå®ƒå¯ä»¥ç”Ÿæˆæ¸…æ™°çš„å›¾åƒï¼Œå¹¶ä¸”å…·æœ‰ä¸åŸå§‹å›¾åƒç›¸ä¼¼çš„é£æ ¼ã€‚æˆ‘ä»¬çš„ VAE-ç”˜å¯ä»¥åˆ›å»ºæ›´å¥å£®çš„å›¾åƒï¼Œè¿™å¯ä»¥åœ¨æ²¡æœ‰é¢å¤–çš„åŠ¨ç”»è„¸å™ªå£°çš„æƒ…å†µä¸‹å®Œæˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ¨¡å‹çš„æ¦‚æ‹¬èƒ½åŠ›ä¸æ˜¯å¾ˆå¥½ï¼Œå®ƒå¾ˆå°‘æ”¹å˜è§’è‰²çš„æ–¹å¼æˆ–æ€§åˆ«ï¼Œæ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬å¯ä»¥å°è¯•æ”¹è¿›çš„ä¸€ç‚¹ã€‚</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="c5c7" class="ni nj iq bd nk nl nm nn no np nq nr ns jw nt jx nu jz nv ka nw kc nx kd ny nz bi translated"><strong class="ak">æœ€ç»ˆç‚¹è¯„</strong></h1><p id="fdb3" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">ä¸ä¸€å®šæ¸…æ¥šå“ªä¸ªæ¨¡å‹æ¯”å…¶ä»–æ¨¡å‹æ›´å¥½ï¼Œè€Œä¸”è¿™äº›æ–¹æ³•éƒ½æ²¡æœ‰ç»è¿‡é€‚å½“çš„ä¼˜åŒ–ï¼Œå› æ­¤å¾ˆéš¾è¿›è¡Œæ¯”è¾ƒã€‚</p><p id="1656" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è¿™ä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œæ‰€ä»¥å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œæˆ‘å»ºè®®ä½ æŠ•å…¥è¿›å»ï¼Œåœ¨ä½ è‡ªå·±çš„å·¥ä½œä¸­å°è¯•ä½¿ç”¨ GANsï¼Œçœ‹çœ‹ä½ èƒ½æƒ³å‡ºä»€ä¹ˆã€‚</p><p id="50ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªå…³äº GANs çš„æ–‡ç« ä¸‰éƒ¨æ›²ï¼Œå¹¶ä¸”ç°åœ¨å¯¹å®ƒä»¬æ˜¯ä»€ä¹ˆã€å®ƒä»¬èƒ½åšä»€ä¹ˆä»¥åŠå¦‚ä½•åˆ¶ä½œä½ è‡ªå·±çš„æœ‰äº†æ›´å¥½çš„äº†è§£ã€‚</p><p id="1a6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼</p><h2 id="ef59" class="ol nj iq bd nk ot ou dn no ov ow dp ns ko ox oy nu ks oz pa nw kw pb pc ny pd bi translated">æ—¶äº‹é€šè®¯</h2><p id="0742" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">å…³äºæ–°åšå®¢æ–‡ç« å’Œé¢å¤–å†…å®¹çš„æ›´æ–°ï¼Œè¯·æ³¨å†Œæˆ‘çš„æ—¶äº‹é€šè®¯ã€‚</p><div class="lc ld gp gr le lf"><a href="https://mailchi.mp/6304809e49e7/matthew-stewart" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">æ—¶äº‹é€šè®¯è®¢é˜…</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">ä¸°å¯Œæ‚¨çš„å­¦æœ¯ä¹‹æ—…ï¼ŒåŠ å…¥ä¸€ä¸ªç”±ç§‘å­¦å®¶ï¼Œç ”ç©¶äººå‘˜å’Œè¡Œä¸šä¸“ä¸šäººå£«ç»„æˆçš„ç¤¾åŒºï¼Œä»¥è·å¾—â€¦</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">mailchi.mp</p></div></div><div class="lo l"><div class="pp l lq lr ls lo lt lu lf"/></div></div></a></div><h1 id="385e" class="ni nj iq bd nk nl pq nn no np pr nr ns jw ps jx nu jz pt ka nw kc pu kd ny nz bi translated">è¿›ä¸€æ­¥é˜…è¯»</h1><p id="8c00" class="pw-post-body-paragraph kf kg iq kh b ki oa jr kk kl ob ju kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated"><strong class="kh ir">åœ¨ COLAB ä¸­è¿è¡Œ BigGAN:</strong></p><ul class=""><li id="04e0" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated"><a class="ae lw" href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/github/tensor flow/hub/blob/master/examples/colab/biggan _ generation _ with _ TF _ hub . ipynb</a></li></ul><p id="1d01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">æ›´å¤šä»£ç å¸®åŠ©+ç¤ºä¾‹:</strong></p><ul class=""><li id="11c2" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated"><a class="ae lw" href="https://www.jessicayung.com/explaining-tensorflow-code-for-a-convolutional-neural-network/" rel="noopener ugc nofollow" target="_blank">https://www . jessicayung . com/explaining-tensor flow-code-for-a-å·ç§¯ç¥ç»ç½‘ç»œ/ </a></li><li id="9eda" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae lw" href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2017/08/20/from-GAN-to-wgan . html</a></li><li id="2a1c" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae lw" href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/åˆå­¦è€…/dcgan_faces_tutorial.html </a></li><li id="d205" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae lw" href="https://github.com/tensorlayer/srgan" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorlayer/srgan</a></li><li id="c925" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae lw" href="https://junyanz.github.io/CycleGAN/" rel="noopener ugc nofollow" target="_blank">https://junyanz.github.io/CycleGAN/</a>T14ã€‘https://affinelayer.com/pixsrv/</li><li id="674f" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae lw" href="https://tcwang0509.github.io/pix2pixHD/" rel="noopener ugc nofollow" target="_blank">https://tcwang0509.github.io/pix2pixHD/</a></li></ul><p id="4b52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">æœ‰å½±å“åŠ›çš„è®ºæ–‡:</strong></p><ul class=""><li id="9dab" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated">https://arxiv.org/pdf/1511.06434v2.pdf</li><li id="4ea2" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">ç“¦ç‘Ÿæ–¯å¦ç”˜<a class="ae lw" href="https://arxiv.org/pdf/1701.07875.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1701.07875.pdf</a></li><li id="9c3f" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘(CGAN)<a class="ae lw" href="https://arxiv.org/pdf/1411.1784v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1411.1784v1.pdf</a></li><li id="1477" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">ä½¿ç”¨å¯¹æŠ—ç½‘ç»œçš„æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”çš„æ·±åº¦ç”Ÿæˆå›¾åƒæ¨¡å‹(æ‹‰æ™®æ ¹)<a class="ae lw" href="https://arxiv.org/pdf/1506.05751.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.05751.pdf</a></li><li id="ac13" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">ä½¿ç”¨ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œ(SRGAN)çš„ç…§ç‰‡çº§å•å¹…å›¾åƒè¶…åˆ†è¾¨ç‡<a class="ae lw" href="https://arxiv.org/pdf/1609.04802.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1609.04802.pdf</a></li><li id="7549" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">ä½¿ç”¨å¾ªç¯ä¸€è‡´å¯¹æŠ—ç½‘ç»œçš„ä¸æˆå¯¹å›¾åƒåˆ°å›¾åƒç¿»è¯‘<a class="ae lw" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1703.10593.pdf</a></li><li id="ec51" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">InfoGAN:é€šè¿‡ä¿¡æ¯æœ€å¤§åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„å¯è§£é‡Šè¡¨ç¤ºå­¦ä¹ <a class="ae lw" href="https://arxiv.org/pdf/1606.03657" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1606.03657</a></li><li id="0ea1" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">https://arxiv.org/pdf/1704.00028.pdf çš„ DCGAN <a class="ae lw" href="https://arxiv.org/pdf/1704.00028.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="2710" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">ç“¦ç‘Ÿæ–¯å¦Â·ç”˜æ–¯çš„å¼ºåŒ–è®­ç»ƒ(WGAN-GP)ã€https://arxiv.org/pdf/1701.07875.pdf T4</li><li id="4fa6" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">åŸºäºèƒ½é‡çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(EBGAN)<a class="ae lw" href="https://arxiv.org/pdf/1609.03126.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1609.03126.pdf</a></li><li id="cd63" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">ä½¿ç”¨å­¦ä¹ çš„ç›¸ä¼¼æ€§åº¦é‡(VAE-ç”˜)å¯¹åƒç´ ä¹‹å¤–çš„å†…å®¹è¿›è¡Œè‡ªåŠ¨ç¼–ç <a class="ae lw" href="https://arxiv.org/pdf/1512.09300.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1512.09300.pdf</a></li><li id="795b" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">å¯¹æŠ—æ€§ç‰¹å¾å­¦ä¹ (https://arxiv.org/pdf/1605.09782v6.pdf<a class="ae lw" href="https://arxiv.org/pdf/1605.09782v6.pdf" rel="noopener ugc nofollow" target="_blank">ç”˜æ¯”</a></li><li id="8c3f" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">å †å ç”Ÿæˆæ•Œå¯¹ç½‘ç»œ(SGAN)<a class="ae lw" href="https://arxiv.org/pdf/1612.04357.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.04357.pdf</a></li><li id="705a" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">StackGAN++ä½¿ç”¨å †å å¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œè¿›è¡Œç°å®å›¾åƒåˆæˆ<a class="ae lw" href="https://arxiv.org/pdf/1710.10916.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1710.10916.pdf</a></li><li id="f2d1" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">é€šè¿‡å¯¹æŠ—è®­ç»ƒ(SimGAN)ä»æ¨¡æ‹Ÿå’Œæ— ç›‘ç£å›¾åƒä¸­å­¦ä¹ <a class="ae lw" href="https://arxiv.org/pdf/1612.07828v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.07828v1.pdf</a></li></ul></div></div>    
</body>
</html>