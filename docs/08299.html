<html>
<head>
<title>Step-by-Step Signal Processing with Machine Learning: PCA, ICA, NMF for source separation, dimensionality reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逐步信号处理与机器学习:主成分分析，独立分量分析，NMF 源分离，降维</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/step-by-step-signal-processing-with-machine-learning-pca-ica-nmf-8de2f375c422?source=collection_archive---------13-----------------------#2019-11-12">https://towardsdatascience.com/step-by-step-signal-processing-with-machine-learning-pca-ica-nmf-8de2f375c422?source=collection_archive---------13-----------------------#2019-11-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d27f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于如何在 Python 中从头开始使用 PCA 执行降维以及使用 ICA 和 NMF 执行源分离的教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/28cd5a4e4f37eadb879d01f0b8bad546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ckANeUGmQBdbC3EW.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://www.surfline.com/contests/wave-of-the-winter/north-shore/2018-2019/" rel="noopener ugc nofollow" target="_blank">https://www.surfline.com/contests/wave-of-the-winter/north-shore/2018-2019/</a></figcaption></figure><p id="cc7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">信号处理在许多数据科学任务中至关重要。一旦我们开始处理音频文件、图像甚至生物测量，了解处理这些数据的技术是很有用的。</p><p id="aee2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将介绍三种算法，你可以用在两个用例中:<strong class="lb iu">主成分分析</strong> (PCA)用于降维和特征提取，<strong class="lb iu">独立成分分析</strong> (ICA)和<strong class="lb iu">非负矩阵分解</strong> (NMF)用于源分离。</p><p id="f6a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这三个方法在<a class="ae ky" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>上都有现成的实现，对您的项目很有用，但出于本文的目的，我将展示如何从头开始实现这些方法，只使用<a class="ae ky" href="https://opencv.org/" rel="noopener ugc nofollow" target="_blank"> OpenCV </a>打开和保存图像，使用<a class="ae ky" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> NumPy </a>处理矩阵。</p><p id="11f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将在整篇文章中提供代码片段，您可以在<a class="ae ky" href="https://github.com/kayoyin/signal-processing" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到完整的代码和示例数据集。</p><h1 id="4671" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">主成分分析</h1><p id="a52c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我将总结实现 PCA 的要点，并向热心的读者推荐这篇<a class="ae ky" href="https://sebastianraschka.com/Articles/2014_pca_step_by_step.html" rel="noopener ugc nofollow" target="_blank">很棒的文章</a>，它给出了更全面的解释。</p><p id="3140" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PCA 的主要目的是找到我们数据集的特征子集<strong class="lb iu">能够最好地捕获整个数据的信息，以便我们能够以最小的信息损失</strong>减少维度<strong class="lb iu">。例如，我们可以通过在将训练图像数据送入深度神经网络进行分类之前降低其维度来缩短计算时间。</strong></p><p id="e090" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有几种降维技术，如高相关滤波器、随机森林或后向特征消除。PCA 通过识别<strong class="lb iu">主成分</strong>来完成这项任务，主成分是原始特征的线性组合。提取这些分量，使得第一主分量包含数据集中的最大方差，第二主分量包含与第一主分量不相关的剩余方差，依此类推。</p><p id="1ee1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个例子，我们将使用手写数字 4 的 982 个图像的数据集。每个图像的尺寸为 28x28，但是在应用 PCA 之前，我们首先<strong class="lb iu">对数据进行整形</strong>,使得每个图像成为大小为 784 的 1D 向量，并且整个数据集的形状为 982x784。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/5e6ac494f297ccae998cb3705c35768b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_d8MvWwhIga3BAK-2R1ZA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Input images</figcaption></figure><p id="fd89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了使用 PCA 将特征的数量减少到<em class="mt"> k </em>，我们首先计算这个数据集的<strong class="lb iu">协方差</strong> <strong class="lb iu">矩阵</strong>。</p><p id="e8cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们计算协方差矩阵的<strong class="lb iu">特征向量</strong>和相应的<strong class="lb iu">特征值</strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="5356" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了选择<em class="mt"> k </em>主分量，我们首先按照特征值的降序对特征向量和特征值对进行排序。我们选择具有<em class="mt"> k 个</em>最大特征值的特征向量，因为它们包含关于数据的最多信息。</p><p id="a7a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后我们得到一个形状为 784 x <em class="mt"> k </em>的矩阵<em class="mt"> W </em>，我们将其与数据集的转置取乘积，得到降维后的数据<em class="mt"> k </em> x 982。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="302c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了重建已经减少的数据，我们简单地将减少的数据与之前获得的矩阵<em class="mt"> W </em>的逆矩阵相乘。这样，我们获得了与原始数据具有相同维度的数据。最后，我们将 2D 数据整形为 982x28x28 维的 3D 矩阵，这样我们就有了一组可以再次可视化的图像。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="714e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了计算重建误差，我们可以通过比较原始图像和重建图像之间的每个图像像素来计算均方误差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/2a48cbd299ff05d2b74749034be9ff9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fqML5LS7WtiOONG8jX8XsA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Results of dimensionality reduction with PCA, error measured with MSE</figcaption></figure><p id="1730" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上结果表明，主成分<strong class="lb iu">越多</strong>我们取，<strong class="lb iu">越低</strong> <strong class="lb iu">重构误差</strong>就越大。这是因为维数降低得越多，我们丢失的可用于重建的原始信息就越多。选择最佳分量数以优化降维和信息之间的权衡的一种常见方法是计算每个分量数的 PCA 的<strong class="lb iu">解释方差</strong>，并选择方差在 95–99%之间的分量数。这确保我们不会丢失太多关于数据的信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/f94be74f9a4be4a3da71493da5e4b5c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nQrZmfQE3zmMnCJLb_MNpQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Left to right: original image, reconstruction from 2, 16, 64, 256 components</figcaption></figure><h1 id="62ed" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">独立成分分析</h1><p id="653a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在，我们有 1000 张图像，是四个手写数字 0，1，4，7 的组合，比例各不相同。我们希望从混合图像中恢复源图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/7f753c96e1715f9e92dcd090794e1ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ApkbGgMlADQonzDYEfPUpQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source images</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/e0cfe0e836f86906dc49035c9550e302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hBR8Krwf1viqZVe0Rgawgg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Mixed data</figcaption></figure><p id="5a8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种分离线性混合信号的方法是通过<strong class="lb iu">独立分量分析</strong> (ICA)。信号的混合可以定义为矩阵乘积<em class="mt"> WH = X </em>，其中<em class="mt"> H </em>是包含不同源信号的矩阵，<em class="mt"> W </em>定义混合期间源的比率，<em class="mt"> X </em>是混合输出。然后 ICA 旨在恢复矩阵<em class="mt"> W </em>，这样我们就可以计算<em class="mt"> H = WinvX </em>，其中<em class="mt"> Winv </em>是<em class="mt"> X </em>的逆。</p><p id="4bba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ICA 不能应用于任何混合信号。首先，混合信号必须是源信号的<strong class="lb iu">线性组合</strong>，原因如前矩阵乘积所示。接下来，ICA 假设源信号是<strong class="lb iu">独立的</strong>，而混合信号不是(因为它们共享相同的源信号)，并利用这一事实进行分离。最后，ICA 还假设源信号为<strong class="lb iu">非高斯信号</strong>，并使用中心极限定理，该定理意味着两个信号之和的分布比两个信号本身的分布更接近高斯分布。</p><p id="051d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用<a class="ae ky" href="https://en.wikipedia.org/wiki/FastICA" rel="noopener ugc nofollow" target="_blank"> FastICA </a>算法高效地进行 ICA。该算法用维度(n_sources，n_samples)的<strong class="lb iu">随机</strong> <strong class="lb iu">权重</strong> <em class="mt"> W </em>初始化，其中我们使用先验信息，我们有 4 个想要分离的源，并且我们有 1000 个图像样本。对于我们期望分离的每个源，我们迭代与该源相关联的权重。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="cb7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<em class="mt"> W </em>中的每个权重向量<em class="mt"> w </em>，该算法寻求最大化投影<em class="mt"> v </em> = <em class="mt"> w.TX </em>的<strong class="lb iu">非高斯程度(其中<em class="mt"> w.T </em>是<em class="mt"> w </em>的转置)。为了测量非高斯性，该算法使用一个<strong class="lb iu">非二次非线性函数</strong><em class="mt">f(v)</em>它的一阶导数<em class="mt"> g(v) </em>和它的二阶导数<em class="mt"> g'(v)。</em></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/efdb612ad7b7d49f869f7fb9a162df06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQilpOITBNGHDkx-DM7BTQ.png"/></div></div></figure><p id="7729" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这些函数，我们使用<strong class="lb iu">梯度下降</strong>来更新权重。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="bdff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们<strong class="lb iu">归一化</strong>和<strong class="lb iu">去相关</strong>权重。去相关是为了确保后续源的迭代不会返回与在前一次迭代中获得的源相同的输出。最后，我们检查<strong class="lb iu">收敛</strong>来决定是停止还是继续迭代。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="1155" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了从 ICA 的输出<em class="mt"> W </em>中获得原始源，我们简单地取<em class="mt"> W </em>和给定输入数据的乘积，以获得维度矩阵(<em class="mt"> n_sources </em>，<em class="mt"> n_features </em>)，然后我们将它整形为原始的 3D 维度以获得图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/0fb3a8e920f1759ae1e2f5b7d9161fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*896mR0cCkH1qndYuYqvMlQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Four sources separated by ICA</figcaption></figure><p id="eca4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述 ICA 的结果表明，它能够在某种程度上提取图像 0、4 和 7，但它不能提取图像 1，相反，它返回的图像似乎是几个源图像叠加在一起。让我们看看我们是否能做得更好…</p><h1 id="50bc" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">非负矩阵分解</strong></h1><p id="6b63" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">分离混合信号的第二种方法是<strong class="lb iu">非负矩阵分解</strong> (NMF)。像 ICA 一样，NMF 也旨在将混合数据<em class="mt"> X </em>分解成矩阵<em class="mt"> WH = X </em>的乘积，但这里有一个附加约束，即每个矩阵<em class="mt"> X，W，H </em>都是<strong class="lb iu">非负的</strong>。这种约束在数据本质上是非负的应用中提供了优势，并且给出了具有更好的可解释性的结果。</p><p id="903d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了应用 NMF，我们从随机初始化 <em class="mt"> W </em>和<em class="mt"> H </em>的<strong class="lb iu">开始。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="230d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，在每次迭代中，我们首先更新新的<em class="mt"> H </em>给定旧的<em class="mt"> H </em>和<em class="mt"> W </em>，然后更新新的<em class="mt"> W </em>给定<em class="mt"> H </em>和旧的<em class="mt"> W </em>，使用<strong class="lb iu">乘法更新规则</strong>直到收敛。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/7c87dca923bb60746cf497d6614a3697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cgEtE0rAEiohgha-h-VWMg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" rel="noopener ugc nofollow" target="_blank">Multiplicative update rules</a></figcaption></figure><p id="2d7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用<em class="mt"> X </em>和<em class="mt"> WH </em>之间的 Frobenius 范数来检查收敛性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="03bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们对获得的矩阵进行归一化，并在保存可视化之前对其进行整形，使其包含 2D 图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/f3a47ba58670cf83be57ccf37a95ffef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d0gLyux-443GUCSwPz-qzA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Four sources separated by NMF</figcaption></figure><p id="d626" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在上面看到的，在我们的例子中，NMF 算法比独立分量分析好得多。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="97b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读我的文章，我希望你喜欢它！我希望开始向我的<a class="ae ky" href="https://github.com/kayoyin/signal-processing" rel="noopener ugc nofollow" target="_blank">信号处理库</a>添加更多的方法，所以如果接下来有什么需要我介绍的，请告诉我:)</p></div></div>    
</body>
</html>