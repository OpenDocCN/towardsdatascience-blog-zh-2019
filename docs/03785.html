<html>
<head>
<title>Simple Linear vs Polynomial Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单线性回归与多项式回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-vs-polynomial-regression-walk-through-83ca4f2363a3?source=collection_archive---------5-----------------------#2019-06-15">https://towardsdatascience.com/linear-vs-polynomial-regression-walk-through-83ca4f2363a3?source=collection_archive---------5-----------------------#2019-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5587" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">蓝鳃鱼:根据年龄预测长度</h2></div><p id="b3ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">鱼越老越大。鱼的长度(厘米)与年龄(年)有多大的预测性？这种关系是否最适合简单的线性回归？</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/162c5541f950e9bf0bd7f15b3d2677e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0wlLDmUl7FP95sZKkWd6Q.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Source of image: Kelly <a class="ae lu" href="https://unsplash.com/photos/92ef3opsrLU" rel="noopener ugc nofollow" target="_blank">Sikkema</a></figcaption></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="82ec" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">目录</strong></h1><ol class=""><li id="2ba4" class="mu mv it kk b kl mw ko mx kr my kv mz kz na ld nb nc nd ne bi translated">获取数据、可视化和初步分析</li><li id="ec3d" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated">线性回归分析</li><li id="9e7a" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated">二次和高次多项式回归分析</li><li id="6925" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated">将数据分为训练和测试</li><li id="4a8d" class="mu mv it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated">每种回归类型的测试模型(线性、二次、六次)</li></ol></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="2c2d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">第一部分:获取数据，可视化，和初步分析</strong></h1><p id="e948" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr nk kt ku kv nl kx ky kz nm lb lc ld im bi translated">首先，让我们引入数据和几个重要模块:</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="6f81" class="ns md it no b gy nt nu l nv nw">%matplotlib inline<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn.metrics import r2_score<br/>import matplotlib.pyplot as plt<br/>from scipy import stats<br/>import seaborn as sns</span><span id="34ae" class="ns md it no b gy nx nu l nv nw">data = pd.read_excel(r”C:\Users\...\fish polynomial.xlsx”)<br/>df = data[[‘age’,’length’]]<br/>df</span></pre><p id="c60b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集中有 77 个实例。下面是头部的原始数据:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e2c5f29e56105e114b9e3d2c45790ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*MJBbqmOdLfKegSvjlGTCzA.png"/></div></figure><p id="5894" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们来想象散点图。我们将尝试根据年龄来预测长度，因此坐标轴位于各自的位置:</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="5fbd" class="ns md it no b gy nt nu l nv nw">x = df['age']<br/>y = df['length']<br/>plt.xlabel('Age (yr)')<br/>plt.ylabel('Length (cm)')<br/>plt.title('Scatterplot of Length vs Age')<br/>plt.scatter(x,y)<br/>plt.show()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/8af29cf4429755b5fe195646a610253a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*vQDlP5BeXMUgerjj62LUig.png"/></div></figure><p id="42df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这似乎是一种趋势。随着鱼年龄的增长，似乎和长度有关系。为了获得更深入的视角，让我们将每个轴分解成它自己的单变量分布直方图。</p><p id="b869" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">Y——鱼的长度:</strong></p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="8bd8" class="ns md it no b gy nt nu l nv nw">stdy = y.std()<br/>meany = y.mean()<br/>plt.hist(y)<br/>plt.xlabel('Length (cm)')<br/>plt.ylabel('Quantity')<br/>plt.title('Length Distribution Histogram')<br/>print ("The std of y is: " + str(stdy) + " The mean of y is: " + str(meany))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oa"><img src="../Images/7e9d93f59975f77f8d2f1af1870d110f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cSrhNUjP-SbzigLceSv0mA.png"/></div></div></figure><p id="2155" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">X——鱼的年龄:</strong></p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="2952" class="ns md it no b gy nt nu l nv nw">stdx = x.std()<br/>meanx = x.mean()<br/>plt.hist(x)<br/>plt.xlabel('Age (yr)')<br/>plt.ylabel('Quantity')<br/>plt.title('Age Distribution Histogram')<br/>print ("The std of x is: " + str(stdx) + " The mean of x is: " + str(meanx))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ob"><img src="../Images/924080209b53f52fcef8203a4f7abdf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j-hPmLCfdD7yQvPrIu71GQ.png"/></div></div></figure><p id="c064" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，平均长度为 143.6 厘米，平均年龄为 3.6 岁。年龄的标准偏差成比例地高于长度的标准偏差，这意味着年龄分布的总体分布更大。这是尽管长度分布具有更高的范围。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="d399" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">第二部分:简单线性回归</strong></h1><p id="7b80" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr nk kt ku kv nl kx ky kz nm lb lc ld im bi translated">简单线性回归是预测模型的基本类型之一。简而言之，它通过对数据拟合线性方程来衡量两个变量之间的关系。一个变量被认为是解释性的(年龄)，另一个被认为是依赖性的(长度)。回归模型最小化直线和每个数据点之间的距离，以找到最佳拟合。</p><p id="0cff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面，我们使用 scipy stats 模块来计算我们产品线的关键指标。下面输出的分别是截距和斜率。</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="b927" class="ns md it no b gy nt nu l nv nw">slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)<br/>intercept,slope</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oc"><img src="../Images/a0e89b53ae7dd75719330f2ebf51e98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_vzpYm0urwqJW2bHiLwB6Q.png"/></div></div></figure><p id="cfe1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将通过线性方程运行 x 的每个值，在原始散点图上绘制这条线:<strong class="kk iu"> y = b0 + b1x </strong></p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="0b5c" class="ns md it no b gy nt nu l nv nw">sns.set(color_codes=True)</span><span id="1f74" class="ns md it no b gy nx nu l nv nw">def linefitline(b):<br/>    return intercept + slope * b</span><span id="8024" class="ns md it no b gy nx nu l nv nw">line = linefitline(x)<br/>plt.scatter(x,y)<br/>plt.xlabel('Age (yr)')<br/>plt.ylabel('Length (cm)')<br/>plt.title('Scatterplot of Length vs Age - Linear Regression')<br/>plt.plot(x,line, c = 'g')<br/>plt.show()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f119cbb38670fe78a49a4ef0c33db7dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*3ZyYd5KRAZTLYWDSsTzprA.png"/></div></figure><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="fad7" class="ns md it no b gy nt nu l nv nw">r2_lin = r_value * r_value</span><span id="2d0c" class="ns md it no b gy nx nu l nv nw">print('The rsquared value is: ' + str(r2_lin))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/1494abd6f4920f11e4cf40815e36715d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*sCrw_APKvKff3lQ3MorKVQ.png"/></div></figure><p id="ddf5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这条回归线的平方值显示年龄解释了 73%的长度变化。R-squared 计算回归线与其拟合的数据的相似程度。我写了另一篇关于 rsquared 的文章，你可以在这里参考<a class="ae lu" rel="noopener" target="_blank" href="/r-squared-recipe-5814995fa39a"/>。</p><blockquote class="of og oh"><p id="eb04" class="ki kj oi kk b kl km ju kn ko kp jx kq oj ks kt ku ok kw kx ky ol la lb lc ld im bi translated">其他可以解释剩余的 27%长度差异的独立变量可能是食物可用性、水质、阳光、鱼类遗传等。如果我们有所有这些属性的数据，我们可以运行多元回归，并有一个更好的模型。但是，唉，我们生活在一个数据有限的世界。</p></blockquote><p id="1fba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直观上，我们的简单线性模型并不经过 y 点的每个簇的中间点。对于第 1 年、第 2 年、第 5 年和第 6 年，它都高于该组的中间值。这很有可能是因为大多数样本是 3 岁和 4 岁的孩子，正如我们前面看到的。这将向上移动线条。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c4d41540ef7b60db0209fff6e6517b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*mUxDgxBP2ArqtA3KQwrFLw.png"/></div></figure><p id="c3a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">似乎最佳曲线需要弯曲以更准确地匹配数据。这就是多项式回归的用武之地。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="cc37" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">第三部分:二次和高次多项式回归分析</strong></h1><p id="9f5c" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr nk kt ku kv nl kx ky kz nm lb lc ld im bi translated">简单来说，多项式回归模型可以弯曲。它们可以构造到 n 次，以最小化平方误差，最大化 rsquared。取决于 n 次，最佳拟合的线可以有更多或更少的曲线。指数越高，曲线越多。</p><p id="c177" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面我们有一些代码来创建新的线，并在我们的散点图上绘制它。这条线是一个二次函数，因为它只有二次幂。二次线只能弯曲一次。正如我们在下图中看到的，新的多项式模型更准确地匹配了数据。</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="a793" class="ns md it no b gy nt nu l nv nw">x = df['age']<br/>y = df['length']<br/>plt.xlabel('Age (yr)')<br/>plt.ylabel('Length (cm)')<br/>plt.title('Scatterplot of Length vs Age')<br/>p = np.poly1d(np.polyfit(x,y,2))<br/>xp = np.linspace(1,6,100)<br/>plt.plot(xp,p(xp),c = 'r')<br/>plt.scatter(x,y)<br/>plt.show()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi om"><img src="../Images/3fa1ba1fa5a20ca40ef06f39ed42f480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*yAOJJCZbjsqOp0cu_RGJOg.png"/></div></figure><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="ceca" class="ns md it no b gy nt nu l nv nw">r2 = r2_score(y, p(x))</span><span id="6865" class="ns md it no b gy nx nu l nv nw">print('The rsquared value is: ' + str(r2))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi on"><img src="../Images/7d2e18bed81c19d5139487f74c3e70e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*p622jT-T5L6Ng2rVZfxtlQ.png"/></div></figure><blockquote class="of og oh"><p id="9b37" class="ki kj oi kk b kl km ju kn ko kp jx kq oj ks kt ku ok kw kx ky ol la lb lc ld im bi translated">与我们在简单线性模型中看到的 0.73 值相比，rsquared 值为 0.80。这意味着在这个新模型中，80%的长度是由它们的年龄来解释的。</p></blockquote><p id="b77f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在可以尝试改变模型的第 n 个值，看看我们是否能找到一个更好的拟合线。然而，我们必须记住，我们走得越高，我们面临的风险就越大。</p><p id="b473" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是一个提升到<strong class="kk iu">6 次</strong>的多项式的例子:</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="4a47" class="ns md it no b gy nt nu l nv nw">x = df['age']<br/>y = df['length']<br/>plt.xlabel('Age (yr)')<br/>plt.ylabel('Length (cm)')<br/>plt.title('Scatterplot of Length vs Age')<br/>p = np.poly1d(np.polyfit(x,y,6))<br/>xp = np.linspace(1,6,100)<br/>plt.plot(xp,p(xp),c = 'b')<br/>plt.scatter(x,y)<br/>plt.show()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/9df597deec7efc58e43d2ac01f7244e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*JBfp7RpuIIHbm186wDwhhg.png"/></div></figure><p id="a92c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它对异常值的迎合有点过于极端，而且与数据过于接近。这个模型的平方值是 0.804，比二次模型高不了多少。</p><p id="95a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这个数据集，大多数人会同意二次函数最匹配。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="3d2e" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">第 4 部分:将数据分割成训练和测试</strong></h1><p id="af6f" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr nk kt ku kv nl kx ky kz nm lb lc ld im bi translated">我们如何知道二次多项式回归直线可能是这个数据集的最佳拟合？这就是测试我们的数据的概念的来源！</p><p id="b05b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们打乱数据集。然后我们需要把它分成训练和测试部分。训练数据用于创建我们的模型；测试数据用于查看模型的匹配程度。有一个<a class="ae lu" href="https://dataandbeyond.wordpress.com/2017/08/24/split-of-train-and-test-data/" rel="noopener ugc nofollow" target="_blank">经验法则</a>来划分 70%的培训和 30%的测试。因为这是一个相对较小的鱼样本量(n = 78)，所以我决定在测试方面多做一点。我做了 50 次作为训练，最后 28 次作为测试。那是 65/35 的分成。</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="7c3d" class="ns md it no b gy nt nu l nv nw">shuffled = df.sample(frac=1).reset_index(drop=True)<br/>shuffx = shuffled['age']<br/>shuffy = shuffled['length']<br/>trainX = shuffx[:50]<br/>testX = shuffx[50:]<br/>trainY = shuffy[:50]<br/>testY = shuffy[50:]<br/>plt.scatter(trainX, trainY)<br/>plt.xlabel('Age (yr)')<br/>plt.ylabel('Length (cm)')<br/>plt.title('Testing Data - Length vs Age')<br/>axes = plt.axes()<br/>axes.set_xlim([1,6])<br/>axes.set_ylim([50, 200])<br/>plt.show()<br/>plt.scatter(testX, testY)<br/>plt.xlabel('Age (yr)')<br/>plt.ylabel('Length (cm)')<br/>plt.title('Testing Data - Length vs Age')<br/>axes = plt.axes()<br/>axes.set_xlim([1,6])<br/>axes.set_ylim([50, 200])<br/>plt.show()</span></pre><p id="73b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与测试相比，训练图中有更多的点，这是由于我们的 65/35 分割:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/effa98d07da1d611d94a253f82f791c5.png" data-original-src="https://miro.medium.com/v2/format:webp/1*ZyjMvA2L4l0dW6z6k26X7g.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/3496f06aa488d27f310f9146d23dd57f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*7j0z9oDzlu9C1ZSwrVJ9Iw.png"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8832" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">第 5 部分:每个回归类型的测试模型</strong></h1><p id="e571" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr nk kt ku kv nl kx ky kz nm lb lc ld im bi translated"><strong class="kk iu">线性模型测试:</strong></p><p id="199a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将根据训练数据创建一个线性模型:</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="f148" class="ns md it no b gy nt nu l nv nw">p1 = np.poly1d(np.polyfit(trainx, trainy, 1))<br/>xp = np.linspace(0, 6, 100)<br/>axes = plt.axes()<br/>axes.set_xlim([1,6])<br/>axes.set_ylim([50, 200])<br/>plt.scatter(trainx, trainy)<br/>plt.xlabel('Age (yr)')<br/>plt.ylabel('Length (cm)')<br/>plt.title('Training Linear Regression')<br/>plt.plot(xp, p1(xp), c='r')<br/>plt.show()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/380856c4992bacaf9250345b68983fab.png" data-original-src="https://miro.medium.com/v2/format:webp/1*xbHjanGVPMma94rqCT9lkw.png"/></div></figure><p id="5f12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这条线与训练集非常匹配。“0.75”的平方值相当不错:</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="7393" class="ns md it no b gy nt nu l nv nw">r2_train = r2_score(trainy, p1(trainx))<br/>print('The rsquared value is: ' + str(r2_train))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/90acfb6e20863389f60364654bb29ff9.png" data-original-src="https://miro.medium.com/v2/format:webp/1*emY4yMmJEwzQktoC72kzCw.png"/></div></figure><p id="7810" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们看看这条线如何与我们保存的测试数据相匹配:</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="615f" class="ns md it no b gy nt nu l nv nw">xp = np.linspace(0, 6, 100)<br/>axes = plt.axes()<br/>axes.set_xlim([1,6])<br/>axes.set_ylim([50, 200])<br/>plt.scatter(testx, testy)<br/>plt.xlabel('Age (yr)')<br/>plt.ylabel('Length (cm)')<br/>plt.title('Testing Linear Regression')<br/>plt.plot(xp, p1(xp), c='r')<br/>plt.show()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/3d65a4e004021267c3cff8300670166e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*beunBEGzvloUPynrW5VvOg.png"/></div></figure><p id="120b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个模型看起来不错。它没有考虑 6 岁时的异常值，而且相对于每个年龄组的中值长度来说，它看起来也有点低。这些担心在较低的平方值中是合理的:</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="acca" class="ns md it no b gy nt nu l nv nw">r2_test = r2_score(testy, p1(testx))<br/>print('The rsquared value is: ' + str(r2_test))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/2c17024265a2e97bf4dc7fc964d918a7.png" data-original-src="https://miro.medium.com/v2/format:webp/1*0stFSB2a8fJ8PwznLvb76g.png"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="864a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">二次模型测试:</strong></p><p id="b6d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看二次回归与简单线性回归的比较。这些计算的代码与上面的计算非常相似，在 numpy.polyfit 方法中定义回归时，只需将中的“1”更改为“2 ”:</p><p id="3755" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="oi">p2</em></strong><em class="oi">= NP . poly 1d(NP . poly fit(trainx，trainy，</em> <strong class="kk iu"> <em class="oi"> 2 </em> </strong> <em class="oi">))。</em></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/1676c6ae2b69b9e91fb43fb5969f5764.png" data-original-src="https://miro.medium.com/v2/format:webp/1*0NvcwtAbBf5FcpIhRWDfow.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/6942a8025cf11279500221f9e22b25f1.png" data-original-src="https://miro.medium.com/v2/format:webp/1*7FWw5az7EAxIpgYG-VlBzw.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/813554f0264445b4be5050442d79d3e1.png" data-original-src="https://miro.medium.com/v2/format:webp/1*lqwOGCKn00nbhxaNiWM-dg.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/423798e2ef480c6e2b09413f41b4d46b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*3KkyIX6FkVk_6WHfYrQxvg.png"/></div></figure><p id="0f7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">二次回归具有较高的训练方差。此外，与测试数据相比，它几乎没有下降多少。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="475b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">性别(n6)回归检验:</strong></p><p id="8ebc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，让我们看看高次多项式回归是如何执行的:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/8a3a141e599f481abaeb614e6bac15b0.png" data-original-src="https://miro.medium.com/v2/format:webp/1*ud1q8n9SMWaDNl-bC1mEjA.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/ad4d31d1c2455f281073793ebca797d0.png" data-original-src="https://miro.medium.com/v2/format:webp/1*M_knNTU-US_dY9JAnKNWBQ.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/c079a7ab978f880d226acea557cbbb99.png" data-original-src="https://miro.medium.com/v2/format:webp/1*wLnZ9IJsyRAJ9cniae1S5g.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/e73e61283e97a2c9a76f532a38362196.png" data-original-src="https://miro.medium.com/v2/format:webp/1*_efm2q_f9lRfJWGlNkzMAw.png"/></div></figure><p id="c302" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型以最高的训练平方值开始，但是当实际测试时，它直线下降。这是<a class="ae lu" href="https://www.investopedia.com/terms/o/overfitting.asp" rel="noopener ugc nofollow" target="_blank">过度装配</a>的症状。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="62fc" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">结论:</strong></h1><p id="8ff8" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr nk kt ku kv nl kx ky kz nm lb lc ld im bi translated">二次回归最适合本例中的数据。然而，在训练/测试分割之前的混洗会影响这些结果。根据哪些行结束于哪个段，rsquared 结果可能会略有不同。对于像这样的小数据集来说尤其如此。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl oo"><img src="../Images/95ebf827a39f9a950510ed135e3b4e93.png" data-original-src="https://miro.medium.com/v2/format:webp/1*vIafYptz1KFcwMBqN1-haQ.png"/></div></figure><p id="ed07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">原始数据来源:<a class="ae lu" href="https://newonlinecourses.science.psu.edu/stat501/node/325/" rel="noopener ugc nofollow" target="_blank">宾夕法尼亚州立大学</a>。</p><p id="2d5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你觉得这有帮助，请订阅、鼓掌或评论。</p><p id="5081" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9b53" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我的其他文章，如果你想了解更多:</p><h2 id="5c8d" class="ns md it bd me op oq dn mi or os dp mm kr ot ou mo kv ov ow mq kz ox oy ms oz bi translated">点击了解逻辑回归<a class="ae lu" rel="noopener" target="_blank" href="/univariate-logistic-regression-example-in-python-acbefde8cc14"/></h2><h2 id="eb02" class="ns md it bd me op oq dn mi or os dp mm kr ot ou mo kv ov ow mq kz ox oy ms oz bi translated">点击了解有关 rsquared <a class="ae lu" rel="noopener" target="_blank" href="/r-squared-recipe-5814995fa39a">的信息:</a></h2></div></div>    
</body>
</html>