<html>
<head>
<title>Explain Any Models with the SHAP Values — Use the KernelExplainer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释任何具有 SHAP 值的模型—使用内核解释器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explain-any-models-with-the-shap-values-use-the-kernelexplainer-79de9464897a?source=collection_archive---------0-----------------------#2019-11-07">https://towardsdatascience.com/explain-any-models-with-the-shap-values-use-the-kernelexplainer-79de9464897a?source=collection_archive---------0-----------------------#2019-11-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="975b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对 SHAP 值使用 KernelExplainer</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/0933a9aaad31a63ab30ab297417c35ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*d5mdjPD7LLQLZEnX7cGR5w.png"/></div></figure><p id="a353" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">自从我发表了文章“<a class="ae kw" rel="noopener" target="_blank" href="/explain-your-model-with-the-shap-values-bc36aac4de3d">用 SHAP 值</a>解释你的模型”，这是建立在一个随机的森林树上，读者们一直在问是否有一个通用的 SHAP 解释器适用于任何 ML 算法——无论是基于树的还是非基于树的算法。这正是<em class="kx">内核解释器</em>，一个模型不可知的方法，被设计来做的。在本文中，我将演示如何使用 KernelExplainer 构建 KNN、SVM、Random Forest、GBM 或 H2O 模块的模型。如果你想了解更多关于 SHAP 价值观的背景，我强烈推荐“<a class="ae kw" rel="noopener" target="_blank" href="/explain-your-model-with-the-shap-values-bc36aac4de3d">用 SHAP 价值观</a>解释你的模型”，其中我详细描述了 SHAP 价值观是如何从沙普利价值观中产生的，什么是博弈论中的沙普利价值观，以及 SHAP 价值观是如何在 Python 中工作的。除了 SHAP，你可能还想在“<a class="ae kw" href="https://medium.com/@Dataman.ai/explain-your-model-with-lime-5a1a5867b423" rel="noopener">用 LIME </a>解释你的模型”中查看 LIME，在“<a class="ae kw" href="https://medium.com/@Dataman.ai/explain-your-model-with-microsofts-interpretml-5daab1d693b4" rel="noopener">用微软的 InterpretML </a>解释你的模型”中查看微软的 InterpretML。我还在“<a class="ae kw" href="https://dataman-ai.medium.com/the-shap-with-more-elegant-charts-bc3e73fa1c0c" rel="noopener">SHAP 和更优雅的图表</a>”和“<a class="ae kw" href="https://medium.com/dataman-in-ai/the-shap-values-with-h2o-models-773a203b75e3" rel="noopener">SHAP 价值和 H2O 模型</a>”中记录了 SHAP 的最新发展。</p><p id="a842" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">想深入了解机器学习算法的读者，可以查看我的帖子<a class="ae kw" href="https://medium.com/analytics-vidhya/a-lecture-note-on-random-forest-gradient-boosting-and-regularization-834fc9a7fa52" rel="noopener">我的随机森林、梯度提升、正则化、H2O.ai </a>讲义。对于深度学习，请查看“<a class="ae kw" href="https://levelup.gitconnected.com/a-tutorial-to-build-from-regression-to-deep-learning-b7354240d2d5" rel="noopener ugc nofollow" target="_blank">以回归友好的方式解释深度学习</a>”。对于 RNN/LSTM/GRU，查看“<a class="ae kw" href="https://medium.com/swlh/a-technical-guide-on-rnn-lstm-gru-for-stock-price-prediction-bce2f7f30346" rel="noopener">RNN/LSTM/GRU 股价预测技术指南</a>”。</p><p id="9f7e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">用 SHAP 价值观诠释你的精致模型</strong></p><p id="de5e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">考虑这个问题:“你复杂的机器学习模型容易理解吗？”这意味着您的模型可以通过具有商业意义的输入变量来理解。你的变量将符合用户的期望，他们已经从先前的知识中学习。</p><p id="ff65" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Lundberg 等人在其出色的论文“<a class="ae kw" href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf" rel="noopener ugc nofollow" target="_blank">解释模型预测的统一方法</a>”中提出了 SHAP (SHapley 附加解释)值，该值为模型提供了高水平的可解释性。SHAP 价值观提供了两大优势:</p><ol class=""><li id="e464" class="ky kz it js b jt ju jx jy kb la kf lb kj lc kn ld le lf lg bi translated"><em class="kx">全局可解释性</em>-SHAP 值可以显示每个预测值对目标变量的贡献大小，无论是正面还是负面。这类似于变量重要性图，但它可以显示每个变量与目标之间的正或负关系(参见下面的汇总图)。</li><li id="bdb1" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn ld le lf lg bi translated"><em class="kx">局部可解释性</em>——每个观察值都有自己的一套 SHAP 值(见下面的个体力图)。这大大增加了它的透明度。我们可以解释为什么一个案例得到它的预测和预测者的贡献。传统的可变重要性算法只显示整个群体的结果，而不是每个案例的结果。本地可解释性使我们能够精确定位和对比这些因素的影响。</li></ol><p id="28d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">SHAP 值可以由<a class="ae kw" href="https://shap.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Python 模块 SHAP </a>产生。</p><p id="192a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">模型的可解释性并不意味着因果关系</strong></p><p id="ed22" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">需要指出的是，SHAP 值并不提供因果关系。在“<strong class="js iu">识别因果关系</strong>”系列文章中，我展示了识别因果关系的计量经济学技术。这些文章涵盖了以下技术:回归不连续性(参见“<a class="ae kw" href="https://medium.com/@Dataman.ai/identify-causality-by-regression-discontinuity-a4c8fb7507df" rel="noopener">通过回归不连续性确定因果关系</a>”)、差异中的差异(DiD)(参见“<a class="ae kw" href="https://medium.com/@Dataman.ai/identify-causality-by-difference-in-differences-78ad8335fb7c" rel="noopener">通过差异中的差异确定因果关系</a>”)、固定效应模型(参见“<a class="ae kw" href="https://medium.com/@Dataman.ai/identify-causality-by-fixed-effects-model-585554bd9735" rel="noopener">通过固定效应模型确定因果关系</a>”)以及采用因子设计的随机对照试验(参见“<a class="ae kw" rel="noopener" target="_blank" href="/design-of-experiments-for-your-change-management-8f70880efcdd">您的变更管理实验设计</a>”)。</p><p id="16ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">内核解释器是做什么的？</p><p id="cf51" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">KernelExplainer 通过使用您的数据、预测以及预测预测值的任何函数来构建加权线性回归。它根据博弈论的 Shapley 值和局部线性回归的系数计算可变的重要性值。</p><p id="140a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">KernelExplainer 的缺点是运行时间长。如果你的模型是基于树的机器学习模型，你应该使用已经优化的树解释器<code class="fe lm ln lo lp b">TreeExplainer()</code>来快速渲染结果。如果你的模型是深度学习模型，使用深度学习讲解器<code class="fe lm ln lo lp b">DeepExplainer()</code>。<a class="ae kw" href="https://shap.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">SHAP Python 模块</a>还没有专门针对所有类型算法的优化算法(比如 KNNs)。</p><p id="bae3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在“<a class="ae kw" rel="noopener" target="_blank" href="/explain-your-model-with-the-shap-values-bc36aac4de3d">用 SHAP 值解释你的模型</a>”中，我用函数<code class="fe lm ln lo lp b">TreeExplainer()</code>创建了一个随机森林模型。为了让您比较结果，我将使用相同的数据源，但是使用函数<code class="fe lm ln lo lp b">KernelExplainer()</code>。</p><p id="e9a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我将为所有算法重复以下四个图:</p><ol class=""><li id="1162" class="ky kz it js b jt ju jx jy kb la kf lb kj lc kn ld le lf lg bi translated">摘要情节:使用<code class="fe lm ln lo lp b">summary_plot()</code></li><li id="bc59" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn ld le lf lg bi translated">依赖情节:<code class="fe lm ln lo lp b">dependence_plot()</code></li><li id="6cec" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn ld le lf lg bi translated">个体力图:<code class="fe lm ln lo lp b">force_plot()</code>对于给定的观察值</li><li id="bf66" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn ld le lf lg bi translated">集体力剧情:<code class="fe lm ln lo lp b">force_plot()</code>。</li></ol><p id="73ff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">完整的代码可以在文章末尾找到，或者通过 Github 找到。</p><div class="lq lr gp gr ls lt"><a href="https://dataman-ai.medium.com/membership" rel="noopener follow" target="_blank"><div class="lu ab fo"><div class="lv ab lw cl cj lx"><h2 class="bd iu gy z fp ly fr fs lz fu fw is bi translated">通过我的推荐链接加入 Medium-Chris Kuo/data man 博士</h2><div class="ma l"><h3 class="bd b gy z fp ly fr fs lz fu fw dk translated">阅读 Chris Kuo/data man 博士的每一个故事。你的会员费直接支持郭怡广/戴塔曼博士和其他…</h3></div><div class="mb l"><p class="bd b dl z fp ly fr fs lz fu fw dk translated">dataman-ai.medium.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh ku lt"/></div></div></a></div><p id="a9e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">随机森林</strong></p><p id="5d66" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，让我们加载在“<a class="ae kw" rel="noopener" target="_blank" href="/explain-your-model-with-the-shap-values-bc36aac4de3d">用 SHAP 值解释您的模型</a>”中使用的相同数据。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="e101" class="mm mn it lp b gy mo mp l mq mr">import pandas as pd<br/>import numpy as np<br/>np.random.seed(0)<br/>import matplotlib.pyplot as plt<br/>df = pd.read_csv('/winequality-red.csv') # Load the data<br/>from sklearn.model_selection import train_test_split<br/>from sklearn import preprocessing<br/>from sklearn.ensemble import RandomForestRegressor<br/># The target variable is 'quality'.<br/>Y = df['quality']<br/>X =  df[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density','pH', 'sulphates', 'alcohol']]<br/># Split the data into train and test data:<br/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)</span></pre><p id="a743" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们构建一个随机森林模型，并打印出变量重要性。SHAP 建立在最大似然算法的基础上。如果你想深入了解机器学习算法，可以查看我的帖子“<a class="ae kw" href="https://medium.com/analytics-vidhya/a-lecture-note-on-random-forest-gradient-boosting-and-regularization-834fc9a7fa52" rel="noopener">我关于随机森林、梯度提升、正则化、H2O.ai </a>的讲义”。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="f6a5" class="mm mn it lp b gy mo mp l mq mr">rf = RandomForestRegressor(max_depth=6, random_state=0, n_estimators=10)<br/>rf.fit(X_train, Y_train)  <br/>print(rf.feature_importances_)</span><span id="b8d2" class="mm mn it lp b gy ms mp l mq mr">importances = rf.feature_importances_<br/>indices = np.argsort(importances)</span><span id="aafe" class="mm mn it lp b gy ms mp l mq mr">features = X_train.columns<br/>plt.title('Feature Importances')<br/>plt.barh(range(len(indices)), importances[indices], color='b', align='center')<br/>plt.yticks(range(len(indices)), [features[i] for i in indices])<br/>plt.xlabel('Relative Importance')<br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/1e152041d3247073bed96f2bce701b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*Z85R_md0RMqISrqQaJjJ2A.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Figure (A) Random Forest Variable Importance Plot</figcaption></figure><p id="8317" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以从这个<a class="ae kw" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">Github</a>pip 安装 SHAP。下面的函数<code class="fe lm ln lo lp b">KernelExplainer()</code>通过采用预测方法<code class="fe lm ln lo lp b">rf.predict</code>和您想要执行 SHAP 值的数据来执行局部回归。这里我使用测试数据集 X_test，它有 160 个观察值。这一步可能需要一段时间。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="ea1a" class="mm mn it lp b gy mo mp l mq mr">import shap<br/>rf_shap_values = shap.KernelExplainer(rf.predict,X_test)</span></pre><ol class=""><li id="a0fe" class="ky kz it js b jt ju jx jy kb la kf lb kj lc kn ld le lf lg bi translated">概要情节</li></ol><p id="706e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此图加载了信息。该图与常规变量重要性图(图 A)的最大区别在于，它显示了预测值与目标变量之间的正相关和负相关关系。它看起来有点古怪，因为它是由列车数据中的所有点组成的。让我向您介绍一下:</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="5be6" class="mm mn it lp b gy mo mp l mq mr">shap.summary_plot(rf_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c788f933177d1d642dc3cd1aa1c47133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*kNoGMmCLErOu546Aq3TkYg.png"/></div></figure><ul class=""><li id="dd8b" class="ky kz it js b jt ju jx jy kb la kf lb kj lc kn mz le lf lg bi translated"><em class="kx">特征重要性:</em>变量按降序排列。</li><li id="d113" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated"><em class="kx">影响:</em>水平位置显示该值<em class="kx">的影响是否与更高或更低的预测</em>相关联。</li><li id="6cbe" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated"><em class="kx">原始值:</em>颜色显示该变量对于该观察值是高(红色)还是低(蓝色)。</li><li id="a43f" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated"><em class="kx">相关性:</em>高<em class="kx"/>水平的“酒精”含量对质量评级有高<em class="kx">和正</em>的影响。“高”来自红色，而“积极的”影响显示在 X 轴上。同样，我们会说“挥发性酸度”与目标变量负相关。</li></ul><p id="bc09" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您想要保存汇总图。虽然 SHAP 没有内置的保存图的功能，但是您可以使用<code class="fe lm ln lo lp b">matplotlib</code>输出图:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="35e2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.依赖图</p><p id="3fdb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kx">部分依赖图</em>，简称依赖图，在机器学习结果中很重要(<a class="ae kw" href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank"> J. H. Friedman 2001 </a>)。它显示了一个或两个变量对预测结果的边际效应。它表明目标和变量之间的关系是线性的、单调的还是更复杂的。我在文章“<a class="ae kw" href="https://dataman-ai.medium.com/how-is-the-partial-dependent-plot-computed-8d2001a0e556" rel="noopener">中提供了更多详细信息:如何计算部分相关图？</a>”</p><p id="350b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设我们想得到“酒精”的依赖图。SHAP 模块包括另一个与“酒精”互动最多的变量。下图显示“酒精”与目标变量之间存在近似线性的正趋势，“酒精”与“残糖”频繁交互。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="402c" class="mm mn it lp b gy mo mp l mq mr">shap.dependence_plot("alcohol", rf_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/a1d8df3c3dfb10820f26db885ad60f32.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*-MTjKC_mevGb5Cs4Xq2AKg.png"/></div></figure><p id="a7a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.个体力图</p><p id="f636" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以为每个观察值绘制一个非常优雅的图，称为<em class="kx">力图</em>。我任意选择了 X_test 数据的第 10 个观测值。下面是 X_test 的平均值，以及第 10 次观察的值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/4086c8332ac631d942f9c1f806b5fa97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*R4atAXnnrOmECXc_xFBaSw.png"/></div></figure><p id="5427" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Pandas 像 base R 一样使用<code class="fe lm ln lo lp b">.iloc()</code>来划分数据帧的行。其他语言开发者可以看我的帖子“<a class="ae kw" rel="noopener" target="_blank" href="/are-you-bilingual-be-fluent-in-r-and-python-7cb1533ff99f">你是双语吗？精通 R 和 Python </a>”中，我比较了 R dply 和 Python Pandas 中最常见的数据争论任务。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="ec87" class="mm mn it lp b gy mo mp l mq mr"># plot the SHAP values for the 10th observation <br/>shap.force_plot(rf_explainer.expected_value, rf_shap_values[10,:], X_test.iloc[10,:])</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/3a26c3343d963607d32831b4be696ed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ulAGAzpRvSzTVp1PVl_alg.png"/></div></div></figure><ul class=""><li id="2054" class="ky kz it js b jt ju jx jy kb la kf lb kj lc kn mz le lf lg bi translated"><em class="kx">输出值</em>是该观测值的预测值(该观测值的预测值为 5.11)。</li><li id="44ad" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated"><em class="kx">基值</em>:<a class="ae kw" href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>解释说，基值 E(y_hat)是“如果我们不知道当前输出的任何特征，将会预测的值。”换句话说，它是均值预测，或均值(yhat)。你可能会奇怪为什么是 5.634。这是因为 Y_test 的预测均值为 5.634。你可以通过产生 5.634 的<code class="fe lm ln lo lp b">Y_test.mean()</code>来测试一下。</li><li id="176a" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated"><em class="kx">红色/蓝色</em>:将预测值推高(向右)的特征显示为红色，将预测值推低的特征显示为蓝色。</li><li id="1d0c" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated">酒精:对质量评级有积极影响。这种酒的酒精度是 9.4，低于平均值 10.48。所以它把预测推到左边。</li><li id="0950" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated">二氧化硫总量:与质量等级正相关。高于平均水平的二氧化硫(= 18 &gt; 14.98)将预测推向右边。</li><li id="7352" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated">该图以位于<code class="fe lm ln lo lp b">explainer.expected_value</code>的 x 轴为中心。所有 SHAP 值都与模型的期望值相关，就像线性模型的效果与截距相关一样。</li></ul><p id="df0a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.集体力量图</p><p id="1b4a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每个观察都有它的力图。如果把所有的力图组合起来，旋转 90 度，水平叠加，我们就得到整个数据 X_test 的力图(见 Lundberg 等贡献者的<a class="ae kw" href="https://github.com/slundberg/shap/blob/master/README.md" rel="noopener ugc nofollow" target="_blank"> GitHub </a>的解释)。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="4152" class="mm mn it lp b gy mo mp l mq mr">shap.force_plot(rf_explainer.expected_value, rf_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/9ffe55f04cee1a64beda952a5a5feafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVMny3FcuD8_6h2_mFB5Yg.png"/></div></div></figure><p id="c094" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">合力图上面的 Y 轴是单个力图的 X 轴。在我们的 X_test 中有 160 个数据点，所以 X 轴有 160 个观察值。</p><p id="3353" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> GBM </strong></p><p id="dee4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我用 500 棵树(缺省值是 100)构建了 GBM，这对于过度拟合应该是相当健壮的。我使用超参数<code class="fe lm ln lo lp b">validation_fraction=0.2</code>指定 20%的训练数据提前停止。如果验证结果在 5 次后没有改善，该超参数和<code class="fe lm ln lo lp b">n_iter_no_change=5</code>将帮助模型更早停止。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="ee65" class="mm mn it lp b gy mo mp l mq mr">from sklearn import ensemble<br/>n_estimators = 500<br/>gbm = ensemble.GradientBoostingClassifier(<br/>            n_estimators=n_estimators,<br/>            validation_fraction=0.2,<br/>            n_iter_no_change=5, <br/>            tol=0.01,<br/>            random_state=0)<br/>gbm = ensemble.GradientBoostingClassifier(<br/>            n_estimators=n_estimators,<br/>            random_state=0)<br/>gbm.fit(X_train, Y_train)</span></pre><p id="6d67" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">像上面的随机森林部分一样，我使用函数<code class="fe lm ln lo lp b">KernelExplainer()</code>来生成 SHAP 值。那我就提供四个情节。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="c9ed" class="mm mn it lp b gy mo mp l mq mr">import shap<br/>gbm_shap_values = shap.KernelExplainer(gbm.predict,X_test)</span></pre><ol class=""><li id="5b75" class="ky kz it js b jt ju jx jy kb la kf lb kj lc kn ld le lf lg bi translated">概要情节</li></ol><p id="f86b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当与随机森林的输出相比较时，GBM 对于前四个变量显示相同的变量排序，但是对于其余变量不同。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="107b" class="mm mn it lp b gy mo mp l mq mr">shap.summary_plot(gbm_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/67def52acc1d3fb51d120e7a9d24aad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*CJxPgk8rx0mkoM82Z-wg8g.png"/></div></figure><p id="8c8f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.依赖图</p><p id="140e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GBM 的依赖图也显示“酒精”和目标变量之间存在近似线性的正趋势。与随机森林的输出相反，GBM 显示“酒精”与“密度”频繁交互。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="be41" class="mm mn it lp b gy mo mp l mq mr">shap.dependence_plot("alcohol", gbm_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/eb3b829744fd238fc3c71c1218be2f9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*MR8_FS4zaVGO2jt-KPCApg.png"/></div></figure><p id="70c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.个体力图</p><p id="d08c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我继续为 X_test 数据的第 10 次观察制作力图。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="c96d" class="mm mn it lp b gy mo mp l mq mr"># plot the SHAP values for the 10th observation <br/>shap.force_plot(gbm_explainer.expected_value,gbm_shap_values[10,:], X_test.iloc[10,:]) </span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nm"><img src="../Images/05b27b7455e72eeb911cc56181eaffad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1NJkKkK5SPsI7VCWq-Gpw.png"/></div></div></figure><p id="542b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该观察的 GBM 预测是 5.00，不同于随机森林的 5.11。推动预测的力量与随机森林的力量相似:酒精、硫酸盐和残余糖分。但是推动预测上升的力量是不同的。</p><p id="de10" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.集体力量图</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="affd" class="mm mn it lp b gy mo mp l mq mr">shap.force_plot(gbm_explainer.expected_value, gbm_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nn"><img src="../Images/06e97c85bd5a02f50969168902063742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*88ACNchOEBbxcpxJo6yaoA.png"/></div></div></figure><p id="f4b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> KNN </strong></p><p id="4cc0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为这里的目标是展示 SHAP 价值观，所以我只设置了 KNN 15 个邻居，而不太关心优化 KNN 模型。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="356f" class="mm mn it lp b gy mo mp l mq mr"># Train the KNN model<br/>from sklearn import neighbors<br/>n_neighbors = 15<br/>knn = neighbors.KNeighborsClassifier(n_neighbors,weights='distance')<br/>knn.fit(X_train,Y_train)</span><span id="b4df" class="mm mn it lp b gy ms mp l mq mr"># Produce the SHAP values<br/>knn_explainer = shap.KernelExplainer(knn.predict,X_test)<br/>knn_shap_values = knn_explainer.shap_values(X_test)</span></pre><ol class=""><li id="21ce" class="ky kz it js b jt ju jx jy kb la kf lb kj lc kn ld le lf lg bi translated">概要情节</li></ol><p id="e4ba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有趣的是，与随机森林或 GBM 的输出相比，KNN 显示了不同的变量排序。这种背离是意料之中的，因为 KNN 容易出现异常值，这里我们只训练一个 KNN 模型。为了缓解这一问题，建议您使用不同数量的邻居构建几个 KNN 模型，然后获取平均值。我的文章“<a class="ae kw" rel="noopener" target="_blank" href="/anomaly-detection-with-pyod-b523fc47db9">异常检测与 PyOD </a>”也分享了这种直觉。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="adfd" class="mm mn it lp b gy mo mp l mq mr">shap.summary_plot(knn_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/d6213f03ce6816a89f0df608b4f5ee8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*N47vYcCd1AOALrkm7mCVRg.png"/></div></figure><p id="2bc9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.依赖图</p><p id="a03f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">KNN 的输出显示在“酒精”和目标变量之间存在近似线性的正趋势。与随机森林的输出不同，KNN 显示“酒精”与“总二氧化硫”的交互频繁。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="6761" class="mm mn it lp b gy mo mp l mq mr">shap.dependence_plot("alcohol", knn_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a88be990b0981b5a781658b5d27240c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*zK6tKaHPh2GdyVPQm-8-Gw.png"/></div></figure><p id="92dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.个体力图</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="ad3b" class="mm mn it lp b gy mo mp l mq mr"># plot the SHAP values for the 10th observation <br/>shap.force_plot(knn_explainer.expected_value,knn_shap_values[10,:], X_test.iloc[10,:])</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nq"><img src="../Images/9492dc36773755c32dafad4d28e3ccf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Csl6a19t1XFW-l43XGrGSw.png"/></div></div></figure><p id="5f97" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该观察的预测值为 5.00，与 GBM 的预测值相似。KNN 确定的驱动力是:“游离二氧化硫”、“酒精”和“残糖”。</p><p id="5a5c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.集体力量图</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="64fe" class="mm mn it lp b gy mo mp l mq mr">shap.force_plot(knn_explainer.expected_value, knn_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nr"><img src="../Images/e5ec5e9c04cd2d7c9e223eae27021602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ioe6hPCO1wCy1VhTVcvBYA.png"/></div></div></figure><p id="50cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> SVM </strong></p><p id="09ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">支持向量机(AVM)找到最佳超平面以将观察结果分成类别。SVM 使用核函数变换到更高维度的空间中进行分离。为什么在更高维的空间中分离会变得更容易？这还得回到 Vapnik-Chervonenkis (VC)理论。它说，映射到更高维度的空间通常会提供更大的分类能力。关于进一步的解释，请参见我的文章“使用 Python 的维度缩减技术”。常见的核函数有径向基函数(RBF)、高斯函数、多项式函数和 Sigmoid 函数。</p><p id="72e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个例子中，我使用带参数<code class="fe lm ln lo lp b">gamma</code>的径向基函数(RBF)。当<code class="fe lm ln lo lp b">gamma</code>的值很小时，模型受到的约束太大，无法捕捉数据的复杂性或“形状”。有两种选择:<code class="fe lm ln lo lp b">gamma='auto'</code>或<code class="fe lm ln lo lp b">gamma='scale'</code>(参见<a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn api </a>)。</p><p id="a2f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一个重要的超参数是<code class="fe lm ln lo lp b">decision_function_shape</code>。超参数<code class="fe lm ln lo lp b">decision_function_shape</code>告诉 SVM 一个数据点离超平面有多近。靠近边界的数据点意味着低置信度决策。有两个选项:一对一对休息(' ovr ')或一对一(' ovo ')(参见<a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn api </a>)。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="dcd8" class="mm mn it lp b gy mo mp l mq mr"># Build the SVM model<br/>from sklearn import svm<br/>svm = svm.SVC(gamma='scale',decision_function_shape='ovo')<br/>svm.fit(X_train,Y_train)</span><span id="37a7" class="mm mn it lp b gy ms mp l mq mr"># The SHAP values<br/>svm_explainer = shap.KernelExplainer(svm.predict,X_test)<br/>svm_shap_values = svm_explainer.shap_values(X_test)</span></pre><ol class=""><li id="3321" class="ky kz it js b jt ju jx jy kb la kf lb kj lc kn ld le lf lg bi translated">概要情节</li></ol><p id="d84a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里，我们再次从随机森林和 GBM 的输出中看到不同的汇总图。这是意料之中的，因为我们只训练了一个 SVM 模型，而 SVM 也容易出现异常值。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="5917" class="mm mn it lp b gy mo mp l mq mr">shap.summary_plot(svm_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e46bdfe853828821d643f0c17dd78d19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*KfNd2OEDIkQ3AVHhOaIpVw.png"/></div></figure><p id="a841" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.依赖图</p><p id="1d73" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">SVM 的输出在“酒精”和目标变量之间显示出温和的线性正趋势。与随机森林的输出相反，SVM 表明“酒精”经常与“固定酸度”相互作用。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="cdd1" class="mm mn it lp b gy mo mp l mq mr">shap.dependence_plot("alcohol", svm_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/3cd3de2413574d69d38e57269069434f.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*kDAACVDQJLWI-LN7ud-egQ.png"/></div></figure><p id="2ef5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.个体力图</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="4f16" class="mm mn it lp b gy mo mp l mq mr"># plot the SHAP values for the 10th observation <br/>shap.force_plot(svm_explainer.expected_value,svm_shap_values[10,:], X_test.iloc[10,:])</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nu"><img src="../Images/fad825e76b6ead72ef1fb0455e456ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BmkQZtufDcZrpJb7WfYk5A.png"/></div></div></figure><p id="bb90" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此次观测的 SVM 预测值为 6.00，不同于随机森林的 5.11。驱使预测降低的力量类似于随机森林的力量；相比之下，“二氧化硫总量”是推动预测上升的强大力量。</p><p id="4065" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.集体力量图</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="3898" class="mm mn it lp b gy mo mp l mq mr">shap.force_plot(svm_explainer.expected_value, svm_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nv"><img src="../Images/1215a0d25d2b588668da50fe5779af32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XIqZFGZszBHG05CwwteRJQ.png"/></div></div></figure><p id="0d91" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">型号内置开源 H2O </strong></p><p id="ada1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">许多数据科学家(包括我自己)喜欢开源的 H2O。它是一个完全分布式的内存平台，支持最广泛使用的算法，如 GBM、RF、GLM、DL 等。它的 AutoML 功能自动运行所有算法及其超参数，生成最佳模型排行榜。其企业版<a class="ae kw" href="https://www.h2o.ai/products/h2o-driverless-ai/?utm_source=google&amp;utm_medium=test&amp;utm_campaign=aiforbiz&amp;utm_content=business-insights&amp;gclid=Cj0KCQiA-4nuBRCnARIsAHwyuPq2IvnV5e8B0KAgwp_IBswaNcTEWf657Kd453yzHW2AzxFTlYaASSQaAvwIEALw_wcB" rel="noopener ugc nofollow" target="_blank"> H2O 无人驾驶 AI </a>内置 SHAP 功能。</p><p id="bf08" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如何将 SHAP 价值观应用于开源的 H2O？我非常感谢<a class="ae kw" href="https://github.com/SeanPLeary/shapley-values-h2o-example/blob/master/shap_h2o_automl_classification.ipynb" rel="noopener ugc nofollow" target="_blank"> seanPLeary </a>在如何用 AutoML 产生 SHAP 价值观方面为 H2O 社区做出的贡献。我使用他的类“H2OProbWrapper”来计算 SHAP 值。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="5aff" class="mm mn it lp b gy mo mp l mq mr"># The code builds a random forest model<br/>import h2o<br/>from h2o.estimators.random_forest import H2ORandomForestEstimator<br/>h2o.init()</span><span id="befc" class="mm mn it lp b gy ms mp l mq mr">X_train, X_test = train_test_split(df, test_size = 0.1)<br/>X_train_hex = h2o.H2OFrame(X_train)<br/>X_test_hex = h2o.H2OFrame(X_test)<br/>X_names =  ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density','pH', 'sulphates', 'alcohol']</span><span id="4bce" class="mm mn it lp b gy ms mp l mq mr"># Define model<br/>h2o_rf = H2ORandomForestEstimator(ntrees=200, max_depth=20, nfolds=10)</span><span id="2c03" class="mm mn it lp b gy ms mp l mq mr"># Train model<br/>h2o_rf.train(x=X_names, y='quality', training_frame=X_train_hex)</span><span id="6e10" class="mm mn it lp b gy ms mp l mq mr">X_test = X_test_hex.drop('quality').as_data_frame()</span></pre><p id="3a1a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们仔细看看 SVM 的代码<code class="fe lm ln lo lp b">shap.KernelExplainer(svm.predict, X_test)</code>。它采用了类<code class="fe lm ln lo lp b">svm</code>的函数<code class="fe lm ln lo lp b">predict</code>，以及数据集<code class="fe lm ln lo lp b">X_test</code>。因此，当我们应用 H2O 时，我们需要传递(I)预测函数，(ii)一个类，以及(iii)一个数据集。棘手的是，H2O 有自己的数据框架结构。为了将 h2O 的预测函数<code class="fe lm ln lo lp b">h2o.preict()</code>传递给<code class="fe lm ln lo lp b">shap.KernelExplainer()</code>,<a class="ae kw" href="https://github.com/SeanPLeary/shapley-values-h2o-example/blob/master/shap_h2o_automl_classification.ipynb" rel="noopener ugc nofollow" target="_blank">Sean pleay</a>将 H2O 的预测函数<code class="fe lm ln lo lp b">h2o.preict()</code>包装在一个名为<code class="fe lm ln lo lp b">H2OProbWrapper</code>的类中。这个漂亮的包装器允许<code class="fe lm ln lo lp b">shap.KernelExplainer()</code>接受类<code class="fe lm ln lo lp b">H2OProbWrapper</code>的函数<code class="fe lm ln lo lp b">predict</code>和数据集<code class="fe lm ln lo lp b">X_test</code>。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="1cb4" class="mm mn it lp b gy mo mp l mq mr">class H2OProbWrapper:<br/>    def __init__(self, h2o_model, feature_names):<br/>        self.h2o_model = h2o_model<br/>        self.feature_names = feature_names</span><span id="287b" class="mm mn it lp b gy ms mp l mq mr">def predict_binary_prob(self, X):<br/>        if isinstance(X, pd.Series):<br/>            X = X.values.reshape(1,-1)<br/>        self.dataframe= pd.DataFrame(X, columns=self.feature_names)<br/>        self.predictions = self.h2o_model.predict(h2o.H2OFrame(self.dataframe)).as_data_frame().values<br/>        return self.predictions.astype('float64')[:,-1] </span></pre><p id="c6c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，我们将计算 H2O 随机森林模型的 SHAP 值:</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="1e11" class="mm mn it lp b gy mo mp l mq mr">h2o_wrapper = H2OProbWrapper(h2o_rf,X_names)</span><span id="fa7a" class="mm mn it lp b gy ms mp l mq mr">h2o_rf_explainer = shap.KernelExplainer(h2o_wrapper.predict_binary_prob, X_test)<br/>h2o_rf_shap_values = h2o_rf_explainer.shap_values(X_test)</span></pre><ol class=""><li id="667b" class="ky kz it js b jt ju jx jy kb la kf lb kj lc kn ld le lf lg bi translated">概要情节</li></ol><p id="26c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当与随机森林的输出相比较时，H2O 随机森林对于前三个变量显示出相同的变量排序。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="1318" class="mm mn it lp b gy mo mp l mq mr">shap.summary_plot(h2o_rf_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/2677c9bf3d6d91ffc7b6435f87ad2c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*m_ws0m1I8i80qI5JSa6f6A.png"/></div></figure><p id="cbcc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.依赖图</p><p id="6e6e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出显示“酒精”和目标变量之间存在线性的正趋势。H2O·兰登森林发现“酒精”经常与“柠檬酸”相互作用。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="cbdd" class="mm mn it lp b gy mo mp l mq mr">shap.dependence_plot("alcohol", h2o_rf_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/148cec95e146de59c9683a705c87f3aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*dVZyua165h6FSqk58callw.png"/></div></figure><p id="61cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.个体力图</p><p id="f590" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此次观测的 H2O 随机森林预测值为 6.07。推动预测向右的力量是“酒精”、“密度”、“残糖”、“总二氧化硫”；左边是“固定酸度”和“硫酸盐”。</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="0958" class="mm mn it lp b gy mo mp l mq mr"># plot the SHAP values for the 10th observation <br/>shap.force_plot(h2o_rf_explainer.expected_value,h2o_rf_shap_values[10,:], X_test.iloc[10,:])</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nv"><img src="../Images/ba3f965077537933aaea9d2b48325019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lYPekmM4741ZrL5h5sIAnQ.png"/></div></div></figure><p id="f2ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.集体力量图</p><pre class="kp kq kr ks gt mi lp mj mk aw ml bi"><span id="7e3a" class="mm mn it lp b gy mo mp l mq mr">shap.force_plot(h2o_rf_explainer.expected_value, h2o_rf_shap_values, X_test)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nv"><img src="../Images/b25258d09ff0e9bb2baf59944c71f95d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MdNrMwMekGHAyf0K6xNzog.png"/></div></div></figure><p id="5194" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">R 中的 SHAP 值如何？</p><p id="1757" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里提到几个 SHAP 值的 R 包很有意思。R package <a class="ae kw" href="https://cran.r-project.org/web/packages/shapper/shapper.pdf" rel="noopener ugc nofollow" target="_blank"> shapper </a>是 Python 库 SHAP 的一个端口。R 包<a class="ae kw" href="https://cran.r-project.org/web/packages/xgboost/xgboost.pdf" rel="noopener ugc nofollow" target="_blank"> xgboost </a>有内置函数。另一个包是<a class="ae kw" href="https://cran.r-project.org/web/packages/iml/index.html" rel="noopener ugc nofollow" target="_blank"> iml </a>(可解释机器学习)。最后，R 包<a class="ae kw" href="https://cran.r-project.org/web/packages/DALEX/DALEX.pdf" rel="noopener ugc nofollow" target="_blank"> DALEX </a>(描述性机器学习解释)也包含各种解释器，帮助理解输入变量和模型输出之间的联系。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="5e1a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">SHAP 价值观没有做到的事情</strong></p><p id="5b2b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">自从我发表了这篇文章和它的姊妹文章“<a class="ae kw" rel="noopener" target="_blank" href="/explain-your-model-with-the-shap-values-bc36aac4de3d">用 SHAP 价值观解释你的模型</a>”之后，读者们分享了他们与客户会面时提出的问题。这些问题不是关于 SHAP 价值观的计算，而是观众思考 SHAP 价值观能做什么。一个主要的评论是“你能为我们确定制定策略的驱动因素吗？”</p><p id="732c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的评论是可信的，表明数据科学家已经提供了有效的内容。然而，这个问题涉及到相关性和因果性。<strong class="js iu">SHAP 值不能识别因果关系，通过实验设计或类似方法可以更好地识别因果关系。</strong>有兴趣的读者，请阅读我的另外两篇文章《<a class="ae kw" rel="noopener" target="_blank" href="/design-of-experiments-for-your-change-management-8f70880efcdd">为你的变革管理设计实验</a>》和《<a class="ae kw" href="https://medium.com/analytics-vidhya/machine-learning-or-econometrics-5127c1c2dc53" rel="noopener">机器学习还是计量经济学？</a></p><p id="bfbf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">所有代码</strong></p><p id="3c0b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了方便起见，所有的行都放在下面的代码块中，或者通过<a class="ae kw" href="https://github.com/dataman-git/codes_for_articles/blob/master/Explain%20any%20models%20with%20the%20SHAP%20values%20-%20the%20KernelExplainer%20for%20article.ipynb" rel="noopener ugc nofollow" target="_blank">这个 Github </a>。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="b7a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您发现这篇文章很有帮助，您可能想要查看模型可解释性系列:</p><p id="1b90" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一部分:<a class="ae kw" rel="noopener" target="_blank" href="/explain-your-model-with-the-shap-values-bc36aac4de3d">用 SHAP 值解释你的模型</a></p><p id="7cbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二部分:<a class="ae kw" href="https://dataman-ai.medium.com/the-shap-with-more-elegant-charts-bc3e73fa1c0c" rel="noopener">图表更优雅的 SHAP</a></p><p id="8347" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第三部分:<a class="ae kw" href="https://dataman-ai.medium.com/how-is-the-partial-dependent-plot-computed-8d2001a0e556" rel="noopener">部分依赖图是如何计算的？</a></p><p id="746a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第六部分:<a class="ae kw" href="https://medium.com/analytics-vidhya/an-explanation-for-explainable-ai-xai-d56ae3dacd13" rel="noopener">对可解释人工智能的解释</a></p><p id="8273" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第五部分:<a class="ae kw" rel="noopener" target="_blank" href="/explain-any-models-with-the-shap-values-use-the-kernelexplainer-79de9464897a">解释任何具有 SHAP 值的模型——使用内核解释器</a></p><p id="c7ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第六部分:<a class="ae kw" href="https://medium.com/dataman-in-ai/the-shap-values-with-h2o-models-773a203b75e3" rel="noopener">H2O 模型的 SHAP 值</a></p><p id="dbbe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第七部分:<a class="ae kw" href="https://medium.com/@Dataman.ai/explain-your-model-with-lime-5a1a5867b423" rel="noopener">用石灰解释你的模型</a></p><p id="79c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第八部分:<a class="ae kw" href="https://medium.com/@Dataman.ai/explain-your-model-with-microsofts-interpretml-5daab1d693b4" rel="noopener">用微软的 InterpretML </a>解释你的模型</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><div class="kp kq kr ks gt lt"><a href="https://dataman-ai.medium.com/membership" rel="noopener follow" target="_blank"><div class="lu ab fo"><div class="lv ab lw cl cj lx"><h2 class="bd iu gy z fp ly fr fs lz fu fw is bi translated">通过我的推荐链接加入 Medium-Chris Kuo/data man 博士</h2><div class="ma l"><h3 class="bd b gy z fp ly fr fs lz fu fw dk translated">阅读 Chris Kuo/data man 博士的每一个故事。你的会员费直接支持郭怡广/戴塔曼博士和其他…</h3></div><div class="mb l"><p class="bd b dl z fp ly fr fs lz fu fw dk translated">dataman-ai.medium.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh ku lt"/></div></div></a></div><p id="62e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">建议读者购买郭怡广的书籍:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oe"><img src="../Images/af0606e4ec6f1e63e47cbc39119cee5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/0*UTx-k1Chh7jWG4Em.png"/></div></div></figure><ul class=""><li id="3364" class="ky kz it js b jt ju jx jy kb la kf lb kj lc kn mz le lf lg bi translated">可解释的人工智能:<a class="ae kw" href="https://a.co/d/cNL8Hu4" rel="noopener ugc nofollow" target="_blank">https://a.co/d/cNL8Hu4</a></li><li id="db42" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated">图像分类的迁移学习:<a class="ae kw" href="https://a.co/d/hLdCkMH" rel="noopener ugc nofollow" target="_blank">https://a.co/d/hLdCkMH</a></li><li id="2632" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated">现代时间序列异常检测:<a class="ae kw" href="https://a.co/d/ieIbAxM" rel="noopener ugc nofollow" target="_blank">https://a.co/d/ieIbAxM</a></li><li id="fa70" class="ky kz it js b jt lh jx li kb lj kf lk kj ll kn mz le lf lg bi translated">异常检测手册:<a class="ae kw" href="https://a.co/d/5sKS8bI" rel="noopener ugc nofollow" target="_blank">https://a.co/d/5sKS8bI</a></li></ul></div></div>    
</body>
</html>