<html>
<head>
<title>Efficient method for running Fully Convolutional Networks (FCNs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">运行全卷积网络的有效方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/efficient-method-for-running-fully-convolutional-networks-fcns-3174dc6a692b?source=collection_archive---------14-----------------------#2019-08-27">https://towardsdatascience.com/efficient-method-for-running-fully-convolutional-networks-fcns-3174dc6a692b?source=collection_archive---------14-----------------------#2019-08-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="89e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">全卷积(深度神经)网络(fcn)通常用于计算机视觉任务，如语义分割、超分辨率等。它们最好的特性之一是它们适用于任何大小的输入，例如不同大小的图像。然而，在大规模输入(如高分辨率图像或视频)上运行这些网络可能会消耗大量 GPU 内存。在这篇文章中，我将介绍一个简单的方法来缓解这个问题。所提出的算法将 GPU 内存使用率降低到只有 3–30%。</p><h1 id="38d6" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">什么是全卷积网络？</strong></h1><p id="a8bf" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">一个完全卷积的网络(从现在开始:FCN)是一个仅由卷积层组成的网络。如果你对卷积层不熟悉，这里的<a class="ae lo" rel="noopener" target="_blank" href="/a-beginners-guide-to-convolutional-neural-networks-cnns-14649dbddce8"/>是初学者指南。为了简单起见，在这篇文章中，我们将关注图像，但这同样适用于视频或其他类型的数据。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/411f6a680fc323e6b58b9ff331f76132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fpcruf9wnrE04N61ghLS3g.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">A fully convolutional network for segmentation. input and output images from <a class="ae lo" href="https://devblogs.nvidia.com/image-segmentation-using-digits-5/" rel="noopener ugc nofollow" target="_blank">Nvidia dev blog</a></figcaption></figure><p id="7a82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于图像处理中的许多任务，要求输入和输出图像具有相同的尺寸是很自然的。这可以通过使用具有适当填充的 fcn 来实现(TensorFlow 中的<a class="ae lo" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" rel="noopener ugc nofollow" target="_blank">‘相同’)。因为这是一个标准程序，从现在开始我们假设它成立。</a></p><p id="3ed6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mf">在这种架构下，输出图像中的每个像素都是对输入图像中的相应补片进行计算的结果。</em>T9】</strong></p><p id="e78c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mf">这个斑块的大小称为网络的感受野(RF)。</em>T13】</strong></p><p id="51b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一个关键点。我们将很快看到算法如何使用这个属性</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/70c83e823eed459870839e7b2c277f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*DMjhTb-F30hs-jc81y9oyw.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">FCN with one layer: 3x3 convolution. each output pixel corresponds to a<strong class="bd mh"> 3x3</strong> patch in the input image</figcaption></figure><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/408650f883c975f80dcd7b02a540c04d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*CDdDj7z69_jFJ5EeOkx1ag.gif"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">FCN with 2 layers: each is a 3x3 convolution. each output pixel corresponds to a <strong class="bd mh">5x5</strong> patch in the input image</figcaption></figure><p id="e5d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面我们可以看到带有一个(顶部)和两个(底部)3×3 conv 层的 fcn 的插图。在一层(顶部)的情况下，右侧的蓝色输出像素是对左侧蓝色输入面片的计算结果。当有两层(底部)时，我们在输入和输出之间有一个特征图。特征图中的每个绿色像素(中间)都是在一个 3×3 绿色输入面片(左侧)上计算的结果，与一个图层的情况相同。类似地，每个蓝色输出像素(右)是对蓝色 3x3 特征图块(中)的计算结果，该特征图块源自 5x5 蓝色输入块(左)。</p><h1 id="1f88" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">所以..有什么问题</strong>？</h1><p id="dfa7" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">如前所述，理论上，我们可以将网络应用于任何输入大小。然而，实际上，计算通过网络的正向传递需要在内存中保存巨大的特征图，这会耗尽 GPU 资源。我在研究视频和 3d 图像时遇到了这个限制。网络根本无法在我们花哨的 NVIDIA v100 GPUs 上运行，这促使我开发了这个解决方案。</p><h1 id="6843" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">通过 FCN 传递大量输入的有效方式</strong></h1><p id="09ff" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">传统 CNN 以完全连接的层结束。因此，每个输出像素都是整个输入的计算结果。在 fcn 中情况并非如此。正如我们所见，只有来自输入的感受野大小的小块影响单个输出像素。因此，要计算单个输出像素，不需要将整个特征图保存在内存中！</p><p id="8b48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说:</p><p id="edc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mf">我们可以一次计算一小部分输出值，同时只传递输入中必要的像素。这大大减少了 GPU 的内存使用！</em> </strong></p><p id="2eb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来看看下面的例子:<br/>我们的输入是一个 28x28 的图像，如下图所示。由于内存限制，我们可以通过最大 12x12 的网络补丁。最简单的方法是从原始输入(下图中的轮廓)输入 12×12 的面片，并使用 12×12 的输出面片来构建 28×28 的输出图像。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/d3283d2d09be2ea4cf28db6114a840d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*mIts9MU7scBa8PXyMP5Ybw.gif"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Each output pixel is the result of a different patch passing through the network. For example, orange pixels are the result of passing the orange 12x12 patch.</figcaption></figure><p id="9c5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不幸的是，这种方法不会产生与一起传递整个 28x28 输入相同的结果。原因是边界效应。</p><h1 id="9cc0" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">解决边界效应</h1><p id="ce15" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">为了理解这个问题，让我们看看下图中标记的红色像素。周围的红色方块代表感受野，在本例中大小为 5x5，跨越蓝色和橙色区域。为了正确计算红色输出像素，我们需要同时计算蓝色和橙色像素。因此，如果我们像在天真的方法中那样分别运行蓝色和橙色的补丁，我们将没有必要的像素来精确地计算它的值。显然，我们需要另一种方法。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/b618a0bec882750072d19b643947ee8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*JVx7eFzL-T9d-HBS0J1Acw.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Red — a boundary pixel and the receptive field around it</figcaption></figure><p id="1828" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么我们如何才能让它变得正确呢？我们可以使用<strong class="jp ir">重叠的</strong>补丁，这样每个 5×5 补丁将包含在通过网络的 12×12 补丁之一中。重叠量应该比感受野(RF -1)少 1 个像素。在我们的例子中，感受野 RF=5，所以我们需要 4 个像素的重叠。下面的动画演示了不同的图像补丁如何在给定的限制下通过网络传递。它显示了每个 12×12 输入面片(轮廓)在输出(填充)中贡献了更少量的像素。例如，蓝色方形轮廓比蓝色像素填充的区域大。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/4b6705cf1d9532d36c91a4a2f83a18f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*9vtP7sNteWdvUERJ9miQsQ.gif"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Overlapping patches. For example, orange pixels are the result of passing the orange 12x12 patch.</figcaption></figure><p id="37b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回到我们标记的红色像素。现在，如下图所示，由于重叠，它可以正确计算。它的 5x5 周围补丁完全包含在橙色 12x12 补丁中(轮廓)</p><p id="6818" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mf">实际上，每个像素都有一个 12×12 的小块包含其感受野大小的周边</em> </strong> <em class="mf">。</em> <strong class="jp ir"> <em class="mf">这样我们可以确保同时运行整个 28x28 图像的效果。</em> </strong></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/95cfef7163c94260862c8e7bce03dddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*7XrVu90o5Zz4d1pYeqGkuw.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">red — a boundary pixel and the receptive field around it</figcaption></figure><h1 id="f4d5" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结果</h1><p id="31fe" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">下表给出了一些关于 GPU 内存使用和运行时间的实验结果。正如所承诺的，内存消耗是显着减少！</p><p id="34c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:尽管每个图像的运行时间较慢，但是使用这种方法我们可以并行传递多个图像，因此<strong class="jp ir">节省了时间</strong>。例如，在第 3 行中，使用该算法我们可以同时运行 13 个图像，这将花费与使用直接方法仅运行 6 个图像一样多的时间。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mk"><img src="../Images/f68af8ba5f9977378a1381b34c3ae761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ov6BTGQtbDRyJvF-tlcyAQ.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">with patch by patch algorithm GPU memory usage is reduced by 67%-97% compared to standard method</figcaption></figure><p id="e545" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实验在 NVIDIA Tesla K80 GPU 上运行。</p><p id="cf95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢本·范斯坦、因娜·格里涅维奇、阿维夫·内塔尼亚胡和<a class="ae lo" href="https://www.facebook.com/profile.php?id=100009603042010&amp;fref=gs&amp;__tn__=%2CdlC-R-R&amp;eid=ARDN5dslDbkN_2uFKrGWbq2vcDPRbaAnjl8U_U692Yemo1nhHAbjzbertqeUHGyto7gIcbc6cElvUy5C&amp;hc_ref=ART9kBGQIb4vP1sv47pW19nMtpWuaglZXreTu9doUCi81dm9iXoDQYHkLOxQt0KO3ns&amp;dti=1215117901926035&amp;hc_location=group" rel="noopener ugc nofollow" target="_blank">阿耶莱特·萨皮尔斯坦</a></p></div></div>    
</body>
</html>