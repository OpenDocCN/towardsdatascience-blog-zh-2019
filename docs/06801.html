<html>
<head>
<title>A classic bedtime story: Cinderella of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">经典睡前故事:神经网络的灰姑娘</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-classic-bedtime-story-cinderella-of-neural-networks-fa9a3414286f?source=collection_archive---------31-----------------------#2019-09-27">https://towardsdatascience.com/a-classic-bedtime-story-cinderella-of-neural-networks-fa9a3414286f?source=collection_archive---------31-----------------------#2019-09-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="953d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">“人工智能冬天”的最后阶段</h2><div class=""/><div class=""><h2 id="b37e" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">一个竞赛，ImageNet，连同一个嘈杂的算法，随机梯度下降，是如何改变 AI 的命运的？</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/c9108efde71b23c2f01198617633f2de.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*HrSR_lXcoM4QdaRnnh50Sg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Picture from The Elders Scroll | Skyrim</figcaption></figure><p id="d60c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">20 世纪 80 年代初，人工智能(AI)的<em class="lw">冬天即将到来</em>，这是一段人工智能研究资金和兴趣减少的时期，后来被称为“AI 冬天”！</p><p id="c2f8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在这个持续到 2000 年代中期的寒冷时期，由于对该领域失去了兴趣，几乎没有关于神经网络的研究论文发表。原因很简单:没有针对传统算法的有效算法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/c4c7f06b2ce3ba4cb932d761c520abdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*WJaf5nv_7vdtiK3RT72aEA.jpeg"/></div></figure><p id="e21c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">当 Geoffrey Hinton，你可以称他为深度学习的教父，和他的团队在 2012 年一个名为 ImageNet 的著名物体识别比赛中提交他们的研究时，AI 的悲惨遭遇发生了巨大的变化。<em class="lw">“ImageNet 2012 事件绝对是引发当今人工智能大爆炸的原因”</em>clari fai 首席执行官马修·泽勒(Matthew Schmidt)说道。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/0dfca89b102f594547ca6cf43193d827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UbuCCvLbt-Zdf6yvzPgY_w.jpeg"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Picture Source: ImageNet</figcaption></figure><p id="346e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在 Geoffrey 的工作之前，算法下的物体识别研究的准确性非常差，分类错误率超过 25%，而人眼表现的错误率大约为 5%。但是，随着 Geoffrey 的团队完美地实现了神经网络，他们成功地将错误率大幅降低了 40%以上！！按照他的步骤，深度学习已经被如此强烈地使用，以至于仅两年后算法产生的错误率比人眼低得多！！目前，在对象和语音识别领域，几乎所有的研究都是由深度学习的子领域 AI 进行的。</p><p id="b5cd" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我还应该提到，在从“人工智能冬天”时期进入辉煌的“伟大的人工智能觉醒”时期时，人工智能从 GPU(小心，而不是 CPU)计算的改进中获得了实质性的支持，这要感谢英伟达，以及海量数据集的可用性，这要感谢我们所有人每天创建超过 2.5 万亿(10 ⁸)字节的数据。</p><p id="ed5e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们把注意力转回深度学习的主干算法。众所周知，算法试图通过最小化误差函数(通常称为成本函数)来优化问题。高效的算法在纳秒内完成，而其他算法可能需要运行数小时。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi md"><img src="../Images/cd53df08ba94f94a135e0b1e66110446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*eqtOmW8OxOFcyYfWkHt8Dw.jpeg"/></div></figure><p id="198e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">机器学习算法需要有效的优化技术来解决凸问题或非凸问题。最小化误差在凸函数中表现得最好，因为无论你的起点在哪里，你总会找到全局极小点！这就是为什么我们喜欢 SVM(支持向量机)或其他线性函数，因为他们有凸损失函数，你保证找到全局最小值！！</p><p id="ddb1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是，在现实生活中，优化函数通常是丑陋的。他们生活在非凸空间中，非凸空间由许多局部最小值(“谷”)和最大值(“山”)组成。神经网络也不例外，甚至更糟！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi me"><img src="../Images/12b39afa1306ab03b6eb58fd99021650.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*1lyC_JXnn2eVMqJ94tRlTQ.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">“Visualizing the Loss Landscape of Neural Nets”, Hao Li et al., 2018</figcaption></figure><p id="5cae" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">神经网络有 1000 多个局部最小值，所以你肯定会陷入其中，永远找不到全局最小值！(告诉我你是否不同意左边的图表，它只显示了一个非凸函数的一小部分。)取决于您的“初始化”点在哪里，您将总是以不同的局部最小值结束(这个问题通常通过随机初始化来处理，或者可能添加一些高斯噪声)。没有人真正喜欢神经网络的原因是，在缺乏负担得起的算法的情况下，它的高度复杂性。</p><p id="2d1d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在寻求合理的局部最小值时，研究人员首先使用了<em class="lw">积极的</em>方法，如梯度下降或更快的<em class="lw">近似</em>牛顿或<em class="lw">近似</em>海森方法(随着数据大小和维度的增加，凸优化方法受到不利影响。例如，基于 Hessian 的方法可以最佳地处理凸优化问题，但不能扩大规模，因此需要近似法[Martens et al .，2012]。这些方法是如此完美的寻找最小值，以至于它们可以立即找到“最近的”洞。虽然这听起来很完美，但实际上并不是！-最近的洞不太可能有理想的最小值，您需要寻找更远的地方。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mf"><img src="../Images/69f6195a72b702da9a0a25f1a03e07c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fZjielcteQnBPHdkEhZFAw.jpeg"/></div></div></figure><p id="409b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">此外，即使你设法看得更远，你也需要寻找宽阔的山谷，而不是狭窄的深度。使用训练数据时发现的深孔在使用测试数据时可能没有那么深。然而，在训练和测试数据集下都可能存在宽谷。</p><p id="8eed" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">哪种算法能够避开最近的洞并在宽广的山谷空间中停留？这正是 SGD 的实际能力！(尽管人们花了很长时间才发现这一点)。谁会想到，曾经被贴上低效和冗余标签的算法，最终会成为几乎所有深度学习模型的关键优化工具？太棒了。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mg"><img src="../Images/becdceb0f153997b633cac42441ef673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ighvUAMrSjIS0mFq.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">picture source : <a class="ae mh" href="https://wikidocs.net/3413" rel="noopener ugc nofollow" target="_blank">https://wikidocs.net/3413</a></figcaption></figure><p id="fa58" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这是怎么发生的？“随机”梯度下降(SGD)方法是如何作为一种非常有效的技术来训练深度神经网络的，在数据集大小上具有线性计算复杂性，给定其非常嘈杂和随机的性质？</p><p id="c37f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">与梯度下降法(GD)相比，SGD 的噪声很大，因为 GD 使用整个数据集来计算其路径(梯度)，而 SGD 仅使用一个示例，即每次迭代随机选择的批量大小为 1 的示例；因此，它遵循一条随机的路径，但最终是朝着正确的方向。随机性意味着它不会收敛到最近的洞，而只会停留在大的山谷中！这是很多其他算法不具备的素质！</p><p id="13b4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">嗯，在我们的童话故事中，在车轮弯曲之前，和故事结束之前，我应该告诉你，我们的主要人物，SGD，由于学习率(步长)有两个主要的限制:如果你选择它太大，那么学习轨迹导致可怕的情况；如果选得太小，它的收敛需要很长时间。所以，明智的选择合适的步长。您可以使用 SGD 的几种变体之一，如 Momentum、Averaging、隐式更新(ISGD)、AdaGrad、Adam 等。或者听听我的建议，开始时迈大步，当它开始来回跳跃时，开始降低你的学习速度，即迈小步。就是这样！</p><p id="f4f3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在桥修好了，我的故事也结束了。</p></div></div>    
</body>
</html>