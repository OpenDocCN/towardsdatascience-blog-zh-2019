<html>
<head>
<title>Building a neural network in C#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 C#构建神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-neural-network-framework-in-c-16ef56ce1fef?source=collection_archive---------1-----------------------#2019-03-31">https://towardsdatascience.com/building-a-neural-network-framework-in-c-16ef56ce1fef?source=collection_archive---------1-----------------------#2019-03-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2be7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">创建具有反向传播能力和基于进化的训练的神经网络。</h2></div><h1 id="2dda" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="e52f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们将建立一个能够通过反向传播和进化进行学习的深度神经网络。代码将是可扩展的，以允许网络架构的改变，允许通过代码容易地修改网络执行的方式。</p><p id="2cc3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">该网络是一个最低限度的可行产品，但可以很容易地扩大。你可以在 GitHub 上找到所有可用的代码，这包括突变和反向传播变体。</p><p id="bc64" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我将解释我们将如何设置前馈功能，设置所有需要的数组，并允许突变驱动的学习。</p><p id="a3c0" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果你想了解神经网络的工作原理，你需要熟悉一些基本的编码。对于反向传播，你需要熟悉梯度下降和微积分。除非您只想将代码用于您的项目。</p><p id="34ff" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">概念时间！</strong></p><p id="b1c6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们的深度神经网络由一个输入层、任意数量的隐藏层和一个输出层组成，为了简单起见，我将只使用完全连接的层，但这些可以有许多不同的风格。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/7360d0560e1ee08379dbc8a4180c7f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7Iy0hK5AVS6yrxB36ZwIw.png"/></div></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">A simple neural network model</figcaption></figure><h1 id="b9e3" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">神经网络体系结构</h1><p id="ff3b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">上面的模型在输入层上有 5 个神经元，如由 5 个实心圆组成的第一列所示。第二层有 4 个隐藏的<strong class="lc iu"> </strong>神经元<strong class="lc iu"> </strong>，输出层有 3 个输出神经元。这些层的大小和隐藏神经元的数量是任意的。但对于这种视觉模型，我选择了较小的尺寸。</p><p id="56a6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">其思想是，来自每个输入节点的值通过每个树突<strong class="lc iu"> </strong>(模型上节点之间的连接线)并乘以相应树突持有的权重，然后传递并保存在下一层的神经元中，然后对每一层继续这一循环，直到获得输出；</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/308acdde52197fe286cbfd7a667deef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*YXGBMD48mM8HHeCuHe-M2w.png"/></div></figure><p id="666c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">每个节点还有一个<strong class="lc iu"> </strong> bias(用 b 表示)，这有助于网络更好地运行。σ符号是这些乘积之和通过的激活函数<strong class="lc iu"/>。其中 w =来自树突的权重，a =前一层中每个神经元的激活。这个过程在每个神经元上进行，直到到达输出层。</p><p id="b520" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">当我们到达编码部分时，我们将更详细地讨论每一步，但是如果你对它没有一个基本的了解，我建议你去 youtube 或 Wikipedia 尝试并找到更多。<a class="ae ms" href="https://www.youtube.com/watch?v=aircAruvnKk" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">这个视频可能会有帮助</strong> </a></p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/45fff888f7b5f352a79ea9530fcc4f4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/0*pJixm0ILwsqOlhQS"/></div></figure><h1 id="d94e" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">编码时间！</h1><p id="11f5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">实现这种前馈功能的先决条件是一种存储所有数据的方法。我们将使用一系列阵列来存储所有数据，并确保网络性能始终处于最佳状态。</p><p id="07f4" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">我们需要什么样的数据类型</strong></p><p id="f913" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">一个输入数组来声明网络的大小，这将被标记为层。例如{5，4，3}，这将创建一个类似于上图的网络模型。</p><p id="0fe2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们还需要一个神经元阵列，用于保存前馈算法产生的值。这将是一个二维交错数组的形式</p><p id="2e6d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们还需要另一个 2d 交错数组来存储每一层的偏差。</p><p id="a5a8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">最后，我们需要一个 3d 交错数组来存储与每个树突相关的权重。偏差和权重是可以学习的，所有这些数组都是浮点数。</p><p id="c655" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">初始化功能</strong></p><p id="9660" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">每当我们创建这个类时，我们需要确保网络的维度已经定义，所有适当的数组都已经初始化。</p><p id="c4e1" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">该函数采用网络维度的输入数组，并填充所有数组</p><pre class="mc md me mf gt mu mv mw mx aw my bi"><span id="4952" class="mz kj it mv b gy na nb l nc nd">private int[] layers;//layers    <br/>private float[][] neurons;//neurons    <br/>private float[][] biases;//biasses    <br/>private float[][][] weights;//weights    <br/>private int[] activations;//layers</span><span id="b1fb" class="mz kj it mv b gy ne nb l nc nd">public float fitness = 0;//fitness</span><span id="0745" class="mz kj it mv b gy ne nb l nc nd">public NeuralNetwork(int[] layers)<br/>{        <br/>  this.layers = new int[layers.Length];        <br/>  for (int i = 0; i &lt; layers.Length; i++)        <br/>  {            <br/>    this.layers[i] = layers[i];        <br/>  }        <br/>  InitNeurons();        <br/>  InitBiases();        <br/>  InitWeights();    <br/>}</span></pre><p id="4a69" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">首先，让我们来处理神经元，它们不需要分配任何值，我们只需要分配存储空间，因为数组的长度是静态的。我们使用列表作为创建数组的临时媒介。</p><pre class="mc md me mf gt mu mv mw mx aw my bi"><span id="a31d" class="mz kj it mv b gy na nb l nc nd">//create empty storage array for the neurons in the network.<br/>private void InitNeurons()<br/>{        <br/>  List&lt;float[]&gt; neuronsList = new List&lt;float[]&gt;();        <br/>  for (int i = 0; i &lt; layers.Length; i++)        <br/>  {            <br/>    neuronsList.Add(new float[layers[i]]);        <br/>  }        <br/>  neurons = neuronsList.ToArray();    <br/>}</span></pre><p id="042e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">然后，我们可以在那里初始化偏差，偏差的大小与神经元的大小相同，只是我们需要填充每个槽，我将生成平均值为 0、标准偏差为 0.5 的每个偏差。当进行反向传播(消失和爆炸)时，有更好的初始化权重和偏差的方法来避免梯度问题，但是这对于遗传实现是不需要的。</p><pre class="mc md me mf gt mu mv mw mx aw my bi"><span id="4da0" class="mz kj it mv b gy na nb l nc nd">//initializes and populates array for the biases being held within the network.<br/>private void InitBiases()    <br/>{        <br/>  List&lt;float[]&gt; biasList = new List&lt;float[]&gt;();        <br/>  for (int i = 0; i &lt; layers.Length; i++)        <br/>  {            <br/>    float[] bias = new float[layers[i]];            <br/>    for (int j = 0; j &lt; layers[i]; j++)            <br/>    {                <br/>      bias[j] = UnityEngine.Random.Range(-0.5f, 0.5f);            <br/>    }            <br/>    biasList.Add(bias);        <br/>  }        <br/>  biases = biasList.ToArray();    <br/>}</span></pre><p id="58f0" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">权重的生成类似于偏差，只是我们为前一层中的每个神经元的数组添加了另一个维度，如下所示:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/61f4559572015e62422f38f04e59709f.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*5FV0P4GtPozmc1YtSYE5dQ.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">the hierarchy for weight storage</figcaption></figure><pre class="mc md me mf gt mu mv mw mx aw my bi"><span id="348a" class="mz kj it mv b gy na nb l nc nd">//initializes random array for the weights being held in the network.<br/>private void InitWeights()   <br/>{        <br/>  List&lt;float[][]&gt; weightsList = new List&lt;float[][]&gt;();        <br/>  for (int i = 1; i &lt; layers.Length; i++)        <br/>  {            <br/>    List&lt;float[]&gt; layerWeightsList = new List&lt;float[]&gt;();   <br/>    int neuronsInPreviousLayer = layers[i - 1];            <br/>    for (int j = 0; j &lt; neurons[i].Length; j++)            <br/>    {                 <br/>      float[] neuronWeights = new float[neuronsInPreviousLayer];<br/>      for (int k = 0; k &lt; neuronsInPreviousLayer; k++)  <br/>      {                                      <br/>        neuronWeights[k] = UnityEngine.Random.Range(-0.5f, 0.5f); <br/>      }               <br/>      layerWeightsList.Add(neuronWeights);            <br/>    }            <br/>    weightsList.Add(layerWeightsList.ToArray());        <br/>  }        <br/>  weights = weightsList.ToArray();    <br/>}</span></pre><h1 id="d790" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">前馈算法</h1><p id="03d2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">前面所有的初始化函数都准备好了，是时候讨论实际的前馈算法和相关概念了。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/e96c58d2e80f1f8f97d2853fd4b691ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*avIvkXeknKXmMXMEx26JAQ.png"/></div></figure><p id="4deb" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如前所述，这是在网络的隐层和输出层中为每个神经元计算的内容。让我们解释一下每个术语。从<strong class="lc iu">激活函数</strong> σ开始，这背后的想法是，你传入<strong class="lc iu">加权和</strong>并返回非线性结果，这提高了性能并保持网络神经元在控制之下，在现实世界中，没有多少事情遵循线性，因此非线性可以帮助近似非线性现象，并允许反向传播。</p><p id="d914" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">加权和</strong>，这是所有输入激活函数的数据。这是前一层中的每个节点乘以树突中的权重，数据通过该权重被传送到当前神经元。</p><p id="5091" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">激活功能</strong>可由您选择，根据应用，不同的激活功能选择可能更有益。</p><p id="360a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">该函数的常见变体包括:</p><ol class=""><li id="d938" class="nh ni it lc b ld lw lg lx lj nj ln nk lr nl lv nm nn no np bi translated">身份</li><li id="d12b" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">二进制步骤</li><li id="2ab3" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">乙状结肠的</li><li id="ecc6" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">双曲正切</li><li id="d8c6" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">热卢</li><li id="5bad" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">泄漏 ReLU</li><li id="56cc" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">Softmax</li></ol><p id="5f63" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在，我将使用 Tanh 作为我选择的<strong class="lc iu">激活函数</strong>，因为它允许正值和负值。尽管其他的将适用于不同的应用。</p><p id="e284" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">Tanh 可以用以下格式表示</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/9c979ddc635309c0808c0698bbfe8a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*cazUGtK5gPxj6NecjrZcIQ.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Tanh function</figcaption></figure><p id="820d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这个非线性函数返回以下结果</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nw"><img src="../Images/e075491b5c00ba53ec39889a8b1a21e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y5yIL4TIq0J_zHcjUfTN3Q.png"/></div></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Tanh Graph</figcaption></figure><pre class="mc md me mf gt mu mv mw mx aw my bi"><span id="ea89" class="mz kj it mv b gy na nb l nc nd">public float activate(float value)    <br/>{        <br/>  return (float)Math.Tanh(value);    <br/>}<br/>//Luckily Unity provides a built in function for Tanh</span></pre><p id="2a1e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在我们来看看前馈函数如何迭代神经元以产生输出。</p><p id="19d6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">该程序遍历每个神经元，获得前一层中每个神经元的值，并将该值乘以连接两者的权重，通过激活函数运行该值，然后将其值设置为激活值。这是节点 1 的一个示例</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e57cc9abe638292df486011364f4b0f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*Jng4YhgXLdk3MAsWLGMGSg.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Example neuron calculation</figcaption></figure><pre class="mc md me mf gt mu mv mw mx aw my bi"><span id="42a8" class="mz kj it mv b gy na nb l nc nd">//feed forward, inputs &gt;==&gt; outputs.<br/>public float[] FeedForward(float[] inputs)    <br/>{        <br/>  for (int i = 0; i &lt; inputs.Length; i++)        <br/>  {            <br/>    neurons[0][i] = inputs[i];        <br/>  }        <br/>  for (int i = 1; i &lt; layers.Length; i++)        <br/>  {            <br/>    int layer = i - 1;            <br/>    for (int j = 0; j &lt; neurons[i].Length; j++)            <br/>    {                <br/>      float value = 0f;               <br/>      for (int k = 0; k &lt; neurons[i - 1].Length; k++)  <br/>      {                    <br/>        value += weights[i - 1][j][k] * neurons[i - 1][k];      <br/>      }                <br/>    neurons[i][j] = activate(value + biases[i][j]);            <br/>    }        <br/>  }        <br/>  return neurons[neurons.Length - 1];    <br/>}</span></pre><p id="4f67" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">有了它，我们就有了一个工作的神经网络，但目前它还没什么用，除非我们有一种方法来训练它。接下来让我们开始工作吧。</p><h1 id="1dd2" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">培养</h1><p id="f25f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于这个网络的实现，我们将使用遗传算法。它们非常容易编码，并且很少涉及数学方面，但是，如果您对这个实现不感兴趣，我已经包含了一个反向传播代码示例，并且可能会使用强化学习方法。</p><p id="4da1" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">遗传算法是训练神经网络很好地执行给定任务的一种方式。它工作得很好，因为你可以给它一个非常简单的适应度函数，来决定网络运行得有多好。它的缺点是在处理大型网络时需要相对较长的训练时间，与反向传播相比可能相当慢，但如果您有大量的计算能力，它可以返回比反向传播更好的结果，因为它们几乎总是能够达到全局最小值。</p><p id="1e36" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">遗传算法背后的想法是基于达尔文主义的<strong class="lc iu">理论</strong>和生物进化是如何发生的，尽管我们将实现一个稍微简化的模型，但同样的总体概念也适用。一个物种的生物，他们有一个衡量他们“适合”的标准，从历史上看，这可能是他们的狩猎能力和繁殖能力，但有了我们的网络，这将是它能走多远，例如。然后我们整理种群，将它们分成两半，将上半部分克隆到下半部分，并对它们进行变异，这样网络的性能就趋向于全局最大值。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/6e50fdd11888fc0326b7b001f27f5ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*y-kpvL73DuxBr3EnRJo0UQ.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Evolution algorithm</figcaption></figure><h1 id="74c3" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">进化驱动的学习实现</strong></h1><p id="d81f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">网络的大部分管理将通过另一个脚本来完成。因此，目前我们需要添加到网络中的只是一种对网络进行分类的方法，一种将一个网络克隆到另一个网络上的方法，以及一种改变网络的方法。</p><p id="6916" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">对于排序，我们让类型 IComparable 的代码，这将允许我们在引用它时直接对它进行排序。它使用网络适应度作为网络排序的值</p><pre class="mc md me mf gt mu mv mw mx aw my bi"><span id="b006" class="mz kj it mv b gy na nb l nc nd">//Comparing For NeuralNetworks performance.<br/>public int CompareTo(NeuralNetwork other)    <br/>{        <br/>  if (other == null) <br/>    return 1;    <br/>  if (fitness &gt; other.fitness)            <br/>    return 1;        <br/>  else if (fitness &lt; other.fitness)            <br/>    return -1;        <br/>  else            <br/>    return 0;    <br/>}</span></pre><p id="5c9c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们还需要能够将可学习的值(权重和偏差)克隆到另一个神经网络上。</p><pre class="mc md me mf gt mu mv mw mx aw my bi"><span id="986d" class="mz kj it mv b gy na nb l nc nd">//this loads the biases and weights from within a file into the neural network.<br/>public void Load(string path)<br/>{        <br/>  TextReader tr = new StreamReader(path);        <br/>  int NumberOfLines = (int)new FileInfo(path).Length;        <br/>  string[] ListLines = new string[NumberOfLines];        <br/>  int index = 1;        <br/>  for (int i = 1; i &lt; NumberOfLines; i++)        <br/>  {            <br/>    ListLines[i] = tr.ReadLine();        <br/>  }        <br/>  tr.Close();        <br/>  if (new FileInfo(path).Length &gt; 0)        <br/>  {            <br/>    for (int i = 0; i &lt; biases.Length; i++)            <br/>    {               <br/>      for (int j = 0; j &lt; biases[i].Length; j++)                <br/>      {                    <br/>        biases[i][j] = float.Parse(ListLines[index]); <br/>        index++;                                   <br/>      }            <br/>    }             <br/>    for (int i = 0; i &lt; weights.Length; i++)            <br/>    {                <br/>      for (int j = 0; j &lt; weights[i].Length; j++)                <br/>      {                    <br/>        for (int k = 0; k &lt; weights[i][j].Length; k++)<br/>        {                        <br/>          weights[i][j][k] = float.Parse(ListLines[index]);    <br/>          index++;                                        <br/>        }                <br/>      }            <br/>    }        <br/>  }    <br/>}</span></pre><p id="53d6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">最后，我们将需要在网络中轻微推动每个可学习的值的能力，这是通过变异来完成的。</p><pre class="mc md me mf gt mu mv mw mx aw my bi"><span id="8aaf" class="mz kj it mv b gy na nb l nc nd">//used as a simple mutation function for any genetic implementations.<br/>public void Mutate(int chance, float val)    <br/>{        <br/>  for (int i = 0; i &lt; biases.Length; i++)        <br/>  {            <br/>    for (int j = 0; j &lt; biases[i].Length; j++)            <br/>    {                <br/>      biases[i][j] = (UnityEngine.Random.Range(0f, chance) &lt;= 5) ? biases[i][j] += UnityEngine.Random.Range(-val, val) : biases[i][j]; <br/>    }        <br/>  }  <br/>       <br/>  for (int i = 0; i &lt; weights.Length; i++)        <br/>  {            <br/>    for (int j = 0; j &lt; weights[i].Length; j++)            <br/>    {                <br/>      for (int k = 0; k &lt; weights[i][j].Length; k++)                <br/>      {                    <br/>        weights[i][j][k] = (UnityEngine.Random.Range(0f, chance) &lt;= 5) ?  weights[i][j][k] += UnityEngine.Random.Range(-val, val) : weights[i]  [j][k];<br/>                <br/>      }            <br/>    }        <br/>  }    <br/>}</span></pre><p id="3a9c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">所有这些代码实现后，我们应该有一个能够学习的工作网络。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/157417840f4eb1b490d3821e9f97850c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*TP_RXLBS-xH_j7_t"/></div></figure><h1 id="55c6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">在 Unity 中实现网络</strong></h1><p id="f5cb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">设置场景，这是我创建的一条赛道，带有检查点，我们的神经网络车必须通过这些检查点才能提高体能。在这种情况下，网络的目标是在轨道上导航，并在不与墙壁碰撞的情况下尽可能走远。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi oa"><img src="../Images/73320b782455f56fcaa885b21d79af3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1lYRWsIZH-BvcFje6DOGhg.png"/></div></div></figure><p id="99fb" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在场景中，我们有一个管理器，它产生、变异克隆体并破坏网络。网络的意义在于提高网络的性能。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/1071e23155f766557eda4c2744408792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*0iwwIr2BPSKvIiRdQuWUbw.png"/></div></figure><p id="ebaa" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">时间框架是每一代机器人训练的时间，群体大小是一次训练多少个机器人，突变机会是每个权重或偏差发生突变的变化，突变强度是微移的标准偏差，游戏速度是游戏内的滴答率，如果你有适当的硬件，增加网络的训练时间。</p><p id="bff6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">管理者创建一群使用神经网络的预设，然后在每个预设中部署一个神经网络。一段时间后，测试将完成，网络将被分类，以便只有表现最好的被保留，没有表现最好的将被最好的网络复制并变异。然后再次部署，训练循环继续进行。</p><p id="b053" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">最后，我们需要制作学习者预置，它的功能有点类似于汽车，可以前后移动，也可以转弯；汽车的这两个输入将是网络的输出。网络的输入将是 5 个距离传感器。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi oc"><img src="../Images/87c6ba32349a5de5415ca4a711d2ca07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9vJZYm81fQd4kisRyk0MkA.png"/></div></div></figure><p id="be79" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">经过几分钟的训练，我们得到了这个结果，这意味着所有的代码都工作了！</p><p id="a752" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">所有代码都可以在 GitHub 上找到，我应该注意到 GitHub 上的代码也有保存和加载功能，这些功能保存和加载网络的权重，以允许网络从它停止的地方恢复。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi od"><img src="../Images/00480f48d6e907e114c9a25607bca348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*OtG23-aCVfsth_Nabzyxhw.gif"/></div></div></figure><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/74207554e9a095a68108da0b38c72b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*sJJKATyipjzWRHEL"/></div></figure><p id="296a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">Google Drive 上的<a class="ae ms" href="https://drive.google.com/open?id=1ZMA-1FWaqmZ1bnv8oX6QnhfwWo68Uefj" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">项目</strong> </a> <strong class="lc iu">。</strong></p><p id="8e44" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><a class="ae ms" href="https://github.com/kipsterbro/MutationNetwork" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu">Github 上的突变项目。</strong>T11】</a></p><p id="56b0" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><a class="ae ms" href="https://github.com/kipsterbro/BackPropNetwork" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu">Github 上的反向传播项目。</strong>T15】</a></p><p id="21ec" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">非常感谢您的阅读，我希望它能有所帮助。我欢迎任何问题或反馈。敬请期待！</p></div></div>    
</body>
</html>