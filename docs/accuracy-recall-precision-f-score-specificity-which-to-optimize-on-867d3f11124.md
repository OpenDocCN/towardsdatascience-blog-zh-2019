# 准确度、召回率、精确度、F 值和特异性，哪一个需要优化？

> 原文：<https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124?source=collection_archive---------0----------------------->

## 根据您的项目，哪个绩效指标需要改进？

![](img/91ea4f8577ac93e680218a5ff735e725.png)

我将使用一个基本示例来解释上的每个性能指标，以便让您真正理解它们之间的差异。以便在您下一个 ML 项目中，您可以选择最适合您的项目的性能度量标准。

## 我们开始吧

一所学校正在对所有学生进行机器学习初级糖尿病扫描。
输出要么是**糖尿病患者** (+ve)，要么是**健康者** (-ve)。

任何学生 ***X*** 最终可能只有 4 种情况。我们稍后将使用以下内容作为参考，所以如果你感到困惑，请不要犹豫重读一遍。

*   真阳性( *TP* ):预测为+ve，X 为糖尿病，*我们想要那个*
*   真阴性( *TN* ):预测是-ve，X 是健康的，*我们也希望如此*
*   假阳性( *FP* ):预测为+ve，X 健康，*虚警，不良*
*   假阴性( *FN* ):预测为-ve，X 为糖尿病，*最差*

## 要记住这一点，有两个窍门

-如果**以** **真开始，那么无论是否患有糖尿病，预测都是正确的**，所以真阳性是正确预测的糖尿病人&真阴性是正确预测的健康人。
相反，如果**以假开始，则预测是不正确的**，因此假阳性是健康人被错误地预测为糖尿病(+) &假阴性是糖尿病人被错误地预测为健康(-)。
- **正或负表示我们程序**的输出。而**真或假则判断该输出**是否正确。

在我继续之前，真正的积极&真正的消极总是好的。我们喜欢真实这个词带来的新闻。留下了假阳性和假阴性。在我们的例子中，误报只是一个错误的警报。在第二次更详细的扫描中，它将被纠正。但是一个错误的负面标签，这意味着他们认为他们是健康的，而实际上他们并不健康，在我们的问题中，这是 4 种情况中最糟糕的一种。
是 *FP* & *FN* 一样差还是其中一个比另一个更差取决于你的问题。这条信息对您选择性能指标有很大的影响，所以在您继续之前请考虑一下。

# 选择哪种性能指标？

## 准确(性)

它是正确标记的主题与整个主题库的比率。
准确是最直观的一个。
**准确性回答以下问题:我们在所有学生中正确标记了多少学生？**
***准确度=(TP+TN)/(TP+FP+FN+TN)*** 分子:所有正确标注的被试(全部为真)
分母:所有被试

## 精确

精度是我们的程序正确标记的**+ve 与所有标记的+ve 之比。Precision 回答了以下问题:那些被我们贴上糖尿病标签的人中有多少人实际上是糖尿病患者？
*精度= TP/(TP+FP)* 分子:+ve 标注糖尿病人。
分母:所有被我们的计划标记为+ve 的人(无论他们实际上是否患有糖尿病)。**

## **回忆(又名敏感度)**

**召回率是我们的程序正确标记的+ve 与现实中所有糖尿病患者的比率。回忆回答了以下问题:在所有的糖尿病患者中，我们能正确预测的有多少？
*回忆= TP/(TP+FN)* 分子:+ve 标注糖尿病人。
分母:所有糖尿病患者(无论是否被我们的计划检测到)**

## **F1 分数(又名 F 分数/ F 度量)**

**F1 评分同时考虑了准确率和召回率。
**它是精度和召回率的调和平均值。** 如果在系统中的精度(p) &召回(r)之间存在某种平衡，F1 得分最好。相反，如果一个指标的提高是以牺牲另一个为代价的，那么 F1 值就不会那么高。
比如*如果 P 为 1 & R 为 0，F1 得分为 0。*
***【F1 得分= 2*(回忆*精度)/(回忆+精度)*****

## **特征**

**特异性是程序给现实中所有健康的人贴上的正确标签。特异性回答了以下问题:在所有健康的人中，我们正确预测了多少？
*特异性= TN/(TN+FP)* 分子:-ve 标记健康人。
分母:现实中所有健康的人(无论是+ve 还是-ve 标注的)**

# **一般注意事项**

**是的，**准确性是一个很好的衡量标准**，但只有当您拥有对称数据集时**(假阴性&假阳性计数接近)，而且，**假阴性&假阳性的成本也差不多。如果误报和漏报的代价不同，那么 F1 就是你的救星。如果你的职业分布不均匀，F1 是最好的选择。******

**精确是你有多确定你真正的阳性，而回忆是你有多确定你没有遗漏任何阳性。**

****如果假阳性的想法远远好于假阴性，选择召回**，换句话说，**如果假阴性的出现是不可接受的/不可容忍的**，那你宁愿得到一些额外的假阳性(假警报)也不愿保存一些假阴性，就像我们的糖尿病例子一样。
你宁愿让一些健康的人被贴上糖尿病的标签，也不愿让一个糖尿病人被贴上健康的标签。**

****如果你想对自己的真实想法更有信心，请选择 precision**。比如垃圾邮件。你宁愿在你的收件箱里有一些垃圾邮件，而不是一些普通的邮件。因此，电子邮件公司想要额外确定电子邮件 *Y* 是垃圾邮件，然后才把它放入垃圾邮件箱，这样你就再也看不到它了。**

****如果您想要覆盖所有真阴性**，选择特异性，这意味着您不想要任何假警报，您不想要任何假阳性。例如，你正在进行一项毒品测试，所有测试呈阳性的人将立即入狱，你不希望任何没有吸毒的人入狱。这里的假阳性是不能容忍的。**

## **底线是**

**—准确度值为 90%意味着每 10 个标签中有 1 个不正确，9 个正确。
—精度值为 80%意味着平均而言，我们计划标记的每 10 名糖尿病学生中有 2 名是健康的，8 名是糖尿病患者。
—召回值为 70%意味着现实中每 10 名糖尿病患者中有 3 人被我们的计划遗漏，7 人被标记为糖尿病患者。
—特异性值为 60%意味着现实中每 10 个健康人中有 4 个被误标记为糖尿病，6 个被正确标记为健康。**

# **混淆矩阵**

**[维基百科](https://en.wikipedia.org/wiki/Confusion_matrix)会比我解释的更好**

> **在机器学习领域，特别是统计分类问题中，混淆矩阵，也称为误差矩阵，是一种特定的表格布局，它允许算法性能的可视化，通常是监督学习算法(在非监督学习中，它通常被称为匹配矩阵)。矩阵的每一行代表预测类中的实例，而每一列代表实际类中的实例(反之亦然)。该名称源于这样一个事实，即它可以很容易地看出系统是否混淆了两个类(即通常将一个类误标为另一个类)。**

**这里的[是一个很好很简单的计算混淆矩阵的方法。](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?fbclid=IwAR124uVUTy-aTa83mO0tJbIFPzduHSbPg3tzv_4R5_WBRzzhLpk1HjGUpdc)**

```
from **sklearn.metrics** import confusion_matrix>>>tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], 
[1, 1, 1, 0]).ravel()
# true negatives, false positives, false negatives, true positives
>>>(tn, fp, fn, tp)
(0, 2, 1, 1)
```