<html>
<head>
<title>Killer Robots in the US Military: Ethics as an Afterthought</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">美国军队中的黑仔机器人:事后的伦理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/killer-robots-in-the-us-military-ethics-as-an-afterthought-baf07e500d9?source=collection_archive---------24-----------------------#2019-10-25">https://towardsdatascience.com/killer-robots-in-the-us-military-ethics-as-an-afterthought-baf07e500d9?source=collection_archive---------24-----------------------#2019-10-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1a76" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">国防部在伟大的人工智能竞赛中争夺第一名的时候，把伦理道德抛在了脑后。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/25716a2f4db40c4e00d6726dfa60b27d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jVnqkmLgnIbuVlFYl5-T_Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by bamenny from Pixabay.</figcaption></figure><p id="b035" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">作为美国战争机器中的代理人，美国军方并没有低估杀手机器人或致命自主武器系统(LAWS)的未来发展。人工智能(AI)自最初由艾伦·图灵(Alan Turing)等先驱创立以来，以及他对能够像人类一样学习思考和行为的机器的思考，已经显示出了很大的前景。机器学习及其子集深度学习激发了机器有朝一日能够发展甚至取代人类认知的希望。这是一项潜在的技术，国防部不能也不会忽视。虽然国防部已经制定了第 3000.09 号指令(T0)，为开发自主武器系统(AWS)和致命的对应法律制定了一个实用的框架，但他们制定一个全面的伦理框架，充分回应国家和国际关注，目前只是事后的想法。但是，当走向机器人可能夺走人类生命的未来时，伦理难道不应该是这项技术的核心吗？</p><h2 id="bca8" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">人工智能军备竞赛</h2><p id="60cf" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">根据美国和国际上的倡导者，AWS 和法律都有可能在战场上拯救生命，<a class="ae lr" href="https://www.armyupress.army.mil/Journals/Military-Review/English-Edition-Archives/May-June-2017/Pros-and-Cons-of-Autonomous-Weapons-Systems/" rel="noopener ugc nofollow" target="_blank">。更少的地面部队意味着更少的军事死亡，更精确和高效的任务意味着更少的平民伤亡。虽然接受战争的不可避免性可能是对人类原始本能的压抑提醒，但世界各地的战争机器运转良好，资金充足，并与时俱进。中国、俄罗斯、以色列、澳大利亚、英国和欧盟都在开发 AWS，随着技术的进步，这些 AWS 有可能成为杀手机器人。美国无意落在后面。事实上，在 2020 年的预算中，国防部希望在无人驾驶和人工智能系统上花费</a><a class="ae lr" href="https://www.defense.gov/Newsroom/Releases/Release/Article/1782623/dod-releases-fiscal-year-2020-budget-proposal/#targetText=On%20March%2011%2C%202019%2C%20President,Department%20of%20Defense%20(DoD)." rel="noopener ugc nofollow" target="_blank">46 亿美元</a>。</p><p id="6704" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">美国已经有了完善的政府机构，如国防高级研究计划局(DARPA)，该机构资助机器人自主和人工智能系统的研究。DARPA 在过去的六十年里一直在运作，现在它正通过它的 AI Next 活动展望未来。DARPA 在考虑国家安全的情况下处理高风险、高回报和长期的项目。DARPA 的 AI Next 项目之一是自主团队的上下文推理(<a class="ae lr" href="https://www.fbo.gov/index?s=opportunity&amp;mode=form&amp;id=03751726307862cbc982ac059c03dc93&amp;tab=core&amp;_cview=0" rel="noopener ugc nofollow" target="_blank"> CREATE </a>)。CREATE 的目标是在较小的自主代理团队中建立决策能力，使他们不依赖于集中控制。想想成群的无人机，它们可以在本地协同工作，并立即做出决定，而不是听从命令。</p><p id="7725" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然 DARPA 致力于一些长期项目，但联合人工智能中心(JAIC)于 2018 年成立，以跟上和部署技术在私营部门和学术界的开发和使用速度。JAIC 监督国防部各部门对人工智能的采用，并正在发展学术界和私营部门的合作伙伴关系，以获得快速发展和可部署的人工智能技术。但是国防部感受到的不仅仅是国家压力。国际人工智能竞赛的想法已经引起了一些关注，甚至是令人恐惧的<a class="ae lr" href="https://www.armscontrol.org/act/2019-03/news/ai-arms-race-gains-speed" rel="noopener ugc nofollow" target="_blank">法律军备竞赛</a>的想法。这种速度和紧迫性是最令人担忧的。没有哪个州想输掉人工智能竞赛。随着俄罗斯总统弗拉基米尔·普京称<a class="ae lr" href="https://www.theverge.com/2017/9/4/16251226/russia-ai-putin-rule-the-world" rel="noopener ugc nofollow" target="_blank">“谁成为这个(人工智能)领域的领导者，谁将成为世界的统治者”</a>，忧心忡忡的国家正在加快步伐。美国很有可能赢得研发和部署自主武器的竞赛，但代价是什么？如果没有一个经过深思熟虑的道德框架的约束，美国可能会以一种比 1945 年广岛和长崎原子弹袭击更令人羞愧的方式开发和部署自主武器。</p><h2 id="215f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">道德问题</h2><p id="3b52" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">像人权观察(HRW)和哈佛法学院国际人权诊所(IHRC)这样的组织正在利用像<a class="ae lr" href="https://www.stopkillerrobots.org/" rel="noopener ugc nofollow" target="_blank">阻止黑仔机器人</a>的运动这样的目标来减缓人工智能军备竞赛，围绕这项技术的伦理展开讨论。当涉及到发动战争时，人们开始质疑超然的道德标准。由于风险更小，自主系统会让开战的决定更容易吗？给机器杀人的权力和意志意味着什么？出了问题谁来负责？有一种风险是，为了让我们在肮脏的战争事务中保持清白，人类军官监管致命机器的经过消毒的作战室将看不到流血。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d4cd52c8dd9be3a22f1f3741851cc710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ke39oflbwk78fznZBP8wGQ.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Traditional horrors of war. Image by Jonny Lindner from Pixabay.</figcaption></figure><p id="1757" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在国际舞台上，“有意义的人类控制”一词被作为法律伦理发展的一个关键组成部分来讨论。然而，对于“有意义的人类控制”在现实中是什么样子，人们还没有达成共识。在 3000.09 指令中，国防部讨论了人类判断的适当水平，而不是控制。国防部不排除完全自主的武器系统能够在没有人类直接参与的情况下选择目标和部署致命武器的可能性。然而，半自动武器系统将(并且确实)需要人类授权才能使用致命武力。</p><p id="331a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在联合国内部,《特定常规武器公约》( CCW)主持了关于法律未来的国际讨论，一些国家和组织呼吁全面禁止。但是世界上的大国(<a class="ae lr" href="https://www.sipri.org/media/press-release/2018/global-military-spending-remains-high-17-trillion" rel="noopener ugc nofollow" target="_blank">和军事支出大国</a>)中国和美国都不希望限制这项技术的潜力。然而，美国公众并没有完全同意。在美国，2019 年 T4 益普索民意调查中，52%的受访者或多或少反对或强烈反对使用法律，而只有 24%的人支持使用法律。如果美国继续发展完全自主的武器，法律伦理将是获得公众支持的关键。民意调查收集了 26 个国家的意见，发现 66%反对使用法律的人认为这种武器越过了道德底线。人们只是对机器决定夺走一个人的生命感到不舒服。</p><h2 id="70d4" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">伦理是事后的想法</h2><p id="9964" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">那么，国防部在法律的道德发展和使用方面的立场是什么？国防部确实承认道德在法律方面的重要性，因为他们最初在宣布需要自己的伦理学家之前从技术行业寻求指导。然而，国防部已经致力于开发人工智能技术一段时间了，像机器人 Shakey 这样的项目可以追溯到 1966 年。最近，2019 年 2 月，国防部公布了其<a class="ae lr" href="https://media.defense.gov/2019/Feb/12/2002088963/-1/-1/1/SUMMARY-OF-DOD-AI-STRATEGY.PDF" rel="noopener ugc nofollow" target="_blank"> 2018 人工智能战略</a>。然而，直到 2019 年 9 月，国防部才透露了聘请伦理学家的计划，这表明制定一个强大的道德框架并不是一个紧迫的优先事项。随着人工智能竞赛的升温，中国的目标是到 2030 年成为人工智能世界的领导者，谁有时间讨论伦理？</p><p id="7483" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">JAIC 主任空军中将杰克·沙纳汉指出，中国与学术界和工业界的紧密联系使他们在人工智能竞赛中占据优势。为了不被超越，国防部计划加强与一系列开发具有双重用途能力的人工智能技术的行为者的联系。根据 2018 年人工智能战略，国防部打算与学术界和工业界合作，为人道主义援助提供人工智能知识和专业知识。这种人道主义的面孔可能使他们成为更有吸引力的合作伙伴，因为一些科技行业的人发现与军队合作在道德上令人反感。<a class="ae lr" href="https://www.bbc.co.uk/news/business-44341490" rel="noopener ugc nofollow" target="_blank">谷歌拒绝继续执行五角大楼的项目 Maven 合同</a>，此前其员工抗议以致命意图开发技术。</p><p id="9250" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与担心落后于中国相比，道德根本不是国防部的首要任务。然而，可能并不是麻木不仁让伦理成为一个落后的优先事项。目前，人工智能技术只达到了让机器执行一些基本任务的水平与人类相同，或者略高。<a class="ae lr" href="https://www.theguardian.com/global/2015/may/13/baidu-minwa-supercomputer-better-than-humans-recognising-images" rel="noopener ugc nofollow" target="_blank">图像识别</a>和<a class="ae lr" href="https://venturebeat.com/2019/05/20/googles-lung-cancer-detection-ai-outperforms-6-human-radiologists/" rel="noopener ugc nofollow" target="_blank">癌症检测</a>只是两个例子。我们还需要几十年才能开发出一种技术，让机器能够完成人类能够完成的一系列复杂认知任务，更不用说将这种技术与能够像人类一样灵活移动的机器人结合起来了。随着人工智能的大肆宣传，人们很容易认为杀手机器人就在眼前，而事实上，我们可能需要几十年才能开发出这种复杂的技术。国防部可能认为他们有足够的时间来严肃对待法律伦理。与此同时，DARPA 和 JAIC 正在大力推进 AWS 的研发。在幕后的某个地方，国防部正在采取一种悠闲的方式来确保伦理学家和形成透明的道德准则，这将使他们在向人工智能终点线的比赛中承担责任(如果有一个的话)。</p><p id="9172" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我们在 bias 中看到的，人工智能系统学到的<a class="ae lr" href="https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/" rel="noopener ugc nofollow" target="_blank">变成了嵌入和复制</a>。如果法律的发展在每个方面都以伦理为核心，而不仅仅是事后的想法，那么我们创造出比我们更好的机器的机会就会增加。在一台机器内完全复制人类的意识可能是不可能的，在这种情况下，反对杀手机器人的论点可能会变得更强。归根结底(如果技术允许的话)，将生死决定权交给一台比我们更有道德的机器可能会带来安慰，而不是引发恐惧。如果国防部想带领美国进入人工智能军备竞赛，它必须从一个强大的道德立场出发，跟上伟大的人工智能竞赛带来的快速技术进步。</p></div></div>    
</body>
</html>