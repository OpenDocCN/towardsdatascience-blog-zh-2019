<html>
<head>
<title>Deep Latent Variable Models: Unravel Hidden Structures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深层潜在变量模型:揭示隐藏结构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-latent-variable-models-unravel-hidden-structures-a5df0fd32ae2?source=collection_archive---------7-----------------------#2019-07-31">https://towardsdatascience.com/deep-latent-variable-models-unravel-hidden-structures-a5df0fd32ae2?source=collection_archive---------7-----------------------#2019-07-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/21262979813a1c8b9ba0174bbb26c46c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SJHg-vyB7w9vrlDmyIEQag.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">source: Unsplash</figcaption></figure><div class=""/><p id="803b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">理解真实世界数据的底层结构是机器学习中最引人注目的任务之一。但是随着深度生成模型的出现，研究者和实践者有了一个强有力的方法来解开它。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="12e9" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">真实世界的数据通常是复杂和高维的。传统的数据分析方法在大多数情况下是无效的，并且只能模拟非常简单的数据分布。现在，我们可以使用机器学习模型来直接学习我们数据的结构。机器学习中最常见的方法是<em class="ld"> </em> <a class="ae ll" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank"> <em class="ld">监督学习</em> </a>，其中我们要求模型学习从输入到输出变量的映射，例如图像 x 到标签 y。然而，带标签的数据是昂贵的，并且容易被人类注释者产生错误或偏见。并且监督模型仅能够根据训练数据的质量来概括其映射函数。为了测试其推广性，使用了来自相同分布的验证集，该验证集将具有相同的误差。使用这种模型，可以执行分类或回归任务，但我们无法了解数据的实际基本组织。</p><p id="e421" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">机器学习社区开始关注<a class="ae ll" href="https://en.wikipedia.org/wiki/Unsupervised_learning" rel="noopener ugc nofollow" target="_blank"> <em class="ld">无监督学习</em> </a>模型的开发。通过结合概率建模和深度学习，最近取得了一些进展。我们称这类模型为<a class="ae ll" href="https://openai.com/blog/generative-models/" rel="noopener ugc nofollow" target="_blank"> <em class="ld">生成型模型</em> </a> <em class="ld">。根据这句名言，</em></p><blockquote class="lm ln lo"><p id="8da9" class="kf kg ld kh b ki kj kk kl km kn ko kp lp kr ks kt lq kv kw kx lr kz la lb lc im bi translated">"我不能创造的东西，我不明白。"—理查德·费曼</p></blockquote><p id="ed3c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">生成模型应该能够发现底层结构，例如数据的有趣模式、聚类、统计相关性和因果结构，并生成类似的数据。</p><p id="c71f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">目前，该领域中的一个著名模型是<em class="ld">生成式广告网络</em>(GANs)【1】，例如，它能够从学习到的数据分布中生成<a class="ae ll" href="https://medium.com/syncedreview/gan-2-0-nvidias-hyperrealistic-face-generator-e3439d33ebaf" rel="noopener">人脸的真实图像</a>。这一类的另一个模型被称为<em class="ld">变分自动编码器</em>(VAE)【2】，它也用于复杂高维分布的无监督学习，将是本文的重点。甘的训练仍然是实验性的，因为他们的训练过程缓慢且不稳定。但是他们也在进步，请看这篇关于 Wasserstein GANs 的博客文章。</p><p id="d23a" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">一般来说，非监督学习比监督学习困难得多，因为这些模型不是预测给定输入的标签或值，而是学习数据分布本身的隐藏结构。本文将介绍我们如何实现这一点的概念，重点是静态数据，如没有序列性质的图像。学习顺序数据的底层结构是一个更困难的问题，我打算将来写一篇关于这个问题的后续文章。</p><p id="e852" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在第一部分中，我们将定义潜在变量模型，在第二部分中，我们将了解如何使用深度神经网络来学习它们的参数。我试图尽可能保持一切直观，但一些概率论和深度学习的先验知识肯定是有帮助的。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="2c0e" class="ls lt ji bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">潜在变量模型</h1><p id="b269" class="pw-post-body-paragraph kf kg ji kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">机器学习中的一个中心问题就是学习一个复杂的概率分布<em class="ld">p(</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>，只需要从这个分布中抽取有限的一组高维数据点<strong class="kh jj"> <em class="ld"> x </em> </strong>。例如，为了学习猫的图像的概率分布，我们需要定义一个分布，该分布可以模拟形成每个图像的所有像素之间的复杂相关性。直接模拟这种分布是一项具有挑战性的任务，甚至在有限的时间内是不可行的。</p><p id="621f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">与其直接建模<em class="ld">p(</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>，我们可以引入一个不可观测的潜在变量<strong class="kh jj"> <em class="ld"> z </em> </strong>并为数据定义一个条件分布<em class="ld">p(</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">|</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld"/>，这就叫做<em class="ld">可能性</em>用概率术语<strong class="kh jj"> <em class="ld"> z </em> </strong>可以解释为连续的<em class="ld">随机变量</em>。对于猫图像的例子，<strong class="kh jj"> <em class="ld"> z </em> </strong>可以包含猫的类型、颜色或形状的隐藏表示。</p><p id="c1d1" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">有了<strong class="kh jj"> <em class="ld"> z </em> </strong>，我们可以进一步引入潜在变量的先验分布<em class="ld">p(</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">)</em>来计算观察变量和潜在变量的联合分布:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a3ca31d7ddedff74de4e3e4f8174bdf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*6xQ2KiYCn47ZN4jSWGxF4Q.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Joint distribution over observed and latent variables; Equation (1)</figcaption></figure><p id="8745" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这种联合分布让我们可以用更易处理的方式来表达复杂的分布<em class="ld">p(</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>。其组成部分，<em class="ld">p(</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">|</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">)</em>和<em class="ld">p(</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">)</em>的定义通常要简单得多，例如通过使用中的分布</p><p id="d6f2" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了获得数据分布<em class="ld">p(</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>我们需要对潜在变量进行边缘化</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/905592664cc00c7820cc1af2e976224e.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*259iLRk1uXhQ5XGokYSZ7g.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Marginalized data distribution p(<strong class="bd nb">x</strong>); Equation (2)</figcaption></figure><p id="225a" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">此外，利用贝叶斯定理我们可以计算后验分布<em class="ld">p(</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">|</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>为</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/46a500d7444ca4c3fba327b8ba1d3dcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*6AY2oRk-RpiAttLlUpZyGw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Posterior distribution p(<strong class="bd nb">z</strong>|<strong class="bd nb">x</strong>); Equation (3)</figcaption></figure><p id="e00b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">后验分布允许我们推断给定观测值的潜在变量。注意，对于我们处理的大多数数据，方程(2)中的积分没有解析解，我们必须应用某种方法来推断方程(3)中的后验概率，这将在下面解释。</p><p id="6c7b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为什么我们要做这个练习并引入一个潜在变量？优势在于，具有潜在变量的模型可以表达数据被创建的生成过程(至少这是我们的希望)。这被称为生成模型。一般来说，这意味着如果我们想要生成一个新的数据点，我们首先需要获得一个样本<strong class="kh jj"><em class="ld">【z】</em></strong>~<em class="ld">p(</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">)</em>，然后使用它从条件分布<em class="ld"/><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">|</em><strong class="kh jj"><em class="ld">中采样一个新的观察值<strong class="kh jj"><em class="ld"/></strong>在这样做的同时，我们还可以评估该模型是否为数据分布<em class="ld">p(</em><strong class="kh jj"><em class="ld">)x</em></strong><em class="ld">)</em>提供了良好的近似。</em></strong></p><p id="2643" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">根据定义，包含潜在变量的数学模型是潜在变量模型。这些潜在变量的维数比观察到的输入向量低得多。这产生了数据的压缩表示。您可以将潜在变量视为一个瓶颈，生成数据所需的所有信息都必须通过这个瓶颈。从<a class="ae ll" href="https://deepai.org/machine-learning-glossary-and-terms/manifold-hypothesis" rel="noopener ugc nofollow" target="_blank"> <em class="ld">流形假设</em> </a>中我们知道，高维数据(例如真实世界的数据)位于嵌入在这个高维空间中的低维流形上。这证明了低维潜在空间的合理性。</p><h2 id="567a" class="nd lt ji bd lu ne nf dn ly ng nh dp mc kq ni nj mg ku nk nl mk ky nm nn mo no bi translated">后验推断</h2><p id="c30c" class="pw-post-body-paragraph kf kg ji kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">后验分布<em class="ld">p(</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">|</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>，是概率推理中的一个关键组成部分，在观察到一个新的数据点后，更新我们对潜在变量的信念。然而，真实世界数据的后验常常是难以处理的，因为它们对于出现在方程(3)分母中的方程(2)中的积分没有解析解。有两种方法可以近似这种分布。一种是叫做<a class="ae ll" rel="noopener" target="_blank" href="/markov-chain-monte-carlo-291d8a5975ae"> <em class="ld">马尔可夫链蒙特卡罗</em>方法</a>的抽样技术。然而，这些方法在计算上是昂贵的，并且不能很好地扩展到大型数据集。第二种方法是确定性近似技术。在这些技术中，VAE 使用的是所谓的<a class="ae ll" href="https://arxiv.org/abs/1601.00670" rel="noopener ugc nofollow" target="_blank"><em class="ld"/></a><em class="ld"/>(VI)【4】。注意，这种方法的缺点是，即使在无限的计算时间内，它们也不能产生精确的结果。</p><p id="ffc9" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">VI 的总体思路是从一个易处理的分布族(例如多元高斯)中取一个近似值<em class="ld">q(</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">)</em>，然后使这个近似值尽可能接近真实的后验<em class="ld">p(</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">|</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>。这通常通过最小化两个分布之间的<em class="ld"> Kullback-Leibler </em> (KL)散度来实现，定义为</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e20528b81643c8adc92f700810da2770.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*QY0s9ZchmhPuJwaedDWYeg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Kullback-Leibler Divergence; Equation (4)</figcaption></figure><p id="0713" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这减少了对优化问题的推断[5]。越相似的<em class="ld">q(</em><strong class="kh jj"><em class="ld"/></strong><em class="ld">)</em><em class="ld">p(</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">|</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>KL 的散度越小。请注意，这个量不是数学意义上的距离，因为如果我们交换分布，它就不是对称的。此外，在我们的例子中，交换分布意味着我们需要获取关于<em class="ld">p(</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">|</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>的期望，这被认为是难以处理的。</p><p id="7f57" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，方程(4)在对数内的分子中仍然有难以处理的后验概率。使用(3)，我们可以将(4)改写为:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/a8a46e0106bffe2f8324071c92fbfbff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*n4cKkychkjwcDoLfWM-S8A.png"/></div></figure><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4102d23517e5c6be4ee2cdedfb7f188b.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*U_u7XMHmVchYdNBPHYvWrg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Evidence Lower Bound (ELBO) F(<strong class="bd nb">q</strong>); Equation (5)</figcaption></figure><p id="1585" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">边际可能性<em class="ld">log p(</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>可以从期望中取出，因为它不依赖于<strong class="kh jj"> <em class="ld"> z </em> </strong>。量<em class="ld">F(</em><strong class="kh jj"><em class="ld">q</em></strong><em class="ld">)</em>就是所谓的<em class="ld">证据下界</em> (ELBO)。KL 总是≥ 0，因此它表示对<em class="ld">证据日志 p(</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>的下限。ELBO 越接近边际似然，变分近似就越接近真实的后验分布。因此，复杂的推理问题被简化为更简单的优化问题。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="d127" class="ls lt ji bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">变分自动编码器</h1><p id="36a0" class="pw-post-body-paragraph kf kg ji kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">我们还没有提到它，但是可能性和先验属于依赖于一些未知参数的分布族。为了使这一点更清楚，请看等式(1)的参数联合分布:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/cf3c6efdf7f6a960debce46268301ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*IiYGi8pk9dGz0c6githZcA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Parametric joint distribution; Equation (6)</figcaption></figure><p id="ede7" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="ld">θ</em>表示模型的未知参数，可以使用深度神经网络(或使用传统方法，如<a class="ae ll" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank"> <em class="ld">期望最大化</em> </a>算法)来学习。</p><p id="a185" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">VAE 使用这种深度神经网络来参数化定义潜在变量模型的概率分布。此外，它提供了一个有效的近似推理过程，规模大的数据集。它由一个<em class="ld">生成模型</em>(潜在变量模型)、一个<em class="ld">推理网络</em>(变分近似)和一种如何学习 VAE 参数的方法来定义。关于 VAE 在 Keras 的非常好的介绍和实现，你可以访问这个漂亮的<a class="ae ll" href="https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">博客帖子</a>。</p><p id="e98d" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">生成模型由等式(6)给出，这里<strong class="kh jj"> <em class="ld"> z </em> </strong>是具有<em class="ld"> K- </em>维的连续潜变量。它的先验通常是具有零均值和单位协方差矩阵的高斯，</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/242313b2804f63a1a857a8a7085c2524.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*J8ExZ54Qw7syAVevwnR5zg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Gaussian prior with zero mean and identity covariance; Equation (7)</figcaption></figure><p id="3efd" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这种可能性被称为<em class="ld">解码器</em>，它通常是连续数据的高斯分布，其参数<em class="ld">θ</em>通过将潜在状态<strong class="kh jj"> <em class="ld"> z </em> </strong>传递通过深度神经网络来计算。这种可能性看起来如下，</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/6ed8e7f73a6e2eda67a064b83df35d55.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*KWbpbjfy_7wCJc0n7NpCQA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Likelihood as continuous Gaussian distribution; Equation (8)</figcaption></figure><p id="99fd" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">均值和方差由两个深度神经网络参数化，其输出向量的维数为<em class="ld"> D </em>，即观测值的维数<strong class="kh jj">T3】xT5。参数<em class="ld">θ</em>是解码器神经网络的权重和偏差。</strong></p><p id="641b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">推理网络被称为<em class="ld">编码器</em>，允许我们计算后验近似的参数。变分参数<em class="ld">φ</em>在所有数据点上共享，而不是每个数据点都有一组参数。同样，在 VAE 设置中，我们使用深度神经网络，该网络获取输入数据点并输出相应高斯变分近似的均值和对角协方差矩阵，</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/81d121462b2e4efcb6582c24ec93a6b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*RL_6H3aH9uLIA8kIZkfrGA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Posterior approximation of VAE; Equation (9)</figcaption></figure><p id="4532" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">共享的变分参数<em class="ld">φ</em>是编码器神经网络的权重和偏差。</p><h2 id="9563" class="nd lt ji bd lu ne nf dn ly ng nh dp mc kq ni nj mg ku nk nl mk ky nm nn mo no bi translated">参数学习和 VAE 的目标函数</h2><p id="9e0a" class="pw-post-body-paragraph kf kg ji kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">如上所述，边际分布<em class="ld">p(</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>(由<em class="ld">θ</em>参数化)在很多情况下是难以处理的，需要近似。使用 ELBO 可以获得一个近似值。为了明确 ELBO 依赖于某个参数，我们可以将它重写为</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/7a042d61aafefd91426c60fd5fa74141.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*uLf8rpw0-23jAH-JPOI53A.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">ELBO parametized by theta; Equation (10)</figcaption></figure><p id="c7a9" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了学习参数，我们可以使用期望最大化(EM)使 ELBO 相对于其参数最大化。对于 VAE 设置来说，最大化反而是超过参数<em class="ld">φ</em>的<em class="ld"> q </em> <strong class="kh jj"> <em class="ld"> </em> </strong>。因此，我们可以将 ELBO 分解为两项:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/c11c7d36ad0361d2addc94395b06d5f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hJ4Eygp5xE11w9mmVF64Jg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Objective Function. The left term represents the reconstruction loss and the right term represents the regularization loss; Equation (11)</figcaption></figure><p id="3ea8" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj"> <em class="ld"> F </em> </strong>的第一项是<em class="ld">重构损失</em>，鼓励似然和推理网络准确重构数据。第二项是<em class="ld">正则化损失</em>并惩罚与先验相差太远的后验近似。具有参数<em class="ld">φ</em>和<em class="ld">θ</em>的两个神经网络都可以通过具有反向传播的梯度下降来有效地计算。此外，参数是联合更新的，而不是像 EM 那样迭代更新。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="60af" class="ls lt ji bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">结论</h1><p id="6406" class="pw-post-body-paragraph kf kg ji kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">在这篇文章中，我介绍了潜在变量模型的概念，以及用深度神经网络来参数化定义潜在变量模型的概率分布。在这里，我们关注了变分自动编码器(一种生成模型)所使用的对后验分布<em class="ld">p(</em><strong class="kh jj"><em class="ld">z</em></strong><em class="ld">|</em><strong class="kh jj"><em class="ld">x</em></strong><em class="ld">)</em>的近似的变分推断。VAE 的真正力量在于，它们可以在完全无人监督的情况下接受训练，并学习捕捉数据自然特征的潜在空间。</p><p id="ed5e" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果我们能够将复杂的高维数据嵌入到一个潜在空间中，从这个潜在空间中我们能够生成与原始数据非常相似的新数据，我们可以假设我们的模型捕捉到了数据的主要特征。这可以给研究人员和从业人员提供许多有用的信息来研究感兴趣的数据，并确定模式、相关性甚至因果结构。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h2 id="7e05" class="nd lt ji bd lu ne nf dn ly ng nh dp mc kq ni nj mg ku nk nl mk ky nm nn mo no bi translated">参考</h2><p id="c5bf" class="pw-post-body-paragraph kf kg ji kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">[1] Goodfellow，Ian，et al. <a class="ae ll" href="https://arxiv.org/pdf/1406.2661.pdf" rel="noopener ugc nofollow" target="_blank">“生成性对抗性网络”</a>，NIPS (2014)</p><p id="8f90" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[2]迪德里克·p·金马，马克斯·韦林。<a class="ae ll" href="https://arxiv.org/abs/1312.6114" rel="noopener ugc nofollow" target="_blank">自动编码变分贝叶斯</a>，arXiv 预印本 arXiv:1312.6114 (2014)。</p><p id="189c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[3] Martin Arjovsky、Soumith Chintala 和 Léon Bottou。<a class="ae ll" href="https://arxiv.org/pdf/1701.07875.pdf" rel="noopener ugc nofollow" target="_blank">《瓦瑟斯坦甘》</a>，arXiv 预印本 arXiv:1701.07875 (2017)。</p><p id="7efd" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[4]戴维·布莱、阿尔普·库库尔比尔、乔恩·麦考利夫。"<a class="ae ll" href="https://arxiv.org/pdf/1601.00670.pdf" rel="noopener ugc nofollow" target="_blank">变分推断:统计学家回顾</a>，arXiv 预印本 arXiv:1601.00670</p><p id="b12d" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[5]凯文·p·墨菲，“机器学习:概率观点”，麻省理工学院出版社(2012 年)</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h2 id="e248" class="nd lt ji bd lu ne nf dn ly ng nh dp mc kq ni nj mg ku nk nl mk ky nm nn mo no bi translated">关于变分自动编码器的进一步阅读</h2><p id="9def" class="pw-post-body-paragraph kf kg ji kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated"><a class="ae ll" href="https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> <strong class="kh jj">阿古斯蒂斯·克里斯蒂的博客</strong> </a> <strong class="kh jj"> : </strong> <em class="ld">变分自动编码器:直觉与实现</em></p><p id="3611" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><a class="ae ll" href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" rel="noopener ugc nofollow" target="_blank"> <strong class="kh jj"> JAAN ALTOSAAR 博客</strong> </a> <strong class="kh jj"> : </strong> <em class="ld">教程——什么是变分自动编码器？</em></p></div></div>    
</body>
</html>