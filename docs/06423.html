<html>
<head>
<title>Maximum Likelihood Estimation VS Maximum A Posterior</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大似然估计与最大后验概率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mle-vs-map-a989f423ae5c?source=collection_archive---------5-----------------------#2019-09-15">https://towardsdatascience.com/mle-vs-map-a989f423ae5c?source=collection_archive---------5-----------------------#2019-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="fb61" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习和深度学习之旅</h2><div class=""/><div class=""><h2 id="ee8d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">MLE 和 MAP 背后的数学，展示了这两种方法的联系和区别</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3963841c212bafaf73166b87949ccf80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_4V2L9hf_oaEuexExDerXw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Image by Author</figcaption></figure><p id="f422" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最大似然估计(MLE)和最大后验估计(MAP)都用于估计分布的参数。MLE 也广泛用于估计机器学习模型的参数，包括朴素贝叶斯和逻辑回归。它如此普遍和受欢迎，以至于有时人们甚至在不太了解它的情况下使用 MLE。例如，当对数据集拟合正态分布时，人们可以立即计算样本均值和方差，并将其作为分布的参数。虽然最大似然法是一种非常流行的参数估计方法，但是它是否适用于所有的情况呢？MLE 是如何工作的？MLE 和 MAP 有什么联系和区别？这篇博客的目的就是要回答这些问题。</p><p id="67dd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">最大似然估计</strong></p><p id="b82d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">MLE 的目标是推断似然函数<em class="md">p(X |θ)中的<em class="md">θ</em>。</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/164797551c1174103b216d37a218011d.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/1*Yn-toKQ53M7jdMMxGBYSbA.gif"/></div></figure><p id="6ff3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用这个框架，首先我们需要导出对数似然函数，然后通过使关于<em class="md">θ</em>的导数等于 0 或通过使用各种优化算法(如梯度下降)来最大化它。由于对偶，最大化对数似然函数等于最小化负对数似然。在机器学习中，最小化负对数似然是优选的。例如，它在逻辑回归中用作损失函数，交叉熵。</p><p id="4b81" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以抛硬币为例，更好的理解 MLE。例如，如果你掷一枚硬币 1000 次，有 700 个正面和 300 个反面。这枚硬币正面朝上的概率是多少？这是一枚公平的硬币吗？</p><p id="fe60" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，每次抛硬币都遵循伯努利分布，因此可能性可以写成:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/70cc19abb223e3b6e4c81ff4326ea9ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/1*RU34N54cKkrseoniCv1GRw.gif"/></div></figure><p id="84a9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在公式中，<em class="md"> xi </em>表示单尾(0 或 1)，而<em class="md"> x </em>表示总头数。然后记录下可能性:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/aaa6bd81242ae8cf94571f0d6d1fa07a.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/1*UaDE6FgzTZuIEhkTzQykdA.gif"/></div></figure><p id="11f5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对数似然函数对<em class="md"> p </em>求导，则我们可以得到:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/acca0c2f8d6bc51b4be5ce8e032a75dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/1*XOUjxUIODWCT7Hnl9ZB3bw.gif"/></div></figure><p id="4e12" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后，<em class="md"> p </em>的估计值为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/dab16d592c3a1c5f1ca9eeff357d66da.png" data-original-src="https://miro.medium.com/v2/resize:fit:112/1*Dlp-k8Ae4msuNq1L9A0dZg.gif"/></div></figure><p id="389b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，在这个例子中，这个典型硬币的正面概率是 0.7。显然，这不是一枚公平的硬币。</p><p id="642e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">根据大数定律，一系列伯努利试验的经验成功概率将收敛于理论概率。然而，如果你把这个硬币抛 10 次，有 7 个正面和 3 个反面。结论还成立吗？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/7429b98e4c01af2b9afbbcc25417361f.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*5XvciKxO5OMCK9ngad7XnQ.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/e0bcd96c22e3543c5a90d2542442a36b.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*Ll0AjSwtSfojde-I37ATfQ.png"/></div></figure><p id="ea09" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">即使<em class="md"> p(Head = 7| p=0.7) </em>大于<em class="md"> p(Head = 7| p=0.5) </em>，我们也不能忽视<em class="md"> p(Head) </em> = 0.5 的可能性仍然存在。那就是 MLE(频率主义推理)的问题。它从不使用或给出假设的概率。</p><p id="5983" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">举一个更极端的例子，假设你掷硬币 5 次，结果都是正面。是不是就可以下结论<em class="md"> p(Head) </em> =1？答案是否定的。这又引出了另一个问题。当样本容量较小时，极大似然估计的结论不可靠。</p><p id="6326" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">最大一条后路</strong></p><p id="8921" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">回想一下，我们可以用贝叶斯法则把后验写成似然性和先验的乘积:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/af6059d32b03ba5cd78917a2ab79ff6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*HiJ4JaVAc6IIFqQqDW87wg.png"/></div></figure><p id="b5d3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">公式中，<em class="md"> p(y|x) </em>为后验概率；<em class="md"> p(x|y) </em>是似然性；<em class="md"> p(y) </em>是先验概率<em class="md"> p(x) </em>是证据。</p><p id="7508" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了得到 MAP，我们可以用后验概率代替 MLE 中的似然性:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/d58c118f94811fcccb2d5a2d0044734d.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*nnB9V9HPHsMdofRyh_UbPw.png"/></div></figure><p id="5c24" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对比 MAP 和 MLE 的方程，我们可以看到唯一的区别就是 MAP 在公式中包含了先验，也就是说可能性是由 MAP 中的先验来加权的。</p><p id="df87" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在先验服从均匀分布的特殊情况下，这意味着我们给<em class="md">θ的所有可能值分配相等的权重。</em>在这种情况下，映射可以写成:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/d551f5167189d67f091f339106fdc495.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*Vp-Wy_o3FIvg_rBnRRGSzg.png"/></div></figure><p id="9230" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">根据上面的公式，我们可以得出结论，当先验服从均匀分布时，最大似然估计是映射的一个特例。这就是地图和 MLE 的联系。</p><p id="249e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们回到前面的例子，抛一枚硬币 10 次，有 7 个正面和 3 个反面。本次应用 MAP 计算<em class="md"> p(头)</em>。贝叶斯分析从选择一些先验概率值开始。这里我们列出三个假设，p(head)等于 0.5，0.6 或者 0.7。相应的先验概率等于 0.8、0.1 和 0.1。类似地，我们计算第 3 列中每个假设下的可能性。注意，列 5，后验，是列 4 的归一化。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/7ca07829f6f28bcc16e39fd0d74775b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*MW_d0rCQRcmAKVKLmj0Jow.png"/></div></figure><p id="120a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这种情况下，即使当 p(head)=0.7 时可能性达到最大，当 p(head)=0.5 时后验性达到最大，因为现在可能性被先验加权。通过使用 MAP，<em class="md"> p(头)</em> = 0.5。但是，如果改变第 2 列中的先验概率，我们可能会有不同的答案。因此，对 MAP(贝叶斯推理)的主要批评之一是，主观先验是主观的。</p><p id="6eb8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">希望看完这篇博客，你清楚 MLE 和 MAP 的联系和区别，以及如何自己手动计算。在下一篇博客中，我将解释 MAP 如何应用于收缩方法，如套索和岭回归。如果你有兴趣，请阅读我的其他博客:</p><div class="mn mo gp gr mp mq"><a href="https://medium.com/@songyangdetang_41589/table-of-contents-689c8af0c731" rel="noopener follow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd jd gy z fp mv fr fs mw fu fw jc bi translated">目录</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">这一系列博客将从理论和实现两个方面对深度学习进行介绍。</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">medium.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne lb mq"/></div></div></a></div></div></div>    
</body>
</html>