<html>
<head>
<title>Machine Learning- Predicting House prices with Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习——用回归预测房价</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-predicting-house-prices-with-regression-2388bb876a6f?source=collection_archive---------26-----------------------#2019-10-19">https://towardsdatascience.com/machine-learning-predicting-house-prices-with-regression-2388bb876a6f?source=collection_archive---------26-----------------------#2019-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="dfe6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">运行算法以获得最准确的结果</p><p id="3cbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文是我关于住房数据集的系列文章的最后一篇。对于门外汉，我已经在前两篇文章中介绍了<a class="ae ko" rel="noopener" target="_blank" href="/exploratory-data-analysis-unravelling-a-story-with-data-b01d70069035"> EDA </a>和<a class="ae ko" rel="noopener" target="_blank" href="/feature-engineering-translating-the-dataset-to-a-machine-ready-format-af4788d15d6c">特性工程</a>。</p><p id="c6a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总结到目前为止的工作——我们在第二篇文章中讨论了 EDA 中极其平凡的数据管理工作和细致的功能再造。我们研究了所有的变量，根据变量与目标值的相关性决定保留什么，放弃什么。我们最终选择了 64 个精心挑选的特征来训练数据集并预测最终的房价！</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/cf269f80971b912f8e49b4b39c13c5f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XMbwmj-4r80bBuIg.jpg"/></div></div></figure><p id="bafa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，我们将数据集以 80:20 的比例分成训练和测试。</p><pre class="kq kr ks kt gt lb lc ld le aw lf bi"><span id="0b0c" class="lg lh it lc b gy li lj l lk ll">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = 42)</span></pre><p id="52f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们使用随机森林回归器来训练数据集，并使用随机搜索 CV 来获得最佳超参数。</p><pre class="kq kr ks kt gt lb lc ld le aw lf bi"><span id="048d" class="lg lh it lc b gy li lj l lk ll">rf = RandomForestRegressor(random_state = 42)</span><span id="b51d" class="lg lh it lc b gy lm lj l lk ll">#Hyperparamater tuning using RanodomSearchCV<br/><br/>random_grid = { <br/>    'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],<br/>    'max_features': ['auto', 'sqrt', 'log2'],<br/>    'max_depth' : [6,7,8,9,10],<br/>    'min_samples_split' : [2, 5, 10],<br/>    'min_samples_leaf' : [1, 2, 4]<br/>}</span><span id="143f" class="lg lh it lc b gy lm lj l lk ll">rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)</span></pre><p id="6336" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们在训练集上拟合模型，并获得可能的最佳分数。</p><pre class="kq kr ks kt gt lb lc ld le aw lf bi"><span id="b290" class="lg lh it lc b gy li lj l lk ll">rf_random.fit(X_train, y_train)<br/>print(rf_random.best_params_)<br/>print(rf_random.best_score_)</span></pre><p id="3df7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最好成绩是 0.87。在进行网格搜索时，最佳得分略微增加到 0.88。</p><pre class="kq kr ks kt gt lb lc ld le aw lf bi"><span id="d8a0" class="lg lh it lc b gy li lj l lk ll">#Hyperparameter tuning using GridSearchCV</span><span id="d917" class="lg lh it lc b gy lm lj l lk ll">param_grid = { <br/>    'n_estimators': [int(x) for x in np.linspace(start = 600, stop = 2000, num = 10)],<br/>    'max_features': ['auto', 'sqrt', 'log2'],<br/>    'max_depth' : [7,8,9,10],<br/>    'min_samples_split' : [2, 5],<br/>    'min_samples_leaf' : [1, 2]<br/>}</span><span id="5359" class="lg lh it lc b gy lm lj l lk ll">grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, <br/>                          cv = 3, n_jobs = -1, verbose = 2)</span><span id="b8c1" class="lg lh it lc b gy lm lj l lk ll">grid_search.fit(X_train, y_train)<br/>print(grid_search.best_params_)<br/>print(grid_search.best_score_)</span></pre><p id="1278" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Lasso 回归和随机搜索给出了一个差得多的最佳分数 0.85。</p><p id="4bd7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">XGBoost 回归和随机搜索 CV 给出了更高的分数 0.9。</p><p id="0351" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而我用岭回归得到了最好的分数。</p><pre class="kq kr ks kt gt lb lc ld le aw lf bi"><span id="d721" class="lg lh it lc b gy li lj l lk ll">#Ridge Regressor</span><span id="2a7b" class="lg lh it lc b gy lm lj l lk ll">params_ridge ={<br/>        'alpha':[0.25,0.5,1],<br/>        'solver':['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']<br/>        }</span><span id="3b79" class="lg lh it lc b gy lm lj l lk ll">ridge = Ridge()<br/>ridge_random = RandomizedSearchCV(estimator = ridge, param_distributions = params_ridge,<br/>                               n_iter=50, cv=5, n_jobs=-1,random_state=42, verbose=2)<br/>ridge_random.fit(X_train, y_train)<br/>print(ridge_random.best_params_)<br/>print(ridge_random.best_score_)</span><span id="216d" class="lg lh it lc b gy lm lj l lk ll">ridge_grid = GridSearchCV(estimator = ridge, param_grid = params_ridge, cv = 5, n_jobs = -1, verbose = 2)</span><span id="a5ae" class="lg lh it lc b gy lm lj l lk ll">ridge_grid.fit(X_train, y_train)<br/>print(ridge_grid.best_params_)<br/>print(ridge_grid.best_score_)</span></pre><p id="dcab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随机搜索和网格搜索给我的最高分都是 0.92。</p><p id="bd95" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，我们从最佳估计量着手，对测试集进行预测。</p><pre class="kq kr ks kt gt lb lc ld le aw lf bi"><span id="bd12" class="lg lh it lc b gy li lj l lk ll">model_ridge = ridge_random.best_estimator_<br/>y_pred_ridge = np.exp(model_ridge.predict(X_test))<br/>output_ridge = pd.DataFrame({'Id': test['Id'], 'SalePrice': y_pred_ridge})<br/>output_ridge.to_csv('prediction_ridge.csv', index=False)</span></pre><p id="9770" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这让我在 Kaggle 上得了 0.12460 分！</p><p id="a84b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">完整代码请参考以下链接:<a class="ae ko" href="https://github.com/pritha21/Kaggle/blob/master/House_Prices.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/pritha 21/ka ggle/blob/master/House _ prices . ipynb</a></p><p id="ae67" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可能需要使用<a class="ae ko" href="https://nbviewer.jupyter.org/" rel="noopener ugc nofollow" target="_blank">https://nbviewer.jupyter.org/</a>来查看</p><p id="fda9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">任何帮助我提高分数的建议都欢迎！</p></div></div>    
</body>
</html>