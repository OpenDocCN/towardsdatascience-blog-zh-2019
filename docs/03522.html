<html>
<head>
<title>Advanced Topics in Deep Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度卷积神经网络的高级课题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advanced-topics-in-deep-convolutional-neural-networks-71ef1190522d?source=collection_archive---------6-----------------------#2019-06-05">https://towardsdatascience.com/advanced-topics-in-deep-convolutional-neural-networks-71ef1190522d?source=collection_archive---------6-----------------------#2019-06-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4b1a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">残差网络、显著图、扩张卷积等等。</h2></div><blockquote class="kf kg kh"><p id="4ac2" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">“如果我们想让机器思考，我们需要教它们看”——<strong class="kl ir"><em class="iq">费-</em></strong></p></blockquote><p id="c63b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在整篇文章中，我将讨论卷积神经网络的一些更复杂的方面，以及它们如何与特定任务相关，如对象检测和面部识别。</p><p id="c815" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">本教程将讨论的主题有:</p><ul class=""><li id="2bfa" class="li lj iq kl b km kn kp kq lf lk lg ll lh lm le ln lo lp lq bi translated"><strong class="kl ir"> CNN 评论</strong></li><li id="b8b9" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated"><strong class="kl ir">感受野和扩张的回旋</strong></li><li id="5e82" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated"><strong class="kl ir">显著图</strong></li><li id="31ad" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated"><strong class="kl ir">转置卷积</strong></li><li id="1bb2" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated"><strong class="kl ir">经典网络</strong></li><li id="fb25" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated"><strong class="kl ir">剩余网络</strong></li><li id="c20a" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated"><strong class="kl ir">迁移学习</strong></li></ul><p id="33b1" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这篇文章是我的文章标题:<a class="ae lw" rel="noopener" target="_blank" href="/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac"> <strong class="kl ir">神经网络简单介绍</strong> </a> <strong class="kl ir">的自然延伸。如果你不熟悉卷积神经网络的思想和功能，我建议你在阅读本文其余部分之前先看看这个。</strong></p><p id="de2d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">由于原始文章过长，我决定省略与对象检测和面部识别系统相关的几个主题，以及研究文献中目前正在试验的一些更深奥的网络架构和实践。我可能会在未来的一篇文章中更具体地讨论深度学习在计算机视觉中的应用。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/850a6edbb237cd38811a0040d58a20a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQ_WrDtAmy-IffwFh-I1Kg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">You Only Look Once (YOLO) — 2016</figcaption></figure><p id="c224" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">所有相关代码现在都可以在我的 GitHub 存储库中找到:</p><div class="mn mo gp gr mp mq"><a href="https://github.com/mrdragonbear/CNNs" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">龙熊先生/CNN</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">github.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne mh mq"/></div></div></a></div></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="3b3b" class="nm nn iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated"><strong class="ak"> CNN 评论</strong></h1><p id="53eb" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">在我的原始文章中，我讨论了为什么全连接网络不足以完成图像分析任务的动机。CNN 的独特之处如下:</p><ul class=""><li id="73a3" class="li lj iq kl b km kn kp kq lf lk lg ll lh lm le ln lo lp lq bi translated">比全连接网络的参数(权重和偏差)更少。</li><li id="6323" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">对对象平移不变-它们不依赖于特征在图像中出现的位置。</li><li id="ba6a" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">可以容忍图像中的一些失真。</li><li id="cdd2" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">能够归纳和学习特征。</li><li id="1b74" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">需要网格输入。</li></ul><p id="3c2c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">卷积层由<strong class="kl ir">滤波器、特征图、激活函数构成。</strong>这些卷积层可以<strong class="kl ir">满</strong>、<strong class="kl ir">同</strong>或<strong class="kl ir">有效。</strong></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/583de265fa27d27469561ad1ce30df18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*FyIckl3PUhYyT70prnZb3A.png"/></div></figure><p id="4460" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">如果已知输入中的层数、<em class="kk"> nᵢ </em>、该级中的滤波器数量、<em class="kk"> f </em>、步长大小、<em class="kk"> s </em>以及图像的像素尺寸、<em class="kk"> p </em>(假设是正方形)，我们就可以确定给定卷积块的输出层数。</p><p id="b13f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">池层用于减少过度拟合。完全连接的图层用于将空间和通道要素混合在一起。在整个图像上绘制了特征图之后，每个过滤层对应于该图像，这就是提取特征的方式。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ok"><img src="../Images/d7be0a4ee55dcacc84ec08e103545266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AQRDg5htlxrZ4k8rA-l_Iw.png"/></div></div></figure><p id="c0d2" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">知道输入和输出层的数量很重要，因为这决定了构成神经网络参数的权重和偏差的数量。网络中的参数越多，需要训练的参数就越多，这导致训练时间更长。训练时间对于深度学习非常重要，因为它是一个限制因素，除非你可以访问强大的计算资源，如计算集群。</p><p id="6108" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">下面是一个示例网络，我们将计算其参数总数。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ol"><img src="../Images/3a7da5064a7f54b6170afd2cf2282ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XDHKbMgrJG67giqkxEM1IQ.png"/></div></div></figure><p id="a285" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在这个网络中，卷积滤波器有 250 个权重和 10 个偏置项。我们没有最大池层的权重。在 max-pooling 层之后，我们有 13 × 13 × 10 = 1，690 个输出元素。我们有一个 200 节点的全连接层，因此全连接层中共有 1，690 × 200 = 338，000 个权重和 200 个偏置项。因此，我们总共有 338，460 个参数要在网络中训练。我们可以看到，大多数训练参数出现在完全连接的输出层。</p><p id="4c58" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">CNN 的每一层都学习越来越复杂的过滤器。第一层学习基本的特征检测过滤器，例如边缘和拐角。中间层学习检测物体部分的过滤器——对于人脸，他们可能会学习对眼睛和鼻子做出反应。最后一层有更高的表现:他们学习识别不同形状和位置的完整物体。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi om"><img src="../Images/6feee7a386c6cd7015496f0b3b20683f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZD3ewOfpfsMAjhp4MYFnog.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Feature maps showing increasing resolution of features through different convolutional layers of a neural network.</figcaption></figure><p id="a3b5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">对于那些在继续之前需要更直观地了解卷积神经网络的人来说，看看这个三维表示可能会有所帮助:</p><div class="mn mo gp gr mp mq"><a href="http://scs.ryerson.ca/~aharley/vis/conv/" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">卷积神经网络的三维可视化</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">编辑描述</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">加利福尼亚洲</p></div></div></div></a></div><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="1532" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在下一节中，我们将更详细地讨论卷积层感受野的概念。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="151b" class="nm nn iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated">感受野和扩张的脑回</h1><p id="fb99" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">感受野被定义为输入空间中特定 CNN 特征所关注的区域(即受其影响)。在一个 5 × 5 的输入图上应用一个内核大小为<em class="kk">k =</em>3<em class="kk">T5×<em class="kk">T7】3，填充大小为<em class="kk"> p = </em> 1 × 1，步距为<em class="kk"> s = </em> 2 × 2 的卷积<em class="kk"> C </em>，我们将得到一个 3 × 3 的输出特征图(绿色图)。</em></em></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi op"><img src="../Images/318b9d980b39e9efa78925f5242f8bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EGrF2VfCCCOykPHzVsRuLQ.png"/></div></div></figure><p id="42b9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在 3 × 3 特征图的顶部应用相同的卷积，我们将得到 2 × 2 特征图(橙色图)。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi oq"><img src="../Images/87cc38e0002e3f0a8060eb5094fc86c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LUH4Q_6jHk6jffXsrdU_Sw.png"/></div></div></figure><p id="4ca1" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">让我们在一维中再次观察感受野，没有填充，步幅为 1，核大小为 3 × 1。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi or"><img src="../Images/486170c5af0121f2c7056a69f51800bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H8N9enoncUuvpTMmK11EEg.png"/></div></div></figure><p id="acf1" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们可以跳过其中的一些连接，以创建一个扩展的卷积，如下所示。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi or"><img src="../Images/4f6651eb85a0f391e8e466ded8f1435f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CkkGVDACUZzI-eLXcJ2Nmg.png"/></div></div></figure><p id="344b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这种扩展卷积的工作方式与正常卷积类似，主要区别在于感受野不再由连续像素组成，而是由其他像素分隔的单个像素组成。下图显示了将扩展卷积图层应用于图像的方式。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/09880593b49347524b95615ada1041cd.png" data-original-src="https://miro.medium.com/v2/0*3cTXIemm0k3Sbask.gif"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Dilated convolution</figcaption></figure><p id="ea55" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">下图显示了二维数据的扩展卷积。红点是 3 × 3 滤波器的输入，绿色区域是每个输入捕捉的感受野。感受野是每个输入(单元)对下一层的初始输入所捕获的隐含区域。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/46022047de14e5688756fdd46897bdfe.png" data-original-src="https://miro.medium.com/v2/format:webp/1*tnDNIyPePgHvb8JIx8SbqA.png"/></div></figure><p id="18a0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">使用扩张卷积背后的动机是:</p><ul class=""><li id="4442" class="li lj iq kl b km kn kp kq lf lk lg ll lh lm le ln lo lp lq bi translated">通过以更高的分辨率处理输入来检测细节。</li><li id="fceb" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">更广泛的输入视图，以获取更多上下文信息。</li><li id="3024" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">运行时间更快，参数更少</li></ul><p id="9ac5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在下一节中，我们将讨论使用显著图来检查卷积网络的性能。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="66e7" class="nm nn iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated">显著图</h1><p id="216b" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">显著图是一种有用的技术，数据科学家可以用它来检查卷积网络。它们可以用来研究神经元的激活模式，以了解图像的哪些特定部分对特定特征很重要。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/ce69111129ba17c464098c8f88b21413.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*hRUZ0aAdeUsGMiHvbbbuIA.png"/></div></figure><p id="c811" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">让我们想象一下，给你一张狗的图片，让你给它分类。这对于人类来说非常简单，但是，深度学习网络可能没有你聪明，可能会将其归类为猫或狮子。它为什么这样做？</p><p id="8d86" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">网络可能错误分类图像的两个主要原因是:</p><ul class=""><li id="91dc" class="li lj iq kl b km kn kp kq lf lk lg ll lh lm le ln lo lp lq bi translated">训练数据中的偏差</li><li id="245d" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">没有正规化</li></ul><p id="0e9f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们想了解是什么让网络给出某个类作为输出——一种方法是使用显著图。显著图是一种测量给定图像中特定类别的空间支持度的方法。</p><p id="5506" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><em class="kk">“当图像 I 通过我的网络时，找出负责具有分数 S(C)的类 C 的像素”。</em></p><p id="77d5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们如何做到这一点？我们差异化！对于任意一个函数<em class="kk"> f(x，y，z) </em>，我们可以通过求变量<em class="kk"> x，y，z </em>在任意一个特定点<em class="kk"> (x₁，y₁，z₁) </em>对这些变量的偏导数，来求变量<em class="kk">x，y，z</em>对该点的影响。类似地，为了找到负责的像素，我们对类<em class="kk"> C </em>取分数函数<em class="kk"> S </em>，并对每个像素取偏导数。</p><p id="05e9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这很难自己实现，但幸运的是，auto-grad 可以做到这一点！该过程工作如下:</p><ol class=""><li id="0d70" class="li lj iq kl b km kn kp kq lf lk lg ll lh lm le ou lo lp lq bi translated">通过网络转发图像。</li><li id="1fe8" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ou lo lp lq bi translated">计算每门课的分数。</li><li id="7fc6" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ou lo lp lq bi translated">对于除类别<em class="kk"> C </em>之外的所有类别，强制最后一层的分数<em class="kk"> S </em>的导数为 0。对于<em class="kk"> C </em>，将其设置为 1。</li><li id="0a68" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ou lo lp lq bi translated">通过网络反向传播这个导数。</li><li id="3504" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ou lo lp lq bi translated">渲染它们，你就有了你的显著图。</li></ol><p id="34f8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">注意:在步骤#2 中，我们没有使用 softmax，而是将其转换为二元分类并使用概率。</p><p id="7dfe" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这里有一些显著图的例子。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ov"><img src="../Images/9ce16aabb41cce8d6b9afb9071da28b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eCTNjl98_xrjCRHFgyae5g.png"/></div></div></figure><p id="6f3d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们如何处理彩色图像？获取每个通道的显著图，或者取最大值、平均值，或者使用所有 3 个通道。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ow"><img src="../Images/6d18ac202c7f3b2f0df1a6d852841c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MzdWXaINBa2s0INOVBmnBA.png"/></div></div></figure><p id="47f9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">概述显著图功能的两篇优秀论文是:</p><ul class=""><li id="5c15" class="li lj iq kl b km kn kp kq lf lk lg ll lh lm le ln lo lp lq bi translated"><a class="ae lw" href="https://arxiv.org/pdf/1312.6034.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir">深层卷积网络:可视化图像分类模型和显著图</strong> </a></li><li id="5368" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated"><a class="ae lw" href="https://arxiv.org/pdf/1704.03549.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir">基于注意力的街景图像结构化信息提取</strong> </a></li></ul><p id="2d00" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">有一个与本文相关的 GitHub 库，我在其中展示了如何生成显著图(这个库可以在这里找到<a class="ae lw" href="https://github.com/mrdragonbear/CNNs" rel="noopener ugc nofollow" target="_blank"/>)。下面是 Jupyter 笔记本中的一段代码:</p><pre class="ly lz ma mb gt ox oy oz pa aw pb bi"><span id="1dbc" class="pc nn iq oy b gy pd pe l pf pg"><strong class="oy ir">from</strong> <strong class="oy ir">vis.visualization</strong> <strong class="oy ir">import</strong> visualize_saliency<br/><strong class="oy ir">from</strong> <strong class="oy ir">vis.utils</strong> <strong class="oy ir">import</strong> utils<br/><strong class="oy ir">from</strong> <strong class="oy ir">keras</strong> <strong class="oy ir">import</strong> activations<br/><br/><em class="kk"># Utility to search for layer index by name. </em><br/><em class="kk"># Alternatively we can specify this as -1 since it corresponds to the last layer.</em><br/>layer_idx = utils.find_layer_idx(model, 'preds')</span><span id="3be2" class="pc nn iq oy b gy ph pe l pf pg">plt.rcParams["figure.figsize"] = (5,5)<br/><strong class="oy ir">from</strong> <strong class="oy ir">vis.visualization</strong> <strong class="oy ir">import</strong> visualize_cam<br/><strong class="oy ir">import</strong> <strong class="oy ir">warnings</strong><br/>warnings.filterwarnings('ignore')<br/><br/><em class="kk"># This corresponds to the Dense linear layer.</em><br/><strong class="oy ir">for</strong> class_idx <strong class="oy ir">in</strong> np.arange(10):    <br/>    indices = np.where(test_labels[:, class_idx] == 1.)[0]<br/>    idx = indices[0]<br/><br/>    f, ax = plt.subplots(1, 4)<br/>    ax[0].imshow(test_images[idx][..., 0])<br/>    <br/>    <strong class="oy ir">for</strong> i, modifier <strong class="oy ir">in</strong> enumerate([<strong class="oy ir">None</strong>, 'guided', 'relu']):<br/>        grads = visualize_cam(model, layer_idx, filter_indices=class_idx, <br/>                              seed_input=test_images[idx], backprop_modifier=modifier)        <br/>        <strong class="oy ir">if</strong> modifier <strong class="oy ir">is</strong> <strong class="oy ir">None</strong>:<br/>            modifier = 'vanilla'<br/>        ax[i+1].set_title(modifier)    <br/>        ax[i+1].imshow(grads, cmap='jet')</span></pre><p id="dcc0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这段代码导致生成以下显著图(假设安装了相关的库<code class="fe pi pj pk oy b">vis.utils</code>和<code class="fe pi pj pk oy b">vis.visualization</code>)。如果您想更全面地了解实现过程，请查看笔记本。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pl"><img src="../Images/29c2ced697b3f8803c3117b181a0e8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_RTuaMwj7cltHM8CvrxgJg.png"/></div></div></figure><p id="446a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在下一节中，我们将讨论通过使用转置卷积进行上采样的想法。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="6eaf" class="nm nn iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated">转置卷积</h1><p id="2678" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">到目前为止，我们所看到的卷积要么保持其输入的大小，要么使其变小。我们可以用同样的技术使输入张量变大。这个过程被称为<strong class="kl ir">上采样</strong>。当我们在一个卷积步骤内进行时，它被称为<strong class="kl ir">转置卷积</strong>或<strong class="kl ir">分数步长</strong>。</p><p id="67a0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">注意:一些作者在卷积反卷积时称之为上采样，但该名称已被以下文章中概述的不同概念所采用:</p><p id="dbc2" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><a class="ae lw" href="https://arxiv.org/pdf/1311.2901.pdf].." rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1311.2901.pdf</a></p><p id="c2c4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">为了说明转置卷积是如何工作的，我们将看一些卷积的示例。</p><p id="8001" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">第一个例子是没有填充的典型卷积层，作用于大小为 5 × 5 的图像。卷积后，我们得到一个 3 × 3 的图像。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pm"><img src="../Images/a3a766dde45cfab78e592d53c571b7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Scg2kgRYjaeYSP-pkjZiJA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Image taken from A. Glassner, Deep Learning, Vol. 2: From Basics to Practice</figcaption></figure><p id="fb27" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">现在我们来看一个填充为 1 的卷积层。原始图像是 5 × 5，卷积后的输出图像也是 5 × 5。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi pn"><img src="../Images/def89d43008e198f090f8b7ed2413ef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KlBxu_7E7v3iyROdJMPm5g.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Image taken from A. Glassner, Deep Learning, Vol. 2: From Basics to Practice</figcaption></figure><p id="890e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">现在我们来看一个填充为 2 的卷积层。原始图像是 3× 3，卷积后的输出图像也是 5 × 5。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi po"><img src="../Images/c3c6e4580cb4031fa341dd54450057b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GaMJ-DHKjz2wxyirXLnddg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Image taken from A. Glassner, Deep Learning, Vol. 2: From Basics to Practice</figcaption></figure><p id="3532" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">当在 Keras 中使用时，例如在可变自动编码器的开发中，这些是使用上采样层实现的。希望，如果你以前见过这个，现在可以理解这些卷积层如何通过使用转置卷积来增加图像的大小。</p><p id="0c34" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在下一节中，我们将讨论一些经典网络的架构。这些网络中的每一个在某种意义上都是革命性的，推动了深度卷积网络的发展。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="2b12" class="nm nn iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated">经典网络</h1><p id="1d9d" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">在这一部分，我将回顾 CNN 的一些经典架构。这些网络被用于深度学习领域的一些开创性工作，并经常用于迁移学习目的(这是未来文章的主题)。</p><p id="ed80" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">第一项提出类似于卷积神经网络的研究是由 Kunihiko Fukushima 在 1980 年提出的，被称为 NeoCognitron1，他的灵感来自于哺乳动物视觉皮层的发现。福岛将 NeoCognitron 应用于手写字符识别。</p><p id="7e1a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">到 20 世纪 80 年代末，发表了几篇论文，大大推进了该领域的发展。反向传播的想法首先由 Yann LeCun 于 1985 年在法语中发表(这也是由其他研究人员独立发现的)，随后不久由 Waiber 等人在 1989 年发表了 TDNN——用反向传播训练的类似卷积的网络的发展。最初的应用之一是由 LeCun 等人在 1989 年使用应用于手写邮政编码识别的反向传播。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h2 id="47c7" class="pc nn iq bd no pp pq dn ns pr ps dp nw lf pt pu ny lg pv pw oa lh px py oc pz bi translated"><strong class="ak"> LeNet-5 </strong></h2><p id="026d" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">与当前实践相比，LeNet-5 的公式有点过时。这是在 20 世纪末深度学习的萌芽阶段开发的第一批神经架构之一。</p><p id="2b2f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">1998 年 11 月，LeCun 发表了他最著名的论文之一，描述了一种用于文档识别的“现代”CNN 架构，称为 LeNet1。这不是他的第一次迭代，事实上，这是 LeNet-5，但这篇论文是谈论 LeNet 时经常引用的出版物。</p><p id="80dc" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">它使用卷积网络，然后是池层，最后是完全连接的层。网络首先从高维特征开始，并在增加信道数量的同时减小其尺寸。这个网络中大约有 60，000 个参数。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qa"><img src="../Images/62236b57f3245d591dfa7053250b891e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j47OBsJiY8h2sCuyDOaH5A.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">LeCun, Yann, et al. “Gradient-based learning applied to document recognition.” Proceedings of the IEEE 86.11 (1998): 2278–2324.</figcaption></figure></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h2 id="27cf" class="pc nn iq bd no pp pq dn ns pr ps dp nw lf pt pu ny lg pv pw oa lh px py oc pz bi translated">AlexNet</h2><p id="1a11" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">AlexNet 架构是深度学习中最重要的架构之一，引用次数超过 25，000 次——这在研究文献中几乎是闻所未闻的。AlexNet 由多伦多大学的 Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 于 2012 年开发，在 2012 年 ImageNet 大规模视觉识别挑战赛(ILSVRC)中击败了竞争对手。</p><p id="cf05" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">该网络在 ImageNet 数据集上进行训练，该数据集是一个由 1000 个不同类别组成的 120 万个高分辨率(227x227x3)图像的集合，使用数据扩充。该模型的深度比当时任何其他网络都大，并使用 GPU 进行了 5-6 天的训练。该网络由 12 层组成，利用了 dropout 和 smart optimizer 层，是首批实现 ReLU 激活功能的网络之一，该功能至今仍被广泛使用。该网络有超过 6000 万个参数需要优化(约 255 MB)。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qb"><img src="../Images/cb82179eba7534002e1975d8cd740fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6RoS2dbSUxvdPfdmgBHRBw.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “Imagenet classification with deep convolutional neural networks,” in Advances in neural information processing systems, pp. 1097–1105, 2012</figcaption></figure><p id="e17a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这个网络通过展示 CNN 令人印象深刻的表现和潜在的好处，几乎单枪匹马地启动了人工智能革命。该网络以 15.3%的 top-5 误差赢得了 ImageNet 大赛，比第二名低了 10.8 个百分点以上。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/ff346ef43e7fd8cdc02cb0854a18ff08.png" data-original-src="https://miro.medium.com/v2/format:webp/1*kiXf4aE2H-vZ7CBOjXIF0Q.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">ImageNet results from 2011 to 2016. <a class="ae lw" rel="noopener" target="_blank" href="/review-trimps-soushen-winner-in-ilsvrc-2016-image-classification-dfbc423111dd">Source</a></figcaption></figure><p id="d70c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们将讨论剩余的赢得 ILSVRC 的网络，因为其中大多数都是处于深度学习研究前沿的革命性网络。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h2 id="a88b" class="pc nn iq bd no pp pq dn ns pr ps dp nw lf pt pu ny lg pv pw oa lh px py oc pz bi translated">ZFNet</h2><p id="18b4" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">这个网络是由纽约大学的马修·泽勒和罗布·弗格斯引入的，他们以 11.2%的错误率赢得了 2013 年的 ILSVRC。该网络减小了过滤器的尺寸，并被训练了 12 天。</p><p id="68d8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">本文提出了一种可视化技术，称为“去卷积网络”，它有助于检查不同的特征激活及其与输入空间的关系。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qc"><img src="../Images/cb70bc2c857f604f7a6db336aa851f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dwmJUP-ZGVUYKC4rcC7tLQ.png"/></div></div></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qd"><img src="../Images/17d499fac7ab5047d9be1c20876c28eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mOcw8a9G5_hKqi4Vo8Wzfw.png"/></div></div></figure></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h2 id="cb33" class="pc nn iq bd no pp pq dn ns pr ps dp nw lf pt pu ny lg pv pw oa lh px py oc pz bi translated">VGG16 和 VGG19</h2><p id="f1e7" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">Simonyan 和 Zisserman(牛津)于 2014 年引入了 VGG 网络。这个网络在固有的简单性和结构上是革命性的。它由 16 或 19 层(因此得名)组成，总共有 1.38 亿个参数(522 MB)，使用 3×3 卷积滤波器，仅使用相同的填充和 1 的跨距，以及 2×2 最大池层，跨距为 2。</p><p id="fe50" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">作者表明，两个 3×3 滤光器具有 5×5 的有效感受野，并且随着空间尺寸减小，深度增加。这个网络被训练了两到三个星期，现在仍然习惯于这样——主要用于迁移学习。该网络最初是为 2014 年的 ImageNet 挑战赛开发的。</p><ul class=""><li id="2aa5" class="li lj iq kl b km kn kp kq lf lk lg ll lh lm le ln lo lp lq bi translated">ImageNet 挑战赛 2014；16 或 19 层</li><li id="42ba" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">1.38 亿个参数(522 MB)。</li><li id="1a70" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">卷积层使用“相同的”填充和步距<em class="kk"> s </em> = 1。</li><li id="1d6c" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">最大池层使用过滤器大小<em class="kk"> f </em> = 2，跨距<em class="kk"> s </em> = 2。</li></ul><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qe"><img src="../Images/9b0a906ddca06debec38f6c36599d4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0XkiXA6KpcMCyesrZr4LRA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Karen Simonyan and Andrew Zisserman, “Very deep convolutional networks for large-scale image recognition,” 2014.</figcaption></figure></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h2 id="5a20" class="pc nn iq bd no pp pq dn ns pr ps dp nw lf pt pu ny lg pv pw oa lh px py oc pz bi translated">谷歌网(第一版)</h2><p id="f51c" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">谷歌网络是由 Szegedy 等人(谷歌)于 2014 年推出的。该网络击败了 VGG 架构，成为 2014 年 ILSVRC 的赢家。该网络引入了初始模块的概念——具有不同滤波器大小的并行卷积层。</p><p id="2953" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这里的想法是，我们不知道哪个滤波器尺寸是最好的，所以我们只是让网络来决定。初始网络是通过连接其他初始模块而形成的。它包括几个 softmax 输出单元，以加强正则化。这是一个在未来架构开发中非常重要的关键思想。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qf"><img src="../Images/e3b75b408b8fef46f790af799acba1e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*92Q2b2Yx6rfuVJlGAwdTOw.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">GoogLeNet (Inception-v1) architecture.</figcaption></figure><p id="3a14" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">另一个有趣的特性是，最终没有完全连接的层，而是用一个平均池层来代替。移除这种完全连接的层导致网络的参数比 AlexNet 少 12 倍，使其训练速度快得多。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qg"><img src="../Images/920c1c88c0271731a7d70630ff05a93f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fYCEw_sKxZVWwNhmsBlAFQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">GoogLeNet Architecture.</figcaption></figure></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="3f2e" class="nm nn iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated"><strong class="ak">剩余网络</strong></h1><p id="6dfa" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">第一个残差网络是由何等人(微软)在 2015 年提出的。该网络在多个类别中赢得了 ILSVRC 2015。这个网络背后的主要思想是剩余块。该网络允许开发非常深的神经网络，其可以包含 100 层或更多层。</p><p id="3af9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这是革命性的，因为到目前为止，深度神经网络的发展受到消失梯度问题的抑制，该问题发生在跨越大量层传播和繁殖小梯度时。</p><p id="3846" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">作者认为，优化残差映射比原型神经架构更容易。此外，如果需要，残余块可以决定“自行关闭”。让我们比较一下普通网络和剩余网络的网络结构。简单网络结构如下:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qh"><img src="../Images/9ce4a3de2b2344c8cc32ac9db1f6986a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqeNDFq9u4Ewdgr3hbSjIw.png"/></div></div></figure><p id="67b3" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">剩余网络结构如下所示:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qi"><img src="../Images/23f741cf3fb806394201ee0e450bd62c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ilwXj5OiZZpgkXKOoPbD2g.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep residual learning for image recognition,” in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.</figcaption></figure><p id="efb0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">描述该网络的方程是:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qj"><img src="../Images/e533800a06ca00c00b98b84ee9319a09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6up_zCxCQNWeNULbDzKbA.png"/></div></div></figure><p id="9a96" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">有了这个额外的连接，渐变可以更容易地向后传播。它成为一个灵活的模块，可以扩展网络的容量，或者简单地转换成一个不影响训练的身份函数。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/31b65117590f638c0a1c5d7115939d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*xDx3v5-m_xvXcISXyHvDiQ.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Example training of an 18- and 34-layer residual network.</figcaption></figure><p id="0e10" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">残差网络顺序堆叠残差块。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ql"><img src="../Images/f1514461a36daaa096b3256462cd145f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-wn-0-m09DPzmzNseiU9w.png"/></div></div></figure><p id="2320" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">想法是允许网络变得更深，而不增加训练的复杂性。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qm"><img src="../Images/4c754f954425c2b93dcbf1ef85a10dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qjC3W0DmZxqWqnVNX2aH_Q.png"/></div></div></figure><p id="cf65" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">剩余网络实现具有卷积层的块，这些卷积层使用“相同”填充选项(即使在最大池化时)。这允许该块学习身份函数。</p><p id="d780" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">设计者可能想要减小特征的尺寸并使用“有效的”填充。—在这种情况下，快捷路径可以实现一组新的卷积层，从而适当减小尺寸。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qn"><img src="../Images/cbb52b0dfd693d03eb2e7f35ed5e7e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YAIlRbFEr62w7MQYtj6-rw.png"/></div></div></figure><p id="af82" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这些网络可能会变得巨大和极其复杂，它们的图表开始看起来类似于描述电厂功能的图表。这是这样一个网络的例子。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qo"><img src="../Images/7a09ba279ba546c27685a5e6bbf72229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UwXDFQa79Ve8jKzuCmx3bQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Source: He2016</figcaption></figure><p id="4341" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">将 ImageNet 以前的获胜者的误差值与 ResNet 公式的误差值进行比较，我们可以看到性能有明显的提高。Alexnet (2012)取得了 15.3%的前 5 名误差(第二名为 26.2%)，其次是 ZFNet (2013)取得了 14.8%的前 5 名误差(特征的可视化)，其次是 GoogLeNet (2014)，误差为 7.8%，然后是 ResNet (2015)，首次实现了 5%以下的准确率。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qp"><img src="../Images/d79f3be91d84f3986149cede55b1230f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LP8-eBEduGmsZq6yTj69LA.png"/></div></div></figure></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h2 id="da2b" class="pc nn iq bd no pp pq dn ns pr ps dp nw lf pt pu ny lg pv pw oa lh px py oc pz bi translated">密集网络</h2><p id="518c" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">最初由黄等人在 2016 年提出，作为 ResNet 哲学的激进扩展。每个块使用每个先前的特征地图作为输入，有效地连接它们。这些连接意味着网络有<em class="kk"> L(L+1)/ 2 </em>个直接连接，其中<em class="kk"> L </em>是网络的层数。人们可以将该架构视为一个展开的递归神经网络。</p><p id="057a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">每一层都将自己的<em class="kk"> k </em>特征地图添加到这个状态中。增长率决定了每一层对全局状态贡献多少新信息。这里的想法是，我们在每个点都有所有可用的先前信息。与直觉相反，这种架构减少了所需参数的总数。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qq"><img src="../Images/eeea8a9b5986103f25c08e74437be1c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lT3CBszPOLgCRgCJtEGE6g.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">A 5-layer dense block with a growth rate of k = 4. Each layer takes all preceding feature-maps as inputs.</figcaption></figure><p id="6b48" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">该网络的工作原理是通过将每一层与每一层直接连接，允许每一层最大限度的信息(和梯度)流动。这样，DenseNets 通过特征重用来挖掘网络的潜力，这意味着不需要学习冗余的特征映射。DenseNet 层相对较窄(例如，12 个过滤器)，并且它们仅添加一小组新的特征地图。</p><p id="2998" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">DenseNet 架构通常具有优于 ResNet 架构的性能，可以用更少的总体参数实现相同或更好的精度，并且网络更容易训练。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qr"><img src="../Images/e2cde54dd7a6b057f161307afeaf6cc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hyK-VbIJS4jO0xQT8y7sew.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Performance comparison of various ResNet and DenseNet architectures.</figcaption></figure><p id="867a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">网络公式一开始可能有点混乱，但它本质上是一种 ResNet 架构，分辨率块被密集块所取代。密集的连接具有正则化效果，这减少了对具有较小训练集大小的任务的过度适应。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/50b8b6fb97b8abdb5d27b429554d58c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAjWVFKAGx-oDkaLx0pcDQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Simplified DenseNet architecture schematic.</figcaption></figure><p id="594d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">请务必注意，DenseNets 不会将图层的输出要素地图与输入要素地图相加，事实上，它们会连接它们:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/00e1c50669c76298b61685bd718ee8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*v_slDXgePikIWhJoZhTH8w.png"/></div></figure><p id="7573" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">特征图的维度在一个块内保持不变，但是滤波器的数量在它们之间变化，这被称为增长率<em class="kk"> k. </em></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/7d08b4cf652d664755b6d08d4efb37c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*1Iqu0JGo3RwpaWu5Xijr0g.png"/></div></figure><p id="6d78" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">下面是密集网络的完整架构。当我们以完整的分辨率来看网络时，它是相当复杂的，这就是为什么通常以抽象的形式更容易可视化(就像我们上面所做的)。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qt"><img src="../Images/5fb24cc5dc170c298d4ab27f7dfb528e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DFl4r6AAywxQVV1sruzSrA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Full architectural layout of a DenseNet.</figcaption></figure><p id="9a99" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">关于 DenseNet 的更多信息，我推荐下面这篇文章。</p><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/densenet-2810936aeebb"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">DenseNet</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">许多论文:</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="qu l nb nc nd mz ne mh mq"/></div></div></a></div></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h2 id="9a05" class="pc nn iq bd no pp pq dn ns pr ps dp nw lf pt pu ny lg pv pw oa lh px py oc pz bi translated">网络总结</h2><p id="ff6c" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">正如我们所看到的，在短短几年的时间里，我们已经从 ImageNet 数据集(如果您记得的话，该数据集由 120 万张图像组成)上约 15%的错误率降至约 3–4%的错误率。如今，最先进的网络能够相当稳定地保持在 3%以下。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qv"><img src="../Images/ffcfb5cfd942aff63d79ca3f2f58214a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E0zyApIjgdZ8hfVE2kKdbg.png"/></div></div></figure><p id="042a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在我们能够为这些网络获得满分之前，还有相当长的路要走，但在过去十年中，进步的速度是相当惊人的，从这一点应该可以明显看出为什么我们目前正在经历一场深度学习革命——我们已经从人类具有高级视觉识别能力的阶段发展到这些网络具有高级视觉能力的阶段(人类无法在 ImageNet 数据集上达到 3%)。</p><p id="3fe8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这推动了机器学习算法向各种需要大量使用图像分析的商业领域的过渡，如医学成像(检查大脑扫描，x 射线，乳房 x 线扫描)和自动驾驶汽车(计算机视觉)。图像分析很容易扩展到视频，因为这只是每秒钟多个图像帧的快速连续，尽管这需要更多的计算能力。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="c1a5" class="nm nn iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated"><strong class="ak">迁移学习</strong></h1><p id="4fba" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">迁移学习是一个重要的话题，它绝对值得有一篇专门的文章。然而，现在，我将概述迁移学习背后的基本思想，以便读者能够做更多的研究，如果他们感兴趣的话。</p><p id="e33c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">如何在 CPU 上做一个可以在几个小时(几分钟)内训练好的图像分类器？</p><p id="7b30" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">通常情况下，图像分类模型可能需要几个小时、几天甚至几周的时间来训练，尤其是在非常大的网络和数据集上训练时。然而，我们知道谷歌和微软等公司有专门的数据科学家团队，他们花了数年时间开发用于图像分类的特殊网络——为什么不直接使用这些网络作为自己图像分类项目的起点呢？</p><p id="cf89" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这是迁移学习背后的思想，使用预先训练的模型，即具有已知权重的模型，以便将它们应用于不同的机器学习问题。显然，仅仅单纯地转移模型是没有帮助的，您仍然必须根据您的新数据来训练网络，但是通常会冻结以前的层的权重，因为这些是更一般化的特征，在训练期间可能不会改变。与随机初始化网络(Keras 中训练网络时的默认情况)相反，您可以将此视为生成预初始化网络的一种智能方式。</p><p id="e536" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">通常，迁移学习中使用的学习速率比典型的网络训练中使用的学习速率要小，因为我们本质上是在调整网络。如果使用大的学习率，并且网络中的早期层没有冻结，迁移学习可能不会提供任何好处。通常，在迁移学习问题中只训练最后一层或最后几层。</p><p id="8c27" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">迁移学习最适用于相当一般的问题，并且在线上有免费的网络可用(例如图像分析)，当用户可用的数据集相对较小，不足以训练神经网络时，这是一个相当常见的问题。</p><p id="d4c4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">总结一下主要思想:网络的早期层学习低级特征，可以通过在后期和完全连接的层改变权重来适应新的域。</p><p id="a701" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这方面的一个例子是使用 ImageNet 训练任何复杂的大型网络，然后在几千个热狗图像上重新训练网络，你就会得到。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qw"><img src="../Images/ffe52416338135f041d4ceaa88201ea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIv8ZRUnb14UdnSTvzCSJg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Hotdog or NotHotDog: <a class="ae lw" href="https://youtu.be/ACmydtFDTGs" rel="noopener ugc nofollow" target="_blank">https://youtu.be/ACmydtFDTGs</a> (offensive language and tropes alert)</figcaption></figure><p id="4932" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">迁移学习涉及的步骤如下:</p><ol class=""><li id="2f24" class="li lj iq kl b km kn kp kq lf lk lg ll lh lm le ou lo lp lq bi translated">获取现有网络权重</li><li id="7b81" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ou lo lp lq bi translated">解冻“头部”完全连接的层，并在您的新图像上训练</li><li id="68df" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ou lo lp lq bi translated">解冻最新的卷积层，并以非常低的学习速率从先前训练的权重开始训练。这将改变最新的层卷积权重，而不会触发大的梯度更新，如果我们没有进行#2，就会发生这种情况。</li></ol><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi qx"><img src="../Images/f9d2f0f598b61e238c2b0738c26f0512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oobq5HKOT6hf_CuF42RvfQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">(Left) Typical imported network to be utilized for transfer learning, (right) newly tuned network with the first four convolutional blocks frozen.</figcaption></figure><p id="0c4d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">要了解更多信息，我推荐几篇其他的媒体文章:</p><div class="mn mo gp gr mp mq"><a href="https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3" rel="noopener follow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">HBO 的硅谷如何用移动 TensorFlow、Keras &amp; React Native 打造“不是热狗”</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">我们如何打败最先进的技术来建立一个现实生活中的人工智能应用。</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">medium.com</p></div></div><div class="mz l"><div class="qy l nb nc nd mz ne mh mq"/></div></div></a></div></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="3be5" class="nm nn iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated"><strong class="ak">最终意见</strong></h1><p id="e3ae" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">恭喜你坚持到这篇文章的结尾！这是一篇涉及深度学习多个方面的长文。读者现在应该已经做好了充分的准备，可以开始深入研究卷积学习和计算机视觉文献。我鼓励读者对我在这里讨论的主题做更多的个人研究，这样他们可以加深他们的知识。</p><p id="8333" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我在下一节中添加了一些进一步阅读的链接，以及一些我在本文中借用图片的研究文章的参考资料。</p><p id="294a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">感谢阅读，深度学习快乐！</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="d989" class="nm nn iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated"><strong class="ak">延伸阅读</strong></h1><ul class=""><li id="cf15" class="li lj iq kl b km oe kp of lf qz lg ra lh rb le ln lo lp lq bi translated">MobileNetV2(【https://arxiv.org/abs/1801.04381】T4)</li><li id="4320" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">盗梦空间-Resnet，v1 和 v2(<a class="ae lw" href="https://arxiv.org/abs/1602.07261" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1602.07261</a>)</li><li id="30d9" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">广网(<a class="ae lw" href="https://arxiv.org/abs/1605.07146" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1605.07146</a>)</li><li id="8a3d" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">例外(<a class="ae lw" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1610.02357</a>)</li><li id="79b9" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">雷斯 next(<a class="ae lw" href="https://arxiv.org/pdf/1611.05431" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.05431</a>)</li><li id="5dc0" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">沙狐球，v1 和 v2(<a class="ae lw" href="https://arxiv.org/abs/1707.01083" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1707.01083</a>)</li><li id="b4c8" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">挤压和激励网(<a class="ae lw" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1709.01507</a>)</li><li id="89c0" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">原装 DenseNet 纸(<a class="ae lw" href="https://arxiv.org/pdf/1608.06993v3.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1608.06993v3.pdf</a>)</li><li id="09e6" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">DenseNet 语义分割(<a class="ae lw" href="https://arxiv.org/pdf/1611.09326v2.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.09326v2.pdf</a>)</li><li id="267a" class="li lj iq kl b km lr kp ls lf lt lg lu lh lv le ln lo lp lq bi translated">用于光流的 DenseNet(【https://arxiv.org/pdf/1707.06316v1.pdf】T2</li></ul></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="e686" class="nm nn iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated"><strong class="ak">参考文献</strong></h1><p id="d707" class="pw-post-body-paragraph ki kj iq kl b km oe jr ko kp of ju kr lf og ku kv lg oh ky kz lh oi lc ld le ij bi translated">Yann LeCun、Leon Bottou、Yoshua Bengio 和 Patrick Haffner，“基于梯度的学习在文档识别中的应用”，IEEE 会议录，第 86 卷，第 11 期，第 2278-2324 页，1998 年。</p><p id="650e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E Hinton，“深度卷积神经网络的图像网络分类”，载于《神经信息处理系统进展》，第 1097–1105 页，2012 年</p><p id="1e92" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">卡伦·西蒙扬和安德鲁·齐泽曼，“用于大规模图像识别的深度卷积网络”，2014 年。</p><p id="80e8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">、陈强和严水成，“网络中的网络”，2013 年。</p><p id="a13e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">Christian Szegedy、、Jia、Pierre Sermanet、Scott Reed、Dragomir Anguelov、Dumitru Erhan、Vincent Vanhoucke 和 Andrew Rabinovich，“用卷积更深入地研究”，载于 2015 年 IEEE 计算机视觉和模式识别会议论文集，第 1–9 页。</p><p id="26c8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">施洛夫、弗洛里安、德米特里·卡列尼琴科和詹姆斯·菲尔宾。" Facenet:人脸识别和聚类的统一嵌入."IEEE 计算机视觉和模式识别会议论文集，第 815-823 页。2015</p><p id="aaf9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">Long，j .，Shelhamer，e .，和 Darrell，T. (2014 年)。语义分割的全卷积网络。从 http://arxiv.org/abs/1411.4038v1<a class="ae lw" href="http://arxiv.org/abs/1411.4038v1" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="a86b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">Chen，l-c .，Papandreou，g .，Kokkinos，I .，Murphy，k .，&amp; Yuille，A. L. (2014 年)。基于深度卷积网和全连通条件随机场的语义图像分割。<em class="kk"> Iclr </em>，1–14。从 http://arxiv.org/abs/1412.7062<a class="ae lw" href="http://arxiv.org/abs/1412.7062" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="25e7" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">于，冯，科尔敦，伏(2016)。基于扩张卷积的多尺度上下文聚合。<em class="kk"> Iclr </em>，1–9。<a class="ae lw" href="http://doi.org/10.16373/j.cnki.ahr.150049" rel="noopener ugc nofollow" target="_blank">http://doi.org/10.16373/j.cnki.ahr.150049</a></p><p id="b7c4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">Oord，A. van den，Dieleman，s .，Zen，h .，Simonyan，k .，Vinyals，o .，Graves，a .，… Kavukcuoglu，K. (2016)。WaveNet:原始音频的生成模型，1–15。从 http://arxiv.org/abs/1609.03499<a class="ae lw" href="http://arxiv.org/abs/1609.03499" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="9f4a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">Kalchbrenner，l . Espeholt，k . Simon Yan，Oord，A. van den，Graves，a .，&amp; Kavukcuoglu，K. (2016 年)。线性时间内的神经机器翻译。<em class="kk"> Arxiv </em>，1–11。从 http://arxiv.org/abs/1610.10099 取回</p></div></div>    
</body>
</html>