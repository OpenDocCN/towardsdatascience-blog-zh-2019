<html>
<head>
<title>Review: FCIS — Winner in 2016 COCO Segmentation (Instance Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:FCIS——2016 年 COCO 细分(实例细分)冠军</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2?source=collection_archive---------19-----------------------#2019-01-15">https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2?source=collection_archive---------19-----------------------#2019-01-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="17f3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">完全卷积的实例感知语义分段，具有位置敏感的内部/外部得分图</h2></div><p id="5f4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事中，<strong class="kh ir">清华大学</strong>和<strong class="kh ir">微软亚洲研究院</strong>的<strong class="kh ir"> FCIS(全卷积实例感知语义分割)</strong>进行了回顾。这是第一个完全卷积的端到端实例分段解决方案<strong class="kh ir">。通过引入<strong class="kh ir">位置敏感的内部/外部得分图</strong>，卷积表示被检测和分割子任务完全共享。获得了高精度和高效率。</strong></p><p id="9703" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最终，FCIS 在 2016 年 COCO 分段挑战赛中获得<strong class="kh ir">第一名，准确率相对高出第二名 12%。</strong>它也在那一刻<strong class="kh ir">在 2016 COCO 检测排行榜上排名第二</strong>。它最初被命名为<strong class="kh ir">翻译感知的完全卷积实例分割(TA-FCN) </strong>。</p><p id="2f43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">在 2015 年 COCO 分割挑战赛中获得第一名的 MNC </a>，每张图像花费 1.4 秒，其中 80%的时间都花在最后的每个 ROI 步骤上。<strong class="kh ir"> FCIS 使用</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet-101</strong></a><strong class="kh ir">(英伟达 K40)每张图像只需 0.24s，比</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"><strong class="kh ir">MNC</strong></a><strong class="kh ir">快很多。</strong>你可能认为推理时间仍然很慢，但是，COCO 分割挑战迫切需要最先进的地图。因此，FCIS 在减少推断时间的同时进一步提高地图并获得第一名已经很令人惊讶了。最终发表在<strong class="kh ir"> 2017 CVPR </strong>超过<strong class="kh ir"> 100 次引用</strong>。(<a class="ll lm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----ee2d61f465e2--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="c9ac" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">概述</h1><ol class=""><li id="188d" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la mt mu mv mw bi translated"><strong class="kh ir">位置敏感内/外得分图</strong></li><li id="9a31" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir"> FCIS 建筑</strong></li><li id="0c94" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">消融研究</strong></li><li id="d42f" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="f2bd" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">1.位置敏感的内/外得分图</h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nc"><img src="../Images/877e6bae994961c4b24b9aabc7cbcf03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R8k0osRaFGpMymS-JaORTw.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">k×k Position-Sensitive Inside/Outside Score Maps with k=3 here</strong></figcaption></figure><ul class=""><li id="5206" class="mm mn iq kh b ki kj kl km ko nt ks nu kw nv la nw mu mv mw bi translated">如果你已经知道了<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>&amp;<a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">instance fcn</a>，你会注意到他们也制作了类似于上面 FCIS 的分数地图。<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> <strong class="kh ir"> R-FCN </strong> </a> <strong class="kh ir">产生用于对象检测的正面敏感得分图</strong>，而<a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> <strong class="kh ir">实例敏感得分图</strong> </a> <strong class="kh ir">产生用于生成分段建议的实例敏感得分图</strong>。如果你已经理解了<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>&amp;<a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">instance fcn</a>，就更容易理解位置敏感的内外得分图。</li><li id="3456" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">在上面的例子中，<strong class="kh ir">每个评分图负责预测对象实例的相对位置。</strong>每个评分图负责捕捉对象实例的相对位置。例如:左上分数图负责捕捉对象实例的左上部分。组装后，可以生成分离的人物面具。</li><li id="d586" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">与<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>&amp;T24】instance fcn 不同的是<strong class="kh ir">有两套比分图</strong>。</li><li id="0904" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir">为了组合 ROI 内部图，在每个正敏感内部得分图上捕捉左上、中上、右上…和右下部分。</strong>类似于阳性敏感外评分图。</li><li id="5da5" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">最后，生成两个得分图。一个是<strong class="kh ir"> ROI 内图</strong>。一个是<strong class="kh ir">图外 ROI</strong>。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ee191ceb8900b73b13fb164d0599a3ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*muNc_BGxWRsd6zck7CU9_A.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Two Pathways</strong></figcaption></figure><ul class=""><li id="4257" class="mm mn iq kh b ki kj kl km ko nt ks nu kw nv la nw mu mv mw bi translated">基于这两幅图，有<strong class="kh ir">两条路径</strong>，一条用于<strong class="kh ir">实例遮罩</strong>，逐像素 softmax 用于分割损失。一种是针对<strong class="kh ir">类别似然度</strong>，通过平均汇集所有像素的似然度来获得检测分数。因此，卷积表示对于检测和分割子任务是完全共享的。</li><li id="274d" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">一些例子:</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ny"><img src="../Images/3057d3a1042591ce3c1928faf1843aa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l2STnbL4qHQbUZjgJjUiwQ.png"/></div></div></figure></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="eb94" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">2.FCIS 建筑</h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nz"><img src="../Images/03433420d3bda96a029c30f8ec5a2096.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*naqQe2GQ2ep7c8oL3i4H1g.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">FCIS (Fully Convolutional Instance-aware Semantic Segmentation) Architecture</strong></figcaption></figure><ul class=""><li id="7c3e" class="mm mn iq kh b ki kj kl km ko nt ks nu kw nv la nw mu mv mw bi translated"><strong class="kh ir"> ImageNet 预训练的</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet-101</strong></a><strong class="kh ir">作为主干</strong>，用 h <strong class="kh ir"> ole 算法</strong>(<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">DeepLab</a>/<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a>)将 conv5 的第一个块的步距从 2 增加到 1。因此，有效特征步距增加到 16。(即<strong class="kh ir">增加输出特征图尺寸。</strong>)</li><li id="a265" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">对于<strong class="kh ir">区域提议网络(RPN) </strong>，为了与<a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a>进行公平的比较，RPN 被<strong class="kh ir">添加到 conv4 层之上，这与</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"><strong class="kh ir">MNC</strong></a><strong class="kh ir">的方式相同。</strong></li><li id="4fee" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">从 conv5 特征图中，使用 1×1 卷积生成<strong class="kh ir">2<em class="oa">k</em>×(<em class="oa">C</em>+1)分数图</strong>。(<em class="oa"> k </em> =7 最后，<em class="oa"> C </em>类加 1 背景)</li><li id="52c0" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir"> ROI 汇集</strong>在这些评分图上执行。</li><li id="cb5c" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir">对于每个 ROI </strong>，获得 2 个(<em class="oa"> C </em> +1)图。<strong class="kh ir">一个(<em class="oa"> C </em> +1)是图内 ROI。一个(<em class="oa"> C </em> +1)是图外 ROI。</strong>然后我们可以计算分割损失和检测分数。</li><li id="6433" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">和<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>一样，还有一个<strong class="kh ir">兄弟 1×1 卷积用于包围盒(bbox)回归</strong>。</li><li id="5a54" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">在推断期间，RPN 生成 300 个具有最高分数的 ROI。然后，它们通过 bbox 回归分支，产生另外 300 个 ROI。</li><li id="3e1b" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">IoU 阈值 0.3 的非最大抑制(NMS)用于滤除高度重叠的 ROI。在剩余 ROI 的池中，对于每个 ROI，我们获得其分类分数和所有类别的前景遮罩(以概率计)。</li><li id="2f00" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">在训练期间，如果 IoU 与最接近的地面实况大于 0.5，则 ROI 为正。有<strong class="kh ir"> 3 个损失项</strong> : <strong class="kh ir">一个超过<em class="oa"> C </em> +1 类别</strong>，<strong class="kh ir">一个仅地面实况类别的 softmax 分割损失</strong>，以及<strong class="kh ir">一个 bbox 回归损失</strong>。后两者仅对正 ROI 有效。</li><li id="8d10" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">因为每 ROI 计算仅涉及 k 细胞分割、分数图复制、softmax、max 和平均池。它既简单又快捷。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="0976" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">3.<strong class="ak">消融研究</strong></h1><h2 id="2fd3" class="ob lv iq bd lw oc od dn ma oe of dp me ko og oh mg ks oi oj mi kw ok ol mk om bi translated">3.1.<strong class="ak"> FCIS 变种</strong></h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/86ffd03cea7ca01690f7b7b844443031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*o1k0BuJN3BeY2xz673B0Mw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">FCIS Variants on PASCAL VOC 2012 Val Set</strong></figcaption></figure><ul class=""><li id="87fa" class="mm mn iq kh b ki kj kl km ko nt ks nu kw nv la nw mu mv mw bi translated"><strong class="kh ir">幼稚 MNC </strong> : <a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a>，59.1% mAP@0.5。</li><li id="6900" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir">inst fcn</strong><strong class="kh ir">+R-FCN</strong>:使用<a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> InstanceFCN </a>进行分段提议，使用<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>预测对象类别并回归包围盒，62.7% mAP@0.5。</li><li id="74be" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir"> FCIS(平移不变量)</strong>:即<em class="oa"> k </em> =1 的 FCIS，52.5% mAP@0.5，可见位置敏感的内外得分图很重要。</li><li id="22b3" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir"> FCIS(单独评分图)</strong>:第一套<em class="oa"> k </em>评分图仅用于分割，第二套仅用于分类，63.9% mAP@0.5，说明需要联合制定。</li><li id="a461" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir"> FCIS </strong> : 65.7% mAP@0.5，联合制定有效。</li></ul><h2 id="5a0b" class="ob lv iq bd lw oc od dn ma oe of dp me ko og oh mg ks oi oj mi kw ok ol mk om bi translated">3.2.不同深度的 ResNet</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/4f5dc94db130686c301aef8f8bb3a483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*QbQEXbjpR6CiQLwuw9Jr9g.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">ResNet with Different Depths on COCO test-dev set</strong></figcaption></figure><ul class=""><li id="6906" class="mm mn iq kh b ki kj kl km ko nt ks nu kw nv la nw mu mv mw bi translated">虽然<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-152 </a>具有更高的 mAP，但是<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-101 </a>用于与最先进的方法进行比较。</li></ul><h2 id="a79d" class="ob lv iq bd lw oc od dn ma oe of dp me ko og oh mg ks oi oj mi kw ok ol mk om bi translated">3.3.OHEM(在线硬示例挖掘)</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi op"><img src="../Images/b871f9d41ca6d517a97936d3951626e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ia_C9o_eIlkd8CWZ5FCsjQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Comparison with </strong><a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"><strong class="bd ns">MNC</strong></a><strong class="bd ns"> on COCO test-dev set with/without OHEM</strong></figcaption></figure><ul class=""><li id="0aa0" class="mm mn iq kh b ki kj kl km ko nt ks nu kw nv la nw mu mv mw bi translated">在<em class="oa"> N </em>建议中，只有具有最高损失的顶部<em class="oa">B</em>ROI 用于反向传播。OHEM 将 MNC 的训练时间从 2.05 秒大幅提高到 3.22 秒，OHEM 对 FCIS 的训练时间影响不大，但改善了地图。这是因为每 ROI 计算对于 FCIS 来说是便宜的。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="dfe5" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">4.与最先进方法的比较</h1><h2 id="d6da" class="ob lv iq bd lw oc od dn ma oe of dp me ko og oh mg ks oi oj mi kw ok ol mk om bi translated">4.1.COCO 细分挑战</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/91d23cf409b10b0b5b4e30132e71a936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*Iw38ZgT1Bw5X-I9QsOoGbw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">COCO Segmentation Challenge Entry</strong></figcaption></figure><ul class=""><li id="04d1" class="mm mn iq kh b ki kj kl km ko nt ks nu kw nv la nw mu mv mw bi translated"><strong class="kh ir"> FAIRCNN </strong>:其实是 2015 年第二名<a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413"> MultiPathNet </a>的队名。</li><li id="1c09" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir">mnc++</strong>:<a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">2015 年获得第一名的 MNC </a>提交结果。</li><li id="024a" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir">G-RMI</strong>:2016 年第 2 名，由谷歌研究和机器智能团队。(该方法不是在对象检测挑战中获胜的方法。)</li><li id="1ed4" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir"> FCIS 基线</strong>:已经比<a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径</a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a>好了。</li><li id="5c1d" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir">+多尺度测试</strong>:使用测试图像金字塔，短边为{480，576，688，864，1200，1400}像素进行测试。</li><li id="8664" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir">+水平翻转</strong>:水平翻转图像，再次测试，然后平均结果。</li><li id="df03" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir">+多尺度训练</strong>:应用与多尺度推理相同尺度的多尺度训练。</li><li id="5c31" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated"><strong class="kh ir">+集合</strong>:集合 6 个网络。</li><li id="500c" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nw mu mv mw bi translated">最后，具有上述技巧的 FCIS 比 G-RMI 高 3.8%(相对地高 11%)。</li></ul><h2 id="254d" class="ob lv iq bd lw oc od dn ma oe of dp me ko og oh mg ks oi oj mi kw ok ol mk om bi translated">4.2.COCO 检测排行榜</h2><ul class=""><li id="f245" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nw mu mv mw bi translated">通过将实例遮罩的包围盒作为检测到的包围盒，在 COCO 测试开发集上实现了 39.7%的物体检测准确率，在当时的 COCO 物体检测排行榜上名列第二。</li></ul><h2 id="3952" class="ob lv iq bd lw oc od dn ma oe of dp me ko og oh mg ks oi oj mi kw ok ol mk om bi translated">4.3.定性结果</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi or"><img src="../Images/e004c4e45c9401f8387770c620371639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tpVj3JXh0Mwd2A92lQxttg.png"/></div></div></figure><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi os"><img src="../Images/9fb1436edd061fb3b644a53eab32809f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xaUY6WFx65z9wY0xHpbqyQ.png"/></div></div></figure></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h2 id="ffa9" class="ob lv iq bd lw oc od dn ma oe of dp me ko og oh mg ks oi oj mi kw ok ol mk om bi translated">参考</h2><p id="1875" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko ot kq kr ks ou ku kv kw ov ky kz la ij bi translated">【2017 CVPR】【FCIS】<br/><a class="ae lk" href="https://arxiv.org/abs/1611.07709" rel="noopener ugc nofollow" target="_blank">全卷积实例感知语义分割</a></p><h2 id="27c3" class="ob lv iq bd lw oc od dn ma oe of dp me ko og oh mg ks oi oj mi kw ok ol mk om bi translated">我的相关评论</h2><p id="4da9" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko ot kq kr ks ou ku kv kw ov ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a></p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a>】<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a>]</p><p id="fc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong></p><p id="3134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割<br/> </strong> [ <a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">清晰度掩码</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ]</p></div></div>    
</body>
</html>