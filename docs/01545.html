<html>
<head>
<title>Sigmoid Neuron Learning Algorithm Explained With Math</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用数学解释的 Sigmoid 神经元学习算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sigmoid-neuron-learning-algorithm-explained-with-math-eb9280e53f07?source=collection_archive---------5-----------------------#2019-03-12">https://towardsdatascience.com/sigmoid-neuron-learning-algorithm-explained-with-math-eb9280e53f07?source=collection_archive---------5-----------------------#2019-03-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/c29a972877860bee8902600bf51ce4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7u9jwcLfPclhczQg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@antoine1003?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Antoine Dautry</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="93fe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是讨论 sigmoid 神经元的工作及其学习算法的两部分系列的第二部分:</p><p id="f4ce" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1 | <a class="ae kf" rel="noopener" target="_blank" href="/sigmoid-neuron-deep-neural-networks-a4cd35b629d7"> Sigmoid 神经元——深度神经网络的构建模块</a></p><p id="b7c5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2 |用数学解释的 Sigmoid 神经元学习算法(当前故事)</p><p id="5d4a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本帖中，我们将详细讨论 sigmoid 神经元学习算法背后的数学直觉。</p><p id="efd6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">引用注:本文内容和结构基于四分之一实验室的深度学习讲座——</em><a class="ae kf" href="https://padhai.onefourthlabs.in" rel="noopener ugc nofollow" target="_blank"><em class="le">pad hai</em></a><em class="le">。</em></p><h1 id="cfd1" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">乙状结肠神经元概述</h1><p id="1c0f" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">sigmoid 神经元类似于感知器神经元，对于每个输入<code class="fe mi mj mk ml b">xi</code>，它都具有与该输入相关联的权重<code class="fe mi mj mk ml b">wi</code>。权重表明决策过程中输入的重要性。与感知器模型不同，sigmoid 的输出不是 0 或 1，而是 0-1 之间的真实值，可以解释为概率。最常用的 sigmoid 函数是逻辑函数，它具有“<strong class="ki iu"> S </strong>形曲线的特征。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="ab gu cl mq"><img src="../Images/55096c9f08a702529cac584e0deafb49.png" data-original-src="https://miro.medium.com/v2/format:webp/1*N7dfPwbiXC-Kk4TCbfRerA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Sigmoid Neuron Representation</figcaption></figure><h1 id="069e" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">学习算法</h1><p id="f30f" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">学习算法的目标是确定参数(<strong class="ki iu"> w </strong>和<strong class="ki iu"> b </strong>)的最佳可能值，使得模型的总损失(平方误差损失)尽可能最小。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="ab gu cl mq"><img src="../Images/506613cc4f1547ad02a8fca85cecef1e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*fBxEzbzP1KkqR7PTexJZdw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Learning Algorithm</figcaption></figure><p id="78ee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们随机初始化<strong class="ki iu"> w </strong>和<strong class="ki iu"> b </strong>。然后，我们迭代数据中的所有观察值，对于每个观察值，使用 sigmoid 函数找到相应的预测结果，并计算平方误差损失。基于损失值，我们将更新权重，使得模型在新参数下的总损失将比模型的当前损失小<strong class="ki iu">。</strong></p><h1 id="3c52" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">学习算法:数学部分</h1><p id="51ad" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">我们看到权重是如何根据损失值更新的。在本节中，我们将了解为什么这个特定的更新规则会减少模型的丢失。为了理解为什么更新工作规则有效，我们需要理解它背后的数学原理。</p><p id="1160" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从学习算法的数学部分开始。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/07507ad8dfd2340a02b2df623a736654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*H9iIYOrRzFXE2WYKw-r4xg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="https://memegenerator.net/instance/81152522/excited-baby-face-lets-do-some-math" rel="noopener ugc nofollow" target="_blank">Cute No?</a></figcaption></figure><p id="e9f5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 sigmoid 神经元函数中，我们有两个参数<strong class="ki iu"> w </strong>和<strong class="ki iu">b。</strong>I 将以向量θ的形式表示这些参数，θ是属于 R 的参数的向量。目标是找到使损失函数最小化的θ的最佳值。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ddad6c8236314cc61de784c099ba2edf.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*ubZqPCHWuj6qvnLjfMnStQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Vector Notation of Parameters</figcaption></figure><p id="23ea" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们达到目标的方法是用一些小的随机值来更新θ，即δθ<strong class="ki iu">δ</strong>θ，它是<strong class="ki iu"> w </strong>的变化和<strong class="ki iu"> b </strong>的变化的集合。<strong class="ki iu">δ</strong>θ也是一个属于 R 的向量，我们来看看θ和<strong class="ki iu">δ</strong>θ的几何表示，</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/d0cb3346045a9bca75f39d4b5a1ba7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*OpKV9yZdOQWPs5I38MOuMA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Geometric Representation</figcaption></figure><p id="a85a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有一个向量θ，我们要给它加上另一个向量<strong class="ki iu">δ</strong>θ。从<a class="ae kf" href="https://www.mathstopia.net/vectors/parallelogram-law-vector-addition" rel="noopener ugc nofollow" target="_blank">向量平行四边形定律</a>我们会得到我们的合成向量θnew，它无非是平行四边形的对角线。从几何表示中，我们可以看到，在θ和θnew 之间，θ的值有很大的变化。与其在学习θ时迈大步，不如保守一点，只往同一个方向移动很小的量。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/3947a0cef5a5e4625657b6420b9eaa38.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*3kHA5U-Ym-kl-iDgmaT_qg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Geometric Representation with Learning Rate</figcaption></figure><p id="e951" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">记住<strong class="ki iu">δ</strong>θ是一个向量，为了在同一个方向上迈小步，我需要让<strong class="ki iu">δ</strong>θ有一个较小的幅度。为了得到一个小的量级，我将把<strong class="ki iu">δ</strong>θ乘以学习率(一个很小的值)。现在，合成矢量是红色矢量，表示θnew。新的θ将是向量θ从初始位置的移动。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/bd8966aa9011ced3d7b15804a9526d0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*3YJccQFBLfPRA9JODIXMhw.png"/></div></figure><h1 id="254a" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">使用泰勒级数</h1><p id="1a55" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">但是我们如何决定<strong class="ki iu">δ</strong>θ的值以及使用什么样的<strong class="ki iu">δ</strong>θ才是正确的呢？。我们如何以主要方式获得正确的<strong class="ki iu">δ</strong>θ，使得新θ的损耗(是<strong class="ki iu"> w </strong>和<strong class="ki iu"> b </strong>的函数)应该小于旧θ的损耗。这个问题的答案来自泰勒级数。</p><p id="4df4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">泰勒级数告诉我们的是，如果我们有一个函数<strong class="ki iu"> f </strong>并且我们知道该函数在特定点<strong class="ki iu"> x </strong>的值，那么函数<strong class="ki iu"> f </strong>在一个非常接近<strong class="ki iu"> x </strong>的新点的值由下面的公式给出:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/fe09d5660365c61919f4580c6fad824e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*JetPdXEGhOy9ao4E76te9Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Taylor Series General Representation</figcaption></figure><p id="bed5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">靠近<strong class="ki iu"> x </strong>的小步长后的函数值等于<strong class="ki iu"> x </strong>处的函数值和其他一些量(出现在红框中)。如果我们观察红框中的数量，它的值完全取决于δ<strong class="ki iu">x</strong>。如果我能够找到 delta <strong class="ki iu"> x </strong>使得红框中表示的数量为负，那么我将知道新<strong class="ki iu"> x </strong>处的新损失小于旧<strong class="ki iu"> x </strong>处的损失。</p><p id="5af9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用 sigmoid 神经元参数和损失函数表示泰勒级数。为便于记记，设<strong class="ki iu">δ</strong>θ= u，则我们有，</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mx"><img src="../Images/d48d2b96c31d359f69f45a1421e6a465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cJ29jyXXvnX_DCQSZNupsw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Replaced x with Parameter theta</figcaption></figure><p id="9e9e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将其与一般的泰勒级数方程进行比较，新θ的损耗等于旧θ的损耗和其他一些量(显示在红框中)。盒子中存在的量的值取决于变化向量 uᵀ.所以我们需要找到变化向量 uᵀ，使得红框中的量变成负数。如果为负，那么我们将知道θnew 处的损耗小于θold 处的损耗。</p><p id="8412" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了找到最佳变化向量 uᵀ，为了简单起见，让我们去掉泰勒级数中的一些项。我们知道学习率η非常小，那么η，η等…将非常接近于零。通过使用此逻辑，泰勒级数可以简化为以下形式:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi my"><img src="../Images/bcecfb0c1aa178415ab2ab9d0f2da2ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*wjh7xdCTegRDNMghx93HZQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Truncated Taylor Series</figcaption></figure><p id="2804" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">只有当新θ处的损耗小于旧θ时，变化矢量 uᵀ才是最佳的，</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/956a0dda546de1ea6043a7a0af45435b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g8XQ_v7tW3dD4_W7MT4g4w.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Optimal change vector</figcaption></figure><p id="330c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过比较上述两个方程(变化向量 uᵀ优化方程和截断泰勒级数方程),</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/58aa0f2fdf5497b68aed87f42d0d2e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*-G1S6BKPSpeEJKAzQlHUGA.png"/></div></figure><p id="04a7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决上面的方程，让我们回到一点线性代数，特别是两个向量之间的余弦角。我们知道<em class="le"> cos(θ)的范围从-1 到 1。</em></p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nb"><img src="../Images/124160ed7a633a814963ca40581d1d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KFc6QjTHK6kzTkm6wk82fQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Cosine Angle between Vectors</figcaption></figure><p id="0040" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于余弦角公式的分母是一个数值，并且它总是正的，我们可以将余弦公式乘以<strong class="ki iu"> k </strong>(分母)而不影响不等式的符号。我们希望不等式之间的量(标在蓝框中)为负，这只有在 uᵀ和∇之间的余弦角等于-1 时才有可能。如果余弦角等于-1，那么我们知道向量之间的角度等于 180⁰.</p><p id="3fbf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果角度等于 180⁰，这意味着我们选择的变化向量 uᵀ的方向，应该与梯度向量相反。我们可以将梯度下降规则总结如下:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/6c61ecee1436224efdff2db09616322f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-vukw3GlYh-1JUbcaFvv6Q.png"/></div></div></figure><p id="30d3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，我们可以如下编写参数更新规则，</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/b4db1e627d79c7612124dc4bf64be993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s797UuAyGsH7mXItzVrY8g.png"/></div></div></figure></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="4043" class="lf lg it bd lh li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc bi translated">计算偏导数</h1><p id="8bf7" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">到目前为止，在前面的章节中，我们已经看到了如何使用泰勒级数和向量之间的余弦角来达到参数更新规则。但是我们如何计算对 w 和 b 的偏导数呢？。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3ee9fd59fadb24b2a8f0e6df4d7f9686.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*Q2h7CDLl-eUF0UEUJO93Mw.png"/></div></figure><p id="a23c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了简单起见，我们假设只有一个点适合于乙状结肠神经元。所以一个点的损失函数可以写成，</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/9f091aa2bfd4fbc36cbbf76750d1b31b.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*KSPtEKlx4cozaX6xji68KQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Loss function for one point</figcaption></figure><p id="6315" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要计算<strong class="ki iu"> w </strong>和<strong class="ki iu"> b </strong>对损失函数的偏导数。让我们推导这些导数，</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/5547010f7941f2486459d08a091fcc82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdvBBgc91XG656lm1OkX0g.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Derivation for partial derivatives of w</figcaption></figure><p id="03a3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们将偏导数推到括号内，接下来，我们有 y，它是常数，它的偏导数为零。在 f(x)的地方，我们用逻辑函数代替，求逻辑函数 w.r .对<strong class="ki iu"> w </strong>的偏导数。上图的右边部分解释了逻辑函数的偏导数。上述最终表达式(用红色标记)仅对一次观察有效，对于两次观察，表达式变为:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e9cafdf925fdc20dbf434933db2bcbaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*OESgdx1KbuColP0zO3Q-3g.png"/></div></figure><p id="99ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，我们可以计算<strong class="ki iu"> b </strong>对损失的偏导数，</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b94fb59549fc54f70b3294b1ca5bbd81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*G9_JKJAUt4Rly9B-wzx0aA.png"/></div></figure><h1 id="7366" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">梯度下降规则</h1><p id="6126" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">现在我们有了使用梯度下降实现参数更新规则所需的一切。让我们举一个例子，看看我们如何使用我们刚刚导出的参数更新规则来实现梯度下降，以最小化模型的损失。</p><p id="9c2b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我已经获取了一些玩具数据，并将参数初始化为<strong class="ki iu"> w </strong> = 0，<strong class="ki iu"> b </strong> = -8，并将学习率设置为 1.0，然后迭代 1000 个时期的所有观察值，并计算不同值的<strong class="ki iu"> w </strong> (-2 到 6)和<strong class="ki iu"> b </strong> (-10 到 2) <strong class="ki iu">的损失。O </strong>一旦我获得了<strong class="ki iu"> w </strong>和<strong class="ki iu"> b </strong>所有可能组合的损失值，我就能够生成下面的 3D 等高线图来显示损失值的变化。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/dfde18e8f16b0c82d40d5849286c7a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*ZNEdwQLPGBAOZcvLPBa_Wg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Loss Value for different w and b</figcaption></figure><figure class="mm mn mo mp gt ju"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="d491" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以上代码片段显示了在 2D 玩具数据的情况下，参数<strong class="ki iu"> w </strong>和<strong class="ki iu"> b </strong>的梯度下降更新规则的实现。我们可以随机初始化参数值，但在这种情况下，我选择这些特定值只是为了更好地说明。</p><p id="c4bf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图表中较深的红色阴影表示损失值较高的区域，而山谷中较深的蓝色阴影表示损失的最低点。下面的动画显示了固定迭代的梯度下降规则，底部的点表示<strong class="ki iu"> w </strong> &amp; <strong class="ki iu"> b </strong>的不同组合，轮廓上的点表示相应参数值的损失值。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/47c1639aa72eec90250720f0b51618d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/1*EXwTzCUf60Qx41oWsTKrYg.gif"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Animation of loss variation</figcaption></figure><p id="2043" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经成功地推导出梯度下降更新规则，该规则保证找到对于<strong class="ki iu"> w </strong>和<strong class="ki iu"> b </strong>的最佳参数值，其中只要我们在与梯度相反的方向上移动，函数的损失最小。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ny"><img src="../Images/ec9d786704d6b718a21d30e8e9839bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rPBOoFGddjPOQlgq"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@jeshoots?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">JESHOOTS.COM</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="9a89" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">继续学习</h1><p id="6cea" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">如果你有兴趣了解更多关于人工神经网络的知识，请查看来自<a class="ae kf" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank"> Starttechacademy </a>的 Abhishek 和 Pukhraj 的<a class="ae kf" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>。还有，课程是用最新版本的 Tensorflow 2.0 (Keras 后端)讲授的。他们还有一个非常好的包，关于 Python 和 R 语言的<a class="ae kf" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">机器学习(基础+高级)</a>。</p><h1 id="854a" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">结论</h1><p id="c6cc" class="pw-post-body-paragraph kg kh it ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">在这篇文章中，我们简要地看了一下乙状结肠神经元。然后，我们看到了学习算法的 sigmoid neuron 数学自由版本，然后我们继续使用泰勒级数、线性代数和偏微分来详细了解学习算法背后的数学直觉。最后，我们证明了我们使用梯度下降导出的更新规则保证通过使用玩具数据找到最佳参数。</p><p id="0fb9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个有点冗长&amp;数学很重的帖子，即使你第一次阅读没有完全理解，也要再浏览一遍。如果你有任何问题或建议，请在回复部分发表。</p><p id="5d0f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">推荐阅读</em></p><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/sigmoid-neuron-deep-neural-networks-a4cd35b629d7"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">Sigmoid 神经元—深度神经网络</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">深层神经网络的构建模块被称为乙状结肠神经元。</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq jz oc"/></div></div></a></div><p id="1013" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在以后的文章中，我们将讨论如何用 python 实现 sigmoid 神经元模型，还将讨论其他一些非线性函数，如 RELU、ELU。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="fc00" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Niranjan Kumar 在汇丰银行数据分析部门实习。他对深度学习和人工智能充满热情。他是<a class="or os ep" href="https://medium.com/u/504c7870fdb6?source=post_page-----eb9280e53f07--------------------------------" rel="noopener" target="_blank"> Medium </a>在<a class="ae kf" href="https://medium.com/tag/artificial-intelligence/top-writers" rel="noopener">人工智能</a>的顶尖作家之一。在<a class="ae kf" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系，或者在<a class="ae kf" href="https://twitter.com/Nkumar_283" rel="noopener ugc nofollow" target="_blank"> twitter </a>上关注我，了解关于深度学习和人工智能的最新文章。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="b260" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">免责声明</strong> —这篇文章中可能有一些相关资源的附属链接。你可以以尽可能低的价格购买捆绑包。如果你购买这门课程，我会收到一小笔佣金。</p></div></div>    
</body>
</html>