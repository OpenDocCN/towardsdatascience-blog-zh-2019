<html>
<head>
<title>Text Summarization using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习的文本摘要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-summarization-using-deep-learning-6e379ed2e89c?source=collection_archive---------6-----------------------#2019-03-27">https://towardsdatascience.com/text-summarization-using-deep-learning-6e379ed2e89c?source=collection_archive---------6-----------------------#2019-03-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9ed2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用数据做很酷的事情！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/0d8fae43903a2605d3a1182363747949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*VQmaXe3vWq-fL_t7aDmtfg.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Text Summarization</figcaption></figure><h1 id="2e12" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">介绍</h1><p id="a809" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">随着互联网的兴起，我们现在可以随时获取信息。我们从许多来源被它轰炸——新闻、社交媒体、办公室邮件等等。要是有人能为我们总结出最重要的信息就好了！深度学习正在实现。通过序列到序列模型的最新进展，我们现在可以开发良好的文本摘要模型。</p><p id="e94d" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">文本摘要有两种类型:</p><p id="8ef9" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">1.摘录摘要—这种方法从源文本中选择段落，然后对其进行排列以形成摘要。思考这个问题的一种方式是用荧光笔在重要的部分下划线。主要思想是摘要文本是源文本的子部分。</p><p id="e523" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">2.抽象总结——相反，抽象方法包括理解意图，并用自己的话写总结。我认为这类似于一支笔。</p><p id="efaf" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">自然，抽象概括在这里是更具挑战性的问题。这是机器学习进展缓慢的一个领域。这是一个困难的问题，因为创建抽象的摘要需要很好地掌握主题和自然语言，这对于机器来说都是困难的任务。此外，从历史上看，我们没有一个好的大数据集来解决这个问题。这里的数据集指的是带有摘要的源文本。因为人类需要写摘要，所以获取大量摘要是一个问题，除了一个领域——新闻！</p><p id="6572" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">在呈现新闻文章时，专业作家不断总结信息，如下面 CNN 新闻的片段所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mn"><img src="../Images/276c75370d61479a79c76aca973df2d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BEvzQ6xM_e6zFqMsoVMjig.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">News snippet</figcaption></figure><p id="c6a2" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">商店亮点是为更大的文章创建的摘要。收集来自 CNN 和每日邮报的新闻数据，以创建用于文本摘要的<a class="ae ms" href="https://github.com/abisee/cnn-dailymail" rel="noopener ugc nofollow" target="_blank">CNN/每日邮报数据集</a>，这是用于训练抽象摘要模型的关键数据集。使用这个数据集作为基准，研究人员一直在尝试深度学习模型设计。</p><p id="bc08" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我喜欢的一个这样的模型是 Abigail See 的<a class="ae ms" href="https://arxiv.org/abs/1704.04368" rel="noopener ugc nofollow" target="_blank">指针生成器网络</a>。我想用这个模型来强调深度学习摘要模型的关键组件。</p><p id="6b7d" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">在我们进入模型之前，让我们讨论一下文本摘要的评估指标<a class="ae ms" href="https://en.wikipedia.org/wiki/ROUGE_(metric)" rel="noopener ugc nofollow" target="_blank"> — Rouge Score </a>。Rouge score 突出显示了摘要文本和源文本之间的单词重叠。Rouge 1-测量源和摘要文本之间的单个单词重叠，而 Rouge 2 测量源和摘要之间的双字母重叠。因为 rouge 得分度量只考虑单词重叠而不考虑文本的可读性，所以它不是一个完美的度量，因为具有高 rouge 得分的文本可能是一个糟糕的摘要。</p><h1 id="8d87" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">通过深度学习的文本摘要</h1><p id="fb51" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">做文本摘要的标准方法是注意使用 seq2seq 模型。参见下面来自<a class="ae ms" href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" rel="noopener ugc nofollow" target="_blank">指针生成器博客</a>的模型结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mt"><img src="../Images/f5458914bfc37da88665196fc3a1eb15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0Qb_5atARbmbEeyW4ok7A.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Encoder-Decoder model architecture</figcaption></figure><p id="65ea" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">序列到序列模型有三个主要方面:</p><p id="ae1b" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">1.编码器—从原始文本中提取信息的双向 LSTM 层。这在上面用红色显示。双向 LSTM 一次读取一个单词，因为它是 LSTM，所以它基于当前单词和它之前已经读取的单词来更新它的隐藏状态。</p><p id="4da7" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">2.解码器—单向 LSTM 层，一次生成一个单词的摘要。解码器 LSTM 开始工作，一旦它得到的信号比完整的源文本已经阅读。它使用来自编码器的信息以及之前写入的信息来创建下一个单词的概率分布。解码器在上面以黄色显示，概率分布以绿色显示。</p><p id="af10" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">3.注意力机制——编码器和解码器是这里的构建模块，但历史上，没有注意力的编码器和解码器架构本身并不太成功。如果不注意，解码器的输入是来自编码器的最终隐藏状态，它可以是 256 或 512 维向量，如果我们想象这个小向量不可能包含所有信息，那么它就成为了信息瓶颈。通过注意机制，解码器可以访问编码器中的中间隐藏状态，并使用所有这些信息来决定下一个单词。注意力在上面以蓝色显示。注意力是一个非常棘手的概念，所以如果我在这里的简短描述令人困惑，请不要担心。你可以通过我的博客<a class="ae ms" rel="noopener" target="_blank" href="/nlp-building-a-question-answering-model-ed0529a68c54">在这里</a>阅读更多关于注意力的内容。</p><p id="d0ea" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">正如指针生成器论文所示，上面的体系结构足够好，可以开始使用，但是它创建的摘要有两个问题:</p><p id="e296" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">1.概要有时会不准确地再现事实细节(例如德国队以 3 比 2 击败阿根廷队)。这对于像 2–0 这样的罕见或不在词汇表中的单词尤其常见。</p><p id="1030" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">2.摘要经常重复出现。(例如德国打败德国打败德国…)</p><p id="03be" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">指针生成器模型通过创建一种指针机制来解决这些问题，这种机制允许它在生成文本和从源代码原样复制之间切换。把指针想象成一个介于 0 和 1 之间的概率标量。如果为 1，则模型抽象生成单词，如果为 0，则提取复制单词。</p><p id="1d8e" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">与序列到注意序列系统相比，指针生成器网络有几个优点:</p><ul class=""><li id="6885" class="mu mv it lo b lp mi ls mj lv mw lz mx md my mh mz na nb nc bi translated">指针生成器网络使得从源文本复制单词变得容易。</li><li id="3806" class="mu mv it lo b lp nd ls ne lv nf lz ng md nh mh mz na nb nc bi translated">指针生成器模型甚至能够从源文本中复制词汇以外的单词。这是一个主要的好处，使我们能够处理看不见的单词，同时还允许我们使用更小的词汇(这需要更少的计算和存储空间)。</li><li id="d9d0" class="mu mv it lo b lp nd ls ne lv nf lz ng md nh mh mz na nb nc bi translated">指针生成器模型训练起来更快，需要更少的训练迭代来实现与序列间注意系统相同的性能。</li></ul><h1 id="628e" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">结果</h1><p id="7ed2" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们已经在 Tensorflow 中实现了这个模型，并在 CNN/Daily Mail 数据集上对其进行了训练。该模型获得了 0.38 的 Rouge-1 分数，这是最先进的。我们的观察是，该模型在根据新闻文章(即它所训练的数据)创建摘要方面做得非常好。然而，如果呈现的文本不是新闻，它仍然可以创建良好的摘要，但这些摘要本质上更具提取性。</p><p id="115e" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我希望你喜欢这个博客。要测试指针生成器模型，请在<a class="ae ms" href="https://github.com/abisee/pointer-generator" rel="noopener ugc nofollow" target="_blank">链接</a>处提取它们的代码。或者与我联系，检查我们的这种模式的版本。</p><p id="d526" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我有自己的深度学习咨询公司，喜欢研究有趣的问题。我已经帮助许多初创公司部署了基于人工智能的创新解决方案。在 http://deeplearninganalytics.org/的<a class="ae ms" href="http://deeplearninganalytics.org/" rel="noopener ugc nofollow" target="_blank">入住我们酒店。</a></p><p id="5012" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">你也可以在 https://medium.com/@priya.dwivedi<a class="ae ms" href="https://medium.com/@priya.dwivedi" rel="noopener">的</a>看到我的其他作品</p><p id="78a9" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">如果你有一个我们可以合作的项目，请通过我的网站或 info@deeplearninganalytics.org 联系我</p><h1 id="2f95" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">参考资料:</h1><p id="2320" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><a class="ae ms" href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" rel="noopener ugc nofollow" target="_blank">关于指针生成器模型的博客</a></p><p id="7e9e" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated"><a class="ae ms" href="https://machinelearningmastery.com/prepare-news-articles-text-summarization/" rel="noopener ugc nofollow" target="_blank">下载 CNN 每日邮报数据集</a></p></div></div>    
</body>
</html>