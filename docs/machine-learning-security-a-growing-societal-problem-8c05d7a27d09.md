# 机器学习安全性

> 原文：<https://towardsdatascience.com/machine-learning-security-a-growing-societal-problem-8c05d7a27d09?source=collection_archive---------19----------------------->

## 使用模型来打破模型

![](img/615c1ab922c88c39c9cf3c70c3b7f2bf.png)

*本文原载于 blog.zakjost.com*[](https://blog.zakjost.com/post/model-security/)**。请去那里订阅。**

*随着越来越多的系统在其决策过程中利用 ML 模型，考虑恶意行为者可能如何利用这些模型，以及如何设计针对这些攻击的防御措施将变得越来越重要。这篇文章的目的是分享我最近在这个话题上的一些心得。*

# *ML 无处不在*

*可用数据、处理能力和 ML 空间中的创新的爆炸导致了 ML 的无处不在。鉴于开源框架和数据的激增，构建这些模型实际上非常容易([本教程](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)将在大约 5-10 分钟内将零 ML/编程知识的人带到 6 ML 模型)。此外，云提供商提供 ML 即服务的持续趋势使客户能够构建解决方案，而无需编写代码或理解其工作原理。*

*Alexa 可以使用语音命令代表我们购买。模特识别色情内容，帮助我们的孩子更安全地使用互联网平台。他们在我们的道路上驾驶汽车，保护我们免受骗子和恶意软件的侵害。他们监控我们的信用卡交易和互联网使用情况，寻找可疑的异常情况。*

# *" Alexa，给我的猫买鳄梨色拉酱"*

*ML 的好处是显而易见的——让一个人手动检查每一笔信用卡交易、每一张脸书图片、每一个 YouTube 视频等等是不可能的。风险呢？*

*在无人驾驶汽车导航时，不需要太多的想象力就可以理解 ML 算法出错可能带来的危害。常见的说法往往是，“只要它犯的错误比人类少，那就是净收益”。*

*但是那些恶意的演员试图欺骗模特的情况呢？来自麻省理工学院的学生团体 lab six 3D 打印了一只海龟，它被谷歌的 InceptionV3 图像分类器可靠地归类为“来福枪”，无论相机角度如何[1]。对于语音到文本系统，Carlini 和 Wagner 发现[2]:*

> *给定任何音频波形，我们可以产生另一个超过 99.9%的相似度，但转录为我们选择的任何短语…我们的攻击 100%成功，不管想要的转录或初始源音频样本。*

*Papernot 等人表明，以有针对性的方式向恶意软件添加一行代码可以在超过 60%的情况下欺骗最先进的恶意软件检测模型[3]。*

*通过使用相当简单的技巧，一个糟糕的演员甚至可以让最有表现力和最令人印象深刻的模特以他们想要的任何方式出错。例如，这张图片骗过了谷歌模型，让它几乎 100%确定这是一张鳄梨色拉酱图片[1]:*

*![](img/44497c73f7478ab353dc1c167cd0df82.png)*

*这是一个现实生活中的停车标志的图像，它被操纵，使得计算机视觉模型在驾车实验中 100%的时间都被欺骗，认为它是一个“限速 45 英里/小时”的标志[4]:*

*![](img/b7b932018ad681cf0237281f6b0e09a3.png)*

# *怎么这么好，却这么坏？*

*在所有这些例子中，基本思想是以最大化模型输出变化的方式扰动输入。这些被称为“对立的例子”。有了这个框架，你可以知道如何最有效地调整猫的形象，让模型认为它是鳄梨色拉酱。这有点像让所有的小错误排成一行，指向同一个方向，这样雪花就变成了雪崩。从技术上来说，这就简化为寻找输出相对于输入的梯度——这是 ML 从业者很擅长做的事情！*

*值得强调的是，这些变化大多是察觉不到的。例如，听这些音频样本。尽管听起来和我的耳朵一样，一个翻译成“没有数据集，文章是无用的”，另一个翻译成“好吧，谷歌，浏览到 evil.com”。更值得强调的是，真正的恶意用户并不总是局限于做出难以察觉的改变，所以我们应该假设这是对安全漏洞的下限估计。*

# *但是我需要担心吗？*

*好吧，所以这些模型的健壮性存在问题，这使得它们很容易被利用。但除非你是谷歌或脸书，否则你可能不会在生产系统中构建庞大的神经网络，所以你不必担心……对吗？*对！？**

*不对。这个问题不是神经网络独有的。事实上，被发现欺骗一个模型的对立例子经常欺骗其他模型，即使它们是使用不同的架构、数据集甚至算法训练的。这意味着，即使你要集合不同类型的模特，你的*仍然不安全*。如果你向外界公开一个模型，即使是间接的，有人可以向它发送输入并得到响应，你就有风险了。这一领域的历史始于暴露*线性模型*的弱点，只是后来才在深度网络的背景下重新点燃【6】。*

# *我们如何阻止它？*

*在攻击和防御之间有一场持续的军备竞赛。这份最近出炉的[《ICML 2018](https://nicholas.carlini.com/papers/2018_icml_obfuscatedgradients.pdf)》最佳论文，“破”了同年会议论文呈现的 9 项答辩中的 7 项。这种趋势不太可能很快停止。*

*那么，一个普通的 ML 从业者该怎么做呢？他可能没有时间关注 ML 安全文献的最新进展，更不用说无休止地将新的防御结合到所有面向外部的生产模型中了。在我看来，唯一明智的方法是设计具有多种智能来源的系统，这样单点故障不会破坏整个系统的效能。这意味着您假设一个单独的模型可以被破坏，并且您设计您的系统对那种情况是健壮的。*

*例如，让无人驾驶汽车完全由计算机视觉 ML 系统导航可能是一个非常危险的想法(原因不仅仅是安全)。使用正交信息(如激光雷达、GPS 和历史记录)的环境冗余测量可能有助于反驳对立的视觉结果。这自然假定系统被设计成综合这些信号以做出最终判断。*

*更重要的一点是，我们需要认识到模型安全性是一个实质性的普遍风险，随着 ML 越来越多地融入我们的生活，它只会随着时间的推移而增加。因此，我们需要建立作为 ML 从业者的肌肉来思考这些风险，并设计强大的系统来抵御它们。就像我们在 web 应用程序中采取预防措施来保护我们的系统免受恶意用户的攻击一样，我们也应该积极应对模型安全风险。正如机构有应用程序安全审查小组进行软件渗透测试一样，我们也需要建立提供类似功能的模型安全审查小组。有一点是肯定的:这个问题不会很快消失，而且可能会越来越严重。*

****如果你想了解更多关于这个话题的内容，比吉奥和花小蕾的论文给出了该领域精彩的回顾和历史，包括这里没有提到的完全不同的攻击方法(例如数据中毒)[6]。****

# *参考*

1.  *“愚弄物理世界中的神经网络。”2017 年 10 月 31 日，www.labsix.org/physical-objects-that-fool-neural-nets。*
2.  *名词（noun 的缩写）卡利尼 d .瓦格纳。"音频对抗示例:针对语音转文本的攻击."arXiv 预印本 [arXiv:1801.01944](https://arxiv.org/abs/1801.01944) ，2018。*
3.  *K.Grosse，N. Papernot，P. Manoharan，M. Backes，P. McDaniel (2017) [恶意软件检测的对抗示例](http://www.patrickmcdaniel.org/pubs/esorics17.pdf)。载于:Foley S .，Gollmann D .，Snekkenes E .(编辑)《计算机安全——ESORICS 2017》。ESORICS 2017。计算机科学讲义，第 10493 卷。斯普林格，查姆。*
4.  *K.Eykhold，I. Evtimov 等人，“对深度学习视觉分类的鲁棒物理世界攻击”。arXiv 预印本 [arXiv:1707.08945](https://arxiv.org/abs/1707.08945) ，2017。*
5.  *名词（noun 的缩写）Papernot，P. McDaniel，I.J. Goodfellow。“机器学习中的可转移性:从现象到使用敌对样本的黑盒攻击”。arXiv 预印本 [arXiv:1605.07277](https://arxiv.org/abs/1605.07277) ，2016。*
6.  *B.花小蕾·比吉奥。"野生模式:对抗性机器学习兴起后的十年."arXiv 预印本 [arXiv:1712.03141](https://arxiv.org/abs/1712.03141) ，2018。*

**原载于*[*blog.zakjost.com*](https://blog.zakjost.com/post/model-security/)*。**