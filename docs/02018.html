<html>
<head>
<title>Build XGBoost / LightGBM models on large datasets — what are the possible solutions?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在大型数据集上构建 XGBoost / LightGBM 模型——可能的解决方案是什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d?source=collection_archive---------4-----------------------#2019-04-03">https://towardsdatascience.com/build-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d?source=collection_archive---------4-----------------------#2019-04-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d685" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">XGBoost 和 LightGBM 已经在许多表格数据集上被证明是性能最好的 ML 算法。但是当数据庞大时，我们如何使用它们呢？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ac4bafa7deb0e90e2cdd5dfce3d9da01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8oAEPp35z01tpBbScU56CA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/photos/KMn4VEeEPR8?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Sean O.</a> on <a class="ae kv" href="https://unsplash.com/search/photos/beach?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0c08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XGBoost 和 LightGBM 在最近所有的 kaggle 表格数据竞争中占据了主导地位。只需进入任何竞赛页面(表格数据)并检查内核，你就会看到。在这些比赛中，数据并不“庞大”——好吧，不要告诉我你正在处理的数据是巨大的，如果它可以在你的笔记本电脑上训练的话。对于这些情况，Jupyter notebook 足够用于 XGBoost 和 LightGBM 模型构造。</p><p id="4715" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当数据变得更大，但不是超级大，而你仍然想坚持使用 Jupyter 笔记本，比方说，来构建模型——一种方法是使用一些内存减少技巧(例如，ArjanGroen 的代码:<a class="ae kv" href="https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/arjanso/reducing-data frame-memory-size-by-65</a>)；或者使用云服务，比如在 AWS 上租一台 EC2。例如，r5.24xlarge 实例拥有 768 GiB 内存，成本为 6 美元/小时，我认为它已经可以处理大量数据，您的老板认为它们真的很“大”。</p><p id="aa7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是如果数据更大呢？</p><p id="e9b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要分布式机器学习工具。据我所知，如果我们想使用可伸缩的 XGBoost 或 LightGBM，我们有以下几种选择:</p><p id="e0e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1 XGBoost 4j on Scala-Spark<br/>2 light GBM on Spark(py Spark/Scala/R)<br/>3 XGBoost with H2O . ai<br/>4 XGBoost on Amazon SageMaker</p><p id="baa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想根据我的个人经验指出每种工具的一些问题，如果您想使用它们，我会提供一些资源。如果你也有类似的问题/你是如何解决的，我也很高兴向你学习！请在下面评论。</p><p id="ce38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我开始使用这些工具之前，有一些事情需要事先了解。</p><h1 id="9933" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">XGBoost 与 LightGBM</h1><p id="2eb6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">XGBoost 是一种非常快速和准确的 ML 算法，但它现在受到了 LightGBM 的挑战，light GBM 运行得更快(对于某些数据集，根据其基准测试，它快了 10 倍)，具有相当的模型准确性，并且有更多的超参数供用户调整。速度上的关键差异是因为 XGBoost 一次将树节点拆分一层，而 LightGBM 一次只拆分一个节点。</p><p id="3564" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，XGBoost 开发人员后来改进了他们的算法，以赶上 LightGBM，允许用户也在逐叶分割模式下运行 XGBoost(grow _ policy = ' loss guide ')。现在，XGBoost 在这一改进下速度快了很多，但根据我在几个数据集上的测试，LightGBM 的速度仍然是 XGB 的 1.3-1.5 倍。(欢迎分享你的测试结果！)</p><p id="37f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">读者可以根据自己的喜好选择任何一个选项。这里再补充一点:XGBoost 有一个 LightGBM 没有的特性——“单调约束”。这将牺牲一些模型精度并增加训练时间，但可以提高模型的可解释性。(参考:<a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html" rel="noopener ugc nofollow" target="_blank">https://xgboost . readthe docs . io/en/latest/tutorials/monotonic . html</a>和【https://github.com/dotnet/machinelearning/issues/1651】T2)</p><h1 id="22e7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">在渐变提升树中找到“甜蜜点”</h1><p id="707d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">对于随机森林算法，建立的树越多，模型的方差越小。但是在某种程度上，你不能通过添加更多的树来进一步改进这个模型。</p><p id="bb5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XGBoost 和 LightGBM 不是这样工作的。当树的数量增加时，模型精度不断提高，但是在某个点之后，性能开始下降——过度拟合的标志；随着树越建越多，性能越差。</p><p id="262d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了找到‘甜蜜点’，你可以做交叉验证或者简单的做训练-验证集分裂，然后利用提前停止时间找到它应该停止训练的地方；或者，你可以用不同数量的树(比如 50、100、200)建立几个模型，然后从中选出最好的一个。</p><p id="b782" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你不在乎极端的性能，你可以设置一个更高的学习率，只构建 10-50 棵树(比如说)。它可能有点不合适，但你仍然有一个非常准确的模型，这样你可以节省时间找到最佳的树的数量。这种方法的另一个好处是模型更简单(构建的树更少)。</p><h1 id="dde0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1.Scala-Spark 上的 XGBoost4j</h1><p id="ed7d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果读者打算选择这个选项，<a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://xgboost . readthedocs . io/en/latest/JVM/xgboost 4j _ spark _ tutorial . html</a>是一个很好的起点。我想在这里指出几个问题(在本文发布时):</p><ol class=""><li id="37e2" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">XGBoost4j 不支持 Pyspark。</li><li id="16d1" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">XGBoost4j 不支持逐叶分割模式，因此速度较慢。<a class="ae kv" href="https://github.com/dmlc/xgboost/issues/3724" rel="noopener ugc nofollow" target="_blank">https://github.com/dmlc/xgboost/issues/3724</a></li><li id="336f" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">因为是在 Spark 上，所以所有缺失值都要进行插补(vector assembler 不允许缺失值)。这可能会降低模型精度。<a class="ae kv" href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm-faq/missing_values.html" rel="noopener ugc nofollow" target="_blank">http://docs . H2O . ai/H2O/latest-stable/H2O-docs/data-science/GBM-FAQ/missing _ values . html</a></li><li id="2811" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">早期停止可能仍然包含 bug。如果你关注他们在 https://github.com/dmlc/xgboost/releases<a class="ae kv" href="https://github.com/dmlc/xgboost/releases" rel="noopener ugc nofollow" target="_blank">的最新发布，你会发现他们最近仍在修复这些漏洞。</a></li></ol><h1 id="fb8f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2.Spark 上的 light GBM(Scala/Python/R)</h1><p id="1e23" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">基于我个人经验的主要问题:</p><ol class=""><li id="4231" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">缺少文档和好的例子。<a class="ae kv" href="https://github.com/Azure/mmlspark/blob/master/docs/lightgbm.md" rel="noopener ugc nofollow" target="_blank">https://github . com/Azure/mmspark/blob/master/docs/light GBM . MD</a></li><li id="d491" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">所有缺失的值都必须进行估算(类似于 XGBoost4j)</li><li id="2fcf" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">我在 spark cross validator 的“提前停止”参数上也有问题。(为了测试它是否正常工作，选择一个较小的数据集，选择一个非常大的回合数，提前停止= 10，并查看训练模型需要多长时间。训练完成后，将模型精度与使用 Python 构建的模型进行比较。如果过拟合得很差，很可能早期停止根本不起作用。)</li></ol><p id="d833" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些示例代码(不包括矢量汇编程序):</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="73d9" class="ni lt iq ne b gy nj nk l nl nm">from mmlspark import LightGBMClassifier<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator<br/>from pyspark.ml.tuning import CrossValidator, ParamGridBuilder</span><span id="ca3d" class="ni lt iq ne b gy nn nk l nl nm">lgb_estimator = LightGBMClassifier(learningRate=0.1, <br/>                                   numIterations=1000,<br/>                                   earlyStoppingRound=10,<br/>                                   labelCol="label")</span><span id="376a" class="ni lt iq ne b gy nn nk l nl nm">paramGrid = ParamGridBuilder().addGrid(lgb_estimator.numLeaves, [30, 50]).build()</span><span id="86e2" class="ni lt iq ne b gy nn nk l nl nm">eval = BinaryClassificationEvaluator(labelCol="label",metricName="areaUnderROC")</span><span id="3248" class="ni lt iq ne b gy nn nk l nl nm">crossval = CrossValidator(estimator=lgb_estimator,<br/>                          estimatorParamMaps=paramGrid, <br/>                          evaluator=eval, <br/>                          numFolds=3)     </span><span id="9cc5" class="ni lt iq ne b gy nn nk l nl nm">cvModel  = crossval.fit(train_df[["features", "label"]])</span></pre><h1 id="6b0e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">3.H2O.ai 上的 XGBoost</h1><p id="cf42" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这是我个人最喜欢的解决方案。该模型可以使用 H2O.ai 构建，集成在 py sparking Water(H2O . ai+py spark)管道中:<br/><a class="ae kv" href="https://www.slideshare.net/0xdata/productionizing-h2o-models-using-sparkling-water-by-jakub-hava" rel="noopener ugc nofollow" target="_blank">https://www . slide share . net/0x data/productionizing-H2O-models-using-sparking-Water-by-jakub-hava</a></p><p id="ba64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很容易建立一个带有交叉验证的优化轮数的模型</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="80e0" class="ni lt iq ne b gy nj nk l nl nm"># binary classification</span><span id="e0a7" class="ni lt iq ne b gy nn nk l nl nm">features = ['A', 'B', 'C']<br/>train['label'] = train['label'].asfactor() # train is an H2O frame</span><span id="2100" class="ni lt iq ne b gy nn nk l nl nm">cv_xgb = H2OXGBoostEstimator(<br/>    ntrees = 1000,<br/>    learn_rate = 0.1,<br/>    max_leaves = 50,<br/>    stopping_rounds = 10,<br/>    stopping_metric = "AUC",<br/>    score_tree_interval = 1,<br/>    tree_method="hist",<br/>    grow_policy="lossguide",<br/>    nfolds=5, <br/>    seed=0)</span><span id="57c0" class="ni lt iq ne b gy nn nk l nl nm">cv_xgb.train(x = features, y = 'label', training_frame = train)</span></pre><p id="89db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并且 XGBoost 模型可以用<code class="fe no np nq ne b">cv_xgb.save_mojo()</code>保存在 Python 中使用。如果您想以 h2o 格式保存模型，请使用<code class="fe no np nq ne b">h2o.save_model()</code>。</p><p id="bc48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我唯一的抱怨是保存的模型(用<code class="fe no np nq ne b">save.mojo</code>保存的那个)不能和 SHAP 包一起使用来生成 SHAP 特性重要性(但是 XGBoost 特性重要性，<code class="fe no np nq ne b">.get_fscore()</code>，工作正常)。似乎最初的 XGBoost 包有一些问题。<br/>T12】https://github.com/slundberg/shap/issues/464<br/>T15】https://github.com/dmlc/xgboost/issues/4276</p><p id="dcc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(更新:似乎他们刚刚在其最新版本中实现了 SHAP-<a class="ae kv" href="https://github.com/h2oai/h2o-3/blob/373ca6b1bc7d194c6c70e1070f2f6f416f56b3d0/Changes.md" rel="noopener ugc nofollow" target="_blank">https://github . com/h2oai/H2O-3/blob/373 ca 6 B1 BC 7d 194 C6 c 70 e 1070 F2 F6 f 416 f 56 B3 d 0/changes . MD</a>)</p><h1 id="a13d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">4.SageMaker 上的 XGBoost</h1><p id="ddc6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这是 AWS 的一个非常新的解决方案。两个主要特性是使用贝叶斯优化的自动超参数调整，并且该模型可以作为端点部署。在他们的 Github 上可以找到几个例子:<a class="ae kv" href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning" rel="noopener ugc nofollow" target="_blank">https://Github . com/aw slabs/Amazon-sage maker-examples/tree/master/introduction _ to _ applying _ machine _ learning</a>。以下是我对此的一些担忧:</p><ol class=""><li id="a115" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">与其他解决方案相比，参数调整工具对用户(数据科学家)不太友好:<br/><a class="ae kv" href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_direct_marketing/hpo_xgboost_direct_marketing_sagemaker_APIs.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/aw slats/Amazon-sagemaker-examples/blob/master/hyperparameter _ tuning/xgboost _ direct _ marketing/hpo _ xgboost _ direct _ marketing _ sagemaker _ APIs . ipynb</a>和<br/><a class="ae kv" href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/aw slats/Amazon-sagemaker-examples/blob/master/hyperparameter _ tuning/Analyze _ results/HPO</a></li><li id="307f" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">贝叶斯优化是否是调优 XGB 参数的最佳选择还是未知数。如果你检查了文件，梯度推进树没有被提及/测试。<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" rel="noopener ugc nofollow" target="_blank">https://docs . AWS . Amazon . com/sage maker/latest/DG/automatic-model-tuning-how-it-works . html</a></li><li id="9dbc" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">该参数通过单个验证集进行调整，而不是交叉验证。</li><li id="b5c2" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">我还没想好如何在 Python XGBoost 中使用其内置的 XGBoost 算法训练出来的模型神器。</li></ol><p id="90ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是除了这些问题，我们仍然可以利用它的端点特性。你可以在任何地方训练你的 XGB 模型，从 Amazon ECR(Elastic Container Registry)把它放在 XGBoost 映像中，然后把它部署成一个端点。</p><p id="2e87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">* * * * *</p><p id="9b20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XGBoost / LightGBM 是相当新的 ML 工具，它们都有潜力变得更强。开发人员已经做了出色的工作，创造了这些工具，使人们的生活变得更容易。我在这里指出我的一些观察和分享我的经验，希望它们能成为更好、更易用的工具。</p></div></div>    
</body>
</html>