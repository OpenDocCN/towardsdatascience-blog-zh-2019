<html>
<head>
<title>Auptimizer: A faster, easier way to do hyperparameter optimization for machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一种更快、更简单的机器学习超参数优化方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/auptimizer-a-faster-easier-way-to-do-hyperparameter-optimization-for-machine-learning-88f37c1fcfb7?source=collection_archive---------40-----------------------#2019-11-06">https://towardsdatascience.com/auptimizer-a-faster-easier-way-to-do-hyperparameter-optimization-for-machine-learning-88f37c1fcfb7?source=collection_archive---------40-----------------------#2019-11-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="146a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由刘佳怡(Jason)发布，Unmesh Kurup 和 Mohak Shah</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/6f9bdfda5a110826d5215e98967188b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*X0EY6mRiOQPOco_jquQ-4A.png"/></div></figure><blockquote class="kt"><p id="807d" class="ku kv iq bd kw kx ky kz la lb lc kk dk translated">Auptimizer 是一个通用的开源超参数优化(HPO)框架，它还允许您将 HPO 培训从 CPU 和 GPU 扩展到本地和 EC2 实例。要开始使用，请使用“pip install auptimizer”。你可以在这里找到我们的文档<a class="ae le" href="https://lge-arc-advancedai.github.io/auptimizer/" rel="noopener ugc nofollow" target="_blank"><em class="ld"/></a><em class="ld">和我们的回购</em> <a class="ae le" href="https://github.com/LGE-ARC-AdvancedAI/auptimizer" rel="noopener ugc nofollow" target="_blank"> <em class="ld">这里</em> </a> <em class="ld">。</em></p></blockquote><p id="01cd" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">在过去的十年里，我们在建立和训练机器学习模型方面取得了重大进展。我们现在可以通过利用算法、学习策略以及分布式计算和内存可用性的改进来优化非常大的模型。然而，随着这些模型的规模和复杂性的增长，基础超参数的数量也在增长。但是处理超参数优化(HPO)的策略在实践中大多仍然局限于最常用的网格和随机搜索方法。虽然有一些针对 HPO 的商业和开源解决方案，但没有一个能广泛适用于各种问题和平台。HPO 既是科学也是艺术，是训练有效机器学习模型的一个关键瓶颈。</p><p id="ed90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个障碍是对最佳可用的 HPO 算法缺乏共识。开发人员必须试验多种算法，以找到最适合他们问题的算法。然而，实现之间缺乏可移植性意味着用户经常被一个特定的算法所困扰，而这个算法已经围绕着他们建立了工具。Auptimizer 使研究人员和从业人员可以轻松地在不同的 HPO 算法之间切换。此外，Auptimizer 还支持跨平台扩展，使用户能够轻松地将他们的实验从桌面扩展到本地集群甚至云。</p><p id="6a76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 ML 研究者来说，用例可以是不同的。研究人员专注于开发<em class="lk">算法</em>来寻找最佳超参数。因此，一个简单的框架来促进他们的算法实现，并根据最先进的算法对他们的结果进行基准测试也很重要。</p><p id="c19f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，Auptimizer 是一个可伸缩、可扩展的 HPO 工具包。Auptimizer 有三个优点:</p><ol class=""><li id="71d1" class="ll lm iq jp b jq jr ju jv jy ln kc lo kg lp kk lq lr ls lt bi translated">各种 HPO 方法的通用接口，允许在它们之间轻松切换；</li><li id="e457" class="ll lm iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">易于扩展，允许用户(a)添加他们自己的 HPO 算法,( b)根据现有的解决方案对它们进行基准测试；和</li><li id="633f" class="ll lm iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">易于部署到 AWS，提供了一种从个人计算机到云的扩展模型训练的方法。</li></ol><p id="1f8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于 Auptimizer 是独立于平台的，它可以与您选择的框架一起工作，包括 TensorFlow、PyTorch、MxNet 和 Caffe。</p><p id="6d4c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Auptimizer 的通用界面通过抽象出 HPO 算法实现之间的差异，使得在这些算法之间切换的过程变得更加容易。在此版本中，我们支持以下 HPO 技术—随机搜索、网格搜索、Hyperband、Hyperopt、留兰香和 EAS(实验)。</p><p id="a134" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Auptimizer 可以轻松支持较新的 HPO 技术的集成和基准测试，包括您自己的定制算法。作为 Auptimizer 易于扩展的一个例子，我们在短短几天内集成了贝叶斯优化和 Hyperband (BOHB)(只需要编写 138 行代码，同时重用原来的 4305 行代码)。</p><p id="2c25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，Auptimizer 还包括自动 HPO 过程的功能。Auptimizer 支持不同的计算资源，例如 CPU、GPU、多个节点和 AWS EC2 实例。它还足够灵活，可以扩展到其他云平台或您的内部解决方案。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="cfb0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">致谢:</strong>这项工作是 LG 电子(美国研究中心)团队的成果。特别感谢 Samarth Tripathi、Vera Serdiukova、gu Junyao Guo、Homa Fashandi、Guohua Ren 和 Olimpiya Saha 作出的贡献。</p></div></div>    
</body>
</html>