<html>
<head>
<title>Mixture modelling from scratch, in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">混合建模从零开始，在 R</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mixture-modelling-from-scratch-in-r-5ab7bfc83eef?source=collection_archive---------1-----------------------#2019-05-12">https://towardsdatascience.com/mixture-modelling-from-scratch-in-r-5ab7bfc83eef?source=collection_archive---------1-----------------------#2019-05-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2d08" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从 K-means 到高斯混合建模，浓缩在几行代码中</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e953b0e46e71e4b8d4976d9e71642da8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaapeE-NBoVZYibjVJkKVA.jpeg"/></div></div></figure><p id="995e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在机器学习文献中，<a class="ae lq" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> K-means </strong> </a>和<a class="ae lq" href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">高斯混合模型</strong> </a> (GMM)是第一个描述的聚类/无监督模型[1–3]，因此，应该是任何数据科学家工具箱的一部分。在 R 中，可以使用<code class="fe lr ls lt lu b">kmeans()</code>、<code class="fe lr ls lt lu b">Mclust()</code>或其他类似的函数，但是要完全理解那些算法，需要从头开始构建它们。网上搜索会引导你找到许多有用的教程，但是你很难找到完全透明的 R 代码。我遇到的最好的例子是 R-bloggers 上的一篇优秀文章'<a class="ae lq" href="https://www.r-bloggers.com/an-intro-to-gaussian-mixture-modeling/" rel="noopener ugc nofollow" target="_blank">高斯混合建模介绍</a>'。</p><p id="5598" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但我仍然有点不满意。这就是我一直在寻找的东西:</p><ul class=""><li id="2aa9" class="lv lw it kw b kx ky la lb ld lx lh ly ll lz lp ma mb mc md bi translated">A-to-Z 算法(数据-&gt;算法-&gt;结果)</li><li id="8152" class="lv lw it kw b kx me la mf ld mg lh mh ll mi lp ma mb mc md bi translated">独占使用 R 基础包(无隐藏计算)</li><li id="d1bf" class="lv lw it kw b kx me la mf ld mg lh mh ll mi lp ma mb mc md bi translated">多变量情况(不仅仅是单变量或双变量)</li><li id="1cd3" class="lv lw it kw b kx me la mf ld mg lh mh ll mi lp ma mb mc md bi translated">优雅的代码(K-means 和 GMM 之间的平行极简)</li><li id="429e" class="lv lw it kw b kx me la mf ld mg lh mh ll mi lp ma mb mc md bi translated">用于绘制算法迭代动画的选项</li></ul><p id="6f2f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我上面提到的代码满足了大约一半的需求(从 A 到 Z，优雅的 GMM 多元代码)。我通过更多的研究完成了剩下的部分，结果如下。请注意，我将这篇文章写得很短，重点放在算法的本质上。方程式可以在维基百科和 ML 教材上找到。r 代码被注释只是为了给出足够多的关于建模步骤的细节。我还为 K-means 和 GMM 使用了相同的结构来突出它们之间的明显相似之处(即分别是<a class="ae lq" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">期望最大化</strong> </a>算法的硬版本和软版本)。</p><h1 id="d317" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">我们的数据</h1><p id="4c50" class="pw-post-body-paragraph ku kv it kw b kx nb ju kz la nc jx lc ld nd lf lg lh ne lj lk ll nf ln lo lp im bi translated">为了说明，我们将把那些聚类算法应用于著名的<a class="ae lq" href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">虹膜数据集</strong> </a>。在 20 世纪初，Edgar Anderson 收集数据量化了三种鸢尾花的形态变异:<em class="ng">鸢尾</em>、<em class="ng">海滨鸢尾</em>和<em class="ng">杂色鸢尾</em>。该数据集由 3 个物种的 50 个样本和 4 个特征组成:萼片和花瓣的长度和宽度[4]。英国统计学家和遗传学家<strong class="kw iu">罗纳德·费雪</strong>(1890–1962)，<em class="ng">一个几乎是单枪匹马为现代统计科学奠定基础的天才</em>【5】，然后在<em class="ng">“分类问题中多重测量的使用</em>’【6】中使用这个数据集作为<a class="ae lq" href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" rel="noopener ugc nofollow" target="_blank">线性判别分析</a> (LDA)的例子。从那时起，Iris 数据集已经成为简单多维聚类问题的教科书示例(尽管 Fisher 显然使用了 LDA 的标签，LDA 是一种监督学习模型)。<code class="fe lr ls lt lu b">iris</code>数据集包含在 R <code class="fe lr ls lt lu b">datasets</code>包中。</p><pre class="kj kk kl km gt nh lu ni nj aw nk bi"><span id="bfed" class="nl mk it lu b gy nm nn l no np">X &lt;- iris[,1:4]<br/>y &lt;- iris[,5]</span><span id="a278" class="nl mk it lu b gy nq nn l no np">y_col &lt;- c('#7DB0DD', '#86B875', '#E495A5')</span><span id="b714" class="nl mk it lu b gy nq nn l no np">pdf('dat_iris.pdf')<br/>pairs(X, lower.panel = NULL, col = y_col[y])<br/>par(xpd = T)<br/>legend(x = 0.1, y = 0.4, legend = as.character(levels(y)), fill = y_col)<br/>dev.off()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/ddd668f828ba8f3fe318267ef60db672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MhyBC77MJHlWpNDDfV6R6Q.jpeg"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">The Anderson/Fisher Iris flower dataset</strong> | Features and labels from the R <code class="fe lr ls lt lu b">datasets</code> package; photographs of the 3 Iris species, source: <a class="ae lq" href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>; and original data, Table 1 of [6].</figcaption></figure><h1 id="7075" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">k 均值</h1><p id="6f09" class="pw-post-body-paragraph ku kv it kw b kx nb ju kz la nc jx lc ld nd lf lg lh ne lj lk ll nf ln lo lp im bi translated">我们从优秀的综述“<em class="ng">数据聚类:50 年后的 K-means</em>”[7]中了解到，聚类方法的发展是一项跨学科的努力，有来自分类学家、生物学家、心理学家、统计学家、工程师等的众多贡献。“数据聚类”一词首次出现在 1954 年一篇关于人类学数据的文章的标题中。在现有的所有聚类算法中，K-means 算法是最流行和最简单的。它有着丰富多样的历史，因为它是在 1956 年至 1967 年间在不同的科学领域独立发现的[8–9]。下面的注释 R 代码对此进行了描述:</p><pre class="kj kk kl km gt nh lu ni nj aw nk bi"><span id="9226" class="nl mk it lu b gy nm nn l no np"># finds partition such that squared error between empirical mean<br/># and points in cluster is minimized over all k clusters<br/>km.fromscratch &lt;- function(X, k){<br/>  p &lt;- ncol(X)  # number of parameters<br/>  n &lt;- nrow(X)  # number of observations<br/>  Delta &lt;- 1; iter &lt;- 0; itermax &lt;- 30<br/>  while(Delta &gt; 1e-4 &amp;&amp; iter &lt;= itermax){<br/>    # initiation<br/>    if(iter == 0){<br/>      centroid &lt;- X[sample(nrow(X), k),]<br/>      centroid_mem &lt;- centroid<br/>    }<br/>    <br/>    # equivalent to E-step<br/>    d &lt;- sapply(1:k, function(c) sapply(1:n, <br/>      function(i) sum((centroid[c,] - X[i,])^2) ))<br/>    cluster &lt;- apply(d, 1, which.min)<br/>    <br/>    # equivalent to M-step<br/>    centroid &lt;- t(sapply(1:k, function(c) <br/>      apply(X[cluster == c,], 2, mean)))<br/>    <br/>    Delta &lt;- sum((centroid - centroid_mem)^2)<br/>    iter &lt;- iter + 1; centroid_mem &lt;- centroid<br/>  }<br/>  return(list(centroid = centroid, cluster = cluster))<br/>}</span><span id="c9d2" class="nl mk it lu b gy nq nn l no np"># run K-means<br/>km &lt;- km.fromscratch(X, 3)<br/>pairs(X, lower.panel = NULL, col = km$cluster)<br/>table(y, km$cluster)</span></pre><p id="29f2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们来对比一下打包的算法<code class="fe lr ls lt lu b">km.pkg &lt;- kmeans(X, 3)</code>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/70dd56e0d91d90e6116d802a6c912107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_jnwOxc-_tv3N86sSd_7qw.jpeg"/></div></div></figure><p id="622a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下动画基于建议的 R 代码，但使用<code class="fe lr ls lt lu b">pairs()</code>功能的面板选项绘制了集群球形决策边界的半径。为了清楚起见，这在前面的代码中没有显示，因为它的大小已经翻倍了！然而，附录中提供了完整的代码。与鸢尾属物种标记相比，我们得到了 88.7% 的<strong class="kw iu">(对于<em class="ng"> setosa </em>没有错误，对于<em class="ng"> versicolor </em>有 3 个错误分类的观察值，对于<em class="ng"> virginica </em>有 14 个错误分类的观察值——注意，结果可能会因随机启动而略有变化)。使用<code class="fe lr ls lt lu b">kmeans()</code>内置的 R 函数会产生类似的结果，但速度更快。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b4e68220561d3274ce28901bcca75ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Xf0q0XcRBBSKNglYSLXr1A.gif"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">K-means clustering of the Iris dataset</strong> | The different iterations to convergence, with radius of the cluster spherical decision boundary plotted.</figcaption></figure><h1 id="f401" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">高斯混合模型</h1><p id="994c" class="pw-post-body-paragraph ku kv it kw b kx nb ju kz la nc jx lc ld nd lf lg lh ne lj lk ll nf ln lo lp im bi translated">高斯混合(高斯混合模型或 GMM)是最广泛使用的混合模型。GMM 可以被描述为具有高斯密度的 K-均值的软版本。下面是多维情况下的 R 代码，这意味着我们采用全张量，这是通过多元正态分布和协方差矩阵的特征分解来了解 GMM 所有复杂性的最佳方式。我们开始吧:</p><pre class="kj kk kl km gt nh lu ni nj aw nk bi"><span id="c9f9" class="nl mk it lu b gy nm nn l no np"># Uses EM algorithm with multivariate normal<br/># distribution to estimate cluster probability<br/>mvnorm.cov.inv &lt;- function(Sigma) {<br/>  # Eigendecomposition of covariance matrix<br/>  E &lt;- eigen(Sigma)<br/>  Lambda.inv &lt;- diag(E$values^-1)   # diagonal matrix<br/>  Q &lt;- E$vectors<br/>  return(Q %*% Lambda.inv %*% t(Q))<br/>}</span><span id="9c18" class="nl mk it lu b gy nq nn l no np">#multivariate Gaussian pdf<br/>mvn.pdf.i &lt;- function(xi, mu, Sigma)<br/>  1/sqrt( (2*pi)^length(xi) * det(Sigma) ) * <br/>  exp(-(1/2) * t(xi - mu) %*% mvnorm.cov.inv(Sigma) <br/>  %*% (xi - mu)  )</span><span id="caed" class="nl mk it lu b gy nq nn l no np">mvn.pdf &lt;- function(X, mu, Sigma)<br/>  apply(X, 1, function(xi) mvn.pdf.i(as.numeric(xi), mu, Sigma))</span><span id="a22f" class="nl mk it lu b gy nq nn l no np">gmm.fromscratch &lt;- function(X, k){<br/>  p &lt;- ncol(X)  # number of parameters<br/>  n &lt;- nrow(X)  # number of observations<br/>  Delta &lt;- 1; iter &lt;- 0; itermax &lt;- 30<br/>  while(Delta &gt; 1e-4 &amp;&amp; iter &lt;= itermax){<br/>    # initiation<br/>    if(iter == 0){<br/>      km.init &lt;- km.fromscratch(X, k)<br/>      mu &lt;- km.init$centroid; mu_mem &lt;- mu<br/>      w &lt;- sapply(1:k, function(i) length(which(km.init$cluster == i)))<br/>      w &lt;- w/sum(w)<br/>      cov &lt;- array(dim = c(p, p, k))<br/>      for(i in 1:p) for(j in 1:p) for(c in 1:k) cov[i, j, c] &lt;- <br/>        1/n * sum((X[km.init$cluster == c, i] - mu[c, i]) *<br/>        (X[km.init$cluster == c, j] - mu[c, j]))<br/>    }<br/>    <br/>    # E-step<br/>    mvn.c &lt;- sapply(1:k, function(c) mvn.pdf(X, mu[c,], cov[,, c]))<br/>    r_ic &lt;- t(w*t(mvn.c)) / rowSums(t(w*t(mvn.c)))<br/>    <br/>    # M-step<br/>    n_c &lt;- colSums(r_ic)<br/>    w &lt;- n_c/sum(n_c)<br/>    mu &lt;- t(sapply(1:k, function(c) 1/n_c[c] * colSums(r_ic[, c] *<br/>      X)))<br/>    for(i in 1:p) for(j in 1:p) for(c in 1:k) cov[i, j, c] &lt;-<br/>      1/n_c[c] * sum(r_ic[, c] * (X[, i] - mu[c, i]) * r_ic[, c] *<br/>      (X[, j] - mu[c, j]))</span><span id="32c7" class="nl mk it lu b gy nq nn l no np">    Delta &lt;- sum((mu - mu_mem)^2)<br/>    iter &lt;- iter + 1; mu_mem &lt;- mu<br/>  }<br/>  return(list(softcluster = r_ic, cluster = apply(r_ic, 1,<br/>    which.max)))<br/>}</span><span id="b8b6" class="nl mk it lu b gy nq nn l no np"># run GMM<br/>gmm &lt;- gmm.fromscratch(X, 3)<br/>pairs(X, lower.panel = NULL, col = gmm$cluster)<br/>table(y, gmm$cluster)</span></pre><p id="adb2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们来对比一下来自<code class="fe lr ls lt lu b">library(mclust)</code>的<code class="fe lr ls lt lu b">gmm.mclust &lt;- Mclust(X, 3)</code>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/86f1cb061cef826613a802a421ed6703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-mmqUzKe_24G-eE83sSLg.jpeg"/></div></div></figure><p id="9e00" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样，为了清楚起见，本代码中没有显示迭代过程中椭圆的绘制，如下面的动画所示(完整代码请参见附录)。与鸢尾属物种标记相比，当将每个观察分类到最高聚类概率时，我们得到了 96.7% 的<strong class="kw iu">准确度(对于<em class="ng"> setosa </em>和<em class="ng"> virginica </em>没有错误，对于<em class="ng"> versicolor </em>有 5 个观察被错误分类)。但是，请注意，结果可能会因随机参数初始化而略有不同；运行几次算法来研究这种可变性。使用<code class="fe lr ls lt lu b">Mclust()</code>也能得到类似的结果(但<code class="fe lr ls lt lu b">Mclust()</code>显然要快得多)。如果我们详细比较两种聚类算法的结果，我们可以看到，是 K-means </strong>做出的<strong class="kw iu">球形假设使其在 Iris 数据集上的性能略差于 GMM。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/81c2afbba9f8dfff71a5ae5a7ade0708.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*uc4oOisa1oYCtaoJxedK9w.gif"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Gaussian Mixture Modelling of the Iris dataset</strong> | The different iterations to convergence, with elliptic distribution of each cluster plotted.</figcaption></figure><p id="de1c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在看看这两种算法之间的相似之处，两者都由三个步骤构成:初始、E 步骤和 M 步骤的<a class="ae lq" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">期望最大化</strong></a><strong class="kw iu">【EM】算法</strong>【10】。EM 是一种简单而强大的迭代算法，它在给定参数的情况下推断聚类(E 步骤)，然后在给定预测聚类的情况下优化参数(M 步骤)之间交替进行。</p><p id="7c7c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="ng">瞧</em>。既然你已经理解了这两个经典聚类算法的步骤，我推荐你使用正式的 R 函数(<code class="fe lr ls lt lu b">kmeans()</code>、<code class="fe lr ls lt lu b">Mclust()</code>、<em class="ng">等)。</em>)，已经过基准测试，效率要高得多。</p><p id="d6e3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[1] C. M. Bishop，<a class="ae lq" href="https://www.springer.com/de/book/9780387310732" rel="noopener ugc nofollow" target="_blank">模式识别和机器学习</a> (2006)，Springer</p><p id="7df3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[1] C. M. Bishop，<a class="ae lq" href="https://www.springer.com/de/book/9780387310732" rel="noopener ugc nofollow" target="_blank">模式识别和机器学习</a> (2006)，Springer</p><p id="1ec9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[2] T. Hastie 等人，<a class="ae lq" href="https://www.springer.com/de/book/9780387848570" rel="noopener ugc nofollow" target="_blank">统计学习、数据挖掘、推理和预测的要素</a> (2009)，Springer，第二版。</p><p id="9646" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[3] K. P. Murphy，<a class="ae lq" href="https://www.cs.ubc.ca/~murphyk/MLbook/" rel="noopener ugc nofollow" target="_blank">机器学习，概率视角</a> (2012)，麻省理工学院出版社</p><p id="28fc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[4] E .安德森，<a class="ae lq" href="https://www.jstor.org/stable/pdf/2394164.pdf" rel="noopener ugc nofollow" target="_blank">鸢尾属植物中的物种问题</a> (1936)，密苏里植物园年报</p><p id="dc4c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[5] A .哈尔德，<a class="ae lq" href="https://www.amazon.com/History-Mathematical-Statistics-Wiley-Probability/dp/0471179124" rel="noopener ugc nofollow" target="_blank">一部数理统计史</a> (1998)，威利</p><p id="fde2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[7] A. K. Jain，<a class="ae lq" href="https://www.sciencedirect.com/science/article/abs/pii/S0167865509002323" rel="noopener ugc nofollow" target="_blank">数据聚类:50 年后的 K-means </a> (2010)，模式识别字母</p><p id="3993" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[8] H .施泰因豪斯，<a class="ae lq" href="http://www.laurent-duval.eu/Documents/Steinhaus_H_1956_j-bull-acad-polon-sci_division_cmp-k-means.pdf" rel="noopener ugc nofollow" target="_blank">《关于各方的材料分工》</a> (1956 年)，《波罗乃兹科学院公报》</p><p id="5167" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[9] J. MacQueen，<a class="ae lq" href="https://projecteuclid.org/download/pdf_1/euclid.bsmsp/1200512992" rel="noopener ugc nofollow" target="_blank">多变量观测值的一些分类和分析方法</a> (1967)，第五届伯克利研讨会</p><p id="4d99" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[10] A. P. Dempster 等人，<a class="ae lq" href="http://web.mit.edu/6.435/www/Dempster77.pdf" rel="noopener ugc nofollow" target="_blank">通过 EM 算法从不完整数据中获得最大似然</a> (1977)，皇家统计学会杂志</p><h2 id="1218" class="nl mk it bd ml oa ob dn mp oc od dp mt ld oe of mv lh og oh mx ll oi oj mz ok bi translated">附录:动画 K 均值</h2><pre class="kj kk kl km gt nh lu ni nj aw nk bi"><span id="cce3" class="nl mk it lu b gy nm nn l no np">wd &lt;- getwd()</span><span id="59ea" class="nl mk it lu b gy nq nn l no np"># finds partition such that squared error between empirical mean<br/># and points in cluster is minimized over all k clusters<br/>km.fromscratch &lt;- function(X, k, plot = F){<br/>  p &lt;- ncol(X)  # number of parameters<br/>  n &lt;- nrow(X)  # number of observations<br/>  Delta &lt;- 1; iter &lt;- 0; itermax &lt;- 30<br/>  class_col &lt;- c('#7DB0DD', '#86B875', '#E495A5')<br/>  while(Delta &gt; 1e-4 &amp;&amp; iter &lt;= itermax){<br/>    # initiation<br/>    if(iter == 0){<br/>      centroid &lt;- X[sample(nrow(X), k),]<br/>      centroid_mem &lt;- centroid<br/>    }<br/>    <br/>    # equivalent to E-step<br/>    d &lt;- sapply(1:k, function(c) sapply(1:n, function(i) sum((centroid[c,] - X[i,])^2) ))<br/>    cluster &lt;- apply(d, 1, which.min)<br/>    # equivalent to M-step<br/>    centroid &lt;- t(sapply(1:k, function(c) apply(X[cluster == c,], 2, mean)))<br/>    <br/>    rad &lt;- sapply(1:k, function(c) max(sqrt(d[cluster == c,c])))<br/>    if(plot){<br/>      i &lt;- 1<br/>      idx &lt;- matrix(rep(seq(p), p), ncol = p, nrow = p)<br/>      idx &lt;- idx[lower.tri(idx)]<br/>      idy &lt;- matrix(rep(seq(p), each=p), ncol = p, nrow = p)<br/>      idy &lt;- idy[lower.tri(idy)]<br/>      theta &lt;- seq(0,1,0.01) * 2*pi<br/>      <br/>      png(paste0(wd, '/fig_kmeans/iter', iter, '.png'))<br/>      pairs(rbind(X, centroid), lower.panel = NULL, asp = 1,<br/>            col = c(class_col[cluster], rep('black', k)), main =<br/>            paste0('iter=',iter), panel=function(x, y, ...) {<br/>              points(x, y, col = c(class_col[cluster], rep('black', k)))<br/>              lines(centroid[, idx[i]][1]+cos(theta)*rad[1], <br/>                    centroid[, idy[i]][1]+sin(theta)*rad[1], <br/>                    col=class_col[1])<br/>              lines(centroid[, idx[i]][2]+cos(theta)*rad[2], <br/>                    centroid[, idy[i]][2]+sin(theta)*rad[2], <br/>                    col=class_col[2])<br/>              lines(centroid[, idx[i]][3]+cos(theta)*rad[3], <br/>                    centroid[, idy[i]][3]+sin(theta)*rad[3], <br/>                    col=class_col[3])<br/>              i &lt;&lt;- i+1<br/>            })<br/>      dev.off()<br/>    }<br/>    <br/>    Delta &lt;- sum((centroid - centroid_mem)^2)<br/>    iter &lt;- iter + 1; centroid_mem &lt;- centroid<br/>  }<br/>  return(list(centroid = centroid, cluster = cluster))<br/>}</span><span id="0eca" class="nl mk it lu b gy nq nn l no np"># run K-means<br/>km &lt;- km.fromscratch(X, 3, plot = T)<br/>table(y, km$cluster)</span><span id="4789" class="nl mk it lu b gy nq nn l no np">library(magick)<br/>list.files(path = paste0(wd, '/fig_kmeans/'), pattern = '*.png', full.names = T) %&gt;% <br/>  image_read() %&gt;%<br/>  image_join() %&gt;%<br/>  image_animate(fps=1) %&gt;%<br/>  image_write('fig_kmeans_anim.gif')</span></pre><h2 id="cdf3" class="nl mk it bd ml oa ob dn mp oc od dp mt ld oe of mv lh og oh mx ll oi oj mz ok bi translated">附录:动画 GMM</h2><pre class="kj kk kl km gt nh lu ni nj aw nk bi"><span id="2fe3" class="nl mk it lu b gy nm nn l no np">library(reshape)   #cast()<br/>wd &lt;- getwd()</span><span id="ac4d" class="nl mk it lu b gy nq nn l no np"># Uses EM algorithm with multivariate normal<br/># distribution to estimate cluster probability<br/>mvnorm.cov.inv &lt;- function(Sigma) {<br/>  # Eigendecomposition of covariance matrix<br/>  E &lt;- eigen(Sigma)<br/>  Lambda.inv &lt;- diag(E$values^-1)   # diagonal matrix with inverse of eigenvalues<br/>  Q &lt;- E$vectors                    # eigenvectors<br/>  return(Q %*% Lambda.inv %*% t(Q))<br/>}</span><span id="b7ca" class="nl mk it lu b gy nq nn l no np">#multivariate Gaussian pdf<br/>mvn.pdf.i &lt;- function(xi, mu, Sigma)<br/>  1/sqrt( (2*pi)^length(xi) * det(Sigma) ) * <br/>  exp(-(1/2) * t(xi - mu) %*% mvnorm.cov.inv(Sigma) %*% (xi - mu)  )</span><span id="5611" class="nl mk it lu b gy nq nn l no np">mvn.pdf &lt;- function(X, mu, Sigma)<br/>  apply(X, 1, function(xi) mvn.pdf.i(as.numeric(xi), mu, Sigma))</span><span id="8dc9" class="nl mk it lu b gy nq nn l no np">gmm.fromscratch &lt;- function(X, k, plot = F){<br/>  p &lt;- ncol(X)  # number of parameters<br/>  n &lt;- nrow(X)  # number of observations<br/>  Delta &lt;- 1; iter &lt;- 0; itermax &lt;- 30<br/>  class_col &lt;- c('#7DB0DD', '#86B875', '#E495A5')<br/>  while(Delta &gt; 1e-4 &amp;&amp; iter &lt;= itermax){<br/>    # initiation<br/>    if(iter == 0){<br/>      km.init &lt;- km.fromscratch(X, k)<br/>      mu &lt;- km.init$centroid; mu_mem &lt;- mu<br/>      w &lt;- sapply(1:k, function(i) length(which(km.init$cluster == i)))<br/>      w &lt;- w/sum(w)<br/>      cov &lt;- array(dim = c(p, p, k))<br/>      for(i in 1:p) for(j in 1:p) for(c in 1:k) cov[i, j, c] &lt;- <br/>        1/n * sum((X[km.init$cluster == c, i] - mu[c, i]) *<br/>        (X[km.init$cluster == c, j] - mu[c, j]))<br/>    }<br/>    <br/>    # E-step<br/>    mvn.c &lt;- sapply(1:k, function(c) mvn.pdf(X, mu[c,], cov[,, c]))<br/>    r_ic &lt;- t(w*t(mvn.c)) / rowSums(t(w*t(mvn.c)))</span><span id="987d" class="nl mk it lu b gy nq nn l no np"># M-step<br/>    n_c &lt;- colSums(r_ic)<br/>    w &lt;- n_c/sum(n_c)<br/>    mu &lt;- t(sapply(1:k, function(c) 1/n_c[c] * colSums(r_ic[, c] *<br/>      X)))<br/>    for(i in 1:p) for(j in 1:p) for(c in 1:k) cov[i, j, c] &lt;- <br/>      1/n_c[c] * sum(r_ic[, c] * (X[, i] - mu[c, i]) * r_ic[, c] *<br/>      (X[, j] - mu[c, j]))</span><span id="b4fe" class="nl mk it lu b gy nq nn l no np">cluster &lt;- apply(r_ic, 1, which.max)<br/>    if(plot){<br/>      i &lt;- 1<br/>      idx &lt;- matrix(rep(seq(p), p), ncol = p, nrow = p)<br/>      idx &lt;- idx[lower.tri(idx)]<br/>      idy &lt;- matrix(rep(seq(p), each=p), ncol = p, nrow = p)<br/>      idy &lt;- idy[lower.tri(idy)]<br/>      <br/>      if(iter &lt; 10) iter4plot &lt;- paste0('0', iter) else iter4plot &lt;- iter<br/>      <br/>      png(paste0(wd, '/figs_gmm/iter', iter4plot, '.png'))<br/>      pairs(rbind(X, mu), lower.panel = NULL, asp = 1, <br/>        col = c(class_col[cluster], rep('black', k)), main =<br/>        paste0('iter=',iter), panel=function(x, y, ...) {<br/>              points(x, y, col = c(class_col[cluster], rep('black', k)))<br/>              xi &lt;- seq(min(X[, idx[i]])-1, max(X[, idx[i]])+1, 0.1)<br/>              yi &lt;- seq(min(X[, idy[i]])-1, max(X[, idy[i]])+1, 0.1)<br/>              grid &lt;- expand.grid(xi = xi, yi = yi)<br/>              grid['z'] &lt;- mvn.pdf(grid, mu[1,c(idx[i],idy[i])],<br/>                cov[c(idx[i],idy[i]),c(idx[i],idy[i]), 1])<br/>              z &lt;- cast(grid, xi ~ yi)<br/>              contour(xi, yi, as.matrix(z[,-1]), <br/>                levels = c(.1, .5, .9), col = class_col[1], <br/>                add = T, lty = 'solid', labels = '')<br/>              grid &lt;- expand.grid(xi = xi, yi = yi)<br/>              grid['z'] &lt;- mvn.pdf(grid, mu[2,c(idx[i],idy[i])], <br/>                cov[c(idx[i],idy[i]),c(idx[i],idy[i]), 2])<br/>              z &lt;- cast(grid, xi ~ yi)<br/>              contour(xi, yi, as.matrix(z[,-1]), <br/>                levels = c(.1, .5, .9), col = class_col[2], <br/>                add = T, lty = 'solid', labels = '')<br/>              grid &lt;- expand.grid(xi = xi, yi = yi)<br/>              grid['z'] &lt;- mvn.pdf(grid, mu[3,c(idx[i],idy[i])], <br/>                cov[c(idx[i],idy[i]),c(idx[i],idy[i]), 3])<br/>              z &lt;- cast(grid, xi ~ yi)<br/>              contour(xi, yi, as.matrix(z[,-1]), <br/>                levels = c(.1, .5, .9), col = class_col[3], <br/>                add = T, lty = 'solid', labels = '')<br/>              i &lt;&lt;- i+1<br/>            })<br/>      dev.off()<br/>    }<br/>    <br/>    Delta &lt;- sum((mu - mu_mem)^2)<br/>    iter &lt;- iter + 1; mu_mem &lt;- mu<br/>  }<br/>  return(list(softcluster = r_ic, cluster = cluster))<br/>}</span><span id="55be" class="nl mk it lu b gy nq nn l no np">gmm &lt;- gmm.fromscratch(X, 3, plot = T)<br/>table(y, gmm$cluster)</span><span id="16d7" class="nl mk it lu b gy nq nn l no np">library(magick)<br/>list.files(path = paste0(wd, "/figs_gmm/"), pattern = "*.png", full.names = T) %&gt;% <br/>  image_read() %&gt;%<br/>  image_join() %&gt;%<br/>  image_animate(fps=1) %&gt;%<br/>  image_write("fig_gmm_anim.gif")</span></pre></div></div>    
</body>
</html>