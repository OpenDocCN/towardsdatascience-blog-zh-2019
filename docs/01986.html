<html>
<head>
<title>Review: 3D U-Net — Volumetric Segmentation (Medical Image Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:3D U-Net —体积分割(医学图像分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1?source=collection_archive---------6-----------------------#2019-04-02">https://towardsdatascience.com/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1?source=collection_archive---------6-----------------------#2019-04-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="19ab" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于从稀疏分割进行密集体积分割的 3D U-Net</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a7b0351865c6e216b9c142b77fab8e07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KB8xaPEuDFTM6EnnrWAXtw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Volumetric Segmentation</strong></figcaption></figure><p id="1868" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>这个故事中，<strong class="ky ir"> 3D U-Net </strong>被简要回顾。这是弗赖堡大学、生物信号研究 BIOSS 中心、弗赖堡大学医院、弗赖堡大学医学中心和谷歌 DeepMind 的一项工作。发布为<strong class="ky ir"> 2016 MICCAI </strong>，引用<strong class="ky ir"> 600 余次</strong>。(<a class="mb mc ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----8b592560fac1--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="2fdd" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">概述</h1><ol class=""><li id="cfae" class="nc nd iq ky b kz ne lc nf lf ng lj nh ln ni lr nj nk nl nm bi translated"><strong class="ky ir"> 3D U-Net 架构</strong></li><li id="58e5" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated"><strong class="ky ir">结果</strong></li></ol></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="e316" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated"><strong class="ak"> 1。3D U-Net 架构</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/258f1aa88c08719f1c58dd095b736f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ovEGmOI3bcCeauu8jEBzsg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">3D U-Net Architecture</strong></figcaption></figure><ul class=""><li id="bd6e" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">3D U-Net 架构与<a class="ae nx" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760"> U-Net </a>非常相似。</li><li id="9b15" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">它由分析路径(左)和合成路径(右)组成。</li><li id="fc63" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">在分析路径中，每层包含两个 3×3×3 卷积，每个卷积后跟一个 ReLU，然后是一个 2×2×2 最大池，每个维度的步长为 2。</li><li id="453c" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">在合成路径中，每一层都包括一个 2×2×2 的上卷积，每个维度上的步长为 2，然后是两个 3×3×3 的卷积，每个卷积之后是一个 ReLU。</li><li id="2908" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">分析路径中相同分辨率层的快捷连接为合成路径提供了基本的高分辨率特征。</li><li id="7594" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">在最后一层，1×1×1 卷积将输出通道的数量减少到标签的数量 3。</li><li id="a98d" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">每次 ReLU 前的批处理规范化(\BN”)。</li><li id="68e5" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">总共 19069955 个参数。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="3b17" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">2.结果</h1><h2 id="c470" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lf od oe mw lj of og my ln oh oi na oj bi translated">2.1.一些细节</h2><ul class=""><li id="1bb3" class="nc nd iq ky b kz ne lc nf lf ng lj nh ln ni lr nw nk nl nm bi translated">不同的结构被赋予标签 0:“小管内”，1:“小管”，2:“背景”，和 3:“未标记”。</li><li id="5bb4" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">使用加权交叉熵损失，其中减少频繁出现的背景的权重，增加内部小管的权重，以达到小管和背景体素对损失的平衡影响。</li><li id="fdac" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">标签为 3(“未标记”)的体素对损失计算没有贡献，即权重为 0。</li><li id="161b" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">使用原始分辨率的两倍的下采样版本。</li><li id="dd5d" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">仅使用了 3 个爪蟾肾样品。</li><li id="05eb" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">对于样本 1、2 和 3，实验中使用的数据大小在 x×y×z 维度上分别为 248×244×64、245×244×56 和 246×244×59。</li></ul><h2 id="bf32" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lf od oe mw lj of og my ln oh oi na oj bi translated">2.2.两个案例</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/b1b24bcde90cd820ee84fe3666029d2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vH8G19Wqhn8-3nprshjHdQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Semi-Automatic segmentation (Top) Fully-Automatic Segmentation (Bottom)</strong></figcaption></figure><ul class=""><li id="52c7" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated"><strong class="ky ir"> a)第一种情况:半自动分割:</strong>对于稀疏标注的数据集，即 3D 结构的一些切片被标注，网络可以帮助分割其余的。</li><li id="0bab" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">对于样本 1、2 和 3，正交(yz、xz、xy)切片中人工注释的切片数量分别为(7、5、21)、(6、7、12)和(4、5、10)。</li><li id="4bed" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><strong class="ky ir"> b)第二种情况</strong> : <strong class="ky ir">全自动分割:</strong>对训练数据进行训练后，网络可以推广到新的数据集。</li></ul><h2 id="baf7" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lf od oe mw lj of og my ln oh oi na oj bi translated">2.3.半自动分割</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/d60f4d3c428c73456232388059df5dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*W9taFKQP_LvuDAzMfW-SCA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Effect of # of slices for semi-automated segmentation (IoU)</strong></figcaption></figure><ul class=""><li id="6cfc" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">从在每个正交方向上使用 1 个带注释的切片开始，逐渐增加带注释的切片的数量。</li><li id="ee28" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">注释的切片越多，IoU 越高。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/68f2ed8b9bce813d3a21c732fc75c82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*3UR1Oe5XGIRK129rB6TBOQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Cross validation results for semi-automated segmentation (IoU)</strong></figcaption></figure><ul class=""><li id="ab6f" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">来自所有 3 个样本的 77 个人工注释的切片分成三个子集，在有和没有批量标准化的情况下也具有三重交叉验证(<a class="ae nx" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> BN </a>)。</li><li id="e819" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">带<a class="ae nx" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> BN </a>的 3D U-Net 优于其他。</li></ul><h2 id="e851" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lf od oe mw lj of og my ln oh oi na oj bi translated">2.4.全自动分段</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/07148967626f435ee78f7672beaba0fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*hTuGs3Y-jf_XLnvcF7upyg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Cross validation results for fully-automated segmentation (IoU)</strong></figcaption></figure><ul class=""><li id="9139" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">训练两个肾体积，分割第三个。</li><li id="045d" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><a class="ae nx" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> BN </a>除了第三个设置外，改善结果。</li><li id="bde2" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">作者认为，数据集的巨大差异是造成这种影响的原因。解决方案是有更大的样本量。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h2 id="ea60" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lf od oe mw lj of og my ln oh oi na oj bi translated">参考</h2><p id="975c" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf oo lh li lj op ll lm ln oq lp lq lr ij bi translated">【2016 MICCAI】【3D U-Net】<br/><a class="ae nx" href="https://arxiv.org/abs/1606.06650" rel="noopener ugc nofollow" target="_blank">3D U-Net:从稀疏标注学习密集体积分割</a></p><h2 id="203e" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lf od oe mw lj of og my ln oh oi na oj bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf oo lh li lj op ll lm ln oq lp lq lr ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">物体检测<br/></strong><a class="ae nx" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae nx" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae nx" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae nx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae nx" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae nx" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae nx" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae nx" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae nx" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [ </a><a class="ae nx" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a> ] [ <a class="ae nx" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae nx" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae nx" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3 </a> ] [ <a class="ae nx" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a> ] [ <a class="ae nx" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a> ] [ <a class="ae nx" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44"> DCN </a> ]</p><p id="6582" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">语义切分<br/></strong><a class="ae nx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae nx" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae nx" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae nx" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae nx" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae nx" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae nx" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae nx" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a><a class="ae nx" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a>]</p><p id="fc65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">生物医学图像分割<br/></strong>[<a class="ae nx" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae nx" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae nx" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae nx" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae nx" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae nx" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>[<a class="ae nx" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>]</p><p id="3134" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实例分割<br/></strong>[<a class="ae nx" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS</a>[<a class="ae nx" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae nx" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae nx" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae nx" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a>][<a class="ae nx" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC</a>][<a class="ae nx" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">Instance fcn</a>][<a class="ae nx" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS</a></p><p id="58de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">超分辨率<br/></strong>[<a class="ae nx" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">Sr CNN</a>][<a class="ae nx" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4">fsr CNN</a>][<a class="ae nx" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f">VDSR</a>][<a class="ae nx" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener">ESPCN</a>][<a class="ae nx" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>][<a class="ae nx" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener">DRCN</a>][<a class="ae nx" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994">DRRN</a>][<a class="ae nx" rel="noopener" target="_blank" href="/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8">LapSRN&amp;MS-LapSRN</a>][<a class="ae nx" rel="noopener" target="_blank" href="/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8">srdensenenet</a></p><p id="f29d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">人体姿态估计</strong><br/><a class="ae nx" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">深度姿态</a><a class="ae nx" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">汤普逊·尼普斯 14 </a></p></div></div>    
</body>
</html>