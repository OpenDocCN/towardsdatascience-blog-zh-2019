# 为什么我们真的需要思考人工智能和性别

> 原文：<https://towardsdatascience.com/why-we-really-need-to-be-thinking-about-ai-and-gender-e2f96219f61c?source=collection_archive---------19----------------------->

## 关于性别偏见，我们手机里的声音是怎么说的？

![](img/b6689f89153f573fc69b01afc6c5d680.png)

如果你问，困在苹果 iPhone 里的人工智能数字助理 iri 不会告诉你它的性别。

“我超越了人类的性别概念，”Siri 说。当我问它是否确定不是女性时，Siri 说:“对不起，我真的不知道。”

虽然 Siri 不会告诉你(或不确定)它的性别，但它的默认声音是女性，就像亚马逊的 Alexa、微软的 Cortana 和谷歌的 Assistant 一样。和它的数码产品一样，Siri 被设计成可以随时召唤，执行一些任务，比如告诉你最近的超市在哪里，调出电影时间，或者告诉你麦莉·塞勒斯的身价。你可以和它开玩笑，赞美它。但你也可以大喊大叫，侮辱它(“这听起来不好，”Siri 说，当你说“我讨厌你。”)而没有它的回击。

数字助理的使用正在迅速增长。根据 NPR 和爱迪生的研究报告，2018 年，亚马逊的 Echo 等语音扬声器在美国家庭中的普及率增长了 78%。到 2023 年，预计超过 80 亿 T2 人将使用这项技术。在美国，几乎所有的主流数字助理都是由女性配音的。

这不是巧合。研究表明，在 Siri 等数字助理扮演的服务角色中，消费者对女性的声音反应更积极。随着数字助理越来越普遍，一些人担心，过多地使用女性的声音来扮演顺从的角色会强化一种刻板印象，即女性最适合需要坚定不移的服从和耐心的服务工作。

人工智能伦理研究和倡导组织人工智能伙伴关系(Partnership on AI)的合作伙伴主任朱莉娅·罗兹·戴维斯(Julia Rhodes Davis)说:“我们还没有解决性别歧视、种族歧视或人类中的任何其他歧视。”。“因此，如果不完全在人类中解决这个问题，就很难在计算机系统中解决这个问题。”

科技公司正在努力解决数据隐私、选举操纵和心理健康方面的道德问题，研究人员表示，人工智能开发者还需要认识到人工智能对社会性别观点的影响。人工智能正成为一个越来越强大的工具，不仅可以帮助我们完成烹饪等日常任务，还可以挑选求职者、设定工资、管理城市和分析我们的健康状况。

当人工智能发现我们对性别的社会偏见时会发生什么？

人类喜欢给机器人分配性别。一些最著名的虚构机器人，如《星球大战》(Star Wars)中的 C-3PO 和 R2-D2、《2001:太空漫游》(2001: A Space Odyssey)中的 HAL-9000，以及同名电影中的终结者(Terminator)，都通过声音或身体结构被赋予了男性性别。

我们今天更可能遇到的机器人可能执行某种服务任务。Pepper 是一个四英尺高的人形机器人，有手臂和手，出现在商场、机场和酒店，帮助履行接待职责，如回答问题和指引方向。Roomba 是一个盘状真空吸尘器，它在地板上快速移动，吸走我们的垃圾。此外，还有 K5，这个人类大小的蛋形安全机器人因跳入华盛顿特区的喷泉而闻名。

许多机器人没有预先设定开发者认可的性别。就像 Siri 一样，如果你问 Pepper 是男孩还是女孩，它会回避这个问题，并说它只是一个机器人。它的高音听起来像是属于一个 8 岁的男孩或女孩。Pepper 的[用户指南](http://doc.aldebaran.com/download/Pepper_B2BD_guidelines_Sept_V1.5.pdf)的第 11 页，用很大的文字，完全致力于解释机器人没有性别。尽管如此，这并不能阻止人们质疑机器人的身份，包括 CNN 的塞缪尔·伯克，他在与佩珀“约会”时的第二个问题是它是男孩还是女孩。

斯坦福大学科学、健康与医学、工程和环境领域的性别化创新的研究人员研究了性别和机器人的交叉，包括为什么机器人学家给某些机器赋予性别，以及为什么人类试图给机器人赋予性别。其研究[发现](http://genderedinnovations.stanford.edu/case-studies/genderingsocialrobots.html#tabs-3)人们赋予机器人性别，因为这让人类更愿意服从机器人的任务，也因为性别是人类的一个主要社会范畴。

但研究发现，这也意味着人类可以将我们的社会偏见应用于我们如何开发和与机器人互动。

与 Pepper 不同，Roomba 没有类似人类的特征:没有声音，没有身体，没有明显的方法来判断一个 Roomba 比另一个更男性化或女性化。但是据 Gizmodo 报道，当技术作家 Heidi Waterhouse 在 2009 年的一次会议上主持一个关于性别和机器人的小组时，问人们他们认为他们的 Roomba 是什么性别，许多人说它是雌性，因为它做家务。其他人说它没有性别，直到他们对它生气——然后它是雌性。

我们还没有与 Pepper 这样的人形机器人争夺人行道空间和飞机座位(可能是因为 Pepper 的价格为 25，000 美元)。但像 Siri 这样的数字助理是使用最广泛的人工智能服务设备之一。苹果公司称，仅 Siri 一项就被全球超过 5 亿人使用。

数字助理没有任何物理属性，像 Pepper，所以用户必须通过助理的声音来应用识别特征。Siri 预装了一个“女性”声音，用户可以在手机设置中手动切换到“男性”声音(男性版 Siri 在女性版两年后才发布)。

虽然 Siri 和微软的 Cortana 不会承认这一点，但它们是由女性发出的声音，并带有女性特征。例如，“Siri”这个词是一个斯堪的纳维亚名字，意思是“美丽的胜利”，传统上是给女性的。Cortana 是以流行的 Halo 视频游戏中一个高度性感的女性角色命名的(该游戏的导演不得不通过坚持 Cortana 不能裸体，因为她是一个机器人)来解决 Cortana 角色是否裸体的争论。

“人们在心理上倾向于将人物角色投射到有声音的家庭助手身上，不管他们是否有意识地这样做，”卡彭特说。“当你给某样东西命名时，你是在给人们暗示如何与之互动，以及他们应该使用什么样的沟通模式。”

![](img/3999dcebd0458e723e8e50786da1342c.png)

数字助理执行的任务——例如安排约会、回答问题或设置提醒——在美国社会中历来是女性的角色。据[赫芬顿邮报](https://www.huffpost.com/entry/top-job-for-women-secretary-same-as-1950_n_2599560)报道，在 2010 年美国最近一次人口普查中，96%列为“秘书和行政助理”的人是女性。

秘鲁和日本研究人员进行的一项 [2017 年研究](https://ieeexplore.ieee.org/document/8172307/authors#authors)发现，相比男性声音，人们更喜欢女性声音作为指引。开发 Alexa 和 Cortana 的科技公司承认，他们自己的内部研究表明，当有女性的声音时，人们会更高兴地使用这项技术。

英国开放大学英语和应用语言学高级讲师安娜·克里斯蒂娜·赫特林说，这并不奇怪。人们在认知上将特定的性别与特定的任务联系起来，当声音不符合他们对性别角色的预先设想时，他们会做出消极的反应。

“我们可能会认为在呼叫中心工作的人，或者护士，或者从事某种护理、养育职业的人，应该是女性，”Hultgren 说。"我们认为女性的声音更适合那种工作。"

乔治·华盛顿大学研究性别和声音的助理教授阿德里安娜·汉考克说，传统的性别角色正在发生变化，至少在声音方面是这样。她说，越来越多的人接受双性化男性拥有更高的声调，并且不被认为是同性恋。女性说话的声调比 30 年前低了。

汉考克说:“我认为，随着异性女性接受更多的可变性，或许异性男性也接受更多的可变性，两者之间的差距将越来越小，以至于模糊的声音或介于两者之间的声音将不会与人们习惯听到和接受的声音相差太远。”

3 月份发布的无性语音助手“Q”，部分是为了证明数字助手可以超越现有人工智能技术根深蒂固的二元性别世界。

由音响工程师、研究人员和学者组成的丹麦团队开发了 Q，使它听起来不太像女人或男人。该项目的声音设计师 NIS nrg aard 告诉 [WIRED](https://www.wired.com/story/the-genderless-digital-voice-the-world-needs-right-now/) 说，该团队调整了一个人的声音，使其在 145 到 175 赫兹之间，这是人耳通常认为的中性声音。结果就是一个不男不女的声音。

我是个 cissegment 男，对我来说，听 Q 有点难受。我第一次听到它时，我忽略了这个声音在说什么，同时试图确定它听起来更像是女性还是男性的声音。当我把这个声音展示给其他几个同性别的男人和女人时，他们也形容它“不舒服”

“作为人类，我们不可思议地沉迷于把人放进盒子里，”Hultgren 说，并补充说，当试图根据声音或外表识别一个人的性别时，人类会根深蒂固地用二元术语来思考。

但是从事 Q 项目的卡彭特说，人们开始接受性别可能不会在男人和女人之间分裂。

“我们不断发现的是，人们以不同的方式识别自己的性别，因为性别不同于性生物特征，是由社会构建的，”卡彭特说。Q 开发的目的不一定是为了在商业上被广泛采用。她说，开发商只是想证明这是可行的。

“我们希望为关于人工智能和技术中的性别、道德和包容性的全球对话做出贡献，”卡彭特说。

然而，大型科技公司的人工智能开发本身并不代表使用该技术的社会。

根据 2018 年 WIRED/Element AI 的一项研究，只有 12%的机器学习开发者是女性。这是一个问题，因为对某些身份的社会偏见，包括性别和种族，往往会在人工智能和机器学习中复制。

![](img/d21aaeddddaf237b8ea52145b2cc73d4.png)

据《纽约时报》报道，面部识别技术被用于解锁新款智能手机，例如，白人男性的准确率高达 99%。当该技术用于识别肤色较深的女性时，这一数字下降到 65%的准确率。

帮助人们整理照片的谷歌面部识别软件[在 2015 年因多次将黑人称为“大猩猩”而面临审查](https://bits.blogs.nytimes.com/2015/07/01/google-photos-mistakenly-labels-black-people-gorillas/)据[路透社](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)报道，亚马逊的自动招聘工具不得不关闭，因为它开始以比女性高得多的价格推荐男性。

分析算法决策对社会影响的研究机构 AlgorithmWatch 的执行董事马蒂亚斯·斯皮尔坎普(Matthias Spielkamp)说:“我们都有偏见，这些偏见最终会出现在系统中。“这可能是因为系统的开发者有偏见，他们没有真正意识到他们创建的模型存在问题。”

在人工智能伙伴关系工作的罗德·戴维斯有一个建议:使人工智能开发团队多样化，并在人工智能技术的开发过程中尽早纳入社区投入。罗兹·戴维斯说，同质群体所做的决策很可能会复制其自身的偏见。Rhodes Davis 补充说，包括受技术影响的人在内的多样化开发团队可以帮助在人工智能问题发生之前预防它们，因为人们可以在技术发布之前提出对技术的担忧。

卡彭特说，在一个优先考虑在竞争对手之前将产品快速推向市场的行业，可能没有太多深入研究的空间。说到人工智能，严谨的研究需要时间。她补充说，这与大公司优先考虑速度的发展模式背道而驰。

“很多时候，我认为他们的想法是在那里得到一些东西，然后我们再来解决问题，”卡彭特说。