<html>
<head>
<title>How to assess a binary Logistic Regressor with scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用 scikit-learn 评估二元逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-interpret-a-binary-logistic-regressor-with-scikit-learn-6d56c5783b49?source=collection_archive---------14-----------------------#2019-04-17">https://towardsdatascience.com/how-to-interpret-a-binary-logistic-regressor-with-scikit-learn-6d56c5783b49?source=collection_archive---------14-----------------------#2019-04-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4c24" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为这个 python 函数添加书签，它使评估您的二进制分类器变得容易。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ec61bef0dd0164eb4c38371a8e57ef82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0HmSnLirg_zQaAJdVgENw.png"/></div></div></figure><h1 id="961b" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">功能概述</h1><p id="4e38" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">逻辑回归因其可解释性而成为一个有价值的分类器。这个代码片段提供了一个剪切和粘贴功能，当逻辑回归用于二元分类问题时，该功能显示了重要的指标。这里的一切都是由 scikit-learn 提供的，但是如果没有这个助手函数，手动调用和可视化会非常耗时和重复。</p><p id="005b" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir"><em class="mk">【evalBinaryClassifier()</em></strong>将拟合的模型、测试特征和测试标签作为输入。它返回 F1 分数，并打印密集输出，包括:</p><ul class=""><li id="c344" class="ml mm iq ll b lm mf lp mg ls mn lw mo ma mp me mq mr ms mt bi translated">标有数量和文本标签的完整混淆矩阵(例如“真阳性”)</li><li id="1558" class="ml mm iq ll b lm mu lp mv ls mw lw mx ma my me mq mr ms mt bi translated">两类预测概率的分布</li><li id="e3f4" class="ml mm iq ll b lm mu lp mv ls mw lw mx ma my me mq mr ms mt bi translated">ROC 曲线，AUC，以及沿着混淆矩阵和分布所代表的曲线的决策点</li><li id="f1c4" class="ml mm iq ll b lm mu lp mv ls mw lw mx ma my me mq mr ms mt bi translated">精确度、召回率和 F1 分数</li></ul><p id="8086" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">有关如何解释这些输出的说明，请跳到代码块后面的。</p><h2 id="b2d9" class="mz ks iq bd kt na nb dn kx nc nd dp lb ls ne nf ld lw ng nh lf ma ni nj lh nk bi translated">必需的进口:</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h2 id="ddbe" class="mz ks iq bd kt na nb dn kx nc nd dp lb ls ne nf ld lw ng nh lf ma ni nj lh nk bi translated"><em class="nn">例句用法:</em></h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h2 id="0e18" class="mz ks iq bd kt na nb dn kx nc nd dp lb ls ne nf ld lw ng nh lf ma ni nj lh nk bi translated">输出:</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/a937f8ec9d0d737d2775459c8921948a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s3swo5pQ0Ia0t_Im8GGZwg.png"/></div></div></figure><h1 id="af51" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">代码</h1><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="c613" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">解释:</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/a937f8ec9d0d737d2775459c8921948a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s3swo5pQ0Ia0t_Im8GGZwg.png"/></div></div></figure><h2 id="4dd0" class="mz ks iq bd kt na nb dn kx nc nd dp lb ls ne nf ld lw ng nh lf ma ni nj lh nk bi translated">左图:混淆矩阵</h2><p id="0aaa" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">混淆矩阵将模型做出的预测描述为正确或错误。它将这些与真实的真相进行比较。一个完美的模型只有真正的正面和真正的负面。一个完全随机的模型将有相似数量的所有 4 个类别。</p><p id="4110" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">如果你有一个阶级不平衡的问题，通常你会看到很多负面的(真的和假的)和很少正面的，反之亦然。(关于这方面的一个很好的例子，请看我关于<a class="ae np" href="https://www.gregcondit.com/projects/disrupt" rel="noopener ugc nofollow" target="_blank">预测颠覆</a>的项目。)</p><h2 id="6576" class="mz ks iq bd kt na nb dn kx nc nd dp lb ls ne nf ld lw ng nh lf ma ni nj lh nk bi translated">中心图:预测的分布</h2><p id="1f9b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">中间的图表是正面结果的预测概率分布。例如，如果您的模型 100%确定样本为阳性，则它将位于最右侧的箱中。两种不同的颜色表示真实的类，而不是预测的类。一个完美的模型将显示绿色和红色分布之间没有任何重叠。一个完全随机的模型将会看到它们彼此完全重叠。</p><p id="04ef" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">决策边界</strong>决定模型的最终预测。在 scikit-learn 中，默认的<strong class="ll ir">决策边界</strong>是. 5；也就是说，高于 0.5 的预测为 1(正)，低于 0.5 的预测为 0(负)。这是理解你的模型和 ROC 曲线的一个重要细节。</p><h2 id="04a6" class="mz ks iq bd kt na nb dn kx nc nd dp lb ls ne nf ld lw ng nh lf ma ni nj lh nk bi translated">右图:ROC 曲线</h2><p id="42c5" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">接收机工作特性曲线描述了<em class="mk">所有可能的</em> <strong class="ll ir">决策界限</strong>。绿色曲线表示可能性，以及不同决策点的真阳性率和假阳性率之间的权衡。这两个极端很容易理解:您的模型可以对所有样本延迟预测 1，并获得完美的真阳性率，但它也会有 1 的假阳性率。类似地，你可以通过懒惰地预测所有事情都是负面的来将你的假阳性率降低到零，但是你的真阳性率也将是零。你的模型的价值在于它增加真阳性率<em class="mk">比增加假阳性率</em>更快的能力。</p><p id="6ea1" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">一个完美的模型应该是 y 轴上的一条垂直线(100%真阳性，0%假阳性)。一个完全随机的模型将位于蓝色虚线上(找到更多的真阳性意味着找到相同数量的假阳性)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/e49e84600fc25153442082403ae5eac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H0OAYhPFOi6tH2opbmOa4w.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk">Left: a perfect model; Right: a purely random model</figcaption></figure><p id="faa1" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">蓝点代表当前确定混淆矩阵的 0.5 决策边界。</p><p id="579a" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">当一种错误类型比另一种更糟糕时，更改这一点是调整模型敏感度的有效方法。举个例子，医疗保健充满了这样的决定:错误地诊断癌症要比错误地诊断健康好得多。在这种情况下，我们需要一个非常低的决策边界，也就是说，只有在我们非常确定的情况下，才预测一个阴性结果(没有癌症)。</p><p id="405d" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">如果使用相同的模型选择不同的边界(例如:0.3 而不是 0.5)，蓝点将沿着绿色曲线向右上方移动。新的边界意味着我们将捕获更多的真阳性，也将捕获更多的假阳性。这也很容易想象为中间图表中的蓝线向左移动，直到它位于 0.3:边界右侧会有更多的“绿色”箱，但也会有更多的“红色”箱。</p><p id="db7b" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">scikit-learn 没有调整决策边界的内置方法，但这可以通过对您的数据调用<strong class="ll ir"> <em class="mk"> predict_proba() </em> </strong>方法来轻松完成，然后根据您选择的边界手动编码一个决策。</p><p id="49d6" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><em class="mk">这对你理解你的模型有帮助吗？还能改进吗？请告诉我，我很乐意收到你的来信！</em></p></div></div>    
</body>
</html>