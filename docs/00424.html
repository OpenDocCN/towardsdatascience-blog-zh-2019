<html>
<head>
<title>Back Propagation, the Easy Way (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播，简单的方法(第 2 部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/back-propagation-the-easy-way-part-2-bea37046c897?source=collection_archive---------7-----------------------#2019-01-19">https://towardsdatascience.com/back-propagation-the-easy-way-part-2-bea37046c897?source=collection_archive---------7-----------------------#2019-01-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="37e1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">反向传播的实际实现</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1061245b7af2b26566ce6e638336203e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQ8yYq4PiC49TTbMS_wD7Q.jpeg"/></div></div></figure><p id="66cf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">更新</strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae ln" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="881e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在<a class="ae ln" rel="noopener" target="_blank" href="/back-propagation-the-easy-way-part-1-6a8cde653f65">第一部分</a>中，我们已经看到反向传播是如何以最小化成本函数的方式导出的。在本文中，我们将看到实现方面，以及一些避免常见陷阱的最佳实践。</p><p id="e93b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们仍然处于简单模式，一次处理一个输入。</p><h2 id="4ce4" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">图层类别</h2><p id="a764" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">考虑下图所示的全连接神经网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/728553a46ba5b5c6a4c18b92f699faac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KA6omkrxNxIhZLQDuB1fcw.png"/></div></div></figure><p id="0e9c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">每个层将由包含权重、激活值(层的输出)、梯度 dZ(图像中未示出)、累积误差δ(𝚫)、以及激活函数<strong class="kt ir"> <em class="mn"> f(x) </em> </strong>及其导数<strong class="kt ir"><em class="mn">f’(x)</em></strong>的层对象来建模。存储中间值的原因是为了避免每次需要时都要计算它们。</p><p id="32ef" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">建议:</strong>最好围绕几个类来组织代码，避免把所有东西都塞进数组，因为很容易丢失。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/755921a64729226da58daff5a856629f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nk5j1fcaD7jTRgRNWK7sgg.png"/></div></div></figure><p id="0612" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请注意，输入图层不会由图层对象表示，因为它只包含一个矢量。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="7729" class="lo lp iq mq b gy mu mv l mw mx"><strong class="mq ir">class </strong>Layer:<br/><br/>    <strong class="mq ir">def </strong>__init__(self, dim, id, act, act_prime, <br/>                 isoutputLayer = <strong class="mq ir">False</strong>):<br/>        self.weight = 2 * np.random.random(dim) - 1<br/>        self.delta = <strong class="mq ir">None<br/>        </strong>self.A = <strong class="mq ir">None<br/>        </strong>self.activation = act<br/>        self.activation_prime = act_prime<br/>        self.isoutputLayer = isoutputLayer<br/>        self.id = id</span></pre><p id="5e4b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Layer 类的构造函数将以下内容作为参数:</p><ul class=""><li id="7f2b" class="my mz iq kt b ku kv kx ky la na le nb li nc lm nd ne nf ng bi translated">dim:权重矩阵的维数，</li><li id="4170" class="my mz iq kt b ku nh kx ni la nj le nk li nl lm nd ne nf ng bi translated">id:整数作为层的 id，</li><li id="d555" class="my mz iq kt b ku nh kx ni la nj le nk li nl lm nd ne nf ng bi translated">act，act_prime:激活函数及其导数，</li><li id="9500" class="my mz iq kt b ku nh kx ni la nj le nk li nl lm nd ne nf ng bi translated">isoutputlayer:如果该层是输出，则为 True，否则为 False。</li></ul><p id="820e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">它将权重随机初始化为-1 到+1 之间的数字，并设置要在对象内部使用的不同变量。</p><p id="50c4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">图层对象有三种方法:</p><ul class=""><li id="6f6a" class="my mz iq kt b ku kv kx ky la na le nb li nc lm nd ne nf ng bi translated">向前，计算层输出。</li><li id="f087" class="my mz iq kt b ku nh kx ni la nj le nk li nl lm nd ne nf ng bi translated">向后，将目标和输出之间的误差传播回网络。</li><li id="69e3" class="my mz iq kt b ku nh kx ni la nj le nk li nl lm nd ne nf ng bi translated">更新，根据梯度下降更新权重。</li></ul><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="8995" class="lo lp iq mq b gy mu mv l mw mx"><strong class="mq ir">def </strong>forward(self, x):<br/>    z = np.dot(x, self.weight)<br/>    self.A = self.activation(z)<br/>    self.dZ = self.activation_prime(z);</span></pre><p id="ae3e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">forward 函数通过输入<strong class="kt ir"> x </strong>计算并返回层的输出，并计算和存储输出 A = activation (W.X)。它还计算并存储 dZ，即输出相对于输入的导数。</p><p id="b859" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">反向函数采用两个参数，目标 y 和 rightLayer，即假设当前层是𝓁.的层(𝓁-1)</p><p id="1c5b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">它计算从输出向左传播到网络起点的累积误差增量。</p><p id="0296" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">重要提示</strong>:一个常见的错误是认为反向传播是某种环回，其中输出被再次注入网络。所以不用<strong class="kt ir"><em class="mn">dZ = self . activation _ prime(z)；</em> </strong>有的用途<strong class="kt ir"> <em class="mn"> self.activation_prime(一)</em> </strong> <em class="mn">。</em>这是错误的，因为我们要做的只是计算出输出 a 相对于输入 z 的变化，这意味着根据链式法则计算导数<strong class="kt ir">∂a/∂z</strong>= ∂g(z)/∂z =<strong class="kt ir">g’(z)</strong>。<strong class="kt ir"> <br/> </strong>这个误差可能是因为在 sigmoid 激活函数<strong class="kt ir"> a = 𝜎(z) </strong>的情况下，导数<strong class="kt ir">𝜎'(z)= 𝜎(z)*(1-𝜎(z)= a *(1-a)。</strong>这给人一种输出被注入网络的错觉，而事实是我们正在计算<strong class="kt ir"> 𝜎'(z).</strong></p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="e835" class="lo lp iq mq b gy mu mv l mw mx"><strong class="mq ir">def </strong>backward(self, y, rightLayer):<br/>    <strong class="mq ir">if </strong>self.isoutputLayer:<br/>        error =  self.A - y<br/>        self.delta = np.atleast_2d(error * self.dZ)<br/>    <strong class="mq ir">else</strong>:<br/>        self.delta = np.atleast_2d(<br/>            rightLayer.delta.dot(rightLayer.weight.T)<br/>            * self.dZ)<br/>    <strong class="mq ir">return </strong>self.delta</span></pre><p id="1809" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">backward 函数的作用是根据以下公式计算并返回增量:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/80833e999934d3145cdb64d25f7175ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9QFZMyhzz-wP5VjnFSCFQ.png"/></div></div></figure><p id="d84e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，更新函数使用梯度下降来更新当前层的权重。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="f2dc" class="lo lp iq mq b gy mu mv l mw mx"><strong class="mq ir">def </strong>update(self, learning_rate, left_a):<br/>    a = np.atleast_2d(left_a)<br/>    d = np.atleast_2d(self.delta)<br/>    ad = a.T.dot(d)<br/>    self.weight -= learning_rate * ad</span></pre><h2 id="283b" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">神经网络类</h2><p id="7d50" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">正如人们可能猜测的那样，层形成了一个网络，因此类 NeuralNetwork 用于组织和协调层。<br/>它的构造器采用层的配置，这是一个长度决定网络层数的数组，每个元素定义相应层中的节点数。<br/>例如[2，4，5，]表示网络有 4 层，输入层有 2 个节点，接下来的隐藏层分别有 4 个和 5 个节点，输出层有 1 个节点。第二个参数是用于所有层的激活函数的类型。</p><p id="5aa3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">fit 函数是所有训练发生的地方。它首先选择一个输入样本，计算所有层上的前向，然后计算网络输出和目标值之间的误差，并通过以相反的顺序调用每层的反向函数(从最后一层开始到第一层)将该误差传播到网络。<br/>最后，为每一层调用更新函数来更新权重。</p><p id="2c3b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这些步骤重复的次数由参数 epoch 确定。</p><p id="36bd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">训练完成后，可以调用预测函数来测试输入。预测功能只是整个网络的一个前馈。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="bbab" class="lo lp iq mq b gy mu mv l mw mx"><strong class="mq ir">class </strong>NeuralNetwork:<br/><br/>    <strong class="mq ir">def </strong>__init__(self, layersDim, activation=<strong class="mq ir">'tanh'</strong>):<br/>        <strong class="mq ir">if </strong>activation == <strong class="mq ir">'sigmoid'</strong>:<br/>            self.activation = sigmoid<br/>            self.activation_prime = sigmoid_prime<br/>        <strong class="mq ir">elif </strong>activation == <strong class="mq ir">'tanh'</strong>:<br/>            self.activation = tanh<br/>            self.activation_prime = tanh_prime<br/>        <strong class="mq ir">elif </strong>activation == <strong class="mq ir">'relu'</strong>:<br/>            self.activation = relu<br/>            self.activation_prime = relu_prime<br/><br/>        self.layers = []<br/>        <strong class="mq ir">for </strong>i <strong class="mq ir">in </strong>range(1, len(layersDim) - 1):<br/>            dim = (layersDim[i - 1] + 1, layersDim[i] + 1)<br/>            self.layers.append(Layer(dim, i, self.activation, self.activation_prime))<br/><br/>        dim = (layersDim[i] + 1, layersDim[i + 1])<br/>        self.layers.append(Layer(dim, len(layersDim) - 1, self.activation, self.activation_prime, <strong class="mq ir">True</strong>))</span><span id="ebb2" class="lo lp iq mq b gy nn mv l mw mx"># train the network<br/>    <strong class="mq ir">def </strong>fit(self, X, y, learning_rate=0.1, epochs=10000):<br/>        <em class="mn"># Add column of ones to X<br/>        # This is to add the bias unit to the input layer<br/>        </em>ones = np.atleast_2d(np.ones(X.shape[0]))<br/>        X = np.concatenate((ones.T, X), axis=1)<br/><br/><br/>        <strong class="mq ir">for </strong>k <strong class="mq ir">in </strong>range(epochs):<br/><br/>            i = np.random.randint(X.shape[0])<br/>            a = X[i]<br/><br/>            <em class="mn"># compute the feed forward<br/>            </em><strong class="mq ir">for </strong>l <strong class="mq ir">in </strong>range(len(self.layers)):<br/>                a = self.layers[l].forward(a)<br/><br/><br/>            <em class="mn"># compute the backward propagation<br/>            </em>delta = self.layers[-1].backward(y[i], <strong class="mq ir">None</strong>)<br/><br/>            <strong class="mq ir">for </strong>l <strong class="mq ir">in </strong>range(len(self.layers) - 2, -1, -1):<br/>                delta = self.layers[l].backward(delta, self.layers[l+1])<br/><br/><br/>            <em class="mn"># update weights<br/>            </em>a = X[i]<br/>            <strong class="mq ir">for </strong>layer <strong class="mq ir">in </strong>self.layers:<br/>                layer.update(learning_rate, a)<br/>                a = layer.A</span><span id="58d8" class="lo lp iq mq b gy nn mv l mw mx"># predict input<br/>    <strong class="mq ir">def </strong>predict(self, x):<br/>        a = np.concatenate((np.ones(1).T, np.array(x)), axis=0)<br/>        <strong class="mq ir">for </strong>l <strong class="mq ir">in </strong>range(0, len(self.layers)):<br/>            a = self.layers[l].forward(a)<br/>        <strong class="mq ir">return </strong>a</span></pre><h2 id="678a" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">运行网络</h2><p id="e372" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">为了运行网络，我们以 Xor 函数的近似为例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/bdf56f3643ce4cbeec7299b83171cf1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/0*c7DIqgCII-oAn4F7.png"/></div></figure><p id="c8d2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们尝试了几种网络配置，使用不同的学习速率和历元迭代。结果如下所示:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="f7d4" class="lo lp iq mq b gy mu mv l mw mx"><br/>Result with tanh<br/>[0 0] [-0.00011187]<br/>[0 1] [ 0.98090146]<br/>[1 0] [ 0.97569382]<br/>[1 1] [ 0.00128179]</span><span id="92be" class="lo lp iq mq b gy nn mv l mw mx">Result with sigmoid<br/>[0 0] [ 0.01958287]<br/>[0 1] [ 0.96476513]<br/>[1 0] [ 0.97699611]<br/>[1 1] [ 0.05132127]</span><span id="6182" class="lo lp iq mq b gy nn mv l mw mx">Result with relu<br/>[0 0] [ 0.]<br/>[0 1] [ 1.]<br/>[1 0] [ 1.]<br/>[1 1] [ 4.23272528e-16]</span></pre><p id="0569" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">建议您尝试不同的配置，自己看看哪种配置能提供最佳和最稳定的结果。</p><h2 id="80d4" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">源代码</h2><p id="5bb6" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">完整的代码可以从<a class="ae ln" href="https://gist.github.com/ZSalloum/54703842f8a06e38fd76934579a6c814" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><h2 id="94b5" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">结论</h2><p id="5638" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">反向传播可能会令人困惑并且难以实现。你可能会有一种错觉，认为你通过理论掌握了它，但事实是，当实施它时，很容易陷入许多陷阱。你应该有耐心和毅力，因为反向传播是神经网络的基石。</p><h2 id="9c0e" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">相关文章</h2><p id="e9d6" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">第一部分:<a class="ae ln" rel="noopener" target="_blank" href="/back-propagation-the-easy-way-part-1-6a8cde653f65">反向传播的简单详解</a> <br/>第三部分:<a class="ae ln" href="https://medium.com/@zsalloum/back-propagation-the-easy-way-part-3-cc1de33e8397" rel="noopener">如何处理矩阵的维数</a></p></div></div>    
</body>
</html>