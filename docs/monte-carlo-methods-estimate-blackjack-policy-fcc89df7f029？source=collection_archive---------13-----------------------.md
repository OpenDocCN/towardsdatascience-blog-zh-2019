# 强化学习—评估 21 点策略

> 原文：<https://towardsdatascience.com/monte-carlo-methods-estimate-blackjack-policy-fcc89df7f029?source=collection_archive---------13----------------------->

## 蒙特卡洛模拟简介

我们已经讨论了几个强化学习问题，在每个问题中，我们都试图通过继续玩游戏和更新我们的估计来获得最优策略。本质上，我们估计`(state, value)`对或`(state, action, value)`对，并基于这些估计，通过采取给出最高值的动作来生成策略。但这不是唯一的方法。

![](img/bb4b9b5fa31912260521b58ce24e5927.png)

在这一段，我将介绍 [*蒙特卡洛法*](https://en.wikipedia.org/wiki/Monte_Carlo_method) ，这是另一种估算一个国家价值，或者一项政策价值的方法。蒙特卡洛方法涉及广泛的方法，但所有的**都遵循相同的原则——抽样**。这个想法简单而直观，如果你不确定一个状态的值，就做采样，就是不断地访问那个状态，并对通过与环境互动的模拟行为得到的回报进行平均。

> "蒙特卡罗方法是解决基于平均样本回报的强化学习问题的方法."—萨顿，强化学习导论

# 我们的 21 点规则

为了更好地理解如何利用蒙特卡罗方法进行强化学习，让我们来看一个来自 Sutton 的书中的例子。

## 一般规则

我们今天玩的游戏是广为人知的赌场纸牌游戏[21 点](https://en.wikipedia.org/wiki/Blackjack)，我们的目标是使用抽样来估计策略。以下是规则的简要说明:

*游戏从发给庄家和玩家的两张牌开始。庄家的一张牌面朝上，另一张面朝下。如果玩家立即有 21(一张 a 和一张 10 的牌)，这叫自然牌。然后他赢了，除非庄家也有一个自然牌，在这种情况下，游戏是平局。如果玩家没有自然牌，那么他可以一张接一张地要求额外的牌(命中)，直到他停止(坚持)或超过 21(破产)。如果他破产了，他就输了；如果他坚持，那么就轮到庄家了。庄家根据固定的策略不加选择地打或坚持:他坚持任何 17 或更大的和，否则就打。* *如果庄家破产，那么玩家获胜；否则，结果——赢、输或平——取决于谁的最终总和更接近 21。如果玩家拿着一张可以算作 11 而不会破产的王牌，那么这张王牌就可以用了。*

## 我们的政策

***我们的政策是，如果我们卡上的金额是 20 或 21，就坚持，否则就给他*** *。为了通过蒙特卡罗方法找到该策略的状态值函数，我们将使用该策略模拟许多二十一点游戏，并对每个状态的回报进行平均。*

关键是一个庄家的政策是固定的，坚持在 sum ≥ 17，我们准备模拟很多二十一点游戏来衡量 sum ≥ 20 的政策坚持。

# 21 点蒙特卡罗实现

## 州

为了实现我们的模拟，让我们首先明确状态。在 21 点中，我们需要考虑的状态包括:首先，**玩家的牌和**，其范围从 12 到 21(我们排除低于 12 的牌和，因为在这些情况下我们总是会击中)；其次，**庄家的出牌**，因为它可能会指示庄家的最终牌值；最后，**玩家可用的王牌**，这也是会稍微影响我们获胜机会的一个因素。所以加起来我们总共有`10(from 12 to 21) * 10(from 1 to 10) * 2(true or false) = 200`个状态。([完整代码此处](https://github.com/MJeremy2017/RL/blob/master/BlackJack/blackjack_mc.py))

## 初始化

由于我们的目标是在我们的固定政策下估计各州的价值，我们将需要一个`(state, value)`字典来记录所有的州和我们游戏模拟中获胜的数量，还需要一个`player_states`来跟踪每场游戏的状态。

`player_win`和`player_draw`用于跟踪游戏的总胜率。

## 交易卡

所需的基本机制是随机给每一方一张牌，在这种情况下，我们假设我们有无限张牌。

(J，Q，K 取 10)

## 经销商政策

我们的发牌政策是当牌值少于 17 时打，当 17 或以上时站。该功能将能够返回一个新的价值的基础上选择的行动，并能够告诉如果游戏结束。

这个函数可以递归调用，直到到达它的结尾，因为它的返回与输入相同。我们跟踪可用的 ace，当当前值小于 10 时，ace 将始终被视为 11，否则为 1；当当前值超过 21 并且庄家手头有可用的 ace 时，可用的 ace 将被视为 1，总值相应地减去 10，可用的 ace 指示器将被设置为`false`。

## 玩家政策

除了玩家在此设置中的年龄为 20 岁或以上之外，实现与庄家策略完全相同。

## 赊账

在每场比赛结束时，将通过向导致获胜的状态加 1 来给获胜者积分。

`player_states`是一个记录游戏结束前所有状态的列表，积分只给最后一个状态，卡值在 12 到 21 之间，也就是说，如果我们有一张卡和 15，并采取行动 HIT，得到一张卡值为 7，结束时总和为 22，那么 15 将被视为最后一个状态，而不是 22。

除了统计`state_value`之外，还会统计玩家赢和抽的次数，会用来衡量这个政策的整体好坏。

## MC 模拟

有了以上所有的准备，我们就可以运行模拟了！整个过程类似于一般的价值迭代，但除了在游戏结束时反向传播奖励，我们通过在`_giveCredit`函数中定义的内容来更新`state, value`对的估计。

在游戏开始时，给庄家 2 张牌，我们假设第一张牌正在显示。然后轮到玩家，只有 12 和 21 之间的牌值被包括在状态中，因为在状态中包括值 22 或以上是没有意义的，因为这些值不会导致获胜，因此不需要被计数。

最后，在游戏结束时，`player_state_value`字典中尚未存在的状态将被初始化为 0，游戏将由`_giveCredit`记入。

## 估计可视化

我们运行了 10000 轮模拟，并绘制了状态、值的估计值:

![](img/2f2c84454a7379c993cb7b820f1ddaa4.png)

MC Simulation

奖励的 z 列是该州的获胜次数。正如所料，当玩家的值为 20 或 21 时，这是在剧情的后面，一个人更有可能获胜。总的来说，与庄家的 HIT17 相比，这种 HIT20 策略给我们带来了 30.6%的赢、24.37%的听牌和 45.03%的输，显然这是一种在赌场中导致输多于赢的策略。([此处全码](https://github.com/MJeremy2017/RL/blob/master/BlackJack/blackjack_mc.py))

让我们也看看可用的 ace 如何导致游戏的细微差别:

![](img/38b2ff2d2fd7813f18d379dd12dd26e0.png)

MC Simulation(usable ace)

![](img/c736a18de2af3f18bee27f93a31627e2.png)

Simulation Result from Sutton’s book

Sutton 的书的结果更清楚，他提出了一个问题:为什么有可用 ace 的最前面的值比没有可用 ace 的值略高？我猜可用的 ace 确实增加了获胜的机会，因为一个有可用 ace 的玩家有更大的机会击中，因为即使值超过 21，他仍然能够通过使可用 ace 不可用来继续。

总之，我们共同探索了使用 MC 方法来评估给定的政策。MC 方法的采样思想可以推广到很多游戏玩的场景，所以如果你到了一个状态，不知道要去哪里，不知道要采取什么行动，就采样吧！

请在此查看[完整代码，欢迎投稿和提出问题！](https://github.com/MJeremy2017/RL/blob/master/BlackJack/blackjack_mc.py)

**参考**

[1]http://incompleteideas.net/book/the-book-2nd.html