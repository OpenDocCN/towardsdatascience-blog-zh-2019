<html>
<head>
<title>Implementing a Simple Auto-Encoder in Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Tensorflow 中实现一个简单的自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-a-simple-auto-encoder-in-tensorflow-1181751f202?source=collection_archive---------10-----------------------#2019-06-07">https://towardsdatascience.com/implementing-a-simple-auto-encoder-in-tensorflow-1181751f202?source=collection_archive---------10-----------------------#2019-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/776a2c12a0dfce4b471b0226c42376f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KvhQyrnSqpnR4H3mmZRCEA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Peele using DeepFake to forge a video of Obama — Source: <a class="ae jd" href="https://www.youtube.com/watch?v=cQ54GDm1eL0" rel="noopener ugc nofollow" target="_blank">BuzzFeed</a>, YouTube</figcaption></figure><div class=""/><p id="70bd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">生成对抗网络(GAN)最近通过展示它们的一些能力而变得越来越受欢迎，最初是通过模仿著名画家的艺术风格，但最近是通过无缝替换视频中的面部表情，同时保持高输出质量。</p><figure class="ll lm ln lo gt is"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Source: <a class="ae jd" href="https://www.youtube.com/watch?v=cQ54GDm1eL0" rel="noopener ugc nofollow" target="_blank">BuzzFeed</a>, YouTube</figcaption></figure><p id="4a23" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">GANs 的支柱之一是使用自动编码器。自动编码器是一种具有两种特性的神经网络:输入和输出数据是相同的，并且网络包括比输入更低维度的层。起初，这可能听起来令人困惑和无用，但通过训练网络复制输入数据，虽然有一个“瓶颈”，但我们真正做的是让网络学习数据的“压缩”版本，然后<br/>解压缩它。用行话来说，这意味着找到我们数据的“潜在空间”表示。</p><p id="8087" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将解释如何用 python 实现自动编码器，以及如何在实践中使用它来编码和解码数据。假设您已经安装了<code class="fe lr ls lt lu b">Python 3</code> <em class="lk"> </em>和<code class="fe lr ls lt lu b">Tensorflow</code>并正在运行，尽管代码只需要很小的改动就可以在<code class="fe lr ls lt lu b">Python 2.</code>上运行</p><p id="c753" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，一个好的自动编码器必须:<br/> 1。“压缩”数据，即潜在尺寸&lt;输入尺寸<br/> 2。很好地复制数据(咄！)<br/> 2。允许我们得到编码的潜在表示法<br/> 3。允许我们解码一个编码的表示</p><figure class="ll lm ln lo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lv"><img src="../Images/cfd65763d4f15b46c430d26c94188204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YINkypAWAN1PbFs5eyMUeg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><a class="ae jd" rel="noopener" target="_blank" href="/applied-deep-learning-part-3-autoencoders-1c083af4d798">Source</a>: another very interesting article, explaining and using auto-encoder to remove noise from images.</figcaption></figure><p id="584b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将向您展示两种方法，一种是严格的/学究式的(取决于您的观点)，它使用低级的<code class="fe lr ls lt lu b">tensorflow</code> API，另一种是更快更随意的，它利用了<code class="fe lr ls lt lu b">keras</code> API，尽管如果您想正确理解第二种 API 的内部工作方式，第一种方法是必要的。</p><h1 id="6606" class="lw lx jg bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">Tensorflow 低级实现</h1><blockquote class="mu"><p id="ff65" class="mv mw jg bd mx my mz na nb nc nd la dk translated">只有最酷的孩子才会用这个——比尔·盖茨</p></blockquote><p id="993a" class="pw-post-body-paragraph kd ke jg kf b kg ne ki kj kk nf km kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi lb translated">他的实现将直接使用<code class="fe lr ls lt lu b">tensorflow core</code> API，这需要一些先决知识；我将简要解释三个基本概念:<code class="fe lr ls lt lu b">tf.placeholder</code>、<code class="fe lr ls lt lu b">tf.Variable</code>和<code class="fe lr ls lt lu b">tf.Tensor</code>。</p><p id="e9a8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个<code class="fe lr ls lt lu b">tf.placeholder</code>仅仅是一个“变量”,它将是模型的输入，但不是模型本身的一部分。它基本上允许我们告诉模型，它应该期望一个特定类型和特定维度的变量。这与强类型语言中的变量声明非常相似。</p><p id="0817" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lr ls lt lu b">tf.Variable</code>与其他编程语言中的变量非常相似，其声明类似于大多数强类型语言的声明。例如，网络的权重是<code class="fe lr ls lt lu b">tf.Variables</code>。</p><p id="9aa3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个<code class="fe lr ls lt lu b">tf.Tensor</code>稍微复杂一点。在我们的例子中，我们可以认为它是一个包含操作的符号表示的对象。例如，给定一个占位符<strong class="kf jh"> <em class="lk"> X </em> </strong>和一个权重变量<strong class="kf jh"> <em class="lk"> W </em> </strong>，矩阵乘法<strong class="kf jh"><em class="lk">W</em></strong><strong class="kf jh"><em class="lk">X</em></strong>是一个张量，但是它的结果，给定一个特定的值<strong class="kf jh"> <em class="lk"> X </em> </strong>和<strong class="kf jh"> <em class="lk"> W，</em> </strong>不是张量。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="1654" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">既然介绍已经做好了，建立关系网的过程就相当简单，但是有点迂腐。我们将使用 MNIST 数据集，这是一个存储为 28x28 图片的手写数字数据集。我们将<em class="lk"> D </em>定义为输入数据，在这种情况下，展平的图像尺寸为 784，将<em class="lk"> d </em>定义为编码尺寸，我将其设置为 128。该网络然后具有以下维度的 3 层:<em class="lk"> D，D，D </em>。</p><p id="cb47" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在将实现一个简单的自动编码器类，将逐行解释。如果你感兴趣的话，这里还有完整版本的代码。</p><p id="1133" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们走吧！首先，我们需要一个输入数据的占位符:</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="116f" class="nu lx jg lu b gy nv nw l nx ny">self.X = tf.placeholder(tf.float32, shape=(None, D))</span></pre><p id="cc8c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们开始编码阶段，将第一层权重(带有额外偏差)定义为变量:</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="502e" class="nu lx jg lu b gy nv nw l nx ny">self.W1 = tf.Variable(tf.random_normal(shape=(D,d)))<br/>self.b1 = tf.Variable(np.zeros(d).astype(np.float32))</span></pre><p id="f154" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意重量的形状是<code class="fe lr ls lt lu b">Dxd</code>，从较高维度到较低维度。接下来，我们为瓶颈层创建张量，作为输入和权重之间的乘积，加上偏差，所有这些都由 relu 激活。</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="9849" class="nu lx jg lu b gy nv nw l nx ny">self.Z = tf.nn.relu( tf.matmul(self.X, self.W1) + self.b1 )</span></pre><p id="00c0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们进入解码阶段，这与编码相同，但从低维到高维。</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="6bf8" class="nu lx jg lu b gy nv nw l nx ny">self.W2 = tf.Variable(tf.random_normal(shape=(d,D)))<br/>self.b2 = tf.Variable(np.zeros(D).astype(np.float32))</span></pre><p id="c87c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们定义输出张量，以及预测变量。为了简单起见，选择了 Sigmoid 激活，因为它总是在区间[0，1]内，该区间与来自输入的归一化像素的范围相同。</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="d35a" class="nu lx jg lu b gy nv nw l nx ny">logits = tf.matmul(self.Z, self.W2) + self.b2 <br/>self.X_hat = tf.nn.sigmoid(logits)</span></pre><p id="8057" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">网络到此为止！我们只需要一个损失函数和一个优化器，我们可以开始训练了。选择的损失是 sigmoid 交叉熵，这意味着我们将该问题视为像素级别的二元分类，这对黑白图像数据集有意义。</p><p id="5d53" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于乐观主义者，这是非常古老的魔法。给定一个问题，也许我们应该创建一个模型来输出使用哪个优化器？</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="c140" class="nu lx jg lu b gy nv nw l nx ny">self.cost = tf.reduce_sum(<br/>    tf.nn.sigmoid_cross_entropy_with_logits(<br/>        # Expected result (a.k.a. itself for autoencoder)<br/>        labels=self.X,<br/>        logits=logits<br/>    )<br/>)<br/>        <br/>self.optimizer = tf.train.RMSPropOptimizer(learning_rate=0.005).minimize(self.cost)</span></pre><p id="48e3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一个稍微有点技术性的术语与会话有关，它是一个充当上下文管理器和后端连接器的对象，并且需要初始化:</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="2774" class="nu lx jg lu b gy nv nw l nx ny">self.init_op = tf.global_variables_initializer()        <br/>if(self.sess == None):<br/>            self.sess = tf.Session()<br/>self.sess = tf.get_default_session()<br/>self.sess.run(self.init_op)</span></pre><p id="21f9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！..或者差不多，现在我们需要拟合模型；但不用担心，这在 tensorflow 中非常简单:</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="e2cb" class="nu lx jg lu b gy nv nw l nx ny"># Prepare the batches <br/>epochs = 10 <br/>batch_size = 64       <br/>n_batches = len(X) // bs<br/>        <br/>for i in range(epochs):<br/>    # Permute the input data<br/>    X_perm = np.random.permutation(X)<br/>    for j in range(n_batches):</span><span id="bf5a" class="nu lx jg lu b gy nz nw l nx ny">        # Load data for current batch<br/>        batch = X_perm[j*batch_size:(j+1)*batch_size]</span><span id="74a8" class="nu lx jg lu b gy nz nw l nx ny">        # Run the batch training!<br/>        _, costs = self.sess.run((self.optimizer, self.cost),<br/>                                    feed_dict={self.X: batch})</span></pre><p id="fd0c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一行确实是唯一有趣的一行。它告诉 tensorflow 运行一个训练步骤，使用<code class="fe lr ls lt lu b">batch</code>作为占位符输入<em class="lk"> X，</em>，并使用给定的优化器和损失函数进行权重更新。</p><p id="10e5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看这个网络给出的一些重建的例子:</p><figure class="ll lm ln lo gt is gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/683d1d538482b33f62e5b85621b07136.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*OE1O4022OH2WVzn_zbspwg.png"/></div></figure></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="cad7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在这一切都很好，但目前我们只训练了一个可以自我重建的网络..我们实际上如何使用自动编码器？我们需要定义另外两个操作，编码和解码..这其实很简单:</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="190e" class="nu lx jg lu b gy nv nw l nx ny">def encode(self, X):<br/>    return self.sess.run(self.Z, feed_dict={self.X: X})</span></pre><p id="8787" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们告诉 tensorflow 计算<code class="fe lr ls lt lu b">Z</code> <em class="lk"> </em>如果你回头看，你会发现是代表编码的张量。解码也很简单:</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="9f1d" class="nu lx jg lu b gy nv nw l nx ny">def decode(self, Z):<br/>    return self.sess.run(self.X_hat, feed_dict={self.Z: Z})</span></pre><p id="cc5c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一次，我们通过<code class="fe lr ls lt lu b">Z</code>显式地给 tensorflow 编码，这是我们之前使用<code class="fe lr ls lt lu b">encode</code> <em class="lk"> </em>函数计算的，我们告诉它计算预测输出<code class="fe lr ls lt lu b">X_hat</code>。</p><p id="6bed" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如你所见，即使对于一个简单的网络来说，这也是相当长的。当然，我们可以对每个权重进行参数化并使用列表而不是单个变量，但是当我们需要快速或自动测试多个结构时会发生什么呢？除了致密层，其他类型的层呢？不要担心，第二种(不那么迂腐的)方法可以让我们轻松地做到所有这些！</p><h1 id="fc3c" class="lw lx jg bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">Keras API 实现</h1><blockquote class="mu"><p id="d1c1" class="mv mw jg bd mx my mz na nb nc nd la dk translated">越简单越好——奥卡姆的威廉</p></blockquote><p id="69e7" class="pw-post-body-paragraph kd ke jg kf b kg ne ki kj kk nf km kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi lb translated">他的第一条路又长又迂腐，而且，说实话，有点讨厌。此外，缺乏简单的概括也无济于事。因此，更简单的解决方案是关键，这可以通过使用<code class="fe lr ls lt lu b">tensorflow</code>中包含的<code class="fe lr ls lt lu b">keras</code>接口来实现。</p><p id="09bc" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有的网络定义、损耗、优化和拟合都在几行中:</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="469d" class="nu lx jg lu b gy nv nw l nx ny">t_model = Sequential()<br/>t_model.add(Dense(256, input_shape=(784,)))<br/>t_model.add(Dense(128, name='bottleneck'))<br/>t_model.add(Dense(784, activation=tf.nn.sigmoid))</span><span id="356e" class="nu lx jg lu b gy nz nw l nx ny">t_model.compile(optimizer=tf.train.AdamOptimizer(0.001),<br/>              loss=tf.losses.sigmoid_cross_entropy)<br/>t_model.fit(x, x, batch_size=32, epochs=10)</span></pre><p id="5158" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就这样，我们有了一个训练有素的网络。生活有时不是很美好吗？</p><p id="9588" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">…但是整个编码/解码过程呢？是的，这时候事情会变得有点棘手，但是不要担心，你的向导在这里。<br/>因此，要做到这一点，我们需要几样东西:</p><ul class=""><li id="f6b2" class="ob oc jg kf b kg kh kk kl ko od ks oe kw of la og oh oi oj bi translated">会话变量</li><li id="85f7" class="ob oc jg kf b kg ok kk ol ko om ks on kw oo la og oh oi oj bi translated">输入张量，在<code class="fe lr ls lt lu b">feed_dict</code>参数中指定输入</li><li id="cdf0" class="ob oc jg kf b kg ok kk ol ko om ks on kw oo la og oh oi oj bi translated">编码张量，用于检索编码，并用作解码<code class="fe lr ls lt lu b">feed_dict</code>参数的输入</li><li id="5b6e" class="ob oc jg kf b kg ok kk ol ko om ks on kw oo la og oh oi oj bi translated">解码/输出张量，用于检索解码值</li></ul><h2 id="31b3" class="nu lx jg bd ly op oq dn mc or os dp mg ko ot ou mk ks ov ow mo kw ox oy ms oz bi translated">胡言乱语。</h2><p id="36d5" class="pw-post-body-paragraph kd ke jg kf b kg pa ki kj kk pb km kn ko pc kq kr ks pd ku kv kw pe ky kz la ij bi translated">我们通过简单地从感兴趣的层中获得所需的张量来实现这一点！注意，通过命名瓶颈层，我使得检索它变得非常容易。</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="4f7a" class="nu lx jg lu b gy nv nw l nx ny">session = tf.get_default_session()<br/>if(self.sess == None):<br/>    self.sess = tf.Session()</span><span id="d211" class="nu lx jg lu b gy nz nw l nx ny"># Get input tensor<br/>def get_input_tensor(model):<br/>    return model.layers[0].input</span><span id="53e0" class="nu lx jg lu b gy nz nw l nx ny"># get bottleneck tensor<br/>def get_encode_tensor(model):<br/>    return model.get_layer(name='encode').output</span><span id="049a" class="nu lx jg lu b gy nz nw l nx ny"># Get output tensor<br/>def get_output_tensor(model):<br/>    return model.layers[-1].output</span></pre><p id="23ec" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！现在，给定一个训练好的模型，您可以通过以下几行获得您需要的所有变量:</p><pre class="ll lm ln lo gt nq lu nr ns aw nt bi"><span id="2a01" class="nu lx jg lu b gy nv nw l nx ny">t_input = get_input_tensor(t_model)<br/>t_enc = get_bottleneck_tensor(t_model)<br/>t_dec = get_output_tensor(t_model)</span><span id="7a07" class="nu lx jg lu b gy nz nw l nx ny">session = tf.get_default_session()</span><span id="f3f4" class="nu lx jg lu b gy nz nw l nx ny"># enc will store the actual encoded values of x<br/>enc = session.run(t_enc, feed_dict={t_input:x})</span><span id="15cf" class="nu lx jg lu b gy nz nw l nx ny"># dec will store the actual decoded values of enc<br/>dec = session.run(t_dec, feed_dict={t_enc:enc})</span></pre></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="3f70" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">我</span>希望你喜欢这篇文章，它是有用的或者至少是有趣的。最初我是专门为会议部分制作的，这部分没有很好的文档记录。我花了几个小时试图理解什么是张量，什么是会话，以及如何实际计算网络的一部分，评估和检索特定的张量，但总而言之，它实际上有助于巩固各种<code class="fe lr ls lt lu b">tensorflow</code>概念，这些概念远非直观，但一旦理解就非常强大。</p><p id="d378" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您的阅读，不要犹豫，评论，并有一个美好的一天！</p></div></div>    
</body>
</html>