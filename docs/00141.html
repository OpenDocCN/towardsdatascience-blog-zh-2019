<html>
<head>
<title>Support Vector Machine — Simply Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机——简单解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496?source=collection_archive---------2-----------------------#2019-01-07">https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496?source=collection_archive---------2-----------------------#2019-01-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="099d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">支持向量机基本概念的简化说明</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9797a6f135c94a337c5819c0ac26d33e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*14fkjRgr8eeim7Vd"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@maltesimo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Malte Schmidt</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4fe3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我总是在逃避我的 ML 书籍中的支持向量机一章。它只是令人生畏，你知道，名称，支持，向量，机器。</p><p id="cb75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，一旦我开始将支持向量机视为“<em class="ls">道路机器</em>”，它就会变得不那么可怕，它可以分离左侧、右侧的汽车、建筑物、行人，并尽可能形成最宽的车道。那些离街道很近的汽车和建筑是支持向量。</p><p id="9e82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感觉好多了。让我们开始了解这个“<em class="ls">路机</em>是如何工作的。</p><p id="0b49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本博客包括三个部分:</p><ol class=""><li id="33fd" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">什么是支持向量机</li><li id="9214" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">在线性可分的情况下，它是如何工作的</li><li id="12b6" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">在线性不可分的情况下，它是如何工作的</li></ol></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="0b31" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">什么是支持向量机(分类器)</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/13985b282c9e9d560c1158f71d90ae7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ox4UFUKHna9BjW5gfNcQlw.png"/></div></div></figure><p id="9a14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到 ML 世界，那些汽车，建筑，行人现在都变成了点。红点代表街道左侧的物体；绿点代表右边的。街道变成了虚线和实线。</p><p id="74b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi nh translated"><span class="l ni nj nk bm nl nm nn no np di"> S </span>支持向量机(“路机”)负责寻找决策边界，以区分不同类别并最大化裕量。</p><p id="a6d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">边距<strong class="ky ir"> </strong>是线条和最靠近线条的点之间的(垂直)距离。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="ff42" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">线性可分情形下的 SVM</h1><p id="0109" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">显然，在上面的例子中，存在无限的线来分隔红色和绿色的点。SVM 需要找到一条最优线，约束条件是要正确地分类这两类中的任何一类:</p><ol class=""><li id="4fa2" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">遵循约束:只查看<strong class="ky ir">单独的</strong> <strong class="ky ir">超平面</strong>(例如，单独的线)，正确分类的超平面</li><li id="e9f9" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">进行优化:选择最大化<strong class="ky ir">余量</strong>的一个</li></ol><p id="3f0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将举例说明分离超平面和边界的概念，但在这篇文章中不会解释如何用约束条件来解决优化问题。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="0539" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么什么是<strong class="ky ir">超平面</strong>？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/5942ec22c0f2eda55efd172ae0085a93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0KTZUoPgsY0NBP2d3LFsw.png"/></div></div></figure><p id="4daf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi nh translated"><span class="l ni nj nk bm nl nm nn no np di"> H </span>超平面是 n 维空间的(n 减 1)维子空间。对于一个 2 维空间，它的超平面将是 1 维的，这只是一条线。对于三维空间，它的超平面将是二维的，这是一个切割立方体的平面。好吧，你明白了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/33c2d479afade1935efcae37bf4e9eca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IJBMkqO47pEWE9aek5yoLA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Any Hyperplane can be written mathematically as above</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f59ca56e184be5b7ad500f390f60a554.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*57zk4SotKYdy6Zr9__blaw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">For a 2-dimensional space, the Hyperplane, which is the line.</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/0385a8d2b0c3e4406e532abc9c3d11a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*y3UR4nmXk_0d4v3_xPTv3A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The dots above this line, are those x1, x2 satisfy the formula above</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/eb4571d141d63748b92a286bdf2691db.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*-JDee4v_CZIh2LYV16B9ZA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The dots below this line, similar logic.</figcaption></figure></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="4d0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">什么是<strong class="ky ir">分离超平面？</strong></p><p id="0b99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设标签 y 是 1(代表绿色)或-1(代表红色)，下面的三条线都是分离超平面。因为它们都有相同的属性——在这条线上，是绿色的；线下面是红色的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/eba9d876a4772dce83d38641204df883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bXbbnOt2rsnbIoL_2FxHnA.png"/></div></div></figure><p id="830b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该属性可以用数学公式再次写成如下形式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/5ba20835d8512db46fdd244a50f89639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qsd9zUC-jfUVP10JfovCTA.png"/></div></div></figure><p id="d558" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们进一步把这两个归纳成一个，它就变成了:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/7e07afc33e6093255e7c226b1e94783f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g_ylg6Abahd2wbn2Oaa_3Q.png"/></div></div></figure><p id="835f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi nh translated"><span class="l ni nj nk bm nl nm nn no np di"> S </span>分离超平面约束用数学方法写在上面。在完美的情况下——线性可分的情况，这个约束可以由 SVM 来满足。但是在不可分的情况下，我们需要放松它。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="f987" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么什么是<strong class="ky ir">保证金呢？</strong></p><ul class=""><li id="cd58" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr od lz ma mb bi translated">假设我们有一个超平面——X 线</li><li id="fa55" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr od lz ma mb bi translated">计算所有这 40 个点到 X 线的垂直距离，将会有 40 个不同的距离</li><li id="9a76" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr od lz ma mb bi translated">40 分中，最小的距离，就是我们的差距！</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/13985b282c9e9d560c1158f71d90ae7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ox4UFUKHna9BjW5gfNcQlw.png"/></div></div></figure><p id="7776" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虚线两侧到实线之间的距离就是边距。我们可以把这条最佳线想象成红点和绿点之间最大延伸的中线。</p><p id="e6c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">综上所述，SVM 在线性可分的情况下:</strong></p><ul class=""><li id="4a64" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr od lz ma mb bi translated">约束/确保每个观察都在超平面的正确一侧</li><li id="9498" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr od lz ma mb bi translated">选择最佳线，使这些最近的点到超平面的距离最大化，即所谓的边距</li></ul></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="1277" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">线性不可分情形下的 SVM</h1><p id="1d08" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">在线性可分的情况下，SVM 试图找到使裕度最大化的超平面，条件是两个类都被正确分类。但在现实中，数据集可能永远不会线性分离，所以超平面 100%正确分类的条件永远不会满足。</p><p id="3165" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">SVM 通过引入两个概念来处理非线性可分的情况:<strong class="ky ir">软余量</strong>和<strong class="ky ir">内核技巧。</strong></p><p id="39d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们来举个例子。如果我在绿色聚类中添加一个红点，数据集就不再是线性不可分的了。这个问题有两个解决方案:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/25d8950543685f99255f4c0779c7c47d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vwRojapdm0po85w8XnyWRQ.png"/></div></div></figure><ol class=""><li id="d412" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated"><strong class="ky ir">软边距:</strong>尝试找到一条线来分隔，但允许一个或几个错误分类的点(例如，用红色圈出的点)</li><li id="3ce4" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><strong class="ky ir">内核技巧:</strong>尝试找到一个非线性决策边界</li></ol></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="38e3" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">软利润</h1><p id="a6ba" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">在软裕度下，SVM 容忍两种类型的错误分类:</p><ol class=""><li id="44a4" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">该点位于决策边界的错误一侧，但位于正确一侧/边缘(如左侧所示)</li><li id="7014" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">该点位于决策边界的错误一侧和页边距的错误一侧(如右图所示)</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/faef96c5109c5eea3724c03a8bc19420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pNJ7IaXvSvxjpyUw3KKVzA.png"/></div></div></figure><p id="bc14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">应用软边距，SVM 容忍一些点被错误分类，并试图在找到一条最大化边距和最小化错误分类的线之间进行平衡。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="83b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">容差程度</em> </strong> <br/>在寻找决策边界时，我们希望给出多少容差(软)是 SVM(线性和非线性解决方案)的一个重要超参数。在 Sklearn 中，它被表示为惩罚项—‘C’。C 越大，SVM 在分类错误时受到的惩罚就越多。因此，边缘越窄，决策边界将依赖的支持向量就越少。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="277b" class="ol mp iq oh b gy om on l oo op"># Default Penalty/Default Tolerance<br/>clf = svm.SVC(kernel='linear', C=1)<br/># Less Penalty/More Tolearance<br/>clf2 = svm.SVC(kernel='linear', C=0.01)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/b8ac511090384111466bdc2f3b3ea7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0vOVPBmYCkw-sUt77HtyGA.png"/></div></div></figure></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="6a2b" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">内核技巧</h1><p id="602e" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">Kernel Trick 所做的是利用现有的特性，应用一些转换，并创建新的特性。这些新特征是 SVM 找到非线性决策边界的关键。</p><p id="61cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 Sklearn — svm。SVC()，我们可以选择'线性'，'多边形'，' rbf '，' sigmoid '，' precomputed '或一个 callable 作为我们的内核/转换。我将给出两个最流行的核的例子——多项式和径向基函数(RBF)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/16acf72f20c046ee02a6e066364793c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ha7EfcfB5mY2RIKsXaTRkA.png"/></div></div></figure></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="ae1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">多项式内核</em> </strong></p><p id="8ce9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将多项式内核视为一个转换器/处理器，通过应用所有现有要素的多项式组合来生成新要素。</p><p id="ca81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了说明应用多项式转换器的好处，我们举一个简单的例子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/9d9a02d67d70133ffc058533a6b08d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gIHnZCcl4Q9fFx2AZsJ7pw.png"/></div></div></figure><p id="0045" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现有特征:X = np.array([-2，-1，0，1，2]) <br/>标签:Y = np.array([1，1，0，1，1]) <br/>我们不可能找到一条线将黄色(1)和紫色(0)的点分开(如左图所示)。</p><p id="e1ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，如果我们应用变换 X 得到:<br/>新特征:X = np.array([4，1，0，1，4]) <br/>通过组合现有的和新的特征，我们当然可以画一条线把黄紫色的点分开(如右图所示)。</p><p id="1f78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具有多项式核的支持向量机可以使用那些多项式特征生成非线性决策边界。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="37c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">径向基函数核</em> </strong></p><p id="67b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将径向基函数核视为一个转换器/处理器，通过测量所有其他点到特定点(中心)的距离来生成新要素。最流行/最基本的 RBF 核是高斯径向基函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/d5bc480e5c6503da90664337b4e51239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*izqr1xGcP89B7Xs1EluIQQ.png"/></div></figure><p id="1de9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> gamma (γ) </strong>控制新特征—<strong class="ky ir">φ(x，center) </strong>对决策边界的影响。灰度系数越高，特征对决策边界的影响就越大，边界的波动就越大。</p><p id="8c7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了说明应用高斯 rbf (gamma = 0.1)的好处，让我们使用相同的示例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/2a3361654f36f1a6625f03e7cdf9da30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M9spISHtIR_wOXKtmFTFvg.png"/></div></div></figure><p id="6cde" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现有特征:X = np.array([-2，-1，0，1，2]) <br/>标签:Y = np.array([1，1，0，1，1]) <br/>同样，我们也不可能找到一条线把点分开(左手)。</p><p id="f4c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，如果我们使用两个中心(-1，0)和(2，0)应用高斯 RBF 变换来获得新特征，那么我们将能够绘制一条线来分隔黄色紫色点(在右侧):<br/>新特征 1: X_new1 = array([1.01，1.00，1.01，1.04，1.09)<br/>新特征 2: X_new2 = array([1.09，1.04，1.01，1.00</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="5e3b" class="ol mp iq oh b gy om on l oo op"># Note for how we get New Feature 1, e.g. <strong class="oh ir">1.01</strong>:<br/>Φ(x1,center1) = np.exp(np.power(-(gamma*(x1-center1)),2)) = 1.01<br/># gamma = 0.1<br/># center1 = [-1,0]<br/># x1 = [-2,0]</span></pre><p id="eed4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似于软边界中的惩罚项 C，伽马是一个超参数，当我们将 SVM 与内核结合使用时，我们可以对其进行调整。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="ba1f" class="ol mp iq oh b gy om on l oo op"># Gamma is small, influence is small<br/>clf = svm.SVC(kernel='rbf', Gamma=1)<br/># Gamma gets bigger, influence increase, the decision boundary get wiggled<br/>clf2 = svm.SVC(kernel='rbf', Gamma=10)<br/># Gamma gets too big, influence too much, the decision boundary get too wiggled<br/>clf3 = svm.SVC(kernel='rbf', Gamma=20)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/7073541a0625cddb02bda4b7dfa95992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r9CO-gp1uuRsYooCLL9UeQ.png"/></div></div></figure></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="6936" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">总而言之，SVM 在线性不可分的情况下:</strong></p><ul class=""><li id="550a" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr od lz ma mb bi translated">通过将<strong class="ky ir">软裕度</strong>(误分类容忍度)和<strong class="ky ir">核技巧</strong>结合在一起，支持向量机能够构造线性不可分情况的决策边界。</li><li id="ddea" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr od lz ma mb bi translated">像 C 或伽马这样的超参数控制着 SVM 决策边界的摆动程度。</li></ul><ol class=""><li id="728a" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">C 越高，SVM 在错误分类时得到的惩罚就越多，因此决策边界的波动就越小</li><li id="0e11" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">伽马值越高，特征数据点对决策边界的影响越大，因此边界的摆动越大</li></ol></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="da75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，就是这样。</p><p id="26b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个博客的代码可以在我的 GitHub 链接中找到。</p><p id="84f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">优化的一个很好的解释/理由可以在<a class="ae kv" href="https://www.youtube.com/watch?v=5yzSv4jYMyI&amp;list=PLgIPpm6tJZoShjm7r8Npia7CMsMlRWeuZ&amp;index=1" rel="noopener ugc nofollow" target="_blank">链接</a>中找到，这里是 SVM uda city 的 YouTube 视频。</p><p id="1330" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请在下面留下任何评论、问题或建议。谢谢大家！</p></div></div>    
</body>
</html>