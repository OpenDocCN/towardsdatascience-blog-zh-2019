<html>
<head>
<title>Transfer Learning using ELMO Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 ELMO 嵌入的迁移学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transfer-learning-using-elmo-embedding-c4a7e415103c?source=collection_archive---------8-----------------------#2019-02-03">https://towardsdatascience.com/transfer-learning-using-elmo-embedding-c4a7e415103c?source=collection_archive---------8-----------------------#2019-02-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="25f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">去年，“自然语言处理”的主要发展是关于迁移学习的。基本上，迁移学习是在大规模数据集上训练模型，然后使用预训练的模型来处理另一个目标任务的学习的过程。迁移学习在自然语言处理领域变得很流行，这要归功于不同算法的最新性能，如 ULMFiT、Skip-Gram、Elmo、BERT 等。</p><p id="5da5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由 Allen NLP 开发的 Elmo embedding 是 Tensorflow Hub 上提供的最先进的预训练模型。Elmo 嵌入从双向 LSTM 的内部状态中学习，并表示输入文本的上下文特征。在各种各样的 NLP 任务中，它已经被证明优于先前存在的预训练单词嵌入，如 word2vec 和 glove。这些任务中的一些是<strong class="jp ir">问题回答</strong>、<strong class="jp ir">命名实体提取</strong>和<strong class="jp ir">情感分析</strong>。</p><h1 id="7412" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">使用 Tensorflow-hub 的 Elmo 嵌入</strong></h1><div class="lj lk gp gr ll lm"><a href="https://github.com/sambit9238/Deep-Learning/blob/master/elmo_embedding_tfhub.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab fo"><div class="lo ab lp cl cj lq"><h2 class="bd ir gy z fp lr fr fs ls fu fw ip bi translated">sambit 9238/深度学习</h2><div class="lt l"><h3 class="bd b gy z fp lr fr fs ls fu fw dk translated">深度学习技术在自然语言处理、计算机视觉等领域的实现。-sambit 9238/深度学习</h3></div><div class="lu l"><p class="bd b dl z fp lr fr fs ls fu fw dk translated">github.com</p></div></div><div class="lv l"><div class="lw l lx ly lz lv ma mb lm"/></div></div></a></div><p id="b744" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">tensorflow-hub 中有一个预先训练好的 Elmo 嵌入模块。该模块支持原始文本字符串或标记化文本字符串作为输入。该模块输出每个 LSTM 层的固定嵌入、3 个层的可学习集合以及输入的固定平均汇集向量表示(对于句子)。要先使用这个模块，让我们把它下载到本地。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="0d33" class="ml km iq mh b gy mm mn l mo mp">#download the model to local so it can be used again and again<br/>!mkdir module/module_elmo2<br/># Download the module, and uncompress it to the destination folder. <br/>!curl -L "https://tfhub.dev/google/elmo/2?tf-hub-format=compressed" | tar -zxvC module/module_elmo2</span></pre><p id="4a4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该模块展示了用于层聚合的 4 个可训练标量权重。输出字典包含:</p><ul class=""><li id="6de5" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk mv mw mx my bi translated"><code class="fe mz na nb mh b"><strong class="jp ir">word_emb</strong></code>:基于字符的具有形状的单词表示<code class="fe mz na nb mh b"><strong class="jp ir">[batch_size, max_length, 512]</strong></code>。</li><li id="0375" class="mq mr iq jp b jq nc ju nd jy ne kc nf kg ng kk mv mw mx my bi translated"><code class="fe mz na nb mh b"><strong class="jp ir">lstm_outputs1</strong></code>:第一个 LSTM 潜州用形<code class="fe mz na nb mh b"><strong class="jp ir">[batch_size, max_length, 1024]</strong></code>。</li><li id="1ee3" class="mq mr iq jp b jq nc ju nd jy ne kc nf kg ng kk mv mw mx my bi translated"><code class="fe mz na nb mh b"><strong class="jp ir">lstm_outputs2</strong></code>:形状<code class="fe mz na nb mh b"><strong class="jp ir">[batch_size, max_length, 1024]</strong></code>的第二个 LSTM 潜州。</li><li id="d430" class="mq mr iq jp b jq nc ju nd jy ne kc nf kg ng kk mv mw mx my bi translated"><code class="fe mz na nb mh b"><strong class="jp ir">elmo</strong></code>:3 层的加权和，其中权重是可训练的。这个张量有形状<code class="fe mz na nb mh b"><strong class="jp ir">[batch_size, max_length, 1024]</strong></code></li><li id="8f2c" class="mq mr iq jp b jq nc ju nd jy ne kc nf kg ng kk mv mw mx my bi translated"><code class="fe mz na nb mh b"><strong class="jp ir">default</strong></code>:形状<code class="fe mz na nb mh b"><strong class="jp ir">[batch_size, 1024]</strong></code>的所有语境化单词表示的固定均值池。</li></ul><p id="637e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要传递原始字符串作为输入:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="8088" class="ml km iq mh b gy mm mn l mo mp">elmo = hub.Module("module/module_elmo2/", trainable=<strong class="mh ir">False</strong>)<br/>embeddings = elmo(<br/>["the cat is on the mat", "what are you doing in evening"],<br/>signature="default",<br/>as_dict=<strong class="mh ir">True</strong>)["elmo"]<br/><strong class="mh ir">with</strong> tf.Session() <strong class="mh ir">as</strong> session:<br/>    session.run([tf.global_variables_initializer(), tf.tables_initializer()])<br/>    message_embeddings = session.run(embeddings)</span></pre><p id="9118" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输出消息嵌入的形状为(2，6，1024)，因为有两个最大长度为 6 个单词的句子，并且为每个单词生成长度为 1024 的 1D 向量。它在内部根据空格对其进行标记。如果提供了一个少于 6 个单词的字符串，它会在内部向其追加空格。</p><p id="6ab7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们还可以向模块提供标记化的字符串，如下所示:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="63e1" class="ml km iq mh b gy mm mn l mo mp">elmo = hub.Module("sentence_wise_email/module/module_elmo2/", trainable=<strong class="mh ir">False</strong>)<br/>tokens_input = [["the", "cat", "is", "on", "the", "mat"],<br/>["what", "are", "you", "doing", "in", "evening"]]<br/>tokens_length = [6, 5]<br/>embeddings = elmo(<br/>inputs={<br/>"tokens": tokens_input,<br/>"sequence_len": tokens_length<br/>},<br/>signature="tokens",<br/>as_dict=<strong class="mh ir">True</strong>)["elmo"]<br/><strong class="mh ir">with</strong> tf.Session() <strong class="mh ir">as</strong> session:<br/>    session.run([tf.global_variables_initializer(), tf.tables_initializer()])<br/>    message_embeddings = session.run(embeddings)</span></pre><p id="fe67" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输出将是相似的。</p><p id="c6ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当对多个输入使用 in REST API 或 backend，而不是为每个调用初始化会话(这是一种开销)时，一种有效的方法是:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="5549" class="ml km iq mh b gy mm mn l mo mp"><strong class="mh ir">def</strong> embed_elmo2(module):<br/>    <strong class="mh ir">with</strong> tf.Graph().as_default():<br/>        sentences = tf.placeholder(tf.string)<br/>        embed = hub.Module(module)<br/>        embeddings = embed(sentences)<br/>        session = tf.train.MonitoredSession()<br/>    <strong class="mh ir">return</strong> <strong class="mh ir">lambda</strong> x: session.run(embeddings, {sentences: x})<br/><br/>embed_fn = embed_elmo2('module/module_elmo2')<br/>embed_fn(["i am sambit"]).shape</span></pre><p id="b6e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，默认情况下，它为每个句子输出大小为 1024 的向量，这是所有上下文化单词表示的固定均值池。通常在分类器中使用它时，我们可以使用这个输出。</p><h1 id="e719" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak"> ELMO 嵌入一个简单的神经网络分类器</strong></h1><p id="c40a" class="pw-post-body-paragraph jn jo iq jp b jq nh js jt ju ni jw jx jy nj ka kb kc nk ke kf kg nl ki kj kk ij bi translated"><strong class="jp ir">数据输入</strong></p><p id="0849" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用<a class="ae nm" href="https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment#Sentiment.csv" rel="noopener ugc nofollow" target="_blank">首次共和党辩论 Twitter 情绪</a>数据，其中包含 2016 年首次共和党总统辩论的约 14K 条推文。我们正在制作一个二元分类器，因此将忽略带有中性情绪的推文。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="f625" class="ml km iq mh b gy mm mn l mo mp">df =  pd.read_csv("sentence_wise_email/Sentiment.csv",encoding="latin")<br/>df = df[df["sentiment"]!="Neutral"]<br/>df.loc[df["sentiment"]=='Negative',"sentiment"]=0<br/>df.loc[df["sentiment"]=='Positive',"sentiment"]=1</span></pre><p id="27ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">数据处理</strong></p><p id="ff9c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将对数据进行一些清理，比如处理像“I'll”、“It's”等缩写。我们还将删除数字、链接、标点符号和电子邮件地址。(名字也应该被删除，但在这里没有这样做，因为这个模型实际上是为其他目的开发的，在这种情况下我懒得更改:-))</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="85a1" class="ml km iq mh b gy mm mn l mo mp"><strong class="mh ir">import</strong> <strong class="mh ir">re</strong><br/><strong class="mh ir">def</strong> cleanText(text):<br/>    text = text.strip().replace("<strong class="mh ir">\n</strong>", " ").replace("<strong class="mh ir">\r</strong>", " ")<br/>    text = replace_contraction(text)<br/>    text = replace_links(text, "link")<br/>    text = remove_numbers(text)<br/>    text = re.sub(r'[,!@#$%^&amp;*)(|/&gt;&lt;";:.?<strong class="mh ir">\'\\</strong>}{]',"",text)<br/>    text = text.lower()<br/>    <strong class="mh ir">return</strong> text<br/>X = np.array(df["text"].apply(cleanText))<br/>y = np.array(df["sentiment"])</span></pre><p id="8b92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">分类器模型构建</strong></p><p id="ad14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，需要为此导入必要的模块。然后，我们需要创建一个函数，对输入执行预训练的 Elmo 嵌入。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="8c53" class="ml km iq mh b gy mm mn l mo mp">embed = hub.Module("module/module_elmo2")<br/><strong class="mh ir">def</strong> ELMoEmbedding(x):<br/>    <strong class="mh ir">return</strong> embed(tf.squeeze(tf.cast(x, tf.string)), signature="default", as_dict=<strong class="mh ir">True</strong>)["default"]</span></pre><p id="d617" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们需要构建架构。这里我们使用<a class="ae nm" rel="noopener" target="_blank" href="/an-intro-to-high-level-keras-api-in-tensorflow-c50f6f5272de">高级 keras api </a>来构建它，因为它更容易使用。我们使用函数方法来建立一个简单的前馈神经网络以及正则化来避免过度拟合。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="8667" class="ml km iq mh b gy mm mn l mo mp"><strong class="mh ir">def</strong> build_model(): <br/>    input_text = Input(shape=(1,), dtype="string")<br/>    embedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)<br/>    dense = Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(embedding)<br/>    pred = Dense(1, activation='sigmoid')(dense)<br/>    model = Model(inputs=[input_text], outputs=pred)<br/>    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])<br/>    <strong class="mh ir">return</strong> model<br/>model_elmo = build_model()</span></pre><p id="0335" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型总结为:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="136a" class="ml km iq mh b gy mm mn l mo mp">_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_2 (InputLayer)         (None, 1)                 0         <br/>_________________________________________________________________<br/>lambda_2 (Lambda)            (None, 1024)              0         <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 256)               262400    <br/>_________________________________________________________________<br/>dense_4 (Dense)              (None, 1)                 257       <br/>=================================================================<br/>Total params: 262,657<br/>Trainable params: 262,657<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="26ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，既然模型架构(已编译)和数据都准备好了，是时候开始训练和保存已训练的权重了。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="66b3" class="ml km iq mh b gy mm mn l mo mp"><strong class="mh ir">with</strong> tf.Session() <strong class="mh ir">as</strong> session:<br/>    K.set_session(session)<br/>    session.run(tf.global_variables_initializer())  <br/>    session.run(tf.tables_initializer())<br/>    history = model_elmo.fit(X, y, epochs=5, batch_size=256, validation_split = 0.2)<br/>    model_elmo.save_weights('./model_elmo_weights.h5')</span></pre><p id="f813" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了查看在训练时关于准确度和损失函数的学习是如何进行的，我们可以画一个图:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="2b77" class="ml km iq mh b gy mm mn l mo mp"><strong class="mh ir">import</strong> <strong class="mh ir">matplotlib.pyplot</strong> <strong class="mh ir">as</strong> <strong class="mh ir">plt</strong><br/>%matplotlib inline<br/><br/>acc = history.history['acc']<br/>val_acc = history.history['val_acc']<br/>loss = history.history['loss']<br/>val_loss = history.history['val_loss']<br/><br/>epochs = range(1, len(acc) + 1)<br/><br/>plt.plot(epochs, acc, 'g', label='Training Acc')<br/>plt.plot(epochs, val_acc, 'b', label='Validation Acc')<br/>plt.title('Training and validation Acc')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Acc')<br/>plt.legend()<br/><br/>plt.show()</span></pre><figure class="mc md me mf gt no gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/305f6476f6e7d2c42b8a3c51aa5b546f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*x3LyGgoRiYltGWnIFb3TDg.png"/></div></figure><p id="e2da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">似乎在第四纪元后，精确度没有太大的变化。你也可以画出同样的损失值。为了更好地了解模型何时达到其目的地，它必须接受更多时期的训练(这意味着增加成本，因为它是非常计算密集型的，需要 GPU :-)。</p><p id="7f7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">用训练好的模型进行预测</strong></p><p id="d1c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，为了使用训练好的模型进行预测，我们需要首先处理文本并对其进行排列。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="055f" class="ml km iq mh b gy mm mn l mo mp">new_text =  ['RT @FrankLuntz: Before the #GOPDebate, 14 focus groupers said they did not have favorable view of Trump.',<br/>             'Chris Wallace(D) to be the 2nd worst partisan pontificating asshole "moderating" #GOPDebate @megynkelly'<br/>            ]<br/>#the texts should go through clean text also<br/>new_text_pr = np.array(new_text, dtype=object)[:, np.newaxis]</span></pre><p id="cdd4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们可以开始一个 tensorflow 会话，我们将首先调用模型架构，然后从保存的文件中加载权重。调用 predict API 调用，会给我们每个文本的情感概率。分数越少，句子中嵌入的负面情绪越多。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="5229" class="ml km iq mh b gy mm mn l mo mp"><strong class="mh ir">with</strong> tf.Session() <strong class="mh ir">as</strong> session:<br/>    K.set_session(session)<br/>    session.run(tf.global_variables_initializer())  <br/>    session.run(tf.tables_initializer())<br/>    model_elmo = build_model() <br/>    model_elmo.load_weights('./model_elmo_weights.h5')<br/>    <strong class="mh ir">import</strong> <strong class="mh ir">time</strong><br/>    t = time.time()<br/>    predicts = model_elmo.predict(new_text_pr)<br/>    print("time: ", time.time() - t)<br/>    print(predicts)</span></pre><p id="5d28" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输出是:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="db06" class="ml km iq mh b gy mm mn l mo mp">time:  0.6370120048522949<br/>[[0.17122008]<br/> [0.3037635 ]]</span></pre><p id="5cdc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我打印了时间只是为了显示需要多少时间，在特斯拉 k80 gpu 中两句话用了 0.63 秒。在 i5 处理器 cpu 中，耗时 14.3 秒。由于这是一个计算非常密集的过程，尤其是由于 Elmo 嵌入的高度复杂的架构，因此需要实时使用加速器。</p><p id="14a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你在上面的训练中看到，我们在 Elmo 嵌入上实现了 0.8094 的准确度，而使用预训练的 word2vec、glove 和在线嵌入，准确度分别为 0.7821、0.7432 和 0.7213。这些是 5 个时期后相同数据处理的结果。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="7452" class="ml km iq mh b gy mm mn l mo mp">Train on 8583 samples, validate on 2146 samples<br/>Epoch 1/5<br/>8583/8583 [==============================] - 63s 7ms/step - loss: 0.8087 - acc: 0.7853 - val_loss: 0.6919 - val_acc: 0.7819<br/>Epoch 2/5<br/>8583/8583 [==============================] - 62s 7ms/step - loss: 0.6015 - acc: 0.8265 - val_loss: 0.6359 - val_acc: 0.7651<br/>Epoch 3/5<br/>8583/8583 [==============================] - 62s 7ms/step - loss: 0.5377 - acc: 0.8371 - val_loss: 0.5407 - val_acc: 0.8169<br/>Epoch 4/5<br/>8583/8583 [==============================] - 62s 7ms/step - loss: 0.4946 - acc: 0.8401 - val_loss: 0.5016 - val_acc: 0.8071<br/>Epoch 5/5<br/>8583/8583 [==============================] - 63s 7ms/step - loss: 0.4836 - acc: 0.8396 - val_loss: 0.4995 - val_acc: 0.8094</span></pre><p id="86d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae nm" href="https://tfhub.dev/google/elmo/2" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/elmo/2</a></p><div class="lj lk gp gr ll lm"><a href="https://arxiv.org/abs/1802.05365" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab fo"><div class="lo ab lp cl cj lq"><h2 class="bd ir gy z fp lr fr fs ls fu fw ip bi translated">深层语境化的词汇表征</h2><div class="lt l"><h3 class="bd b gy z fp lr fr fs ls fu fw dk translated">我们介绍了一种新的深度上下文化的单词表示，它模拟了(1)单词的复杂特征…</h3></div><div class="lu l"><p class="bd b dl z fp lr fr fs ls fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="lj lk gp gr ll lm"><a href="https://arxiv.org/abs/1806.06259" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab fo"><div class="lo ab lp cl cj lq"><h2 class="bd ir gy z fp lr fr fs ls fu fw ip bi translated">下游和语言探测任务中句子嵌入的评估</h2><div class="lt l"><h3 class="bd b gy z fp lr fr fs ls fu fw dk translated">尽管新的句子嵌入方法发展速度很快，但要找到全面的句子嵌入方法仍然具有挑战性</h3></div><div class="lu l"><p class="bd b dl z fp lr fr fs ls fu fw dk translated">arxiv.org</p></div></div></div></a></div></div></div>    
</body>
</html>