<html>
<head>
<title>k-Nearest Neighbors and the Curse of Dimensionality</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k 近邻和维数灾难</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d?source=collection_archive---------10-----------------------#2019-07-22">https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d?source=collection_archive---------10-----------------------#2019-07-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b3a7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为什么 k-最近邻算法对额外的维度特别敏感</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/620956970435e7f307a62596da73f402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7SYBgrbNOfbD45I3NRxeKA.jpeg"/></div></div></figure><p id="0789" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<a class="ae lq" rel="noopener" target="_blank" href="/introducing-k-nearest-neighbors-7bcd10f938c5">我的上一篇文章</a>中，我们学习了 k 近邻建模算法。该算法基于附近数据点与测试点相似的假设，对所需数据点进行分类预测。为了发挥作用，它需要两样东西:</p><ul class=""><li id="4460" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">首先，它需要你相信它附近的邻居与它相似的假设。在某些情况下，这是正确的。例如，杂货店中的产品通常被分类到相似的组中。嘎拉苹果和嘎拉苹果存放在一起，嘎拉苹果存放在富士苹果旁边。假设一个节日苹果旁边的物品是另一个节日苹果是非常安全的，假设相邻的物品是一个苹果也是非常安全的。然而，在其他情况下，相邻项目相似的假设是没有用的。我醉抽屉里的一个物品是电池，不代表旁边的物品也是电池。它很可能是一支记号笔、一个回形针或一根橡皮筋。</li><li id="3e8b" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">第二，你需要有一种测量距离的方法。注意，这是空间距离，不一定是物理距离。它指的是你试图比较的两个数据点之间的差异。在上面的例子中，我使用物理距离作为距离，但它也可以是两个房子的平方英尺的差异，两个设备之间的效率差异，等等。</li></ul><p id="5e6b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这意味着使用 k-最近邻算法的成功很大程度上取决于拥有密集的数据集。这使得它特别容易受到“维数灾难”的影响。</p><h2 id="cca6" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">什么是“维数灾难”？</h2><p id="5f03" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">“维数灾难”是一种半开玩笑的说法，即在高维数据集中有大量的<em class="nd">空间</em>。数据空间的大小随着维度的数量呈指数增长。这意味着，为了保持相同的密度，数据集的大小也必须呈指数增长。如果你不这样做，那么数据点开始变得越来越远。</p><h2 id="22cd" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">为什么这对于 k 近邻来说特别成问题？</h2><p id="7eec" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">从表面上看，k 近邻似乎对这个问题并不特别敏感。每个机器学习算法都需要密集的数据集，以便在整个数据空间中进行准确预测。如果数据之间有间隙，所有算法都会出错。那么是什么让 k 近邻变得特别呢？</p><p id="13df" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">k-最近邻的特殊挑战是它要求一个点在每一维上都是接近的。一些算法可以基于单个维度创建回归，并且只需要点沿着该轴靠近。k 近邻不是这样工作的。它需要所有的点在数据空间的每个轴上都是靠近的。而每增加一个新的轴，通过增加一个新的维度，使得每一个轴上的两个特定点越来越难相互靠近。</p><p id="3bc7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Joel Grus 在《从零开始的数据科学》中很好地描述了这个问题。在那本书里，他计算了随着维度数量的增加，一个维度空间中两点之间的平均和最小距离。他计算了 10，000 个点之间的距离，维数从 0 到 100 不等。然后，他继续绘制两点之间的平均和最小距离，以及最近距离与平均距离的比率(Distance _ Closest/Distance _ Average)。</p><p id="f4cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这些图中，Joel 显示了最近距离与平均距离的比率从 0 维的 0 增加到 100 维的 0.8。这显示了当使用 k-最近邻算法时对维度的基本挑战；随着维数的增加，最近距离与平均距离的比率接近 1，算法的预测能力降低。如果最近点几乎与平均点一样远，那么它的预测能力仅比平均点略高。</p><p id="90dc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">思考这个问题如何应用到我们上面的苹果例子中。这有点违背直觉，因为苹果的例子本来就是三维的，但是想象一下，到农产品区中下一个最近的商品的距离大约与平均距离相同。突然，你不能确定最近的东西是另一个苹果，还是一个富士苹果，或者一个橘子，或者一束欧芹。就 k-最近邻算法而言，整个生产部分将会混杂在一起。</p><h2 id="1508" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">当使用 k-最近邻算法时，我如何克服维数灾难？</h2><p id="5cd9" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">从根本上来说，问题在于没有足够的数据来衡量维度的数量。随着维度数量的增加，数据空间的大小也会增加，保持密度所需的数据量也会增加。如果数据集的大小没有显著增加，k 近邻将失去所有的预测能力。这使得这个问题的一个可能的解决方案很简单:添加更多的数据。添加越来越多的数据以确保即使在添加更多维度时也有足够的数据密度是完全可能的。如果你有硬件来处理这么大的数据量，这是一个完全合理的解决方案。</p><p id="6958" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当然，您不可能总是拥有添加那么多数据所必需的硬件。不是每个数据科学家都买得起超级计算机。即使这样，也有可能有足够大的数据集，即使是超级计算机也无法在合理的时间内处理它。这就是降维概念发挥作用的地方。降维超出了本文的范围，但是我将在下一篇文章中讨论它。本质上，它指的是识别数据集中的趋势，这些趋势沿着数据集中没有明确指出的维度运行。然后，您可以创建与这些轴匹配的新维度，并删除原始轴，从而减少数据集中的轴总数。</p><p id="1ee9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">举个例子，假设你在一个城市里绘制了花园侏儒的位置。你得到每个花园侏儒的 GPS 坐标，并把它们标在地图上。你有南北和东西两个维度。现在想象一下，出于某种原因，所有的花园侏儒都被放置在一条穿过城镇的近乎对角线的线上。一个在东南角，一个在西北角，两者之间几乎是一条直线。现在，您可以创建一个名为东南-东北轴的新轴(或者，如果您想变傻，也可以称为“花园侏儒分界线”)，并删除南北和东西轴。这样你就把你的数据集从二维减少到了一维，并且使得 k-最近邻算法更容易成功。</p><h2 id="09a9" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">包装它</h2><p id="821f" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">k-最近邻算法依赖于数据点相互靠近。随着维数的增加，这变得具有挑战性，被称为“维数灾难”对于 k-nearest neighbors 算法来说尤其困难，它要求两个点在每个轴的<em class="nd">上非常接近，并且增加一个新的维度为点的远离创造了另一个机会。随着维数的增加，两点之间的最近距离接近两点之间的平均距离，从而消除了 k-最近邻算法提供有价值的预测的能力。</em></p><p id="8b4f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了克服这一挑战，您可以向数据集中添加更多的数据。通过这样做，您可以增加数据空间的密度，使最近的点更加接近，并恢复 k-最近邻算法提供有价值的预测的能力。只要您有在数据集上执行计算所需的硬件，这就是一个有价值的解决方案。随着数据集变得越来越大，您需要越来越多的计算能力来处理它。最终，数据集的规模将超过你的计算能力。此时，您需要使用维度缩减来用更少的维度呈现所有有价值的信息。该主题将在未来的文章中进行更详细的描述。</p></div></div>    
</body>
</html>