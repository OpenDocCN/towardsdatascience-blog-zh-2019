<html>
<head>
<title>Overcome Overfitting During Instance Segmentation with Mask-RCNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Mask-RCNN 克服实例分割过程中的过拟合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overcome-overfitting-during-instance-segmentation-with-mask-rcnn-32db91f400bc?source=collection_archive---------3-----------------------#2019-12-20">https://towardsdatascience.com/overcome-overfitting-during-instance-segmentation-with-mask-rcnn-32db91f400bc?source=collection_archive---------3-----------------------#2019-12-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="14a3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">仅在 1，349 幅带注释的图像上训练 Mask-RCNN</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c561604bb68099ef6b885ba76cd6975a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5EsdDTv9-5kqK0hu9fIJw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Not bad for a model trained on such a tiny dataset :)</figcaption></figure><h1 id="0862" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="f1a6" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">计算机视觉的进步拥有许多有前途的应用，如自动驾驶汽车或医疗诊断。在这些任务中，我们依靠机器识别物体的能力。</p><p id="a91a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们经常看到的与物体识别相关的任务有<strong class="ls iu">四个</strong>:分类定位、物体检测、语义分割、实例分割。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mr"><img src="../Images/d48768b30881308961bf2eeaa407dd30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iT3zaWgqiOd18vtiEOWEEw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ms" href="https://www.oreilly.com/ideas/introducing-capsule-networks?cmp=tw-data-na-article-ainy18_thea" rel="noopener ugc nofollow" target="_blank">https://www.oreilly.com/ideas/introducing-capsule-networks?cmp=tw-data-na-article-ainy18_thea</a></figcaption></figure><p id="5a1a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在<strong class="ls iu">分类和定位</strong>中，我们感兴趣的是给图像中的物体分配类别标签，并在物体周围绘制一个包围盒。在该任务中，要检测的对象的数量是<strong class="ls iu">固定的</strong>。</p><p id="aed8" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">物体检测</strong>不同于分类和定位，因为在这里，我们不预先假设图像中物体的数量。我们从一组固定的对象类别开始，我们的目标是分配类别标签，并在每次这些类别中的对象出现在图像中时绘制边界框<strong class="ls iu">。</strong></p><p id="d33a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在<strong class="ls iu">语义分割</strong>中，我们给<strong class="ls iu">每个图像像素</strong>分配一个类标签:所有属于草的像素都标为“草”，属于羊的标为“羊”。例如，值得注意的是，这项任务并不能区分两只羊。</p><p id="5002" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们在这个任务中的任务是<strong class="ls iu">实例分割</strong>，它建立在对象检测和语义分割的基础上。与对象检测一样，我们的目标是标记和定位预定义类别中对象的所有实例。然而，我们不是为检测到的对象生成边界框，而是进一步识别哪些像素属于该对象，就像在语义分割中一样。语义分割的不同之处在于，实例分割为每个对象实例绘制一个<strong class="ls iu">单独的掩码</strong>，而语义分割将为同一类的所有实例使用相同的掩码</p><p id="3709" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在本文中，我们将在一个微型 Pascal VOC 数据集上训练一个实例分割模型，只有 1349 张图像用于训练，100 张图像用于测试。这里的主要挑战将是在不使用外部数据的情况下防止模型过度拟合。</p><p id="ed72" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">你可以在<a class="ae ms" href="https://github.com/kayoyin/tiny-instance-segmentation" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到所使用的数据集以及完整的训练和推理管道。</p><h1 id="885a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">数据处理</h1><p id="cd54" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">注释是 COCO 格式的，所以我们可以使用来自<strong class="ls iu"> pycocotools </strong>的函数来检索类标签和掩码。在这个数据集中，总共有<strong class="ls iu"> 20 </strong>个类别。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/a4cb0f37c8ff18835cc6f81fdae35904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjBjE0Dnbx7gS9o0hMUVUg.png"/></div></div></figure><p id="96ea" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">以下是训练图像和相关遮罩的一些可视化效果。遮罩的不同阴影表示同一对象类别的几个实例的单独遮罩。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/507d3a6ee1df72ce43f9bf70019bc6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2tcDolLIKreYV3nTlbEd9A.png"/></div></div></figure><p id="c00c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这些图像具有不同的大小和长宽比，因此在将图像输入模型之前，我们<strong class="ls iu">调整</strong>每张图像的尺寸为<strong class="ls iu"> 500x500 </strong>。当图像尺寸小于 500 时，我们放大图像，使最大的边长为 500，并根据需要添加零填充以获得正方形图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/aebaa1831dde7a78d440e1aa8ffe8af8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O05MhnIE1ZMCi_gdpFU02Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Example input before (left) and after resizing with the associated annotations (right)</figcaption></figure><p id="cba5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">为了让模型更好地泛化，尤其是在像这样的有限数据集上，<strong class="ls iu">数据扩充</strong>是克服过度拟合的关键。对于每个图像，以概率 0.5 执行水平翻转，将图像随机裁剪为原始尺寸的 0.9 到 1 倍之间的比例，以概率 0.5 执行具有随机标准偏差的高斯模糊，以 0.75 到 1.5 之间的比例调整对比度，以 0.8 到 1.2 之间的比例调整亮度，并且还应用一系列随机仿射变换，例如缩放、平移、旋转和剪切。</p><h1 id="0d47" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">掩模-RCNN</h1><p id="8e45" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们将使用 matterport 的 Mask-RCNN 的<a class="ae ms" href="https://github.com/matterport/Mask_RCNN" rel="noopener ugc nofollow" target="_blank">实现</a>进行训练。虽然很诱人，但我们将<strong class="ls iu">而不是</strong>使用他们为 COCO 女士预先训练的权重来展示我们如何仅使用 1，349 张训练图像就可以获得良好的结果。</p><p id="2175" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">Mask-RCNN 于 2017 年在<a class="ae ms" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank"> Mask-RCNN 论文</a>中提出，是同一作者对<strong class="ls iu">fast-RCNN</strong>的扩展。fast-RCNN 广泛用于对象检测，其中模型在检测到的对象周围生成边界框。Mask-RCNN 更进一步，生成对象<strong class="ls iu">屏蔽</strong>。</p><p id="052e" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我将在下面提供一个模型架构的快速概述，matterport 发表了一篇很棒的<a class="ae ms" href="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46" rel="noopener ugc nofollow" target="_blank">文章</a>，详细介绍了他们的模型实现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/f58823fa6c980d7ab113adae7b43bcaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zfdE_dJsW5YbMHoe.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ms" href="https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272" rel="noopener">https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272</a></figcaption></figure><p id="f077" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">首先，我们使用一个<strong class="ls iu">主干</strong>模型从输入图像中提取相关特征。这里，我们使用 T21 架构作为主干。图像从形状张量(500，500，3)转换成形状特征图(32，32，2048)。</p><p id="b15f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">先前获得的特征然后被输入到<strong class="ls iu">区域提议网络</strong> (RPN)。RPN 扫描特征图的区域，称为<strong class="ls iu">锚</strong>，并试图确定包含对象的区域。这些锚具有不同的尺寸和纵横比。RPN 为每个锚点分配一个锚点类别:前景(正锚点)或背景(负锚点)。中性主播是不影响训练的主播。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/63eed6be1f32bd52e07f152304180c7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KOpsYriccrC-DCIjnpuu0g.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Positive anchors (left), neutral anchors (center), negative anchors (right)</figcaption></figure><p id="bbf5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">一个<strong class="ls iu">提议</strong>层然后挑选最有可能包含一个对象的锚，并<strong class="ls iu">提炼</strong>锚框以更紧密地适应对象。当太多锚点重叠时，仅保留前景分数最高的锚点(<strong class="ls iu">非最大抑制</strong>)。这样，我们就获得了感兴趣的区域<strong class="ls iu"/>(ROI)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/fd83b853f2229db8a01e46087e29fe6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MdF0aZ9B6Nz1z5qI1GUsXg.png"/></div></div></figure><p id="b506" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">对于包含由 ROI <strong class="ls iu">分类器</strong>选择的对象的每个区域，该模型然后生成 28×28 个掩模。在训练期间，地面真实遮罩被缩小以利用预测遮罩计算损失，并且在推断期间，生成的遮罩被放大到 ROI 边界框大小。</p><h1 id="119d" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">迁移学习</h1><p id="5595" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">更快更好地训练模型的关键是<strong class="ls iu">迁移学习</strong>，尤其是在数据有限的情况下。Imagenet 数据集是一个巨大的自然图像库，类似于我们的图像。因此，我们可以将 Resnet101 主干模型的权重初始化为在 Imagenet 上预先训练的权重。这将提高我们获得的特征地图的准确性，并因此提高整个模型的准确性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/86332835dbc44bfeb8abd3f048254271.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bq-XGeMBZ5Dfocoa.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ms" href="https://www.researchgate.net/figure/Examples-in-the-ImageNet-dataset_fig7_314646236" rel="noopener ugc nofollow" target="_blank">ImageNet sample images</a></figcaption></figure><p id="7d2b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">为了微调预先在 Imagenet 上训练的模型，我们首先只训练<strong class="ls iu">模型</strong> <strong class="ls iu">头部</strong>。然后，我们训练从 ResNet <strong class="ls iu">阶段 4 到剩余时期的</strong>的层。这个训练方案也有助于<strong class="ls iu">将</strong> <strong class="ls iu">过拟合</strong>最小化。我们可以允许自己冻结并且从不微调第一层，因为我们可以重用模型学习的权重来从自然图像中提取特征。</p><h1 id="a1f3" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">结果和检测管道可视化</h1><p id="1dae" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">训练 20 个时期的模型头部，以及 25 个额外时期的其余选定模型层，允许我们在测试集<strong class="ls iu"> 0 上获得地图分数。53650 </strong>。以下是随机选择的测试图像上模型输出的一些可视化效果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/2b362762c9d5d1947766cde521ed0ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b-3Y5sdY5KSXuK5DL9CD_Q.png"/></div></div></figure><p id="91b9" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们还可以可视化算法不同步骤的输出。下面，我们有边界框细化前的得分排名靠前的锚点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/6405e9d04659281be77ac8ce175c46bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73gcx-BQ2Hiaw7QOm8j1gA.png"/></div></div></figure><p id="13d3" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">接下来，在边界框细化和非最大值抑制之后，我们有了输出锚点。这些建议然后被输入分类网络。请注意，在这一点上，我们有一些框紧密地绑定了一些对象，如标志，它们不属于我们已经定义的对象类别。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/0d8cd9f98988cee7f6e777575d123fc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UxiABJ839vNHPF8McnXbHw.png"/></div></div></figure><p id="ec7e" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">分类网络运行该提议以确定肯定检测，生成类别概率和边界框回归。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/bece91ac22ac892f9cf77edaae798f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Se0TheuvyvMNwbTNv5u5iA.png"/></div></div></figure><p id="83a8" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在获得边界框并对它们进行细化之后，实例分割模型为每个检测到的对象生成遮罩。在训练期间，遮罩是软遮罩(具有浮点像素值)并且大小为 28×28。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/ba84a565fe682c4bb1df7fb91323eeab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*czEyji_49onp2fL6by2zpQ.png"/></div></div></figure><p id="db8a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">最后，预测的遮罩被重新缩放到边界框尺寸，我们可以将它们覆盖在原始图像上，以可视化最终输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/52a8837e60517a935b837e797a6bc45c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rg1VpkYqMQ-Y4mWx6KiA2g.png"/></div></div></figure></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><p id="3c95" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">感谢阅读这篇文章，我希望你喜欢！</p><p id="bf95" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">还有许多其他最先进的架构<a class="ae ms" href="https://paperswithcode.com/task/instance-segmentation" rel="noopener ugc nofollow" target="_blank">例如分割，您也可以尝试，如果您在这个小小的 COCO 数据集上尝试了这些架构，请告诉我:)</a></p><p id="a428" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">您可能还想了解我如何在 3859 张灰度图像上训练 ResNet 分类器，从而在我的课堂 Kaggle 挑战赛中获得第二名:</p><div class="nm nn gp gr no np"><a rel="noopener follow" target="_blank" href="/latest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">利用有限数据进行 Kaggle 图像分类的最新成功技术</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">关于如何防止模型在小数据集上过度拟合但仍能进行准确分类的教程</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">towardsdatascience.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od ks np"/></div></div></a></div></div></div>    
</body>
</html>