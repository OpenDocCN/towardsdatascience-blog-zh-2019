<html>
<head>
<title>Principal Component Analysis and SVM in a Pipeline with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现流水线中的主成分分析和 SVM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualizing-support-vector-machine-decision-boundary-69e7591dacea?source=collection_archive---------3-----------------------#2019-07-13">https://towardsdatascience.com/visualizing-support-vector-machine-decision-boundary-69e7591dacea?source=collection_archive---------3-----------------------#2019-07-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="683c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">管道、网格搜索和等高线图</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/53afc304311f41f8285355d4e4919a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sjn1i4MXE6yHjQIDtC_33Q.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Decision Boundary (Picture: Author’s Own Work, Saitama, Japan)</figcaption></figure><p id="1e6c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在之前的一篇文章中，我已经详细描述了关于主成分分析的<a class="ae lu" rel="noopener" target="_blank" href="/dive-into-pca-principal-component-analysis-with-python-43ded13ead21">和支持向量机(SVM)算法</a>背后的<a class="ae lu" rel="noopener" target="_blank" href="/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e">数学。在这里，我将结合 SVM、主成分分析和网格搜索交叉验证来创建一个管道，以找到二元分类的最佳参数，并最终绘制一个决策边界来展示我们的算法表现得有多好。你希望在这篇文章中学到/回顾的东西—</a></p><ul class=""><li id="17b6" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">通过<a class="ae lu" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank"> Seaborn Library </a>以有意义的方式联合绘制和表示数据。</li><li id="8c03" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">如果主成分分析中有 2 个以上的成分，如何选择和表示哪 2 个成分比其他成分更相关？</li><li id="7f8a" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">使用 PCA 和 SVM 创建管道，通过网格搜索交叉验证找到最佳拟合参数。</li><li id="c835" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">最后，我们选择 2 个主成分来表示 3d/2d 图中的 SVM 决策边界，使用<a class="ae lu" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"> Matplotlib </a>绘制。</li></ul><h2 id="e26b" class="mj mk it bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">1.更好地了解数据集:联合图和 Seaborn</h2><p id="8d17" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在这里，我使用了<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn 癌症数据集</a>，这是一个相对简单的数据集，用于研究二元分类，分为恶性和良性两类。让我们来看几行数据帧。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/381775fb16c8a04cda2a177ca9642bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S5hxEiQwjBbTygSqQbc6ug.png"/></div></div></figure><p id="feea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如我们所见，数据集中总共有 569 个样本和 30 个特征，我们的任务是将恶性样本与良性样本进行分类。在确认<a class="ae lu" rel="noopener" target="_blank" href="/data-handling-using-pandas-cleaning-and-processing-3aa657dc9418">没有缺失数据</a>后，我们检查特征名称并检查平均特征的相关图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/d24cdcaa944f3188bdbcb289c45eccec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5vCHBGKnrN7XUGwoVWNmdg.png"/></div></div></figure><p id="6a23" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是使用<a class="ae lu" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank"> seaborn 库</a>封装的平均特征的相关图。正如所料，“面积”、“周长”和“半径”高度相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/8042eba9f6ef26d03157266a1e81810f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eEhpOmaKH8k4IQFmiOZtHg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 1: Correlation plot of mean features.</figcaption></figure><p id="fc49" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以使用'<a class="ae lu" href="https://seaborn.pydata.org/generated/seaborn.jointplot.html" rel="noopener ugc nofollow" target="_blank"> seaborn jointplot </a>'来了解各个特征之间的关系。让我们看看下面的两个例子，作为散点图的替代，我选择了 2D 密度图。在右边的面板上，我使用了“十六进制”设置，通过直方图，我们可以了解一个小的六边形区域内的点的数量的集中程度。六边形越暗，落在该区域的点(观察值)越多，这种直觉也可以用绘制在两个特征边界上的直方图来检验。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/dcb571e879d8a14d2fcbb6fdc1812c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2k4XVwsras-D3QKR-69byQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 2: Joint-plots can carry more info than simple scatter plots.</figcaption></figure><p id="e82b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在左侧，除了绘制在边界上的单个要素的直方图之外，等值线表示 2D 核密度估计(KDE)。不仅仅是离散的直方图，KDE 直方图通常是有用的，你可以在这里找到一个奇妙的解释<a class="ae lu" href="https://mathisonian.github.io/kde/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="a420" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还可以绘制一些配对图，以研究哪些特征与区分良性样本和恶性样本更相关。让我们看下面的一个例子——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/df61207855aba66b44d9521893cdf984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uo4cJSIxMpJArRvwEJCxYA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 3: Pair plots of few features in Cancer data-set. Code can be found in my GitHub.</figcaption></figure><p id="d6dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们玩够了数据集来探索和理解我们手头的东西，那么，让我们转向主要的分类任务。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="149b" class="mj mk it bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">2.管道、GridSearchCV 和支持向量机:</h2><p id="7d1c" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated"><strong class="la iu">2.1<em class="nt">。主成分:</em> </strong></p><p id="dd52" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们将按照以下步骤，使用 StandardScaler、PCA 和支持向量机创建一个管道—</p><ul class=""><li id="d6f7" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">从分割训练集和测试集中的数据集开始</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/322ce70308f147f9c522d14cc9e03256.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ph6f4xvrXu8IwVPVUghuw.png"/></div></div></figure><ul class=""><li id="b25a" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">检查主成分分析的效果:PCA 以这样一种方式降低特征空间的维度，使得最终特征彼此正交。因为我们在癌症数据集中有 30 个特征，所以可视化 PCA 实际上做什么是很好的。你可以在我关于 PCA 的<a class="ae lu" rel="noopener" target="_blank" href="/dive-into-pca-principal-component-analysis-with-python-43ded13ead21">另一篇文章中读到更多细节。在应用 PCA 之前，我们通过减去平均值来标准化我们的特征，并使用标准缩放器将其缩放到单位方差。所以，我们从选择 4 个正交分量开始—</a></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/31e71c5869f8e67b132b6a2818ae8061.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LccgW3ETrgeoopsjEhwKw.png"/></div></div></figure><p id="4ad4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们绘制这四个主要成分的癌症数据——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/df8190eb7e5f73c4d2e81f2a86d46af6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TAtXUWRAd8xut29aJPeJcA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 4: Which principal components are more relevant?</figcaption></figure><p id="6b55" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从图中可以看出，前两个主成分与区分恶性和良性样本更相关。方差比怎么样？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/bc31c3cd6d05ae91c021909ea11ee294.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e3YhuueeeZJLxSWePbcK7g.png"/></div></div></figure><p id="d6c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如预期的那样，前两个因素占总方差的 80%。这与在选择 2 个分量来绘制决策边界之前显示相关，因为您可能有一些具有许多特征的数据集，其中选择 2 个主分量在百分比方差比方面是不合理的。在使用 PCA 和一些分类器创建管道之前，最好检查两个主成分的选择。</p><p id="5fab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="nt"> 2.2。管道&amp;网格搜索交叉验证:</em>T3</strong></p><p id="74dd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们看到 PCA 对于分类和为分类器绘制决策边界是多么重要，现在，我们用标准缩放器、PCA 和 SVM 创建一个流水线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/57bb562c76383368c46b3a0c00bd1ec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YON-WSij1Gu0mGfwR8KecA.png"/></div></div></figure><p id="c060" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以查看更多关于管道和网格搜索交叉验证的细节，那是我单独写的<a class="ae lu" rel="noopener" target="_blank" href="/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976"/>。我选择了 2 个主成分，因为我们的目标是在 2D/3D 图中绘制决策边界，并且使用径向基函数核的 SVM 的最佳参数“C”和“Gamma”是通过固定数量的主成分值获得的。让我们检查一下拟合参数—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/c94bc7ddabca25d24fb81a0826e42e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VQ5rwffPnnHN6ZruqvlbUw.png"/></div></div></figure><p id="d9a5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我们看到，使用 2 个主成分和 4 个交叉验证，我们的管道与 SVM 分类器获得了 94%的准确性。当然，你可以使用不同的值或者使用不同的内核。关于内核函数背后的数学，你可以查看我的另一篇文章。在进入下一部分之前，我们可以通过绘制混淆矩阵来完成分析，从中可以获得精确度、召回率和 F1 分数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/3dca1c87ee5aef71746a2de79e325ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-rkxX8UoZDn-S39pKLrsBw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 5: Confusion Matrix obtained using our pipeline with SVM classifier and RBF kernel.</figcaption></figure></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="52c0" class="mj mk it bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">3.地块 SVM 决定边界:</h2><p id="1391" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">我已经按照 Scikit-Learn 的教程绘制了<a class="ae lu" href="https://scikit-learn.org/0.20/auto_examples/svm/plot_iris.html" rel="noopener ugc nofollow" target="_blank">最大间隔分离超平面</a>，但是这里使用的不是教程中使用的线性核，而是径向基函数核。我们将使用<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.decision_function" rel="noopener ugc nofollow" target="_blank">决策函数</a>方法，该方法返回样本中每个类的决策函数。直观上，对于二元分类，我们可以把这种方法想象成它告诉我们，我们在分类器生成的超平面的哪一边，有多远。对于 SVM 决策规则的数学公式，如果你有兴趣，可以查看<a class="ae lu" rel="noopener" target="_blank" href="/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e">我以前的帖子</a>。让我们看看决策函数的轮廓</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/7e408db54999072b7cde8fc00d56a7a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpggDi84gF-4bWy8KYGKAg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 6: Decision Function contours with Radial Basis Function kernel is shown here along with the support vectors. Best fit-parameters are obtained from 5 fold grid search cross-validation.</figcaption></figure><p id="32cc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来理解我用来绘制上图的代码</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/a9dcb75687090aeb9f89ef5b3b8a7735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ic-oyut0TzH2v9awYOII1A.png"/></div></div></figure><ul class=""><li id="59bb" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">从选择 2 个主成分进行主成分分析开始。只有 2 个组件，因为它有助于我们在 2D/3D 绘图中可视化边界。将 2 个以上的组件与决策函数轮廓一起可视化是有问题的！</li><li id="44ee" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">设置具有径向基函数核的 SVM 分类器，并且将“C”、“gamma”参数设置为从网格搜索交叉验证中获得的最佳拟合值。</li><li id="b2de" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">定义一个函数来创建由 x、y(最终选择的主成分)和 Z(SVM 的决策函数)组成的等值线。</li><li id="8efb" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">我们通过函数 make-meshgrid 用一个 x 值数组和一个 y 值数组创建一个矩形网格。检查<a class="ae lu" href="https://stackoverflow.com/questions/36013063/what-is-the-purpose-of-meshgrid-in-python-numpy" rel="noopener ugc nofollow" target="_blank"> Numpy Meshgrid </a>的必要性。</li><li id="19cf" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">最后，我们将决策函数绘制为 2D 等高线图，并将支持向量绘制为分散点。</li></ul><p id="33ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">希望这篇文章能帮助你理解建立支持向量机分类器的策略，并有效地使用主成分分析来可视化决策边界。</p><p id="be86" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来看看它的 3D 动画——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/484ee628ff90e5c338835eb1eef02c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*z_-wtRw98AJx-9qBns_rRg.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 7: Decision Function contours as viewed from top. 3D representation for the figure 6.</figcaption></figure></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="918f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我强调检查你的理解总是好的，在这里我们可以看到伽马因子如何影响决策函数轮廓。</p><p id="4f85" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于上面的图，我们有γ = 0.5，C = 1。你可以阅读我的<a class="ae lu" rel="noopener" target="_blank" href="/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d">关于 SVM 内核的另一篇文章</a>，在那里我已经讨论了增加γ参数如何导致复杂的决策边界。</p><p id="f3b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们通过将“C”参数固定为最佳拟合值，使用两个不同的“Gamma”值来检查这一点</p><ul class=""><li id="ce40" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated"><strong class="la iu">为γ = 0.01 </strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/8256420dd9c21adc9ea58e8fc94d63d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lSGgwEqxl2jtIpudB9-Mbg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 8: Decision function contours plotted with low gamma (γ=0.01) value. Can be compared with figure 6, where γ = 0.5.</figcaption></figure><ul class=""><li id="7aca" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated"><strong class="la iu">对于γ = 10.0 </strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/3f88b32cb6d0624ef605163de534679b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c0wkdj0KCTtACO4HEdIUyg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 9: High gamma parameter is causing extremely complicated decision boundaries.</figcaption></figure><p id="f06d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如你所看到的，增加伽马参数会产生非常复杂的轮廓。最重要的是，几乎所有高伽马值的样本(每个类别的)都充当支持向量。这无疑是对如此简单、容易分类的癌症数据集的过度拟合。</p><blockquote class="oe of og"><p id="ee39" class="ky kz nt la b lb lc ju ld le lf jx lg oh li lj lk oi lm ln lo oj lq lr ls lt im bi translated">“永远检查你的直觉和理解力！!"—匿名</p></blockquote></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="34d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了进一步阅读，我建议你查看塞巴斯蒂安·拉什卡(Sebastian Raschka)的《用 Python 进行<a class="ae lu" href="https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka-ebook/dp/B00YSILNL0" rel="noopener ugc nofollow" target="_blank">机器学习》(第 76–88 页，2017 年 9 月第二版)一书中给出的几个精彩演示。</a></p><p id="5c16" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">保持坚强和快乐，干杯！</p><p id="56c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<a class="ae lu" href="https://github.com/suvoooo/Machine_Learning/tree/master/SVM_Decision_Boundary" rel="noopener ugc nofollow" target="_blank"> Github </a>中找到这篇文章使用的代码。</p></div></div>    
</body>
</html>