<html>
<head>
<title>Comparing Different Classification Machine Learning Models for an imbalanced dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不平衡数据集上不同分类机器学习模型的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparing-different-classification-machine-learning-models-for-an-imbalanced-dataset-fdae1af3677f?source=collection_archive---------1-----------------------#2019-02-02">https://towardsdatascience.com/comparing-different-classification-machine-learning-models-for-an-imbalanced-dataset-fdae1af3677f?source=collection_archive---------1-----------------------#2019-02-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="5b24" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果一个数据集包含来自一个类的样本比来自其余类的样本多得多，则称之为不平衡数据集。当至少一个类仅由少量训练样本(称为少数类)表示，而其他类占大多数时，数据集是不平衡的。在这种情况下，由于较大的多数类的影响，分类器可以对多数类具有良好的准确性，但对少数类的准确性非常差。这种数据集的常见示例是信用卡欺诈检测，其中欺诈= 1 的数据点通常比欺诈= 0 的数据点少得多。</p><p id="ec2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据集可能不平衡的原因有很多:第一个目标类别在人群中可能非常罕见，或者数据可能很难收集。</p><p id="4eb5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们通过处理一个这样的数据集来解决不平衡数据集的问题。</p><p id="2402" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">关于数据:</strong>我这里是一家游戏公司的数据，他们想知道哪个客户会成为 VIP 客户。这里的目标变量有两个值:0(代表非 VIP 客户)和 1(代表 VIP 客户)。在可视化该目标列时，我们可以从下面的饼图中看到，只有 1.5%的数据是 VIP 客户的，其余 98.5%的数据是非 VIP 客户的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/9ba9e166b7dd9a65aed0875aaafd2c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*ofOlu6RNcoaG0mHlixIE1g.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk"><strong class="bd la">Distribution of Majority and Minority Class</strong></figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi lb"><img src="../Images/69eca262fb1bfed7b8805f295122b3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5yb4y_v2nNf9zvMg5CCug.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk"><strong class="bd la">First few rows of the dataset</strong></figcaption></figure><p id="a9eb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们使用这些数据来训练我们的预测模型，那么该模型将表现不佳，因为该模型没有在代表 VIP 客户的足够数量的数据上进行训练。为了演示如果我们使用这些数据，预测模型的结果会如何，下面是一些算法的结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/7826678b7930c178876f1e64db3a850d.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*ZV2TW3psvVfSOYZVl4-8jg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk"><strong class="bd la">Results from the k-NN Algorithm</strong></figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/6d92736abb811c06dcd6de17e1b891c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*G_fqT4AjOTJPf49-g8gc-A.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk"><strong class="bd la">Results from the Gradient Boosting Algorithm</strong></figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/92d231ac0046e7fc0e77653987d5f68b.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*HQ-2guOKZYWhUy3Rs5SQqA.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk"><strong class="bd la">Results from the Logistic Regression Algorithm</strong></figcaption></figure><p id="88ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从上面的结果可以清楚地看出，机器学习算法几乎无法预测 VIP 客户(因为 VIP 客户的 f1 分数非常低)。</p><p id="6759" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在监督学习中，克服类别不平衡问题的一种常见策略是对原始训练数据集进行重新采样，以降低类别不平衡的整体水平。重采样是通过对少数(正)类进行过采样和/或对多数(负)类进行欠采样来完成的，直到这些类被近似相等地表示。</p><p id="93ff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在使用不同的重采样技术之前，需要记住的最重要的一点是<strong class="js iu">所有的重采样操作都必须只应用于训练数据集</strong>。如果在将数据集分割成训练集和验证集之前进行上采样，则可能在两个数据集中得到相同的观察结果。因此，当在验证集上进行预测时，机器学习模型将能够完美地预测这些观察值，从而提高准确性和召回率。</p><p id="e3cf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">重采样技术的类型:</p><p id="5a44" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">欠采样/下采样:</strong>该方法使用多数类的子集来训练分类器。由于许多多数类示例被忽略，训练集变得更加平衡，训练过程变得更快。最常见的预处理技术是随机多数欠采样(RUS)，在 RUS，多数类的实例从数据集中随机丢弃。下面是实现缩减采样的 python 代码</p><pre class="kp kq kr ks gt li lj lk ll aw lm bi"><span id="7591" class="ln lo it lj b gy lp lq l lr ls">from sklearn.utils import resample</span><span id="4395" class="ln lo it lj b gy lt lq l lr ls">df_majority_downsample = resample(df_majority, <br/>                                 replace=True,     <br/>                                 n_samples=105,    <br/>                                 random_state=123) <br/>df_train = pd.concat([df_majority_downsample, df_minority_upsampled])<br/><br/> <br/># Display new class counts<br/>print (df_train.IsVIP_500.value_counts())</span><span id="9f32" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">Output:<br/></strong>0    105<br/>1    105<br/>Name: IsVIP_500, dtype: int64</span></pre><p id="56b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">过采样:</strong>它通过复制少数类实例来实现更均衡的类分布。过采样不会丢失任何信息，因为少数类和多数类的所有原始实例都保留在过采样数据集中。下面是实现上采样的 python 代码。</p><pre class="kp kq kr ks gt li lj lk ll aw lm bi"><span id="d1d1" class="ln lo it lj b gy lp lq l lr ls">df_minority_upsampled = resample(df_minority, <br/>                                 replace=True,     <br/>                                 n_samples=5000,    <br/>                                 random_state=123) <br/>df_train = pd.concat([df_majority, df_minority_upsampled])<br/><br/> <br/># Display new class counts<br/>print (df_train.IsVIP_500.value_counts())</span><span id="85b7" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">Output:<br/></strong>0    6895<br/>1    5000<br/>Name: IsVIP_500, dtype: int64</span></pre><p id="c600" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">尽管这两种方法都解决了阶级不平衡的问题，但它们也有一些缺点。随机欠采样方法可能会删除某些重要的数据点，而随机过采样可能会导致过拟合。</p><p id="d29d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> SMOTE: </strong>合成少数过采样技术被设计用来生成与少数类分布一致的新样本。主要思想是考虑样本之间存在的关系，并沿着连接一组邻居的线段创建新的合成点。下面是实现 SMOTE 的 python 代码。</p><pre class="kp kq kr ks gt li lj lk ll aw lm bi"><span id="dcee" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu">#SMOTE</strong><br/>from imblearn.over_sampling import SMOTE<br/>import numpy as np<br/></span><span id="494d" class="ln lo it lj b gy lt lq l lr ls">sm = SMOTE(random_state=12)<br/>x_train_res, y_train_res = sm.fit_sample(X_train, Y_train)</span><span id="48d3" class="ln lo it lj b gy lt lq l lr ls">print (Y_train.value_counts() , np.bincount(y_train_res))</span><span id="2fed" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">Output: <br/>#previous distribution of majority and minority classes</strong></span><span id="64fb" class="ln lo it lj b gy lt lq l lr ls">0    6895<br/>1     105</span><span id="12db" class="ln lo it lj b gy lt lq l lr ls">#<strong class="lj iu">After SMOTE, distirbution of majority and minority classes</strong><br/>0    6895<br/>1    6895</span></pre><p id="aff5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们的数据已经准备好了，让我们在 SMOTE 创建的数据集上应用一些机器学习算法。我尝试了以下算法:Logistic 回归，K 近邻，梯度推进分类器，决策树，随机森林，神经网络。以下是最高性能的机器学习算法的结果和解释:</p><p id="d380" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">随机森林:</strong>基本思想与 bagging 非常相似，我们引导样本，因此我们对训练数据集进行重新采样。然后我们在每个引导样本上重建分类或回归树。一个区别是，在每次拆分时，当我们在分类树中每次拆分数据时，我们也引导变量。换句话说，在每次潜在拆分时，只考虑变量的子集。这有助于建立一系列不同的潜在树木。所以我们的想法是种植大量的树木。对于预测，我们要么投票，要么对这些树进行平均，以获得对新结果的预测。下面是实现随机森林分类器的 python 代码。</p><pre class="kp kq kr ks gt li lj lk ll aw lm bi"><span id="310d" class="ln lo it lj b gy lp lq l lr ls">from sklearn.ensemble import RandomForestClassifier</span><span id="e59a" class="ln lo it lj b gy lt lq l lr ls">model = RandomForestClassifier(n_estimators=1000)<br/>model.fit(x_train_res , y_train_res)<br/>y_pred = model.predict(X_test)<br/>target_names = ['NON-VIP', 'VIP']<br/>print(classification_report(Y_test, y_pred,target_names=target_names))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/b12a1a58d25c461ba83b2d608dea0aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*snI94yo5I-d0dE2kTydAxA.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk"><strong class="bd la">The result of the Random Forest applied to dataset derived from SMOTE</strong></figcaption></figure><p id="dd0d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">梯度增强:</strong>与并行构建和组合随机不同树的森林的随机森林方法不同，梯度增强决策树的关键思想是它们构建一系列树。其中每棵树都被训练，以便它试图纠正系列中前一棵树的错误。以非随机的方式构建，创建一个模型，随着树的增加，错误越来越少。模型建立后，使用梯度增强树模型进行预测速度很快，并且不会占用太多内存。下面是实现梯度增强分类器的 python 代码。</p><pre class="kp kq kr ks gt li lj lk ll aw lm bi"><span id="cbfb" class="ln lo it lj b gy lp lq l lr ls">model_GB = GradientBoostingClassifier(n_estimators=1000)<br/>model_GB.fit(x_train_res , y_train_res)<br/>y_pred = model_GB.predict(X_test)<br/>target_names = ['NON-VIP', 'VIP']<br/>print(classification_report(Y_test, y_pred,target_names=target_names))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/f1db0742597eb5c585d4e224029cb01e.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*J6zd7RTm05ZsQxCBHvAIIA.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk"><strong class="bd la">The result of the Gradient Boosting applied to dataset derived from SMOTE</strong></figcaption></figure><p id="e3a6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> Ada Boost: </strong>这是一种迭代集成方法。AdaBoost 分类器通过组合多个性能较差的分类器来构建一个强分类器，以获得高精度的强分类器。Adaboost 背后的基本概念是设置分类器的权重，并在每次迭代中训练数据样本，以确保对异常观察值的准确预测。下面是实现 Ada Boost 分类器的 python 代码。</p><pre class="kp kq kr ks gt li lj lk ll aw lm bi"><span id="e5f6" class="ln lo it lj b gy lp lq l lr ls">from sklearn.ensemble import AdaBoostClassifier</span><span id="b695" class="ln lo it lj b gy lt lq l lr ls">model_ad = AdaBoostClassifier()<br/>model_ad.fit(x_train_res , y_train_res)<br/>y_pred = model_GB.predict(X_test)<br/>target_names = ['NON-VIP', 'VIP']<br/>print(classification_report(Y_test, y_pred,target_names=target_names))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/a2c36bbd0e38f68c4923448bc76dde01.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*Du62lkKP18keCCfTDWiV7g.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk"><strong class="bd la">The result of the Ada Boosting applied to dataset derived from SMOTE</strong></figcaption></figure><p id="ab73" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">注意:</strong>预测准确性，一种评估分类器性能的流行选择，在数据不平衡时可能不合适。不应该使用它，因为它不能提供真实的情况。例如，模型的准确度可能是 97%,并且人们可能认为该模型表现得非常好，但是实际上，该模型可能仅预测多数类，并且如果该模型的主要目的是预测少数类，则该模型是无用的。因此，应该使用召回率、精确度和 f1 分数来衡量模型的性能。</p><p id="0af1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">结论:</strong>到目前为止，我们看到，通过对不平衡数据集进行重新采样，并选择正确的机器学习算法，我们可以提高少数类的预测性能。我们性能最好的模型是 Ada，梯度增强在使用 SMOTE 合成的新数据集上运行。使用这些模型，我们为少数民族类获得了 0.32 的 f1 分数，而使用原始数据以及逻辑和 k-nn 等算法，少数民族类的 f1 分数为 0.00</p><p id="b662" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">进一步改进:</strong>为进一步改进模型，可考虑以下选项:</p><ol class=""><li id="bf51" class="lx ly it js b jt ju jx jy kb lz kf ma kj mb kn mc md me mf bi translated">尝试使用 SMOTE 的变体。</li><li id="bbb1" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated">超参数(学习率、最大深度等)的调整。)以上型号。</li><li id="ebc5" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated">此外，可以使用不同的机器学习模型，如堆叠或混合机器学习算法、深度学习模型。</li></ol><p id="efd8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">谢谢，学习愉快:)</p></div></div>    
</body>
</html>