<html>
<head>
<title>Machines that learn by doing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过实践学习的机器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machines-that-learn-by-doing-92745ef18a81?source=collection_archive---------12-----------------------#2019-06-22">https://towardsdatascience.com/machines-that-learn-by-doing-92745ef18a81?source=collection_archive---------12-----------------------#2019-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d2bf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习与人工智能之路</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ec3669447836751d2fdf5012fea30c3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PiVZzBwamwkge2BRoYKUQA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Go Champion Lee Sedol (right) playing against AlphaGo in 2016 (<a class="ae ky" href="https://www.newscientist.com/article/2117067-deepminds-alphago-is-secretly-beating-human-players-online/" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="f89b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我 25 岁左右的时候，我第一次学会了打网球。关于网球的事情是，一旦你开始，就很难让球落在球场的对面(而不是球场后面的树上)。诀窍是在击球的瞬间，大致垂直地握住球拍，并给予球足够的上旋力。经过几个小时的训练和与朋友的练习，我慢慢学会了如何在击球时将球拍调整到合适的角度。</p><p id="9d55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的大脑能够学习一项新的任务，打网球，主要是通过频繁的练习。它是怎么做到的？机器能做同样的事情吗？机器可以边做边学吗？</p><h2 id="428a" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">人工智能:机器如何学习</h2><p id="a414" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">人工智能大致有两种不同的方法，研究人员称之为<em class="lv">专业化</em>和<em class="lv">通用</em>方法。</p><p id="5f52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，<em class="lv">图像分类</em>的最新进展就是高度专业化的人工智能形式。图像分类模型通常也非常复杂:<em class="lv"> AlexNet </em>例如，它在计算机视觉领域最有影响力的<a class="ae ky" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">论文之一</a>中提出，包含 6000 万个自由参数，需要用大量人类标记的训练数据进行调整。尽管这种系统可以在现实世界中得到有用的应用，但对标记训练数据的需求，以及专业化的程度，使它们在范围上受到了根本的限制:例如，一个经过训练可以区分猫和狗的人工智能系统，无法在 x 光胸片中检测出肺炎。不用说，这样一个系统甚至不知道这些概念是什么意思。称这样一个系统是智能的几乎有点言过其实。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/dbd6469caeb8115fa942e8d757f8259c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*pj_XBvjv71-U_X2KyCQKOQ.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Chest X-ray (left), overlayed with predicted pneumonia probability density map (right) (<a class="ae ky" href="https://www.eenewseurope.com/news/algorithm-beats-radiologists-diagnosing-x-rays/page/0/1" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="e36c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通用人工智能是一种完全不同的方法。这里的想法是开发更简单，同时更通用的算法和方法，让机器在没有人类参与的情况下学习它们的环境。在我看来，<em class="lv">强化学习领域</em>是当今最有希望实现通用人工智能的方法之一。那是什么？</p><h2 id="c713" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">强化学习:学习做什么</h2><p id="f242" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated"><em class="lv">强化学习意味着学习做什么:将情境映射到行动。如果这听起来很熟悉，那是因为这本质上是我们人类一直在做的事情。这种想法从根本上不同于监督和非监督机器学习。让我解释一下。</em></p><p id="e87f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在监督学习中，机器本质上学会给出“正确”的答案:狗还是猫？肺炎还是没有肺炎？在无监督学习中，机器根据现有数据点的相似性(但没有标签)来学习数据的结构:例如，在你的数据库中有五种不同类型的客户，下面是他们的偏好和行为如何不同。<em class="lv">强化学习不是做这两件事——它是学习在给定的情况下做什么。这是一种非常通用同时又非常强大的方法:它被设计成甚至可以在完全未知的领域工作。</em></p><p id="3d47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从技术上讲，RL 问题被框定为所谓的<em class="lv">马尔可夫决策过程</em>，这基本上意味着有两个‘当事人’，<em class="lv">代理</em>和<em class="lv">环境</em>。在任何给定的时间，环境都处于某种状态。代理执行改变其环境状态的动作，并获得回报，回报可以是积极的或消极的。代理人的目标是选择行动，以便在任何给定时间最大化他们未来总报酬的期望值。代理决定动作的逻辑被称为<em class="lv">策略</em>。强化学习问题中的主要挑战是代理人<em class="lv">学习最优策略</em>，这是将最大化未来回报期望的策略。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/8d562eed403568ca43f6b4eb96644439.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*-SwnWvR-VhZRhX-a9ruF6Q.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Illustration of a Markov decision process (<a class="ae ky" href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;ved=2ahUKEwiUnuyXkfniAhUFKawKHX95Bj4QjRx6BAgBEAU&amp;url=https%3A%2F%2Fdeepai.org%2Fmachine-learning-glossary-and-terms%2Fmarkov-decision-process&amp;psig=AOvVaw0G30XMOgV8qFYdhlz4ypXm&amp;ust=1561156997995276" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="e208" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这都是高度理论化的，所以希望一些例子能阐明这个想法。以下是可以被公式化为强化学习问题的所有例子:</p><ul class=""><li id="53f1" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">收集空罐子的清洁机器人。</li><li id="4e0c" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">电厂中控制核反应堆的系统。</li><li id="7e13" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">一个正在学习<a class="ae ky" href="https://www.youtube.com/watch?v=imOt8ST4Ejc" rel="noopener ugc nofollow" target="_blank">走路的机器人</a>。</li><li id="4bf2" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">一种交易股票的系统，旨在实现利润最大化。</li><li id="6d50" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">学习下棋的机器，如国际象棋。</li></ul><p id="2c40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，强化学习植根于<em class="lv">心理学</em>领域:政策与<em class="lv">刺激反应</em>的心理学概念有关，而奖励则对应于一种快乐或痛苦的形式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/cca26952b0a7b2494932292ceb5c51d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KgsIXET0AfYWyx9u4lxe3A.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo credit: Luca Baggio, <a class="ae ky" href="https://unsplash.com/photos/ET244M6ZMN4" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="66b1" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">探索与利用的权衡</h2><p id="0934" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">强化学习的一个典型特征是存在着<em class="lv">探索与利用的权衡</em>:在同样的情况下，一个智能体是应该坚持一个已知回报的行为(<em class="lv">利用</em>)，还是尝试一个新的行为以期获得更高的回报(<em class="lv">探索</em>)？</p><p id="12a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑所谓的<em class="lv">多臂强盗问题</em>:给定一个吃角子老虎机(环境)，比方说，有 10 个臂，每个臂产生不同的、未知的支付金额(奖励)分布，你(代理人)如何决定玩哪个臂？一个被称为<em class="lv">贪婪</em>策略的策略是，总是在你过去看到最高回报的地方拉手臂。贪婪的策略纯粹是剥削。为了给你的策略注入一些探索，你可以以小概率(我们称之为ε)随机选择一只手臂:这种策略被称为<em class="lv">ε-贪婪。</em></p><p id="b374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果其他分支给出更高的回报，那么贪婪将帮助你找到它们，同时充分利用你已经找到的高回报分支。贪婪是解决探索与剥削两难的一种方式。如果收益随时间变化，这种偶尔随机探索的策略尤其重要。我们称这样的问题为<em class="lv">非平稳的</em>，它们通常是强化学习的标准。贪婪的策略可能会错过收益转移的时刻；e<em class="lv">psilon-贪婪策略适应</em>。</p><p id="424e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我继续之前，让我简单地指出，这里也有一个微妙的人生教训，简单地说就是这个:<em class="lv">不时地尝试一些随机的事情</em>。不要墨守成规，即使你的墨守成规有很好的回报。你永远不知道是否有别的东西可以给你更高的回报，你可能会通过一些随机的探索找到它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/2639efa7017b7e1ecc1b4d6abb26cafe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_A6sbZEDb9nhM98o8jnofg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Kasparov moving his first piece in the first game of the rematch against Deep Blue (<a class="ae ky" href="https://rarehistoricalphotos.com/kasparov-deep-blue-1997/" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h2 id="94d3" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">最优策略与计算可行性</h2><p id="ab7b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">原则上，给定无限的时间和计算能力，代理可以学习任何情况下的真正最优策略:这样的代理永远不会出错！</p><p id="8ddc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而实际上，时间和计算能力都是有限的。因此，代理通常只能用<em class="lv">逼近</em>最优策略。例如，在游戏中，代理可以通过忽略很少发生的情况来加快计算速度:如果它很少发生或从不发生，为什么要费心去想如果发生了该怎么办？</p><p id="564d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1997 年，加里·卡斯帕罗夫决定与 IBM 的“深蓝”再赛一场(一年前他赢了一场与这台机器的比赛)。正如内特·西尔弗在《信号与噪音》一书中解释的那样，卡斯帕罗夫的策略是“将程序带出数据库，让它再次盲目飞行”。换句话说，他试图迫使游戏进入一种他可以预期深蓝没有学到最优策略的情况。这就是他在第一轮比赛中的第三步所做的，他忽略了对他的骑士的威胁，而是移动了一个棋子，这是一个非常不寻常的举动。西尔弗指出，棋盘的最终状态也很不寻常，在大师级比赛中只出现过一次。</p><p id="1ff0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比赛是怎么结束的？卡斯帕罗夫赢得了这一轮比赛，这可能是也可能不是因为他不寻常的举动迫使深蓝盲目飞行。坏消息是:这是卡斯帕罗夫赢得的最后一轮比赛。深蓝赢了这场比赛，这使得 1997 年成为艾征服国际象棋的一年。</p><h2 id="26d0" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">向机器学习:传奇之举 37</h2><p id="2f91" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">让我们离开象棋的世界，进入围棋的世界。在具有历史意义的 2016 年 Deepmind 的 AlphaGo 与世界顶级围棋选手 Lee Sedol 的围棋比赛中，第二局发生了奇怪的事情。AlphaGo 先走 37 步<a class="ae ky" href="https://www.wired.com/2016/03/googles-ai-viewed-move-no-human-understand/" rel="noopener ugc nofollow" target="_blank">看起来是个失误</a>。评论员和专家不理解这一举动。Sedol 花了超过 15 分钟来决定一个回应。此前输给 AlphaGo 的欧洲围棋冠军范辉后来评论说:“(T2)这不是人的举动。我从未见过人类玩这一招。</p><p id="afad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第 37 步不是一个错误。人类不理解这步棋，仅仅是因为 AlphaGo 预见了<em class="lv">那么多步棋</em>。事实上，AlphaGo 赢得了比赛，以及整场比赛。它发现了一步人类从未想到过的棋，改变了当今最好的棋手对围棋的看法。</p><p id="d3fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，围棋只是一种游戏——但诸如药物发现、疾病检测或灾难预测等问题呢？这里的信息是，人工智能有可能为我们指出新的、未经探索的解决方案，以解决我们从未想到的问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/80d53a332b33333ba2ad78932aadba7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AqJOnaB1MzZeLSrsikgtQQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo credit: Mauro Mora, <a class="ae ky" href="https://unsplash.com/photos/31-pOduwZGE" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="aac4" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">学习就是做人</h2><blockquote class="nn no np"><p id="9f63" class="kz la lv lb b lc ld ju le lf lg jx lh nq lj lk ll nr ln lo lp ns lr ls lt lu im bi translated">所有的经历都被我们的新视野丰富地交织在一起，然后新的联系开始出现。城市人行道上流淌的雨水将教会钢琴家如何流动。一片随风轻飘的树叶会教会控制者如何放手。一只家猫会教我如何移动。所有的时刻都变成了每个时刻。—乔希·怀兹金，《学习的艺术》</p></blockquote><p id="7a23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在《学习的艺术》一书中，乔希·怀兹金讲述了他年轻时成为国际象棋冠军，后来成为武术冠军的经历。他解释了掌握象棋如何帮助他学习武术，反之亦然。建立联系，将一个主题的知识转移到另一个主题的能力，可能是我们拥有的最强大的技能之一，当然也是人工智能极难模仿的技能。</p><p id="2d9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实是，机器不善于将一项任务中的专业知识转移到另一项任务中。此外，他们没有<em class="lv">直觉</em>的概念，直觉在我们以前没有解决的任务中指导我们人类。即使强化学习领域迄今为止已经显示出极其有前途的结果，我相信这可能是我们在走向通用 AI 的旅程中需要克服的两个主要限制。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h2 id="4e6f" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated"><em class="oa">资源</em></h2><ul class=""><li id="eac6" class="mw mx it lb b lc mp lf mq li ob lm oc lq od lu nb nc nd ne bi translated">理查德·萨顿和安德鲁·巴尔托，<em class="lv">强化学习</em></li><li id="e9a0" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">内特·西尔弗，<em class="lv">信号和噪音</em></li><li id="ef19" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">乔希·威茨金<em class="lv">学习的艺术</em></li></ul></div></div>    
</body>
</html>