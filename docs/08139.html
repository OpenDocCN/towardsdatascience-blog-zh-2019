<html>
<head>
<title>Hey Model, Why Do You Say This Is Spam?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">嘿，模特，你为什么说这是垃圾邮件？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hey-model-why-do-you-say-this-is-spam-7c945cc531f?source=collection_archive---------16-----------------------#2019-11-07">https://towardsdatascience.com/hey-model-why-do-you-say-this-is-spam-7c945cc531f?source=collection_archive---------16-----------------------#2019-11-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="353d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">将原因附加到模型预测</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d0ec9de42d94ed30b3c36ec0d5700a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHjQ2BNJElQ21biueQgXMQ.png"/></div></div></figure></div><div class="ab cl kr ks hu kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ij ik il im in"><p id="711e" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">Shapley 值在机器学习中用于解释复杂预测模型的预测，<em class="lu">又名</em>“黑盒”。在这篇文章中，我将使用 Shapley 值来确定 YouTube 评论的关键术语，这些术语解释了为什么一个评论被预测模型预测为垃圾或合法。特定关键术语的“联盟”可以被认为是模型返回给定预测的“原因”。此外，我将使用聚类来识别在如何使用这些关键术语方面有相似之处的评论组。最后，我将进一步概括预测原因，以便使用代表原因类别的更少关键术语的字典对评论组进行分类。</p><h2 id="6564" class="lv lw iq bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">序文</h2><p id="f13e" class="pw-post-body-paragraph ky kz iq la b lb mo jr ld le mp ju lg lh mq lj lk ll mr ln lo lp ms lr ls lt ij bi translated">遵循这篇文章中的代码片段需要 Python 和 r。包含所有代码片段的完整 Jupyter 笔记本可以在<a class="ae mt" href="https://gist.github.com/alessiot/769ebe433adf79725b08687cf889f4cb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。运行代码所需的 Python 库如下。我们将使用<em class="lu"> rpy2 </em>库在 Python 中运行 R 的实例。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="2ea1" class="lv lw iq mv b gy mz na l nb nc">import rpy2.ipython</span><span id="4070" class="lv lw iq mv b gy nd na l nb nc">from rpy2.robjects import pandas2ri</span><span id="8011" class="lv lw iq mv b gy nd na l nb nc">pandas2ri.activate()</span><span id="3ea6" class="lv lw iq mv b gy nd na l nb nc">%reload_ext rpy2.ipython</span><span id="ad50" class="lv lw iq mv b gy nd na l nb nc">from sklearn import model_selection, preprocessing, metrics, svm<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn import decomposition, ensemble</span><span id="4cc5" class="lv lw iq mv b gy nd na l nb nc">import pandas as pd<br/>import numpy as np</span><span id="f137" class="lv lw iq mv b gy nd na l nb nc">import string</span><span id="8338" class="lv lw iq mv b gy nd na l nb nc">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>%matplotlib inline</span><span id="2fc2" class="lv lw iq mv b gy nd na l nb nc">import xgboost</span><span id="3a0a" class="lv lw iq mv b gy nd na l nb nc">import shap, time</span></pre><h2 id="87a1" class="lv lw iq bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated"><em class="ne">沙普利法</em></h2><p id="8bd6" class="pw-post-body-paragraph ky kz iq la b lb mo jr ld le mp ju lg lh mq lj lk ll mr ln lo lp ms lr ls lt ij bi translated">沙普利值是在合作博弈理论中进行的一项研究的结果。他们告诉如何在玩家之间公平分配“奖金”。当使用先前训练的模型预测数据实例的标签时，可以通过假设数据实例的每个特征值是游戏中的玩家来解释该预测，其中该预测是支出。Shapley 值是所有可能的要素组合中某个要素值的平均边际贡献。Christoph Molnar 的在线书籍提供了更多的细节。</p><p id="7e3a" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们可以用一个简单的例子来介绍 Shapley 方法:XOR 表。我们将在接下来的章节中进行更详细的讨论。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="3b59" class="lv lw iq mv b gy mz na l nb nc">data = {'F1':[0,0,1,1], 'F2':[0,1,0,1], "Target":[0,1,1,0]} #XOR<br/>df = pd.DataFrame(data)</span><span id="d3ff" class="lv lw iq mv b gy nd na l nb nc">X = df[["F1","F2"]]<br/>y = df["Target"].values.tolist()</span><span id="322e" class="lv lw iq mv b gy nd na l nb nc">df</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/b57b06f7481358643412ee28449bfc45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LyC6SMcvQyz6lvbTmgzNeg.png"/></div></div></figure><p id="e865" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">为了计算 Shapley 值，我们首先需要训练一个模型。例如，让我们使用径向基函数核的 SVM。我们现在可以使用 Python 库<em class="lu"> shap </em>计算 Shapley 值。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="67ff" class="lv lw iq mv b gy mz na l nb nc">clf = sklearn.svm.SVC(kernel='rbf', probability=False).fit(X, y) </span><span id="46ee" class="lv lw iq mv b gy nd na l nb nc">df["Prediction"] = clf.predict(X)</span><span id="f12e" class="lv lw iq mv b gy nd na l nb nc"># Explaining the probability prediction results<br/>explainer = shap.KernelExplainer(clf.predict, X)<br/>shap_values = explainer.shap_values(X)</span><span id="e4a0" class="lv lw iq mv b gy nd na l nb nc">pd.concat([df, pd.DataFrame(shap_values, <br/>           columns='shap_'+ X.columns.values)], axis=1)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/405ada786d1e1553268f57ca21e8ffaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tMLvWD8KOm3sCIDaoRzr_w.png"/></div></div></figure><p id="37ab" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">从上述结果可以看出，当每个数据实例的两个特征的计算贡献(Shapley 值)都为负时，预测为 0，当 Shapley 值都为正时，预测为 1。请注意，这两个贡献加起来就是该实例<em class="lu">的预测与预期预测𝐸(𝑓之间的初始差值，它是实际目标值的平均值，如下所示:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/28c9ea688f3374a250b97fb1a9a5b8c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PHFrMnvTUQEbWS7H1t45zg.png"/></div></div></figure><h2 id="0e2c" class="lv lw iq bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">YouTube 垃圾评论</h2><p id="650f" class="pw-post-body-paragraph ky kz iq la b lb mo jr ld le mp ju lg lh mq lj lk ll mr ln lo lp ms lr ls lt ij bi translated">我们将使用一个文本分类数据集，从 5 个不同的 YouTube 视频中收集<a class="ae mt" href="http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/" rel="noopener ugc nofollow" target="_blank"> 1956 条评论。这些评论是通过 YouTube API 从 2015 年上半年 YouTube 上观看次数最多的十个视频中的五个收集的。数据集包含被标记为合法邮件或垃圾邮件的非编码邮件。由于我将在本文稍后再次使用 R，我决定使用我在搜索数据集时找到的</a><a class="ae mt" href="https://github.com/christophM/interpretable-ml-book/blob/master/R/get-SpamTube-dataset.R" rel="noopener ugc nofollow" target="_blank"> R 片段</a>下载数据集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="5944" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">上面的代码片段会将 csv 格式的文件下载到本地文件夹。我们可以使用<em class="lu">熊猫</em>图书馆来探索内容。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="0d82" class="lv lw iq mv b gy mz na l nb nc">youtube_data = pd.read_csv("youtube_data/TubeSpam.csv")<br/>youtube_data.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/d059b4b1043defddae225625cdefa1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zdfvqsuu3lMVPwaWebrJdw.png"/></div></div></figure><p id="d786" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们可以选择稍后将用于建模的列内容和类，并删除缺少内容的行。运行下面的代码片段将返回 1954 行，并显示标签 0(合法评论)和 1(垃圾评论)在删除重复行后出现的次数大致相同。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="753e" class="lv lw iq mv b gy mz na l nb nc">youtube_df = pd.DataFrame()<br/>youtube_df['text'] = youtube_data['CONTENT']<br/>youtube_df['label'] = youtube_data['CLASS']<br/>youtube_df.dropna(inplace=True)</span><span id="ca91" class="lv lw iq mv b gy nd na l nb nc">youtube_df.groupby('label').describe()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/a508976d1b7d0862d960a04abfcf7dec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xiAkKhxaqoNO25qDWslg9g.png"/></div></div></figure><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="c034" class="lv lw iq mv b gy mz na l nb nc">youtube_df.drop_duplicates(inplace=True)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/e4b3162f1a297dd8e6c88435a2231396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nsa_8A00kiaOL4wOX84apQ.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">\</figcaption></figure><p id="a33e" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们可以看看合法评论和垃圾评论在长度上是否有明显的区别</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="ad36" class="lv lw iq mv b gy mz na l nb nc">youtube_df['length'] = youtube_df['text'].apply(len)<br/>youtube_df.hist(column='length', by ='label', bins=50, figsize = (10,4))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/5e39ca407dc4e94e8c767824f005334f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M9Awi-j5rgT6Q9QHjx5bUg.png"/></div></div></figure><p id="4fb1" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">显然，垃圾评论平均来说要长一些。最长的注释之一是，例如:</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="c66b" class="lv lw iq mv b gy mz na l nb nc">youtube_df[youtube_df['length'] == 1077]['text'].iloc[0]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/9d1cdc4355e035772ebc024877bd1899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0MKglAB3nS_i69rXfKCxsQ.png"/></div></div></figure><h2 id="7fb0" class="lv lw iq bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">预处理注释</h2><p id="a8a0" class="pw-post-body-paragraph ky kz iq la b lb mo jr ld le mp ju lg lh mq lj lk ll mr ln lo lp ms lr ls lt ij bi translated">在使用文本数据进行建模之前，我们将使用一些标准的自然语言处理(NLP)技术对其进行清理。特别是，我们将删除数字，标点符号，额外的空格，将所有单词转换为小写，删除停用词，词干和词条。为此，我们将使用 Python 库<em class="lu"> nltk </em>。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="f7d7" class="lv lw iq mv b gy mz na l nb nc">from nltk.tokenize import word_tokenize<br/>from nltk.corpus import stopwords # nltk.download('stopwords')<br/>from nltk.stem import PorterStemmer<br/>from nltk.stem import WordNetLemmatizer #nltk.download('wordnet')</span><span id="1cf1" class="lv lw iq mv b gy nd na l nb nc">import string, re</span><span id="ef37" class="lv lw iq mv b gy nd na l nb nc">stop_words = list(set(stopwords.words('english')))<br/>stemmer= PorterStemmer()<br/>lemmatizer=WordNetLemmatizer()</span><span id="e08a" class="lv lw iq mv b gy nd na l nb nc">translate_table = dict((ord(char), None) for char in string.punctuation)</span><span id="70ed" class="lv lw iq mv b gy nd na l nb nc">def nltk_text_preproc(text_in):<br/>    text_out = re.sub(r'\d+', '', text_in) # rm numbers<br/>    text_out = text_out.translate(translate_table) # rm punct<br/>    text_out = text_out.strip() # rm white spaces</span><span id="926c" class="lv lw iq mv b gy nd na l nb nc">    return text_out</span><span id="79bf" class="lv lw iq mv b gy nd na l nb nc">def nltk_token_processing(tokens):<br/>    tokens = [i.lower() for i in tokens]<br/>    tokens = [i for i in tokens if not i in stop_words]<br/>    tokens = [stemmer.stem(i) for i in tokens]<br/>    tokens = [lemmatizer.lemmatize(i) for i in tokens]<br/>    <br/>    return tokens</span></pre><p id="1d3f" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">之前，我们可以查看数据集中的前 10 条评论</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="e86d" class="lv lw iq mv b gy mz na l nb nc">pd.options.display.max_colwidth = 1000<br/>youtube_df['text'].head(10)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/7a3045e04320429e2781679deec2c803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3pu9f3TKNBlQqhr2haWgkA.png"/></div></div></figure><p id="b1bf" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">之后，使用我们的预处理步骤</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="6997" class="lv lw iq mv b gy mz na l nb nc">youtube_df['text'].head(10).map(lambda x: nltk_text_preproc(x)).map(lambda x: nltk_token_processing(word_tokenize(''.join(x)))).map(lambda x: ' '.join(x))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/6c265d382ca051ce0185f83055da05a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*__NrF3YH4Ua6cd_uh0lT4w.png"/></div></div></figure><p id="a395" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在继续之前，我们现在准备处理数据集中的所有文本。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="cad3" class="lv lw iq mv b gy mz na l nb nc">youtube_df['text'] = youtube_df['text'].map(lambda x: nltk_text_preproc(x))<br/>youtube_df['text'] = youtube_df['text'].map(lambda x: nltk_token_processing(word_tokenize(''.join(x))))<br/>youtube_df['text'] = youtube_df['text'].map(lambda x: ' '.join(x))</span></pre><p id="0d44" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">新数据集包含 1735 行。</p><h2 id="8691" class="lv lw iq bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">从文本创建要素</h2><p id="abe7" class="pw-post-body-paragraph ky kz iq la b lb mo jr ld le mp ju lg lh mq lj lk ll mr ln lo lp ms lr ls lt ij bi translated">在这一部分，预处理的文本数据将被转换成特征向量。我们将使用<em class="lu"> nltk </em> Python 库的<em class="lu">计数矢量器</em>方法。这将文本行转换成一个矩阵，其中每一列代表所有文本中的一个术语，每个单元格代表特定术语在给定行中出现的次数。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="b66e" class="lv lw iq mv b gy mz na l nb nc">count_vect = CountVectorizer(min_df=0.01, <br/>                             max_df=1.0, ngram_range=(1,3)) <br/>count_vect.fit(youtube_df['text'])<br/>youtube_df_text = count_vect.transform(youtube_df['text'])</span></pre><p id="3657" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里，我们请求删除出现在不到 1%的评论或所有评论中的术语，并要求 CountVectorizer 计算最多 2 个连续术语的 n 元语法。</p><p id="1114" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">例如，第二个注释(行)</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="ac25" class="lv lw iq mv b gy mz na l nb nc">text_example_orig = youtube_df['text'][1]<br/>print(text_example_orig)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/ab50f4f9faff2426eba6d7516b30117e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rb58Jy-YYtWc1hUa6eacDg.png"/></div></div></figure><p id="8a3b" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">成为</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="1da3" class="lv lw iq mv b gy mz na l nb nc">text_example = count_vect.transform([text_example])<br/>print(text_example)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/70ddba85ef0a5bd558f53e7792ad3cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MCtrjnXdM8s1_S-KkO7fjA.png"/></div></div></figure><p id="c507" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如果我们想知道哪些项对应于 CountVectorizer 返回的索引，我们可以使用下面的代码</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="c3a1" class="lv lw iq mv b gy mz na l nb nc">for row, col in zip(*text_example_transf.nonzero()):<br/>    val = text_example_transf[row, col]<br/>    #print((row, col), val)<br/>    print(count_vect.get_feature_names()[col])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/44224f3a98fea8acb8d65aad713efebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vOK6Besee6ek54pUEXHcpA.png"/></div></div></figure><p id="4883" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">计数矩阵不是标准矩阵，而是稀疏矩阵</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="f6ca" class="lv lw iq mv b gy mz na l nb nc">print ('Shape of Sparse Matrix: ', youtube_df_text.shape)<br/>print ('Amount of Non-Zero occurences: ', youtube_df_text.nnz)<br/>print ('sparsity: %.2f%%' % (100.0 * youtube_df_text.nnz /<br/>                             (youtube_df_text.shape[0] * youtube_df_text.shape[1])))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/97306e9c640afb7d8edd92328b101d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ia0xuEOddjZB8QJ5K1kvqg.png"/></div></div></figure><p id="b535" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">稀疏矩阵是包含极少非零元素的矩阵的最佳表示。事实上，用二维数组表示稀疏矩阵会导致大量内存浪费。在我们的例子中，有 1，735 行* 166 个项，这将导致具有 288，010 个元素的二维矩阵，但是只有 7，774 个具有非零出现(2.7%稀疏度)，因为不是所有行都包含所有项。</p><h2 id="3ac9" class="lv lw iq bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">建模</h2><p id="ae80" class="pw-post-body-paragraph ky kz iq la b lb mo jr ld le mp ju lg lh mq lj lk ll mr ln lo lp ms lr ls lt ij bi translated">我们将把数据集分成训练集和验证集。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="6697" class="lv lw iq mv b gy mz na l nb nc">train_x, valid_x, train_y, valid_y, index_train, index_val = model_selection.train_test_split(youtube_df_text, youtube_df['label'], range(len(youtube_df['label'])), stratify=youtube_df['label'], random_state=1, train_size=0.8)</span></pre><p id="08f8" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">现在属于训练集和验证集的合法评论和垃圾评论的数量是</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="d18f" class="lv lw iq mv b gy mz na l nb nc">np.unique(train_y, return_counts=True)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/057212adf54a22dc728cbc7ff0579e73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SCcRUU_zv3b-YmyJONqwvA.png"/></div></div></figure><p id="d6a1" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">现在是时候训练我们的模型了</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="fb7f" class="lv lw iq mv b gy mz na l nb nc">def train_model(classifier, feature_vector_train, label, feature_vector_valid, valid_label):<br/>    # fit the training dataset on the classifier<br/>    classifier.fit(feature_vector_train, label)<br/>    <br/>    # predict the labels on validation dataset<br/>    predictions = classifier.predict(feature_vector_valid)<br/>    <br/>    return classifier, metrics.accuracy_score(predictions, valid_label), metrics.classification_report(predictions, valid_label)</span></pre><p id="52d8" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">看看它在验证集上的表现。我们将使用 XGBoost 分类器。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="2d71" class="lv lw iq mv b gy mz na l nb nc">classifier, accuracy, confusion_matrix = train_model(xgboost.XGBClassifier(), <br/>            train_x, train_y, valid_x, valid_y)<br/>print("Xgb, Accuracy: ", accuracy)<br/>print(confusion_matrix)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/66519f60805eeb3f840b08a178120e9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5jUrK3j1iamAORrbCXNZMA.png"/></div></div></figure><p id="d115" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">对于这篇文章来说，一个高性能的模型是不必要的。事实上，我们正在寻找一个关于特性和标签如何相互关联的一般性描述。事实上，这里我们只需要一个像样的模型。</p><h2 id="d06b" class="lv lw iq bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">用 SHAP 计算沙普利值</h2><p id="0aee" class="pw-post-body-paragraph ky kz iq la b lb mo jr ld le mp ju lg lh mq lj lk ll mr ln lo lp ms lr ls lt ij bi translated">Python 库<a class="ae mt" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">SHAP</a>(SHapley Additive explaints)可以解释任何机器学习模型的输出，特别是它为<a class="ae mt" href="https://arxiv.org/abs/1802.03888" rel="noopener ugc nofollow" target="_blank">树集成方法</a>提供了高速精确算法。这就是为什么我们的模型是 XGBoost 模型的原因之一。在我们的例子中，计算 SHAP 值只需要几分之一秒。这里，我们计算整个数据集的 SHAP 值。请注意，将 XGBoost 与逻辑目标函数结合使用时，SHAP 值是对数优势。要将对数优势差额转换为概率，我们可以使用公式 odds = exp(对数优势)，其中 p = odds/(1+odds)。在本节的最后，我们将做这个练习，但现在让我们首先计算 SHAP 值并研究它们。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="c4f2" class="lv lw iq mv b gy mz na l nb nc">t0 = time.time()<br/>explainer = shap.TreeExplainer(classifier)<br/>shap_values_train = explainer.shap_values(youtube_df_text)<br/>t1 = time.time()<br/>timeit=t1-t0<br/>print('time to compute Shapley values (s):', timeit)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/07c25fc00ec013f323fbbe8eca4caa35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MgHqp4SLN9dIx6MVL_KaSQ.png"/></div></div></figure><p id="71f0" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">将稀疏矩阵转换成密集矩阵是很方便的。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="e373" class="lv lw iq mv b gy mz na l nb nc">txt_dense_df = pd.DataFrame(youtube_df_text.todense(), <br/>                            columns=count_vect.get_feature_names())<br/>shap_values_train_df = pd.DataFrame(shap_values_train, <br/>                                    columns=txt_dense_df.columns)</span></pre><p id="1304" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">有了这个数据框，我们就可以计算整体要素的重要性</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="ebb9" class="lv lw iq mv b gy mz na l nb nc">shap_sum = np.abs(shap_values_train_df).mean(axis=0)<br/>importance_df = pd.DataFrame([txt_dense_df.columns.tolist(), <br/>                              shap_sum.tolist()]).T<br/>importance_df.columns = ['column_name', <br/>                         'shap_importance (log-odds)']<br/>importance_df = importance_df.sort_values('shap_importance (log-odds)', ascending=False)<br/>importance_df['shap_importance (%)'] = importance_df['shap_importance (log-odds)'].apply(lambda x: 100*x/np.sum(importance_df['shap_importance (log-odds)']))</span></pre><p id="00b7" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">并且选择例如前 20 个特征</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="783d" class="lv lw iq mv b gy mz na l nb nc">topN = 20<br/>top20 = importance_df.iloc[0:topN]["column_name"]</span><span id="ff2e" class="lv lw iq mv b gy nd na l nb nc">print('Cumulative Importance', <br/>      np.sum(importance_df.iloc[0:topN]["shap_importance (%)"]))</span><span id="3373" class="lv lw iq mv b gy nd na l nb nc">shap_values_imp = shap_values_train_df[top20]</span><span id="5d04" class="lv lw iq mv b gy nd na l nb nc">shap.summary_plot(shap_values_train_df, <br/>                  txt_dense_df, plot_type="bar")</span><span id="3987" class="lv lw iq mv b gy nd na l nb nc">importance_df.iloc[0:topN]</span></pre><div class="kg kh ki kj gt ab cb"><figure class="nu kk nv nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/223eaf3160a9980d57694279d4cb3780.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*qIiJseWhfmoTV1zOa3_KTg.png"/></div></figure><figure class="nu kk oa nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/bf1a3682b7e4b8e4431e1cd9d4a13dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*Uq07naYrExVP18kbzTx6OQ.png"/></div></figure></div><p id="471f" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">其累计“重要性”约为 94%。上面的条形图显示了每个特征的 SHAP 值的平均绝对值。</p><p id="9816" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">也可以使用更多的可视化来解释模型结果。举个例子，</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="b0a3" class="lv lw iq mv b gy mz na l nb nc">j = 1<br/>youtube_df.iloc[j]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/3f386efc0a95e86c7f3117d04511d139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q695X9kNoge_IvJALzqrAQ.png"/></div></div></figure><p id="188a" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">是典型的垃圾评论。我们可以显示有助于将模型输出从基础值(我们使用的训练数据集的平均模型输出)推至模型输出的项。将预测推向更高的对数优势值的术语显示为红色，将预测推向更低的对数优势值的术语显示为蓝色。在这个特殊的例子中，有一个典型的垃圾评论的所有“成分”。事实上，所有术语的 SHAP 值都将模型输出推至一个比平均输出值高得多的值。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="4ac4" class="lv lw iq mv b gy mz na l nb nc">shap.initjs()</span><span id="3ecf" class="lv lw iq mv b gy nd na l nb nc"># visualize the j-prediction's explanation (use matplotlib=True to avoid Javascript)<br/>shap.force_plot(explainer.expected_value, shap_values_imp.iloc[j].to_numpy(), <br/>                txt_dense_df.iloc[j][top20])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/253e126f7c3e3040eed875cd88b3b6bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L1ZyEKTByrXAjkl0P7L83A.png"/></div></div></figure><p id="fe71" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">对于合法的评论，如</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/7d954dd18f963e2e9168ceab08f8c7a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z_3qGUHzwmDGV0346zqgbg.png"/></div></div></figure><p id="de71" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">上面使用的可视化变成了</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/efdd78f9757fe352f41b15f903a3f09c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vviNyporVkAeiEP6KtCE-Q.png"/></div></div></figure><p id="9088" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在我的<a class="ae mt" href="https://gist.github.com/alessiot/769ebe433adf79725b08687cf889f4cb" rel="noopener ugc nofollow" target="_blank">笔记本</a>中，我添加了更多的可视化内容，这并不是这篇文章的主要目的。</p><p id="0c48" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">最后，让我们做一个将对数几率转换成概率的练习，看看 SHAP 值与它们有什么关系。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="a41c" class="lv lw iq mv b gy mz na l nb nc">j = 1</span><span id="d857" class="lv lw iq mv b gy nd na l nb nc"># log odds margin to prob<br/># odds = exp(log odds), p = odds/(1+odds)</span><span id="2ed5" class="lv lw iq mv b gy nd na l nb nc">log_odds = np.sum(shap_values_train_df.iloc[j].to_numpy())<br/>avg_model_output = np.mean(youtube_df['label']) # prob<br/>log_odds_avg_model_output = np.log(avg_model_output/(1-avg_model_output))<br/>predicted_prob = classifier.predict_proba(youtube_df_text.tocsc())[j][1] #target=1<br/>predicted_log_odds = np.log(predicted_prob/(1-predicted_prob))</span><span id="312b" class="lv lw iq mv b gy nd na l nb nc">print("Sum of Shaphley values (log-odds)for j-instance:", log_odds, <br/>      'prob:', np.exp(log_odds)/(1.0+np.exp(log_odds)))<br/>print("Average model output:", avg_model_output)<br/>print("Predicted probability value for j-instance:", predicted_prob,<br/>      "Predicted value:", classifier.predict(youtube_df_text.tocsc())[j])</span><span id="330f" class="lv lw iq mv b gy nd na l nb nc">print('log_odds:', log_odds, 'is expected to be equal to pred-expected:', predicted_log_odds-log_odds_avg_model_output)<br/>print('pred-expected (prob):', predicted_prob-avg_model_output)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/7a2e01d1a89c0652e2b3512843eecd63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1uqMaqgWJ-Gw-blZ6Y6PXw.png"/></div></div></figure><p id="e50b" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">对于评论 j = 1，我们基于对数-赔率(SHAP 值)到概率的转换，计算了 SHAP 值(6.59)，对应于该评论是垃圾邮件(标签= 1)的概率为 99.86%。从模型预测的概率是 99.85%，略有不同，但足够接近。正如我在本文开头所说的，我们期望 SHAP 值(对数优势)等于预测值减去预期概率，这是所有实际目标的平均值(0.48)。</p><h2 id="5ac6" class="lv lw iq bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">聚类 SHAP 值</h2><p id="8e04" class="pw-post-body-paragraph ky kz iq la b lb mo jr ld le mp ju lg lh mq lj lk ll mr ln lo lp ms lr ls lt ij bi translated">本文之前选出的前 20 个术语可以用字典总结成更通用的“主题”:</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="eb08" class="lv lw iq mv b gy mz na l nb nc">top20_dict = {'ACTION': ['check','subscrib','view','share','follow','help', 'visit'],<br/> 'REQUEST': ['plea','hey'],<br/> 'VALUE': ['money','free','billion', 'new', "good", 'best'],<br/> 'YOUTUBE': ['channel', 'song', 'onlin'],<br/> 'COMMENT': ['love', 'like comment']             <br/>}</span></pre><p id="8ed5" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">因此，对于每个评论，我们可以确定运行以下代码片段的首要原因。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="2f30" class="lv lw iq mv b gy mz na l nb nc">from itertools import chain</span><span id="c581" class="lv lw iq mv b gy nd na l nb nc">ntopreason = 1 #change this to allow more reasons to be captured</span><span id="1287" class="lv lw iq mv b gy nd na l nb nc">top20_dict_values = list(top20_dict.values())<br/>top20_dict_keys = list(top20_dict.keys())</span><span id="b4c3" class="lv lw iq mv b gy nd na l nb nc">shap_values_imp_r = shap_values_imp.copy()<br/>target_values_r = pd.Series(predictions)</span><span id="4325" class="lv lw iq mv b gy nd na l nb nc"># Create summarizing labels<br/>top_reasons_all = []<br/>for i in range(shap_values_imp_r.shape[0]):<br/>    <br/>    shap_feat = shap_values_imp_r.iloc[i]<br/>    shap_feat = shap_feat.iloc[np.lexsort([shap_feat.index, shap_feat.values])]<br/>    <br/>    topN = shap_feat.index.to_list()[-1:] <br/>    topN_value = shap_feat.values[-1:]<br/>    <br/>    topN_idx = []<br/>    for topn in topN:<br/>        for idx in range(len(top20_dict_values)):<br/>            if topn in top20_dict_values[idx] and idx not in topN_idx:<br/>                topN_idx.append(idx)<br/>                <br/>    #topN_idx = [idx for idx in range(len(top20_dict_values)) for topn in topN if topn in top20_dict_values[idx]]<br/>    <br/>    #Ordered by increasing importance<br/>    top_reasons = [top20_dict_keys[x] for x in topN_idx]</span><span id="7c7a" class="lv lw iq mv b gy nd na l nb nc">#print(i, topN, topN_idx, top_reasons)</span><span id="9366" class="lv lw iq mv b gy nd na l nb nc">top_reasons_all.append(','.join(top_reasons))<br/>    <br/>shap_values_imp_r['target'] = target_values_r<br/>shap_values_imp_r['top_reasons'] = top_reasons_all</span></pre><p id="2b26" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在一个单独的 Jupyter 单元格中，我们将使用 R 来选择用于对评论的 SHAP 值进行分组的最佳聚类数。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="7815" class="lv lw iq mv b gy mz na l nb nc">%%R -i shap_values_imp_r -w 800 -h 800</span><span id="c951" class="lv lw iq mv b gy nd na l nb nc">library(gplots)</span><span id="2f0e" class="lv lw iq mv b gy nd na l nb nc">d_shap &lt;- dist(shap_values_imp_r[1:(ncol(shap_values_imp_r)-2)]) <br/>hc &lt;- hclust(d_shap, method = "ward.D2")<br/>dend &lt;- as.dendrogram(hc)</span><span id="c086" class="lv lw iq mv b gy nd na l nb nc">library(dendextend)<br/>library(colorspace)</span><span id="2416" class="lv lw iq mv b gy nd na l nb nc">## Find optimal number of clusters<br/>clusters_test = list()<br/>for (ncl in 2:50){<br/>    clusters_test[[ncl]] &lt;- cutree(hc, k=ncl)<br/>}</span></pre><p id="8f5f" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里，我们使用层次聚类，聚类数量在 2 到 50 之间。回到 Python，我们计算每个聚类结果的<em class="lu">轮廓</em>分数。通过查看轮廓分数和聚类的可视化来决定要使用的聚类数量。在选择集群数量时，总是需要做出决定。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="b545" class="lv lw iq mv b gy mz na l nb nc">h_clusters = rpy2.robjects.r['clusters_test']</span><span id="5d7b" class="lv lw iq mv b gy nd na l nb nc">h_clusters_sil = []<br/>cl_id = 0<br/>for cl in h_clusters:<br/>    if cl is not rpy2.rinterface.NULL:<br/>        sil = metrics.silhouette_score(shap_values_imp_r.drop(shap_values_imp_r.columns[len(shap_values_imp_r.columns)-2:], axis=1, inplace=False), <br/>                                       cl, metric='euclidean')<br/>        h_clusters_sil.append(sil)<br/>        #print(cl_id, sil)<br/>        cl_id += 1<br/>    else:<br/>        cl_id += 1</span><span id="cea8" class="lv lw iq mv b gy nd na l nb nc">plt.plot(range(2, 51), h_clusters_sil)<br/>plt.title('Silhouette')<br/>plt.xlabel('Number of clusters')<br/>plt.ylabel('Silhouette Index') #within cluster sum of squares<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/b142c79a96c75bd6f8120feaf1a91ccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vzHgAUqAIq7HcB3Wi6tAvA.png"/></div></div></figure><p id="c9ea" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们使用 Ward 的方法进行聚类，该方法使总的组内方差最小化。在每一步，具有最小聚类间距离的聚类对被合并。合并的高度(见下图)表示两个实例之间的相似性。合并的高度越高，实例越不相似。数据的平均<a class="ae mt" href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_silhouette_method" rel="noopener ugc nofollow" target="_blank">轮廓</a>是评估最佳聚类数的标准。数据实例的轮廓是一种度量，它与它的分类中的数据匹配得有多紧密，与相邻分类的数据匹配得有多松散。剪影得分范围在-1 和 1 之间，最好为 1。在我们的例子中，我们选择 30 个聚类，以便在评论类型中有足够的粒度，并获得大约 0.70 的良好轮廓分数。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="2f6f" class="lv lw iq mv b gy mz na l nb nc">%%R -i shap_values_imp_r -w 800 -h 800</span><span id="e17e" class="lv lw iq mv b gy nd na l nb nc">library(gplots)</span><span id="53e7" class="lv lw iq mv b gy nd na l nb nc">d_shap &lt;- dist(shap_values_imp_r[1:(ncol(shap_values_imp_r)-2)], 'euclidean') <br/>hc &lt;- hclust(d_shap, method = "ward.D2")<br/>dend &lt;- as.dendrogram(hc)</span><span id="fedf" class="lv lw iq mv b gy nd na l nb nc">library(dendextend)<br/>library(colorspace)</span><span id="fa24" class="lv lw iq mv b gy nd na l nb nc">n_clust &lt;- 30</span><span id="cf65" class="lv lw iq mv b gy nd na l nb nc">dend &lt;- color_branches(dend, k=n_clust) #, groupLabels=iris_species)</span><span id="daa1" class="lv lw iq mv b gy nd na l nb nc">clusters &lt;- cutree(hc, k=n_clust)<br/>#print(head(clusters))</span><span id="a25a" class="lv lw iq mv b gy nd na l nb nc">print(format(prop.table(table(clusters,shap_values_imp_r$top_reasons, shap_values_imp_r$target), margin=1), <br/>             digit=2, scientific = F))<br/>print(format(prop.table(table(clusters,shap_values_imp_r$top_reasons), margin=1), <br/>             digit=2, scientific = F))</span><span id="599f" class="lv lw iq mv b gy nd na l nb nc">heatmap.2(as.matrix(shap_values_imp_r[1:(ncol(shap_values_imp_r)-2)]), <br/>          dendrogram = "row",<br/>          Rowv = dend,<br/>          Colv = "NA", # this to make sure the columns are not ordered<br/>          key.xlab = "Predicted - Average log-odds",<br/>          #hclustfun=function(d) hclust(d, method="complete"), <br/>          srtCol=45,  adjCol = c(1,1))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/357d192d7b58d3d65e9095bb1006a8eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ms4mdz4564qRt-DOHyrPNQ.png"/></div></div></figure><p id="60c6" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在上面的热图中，我们将分层聚类(左侧 y 轴)与前 20 个术语的重要性(x 轴，根据评论中相应术语对模型预测的影响而着色的单元格)相结合，前者显示了更接近的相似评论(右侧 y 轴上的数字是评论行号)。</p><p id="15d8" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们可以通过汇总每个集群中的评论来进一步总结上面的热图。为了使这一步更通用，我们假设我们的数据集是一个标签未知的新数据集，并使用 k=1 的 k-最近邻分类器基于聚类结果预测标签。然后，在按预测标签分组后，我们可以聚合每个聚类中的 SHAP 值。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="e898" class="lv lw iq mv b gy mz na l nb nc">from sklearn.neighbors import KNeighborsClassifier</span><span id="0beb" class="lv lw iq mv b gy nd na l nb nc">cluster_model = KNeighborsClassifier(n_neighbors=1)<br/>cluster_model.fit(shap_values_imp,rpy2.robjects.r['clusters'])</span><span id="2963" class="lv lw iq mv b gy nd na l nb nc">predicted = cluster_model.predict(shap_values_imp_r[shap_values_imp_r.columns[:-2]]) </span><span id="ee00" class="lv lw iq mv b gy nd na l nb nc">grouped = pd.concat([shap_values_imp, pd.Series(youtube_df['label'].tolist(), name='avg_tgt')], axis=1).groupby(predicted)</span><span id="1bbb" class="lv lw iq mv b gy nd na l nb nc"># compute average impact to model prediction output. <br/>sums = grouped.apply(lambda x: np.mean(x))</span></pre><p id="6487" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">下面的热图显示了 30 个分类(y 轴)中的每个分类的前 20 个术语对模型预测的影响(z 轴上显示的对数优势)。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="7fa6" class="lv lw iq mv b gy mz na l nb nc">import plotly<br/>import plotly.graph_objs as go<br/>plotly.offline.init_notebook_mode()</span><span id="ee48" class="lv lw iq mv b gy nd na l nb nc">data = [go.Heatmap(z=sums[sums.columns[1:-1]].values.tolist(), <br/>                   y=sums.index,<br/>                   x=sums.columns[1:-1],<br/>                   colorscale='Blues')]</span><span id="f66e" class="lv lw iq mv b gy nd na l nb nc">plotly.offline.iplot(data, filename='pandas-heatmap')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/458e24d61559d7d7bcbdf4f5b39a9867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nX6EZaDXBAhCdNsTcBSptQ.png"/></div></div></figure><p id="06c6" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">例如，从热图中，我们可以看到，聚类 30 主要是“喜欢的评论”2 元语法，如果我们查看平均预测标签，我们可以看到这是合法评论的聚类(“总和”数据帧的“avg_tgt”值)。聚类 8 平均来说是垃圾邮件聚类，并且以术语“视图”为主。</p><p id="e112" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们还可以聚集每个聚类中评论的 SHAP 值，以显示每个聚类中什么“主题”占主导地位。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="bc5e" class="lv lw iq mv b gy mz na l nb nc">agg_sums = pd.DataFrame({k: sums[v].mean(axis=1) for (k, v) in top20_dict.items()})</span><span id="d049" class="lv lw iq mv b gy nd na l nb nc">data = [go.Heatmap(z=agg_sums[agg_sums.columns].values.tolist(), <br/>                   y=agg_sums.index,<br/>                   x=agg_sums.columns,<br/>                   colorscale='Blues')]</span><span id="2f25" class="lv lw iq mv b gy nd na l nb nc">plotly.offline.iplot(data, filename='pandas-heatmap')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/d032ff5906eb560fdd1c980925384cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04a3H_MWCse1zFp9uipdOQ.png"/></div></div></figure><p id="f977" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">第 30 组的意见是</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="2995" class="lv lw iq mv b gy mz na l nb nc">pd.options.display.max_colwidth = 1000</span><span id="22ce" class="lv lw iq mv b gy nd na l nb nc">cluster_no = 30</span><span id="29b2" class="lv lw iq mv b gy nd na l nb nc">ex_2 = youtube_df.iloc[rpy2.robjects.r['clusters']==cluster_no]<br/>ex_2_pred = pd.Series(predictions[rpy2.robjects.r['clusters']==cluster_no])<br/>ex_2_top = shap_values_imp_r.iloc[rpy2.robjects.r['clusters']==cluster_no]['top_reasons']</span><span id="09e3" class="lv lw iq mv b gy nd na l nb nc">ex_2</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/bc42c2df2bb37232c559ec8c7ec1c745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZUSJ_O1X_BGG1kYk1PNQzw.png"/></div></div></figure><p id="e4e2" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">第 8 组的意见是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/960d03f75b5de9138b9c32dd87c7e1f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K7QKhKCxGT283OTQGjDEDw.png"/></div></div></figure><h2 id="a8a3" class="lv lw iq bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">结论</h2><p id="af6d" class="pw-post-body-paragraph ky kz iq la b lb mo jr ld le mp ju lg lh mq lj lk ll mr ln lo lp ms lr ls lt ij bi translated">在这篇文章中，我展示了如何使用 Shapley 值向模型分数添加信息，这是获得预测标签的“原因”。事实上，一个模型分数应该总是跟随着对这个分数的解释。此外，聚类 Shapley 值可用于识别数据实例组，这些数据实例可以用代表多种原因的更通用的主题来解释。</p><p id="248d" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我要感谢<a class="ok ol ep" href="https://medium.com/u/b61b6673cdce?source=post_page-----7c945cc531f--------------------------------" rel="noopener" target="_blank"> Sundar Krishnan </a>和<a class="ok ol ep" href="https://medium.com/u/4d9e48e82766?source=post_page-----7c945cc531f--------------------------------" rel="noopener" target="_blank"> Praveen Thoranathula </a>的有益讨论。</p></div></div>    
</body>
</html>