# 大规模使用空间的几个技巧

> 原文：<https://towardsdatascience.com/a-couple-tricks-for-using-spacy-at-scale-54affd8326cf?source=collection_archive---------2----------------------->

## Python 包 spaCy 是自然语言处理的一个很好的工具。下面是我在大型数据集上使用它所做的一些事情。

![](img/b7ba2bea75217f988c1f58c402d9d2ad.png)

Me processing text on a Spark cluster (artist’s rendition).

***编辑:这个帖子现在已经过时了(看看一些评论)。自从我写这篇文章以来，SpaCy 引入了一个*** `***pipe***` ***方法，它做了我在这里所做的事情，只不过它不需要黑客，而且更快。建议大家都用那个方法。***

当我正在做的项目需要自然语言处理时，我倾向于首先求助于 SpaCy。Python 还有其他几个 NLP 包，[每个包都有自己的优缺点](https://medium.com/activewizards-machine-learning-company/comparison-of-top-6-python-nlp-libraries-c4ce160237eb)。我通常发现 spaCy 很快，它的约定很容易学习。

我在 Spacy 遇到的一个困难是需要处理大量的小文本。例如，我最近有几十万个不同的标签，每个标签有 1 到 15 个单词，我决定用 word2vec 来比较它们。我对 spaCy 的单词矢量化很满意，但我还需要处理每个字符串，首先删除不相关的单词和词性，对一些标记进行词汇化，等等。花了很长时间。以下是我加快速度的两种方法。

## 将所有内容作为一个文档处理，然后拆分成多个部分

处理一堆独立记录的直观方法是分别处理它们。我开始遍历每条记录，并对每条记录调用 spaCy 的`nlp`。我不知道处理每件事要花多长时间，因为我已经没有耐心了，只是在进程结束前就终止了它。所以我想了想，又读了一些文档，得出了这个结论:

这将所有文本连接成一个字符串，每个原始文本由一些字符序列分隔，这些字符序列不太可能自然出现。我选择了三个管道，每边都有一个空间。然后我给`nlp`打了一次电话。处理大约 25，000 条记录只需几秒钟。之后，我可以遍历标记列表，识别特殊的字符序列，并使用它将文档分割成多个部分。这些跨度包含单独的标记，并且具有文档的属性，例如单词向量。

## 使用火花

这并不像听起来那么简单。根据我的经验，做 NLP 最昂贵的部分是加载语料库。一旦你把所有的东西都装上了，就只需要高效地查找你需要的信息了。如果你正在使用 spaCy 提供的最大的英语语料库，就像我一样，你将面临一些困难来把这些语料库放到你的 Spark 执行器上。

首先，spaCy 使用了与 PySpark 不同的 pickling 库，这导致在试图将语料库包含在用户定义的字段中时出错——无论是作为广播变量还是其他。无论如何，序列化庞大的语料库，然后将它转移到执行器，然后再反序列化，这不一定有意义，因为您可以将它加载到执行器本身。

这提出了一个新问题。加载语料库需要很长时间，所以你想尽量减少需要加载的次数。我发现，在将 Scikit-learn 模型部署到 Pyspark 时，遵循我过去使用的一个实践[非常有用:将记录分组到一个相当长的记录列表中，然后调用列表中的一个 UDF。这允许你做一次昂贵的事情，代价是必须连续做一小组相对便宜的事情。](/deploy-a-python-model-more-efficiently-over-spark-497fc03e0a8d)

我的意思是:

因此，我将我的 spark 数据框随机分成 20 个部分，将我所有的文本收集到一个列表中。我的 UDF 从 spaCy 加载语料库，然后遍历文本进行处理。

需要注意两件事:

*   我在一个循环中分别处理每个文档。我没有理由要那么做。我可以使用第一个技巧，将所有文档作为一个整体处理，然后拆分成多个部分。这两个技巧并不相互排斥。
*   出于我的目的，我只需要单词向量，所以这就是我返回的全部内容。我不知道 PySpark 是否很难将整个 spaCy 文档或 span 对象从执行者那里移回来。我从来没试过。无论如何，我发现使用 Spark 只获取自己需要的东西是一个好习惯，因为移动东西是很昂贵的。所以如果你只需要单词向量，写一个只返回这些向量的 UDF。如果您只需要命名实体或词类之类的东西，编写一个 UDF 来返回它们。如果您需要所有这些内容，请在 UDF 中将其分离出来，并作为单独的字段返回。

使用这两种方法，处理大量文本对我来说变得更有效率。