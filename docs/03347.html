<html>
<head>
<title>A Beginner’s guide to XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost 初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7?source=collection_archive---------0-----------------------#2019-05-29">https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7?source=collection_archive---------0-----------------------#2019-05-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f2df" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这篇文章会有树…很多树</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/493aec412535a0cb88d2c81e6ff86138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*w4CMLEKAFp_bVYNk.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Trees… lots of them</figcaption></figure><blockquote class="ky kz la"><p id="b5a9" class="lb lc ld le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">想获得灵感？快来加入我的<a class="ae ly" href="https://www.superquotes.co/?utm_source=mediumtech&amp;utm_medium=web&amp;utm_campaign=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="le iu">超级行情快讯</strong> </a>。😎</p></blockquote><p id="cab9" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">XGBoost 是一个开源库，提供了梯度提升决策树的高性能实现。底层的 C++代码库与顶层的 Python 接口相结合，构成了一个极其强大而又易于实现的包。</p><p id="8398" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">XGBoost 的性能不是开玩笑的——它已经成为赢得许多 Kaggle 比赛的首选库。它的梯度增强实现是首屈一指的，随着该库继续获得好评，只会有更多的实现。</p><p id="5b66" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">在这篇文章中，我们将介绍 XGBoost 库的基础知识。我们将从梯度增强如何实际工作的实际解释开始，然后通过一个 Python 例子来说明 XGBoost 如何使它变得如此快速和简单。</p><h1 id="521b" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">助推树木</h1><p id="a380" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk lz mw ln lo ma mx lr ls mb my lv lw lx im bi translated">对于常规的机器学习模型，如决策树，我们只需在数据集上训练一个模型，并使用它进行预测。我们可能会对参数稍加改动或增加数据，但最终我们仍然使用单一模型。即使我们建立了一个集合，所有的模型都被单独训练和应用于我们的数据。</p><p id="5996" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated"><strong class="le iu">助推，另一方面，</strong>采取一种更加<em class="ld">迭代</em>的方法。从技术上来说，这仍然是一种合奏技术，因为许多模型被组合在一起以完成最终的一个，但采用了一种更聪明的方法。</p><p id="9a1a" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">boosting 不是孤立地训练所有模型，而是连续训练模型，每个新模型都被训练来纠正前一个模型所犯的错误。模型按顺序添加，直到不能再进一步改进。</p><p id="ab11" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">这种迭代方法的优点是，添加的新模型专注于纠正由其他模型引起的错误。在标准的集成方法中，模型是独立训练的，所有的模型最终可能会犯同样的错误！</p><p id="3ddc" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated"><strong class="le iu">梯度推进</strong>特别是一种训练新模型来预测先前模型的残差(即误差)的方法。我在下面的图表中概述了这种方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/893b0e1bfd9f90d3607bf9895d875816.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*A9myadIB_CqJv-EJA-G_bA.png"/></div></figure><h1 id="9001" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">XGBoost 入门</h1><p id="1387" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk lz mw ln lo ma mx lr ls mb my lv lw lx im bi translated">让我们开始使用这个强大的库— XGBoost。</p><p id="6661" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">我们要做的第一件事是安装库，这是通过 pip 最容易做到的。在<a class="ae ly" rel="noopener" target="_blank" href="/how-to-setup-a-python-environment-for-machine-learning-354d6c29a264"> Python 虚拟环境</a>中这样做也可能更安全。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="e916" class="nf md it nb b gy ng nh l ni nj">pip install xgboost</span></pre><h2 id="e4e6" class="nf md it bd me nk nl dn mi nm nn dp mm lz no np mo ma nq nr mq mb ns nt ms nu bi translated">用 XGBoost 设置数据</h2><p id="d4dd" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk lz mw ln lo ma mx lr ls mb my lv lw lx im bi translated">在余下的教程中，我们将使用鸢尾花数据集。我们可以使用 Scikit Learn 将它加载到 Python 中。同时，我们还将导入新安装的 XGBoost 库。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="253b" class="nf md it nb b gy ng nh l ni nj"><em class="ld">from </em>sklearn <em class="ld">import </em>datasets<br/><em class="ld">import </em>xgboost <em class="ld">as </em>xgb<br/><br/>iris = datasets.load_iris()<br/>X = iris.data<br/>y = iris.target</span></pre><p id="c88b" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">让我们把所有的数据都准备好。我们将从创建一个训练测试分割开始，这样我们就可以看到 XGBoost 执行得有多好。这次我们要 80%-20%的分成。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="6d95" class="nf md it nb b gy ng nh l ni nj">from sklearn.model_selection import train_test_split<br/><br/>X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)</span></pre><p id="b7f4" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">为了让 XGBoost 能够使用我们的数据，我们需要将它转换成 XGBoost 能够处理的特定格式。这种格式被称为<strong class="le iu">数据矩阵</strong>。将 numpy 数组的数据转换为 DMatrix 格式非常简单:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="bd8e" class="nf md it nb b gy ng nh l ni nj">D_train = xgb.DMatrix(X_train, label=Y_train)<br/>D_test = xgb.DMatrix(X_test, label=Y_test)</span></pre><h2 id="7368" class="nf md it bd me nk nl dn mi nm nn dp mm lz no np mo ma nq nr mq mb ns nt ms nu bi translated">定义 XGBoost 模型</h2><p id="0f1b" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk lz mw ln lo ma mx lr ls mb my lv lw lx im bi translated">现在我们的数据都加载了，我们可以定义我们的梯度推进系综的参数。我们在下面设置了一些最重要的，让我们开始。对于更复杂的任务和模型，可能的参数的完整列表可以在官方<a class="ae ly" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> XGBoost 网站</a>上获得。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="52f1" class="nf md it nb b gy ng nh l ni nj">param = {<br/>    'eta': 0.3, <br/>    'max_depth': 3,  <br/>    'objective': 'multi:softprob',  <br/>    'num_class': 3} <br/><br/>steps = 20  # The number of training iterations</span></pre><p id="78ae" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">最简单的参数是<em class="ld"> max_depth </em>(被训练的决策树的最大深度)<em class="ld"> objective </em>(正在使用的损失函数)，以及<em class="ld"> num_class </em>(数据集中的类的数量)。<em class="ld"> eta </em>算法需要特别注意。</p><p id="abfb" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">从我们的理论来看，梯度推进包括顺序地创建决策树并将其添加到集成模型中。创建新的树来校正来自现有集合的预测中的残留误差。</p><p id="a0e6" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">由于集合的性质，即，将几个模型放在一起形成本质上非常大的复杂模型，使得该技术易于过度拟合。<strong class="le iu"> eta </strong>参数为我们提供了一个防止过度拟合的机会</p><p id="112c" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">eta 可以更直观地认为是一个学习率<em class="ld">。不是简单地将新树的预测添加到具有全权重的集合中，而是将 eta 乘以所添加的残差以减少它们的权重。这有效地降低了整个模型的复杂性。</em></p><p id="989f" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">通常具有 0.1 至 0.3 范围内的小值。这些残差的较小权重仍将帮助我们训练一个强大的模型，但不会让该模型陷入更可能发生过度拟合的深度复杂性。</p><h2 id="c36b" class="nf md it bd me nk nl dn mi nm nn dp mm lz no np mo ma nq nr mq mb ns nt ms nu bi translated">培训和测试</h2><p id="f388" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk lz mw ln lo ma mx lr ls mb my lv lw lx im bi translated">我们最终可以像使用 Scikit Learn 那样训练我们的模型:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="16d1" class="nf md it nb b gy ng nh l ni nj">model = xgb.train(param, D_train, steps)</span></pre><p id="cf6a" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">现在让我们进行评估。同样，该过程与 Scikit Learn 中的培训模型非常相似:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="e8de" class="nf md it nb b gy ng nh l ni nj"><em class="ld">import </em>numpy <em class="ld">as </em>np<br/><em class="ld">from </em>sklearn.metrics <em class="ld">import </em>precision_score, recall_score, accuracy_score<br/><br/>preds = model.predict(D_test)<br/>best_preds = np.asarray([np.argmax(line) <em class="ld">for </em>line <em class="ld">in </em>preds])<br/><br/>print("Precision = {}".format(precision_score(Y_test, best_preds, average='macro')))<br/>print("Recall = {}".format(recall_score(Y_test, best_preds, average='macro')))<br/>print("Accuracy = {}".format(accuracy_score(Y_test, best_preds)))</span></pre><p id="a8c9" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">厉害！</p><p id="1331" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">如果到目前为止你已经遵循了所有的步骤，你应该得到至少 90%的准确率！</p><h1 id="2c76" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">XGBoost 的进一步探索</h1><p id="2ad4" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk lz mw ln lo ma mx lr ls mb my lv lw lx im bi translated">这就概括了 XGBoost 的基本知识。但是，还有一些更酷的功能可以帮助您充分利用您的模型。</p><ul class=""><li id="0715" class="nv nw it le b lf lg li lj lz nx ma ny mb nz lx oa ob oc od bi translated"><strong class="le iu"> gamma </strong>参数也有助于控制过度拟合。它指定了在树的叶节点上进行进一步划分所需的损失的最小减少量。也就是说，如果创建一个新节点不能减少一定数量的损失，那么我们根本不会创建它。</li><li id="6211" class="nv nw it le b lf oe li of lz og ma oh mb oi lx oa ob oc od bi translated"><strong class="le iu">助推器</strong>参数允许你设置你将在构建合奏时使用的模型类型。默认的是<em class="ld"> gbtree </em>，它构建了一个决策树集合。如果你的数据不太复杂，你可以使用更快更简单的<em class="ld"> gblinear </em>选项，它构建了一个线性模型集合。</li><li id="216a" class="nv nw it le b lf oe li of lz og ma oh mb oi lx oa ob oc od bi translated">设置任何 ML 模型的最佳超参数都是一项挑战。那么为什么不让 Scikit Learn 帮你做呢？我们可以很容易地将 Scikit Learn 的网格搜索与 XGBoost 分类器结合起来:</li></ul><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="b8b9" class="nf md it nb b gy ng nh l ni nj"><em class="ld">from </em>sklearn.model_selection <em class="ld">import </em>GridSearchCV<br/><br/>clf = xgb.XGBClassifier()<br/>parameters = {<br/>     "eta"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,<br/>     "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],<br/>     "min_child_weight" : [ 1, 3, 5, 7 ],<br/>     "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],<br/>     "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]<br/>     }<br/><br/>grid = GridSearchCV(clf,<br/>                    parameters, n_jobs=4,<br/>                    scoring="neg_log_loss",<br/>                    cv=3)<br/><br/>grid.fit(X_train, Y_train)</span></pre><p id="a888" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">如果你有时间的话，只在大数据集上这样做——进行网格搜索本质上是多次训练决策树的集合！</p><ul class=""><li id="bde1" class="nv nw it le b lf lg li lj lz nx ma ny mb nz lx oa ob oc od bi translated">一旦您的 XGBoost 模型经过训练，您就可以将它的可读描述转储到一个文本文件中:</li></ul><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="a1a7" class="nf md it nb b gy ng nh l ni nj">model.dump_model('dump.raw.txt')</span></pre><p id="fb0d" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk lz lm ln lo ma lq lr ls mb lu lv lw lx im bi translated">这是一个总结！</p></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><h1 id="a4c4" class="mc md it bd me mf oq mh mi mj or ml mm jz os ka mo kc ot kd mq kf ou kg ms mt bi translated">喜欢学习？</h1><p id="f839" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk lz mw ln lo ma mx lr ls mb my lv lw lx im bi translated">在 twitter 上关注我，我会在这里发布所有最新最棒的人工智能、技术和科学！也在 LinkedIn 上与我联系！</p></div></div>    
</body>
</html>