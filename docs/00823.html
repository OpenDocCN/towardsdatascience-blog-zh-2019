<html>
<head>
<title>Automatically Summarize Trump’s State of the Union Address</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动总结特朗普的国情咨文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automatically-summarize-trumps-state-of-the-union-address-6757c6af6534?source=collection_archive---------18-----------------------#2019-02-07">https://towardsdatascience.com/automatically-summarize-trumps-state-of-the-union-address-6757c6af6534?source=collection_archive---------18-----------------------#2019-02-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/0f72362b964ac55838826ea5db6a32be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nFuMVdH7XycJJbwFA9JAVg.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo credit: Pixabay</figcaption></figure><div class=""/><div class=""><h2 id="016f" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">文本排名，潜在语义分析，Gensim，Sumy，NLTK</h2></div><p id="7163" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://machinelearningmastery.com/gentle-introduction-text-summarization/" rel="noopener ugc nofollow" target="_blank">自动文本摘要，是为较长的文档创建简短、简明和连贯版本的过程</a>。这是<a class="ae lq" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP </a>领域中最有趣、最具挑战性的问题之一。</p><p id="848b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">自从川普昨晚发表了他的<a class="ae lq" href="https://www.whitehouse.gov/briefings-statements/remarks-president-trump-state-union-address-2/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw jg">国情咨文</strong> </a>，<a class="ae lq" href="https://www.npr.org/2019/02/06/691870683/7-takeaways-from-president-trumps-state-of-the-union-address" rel="noopener ugc nofollow" target="_blank">关键要点</a>，<a class="ae lq" href="https://www.cnn.com/2019/02/05/politics/fact-check-trump-state-of-the-union/index.html" rel="noopener ugc nofollow" target="_blank">事实核查</a>，<a class="ae lq" href="https://www.nytimes.com/interactive/2019/02/05/us/politics/state-of-union-live-chat.html" rel="noopener ugc nofollow" target="_blank">分析</a>，<a class="ae lq" href="https://www.cnbc.com/2019/02/06/sotu-how-the-world-reacted-to-trump-speech.html" rel="noopener ugc nofollow" target="_blank">反应</a>充斥着新闻媒体。如果你像我一样，不想听完 82 分钟的整个演讲，或阅读整个演讲，并且你不想错过任何重要的东西，那么我们将尝试探索文本摘要的领域并建立一个文本摘要器。希望我们从文本摘要器中得到的摘要尽可能地接近原文，但要短得多。我们开始吧！</p><h1 id="3749" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">带 NLTK 的 TextRank</h1><p id="79d3" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">TextRank 是一种无监督的文本摘要技术，它使用<a class="ae lq" href="https://en.wikipedia.org/wiki/PageRank" rel="noopener ugc nofollow" target="_blank"> PageRank </a>算法背后的直觉来对句子进行排序。在我们的项目中使用 NLTK 时，我们有以下步骤:</p><ul class=""><li id="5649" class="mo mp jf kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated">在网上获取国情咨文。</li><li id="6825" class="mo mp jf kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">基本文本清理。</li><li id="6b00" class="mo mp jf kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">找出每个句子的向量表示(单词嵌入)。</li><li id="9d19" class="mo mp jf kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">然后计算句子向量之间的相似度并存储在矩阵中。</li><li id="400a" class="mo mp jf kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">相似度矩阵转换成图，以句子为顶点，相似度得分为边。</li><li id="062f" class="mo mp jf kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">对该句子图应用<a class="ae lq" href="https://en.wikipedia.org/wiki/PageRank" rel="noopener ugc nofollow" target="_blank"> PageRank </a>算法进行句子等级计算。</li><li id="957c" class="mo mp jf kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">打印出几个排名靠前的句子。</li></ul><h2 id="5004" class="nc ls jf bd lt nd ne dn lx nf ng dp mb ld nh ni md lh nj nk mf ll nl nm mh nn bi translated">数据</h2><p id="e4fe" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">这些数据可以在白宫网站上找到，并于今天发布。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="ca6d" class="nc ls jf nt b gy nx ny l nz oa">def get_only_text(url):<br/>    """ <br/>    return the title and the text of the article<br/>    at the specified url<br/>    """<br/>    page = urlopen(url)<br/>    soup = BeautifulSoup(page, "lxml")<br/>    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))<br/>  <br/>    print ("=====================")<br/>    print (text)<br/>    print ("=====================")<br/> <br/>    return soup.title.text, text    <br/> <br/>     <br/>url="<a class="ae lq" href="https://www.whitehouse.gov/briefings-statements/remarks-president-trump-state-union-address-2/" rel="noopener ugc nofollow" target="_blank">https://www.whitehouse.gov/briefings-statements/remarks-president-trump-state-union-address-2/</a>"<br/>text = get_only_text(url)</span></pre><p id="cd33" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">首先，我们来窥见几个句子:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="d505" class="nc ls jf nt b gy nx ny l nz oa">sentences = []<br/>for s in text:<br/>    sentences.append(sent_tokenize(s))</span><span id="b2fe" class="nc ls jf nt b gy ob ny l nz oa">sentences = [y for x in sentences for y in x]<br/>sentences[30:40]</span></pre><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/28bc28b6b5d557fcd2b95bcb2560c38a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zGaLrqAT9FWmbSGbEs3r1A.png"/></div></div></figure><p id="a736" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">听起来差不多。</p><p id="2bef" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将使用预先训练的单词向量来为国情咨文中的句子创建向量。我已经从<a class="ae lq" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>下载了数据并保存在我的工作目录中。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="2d6e" class="nc ls jf nt b gy nx ny l nz oa">word_embeddings = {}<br/>f = open('glove.6B.100d.txt', encoding='utf-8')<br/>for line in f:<br/>    values = line.split()<br/>    word = values[0]<br/>    coefs = np.asarray(values[1:], dtype='float32')<br/>    word_embeddings[word] = coefs<br/>f.close()</span></pre><p id="61a2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一些基本的文本预处理，如删除停用词和删除特殊字符。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="2104" class="nc ls jf nt b gy nx ny l nz oa">clean_sentences = pd.Series(sentences).str.replace("[^a-zA-Z]", " ")<br/>clean_sentences = [s.lower() for s in clean_sentences]<br/>stop_words = stopwords.words('english')<br/><br/>def remove_stopwords(sen):<br/>    sen_new = " ".join([i for i in sen if i not in stop_words])<br/>    return sen_new<br/>clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]</span></pre><p id="b31a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在下面的代码脚本中，我们为句子创建向量。我们首先获取句子中组成单词的向量(每个向量的大小为 100 个元素),然后取这些向量的平均值，以得到该句子的合并向量。我们创建一个空的相似度矩阵，并用句子的余弦相似度填充它。最后，我们用余弦相似性分数初始化矩阵，并打印出排名前 15 的句子作为摘要表示。</p><figure class="no np nq nr gt is"><div class="bz fp l di"><div class="od oe l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">sentences_vector</figcaption></figure><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi of"><img src="../Images/0051e21585a03be668d52c514bb53c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FMezcasG4Ai8VgaKZ2pqig.png"/></div></div></figure><h1 id="0727" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">Sumy Python 模块</h1><p id="e788" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated"><a class="ae lq" href="https://pypi.org/project/sumy/" rel="noopener ugc nofollow" target="_blank"> Sumy </a>是一个 Python 库，用于从 HTML 页面或纯文本中提取摘要。是由<a class="ae lq" href="https://github.com/miso-belica" rel="noopener ugc nofollow" target="_blank"> Miso-Belica </a>开发的。我们将把以下总结方法应用到国情咨文中，并为每种方法打印出 10 个句子:</p><ul class=""><li id="6391" class="mo mp jf kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated"><em class="og"> LsaSummarizer </em>。<a class="ae lq" href="http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf" rel="noopener ugc nofollow" target="_blank">潜在语义分析</a>结合词频和奇异值分解。</li></ul><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="3b2f" class="nc ls jf nt b gy nx ny l nz oa">LANGUAGE = "english"<br/>SENTENCES_COUNT = 10<br/>url="<a class="ae lq" href="https://www.whitehouse.gov/briefings-statements/remarks-president-trump-state-union-address-2/" rel="noopener ugc nofollow" target="_blank">https://www.whitehouse.gov/briefings-statements/remarks-president-trump-state-union-address-2/</a>"<br/>parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))<br/>print ("--LsaSummarizer--")    <br/>summarizer = LsaSummarizer()<br/>summarizer = LsaSummarizer(Stemmer(LANGUAGE))<br/>summarizer.stop_words = get_stop_words(LANGUAGE)<br/>for sentence in summarizer(parser.document, SENTENCES_COUNT):<br/>    print(sentence)</span></pre><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/acc3c22d211f9b35e9400568887dbf16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zeikgAdMMEMhbfQmRfWPKg.png"/></div></div></figure><ul class=""><li id="0985" class="mo mp jf kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated"><em class="og">卢恩总结者</em>。一种基于 TF-IDF 并查看高重要性单词之间不重要单词的“窗口大小”的简单方法。它还为出现在文档开头附近的句子分配较高的权重。</li></ul><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="d4a7" class="nc ls jf nt b gy nx ny l nz oa">print ("--LuhnSummarizer--")     <br/>summarizer = LuhnSummarizer() <br/>summarizer = LuhnSummarizer(Stemmer(LANGUAGE))<br/>summarizer.stop_words = ("I", "am", "the", "you", "are", "me", "is", "than", "that", "this")<br/>for sentence in summarizer(parser.document, SENTENCES_COUNT):<br/>    print(sentence)</span></pre><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/4ecda98ba286090b5a8c97fa5c1529a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmaUjv1v4ZSpgY4gVYZI3w.png"/></div></div></figure><ul class=""><li id="8bc9" class="mo mp jf kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated"><em class="og">词法分析器</em>。受 PageRank 算法启发的无监督方法。它找出文档中所有单词的相对重要性，并选择包含最多高分单词的句子。</li></ul><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="0112" class="nc ls jf nt b gy nx ny l nz oa">print ("--LexRankSummarizer--")   <br/>summarizer = LexRankSummarizer()<br/>summarizer = LexRankSummarizer(Stemmer(LANGUAGE))<br/>summarizer.stop_words = ("I", "am", "the", "you", "are", "me", "is", "than", "that", "this")<br/>for sentence in summarizer(parser.document, SENTENCES_COUNT):<br/>    print(sentence)</span></pre><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/52b2fce06f158cfc1c8e42fde0edcf39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FURjPZZcP-MxPMuiDmctBQ.png"/></div></div></figure><ul class=""><li id="7f4e" class="mo mp jf kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated"><em class="og">埃德蒙森总结者。</em>使用<em class="og"> EdmundsonSummarizer 时，</em>我们需要输入我们希望在摘要中看到的有意义的单词，不重要的 stigma_words，以及停用单词 null_words。</li></ul><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="d992" class="nc ls jf nt b gy nx ny l nz oa">print ("--EdmundsonSummarizer--")     <br/>summarizer = EdmundsonSummarizer() <br/>words1 = ("economy", "fight", "trade", "china")<br/>summarizer.bonus_words = words1<br/>     <br/>words2 = ("another", "and", "some", "next")<br/>summarizer.stigma_words = words2<br/>    <br/>words3 = ("another", "and", "some", "next")<br/>summarizer.null_words = words3<br/>for sentence in summarizer(parser.document, SENTENCES_COUNT):<br/>    print(sentence)</span></pre><p id="2646" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这是<em class="og">edmundsonsumrizer</em>在我设置了上述单词标准后的输出:</p><figure class="no np nq nr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/ae2aa61f264e19db8a134ae7c1089f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ja_0nn6fTtKfNo0h6NqmAA.png"/></div></div></figure><p id="cc9e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在我们确定哪种方法是总结特朗普国情咨文的最佳方法之前，似乎有很多参数需要调整。不管怎样，我个人很享受这次学习之旅。希望你也是。</p><p id="74dd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Jupyter 笔记本可以在<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Automatically%20Summarize%20Trump's%20State%20of%20the%20Union%20Address.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。享受这周剩下的时光吧！</p><p id="781b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">参考资料:</p><div class="ip iq gp gr ir ol"><a href="https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd jg gy z fp oq fr fs or fu fw je bi translated">使用 TextRank 算法的文本摘要介绍(Python 实现)</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">引言文本摘要是自然语言处理(NLP)的应用之一</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ix ol"/></div></div></a></div><div class="ip iq gp gr ir ol"><a href="https://nlpforhackers.io/textrank-text-summarization/" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd jg gy z fp oq fr fs or fu fw je bi translated">黑客用文本摘要的文本排名</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">摘要任务是一个经典的任务，已经从不同的角度进行了研究。这项任务包括…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">nlpforhackers.io</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz ix ol"/></div></div></a></div><div class="ip iq gp gr ir ol"><a href="http://ai.intelligentonlinetools.com/ml/text-summarization/" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd jg gy z fp oq fr fs or fu fw je bi translated">使用 Python 文本分析技术的自动文本摘要</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">自动文本摘要是用软件缩短文本文档的过程，以便创建摘要…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">ai.intelligentonlinetools.com</p></div></div></div></a></div></div></div>    
</body>
</html>