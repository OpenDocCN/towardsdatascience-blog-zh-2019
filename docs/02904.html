<html>
<head>
<title>Feature Selection Why &amp; How Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择为什么和如何解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-why-how-explained-part-1-c2f638d24cdb?source=collection_archive---------5-----------------------#2019-05-11">https://towardsdatascience.com/feature-selection-why-how-explained-part-1-c2f638d24cdb?source=collection_archive---------5-----------------------#2019-05-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cfb2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解特征冗余和检测措施的影响</h2></div><p id="adf1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">机器学习不仅仅是花哨的模型和复杂的优化算法。如果获胜者采用了更先进的模型或设计了一些出色的功能，包括我在内的许多 Kagglers 都乐于承认失败，但只有少数人在这种情况下忍心接受失败——被一个训练有更少功能的简单模型击败。是的，并不是所有的从业者都认可极简主义在模特训练中的作用。在这第一篇文章中，我将解释冗余特征如何损害模型和一些直观的检测方法。</p><h1 id="ff18" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">越多越好，真的吗？</h1><p id="8c63" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">我理解在你的模型中尽可能多的打包功能是多么棒的感觉。我记得我多么喜欢看着我的训练 CSV 文件的大小，想象这个模型会有多好。然而，情况并非总是如此。原因如下:</p><h2 id="c104" class="ly lc iq bd ld lz ma dn lh mb mc dp ll ko md me ln ks mf mg lp kw mh mi lr mj bi translated">1.多余的功能减缓了训练过程</h2><p id="2226" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">这是显而易见的，特征的数量与训练时间正相关。功能越多，计算速度越慢。然而，还有一个隐藏的因素会显著降低训练速度。</p><p id="f6e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在训练集中具有相关的特征使得损失图变得病态(稍后定义)。例如，在一个 2-D 线性回归问题中，如果两个特征都是标准化的，损失的等高线图应该接近圆形<a class="ae mk" href="https://medium.com/@zhangzix/understand-data-normalization-in-machine-learning-8ff3062101f0" rel="noopener">(参见我的另一篇关于数据标准化的文章)。</a></p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="cf17" class="ly lc iq mq b gy mu mv l mw mx"># two features <br/>X1 = np.random.randn(1000,1)<br/>X1 = (X1 -np.mean(X1))/np.std(X1)<br/>X2 = np.random.randn(1000,1)<br/>X2 = (X2 - np.mean(X2))/np.std(X2)<br/>Y = 2.3*X1-0.8*X2 + 0.3*np.random.rand(1000,1)</span></pre><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div class="gh gi my"><img src="../Images/8ac0a54fd305f74e986d889ee011e53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*UCrU74aIk-pmjmDP-aOhjQ.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">gradient descent is smooth when features are independent, learning rate = 2e-3</figcaption></figure><p id="6baf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当要素相互关联时，景观会变成椭球形，梯度下降往往会采用之字形路径。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="1d31" class="ly lc iq mq b gy mu mv l mw mx"># two features <br/>X1 = np.random.randn(1000,1)<br/>X1 = (X1 -np.mean(X1))/np.std(X1)<br/>X2 = X1 + 0.5* np.random.randn(1000,1)<br/>X2 = (X2 - np.mean(X2))/np.std(X2)<br/>Y = 2.3*X1-0.8*X2 + 0.3*np.random.rand(1000,1)</span></pre><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0da5f685ce130591ef33cc6a657f63d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*w86mOqV_ByNTlJDOz0pcdw.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">learning rate = 2e-3</figcaption></figure><p id="f076" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这可能会有问题，因为梯度下降算法可能会振荡，并且需要太长时间才能收敛(<em class="nh">您可以通过降低学习速率来减少振荡，但这也会延长训练时间</em>)。请看两个特征高度相关的极端情况:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="0587" class="ly lc iq mq b gy mu mv l mw mx">X1 = np.random.randn(1000,1)<br/>X1 = (X1 -np.mean(X1))/np.std(X1)<br/>X2 = X1 + 0.02*np.random.randn(1000,1)<br/>X2 = (X2 - np.mean(X2))/np.std(X2)<br/>Y = 2.3*X1 - 0.8*X2 + 0.3*np.random.rand(1000,1)</span></pre><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/9038f3d9fdc7024f9d3a99e1bcaeaa16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*DHLkUQR3T2FNvgXN0vlIUA.png"/></div></figure><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/881ceb04ffcdc56f92c8ac1dc00bbfd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C707Y0x6KrgOMKSJQVdE5A.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Loss, learning rate = 2e-3(learning rate greater than this may diverge)</figcaption></figure><p id="1157" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如你所看到的，梯度下降在条件恶劣的损失景观中挣扎，需要更多的迭代(甚至 18，000 次迭代也不够！)实现合理亏损。基于高度相关特征的模型需要更多的迭代，因此需要更长的训练时间。</p><h2 id="58f1" class="ly lc iq bd ld lz ma dn lh mb mc dp ll ko md me ln ks mf mg lp kw mh mi lr mj bi translated">2.降低的估计能力</h2><p id="6c4f" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">我们现在知道，基于高度相关特征的模型需要更多的迭代来训练。因此，如果您在其他项目中使用合理的迭代次数，如 6000 或 10000(在大多数情况下非常足够)，算法可能会过早终止，这会损害模型的性能(请参见上面的损失表)。</p><p id="bfc1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有另一个原因:在下面的多元线性回归模型中</p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div class="gh gi no"><img src="../Images/4f96604c8871e4665b2436bed057fc5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*aTK9tJTWW87kZF7XfDH9DA.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">X is the training data matrix, Y is the output vector</figcaption></figure><p id="ddc9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理论上的最佳估计值是:</p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div class="gh gi np"><img src="../Images/14847fd27d1118a4ca0211aa100e45b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*J1v89O4UhE53wC4dPnxpNw.png"/></div></figure><p id="9573" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果 X 有冗余(共线性或多重共线性)，则 X 的秩未满。因此，我们不能得到最好的估计，因为 X^T*X 的逆不存在。(<a class="ae mk" href="https://www.stat.cmu.edu/~larry/=stat401/lecture-17.pdf" rel="noopener ugc nofollow" target="_blank">延伸阅读</a>)</p><h2 id="4286" class="ly lc iq bd ld lz ma dn lh mb mc dp ll ko md me ln ks mf mg lp kw mh mi lr mj bi translated">3.你的模型很难解释</h2><p id="3722" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">机器学习并不总是做预测，在某些应用中，支持基于模型的决策更重要。当涉及到模型分析时，简单的统计方法，如假设检验，就变得又好又方便。例如，人们使用标准误差及其相关的 p 值来判断某个特征是否与结果相关。</p><p id="c1ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当数据集中存在共线性时，这些要素的 t 分值通常较小，因为相关要素的权重具有较大的方差。换句话说，基于不同分区的模型可能具有非常不同的特征参数。例如:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="bd56" class="ly lc iq mq b gy mu mv l mw mx">X1 = np.random.randn(1000,1)<br/>X1 = (X1 -np.mean(X1))/np.std(X1)<br/>X2 = X1 <br/>Y = 0*X1 + 6*X2 + 0.3*np.random.rand(1000,1)<br/>#essentially 0*X1(X2) + 6*X1(X2) + 0.3*np.random.rand(1000,1)</span></pre><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nq"><img src="../Images/b513034354afed9a34b468410116f109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v-7Nfsm01-bxvT2DxTSnwA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">anywhere on the blue line has the minimum loss</figcaption></figure><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nr"><img src="../Images/fe9186b1ccdaf9e987a7f9dbcb48020b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HzCcNp0ys__9qZfQSDyWvA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">distribution of the estimated coefficient of X1 (w1)</figcaption></figure><p id="9a6f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于蓝线上的任何位置都能成功地使 MSE 最小化，因此只要 w1+w2=6，w1 和 w2 就可能相差很大。如果我们用 X1 不相关的零假设进行显著性检验，很可能我们无法拒绝零假设，因为标准误差很大。例如:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="c7a4" class="ly lc iq mq b gy mu mv l mw mx">import statsmodels.api as sma<br/>X1 = np.random.randn(1000,1)<br/>X1 = (X1 -np.mean(X1))/np.std(X1)<br/>X2 = X1 + 0.01*np.random.randn(1000,1)<br/>X2 = (X2 - np.mean(X2))/np.std(X2)<br/>X = np.concatenate((X1,X2), axis=1)<br/>Y = 0.5*X1 + X2 + 0.3*np.random.rand(1000,1)</span><span id="f836" class="ly lc iq mq b gy ns mv l mw mx"><strong class="mq ir"># we know both X1 and X2 contribute to the outcome</strong><br/>mod = sma.OLS(Y,X)<br/>res= mod.fit()<br/>print(res.summary()</span></pre><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nt"><img src="../Images/7dedc1a36fef58cb2084de6926526b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPTekoweouIo00MfQFv3YA.png"/></div></div></figure><p id="9914" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X1 和 X2 的系数都具有较小的 t 值(因此具有较大的 P 值)。因此，我们错误地得出结论，X1 和 X2 都不相关(<a class="ae mk" href="https://www.dummies.com/education/math/statistics/what-a-p-value-tells-you-about-statistical-data/" rel="noopener ugc nofollow" target="_blank">解释 P 值</a>)。特征冗余会导致较大的系数标准误差，并掩盖特征在回归中的真实作用。</p><p id="8333" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们了解了特征冗余是如何损害建模和数据分析的，识别并移除特征冗余(也称为特征选择)对我们最有利。</p><h1 id="cbc9" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">如何识别特征冗余？</h1><h2 id="3443" class="ly lc iq bd ld lz ma dn lh mb mc dp ll ko md me ln ks mf mg lp kw mh mi lr mj bi translated">检测相关特征:条件编号</h2><p id="dc6c" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">检测数据中多重共线性的一种流行方法称为特征系统分析，它使用条件数的概念。条件数的定义是:</p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/a7fbc9f942c960bfaff801a9774d4f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*V6-gW1BTOX60gyuVjBWuCA.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">condition number</figcaption></figure><p id="252f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定一个矩阵(输入数据 X)，求 X Corr(X)的相关矩阵:</p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nv"><img src="../Images/4dd603094a6dc1eeeed24a49ea9e4407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-6vQHnJXTzrRhOqJLB6Qg.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">correlation matrix of X</figcaption></figure><p id="77bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果 X 的相关矩阵有很大的条件数，说明共线性严重。</p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d8fc15d3e99c54367ba532e1c5699a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*2G943QXdPFKYZ9I8_WLQBA.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk"><a class="ae mk" href="https://pareonline.net/getvn.asp?v=13&amp;n=5" rel="noopener ugc nofollow" target="_blank">“Revisiting the Collinear Data Problem: An Assesment of Estimator ‘Ill-Conditioning’ in Linear Regression”</a></figcaption></figure><p id="f74f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是一个简短的示例，可以帮助您理解条件编号是如何工作的:</p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nx"><img src="../Images/c35c2b709b5e6acb4653f3f808d5ec75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8eT0hNxtehxpSRn00oT8ag.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">correlation between X1 and X2 determines the condition number</figcaption></figure><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/1339dccc2472c5d048f485e82aef9e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*GOqYolYYe6XGnlWWy530NQ.png"/></div></figure><h2 id="7034" class="ly lc iq bd ld lz ma dn lh mb mc dp ll ko md me ln ks mf mg lp kw mh mi lr mj bi translated">检测不相关的特征</h2><p id="80b9" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">检测无关特征的核心是发现输出是否受到给定特征的影响。基于输出的类型(连续或分类)和特性<strong class="kh ir">1，有几种数学方法可以使用。分类特征分类反应—卡方检验 </strong></p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/9d32bac5c287264dc29b1c325136d162.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*1_F9GeJQSikCXFFssbFy8g.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Categorical feature categorical response</figcaption></figure><p id="d5ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">卡方检验的本质是假设输入和输出之间没有关系，并检查该假设的有效性。如果该特征确实影响响应，我们期望看到低 P 值。因此，通过选择具有低 p 值的特征来完成特征选择。<a class="ae mk" href="https://www.khanacademy.org/math/statistics-probability/inference-categorical-data-chi-square-tests/chi-square-goodness-of-fit-tests/v/pearson-s-chi-square-test-goodness-of-fit" rel="noopener ugc nofollow" target="_blank">点击此处查看更多卡方检验信息</a>。</p><p id="f1d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我创建了一些简单的人工数据集，让你更好地理解卡方为什么有用</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="6952" class="ly lc iq mq b gy mu mv l mw mx">import pandas as pd<br/>import scipy.stats as sst</span><span id="e769" class="ly lc iq mq b gy ns mv l mw mx">X = np.random.randint(5,size=1000)<br/>Y = np.ones(X.shape)<br/>index = X&lt;=2 <br/>Y[index] =0 # when X &lt;=2, Y = 0. Therefore X influences Y<br/>crosstab = pd.crosstab(X,Y) <br/>chi2, p,_,_ = sst.chi2_contingency(crosstab)<br/><strong class="mq ir">###P value =  3.569412779777166e-215-&gt; reject the null hypothesis   -&gt;feature is relevant</strong></span><span id="44ce" class="ly lc iq mq b gy ns mv l mw mx">X = np.random.randint(5,size=1000)<br/>Y = np.random.randint(2, size = 1000) # no relatinoship<br/>crosstab = pd.crosstab(X,Y)<br/>chi2, p,_,_ = sst.chi2_contingency(crosstab)<br/><strong class="mq ir">###P value =  0.5244308199595783-&gt; large P value, relationship is statistically insignificant</strong></span></pre><p id="2745" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">2<em class="nh">。类别特征连续反应—方差分析</em>T5】</strong></p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oa"><img src="../Images/9621df67628debb8f1803d6b2cb123cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DkGqdnojmxbKA7HrChExRw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Categorical feature continuous response</figcaption></figure><p id="d9f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">方差分析(ANOVA)比较不同组的平均值(每个分类特征数据的响应平均值),并检验组间差异是否具有统计学意义。如果一个特征是相关的，我们期望看到不同组的均值之间的显著差异。<a class="ae mk" href="https://www.youtube.com/watch?v=-yQb_ZJnFXw" rel="noopener ugc nofollow" target="_blank">点击此处查看更多关于方差分析的信息</a></p><p id="6c83" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">示例:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="a057" class="ly lc iq mq b gy mu mv l mw mx">X = np.random.randint(3,size=1000)<br/>Y = np.random.rand(1000) # no relationship <br/>one = Y[X==1]<br/>two = Y[X==2]<br/>zero = Y[X==0]<br/>sst.f_oneway(one,two,zero)<br/><strong class="mq ir">###statistic=0.07592457518151591, pvalue=0.9268914727618249-&gt; large P value-&gt; relationship is insignificant<br/></strong>X = np.random.randint(3,size=1000)<br/>Y = np.random.rand(1000) + 0.1*X # X is part of Y<br/>one = Y[X==1]<br/>two = Y[X==2]<br/>zero = Y[X==0]<br/>sst.f_oneway(one,two,zero)<br/><strong class="mq ir">### F_onewayResult(statistic=38.076396290550555, pvalue=1.1607768540773696e-16)-&gt; reject the null hypothesis-&gt; feature is relevant</strong></span></pre><p id="57bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nh"> 3。连续特征连续反应—相关或卡方/方差分析</em> </strong></p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ob"><img src="../Images/0a3715e2cfb11df07989bfee5e4c5c64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQqymZpb1_CaFyAvAu2uFg.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">continuous features and continuous response</figcaption></figure><p id="e834" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最常见的相关系数称为<a class="ae mk" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">皮尔逊系数</a>，用于测试特征和响应之间的线性关系。当关系为非线性时，Pearson 相关返回低值。一个更好的选择叫做<a class="ae mk" href="https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php" rel="noopener ugc nofollow" target="_blank"> Spearman 相关性</a>，它测试单调关系是否存在。以下是一些例子:</p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oc"><img src="../Images/bf806c411040219d8cd272afc9b74744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*upN18tk9zT2BZ-B1hTYa_A.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Pearson’s Correlation Vs. Spearman’s Correlation</figcaption></figure><p id="a21a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如您所见，Spearman 的相关性对于我们的应用程序有更好的性能，因为我们想要检测不相关的特征(Pearson 的给出更多的错误警报)。然而，当关系是非单调的(第四行)时，这两个指标的表现都很差。</p><p id="053d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一种方法是将两个变量离散化，并使用卡方检验，正如您所看到的，它成功地确定了非单调关系:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="2993" class="ly lc iq mq b gy mu mv l mw mx">X = 10*np.random.rand(1000)<br/>Y = np.sin(X)<br/>X_ca = pd.qcut(X,10, labels=False) # discretize into 10 classes<br/>Y_ca = pd.qcut(Y,10, labels=False)</span><span id="b3fa" class="ly lc iq mq b gy ns mv l mw mx">crosstab = pd.crosstab(X_ca,Y_ca)<br/>chi2, p,_,_ = sst.chi2_contingency(crosstab)<br/>print ('P value = ', p)<br/><strong class="mq ir">### P value =  0.0-&gt; a strong relationship</strong></span></pre><p id="d275" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nh"> 4。连续特征分类反应—卡方检验</em> </strong></p><figure class="ml mm mn mo gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi od"><img src="../Images/990c87fc31f2d09bfd45cb9d685fe0bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UoXU84PhTeneqU4uOuq0kA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">continuous feature categorical response</figcaption></figure><p id="762c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这个问题，我们可以简单地通过将数据分成不同的“组”来离散化特征，就像我们在第 3 部分中所做的那样。离散化后，我们可以再次应用卡方检验。</p><h1 id="9f9a" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">摘要</h1><p id="3e4c" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">好的特征可以显著提高模型的准确性和稳定性。因此，添加尽可能多的功能，以便找到好功能的可能性更高，这似乎是合理的。然而，使用大量特征在训练速度、准确性和可解释性方面可能是有害的，这就是为什么特征选择受到许多数据从业者的青睐。在下一篇文章中，你会看到一些系统化的算法，这些算法会自动选择相关特征并消除共线特征。</p></div></div>    
</body>
</html>