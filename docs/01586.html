<html>
<head>
<title>Loss functions based on feature activation and style loss.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于特征激活和风格损失的损失函数。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/loss-functions-based-on-feature-activation-and-style-loss-2f0b72fd32a9?source=collection_archive---------9-----------------------#2019-03-14">https://towardsdatascience.com/loss-functions-based-on-feature-activation-and-style-loss-2f0b72fd32a9?source=collection_archive---------9-----------------------#2019-03-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d722a86795280bf91c88d63541ac7e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kE4ifJgs2KX9vz6uUg5vQQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Feature activations in Convolutional Neural Networks. Source: <a class="ae kc" href="https://arxiv.org/pdf/1311.2901.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1311.2901.pdf</a></figcaption></figure><p id="b0c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用这些技术的损失函数可以在基于 U-Net 的模型架构的训练期间使用，并且可以应用于正在生成图像作为其预测/输出的其他卷积神经网络的训练。</p><p id="15ce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我已经将这一点从我关于超分辨率的文章中分离出来(<a class="ae kc" rel="noopener" target="_blank" href="/deep-learning-based-super-resolution-without-using-a-gan-11c9bb5b6cd5">https://towards data science . com/deep-learning-based-Super-Resolution-without-use-a-gan-11 C9 bb 5b 6 CD 5</a>)，以更加通用，因为我在其他基于 U-Net 的模型上使用类似的损失函数对图像数据进行预测。将此分开，便于参考，也便于理解我的其他文章。</p><p id="c636" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是基于 Fastai 深度学习课程中演示和教授的技术。</p><p id="dd3b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该损失函数部分基于论文《实时风格传输和超分辨率的损失》中的研究以及 Fastai 课程(v3)中所示的改进。</p><p id="ed4a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文重点研究特征损失(文中称为感知损失)。这项研究没有使用 U-Net 架构，因为当时机器学习社区还不知道它们。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lb"><img src="../Images/3733f02dc75e5861c6830c2067b5feb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iwXnji_6lny5slPo"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Source: Convolutional Neural Network (CNN) Perceptual Losses for Real-Time Style Transfer and Super-Resolution: <a class="ae kc" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1603.08155</a></figcaption></figure><p id="cc00" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所使用的损失函数类似于论文中的损失函数，使用 VGG-16，但是也结合了像素均方误差损失和 gram 矩阵风格损失。Fastai 团队发现这非常有效。</p><h2 id="63f8" class="lg lh iq bd li lj lk dn ll lm ln dp lo ko lp lq lr ks ls lt lu kw lv lw lx ly bi translated">VGG-16</h2><p id="0644" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">VGG 是 2014 年设计的另一种卷积神经网络(CNN)架构，16 层版本用于训练该模型的损失函数。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/5935cad94bff00a4018a595b000abce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QmJ2BSqbH_lw3Fel"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">VGG-16 Network Architecture. Source: <a class="ae kc" href="https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png" rel="noopener ugc nofollow" target="_blank">https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png</a></figcaption></figure><p id="cf29" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">VGG 模式。在 ImageNet 上预先训练的网络用于评估发电机模型的损耗。通常这将被用作分类器来告诉你图像是什么，例如这是一个人，一只狗还是一只猫。</p><p id="94e1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">VGG 模型的头部是最后几层，在上图中显示为全连接和 softmax。该头部被忽略，损失函数使用网络主干中的中间激活，其代表特征检测。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mf"><img src="../Images/e3c70cdb912861b9425f714e4791362e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3oRr_BjTm49I8qge"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Different layers in VGG-16. Source: <a class="ae kc" href="https://neurohive.io/wp-content/uploads/2018/11/vgg16.png" rel="noopener ugc nofollow" target="_blank">https://neurohive.io/wp-content/uploads/2018/11/vgg16.png</a></figcaption></figure><p id="8277" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些激活可以通过查看 VGG 模型找到所有的最大池层。这些是检测网格大小变化和特征的地方。</p><p id="b85a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图显示了各种图像的图层激活热图。这显示了在网络的不同层中检测到的各种特征的例子。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mg"><img src="../Images/51a4ca4bc3ae65ebea26753a392b57b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fATCVASBd41VXSxM"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Visualisation of feature activations in CNNs. Source: page 4 of <a class="ae kc" href="https://arxiv.org/pdf/1311.2901.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1311.2901.pdf</a></figcaption></figure><p id="e26c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于 VGG 模型的激活，模型的训练可以使用这个损失函数。损失函数在整个训练过程中保持固定，不像 GAN 的关键部分。</p><h2 id="9b8d" class="lg lh iq bd li lj lk dn ll lm ln dp lo ko lp lq lr ks ls lt lu kw lv lw lx ly bi translated">特征损失</h2><p id="bd0b" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">特征图有 256 个 28×28 的通道，用于检测毛发、眼球、翅膀和类型材料等特征以及许多其他类型的特征。使用基本损失的均方误差或最小绝对误差(L1)误差来比较(目标)原始图像和生成图像在同一层的激活。这些是特征损失。该误差函数使用 L1 误差。</p><p id="8024" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这使得损失函数能够了解目标地面真实影像中的特征，并评估模型预测的特征与这些特征的匹配程度，而不仅仅是比较像素差异。这允许用该损失函数训练的模型在生成/预测的特征和输出中产生更精细的细节。</p><h2 id="9c82" class="lg lh iq bd li lj lk dn ll lm ln dp lo ko lp lq lr ks ls lt lu kw lv lw lx ly bi translated">格拉姆矩阵风格损失</h2><p id="f384" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">gram 矩阵定义了关于特定内容的样式。通过计算目标/地面真实图像中每个特征激活的 gram 矩阵，允许定义该特征的风格。如果从预测的激活中计算出相同的 gram 矩阵，则可以比较这两者来计算特征预测的风格与目标/地面真实图像的接近程度。</p><p id="55aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">gram 矩阵是每个激活和激活矩阵的转置的矩阵乘法。</p><p id="48f9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这使得模型能够学习并生成图像的预测，这些图像的特征在其风格和上下文中看起来是正确的，最终结果看起来更有说服力，并且看起来与目标/背景真相更接近或相同。</p><h2 id="54bb" class="lg lh iq bd li lj lk dn ll lm ln dp lo ko lp lq lr ks ls lt lu kw lv lw lx ly bi translated">用此损失函数训练的模型的预测</h2><p id="dc67" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">使用基于这些技术的损失函数从训练模型生成的预测具有令人信服的细节和风格。风格和细节可能是预测精细像素细节或预测正确颜色的图像质量的不同方面。</p><p id="e9e1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用基于这种技术的损失函数训练的模型的两个例子，显示了用这种特征和风格损失函数训练的模型是多么有效:</p><p id="1cc8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自我的超分辨率实验:<a class="ae kc" rel="noopener" target="_blank" href="/deep-learning-based-super-resolution-without-using-a-gan-11c9bb5b6cd5">https://towardsdatascience . com/deep-learning-based-super-resolution-without-use-a-gan-11 c 9 bb 5b 6 CD 5</a></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="ab gu cl mh"><img src="../Images/f06635b4e127cf92825a9e5d8678f44a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*x0v0oQEGCVI1yeZmboIr9w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Super resolution on an image from the Div2K validation dataset</figcaption></figure><p id="58fc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据我的着色实验，文章发表时会添加一个链接:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="ab gu cl mh"><img src="../Images/91c9c6f7882d60ed4d12d56d0c828054.png" data-original-src="https://miro.medium.com/v2/format:webp/1*8xBSCd60j3XPq_QJ9QGXaA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Enhancement of a Greyscale 1 channel image to a 3 channel colour image.</figcaption></figure><h1 id="daa5" class="mi lh iq bd li mj mk ml ll mm mn mo lo mp mq mr lr ms mt mu lu mv mw mx lx my bi translated">法斯泰</h1><p id="b6e9" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">感谢 Fastai 团队，没有你们的课程和软件库，我怀疑我是否能够学习这些技术。</p></div></div>    
</body>
</html>