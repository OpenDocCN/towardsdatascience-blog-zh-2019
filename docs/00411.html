<html>
<head>
<title>Deep Q-Network Implementation with SONY’s NNabla</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用索尼的 NNabla 实现深度 Q 网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-q-network-implementation-with-sonys-nnabla-490d945deb8e?source=collection_archive---------10-----------------------#2019-01-18">https://towardsdatascience.com/deep-q-network-implementation-with-sonys-nnabla-490d945deb8e?source=collection_archive---------10-----------------------#2019-01-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/26746e322594afe6cebe9aae44ab6081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rGAgI8Ht3l6cA64ND6Mnjg.png"/></div></div></figure><h1 id="75d5" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">NNABLA 是什么？</h1><p id="d2e5" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">索尼发布了<a class="ae lu" href="https://nnabla.org" rel="noopener ugc nofollow" target="_blank">神经网络库</a>，简称“NNabla”。</p><p id="6f19" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">NNabla 是设备就绪的，通过有效的内存管理，具有快速的 GPU 训练速度。最有趣的特性是 NNabla 默认允许运行定义和运行定义。例如，定义并运行风格的代码如下所示。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="1820" class="mj jz iq mf b gy mk ml l mm mn"># build static graph like tensorflow<br/>x = nn.Variable((2, 3))<br/>out = PF.affine(x, 10)</span><span id="10c3" class="mj jz iq mf b gy mo ml l mm mn"># inference like sess.run<br/>x.d = np.random.random((2, 3))<br/>out.forward()<br/>value = out.d</span></pre><p id="1631" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">另一方面，由运行定义的样式可以写成这样。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="8573" class="mj jz iq mf b gy mk ml l mm mn">with nn.auto_forward():<br/>    x = nn.Variable.from_numpy_array(np.random.random(2, 3))<br/>    out = PF.affine(x, 10)<br/>value = out.d</span></pre><p id="d207" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">更多示例代码可在<a class="ae lu" href="https://github.com/sony/nnabla-examples" rel="noopener ugc nofollow" target="_blank">https://github.com/sony/nnabla-examples</a>获得。</p><h1 id="0387" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">NNABLA 中的深度强化学习</h1><p id="b350" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">虽然索尼分享了许多监督学习实现，但没有任何深度强化学习(DRL)的例子。如果你曾经尝试过实现你自己的 DRL 代码，你知道有一些在监督学习中很少见到的技巧。</p><p id="9549" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">在本帖中，我将向您展示 Deep Q-Network (DQN)的实现，作为 DRL NNA bla 的第一步。完整的实现可在<a class="ae lu" href="https://gist.github.com/takuseno/e648c3ebfc03b2f6f19709043b8fc69f" rel="noopener ugc nofollow" target="_blank">这里</a>获得。超参数和实现细节遵循最初的 DQN，您可以将此代码用作学术论文的基线。</p><h2 id="640c" class="mj jz iq bd ka mp mq dn ke mr ms dp ki lh mt mu km ll mv mw kq lp mx my ku mz bi translated">q 函数</h2><p id="c054" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">由于算法的概述在许多其他博客帖子中有解释，我们只关注神经网络部分。这是一个完整的神经网络定义。</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="d892" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">我必须解释上面代码中的两个技巧。</p><h2 id="66c8" class="mj jz iq bd ka mp mq dn ke mr ms dp ki lh mt mu km ll mv mw kq lp mx my ku mz bi translated">参数同步(更新目标)</h2><p id="b0f1" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如果熟悉 tensorflow，就知道参数同步可以用<code class="fe nc nd ne mf b">tf.assign</code>来写。但是，NNabla 目前在图形定义中没有参数赋值。要做到这一点，你必须直接操纵变量。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="8daf" class="mj jz iq mf b gy mk ml l mm mn"># copy source variable to destination variable<br/>variable_dst.data.copy_from(variable_src.data)</span></pre><p id="432b" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated"><code class="fe nc nd ne mf b">data</code>属性表示参数的引用。您可以通过该属性访问和操作它们。</p><p id="6de5" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">由于我通常是一个 tensorflow 程序员，我真的希望<code class="fe nc nd ne mf b">tf.assign</code>实现这种行为。最终我提交了一个<a class="ae lu" href="https://github.com/sony/nnabla/pull/244" rel="noopener ugc nofollow" target="_blank"> pull 请求</a>让 NNabla <code class="fe nc nd ne mf b">F.assign</code>函数。</p><h2 id="b1ec" class="mj jz iq bd ka mp mq dn ke mr ms dp ki lh mt mu km ll mv mw kq lp mx my ku mz bi translated">渐变剪辑(序列)</h2><p id="b686" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">为了避免参数变化太大，在 DQN 中使用了剪辑渐变。实际上，NNabla 有<code class="fe nc nd ne mf b">F.clip_grad_by_norm</code>功能，但不清楚如何使用，因为关于这方面的文档较少。所以，我实现了如下。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="7b3f" class="mj jz iq mf b gy mk ml l mm mn">grad = 10.0 * variable.grad / np.sqrt(np.sum(variable.g ** 2))<br/>variable.grad = grad</span></pre><p id="eb19" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated"><code class="fe nc nd ne mf b">10.0</code>是超参数尺寸。你也可以像<code class="fe nc nd ne mf b">data</code>属性一样访问渐变。在更新参数之前，您可以直接裁剪渐变。</p><h1 id="cf44" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论</h1><p id="9520" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我用 NNabla 展示了 DQN 的例子，它也展示了你可以用 NNabla 编写你自己的代码。如果你在 GPU 机器上运行全部代码，你可能会注意到它的速度。在大多数情况下，由于良好的内存管理架构，NNabla 是最快的。此外，索尼报告了用 2176 个 GPU 进行最快的 ImageNet 训练。此外，NNabla 提供了简单的 Python APIs，如果你已经在其他库中编写了深度学习算法，你可以在一分钟内编写自己的代码。特别是，NNabla 针对嵌入式设备进行了优化。如果你对边缘深度学习感兴趣，NNabla 将是一个不错的选择。</p></div></div>    
</body>
</html>