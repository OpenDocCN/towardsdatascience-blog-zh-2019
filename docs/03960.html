<html>
<head>
<title>Dimension Reduction Techniques with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 的降维技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimension-reduction-techniques-with-python-f36ca7009e5c?source=collection_archive---------0-----------------------#2019-06-22">https://towardsdatascience.com/dimension-reduction-techniques-with-python-f36ca7009e5c?source=collection_archive---------0-----------------------#2019-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c6e6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">实践用户实用指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/c1144a9d8bf079666831e7c1e63e348a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*6buWUeEBnc4AqKOceF6ErA.png"/></div></figure><p id="490d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">(2021 年 10 月 12 日修订)</p><p id="22cf" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为什么我们需要降维？</p><p id="a3d6" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">高维数据集是具有大量列(或变量)的数据集。这样的数据集提出了许多数学或计算挑战<strong class="ks iu">。</strong>好消息是变量(或称为特征)通常是相关的——高维数据“表面上”由少量简单变量支配。我们可以找到变量的子集来表示数据中相同级别的信息，或者将变量转换为一组新的变量，而不会丢失太多信息。虽然高性能计算可以处理高维数据，但在许多应用中仍然需要降低原始数据的维数。</p><p id="7c95" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">当我们想到降维时，主成分分析(PCA)可能是最流行的技术。在本文中，我将从 PCA 开始，然后介绍其他降维技术。Python 代码将包含在每项技术中。</p><p id="80ea" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">降维也能发现离群值</strong></p><p id="b682" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">数据科学家可以使用降维技术来识别异常。为什么？我们不就是想降维吗？直觉在于离群值本身。D.M .霍金斯说:“异常值是一个与其他观测值相差如此之大的观测值，以至于让人怀疑它是由不同的机制产生的。”一旦维度被减少到更少的主维度，模式被识别并且<strong class="ks iu">然后离群值被揭示</strong>。我们可以说离群点检测是降维的副产品，如文章“<a class="ae lm" rel="noopener" target="_blank" href="/anomaly-detection-with-autoencoder-b4cdce4866a6">使用自动编码器的异常检测变得简单</a>”中所述。</p><p id="e1d8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我写过关于各种数据科学主题的文章。为了方便使用，你可以将我的总结文章“<a class="ae lm" href="https://medium.com/@Dataman.ai/dataman-learning-paths-build-your-skills-drive-your-career-e1aee030ff6e" rel="noopener">数据人学习之路——培养你的技能，推动你的职业发展</a>”加入书签，其中列出了所有文章的链接。</p><p id="3c69" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">值得一提的是，现实生活中很多分类问题都是多类的。如果你曾经面临对多类问题建模的需求，请参见我的帖子“<a class="ae lm" href="https://medium.com/dataman-in-ai/a-wide-choice-for-modeling-multi-class-classifications-d97073ff4ec8" rel="noopener">多类分类的各种模型</a>”。</p><p id="5b91" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">主成分分析</strong></p><p id="cdb7" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">主成分分析(PCA)的思想是降低由大量相关变量组成的数据集的维度，同时尽可能多地保留数据中的方差。PCA 找到一组新变量，原来的变量只是它们的线性组合。新的变量被称为<em class="ln">主成分(PCs) </em>。这些主成分是<em class="ln">正交的</em>:在三维情况下，主成分彼此垂直。x 不能用 Y 表示或者 Y 不能用 z 表示。</p><p id="1a6d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">图(A)显示了 PCA 的直觉:它“旋转”轴以更好地与您的数据对齐。第一个主成分将捕获数据中的大部分差异，然后是第二个、第三个，依此类推。因此，新数据的维度会更少。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/e82d810dbe3245e920d5c3e57a237e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*d30YKpg-mAMWI3ekYM1plA.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Figure (A): PCA</figcaption></figure><p id="2bf0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们使用 iris 数据集来说明 PCA:</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="cf6c" class="ly lz it lu b gy ma mb l mc md"># Use the iris dataset to illustrate PCA:<br/>import pandas as pd<br/>url = “<a class="ae lm" href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a>"<br/># load dataset into Pandas DataFrame<br/>df = pd.read_csv(url, names=[‘sepal length’,’sepal width’,’petal length’,’petal width’,’target’])<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/e21f65943e8f040877b063380f3bfb01.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*z-YRY-YbxE2YLRyipzl5aQ.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">IRIS dataset</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/8fb52e4b0829f1a82f13c81c50ba5f78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nfK3vGZkTa4GrO7yWpcS-Q.png"/></div></div></figure><p id="151c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">注意这个 IRIS 数据集带有目标变量。在 PCA 中，只转换 X 变量，而不转换目标 Y 变量。</p><p id="bb96" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">标准化:</strong>在应用 PCA 之前，所有变量应该在相同的尺度上，否则，具有大值的特征将支配结果。这一点在我的文章“避免这些致命的建模错误，这些错误可能会让你失去职业生涯”中有进一步的解释。下面我使用 scikit 中的<strong class="ks iu"> StandardScaler </strong>来学习将数据集的特征标准化到单位尺度上(均值= 0，方差= 1)。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="47f3" class="ly lz it lu b gy ma mb l mc md">from sklearn.preprocessing import StandardScaler<br/>variables = [‘sepal length’, ‘sepal width’, ‘petal length’, ‘petal width’]<br/>x = df.loc[:, variables].values<br/>y = df.loc[:,[‘target’]].values<br/>x = StandardScaler().fit_transform(x)<br/>x = pd.DataFrame(x)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/137717b16c88a5c8529fb2b5d43c84b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*hExuh028sgLCVCYnb0t2dA.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Standardized features</figcaption></figure><p id="a976" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">原始数据中有四个特征。因此 PCA 将提供相同数量的主成分。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="716f" class="ly lz it lu b gy ma mb l mc md">from sklearn.decomposition import PCA<br/>pca = PCA()<br/>x_pca = pca.fit_transform(x)<br/>x_pca = pd.DataFrame(x_pca)<br/>x_pca.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/98eb4448456b4ff42a2aa07affe447d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*jeHJde_TqC7n3iW-EDVipQ.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">The Principal Components for the IRIS Dataset</figcaption></figure><p id="3c44" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">每个主成分解释的差异是什么？使用<code class="fe mm mn mo lu b">pca.explained_variance_ratio_</code>返回方差的向量:</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="7e61" class="ly lz it lu b gy ma mb l mc md">explained_variance = pca.explained_variance_ratio_<br/>explained_variance</span><span id="5a67" class="ly lz it lu b gy mp mb l mc md">array([0.72770452, 0.23030523, 0.03683832, 0.00515193])</span></pre><p id="87a9" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">结果表明，第一主成分解释了 72.22%的方差，第二、第三和第四主成分分别解释了 23.9%、3.68%和 0.51%的方差。我们可以说 72.22 + 23.9 = 96.21%的信息被第一和第二主成分捕获。我们通常希望只保留重要的特性，而放弃不重要的特性。一个经验法则是保留捕捉显著差异的顶部主成分，忽略小的主成分。</p><p id="5fb0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们可以使用前两个组件来绘制结果。让我们将目标变量 y 附加到新数据 x_pca 上:</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="dbec" class="ly lz it lu b gy ma mb l mc md">x_pca[‘target’]=y<br/>x_pca.columns = [‘PC1’,’PC2',’PC3',’PC4',’target’]<br/>x_pca.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/b1290593cf156d524e97fcd57e7210c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*pHwvmVMzCx5oWNN4ImWOCQ.png"/></div></figure><p id="34c7" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">结果显示数据在新的空间中是可分的。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="efd1" class="ly lz it lu b gy ma mb l mc md">import matplotlib.pyplot as plt<br/>fig = plt.figure()<br/>ax = fig.add_subplot(1,1,1) <br/>ax.set_xlabel(‘Principal Component 1’) <br/>ax.set_ylabel(‘Principal Component 2’) <br/>ax.set_title(‘2 component PCA’) <br/>targets = [‘Iris-setosa’, ‘Iris-versicolor’, ‘Iris-virginica’]<br/>colors = [‘r’, ‘g’, ‘b’]<br/>for target, color in zip(targets,colors):<br/> indicesToKeep = x_pca[‘target’] == target<br/> ax.scatter(x_pca.loc[indicesToKeep, ‘PC1’]<br/> , x_pca.loc[indicesToKeep, ‘PC2’]<br/> , c = color<br/> , s = 50)<br/>ax.legend(targets)<br/>ax.grid()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/6357d9cc4c9d6220fcef7fa5fc6f6c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*1H8KClqGklqXaAMGp5HT4A.png"/></div></figure><p id="d6a3" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们如何使用 PCA 来检测异常值？让我给你一个直觉。变换后，“正常”数据点将沿着具有小特征值的特征向量(新轴)对齐。离群点远离特征值大的特征向量。因此，每个数据点到特征向量之间的距离成为离群值的度量。大距离表示异常。有关更多信息，请参见“使用 PyOD 的<a class="ae lm" rel="noopener" target="_blank" href="/anomaly-detection-with-pyod-b523fc47db9">异常检测</a>”。</p><div class="ms mt gp gr mu mv"><a href="https://dataman-ai.medium.com/membership" rel="noopener follow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">通过我的推荐链接加入 Medium-Chris Kuo/data man 博士</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">阅读 Chris Kuo/data man 博士的每一个故事。你的会员费直接支持郭怡广/戴塔曼博士和其他…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">dataman-ai.medium.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ko mv"/></div></div></a></div><p id="b709" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">内核 PCA (KPCA) </strong></p><p id="5c42" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">PCA 应用线性变换，这正是它的局限性。<em class="ln">内核 PCA </em>将 PCA 扩展到非线性。它首先将原始数据映射到某个非线性特征空间(通常是一个更高的维度)，然后应用 PCA 提取该空间中的主成分。这可以从图(B)中理解。左侧的图表显示，使用任何线性变换都无法分离蓝点和红点。但是如果所有的点都投影到一个 3D 空间，结果就变成线性可分了！然后我们应用主成分分析来分离这些成分。</p><p id="6768" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">直觉从何而来？为什么在更高维度的空间中，组分分离变得更容易？这还得回到 Vapnik-Chervonenkis (VC)理论。它说，映射到更高维度的空间通常会提供更大的分类能力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nk"><img src="../Images/1531c70983cd9898483d24e192023a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DqzaXJNiyRP0JTVyUkee1g.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Figure (B)</figcaption></figure><p id="8b13" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">以下 Python 代码制作了一个由红色和蓝色圆点组成的圆形图。没有办法用一条线把红点和蓝点分开(线性分离)。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="e0c9" class="ly lz it lu b gy ma mb l mc md">print(__doc__)<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.decomposition import PCA, KernelPCA<br/>from sklearn.datasets import make_circles</span><span id="1318" class="ly lz it lu b gy mp mb l mc md">np.random.seed(0)<br/>X, y = make_circles(n_samples=400, factor=.3, noise=.05)</span><span id="6253" class="ly lz it lu b gy mp mb l mc md">plt.figure(figsize=(10,10))<br/>plt.subplot(2, 2, 1, aspect='equal')<br/>plt.title("Original space")<br/>reds = y == 0<br/>blues = y == 1</span><span id="64de" class="ly lz it lu b gy mp mb l mc md">plt.scatter(X[reds, 0], X[reds, 1], c="red",s=20, edgecolor='k')<br/>plt.scatter(X[blues, 0], X[blues, 1], c="blue",s=20, edgecolor='k')<br/>plt.xlabel("$x_1$")<br/>plt.ylabel("$x_2$")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/86305288164edcbb2f68f242de5a7cea.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*9ohZfj3sdcxxqM4DgJKBeA.png"/></div></figure><p id="1018" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">然而，当我们将圆投影到一个更高维的空间并使用 PCA 分离时，针对第一和第二主成分的数据观察是可分离的！下面是点相对于第一和第二主成分绘制的结果。我画了一条线来区分红色和蓝色的点。在 KernelPCA 中，我们指定 kernel='rbf '，这是<a class="ae lm" href="https://en.wikipedia.org/wiki/Radial_basis_function" rel="noopener ugc nofollow" target="_blank">径向基函数</a>，或者欧几里德距离。RBF 通常用作机器学习技术的核心，例如<a class="ae lm" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank">支持向量机(SVM) </a>。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="dbf1" class="ly lz it lu b gy ma mb l mc md">kpca = KernelPCA(kernel=”rbf”, fit_inverse_transform=True, gamma=10)<br/>X_kpca = kpca.fit_transform(X)<br/>pca = PCA()<br/>X_pca = pca.fit_transform(X)</span><span id="3249" class="ly lz it lu b gy mp mb l mc md">plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=”red”,s=20, edgecolor=’k’)<br/>plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=”blue”,s=20, edgecolor=’k’)<br/>x = np.linspace(-1, 1, 1000)<br/>plt.plot(x, -0.1*x, linestyle=’solid’)<br/>plt.title(“Projection by KPCA”)<br/>plt.xlabel(r”1st principal component in space induced by $\phi$”)<br/>plt.ylabel(“2nd component”)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/02ac621ac70de99492f9387ae908d675.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*95gMmBiWU8CFV12hiDQyJA.png"/></div></figure><p id="2507" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果我们把核指定为“线性的”，如下面的代码(KernelPCA(kernel='linear ')，就变成了只有线性变换的标准 PCA，红蓝点不可分。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="2139" class="ly lz it lu b gy ma mb l mc md">kpca = KernelPCA(kernel=”linear”, fit_inverse_transform=True, gamma=10)<br/>X_kpca = kpca.fit_transform(X)<br/>pca = PCA()<br/>X_pca = pca.fit_transform(X)</span><span id="2662" class="ly lz it lu b gy mp mb l mc md">plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=”red”,s=20, edgecolor=’k’)<br/>plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=”blue”,s=20, edgecolor=’k’)<br/>x = np.linspace(-1, 1, 1000)<br/>plt.plot(x, -0.1*x, linestyle=’solid’)<br/>plt.title(“Projection by KPCA”)<br/>plt.xlabel(r”1st principal component in space induced by $\phi$”)<br/>plt.ylabel(“2nd component”)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/eef5fcb92eba9fcc5fc9213009ccb5e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*jf1MI2UskFKLzQ34CJjnHw.png"/></div></figure><p id="db27" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">线性判别分析</strong></p><p id="3011" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">LDA 的起源不同于 PCA。PCA 是一种无监督的学习方法，它将原始特征转换成一组新的特征。我们不关心新的特征集是否能为目标变量提供最好的区分能力。相比之下，线性判别分析(LDA)寻求尽可能多地保留因变量的判别能力，同时将原始数据矩阵投影到低维空间。LDA 是一种监督学习技术。它利用因变量中的类别将预测器空间划分为<em class="ln">个区域</em>。所有的<em class="ln">区域</em>都应该有<em class="ln">线性</em>边界。线性的名称由此而来。该模型预测一个区域内的所有观测值都属于同一类因变量。</p><p id="2c2d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">LDA 通过三个主要步骤实现了上述目标。首先，它计算因变量的不同类之间的可分性，称为<em class="ln">类间方差</em>，如图 LDA 的(1)所示。其次，计算每一类的均值与样本之间的距离，称为<em class="ln">类内方差，</em>如(2)所示。然后以最大化类间方差和最小化类内方差为准则构造低维空间。这个标准的解决方案是计算特征值和特征向量。得到的特征向量代表新空间的方向，相应的特征值代表特征向量的长度。因此，每个特征向量代表 LDA 空间的一个轴，特征值代表该特征向量的长度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/59793a4848ec6426c12369f53ab44d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*W1xnDANqnLkjRUg9r0Zm8w.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Figure: LDA</figcaption></figure><p id="9e0c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">直觉是这样的:如果两个阶级能真正分开，那么两个阶级尽可能远，同时每个阶级尽可能同质，这将是理想的。这种直觉可以用三个步骤来表达:</p><ul class=""><li id="72ce" class="np nq it ks b kt ku kw kx kz nr ld ns lh nt ll nu nv nw nx bi translated">第一步:找到不同阶层之间的分野。这也被称为类间方差。是不同阶级手段之间的距离。见上图(1)。</li><li id="7397" class="np nq it ks b kt ny kw nz kz oa ld ob lh oc ll nu nv nw nx bi translated">第二步:求类内方差。这是每个类的平均值和样本之间的距离。见上图(2)。</li><li id="7996" class="np nq it ks b kt ny kw nz kz oa ld ob lh oc ll nu nv nw nx bi translated">第三步:在低维空间中最大化第一步(类间方差)，最小化第二步(类内方差)。这也被称为费雪准则。</li></ul><p id="4ac3" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我将在 Kaggle 比赛中使用“<a class="ae lm" href="https://www.kaggle.com/piyushgoyal443/red-wine-dataset#wineQualityInfo.txt" rel="noopener ugc nofollow" target="_blank">红酒质量</a>”数据集。该数据集有 11 个输入变量和一个输出变量“质量”。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="3549" class="ly lz it lu b gy ma mb l mc md">import matplotlib.pyplot as plt<br/>from sklearn.decomposition import PCA<br/>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/>wine = pd.read_csv(‘winequality-red.csv’)<br/>wine.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi od"><img src="../Images/9f34cac92bd5e7fac67f8d765132ca00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFVu1n3dzQ8znn9IwSL-qg.png"/></div></div></figure><p id="bda5" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了简单起见，我将输出变量重新分组为三个值。<code class="fe mm mn mo lu b">wine[‘quality2’] = np.where(wine[‘quality’]&lt;=4,1, np.where(wine[‘quality’]&lt;=6,2,3)).</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/e5fbae9bdab8469bb3aa33e75c975ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*IMzZOFQE1J8H9jFxuTsJ6w.png"/></div></figure><p id="066f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">下面的代码执行 PCA 和 LDA。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="5db4" class="ly lz it lu b gy ma mb l mc md">X = wine.drop(columns=[‘quality’,’quality2'])<br/>y = wine[‘quality2’]<br/>target_names = np.unique(y)<br/>target_names</span><span id="a069" class="ly lz it lu b gy mp mb l mc md">pca = PCA(n_components=2)<br/>X_r = pca.fit(X).transform(X)</span><span id="b364" class="ly lz it lu b gy mp mb l mc md">lda = LinearDiscriminantAnalysis(n_components=2)<br/>X_r2 = lda.fit(X, y).transform(X)</span></pre><p id="c006" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">然后绘制 PCA 和 LDA 的结果:</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="0802" class="ly lz it lu b gy ma mb l mc md"># Percentage of variance explained for each components<br/>print(‘explained variance ratio (first two components): %s’<br/> % str(pca.explained_variance_ratio_))</span><span id="5a0b" class="ly lz it lu b gy mp mb l mc md">plt.figure()<br/>colors = [‘navy’, ‘turquoise’, ‘darkorange’]<br/>lw = 2</span><span id="bca1" class="ly lz it lu b gy mp mb l mc md">for color, i, target_name in zip(colors, target_names, target_names):<br/> plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,<br/> label=target_name)<br/>plt.legend(loc=’best’, shadow=False, scatterpoints=1)<br/>plt.title(‘PCA of WINE dataset’)</span><span id="e600" class="ly lz it lu b gy mp mb l mc md">plt.figure()<br/>for color, i, target_name in zip(colors, target_names, target_names):<br/> plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,<br/> label=target_name)<br/>plt.legend(loc=’best’, shadow=False, scatterpoints=1)<br/>plt.title(‘LDA of WINE dataset’)</span><span id="eaa1" class="ly lz it lu b gy mp mb l mc md">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/b55bea89bcbaa422902acb8f66d0cd02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*nMd65CFiRCHvuyGHIdzZmA.png"/></div></figure><p id="903b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">LDA 适用于多类分类问题。如果你甚至需要对一个多类问题建模，请看我的文章“<a class="ae lm" href="https://medium.com/dataman-in-ai/a-wide-choice-for-modeling-multi-class-classifications-d97073ff4ec8" rel="noopener">多类分类的多种模型</a>”。</p><p id="6c3a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">奇异值分解</strong></p><p id="c896" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">SVD 是一种类似于 PCA 的数据汇总方法。它从数据中提取重要特征。但是 SVD 还有一个优点:将原始数据集重构为一个小数据集。奇异值分解在图像压缩中有着广泛的应用。例如，如果您有一个 32*32 = 1，024 像素的图像，SVD 可以将其总结为 66 像素。66 个像素可以代表 32*32 像素的图像，而不会丢失任何重要信息。奇异值分解是线性代数的基础，但它似乎“并不像它应该的那样有名”。这个伟大的评论是在经典教科书“线性代数及其应用”由吉尔伯特斯特朗。</p><p id="e462" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了正确地介绍奇异值分解，让我们从矩阵运算开始。如果 A 是对称实矩阵<em class="ln"> n × n </em>，则存在正交矩阵<em class="ln"> V </em>和对角矩阵<em class="ln"> D </em>使得</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/bfa38d53ba7fb4eb7a8759c1d20ff9ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/format:webp/1*rdEtBbHkOk4XF9dPoGQX3w.png"/></div></figure><p id="a347" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">列<em class="ln"> V </em>是 A 的特征向量，<em class="ln"> D </em>的对角元素是 A 的特征值，这个过程被称为矩阵<em class="ln"> A </em>的<em class="ln">特征值分解</em>或<em class="ln"> EVD </em>。它告诉我们如何选择<em class="ln">标准正交</em>基，使变换用一个尽可能简单的矩阵表示，即对角矩阵。(对于想浏览对角化矩阵步骤的读者来说，<a class="ae lm" href="https://yutsumura.com/how-to-diagonalize-a-matrix-step-by-step-explanation/" rel="noopener ugc nofollow" target="_blank">这里的</a>是一个很好的例子。)术语<strong class="ks iu">正交的</strong>意味着两个向量是垂直的。</p><p id="fabb" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">扩展对称矩阵，SVD 可以处理任何实矩阵<em class="ln">m×n</em>A<em class="ln">T30】。给定一个实 m <em class="ln"> × n </em>矩阵<em class="ln"> A </em>，存在一个正交<em class="ln"> m × m </em>矩阵<em class="ln"> U，</em>一个正交矩阵<em class="ln"> m × m V，</em>和一个对角<em class="ln"> m × n </em>矩阵<em class="ln">σ</em>使得</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/c12cd744ca329752cb03e01ecb2c674c.png" data-original-src="https://miro.medium.com/v2/resize:fit:226/format:webp/1*eLi5Mt2ewpvLxEcC3FzXlQ.png"/></div></figure><p id="34de" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">注意，正交矩阵是方阵，使得其自身与其逆矩阵的乘积是单位矩阵。对角矩阵是指对角线以外的元素都为零的矩阵。</p><p id="f9c0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">下面我将再次使用 iris 数据集向您展示如何应用奇异值分解。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="39f7" class="ly lz it lu b gy ma mb l mc md">from numpy import *<br/>import operator<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>from numpy.linalg import *</span><span id="9e92" class="ly lz it lu b gy mp mb l mc md">url = “<a class="ae lm" href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a>"<br/># load dataset into Pandas DataFrame<br/>df = pd.read_csv(url, names=[‘sepal length’,’sepal width’,’petal length’,’petal width’,’target’])</span><span id="5076" class="ly lz it lu b gy mp mb l mc md"># Only the X variables<br/>data = df[[‘sepal length’,’sepal width’,’petal length’,’petal width’]]</span><span id="11ef" class="ly lz it lu b gy mp mb l mc md">#calculate SVD<br/>n = 2 # We will take two Singular Values<br/>U, s, V = linalg.svd( data )</span><span id="7294" class="ly lz it lu b gy mp mb l mc md"># eye() creates a matrix with ones on the diagonal and zeros elsewhere<br/>Sig = mat(eye(n)*s[:n])<br/>newdata = U[:,:n]<br/>newdata = pd.DataFrame(newdata)<br/>newdata.columns=[‘SVD1’,’SVD2']<br/>newdata.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/2e706c6ab20c31bcb9d386c4284c5f2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*r4_EB62AwuXQkF52Bd36gA.png"/></div></figure><p id="7dc6" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">你可以比较奇异值分解和主成分分析的结果。两者的结果相似。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="bd72" class="ly lz it lu b gy ma mb l mc md"># Add the actual target to the data in order to plot it<br/>newdata[‘target’]=df[‘target’]</span><span id="9201" class="ly lz it lu b gy mp mb l mc md">fig = plt.figure()<br/>ax = fig.add_subplot(1,1,1) <br/>ax.set_xlabel(‘SVD 1’) <br/>ax.set_ylabel(‘SVD 2’) <br/>ax.set_title(‘SVD’) <br/>targets = [‘Iris-setosa’, ‘Iris-versicolor’, ‘Iris-virginica’]<br/>colors = [‘r’, ‘g’, ‘b’]<br/>for target, color in zip(targets,colors):<br/> indicesToKeep = newdata[‘target’] == target<br/> ax.scatter(newdata.loc[indicesToKeep, ‘SVD1’]<br/> , newdata.loc[indicesToKeep, ‘SVD2’]<br/> , c = color<br/> , s = 50)<br/>ax.legend(targets)<br/>ax.grid()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/a3b71b5a3d3014b86897ca8fef064e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*LHcoeSP9ZQhpgwRF-K4R0w.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Figure: SVD</figcaption></figure><p id="ac72" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"><em class="ln">t</em>-分布式随机邻居嵌入(t-SNE) </strong></p><p id="30f2" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><em class="ln">t</em>——SNE 由<a class="ae lm" href="http://www.cs.toronto.edu/~hinton/absps/tsne.pdf" rel="noopener ugc nofollow" target="_blank">劳伦斯·范·德·马滕和乔治·辛顿</a>开发。它是一种可视化的机器学习算法，将高维数据嵌入到二维或三维的低维空间中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/523c86ffaefc2c0f173d7d3608e6141b.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*0MgoGw_4-lw5Wj85-Ql48Q.png"/></div></figure><p id="fed2" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">把上面的三维瑞士卷呈现为二维的最好方法是什么？直觉上，我们想把瑞士面包卷“展开”成一块扁平的蛋糕。在数学中，这意味着相似的点将成为附近的点，不同的点将成为遥远的点。</p><p id="a0da" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">图(C)显示了另一个例子。它是一个三维四面体，数据点聚集在顶点角上。如果我们只是像面板(A)那样将 3 维图形折叠成 2 维图形，效果并不好，因为组(A)变成了中心聚类。相比之下，画面(B)可能是更好的 2d 展示，其保留了聚类(A)-(E)之间的远距离，同时保持了每个聚类中的点的局部距离。SNE，一种非线性降维技术，旨在保持局部邻域。如果 t-SNE 图上的一组点聚集在一起，我们可以相当确定这些点彼此靠近。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/c2d85d64bb2aa33b96cf759f36fe6aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*s9oDAdzGHD20C034aOuP7w.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Figure (C): t-SNE</figcaption></figure><p id="8234" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">SNE 为点之间的相似性建模。它是如何定义相似性的？首先，它由点<em class="ln"> Xi </em>和<em class="ln"> Xj </em>之间的欧几里德距离定义。第二，它被定义为“数据点<em class="ln"> i </em>到点<em class="ln"> j </em>的相似度是条件概率<em class="ln"> p </em>如果在高斯分布下根据其概率挑选其他邻居，则点<em class="ln"> i </em>将挑选数据<em class="ln"> j </em>作为其邻居。”在下面的条件表达式中，如果点<em class="ln"> j </em>比其他点更靠近点<em class="ln"> i </em>，那么它被选中的概率更大(注意负号)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/333568afbee2bc505858f527955de922.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*7-Vz3Qh3wdnVtkRR799cXw.png"/></div></figure><p id="9777" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">t-SNE 的目的是通过点<em class="ln"> Yi </em>和点<em class="ln"> Yj，</em>之间的低维空间<em class="ln"> q </em>尽可能匹配上述条件概率<em class="ln"> p </em>，如下图所示。概率<em class="ln"> q </em>遵循厚尾 Student-t 分布，因此 t-SNE 中的“<em class="ln"> t </em>”由此而来。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/3bd5ca26916dae9a8a1bd08df07afba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*ywDzSjc3gaC7ihlDxHKW_Q.png"/></div></figure><p id="6133" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">下一步是找到<em class="ln"> Yi </em>，使分布<em class="ln"> q </em>尽可能接近分布<em class="ln"> p </em>。t-SNE 使用梯度下降技术，一种优化技术，来找到这些值。</p><p id="67bc" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">下面我演示了 t-SNE 技术是如何用于虹膜数据集的。</p><pre class="kj kk kl km gt lt lu lv lw aw lx bi"><span id="8589" class="ly lz it lu b gy ma mb l mc md">from<!-- --> <!-- -->sklearn.manifold import<!-- --> <!-- -->TSNE<br/>from<!-- --> <!-- -->sklearn.datasets import<!-- --> <!-- -->load_iris<br/>from<!-- --> <!-- -->sklearn.decomposition import<!-- --> <!-- -->PCA<br/>import<!-- --> <!-- -->matplotlib.pyplot as plt<br/>iris =<!-- --> <!-- -->load_iris()<br/>X_tsne =<!-- --> <!-- -->TSNE(learning_rate=100).fit_transform(iris.data)<br/>X_pca =<!-- --> <!-- -->PCA().fit_transform(iris.data)<br/>plt.figure(figsize=(10, 5))<br/>plt.subplot(121)<br/>plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=iris.target)<br/>plt.subplot(122)<br/>plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/d1dcc6bf36cfdb21e2654dbb03b3a954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*7FZOWHdYI5L_hThheKlJ7Q.png"/></div></figure><p id="044c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">更多信息，这篇文章“<a class="ae lm" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank">如何更有效地使用 t-SNE</a>”提供了更多的讨论。</p><div class="ms mt gp gr mu mv"><a href="https://dataman-ai.medium.com/membership" rel="noopener follow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">通过我的推荐链接加入 Medium-Chris Kuo/data man 博士</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">阅读 Chris Kuo/data man 博士的每一个故事。你的会员费直接支持郭怡广/戴塔曼博士和其他…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">dataman-ai.medium.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ko mv"/></div></div></a></div></div></div>    
</body>
</html>