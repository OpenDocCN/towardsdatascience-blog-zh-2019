<html>
<head>
<title>Proper Balancing for Cross Validation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交叉验证的适当平衡</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/proper-balancing-for-cross-validation-d95c17ff0ab4?source=collection_archive---------17-----------------------#2019-10-11">https://towardsdatascience.com/proper-balancing-for-cross-validation-d95c17ff0ab4?source=collection_archive---------17-----------------------#2019-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/c667dbf3bc1fecf99f4cda993ba47bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zUIylcXGRo01Lr5pejldVA.jpeg"/></div></div></figure><p id="cdf3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当手头的数据集在每个目标类值的实例数量方面不平衡时，谁没有遇到过应用交叉验证技术的需要呢？</p><p id="6d1a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> <em class="kz">这里的问题是，我们是否恰当地运用了它？</em> </strong></p><p id="c453" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi la translated">这篇文章的目的是展示一种在交叉验证中使用平衡方法的方法，而不是在 CV 测试折叠上强制平衡；从而得到更真实的 CV 评估结果。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><p id="f9cd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个<strong class="kd iu">通常</strong> <strong class="kd iu">将</strong>他的数据分割成训练&amp;测试(&amp;保持)子集，<br/>平衡训练子集，在其上拟合他的模型，并得到不平衡测试&amp;保持集的结果。</p><p id="70b3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">或者，我们使用<strong class="kd iu">交叉验证</strong>，最有可能使用分层折叠，<br/>以便每个折叠保留每个类别的样本百分比。</p><p id="9dd3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，像 sickit-learn 的<code class="fe lq lr ls lt b">cross_validate</code> <br/>这样的预置函数默认选择不处理类不平衡。</p><p id="a64f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">在本教程中，我们将首先介绍一些数据平衡的组合应用&amp;交叉验证(CV)。然后，我们将检查一种在 CV 测试折叠上产生评估度量的方法，通过为测试折叠保持维持集(未知测试集)具有的相同的类分布。</strong></p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><p id="d952" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们用一个简单的<strong class="kd iu"> python 例子</strong>来检验这个思路。<br/>我在这里使用这个<a class="ae lu" href="https://www.kaggle.com/mlg-ulb/creditcardfraud/data" rel="noopener ugc nofollow" target="_blank">信用卡欺诈检测</a> Kaggle 数据集，<br/>遵循以下步骤。</p><ul class=""><li id="0ca7" class="lv lw it kd b ke kf ki kj km lx kq ly ku lz ky ma mb mc md bi translated">我们会保留整个&amp;工作的一个<strong class="kd iu">样本</strong>。</li><li id="ee66" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">然后<strong class="kd iu">将</strong>我们的数据拆分成训练&amp;测试集数据。</li><li id="f977" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">我们将把这个测试集视为<strong class="kd iu">维持</strong>/未知集。</li><li id="b115" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">并且将仅对训练集应用交叉验证。</li><li id="97b6" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">我们将在交叉验证期间，在&amp;之前，在&amp; oversampling、<br/> &amp;平衡下一起试用。</li><li id="7533" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">由于最合适的信用欺诈指标是<strong class="kd iu">精度</strong>，<br/>，我们将比较 CV 测试折叠的平均精度<br/>和维持集的模型精度。</li></ul><p id="6d24" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> <em class="kz">抬头:</em> </strong> <em class="kz">相关代码在文末。</em></p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><h2 id="c14a" class="mj mk it bd ml mm mn dn mo mp mq dp mr km ms mt mu kq mv mw mx ku my mz na nb bi translated"><strong class="ak">第 1 部分:无平衡</strong></h2><p id="774f" class="pw-post-body-paragraph kb kc it kd b ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku ng kw kx ky im bi translated">这里我们绘制了完全没有平衡的精确结果:</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/e465c7f1253163c7376a4decdc945c93.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*4T3vpTkePsQn76TKiz7mGA.png"/></div></figure><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="8ce5" class="mj mk it lt b gy nq nr l ns nt">Average Train Precision among C-V folds: 76.8 %<br/>Average Test Precision among C-V folds: 71.98 %<br/>Single Test set precision: 75.86 %<br/>Single Test set Low Class(No-Fraud) Precision: 99.9 %<br/>Single Test set High Class(Fraud) Precision: 75.9 %</span></pre><ul class=""><li id="589c" class="lv lw it kd b ke kf ki kj km lx kq ly ku lz ky ma mb mc md bi translated">CV 褶皱的精确测量似乎很不稳定。</li><li id="dd5c" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">一些精度度量高于它们在训练集上的相应精度。</li><li id="9a69" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">CV 折叠的测试集精度之间存在很大差异。</li><li id="956f" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">CV 的平均测试集精度与未知的欺诈数据集相差不是很远，但差异仍然很重要。</li><li id="ed88" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">因此，我们将检查数据集的平衡。</li></ul><h2 id="0163" class="mj mk it bd ml mm mn dn mo mp mq dp mr km ms mt mu kq mv mw mx ku my mz na nb bi translated">第 2 部分:C-V 外部平衡(欠采样)</h2><p id="41b0" class="pw-post-body-paragraph kb kc it kd b ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku ng kw kx ky im bi translated">这里，我们绘制了在对其应用 CV 之前，在欠采样情况下，仅对训练子集进行平衡的精确结果:</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/5e7ea46830e303c8a9bebed08eb2a637.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*hsyr1sBbo0QGskZtenQUzg.png"/></div></figure><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="0b61" class="mj mk it lt b gy nq nr l ns nt">Average Train Precision among C-V folds: 99.81 %<br/>Average Test Precision among C-V folds: 95.24 %<br/>Single Test set precision: 3.38 %<br/>Single Test set Low Class(No-Fraud) Precision: 100.0 %<br/>Single Test set High Class(Fraud) Precision: 3.40 %</span></pre><ul class=""><li id="b33c" class="lv lw it kd b ke kf ki kj km lx kq ly ku lz ky ma mb mc md bi translated">像以前一样:<br/> - CV 成绩看起来大多很高，但是不稳定。<br/> -一些精度指标高于其在列车组上的相应精度。<br/>-CV 褶皱的测试集精度出现较大差异。</li><li id="42a2" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">此外，对上述一些高数值的任何正面解释都可能是错误的，因为“未知”不平衡检验给出了很差的精度。</li><li id="03a3" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">上述差异源于以下事实:<br/> -每个褶皱的测试集是平衡的，<br/> -每个褶皱的模型是在平衡的列车褶皱数据上拟合的。</li><li id="95b2" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">因此，该模型在 CV 中运行良好，因为它是在相似的类分布(平衡的)上训练和测试的。</li><li id="95e5" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">然而,“未知”欺诈数据集不会平衡，因此我们在测试集上看到非常低的精度。</li></ul><h2 id="698c" class="mj mk it bd ml mm mn dn mo mp mq dp mr km ms mt mu kq mv mw mx ku my mz na nb bi translated">第 3 部分:C-V 内部平衡(欠采样)</h2><p id="6b3c" class="pw-post-body-paragraph kb kc it kd b ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku ng kw kx ky im bi translated">在这里，我们绘制了平衡的精确结果，在欠采样的情况下，<br/>在拟合模型之前，只对每个 CV 折叠的训练集进行采样，<br/>对 CV 折叠的测试集进行预测:</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/53dadcaadde27f60df239d71f747b7bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*fRIpIn8YGOVrbYvIlUVoGQ.png"/></div></figure><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="91b5" class="mj mk it lt b gy nq nr l ns nt">Average Train Precision among C-V folds: 99.21 %<br/>Average Test Precision among C-V folds: 4.2 %<br/>Single Test set precision: 3.38 %<br/>Single Test set Low Class(No-Fraud) Precision: 100.0 %<br/>Single Test set High Class(Fraud) Precision: 3.40 %</span></pre><ul class=""><li id="2c1c" class="lv lw it kd b ke kf ki kj km lx kq ly ku lz ky ma mb mc md bi translated">我们在 CV 折叠中看到稳定的精度结果，但是不够有效。</li><li id="1609" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">我们看到未知数据的精度非常接近 CV 折叠的平均精度。</li><li id="c3c2" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">我们在这里所做的是，我们让每个 fold 的模型识别欺诈交易的存在，并尝试在未知测试(fold)数据的现实、不平衡版本上进行预测。</li><li id="94e6" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">与不平衡版本的大约 76%相比，精度仍然很差。</li><li id="7efe" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">这是否意味着我们放弃了平衡？</li><li id="9eb8" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">下一步可能是检查不同的平衡比率。</li><li id="c84b" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">但首先让我们检查过采样而不是欠采样，因为我们的欺诈交易太少。</li></ul><h2 id="8784" class="mj mk it bd ml mm mn dn mo mp mq dp mr km ms mt mu kq mv mw mx ku my mz na nb bi translated">第 4 部分:利用过采样平衡外部 C-V</h2><p id="148d" class="pw-post-body-paragraph kb kc it kd b ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku ng kw kx ky im bi translated">在这里，我们绘制了平衡的精确结果，在对其应用 CV 之前，<br/>只对训练子集进行过采样:</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/3371566224ee6160053de6a43419f84b.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*L3qTao_hR6Ame4JDiqCnmg.png"/></div></figure><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="4ed2" class="mj mk it lt b gy nq nr l ns nt">Average Train Precision among C-V folds: 98.51 %<br/>Average Test Precision among C-V folds: 98.52 %<br/>Single Test set precision: 12.61 %<br/>Single Test set Low Class(No-Fraud) Precision: 100.0 %<br/>Single Test set High Class(Fraud) Precision: 12.6 %</span></pre><ul class=""><li id="1dbe" class="lv lw it kd b ke kf ki kj km lx kq ly ku lz ky ma mb mc md bi translated">由于过采样，结果比第 2 部分更稳定；<br/>欠采样留下的实例太少。</li><li id="50f3" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">然而，简历分数并不代表未知测试集的真实情况。</li></ul><h2 id="850b" class="mj mk it bd ml mm mn dn mo mp mq dp mr km ms mt mu kq mv mw mx ku my mz na nb bi translated">第 5 节:C-V 内部的平衡(过采样)</h2><p id="ed25" class="pw-post-body-paragraph kb kc it kd b ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku ng kw kx ky im bi translated">在这里，我们绘制了平衡的精确结果，在对每个 CV 褶皱拟合模型并对 CV 褶皱的测试集进行预测之前，仅使用过采样<br/>每个 CV 褶皱的训练集:</p><figure class="ni nj nk nl gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d12e0d3c38f0805a84b5931860b39361.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*bZ5nYX9z9WX4wT_xbNNMQw.png"/></div></figure><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="659d" class="mj mk it lt b gy nq nr l ns nt">Average Train Precision among C-V folds: 98.38 %<br/>Average Test Precision among C-V folds: 8.7 %<br/>Single Test set precision: 12.61 %<br/>Single Test set Low Class(No-Fraud) Precision: 100.0 %<br/>Single Test set High Class(Fraud) Precision: 12.6 %</span></pre><ul class=""><li id="a7ba" class="lv lw it kd b ke kf ki kj km lx kq ly ku lz ky ma mb mc md bi translated">由于过采样，精度分数比第 3 部分的分数高。</li><li id="3a51" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">尽管如此，我们得到的每个 CV 倍预测分数结果足够接近模型在未知测试数据集上产生的结果。</li></ul><p id="8c60" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">很明显，到目前为止，平衡并没有帮助获得好的测试结果。<br/>然而，这超出了本文的范围(:-)，本文的目标已经实现:<br/>让模型在每个 CV 文件夹的测试集上产生类似于在未知数据集上产生的评估度量分数，<br/>在训练数据平衡的情况下。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><p id="4c07" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后，我不推荐严格意义上的改编，<br/>但是你可以从上面保留一个建议&amp;供你思考！</p><p id="495d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> <em class="kz">感谢阅读本文！我对任何意见或建议都感兴趣。</em> </strong></p><p id="a151" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> PS: </strong>下面跟着相关的代码，这样你就可以在上面做实验了。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><p id="0f92" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">数据的导入和拆分:</p><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="8f7d" class="mj mk it lt b gy nq nr l ns nt">import pandas as pd<br/>import numpy as np<br/>from sklearn import datasets<br/>from sklearn.model_selection import cross_validate<br/>from sklearn.metrics import accuracy_score, precision_score<br/>from sklearn.linear_model import LogisticRegression<br/>import matplotlib.pyplot as plt<br/>from sklearn.model_selection import train_test_split,StratifiedKFold<br/>from imblearn.over_sampling import SMOTE<br/>from imblearn.under_sampling import RandomUnderSampler<br/><br/>df = pd.read_csv('creditcard.csv').sample(50000, random_state=0)<br/>train, test = train_test_split(df, test_size=0.3, random_state=0, shuffle=True)<br/>y_train = np.array(train["Class"])<br/>y_test = np.array(test["Class"])<br/>del train["Class"]<br/>del test["Class"]<br/>train = train.reset_index(drop=True)<br/>test = test.reset_index(drop=True)</span></pre><p id="a054" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此时每个类的实例数是:</p><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="0bc8" class="mj mk it lt b gy nq nr l ns nt">Train Set Class 0:  34941<br/>Train Set Class 1:  59<br/><br/>Test Set Class 0:  14967<br/>Test Set Class 1:  33</span></pre><p id="4e92" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们将需要一些<strong class="kd iu">助手</strong>功能。</p><p id="1fdc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个绘制精度分数:</p><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="9b0d" class="mj mk it lt b gy nq nr l ns nt">def <strong class="lt iu">plotit</strong>(tr,te):<br/>    fig, ax = plt.subplots()<br/>    r = range(1,11)<br/>    ax.scatter(r, tr,label='cv train scores')<br/>    ax.plot(r, tr, 'C3', lw=1)<br/>    ax.scatter(r, te,label='cv test scores')<br/>    ax.plot(r, te, 'C2', lw=1)<br/>    ax.set_xlabel('CV Folds')<br/>    ax.set_ylabel('Precision')<br/>    ax.legend()<br/>    plt.show()</span></pre><p id="eee1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个用于每个类的值精度:</p><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="7eff" class="mj mk it lt b gy nq nr l ns nt">def <strong class="lt iu">class_precisions</strong>(yt,yp):<br/>    df = pd.DataFrame(np.transpose([yp,yt]),<br/>    columns=('test_actual','test_preds'))<br/>    mask1 = (df['test_actual']==0)<br/>    mask2 = (df['test_actual']==df['test_preds'])<br/>    low_CORRECT = df[mask1 &amp; mask2].count()[0]<br/>    low_NOFRAUD = df[mask1].count()[0]<br/>    print("Single Test set Low Class(No-Fraud) Precision:", <br/>    np.round(low_CORRECT/low_NOFRAUD,3)*100,"%")<br/>    high_class_df = pd.DataFrame(np.transpose([yp,yt]),<br/>    columns=('test_actual','test_preds'))<br/>    mask1 = (df['test_actual']==1)<br/>    mask2 = (df['test_actual']==df['test_preds'])<br/>    high_CORRECT = df[mask1 &amp; mask2].count()[0]<br/>    high_FRAUD = df[mask1].count()[0]<br/>    print("Single Test set High Class(Fraud) Precision:", <br/>    np.round(high_CORRECT/high_FRAUD,3)*100,"%")</span></pre><p id="d901" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个汇集了整个流程的系统:</p><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="9861" class="mj mk it lt b gy nq nr l ns nt">def <strong class="lt iu">compare</strong>(x_train,y,x_test,yt,custom=False,method='over'):<br/>    if custom:<br/>        tr,te = custom_balancing_cv(x_train, y,    <br/>                cv=10,method=method)<br/>    else:<br/>        results = cross_validate( LogisticRegression(              <br/>                  random_state=0, solver='liblinear',   <br/>                  max_iter=10000), x_train, y, cv=10,<br/>                  return_train_score=True,scoring=['precision'])<br/>        tr = results['train_precision']<br/>        te = results['test_precision']<br/>    plotit(tr,te)<br/>    clf = LogisticRegression(random_state=0,solver='liblinear',                     <br/>                             max_iter=10000)<br/>    # If we customly balance train set of folds,<br/>    # we need to compare with a balanced single prediction<br/>    if custom:<br/>        if method=='under':<br/>            s = RandomUnderSampler(random_state=0)<br/>        else:<br/>            s = SMOTE(random_state=0)<br/>        x_train, y = s.fit_sample(x_train, y)<br/>    clf.fit(x_train,y)<br/>    y_pred = clf.predict(x_test)<br/>    print("Average Train Precision among C-V            <br/>          folds:",np.round(np.average(tr)*100,2),"%")<br/>    print("Average Test Precision among C-V <br/>          folds:",np.round(np.average(te)*100,2),"%")<br/>    print("Single Test set   precision:", <br/>          np.round(precision_score(yt, y_pred)*100,2),"%")<br/>    class_precisions(yt,y_pred)</span></pre><p id="29c6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第 1 部分</strong>:无平衡</p><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="894c" class="mj mk it lt b gy nq nr l ns nt">compare(train, y_train,test,y_test)</span></pre><p id="cb09" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第 2 节</strong>:C-V 外平衡(欠采样)</p><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="bad2" class="mj mk it lt b gy nq nr l ns nt">rus = RandomUnderSampler(random_state=0)<br/>X_res, y_res = rus.fit_sample(train, y_train)<br/>compare(X_res, y_res,test,y_test)</span></pre><p id="55bf" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第 3 节</strong>:C-V 内部平衡(欠采样)</p><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="35c0" class="mj mk it lt b gy nq nr l ns nt">compare(train, y_train,test,y_test,custom=True, method='under')</span></pre><p id="78b8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第 4 节</strong>:C-V 外部平衡(过采样)</p><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="7b2c" class="mj mk it lt b gy nq nr l ns nt">sm = SMOTE(random_state=0)<br/>X_res, y_res = sm.fit_sample(train, y_train)<br/>compare(X_res, y_res,test,y_test)</span></pre><p id="c122" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第 5 节</strong>:C-V 内部平衡(过采样)</p><pre class="ni nj nk nl gt nm lt nn no aw np bi"><span id="b7db" class="mj mk it lt b gy nq nr l ns nt">compare(train, y_train,test,y_test,custom=True, method='over')</span></pre><p id="d6e3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">再次感谢 BB 看完这个！</strong></p></div></div>    
</body>
</html>