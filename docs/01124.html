<html>
<head>
<title>Understanding Logistic Regression step by step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逐步理解逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-logistic-regression-step-by-step-704a78be7e0a?source=collection_archive---------4-----------------------#2019-02-21">https://towardsdatascience.com/understanding-logistic-regression-step-by-step-704a78be7e0a?source=collection_archive---------4-----------------------#2019-02-21</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""/><div class=""><h2 id="7dbe" class="pw-subtitle-paragraph jo iq ir bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">训练逻辑回归分类器，根据人们的体重和身高预测他们的性别。</h2></div><div class="kg kh ki kj gu ab cb"><figure class="kk kl km kn ko kp kq paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/6402612d2b4074d2e61195e2711bb8aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*sR6PgnEB5HmuRuz62DfWzQ.png"/></div></figure><figure class="kk kl kx kn ko kp kq paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/b91a8063c764aba52ee9b9f39510deb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*ArM2nhi7ZQNgdegWAWgG4Q.png"/></div></figure></div><p id="484c" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">逻辑回归是一种流行的用于二元分类的统计模型，即用于类型<em class="lu">这个或那个</em>、<em class="lu">是或否</em>、<em class="lu"> A 或 B </em>等的预测。然而，逻辑回归可用于多类分类，但这里我们将集中讨论其最简单的应用。</p><p id="5c61" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">作为一个例子，考虑<strong class="la is">根据某人的<strong class="la is">体重</strong>和<strong class="la is">身高</strong>预测其性别</strong>(男/女)的任务。</p><p id="944c" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">为此，我们将<em class="lu">从一万个人的体重和身高样本的数据集训练</em>一个机器学习模型。数据集取自<a class="ae lv" href="https://amzn.to/2Xjkuqa" rel="noopener ugc nofollow" target="_blank">Conway&amp;Myles Machine Learning for Hackers 一书第二章</a>，可以直接下载<a class="ae lv" href="https://raw.githubusercontent.com/johnmyleswhite/ML_for_Hackers/master/02-Exploration/data/01_heights_weights_genders.csv" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="e800" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">这是数据的预览:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj lw"><img src="../Images/8fbe9eed1ba72a90642d8015534dc692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k76HeldKp4zmbSij9rxOXw.png"/></div></div></figure><p id="fd22" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">每个样本包含三列:<em class="lu">身高</em>、<em class="lu">体重</em>和<em class="lu">男性</em>。</p><ul class=""><li id="2c66" class="lx ly ir la b lb lc le lf lh lz ll ma lp mb lt mc md me mf bi translated"><em class="lu">高度</em>英寸</li><li id="18d7" class="lx ly ir la b lb mg le mh lh mi ll mj lp mk lt mc md me mf bi translated">以磅为单位的重量</li><li id="e801" class="lx ly ir la b lb mg le mh lh mi ll mj lp mk lt mc md me mf bi translated"><em class="lu">男性:</em> 1 表示测量对应男性，0 表示测量对应女性。</li></ul><p id="4d13" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">有 5000 个来自男性的样本，5000 个来自女性的样本，因此数据集是平衡的，我们可以继续进行训练。</p><p id="883f" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">Python 的 scikit-learn 代码训练逻辑回归分类器并做出预测非常简单:</p><figure class="kg kh ki kj gu kl"><div class="bz fq l di"><div class="ml mm l"/></div></figure><p id="319b" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">一般的工作流程是:</p><ol class=""><li id="8163" class="lx ly ir la b lb lc le lf lh lz ll ma lp mb lt mn md me mf bi translated">获取数据集</li><li id="b20a" class="lx ly ir la b lb mg le mh lh mi ll mj lp mk lt mn md me mf bi translated">训练分类器</li><li id="ce5f" class="lx ly ir la b lb mg le mh lh mi ll mj lp mk lt mn md me mf bi translated">使用这种分类器进行预测</li></ol><h1 id="e8e2" class="mo mp ir bd mq mr ms mt mu mv mw mx my jx mz jy na ka nb kb nc kd nd ke ne nf bi translated">逻辑回归假设</h1><p id="e8fe" class="pw-post-body-paragraph ky kz ir la b lb ng js ld le nh jv lg lh ni lj lk ll nj ln lo lp nk lr ls lt ik bi translated">逻辑回归分类器可以通过类比<strong class="la is"> <em class="lu">线性回归假设</em> </strong> <em class="lu">得出，即</em>:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj nl"><img src="../Images/1791deceabcc24d4207c01b22c93fbeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SBm3GhKjB5W8DEYxHidNng.png"/></div></div><figcaption class="nm nn gk gi gj no np bd b be z dk">Linear regression hypothesis.</figcaption></figure><p id="9dd2" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">然而，逻辑回归假设<em class="lu">从线性回归假设中概括出</em>，因为它使用了<strong class="la is"> <em class="lu">逻辑函数</em> </strong>:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj nq"><img src="../Images/924c583969fe1872c5673ec09aa93097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o4Dy1w4n2kDOLA8UEwGC9g.png"/></div></div></figure><p id="6038" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">结果是逻辑回归假设:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj nr"><img src="../Images/6254a6146a40b8a3391614aed7b66993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8MbhNzfr4vaf7sZmQ6QZ4A.png"/></div></div><figcaption class="nm nn gk gi gj no np bd b be z dk">Logistic regression hypothesis.</figcaption></figure><p id="7d0a" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">函数<strong class="la is"> g(z) </strong>是<strong class="la is">逻辑函数</strong>，也称为<em class="lu">s 形函数</em>。</p><p id="439c" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">逻辑函数在 0 和 1 处有渐近线，它在 0.5 处穿过 y 轴。</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj ns"><img src="../Images/5774f07b620219e6ea49753e1ed10f0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sR6PgnEB5HmuRuz62DfWzQ.png"/></div></div><figcaption class="nm nn gk gi gj no np bd b be z dk">Logistic function.</figcaption></figure><h1 id="1d88" class="mo mp ir bd mq mr ms mt mu mv mw mx my jx mz jy na ka nb kb nc kd nd ke ne nf bi translated">逻辑回归决策边界</h1><p id="52bf" class="pw-post-body-paragraph ky kz ir la b lb ng js ld le nh jv lg lh ni lj lk ll nj ln lo lp nk lr ls lt ik bi translated">由于我们的数据集有两个特征:身高和体重，逻辑回归假设如下:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj nt"><img src="../Images/0eee6421bfd289d34c10526650a607e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*siWAvCzFy2zp81pDNoa4UA.png"/></div></div></figure><p id="13ee" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">逻辑回归分类器将预测“男性”,如果:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj nu"><img src="../Images/34eb12a1c72fa1cc88a612041c180d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8jzD7NjuFGSSlKUMhO8jg.png"/></div></div></figure><p id="545d" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">这是因为逻辑回归“<em class="lu">阈值</em>”被设置为 g(z)=0.5，请参见上面的逻辑回归函数图进行验证。</p><p id="e469" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">对于我们的数据集，θ值为:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj nv"><img src="../Images/9012cdce1bd503ff0f0af19ea53c805f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FCaWw66MGQ49NpN9TT9mdA.png"/></div></div></figure><p id="93e2" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">要获得 scikit-learn 计算的θ参数，可以:</p><pre class="kg kh ki kj gu nw nx ny nz aw oa bi"><span id="2ff2" class="ob mp ir nx b gz oc od l oe of"><strong class="nx is"># For theta_0:</strong></span><span id="f167" class="ob mp ir nx b gz og od l oe of">print( fitted_model.intercept_ )</span><span id="f7a9" class="ob mp ir nx b gz og od l oe of"><strong class="nx is"># For theta_1 and theta_2:</strong></span><span id="c5bf" class="ob mp ir nx b gz og od l oe of">print( fitted_model.coef_ )</span></pre><p id="8e5b" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">有了这些系数，一个<em class="lu">手动</em>预测(也就是说，不使用函数<strong class="la is"> clf.predict() </strong>)将只需要计算向量积</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj oh"><img src="../Images/0378aa084627e29d6ba0c03dab49199c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-HaR4S9gQvAH49dYpSILvQ.png"/></div></div></figure><p id="8a03" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">并检查结果标量是否大于或等于零(预测男性)，否则(预测女性)。</p><p id="2746" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated"><strong class="la is">以</strong>为例，假设我们想要预测某个身高<strong class="la is"><em class="lu">= 70 英寸</em></strong><strong class="la is"><em class="lu">体重= 180 磅</em> </strong>的人的性别，就像上面脚本 LogisticRegression.py 的第 14 行一样，我们可以简单地做:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj oh"><img src="../Images/656ed41ed69db02bcdf1b65e7efde05c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aG7XM8nBIWGosG5lVrmx2w.png"/></div></div><figcaption class="nm nn gk gi gj no np bd b be z dk">Making a prediction using the Logistic Regression parameter θ.</figcaption></figure><p id="2fc1" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">由于乘积的结果大于零，分类器将预测男性。</p><p id="daa8" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">这里可以看到<strong class="la is"> <em class="lu">决策边界</em> </strong>和完整数据集的可视化:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj oi"><img src="../Images/8c7717d84f8c1d6947d234ab7f2d2f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ArM2nhi7ZQNgdegWAWgG4Q.png"/></div></div></figure><p id="76d9" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">正如你所看到的，在决策边界的上方是大多数对应于男性类别的<strong class="la is">蓝色点，在它的下方是所有对应于女性类别</strong>的<strong class="la is">粉色点。</strong></p><p id="cf32" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">此外，只看数据你就能知道预测不会完美。这可以通过包括更多的特征(除了体重和身高)以及潜在地使用不同的判定边界来改进。</p><p id="c9ff" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">逻辑回归决策边界也可以是非线性函数，例如高次多项式。</p><h1 id="1855" class="mo mp ir bd mq mr ms mt mu mv mw mx my jx mz jy na ka nb kb nc kd nd ke ne nf bi translated">计算逻辑回归参数</h1><p id="f174" class="pw-post-body-paragraph ky kz ir la b lb ng js ld le nh jv lg lh ni lj lk ll nj ln lo lp nk lr ls lt ik bi translated">scikit-learn 库在抽象逻辑回归参数θ的计算方面做得很好，它是通过解决一个优化问题来完成的。</p><p id="0302" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">我们先来定义两个兴趣点的逻辑回归代价函数:y=1，y=0，也就是假设函数预测男性或女性的时候。</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj oj"><img src="../Images/d0fa1a87e73012f699cc94db098160ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uFMcqEpoDZ-5jE927oln3Q.png"/></div></div></figure><p id="4544" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">然后，我们在这两项的<strong class="la is"> y </strong>中取一个凸组合，得出逻辑回归成本函数:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj nt"><img src="../Images/a4383cd5b7070a063aeb4645bc0803c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxvtoAaBqtV_QkF8w8SUHQ.png"/></div></div><figcaption class="nm nn gk gi gj no np bd b be z dk">Logistic regression cost function.</figcaption></figure><p id="e317" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">逻辑回归成本函数是凸的。因此，为了计算θ，需要解决以下(无约束)优化问题:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj nv"><img src="../Images/e6c78487bbe7cdf2a6c52c0f3ca5bef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYPzL-47DxVTO_uGNOnx_Q.png"/></div></div></figure><p id="0185" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">有多种方法可用于解决这种无约束优化问题，例如一阶方法<strong class="la is"> <em class="lu">梯度下降</em> </strong>需要逻辑回归成本函数的梯度，或者二阶方法例如<strong class="la is"> <em class="lu">牛顿法</em> </strong>需要逻辑回归成本函数的梯度和 Hessian 这是上述 scikit-learn 脚本中规定的方法。</p><p id="96d7" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">对于梯度下降的情况，搜索方向是逻辑回归成本函数相对于参数θ的负偏导数:</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj ok"><img src="../Images/5e8818a5e2aa28342c9b5b63a1e11503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7IayJm58G_GGmMM4v4lNrQ.png"/></div></div><figcaption class="nm nn gk gi gj no np bd b be z dk">Partial derivative of the logistic regression cost function.</figcaption></figure><p id="3ee0" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">在其最基本的形式中，梯度下降将沿着θ的负梯度方向迭代(称为<em class="lu">最小化序列</em>)，直到达到收敛。</p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gi gj ol"><img src="../Images/c7e51006f3bdfd9b7f3c68e891927faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4H7KVT3jzOxqdTLe-MdbDg.png"/></div></div><figcaption class="nm nn gk gi gj no np bd b be z dk">Prototype of gradient descent.</figcaption></figure><p id="8c25" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">注意，常数α通常被称为<em class="lu">学习速率</em>或<em class="lu">搜索步骤</em>，它必须被仔细调整以达到收敛。<em class="lu"> </em>算法如<a class="ae lv" href="https://en.wikipedia.org/wiki/Backtracking_line_search" rel="noopener ugc nofollow" target="_blank"> <em class="lu">回溯线搜索</em> </a>辅助确定α。</p></div><div class="ab cl om on hv oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="ik il im in io"><p id="4cc4" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">总之，这是您下次使用或实现逻辑回归分类器时应该记住的三个基本概念:</p><blockquote class="ot ou ov"><p id="54f6" class="ky kz lu la b lb lc js ld le lf jv lg ow li lj lk ox lm ln lo oy lq lr ls lt ik bi translated">1.逻辑回归假设</p><p id="de0f" class="ky kz lu la b lb lc js ld le lf jv lg ow li lj lk ox lm ln lo oy lq lr ls lt ik bi translated">2.逻辑回归决策边界</p><p id="f9cc" class="ky kz lu la b lb lc js ld le lf jv lg ow li lj lk ox lm ln lo oy lq lr ls lt ik bi translated">3.逻辑回归成本函数</p></blockquote><p id="efde" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">对于应用于具有更多特征的数据集的逻辑回归分类器的讨论(也使用 Python ),我推荐<a class="oz pa ep" href="https://medium.com/u/731d8566944a?source=post_page-----704a78be7e0a--------------------------------" rel="noopener" target="_blank"> Susan Li </a>的这篇<a class="ae lv" rel="noopener" target="_blank" href="/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8">中期文章</a>。</p><h2 id="e77e" class="ob mp ir bd mq pb pc dn mu pd pe dp my lh pf pg na ll ph pi nc lp pj pk ne pl bi translated">参考资料和进一步阅读:</h2><ul class=""><li id="e38a" class="lx ly ir la b lb ng le nh lh pm ll pn lp po lt mc md me mf bi translated"><a class="ae lv" href="https://github.com/gchavez2/code_machine_learning_algorithms" rel="noopener ugc nofollow" target="_blank">https://github . com/gchave z2/code _ machine _ learning _ algorithms</a></li><li id="da08" class="lx ly ir la b lb mg le mh lh mi ll mj lp mk lt mc md me mf bi translated"><a class="ae lv" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">吴恩达关于逻辑回归的讲座</a></li><li id="7399" class="lx ly ir la b lb mg le mh lh mi ll mj lp mk lt mc md me mf bi translated"><a class="ae lv" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn 的逻辑回归类</a></li></ul></div><div class="ab cl om on hv oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="ik il im in io"><p id="5b7d" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated">我是劳伦斯·伯克利国家实验室的博士后研究员，我在机器学习和高性能计算的交叉领域工作。</p><p id="6a33" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated"><em class="lu">如果你觉得这篇文章很有意思，可以随意在</em><a class="ae lv" href="https://www.linkedin.com/in/guschavezcs/" rel="noopener ugc nofollow" target="_blank"><em class="lu">LinkedIn</em></a><em class="lu">打个招呼，我总是很乐意与该领域的其他专业人士联系。</em></p><p id="bc7f" class="pw-post-body-paragraph ky kz ir la b lb lc js ld le lf jv lg lh li lj lk ll lm ln lo lp lq lr ls lt ik bi translated"><em class="lu">一如既往:欢迎评论、提问和分享！❤️ </em></p><figure class="kg kh ki kj gu kl gi gj paragraph-image"><a href="http://eepurl.com/giSKB9"><div class="gi gj pp"><img src="../Images/fc9812283b6ecba784677161ec8ba4ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FCyfjdltxWoVuR5fiKmlbA.png"/></div></a><figcaption class="nm nn gk gi gj no np bd b be z dk">No Spam, ever.</figcaption></figure></div></div>    
</body>
</html>