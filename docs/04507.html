<html>
<head>
<title>LIME: Explaining predictions of machine learning models (1/2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">莱姆:解释机器学习模型的预测(1/2)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lime-explaining-predictions-of-machine-learning-models-1-2-1802d56addf9?source=collection_archive---------17-----------------------#2019-07-11">https://towardsdatascience.com/lime-explaining-predictions-of-machine-learning-models-1-2-1802d56addf9?source=collection_archive---------17-----------------------#2019-07-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="946b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在我之前的<a class="ae ki" href="http://datascienceninja.com/2019/07/10/how-to-interpret-machine-learning-models/" rel="noopener ugc nofollow" target="_blank">博客</a>中，我提到了以下理解模型预测的基本技术。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi kj"><img src="../Images/47bde44c1f2bb60121c4d5d4f511a463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/0*IPPbDln4Lwyc26sn"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae ki" href="https://unsplash.com/@alexbertha?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Alex Bertha</a> on <a class="ae ki" href="https://unsplash.com/search/photos/ariel-view?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><ol class=""><li id="6a28" class="kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">使用基尼指数的特征重要性</li><li id="ca23" class="kv kw it kx b ky ln la lo lc lp le lq lg lr li lj lk ll lm bi translated">使用排列重要性的特征重要性</li><li id="ce72" class="kv kw it kx b ky ln la lo lc lp le lq lg lr li lj lk ll lm bi translated">部分相关图</li></ol><p id="b0be" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">首先，我想问一个问题:<em class="mf">“我们可以仅仅因为模型在测试数据上的表现令人信服地高，就相信模型的预测吗？”</em>许多人可能会回答这个问题为<em class="mf">【是的】</em>。但这并不总是正确的。高模型性能不应被视为信任模型预测的指标，因为模型拾取的信号可能是随机的，可能没有商业意义。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi kj"><img src="../Images/dd0b5d6f510c41ab57a78e9d3dc24465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/0*RXkTl_sbtzic-1f2"/></div></figure><p id="8a65" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">在这篇博客中，我将谈论 LIME，一种在实例层面上帮助理解预测背后的原因的方法。这是一种与模型无关的技术，也就是说，你可以将它用于任何模型，无论是神经网络、基于树的模型、支持向量机等等。</p><p id="4719" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">LIME 代表<strong class="kx iu"><em class="mf">L</em></strong>ocal<strong class="kx iu"><em class="mf">I</em></strong>n interpretable<strong class="kx iu"><em class="mf">M</em></strong>odel-Agnostic<strong class="kx iu"><em class="mf">E</em></strong>解释。对许多人来说，本地这个词似乎是最令人困惑的。所以我先来解释一下为什么<em class="mf">这个词本地</em>。</p><p id="6995" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">目标和自变量之间的关系在全局水平上可能变得非常复杂，例如参考下图。查看这个全局视图并不能提供太多关于独立特征和目标之间关系的洞察，所以 LIME 放大在非常非常局部的水平上进行。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/aefeb2b95844a1319d032a2d0a942fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*KQW5_OS1NTqqxklj"/></div></figure><p id="0ecf" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">让我们假设被解释的实例用绿色星号标记。LIME 背后的主要直觉是，在放大和局部分析目标和独立特征之间的关系之后，我们可以使用线性模型来近似这种关系。</p><p id="1d25" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">现在，让我们了解石灰的工作步骤:</p><ul class=""><li id="7dce" class="kv kw it kx b ky kz la lb lc ld le lf lg lh li mh lk ll lm bi translated">对原始数据中靠近绿色星号的实例进行采样和 SMOTE 处理(正在解释实例)</li><li id="87de" class="kv kw it kx b ky ln la lo lc lp le lq lg lr li mh lk ll lm bi translated">计算采样实例和被解释实例之间的距离</li><li id="7d42" class="kv kw it kx b ky ln la lo lc lp le lq lg lr li mh lk ll lm bi translated">对于这些合成生成的实例，使用原始全局模型进行预测</li><li id="1212" class="kv kw it kx b ky ln la lo lc lp le lq lg lr li mh lk ll lm bi translated">对此数据集拟合一个简单的线性模型</li><li id="ec11" class="kv kw it kx b ky ln la lo lc lp le lq lg lr li mh lk ll lm bi translated">基于在步骤 2 中计算的相似性指数对该线性模型进行加权。这是为了确保最接近原始实例的实例上的错误比其他实例更有价值</li></ul><p id="35b6" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">现在让我们进入例子来解释石灰的结果。我已经使用了来自<a class="ae ki" href="https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的数据集。目标是确定拖欠信用卡付款的最强预测因素。我安装了一个 Scikit-learn 实现的梯度增强树。</p><pre class="kk kl km kn gt mi mj mk ml aw mm bi"><span id="4b3a" class="mn mo it mj b gy mp mq l mr ms">Correctly classified <em class="mf">‘Non Default Payment’</em> instance:</span></pre><p id="784b" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated"><em class="mf">橙色</em>表示贡献给<em class="mf">‘违约金’，蓝色</em>表示贡献给<em class="mf">‘非违约金’。</em>全局模型对此实例的预测是 0.072，而局部模型的预测是 0.066。所以，对于这个特殊的例子，全局模型和局部模型的预测非常接近。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi kj"><img src="../Images/0b2619c8e33ec2e97cded9bdc3075782.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/0*mpJolAkGKthriJu8"/></div></figure><p id="d128" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">上面的例子表明，这个被标记为<em class="mf">‘非违约付款’</em>的特定实例的主要贡献者是上个月的还款状态(<em class="mf"> PAY_0 = 1 </em>)。这意味着在前一个月付款是按时完成的。其他变量的解释也类似(<em class="mf"> PAY_2，PAY_3 </em>等)。).这表明，如果前几个月的付款按时完成，那么这个人下个月不违约的可能性很高。</p><p id="a4c5" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated"><em class="mf"> LIMIT_BAL </em>的高金额也促成了<em class="mf">非违约付款。</em>这也是有意义的，因为高<em class="mf"> LIMIT_BAL </em>意味着剩余补充信用低。因此，下个月的账单有可能会很低，因此违约的几率会更小。现在，对于连续要素，LIME 的输出提供了更多详细信息，因为它指定了导致该要素产生影响的一系列特征值。例如<em class="mf">'限额 _ 余额&gt; 24 万'</em></p><p id="4195" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">我个人认为分析错误预测的实例是非常有见地的。这有助于理解哪些特征实际上导致了预测不正确。</p><pre class="kk kl km kn gt mi mj mk ml aw mm bi"><span id="5626" class="mn mo it mj b gy mp mq l mr ms">Incorrectly classified <em class="mf">‘Non Default Payment’</em> instance (Actual: <em class="mf">‘Default Payment’</em>):</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/6d718ddba51efbd8b60575e9e2896817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Cy5SLG_n0mFdeAYK"/></div></div></figure><p id="e1d6" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">由于表示先前付款历史的特征(<em class="mf"> PAY_0 </em>、<em class="mf"> PAY_2 </em>、<em class="mf"> PAY_3 </em>)，上述实例被错误地预测为<em class="mf">非违约付款</em>。在这种情况下，以前的付款是按时完成的。这可能使预测变得不正确。</p><p id="5d20" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">根据上面的例子，石灰的一般概念听起来是合理的，但是石灰也有一些潜在的缺点。</p><pre class="kk kl km kn gt mi mj mk ml aw mm bi"><span id="96ea" class="mn mo it mj b gy mp mq l mr ms"><strong class="mj iu">1. Local linear behavior</strong></span></pre><p id="745a" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">LIME 目前在本地实现线性模型来获得解释。而这种假设对于正在解释的实例周围的小区域是有效的。但是对于更复杂的数据集，随着我们增加局部区域的面积，这个假设可能不成立。因此，来自局部可解释模型的解释可能无法解释全局线性模型的行为。</p><pre class="kk kl km kn gt mi mj mk ml aw mm bi"><span id="44a8" class="mn mo it mj b gy mp mq l mr ms"><strong class="mj iu">2. Number of features</strong></span></pre><p id="ca78" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">需要优化选择用于解释的特征的数量。应该选择特征的数量，以便保持模型的复杂性和解释的简单性。在 LIME 的 python 实现中，它提供了以下选项:<em class="mf">‘forward _ selection’，‘lasso _ path’，‘none’或‘auto’</em>。</p><pre class="kk kl km kn gt mi mj mk ml aw mm bi"><span id="757b" class="mn mo it mj b gy mp mq l mr ms"><strong class="mj iu">3. Models without probability scores</strong></span></pre><p id="e57d" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">LIME 目前不支持没有概率分数的分类器模型。</p><pre class="kk kl km kn gt mi mj mk ml aw mm bi"><span id="dc51" class="mn mo it mj b gy mp mq l mr ms"><strong class="mj iu">4. No aggregated view</strong></span></pre><p id="3a7c" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">LIME 实现不提供特性解释的聚合视图。用户必须依次分析各个实例。</p><pre class="kk kl km kn gt mi mj mk ml aw mm bi"><span id="19d1" class="mn mo it mj b gy mp mq l mr ms"><strong class="mj iu">5. Similarity score</strong></span></pre><p id="86a8" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">对于表格数据，LIME 通过单独扰动每个要素来创建新样本，从正态分布中提取该要素的平均值和标准差。对于连续特征，LIME 提供了一个选项，用于从以被解释的实例为中心的法线或从以特征数据集的平均值为中心的法线扰动样本。越靠近感兴趣的实例，样本的石灰权重越高。现在，在高维空间中，每个特征具有不同范围的值，计算相似性指数具有挑战性，并且可能不会产生清晰的结果。</p><pre class="kk kl km kn gt mi mj mk ml aw mm bi"><span id="e5f3" class="mn mo it mj b gy mp mq l mr ms"><strong class="mj iu">6. Definition of the neighborhood</strong></span></pre><p id="50fc" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">定义一个正确的邻域来运行局部可解释模型是一个困难的问题。那么，为什么我们需要沿着被解释的实例定义一个局部邻域呢？解释模型对原始模型的准确性是通过在由局部核加权的简化输入空间中的一组样本上的损失函数来加强的。目前，LIME 使用指数平滑内核。这个内核的宽度决定了对正在解释的实例的局部模型的影响，同样，没有确定内核宽度的最佳方法。</p><pre class="kk kl km kn gt mi mj mk ml aw mm bi"><span id="0d6c" class="mn mo it mj b gy mp mq l mr ms"><strong class="mj iu">7. Consistency of the model explanations</strong></span></pre><p id="c386" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">由于采样偏差、相似性得分的计算和邻域的定义，石灰解释有时可能缺乏一致性。</p><h1 id="40d3" class="my mo it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">结论:</h1><p id="2cd6" class="pw-post-body-paragraph ls lt it kx b ky np ju lu la nq jx lv lc nr lx ly le ns ma mb lg nt md me li im bi translated">LIME 提供了非常简单的可解释的解释，并且是分析每个特征的贡献的快速方法。LIME 也可以用于文本和图像数据，与其他可解释的技术(如 SHAP)相比，LIME 的执行时间更少。我喜欢 LIME 的另一个特性是，局部代理模型实际上可以使用独立的特性，而不是全局模型中使用的特性。</p><p id="31bb" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">其他阅读资源:</p><ol class=""><li id="a752" class="kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><a class="ae ki" href="https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf" rel="noopener ugc nofollow" target="_blank">https://www . KDD . org/KDD 2016/papers/files/RFP 0573-ribeiroa . pdf</a></li><li id="a5e4" class="kv kw it kx b ky ln la lo lc lp le lq lg lr li lj lk ll lm bi translated">【https://github.com/marcotcr/lime T4】</li><li id="7649" class="kv kw it kx b ky ln la lo lc lp le lq lg lr li lj lk ll lm bi translated"><a class="ae ki" href="https://christophm.github.io/interpretable-ml-book/lime.html" rel="noopener ugc nofollow" target="_blank">https://christophm . github . io/interpretable-ml-book/lime . html</a></li></ol><h1 id="183f" class="my mo it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">下一步是什么？</h1><p id="ee05" class="pw-post-body-paragraph ls lt it kx b ky np ju lu la nq jx lv lc nr lx ly le ns ma mb lg nt md me li im bi translated">在下一篇博客中，我将解释另一种流行的技术:用于解释模型预测的 SHAP。</p><p id="0c33" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">这个可解释的人工智能领域正在迅速发展，在工具和框架方面有很多新的发展。请在评论区写下你对博客的反馈，以及你在这个领域使用的最新工具。此外，如果你想让我写任何特定的主题，请发表评论。</p><p id="9b08" class="pw-post-body-paragraph ls lt it kx b ky kz ju lu la lb jx lv lc lw lx ly le lz ma mb lg mc md me li im bi translated">这些内容最初发表在我的个人博客网站:<a class="ae ki" href="http://datascienceninja.com/" rel="noopener ugc nofollow" target="_blank">http://datascienceninja.com/</a>。点击<a class="ae ki" href="http://datascienceninja.com/2019/07/10/lime-explaining-predictions-of-machine-learning-models-1-2/" rel="noopener ugc nofollow" target="_blank">此处</a>查看并订阅即时接收最新博客更新。</p></div></div>    
</body>
</html>