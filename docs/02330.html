<html>
<head>
<title>The Complete Guide to Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树完全指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14?source=collection_archive---------0-----------------------#2019-04-17">https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14?source=collection_archive---------0-----------------------#2019-04-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4b9d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于机器学习中的顶级算法，你需要知道的一切</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/57331b292935455c1f413ab16e4bd8a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8k5UmElJ_TyUwAcF"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@imperiumnordique?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Helena Hertz</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1c18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一开始，学习机器学习(ML)可能会令人生畏。像“梯度下降”、“潜在狄利克雷分配”或“卷积层”这样的术语会吓到很多人。但是也有友好的方式进入这个领域，我认为从决策树开始是一个明智的决定。</p><p id="b378" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树可能是最有用的<em class="lv">监督学习</em>算法之一。与无监督学习(没有输出变量来指导学习过程，数据由算法探索以找到模式)相反，在<strong class="lb iu">监督学习</strong>中，你的现有数据已经被标记，你知道你想要在你获得的新数据中预测哪种行为。这是自动驾驶汽车用来识别行人和物体的算法类型，或者是组织用来估计客户终身价值和客户流失率的算法类型。</p><blockquote class="lw"><p id="01a5" class="lx ly it bd lz ma mb mc md me mf lu dk translated">在某种程度上，监督学习就像跟着老师学习，然后将知识应用到新数据中。</p></blockquote><p id="b33f" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated">DTs 是 ML 算法，它基于一个描述性特征将数据集逐步分成更小的数据组，直到它们达到足够小的集合，可以用某个<strong class="lb iu">标签</strong>来描述。他们要求你有被标记的数据(用一个或多个标签标记，像植物图片中的植物名称)，所以他们试图根据这些知识标记新的数据。</p><p id="5368" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DTs 算法完美地解决了分类问题(机器将数据分类到类别中，如电子邮件是否是垃圾邮件)和回归问题(机器预测值，如房地产价格)。当因变量为连续变量或定量变量时，使用回归树(例如，如果我们想要估计客户拖欠贷款的概率)，当因变量为分类变量或定性变量时，使用分类树(例如，如果我们想要估计一个人的血型)。</p><p id="69a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DTs 的重要性依赖于它们在现实世界中有很多应用的事实。作为 ML 中最常用的算法之一，它们被应用于几个行业中的不同功能:</p><ul class=""><li id="a83e" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated">DTs 正被用于医疗保健行业，以改善早期检测<strong class="lb iu">认知障碍</strong>中阳性病例的筛查，并识别未来发展为某种类型痴呆症的主要<a class="ae ky" href="https://www.news-medical.net/news/20190117/Scientists-design-two-AI-algorithms-to-improve-early-detection-of-cognitive-impairment.aspx" rel="noopener ugc nofollow" target="_blank">风险因素。</a></li><li id="2793" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><em class="lv">索菲娅</em>，<a class="ae ky" href="https://www.theverge.com/2017/11/10/16617092/sophia-the-robot-citizen-ai-hanson-robotics-ben-goertzel" rel="noopener ugc nofollow" target="_blank">成为沙特公民的机器人</a>，使用 DTs 算法与人类聊天。事实上，使用这些算法的<strong class="lb iu">聊天机器人</strong>已经通过应用创新的调查和友好的聊天从客户那里收集数据，为<a class="ae ky" href="https://yourstory.com/2018/11/age-alexa-siri-chatbots-next-health-assistants/" rel="noopener ugc nofollow" target="_blank">健康保险</a>等行业带来了好处。谷歌最近收购了<em class="lv"> Onward </em>，这是一家使用 DTs 开发<a class="ae ky" href="https://martechseries.com/predictive-ai/ai-platforms-machine-learning/google-purchases-customer-service-automation-firm-onward/" rel="noopener ugc nofollow" target="_blank">聊天机器人的公司，这些聊天机器人在提供世界一流的客户服务方面具有非凡的功能</a>，而<a class="ae ky" href="https://venturebeat.com/2019/01/10/why-25-of-companies-are-copying-amazons-customer-support-model-using-bots-and-ai-vb-live/" rel="noopener ugc nofollow" target="_blank">亚马逊也在向同一方向投资</a>，以引导客户快速找到解决方案。</li><li id="3b37" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">通过训练 DTs 从卫星图像中识别<strong class="lb iu">森林损失</strong>的不同原因，有可能<a class="ae ky" href="https://www.europeanscientist.com/en/agriculture/new-analysis-reveals-causes-of-global-forest-loss/" rel="noopener ugc nofollow" target="_blank">预测森林干扰</a>的最可能原因，如野火、植树造林、大规模或小规模农业以及城市化。DTs 和卫星图像也被用于<strong class="lb iu">农业</strong>对<a class="ae ky" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4567051/" rel="noopener ugc nofollow" target="_blank">不同的作物类型</a>进行分类，并识别它们的物候阶段。</li><li id="9833" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">DTs 是<a class="ae ky" href="https://www.infoq.com/articles/sentiment-analysis-whats-with-the-tone" rel="noopener ugc nofollow" target="_blank">对文本进行情感分析</a>并识别其背后的情感的伟大工具。情绪分析是一种强大的技术，可以帮助组织了解客户的选择和他们的决策驱动因素。</li><li id="af0a" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">在<strong class="lb iu">环境科学</strong>中，DTs 可以帮助确定<a class="ae ky" href="https://phys.org/news/2018-05-ai-canada-invasive-species.html" rel="noopener ugc nofollow" target="_blank">应对入侵物种</a>的最佳策略，范围从根除到遏制，以及减缓扩散。</li><li id="ff6d" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">DTs 还用于改进<strong class="lb iu">金融欺诈检测</strong>。麻省理工学院表明，它可以通过使用经过多种原始数据来源训练的 DTs 来显著提高替代性 ML 模型的性能，以<a class="ae ky" href="http://news.mit.edu/2018/machine-learning-financial-credit-card-fraud-0920" rel="noopener ugc nofollow" target="_blank">找到与欺诈案件相匹配的交易和信用卡模式</a>。</li></ul><p id="0fc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于各种原因，DTs 非常受欢迎，因为它们的可解释性可能是它们最重要的优势。它们可以很快被训练出来，而且很容易理解，这为它们打开了通向远远超越科学壁垒的新领域的可能性。如今，DTs 在商业环境中非常流行，其使用也扩展到了民用领域，其中一些应用引起了极大的关注。</p><p id="f46b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">芝麻信用公司(隶属于阿里巴巴的一家公司)使用<a class="ae ky" href="https://bluenotes.anz.com/posts/2016/11/china-credit-and-seeing-unforseen-risk" rel="noopener ugc nofollow" target="_blank"> DTs 和其他算法来构建一个社会评估系统</a>，考虑了各种因素，如支付账单和其他在线活动的准时性。在中国，一个好的“芝麻分”的好处包括在交友网站上更高的知名度，以及在你需要看医生时不用排队。事实上，在中国政府宣布将把所谓的社会信用体系应用于航班和火车，并在一年内禁止有不良行为的人乘坐此类交通工具后，人们担心该体系最终会产生一个庞大的“ML 支持的老大哥”。</p><h1 id="295f" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated"><strong class="ak">基础知识</strong></h1><p id="5b15" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">在电影<em class="lv"> Bandersnatch </em>(来自网飞的独立黑镜剧集)中，观众可以互动地选择不同的叙事路径，到达不同的故事线和结局。在电影讲故事的背后隐藏着一系列复杂的决定，让观众以一种选择自己的冒险模式移动，为此网飞必须找到一种方法，在以简单的方式呈现每个场景的同时加载多个版本。实际上，网飞制片人所做的是将电影分段，并为观众设置不同的分支点，从而得出不同的结果。换句话说，这就像建造一台 DT 一样。</p><p id="52fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DTs 由节点、分支和叶子组成。每个<strong class="lb iu">节点</strong>代表一个属性(或特征)，每个<strong class="lb iu">分支</strong>代表一个规则(或决策)，每个<strong class="lb iu">叶子</strong>代表一个结果。树的<strong class="lb iu">深度</strong>由层数定义，不包括根节点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/d607af81162dd59535af818126dc0151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IS9xKHt83nuERC9P"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="nx">In this example, a DT of 2 levels.</em></figcaption></figure><p id="30a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DTs 对数据应用自上而下的方法，因此给定一个数据集，它们试图对它们之间相似的观察值进行分组和标记，并寻找最佳规则来拆分它们之间不相似的观察值，直到它们达到一定程度的相似性。</p><blockquote class="ny nz oa"><p id="571a" class="kz la lv lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated">他们使用分层拆分过程，在每一层，他们尝试将数据拆分成两个或更多组，以便属于同一组的数据彼此最相似(<strong class="lb iu">同质性</strong>)，而组之间尽可能不同(<strong class="lb iu">异质性</strong>)。</p></blockquote><p id="4e24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分割可以是<strong class="lb iu">二进制</strong>(将每个节点分割成<em class="lv">最多</em>两个子组，并尝试找到最佳分割)，或<strong class="lb iu">多路</strong>(将每个节点分割成多个子组，使用与现有不同值一样多的分割)。在实践中，通常会看到带有二进制拆分的 DTs，但是知道多路拆分有一些优点是很重要的。多路分割耗尽了一个名义属性中的所有信息，这意味着一个属性很少在从根到叶的任何路径中出现一次以上，这使得 DTs 更容易理解。事实上，分割数据的最佳方式可能是为给定的要素找到一组间隔，然后根据这些间隔将数据分割成几个组。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/79a9e45ec5a8dde8a7326e438970975d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EhzZNP_6Y7jdJgO9"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="nx">On the left hand side, a DT with binary splitting, as opposed to a DT with multiway splitting on the right.</em></figcaption></figure><p id="fb62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在二维术语中(仅使用 2 个变量)，DTs 将数据领域划分为一组矩形，并在每个矩形中建立一个模型。它们简单而强大，对于数据科学家来说是一个很好的工具。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/2e01b11dfb6939bfc2f06f4779a5a0c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cant-HQdfMju-GxG"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="nx">Right figure shows the partition of the bidimensional data space produced by the DT on the left (binary split). In practice, however, DTs use numerous variables (usually more than 2).</em></figcaption></figure><p id="26c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DT 中的每个节点都充当某个条件的测试用例，从该节点开始的每个分支都对应于该测试用例的一个可能的答案。</p><h1 id="212a" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">修剪那棵树</h1><p id="9941" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">随着 DTs 中拆分数量的增加，它们的复杂性也会增加。一般来说，简单的 DTs 比超级复杂的 DTs 更受欢迎，因为它们更容易理解，并且不太可能陷入过度拟合。</p><blockquote class="lw"><p id="4e46" class="lx ly it bd lz ma mb mc md me mf lu dk translated"><strong class="ak">过度拟合</strong>指的是一种模型，它对<strong class="ak">训练数据</strong>(它用来学习的数据)学习得如此之好，以至于它在推广到新的(看不见的)数据时遇到了问题。</p></blockquote><p id="780e" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated">换句话说，模型学习训练数据中的细节和噪声(数据集中不相关的信息或随机性),以至于对模型在新数据上的性能产生负面影响。这意味着训练数据中的噪声或随机波动被模型拾取并学习为概念。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/20f5178c8a2159ae0196bee0cec66051.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*c3W5mjgvBRIOFA8ye1JEXg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="nx">While the black line fits the data well, the green line is overfitting</em></figcaption></figure><p id="58a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，您的模型可以很好地处理您预先提供的数据，但是当您将相同的模型暴露给新数据时，它就会崩溃。它无法重复其高度细致的表演。</p><p id="a2fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，如何避免 DTs 中的过拟合呢？您需要排除非常适合数据的分支。您需要一个能够归纳新数据并很好地处理新数据的 DT，即使这可能意味着失去训练数据的精度。避免像鹦鹉学舌一样学习和重复特定细节的 DT 模型总是更好，并尝试开发一种具有能力和灵活性的模型，以便在您提供给它的新数据上有体面的性能。</p><blockquote class="ny nz oa"><p id="ca26" class="kz la lv lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated"><strong class="lb iu">修剪</strong>是一种用于处理过度拟合的技术，它通过删除树中几乎没有预测或分类能力的部分来减小 DTs 的大小。</p></blockquote><p id="cef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一过程的目的是通过减少过拟合的影响和去除可能基于噪声或错误数据的 DT 部分来降低复杂性和获得更好的精度。在 DTs 上执行修剪有两种不同的策略:</p><ul class=""><li id="2069" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated"><strong class="lb iu">预修剪:</strong>当信息变得不可靠时，停止生长 DT 分支。</li><li id="692b" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu">后期剪枝:</strong>当您获取一个完全成长的 DT，然后仅当它导致更好的模型性能时才删除叶节点。这样，当无法进一步改进时，您就可以停止删除节点。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/572ba9fd61ec4a0877a5c00e532d6e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ubL8_k7w3JNEvZ18"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="nx">Example of an unpruned DT, as taken from </em><a class="ae ky" href="https://www.datacamp.com/community/tutorials/decision-tree-classification-python" rel="noopener ugc nofollow" target="_blank"><em class="nx">DataCamp</em></a></figcaption></figure><p id="76af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，正确分类或预测训练数据的每个示例的大 DT 可能不如不完全适合所有训练数据的较小 DT 好。</p><h1 id="0cb7" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated"><strong class="ak">主要 DTs 算法</strong></h1><p id="5854" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">现在您可能会问自己:DTs 如何知道选择哪些特性以及如何拆分数据？为了理解这一点，我们需要了解一些细节。</p><blockquote class="ny nz oa"><p id="acce" class="kz la lv lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated">所有的 DTs 基本上执行相同的任务:它们检查数据集的所有属性，通过将数据分成子组来找到可能给出最佳结果的属性。他们通过将子组分成越来越小的单元来递归地执行这个任务，直到树完成(由特定标准停止)。</p></blockquote><p id="19ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">进行拆分的决策会严重影响树的准确性和性能，对于该决策，DTs 可以使用<strong class="lb iu">不同的算法</strong>，这些算法在树的可能结构(例如，每个节点的拆分数量)、如何执行拆分的标准以及何时停止拆分方面有所不同。</p><p id="5104" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，我们如何定义拆分哪些属性，何时拆分以及如何拆分呢？要回答这个问题，我们必须回顾一下主要的 DTs 算法:</p><h2 id="19f5" class="oi na it bd nb oj ok dn nf ol om dp nj li on oo nl lm op oq nn lq or os np ot bi translated">CHAID</h2><p id="36ae" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">卡方自动交互检测(CHAID)是最古老的 DT 算法方法之一，它产生适合分类和回归任务的<strong class="lb iu">多路</strong>DT(分裂可以有两个以上的分支)。在构建<strong class="lb iu">分类树</strong>(因变量本质上是分类的)时，CHAID 依靠卡方独立性测试来确定每一步的最佳分割。<strong class="lb iu">卡方检验</strong>检查两个变量之间是否存在关系，并应用于 DT 的每个阶段，以确保每个分支与响应变量的统计显著预测值显著相关。</p><blockquote class="lw"><p id="ce87" class="lx ly it bd lz ma mb mc md me mf lu dk translated">换句话说，它选择与因变量相互作用最强的自变量。</p></blockquote><p id="9353" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated">此外，就因变量而言，如果每个预测值的类别彼此之间没有显著差异，则将它们合并。在<strong class="lb iu">回归树</strong>(因变量是连续的)的情况下，CHAID 依靠<strong class="lb iu"> F 检验</strong>(而不是卡方检验)来计算两个总体均值之间的差异。如果 f 检验显著，则创建一个新分区(子节点)(这意味着该分区在统计上不同于父节点)。另一方面，如果目标平均值之间的 f 检验结果不显著，则类别被合并到单个节点中。</p><p id="c56f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CHAID 不会替换丢失的值，并将它们作为一个单独的类来处理，如果合适的话，该类可以与另一个类合并。它还产生了倾向于更宽而不是更深的 DTs(多路特性)，这可能是不切实际的短，并且很难与真实的业务条件相关联。此外，它没有修剪功能。</p><p id="52d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然不是最强大的(在检测最小可能的差异方面)或最快的 DT 算法，但 CHAID 易于管理，灵活，非常有用。</p><p id="a8bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这个<a class="ae ky" href="https://www.r-bloggers.com/chaid-and-r-when-you-need-explanation-may-15-2018/" rel="noopener ugc nofollow" target="_blank">链接</a>中找到 CHAID 和 R 的实现</p><h2 id="1c05" class="oi na it bd nb oj ok dn nf ol om dp nj li on oo nl lm op oq nn lq or os np ot bi translated">手推车</h2><p id="6f69" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">CART 是一种 DT 算法，根据因变量(或目标变量)是分类变量还是数值变量，分别生成<strong class="lb iu">二进制</strong> <em class="lv">分类</em>或<em class="lv">回归</em>树。它以原始形式处理数据(不需要预处理)，并且可以在同一个 DT 的不同部分多次使用相同的变量，这可以揭示变量集之间复杂的相互依赖关系。</p><p id="38ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">分类树</strong>的情况下，CART 算法使用一种称为<em class="lv">基尼杂质</em><strong class="lb iu">T5】的度量来创建分类任务的决策点。<strong class="lb iu"> Gini 杂质</strong>给出了分裂有多精细的概念(节点“纯度”的度量)，通过分裂产生的两个组中类的混合程度。当所有的观察值都属于同一个标签时，就有了一个完美的分类，基尼不纯值为 0(最小值)。另一方面，当所有观察值在不同标签中平均分布时，我们面临最坏情况的分裂结果和 0.5(最大值)的基尼杂质值。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/a9bb1ba5cf7a880112a8136c6d57a9a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*8FVbz8azTkk5Titf"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="nx">On the left-hand side, a high Gini Impurity value leads to a poor splitting performance. On the right-hand side, a low Gini Impurity value performs a nearly perfect splitting</em></figcaption></figure><p id="b074" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">回归树</strong>的情况下，CART 算法寻找最小化<em class="lv">最小平方偏差(LSD) </em>的分割，在所有可能的选项中选择最小化结果的分割。<strong class="lb iu"> LSD </strong>(有时被称为“方差减少”)度量最小化观察值和预测值之间的平方距离(或偏差)的总和。预测值和观察值之间的差异称为“残差”，这意味着 LSD 选择参数估计值，以使残差平方和最小。</p><blockquote class="ny nz oa"><p id="cb46" class="kz la lv lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated">LSD 非常适合度量数据，与其他算法相比，它能够正确捕获更多有关分割质量的信息。</p></blockquote><p id="bf0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CART 算法背后的思想是产生一系列的 DTs，每个 DTs 都是“最优树”的候选者。通过<strong class="lb iu">测试</strong>(使用 DT 以前从未见过的新数据)或执行<strong class="lb iu">交叉验证</strong>(将数据集分成“k”个折叠，并在每个折叠上执行测试)来评估每棵树的性能，从而确定最佳树。</p><p id="0890" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">购物车<strong class="lb iu">不使用内部绩效评估</strong>进行树选择。相反，DTs 的性能总是通过测试或交叉验证来衡量，只有在评估完成后才进行树的选择。</p><h2 id="49e5" class="oi na it bd nb oj ok dn nf ol om dp nj li on oo nl lm op oq nn lq or os np ot bi translated">ID3</h2><p id="1693" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">迭代二分法 3 (ID3)是一种 DT 算法，主要用于产生<em class="lv">分类树</em>。由于在原始数据中构建回归树还没有被证明是如此有效，ID3 主要用于分类任务(尽管一些技术如构建数值区间可以提高其在回归树上的性能)。</p><blockquote class="lw"><p id="bb47" class="lx ly it bd lz ma mb mc md me mf lu dk translated">ID3 分割数据属性(二分法)以找到最主要的特征，以自顶向下的方法迭代地执行这个过程以选择 DT 节点。</p></blockquote><p id="a347" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated">对于分割过程，ID3 使用<em class="lv">信息增益</em>度量来选择对分类最有用的属性。<strong class="lb iu">信息增益</strong>是从信息论中提取的一个概念，指的是一组数据中随机性水平的降低:基本上它衡量的是一个特征给我们提供了多少关于一个类的“信息”。ID3 将总是试图最大化这个度量，这意味着具有最高信息增益的属性将首先分裂。</p><p id="997f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">信息增益与<strong class="lb iu">熵</strong>的概念直接相关，熵是对数据中不确定性或随机性的度量。熵值范围从 0(当所有成员属于同一类或样本完全同质时)到 1(当存在完全随机性或不可预测性，或样本被平分时)。</p><p id="d158" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以这样想:如果你想做一次无偏的抛硬币，有<em class="lv">完全随机性</em>或者熵值为 1(“正面”和“反面”一样，概率各为 0.5)。另一方面，如果你掷一枚硬币，例如一枚两面都有“反面”的硬币，事件中的随机性被消除，熵值为 0(得到“反面”的概率将跳到 1，正面”的概率将降到 0)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/d825fc7ac50fd13716567b18737f5fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*5lJWYOtD23uGMwn0"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="nx">In this graph you can see the relationship between Entropy and the probability of different coin tosses. At the highest level of Entropy, the probability of getting “tails” is equal to the one of getting “heads” (0.5 each), and we face complete uncertainty. Entropy is directly linked to the probability of an event. Example taken from </em><a class="ae ky" href="https://nullpointerexception1.wordpress.com/2017/12/13/entropy-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"><em class="nx">The Null Pointer Exception</em></a></figcaption></figure><p id="335d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这很重要，因为<strong class="lb iu">信息增益是熵</strong>的减少，并且产生最大信息增益的属性被选择用于 DT 节点。</p><p id="596e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是 ID3 也有一些缺点:它不能处理数值属性，也不能处理缺失值，这可能会带来严重的限制。</p><h2 id="90fa" class="oi na it bd nb oj ok dn nf ol om dp nj li on oo nl lm op oq nn lq or os np ot bi translated">C4.5</h2><p id="b9e6" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">C4.5 是 ID3 的继任者，代表了几个方面的改进。C4.5 可以处理连续数据和分类数据，适合生成<em class="lv">回归</em>和<em class="lv">分类树</em>。此外，它可以通过忽略包含不存在数据的实例来处理缺失值。</p><p id="e3fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与 ID3(使用信息增益作为分裂标准)不同，C4.5 使用<em class="lv">增益比</em>进行分裂过程。<strong class="lb iu">增益比</strong>是对信息增益概念的修改，通过在选择属性时考虑分支的数量和大小，减少了具有大量分支的 DTs 的偏差。由于信息增益显示出对具有许多结果的属性的不公平偏好，增益比通过考虑每个分割的内在信息来纠正这种趋势(它基本上通过使用分割信息值来“标准化”信息增益)。这样，<strong class="lb iu">具有最大增益比的属性被选择作为分割属性</strong>。</p><p id="a0f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，C4.5 包括一项名为<em class="lv"> windowing </em>的技术，该技术最初是为了克服早期计算机的内存限制而开发的。<strong class="lb iu">开窗</strong>意味着算法随机选择训练数据的子集(称为“窗口”)，并根据该选择构建 DT。这个 DT 然后被用于分类剩余的训练数据，并且如果它执行了正确的分类，DT 就完成了。否则，所有错误分类的数据点都被添加到窗口中，并且循环重复，直到训练集中的每个实例都被当前 DT 正确分类。由于随机化的使用，这种技术通常导致比标准过程产生的 DTs 更准确的 DTs，因为它捕获了所有“罕见”的实例以及足够多的“普通”病例。</p><blockquote class="lw"><p id="bebf" class="lx ly it bd lz ma mb mc md me mf lu dk translated">C4.5 的另一个能力是它可以<strong class="ak">修剪</strong> DTs。</p></blockquote><p id="961d" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated">C4.5 的剪枝方法基于估计每个内部节点的错误率，并且如果叶的估计错误较低，则用叶节点替换它。简而言之，如果算法估计如果删除一个节点的“子节点”并且该节点成为叶节点，DT 将更准确，那么 C4.5 将删除这些子节点。</p><p id="8267" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该算法的最新版本被称为 C5.0，它是在专有许可下发布的，对 C4.5 进行了一些改进，如:</p><ul class=""><li id="e426" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated"><strong class="lb iu">速度提升:</strong> C5.0 明显快于 C4.5(快了几个数量级)。</li><li id="c237" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu">内存使用:</strong> C5.0 比 C4.5 更有内存效率。</li><li id="2abb" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu">可变误分类成本:</strong>在 C4.5 中，所有的错误都被同等对待，但在实际应用中，一些分类错误比其他分类错误更严重。C5.0 允许为每个预测/实际类对定义单独的成本。</li><li id="3020" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu">更小的决策树:</strong> C5.0 得到的结果与 C4.5 相似，但 DTs 要小得多。</li><li id="c3e1" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu">其他数据类型:</strong> C5.0 可以处理日期、时间，并允许将值标注为“不适用”。</li><li id="5a3f" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><strong class="lb iu"> Winnowing: </strong> C5.0 可以在构造分类器之前自动对属性进行 Winnowing，丢弃那些可能没有帮助或者看起来不相关的属性。</li></ul><p id="f60c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里找到 C4.5 和 C5.0 的比较<a class="ae ky" href="https://rulequest.com/see5-comparison.html" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="8414" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated"><strong class="ak">树的阴暗面</strong></h1><p id="7460" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">当然，DTs 有很多优点。由于其简单性以及易于理解和实施的事实，它们被广泛用于大量行业中的不同解决方案。但是你也需要意识到它的缺点。</p><blockquote class="lw"><p id="3cc8" class="lx ly it bd lz ma mb mc md me mf lu dk translated">DTs 倾向于在他们的训练数据上<strong class="ak">过度拟合</strong>，如果之前显示给他们的数据与之后显示给他们的数据不匹配，他们就会表现不佳。</p></blockquote><p id="e206" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated">他们还受到<strong class="lb iu">高方差</strong>的影响，这意味着数据中的一个小变化会导致一组非常不同的分裂，使解释变得有些复杂。它们遭受固有的不稳定性，因为由于它们的等级性质，顶部分裂中的误差的影响向下传播到下面的所有分裂。</p><p id="9c9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在分类树中，<strong class="lb iu">错误分类观察的后果在某些类</strong>中比其他类更严重。例如，当一个人实际上会心脏病发作时，预测他/她不会心脏病发作可能比反之更糟糕。这个问题在像 C5.0 这样的算法中得到缓解，但在其他算法中仍然是一个严重的问题。</p><p id="6642" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果一些类支配其他类，DTs 也可以创建<strong class="lb iu">偏向</strong>树。这是不平衡数据集中的一个问题(数据集中的不同类具有不同的观察值)，在这种情况下，建议在构建 DT 之前平衡 de 数据集。</p><p id="87e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在回归树的情况下，DTs 只能在他们基于之前看到的数据创建的值的范围内进行预测，这意味着他们对他们可以产生的值有<strong class="lb iu">边界</strong>。</p><p id="dd7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在每一级，DTs 寻找可能的最佳分割，以便优化相应的分割标准。</p><blockquote class="ny nz oa"><p id="5c79" class="kz la lv lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated">但是 DTs 分裂算法不能看到远远超出它们正在操作的当前级别(它们是<strong class="lb iu">【贪婪】</strong>)，这意味着它们在每一步都寻找局部最优而不是全局最优。</p></blockquote><p id="01ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DTs 算法根据一些分裂标准一次生长一个节点的树，并且不实现任何回溯技术。</p><h1 id="2b30" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated"><strong class="ak">群众的力量</strong></h1><p id="f8e1" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">但是好消息是:有不同的策略来克服这些缺点。<strong class="lb iu">集成方法</strong>将几个 DTs 结合起来，以提高单个 DTs 的性能，并且是克服已经描述的问题的重要资源。这个想法是使用相同的学习算法训练多个模型，以获得更好的结果。</p><blockquote class="lw"><p id="2601" class="lx ly it bd lz ma mb mc md me mf lu dk translated">大概执行集合 DTs 的两个最常见的技术是<em class="nx">打包</em>和<em class="nx">助推</em>。</p></blockquote><p id="cc60" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated"><strong class="lb iu">当目标是减少 DT 的方差时，使用 Bagging </strong>(或 Bootstrap Aggregation)。<strong class="lb iu">方差</strong>与 DTs 非常不稳定的事实有关，因为数据的微小变化可能会导致生成完全不同的树。因此，Bagging 的想法是通过并行创建<strong class="lb iu"/>随机数据子集(来自训练数据)来解决这个问题，其中任何观察都有<strong class="lb iu">相同的概率</strong>出现在新的子集数据中。接下来，子集数据的每个集合用于训练 DTs，产生不同 DTs 的集合。最后，使用这些不同 DTs 的所有预测的平均值，这产生了比单个 DTs 更健壮的性能。<strong class="lb iu">随机森林</strong>是 Bagging 的扩展，它需要额外的一步:除了获取数据的随机子集，它还需要随机选择特征，而不是使用所有特征来增长 DTs。</p><p id="9617" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Boosting </strong>是另一种创建预测值集合以减少 DT 方差的技术，但采用了不同的方法。它使用一种<strong class="lb iu">顺序方法</strong>来拟合连续的 DTS，并且在每一步都试图减少来自先前树的错误。使用增强技术，每个分类器都根据数据进行训练，并考虑到先前分类器的成功。在每个训练步骤之后，权重基于之前的表现被重新分配。通过这种方式，<strong class="lb iu">错误分类的数据会增加其权重</strong>以强调最困难的情况，以便后续的 DTs 在训练阶段关注它们并提高其准确性。与 Bagging 不同，在 Boosting 中，观察值是加权的，因此它们中的一些将更频繁地参与新的数据子集。作为该过程的结果，整个集合的组合提高了 DTs 的性能。</p><p id="5a31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 Boosting 中，有几种替代方法可以确定在训练和分类步骤中使用的权重(例如，Gradient Boost、XGBoost、AdaBoost 等)。</p><p id="7371" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里找到这两种技术<a class="ae ky" href="https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/" rel="noopener ugc nofollow" target="_blank">的异同点</a></p></div><div class="ab cl ow ox hx oy" role="separator"><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb"/></div><div class="im in io ip iq"><blockquote class="ny nz oa"><p id="fcde" class="kz la lv lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated">对这些话题感兴趣？在<a class="ae ky" href="https://www.linkedin.com/in/lopezyse/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>或<a class="ae ky" href="https://twitter.com/lopezyse" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我</p></blockquote></div></div>    
</body>
</html>