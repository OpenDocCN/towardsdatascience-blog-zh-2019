<html>
<head>
<title>Word Embedding (Part II)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入(第二部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-embedding-part-ii-intuition-and-some-maths-to-understand-end-to-end-glove-model-9b08e6bf5c06?source=collection_archive---------9-----------------------#2019-04-26">https://towardsdatascience.com/word-embedding-part-ii-intuition-and-some-maths-to-understand-end-to-end-glove-model-9b08e6bf5c06?source=collection_archive---------9-----------------------#2019-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dac0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">直觉和(一些)数学来理解端到端手套模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8cfa589a64828d3a6dc86a395751b803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X8V0kQ3ZFdyWTddK5nsCyw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The power of <strong class="bd ky">GloVe</strong></figcaption></figure><p id="458a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NLP(自然语言处理)的原始问题是将单词/句子编码成计算机处理可理解的格式。向量空间中单词的表示允许 NLP 模型学习单词含义。在我们的<a class="ae lv" href="https://medium.com/@matyasamrouche19/word-embeddings-intuition-and-some-maths-to-understand-end-to-end-skip-gram-model-cab57760c745" rel="noopener">上一篇</a>文章中，我们看到了<strong class="lb iu">的跳格</strong>模型，该模型根据单词在当地<strong class="lb iu">的上下文</strong>来捕捉单词的意思。让我们记住，我们所说的<strong class="lb iu">上下文</strong>是指围绕目标单词的大小为<em class="lw"> n </em>的固定单词窗口。</p><p id="9e09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将研究<strong class="lb iu">手套</strong>模型(全局向量),它被创建来查看单词的局部上下文和全局统计数据以嵌入它们。<strong class="lb iu"> GloVe </strong>模型背后的主要思想是关注文本语料库中单词的<strong class="lb iu">共现概率</strong>(下面的等式 0)以将它们嵌入到有意义的向量中。换句话说，我们要看看在我们所有的文本语料库中，单词<em class="lw"> j </em>在单词<em class="lw"> i </em>的上下文中出现的频率。</p><p id="899f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，假设<strong class="lb iu"> X </strong>是我们的单词-单词共现矩阵(<a class="ae lv" href="https://www.google.com/search?q=word-word+co-occurrence+matrix&amp;safe=strict&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwjJt73es-vhAhVM4eAKHSvEBLYQ_AUIDigB&amp;biw=1440&amp;bih=789#imgrc=RGTzlslBGMJncM:" rel="noopener ugc nofollow" target="_blank">共现矩阵示例</a>)并且<strong class="lb iu"> X_ij </strong>是<strong class="lb iu"> </strong>单词<em class="lw"> j </em>在单词<em class="lw"> i </em>的上下文中出现的次数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/fadbf292f56b31dfc0953c1c05f365d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*mYJKp_EcM6ff15y6NXq8qg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equation 0: The co-occurence probability of a word <strong class="bd ky">j</strong> to occur given a word <strong class="bd ky">i </strong>is the the ratio of the number of times word <strong class="bd ky">j</strong> occurs in the context of word <strong class="bd ky">i</strong> and the number of times any word appears in the context of word <strong class="bd ky">i</strong>.</figcaption></figure><p id="1458" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> GloVe </strong>会查看那些同现概率之间的<strong class="lb iu">比率</strong>来提取单词的内在含义。更具体地说，我们将关注图 1 中表格的最后一行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ly"><img src="../Images/646d41622c3208d630961a5c2aa10102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M7KM98hd9MFlL9HdqYDQIA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 1: The 2 first rows of the table show the probabilities of the words <strong class="bd ky">solid</strong>, <strong class="bd ky">gas</strong>, <strong class="bd ky">water</strong> and <strong class="bd ky">fashion</strong> to occur in the context of the words<strong class="bd ky"> ice </strong>and<strong class="bd ky"> steam. </strong>The last row shows the probabilities ratio which is the key learning under the hood of the <strong class="bd ky">GloVe</strong> model.</figcaption></figure><p id="47e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于与"<em class="lw">冰"</em>相关但不与"<em class="lw">蒸汽</em>"像"<em class="lw">固体"</em>相关的词，比例会高。相反，对于与"<em class="lw">蒸汽</em>相关但不与"<em class="lw">冰</em>"相关的单词，该比率将较低，而对于与两者都相关或都不相关的单词，如"<em class="lw">水</em>"和"<em class="lw">时尚</em>"，该比率将接近 1。<br/>乍一看，<strong class="lb iu">同现概率比</strong>收集了比原始概率更多的信息，并且更好地捕捉了“<em class="lw">冰”</em>和“<em class="lw">蒸汽”之间的关系。的确，只看原始概率，单词"<em class="lw">水</em>"最好地代表了"<em class="lw">冰"</em>和"<em class="lw">蒸汽"</em>的意思，我们将无法区分这两个单词的内在含义。</em></p><p id="a93a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们已经理解了<strong class="lb iu">共现概率比</strong>捕获了关于单词关系的相关信息，那么<strong class="lb iu"> GloVe </strong>模型旨在构建一个函数<strong class="lb iu"> <em class="lw"> F </em> </strong>，该函数将在给定两个单词向量<strong class="lb iu"> w_i </strong>和<strong class="lb iu"> w_j </strong>以及一个上下文单词向量<strong class="lb iu"> w_k </strong>作为输入的情况下预测这些比率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/58063152a58f304bd3b1dd156273905d.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*3dXwcN4fQ2sblxgNJypXug.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equation 1: The <strong class="bd ky">GloVe </strong>model will learn meaningful word vectors representations <strong class="bd ky">w_i</strong>, <strong class="bd ky">w_j</strong> and <strong class="bd ky">w_k</strong> to feed <strong class="bd ky">F</strong> and correctly predict the probabilities ratios.</figcaption></figure><p id="75d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望对<strong class="lb iu"> GloVe </strong>有一个高层次的概述的读者可能想要跳过下面的等式(从等式 2 到等式 6 ),这些等式更深入地理解了<strong class="lb iu"> GloVe </strong>模型如何构造这个<strong class="lb iu"> <em class="lw"> F </em> </strong>函数来学习单词向量表示。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><p id="4d2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看这个<strong class="lb iu"> <em class="lw"> F </em> </strong>函数是如何构建的，一步一步地去捕捉最终公式背后的逻辑，这个公式乍一看相当复杂(方程式 6)。</p><p id="e21c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要比较向量<strong class="lb iu"> w_i </strong>和<strong class="lb iu"> w_j </strong>这两个向量是线性结构，最直观的方法就是通过相减，就这么做吧。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/bbe413af88b93ce15a992324a9faa783.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*ps3NpOAzrPrUuWYz2FbjBA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equation 2: Comparing two vectors by making the difference.</figcaption></figure><p id="51a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有两个向量作为<strong class="lb iu"> <em class="lw"> F </em> </strong>的输入，一个标量在等式的右边，从数学上讲，如果我们保持这种方式，它会增加我们想要构建的线性结构的复杂性。将标量值与标量值关联起来更容易，这样我们就不必玩向量维数了。因此<strong class="lb iu">手套</strong>模型使用两个输入向量的点积。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/053079f12dbd57c983b722e84dbac526.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*effkZ-2tpfQ9cTGQZCwM_g.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equation 3: Scalar values to scalar values thanks to the dot product.</figcaption></figure><p id="0736" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一直以来，我们将单词向量与上下文单词向量分开。但是，这种分离只是一个角度的问题。的确，如果说“<em class="lw">水</em>”是“<em class="lw">汽</em>”的语境词，那么“<em class="lw">汽</em>”就可以是“<em class="lw">水</em>”的语境词。在构建<strong class="lb iu"><em class="lw">【F</em></strong>时，必须考虑到<strong class="lb iu"> X </strong>矩阵(我们的共生矩阵)的对称性，我们必须能够切换<strong class="lb iu"> w_i </strong>和<strong class="lb iu"> w_k </strong>。首先我们需要<strong class="lb iu"> <em class="lw"> F </em> </strong>是一个<a class="ae lv" rel="noopener" target="_blank" href="/emnlp-what-is-glove-part-iii-c6090bed114">同态</a> (F(a+b)=F(a)F(b))。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/9db0cbe514d3b18f66b0093aff0a690a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R6ez-IARb8jNmS2H_jagDg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equation 4: Using the homomorphism property of <strong class="bd ky">F </strong>to associate word vectors dot product (which can be interpreted as similarities between words) to the probability they occur in a same context.</figcaption></figure><p id="733d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lw"> exp </em>函数是方程 4 的解，<em class="lw">exp</em>(a-b)=<em class="lw">exp</em>(a)/<em class="lw">exp</em>(b)，我们就用它吧。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mk"><img src="../Images/7fbfe52e90475cc80a7a4c26b5d6ebb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*myAzhCwkyNNGZ03imSDnkg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equation 5: Almost symmetric if there was not <strong class="bd ky">b_i </strong>term.</figcaption></figure><p id="8fc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了恢复对称性，为矢量<strong class="lb iu"> w_k </strong>增加一个偏置<strong class="lb iu"> b_k </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/dfb68b7855401e9205575ac85129cf90.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*1y8WHvib7JnhY3XQ5hBpgw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equation 6: We can express our word vectors given corpus statistics and symmetry is respected (we can switch <strong class="bd ky">w_i</strong> and <strong class="bd ky">w_k</strong>).</figcaption></figure></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><p id="89e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们的<strong class="lb iu"> <em class="lw"> F </em> </strong>函数，我们现在能够使用我们的词向量表示来定义成本/目标函数(等式 7)。在训练期间<strong class="lb iu">手套</strong>将学习正确的字向量<strong class="lb iu"> w_i </strong>和<strong class="lb iu"> w_j </strong>以最小化这个<strong class="lb iu">加权最小平方</strong>问题。事实上，必须使用权重函数<em class="lw"> f(X_ij) </em>来限制非常常见的同现(如“this is”)的重要性，并防止罕见的同现(如“下雪的撒哈拉”)具有与平常相同的重要性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/547892822cae484a1d7a429a1d75da8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*oDcCpHSK7-Lt06zoW4NPMA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equation 7: Final <strong class="bd ky">GloVe</strong> Equation</figcaption></figure><p id="26ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，<strong class="lb iu">手套</strong>模型为我们要求他执行的单词类比任务使用了一个有意义的知识来源:同现概率比<strong class="lb iu">。<strong class="lb iu"> </strong>然后，它构建一个目标函数<strong class="lb iu"> <em class="lw"> J </em> </strong>将单词向量关联到文本统计。<strong class="lb iu"> <em class="lw"> </em> </strong>最后，<strong class="lb iu">手套</strong>最小化这个<strong class="lb iu"> <em class="lw"> J </em> </strong>功能<strong class="lb iu"> <em class="lw"> </em> </strong>通过学习有意义的单词向量来表示。</strong></p><p id="1edb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瞧啊！</p><p id="38ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考资料和其他有用资源:<br/> </strong> - <strong class="lb iu"> </strong> <a class="ae lv" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">原始手套论文</a> <br/> - <a class="ae lv" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/syllabus.html" rel="noopener ugc nofollow" target="_blank">斯坦福 NLP 资源</a> <br/> - <a class="ae lv" href="http://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/" rel="noopener ugc nofollow" target="_blank">很好解释的文章比较 Word2vec 与 Glove </a></p></div></div>    
</body>
</html>