<html>
<head>
<title>Recommender system using Bayesian personalized ranking</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用贝叶斯个性化排序的推荐系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recommender-system-using-bayesian-personalized-ranking-d30e98bba0b9?source=collection_archive---------3-----------------------#2019-02-04">https://towardsdatascience.com/recommender-system-using-bayesian-personalized-ranking-d30e98bba0b9?source=collection_archive---------3-----------------------#2019-02-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f00d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用矩阵分解直观理解贝叶斯个性化排序优化准则</h2></div><p id="6e46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将讨论贝叶斯个性化排序(BPR)，这是推荐系统中使用的著名的学习排序算法之一。在深入 BPR 算法的细节之前，我将概述一下推荐系统是如何工作的，以及我的音乐推荐系统项目。这将有助于一些第一次阅读推荐系统的人，并作为其他人的复习资料。</p><h1 id="350c" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated"><strong class="ak">内容:</strong></h1><ol class=""><li id="b83c" class="lt lu iq kh b ki lv kl lw ko lx ks ly kw lz la ma mb mc md bi translated">概观</li><li id="2d40" class="lt lu iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">学习推荐系统</li><li id="a28c" class="lt lu iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">使用交替最小二乘法的矩阵分解</li><li id="267c" class="lt lu iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">使用贝叶斯个性化排序的矩阵分解</li></ol><h1 id="add9" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated"><strong class="ak"> 1。概述</strong></h1><p id="eddd" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在网飞奖诞生的早期，大多数推荐系统都是基于显式数据(评分数据)的，用户明确地给出评分来表达他们的观点。从那以后，很多事情都变了。随着数据收集技术的增强和在客户中给出明确评级的趋势的减少，隐式反馈数据在学术界和工业界都变得更加流行，以构建健壮的推荐系统。</p><h2 id="2dbc" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">隐含数据</strong></h2><p id="40f4" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">隐式数据就是我们从客户那里收集的反馈，包括点击量、购买量、浏览量等。隐式数据的主要特征是</p><ol class=""><li id="88ab" class="lt lu iq kh b ki kj kl km ko my ks mz kw na la ma mb mc md bi translated"><strong class="kh ir">无负面反馈:</strong>在显性数据中，客户明确表达了他们的正面和负面反馈。其余没有值的数据点被视为缺失值。但在隐性数据中，我们只有正面反馈，如点击、购买等，没有办法判断缺失的数据是因为客户不喜欢该商品还是没有意识到这一点</li><li id="fa3e" class="lt lu iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated"><strong class="kh ir">固有噪声:</strong>尽管隐式数据固有噪声，但其庞大的数据量弥补了这一缺陷，有助于构建健壮的推荐系统。</li><li id="092d" class="lt lu iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated"><strong class="kh ir">偏好与信心:</strong>在显性数据中，评级专指客户的偏好，用数值表示偏好的大小。在隐含数据的情况下，数值通常指的是频率，它不一定反映客户偏好的大小。因此，需要推导出显示客户信心的信心度量</li></ol><h2 id="16db" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">潜在因素模型</strong></h2><p id="c7b5" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">对于隐式数据，潜在因素模型提供了邻居模型(基于相似性度量的模型)的替代方法，有助于揭示用户和项目的潜在特征。这种模型的例子是矩阵分解、潜在语义模型、潜在狄利克雷分配等。其中，矩阵分解由于其吸引人的准确性和可扩展性而变得更加流行。</p><p id="cf3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然一个简单的奇异值分解为此目的工作，但它遭受过拟合，这将我们带到下一组涉及正则化的模型。</p><h1 id="3ef7" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated"><strong class="ak"> 2。学习推荐系统</strong></h1><p id="06f1" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">学习推荐系统就是这样一种方法，其中矩阵分解可以转化为一个带有损失函数和约束的优化问题。在这种方法中，我们在优化过程中从一系列推荐器中选择最佳推荐器。这种方法的最终结果是一个潜在因素模型，它帮助我们使用参数估计方法来揭示用户和项目的潜在特征。</p><p id="70e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了详细说明这一点，让我们看看学习推荐系统的结构</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nb"><img src="../Images/17d8eb1d69933cb7018cbd839855d2b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-Fnk5UIiBdQdh74GThFDw.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Figure 1</figcaption></figure><p id="9e3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上图所示，学习推荐系统有 3 个组成部分:</p><h2 id="286f" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak"> 1。型号:</strong></h2><p id="5cd7" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">模型可以是矩阵分解模型或线性回归模型。它有一些参数，如矩阵分解中的矩阵，我们将在此过程中进行优化。在我们的例子中，模型和参数一起构成了推荐系统</p><h2 id="682f" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak"> 2。效用函数或损失函数:</strong></h2><p id="4a6e" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">这就像我们在机器学习算法中的任何其他损失函数一样，将被最小化以达到最优解</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1b4a2c2bbb624164725ada83ffaa89e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*uXfUtXZNlKIlGemX6yDplg.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Equation 1</figcaption></figure><p id="6861" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">θ:推荐模型的参数，如矩阵分解中的用户矩阵和项目矩阵。</p><p id="f5f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">g(θ):我们试图最小化的损失函数</p><h2 id="ff5b" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak"> 3。优化算法:</strong></h2><p id="e466" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">我们可以选择任何符合我们目的的优化算法。对于隐式数据，最好的优化算法之一是交替最小二乘法</p><h1 id="8376" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">3.使用交替最小二乘法的矩阵分解</h1><p id="d379" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">这些推荐系统的性能通常取决于所使用的优化算法。</p><p id="ee56" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你猜对了。下一部分是关于优化算法，我们越来越接近贝叶斯个性化排名。</p><h2 id="3880" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">交替最小二乘法</strong></h2><p id="f015" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">交替最小二乘法(ALS)就是这样一种算法，由胡一帆、耶胡达·科伦和克里斯·沃林斯基在针对隐式反馈的协作过滤中提出。如果你不知道这个算法，我强烈建议你阅读这篇由<a class="ae ns" href="https://medium.com/@victorkohler" rel="noopener"> Victor </a>撰写的<a class="ae ns" href="https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe" rel="noopener">文章</a>，以详细了解 ALS。然而，为了保持贝叶斯个性化排序方法的逻辑连续性，我将简要概述 ALS 模型。</p><p id="360c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ALS 是一种算法，通过在特定迭代期间保持一个向量不变来迭代地优化用户和项目潜在向量。</p><p id="8153" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在他们的论文中，胡一帆和其他人提出了这样一个概念:在执行一个动作时，把自信归因于用户的选择。他们制定了一个新的平方损失函数，其中包括偏好和信心指标，反过来将使用 ALS 方法进行优化。</p><p id="0143" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">损失函数:</strong></p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nt"><img src="../Images/17c6c87c6c2d19b6978081000f5e860b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OWN5hHaabNiX1imBm5Hnrg.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Equation 2</figcaption></figure><p id="665b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">徐</strong>和<strong class="kh ir">易</strong>分别是用户潜在向量和项目潜在向量。</p><p id="8ebf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">崔:</strong>置信度度量</p><p id="d9a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> pui : </strong>偏好指标</p><h2 id="3c87" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">实施 ALS 创建音乐推荐系统</strong></h2><p id="7646" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">为了更好地理解使用 ALS 的推荐系统，我实现了这些概念来创建一个使用开源音乐数据集(lastfm 数据集)的音乐推荐系统。在构建这个音乐推荐系统时，我从 Ben Frederickson 和 Jesse Steinweg-Woods 的作品中获得了灵感和一些代码帮助。</p><p id="ffd5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以在我的 github 资源库中找到该项目的代码。</p><div class="nu nv gp gr nw nx"><a href="https://github.com/akhilesh-reddy/Implicit-data-based-recommendation-system" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">akhilesh-Reddy/基于隐含数据的推荐系统</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">基于用户所听歌曲的数量和类型的音乐推荐系统|实现了约 90%的 AUC</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">github.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol nl nx"/></div></div></a></div><h2 id="a8cf" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">结果</strong></h2><p id="9d64" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">我使用 AUC 度量作为这个推荐系统的评估标准。值得注意的是，这个数据集非常稀疏，当我对 40000 名用户和 100000 名艺术家进行采样时，稀疏度为 99.9%。尽管数据集中有很大的稀疏性，推荐系统给出了大约 90%的 AUC 值。</p><p id="9d79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管该算法在查找相似艺术家方面表现更好，但在向特定用户推荐艺术家方面，我并没有获得令人满意的结果。这就把我带到了这篇文章的最后一部分。</p><h2 id="81a3" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">为什么要贝叶斯个性化排名？</strong></h2><p id="db64" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">尽管胡一帆的 ALS 方法使用置信度和偏好度量减少了缺失数据的影响，但它并不直接优化其用于排名的模型参数。相反，它优化预测一个项目是否被用户选中。贝叶斯个性化排名优化标准涉及成对的项目(两个项目的用户特定顺序),以便为每个用户提供更个性化的排名。</p><blockquote class="om"><p id="ecde" class="on oo iq bd op oq or os ot ou ov la dk translated"><em class="ow">首先，很明显这种优化是在实例级(一项)而不是像 BPR 那样在对级(两项)。除此之外，它们的优化是最小二乘法，已知对应于正态分布随机变量的最大似然估计。然而，项目预测的任务实际上不是回归(定量)，而是分类(定性)，因此逻辑优化更合适。— </em></p><p id="adc1" class="on oo iq bd op oq or os ot ou ov la dk translated">Steffen Rendle，Christoph Freudenthaler，Zeno Gantner 和 Lars Schmidt-Thieme 在 BPR:根据隐式反馈进行贝叶斯个性化排序</p></blockquote><h1 id="d865" class="lb lc iq bd ld le lf lg lh li lj lk ll jw ox jx ln jz oy ka lp kc oz kd lr ls bi translated"><strong class="ak">使用贝叶斯个性化排序的矩阵分解</strong></h1><p id="ea02" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">个性化排序的主要任务是向用户提供排序后的项目列表。在下一节中，我总结了这种方法，尽量减少数学方程的数量，以方便第一次阅读的读者。关于数学的详细理解可以参考<a class="ae ns" href="https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>。</p><h2 id="63a6" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">数据准备:</strong></h2><p id="1ca6" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">设 U 是所有用户的集合，I 是所有项目的集合。下图显示了在一般项目推荐者的情况下，隐式数据是如何处理的。</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pa"><img src="../Images/79d2a52e43465d116f20d7cc8254e7f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aBthma8yMZ-GjOySu8taCA.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Figure 2</figcaption></figure><p id="3d7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常的方法是预测项目的个性化得分 xui，该得分反映了用户对该项目的偏好。之后，项目将根据该分数进行排名。如上图所示，用户和项目之间的所有现有交互被标记为正类(1)，其余交互被标记为负类(0)。</p><p id="7847" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着，如果我们的模型与训练数据完全吻合，它将以同样的方式对待训练数据中不存在的所有相互作用，因为它们都被标记为 0。这种方法在将来的推荐中不一定考虑排名。</p><p id="e446" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 BPR 方法中，<em class="pb">项目对</em>将被视为训练数据，而不是取一个项目。将基于这些用户-项目对的排名来执行优化，而不是仅仅根据用户-项目交互来评分。将被考虑的数据集被公式化如下</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/e29d59c697d5ee464e80e9d77530afdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*38aflsMiwPIb0foeogZStA.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Equation 3</figcaption></figure><p id="9660" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(u，I，j) ∈ DS 的语义是假设用户 u 更喜欢 I 而不是 j。</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pd"><img src="../Images/f6ca22714ad0851006fc4423bc6a8d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*us7ckmUFdFe55Dol77qAMg.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Figure 3</figcaption></figure><p id="bc0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，为训练数据生成的三元组是一对项目之间的用户特定的成对偏好。</p><p id="6b48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上图中，用户 u1 已经查看了项目 i2，但没有查看项目 i1，因此该算法假设该用户更喜欢项目 i2 而不是 i1( i2 &gt; i1 ),并给出一个正号。无法推断用户对既已看到又显示为<strong class="kh ir">的项目的偏好。</strong>马克。对于用户还没有看到的两个项目也是如此(例如，用户 u1 的项目 i1 和 i4)。相反，您可以观察到(i1，j2)的负号，因为用户更喜欢 item2 而不是 item1。</p><h2 id="6554" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">业务流程重组选项</strong></h2><p id="e24d" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">像在任何贝叶斯方法，我们有一个似然函数，先验概率和后验概率。</p><blockquote class="om"><p id="8943" class="on oo iq bd op oq or os ot ou ov la dk translated"><strong class="ak">为所有项目 i ∈ I 找到正确的个性化排序的贝叶斯公式是最大化以下后验概率，其中θ表示任意模型类的参数向量(例如，矩阵分解)。</strong></p></blockquote><h2 id="54c6" class="mm lc iq bd ld mn pe dn lh mp pf dp ll ko pg ms ln ks ph mu lp kw pi mw lr mx bi translated"><strong class="ak">似然函数</strong></h2><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/8d38a1fee7e815b7cb26ec5a6231b1e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*tlpPqk5QZjR67_0vzJqhfA.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Equation 4</figcaption></figure><p id="9107" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，&gt; u 是用户 u 的期望但潜在的偏好结构。同样重要的是要注意，p( &gt;u | θ)是用户特定的似然函数。</p><p id="ba7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设用户将独立行动，并且特定用户的每对项目(I，j)的排序独立于每对其他项目的排序，我们可以将用户更喜欢项目 I 而不是项目 j 的个体概率公式化如下:</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pk"><img src="../Images/6b47e29ece6ac64b18268035ab8512f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I4cx53-qwZTXLMkOpB5Y4A.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Equation 5</figcaption></figure><p id="61a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述等式中的 x^uij(θ)是表示用户 u、项目 I 和项目 j 之间的关系的实值函数，并且通常通过使用矩阵分解模型来计算。换句话说，捕捉用户 u、项目 I 和项目 j 之间的关系的分数将使用三元组训练数据和矩阵分解来计算，并且被包装在 sigmoid 函数中。该 sigmoid 函数给出了在该过程中将被优化的个体概率。</p><h2 id="366e" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">先验概率</strong></h2><p id="0716" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">p(θ)是先验概率，它是具有零均值和方差-协方差矩阵的正态分布</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/6e8deccf7980d4bd775cf28f01878bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*R9RBf7hPDVvfghn7RTpUkg.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Equation 6</figcaption></figure><h2 id="d236" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated">BPR 标准</h2><p id="6b02" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">以下等式是最终的 BPR-OPT 标准，必须进行优化</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/9b66738c29da7409342788e561cf9a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*9dw0AgeCvCfJp9z9iPbnnA.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Equation 7</figcaption></figure><p id="bb2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中λθ是模型特定的正则化参数</p><h2 id="f6a5" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">为什么要记录可能性？</strong></h2><p id="f2dc" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">现在我们有了概率函数，评估它的一个常用方法是对数似然函数。</p><p id="436f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用对数函数的原因是数值稳定性。事实证明，对于非常大的数据集，我们有可能获得非常低的概率，而系统很难记录这些概率。通过取概率的对数，我们得到一个总和，而不是每个点的概率的乘积，这将有效地存储在计算机存储器中。</p><h2 id="f0da" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">类比 AUC 优化</strong></h2><p id="1867" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">对于不了解 AUC 的人来说，它是 ROC 曲线下的面积，该曲线以真阳性率作为 y 轴，假阳性率作为 x 轴，用于所考虑的模型的各种阈值。换句话说，对于一组项目推荐，我们可以通过计算每个阈值的好项目推荐百分比和坏项目推荐百分比来绘制 ROC 曲线。</p><p id="523a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有趣的是，AUC 指标是一个基于排名的指标，也可以被视为正确排名点的百分比。</p><p id="ece0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从数学上来说，分类器的 AUC 等于该分类器将随机选择的正例排序高于随机选择的负例的概率。</p><p id="2f7d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为我们的用例实现 AUC 定义为我们提供了以下等式:</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/7a76840f91462b0cb85edcd2f88126f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*V78XVkhNNUNwhD_YkJ5cHg.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Equation 8</figcaption></figure><p id="1284" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">s(I；u)&gt; S(j；u)是我们之前讨论过的 x^uij 分数。</p><p id="8962" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">函数 1{foo}是一个不可微的 Heaviside 函数。优化 AUC(基于秩的度量)的一般做法是用类似 sigmoid 函数的可微分函数代替 Heaviside 函数。在我们的例子中，我们选择了对数(sigmoid)。这个概念总结在下面的<a class="ae ns" href="http://www.ipipan.waw.pl/~sj/pdf/PKDD07web.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">论文</strong> </a> <strong class="kh ir">。</strong></p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi po"><img src="../Images/02e7991cd37b5070dd5851304d9fd059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*Yo-AyyqIUVbXX29eTa4LeA.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Heaviside function</figcaption></figure><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pp"><img src="../Images/a1bf1ed8610dfdcbcf367d9e1f596e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ecCtRvOxCq30km1GTmHWQ.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Figure 4</figcaption></figure><h2 id="6ba5" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated">AUC 与 BPR-OPT 的比较</h2><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/1640cfc1e1a4bff720b8462d4c4dafe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*AqDHE6c_gnddB80cxkpLRg.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Equation 10</figcaption></figure><p id="6552" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">针对 BPR-OPT 标准的优化将类似于针对 AUC 的优化，AUC 是一种基于排名的指标，我们可以从上面的等式中看出。</p><h2 id="4274" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">使用自举的梯度下降</strong></h2><p id="6dd0" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在这种情况下，当标准是可微的时，明显的选择是在优化标准上使用梯度下降。但在这种情况下，模型遍历训练对的顺序至关重要。遵循一般的随机梯度方法将导致较差的收敛性，因为在相同的用户-项目对上有如此多的连续更新，即对于一个用户-项目对(u，I)，有许多具有(u，I，j) ∈ DS 的 j。</p><p id="dfb8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了解决这个问题，在该算法中采用了自举方法。不是以连续的顺序获取数据点，即对于特定用户的所有项目(I)和项目(j ),而是随机获取数据点导致更快的收敛。BPR 方法的作者已经从经验上证实了这一点。</p><h2 id="6274" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated"><strong class="ak">通过使用梯度下降优化 BPR-OPT 标准计算 xuij</strong></h2><p id="ff84" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">这篇文章的最后一步是了解矩阵分解模型如何计算 x^uij 分数。矩阵分解通常试图模拟用户对某个项目的隐藏偏好。它是每个用户项目对(u，I)的实数 x^ui。</p><p id="db0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我们的数据中有三个一组，估计量 x^uij 将分解如下:</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/c5249ce99e32582c146fcb75dcc8eda9.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*h-a8azLQsUqoQzOx8mK3Wg.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Equation 11</figcaption></figure><p id="ad8c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以使用任何标准的协同过滤模型代替矩阵分解来预测 x^ui 和 x^uj.BPR-OPT 标准是对任务进行排序的最佳标准，因为我们正在对两个预测的差异进行分类(x^ui -x^uj).</p><p id="8ede" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Ben Frederickson 的隐式打包有一个用于 BPR 的模块，对于任何数据集都很容易实现。您可能想看看如何用 python 为您的推荐系统实现这种方法。</p><p id="640a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是所有的乡亲。我希望你现在对贝叶斯个性化排序方法有了很好的理解。我将把这作为我的音乐推荐系统的下一步来实现，并检查它在我的推荐排名方面的表现。</p><p id="a203" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请继续关注未来几周关于数据科学和数据可视化统计的更多帖子！</p><h2 id="00d1" class="mm lc iq bd ld mn mo dn lh mp mq dp ll ko mr ms ln ks mt mu lp kw mv mw lr mx bi translated">参考</h2><p id="4496" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">[1]<a class="ae ns" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.3727&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">http://citeseerx.ist.psu.edu/viewdoc/download?doi = 10 . 1 . 1 . 2 . 3727&amp;rep = rep 1&amp;type = pdf</a></p><p id="0453" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]<a class="ae ns" href="https://www.kaggle.com/otmanesakhi/the-rank-statistic-differentiable-auc-estimate" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/otmanesakhi/the-rank-statistic-differentiable-AUC-estimate</a></p><p id="b2f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]http://www.benfrederickson.com/matrix-factorization/<a class="ae ns" href="http://www.benfrederickson.com/matrix-factorization/" rel="noopener ugc nofollow" target="_blank"/></p><p id="01b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4]<a class="ae ns" href="http://www.benfrederickson.com/fast-implicit-matrix-factorization/" rel="noopener ugc nofollow" target="_blank">http://www . benfrederickson . com/fast-implicit-matrix-factorization/</a></p><p id="8aa3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5]<a class="ae ns" href="https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe" rel="noopener">https://medium . com/radon-dev/als-implicit-collaborative-filtering-5ed 653 ba 39 Fe</a></p><p id="ee6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae ns" href="https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf</a></p><p id="1780" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae ns" href="http://yifanhu.net/PUB/cf.pdf" rel="noopener ugc nofollow" target="_blank">http://yifanhu.net/PUB/cf.pdf</a></p><p id="1480" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[8]h<a class="ae ns" href="https://hpi.de/fileadmin/user_upload/fachgebiete/naumann/lehre/SS2011/Collaborative_Filtering/pres1-matrixfactorization.pdf" rel="noopener ugc nofollow" target="_blank">ttps://HPI . de/file admin/user _ upload/fach gebiete/naumann/lehre/ss 2011/Collaborative _ Filtering/pres 1-matrix factorization . pdf</a></p></div></div>    
</body>
</html>