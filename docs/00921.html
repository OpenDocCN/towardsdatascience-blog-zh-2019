<html>
<head>
<title>Finding Familiar Faces with a Tensorflow Object Detector, Pytorch Feature Extractor, and Spotify’s Annoy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Tensorflow 对象检测器、Pytorch 特征提取器和 Spotify 的 airy 查找熟悉的面孔</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/finding-familiar-faces-with-a-tensorflow-object-detector-pytorch-feature-extractor-and-spotifys-3e78858a8148?source=collection_archive---------31-----------------------#2019-02-11">https://towardsdatascience.com/finding-familiar-faces-with-a-tensorflow-object-detector-pytorch-feature-extractor-and-spotifys-3e78858a8148?source=collection_archive---------31-----------------------#2019-02-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="9370" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在一些不同的帖子中，我已经使用对象检测器和类似的暹罗网络将面部识别管道或类似的东西放在一起进行特征提取，以找到类似的图像，但我还没有真正深入研究如何以更实际的大规模方式使用这些特征向量。我在这里的意思是，你不希望在整个数据库中进行成对的比较，这是不实际的。所以在这篇文章中，我将演示如何使用 Spotify 的近似最近邻居库(<a class="ae ko" href="https://github.com/spotify/annoy" rel="noopener ugc nofollow" target="_blank">骚扰</a>)来根据一些初始图像找到相似的游戏角色。</p><p id="4ded" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Annoy 是 Spotify 开发的一个库，用于帮助大规模推荐音乐。它还有一些其他有用的属性，因为您可以预先计算索引，稍后可以调用这些索引来查找类似的项目，这在您进行大规模工作时很有帮助。它通过传递你所关心的任何东西(音乐、文本、动画人物的图片)的某种向量表示来工作，并基于此建立模型和生成索引。</p><p id="0b18" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个管道的最简单的版本是使用一些神经网络进行特征提取，然后将这些特征向量传递给 airy，然后使用 airy 来查找相似的图像。这取决于您的用例。在这个具体的例子中，我想看看我是否会因为在不同的图像中返回相似或相同的字符而感到恼火。我对此进行了测试，并最终通过建立面部识别网络和传递面部生成的特征向量来尝试和集中管道以找到相似的字符，从而完成了两个阶段的方法。</p><p id="67d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于这个项目的数据集，我将使用相同的命运大订单数据集，我已经在一些职位。主要是因为它是我最近建立的约 410 张图片的小数据集中最大的，所以它是这类事情的一个很好的测试案例。</p><p id="3678" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">进入这个项目时，我发现关键(像许多数据科学项目一样)是找到一种好的方法来表示我想要解决的问题的数据集。对于这种类型的任务，将 3D 像素阵列压缩成 1D 特征向量在机械上是正确的，但是确保最终的 1D 特征向量描述了您想要的也是重要的。</p><blockquote class="kp kq kr"><p id="cf9f" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">请随意查看我为这个<a class="ae ko" href="https://github.com/sugi-chan/fgo_face_similarity" rel="noopener ugc nofollow" target="_blank">在这里</a>使用的笔记本。这些笔记本不是超级可读的，因为我很快就把东西处理完了。我也不像往常一样在 repo 中提供模型/数据集文件。</p></blockquote><h1 id="44c3" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">版本 1:完整图像特征提取</h1><p id="b1e9" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">因为这是最容易做到的事情，并且是为后面的测试设置我的通用管道的好方法，所以我使用了 pytorch 预训练网络(一个 ResNet101，它为每个矢量生成 1000 个特征以输入到 aroy ),并向它传递完整的图像，然后由 aroy 使用这些图像来查找类似的图像。这个初始过程产生了如下结果。最左边的图像是基础图像，接下来的 4 个是“数据库”中最相似的 4 个。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/4d443ef8aae29bb8b75509e84eb4b43e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g5WuzIqSVOt8eP-rUBRIxQ.jpeg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Left most image is the seed image followed by 4 of the top ones. (there was a duplicate in the dataset for this one)</figcaption></figure><p id="892a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看起来特征提取器从原始图像中获得了许多类似的深色细节，其中红色和黑色是焦点。然而，在这种情况下，我试图看看我是否可以得到类似的字符，所以这不是最好的输出。</p><p id="7ea5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是另一个图像示例。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mp"><img src="../Images/8084b328ce0f1adb2b696be0f56f295f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10PEKkyU75maRv327KHvgg.jpeg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Left most image is the seed image followed by 4 of the top ones.</figcaption></figure><p id="3bab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一个有点棘手，因为在主图像中有两个角色，但是当整个图像被传递到烦人的模型中时，相似的图像有相似的颜色味觉(可能…)。</p><p id="f14c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，从机械上来说，这一过程是可行的，但我要说的是输出很差。下一步是想办法让输出更连贯。在这种情况下，因为我想建立的是一些显示类似的字符给定一些基础图像，为什么不尝试，使它的图像是基于只是字符，而不是完整的图像？我可以想到很多方法来做到这一点，但对我来说最简单的是建立一个单一的类对象检测器来识别图像中的人脸，将该人脸传递到一个预训练的网络进行特征提取，然后将该特征向量传递到 are。当骚扰拉相似的图像，我可以让它返回基本图像，所以基本上它会找到数据库中的哪些面部与新提取的面部最相似，并返回这些面部出现的图像。</p><h1 id="bef6" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">基于人脸识别的相似度</h1><p id="3e46" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">对于这个项目，我建立了一个简单的 1 类对象检测器，只识别图像中的动画人物的脸。我使用 Labelimg 来快速注释图像，我非常确定我只花了大约 20 分钟来标记我的测试和训练分割的 400 张图像。在这一点上，我已经做了相当多的工作，只有一个单独的类可以大大加快这个过程。在训练探测器时，我使用了一个更快的 RCNN-Inceptionv2 模型，该模型具有来自 Tensorflow 模型<a class="ae ko" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" rel="noopener ugc nofollow" target="_blank"> zoo </a>的 coco 权重，并训练该模型大约 3 个小时。我从周五午夜开始训练，一直到周六凌晨 3 点左右，这打乱了我的睡眠时间表，因为那时我还在忙其他的事情。</p><div class="ma mb mc md gt ab cb"><figure class="mq me mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><img src="../Images/d6249d9e93605864c12a186882d60b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*apVMnIgdZ7h6u5Ecu6KzFw.jpeg"/></div></figure><figure class="mq me mw ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><img src="../Images/9a4fcc3eb43b133c9aa3d5f3ef24be6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*u-18nTSapDuL4mASwhmzjw.jpeg"/></div></figure><figure class="mq me mx ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><img src="../Images/a7e5f402459b2361324f721447f11cc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*cVJBtJVcScqhmLfCoHMRhw.jpeg"/></div></figure></div><div class="ab cb"><figure class="mq me my ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><img src="../Images/22869c21b4bf900afd650ce53d08c020.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*OH3IFJMNaVffB-0yhU3Xwg.png"/></div></figure><figure class="mq me mz ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><img src="../Images/9c3d847825b74896b9e67125b8e74ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*63Ii9rXP7rVHuLSzEEtpDw.jpeg"/></div></figure><figure class="mq me na ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><img src="../Images/7e7c87c96b8746011adcc1f68dc83c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*1zpVJCAUt8vahSPNttLYOQ.jpeg"/></div></figure></div><div class="ab cb"><figure class="mq me nb ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><img src="../Images/98a9539a59c17051a5abc980bc3a20dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*93stNnKHvLkmjp393sHLHw.jpeg"/></div></figure><figure class="mq me nc ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><img src="../Images/12501204c8dae3f41c672476d909dd5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*3WZ4oHVoKsE8ORzNR_f5Rg.png"/></div></figure><figure class="mq me nd ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><img src="../Images/50dba9b6c409f5702d1c9a1e4dd10c9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*PgukwOEAlW0gVu-E7SCNeQ.jpeg"/></div></figure></div><p id="fbd2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对象检测器训练得相当快，输出看起来相当干净。这是令人振奋的，因为这将是在这个管道中找到更多的角色特定的相似图像的关键部分。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ne"><img src="../Images/bf1129e5f594a264f696eaea6689e877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V5zEfI4GlO7xM2BLsMeIEQ.jpeg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Example of some cropped image outputs</figcaption></figure><p id="7a43" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当使用对象检测器从原始数据集中裁剪头部时，我保存了头部到其原始图像的 csv 映射。我的想法是，我可以在头像上运行一个特征提取器，并将其存储在骚扰模型中，到时候我可以将骚扰输出与原始图像进行匹配。</p><h1 id="25f8" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">Pytorch 和 Annoy 的特征提取</h1><p id="938e" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">现在我可以从图像中提取头部，我所要做的就是将这些头部通过特征提取器(再次是 ResNet101)，然后将这些特征向量传递给 aroy。</p><p id="0033" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为一个演示，这里有一张之前的图片，原始图片模型有一些问题。这是对象检测器检测图像中的两张脸的示例输出。因此，每张图像都有从中提取的特征，然后与更大的数据库进行匹配。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nf"><img src="../Images/ceb3ea09f80dc9e46520bfd5a25f367c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D0dYh2rOu-loyWdqo_5bUQ.jpeg"/></div></div></figure><p id="3a0e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一个输出来自主图像左边的角色(他出现在数据集中), 4 个相似图像的前两个和最后一个图像是该角色。这是对该图像的原始图像输入的改进，其中有 0 个匹配。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ng"><img src="../Images/2b6038e325d2df56291ff9f91a724f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*taQ6iK_13H125LX94dAHpQ.jpeg"/></div></div></figure><p id="4f33" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二个角色(右边的一个)实际上并没有出现在数据库中…但是她的面部特征与左边的一个基本相同，所以四个图像 2 匹配(第一相似和第四相似)</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nh"><img src="../Images/061cc6665d2ce2513795045e4c37eb00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jryfpua3xKali6a_86NEcw.jpeg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Left is the base image, followed by 4 most similar</figcaption></figure><p id="0339" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与只使用基本图像相比，这似乎是一个很好的改进，因为目标是返回相似的字符。现在让我们看看我之前为基本模型使用的另一个示例图像。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ni"><img src="../Images/5b0f965b8fff1ffe8a69f05ee4328529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OH3IFJMNaVffB-0yhU3Xwg.png"/></div></div></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nj"><img src="../Images/594f725b9a170f3f95ec728c6e36b02f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*na69d3qscVbFxz4mn8XEtw.jpeg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Left is the base image, followed by 4 most similar</figcaption></figure><p id="962e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，在这张照片中，他们看起来更有针对性，而不是仅仅得到一堆红色和黑色的图像。虽然第一、第二和第四个相似图像具有不同的特征，但是第三个图像具有相同的特征。这一次，所有人都至少与基本图像的性别相同。另一个只是让所有的男性角色和一个女性角色的基本图像配对。虽然这不是一个很好的结果，但似乎比以前的版本有所改进。</p><h1 id="a526" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">结束语</h1><p id="32d0" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">在查看了这两条管道的输出后，我觉得这些结果是可以接受的，但不是很好。使用基本图像返回具有相似感觉但不一定相似特征的图像。虽然面部检测器有助于将输出集中到相似的字符，但它通常不会返回总体相似风格的图像。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nk"><img src="../Images/f239350fed10d3d7dd112e6eed925db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0V4aQAXcFWvgTlt0R0dpbQ.jpeg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">original full image model</figcaption></figure><p id="1df2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然 4 个返回图像中的 2 个不是相同的字符，但我确实喜欢中间的第二个结果，因为它与基础图像有相似的“感觉”。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nl"><img src="../Images/f410ceab9433223c55b2a9e4d40a4960.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Xa8sPaqnXgSh7Yfw0VULA.jpeg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">headshot based model</figcaption></figure><p id="dae5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我之前提到的，基于头像的模型很好地聚焦于所讨论的角色。在这种情况下，所有字符都是相同的。然而，它不符合原始图像的感觉。我真正想要的是两者的某种结合，我可以得到相似的角色和相似的整体形象(基本上我自私地想要鱼和熊掌兼得)。</p><p id="2643" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">经过一些试验后，我发现我能够非常接近那个。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nm"><img src="../Images/23bc0dad5d59cb5dde626143a193fe96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kYVNzYLqC10lX-t1_sPPhA.jpeg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">New model!</figcaption></figure><p id="c6f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我在本文开始时提到的，从这条管道获得更好的输出基本上可以归结为修改哪些数据被压缩到最终的特征向量中，这些向量被传递到 annoy 中。虽然在大多数情况下鱼与熊掌兼得是不可行的，但在这种情况下却是可行的！我认为，这个“新模型”比其他两个做得更好，因为它获得了所有正确的字符(击败了基本图像模型)，并且显示的图像比头像模型更接近基本图像。</p><p id="2049" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通常情况下，这只是一种我必须从新的角度解决问题的情况。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a908a6f33083403f6f6a54118e6623d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/1*JdqUOMsLpdC-TVEBFwkoVg.gif"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Still really enjoy big hero 6 and code to Immortals as a theme song for my life</figcaption></figure><p id="62ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我只是不得不重新思考我在最终的特征向量中编码了什么信息。我最终做的是将来自检测到的头像和基础图像的信息传递给一个组合的特征向量，该向量捕获了关于角色面部(以获得相似的角色)和基础图像(以获得整体“感觉”)的信息。</p><p id="ec5b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，这个最终模型并不那么简单，我花了一点时间才弄明白，所以我会再发一篇后续文章来保持这篇文章的合理长度。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi no"><img src="../Images/4c6c82a587e5c8ea843975bf230b6bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/1*KdCvbnUFpBM1c7US85d8IA.gif"/></div></figure><p id="7623" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，下次请继续收听，我将介绍我是如何将面部和基本图像信息组合成密集的表示形式，以让 Spotify 的 annoy 在性格和感觉方面找到相似的图像。</p><p id="eaee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">跟进博文</strong> <a class="ae ko" rel="noopener" target="_blank" href="/deeper-dive-into-finding-similar-faces-with-spotifys-annoy-tensorflow-and-pytorch-c434897bd627"> <strong class="js iu">此处</strong> </a></p><blockquote class="kp kq kr"><p id="846e" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">再一次，在这里你可以随意查看我为这个<a class="ae ko" href="https://github.com/sugi-chan/fgo_face_similarity" rel="noopener ugc nofollow" target="_blank">使用的笔记本。这些笔记本不是超级可读的，因为我很快就把东西处理完了。我也不像往常一样在 repo 中提供模型/数据集文件。</a></p></blockquote></div></div>    
</body>
</html>