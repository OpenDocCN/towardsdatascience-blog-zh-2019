# 使用编码器-解码器模型的单词级英语到马拉地语神经机器翻译

> 原文：<https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7?source=collection_archive---------0----------------------->

## 使用 LSTM 构建序列对序列模型的指南

# 目录

1.  介绍
2.  先决条件
3.  编码器-解码器架构
4.  编码器 LSTM
5.  解码器 LSTM —训练模式
6.  解码器 LSTM —推理模式
7.  代码遍历
8.  结果和评价
9.  未来的工作
10.  结束注释
11.  参考

# 1.介绍

已经发现，在给定大量数据的情况下，递归神经网络(或者更准确地说，LSTM/GRU)在解决复杂的序列相关问题方面非常有效。它们在语音识别、自然语言处理(NLP)问题、时间序列预测等方面有实时应用。这篇 [*博客*](https://www.analyticsvidhya.com/blog/2018/04/sequence-modelling-an-introduction-with-practical-use-cases/) 很好地解释了其中的一些应用。

序列对序列(通常缩写为 seq2seq)模型是一类特殊的递归神经网络架构，通常用于(但不限于)解决复杂的语言相关问题，如机器翻译、问答、创建聊天机器人、文本摘要等。

![](img/32156a17f3fa529c08927e0ba64e0747.png)

Source [https://www.analyticsvidhya.com/blog/2018/04/sequence-modelling-an-introduction-with-practical-use-cases/](https://www.analyticsvidhya.com/blog/2018/04/sequence-modelling-an-introduction-with-practical-use-cases/)

这篇博文的目的是详细解释序列对序列模型是如何构建的，并给出它们如何解决这些复杂任务的直观理解。

我们将把机器翻译的问题(将文本从一种语言翻译成另一种语言，在我们的例子中是从英语翻译成马拉地语)作为本博客中的运行示例。然而，技术细节通常适用于任何序列对序列的问题。

由于我们使用神经网络来执行机器翻译，更常见的是称为神经机器翻译(NMT)。

# 2.先决条件

这篇文章假设你:

a.了解机器学习和神经网络的基本概念

b.知道高中线性代数和概率

c.有 Python 和 Keras 的 LSTM 网络的工作知识

[](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*(解释了 RNNs 如何被用来建立语言模型)和 [*理解 LSTM 网络*](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (用坚实的直觉解释了 LSTMs 的工作原理)是两篇精彩的博客，如果你还没有看过，我强烈建议你浏览一下。这些博客中解释的概念在我的文章中被广泛使用。*

# *3.编码器-解码器架构*

*用于构建 Seq2Seq 模型的最常见架构是编码器解码器架构。这是我们将在这篇文章中使用的。下面是该架构的高级视图。*

*![](img/d2a5d96ba2ef961fe09639038ed328b2.png)*

*Source: [https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d](/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d)*

*注意事项:*

*a.编码器和解码器都是典型的 LSTM 模型(或者有时是 GRU 模型)*

*b.编码器读取输入序列，并在称为内部状态向量的东西中总结信息(在 LSTM 的情况下，这些称为隐藏状态和单元状态向量)。我们丢弃编码器的输出，只保留内部状态。*

*c.解码器是一个 LSTM，其初始状态被初始化为编码器 LSTM 的最终状态。使用这些初始状态，解码器开始产生输出序列。*

*d.在训练和推理过程中，解码器的表现稍有不同。在训练过程中，我们使用了一种称为教师强制的技术，这有助于更快地训练解码器。在推理过程中，解码器在每个时间步的输入是前一个时间步的输出。*

*e.直观地说，编码器将输入序列总结为状态向量(有时也称为思维向量)，然后将状态向量馈送给解码器，解码器开始根据思维向量生成输出序列。解码器只是一个以初始状态为条件的语言模型。*

*现在我们将通过考虑将一个英语句子(输入序列)翻译成其对等的马拉地语句子(输出序列)的例子来详细理解上述所有步骤。*

# *4.编码器 LSTM*

*本节简要概述了编码器 LSTM 的主要组件。我将保持这种直觉，而不进入数学领域。这就是它们的内容:*

*![](img/95f26ef0aa4f98aecebcf7228a01caae.png)*

*LSTM processing an input sequence of length ‘k’*

*LSTM 一个接一个地读取数据。因此，如果输入是一个长度为“k”的序列，我们说 LSTM 在“k”个时间步中读取它(把这想象成一个具有“k”次迭代的 for 循环)。*

*参考上图，以下是 LSTM 的 3 个主要组成部分:*

*a.Xi = >时间步长 I 的输入序列*

*b.hi 和 ci => LSTM 在每个时间步保持两种状态(“h”代表隐藏状态，“c”代表单元状态)。这些组合在一起，就是 LSTM 在时间步 I 的内部状态*

*c.Yi = >时间步长 I 的输出序列*

***重要提示**:从技术上来说，所有这些组件(Xi、hi、ci 和 Yi)实际上都是浮点数的向量(解释如下)*

*让我们试着在我们问题的背景下把所有这些联系起来。回想一下，我们的问题是将一个英语句子翻译成马拉地语的对等物。出于这个博客的目的，我们将考虑下面的例子。比方说，我们有下面这个句子*

*输入句子(英语)= >“Rahul 是个好孩子”*

*输出句子(马拉地语)= >“राहुलचांगलामुलगाआहे”*

*现在只关注输入，即英语句子*

## ***对 Xi 的解释:***

*现在，一个句子可以被看作是单词或字符的序列。例如，在单词的情况下，上面的英语句子可以被认为是 5 个单词的序列(“Rahul”、“is”、“a”、“good”、“boy”)。在字符的情况下，它可以被认为是一个 19 个字符的序列(' R '，' a '，' h '，' u '，' l '，' '，……，' y ')。*

*我们将按单词分解句子，因为这种方案在现实应用中更常见。因此得名“单词级 NMT”。因此，参考上图，我们有以下输入:*

*X1 = '拉胡尔'，X2 = '是'，X3 = 'a '，X4 = '好，X5 = '小子'。*

*LSTM 将按如下 5 个时间步长逐字读这句话*

*![](img/08edacc95db8357eae59ce493e8230ba.png)*

*Encoder LSTM*

*但我们必须回答的一个问题是，如何将每个 Xi(每个单词)表示为一个向量？*

*有各种将单词映射(嵌入)到固定长度向量的单词嵌入技术。我假设读者熟悉单词嵌入的概念，不会详细讨论这个主题。但是，我们将使用 Keras API 的内置嵌入层将每个单词映射到一个固定长度的向量。*

## *hi 和 ci 的解释:*

*下一个问题是内部状态(hi 和 ci)在每个时间步的作用是什么？*

*简单地说，他们记得 LSTM 到目前为止读过(学过)的东西。例如:*

*h3，c3 = >这两个向量会记住网络到现在都是读“Rahul 是 a”。基本上是存储在向量 h3 和 c3 中的直到时间步长 3 的信息的汇总(因此称为时间步长 3 的状态)。*

*类似地，我们因此可以说 h5，c5 将包含整个输入句子的摘要，因为这是句子结束的地方(在时间步骤 5)。这些从最后一个时间步骤出来的状态也被称为“**思维向量**，因为它们以向量的形式总结了整个序列。*

*那 h0，c0 呢？这些向量通常被初始化为零，因为模型还没有开始读取输入。*

***注意**:这两个向量的大小等于 LSTM 细胞中使用的单位(神经元)数量。*

## *易解释:*

*最后，每一个时间点的易呢？这些是 LSTM 模型在每个时间步的输出(预测)。*

*但是 Yi 是什么类型的向量呢？更具体地，在单词级语言模型的情况下，每个 Yi 实际上是通过使用 softmax 激活生成的整个词汇表上的概率分布。因此，每个 Yi 是代表概率分布的大小为“vocab_size”的向量。*

*根据问题的背景，它们有时会被使用，有时会被丢弃。*

*在我们的例子中，除非我们已经阅读了整个英语句子，否则我们没有任何输出。因为一旦我们阅读了整个英语句子，我们将开始生成输出序列(等价的马拉地语句子)。因此，对于我们的问题，我们将丢弃编码器的 Yi。*

## *编码器概述:*

*我们将逐字读出输入序列(英语句子),并保存在最后一个时间步 hk，ck 之后产生的 LSTM 网络的内部状态(假设该句子有“k”个单词)。这些向量(状态 hk 和 ck)被称为输入序列的**编码**，因为它们以向量形式编码(汇总)整个输入。由于我们将在读取整个序列后开始生成输出，因此编码器在每个时间步长的输出(Yi)将被丢弃。*

*此外，你还必须了解什么类型的载体是 Xi，嗨，慈和易。它们的大小(形状)是什么，代表什么。如果你对理解这一部分有任何困惑，那么你需要首先加强你对 LSTM 和语言模型的理解。*

# *5.解码器 LSTM —训练模式*

*不同于在训练阶段和推理阶段扮演相同角色的编码器 LSTM，解码器 LSTM 在这两个阶段扮演稍微不同的角色。*

*在本节中，我们将尝试了解如何在训练阶段配置解码器，而在下一节中，我们将了解如何在推理过程中使用它。*

*回想一下，给定输入句子“Rahul 是个好孩子”，训练过程的目标是训练(教导)解码器输出“राहुल चांगला मुलगा आहे".”正如编码器逐字扫描输入序列一样，解码器也会逐字生成输出序列。*

*出于一些技术原因(稍后解释)，我们将在输出序列中添加两个令牌，如下所示:*

*输出序列= > "开始 _राहुलचांगलामुलगाआहे_ 结束"*

*现在考虑下图:*

*![](img/fd7575e5d904c21158a8edf9e7733529.png)*

*Decoder LSTM — Training Mode*

*最重要的一点是，解码器的初始状态(h0，c0)被设置为编码器的最终状态。这直观地意味着解码器被训练成根据编码器编码的信息开始生成输出序列。显然，翻译的马拉地语句子必须依赖于给定的英语句子。*

*在第一个时间步骤中，我们提供 START_ token，以便解码器开始生成下一个 token(马拉地语句子的实际第一个单词)。在马拉地语句子的最后一个单词之后，我们让解码器学习预测 _END 标记。这将在推理过程中用作停止条件，基本上它将表示翻译句子的结束，我们将停止推理循环(稍后将详细介绍)。*

*我们使用一种被称为“教师强制”的技术，其中每个时间步长的输入被给定为前一个时间步长的**实际**输出(而不是预测输出)。这有助于更快更有效地训练网络。想了解更多关于老师强迫的事情，参考这个 [*博客*](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/) 。*

*最后，根据每个时间步长的预测输出计算损耗，误差随时间反向传播，以更新网络参数。用足够多的数据在更长的时间内训练网络会产生相当好的预测(翻译),我们将在后面看到。*

*整个训练过程(编码器+解码器)可以总结在下图中:*

*![](img/f7ab8b8b595499cc4cd6860063816769.png)*

*Summary of the training process*

# *6.解码器 LSTM —推理模式*

*现在让我们试着理解推理所需的设置。如前所述，编码器 LSTM 扮演着读取输入序列(英语句子)和生成思维向量(hk，ck)的相同角色。*

*然而，解码器现在必须预测给定这些思想向量的整个输出序列(马拉地语句子)。*

*我们举同一个例子，试着直观的理解一下。*

*input sequence = >“Rahul 是个好孩子”*

*(预期)输出序列=> "राहुल चांगला मुलगा आहे"*

***步骤 1** :将输入序列编码成思维向量；*

*![](img/0b6e7dd6558a5aeaff297aad8c14956b.png)*

*Encode the input sequence into thought vectors*

***第二步**:开始循环产生输出序列，一个字一个字:*

*在 t = 1 时*

*![](img/f0bc462bdedac86eaf45f81a85876d23.png)*

*Decoder at t=1*

*在 t = 2 时*

*![](img/d3909f3b1d7583ed28d251e4a3a3e31a.png)*

*Decoder at t = 2*

*在 t = 3 时*

*![](img/d1c7f94c488ace4a2568d072dadee6c9.png)*

*Decoder at t = 3*

*在 t = 4 时*

*![](img/626f84d73bfa85d6d7c0909e4ffb2161.png)*

*Decoder at t = 4*

*在 t = 5 时*

*![](img/68c51aa945b62a0bdc092d7fc6964cc4.png)*

*Decoder at t = 5*

## *推理算法:*

*a.在推理过程中，我们一次生成一个单词。因此，解码器 LSTM 在一个循环中被调用，每次只处理一个时间步。*

*b.解码器的初始状态被设置为编码器的最终状态。*

*c.解码器的初始输入始终是 START_ token。*

*d.在每个时间步，我们保留解码器的状态，并将其设置为下一个时间步的初始状态。*

*e.在每个时间步，预测的输出作为下一个时间步的输入。*

*f.当解码器预测 END_ token 时，我们中断循环。*

*整个推理过程可以总结在下图中:*

*![](img/c4ebfbbdc0b3dc74eaf8110bcef38369.png)*

*Summary of the inference process*

# *7.代码遍历*

*当我们实际实现代码时，没有什么比理解代码更重要了，无论我们为理解理论付出了多少努力(然而这并不意味着我们不讨论任何理论，但是我想说的是理论必须总是在实现之后)。*

## ***数据集**:*

*从 [*这里*](http://www.manythings.org/anki/) 下载并解压 mar-eng.zip 文件。*

*在我们开始构建模型之前，我们需要执行一些数据清理和准备工作。在不涉及太多细节的情况下，我将假设读者理解下面的(不言自明的)步骤，这些步骤通常是任何语言处理项目的一部分。*

*Code to perform Data Cleaning*

*下面我们计算英语和马拉地语的词汇。我们还计算了两种语言的词汇量和最大序列长度。最后，我们创建了 4 个 Python 字典(每种语言两个),用于将给定的令牌转换成整数索引，反之亦然。*

*Code for Data Preparation*

*然后，我们进行 90–10 训练和测试分割，并编写 Python 生成器函数来批量加载数据，如下所示:*

*Code for loading Batches of Data*

*然后，我们将培训所需的模型定义如下:*

*Code to define the Model to be trained*

*你应该能够从概念上将每一行与我在上面第 4 和第 5 节中提供的解释联系起来。*

*让我们看看从 Keras 的 plot_model 实用程序生成的模型架构。*

*![](img/e308e72dce6bbd812c50472c27065c87.png)*

*Training the network*

*我们以 128 的批量训练网络 50 个时期。在 P4000 GPU 上，训练时间略多于 2 小时。*

## *推理设置:*

*Code for setting up Inference*

*最后，我们通过在一个循环中调用上述设置来生成输出序列，如下所示:*

*Code to decode the output sequence in a loop*

*在这一点上，你必须能够在概念上将上面两个模块中的每一行代码与第 6 节中提供的解释联系起来。*

# *8.结果和评价*

*这篇博文的目的是给出一个直观的解释，说明如何使用 LSTM 构建基本级别的序列到序列模型，而不是开发一个高质量的语言翻译器。所以请记住，由于多种原因，这些结果不是世界级的(而且你不要开始与谷歌翻译进行比较)。最重要的原因是数据集很小，只有 33000 对句子(是的，这些太少了)。如果你想提高翻译质量，我会在博客末尾列出一些建议。然而现在，让我们看看从上面的模型中产生的一些结果(它们也不太坏)。*

## *对训练数据集的评估:*

*Evaluation on Training Dataset*

## *对测试数据集的评估:*

*Evaluation on Test Dataset*

## *我们能得出什么结论？*

*尽管结果不是最好的，但也没那么糟糕。当然比随机产生的序列要好得多。在一些句子中，我们甚至可以注意到预测的单词是不正确的，但是它们在语义上非常接近正确的单词。*

*此外，需要注意的另一点是，训练集的结果略好于测试集的结果，这表明该模型可能有点过度拟合。*

# *9.未来的工作*

*如果你有兴趣提高质量，你可以尝试以下措施:*

*a.获取更多数据。高质量的翻译接受了数百万对句子的训练。*

*b.建立更复杂的模型，比如注意力。*

*c.使用剔除和其他形式的正则化技术来减轻过度拟合。*

*d.执行超参数调整。玩学率，批量，辍学率等。尝试使用双向编码器 LSTM。尝试使用多层 LSTMs。*

*e.尝试使用波束搜索，而不是贪婪的方法。*

*f.尝试 BLEU score 来评估您的模型。*

*g.这个清单永无止境，还在继续。*

# *10.结束注释*

*如果文章吸引你，一定要提供一些评论、反馈、建设性的批评等。*

*完整代码在我的 GitHub repo [*这里*](https://github.com/hlamba28/Word-Level-Eng-Mar-NMT.git) 。*

*如果你喜欢我的解释，你可以关注我，因为我计划发布一些与深度学习和人工智能相关的更有趣的博客。*

*你可以在 LinkedIn [*这里*](https://www.linkedin.com/in/harshall-lamba-55a14a4b)*

# *11.参考*

*a.[https://medium . com/@ dev . elect . iitd/neural-machine-translation-using-word-level-seq 2 seq-model-47538 CBA 8 CD 7](https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7)*

*b.[https://blog . keras . io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras . html](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)*

*c.[https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)*

*d.[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)*