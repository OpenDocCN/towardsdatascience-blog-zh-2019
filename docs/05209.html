<html>
<head>
<title>A Comprehensive Guide to the Correlational Neural Network with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras 相关神经网络综合指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-comprehensive-guide-to-correlational-neural-network-with-keras-3f7886028e4a?source=collection_archive---------13-----------------------#2019-08-04">https://towardsdatascience.com/a-comprehensive-guide-to-correlational-neural-network-with-keras-3f7886028e4a?source=collection_archive---------13-----------------------#2019-08-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fd2e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从理论到实施</h2></div><p id="d3ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(本博客的改进在<a class="ae lb" href="https://theaiacademy.blogspot.com/2020/05/a-comprehensive-guide-to-correlational.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">这里</strong> </a> <strong class="kh ir"> ) </strong></p><p id="8cf8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">人类和许多其他动物一样有五种基本感觉:<em class="lc">视觉、听觉、味觉、嗅觉和触觉</em>。我们还有额外的感觉，比如平衡感和加速度感，时间感等等。人类大脑每时每刻都在处理来自所有这些来源的信息，每一种感官都影响着我们的决策过程。在任何交谈中，嘴唇的运动、面部表情以及声带发出的声音都有助于我们完全理解说话者所说的话的含义。我们甚至可以只通过看嘴唇的运动而不发出任何声音来理解单词。这种视觉信息不仅是补充性的，而且是必要的。这首先在<strong class="kh ir">麦克古克效应</strong>(麦古克&amp;麦克唐纳，1976)中得到例证，其中带有有声<strong class="kh ir"> /ba/ </strong>的视觉<strong class="kh ir"> /ga/ </strong>被大多数受试者感知为<strong class="kh ir"> /da/ </strong>。由于我们希望我们的机器学习模型达到人类水平的性能，因此也有必要使它们能够使用来自各种来源的数据。</p><p id="6838" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在机器学习中，来自不同异构源的这些类型的数据被称为多模态数据。例如用于语音识别的音频和视频信息。很难在训练中直接使用这些多模态数据，因为它们可能具有不同的维度和数据类型。如此多的注意力放在学习这些多模态数据的通用表示上。学习这种多视图数据的通用表示将有助于一些下游应用程序。例如，学习视频及其音频的通用表示可以帮助为该视频生成比仅使用音频生成更准确的字幕。但是你如何学习这种常见的表示法呢？</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/6b60c81ed1222b854418d4b0d272969c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AJcD5AFwJGSek1i03MJZHw.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd lt">An abstract view of CorrNet. It tires to learn a common representation of both views of data and tries to reconstruct both the view from that encoded representation.</strong></figcaption></figure><p id="3edf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相关神经网络是学习常见表示的方法之一。其架构几乎与传统的单视图深度自动编码器相同。但是它为每种数据形式包含一个编码器-解码器对。</p><p id="1999" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们考虑一个两视图输入，<strong class="kh ir">Z =【Ia，Iv】</strong>，其中<strong class="kh ir"> Ia </strong>和<strong class="kh ir"> Iv </strong>是音频和视频等数据的两个不同视图。在下图中，显示了一个包含这些数据的 CorrNet 的简单架构。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi lu"><img src="../Images/c014b1b4e4e138bf499b1f4d5ef7c103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3xccrXaVLAsaPf86s9nLQ.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Example of CorrNet with bimodal data<strong class="bd lt"> Z = [Ia, Iv]</strong> where Ia and Iv are two different views of data like (audio and video )</figcaption></figure><p id="ca69" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中编码器和解码器都是单层的。<strong class="kh ir"> <em class="lc"> H </em> </strong>是编码表示。<strong class="kh ir"><em class="lc">H</em></strong>a<strong class="kh ir"><em class="lc">= f(W</em></strong>a<strong class="kh ir"><em class="lc">)。I</em></strong>a<strong class="kh ir"><em class="lc">+b)</em></strong>是编码表示 Ia。<strong class="kh ir"> <em class="lc"> f() </em> </strong>是任意非线性(sigmoid，tanh 等。).同样的情况还有<strong class="kh ir"><em class="lc">Hv</em></strong>=<strong class="kh ir"><em class="lc">f(W</em></strong>a<strong class="kh ir"><em class="lc">。I </em> </strong> a <strong class="kh ir"> <em class="lc"> +b)。</em></strong><strong class="kh ir"><em class="lc"/></strong>双峰数据的常见表示法<strong class="kh ir"> Z </strong>给出为:</p><p id="c445" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">H =<em class="lc">f(</em><em class="lc">W</em>a<em class="lc">。I </em> a + <em class="lc"> Wv。Iv + b)。</em>T55】</strong></p><p id="744e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在解码器部分，模型试图通过<strong class="kh ir"><em class="lc">I ' a</em></strong>=<strong class="kh ir"><em class="lc">g(</em>W ' a . H+b’<em class="lc">)</em></strong>和<strong class="kh ir"><em class="lc">I ' v = g(</em>W ' VH+b’<em class="lc">)</em>从普通表示<strong class="kh ir"> H </strong>中重构输入</strong></p><p id="c961" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在训练期间，基于三个损失计算梯度:</p><p id="9b9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">I)最小化自重构误差，即最小化从<strong class="kh ir"><em class="lc">Ia</em><strong class="kh ir"><em class="lc"> Iv </em> </strong>从<strong class="kh ir"> <em class="lc"> Iv </em> </strong>重构<strong class="kh ir"><em class="lc">Ia</em></strong>Ia<em class="lc">Ia</em></strong>和<em class="lc">Iv</em>的误差。</p><p id="19c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ii)最小化交叉重建误差，即最小化从<strong class="kh ir">Ia</strong>到<strong class="kh ir"> Iv </strong>和从<strong class="kh ir"> Ia </strong>到<strong class="kh ir"> Iv </strong>的重建误差。</p><p id="ae8c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">iii)最大化两个视图的隐藏表示之间的相关性，即最大化<strong class="kh ir"> Ha </strong>和<strong class="kh ir"> Hv </strong>之间的相关性。</p><p id="5c1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总损耗可以写成</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi lv"><img src="../Images/00b1cf9eaa95570f2844e1a207eaf22d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z1dcFbJUggZcKPHnC_Gonw.png"/></div></div></figure><p id="84b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，<strong class="kh ir"> Lr() </strong>表示重建损失，可以是“<strong class="kh ir">均方误差”</strong>或“<strong class="kh ir">平均绝对误差”。我们的目标是尽量减少这种损失。当我们想要增加相关性时，从损耗中减去相关性，即相关性越高，损耗越低。</strong></p><h2 id="becc" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">履行</h2><p id="0fad" class="pw-post-body-paragraph kf kg iq kh b ki mp jr kk kl mq ju kn ko mr kq kr ks ms ku kv kw mt ky kz la ij bi translated">整个实现可以分为三个部分:模型建立、设置损失函数和训练。</p><p id="a8c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在模型构建阶段，我们必须创建自动编码架构。首先，我们必须包含所有必需的包。</p><pre class="le lf lg lh gt mu mv mw mx aw my bi"><span id="6893" class="lw lx iq mv b gy mz na l nb nc">from <strong class="mv ir">keras </strong>import  <strong class="mv ir">Model</strong><br/>from <strong class="mv ir">keras.layers import  Input,Dense,concatenate,Add</strong><br/>from <strong class="mv ir">keras </strong>import backend as <strong class="mv ir">K,activationsfrom tensorflow</strong> <br/>import <strong class="mv ir">Tensor </strong>as Tfrom<strong class="mv ir"> keras.engine.topology</strong> <br/>import <strong class="mv ir">Layer</strong><br/>import numpy as <strong class="mv ir">np </strong></span></pre><p id="350a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们必须创建 CorrNet 架构。为简单起见，它有一个单层的编码器和解码器。</p><pre class="le lf lg lh gt mu mv mw mx aw my bi"><span id="fb8b" class="lw lx iq mv b gy mz na l nb nc">class <strong class="mv ir">ZeroPadding</strong>(Layer):<br/>     def <strong class="mv ir">__init__</strong>(self, **kwargs):<br/>          super(ZeroPadding, self).__init__(**kwargs)</span><span id="b4bd" class="lw lx iq mv b gy nd na l nb nc">     def <strong class="mv ir">call</strong>(self, x, mask=None):<br/>          return K.zeros_like(x) <br/>     <br/>     def <strong class="mv ir">get_output_shape_for</strong>(self, input_shape):<br/>          return input_shape</span><span id="dce4" class="lw lx iq mv b gy nd na l nb nc">#inputDimx,inputDimy are the dimentions two input modalities. And #hdim_deep is the dimentions of shared representation( <em class="lc">kept as #global veriable</em>) </span><span id="4971" class="lw lx iq mv b gy nd na l nb nc">inpx = <strong class="mv ir">Input</strong>(<strong class="mv ir">shape</strong>=(inputDimx,))               <br/>inpy = <strong class="mv ir">Input</strong>(<strong class="mv ir">shape</strong>=(inputDimx,)) <br/><strong class="mv ir"><em class="lc">#Encoder</em></strong>                <br/>hl = <strong class="mv ir">Dense</strong>(hdim_deep,<strong class="mv ir">activation</strong>='relu')(inpx)                              hr = <strong class="mv ir">Dense</strong>(hdim_deep,<strong class="mv ir">activation</strong>='relu')(inpy)                             h =  Add()([hl,hr]) <strong class="mv ir"><em class="lc">#Common representation/Encoded representation</em></strong></span><span id="93a2" class="lw lx iq mv b gy nd na l nb nc"><strong class="mv ir"><em class="lc">#decoder</em></strong></span><span id="b24a" class="lw lx iq mv b gy nd na l nb nc">recx = <strong class="mv ir">Dense</strong>(inputDimx,activation='relu')(h)                                         recy = <strong class="mv ir">Dense</strong>(inputDimy,activation='relu')(h)</span><span id="541f" class="lw lx iq mv b gy nd na l nb nc"><strong class="mv ir">CorrNet</strong> = <strong class="mv ir">Model</strong>( [inpx,inpy],[recx,recy,h])<br/> <br/><strong class="mv ir">CorrNet</strong>.summary()</span><span id="6aca" class="lw lx iq mv b gy nd na l nb nc">'''we have to create a separate model for training this <strong class="mv ir">CorrNet</strong><br/>As during training we have to take gradient from 3 different loss function and which can not be obtained from signle input.If you look closely to the loss function, we will see it has different input parameter'''</span><span id="30ce" class="lw lx iq mv b gy nd na l nb nc">[recx0,recy0,h1] = <strong class="mv ir">CorrNet</strong>( [inpx, inpy])<br/>[recx1,recy1,h1] = <strong class="mv ir">CorrNet</strong>( [inpx, ZeroPadding()(inpy)])[recx2,recy2,h2] = <strong class="mv ir">CorrNet</strong>( [ZeroPadding()(inpx), inpy ])                <br/>H= <strong class="mv ir">concatenate</strong>([h1,h2]) <br/><strong class="mv ir">model</strong> = Model( [inpx,inpy],[recx0,recx1,recx2,recy0,recy1,recy2,H])</span></pre><p id="e922" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们必须为我们的模型写出相关损失函数。</p><pre class="le lf lg lh gt mu mv mw mx aw my bi"><span id="5d84" class="lw lx iq mv b gy mz na l nb nc"><strong class="mv ir">Lambda </strong>= 0.01<br/>#by convention error function need to have two argument, but in #correlation loss we do not need any argument. Loss is computed #competely based on correlation of two hidden representation. For #this a '<strong class="mv ir">fake</strong>' argument is used.</span><span id="7ed6" class="lw lx iq mv b gy nd na l nb nc">def <strong class="mv ir"><em class="lc">correlationLoss</em></strong>(<em class="lc">fake</em>,H):<br/>  y1 = H[:,:hdim_deep]<br/>  y2 = H[:,hdim_deep:]<br/>  y1_mean = K.mean(y1, axis=0)<br/>  y1_centered = y1 - y1_mean<br/>  y2_mean = K.mean(y2, axis=0)<br/>  y2_centered = y2 - y2_mean<br/>  corr_nr = K.sum(y1_centered * y2_centered, axis=0) <br/>  corr_dr1 = K.sqrt(K.sum(y1_centered * y1_centered, axis=0) + 1e-8)<br/>  corr_dr2 = K.sqrt(K.sum(y2_centered * y2_centered, axis=0) + 1e-8)<br/>  corr_dr = corr_dr1 * corr_dr2<br/>  corr = corr_nr / corr_dr <br/>  <strong class="mv ir">return </strong>K.sum(corr) * Lambda<br/>def <strong class="mv ir"><em class="lc">square_loss</em></strong>(y_true, y_pred):<br/>  error = ls.mean_squared_error(y_true,y_pred)<br/>  <strong class="mv ir">return </strong>error</span></pre><p id="0195" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们必须编译模型并进行训练</p><pre class="le lf lg lh gt mu mv mw mx aw my bi"><span id="dd2e" class="lw lx iq mv b gy mz na l nb nc">model.<strong class="mv ir">compile</strong>(<strong class="mv ir">loss</strong>=[<em class="lc">square_loss,square_loss,square_loss,  square_loss,square_loss,square_loss,correlationLoss</em>],<strong class="mv ir">optimizer</strong>="adam")<br/>model.<strong class="mv ir">summary</strong>()<br/>'''<br/>Suppose you have already prepared your data and kept one moadlity data in Ia(e.g. Audio) and another in Iv( e.g. Video).To be used by this model Audios and videos must be converted into 1D tensor.<br/>'''</span><span id="492a" class="lw lx iq mv b gy nd na l nb nc">model.<strong class="mv ir">fit</strong>([Ia,Iv],[Ia,Ia,Ia,Iv,Iv,Iv,<em class="lc">np.ones((Ia.shape[0],Ia.shape[1]))</em>],nb_epoch=100)</span><span id="f829" class="lw lx iq mv b gy nd na l nb nc">'''<br/><strong class="mv ir"><em class="lc">np.ones((Ia.shape[0],Ia.shape[1]))</em></strong> is fake tensor that will be   passed to <strong class="mv ir"><em class="lc">correlationLoss </em></strong>function but will have no use<br/>'''<br/>using this model we can generate Ia to Iv.For example, from video Iv we can generate corresponding audio Ia<br/>np.zeros(Ia.shape) gives tensors of 0 of dimestions same as output tensor <strong class="mv ir"><em class="lc">audio</em></strong>  ''' <br/><strong class="mv ir">audio</strong>,_,_ = <strong class="mv ir">CorrNet.</strong>predict([np.zeros(Ia.shape),Iv])</span></pre><p id="9f2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">经过训练后，该模型学习到的公共表示可以用于不同的预测任务。例如，使用 CorrNet 学习的常见表示可以用于:跨语言文档分类或音译等价检测。各种研究发现，使用公共表示可以提高性能。它也可以用于数据生成。例如，在某个数据集中，您有 10000 个音频剪辑和相应的视频，5000 个音频剪辑和 5000 个视频。在这种情况下，我们可以用 10000 个音频和视频训练一个 CorrNet，并可以用来为音频生成缺失的视频，反之亦然。</p></div></div>    
</body>
</html>