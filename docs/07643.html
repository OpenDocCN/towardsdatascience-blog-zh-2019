<html>
<head>
<title>Simple Neural Networks in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的简单神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/inroduction-to-neural-networks-in-python-7e0b422e6c24?source=collection_archive---------0-----------------------#2019-10-24">https://towardsdatascience.com/inroduction-to-neural-networks-in-python-7e0b422e6c24?source=collection_archive---------0-----------------------#2019-10-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="7841" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">什么是神经网络？</h1><p id="edf0" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">神经网络大致基于人类大脑的工作方式:许多神经元连接到其他神经元，通过它们的连接传递信息，并在神经元的输入超过特定阈值时触发。我们的人工神经网络将由人工神经元和突触组成，信息在它们之间传递。突触或连接将根据神经元对决定输出的影响强度进行加权。这些突触权重将经历一个被称为<strong class="kq iu">反向传播</strong>的优化过程。对于训练过程中的每次迭代，反向传播将用于返回网络的各层，并根据它们对神经网络误差的贡献来调整权重。</p><p id="0345" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">神经网络本质上是将输入映射到正确输出的自我优化功能。然后，我们可以将一个新的输入放入函数中，它将根据用训练数据创建的函数来预测输出。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/3edde70f351f8722b4c22d64d17801ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ekOVM7MUcWBLx-egd60Yug.png"/></div></div></figure><h1 id="9224" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">神经网络的目标</h1><p id="4ced" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">像所有的神经网络一样，这个神经网络必须学习数据中的重要特征，以产生输出。特别地，该神经网络将被给予具有六个样本的输入矩阵，每个样本具有三个仅由 0 和 1 组成的特征列。例如，训练集中的一个样本可能是[0，1，1]。每个样本的输出将是单一的 1 或 0。输出将由数据样本的第一个特征列中的数字决定。使用前面给出的例子，[0，1，1]的输出将是 0，因为第一列包含 0。下面将给出一个示例图表来演示每个输入样本的输出。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi md"><img src="../Images/243c324d904199fab514ea14666a9e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8FHAzqRyn3kAkwzn2u40nw.png"/></div></div></figure><h1 id="1d9f" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">完整代码</h1><figure class="ls lt lu lv gt lw"><div class="bz fp l di"><div class="me mf l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk"><a class="ae mk" href="https://gist.github.com/a-i-dan/8d0a40b8690b40d9a46c4cb1d326fce5" rel="noopener ugc nofollow" target="_blank">https://gist.github.com/a-i-dan/8d0a40b8690b40d9a46c4cb1d326fce5</a></figcaption></figure><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="47fa" class="mq jr it mm b gy mr ms l mt mu">Output:</span><span id="0941" class="mq jr it mm b gy mv ms l mt mu">[[0.99089925]] - Correct: 1<br/>[[0.006409]] - Correct: 0</span></pre><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mw"><img src="../Images/f49bd4a5e5c62a236149c2940b930d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MK7_0r_s63_vZ31lBApVJg.jpeg"/></div></div></figure><h1 id="2a3e" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">代码分解</h1><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="81a4" class="mq jr it mm b gy mr ms l mt mu">import numpy as np<br/>import matplotlib.pyplot as plt</span></pre><p id="77df" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">在开始之前，我们需要导入必要的库。这个例子只需要两个库，如果不画出损失，我们只需要 Numpy。Numpy 是一个 python 数学库，主要用于线性代数应用。Matplotlib 是一个可视化工具，我们将使用它来创建一个图，以显示我们的误差如何随着时间的推移而减少。</p><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="be7a" class="mq jr it mm b gy mr ms l mt mu">inputs <strong class="mm iu">=</strong> np<strong class="mm iu">.</strong>array([[0, 1, 0],<br/>                   [0, 1, 1],<br/>                   [0, 0, 0],<br/>                   [1, 0, 0],<br/>                   [1, 1, 1],<br/>                   [1, 0, 1]])<br/><br/>outputs <strong class="mm iu">=</strong> np<strong class="mm iu">.</strong>array([[0], [0], [0], [1], [1], [1]])</span></pre><p id="6d8c" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">如前所述，神经网络需要数据来学习。我们将使用 Numpy 的<code class="fe mx my mz mm b">.array()</code>函数创建输入数据矩阵和相应的输出矩阵。输入中的每个样本由三个由 0 和 1 组成的特征列组成，这些特征列产生一个 0 或 1 的输出。我们想让神经网络知道输出是由每个样本中的第一个特征列决定的。</p><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="fdec" class="mq jr it mm b gy mr ms l mt mu"><strong class="mm iu">class</strong> <strong class="mm iu">NeuralNetwork</strong>:<br/><br/>    <strong class="mm iu">def</strong> <strong class="mm iu">__init__</strong>(self, inputs, outputs):<br/>        self<strong class="mm iu">.</strong>inputs  <strong class="mm iu">=</strong> inputs<br/>        self<strong class="mm iu">.</strong>outputs <strong class="mm iu">=</strong> outputs<br/>        self<strong class="mm iu">.</strong>weights <strong class="mm iu">=</strong> np<strong class="mm iu">.</strong>array([[<strong class="mm iu">.</strong>50], [<strong class="mm iu">.</strong>50], [<strong class="mm iu">.</strong>50]])<br/>        self<strong class="mm iu">.</strong>error_history <strong class="mm iu">=</strong> []<br/>        self<strong class="mm iu">.</strong>epoch_list <strong class="mm iu">=</strong> []</span></pre><p id="e481" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">我们将采用面向对象的方法来构建这个特定的神经网络。我们可以首先创建一个名为“NeuralNetwork”的类，并通过定义<code class="fe mx my mz mm b">__init__</code>函数来初始化该类。我们的<code class="fe mx my mz mm b">__init__</code>函数将输入和输出作为参数。我们还需要定义我们的权重，为了简单起见，从每个权重为. 50 开始。因为数据中的每个要素都必须连接到隐藏层，所以我们需要数据中每个要素的权重(三个权重)。出于绘图目的，我们还将创建两个空列表:loss_history 和 epoch_list。这将跟踪我们的神经网络在训练过程中每个时期的错误。</p><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="2579" class="mq jr it mm b gy mr ms l mt mu"><strong class="mm iu">def</strong> <strong class="mm iu">sigmoid</strong>(self, x, deriv<strong class="mm iu">=</strong>False):<br/>    <strong class="mm iu">if</strong> deriv <strong class="mm iu">==</strong> True:<br/>        <strong class="mm iu">return</strong> x <strong class="mm iu">*</strong> (1 <strong class="mm iu">-</strong> x)<br/>    <strong class="mm iu">return</strong> 1 <strong class="mm iu">/</strong> (1 <strong class="mm iu">+</strong> np<strong class="mm iu">.</strong>exp(<strong class="mm iu">-</strong>x))</span></pre><p id="020e" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">该神经网络将使用 sigmoid 函数或逻辑函数作为激活函数。sigmoid 函数是一种流行的非线性激活函数，其范围为(0–1)。该函数的输入将总是被压缩，以适应在<em class="na"> y=0 </em>和<em class="na"> y=1 </em>处的 sigmoid 函数的两条水平渐近线。sigmoid 函数有一些限制其使用的众所周知的问题。当我们看下面的曲线时，我们注意到当我们到达曲线的两端时，这些点的导数变得非常小。当这些小导数在反向传播过程中相乘时，它们会变得越来越小，直到变得无用。由于导数或梯度变得越来越小，神经网络中的权重将不会更新太多，如果有的话。这将导致神经网络停滞不前，每增加一次训练迭代，情况就会变得越来越糟。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/e4e7bc62bb81f3a0e9b115c52ff885a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*egqZAB-lOq9ySLEUu3XkBg.jpeg"/></div></figure><p id="c056" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">sigmoid 函数可以写成:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/8affc10c0139114087603c69ff317967.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*obmnBD8VKXBqt_pmJSFoBQ.png"/></div></figure><p id="bd2f" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">sigmoid 函数的导数可以写成:</p><p id="4f6a" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated"><em class="na">s′(x)=s(x)⋅(1−s(x)</em></p><h1 id="8be9" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">如何求导</h1><p id="a6a0" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">导数只是一个花哨的词，用来表示给定点的斜率或切线。仔细看看上图中的 sigmoid 函数曲线。其中<em class="na"> x=0 </em>处的斜率远大于<em class="na"> x=4 </em>或<em class="na"> x=-4 </em>处的斜率。权重的更新量基于导数。如果斜率是较低的值，则神经网络对其预测有信心，并且需要较少的权重移动。如果斜率是更高的值，则神经网络的预测更接近 0.50，或 50%(对于 sigmoid 函数，可能的最高斜率值是在<em class="na"> x=0 </em>和<em class="na">y = 0.5</em>。<em class="na"> y </em>是预测。).这意味着神经网络对其预测不是很有信心，并且需要对权重进行更大的更新。</p><p id="d5cf" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">我们可以通过以下步骤找到 sigmoid 函数的导数:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1505c3769874a4f9e8f8440cc40fe498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*z7YlQW6Q6RGN78vhxZJCYQ.png"/></div></figure><p id="d713" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">然后我们可以用一个很酷的技巧来继续简化:给<em class="na"> e^-x </em>加一减一。加一减一不会改变什么，因为它们相互抵消了。这是一种奇特的加零方式。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/4e62690b447fcaa63983427d9a79b021.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*fQzu3nFtE3LGbE5H2WgWbQ.png"/></div></figure><p id="fefe" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">通过在分子中加减一，我们可以再次拆分分数，并拉出另一个 sigmoid 函数！</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/77b6fd88a457c3631fce8ded1f235bf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*6_XjfStMY3KyLhKH-8KvdQ.png"/></div></figure><p id="7279" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">现在我们可以简化，最后得到 sigmoid 函数的简化导数。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ad4eb2b6cecb5a83b6cb0802911a64c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*SNptsgb2S3egVhyJMGTF7Q.png"/></div></figure><p id="f953" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">如果我们将 sigmoid 函数写成<em class="na"> S(x) </em>，那么导数可以写成:</p><p id="a481" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">=<em class="na">(s(x)⋅(1−s(x))</em></p><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="d793" class="mq jr it mm b gy mr ms l mt mu"><strong class="mm iu">def</strong> <strong class="mm iu">feed_forward</strong>(self):<br/>    self<strong class="mm iu">.</strong>hidden <strong class="mm iu">=</strong> self<strong class="mm iu">.</strong>sigmoid(np<strong class="mm iu">.</strong>dot(self<strong class="mm iu">.</strong>inputs, self<strong class="mm iu">.</strong>weights))</span></pre><p id="668f" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">在我们的神经网络的训练过程中，输入数据将通过网络的权重和函数被前馈。这个前馈函数的结果将是隐藏层的输出，或者是隐藏层对给定权重的最佳猜测。输入数据中的每个要素对于其与隐藏图层的连接都有自己的权重。我们将从每个特征的总和乘以其相应的权重开始。一旦我们将输入矩阵和权重矩阵相乘，我们就可以将结果通过 sigmoid 函数压缩成介于(0–1)之间的概率。前向传播函数可以写成这样，其中<em class="na"> xᵢ </em>和<em class="na"> wᵢ </em>是矩阵中的单个特征和权重:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/ef09dcaa5a284001e6851b86b7869298.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*9E4amGjWeO7PJBiPC5mVzA.png"/></div></figure><p id="61ec" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">再次重申，隐藏层将按以下步骤计算:</p><ul class=""><li id="df2e" class="ni nj it kq b kr lm kv ln kz nk ld nl lh nm ll nn no np nq bi translated">将每个特征列与其权重相乘</li><li id="d08a" class="ni nj it kq b kr nr kv ns kz nt ld nu lh nv ll nn no np nq bi translated">对特征和权重的乘积求和</li><li id="6836" class="ni nj it kq b kr nr kv ns kz nt ld nu lh nv ll nn no np nq bi translated">将总和传递给 sigmoid 函数以产生输出$\hat y$。</li></ul><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/3a66792f292d418835186f986558772d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2TLvyzyD_VdhOSfVXMgwWw.png"/></div></div></figure><p id="cc52" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">上图显示了将每个特征与其对应的权重相乘，然后对乘积求和的过程。训练数据中的每一行都将这样计算。得到的 4x1 矩阵将被输入到 sigmoid 激活函数中，如下所示:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/738901b27a661162a5131734e9aeb01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XifHDEym3yVEsSt9ePxgXA.png"/></div></div></figure><p id="7791" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">上述过程将导致隐藏层的预测。<em class="na"> ∑xw </em>矩阵中的每一行都将进入 sigmoid 函数。颜色代表∑ <em class="na"> xw </em>矩阵中每一行的单独过程。<strong class="kq iu">注意:</strong>这个计算只代表<strong class="kq iu">一次训练迭代</strong>，所以得到的<em class="na"> ŷ </em>矩阵不会很精确。通过以这种方式计算隐藏层，然后使用反向传播进行多次迭代，结果将更加准确。</p><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="e65f" class="mq jr it mm b gy mr ms l mt mu"><strong class="mm iu">def</strong> <strong class="mm iu">backpropagation</strong>(self):<br/>    self<strong class="mm iu">.</strong>error  <strong class="mm iu">=</strong> self<strong class="mm iu">.</strong>outputs <strong class="mm iu">-</strong> self<strong class="mm iu">.</strong>hidden<br/>    delta <strong class="mm iu">=</strong> self<strong class="mm iu">.</strong>error <strong class="mm iu">*</strong> self<strong class="mm iu">.</strong>sigmoid(self<strong class="mm iu">.</strong>hidden, deriv<strong class="mm iu">=</strong>True)<br/>    self<strong class="mm iu">.</strong>weights <strong class="mm iu">+=</strong> np<strong class="mm iu">.</strong>dot(self<strong class="mm iu">.</strong>inputs<strong class="mm iu">.</strong>T, delta)</span></pre><p id="a926" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这是整个神经网络中最酷的部分:反向传播。反向传播将通过神经网络的层返回，确定哪些权重对输出和误差有贡献，然后基于隐藏层输出的梯度改变权重。这将被进一步解释，但是现在，整个过程可以被写成这样，其中<em class="na"> y </em>是正确的输出，<em class="na"> ŷ </em>是隐藏层预测:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi nw"><img src="../Images/653a3a8ff2327d1d6763a13a6f35ca30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uOyCCgiVriFtODqr2GyduQ.png"/></div></div></figure><p id="1adf" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">为了计算隐藏层预测的误差，我们将简单地取正确输出矩阵<em class="na"> y </em>和隐藏层矩阵<em class="na"> ŷ </em>之间的差。这个过程将在下面显示。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/404db57758d2e187b4ca898f982bde7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DWrIKv4FyIaE1KKTT0C5iw.png"/></div></div></figure><p id="813a" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">我们现在可以将误差和隐藏层预测的导数相乘。我们知道 sigmoid 函数的导数是<em class="na"> S(x)(1 — S(x)) </em>。因此，每个隐藏层预测的导数将是[(<em class="na">ŷ)(1-ŷ)]</em>。例如，隐藏层的预测矩阵中的第一行包含值$0.62$。我们可以用 0.62 美元代替<em class="na"> ŷ </em>，结果将是预测的导数。<em class="na">0.62 *(1–0.62)= 0.2356</em>。对<em class="na"> ŷ </em>矩阵中的每一行重复这一过程，将得到一个 4×1 的导数矩阵，然后与误差矩阵相乘。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/38f2afdce86821702e158962d3d2b014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pnBUEXlJ-P2Rnp9PIEqTOA.png"/></div></div></figure><p id="0eb1" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">将误差和导数相乘可以得到所需的变化。当 sigmoid 函数输出具有更高置信度的值(接近 0 或接近 1)时，导数将更小，因此所需的变化将更小。如果 sigmoid 函数输出更接近. 50 的值，则导数是更大的值，这意味着需要更大的变化，以便神经网络变得更有信心。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/89fd631d81c1e538b55ba2e3eb590496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H1OlGr4EA_BPuGB-6vk97Q.png"/></div></div></figure><p id="f3d2" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这一步将导致添加到权重中的更新。我们可以通过将上述步骤中的“误差加权导数”与输入相乘来获得此更新。如果输入中的要素为 0，则权重的更新将为 0，如果输入中的要素为 1，则更新将添加到中。这将产生一个(3×1)矩阵，它与我们的权重矩阵的形状相匹配。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/c844e75fc5dd1c884ecbcbf21e757475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CURMBBdh2lqGO6t8zJ2_qQ.png"/></div></div></figure><p id="a214" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">一旦我们有了更新的矩阵，我们可以将它添加到我们的权重矩阵中，以正式更改权重，使其变得更强。即使在一次训练迭代之后，也有一些明显的进步！如果查看更新后的权重矩阵，您可能会注意到矩阵中的第一个权重值更高。记住，我们的神经网络必须知道输入中的第一个特征决定了输出。我们可以看到，在每个输入示例中，我们的神经网络已经为连接到第一个特征的权重分配了更高的值！</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/bd7dc3d345ec8d7a23fb8273a774d3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3b-epov80Ijv-34sgG2Bg.png"/></div></div></figure><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="d8bb" class="mq jr it mm b gy mr ms l mt mu"><strong class="mm iu">def</strong> <strong class="mm iu">train</strong>(self, epochs<strong class="mm iu">=</strong>25000):<br/>    <strong class="mm iu">for</strong> epoch <strong class="mm iu">in</strong> range(epochs):<br/>        self<strong class="mm iu">.</strong>feed_forward()<br/>        self<strong class="mm iu">.</strong>backpropagation()<br/><br/>        self<strong class="mm iu">.</strong>error_history<strong class="mm iu">.</strong>append(np<strong class="mm iu">.</strong>average(np<strong class="mm iu">.</strong>abs(self<strong class="mm iu">.</strong>error)))<br/>        self<strong class="mm iu">.</strong>epoch_list<strong class="mm iu">.</strong>append(epoch)</span></pre><p id="52dc" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">训练神经网络的时候到了。在训练过程中，神经网络将“学习”输入数据中的哪些特征与其输出相关，并且它将学习做出准确的预测。为了训练我们的神经网络，我们将创建具有 25，000 个历元或迭代次数的训练函数。这意味着神经网络将重复权重更新过程 25，000 次。在 train 函数中，我们将调用我们的<code class="fe mx my mz mm b">feed_forward()</code>函数，然后调用<code class="fe mx my mz mm b">backpropagation()</code>函数。对于每次迭代，我们还将跟踪在<code class="fe mx my mz mm b">feed_forward()</code>函数完成后产生的错误。我们将通过将错误和纪元附加到先前初始化的列表来跟踪这一点。我确信有一种更简单的方法可以做到这一点，但是对于快速原型开发来说，这种方法现在已经很好了。</p><p id="8f07" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">对于我们神经网络中的每个权重，训练过程遵循以下等式:</p><ul class=""><li id="3b97" class="ni nj it kq b kr lm kv ln kz nk ld nl lh nm ll nn no np nq bi translated"><em class="na"> xᵢ </em> —输入数据中的特征</li><li id="ca8c" class="ni nj it kq b kr nr kv ns kz nt ld nu lh nv ll nn no np nq bi translated"><em class="na"> wᵢ </em> —正在更新的重量</li><li id="b706" class="ni nj it kq b kr nr kv ns kz nt ld nu lh nv ll nn no np nq bi translated"><em class="na"> Xᵀ </em> —转置输入数据</li><li id="e80c" class="ni nj it kq b kr nr kv ns kz nt ld nu lh nv ll nn no np nq bi translated"><em class="na"> y </em> —正确输出</li><li id="3852" class="ni nj it kq b kr nr kv ns kz nt ld nu lh nv ll nn no np nq bi translated"><em class="na"> ŷ </em> —预测产量</li><li id="1f92" class="ni nj it kq b kr nr kv ns kz nt ld nu lh nv ll nn no np nq bi translated">(y — <em class="na"> ŷ </em> ) —错误</li><li id="f293" class="ni nj it kq b kr nr kv ns kz nt ld nu lh nv ll nn no np nq bi translated">∑<em class="na">xᵢw</em>ᵢ-输入特征和权重的乘积之和</li><li id="6268" class="ni nj it kq b kr nr kv ns kz nt ld nu lh nv ll nn no np nq bi translated"><em class="na"> S(∑xᵢwᵢ) </em> —乙状结肠功能</li></ul><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/bb29a97f1cbe1044833baf0906850acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*lvpCnN7fBjNe4N9LCkhimQ.png"/></div></figure><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="0087" class="mq jr it mm b gy mr ms l mt mu"><strong class="mm iu">def</strong> <strong class="mm iu">predict</strong>(self, new_input):<br/>    prediction <strong class="mm iu">=</strong> self<strong class="mm iu">.</strong>sigmoid(np<strong class="mm iu">.</strong>dot(new_input, self<strong class="mm iu">.</strong>weights))<br/>    <strong class="mm iu">return</strong> prediction</span></pre><p id="6742" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">既然神经网络已经被训练并且已经学习了输入数据中的重要特征，我们就可以开始进行预测了。预测函数看起来类似于隐藏层，或<code class="fe mx my mz mm b">feedforward()</code>函数。前向传播函数本质上也进行预测，然后反向传播检查误差并更新权重。我们的预测函数将使用与前馈函数相同的方法:将输入矩阵和权重矩阵相乘，然后通过 sigmoid 函数返回 0-1 之间的值。希望我们的神经网络能够做出尽可能接近实际输出的预测。</p><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="e8d8" class="mq jr it mm b gy mr ms l mt mu">NN = NeuralNetwork(inputs, outputs)</span></pre><p id="1622" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">我们将从 NeuralNetwork 类创建 NN 对象，并传入输入矩阵和输出矩阵。</p><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="efb9" class="mq jr it mm b gy mr ms l mt mu">NN.train()</span></pre><p id="898d" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">然后我们可以在我们的神经网络对象上调用<code class="fe mx my mz mm b">.train()</code>函数。</p><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="8d02" class="mq jr it mm b gy mr ms l mt mu">example <strong class="mm iu">=</strong> np<strong class="mm iu">.</strong>array([[1, 1, 0]])<br/>example_2 <strong class="mm iu">=</strong> np<strong class="mm iu">.</strong>array([[0, 1, 1]])<br/><br/><strong class="mm iu">print</strong>(NN<strong class="mm iu">.</strong>predict(example), ' - Correct: ', example[0][0])<br/><strong class="mm iu">print</strong>(NN<strong class="mm iu">.</strong>predict(example_2), ' - Correct: ', example_2[0][0])</span></pre><p id="19c4" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">输出</p><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="eec4" class="mq jr it mm b gy mr ms l mt mu">[[0.99089925]]  - Correct:  1<br/>[[0.006409]]  - Correct:  0</span></pre><p id="450c" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">现在我们可以创建两个新的例子，我们希望我们的神经网络对其进行预测。我们将这些称为“示例”和“示例 _2”。然后我们可以调用<code class="fe mx my mz mm b">.predict()</code>函数并传递数组。我们知道，输入中的第一个数字或特征决定了输出。第一个示例“example”在第一列中有一个 1，因此输出应该是 1。第二个示例在第一列中有一个 0，因此输出应该是 0。</p><pre class="ls lt lu lv gt ml mm mn mo aw mp bi"><span id="fe6b" class="mq jr it mm b gy mr ms l mt mu">plt<strong class="mm iu">.</strong>figure(figsize<strong class="mm iu">=</strong>(15,5))<br/>plt<strong class="mm iu">.</strong>plot(NN<strong class="mm iu">.</strong>epoch_list, NN<strong class="mm iu">.</strong>error_history)<br/>plt<strong class="mm iu">.</strong>xlabel('Epoch')<br/>plt<strong class="mm iu">.</strong>ylabel('Loss')</span></pre><p id="2fd1" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">训练完成后，我们可以绘制每次训练迭代的误差。该图显示，在较早的时期，误差有很大的下降，但是在大约 5000 次迭代之后，误差稍微稳定下来。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mw"><img src="../Images/f49bd4a5e5c62a236149c2940b930d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MK7_0r_s63_vZ31lBApVJg.jpeg"/></div></div></figure></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="83c9" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">我希望这是对使用 python 和面向对象方法的神经网络的一个很好的介绍。</p></div></div>    
</body>
</html>