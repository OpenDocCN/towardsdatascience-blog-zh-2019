<html>
<head>
<title>Brief Introduction to Attention Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意力模型简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-networks-c735befb5e9f?source=collection_archive---------6-----------------------#2019-09-06">https://towardsdatascience.com/attention-networks-c735befb5e9f?source=collection_archive---------6-----------------------#2019-09-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="7cb7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在自然语言处理、机器翻译领域已经有了新的发展，并且大多数最新的(SOTA)结果已经使用注意力模型实现。胶水基准组长<a class="ae ko" href="https://gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank">板上最上面的模型</a>使用自关注模型<em class="kp">变形金刚。</em></p><p id="71af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了从最基本的意义上理解注意力，让我们从<em class="kp">吴恩达教授关于深度学习的</em>课程中举个例子</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/1143eafb2392792ac07e03cdac7eb57a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ruUg8XLgKr5VzuG8BCKHbQ.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Source — Deep Learning Coursera</figcaption></figure><p id="2aff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以上<em class="kp">注意</em>模型基于“<em class="kp"> Bahdanau 等人，2014 联合学习对齐和翻译的神经机器翻译”</em>的一篇论文。这是一个使用双向递归神经网络和<em class="kp">注意力进行序列到序列句子翻译的例子。</em>此处，上图中的符号“alpha”代表输出向量的每个时间步长的注意力权重。有几种方法计算这些权重“α”,例如使用<strong class="js iu">点积、具有单个隐藏层的神经网络模型</strong>等。这些权重乘以源中的每个单词，然后该乘积与来自前一层的输出一起被馈送到语言模型，以获得当前时间步长的输出。这些字母值决定了源中每个单词的重要性，从而决定了输出的句子。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi lg"><img src="../Images/e43640d53f57231d5c6aafcd100a0d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dG_bQM0BA1weaM-IXFraJQ.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Souce-Lilian Weng Github post.</figcaption></figure><p id="0216" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它是可用于计算注意力权重(alpha)的不同函数的列表，更普遍的说法是<em class="kp">对齐分数。</em>在(加性)函数中(<em class="kp"> s &lt; t &gt;，h &lt; i &gt; ) </em>将前一时间步和源嵌入的输出串联起来，通过单层神经网络，输出为注意权重(α)。该神经网络模型与 RNN 模型并行训练，并且相应地调整这些权重。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h2 id="480a" class="lo lp it bd lq lr ls dn lt lu lv dp lw kb lx ly lz kf ma mb mc kj md me mf mg bi translated">注意力模型的类型:</h2><ol class=""><li id="c9e6" class="mh mi it js b jt mj jx mk kb ml kf mm kj mn kn mo mp mq mr bi translated">全球和地方关注(地方-m，地方-p)</li><li id="ebe5" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">软硬注意</li><li id="e481" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">自我关注</li></ol><p id="23b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">全局注意力模型</strong></p><p id="1bcc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这和上面讨论的注意力模型是一样的。来自当前状态之前的每个源状态(编码器)和解码器状态的输入被考虑来计算输出。下面是全球注意力模型的图表。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/114e8298a533ccebc03bddcd407dd7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*WmXfpYa_aIOMYTavmFW4aw.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Source-(Luong et.al. 2015)</figcaption></figure><p id="bdfb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从图中可以看出，<em class="kp"> (a &lt; t &gt; ) </em>对齐权重或注意权重是使用每个编码器步骤和<em class="kp"> (h &lt; t &gt; ) </em>解码器前一步骤计算的。然后使用<em class="kp"> (a &lt; t &gt; ) </em>通过取<em class="kp">全局对齐权重</em>和<em class="kp">每个编码器步长</em>的乘积来计算上下文向量。然后，它被馈送到 RNN 单元，以找到解码器输出。</p><p id="12b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">局部注意力模型</strong></p><p id="1a6e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它与<em class="kp">全局注意力模型</em>的不同之处在于，在<em class="kp">局部注意力模型</em>中，仅使用来自源(编码器)的少数位置来计算对齐权重<em class="kp"> (a &lt; t &gt;)。下面是局部注意力模型的示意图。</em></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/16f5c7c900c064657eb90e9e103fb741.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*rcHVQN4QGwNWBOs69QeJsg.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Source-(Luong et.al. 2015)</figcaption></figure><p id="3d8a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从图中可以看出，首先找到单对齐位置<em class="kp"> (p &lt; t &gt; ) </em>，然后使用来自源(编码器)层的单词窗口连同<em class="kp"> (h &lt; t &gt; ) </em>来计算对齐权重和上下文向量。</p><p id="453c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kp">局部注意- </em>有单调对齐和预测对齐两种。在单调比对中，我们简单地将位置<em class="kp"> (p &lt; t &gt; ) </em>设置为“t”，而在预测比对中，位置<em class="kp"> (p &lt; t &gt; ) </em>不是仅仅假设为“t”，而是由预测模型预测。</p><p id="7b3e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">软硬注意</strong></p><p id="c9c0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">软注意几乎和全局注意模型一样。</p><p id="cfad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">硬注意模型和局部注意模型的区别在于，局部模型在每一点上几乎都是不同的，而硬注意却不是。局部注意是硬注意和软注意的混合。最后给出了进一步研究的链接。</p><p id="3ff7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">自我关注模型</strong></p><blockquote class="mz na nb"><p id="880b" class="jq jr kp js b jt ju jv jw jx jy jz ka nc kc kd ke nd kg kh ki ne kk kl km kn im bi translated">关于同一输入序列的不同位置。理论上，自我注意可以采用上述任何得分函数，但只是用相同的输入序列替换目标序列。</p></blockquote><p id="f8d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">变压器网络</strong></p><p id="6236" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">变压器网络完全建立在自我关注机制上，不使用递归网络架构。变形金刚是用多头自关注模型制作的。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi nf"><img src="../Images/37a1e1a082159654e01dab1eb41386b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7pPAgaX58QkKnm0MO28X4Q.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Source- Attention is all you need.</figcaption></figure><p id="1e20" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">编码器层由两个子层组成，一个是多头注意，另一个是前馈神经网络。解码器由三个子层两个多头注意力网络构成，然后反馈给前馈网络。解码注意力网络从编码器输出获得输入，从解码器的前一层获得输出。</p><p id="dab9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">参考链接</strong></p><ol class=""><li id="35d8" class="mh mi it js b jt ju jx jy kb ng kf nh kj ni kn mo mp mq mr bi translated"><a class="ae ko" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">注意？立正！</a></li><li id="db74" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated"><a class="ae ko" href="https://nlp.stanford.edu/pubs/emnlp15_attn.pdf" rel="noopener ugc nofollow" target="_blank">神经机器翻译的有效方法</a></li><li id="9bb8" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated"><a class="ae ko" href="https://www.coursera.org/lecture/nlp-sequence-models/attention-model-lSwVa" rel="noopener ugc nofollow" target="_blank"> Coursera 注意</a></li><li id="6895" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated"><a class="ae ko" href="https://skymind.ai/wiki/attention-mechanism-memory-network" rel="noopener ugc nofollow" target="_blank">初学者注意指南</a></li><li id="a965" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated"><a class="ae ko" href="https://gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank">胶水基准排行榜</a></li><li id="fd4b" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated"><a class="ae ko" href="http://proceedings.mlr.press/v37/xuc15.pdf" rel="noopener ugc nofollow" target="_blank">关注 CNN </a></li></ol><p id="1673" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">引文</strong></p><ol class=""><li id="e448" class="mh mi it js b jt ju jx jy kb ng kf nh kj ni kn mo mp mq mr bi translated">通过联合学习对齐和翻译的神经机器翻译</li><li id="fed1" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">神经机器翻译的有效方法。</li></ol><p id="6081" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果有些地方不正确，或者我应该再补充些什么，请在下面评论。</p></div></div>    
</body>
</html>