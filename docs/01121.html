<html>
<head>
<title>You Don’t Know SVD (Singular Value Decomposition)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你不知道 SVD(奇异值分解)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/svd-8c2f72e264f?source=collection_archive---------1-----------------------#2019-02-21">https://towardsdatascience.com/svd-8c2f72e264f?source=collection_archive---------1-----------------------#2019-02-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0b29" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">真正理解 SVD——直观的核心思想</h2></div><p id="196a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回到基础力学，你知道任何力矢量都可以分解成沿<em class="le"> x </em>和<em class="le"> y </em>轴的分量:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/9f07e2877b7f8a51dad8bec7875fa075.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/1*AP19J0U3bJUy8zbodGS_4g.gif"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(Animation by author)</figcaption></figure><blockquote class="lr ls lt"><p id="c732" class="ki kj le kk b kl km ju kn ko kp jx kq lu ks kt ku lv kw kx ky lw la lb lc ld im bi translated"><strong class="kk iu"> <em class="it">祝贺。</em> </strong> <em class="it">现在你知道什么是奇异值分解了。</em></p></blockquote><p id="aaa9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">令人失望的是，几乎 SVD <strong class="kk iu">的每个教程都把它变得比必要的更复杂，而核心思想却非常简单。</strong></p><p id="58a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于数学只是为同一概念赋予不同名称的艺术，<strong class="kk iu"> SVD 只不过是将向量分解到正交轴上</strong>——我们只是决定它可能需要一个更豪华的名称。</p><p id="20f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来看看这是怎么回事。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="07e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当向量(<strong class="kk iu"> <em class="le"> a </em> </strong>)被分解时，我们得到 3 条信息:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi me"><img src="../Images/52ec7aeb433edc2439f69b196c363359.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*6SbRBz9ZPqwCeV9jL7AzNQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(Image by author)</figcaption></figure><ol class=""><li id="8e8d" class="mf mg it kk b kl km ko kp kr mh kv mi kz mj ld mk ml mm mn bi translated">投影的<strong class="kk iu">方向</strong>—<strong class="kk iu">单位</strong>矢量(<strong class="kk iu"> <em class="le"> v </em> ₁ </strong>和<strong class="kk iu"> <em class="le"> v </em> ₂ </strong> ) <strong class="kk iu">代表我们投影(分解)的方向</strong>。在上面，它们是 x 轴<em class="le">和 y 轴<em class="le">和</em>，但是可以是任何其他正交轴。</em></li><li id="cc1e" class="mf mg it kk b kl mo ko mp kr mq kv mr kz ms ld mk ml mm mn bi translated">投影的<strong class="kk iu">长度</strong>(线段<strong class="kk iu">s<strong class="kk iu"><em class="le">ₐ</em></strong>₁和 s<strong class="kk iu"><em class="le">ₐ</em></strong>₂)——它告诉我们<strong class="kk iu">在每个投影方向上包含了</strong>多少矢量(更多的矢量<strong class="kk iu"> <em class="le"> a </em> </strong>是向<strong class="kk iu"> <em class="le"> v </em> ₁ </strong>倾斜的)</strong></li><li id="311a" class="mf mg it kk b kl mo ko mp kr mq kv mr kz ms ld mk ml mm mn bi translated">投影的<strong class="kk iu">个矢量</strong>(<strong class="kk iu"><em class="le">pₐ</em>₁</strong>和<strong class="kk iu"><em class="le">pₐ</em>₂</strong>)——用于将<strong class="kk iu">个原始矢量<strong class="kk iu"><em class="le"/></strong>相加(作为矢量和)， 而对于这一点很容易验证，<strong class="kk iu"><em class="le">pₐ</em></strong>= s<strong class="kk iu"><em class="le">ₐ</em></strong>₁*<strong class="kk iu"><em class="le">v</em>₁</strong>和<strong class="kk iu"><em class="le">pₐ</em>₂</strong>= s<strong class="kk iu"><em class="le">ₐ₂</em></strong>*<strong class="kk iu"><em class="le">v</em>— </strong></strong></li></ol><h1 id="b2c5" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">关键结论:</h1><blockquote class="nl"><p id="e882" class="nm nn it bd no np nq nr ns nt nu ld dk translated">任何向量都可以表示为:</p><p id="d2be" class="nm nn it bd no np nq nr ns nt nu ld dk translated">1.投影方向单位向量(v₁、v₂……)。</p><p id="1d9c" class="nm nn it bd no np nq nr ns nt nu ld dk translated">2.投影到它们上面的长度(sₐ₁，sₐ₂，…)。</p></blockquote><p id="77e9" class="pw-post-body-paragraph ki kj it kk b kl nv ju kn ko nw jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">SVD 所做的就是<strong class="kk iu">将这个结论</strong>扩展到不止一个向量(或点)和所有维度:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/c6301fa467c5f082414967811d9da0af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*If--f6UO2UEr0cUFIZfaYQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">An example of a dataset (<strong class="bd ob">a point can be considered a vector through the origin</strong>). (Image by author)</figcaption></figure><p id="3036" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在的问题是知道如何处理这个烂摊子。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="ac7f" class="mt mu it bd mv mw oc my mz na od nc nd jz oe ka nf kc of kd nh kf og kg nj nk bi translated">如何处理这个烂摊子</h1><p id="c3b1" class="pw-post-body-paragraph ki kj it kk b kl oh ju kn ko oi jx kq kr oj kt ku kv ok kx ky kz ol lb lc ld im bi translated">如果不先处理一个向量，我们就无法处理这些混乱！</p><p id="4ae5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你观察数学中的许多概括，你会发现它们主要利用<strong class="kk iu">矩阵。</strong></p><blockquote class="nl"><p id="10c9" class="nm nn it bd no np nq nr ns nt nu ld dk translated">所以我们必须找到一种方法，用矩阵来表达向量分解的运算。</p></blockquote><p id="1f99" class="pw-post-body-paragraph ki kj it kk b kl nv ju kn ko nw jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">这是一件很自然的事情:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/3ebb6145fa99d40494d4e95fb0acec37.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*PjtkRl4S1Gxx3c-g22-IiA.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Same figure as before, but tilting the axes of projection to convince you they aren’t confined to x and y. (aₓ and aᵧ are the coordinates of vector <strong class="bd ob">a</strong>, put into a column matrix (aka column vector), as per convention. Same for v₁ and v₂). (Image by author)</figcaption></figure><p id="d6e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们要分解(投影)矢量<strong class="kk iu"> <em class="le"> a </em> </strong>沿着单位矢量<strong class="kk iu"> <em class="le"> v </em> ₁ </strong>和<strong class="kk iu"> <em class="le"> v </em> ₂ </strong>。</p><p id="5336" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能已经知道(特别是如果你已经看过这个)投影是由<strong class="kk iu">点积</strong>完成的——它给出了投影的<strong class="kk iu">长度</strong>(s<strong class="kk iu"><em class="le">ₐ</em></strong>₁和 s <strong class="kk iu"> <em class="le"> ₐ </em> </strong> ₂):)</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/892c3d35870354261b7ffe3aac4b370c.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*Eir4xqcONlBP23A-9zVOeg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Projecting (a) onto v1 and v2. (Image by author)</figcaption></figure><p id="0d1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但那是多余的。我们可以利用矩阵的效率…</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/7620620579df006c08f723be76970ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*K0JAIU1a4sw0Ffy669lhLA.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">…to write both equations in one go, by adding an <strong class="bd ob">extra column</strong> for each unit vector. (Image by author)</figcaption></figure><p id="8b1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们甚至可以添加更多的点…</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/6fc2855779f04833faaf05861829f59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*CQ_iIUbILdzYKKDTuxgayA.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">…by adding an <strong class="bd ob">extra row</strong> for each point. <strong class="bd ob">S</strong> is the matrix containing the lengths of projections. (Image by author)</figcaption></figure><p id="02fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是添加那个点<strong class="kk iu"> <em class="le"> b </em> </strong>后的样子:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/df7c69e23910d88bb2684394a63ef2c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*PWK35jFr02pYNAZlt2VrbA.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(Image by author)</figcaption></figure><p id="b243" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在很容易推广到任意数量的点和维度:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/4630261832c5e47440d76c2c835ad491.png" data-original-src="https://miro.medium.com/v2/format:webp/1*ZzGoR55MU_wVkbT-Jez2qg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd ob">n</strong> = no. of points, <strong class="bd ob">d</strong> = no. of dimensions, <strong class="bd ob">A</strong> = matrix containing points, <strong class="bd ob">V</strong> = matrix containing the decomposition axes, <strong class="bd ob">S</strong> = matrix containing lengths of projection. (Image by author)</figcaption></figure><p id="98d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最佳的数学优雅。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/00cfd265dd245adbd910aca5681293d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/1*j1BY-Eblk6l7Y37FPvrQGQ.gif"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(Animation by author)</figcaption></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="1158" class="mt mu it bd mv mw oc my mz na od nc nd jz oe ka nf kc of kd nh kf og kg nj nk bi translated">概述:-</h1><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/f388631b33ce11ec9c7af4e4d27a1643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*23DHw0WOQ5sVYaM669yIjQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">The dot product in this case is just <strong class="bd ob">ordinary matrix multiplication</strong>. (Image by author)</figcaption></figure><p id="9629" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就相当于说:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/922612a2729f552e1eb24f83a2907f2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*LoumaKBHZhq7ggInRdISAQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Because <strong class="bd ob">V</strong> contains orthonormal columns, its inverse = its transpose (property of orthogonal matrices). (Image by author)</figcaption></figure><p id="6b78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SVD 就是这么说的(记住关键结论):</p><blockquote class="nl"><p id="61f8" class="nm nn it bd no np nq nr ns nt nu ld dk translated">任何一组向量(A)都可以用它们在某一组正交轴(V)上的投影长度(S)来表示。</p></blockquote><p id="4586" class="pw-post-body-paragraph ki kj it kk b kl nv ju kn ko nw jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">然而，我们还没有到达那一步。常规的奇异值分解公式表示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/ad9a6dfba1f935ecdef96f5a0cd470ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*gkwt4xzv1xxdmfFdCDJ0DA.png"/></div></figure><p id="2c69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但这只是意味着我们想看看:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/fcd6a74ad87b6b2acc9d6b67ef1a1175.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*uG99-n1AG7-vuAW1dWKnQw.png"/></div></figure><p id="d515" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是我们要做的。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="73e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你仔细观察矩阵<strong class="kk iu"> <em class="le"> S </em> </strong>，你会发现它包括:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/4c9ed0a55ca661af0504b7ce4192a285.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*qZT0ItYbaUPztABRBfDKTw.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(Image by author)</figcaption></figure><p id="b47a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实证明(原因将在后面看到)，如果我们能够<strong class="kk iu">规格化</strong>这些列向量，也就是说，使它们成为单位长度的<strong class="kk iu">，那是最好的。</strong></p><p id="fb7a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是通过做相当于<strong class="kk iu">将每个列向量除以其大小来完成的，但是以矩阵形式</strong>。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="cd45" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是首先，用一个数字例子来看看这个“除法”是怎么做的。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/73ebaf829ecd340413bece7a81c8b393.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*42LlKzv9IIxiy82ER1mXjQ.png"/></div></figure><p id="0a73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们要将<strong class="kk iu"> <em class="le"> M </em> </strong>的<strong class="kk iu"> <em class="le">第 1</em></strong>列除以<strong class="kk iu"> 2 </strong>。我们<strong class="kk iu"> </strong>肯定要<strong class="kk iu">乘以另一个矩阵</strong>才能保持等式:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/d4dbdc119268f3b08c03d1c887cbf199.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*2R5SdqhPyOp8kMNyJM2Viw.png"/></div></figure><p id="f529" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">验证未知矩阵无非是<strong class="kk iu">个单位矩阵，用除数</strong> <strong class="kk iu"> = 2 </strong>代替<em class="le">个第 1 个</em>元素:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/fef458827700f9f8a1988f0c4a0c21b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*fD0ktb2aovmHRgfFxQPqcw.png"/></div></figure><p id="04d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将<strong class="kk iu"> <em class="le"> 2nd </em> </strong>列除以<strong class="kk iu"> 3 </strong>现在变成了一件直接的事情——只需将单位矩阵的<strong class="kk iu"> <em class="le"> 2nd </em> </strong>元素替换为<strong class="kk iu"> 3 </strong>:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/1938094c14b2746895a4af6b8f586edf.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*xS6qwize_vqHXWjymVty_A.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">It should be obvious how this operation can be generalized to any matrix of any size.</figcaption></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="c2a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在要把上面的“除法”概念应用到矩阵<strong class="kk iu"> <em class="le"> S </em> </strong>上。</p><p id="a68e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了归一化<strong class="kk iu"> <em class="le"> S </em> </strong>的列，我们按照它们的大小来划分…</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="pd pe di pf bf pg"><div class="gh gi pc"><img src="../Images/0ad3263bb630d5736e9faf3407a5040d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lw7Mi_Hn5_--5CUpPIXj1g.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(Image by author)</figcaption></figure><p id="af97" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">…通过对上例中的<strong class="kk iu"> <em class="le"> S </em> </strong>做我们对<strong class="kk iu"> <em class="le"> M </em> </strong>做的事情:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="pd pe di pf bf pg"><div class="gh gi ph"><img src="../Images/fc6fe7c9f2da247cd17cc9803b46273e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBLxibQrOghXqUk6nKSDJQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(Image by author)</figcaption></figure><h1 id="e68a" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">最后…</h1><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/7415dbb4fc7a149fc17234d6b0ae8915.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*nTRaEa_ZlXcFKXT-rxQHIg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Singular Value Decomposition (Compact or Thin version)</figcaption></figure><p id="c89e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，一些精细的细节和严谨的数学被无可非议地掩盖起来，以便<em class="le">而不是</em>偏离核心概念。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="4e15" class="mt mu it bd mv mw oc my mz na od nc nd jz oe ka nf kc of kd nh kf og kg nj nk bi translated">解释</h1><p id="a599" class="pw-post-body-paragraph ki kj it kk b kl oh ju kn ko oi jx kq kr oj kt ku kv ok kx ky kz ol lb lc ld im bi translated">先说这个<strong class="kk iu"> <em class="le"> U </em> </strong>和<strong class="kk iu">σ</strong>…</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="pd pe di pf bf pg"><div class="gh gi pj"><img src="../Images/0f76557abdfedf93089b29ec195b3795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c6nLorFvkOjsE9AaSJalMA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(Image by author)</figcaption></figure><p id="e22b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">西格玛夫妇呢。为什么我们要用正常化的<strong class="kk iu"><em class="le"/></strong>来寻找它们呢？</p><p id="0b17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经看到，(σ <strong class="kk iu"> <em class="le"> ᵢ </em> </strong>)是投影长度平方之和的<strong class="kk iu">平方根，所有点的</strong>，<strong class="kk iu"> </strong>到第<em class="le"> i </em>个单位矢量<strong class="kk iu"> <em class="le"> vᵢ </em> </strong>。</p><p id="953d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那是什么意思？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/94686874ae91002444abf44fdb7572fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/1*scgpE4GP9IHs_j8ZA9U-DA.gif"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Red segments = projections on v1. Blue segments = projections on v2. <strong class="bd ob">The closer the points to a specific axis of projection, the larger the value of the corresponding σ. </strong>(Animation by author)</figcaption></figure><p id="c1ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于 sigmas 在其定义中包含了特定轴上投影长度的总和，<strong class="kk iu">它们代表了所有点离该轴有多近。</strong></p><p id="5879" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，如果σ₁ &gt; σ₂，那么大多数点数更接近于<strong class="kk iu"> <em class="le"> v </em> ₁ </strong>而不是<strong class="kk iu"> <em class="le"> v </em> ₂ </strong>，反之亦然。</p><p id="9bee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这在 SVD 的无数应用中有着巨大的用途。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="92d3" class="mt mu it bd mv mw oc my mz na od nc nd jz oe ka nf kc of kd nh kf og kg nj nk bi translated">主要应用程序</h1><p id="5b52" class="pw-post-body-paragraph ki kj it kk b kl oh ju kn ko oi jx kq kr oj kt ku kv ok kx ky kz ol lb lc ld im bi translated">求矩阵的奇异值分解的算法不是随机选择投影方向(矩阵的列<strong class="kk iu"> <em class="le"> V </em> </strong>)。</p><blockquote class="nl"><p id="1bc2" class="nm nn it bd no np nq nr ns nt nu ld dk translated">他们选择它们作为数据集的主要组成部分(矩阵 A)。</p></blockquote><p id="4b9f" class="pw-post-body-paragraph ki kj it kk b kl nv ju kn ko nw jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">如果你读过我的第一篇文章，你会很清楚主要成分是什么…</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/3c57928fdb626401efa4073c57707f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*jDjGTd1WZhmupuJVDfR6rQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">…they’re the lines of largest variation (largest variance). (Image by author)</figcaption></figure><p id="d8ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从<a class="ae on" rel="noopener" target="_blank" href="/https-medium-com-abdullatif-h-dimensionality-reduction-for-dummies-part-1-a8c9ec7b7e79">同样的</a>你也知道<strong class="kk iu">降维</strong>的目标是<strong class="kk iu">将数据集投影到方差最大</strong>的直线(或平面)上:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/0cc63f66043055b66f2302db739460b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*N2_X-fLxjAx_ezENkO6ljQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Magenta: points before projection. Violet: points after projection (reduced dimensionality). (Image by author)</figcaption></figure><p id="34fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，使用 SVD 投影数据集的行为变得轻而易举，因为<strong class="kk iu">所有的点都已经<em class="le">投影</em> </strong>(分解)在所有的主成分上(即<strong class="kk iu"> <em class="le"> vᵢ </em> </strong>单位向量):</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="pd pe di pf bf pg"><div class="gh gi pm"><img src="../Images/a4c89ff499a938c23c4d38a2934c64da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*krHwW9Yj7YuuUImZU3nraA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(Image by author)</figcaption></figure><p id="c447" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，要将数据集投影到第一个主成分上…</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/a4005a753820d2a9f1de6cc5ae80749a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*UIuoIOcvo_3S1j8OMqCAuA.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd ob">…all we have to do is remove all columns not related to the 1st principal component</strong>. The projected dataset in now <strong class="bd ob">A’</strong>. (Image by author)</figcaption></figure><p id="595f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将两个矩阵(上面的 s 和 V <em class="le"> ᵀ </em>相乘得到矩阵 a ’,其包含最后一个图中的投影点(紫色)。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="632f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">仅此而已…暂时如此。</p></div></div>    
</body>
</html>