<html>
<head>
<title>Reinforcement Learning Tutorial Part 1: Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习教程第 1 部分:Q-学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-tutorial-part-1-q-learning-cadb36998b28?source=collection_archive---------9-----------------------#2019-01-31">https://towardsdatascience.com/reinforcement-learning-tutorial-part-1-q-learning-cadb36998b28?source=collection_archive---------9-----------------------#2019-01-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d253" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是关于强化学习的系列教程的第一部分。我们将从一些理论开始，然后在下一部分的<a class="ae kl" rel="noopener" target="_blank" href="/reinforcement-learning-tutorial-part-2-cloud-q-learning-fc101e1cdcf5">中继续讨论更实际的东西。在这个系列中，您不仅将学习如何训练您的模型，还将学习使用</a><a class="ae kl" href="https://valohai.com/" rel="noopener ugc nofollow" target="_blank"> Valohai </a>深度学习管理平台在具有完全版本控制的云中训练它的最佳工作流程。</p><h1 id="2474" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">什么时候使用强化学习？</h1><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/aa3e2e64e026ef11760a46132d0d23eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qC_TEqwMWxXPyzZ5WCcbgA.jpeg"/></div></div></figure><p id="f44e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当你没有关于问题的训练数据或足够具体的专业知识时，强化学习是有用的。</p><p id="ed01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在高层次上，你知道自己想要什么，但不知道如何去实现。毕竟，就连<a class="ae kl" href="https://en.wikipedia.org/wiki/Lee_Sedol" rel="noopener ugc nofollow" target="_blank">李·塞多尔</a>也不知道如何在围棋中击败自己。</p><p id="776b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">幸运的是，你所需要的只是一个奖励机制，如果你让它“玩”足够长的时间，强化学习模型会找出如何最大化奖励。这类似于教狗用零食坐下。起初，这只狗毫无头绪，在你的命令下尝试随机的事情。在某个时候，它意外地用屁股着地，得到了突然的奖励。随着时间的推移，并给予足够的迭代，它会找出专家的策略，坐在提示。</p><h1 id="1f41" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">地牢中的会计师示例</h1><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lw"><img src="../Images/076a65562d70778634da32f9c38e76b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T5saQeWUV9A5GYmHAaHnKQ.jpeg"/></div></div></figure><p id="5e56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是强化学习的官僚版本。一个会计发现自己在一个黑暗的地牢里，他所能想到的就是走来走去填写电子表格。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/407d1ce0067363e8222719d4e7c4ee6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/0*eKH3tx237Cg6Penx"/></div></figure><p id="16f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">会计知道的:</p><ul class=""><li id="cb88" class="ly lz iq jp b jq jr ju jv jy ma kc mb kg mc kk md me mf mg bi translated">地牢有 5 块瓷砖长</li><li id="5235" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">可能的操作有向前和向后</li><li id="3018" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">向前总是一步，除了最后一个瓷砖碰到墙</li><li id="8362" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">后退总是带你回到起点</li><li id="43aa" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">有时会有风把你的动作吹向相反的方向</li><li id="7f24" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">您将在一些瓷砖上获得奖励</li></ul><p id="3c31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">会计<strong class="jp ir">不知道的是:</strong></p><ul class=""><li id="6bba" class="ly lz iq jp b jq jr ju jv jy ma kc mb kg mc kk md me mf mg bi translated">进入最后一张牌会给你+10 奖励</li><li id="0805" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">进入第一个方块给你+2 奖励</li><li id="438a" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">其他瓷砖没有奖励</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/563fb80718440f43cd4ed1f79965f527.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*zxdAF4lOLkl80UWCCk0WJQ.png"/></div></figure><p id="85ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">会计师，作为一名会计师，将遵循一个安全(但幼稚)的策略:</p><ul class=""><li id="166a" class="ly lz iq jp b jq jr ju jv jy ma kc mb kg mc kk md me mf mg bi translated">总是根据你的会计选择最有利可图的行动</li><li id="cd34" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">如果会计显示所有选项都为零，请选择一个随机操作</li></ul><p id="7978" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看会发生什么:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/9f4b163777873c9aed6e7015fc7ddf95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*gE1YkplgEMkRDVORRW1dUw.png"/></div></figure><p id="6e48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">会计师似乎总是喜欢向后走，即使我们知道最优策略可能总是向前走。最好的奖励(+10)毕竟在地下城的尽头。因为有一个随机因素有时会使我们的行动转向相反的方向，会计师实际上有时会不情愿地到达另一端，但根据电子表格仍然犹豫选择向前。这是为什么呢？</p><p id="499b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">很简单，因为他选择了一个非常贪婪的策略。在非常短暂的初始随机性中，会计师可能愿意选择向前一次或两次，但一旦他选择向后一次，他就注定永远选择向后。玩这个地下城需要长期的计划，拒绝较小的直接奖励，以获得更大的奖励。</p><h1 id="9c4a" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">什么是 Q-learning？</h1><p id="6393" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">q 学习是所有强化学习的核心。AlphaGO 战胜 Lee Sedol 或<a class="ae kl" href="https://medium.freecodecamp.org/explained-simply-how-deepmind-taught-ai-to-play-video-games-9eb5f38c89ee" rel="noopener ugc nofollow" target="_blank"> DeepMind 击败老款 Atari 游戏</a>从根本上来说都是在糖的基础上进行的 Q 学习。</p><p id="bba9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Q-learning 的核心是像<a class="ae kl" href="https://www.geeksforgeeks.org/markov-decision-process/" rel="noopener ugc nofollow" target="_blank">马尔可夫决策过程(MDP) </a>和<a class="ae kl" href="https://en.wikipedia.org/wiki/Bellman_equation" rel="noopener ugc nofollow" target="_blank">贝尔曼方程</a>这样的东西。虽然详细理解它们可能会有好处，但现在让我们将其简化为一种更简单的形式:</p><p id="0f38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">行动的价值=即时价值+所有最佳未来行动的总和</strong></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/9a4d83742f5340a2c86856f7419fd588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*11la3rrMt1y5-vCYcDowow.png"/></div></figure><p id="5a76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就像估算一个大学学位相对于退学的经济价值。你需要考虑的不仅仅是你第一份薪水的直接价值，而是你一生中所有未来薪水的总和。一个行动总是导致更多的行动，你选择的道路总是由你的第一个行动决定。</p><p id="2a34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">也就是说，仅仅关注行动是不够的。执行的时候需要考虑自己所处的状态。当你 17 岁和 72 岁时进入大学是完全不同的。第一个可能是一个财务上积极的赌注，而后者可能不是。</p><p id="970a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么你如何知道哪些未来行动是最优的呢？嗯——你不知道。你要做的是根据你当时掌握的信息做出最佳选择。</p><p id="65af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想象一下，你可以不止一次，而是一万次地重演你的人生。起初，你会很随意地去做，但是在几千次尝试之后，你会有越来越多的关于产生最佳回报的选择的信息。在 Q-learning 中,“你所拥有的信息”是从你以前的游戏中收集的信息，并编码到一个表格中。</p><p id="c36f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以从某种意义上说，你就像上一个例子中的会计，总是随身携带一个电子表格。不过，你将会以一种更微妙的方式更新和阅读你的电子表格。你也会把它叫做 Q 表而不是电子表格，因为它听起来更酷，对吗？</p><h1 id="0553" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">Q 学习算法</h1><p id="c372" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">Q-learning 算法的形式如下:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mu"><img src="../Images/a2a24c1ed0c9061628d758ae08110e6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r-F8AfutP0a8gPWs_5BBLQ.png"/></div></div></figure><p id="ad1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它看起来有点吓人，但是它做的很简单。我们可以将其概括为:</p><p id="2ac7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">根据我们得到的奖励和我们下一步期望的奖励，更新行动的价值评估。</strong></p><p id="eb1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是我们正在做的根本事情。T2 学习率 T3 和 T4 折扣 T5 虽然是必需的，但只是用来调整行为。折扣将决定我们对未来预期行动价值与我们刚刚经历的行动价值的权衡程度。学习率是一个总的油门踏板。开得太快，你会错过最佳时机，开得太慢，你永远也到不了。</p><p id="9ac3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是这个算法是不够的——它只是告诉我们如何更新我们的电子表格，但没有告诉我们如何使用电子表格在地牢中的行为！</p><h1 id="3bfd" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">地牢里的赌徒的例子</h1><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mv"><img src="../Images/6300f98a625974cb3048e67acf857d78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A21lsa63XHIxxGUFXyN4nw.jpeg"/></div></div></figure><p id="c949" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看我们将如何在一个地牢里用我们的花式 Q 表和一点赌博来行动。我们将选择一些更加细致入微的东西，而不是我们的会计师使用的照章办事的策略</p><p id="5a8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的战略:</p><ul class=""><li id="0ac2" class="ly lz iq jp b jq jr ju jv jy ma kc mb kg mc kk md me mf mg bi translated">默认情况下，从我们的电子表格中选择最有利可图的行动</li><li id="7bbf" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">有时赌博并选择随机行动</li><li id="7c39" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">如果会计显示所有选项都为零，请选择一个随机操作</li><li id="86b5" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">从 100%赌博(探索)开始，慢慢向 0%移动</li><li id="721d" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">使用折扣= 0.95</li><li id="ec78" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">使用 learning_rate = 0.1</li></ul><p id="9889" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么我们需要赌博和随机行动？原因和会计被困住了一样。因为我们的默认策略仍然是贪婪的，也就是说，我们默认选择最有利可图的选项，我们需要引入一些随机性来确保所有可能的<state action="">对都被探索。</state></p><p id="6314" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么我们需要停止赌博，降低我们的探索率？是因为达成最优策略的最佳方式是先积极探索，然后慢慢走向越来越保守。这个机制是所有机器学习的核心。</p><p id="3042" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">说够了。我们的赌徒正在进入地牢！</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/cb19b264df0112124adf99dfcd11dc46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*jKS_hINNL1NfNtyXd4sLjA.png"/></div></figure><p id="18cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">唷！那是一大堆东西。</p><p id="e377" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，为了举例，我们做了很多赌博来证明一个观点。不仅仅是赌博，我们把硬币抛向右边，所以这通常是非常不寻常的前十几步！</p><p id="8a74" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与会计师的策略相比，我们可以看到明显的不同。这种策略收敛较慢，但我们可以看到顶行(向前)比底行(向后)得到的估值更高。</p><p id="cc85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还要注意在最初的+10 奖励后，估价开始从右到左“泄漏”到顶行。这是允许 Q 表“预见未来”的基本机制。关于发生在最后一块瓷砖上的奖励丰厚的事情的消息慢慢流向左侧，最终到达我们电子表格的最左侧，这种方式允许我们的代理提前许多步预测好事情。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/648f1b56d3a7222081e9e9a4cf124666.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*Vxr_gxawp3GqLV0x7aXEnQ.png"/></div></figure><p id="ff46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你被上面一步一步运行的信息过载搞糊涂了，就冥想这张图片，因为它是本文中最重要的一张。如果你理解了为什么信息会“泄露”以及为什么它是有益的，那么你就会理解整个算法是如何工作的以及为什么会工作。</p><p id="9f04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就是这样！在<a class="ae kl" rel="noopener" target="_blank" href="/reinforcement-learning-tutorial-part-2-cloud-q-learning-fc101e1cdcf5">的下一部分</a>中，我们将提供一个教程，介绍如何使用<a class="ae kl" href="https://valohai.com/" rel="noopener ugc nofollow" target="_blank"> Valohai </a>深度学习管理平台在云中用代码实现这一点并运行它！</p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><p id="2a26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://blog.valohai.com/reinforcement-learning-tutorial-part-1-q-learning" rel="noopener ugc nofollow" target="_blank">第一部分:Q-Learning </a></p><p id="5725" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://blog.valohai.com/reinforcement-learning-tutorial-cloud-q-learning" rel="noopener ugc nofollow" target="_blank">第二部分:云 Q 学习</a></p><p id="45ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://blog.valohai.com/reinforcement-learning-tutorial-basic-deep-q-learning" rel="noopener ugc nofollow" target="_blank">第三部分:基础深度 Q 学习</a></p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><p id="5179" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nf">原载于</em><a class="ae kl" href="https://blog.valohai.com/reinforcement-learning-tutorial-part-1-q-learning" rel="noopener ugc nofollow" target="_blank"><em class="nf">blog.valohai.com</em></a><em class="nf">。</em></p></div></div>    
</body>
</html>