<html>
<head>
<title>Discriminating network for Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类判别网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/discriminating-network-for-classification-fb18d87ba21b?source=collection_archive---------3-----------------------#2019-04-13">https://towardsdatascience.com/discriminating-network-for-classification-fb18d87ba21b?source=collection_archive---------3-----------------------#2019-04-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="863a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我如何使用暹罗网络建立一个只有很少图像的分类器</h2></div><h1 id="c0c3" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">一个有鉴别能力的网络——它是什么，我们为什么需要它，我们如何建立它？</h1><p id="901f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们经常遇到的问题是，我们没有足够的高质量图像和标签数据集来训练一个健壮的基于 CNN 的分类器——要么我们没有足够的图像，要么我们没有手动资源来标记它们。用很少的图像建立一个健壮的分类器通常是一个挑战，因为我们需要成千上万的标签图像来训练一个健壮的神经网络体系结构</p><p id="212f" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在面部识别中经常使用有区别的连体结构。根据这篇研究论文，现代面部识别模型通常是使用暹罗网络(也称为一次性学习)建立的</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="md me l"/></div></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/77bc5ba047abb1b187a85f382a4e5a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9J-N9-6sq9FhEgCbnGJ6aQ.png"/></div></div></figure><p id="4b58" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">以上是一张连体网。这是简单明了的两个网络——在本例中，两个 Resnet-50 架构的权重相同——一个左网络和一个右网络。每个网络为其对应的图像输出数字串的编码。暹罗网络广泛用于面部识别。在这种情况下，我已经用它对猫和狗进行了分类。这种建筑学习的方式是——一只猫和另一只猫比和一只狗更相似。不同的猫会有不同的特征，但是猫之间的不同会比猫和狗之间的不同小。这两种编码之间的差异是猫和狗的编码之间的欧几里德距离。如果编码属于两个狗或猫图像，我们将目标标记为阳性或 1，否则如果一个图像是狗，另一个图像是猫，反之亦然，我们将目标标记为阴性类别或 0</p><p id="c0eb" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">这方面另一篇有趣的研究论文:</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="32b9" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">编码片段:</p><p id="8fac" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">正在加载所有必需的库</p><pre class="ly lz ma mb gt mm mn mo mp aw mq bi"><span id="2122" class="mr kg iq mn b gy ms mt l mu mv">from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Activation, Dropout<br/>from keras.models import Model, Sequential<br/>from keras.regularizers import l2<br/>from keras import backend as K<br/>from keras.optimizers import Adam<br/>from keras import optimizers<br/>#from skimage.io import imshow<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import pandas as pd<br/>import random</span><span id="a9e0" class="mr kg iq mn b gy mw mt l mu mv">from keras.backend.tensorflow_backend import set_session<br/>from keras.applications import resnet50, vgg16, vgg19, xception, densenet, inception_v3, mobilenet, mobilenetv2, nasnet, inception_resnet_v2<br/>import tensorflow as tf<br/>from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, EarlyStopping<br/>from keras.applications.resnet50 import preprocess_input<br/>#from keras.applications.xception import preprocess_input<br/>import os<br/>import datetime<br/>import json<br/>from keras.preprocessing.image import ImageDataGenerator<br/></span></pre><p id="328b" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">让我们使用 OpenCV 库抓取和处理图像。我用过 100 张猫的图片和 100 张狗的图片。猫和狗的形象属于不同的品种。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/b783fb562151d79e81c1f93125eb53ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*RIp1_zY55gCCP7AwJM2qtw.png"/></div></figure><p id="fd8c" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">100 张猫咪图片:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi my"><img src="../Images/6a43cc5779b58dfb6d78beaf3f86f31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPUzk2KT6Pxhi7ruRbWS5g.png"/></div></div></figure><p id="e663" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">100 张狗狗图片:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mz"><img src="../Images/ebfd12db7d068193760ff36be44aeca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMkDhYcjPiD_wOoFvtYRig.png"/></div></div></figure><p id="e7ae" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">加载和预处理图像的代码片段:</p><pre class="ly lz ma mb gt mm mn mo mp aw mq bi"><span id="8607" class="mr kg iq mn b gy ms mt l mu mv">import glob<br/>import cv2<br/>from random import shuffle</span><span id="5dc8" class="mr kg iq mn b gy mw mt l mu mv">dog_path = 'Y:\\Partha\\dog_cats_100\\dog\\*.jpg'<br/>cat_path = 'Y:\\Partha\\dog_cats_100\\cat\\*.jpg'</span><span id="ba44" class="mr kg iq mn b gy mw mt l mu mv">addrsd = glob.glob(dog_path)<br/>addrsc = glob.glob(cat_path)<br/>    <br/>labelsd = [1 for addr in addrsd]  # 1 = dog, 0 =  cat<br/>labelsc = [0 for addr in addrsc]</span><span id="a6ce" class="mr kg iq mn b gy mw mt l mu mv"># loop over the input images</span><span id="a0eb" class="mr kg iq mn b gy mw mt l mu mv">datad = []</span><span id="cb89" class="mr kg iq mn b gy mw mt l mu mv">for imagePath in addrsd:<br/># load the image, pre-process it, and store it in the data list<br/>    img = cv2.imread(imagePath)<br/>    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)<br/>    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>    datad.append(img)</span><span id="4699" class="mr kg iq mn b gy mw mt l mu mv">datac = []<br/>for imagePath in addrsc:<br/># load the image, pre-process it, and store it in the data list<br/>    img = cv2.imread(imagePath)<br/>    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)<br/>    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>    datac.append(img)</span><span id="00f5" class="mr kg iq mn b gy mw mt l mu mv"># to shuffle data<br/>shuffle_data = True<br/>if shuffle_data:<br/>    d = list(zip(datad, labelsd))<br/>    c = list(zip(datac, labelsc))<br/>    e = d + c<br/>    shuffle(e)<br/>    data, labels = zip(*e)</span><span id="2d70" class="mr kg iq mn b gy mw mt l mu mv">del datad<br/>del datac<br/>del addrsd<br/>del addrsc<br/>    <br/>Y_train = np.array(labels)<br/>X_train = np.array(data, dtype="int8")</span><span id="337d" class="mr kg iq mn b gy mw mt l mu mv">#preprocess for Resnet- 50<br/>X_train =  preprocess_input(X_train)</span></pre><p id="d87f" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">定义架构的代码片段:</p><pre class="ly lz ma mb gt mm mn mo mp aw mq bi"><span id="9c7b" class="mr kg iq mn b gy ms mt l mu mv"># Two inputs one each - left and right image<br/>left_input = Input((224,224,3))<br/>right_input = Input((224,224,3))</span><span id="f0e8" class="mr kg iq mn b gy mw mt l mu mv">#Import Resnetarchitecture from keras application and initializing each layer with pretrained imagenet weights.</span><span id="6409" class="mr kg iq mn b gy mw mt l mu mv">'’'<br/>Please note that it’s usually better to intialize the layers with imagenet initializations than random. While training I will be updating the weights for each layer in each epoch. we don’t want to confuse this activity with transfer learning as I am not freezing any layer but initilializing each layer with imagenet weights<br/>'’'</span><span id="97b1" class="mr kg iq mn b gy mw mt l mu mv">convnet = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))</span><span id="fecb" class="mr kg iq mn b gy mw mt l mu mv"># Add the final fully connected layers</span><span id="6466" class="mr kg iq mn b gy mw mt l mu mv">x = convnet.output<br/>x = Flatten()(x)<br/>x = Dense(1024, activation="relu")(x)<br/>preds = Dense(18, activation='sigmoid')(x) # Apply sigmoid<br/>convnet = Model(inputs=convnet.input, outputs=preds)</span><span id="42b7" class="mr kg iq mn b gy mw mt l mu mv">#Applying above model for both the left and right images<br/>encoded_l = convnet(left_input)<br/>encoded_r = convnet(right_input)</span><span id="fa4c" class="mr kg iq mn b gy mw mt l mu mv"># Euclidian Distance between the two images or encodings through the Resnet-50 architecture</span><span id="37a3" class="mr kg iq mn b gy mw mt l mu mv">Euc_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))</span><span id="1118" class="mr kg iq mn b gy mw mt l mu mv"># use and add the distance function<br/>Euc_distance = Euc_layer([encoded_l, encoded_r])</span><span id="3168" class="mr kg iq mn b gy mw mt l mu mv">#identify the prediction<br/>prediction = Dense(1,activation='sigmoid')(Euc_distance)</span><span id="6e56" class="mr kg iq mn b gy mw mt l mu mv">#Define the network with the left and right inputs and the ouput prediction<br/>siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)</span><span id="33b5" class="mr kg iq mn b gy mw mt l mu mv">#define the optimizer. Here I have used SGD with nesterov momentum</span><span id="db4e" class="mr kg iq mn b gy mw mt l mu mv"><br/>optim = optimizers.SGD(lr=0.001, decay=.01, momentum=0.9, nesterov=True)</span><span id="ae6e" class="mr kg iq mn b gy mw mt l mu mv">#compile the network using binary cross entropy loss and the above optimizer</span><span id="3e8a" class="mr kg iq mn b gy mw mt l mu mv"><br/>siamese_net.compile(loss="binary_crossentropy",optimizer=optim,metrics=[’accuracy’])</span></pre><p id="ce64" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">现在，我已经创建了图像对。将有两个标签——1 和 0，或者我们可以说输出的正或负标签或类别。</p><p id="845f" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">创建测试训练数据集的代码段</p><pre class="ly lz ma mb gt mm mn mo mp aw mq bi"><span id="b2b5" class="mr kg iq mn b gy ms mt l mu mv">image_list = X_train[:180]<br/>label_list = Y_train[:180]</span><span id="faab" class="mr kg iq mn b gy mw mt l mu mv">left_input = []<br/>right_input = []<br/>targets = []</span><span id="81c9" class="mr kg iq mn b gy mw mt l mu mv">#Number of pairs per image<br/>pairs = 8</span><span id="3b2b" class="mr kg iq mn b gy mw mt l mu mv">#create the dataset to train on<br/>for i in range(len(label_list)):<br/>    for j in range(pairs):<br/># we need to make sure that we are not comparing with the same image<br/>        compare_to = i<br/>        while compare_to == i: <br/>            compare_to = random.randint(0,179)<br/>        left_input.append(image_list[i])<br/>        right_input.append(image_list[compare_to])<br/>        if label_list[i] == label_list[compare_to]:<br/>            # if the images are same then label - 1<br/>            targets.append(1.)<br/>        else:<br/>            # if the images are different then label - 0<br/>            targets.append(0.)<br/>            <br/>#remove single-dimensional entries from the shape of the arrays and making them ready to create the train &amp; datasets <br/> <br/>#the train data - left right images arrays and target label<br/>left_input = np.squeeze(np.array(left_input))<br/>right_input = np.squeeze(np.array(right_input))<br/>targets = np.squeeze(np.array(targets))</span><span id="1968" class="mr kg iq mn b gy mw mt l mu mv"># Creating test datasets - left, right images and target label</span><span id="d823" class="mr kg iq mn b gy mw mt l mu mv">dog_image = X_train[4] #dog_image = 1, cat_image = 0</span><span id="7bf4" class="mr kg iq mn b gy mw mt l mu mv">test_left = []<br/>test_right = []<br/>test_targets = []</span><span id="2177" class="mr kg iq mn b gy mw mt l mu mv">for i in range(len(Y_train)-180):<br/>    test_left.append(dog_image)<br/>    test_right.append(X_train[i+180])<br/>    test_targets.append(Y_train[i+180])</span><span id="421f" class="mr kg iq mn b gy mw mt l mu mv">test_left = np.squeeze(np.array(test_left))<br/>test_right = np.squeeze(np.array(test_right))<br/>test_targets = np.squeeze(np.array(test_targets))</span></pre><p id="2c7f" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在 GPU 中训练网络的代码</p><pre class="ly lz ma mb gt mm mn mo mp aw mq bi"><span id="1194" class="mr kg iq mn b gy ms mt l mu mv">import tensorflow as tf<br/>import os<br/>config = tf.ConfigProto()<br/>config.gpu_options.allow_growth = True<br/>session = tf.Session(config=config)<br/>#from keras_input_pipeline import *<br/>os.environ['CUDA_VISIBLE_DEVICES'] = '1'</span><span id="a708" class="mr kg iq mn b gy mw mt l mu mv">siamese_net.summary()<br/>with tf.device('/gpu:1'):<br/>    siamese_net.fit([left_input,right_input], targets,<br/>          batch_size=16,<br/>          epochs=30,<br/>          verbose=1,<br/>          validation_data=([test_left,test_right],test_targets))</span></pre><p id="f9ca" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">结果:</p><pre class="ly lz ma mb gt mm mn mo mp aw mq bi"><span id="fd1a" class="mr kg iq mn b gy ms mt l mu mv">__________________________________________________________________________________________________<br/>Layer (type)                    Output Shape         Param #     Connected to                     <br/>==================================================================================================<br/>input_10 (InputLayer)           (None, 224, 224, 3)  0                                            <br/>__________________________________________________________________________________________________<br/>input_11 (InputLayer)           (None, 224, 224, 3)  0                                            <br/>__________________________________________________________________________________________________<br/>model_7 (Model)                 (None, 18)           126367634   input_10[0][0]                   <br/>                                                                 input_11[0][0]                   <br/>__________________________________________________________________________________________________<br/>lambda_4 (Lambda)               (None, 18)           0           model_7[1][0]                    <br/>                                                                 model_7[2][0]                    <br/>__________________________________________________________________________________________________<br/>dense_12 (Dense)                (None, 1)            19          lambda_4[0][0]                   <br/>==================================================================================================<br/>Total params: 126,367,653<br/>Trainable params: 126,314,533<br/>Non-trainable params: 53,120<br/>__________________________________________________________________________________________________<br/>Train on 1440 samples, validate on 22 samples<br/>Epoch 1/10<br/>1440/1440 [==============================] - 91s 64ms/step - loss: 0.7086 - acc: 0.5354 - val_loss: 0.6737 - val_acc: 0.5455<br/>Epoch 2/10<br/>1440/1440 [==============================] - 76s 53ms/step - loss: 0.5813 - acc: 0.7049 - val_loss: 0.6257 - val_acc: 0.5909<br/>Epoch 3/10<br/>1440/1440 [==============================] - 76s 53ms/step - loss: 0.4974 - acc: 0.8257 - val_loss: 0.6166 - val_acc: 0.5909<br/>Epoch 4/10<br/>1440/1440 [==============================] - 76s 53ms/step - loss: 0.4494 - acc: 0.8799 - val_loss: 0.6190 - val_acc: 0.5909<br/>Epoch 5/10<br/>1440/1440 [==============================] - 76s 53ms/step - loss: 0.4190 - acc: 0.9042 - val_loss: 0.5966 - val_acc: 0.6364<br/>Epoch 6/10<br/>1440/1440 [==============================] - 76s 53ms/step - loss: 0.3968 - acc: 0.9243 - val_loss: 0.5821 - val_acc: 0.6818<br/>Epoch 7/10<br/>1440/1440 [==============================] - 76s 53ms/step - loss: 0.3806 - acc: 0.9368 - val_loss: 0.5778 - val_acc: 0.6818<br/>Epoch 8/10<br/>1440/1440 [==============================] - 76s 53ms/step - loss: 0.3641 - acc: 0.9535 - val_loss: 0.5508 - val_acc: 0.7273<br/>Epoch 9/10<br/>1440/1440 [==============================] - 76s 53ms/step - loss: 0.3483 - acc: 0.9715 - val_loss: 0.5406 - val_acc: 0.7273<br/>Epoch 10/10<br/>1440/1440 [==============================] - 76s 53ms/step - loss: 0.3390 - acc: 0.9778 - val_loss: 0.5341 - val_acc: 0.7273</span></pre><p id="f299" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我们可以看到，仅用 100 张猫和 100 张狗的图像，我们就在 8 个时期的验证数据集中实现了 72%的准确率，这些图像对是从 200 张图像中创建的。</p><h1 id="51d8" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结论:</h1><p id="f3d3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我写这篇文章的动机是，我们通常没有成百上千的高质量标记图像来创建健壮的基于 CNN 的分类器。或者，我们可以用很少的图像来训练一个广泛用于面部识别的连体网络，以建立一个分类器。我已经在我的一个材料缺陷检测用例中使用了这种技术，在这个用例中，我们几乎没有缺陷材料的可用图像。为了保持作品的保密性，我在这个博客中用猫和狗的图像来演示这个讨论。如果你喜欢我的博客，请点击拍手按钮，并请继续关注我们未来的博客，因为我经常在机器学习和深度学习方面发表博客。</p></div></div>    
</body>
</html>