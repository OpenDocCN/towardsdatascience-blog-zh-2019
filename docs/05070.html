<html>
<head>
<title>Pruning Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">修剪深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505?source=collection_archive---------5-----------------------#2019-07-30">https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505?source=collection_archive---------5-----------------------#2019-07-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="852b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">TL；修剪的不同方法，DR:通过修剪，基于 VGG-16 的分类器变得快 3 倍，小 4 倍</h2></div><p id="176b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如今，深度学习模型需要大量的计算、内存和能力，这在我们需要实时推理或在计算资源有限的边缘设备和浏览器上运行模型的情况下成为瓶颈。能效是当前深度学习模型的主要关注点。解决这种效率的方法之一是启用推理效率。</p><p id="b4e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">更大的型号= &gt;更多的内存引用= &gt;更多的能量</strong></p><p id="5b86" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di">P</span>running 是一种推理方法，可以有效地生成尺寸更小、更节省内存、更省电、推理速度更快的模型，同时精确度损失最小，其他类似的技术还有权重共享和量化。深度学习从神经科学领域获得了灵感。深度学习中的修剪也是一个生物启发的概念，我们将在本文稍后讨论。</p><p id="d52b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着深度学习的进展，最先进的模型越来越准确，但这种进展是有代价的。我将在这个博客中讨论其中的一些。</p><h2 id="1b4d" class="ln lo it bd lp lq lr dn ls lt lu dp lv kr lw lx ly kv lz ma mb kz mc md me mf bi translated"><strong class="ak">第一个挑战——模特越来越大</strong></h2><p id="a30a" class="pw-post-body-paragraph ki kj it kk b kl mg ju kn ko mh jx kq kr mi kt ku kv mj kx ky kz mk lb lc ld im bi translated">难以通过空中更新分发大型模型</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ml"><img src="../Images/1430b9c01dde8f13b15e9bb174b2782e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b21KCzVEkc1E_vzV77amQg.png"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk">Dally, NIPS’2016 workshop on Efficient Methods for Deep Neural Networks</figcaption></figure><h2 id="f2be" class="ln lo it bd lp lq lr dn ls lt lu dp lv kr lw lx ly kv lz ma mb kz mc md me mf bi translated"><strong class="ak">第二个挑战:速度</strong></h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nb"><img src="../Images/7ffde3d092682a87691b199fbf3dae7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6uCSuFU6HdLN8M-vLIm7Qg.png"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk">Training time benchmarked with fb.resnet.torch using four M40 GPUs</figcaption></figure><p id="f738" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如此长的训练时间限制了 ML 研究员的生产力。</p><h2 id="c452" class="ln lo it bd lp lq lr dn ls lt lu dp lv kr lw lx ly kv lz ma mb kz mc md me mf bi translated"><strong class="ak">第三个挑战:能源效率</strong></h2><p id="5b58" class="pw-post-body-paragraph ki kj it kk b kl mg ju kn ko mh jx kq kr mi kt ku kv mj kx ky kz mk lb lc ld im bi translated">AlphaGo: 1920 个 CPU 和 280 个 GPU，每局 3000 美元电费</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/ab75791dba8c78aa6d186eb74d8d4239.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*Y3xXEtYzFODF3XKNIgTTbA.png"/></div></figure><ul class=""><li id="1e16" class="nd ne it kk b kl km ko kp kr nf kv ng kz nh ld ni nj nk nl bi translated">在移动设备上:耗尽电池</li><li id="464c" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">在数据中心:增加总拥有成本</li></ul><h2 id="0e7a" class="ln lo it bd lp lq lr dn ls lt lu dp lv kr lw lx ly kv lz ma mb kz mc md me mf bi translated"><strong class="ak">解答—高效的推理算法</strong></h2><ul class=""><li id="b666" class="nd ne it kk b kl mg ko mh kr nr kv ns kz nt ld ni nj nk nl bi translated"><strong class="kk iu">修剪</strong></li><li id="87ad" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">重量共享</li><li id="cc33" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">量化</li><li id="226b" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">低秩近似</li><li id="41f9" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">二元/三元网络</li><li id="9d89" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">威诺格拉变换</li></ul><h2 id="b2b5" class="ln lo it bd lp lq lr dn ls lt lu dp lv kr lw lx ly kv lz ma mb kz mc md me mf bi translated">修剪的生物灵感</h2><p id="bb6c" class="pw-post-body-paragraph ki kj it kk b kl mg ju kn ko mh jx kq kr mi kt ku kv mj kx ky kz mk lb lc ld im bi translated">人工神经网络中的修剪是从人脑中的<a class="ae nu" href="https://en.wikipedia.org/wiki/Synaptic_pruning" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">突触修剪</strong> </a> <strong class="kk iu"> </strong>中得到的想法，其中<a class="ae nu" href="https://en.wikipedia.org/wiki/Axon" rel="noopener ugc nofollow" target="_blank">轴突</a>和<a class="ae nu" href="https://en.wikipedia.org/wiki/Dendrite" rel="noopener ugc nofollow" target="_blank">树突</a>完全衰退和死亡，导致许多<a class="ae nu" href="https://en.wikipedia.org/wiki/Mammal" rel="noopener ugc nofollow" target="_blank">哺乳动物在幼儿期和青春期开始之间发生突触消除</a>。修剪从出生时开始，一直持续到 25 岁左右。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nv"><img src="../Images/1c7cb7bc9f178208129772be7e0ed6c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j12SW3q3cv66rOqtkrzzwQ.png"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk">Christopher A Walsh. Peter Huttenlocher (1931–2013). Nature, 502(7470):172–172, 2013.</figcaption></figure><h2 id="e9c8" class="ln lo it bd lp lq lr dn ls lt lu dp lv kr lw lx ly kv lz ma mb kz mc md me mf bi translated"><strong class="ak">修剪深度神经网络</strong></h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nw"><img src="../Images/aa0d08e01440a3cf948caba5797f7593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4dJE_vHfGpPBtXLLXLmnBQ.png"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk">[Lecun et al. NIPS’89] [Han et al. NIPS’15]</figcaption></figure><p id="8f78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">网络通常看起来像左边的:下面一层中的每个神经元都与上面一层有联系，但这意味着我们必须将许多浮点相乘。理想情况下，我们只需将每个神经元连接到少数几个其他神经元上，省下一些乘法运算；这被称为“稀疏”网络。</p><p id="ca4f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">稀疏模型更容易压缩，我们可以在推断过程中跳过零，以改善延迟。</p><p id="66c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你可以根据神经元的贡献大小对网络中的神经元进行排序，那么你就可以从网络中移除排序较低的神经元，从而形成一个更小、更快的网络。</p><p id="38e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">获得更快/更小的网络对于在移动设备上运行这些深度学习网络很重要。</strong></p><p id="81c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，可以根据神经元权重的 L1/L2 范数来进行排序。剪枝之后，准确率会下降(如果排序巧妙的话希望不会下降太多)，通常会对网络进行训练-剪枝-训练-剪枝迭代恢复。如果我们一次修剪太多，网络可能会被严重破坏，无法恢复。所以在实践中，这是一个迭代的过程——通常称为“迭代修剪”:修剪/训练/重复。</p><p id="d6b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参见<a class="ae nu" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>团队编写的<a class="ae nu" href="https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb" rel="noopener ugc nofollow" target="_blank">代码，了解迭代修剪。</a></p><h2 id="005d" class="ln lo it bd lp lq lr dn ls lt lu dp lv kr lw lx ly kv lz ma mb kz mc md me mf bi translated">权重修剪</h2><ul class=""><li id="3cae" class="nd ne it kk b kl mg ko mh kr nr kv ns kz nt ld ni nj nk nl bi translated">将权重矩阵中的单个权重设置为零。这相当于删除连接，如上图所示。</li><li id="d08e" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">这里，为了实现 k%的稀疏性，我们根据权重矩阵 W 中各个权重的大小对它们进行排序，然后将最小的 k%设置为零。</li></ul><pre class="mm mn mo mp gt ny nz oa ob aw oc bi"><span id="aa2c" class="ln lo it nz b gy od oe l of og">f = h5py.File("model_weights.h5",'r+')<br/>for k in [.25, .50, .60, .70, .80, .90, .95, .97, .99]:<br/> ranks = {}<br/> for l in list(f[‘model_weights’])[:-1]:<br/> data = f[‘model_weights’][l][l][‘kernel:0’]<br/> w = np.array(data)<br/> ranks[l]=(rankdata(np.abs(w),method=’dense’) — 1).astype(int).reshape(w.shape)<br/> lower_bound_rank = np.ceil(np.max(ranks[l])*k).astype(int)<br/> ranks[l][ranks[l]&lt;=lower_bound_rank] = 0<br/> ranks[l][ranks[l]&gt;lower_bound_rank] = 1<br/> w = w*ranks[l]<br/> data[…] = w</span></pre><h2 id="c524" class="ln lo it bd lp lq lr dn ls lt lu dp lv kr lw lx ly kv lz ma mb kz mc md me mf bi translated">单元/神经元修剪</h2><ul class=""><li id="f66b" class="nd ne it kk b kl mg ko mh kr nr kv ns kz nt ld ni nj nk nl bi translated">将权重矩阵中的所有列设置为零，实际上删除了相应的输出神经元。</li><li id="c769" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated">这里，为了实现 k%的稀疏性，我们根据它们的 L2 范数对权重矩阵的列进行排序，并删除最小的 k%。</li></ul><pre class="mm mn mo mp gt ny nz oa ob aw oc bi"><span id="fe4e" class="ln lo it nz b gy od oe l of og">f = h5py.File("model_weights.h5",'r+')<br/>for k in [.25, .50, .60, .70, .80, .90, .95, .97, .99]:<br/> ranks = {}<br/> for l in list(f[‘model_weights’])[:-1]:<br/>     data = f[‘model_weights’][l][l][‘kernel:0’]<br/>     w = np.array(data)<br/>     norm = LA.norm(w,axis=0)<br/>     norm = np.tile(norm,(w.shape[0],1))<br/>     ranks[l] = (rankdata(norm,method=’dense’) —     1).astype(int).reshape(norm.shape)<br/>     lower_bound_rank = np.ceil(np.max(ranks[l])*k).astype(int)<br/>     ranks[l][ranks[l]&lt;=lower_bound_rank] = 0<br/>     ranks[l][ranks[l]&gt;lower_bound_rank] = 1<br/>     w = w*ranks[l]<br/>     data[…] = w</span></pre><p id="deda" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自然地，当你增加稀疏度和删除更多的网络时，任务性能会逐渐降低。你认为稀疏性对性能的退化曲线会是怎样的？</p><p id="ac6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用简单的神经网络架构在<a class="ae nu" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据集上修剪图像分类模型的结果，如下所示</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/97836f7c1f8bff2446417de85cbc869d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*fWA3QG6ABLDlBt20HABS2Q.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk">The architecture that I used in the code given in reference</figcaption></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/900e976a26addc03b390b2a2718a0c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*pQeZG3Dp91OZ8WWV-VJ9Mw.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk">Use the <a class="ae nu" href="https://drive.google.com/open?id=1GBLFxyFQtTTve_EE5y1Ulo0RwnKk_h6J" rel="noopener ugc nofollow" target="_blank">code</a> to regenerate the graph</figcaption></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/8ec2a768cc39b0360a6b2d79ed466598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*-JEQD5aLfNn9YlZZkR2ECA.png"/></div></figure><h2 id="99ae" class="ln lo it bd lp lq lr dn ls lt lu dp lv kr lw lx ly kv lz ma mb kz mc md me mf bi translated">关键要点</h2><p id="1354" class="pw-post-body-paragraph ki kj it kk b kl mg ju kn ko mh jx kq kr mi kt ku kv mj kx ky kz mk lb lc ld im bi translated">许多研究人员认为修剪是一种被忽视的方法，在实践中会得到更多的关注和使用。我们展示了如何使用一个非常简单的神经网络架构在玩具数据集上获得好的结果。我认为深度学习在实践中用来解决的许多问题都类似于这个问题，在有限的数据集上使用迁移学习，所以他们也可以从修剪中受益。</p><h2 id="88d9" class="ln lo it bd lp lq lr dn ls lt lu dp lv kr lw lx ly kv lz ma mb kz mc md me mf bi translated">参考</h2><ul class=""><li id="9f9e" class="nd ne it kk b kl mg ko mh kr nr kv ns kz nt ld ni nj nk nl bi translated"><a class="ae nu" href="https://drive.google.com/open?id=1GBLFxyFQtTTve_EE5y1Ulo0RwnKk_h6J" rel="noopener ugc nofollow" target="_blank"> <em class="nx">本帖代号</em></a><em class="nx"/></li><li id="00cc" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated"><a class="ae nu" href="https://arxiv.org/pdf/1710.01878.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nx">修剪，还是不修剪:探索模型压缩中修剪的功效</em> </a> <em class="nx">，Michael H. Zhu，Suyog Gupta，2017 </em></li><li id="6a5a" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated"><a class="ae nu" href="https://arxiv.org/pdf/1801.07365.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nx">学习卷积神经网络中的剪枝滤波器</em> </a> <em class="nx">，黄千贵等。艾尔，2018 </em></li><li id="51b2" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated"><a class="ae nu" href="https://jacobgil.github.io/deeplearning/pruning-deep-learning" rel="noopener ugc nofollow" target="_blank"> <em class="nx">修剪深层神经网络使其快速而小巧</em> </a></li><li id="ab10" class="nd ne it kk b kl nm ko nn kr no kv np kz nq ld ni nj nk nl bi translated"><a class="ae nu" href="https://www.tensorflow.org/model_optimization" rel="noopener ugc nofollow" target="_blank"> <em class="nx">用 Tensorflow 模型优化工具包</em>优化机器学习模型 </a> <em class="nx"/></li></ul></div></div>    
</body>
</html>