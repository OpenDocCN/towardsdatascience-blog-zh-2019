<html>
<head>
<title>Reinforcement Learning — Policy Approximation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——策略近似</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-policy-approximation-f5606ad3d909?source=collection_archive---------14-----------------------#2019-09-21">https://towardsdatascience.com/reinforcement-learning-policy-approximation-f5606ad3d909?source=collection_archive---------14-----------------------#2019-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d864" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">政策梯度法的理论与应用</h2></div><p id="3e8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，所有引入的算法都是基于值函数或 Q 函数的梯度算法，也就是说，我们假设对于不同的状态<code class="fe le lf lg lh b">S(or [S, A])</code>存在真实值<code class="fe le lf lg lh b">V(or Q)</code>，并且为了接近真实值，我们使用公式中带有<code class="fe le lf lg lh b">∇V or ∇Q</code>的梯度方法，并且在学习过程的最后，通过基于<code class="fe le lf lg lh b">V or Q</code>函数估计在每个状态选择最有回报的动作来生成策略<code class="fe le lf lg lh b">π(A | S)</code>。然而，策略梯度方法对强化学习问题提出了完全不同的观点，人们可以直接学习或更新策略，而不是学习一个值函数。所以在本帖中，我们将:</p><ol class=""><li id="6f4f" class="li lj it kk b kl km ko kp kr lk kv ll kz lm ld ln lo lp lq bi translated">学习政策梯度法的理论</li><li id="181c" class="li lj it kk b kl lr ko ls kr lt kv lu kz lv ld ln lo lp lq bi translated">将其应用于短走廊示例</li></ol><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi lw"><img src="../Images/315aaf7141d2ea22b3943d57a244db1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ao1QDr_KXzJH7aBz93Hbww.jpeg"/></div></div></figure><h1 id="013c" class="mi mj it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">政策梯度理论</h1><p id="1ea5" class="pw-post-body-paragraph ki kj it kk b kl na ju kn ko nb jx kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">记得在以前的帖子中，学习过程中使用的策略总是ϵ-greedy，这意味着代理将在某个概率下采取随机行动，在其余概率下采取贪婪行动。然而，在梯度策略方法中，问题被公式化为，<code class="fe le lf lg lh b">P(A|S, θ) = π(A|S, θ)</code>，也就是说，对于每个状态，策略给出了从该状态可能采取的每个动作的概率，并且为了优化策略，它被参数化为<code class="fe le lf lg lh b">θ</code>(类似于我们之前介绍的价值函数中的权重参数<code class="fe le lf lg lh b">w</code>)。</p><p id="2f2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一件值得一提的事情是，在这种情况下，性能变量被定义为:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8ec0eb57e4f82e2089c1597a107186c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*zXPEBNtFVYwb86Y_FRReKw.png"/></div></figure><p id="5123" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe le lf lg lh b"><em class="ng">J(θ)</em></code> <em class="ng"> </em>是当前参数化策略<code class="fe le lf lg lh b"><em class="ng">π</em></code>、<strong class="kk iu">下<code class="fe le lf lg lh b"><em class="ng">V</em></code>的真值，目标是最大化性能</strong> <code class="fe le lf lg lh b"><em class="ng">J</em></code>，因此我们得到一个梯度更新过程:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nh"><img src="../Images/ae4272e1f3fe1f1ba9547a17126c083c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PXXJodJKK8kRDqL94KnivQ.png"/></div></div></figure><p id="f469" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并且由于<code class="fe le lf lg lh b"><em class="ng">J</em></code>是策略<code class="fe le lf lg lh b">π</code>的一个表示，我们知道<code class="fe le lf lg lh b">θ</code>的更新会包含当前的策略，经过一系列的推导(详情请参考 Sutton 的书，第 13 章)，我们得到更新过程:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ni"><img src="../Images/529bd55f7f2056ed1662aa0ef6d60125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gSHjNa3xyGj0u9_AM7hqTg.png"/></div></div></figure><p id="f3c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的第一个政策梯度方法是:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nj"><img src="../Images/17ba07556c9420a8e0fc4fe7a0fcbfec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mzRIL4S9qq8WG5IIBD-X6Q.png"/></div></div></figure><p id="fb9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe le lf lg lh b">G</code>仍为累计折扣奖励，参数<code class="fe le lf lg lh b">θ</code>将更新为当前保单的衍生产品。</p><h2 id="3cf8" class="nk mj it bd mk nl nm dn mo nn no dp ms kr np nq mu kv nr ns mw kz nt nu my nv bi translated">策略近似的优势</h2><p id="6395" class="pw-post-body-paragraph ki kj it kk b kl na ju kn ko nb jx kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">首先，策略梯度方法的明显优点是，与值函数近似法相比，该方法简化了过程，在值函数近似法中，我们首先学习一个值函数<code class="fe le lf lg lh b">V(S, A)</code>，并从中推断出策略<code class="fe le lf lg lh b">π(A|S)</code>，在策略近似法中，在每次迭代中，通过更新<code class="fe le lf lg lh b">π(A|S, θ)</code>中的参数<code class="fe le lf lg lh b">θ</code>来直接更新策略。</p><p id="8b92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其次，在价值函数更新过程中，我们通常使用ϵ-greedy 方法，这样代理人总是有一个ϵ采取随机行动的概率。然而，策略梯度方法能够通过将某些动作的概率更新为零来逼近确定性策略。</p><p id="f926" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第三，如果你还记得，在以前的文章中，我们总是要求动作空间是离散的，因为一般的价值函数逼近方法在处理具有严格连续空间的动作时有点棘手。但是在策略近似定义中，动作实际上可以相对于状态是连续的，例如，可以将策略定义为:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nw"><img src="../Images/f5c833d890183f59e94c537c5a12b388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qtr2lVT1mwudgVE9tmRE8g.png"/></div></div></figure><p id="aa67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，动作<code class="fe le lf lg lh b">a</code>将具有正态分布的概率，μ和σ由状态<code class="fe le lf lg lh b">s</code>定义。</p><p id="9dc8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后但同样重要的是，在最优策略是非确定性的情况下，它允许选择具有任意概率的动作，并且总是选择具有最优概率的动作。</p><h1 id="c401" class="mi mj it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">短走廊示例</h1><p id="8363" class="pw-post-body-paragraph ki kj it kk b kl na ju kn ko nb jx kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">现在让我们举一个例子来应用我们的知识。</p><h2 id="eb0e" class="nk mj it bd mk nl nm dn mo nn no dp ms kr np nq mu kv nr ns mw kz nt nu my nv bi translated">问题描述</h2><p id="508c" class="pw-post-body-paragraph ki kj it kk b kl na ju kn ko nb jx kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">考虑下图中插图所示的小走廊网格世界。奖励是-1 每一步，像往常一样。在三种非终结状态的每一种状态下，都只有两个动作，右和左。这些动作在第一和第三种状态下有它们通常的结果(在第一种状态下左不动)，但是在第二种状态下它们是相反的，所以右向左移动，左向右移动。这个问题很难，因为在函数近似下，所有的状态都是相同的。特别是，我们为所有的<code class="fe le lf lg lh b">s</code>定义了<code class="fe le lf lg lh b">x(s, right) = [1, 0])</code>和<code class="fe le lf lg lh b">x(s, left) = [0, 1])</code>。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nx"><img src="../Images/468541014da20d9c90a7cfb964a52abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*05SUEn1Mc8_EKVUAR5qa4A.png"/></div></div></figure><p id="2a99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，状态、动作表示使用最简单的格式，并将所有状态视为相同(二进制表示与<a class="ae ny" rel="noopener" target="_blank" href="/reinforcement-learning-tile-coding-implementation-7974b600762b">图块编码</a>中的表示相同)。</p><p id="d400" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面列出了算法和组件:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nz"><img src="../Images/7f6f0ecc41b7886163bcdcd8f5438685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7dE9kxAbyApR1xvwW8kOZA.png"/></div></div></figure><p id="62a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，在这里，使用 softmax 函数(也可以应用其他函数)定义策略<code class="fe le lf lg lh b">π(a|s, θ)</code>，该函数生成 0 到 1 之间的概率，并且组件<code class="fe le lf lg lh b">h</code>被定义为具有参数<code class="fe le lf lg lh b">θ</code>的简单线性函数。</p><p id="5cd8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们开始实现。</p><h2 id="1a0d" class="nk mj it bd mk nl nm dn mo nn no dp ms kr np nq mu kv nr ns mw kz nt nu my nv bi translated">初始化</h2><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="1cc6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<code class="fe le lf lg lh b">init</code>函数中，我们有上面讲过的<code class="fe le lf lg lh b">self.x</code>和<code class="fe le lf lg lh b">self.theta</code>的定义，其初始权重为<code class="fe le lf lg lh b">self.x</code>，起始状态为 0。</p><h2 id="b910" class="nk mj it bd mk nl nm dn mo nn no dp ms kr np nq mu kv nr ns mw kz nt nu my nv bi translated">行动</h2><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="1313" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">函数<code class="fe le lf lg lh b">softmax</code>接受一个向量，并返回向量中每个分量的概率。动作由<code class="fe le lf lg lh b">softmax</code>生成的概率选择，<code class="fe le lf lg lh b">takeAction</code>函数接受一个动作并返回下一个状态(注意第二个状态是相反的)。</p><h2 id="80ce" class="nk mj it bd mk nl nm dn mo nn no dp ms kr np nq mu kv nr ns mw kz nt nu my nv bi translated">报酬</h2><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="11a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如问题所述，除了最终状态，所有状态的奖励都是-1。</p><h2 id="ad33" class="nk mj it bd mk nl nm dn mo nn no dp ms kr np nq mu kv nr ns mw kz nt nu my nv bi translated">运行游戏！</h2><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="87f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在每一集里，代理都要经历一个过程<code class="fe le lf lg lh b">current_state -&gt; take_action -&gt; next_state -&gt; get_reward</code>。当它到达状态(3)的终点时，我们计算累计奖励<code class="fe le lf lg lh b">G</code>。梯度<code class="fe le lf lg lh b">grad</code>是为该步骤中采取的特定动作计算的(<code class="fe le lf lg lh b">j</code>是该动作的索引)。<code class="fe le lf lg lh b">reward_sum</code>只记录每集的奖励，用于显示目的。</p><p id="e038" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们用<code class="fe le lf lg lh b">alpha = 2e-4</code>和<code class="fe le lf lg lh b">gamma = 1</code>运行游戏 1000 集，得到结果:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi oc"><img src="../Images/f257a75d2c7433da0ddaf57d06342fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oLNTHaPUeFiO4cIYsaO0Jw.png"/></div></div></figure><p id="29c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从增加回合中的概率来看，我们看到 agent 在逐渐更新策略，实际上在这个博弈设定中，最优策略是概率<code class="fe le lf lg lh b">[0.4, 0.6]</code>，我们的学习结果正在接近最优值(<a class="ae ny" href="https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/ShortCorridor/ShortCorridor.py" rel="noopener ugc nofollow" target="_blank">完全实现</a>)。</p><p id="ea17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考</strong>:</p><ul class=""><li id="1516" class="li lj it kk b kl km ko kp kr lk kv ll kz lm ld od lo lp lq bi translated"><a class="ae ny" href="http://incompleteideas.net/book/the-book-2nd.html?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/the-book-2nd.html</a></li><li id="8a5d" class="li lj it kk b kl lr ko ls kr lt kv lu kz lv ld od lo lp lq bi translated"><a class="ae ny" href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction" rel="noopener ugc nofollow" target="_blank">https://github . com/Shang tong Zhang/reinforcement-learning-an-introduction</a></li></ul></div></div>    
</body>
</html>