<html>
<head>
<title>Learning Theory: (Agnostic) Probably Approximately Correct Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习理论:(不可知论者)大概近似正确的学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-theory-agnostic-probably-approximately-correct-learning-dfd0d7c76467?source=collection_archive---------3-----------------------#2019-03-23">https://towardsdatascience.com/learning-theory-agnostic-probably-approximately-correct-learning-dfd0d7c76467?source=collection_archive---------3-----------------------#2019-03-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1c3b46fd5733045073611bd81e7fbaa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w-udKm0c_ubegesw"/></div></div></figure><div class=""/><p id="86b2" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我以前的文章中，我讨论了什么是经验风险最小化，以及它在某些假设下产生令人满意的假设的证明。现在我想讨论一下大概正确的学习(这有点拗口，但是有点酷)，这是 ERM 的一个概括。对于那些不熟悉 ERM 的人来说，我建议阅读我在该主题上的<a class="ae kz" rel="noopener" target="_blank" href="/learning-theory-empirical-risk-minimization-d3573f90ff77">前一篇文章</a>，因为这是理解 PAC learning 的先决条件。</p><p id="bf7e" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请记住，在分析 ERM 时，我们得出的结论是，对于有限的假设空间<strong class="kd jf"> H </strong>我们可以得出一个假设，当假设假设空间中存在这样一个假设时，它的误差以一定的概率低于<strong class="kd jf">ε</strong>。基于这些参数，我们可以计算出需要多少样本才能达到这样的精度，我们得出了样本的下限值:</p><figure class="lb lc ld le gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi la"><img src="../Images/cebc85b885e0285a4e24413ad9ddbcb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W006RicDVuqiKtjUx7skhA.png"/></div></div></figure><p id="891a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这可以放入通用的 PAC 学习框架，下面的正式定义来自《理解机器学习》一书:</p><figure class="lb lc ld le gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lf"><img src="../Images/3a3afb6789cd903e2b6aa6d7579ab454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11Sd7GFBJlpVO_L9Ew2jng.png"/></div></div></figure><p id="cba1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">至少对我来说，这个定义一开始有点混乱。这是什么意思？该定义规定，如果存在函数<strong class="kd jf"> m_H </strong>和算法，则<strong class="kd jf">假设类</strong>是 PAC 可学习的，该算法对于输入域<strong class="kd jf"> X、δ</strong>和<strong class="kd jf">ε</strong>上的任何标记函数<strong class="kd jf"> f、</strong>分布<strong class="kd jf"> D </strong>，其中<strong class="kd jf"> m ≥ m_H </strong>产生假设<strong class="kd jf"> h </strong>，使得概率为 1-<strong class="kd jf">δ</strong>一个标注函数无非就是说我们有一个确定的函数<strong class="kd jf"> f </strong>标注域内的数据。</p><p id="335f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里，假设类可以是任何类型的二元分类器，因为将标签分配给来自域的例子的标签函数分配标签<strong class="kd jf"> 0 </strong>或<strong class="kd jf"> 1。</strong><strong class="kd jf">m _ H</strong>函数为我们提供了一个最小样本数的界限，我们需要该界限来实现低于<strong class="kd jf">ε</strong>的误差，并具有置信度<strong class="kd jf">δ。</strong>精确度<strong class="kd jf">ε</strong>逻辑上控制必要的样本大小，因为我们的精确度越高，逻辑上我们需要我们的训练集是来自领域的更忠实的样本，因此，增加实现这种精确度所需的样本数量。</p><h2 id="d2be" class="lg lh je bd li lj lk dn ll lm ln dp lo km lp lq lr kq ls lt lu ku lv lw lx ly bi translated">使模型不可知</h2><figure class="lb lc ld le gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lz"><img src="../Images/73c0b7c5720526f9be3d379c4a485c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Fr9cI_9y5KJclXGd"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Photo by <a class="ae kz" href="https://unsplash.com/@evan__bray?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Evan Dennis</a> on <a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4278" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">上述模型有一定的缺点，由于可实现性假设(在<a class="ae kz" rel="noopener" target="_blank" href="/learning-theory-empirical-risk-minimization-d3573f90ff77">经验风险最小化</a>中解释)，它不够通用——没有人保证存在一个假设，由于模型的失败，该假设将导致我们当前假设空间中的真实误差为 0。另一种看待它的方式是，也许标签没有被数据很好地定义，因为缺少特征。</p><p id="ba69" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们绕过可实现性假设的方法是用数据标签分布代替标签函数。您可以将此视为在标注函数中引入了不确定性，因为一个数据点可以共享不同的标注。那么为什么叫<strong class="kd jf">不可知 PAC 学习</strong>？不可知论一词来源于这样一个事实，即学习对于数据标签分布是不可知论的——这意味着它将通过不对数据标签分布做任何假设来学习最佳标签函数<strong class="kd jf"> f </strong>。这种情况下有什么变化？嗯，<strong class="kd jf">真误差</strong>定义改变了，因为一个标签到一个数据点是在多个标签上的分布。我们不能保证学习者将达到最小可能的真实错误，因为我们没有数据标签分布来说明标签的不确定性。</p><p id="da73" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">经过这些考虑，我们从一书中得出了以下正式定义:</p><figure class="lb lc ld le gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi me"><img src="../Images/00f30bf6079fd089669f40ff119597d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qs2-BgqJw1iCbPa44Mv72w.png"/></div></div></figure><p id="6438" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意定义中关于<strong class="kd jf"> PAC </strong>可学习性的变化。通过引入数据标签分布<strong class="kd jf"> D </strong>，我们考虑到学习假设的<strong class="kd jf">真实误差</strong>将小于或等于<strong class="kd jf">最优假设的误差</strong>加上因子<strong class="kd jf">ε</strong>的事实。这也包含了<strong class="kd jf"> PAC </strong>在假设空间中存在最优假设的情况下的自我学习，该假设产生的<strong class="kd jf">真误差</strong>为 0，但我们也考虑到可能不存在这样的假设。这些定义将在稍后解释<strong class="kd jf"> VC 维度</strong>和证明<strong class="kd jf">没有免费的午餐定理时有用。</strong></p><p id="de23" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果术语对你来说有点陌生，我建议你看一下<a class="ae kz" rel="noopener" target="_blank" href="/learning-theory-empirical-risk-minimization-d3573f90ff77">学习理论:经验风险最小化</a>或者更详细地看一下文章中提到的 Ben-David 的精彩著作。除此之外，继续机器学习！</p></div></div>    
</body>
</html>