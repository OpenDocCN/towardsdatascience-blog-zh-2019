<html>
<head>
<title>10 Tips for Choosing the Optimal Number of Clusters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">选择最佳集群数量的 10 个技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92?source=collection_archive---------2-----------------------#2019-01-27">https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92?source=collection_archive---------2-----------------------#2019-01-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e1fcc958bf057d7a83a3365cd6b7460a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nh_06G9VBvDKHbFHpyFayA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/photos/EJMTKCZ00I0?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Pakata Goh</a> on <a class="ae kc" href="https://unsplash.com/search/photos/programming?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0f6c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">聚类是最常见的无监督机器学习问题之一。使用一些观测值间距离度量或基于相关性的距离度量来定义观测值之间的相似性。</p><p id="b8e7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有 5 类聚类方法:</p><blockquote class="lb lc ld"><p id="ef8d" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">+层次聚类<br/> +划分方法(k-means，PAM，CLARA) <br/> +基于密度的聚类<br/> +基于模型的聚类<br/> +模糊聚类</p></blockquote><p id="9c50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我写这篇文章的愿望主要来自于阅读关于<a class="ae kc" href="https://github.com/lazappi/clustree" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> clustree </strong>包</a>、<a class="ae kc" href="https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> dendextend </strong> </a>文档以及由 Alboukadel Kassambara 写的 R 书中的<a class="ae kc" href="https://www.datanovia.com/en/product/practical-guide-to-cluster-analysis-in-r/" rel="noopener ugc nofollow" target="_blank">聚类分析实用指南</a><a class="ae kc" href="http://www.sthda.com/english/wiki/r-packages" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">facto extra</strong></a><strong class="kf ir"/>包的作者。</p><h1 id="8b3e" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">数据集</h1><p id="1430" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">我将使用来自<strong class="kf ir">集群</strong>包的一个不太为人所知的数据集:<a class="ae kc" href="https://www.rdocumentation.org/packages/cluster.datasets/versions/1.0-1/topics/all.mammals.milk.1956" rel="noopener ugc nofollow" target="_blank"> all .哺乳动物. milk.1956 </a>，一个我以前没有看过的数据集。</p><p id="45b9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个小数据集包含 25 种哺乳动物及其乳汁成分(水、蛋白质、脂肪、乳糖、灰分百分比)的列表，来自<a class="ae kc" href="http://people.sc.fsu.edu/~jburkardt/datasets/hartigan/hartigan.html" rel="noopener ugc nofollow" target="_blank"> John Hartigan，Clustering Algorithms，Wiley，1975 </a>。</p><p id="079e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先让我们加载所需的包。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="d421" class="mu lj iq mq b gy mv mw l mx my">library(tidyverse)<br/>library(magrittr)<br/>library(cluster)<br/>library(cluster.datasets)<br/>library(cowplot)<br/>library(NbClust)<br/>library(clValid)<br/>library(ggfortify)<br/>library(clustree)<br/>library(dendextend)<br/>library(factoextra)<br/>library(FactoMineR)<br/>library(corrplot)<br/>library(GGally)<br/>library(ggiraphExtra)<br/>library(knitr)<br/>library(kableExtra)</span></pre><p id="9ee9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在加载数据。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="a59a" class="mu lj iq mq b gy mv mw l mx my">data("all.mammals.milk.1956")<br/>raw_mammals &lt;- all.mammals.milk.1956</span><span id="9152" class="mu lj iq mq b gy mz mw l mx my"># subset dataset<br/>mammals &lt;- raw_mammals %&gt;% select(-name) # set rownames<br/>mammals &lt;- as_tibble(mammals)</span></pre><p id="3db8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们探索并可视化这些数据。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="037d" class="mu lj iq mq b gy mv mw l mx my"># Glimpse the data set<br/>glimpse(mammals)</span><span id="4ebc" class="mu lj iq mq b gy mz mw l mx my">Observations: 25<br/>Variables: 5<br/>$ water   <em class="le">&lt;dbl&gt;</em> 90.1, 88.5, 88.4, 90.3, 90.4, 87.7, 86.9, 82.1, 81.9, 81.6, 81.6, 86.5, 90.0,...<br/>$ protein <em class="le">&lt;dbl&gt;</em> 2.6, 1.4, 2.2, 1.7, 0.6, 3.5, 4.8, 5.9, 7.4, 10.1, 6.6, 3.9, 2.0, 7.1, 3.0, 5...<br/>$ fat     <em class="le">&lt;dbl&gt;</em> 1.0, 3.5, 2.7, 1.4, 4.5, 3.4, 1.7, 7.9, 7.2, 6.3, 5.9, 3.2, 1.8, 5.1, 4.8, 6....<br/>$ lactose <em class="le">&lt;dbl&gt;</em> 6.9, 6.0, 6.4, 6.2, 4.4, 4.8, 5.7, 4.7, 2.7, 4.4, 4.9, 5.6, 5.5, 3.7, 5.3, 4....<br/>$ ash     <em class="le">&lt;dbl&gt;</em> 0.35, 0.24, 0.18, 0.40, 0.10, 0.71, 0.90, 0.78, 0.85, 0.75, 0.93, 0.80, 0.47,...</span></pre><p id="d94e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有的变量都用数字表示。统计分布呢？</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="f7fd" class="mu lj iq mq b gy mv mw l mx my"># Summary of data set<br/>summary(mammals) %&gt;% kable() %&gt;% kable_styling()</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/0651bd1f0f0b73c4abc2f69f9ad1ca44.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*_LTOQcmFOfw6gAhAw6QmpA.png"/></div></figure><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="e949" class="mu lj iq mq b gy mv mw l mx my"># Historgram for each attribute<br/>mammals %&gt;% <br/>  gather(Attributes, value, 1:5) %&gt;% <br/>  ggplot(aes(x=value)) +<br/>  geom_histogram(fill = "lightblue2", color = "black") + <br/>  facet_wrap(~Attributes, scales = "free_x") +<br/>  labs(x = "Value", y = "Frequency")</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/6786057ea0432f70ef11fedafcc2f1f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i76oVZvwpPxZCYdovmanIw.png"/></div></div></figure><p id="0968" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不同属性之间有什么关系？使用' corrplot()'创建相关矩阵。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="0e50" class="mu lj iq mq b gy mv mw l mx my">corrplot(cor(mammals), type = "upper", method = "ellipse", tl.cex = 0.9)</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/67fad816939036f26b6fd023211d3875.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*P5xSIYPsSttLh5Rb5uXwUQ.png"/></div></figure><p id="935b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当您有以不同尺度测量的变量时，缩放数据是有用的。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="f205" class="mu lj iq mq b gy mv mw l mx my">mammals_scaled &lt;- scale(mammals)<br/>rownames(mammals_scaled) &lt;- raw_mammals$name</span></pre><p id="f82b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">降维有助于数据可视化(<em class="le">如</em> PCA 方法)。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="f33b" class="mu lj iq mq b gy mv mw l mx my">res.pca &lt;- PCA(mammals_scaled,  graph = FALSE)</span><span id="2c04" class="mu lj iq mq b gy mz mw l mx my"># Visualize eigenvalues/variances<br/>fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c89b528f58b65c932fa2b0428c51288e.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*kCLvb6sYp-QyfCifw7Kyow.png"/></div></figure><p id="afc4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些是捕获了 80%方差的<strong class="kf ir"> 5 件。scree 图显示<strong class="kf ir"> PC1 捕获了约 75%的方差</strong>。</strong></p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="e6e3" class="mu lj iq mq b gy mv mw l mx my"># Extract the results for variables<br/>var &lt;- get_pca_var(res.pca)</span><span id="cbc2" class="mu lj iq mq b gy mz mw l mx my"># Contributions of variables to PC1<br/>fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)</span><span id="d907" class="mu lj iq mq b gy mz mw l mx my"># Contributions of variables to PC2<br/>fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)</span><span id="a1e3" class="mu lj iq mq b gy mz mw l mx my"># Control variable colors using their contributions to the principle axis<br/>fviz_pca_var(res.pca, col.var="contrib",<br/>             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),<br/>             repel = TRUE # Avoid text overlapping<br/>             ) + theme_minimal() + ggtitle("Variables - PCA")</span></pre><div class="ml mm mn mo gt ab cb"><figure class="nd jr ne nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/44db43b3ac5f8a768bf2e5bf3e56f511.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6JIZBX699nUPNq-0I9A0Bg.png"/></div></figure><figure class="nd jr ne nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/76541709906029213802193d9a2ff554.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*O9lhD0BkpKtAv0wZlV6ZzA.png"/></div></figure><figure class="nd jr ne nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/51a15e1751acadce23999f464b946d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*wdGFx8hVFmij0gGHckY_Dw.png"/></div></figure></div><p id="6c16" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这些图像中可以明显看出，水和乳糖会一起增加，蛋白质、灰分和脂肪也会一起增加；这两组是反向相关的。</p><h1 id="dd05" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">朴素(K 均值)方法</h1><p id="a219" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">分区聚类方法，如 k-means 和 Medoids 分区(PAM ),要求您指定要生成的聚类数。</p><p id="44ee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">k-means 聚类可能是最著名的划分方法之一。k-means 聚类背后的思想包括定义聚类的<strong class="kf ir">总的类内变化</strong>，其测量最小化的聚类的紧密度。</p><p id="7e93" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以用<strong class="kf ir"> kmeans() </strong>函数计算 R 中的 k 均值:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="81e6" class="mu lj iq mq b gy mv mw l mx my">km2 &lt;- kmeans(mammals_scaled, centers = 2, nstart = 30)</span></pre><p id="6c78" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的例子将数据分成两个集群，<strong class="kf ir">中心= 2 </strong>，并尝试多个初始配置，报告最佳配置。例如，由于该算法对群集质心的初始位置敏感，所以添加<strong class="kf ir"> nstart = 30 </strong>将生成 30 个初始配置，然后对所有质心结果进行平均。</p><p id="320d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为在我们开始之前需要设置簇的数量(<strong class="kf ir"> k </strong>)，所以检查几个不同的<strong class="kf ir"> k </strong>值是有利的。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="110e" class="mu lj iq mq b gy mv mw l mx my">kmean_calc &lt;- function(df, ...){<br/>  kmeans(df, scaled = ..., nstart = 30)<br/>}</span><span id="51b0" class="mu lj iq mq b gy mz mw l mx my">km2 &lt;- kmean_calc(mammals_scaled, 2)<br/>km3 &lt;- kmean_calc(mammals_scaled, 3)<br/>km4 &lt;- kmeans(mammals_scaled, 4)<br/>km5 &lt;- kmeans(mammals_scaled, 5)<br/>km6 &lt;- kmeans(mammals_scaled, 6)<br/>km7 &lt;- kmeans(mammals_scaled, 7)<br/>km8 &lt;- kmeans(mammals_scaled, 8)<br/>km9 &lt;- kmeans(mammals_scaled, 9)<br/>km10 &lt;- kmeans(mammals_scaled, 10)<br/>km11 &lt;- kmeans(mammals_scaled, 11)</span><span id="7e00" class="mu lj iq mq b gy mz mw l mx my">p1 &lt;- fviz_cluster(km2, data = mammals_scaled, frame.type = "convex") + theme_minimal() + ggtitle("k = 2") <br/>p2 &lt;- fviz_cluster(km3, data = mammals_scaled, frame.type = "convex") + theme_minimal() + ggtitle("k = 3")<br/>p3 &lt;- fviz_cluster(km4, data = mammals_scaled, frame.type = "convex") + theme_minimal() + ggtitle("k = 4")<br/>p4 &lt;- fviz_cluster(km5, data = mammals_scaled, frame.type = "convex") + theme_minimal() + ggtitle("k = 5")<br/>p5 &lt;- fviz_cluster(km6, data = mammals_scaled, frame.type = "convex") + theme_minimal() + ggtitle("k = 6")<br/>p6 &lt;- fviz_cluster(km7, data = mammals_scaled, frame.type = "convex") + theme_minimal() + ggtitle("k = 7")</span><span id="8acf" class="mu lj iq mq b gy mz mw l mx my">plot_grid(p1, p2, p3, p4, p5, p6, labels = c("k2", "k3", "k4", "k5", "k6", "k7"))</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/30e92bf1701dcd255610775bce2370fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Lfl-pqYn44gypN1qlQMmg.png"/></div></div></figure><p id="b334" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管这种视觉评估告诉我们在聚类之间的何处发生了描绘，但是它没有告诉我们最优的聚类数目是多少。</p><h1 id="a22c" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">确定最佳聚类数</h1><p id="a5b6" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">在文献中已经提出了多种评估聚类结果的方法。术语<strong class="kf ir">聚类验证</strong>用于设计评估聚类算法结果的程序。有三十多种指标和方法可以用来确定最佳集群数量，所以我在这里只关注其中的几种，包括非常简洁的<strong class="kf ir"> clustree </strong>包。</p><h2 id="68fd" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">“肘”法</h2><p id="c379" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">可能是最广为人知的方法，即肘形法，在该方法中，计算每个聚类数的平方和并绘制图形，用户寻找从陡到浅的斜率变化(肘形)，以确定最佳聚类数。这种方法不精确，但仍有潜在的帮助。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="44a1" class="mu lj iq mq b gy mv mw l mx my">set.seed(31)<br/># function to compute total within-cluster sum of squares<br/>fviz_nbclust(mammals_scaled, kmeans, method = "wss", k.max = 24) + theme_minimal() + ggtitle("the Elbow Method")</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/3af4bc07207e1feb1e2193fed69ca90c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQUkkjBgQBEWigJVNJc9DA.png"/></div></div></figure><p id="7652" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">肘形曲线法很有帮助，因为它显示了增加聚类数如何有助于以有意义的方式而不是边际方式分离聚类。弯曲表示超过第三个的其他聚类没有什么价值(参见[ <a class="ae kc" href="http://web.stanford.edu/~hastie/Papers/gap.pdf" rel="noopener ugc nofollow" target="_blank">此处的</a> ]以获得该方法的更精确的数学解释和实现)。肘方法是相当清楚的，如果不是一个基于组内方差的天真的解决方案。间隙统计是一种更复杂的方法，用于处理分布没有明显聚类的数据(对于球状、高斯分布、轻度不相交的数据分布，可以找到正确数量的<em class="le"> k </em>)。</p><h2 id="651c" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">差距统计</h2><p id="5a3b" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated"><a class="ae kc" href="http://www.web.stanford.edu/~hastie/Papers/gap.pdf" rel="noopener ugc nofollow" target="_blank">缺口统计量</a>将<strong class="kf ir"> k </strong>的不同值的总体组内变异与数据的零参考分布下的预期值进行比较。最佳聚类的估计将是最大化间隙统计的值(<em class="le">，即</em>，其产生最大的间隙统计)。这意味着聚类结构远离点的随机均匀分布。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="55ce" class="mu lj iq mq b gy mv mw l mx my">gap_stat &lt;- clusGap(mammals_scaled, FUN = kmeans, nstart = 30, K.max = 24, B = 50)</span><span id="d16f" class="mu lj iq mq b gy mz mw l mx my">fviz_gap_stat(gap_stat) + theme_minimal() + ggtitle("fviz_gap_stat: Gap Statistic")</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/0f2df6898663d0bb7c704a2d4f836b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1KMRhZEIy0je24ksyiByxQ.png"/></div></div></figure><p id="23c5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">gap stats 图按聚类数(<strong class="kf ir"> k </strong>)显示统计数据，标准误差用垂直线段绘制，最佳值<strong class="kf ir"> k </strong>用垂直蓝色虚线标记。根据这一观察<strong class="kf ir"> k = 2 </strong>是数据中聚类的最佳数量。</p><h2 id="6ada" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">剪影法</h2><p id="db20" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">另一种有助于确定最佳聚类数的可视化方法称为剪影法。平均轮廓法计算不同 k 值的观测值的平均轮廓。最佳聚类数 k 是在一系列可能的 k 值<strong class="kf ir">上使平均轮廓最大化的聚类数。</strong></p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="5157" class="mu lj iq mq b gy mv mw l mx my">fviz_nbclust(mammals_scaled, kmeans, method = "silhouette", k.max = 24) + theme_minimal() + ggtitle("The Silhouette Plot")</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/c06cd7986214102aa7998edd7c686b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gtRd99n__IAB3Z1J-hOcpw.png"/></div></div></figure><p id="bde8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这也表明 2 个集群是最佳的。</p><h2 id="8561" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">平方和法</h2><p id="2192" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">另一种聚类验证方法是通过最小化类内平方和(衡量每个类紧密程度的指标)和最大化类间平方和(衡量每个类与其他类的分离程度的指标)来选择最佳的类数。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="a18d" class="mu lj iq mq b gy mv mw l mx my">ssc &lt;- data.frame(<br/>  kmeans = c(2,3,4,5,6,7,8),<br/>  within_ss = c(mean(km2$withinss), mean(km3$withinss), mean(km4$withinss), mean(km5$withinss), mean(km6$withinss), mean(km7$withinss), mean(km8$withinss)),<br/>  between_ss = c(km2$betweenss, km3$betweenss, km4$betweenss, km5$betweenss, km6$betweenss, km7$betweenss, km8$betweenss)<br/>)</span><span id="0438" class="mu lj iq mq b gy mz mw l mx my">ssc %&lt;&gt;% gather(., key = "measurement", value = value, -kmeans)</span><span id="ba57" class="mu lj iq mq b gy mz mw l mx my">#ssc$value &lt;- log10(ssc$value)</span><span id="a414" class="mu lj iq mq b gy mz mw l mx my">ssc %&gt;% ggplot(., aes(x=kmeans, y=log10(value), fill = measurement)) + geom_bar(stat = "identity", position = "dodge") + ggtitle("Cluster Model Comparison") + xlab("Number of Clusters") + ylab("Log10 Total Sum of Squares") + scale_x_discrete(name = "Number of Clusters", limits = c("0", "2", "3", "4", "5", "6", "7", "8"))</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/83852ffc27e884e143bda1bfe227dcfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9vEjv6Xir4HX58pBjNMfkQ.png"/></div></div></figure><p id="a6a3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据这一测量，似乎 7 个集群将是合适的选择。</p><h2 id="0a92" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">NbClust</h2><p id="c0c9" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated"><strong class="kf ir"> NbClust </strong>包提供了 30 个指数，用于确定相关的聚类数，并从通过改变聚类数、距离度量和聚类方法的所有组合获得的不同结果中向用户建议最佳聚类方案。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="8b07" class="mu lj iq mq b gy mv mw l mx my">res.nbclust &lt;- NbClust(mammals_scaled, distance = "euclidean",<br/>                  min.nc = 2, max.nc = 9, <br/>                  method = "complete", index ="all")</span><span id="acfb" class="mu lj iq mq b gy mz mw l mx my">factoextra::fviz_nbclust(res.nbclust) + theme_minimal() + ggtitle("NbClust's optimal number of clusters")</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/b9dadf2921023c96a4d635af2a1a00f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QzzyNowIPofo4Ak-z42n-w.png"/></div></div></figure><p id="308a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这表明聚类的最佳数目是 3。</p><h2 id="1be7" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">克鲁斯特里</h2><p id="6ae2" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">上面统计方法产生一个分数，该分数每次只考虑一组聚类。<a class="ae kc" href="https://github.com/lazappi/clustree" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> clustree </strong> </a> R 软件包采用了另一种方法，即考虑样本如何随着聚类数量的增加而改变分组。这有助于显示哪些聚类是独特的，哪些是不稳定的。它没有明确地告诉你哪一个选择是<em class="le">最优</em>集群，但是它对于探索可能的选择是有用的。</p><p id="8ba4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们来看看 1 到 11 个集群。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="8813" class="mu lj iq mq b gy mv mw l mx my">tmp &lt;- NULL<br/>for (k in 1:11){<br/>  tmp[k] &lt;- kmeans(mammals_scaled, k, nstart = 30)<br/>}</span><span id="00ed" class="mu lj iq mq b gy mz mw l mx my">df &lt;- data.frame(tmp)</span><span id="1a28" class="mu lj iq mq b gy mz mw l mx my"># add a prefix to the column names<br/>colnames(df) &lt;- seq(1:11)<br/>colnames(df) &lt;- paste0("k",colnames(df))</span><span id="3543" class="mu lj iq mq b gy mz mw l mx my"># get individual PCA<br/>df.pca &lt;- prcomp(df, center = TRUE, scale. = FALSE)</span><span id="a01e" class="mu lj iq mq b gy mz mw l mx my">ind.coord &lt;- df.pca$x<br/>ind.coord &lt;- ind.coord[,1:2]</span><span id="8742" class="mu lj iq mq b gy mz mw l mx my">df &lt;- bind_cols(as.data.frame(df), as.data.frame(ind.coord))</span><span id="2087" class="mu lj iq mq b gy mz mw l mx my">clustree(df, prefix = "k")</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/8e898675bcc8ecbd406ac39bcb0f5f6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T43RY6iKGjqtKSFvr8YsLg.png"/></div></div></figure><p id="f31c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在该图中，每个节点的大小对应于每个聚类中的样本数，并且箭头根据每个聚类接收的样本数来着色。一组独立的箭头，透明的，称为输入节点比例，也是彩色的，显示了一个组中的样本如何在另一个组中结束——聚类不稳定性的指标。</p><p id="8680" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个图中，我们看到，当我们从<strong class="kf ir"> k=2 </strong>移动到<strong class="kf ir"> k=3 </strong>时，来自观察者左侧群集的许多物种被重新分配到右侧的第三个群集。当我们从<strong class="kf ir"> k=8 </strong>移动到<strong class="kf ir"> k=9 </strong>时，我们看到一个节点具有多个传入边，这表明我们过度聚集了数据。</p><p id="00b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将这个维度覆盖到数据中的其他维度上也是有用的，特别是那些来自降维技术的维度。我们可以使用<strong class="kf ir"> clustree_overlay() </strong>函数来实现:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="86aa" class="mu lj iq mq b gy mv mw l mx my">df_subset &lt;- df %&gt;% select(1:8,12:13)</span><span id="d916" class="mu lj iq mq b gy mz mw l mx my">clustree_overlay(df_subset, prefix = "k", x_value = "PC1", y_value = "PC2")</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/8cd386007a7e21cfc0f5554e433b09d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hiVccajlxLATh9DJjacMhw.png"/></div></div></figure><p id="211b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我更喜欢从侧面看，显示 x 或 y 维度中的一个相对于分辨率维度。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="e5c5" class="mu lj iq mq b gy mv mw l mx my">overlay_list &lt;- clustree_overlay(df_subset, prefix = "k", x_value = "PC1",<br/>                                 y_value = "PC2", plot_sides = TRUE)</span><span id="d11e" class="mu lj iq mq b gy mz mw l mx my">overlay_list$x_side</span><span id="d6b4" class="mu lj iq mq b gy mz mw l mx my">overlay_list$y_side</span></pre><div class="ml mm mn mo gt ab cb"><figure class="nd jr nx nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/a2c5c9c526ceb24befe04ca3c803ad2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*mS_Ix9xschUWuAwjLHYHRg.png"/></div></figure><figure class="nd jr nx nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/559fd74a1d450aeff27ab879a71291db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*cLXIl9t0312kiSjvopoE1g.png"/></div></figure></div><p id="081b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这表明，我们可以通过检查边缘来指示正确的聚类分辨率，并且我们可以通过过多的信息来评估聚类的质量。</p><h1 id="8ce5" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">选择合适的算法</h1><p id="37ec" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">如何选择合适的聚类算法？<strong class="kf ir"> cValid </strong>包可用于同时比较多个聚类算法，以确定最佳聚类方法和最佳聚类数。我们将比较 k-means、层次聚类和 PAM 聚类。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="a8be" class="mu lj iq mq b gy mv mw l mx my">intern &lt;- clValid(mammals_scaled, nClust = 2:24, <br/>              clMethods = c("hierarchical","kmeans","pam"), validation = "internal")</span><span id="7295" class="mu lj iq mq b gy mz mw l mx my"># Summary<br/>summary(intern) %&gt;% kable() %&gt;% kable_styling()</span><span id="376a" class="mu lj iq mq b gy mz mw l mx my">Clustering Methods:<br/> hierarchical kmeans pam <br/><br/>Cluster sizes:<br/> 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <br/><br/>Validation Measures:<br/>                                 2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24<br/>                                                                                                                                                                                                                  <br/>hierarchical Connectivity   4.1829 10.5746 13.2579 20.1579 22.8508 25.8258 32.6270 35.3032 38.2905 39.2405 41.2405 45.7742 47.2742 50.6075 52.6075 55.8575 58.7242 60.7242 63.2242 65.2242 67.2242 69.2242 71.2242<br/>             Dunn           0.3595  0.3086  0.3282  0.2978  0.3430  0.3430  0.4390  0.4390  0.5804  0.5938  0.5938  0.8497  0.8497  0.5848  0.5848  0.4926  0.9138  0.9138  0.8892  0.9049  0.9335  1.0558  2.1253<br/>             Silhouette     0.5098  0.5091  0.4592  0.4077  0.4077  0.3664  0.3484  0.4060  0.3801  0.3749  0.3322  0.3646  0.3418  0.2650  0.2317  0.2166  0.2469  0.2213  0.1659  0.1207  0.1050  0.0832  0.0691<br/>kmeans       Connectivity   7.2385 10.5746 15.8159 20.1579 22.8508 25.8258 33.5198 35.3032 38.2905 39.2405 41.2405 45.7742 47.2742 51.8909 53.8909 57.1409 58.7242 60.7242 63.2242 65.2242 67.2242 69.2242 71.2242<br/>             Dunn           0.2070  0.3086  0.2884  0.2978  0.3430  0.3430  0.3861  0.4390  0.5804  0.5938  0.5938  0.8497  0.8497  0.5866  0.5866  0.5725  0.9138  0.9138  0.8892  0.9049  0.9335  1.0558  2.1253<br/>             Silhouette     0.5122  0.5091  0.4260  0.4077  0.4077  0.3664  0.3676  0.4060  0.3801  0.3749  0.3322  0.3646  0.3418  0.2811  0.2478  0.2402  0.2469  0.2213  0.1659  0.1207  0.1050  0.0832  0.0691<br/>pam          Connectivity   7.2385 14.1385 17.4746 24.0024 26.6857 32.0413 33.8913 36.0579 38.6607 40.6607 42.7869 45.7742 47.2742 51.7242 53.7242 56.9742 58.7242 60.7242 62.7242 64.7242 66.7242 69.2242 71.2242<br/>             Dunn           0.2070  0.1462  0.2180  0.2180  0.2978  0.2980  0.4390  0.4390  0.4390  0.4390  0.4390  0.8497  0.8497  0.5314  0.5314  0.4782  0.9138  0.9138  0.8333  0.8189  0.7937  1.0558  2.1253<br/>             Silhouette     0.5122  0.3716  0.4250  0.3581  0.3587  0.3318  0.3606  0.3592  0.3664  0.3237  0.3665  0.3646  0.3418  0.2830  0.2497  0.2389  0.2469  0.2213  0.1758  0.1598  0.1380  0.0832  0.0691<br/><br/>Optimal Scores:<br/><br/>             Score  Method       Clusters<br/>Connectivity 4.1829 hierarchical 2       <br/>Dunn         2.1253 hierarchical 24      <br/>Silhouette   0.5122 kmeans       2</span></pre><p id="903c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">连通性和轮廓都是连通性的度量，而邓恩指数是不在同一聚类中的观测值之间的最小距离与最大聚类内距离的比率。</p><h1 id="ebc8" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">提取聚类的特征</h1><p id="435a" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">如前所述，很难评估聚类结果的质量。我们没有真正的标签，所以不清楚如何在内部验证方面衡量"<em class="le">它实际上有多好</em>。但是，集群是一个很好的 EDA 起点，可以用来更详细地探索集群之间的差异。把聚类想象成制造衬衫尺寸。我们可以选择只做三种尺寸:<em class="le">小号</em>、<em class="le">中号</em>和<em class="le">大号。我们肯定会削减成本，但并不是每个人都会非常适应。想想现在的裤子尺码(或者有很多尺码的衬衫品牌(XS、XL、XXL、<em class="le">等)。你有更多的类别(或集群)。对于某些领域，最佳集群的选择可能依赖于一些外部知识，如生产 k 个集群以满足客户的最佳需求的成本。在其他领域，如生物学，你试图确定细胞的准确数量，需要一个更深入的方法。例如，上面的许多试探法在群集的最佳数量上互相矛盾。请记住，这些都是在不同数量的<em class="le"> k 处评估<em class="le"> k 均值</em>算法。</em>这可能意味着<em class="le"> k 均值</em>算法失败，并且没有<em class="le"> k </em>是好的。<em class="le"> k-means </em>算法不是一个非常健壮的算法，它对异常值非常敏感，而且这个数据集非常小。最好的办法是在其他算法的输出上探索上述方法(例如<strong class="kf ir"> clValid </strong>建议的层次聚类)，收集更多的数据，或者如果可能的话花一些时间为其他 ML 方法标记样本。</em></em></p><p id="f322" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最终，我们想回答这样的问题:“是什么让这个集群与众不同？”以及“彼此相似的集群是什么”。让我们选择五个集群，并询问这些集群的特征。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="f696" class="mu lj iq mq b gy mv mw l mx my"># Compute dissimilarity matrix with euclidean distances<br/>d &lt;- dist(mammals_scaled, method = "euclidean")</span><span id="1adf" class="mu lj iq mq b gy mz mw l mx my"># Hierarchical clustering using Ward's method<br/>res.hc &lt;- hclust(d, method = "ward.D2" )</span><span id="3cf5" class="mu lj iq mq b gy mz mw l mx my"># Cut tree into 5 groups<br/>grp &lt;- cutree(res.hc, k = 5)</span><span id="a7b9" class="mu lj iq mq b gy mz mw l mx my"># Visualize<br/>plot(res.hc, cex = 0.6) # plot tree<br/>rect.hclust(res.hc, k = 5, border = 2:5) # add rectangle</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/88508fda914db5fcb7ab61a7e99eead4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZQoXKPBGdheBlektbKU3A.png"/></div></div></figure><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="9e17" class="mu lj iq mq b gy mv mw l mx my"># Execution of k-means with k=5<br/>final &lt;- kmeans(mammals_scaled, 5, nstart = 30)</span><span id="8b95" class="mu lj iq mq b gy mz mw l mx my">fviz_cluster(final, data = mammals_scaled) + theme_minimal() + ggtitle("k = 5")</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/2aef3d4d34c56dd2ff94e126fd3aa69d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6LJgn9cf8ltcNDGGW206GA.png"/></div></div></figure><p id="039a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们提取聚类并将它们添加回初始数据，以便在聚类级别进行一些描述性统计:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="0485" class="mu lj iq mq b gy mv mw l mx my">as.data.frame(mammals_scaled) %&gt;% mutate(Cluster = final$cluster) %&gt;% group_by(Cluster) %&gt;% summarise_all("mean") %&gt;% kable() %&gt;% kable_styling()</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/ed1f678cadbf8a7837326100c28e571c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5m_K-dg0Ky8Pc-P7ZFZzaA.png"/></div></div></figure><p id="b629" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们看到仅由兔子组成的簇 2 具有高灰分含量。由海豹和海豚组成的第 3 组脂肪含量高，这在如此寒冷的气候下有着苛刻的要求，而第 4 组含有大量的乳糖。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="55e3" class="mu lj iq mq b gy mv mw l mx my">mammals_df &lt;- as.data.frame(mammals_scaled) %&gt;% rownames_to_column()</span><span id="1d78" class="mu lj iq mq b gy mz mw l mx my">cluster_pos &lt;- as.data.frame(final$cluster) %&gt;% rownames_to_column()<br/>colnames(cluster_pos) &lt;- c("rowname", "cluster")</span><span id="c464" class="mu lj iq mq b gy mz mw l mx my">mammals_final &lt;- inner_join(cluster_pos, mammals_df)</span><span id="a965" class="mu lj iq mq b gy mz mw l mx my">ggRadar(mammals_final[-1], aes(group = cluster), rescale = FALSE, legend.position = "none", size = 1, interactive = FALSE, use.label = TRUE) + facet_wrap(~cluster) + scale_y_discrete(breaks = NULL) + # don't show ticks<br/>theme(axis.text.x = element_text(size = 10)) + scale_fill_manual(values = rep("#1c6193", nrow(mammals_final))) +<br/>scale_color_manual(values = rep("#1c6193", nrow(mammals_final))) +<br/>ggtitle("Mammals Milk Attributes")</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/1810f554b5058c7b4b327739dfdeefd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Rvv6zH-BRt2mP7QfAbKXg.png"/></div></div></figure><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="6d02" class="mu lj iq mq b gy mv mw l mx my">mammals_df &lt;- as.data.frame(mammals_scaled)<br/>mammals_df$cluster &lt;- final$cluster<br/>mammals_df$cluster &lt;- as.character(mammals_df$cluster)</span><span id="35dc" class="mu lj iq mq b gy mz mw l mx my">ggpairs(mammals_df, 1:5, mapping = ggplot2::aes(color = cluster, alpha = 0.5), <br/>        diag = list(continuous = wrap("densityDiag")), <br/>        lower=list(continuous = wrap("points", alpha=0.9)))</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/7f7aa3bec6dab64618621be696021e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a0IlGgZD9xWFbvUaYj8tFQ.png"/></div></div></figure><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="d2da" class="mu lj iq mq b gy mv mw l mx my"># plot specific graphs from previous matrix with scatterplot</span><span id="75bb" class="mu lj iq mq b gy mz mw l mx my">g &lt;- ggplot(mammals_df, aes(x = water, y = lactose, color = cluster)) +<br/>        geom_point() +<br/>        theme(legend.position = "bottom")<br/>ggExtra::ggMarginal(g, type = "histogram", bins = 20, color = "grey", fill = "blue")</span><span id="3a7a" class="mu lj iq mq b gy mz mw l mx my">b &lt;- ggplot(mammals_df, aes(x = protein, y = fat, color = cluster)) +<br/>        geom_point() +<br/>        theme(legend.position = "bottom")<br/>ggExtra::ggMarginal(b, type = "histogram", bins = 20, color = "grey", fill = "blue")</span></pre><div class="ml mm mn mo gt ab cb"><figure class="nd jr nx nf ng nh ni paragraph-image"><img src="../Images/8da7b27d15b01eda05565b2416e5412f.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*_HjPcJFvCmTX_EEeBa_wKw.png"/></figure><figure class="nd jr nx nf ng nh ni paragraph-image"><img src="../Images/444f0ba52de8af6129d3f15f989e8f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*xsRDYljJzvxWm4n4zYARJA.png"/></figure></div><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="6fc3" class="mu lj iq mq b gy mv mw l mx my">ggplot(mammals_df, aes(x = cluster, y = protein)) + <br/>        geom_boxplot(aes(fill = cluster))</span><span id="d17b" class="mu lj iq mq b gy mz mw l mx my">ggplot(mammals_df, aes(x = cluster, y = fat)) + <br/>        geom_boxplot(aes(fill = cluster))</span><span id="f8ae" class="mu lj iq mq b gy mz mw l mx my">ggplot(mammals_df, aes(x = cluster, y = lactose)) + <br/>        geom_boxplot(aes(fill = cluster))</span><span id="32f7" class="mu lj iq mq b gy mz mw l mx my">ggplot(mammals_df, aes(x = cluster, y = ash)) + <br/>        geom_boxplot(aes(fill = cluster))</span><span id="d2e9" class="mu lj iq mq b gy mz mw l mx my">ggplot(mammals_df, aes(x = cluster, y = water)) + <br/>        geom_boxplot(aes(fill = cluster))</span></pre><div class="ml mm mn mo gt ab cb"><figure class="nd jr ne nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/be6db7b2202865b20806e1716ebbe5e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*4inw4x2G7yVUZD_pN_vhwA.png"/></div></figure><figure class="nd jr ne nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/6ee58b8d055242a8ae91975096ad3f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*aL05uDbYvpMsnj6SzNa_yA.png"/></div></figure><figure class="nd jr ne nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/aabd285a220a11d600a96a6ae09f88a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ZPZ1PdSOhhQmFyn8EalbvQ.png"/></div></figure></div><div class="ab cb"><figure class="nd jr oa nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/c3bb0848f853a4396b40ba64d22bfcc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*tZXqag6Dd-bqP2WSFsli2w.png"/></div></figure><figure class="nd jr ob nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/231909ca37bffc9c25ac22f626f4b93f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*w4FzmICs6Z1HFxn7tzr8QQ.png"/></div></figure></div><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="c121" class="mu lj iq mq b gy mv mw l mx my"># Parallel coordiante plots allow us to put each feature on seperate column and lines connecting each column</span><span id="731f" class="mu lj iq mq b gy mz mw l mx my">ggparcoord(data = mammals_df, columns = 1:5, groupColumn = 6, alphaLines = 0.4, title = "Parallel Coordinate Plot for the Mammals Milk Data", scale = "globalminmax", showPoints = TRUE) + theme(legend.position = "bottom")</span></pre><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/bcb4e30db4df0780a4ee559643105beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7-3LoneZQfZfM8EzLNe6hQ.png"/></div></div></figure><p id="3b73" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你觉得这篇文章有用，请随意与他人分享或推荐这篇文章！😃</p><p id="1d69" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一如既往，如果您有任何问题或意见，请随时在下面留下您的反馈，或者您可以随时通过 LinkedIn 联系我。在那之前，下一篇文章再见！😄</p></div></div>    
</body>
</html>