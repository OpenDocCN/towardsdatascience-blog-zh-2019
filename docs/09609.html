<html>
<head>
<title>How to Do Ridge Regression Better</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何更好的做岭回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-do-ridge-regression-better-34ecb6ee3b12?source=collection_archive---------25-----------------------#2019-12-17">https://towardsdatascience.com/how-to-do-ridge-regression-better-34ecb6ee3b12?source=collection_archive---------25-----------------------#2019-12-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4b4b" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/optimization-and-ml" rel="noopener" target="_blank">优化和机器学习</a></h2><div class=""/><div class=""><h2 id="702b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用优化器来寻找性能最佳的正则化矩阵</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/c3b87998ed56c5419913b853114a3330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Q1ilEPL7L7x8KhnMVdsbg.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@therawhunter?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Massimiliano Morosinotto</a> on <a class="ae le" href="https://unsplash.com/s/photos/peak?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="aa2a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让<strong class="lh ja"> X </strong>和<strong class="lh ja"> y </strong>表示训练数据的样本，其中<strong class="lh ja"> X </strong>是具有 n 行特征向量的矩阵，而<strong class="lh ja"> y </strong>是 n 个对应目标值的向量。如果<strong class="lh ja">𝐱′</strong>是具有未知目标值 y’的样本外特征向量，那么我们可以拟合线性模型<strong class="lh ja"> b̂ </strong>，目标是<strong class="lh ja"> </strong>最小化预期样本外误差</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/4012741ac9e8c453f142fc94369a4ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*pX-woVM5O0sfJj5YcuXtBQ@2x.png"/></div></figure><p id="c90d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一种拟合模型的方法，最小二乘法，选择<strong class="lh ja"> b̂ </strong>来最小化训练数据的误差平方和</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/15aeed19f3605028e382a7f97a4c2267.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*pcFQWBhoyn0zWCfuJ1RNnA@2x.png"/></div></figure><p id="0da1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然在给定足够的训练数据的情况下，这可能是一个很好的选择，但如果训练数据较少、噪声较多或预测器较弱的特征较多，它可能会过度拟合数据，从而使<strong class="lh ja"> b̂ </strong>更多地反映噪声，而不是潜在的统计关系。</p><p id="95c3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">岭回归修正了最小二乘法以最小化</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi md"><img src="../Images/0e5343946f7756e4b523f305c6c0e869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71Dd5laKbMexRNPMc-U0ZQ@2x.png"/></div></div></figure><p id="cdc6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">利用合适的矩阵<strong class="lh ja">γ</strong>，岭回归可以收缩或者限制<strong class="lh ja"> b̂ </strong>的系数，以减少过拟合并提高样本外预测的性能。挑战在于正确选择<strong class="lh ja">γ</strong>。</p><p id="a684" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通常，<strong class="lh ja">γ</strong>仅限于形式</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi me"><img src="../Images/39f50cf369b4da640098fca273738ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*2pTk6RB_tZjxl7N4ipnW5A@2x.png"/></div></figure><p id="ab7e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">并且通过在训练数据的<a class="ae le" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="noopener ugc nofollow" target="_blank">交叉验证</a>上试验不同的值并挑选具有最佳分数的值来选择α。我会参考这个，或者其他类似这样的选择<strong class="lh ja">γ</strong>、<strong class="lh ja"> </strong>的方法作为微调。调整的缺点是会导致额外的计算，并且除了最简单的<strong class="lh ja">γ</strong>的参数化之外，不能扩展到任何参数化。例如，如果我们希望每个变量都有一个单独的正则项</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/d60f9c77da77d2187a7cbae76f08010f.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*63JMu784jNsSboWUkDST5w@2x.png"/></div></figure><p id="c599" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们有很多变量，通过调整来设置<strong class="lh ja">γ</strong>是不可能的。</p><p id="85bb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇博文中，我将展示如何将某些交叉验证的性能最大化<strong class="lh ja">γ</strong>设置为适当的优化问题，其中我们计算一阶、二阶导数，并在优化器的帮助下有效地迭代到最佳性能参数。这将允许我们扩展到具有许多变量的参数化，并且经常会导致比调整方法更好的结果。</p><h2 id="84c1" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lo mp mq mr ls ms mt mu lw mv mw mx iw bi translated">留一交叉验证</h2><p id="1964" class="pw-post-body-paragraph lf lg iq lh b li my ka lk ll mz kd ln lo na lq lr ls nb lu lv lw nc ly lz ma ij bi translated">让</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/4fb4a0a669683c0b0c63df8d6241da7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*6jL9Wu-qOegmIpNIMdRXvw@2x.png"/></div></figure><p id="3f51" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">表示移除第 I 个条目的特征矩阵和目标向量</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ne"><img src="../Images/566ebd8f5f629625d872d65a59e27dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-slnBXIsHbIgfvFYksUxQ@2x.png"/></div></div></figure><p id="eb94" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">表示适合这些数据的回归量。我们将<a class="ae le" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation" rel="noopener ugc nofollow" target="_blank">留一交叉验证</a> (LOOCV)定义为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/84abc8a999acb4141df38b8fb568f19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*i4zTzuoE5_B_9Q2p6ss2OA@2x.png"/></div></figure><p id="3571" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">并且使用有效的优化器来寻求相对于<strong class="lh ja">γ</strong>最小化该值。优化器通过对函数的导数进行局部逼近来工作。他们使用近似来采取连续的步骤来改进目标，直到达到局部最优。但是在我们计算 LOOCV 的导数以提供给优化器之前，我们需要为目标导出一个更易处理的形式。</p><p id="d0b0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">观察</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ng"><img src="../Images/cb887159f06be22041facc8311ddf22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xQh3kuo_CPvtuiOKYOmNNA@2x.png"/></div></div></figure><p id="d7e5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">放</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/2d0e31577a0590a2219baae56982c55f.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*1jTsslt4rnPOExdYhISsqA@2x.png"/></div></figure><p id="ecf4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">那么对于任何一个<strong class="lh ja"> z </strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ni"><img src="../Images/3e4b14380146872349b6dd2f2ba0633c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9TMkmjJ5g5_nj45_q4wpaQ@2x.png"/></div></div></figure><p id="1630" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/4366b1d97e28b70cade9427b30b2b5f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*YIRrHR5MQug3Eaxdhw-H0A@2x.png"/></div></figure><p id="0b23" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，我们可以将岭回归方程改写为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nk"><img src="../Images/a41c831261bdda8ec55565aece3810d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g8BQSniJkXyRE41xNphcNw@2x.png"/></div></div></figure><p id="6449" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因为<strong class="lh ja"> z </strong>和<strong class="lh ja"> y </strong>不依赖于<strong class="lh ja"> b </strong>，并且<strong class="lh ja"> A </strong>是<a class="ae le" href="https://en.wikipedia.org/wiki/Definiteness_of_a_matrix" rel="noopener ugc nofollow" target="_blank">正定</a>，因此当</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/37f2da270865dd5f8113371cb922bb20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*-crsvLgpHdldS9ruMynltw@2x.png"/></div></figure><p id="8149" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/8cb1422d1af9830fcb30f84f7be74a49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNmKFPbFwqkJXkuySVybKA@2x.png"/></div></div></figure><p id="6d2c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">像这样为每个条目求解岭回归仍然是昂贵的，但幸运的是，我们可以进一步操纵方程来实现更有效的东西。注意到</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/b600804a0f692845a57c7f1499355bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFufC-pV_P8CkhjooPw6_A@2x.png"/></div></div></figure><p id="4b60" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据谢尔曼-莫里森公式，我们有</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/f1495cbd1f5d6f91d775a06c0399de22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*olYOhilSG2JhMQ1LuEKe8w@2x.png"/></div></div></figure><p id="cff6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3aaeb6ecde605f57592112881f57972a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*2qzxJ29AHWWP7IX3avEvxA@2x.png"/></div></figure><p id="17e0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">放</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b579a4a2dfa13f5eccacd9c5054dc81a.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*7EDuD3HdvoGD0szxL6p97g@2x.png"/></div></figure><p id="3ea7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后自从</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/9d3c3c42c4f285ba5e1cf28ae7b8f94c.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*fcZGhcOh1aUuDtqRAcUg2A@2x.png"/></div></figure><p id="26c4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以将第 I 个留一岭回归解改写为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/d725ab0964ac9a07e9deb4c977d8d289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pkPxuVySE_Rzy23VV8A5og@2x.png"/></div></div></figure><p id="4062" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">将此代入 LOOCV 的第 I 项，我们得到</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/00a9dc30fa8385a41835d2f1e14d5dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cdoau6g_u16PpSW6NycqMA@2x.png"/></div></div></figure><p id="e5a4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">LOOCV 变成了</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c6a3341e526aed0f24b22dbd7ec05806.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*r0ud14cSjuJZuDVRNdYXmg@2x.png"/></div></figure><h2 id="230c" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lo mp mq mr ls ms mt mu lw mv mw mx iw bi translated">计算 LOOCV 梯度</h2><p id="7811" class="pw-post-body-paragraph lf lg iq lh b li my ka lk ll mz kd ln lo na lq lr ls nb lu lv lw nc ly lz ma ij bi translated">我们可以用上一节的公式来推导导数的方程。设<strong class="lh ja"> α </strong>表示<strong class="lh ja">γ</strong>的参数向量，定义 L( <strong class="lh ja"> α </strong>)为给定参数的 LOOCV，其中<strong class="lh ja"> L </strong> _i 表示 LOOCV 求和的第 I 项。我假设<strong class="lh ja">γ</strong>由对角矩阵参数化</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/d60f9c77da77d2187a7cbae76f08010f.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*63JMu784jNsSboWUkDST5w@2x.png"/></div></figure><p id="5e0b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">尽管这些方程可以容易地适用于其他参数化和单变量参数化</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi me"><img src="../Images/39f50cf369b4da640098fca273738ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*2pTk6RB_tZjxl7N4ipnW5A@2x.png"/></div></figure><p id="82ae" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">可以简单地计算为多变量偏导数的和。</p><p id="949b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">放</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c6570f00f4859028d66dbaa73e1a4006.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*0Jb7oGHW8llQbeQZuXU_Jw@2x.png"/></div></figure><p id="b0fc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">那么第 I 个 LOOCV 项的导数就是</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/025c189846e75e2ab59cc2700f5fbc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*27tbFfItHtMzLFDid-xxtg@2x.png"/></div></figure><p id="eb58" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">和</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/76af04a8f643cd3bc5b73a82263c3b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FUQMdCf-WFv56yK_nt59MQ@2x.png"/></div></div></figure><p id="dfd2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于<strong class="lh ja"> ŷ </strong>的偏导数，我们有</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/21703028d69b30e281243d750f2d9138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xLhapdhX283us6TDLgcyUQ@2x.png"/></div></div></figure><p id="d719" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">注意:在这里，我们已经利用这个公式来微分一个逆矩阵</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e4705c4b031721c1cd5563bc4371a880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*Za0qHpNIK3_RM3x9OJXBIA@2x.png"/></div></figure><p id="bb1f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">同样，我们可以计算出<strong class="lh ja"> h </strong>的偏导数</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/0162dc6cb5af91c91d8e94c8bfe3cf98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tq1UXtqVdKn_TRVK_oXh5g@2x.png"/></div></div></figure><p id="6fff" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">结合各项并求和，则全微分为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/1d7a20a8a8598abf4125ddd9a2424f3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*0-N7qnYkd66G3AM_hEmbJg@2x.png"/></div></figure><p id="7d44" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有了导数和值的有效公式，我们可以从任何初始猜测开始，使用优化程序，快速下降到最小化 LOOCV 的参数。</p><p id="5849" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">注意:为优化者提供 LOOCV 二阶导数的 hessian 矩阵也是有益的。这些方程更复杂，这篇博文只导出了一阶导数，但是它们可以用类似于梯度的方式计算。</p><h2 id="b072" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lo mp mq mr ls ms mt mu lw mv mw mx iw bi translated">模拟结果</h2><p id="fc48" class="pw-post-body-paragraph lf lg iq lh b li my ka lk ll mz kd ln lo na lq lr ls nb lu lv lw nc ly lz ma ij bi translated">让我们比较一下使用单个正则项的岭回归和对每个回归变量使用单独正则项的岭回归的性能。</p><p id="be2e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我将使用模拟</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/7e9d45f85eebf324a5e49dc41a7458ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*YMC1kpK0bmFsjCUS5gmNUQ@2x.png"/></div></figure><p id="6996" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于模拟的每次试验，模型将适合于生成的具有独立同分布随机变量的 n×3 特征矩阵的训练数据，其中</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/a87c19436dfad5aa80b8d0a12903c797.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*VkXhlQEBf0rUv4P5Zu6rvQ@2x.png"/></div></div></figure><p id="797f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">目标值将由以下各项生成</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/080ded93a78bf35c67d7f47c9c554a21.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*Uxn5h05bzwXDPrqLnZf9aA@2x.png"/></div></figure><p id="ed33" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">随着</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/4ad38ecc07ddaec64d87387ed030a299.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*1RXuJZgiYjuVJ8T4SnpIqg@2x.png"/></div></figure><p id="75d0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">试验的结果就是预测误差</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2a6c5292967d05e9ad9ca75e057e30fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*d-m5Mlf3M5MbVlWInfwW7Q@2x.png"/></div></figure><p id="fe07" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在样本外数据点<strong class="lh ja">𝐱′上，从与训练数据相同的分布中生成</strong>y′<strong class="lh ja"/>。误差是多次试运行的平均值，训练大小 n 是 varied⁴.比较的三个模型是</p><ul class=""><li id="2ab7" class="oh oi iq lh b li lj ll lm lo oj ls ok lw ol ma om on oo op bi translated"><strong class="lh ja"> LS </strong>:最小二乘回归</li><li id="4dcc" class="oh oi iq lh b li oq ll or lo os ls ot lw ou ma om on oo op bi translated"><strong class="lh ja"> RR-1 </strong>:单正则化的岭回归，由二阶优化器拟合，以最小化训练数据上的 LOOCV 误差。</li><li id="7ca8" class="oh oi iq lh b li oq ll or lo os ls ot lw ou ma om on oo op bi translated"><strong class="lh ja"> RR-p </strong>:针对每个特征变量使用单独正则化器的岭回归，由二阶优化器拟合，以最小化训练数据上的 LOOCV 误差。</li></ul><p id="0f8f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下图显示了每个 n 值的模型平均预测误差，以及代表平均值 95%置信区间的误差条(使用 t 统计)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/52c517a73a0d972dca45bc48570ff072.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*yzSfQLpk4CD1vdfWIrVlPg.png"/></div></figure><p id="8dd5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然结果对模拟参数敏感，但这表明，至少对于某些问题，单独的正则化子可以提供比单个正则化子更好的性能。</p><h2 id="9a63" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lo mp mq mr ls ms mt mu lw mv mw mx iw bi translated">结论</h2><p id="2eae" class="pw-post-body-paragraph lf lg iq lh b li my ka lk ll mz kd ln lo na lq lr ls nb lu lv lw nc ly lz ma ij bi translated">我们展示了如何有效地计算岭回归的 LOOCV，并推导出其导数的方程。这允许我们使用优化器来找到最小化 LOOCV 误差的正则化参数。优化器消除了挑选和试验不同参数的工作；但最重要的是，它打开了使用正则化矩阵的更复杂的多变量参数化的大门。我们展示了一个这样的多变量参数化(对每个特征变量使用单独的正则化)如何在测试问题上产生更好的性能。</p><p id="bba6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="ow">在下一期</em> <a class="ae le" rel="noopener" target="_blank" href="/what-form-of-cross-validation-should-you-use-76aaecc45c75"> <em class="ow">文章</em> </a> <em class="ow">中，我将解释为什么留一法通常不是交叉验证的正确形式，并介绍我们应该使用的广义交叉验证。我还将在一个现实世界的问题上比较不同岭回归方法的性能。</em></p><p id="4114" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="ow">如果您对自动调整正则化参数或使用多个正则化子感兴趣，请查看</em><a class="ae le" href="https://buildingblock.ai" rel="noopener ugc nofollow" target="_blank"><em class="ow">building block . ai</em></a><em class="ow">。</em></p></div><div class="ab cl ox oy hu oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="ij ik il im in"><p id="37b6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">脚注</p><p id="cd95" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[1]: <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html" rel="noopener ugc nofollow" target="_blank"> sklearn.linear_model。RidgeCV </a>(同样<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" rel="noopener ugc nofollow" target="_blank"> sklearn.model_selection。例如，GridSearchCV </a>)通过强力计算预先选择的参数列表的交叉验证，以找到得分最高的参数。</p><p id="a56f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]:另请参见<a class="ae le" href="https://github.com/rnburn/ridge-regression-doc/blob/master/notebooks/loocv-verify.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/rn burn/ridge-regression-doc/blob/master/notebooks/loo cv-verify . ipynb</a>以验证 loocv 方程</p><p id="59d5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3]:像这样推导导数方程很容易出错，但幸运的是，使用有限差分进行测试也很容易。参见<a class="ae le" href="https://github.com/rnburn/ridge-regression-doc/blob/master/notebooks/loocv-gradient-verify.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/rn burn/ridge-regression-doc/blob/master/notebooks/loo cv-gradient-verify . ipynb</a></p><p id="d4a6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4]:完整模拟可作为笔记本:<a class="ae le" href="https://github.com/rnburn/ridge-regression-doc/blob/master/notebooks/loocv-simulation.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/rn burn/ridge-regression-doc/blob/master/notebooks/loo cv-simulation . ipynb</a></p></div></div>    
</body>
</html>