<html>
<head>
<title>PCA: Eigenvectors and Eigenvalues</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析:特征向量和特征值</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pca-eigenvectors-and-eigenvalues-1f968bc6777a?source=collection_archive---------1-----------------------#2019-07-13">https://towardsdatascience.com/pca-eigenvectors-and-eigenvalues-1f968bc6777a?source=collection_archive---------1-----------------------#2019-07-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/cc54ea1aa0d994ab2d36351ba80034e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SETSiqIb7yYBZ_Mxzk7etg.png"/></div></div></figure><div class=""/><p id="7f9f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">每当你在处理数据的时候，你总是会面对相对的特征。后者是我们在描述数据时考虑的变量。也就是说，如果你正在收集一些关于米兰房屋的数据，典型的特征可能是位置、尺寸、楼层等等。</p><p id="d6f1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，经常发生的情况是，你的数据提供给你许多功能，有时是数百个…但是你需要全部吗？好吧，记住简约法则，我们宁愿处理一个只有很少特征的数据集:训练起来会容易得多，也快得多。另一方面，我们不希望在删除某些功能时丢失重要信息。</p><p id="08de" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们如何处理简单性和信息量之间的权衡呢？这个问题的答案是主成分分析(PCA)的结果。</p><p id="8fbe" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">主成分是由初始变量的线性组合构成的新变量。这些组合是以这样的方式完成的，即这些新变量是不相关的，并且初始变量中的大部分信息被存储到第一分量中。因此，这个想法是，<em class="kz"> k </em>维数据给你<em class="kz"> k </em>主成分，但是 PCA 试图把最大可能的信息放在第一个主成分中，这样，如果你想减少数据集的维数，你可以把你的分析集中在前几个成分上，而不会遭受信息损失的巨大损失。</p><p id="93ab" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这种分析中，度量信息量的是方差，并且主成分可以在几何上被视为高维数据的方向，其捕捉最大数量的方差并将其投影到较小维度的子空间，同时保留大部分信息。因此，第一主成分占最大可能的方差；第二个分量将直观地说明第二大方差(在一个条件下:它必须与第一个主分量不相关)等等。</p><p id="a1b5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了更深入地理解 PCA，我们需要引入一些进一步的概念。</p><h2 id="87b0" class="la lb je bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">协方差矩阵</h2><p id="4eab" class="pw-post-body-paragraph kb kc je kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">让我们考虑一个场景，其中我们只有两个特征，x 和 y。我们可以在 2D 图中表示我们的数据如下:</p><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ly"><img src="../Images/255758de64325e778620413f4547868d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HY9lPpbjzTu6RiSSNi_ErA.png"/></div></div></figure><p id="b439" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，我们可以计算所谓的协方差矩阵:它是一个对称的，<em class="kz"> dxd </em>矩阵(其中<em class="kz"> d </em>是特征的数量，因此在这种情况下<em class="kz"> d </em> =2 ),其中存储了每个特征的方差和交叉特征协方差:</p><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div class="gh gi md"><img src="../Images/4f5dc52fa0c50fa6e82dc4aa7e26a78d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*cTfEBqBYfLXcgZeBXIFX3Q.png"/></div></figure><p id="a662" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因为 Cov(x，y)等于 Cov(y，x)，所以如前所述，矩阵是对称的，并且特征的方差位于主对角线上。</p><p id="bb08" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">协方差矩阵可以根据数据的形状采用不同的值。让我们检查一些场景:</p><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi me"><img src="../Images/1e4bb8ea9ca24aad70ab5205878f53bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NlAfmaMOVwIvQyu9aFKSKA.png"/></div></div></figure><p id="aa5d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当两个特征正相关时，协方差大于零，否则，它具有负值。此外，如果没有证据表明它们之间存在相关性，则协方差等于零。</p><p id="648f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如您所见，协方差矩阵定义了数据的分布(方差)和方向(协方差)。这个矩阵还可以分配两个元素:一个代表向量和一个表示其大小的数。向量将指向数据的较大分布的方向，数字将等于该方向的分布(方差)。这两个元素分别是特征向量和特征值。让我们想象一下:</p><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mf"><img src="../Images/934348a946cb25f5b746165903411724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PimvwRx26O9SmJxq4CHfOA.png"/></div></div></figure><p id="9259" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">绿色的方向是特征向量，它有一个对应的值，叫做特征值，特征值描述了它的大小。让我们更详细地看看它是如何工作的。</p><h2 id="48b3" class="la lb je bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">特征向量和特征值</h2><p id="87d5" class="pw-post-body-paragraph kb kc je kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">为了更好地理解这些概念，让我们考虑下面的情况。我们被提供了二维向量 v1，v2，…，vn。然后，如果我们将线性变换 T(一个 2x2 矩阵)应用于我们的向量，我们将获得新的向量，称为 b1，b2，…，bn。</p><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/97f803a001bc5a8e9323c72a0356e173.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*nzsY92hHEB6flO0Hk6JFOw.png"/></div></figure><p id="1f07" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是，其中的一些(更具体地说，与特征的数量一样多)有一个非常有趣的特性:确实，一旦应用了变换 T，它们会改变长度，但不会改变方向。这些向量称为特征向量，代表特征向量倍数的标量称为特征值。</p><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/af65b3f7c2a07455cc163d609f6336bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*azOgWAL9UR6BWwABt-gnJQ.png"/></div></figure><p id="a29c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，每个特征向量都有一个对应的特征值。现在，如果我们考虑我们的矩阵σ，并将所有相应的特征向量收集到矩阵 V 中(其中，作为特征向量的列数将等于σ的行数)，我们将获得如下结果:</p><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/2e55b6caaca944bc721e086198fbcaa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*jmz3HkYiC1DGt6oLYQt9Dw.png"/></div></figure><p id="d0d0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中 L 是存储所有特征值(与特征向量一样多)的向量。如果我们考虑两个特征(x 和 y)的例子，我们将获得以下结果:</p><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mj"><img src="../Images/4b420b34f2848e88da0eb7af06e0d8e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jhaTRyyEl-EUVLLupwPGsQ.png"/></div></div></figure><p id="a966" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，如果我们按照特征值降序排列我们的特征向量，我们将得到第一特征向量占数据中最大的分布，第二特征向量占第二大的分布，等等(在描述新空间的所有这些新方向是独立的，因此彼此正交的条件下)。</p><p id="0905" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，如果我们想降低数据集的维度，我们应该怎么做呢？假设我们从 5 个特性开始，我们想要处理 2 个特性。因此，程序如下:</p><ul class=""><li id="efcf" class="mk ml je kd b ke kf ki kj km mm kq mn ku mo ky mp mq mr ms bi translated">计算σ矩阵我们的数据，这将是 5x5</li><li id="e2af" class="mk ml je kd b ke mt ki mu km mv kq mw ku mx ky mp mq mr ms bi translated">计算特征向量矩阵和相应的特征值</li><li id="ecf7" class="mk ml je kd b ke mt ki mu km mv kq mw ku mx ky mp mq mr ms bi translated">按降序排列我们的特征向量</li><li id="a55e" class="mk ml je kd b ke mt ki mu km mv kq mw ku mx ky mp mq mr ms bi translated">构建所谓的投影矩阵 W，其中将存储我们想要保留的 k 个特征向量(在本例中，2 是我们想要处理的特征的数量)。因此，在我们的例子中，我们的 W 将是一个 5x2 矩阵(通常，它是一个<em class="kz"> dxk </em>矩阵，其中<em class="kz">d</em>=原始特征的数量，<em class="kz">k</em>=期望特征的数量)。</li><li id="8172" class="mk ml je kd b ke mt ki mu km mv kq mw ku mx ky mp mq mr ms bi translated">通过投影矩阵 W 变换原始数据，原始数据可以表示为矩阵 X(其中,<em class="kz">n</em>=观察值的数量，而<em class="kz">d</em>=特征的数量),从而获得新的数据集或矩阵 Y，其将为<em class="kz"> nxk </em>。</li></ul><p id="c95c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个新的，转换后的空间 Y 的两列，是我们用来代替原始变量的主要成分。如上所述，它们被构造成存储尽可能多的信息。</p><p id="d4d9" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">PCA 广泛用于机器学习任务:事实上，为了使我们的算法有效，训练过程必须尽可能快，但这并不意味着我们可以在没有特定标准的情况下降低维度，从而有丢失相关信息的风险。</p></div></div>    
</body>
</html>