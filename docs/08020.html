<html>
<head>
<title>Machine Learning 101: Predicting Drug Use Using Logistic Regression In R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习 101:使用逻辑回归预测 R</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-101-predicting-drug-use-using-logistic-regression-in-r-769be90eb03d?source=collection_archive---------30-----------------------#2019-11-04">https://towardsdatascience.com/machine-learning-101-predicting-drug-use-using-logistic-regression-in-r-769be90eb03d?source=collection_archive---------30-----------------------#2019-11-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0347" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习:监督学习</h2><div class=""/><div class=""><h2 id="0eec" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">基础、链接功能和图</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ea4eae1c00c0564535286f3efcc43284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6RpO7MXOjoE-_v1AGN7BAA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><a class="ae lh" href="https://www.flickr.com/photos/mikemacmarketing/" rel="noopener ugc nofollow" target="_blank">Mike MacKenzie</a> on Flickr</figcaption></figure></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><p id="cf1b" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lr jd">执行摘要</strong></p><ul class=""><li id="cbb7" class="ml mm it lr b ls lt lv lw ly mn mc mo mg mp mk mq mr ms mt bi translated">广义线性模型(GLM)</li><li id="cfed" class="ml mm it lr b ls mu lv mv ly mw mc mx mg my mk mq mr ms mt bi translated">三种类型的链接函数:Logit、Probit 和互补双对数(cloglog)</li><li id="658d" class="ml mm it lr b ls mu lv mv ly mw mc mx mg my mk mq mr ms mt bi translated">建立逻辑回归来预测药物使用并比较这三种类型的 GLM</li></ul></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><p id="e087" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">在机器学习 101 课程中，统计学教授介绍了线性回归后的 GLM，作为成为数据科学家的下一个垫脚石。GLM 有几种形式，最著名的是 logit、probit 和 cloglog。</p><p id="fd89" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">这些 glm 非常适合分类问题:生存还是死亡，投票还是不投票，点击还是不点击。</p><p id="f80d" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><em class="mz">完整的项目描述和完整的 R 代码，请查看我的</em> <a class="ae lh" href="https://github.com/LeihuaYe/Logistic-Regression" rel="noopener ugc nofollow" target="_blank"> <em class="mz"> Github </em> </a> <em class="mz">。</em></p></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><p id="145d" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lr jd">基础知识</strong></p><p id="449b" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">通常，二进制数据的 GLM 可以用以下形式表示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi na"><img src="../Images/fac5c917fa6b2e71532ef51b22a332f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*f8NvAokBm1_4ZW4R0-LLKg.png"/></div></figure><p id="2ce5" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">其中 g 表示概率 p 的预测值(在右边)的线性关系，g 是将 p ∈[0，1]映射到ℝ.的函数</p><p id="af72" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">有三种方法可以连接左侧和右侧的组件。</p><p id="0e0b" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">Logit:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b92caa006e558707e5c63079b2664521.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*PbGuwP7-Vl8UWLJq-dfg9A.png"/></div></figure><p id="16a3" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">在文字中，p 的对数形式。</p><p id="fd61" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">概率单位:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/42b2bd33a009bf9fef2d62da4b7c60e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*GjJM4YmHbws-YUGG2lCXQA.png"/></div></figure><p id="d6cd" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">换句话说，正态分布的累积密度函数的倒数。</p><p id="1443" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">Cloglog:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/37bece897ea1c34a9121533f372c96b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*x2TjRO1g5ligWoUBDF8g6Q.png"/></div></figure><p id="8462" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">换句话说，不发生概率的对数形式的负值的对数形式。迷茫？至少，我是。这个函数的链接功能很简单。</p><p id="d04c" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">好吧，让我们继续，建立 GLM 模型来预测谁更容易吸毒，并学习阅读情节。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/dbe69e6a271d0d9495c01fe2648704b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e6ctNFQ9z-AI_WLO0laHKA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@nicolehoneywill_sincerelymedia?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Nicole Honeywill / Sincerely Media</a> on <a class="ae lh" href="https://unsplash.com/s/photos/surfing?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><ol class=""><li id="f749" class="ml mm it lr b ls lt lv lw ly mn mc mo mg mp mk nf mr ms mt bi translated"><strong class="lr jd">加载、清理和分割数据集</strong></li></ol><pre class="ks kt ku kv gt ng nh ni nj aw nk bi"><span id="7653" class="nl nm it nh b gy nn no l np nq">library(readr)<br/>drug_use &lt;- read_csv(‘drug.csv’,col_names=c(‘ID’,’Age’,’Gender’,’Education’,’Country’,’Ethnicity’,’Nscore’,’Escore’,’Oscore’,’Ascore’,’Cscore’,’Impulsive’,’SS’,’Alcohol’,’Amphet’,’Amyl’,’Benzos’,’Caff’,’Cannabis’,’Choc’,’Coke’,’Crack’,’Ecstasy’,’Heroin’,’Ketamine’,’Legalh’,’LSD’,’Meth’,’Mushrooms’,’Nicotine’,’Semer’,’VSA’))<br/>library(dplyr)<br/>drug_use &lt;- drug_use %&gt;% mutate_at(as.ordered, .vars=vars(Alcohol:VSA)) <br/>drug_use &lt;- drug_use %&gt;%<br/> mutate(Gender = factor(Gender, labels=c(“Male”, “Female”))) %&gt;%<br/> mutate(Ethnicity = factor(Ethnicity, labels=c(“Black”, “Asian”, “White”,<br/> “Mixed:White/Black”, “Other”,<br/> “Mixed:White/Asian”,<br/> “Mixed:Black/Asian”))) %&gt;%<br/> mutate(Country = factor(Country, labels=c(“Australia”, “Canada”, “New Zealand”, <br/> “Other”, “Ireland”, “UK”,”USA”)))</span><span id="9f7e" class="nl nm it nh b gy nr no l np nq">#create a new factor variable called recent_cannabis_use<br/>drug_use = drug_use %&gt;% <br/>mutate(recent_cannabis_use=as.factor(ifelse(Cannabis&gt;=”CL3",”Yes”,”No”)))</span><span id="7fd2" class="nl nm it nh b gy nr no l np nq">#create a new tibble that includes a subset of the original variable <br/>#data split into training and test sets<br/>drug_use_subset &lt;- drug_use %&gt;% select(Age:SS, recent_cannabis_use)<br/>set.seed(1)<br/>traint.indices = sample(1:nrow(drug_use_subset),1500)<br/>drug_use_train = drug_use_subset[traint.indices,]<br/>drug_use_test = drug_use_subset[-traint.indices,]<br/>dim(drug_use_train)<br/>dim(drug_use_test)</span><span id="95e6" class="nl nm it nh b gy nr no l np nq">[1] 1500   13<br/>[1] 385  13</span></pre><p id="c700" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">因此，训练集的尺寸为 1500*13，测试集的尺寸为 385*13。</p><p id="eaa3" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lr jd"> 2。拟合逻辑回归</strong></p><pre class="ks kt ku kv gt ng nh ni nj aw nk bi"><span id="a5a7" class="nl nm it nh b gy nn no l np nq">#use logit as the link function<br/>glm_fit = glm(recent_cannabis_use ~ .,data=drug_use_train,family=binomial(link= “logit”))<br/>summary(glm_fit)</span><span id="8bed" class="nl nm it nh b gy nr no l np nq">Call:<br/>glm(formula = recent_cannabis_use ~ ., family = binomial(link = "logit"), <br/>    data = drug_use_train)<br/><br/>Deviance Residuals: <br/>    Min       1Q   Median       3Q      Max  <br/>-3.0024  -0.5996   0.1512   0.5410   2.7525  <br/><br/>Coefficients:<br/>                           Estimate Std. Error z value Pr(&gt;|z|)    <br/>(Intercept)                1.33629    0.64895   2.059 0.039480 *  <br/>Age                       -0.77441    0.09123  -8.489  &lt; 2e-16 ***<br/>GenderFemale              -0.65308    0.15756  -4.145 3.40e-05 ***<br/>Education                 -0.41192    0.08006  -5.145 2.67e-07 ***<br/>CountryCanada             -0.67373    1.23497  -0.546 0.585377    <br/>CountryNew Zealand        -1.24256    0.31946  -3.890 0.000100 ***<br/>CountryOther               0.11062    0.49754   0.222 0.824056    <br/>CountryIreland            -0.50841    0.69084  -0.736 0.461773    <br/>CountryUK                 -0.88941    0.39042  -2.278 0.022720 *  <br/>CountryUSA                -1.97561    0.20101  -9.828  &lt; 2e-16 ***<br/>EthnicityAsian            -1.19642    0.96794  -1.236 0.216443    <br/>EthnicityWhite             0.65189    0.63569   1.025 0.305130    <br/>EthnicityMixed:White/Black 0.10814    1.07403   0.101 0.919799    <br/>EthnicityOther             0.66571    0.79791   0.834 0.404105    <br/>EthnicityMixed:White/Asian 0.48986    0.96724   0.506 0.612535    <br/>EthnicityMixed:Black/Asian13.07740  466.45641   0.028 0.977634    <br/>Nscore                    -0.08318    0.09163  -0.908 0.363956    <br/>Escore                    -0.11130    0.09621  -1.157 0.247349    <br/>Oscore                     0.64932    0.09259   7.013 2.33e-12 ***<br/>Ascore                     0.09697    0.08235   1.178 0.238990    <br/>Cscore                    -0.30243    0.09179  -3.295 0.000984 ***<br/>Impulsive                 -0.14213    0.10381  -1.369 0.170958    <br/>SS                         0.70960    0.11793   6.017 1.78e-09 ***<br/>---<br/>Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></pre><p id="acc1" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">的解释很简单，也是重要的变量</p><p id="375a" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">包括:年龄、女性、教育程度、国家变量(ZJ、英国、美国)、oscore、分数和 SS。</p><p id="3a92" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lr jd"> 3。Probit 和 cloglog </strong></p><pre class="ks kt ku kv gt ng nh ni nj aw nk bi"><span id="2437" class="nl nm it nh b gy nn no l np nq">#probit link function<br/>glm_fit_probit = glm(recent_cannabis_use ~ .,data=drug_use_train,family = binomial(link = “probit”))<br/>prob_training_probit = predict(glm_fit_probit, type=”response”)</span><span id="2c95" class="nl nm it nh b gy nr no l np nq">#c-log-log” link<br/>glm_fit_clog = glm(recent_cannabis_use ~ .,data=drug_use_train,family = binomial(link = “cloglog”))<br/>prob_training_clog = predict(glm_fit_clog, type=”response”)</span></pre><p id="b0ce" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lr jd"> 4。比较这三个图</strong></p><pre class="ks kt ku kv gt ng nh ni nj aw nk bi"><span id="cb9a" class="nl nm it nh b gy nn no l np nq"># compare logit and probit<br/>plot(prob_training_logit,prob_training_probit,xlab = “Fitted Values of Logit Model”,ylab= “Fitted Values of Probit Model”, main= “Plot 1: Fitted Values for Logit and Probit Regressions”, pch=19, cex=0.2,abline(a=0,b=1,col=”red”))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/d33cc3e1f2c6e71f851a15f6bfa853b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*if802FgNHZybPm4vtbUo_w.png"/></div></div></figure><p id="b40f" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">众所周知，probit 和 logit 预测几乎相同的值，因为它们紧密地排列在 45 度线上。可能，唯一的区别在于 0.5 到 0.8 之间的中间范围，其中 probit 模型预测的值略低于 abline。</p><pre class="ks kt ku kv gt ng nh ni nj aw nk bi"><span id="0ce3" class="nl nm it nh b gy nn no l np nq"># compare logit and cloglog<br/>plot(prob_training_logit,prob_training_clog,xlab = “Fitted Values of Logit Model”,ylab= “Fitted Values of Cloglog Model”, main= “Plot 2: Fitted Values for Logit and Cloglog Regressions”, pch=19, cex=0.2,abline(a=0,b=1,col=”red”))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/2a547ed709545905f4e43242c3b4c3d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VFkN9LDf2KWDtwZiU97tUA.png"/></div></div></figure><p id="f03b" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">这是一个有趣的情节。C-loglog 在早期生成的预测值较高，随后是比中间范围的 logit 更低的离散预测值。</p></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><p id="a375" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">在机器学习中，逻辑回归是数据科学家可以应用的 101 技术。</p></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><h1 id="21be" class="nu nm it bd nv nw nx ny nz oa ob oc od ki oe kj of kl og km oh ko oi kp oj ok bi translated">喜欢读这本书吗？</h1><blockquote class="ol om on"><p id="99b8" class="lp lq mz lr b ls lt kd lu lv lw kg lx oo lz ma mb op md me mf oq mh mi mj mk im bi translated">请在<a class="ae lh" href="https://www.linkedin.com/in/leihuaye/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://www.youtube.com/channel/UCBBu2nqs6iZPyNSgMjXUGPg" rel="noopener ugc nofollow" target="_blank"> Youtube </a>上找到我。</p><p id="96ca" class="lp lq mz lr b ls lt kd lu lv lw kg lx oo lz ma mb op md me mf oq mh mi mj mk im bi translated">还有，看看我其他关于人工智能和机器学习的帖子。</p></blockquote></div></div>    
</body>
</html>