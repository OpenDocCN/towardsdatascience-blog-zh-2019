<html>
<head>
<title>Review: ShuffleNet V1 — Light Weight Model (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:shuffle net V1-轻量级模型(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f?source=collection_archive---------7-----------------------#2019-05-06">https://towardsdatascience.com/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f?source=collection_archive---------7-----------------------#2019-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f5f0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过频道洗牌，性能优于<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/e1b2548bd214716ab0b6697ff966701a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iCVLACN7HUbBh827"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">ShuffleNet, Light Weight Models for Limited Computational Budget Devices such as Drones </strong>(<a class="ae kf" href="https://unsplash.com/photos/DiTiYQx0mh4" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/DiTiYQx0mh4</a>)</figcaption></figure><p id="601c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di">在</span>这个故事里，对<strong class="kz ir">旷视科技公司【Face ++】</strong>出品的<strong class="kz ir"> ShuffleNet V1 </strong>进行了简要回顾。ShuffleNet 在数十或数百 MFLOPs 的非常有限的计算预算中追求<strong class="kz ir">最佳精度，专注于常见的移动平台，如<strong class="kz ir">无人机</strong>、<strong class="kz ir">机器人</strong>和<strong class="kz ir">智能手机</strong>。通过混洗频道，ShuffleNet 胜过了<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>。在 ARM 设备中，ShuffleNet 比<a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> AlexNet </a>实现了 13 倍的实际加速，同时保持了相当的精度。这是一篇<strong class="kz ir"> 2018 CVPR </strong>的论文，引用超过<strong class="kz ir"> 300 次</strong>。(<a class="mc md ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----5b253dfe982f--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</strong></p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="df2a" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">概述</h1><ol class=""><li id="c472" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls nk nl nm nn bi translated"><strong class="kz ir">群组卷积的信道混洗</strong></li><li id="e403" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">洗牌机单元</strong></li><li id="89d5" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir"> ShuffleNet 架构</strong></li><li id="0fee" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">消融研究</strong></li><li id="9295" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="c70d" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 1。群组卷积的信道混洗</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nt"><img src="../Images/c390f44b88d30bedf51e5702cc6392b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pMZGe-v5VtQuTWNMpXCnqw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">(a) Two Stacked Group Convolutions (GConv1 &amp; GConv2), (b) Shuffle the channels before convolution, (c) Equivalent implementation of (b)</strong></figcaption></figure><ul class=""><li id="02fe" class="nd ne iq kz b la lb ld le lg nu lk nv lo nw ls nx nl nm nn bi translated"><strong class="kz ir">组卷积用于</strong><a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"><strong class="kz ir">Alex net</strong></a><strong class="kz ir">和</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"><strong class="kz ir">ResNeXt</strong></a><strong class="kz ir">。</strong></li><li id="c521" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated"><strong class="kz ir"> (a) </strong>:没有通道混洗，<strong class="kz ir">每个输出通道只与组内的输入通道相关。</strong>这个特性<strong class="kz ir">阻断了通道组之间的信息流，削弱了代表性</strong>。</li><li id="67b9" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated"><strong class="kz ir"> (b) </strong>:如果我们<strong class="kz ir">允许组卷积获得不同组的输入数据，那么输入和输出通道将完全相关。</strong></li><li id="5de9" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated"><strong class="kz ir">【c】</strong>:( b)中的操作可以通过信道混洗操作来高效且优雅地<strong class="kz ir">实现。</strong>假设一个卷积层有<em class="ny"> g </em>个组，其输出有<em class="ny"> g </em> × <em class="ny"> n </em>个信道；我们先用<strong class="kz ir">将</strong>输出的通道尺寸整形为(<em class="ny"> g </em>，<em class="ny"> n </em>)，<strong class="kz ir">转置</strong>，然后<strong class="kz ir">将其展平</strong>作为下一层的输入。</li><li id="e0d2" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">信道混洗也是<strong class="kz ir">可微分的</strong>，这意味着它可以嵌入到网络结构中用于<strong class="kz ir">端到端训练</strong>。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="bf05" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">2.<strong class="ak">洗牌机单元</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nz"><img src="../Images/a8b5ddc85f47f178d0f217cf3b43b376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kCE0AUZ_KwhoJBFhIwo5cw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">(a) bottleneck unit with depthwise convolution (DWConv), (b) ShuffleNet unit with pointwise group convolution (GConv) and channel shuffle, (c) ShuffleNet unit with stride = 2.</strong></figcaption></figure><ul class=""><li id="dcbd" class="nd ne iq kz b la lb ld le lg nu lk nv lo nw ls nx nl nm nn bi translated"><strong class="kz ir"> (a)瓶颈单元</strong>:这是一个标准的剩余瓶颈单元，但是使用了深度方向卷积。(<strong class="kz ir">深度方向卷积用于</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"><strong class="kz ir">MobileNetV1</strong></a><strong class="kz ir">)。</strong>)使用 1×1 然后 3×3 DW 然后 1×1 卷积，它也可以被视为<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c"> MobileNetV2 </a>中使用的深度方向可分离卷积的瓶颈类型。</li><li id="78d6" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated"><strong class="kz ir"> (b) ShuffleNet 单元</strong>:第一个和第二个 1×1 卷积被组卷积替换。在第一个 1×1 卷积之后，应用通道混洗。</li><li id="4374" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated"><strong class="kz ir">(c)Stride = 2 的 ShuffleNet 单元:</strong>应用 Stride 时，在快捷路径上添加 3×3 平均池。此外，逐元素相加被信道级联取代，这使得很容易以很少的额外计算成本来扩大信道维度。</li><li id="2d74" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">给定输入<em class="ny">c</em>×<em class="ny">h</em>×<em class="ny">w</em>和瓶颈通道<em class="ny"> m </em>，<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>单元要求<em class="ny">HW</em>(2<em class="ny">cm</em>+9<em class="ny">m</em>)FLOPs 和<a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>要求<em class="ny"> hw </em> (2 <em class="ny"> cm </em></li><li id="ac18" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">换句话说，给定计算预算，<strong class="kz ir"> ShuffleNet 可以使用更宽的特征地图</strong>。我们发现这对小型网络来说<strong class="kz ir">至关重要，因为小型网络通常没有足够数量的通道来处理信息。</strong></li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="5a6a" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 3。ShuffleNet 架构</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oa"><img src="../Images/ef60b1271acfe7e0b8bfc061b3b65a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hwb8DNYBWz_b5C-GfdK-8A.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">ShuffleNet Architecture</strong></figcaption></figure><ul class=""><li id="c668" class="nd ne iq kz b la lb ld le lg nu lk nv lo nw ls nx nl nm nn bi translated">所提出的网络主要由分成三级的一堆洗牌网络单元组成。</li><li id="d165" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">对于每个洗牌机单元，瓶颈通道的数量被设置为输出通道的 1/4。</li><li id="4e68" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">一个<strong class="kz ir">比例因子<em class="ny"> s </em> </strong>应用于通道的数量。上表中的网络表示为“ShuffleNet 1×”,那么“ShuffleNet <em class="ny"> s </em> ×”意味着将 ShuffleNet 1×中的滤波器数量缩放了<em class="ny"> s </em>倍，因此总体复杂度将大致为 ShuffleNet 1×的<em class="ny"> s </em>倍。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="ddb6" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 4。消融研究</strong></h1><ul class=""><li id="e67d" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls nx nl nm nn bi translated">使用 ImageNet 2012 分类验证集。</li></ul><h2 id="8d3a" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">4.1.不同数量的组卷积<em class="on"> g </em></h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oo"><img src="../Images/dd5c337b6ea13283eb66fa6efa6fc0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BsyND5-KsYtonLqtz7eU1A.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Different number of group convolutions g</strong></figcaption></figure><ul class=""><li id="d48c" class="nd ne iq kz b la lb ld le lg nu lk nv lo nw ls nx nl nm nn bi translated">其中<em class="ny"> g </em> = 1，即没有逐点群卷积。</li><li id="dd6d" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">具有组卷积(<em class="ny"> g </em> &gt; 1)的模型始终比没有点态组卷积(<em class="ny"> g </em> = 1)的模型表现得更好。</li><li id="8439" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">较小的模型往往从群体中获益更多。</li><li id="dcda" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">例如，对于 ShuffleNet 1 倍的最佳条目(<em class="ny"> g </em> = 8)比对应条目好 1.2%，而对于 ShuffleNet 0.5 倍和 0.25 倍，差距分别变为 3.5%和 4.4%。</li><li id="3ed3" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">对于某些型号(如 ShuffleNet 0.5×)当<strong class="kz ir">组数变得相对较大(如<em class="ny"> g </em> = 8)时，分类得分饱和甚至下降。</strong>随着组号的增加(因此特征映射更宽)，<strong class="kz ir">每个卷积滤波器的输入通道变得更少，这可能损害表示能力。</strong></li></ul><h2 id="d3f1" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">4.2.洗牌还是不洗牌</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi op"><img src="../Images/1223c6753f63412db60f195d7f9e96eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cf5LuwbJn5uHBcjX6AD4tQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Shuffle vs No Shuffle</strong></figcaption></figure><ul class=""><li id="51be" class="nd ne iq kz b la lb ld le lg nu lk nv lo nw ls nx nl nm nn bi translated">频道洗牌持续提升不同设置的分类分数，这显示了跨组信息交换的重要性。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="c086" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 5。与最先进方法的比较</strong></h1><h2 id="796a" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">5.1.与其他结构单元的比较</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oq"><img src="../Images/6ca3a8a2c5435fa827161e7cd9db83cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yamh1yguAyFEQ9krh6CfUQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Comparison with Other Structure Units</strong></figcaption></figure><ul class=""><li id="bfbf" class="nd ne iq kz b la lb ld le lg nu lk nv lo nw ls nx nl nm nn bi translated"><a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568">exception</a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>没有完全探索低复杂度条件。</li><li id="1f08" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">为了公平比较，在上表中，具有其他结构的阶段 2-4 中的洗牌网络单元被其他网络的单元替换，然后调整通道的数量以确保复杂度保持不变。</li><li id="6b95" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">在不同的复杂性下，ShuffleNet 模型的表现远远超过大多数其他模型。</li><li id="bf1d" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">例如，在 38 MFLOPs 的复杂度下，<a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">类 VGG </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568">exception</a>类 ShuffleNet 模型的阶段 4(见表 1)的输出通道分别为 50、192、192、288、576，这与精度的提高是一致的。</li><li id="93a3" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">由于需要调整的超参数太多，GoogLeNet 或 Inception 系列不包括在内。</li><li id="0e9d" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">另一个名为 PVANET 的轻量级网络结构的分类误差为 29.7%，计算复杂度为 557 MFLOPs，而我们的 shuffle net 2×model(<em class="ny">g</em>= 3)的分类误差为 26.3%，计算复杂度为 524 MFLOPs。</li></ul><h2 id="52d1" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">5.2.与<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>的比较</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi or"><img src="../Images/88ecbcc9443a04f2b0a66c0599a84acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ueuUcHdkHFBzKt7VcUXfA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Comparison with </strong><a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"><strong class="bd kw">MobileNetV1</strong></a></figcaption></figure><ul class=""><li id="d309" class="nd ne iq kz b la lb ld le lg nu lk nv lo nw ls nx nl nm nn bi translated">ShuffleNet 模型在所有复杂性方面都优于<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>。</li><li id="7792" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">虽然 ShuffleNet 网络是专门为小模型设计的(&lt; 150 MFLOPs), it is still better than <a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>计算成本较高，例如比<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>高 3.1%的精度，成本为 500 MFLOPs。</li><li id="7c0c" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">简单的架构设计也使得为 ShuffeNets 配备最新技术变得容易，例如挤压和激励(SE)模块。(希望以后可以复习 SENet。)</li><li id="8a99" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">具有 SE 模块的 ShuffleNet 将 shuffle net 的 top-1 误差提高了 2 倍至 24.7%，但通常比移动设备上的“原始”shuffle net 慢 25%至 40%，这意味着实际加速评估对低成本架构设计至关重要。</li></ul><h2 id="084c" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">5.3.与其他模型的比较</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi os"><img src="../Images/4a518397696571e6b2dbdce66344a77c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pinkZs1_8cxynnTlQyuNIw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Comparison with Other Models</strong></figcaption></figure><ul class=""><li id="8fa8" class="nd ne iq kz b la lb ld le lg nu lk nv lo nw ls nx nl nm nn bi translated">在准确率差不多的情况下，ShuffleNet 比<a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>、<a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener"> GoogLeNet </a>、<a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> AlexNet </a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-squeezenet-image-classification-e7414825581a"> SqueezeNet </a>效率要高得多。</li></ul><h2 id="7fe6" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">5.4.概括能力</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ot"><img src="../Images/511de0588e142438652d3bf002458896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IB3JnR_biyj3wwEkivSERw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Object detection results on MS COCO</strong></figcaption></figure><ul class=""><li id="17a7" class="nd ne iq kz b la lb ld le lg nu lk nv lo nw ls nx nl nm nn bi translated">这里，COCO minival 女士的图像用于测试。</li><li id="baba" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">使用更快的 R-CNN </a>作为检测框架。</li><li id="345a" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">将 ShuffleNet 2×与复杂度相当(524 对 569 MFLOPs)的<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>进行比较，我们的 ShuffleNet 2×在两种分辨率上都远远超过<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>。</li><li id="df89" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">ShuffleNet 1×在 600×分辨率上也获得了与 MobileNet 相当的结果，但复杂度降低了约 4 倍。</li><li id="d172" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">作者推测这种显著的收益部分是由于 ShuffleNet 的简单的架构设计。</li></ul><h2 id="177c" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">5.5.实际加速评估</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ou"><img src="../Images/073b5337e7439de0639618b501bacbd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XxOuNBIoO-9RYeGxEXvNMw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Actual Speedup Evaluation on ARM device</strong></figcaption></figure><ul class=""><li id="b0e0" class="nd ne iq kz b la lb ld le lg nu lk nv lo nw ls nx nl nm nn bi translated">经验上<em class="ny"> g </em> = 3 通常在准确性和实际推断时间之间有一个适当的权衡。</li><li id="c5e1" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">由于内存访问和其他开销，我们发现在我们的实现中，每 4 倍的理论复杂度降低通常会导致 2.6 倍的实际加速。</li><li id="955d" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nx nl nm nn bi translated">与<a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> AlexNet </a>相比，ShuffleNet 0.5×模型在分类精度相当的情况下仍然实现了~13×的实际加速比(理论加速比 18×)。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="b3ff" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">希望我能在未来的日子里回顾 V2 沙狐球网。:)</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="83a8" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">参考</h2><p id="6173" class="pw-post-body-paragraph kx ky iq kz b la nf jr lc ld ng ju lf lg ov li lj lk ow lm ln lo ox lq lr ls ij bi translated">【2018 CVPR】【shuffle net V1】<br/><a class="ae kf" href="https://arxiv.org/abs/1707.01083" rel="noopener ugc nofollow" target="_blank">shuffle net:一种针对移动设备的极其高效的卷积神经网络</a></p><h2 id="3eee" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kx ky iq kz b la nf jr lc ld ng ju lf lg ov li lj lk ow lm ln lo ox lq lr ls ij bi translated">)(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(?)(我)(们)(都)(不)(在)(这)(些)(情)(况)(上)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(况)(。 [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]</p><p id="8b77" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">物体检测<br/></strong><a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae kf" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4">G-RMI</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a></p><p id="6582" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">语义切分<br/></strong><a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae kf" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生物医学图像分割<br/></strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1">M FCN<br/></a></p><p id="3134" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">实例分割<br/> </strong> [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></p><p id="58de" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"/><br/><a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">【DeepPose】</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">【汤普森 NIPS'14】</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">【汤普森 CVPR'15】</a></p></div></div>    
</body>
</html>