<html>
<head>
<title>Neural Networks Intuitions: 6. EAST: An Efficient and Accurate Scene Text Detector — Paper Explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络直觉:6。EAST:一种高效准确的场景文本检测器——论文解读</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-intuitions-6-east-5892f85a097?source=collection_archive---------8-----------------------#2019-12-24">https://towardsdatascience.com/neural-networks-intuitions-6-east-5892f85a097?source=collection_archive---------8-----------------------#2019-12-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="606e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">嘿伙计们！再次回来真好:-)</h2></div><p id="420f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">距离我发表上一篇文章已经有一段时间了。在我的系列文章<em class="le">“神经网络直觉”的第六部分中，</em>我将谈论一种使用最广泛的场景文本检测器——<a class="ae lf" href="https://arxiv.org/abs/1704.03155" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"><em class="le">EAST(高效准确的场景文本检测)</em> </strong> </a>，顾名思义，与文本检测器相比，它不仅准确，而且更加高效<em class="le"/>。</p><p id="7f9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们从总体上看一下<em class="le">场景文本检测</em>的问题，然后深入研究<em class="le">东</em> :-)</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="5803" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">场景文本检测:</h1><p id="797d" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated"><strong class="kk iu">问题:</strong>上面已经提到的问题是检测自然场景图像中的文本。场景文本检测是对象检测的一个特例，这里的对象归结为一个单独的实体— <strong class="kk iu">“文本”。</strong></p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mk"><img src="../Images/08b503b361b37a287950ba4c2f767d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FtOfzBnItMNnIpmjqaywEA.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Scene Text Detection</figcaption></figure><p id="2974" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是我们在什么水平上检测文本呢？</p><blockquote class="na"><p id="5a11" class="nb nc it bd nd ne nf ng nh ni nj ld dk translated"><em class="nk">我们可以在</em>字符级<em class="nk">或</em>单词级<em class="nk">检测文本。这完全取决于如何对文本检测数据集进行注释，以及我们希望网络如何学习。通常，自然场景图像在单词级别被标记，从而使网络学会检测单词(以及单词之间的空格，以便区分任何两个 <strong class="ak">单词实例</strong>)。</em></p></blockquote><p id="0e8d" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated"><strong class="kk iu">解决方案:</strong>既然这里的问题是物体检测，既然我们已经知道了物体检测的基础<em class="le"> ( </em> <a class="ae lf" rel="noopener" target="_blank" href="/neural-networks-intuitions-5-anchors-and-object-detection-fc9b12120830">神经网络直觉:5 .锚和对象检测</a> <em class="le"> ) </em>，我们可以简单地使用现有的对象检测器之一——比如 SSD、Faster-RCNN 或 RetinaNet。他们应该能帮我们完成这项工作。</p><blockquote class="na"><p id="dd37" class="nb nc it bd nd ne nf ng nh ni nj ld dk translated">但是我们真的需要每个特征地图单元有多个定位框吗？或者我们是否需要使用锚的概念来完成更具体更简单的任务，比如文本检测？</p></blockquote><p id="0868" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">我之所以说文本检测(或者更具体地说单词检测)是一项更简单的任务，是因为:</p><p id="edf1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">a.<strong class="kk iu"> <em class="le">单词</em> </strong>的模式/特征并没有那么复杂——尤其是在语言固定的情况下(比如英语)，我们基本上有 26*2(小写和大写字母)+ 10(数字)。</p><blockquote class="na"><p id="e296" class="nb nc it bd nd ne nf ng nh ni nj ld dk translated">但是我们不是在字符级别检测，对吗？那么，如果有大量的单词组合，这个任务是如何简单的呢？</p></blockquote><p id="6dfc" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">让我们看下一点来看原因！</p><p id="4b1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">b.如果您以前有过文本检测的工作经验，您可以看到文本检测器可以学习预测没有经过训练的语言中的单词(即使并不完美)。例如<strong class="kk iu"> <em class="le">英文</em> </strong>单词检测器检测<strong class="kk iu"> <em class="le">日文/泰米尔文</em> </strong>单词。</p><blockquote class="na"><p id="fd5a" class="nb nc it bd nd ne nf ng nh ni nj ld dk translated">这很明显地表明，所学的模式并不完全是“语言单词”。那么网络到底在学什么呢？—它可以是学习一起出现在组中的形状，由一定数量的空白分隔，例如，一个连接的组件。</p></blockquote><p id="ea7c" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">至少这是我对单词检测器可以学习什么的直觉:-)</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="87d7" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">我们为上述问题制定一个解决方案怎么样？</h1><h2 id="07e0" class="nq lo it bd lp nr ns dn lt nt nu dp lx kr nv nw lz kv nx ny mb kz nz oa md ob bi translated">伪代码:</h2><ol class=""><li id="2d2e" class="oc od it kk b kl mf ko mg kr oe kv of kz og ld oh oi oj ok bi translated">将输入场景文本图像传递给 ConvNet(特征提取器)。</li><li id="e62f" class="oc od it kk b kl ol ko om kr on kv oo kz op ld oh oi oj ok bi translated">在特征融合阶段合并多尺度特征。</li><li id="ecf6" class="oc od it kk b kl ol ko om kr on kv oo kz op ld oh oi oj ok bi translated">在特征卷上运行<em class="le"> 1x1 conv 滤波器(class head) </em>以获得范围为 0–1 的<em class="le">激活图/热图，其中<em class="le">1 表示图像中存在文本</em>，而<em class="le"> 0 表示背景</em>。</em></li><li id="09c6" class="oc od it kk b kl ol ko om kr on kv oo kz op ld oh oi oj ok bi translated">对激活图进行阈值处理，并使用 cv2.findContours()之类的逻辑来查找文本区域，并最终从中找到文本单词。</li></ol><blockquote class="na"><p id="a641" class="nb nc it bd nd ne oq or os ot ou ld dk translated">如果我们注意第 3 步，我们可以注意到锚的<em class="nk">用法，</em>与大小为 1x1 的<em class="nk">过滤器相关联，并且每个特征地图单元</em> 只有<strong class="ak"> <em class="nk">一个锚框。这一点很关键，因为它有助于我们与传统的单次目标探测器进行比较。</em></strong></p></blockquote><p id="2d06" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">这几乎就是 EAST 所做的——除了没有使用一些过度的逻辑来查找文本区域，它还有另一个头— <em class="le"> box head </em>,它将输出到最近的[minx，miny，maxx，maxy]边界的 4 个距离值(特征地图中的每个像素)。</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="a35a" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">EAST——一种高效准确的场景文本检测器；</h1><p id="27b0" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated"><strong class="kk iu"> a .架构:</strong>每个单次拍摄物体探测器都包含 3 个主要阶段:</p><ol class=""><li id="1abf" class="oc od it kk b kl km ko kp kr ov kv ow kz ox ld oh oi oj ok bi translated">特征提取阶段。</li><li id="27e1" class="oc od it kk b kl ol ko om kr on kv oo kz op ld oh oi oj ok bi translated">特征融合阶段。</li><li id="eef7" class="oc od it kk b kl ol ko om kr on kv oo kz op ld oh oi oj ok bi translated">预测网络。</li></ol><p id="3673" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单触发探测器的所有变体在上述三个阶段中的一个阶段有所不同。东方也遵循上述相同的范式。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/9ebc4704e66b459b101d8a93b7672129.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*PdqPWjR7EClt4yEps-xUMA.png"/></div></figure><p id="a320" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> b .投入产出:</strong></p><ol class=""><li id="3745" class="oc od it kk b kl km ko kp kr ov kv ow kz ox ld oh oi oj ok bi translated">该网络接收输入图像，并通过某组 conv 层(特征提取器干)来获得四级特征图——f1、f2、f3、f4。</li><li id="8d5a" class="oc od it kk b kl ol ko om kr on kv oo kz op ld oh oi oj ok bi translated">然后，特性图被解池(x2)、连接(沿通道维度)，然后通过 1x1，再通过 3x3 转换器。合并来自不同空间分辨率的特征的原因是为了预测更小的单词区域。</li></ol><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/6f5a9ca4eba726958491e1dc45930cd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*CKaoTOparkduFsOWH2FHFA.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Feature merging</figcaption></figure><p id="4974" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.然后，最终的特征体积将用于进行分数和盒子预测-深度=1 的 1x1 过滤器用于生成分数图，深度=5 的另一个 1x1 过滤器用于生成 RBOX(旋转的盒子)-四个盒子偏移和旋转角度，深度=8 的另一个 1x1 过滤器用于生成四边形(具有 8 个偏移的四边形)。</p><p id="4e6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> c .损失函数:</strong></p><p id="54ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在进入损失函数之前，让我们先来解释一下网络输出。</p><blockquote class="na"><p id="0d26" class="nb nc it bd nd ne nf ng nh ni nj ld dk translated">1.类头输出可以解释为类似于传统检测器的类输出，只是这里每个特征地图像元只有锚框，因此输出将为 HxWx1 形状，其中 1 表示每个像元的锚框数量。</p><p id="89a2" class="nb nc it bd nd ne nf ng nh ni nj ld dk translated">2.但是在盒子头的情况下，输出(形状 HxWx4)应该在“像素”级别解释，并且没有锚盒的概念。每个像素都有 4 个与之相关的数字—到最近的[minx，miny，maxx，maxy]框的距离。这里需要注意的重要一点是，最终的字级输出是从这个每像素级输出中导出的。</p></blockquote><p id="3b26" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated"><strong class="kk iu"> c1。分类损失</strong></p><p id="4c24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们都很清楚对象检测数据集中存在的类不平衡问题。背景类的样本数量通常非常多，现在我们<em class="le">将每个 1x1 框(基本上是每个像素)作为输出</em>，背景样本的数量变得巨大。</p><p id="3f9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决这个类不平衡的问题，EAST 使用了交叉熵的一个修改版本，叫做<strong class="kk iu"> <em class="le">平衡/加权交叉熵。</em>T9】</strong></p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/a0b4aca5647548ab94247d80b7dcd680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*6jiMU9TQ2N0uDIS_2ylYCA.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Balanced Cross Entropy (BCE)</figcaption></figure><blockquote class="na"><p id="8468" class="nb nc it bd nd ne oq or os ot ou ld dk translated">在 BCE 中，高代表类的分数通常与低代表类的损失项相乘(类似地，高代表类的损失项也是如此),以便控制高代表类和低代表类的贡献。<em class="nk">注:背景类:=高表示，前景类:=低表示。</em></p></blockquote><p id="788b" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">查看这个博客对 BCE <em class="le"> ( </em> <a class="ae lf" rel="noopener" target="_blank" href="/neural-networks-intuitions-1-balanced-cross-entropy-331995cd5033">神经网络直觉的详细解释:1。平衡交叉熵</a> <em class="le">)。</em></p><p id="619f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> c2。IOU 损耗:</strong>这里使用的 IOU 损耗不同于传统的包围盒损耗。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/8ef5fea01ea22c77f1c7eed30e36bd85.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*xqcnTp6vAj65W4r-XSeK5g.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">IOU Loss</figcaption></figure><p id="93fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个像素将有 4 个与之相关的数字——到最近的顶部、左侧、底部、右侧框边界的距离，根据该距离计算 IOU，然后使用<em class="le">负对数似然作为损失，当 IOU 小于 1 时进行惩罚。</em></p><blockquote class="na"><p id="b082" class="nb nc it bd nd ne nf ng nh ni nj ld dk translated">很明显，gt/pred 框的宽度和高度可以通过简单地将它们的<em class="nk"> x 和 y 偏移量</em>相加来计算，由此获得 gt 和 pred 框的面积。</p><p id="43bb" class="nb nc it bd nd ne nf ng nh ni nj ld dk translated">为了找到相交矩形的宽度和高度，</p></blockquote><figure class="pd pe pf pg ph mp gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/e9b045946ac50f8c4748689cbb7c81d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*7xGB0aIhg0gqmmsgYkb80Q.png"/></div></figure><p id="267a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们有了 gt 盒的面积，预测盒的面积和相交盒的面积，我们可以计算 IOU 了！</p><p id="f068" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们计算角度损失，</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/edd96a7ef164de76dbc27e0a8ec3190c.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*12R9Cw8mg6DptsMa6Frm7A.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">1 — cos(predicted angle — gt angle)</figcaption></figure><p id="d21a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总损失写为，</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/a4f1327e6f3113f867e69fdbb812d85c.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*1RseMMoruLrB4onpXVugwQ.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">A weighted sum of both losses.</figcaption></figure></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><p id="5183" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我接下来的文章中(编辑到同一篇文章)，为了完整起见，我将解释关于如何从网络输出中计算边界框的<em class="le"> EAST post processing </em>。</p><p id="92dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">论文链接:【https://arxiv.org/abs/1704.03155 T2】</p><p id="dae0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">干杯！</p></div></div>    
</body>
</html>