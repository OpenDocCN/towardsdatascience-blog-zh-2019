<html>
<head>
<title>Gradient Descent: Show Me the Math!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降:给我看看数学！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-show-me-the-math-7ba7d1caef09?source=collection_archive---------4-----------------------#2019-09-16">https://towardsdatascience.com/gradient-descent-show-me-the-math-7ba7d1caef09?source=collection_archive---------4-----------------------#2019-09-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a8cb1517975394328656663da9d2786b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*idZ65rn0PSIcX1v5"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@martinjernberg?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Martin Jernberg</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dd8e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">梯度下降是一种迭代学习算法，也是神经网络的主力。对于<a class="ae kc" href="https://github.com/pytorch/examples" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>或<a class="ae kc" href="https://github.com/keras-team/keras/tree/master/examples" rel="noopener ugc nofollow" target="_blank"> Keras </a>来说，有了许多可定制的例子，构建一个千篇一律的神经网络就成了一项微不足道的工作。然而，当事情出错时，掌握基础知识可以节省大量冗长的调试时间。</p><p id="b307" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们将深入挖掘<a class="ae kc" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>的数学公式，为它建立一个更好的直觉。有些例子很简单，可以手工解决。此外，写下东西有助于记忆，所以请随意拿一张纸和一个计算器！</p><h1 id="b556" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">误差度量</h1><p id="b916" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">神经网络(NN)做出的预测经常会错过目标，有时会相差很大。我们需要以这样一种方式量化误差，即符号总是正的，并且大的误差被放大。我们定义误差平方和(SSE)，用大写字母<strong class="kf ir"> <em class="me"> E </em> </strong>表示:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/9adcd7af59b0264e78b5d1003641f26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*Q5CEiW5G4B74Au_wSjEJ7g.png"/></div></figure><ul class=""><li id="78f2" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">y=基本事实</li><li id="8a4a" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">ŷ (y hat)=预测</li><li id="97fd" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">j=网络的输出单位</li><li id="2b4a" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated"><em class="me"> μ ( </em>希腊字母 mu <em class="me"> )= </em>所有数据点</li></ul><p id="be38" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 j 上的第一个和中，找出每个真值和它对应的预测值之间的差，求这些差的平方，然后把它们加起来。</p><p id="856e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来我们有<em class="me"> μ </em>个数据点<em class="me">。</em>在第二次求和中，对于我们计算平方距离的内部和的每个数据点，我们对所有这些进行求和。</p><p id="dd9c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回想一下，神经网络的预测<em class="me"> ŷ </em>是由其权重决定的:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/f60430cd9d4ee853a60a193f15c2ab96.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*DdVuZx61Wrlj7XW4WwMvuw.png"/></div></figure><ul class=""><li id="c68d" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">w=重量</li><li id="d223" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">x=特征</li><li id="c390" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">i=特征数量</li></ul><p id="8a08" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，在 SSE 方程中插入<em class="me"> ŷ </em>的方程，我们可以看到误差取决于权重:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/0ebc7c22c430fdb3b381abf0e87e824b.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*ZytzOjww2smSGrDwCch9tQ.png"/></div></figure><p id="d849" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们的目标是确定最小化误差的权重。后者有时被称为由<em class="me"> J(w) </em>表示的成本函数，其思想是我们对错误的预测进行惩罚。</p><h1 id="c109" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">链式法则</h1><h2 id="8735" class="na lc iq bd ld nb nc dn lh nd ne dp ll ko nf ng lp ks nh ni lt kw nj nk lx nl bi translated">理论</h2><p id="8c38" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了找出函数的最小值出现在哪里，我们对它求导。鉴于我们有许多变量，我们将看到<a class="ae kc" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">链规则</a>如何帮助我们制定这个导数。</p><p id="65f2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在单个输出单元的情况下，其中 j=1，我们可以得到误差<em class="me"> E </em>相对于权重的导数为:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/18443586c819967984f516b090b569e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*jLmJCDstmrYZZhToyZ6Acw.png"/></div></figure><p id="0272" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设<em class="me"> ŷ </em>是单个激活函数<em class="me"> f </em>的结果，该函数接受输入<em class="me"> h </em>，使得 h 接受输入并将其乘以其权重:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/895e3e4de186f4f64d92626bb3df56da.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*itczMhM7pz_QoS33byPnCA.png"/></div></figure><p id="dc92" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在误差的导数中替换:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/fc436acf424bbed7366a6252c3ada8c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*JTCMBuqc8RiIM2MhRSVRaQ.png"/></div></figure><p id="db9c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意求和项的偏导数，如果我们写出随机权重 wi:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/3aca94ac84d85b077c7148bf21ab19ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xibfF8r8bgNdkIjTXcu-fQ.png"/></div></div></figure><p id="cc77" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和对 I 的偏导数只有 xi，其他都等于零。我们可以代入 E 的偏导数:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/5903e2ed1c2a57990df512f8d814dfb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*TnVT1q63D8u49g_oP2GorQ.png"/></div></figure><p id="232f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相对于某个权重的平方误差的梯度是误差乘以激活函数在 T2 h T3 的导数再乘以输入值 T4 Xi T5 的负值。我们将其乘以学习速率<em class="me"> η </em>(希腊字母 eta)以获得权重步长:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ad9a2c91dd48155f17c8d749bec4dd49.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*V5SOBdHhL1Bmx97M392xxw.png"/></div></figure><p id="d799" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了使我们的符号更容易理解，我们将误差项<em class="me"> δ </em> <strong class="kf ir"> </strong>定义为误差乘以 h 处激活函数的导数:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/da4da0ecebc9bab5ad1410171fb3fe61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*oJmRnY9Gz1Td0Mpw3Kwu1A.png"/></div></figure><p id="0f6f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将重量步骤简化为:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/89d965088c94c4b0306ddca83f4e9c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*xPFkJ9WOVQu_Upu4eebNRA.png"/></div></figure><p id="876b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用更一般的符号来写:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/04f8d69c88786aa4b03b38ed87cb5bea.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*LSGiwDl9nmBKG3IxH8eppA.png"/></div></figure><h2 id="b904" class="na lc iq bd ld nb nc dn lh nd ne dp ll ko nf ng lp ks nh ni lt kw nj nk lx nl bi translated">单输出单元示例</h2><p id="f9f0" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">现在我们有了如何进行的想法，我们可以将它应用到单个输出单元上。我们使用由 S 表示的<a class="ae kc" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid 函数</a>作为激活函数。回想一下，激活函数的一般符号是 f(h):</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/adfe5cc14bf4bdead319e36bb35ab13c.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*lNO0HR8oxYigoaKz79aA7g.png"/></div></figure><p id="218a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">sigmoid 函数的一个特殊之处在于其导数的最大值为 0.25。查看这篇<a class="ae kc" rel="noopener" target="_blank" href="/derivative-of-the-sigmoid-function-536880cf918e">帖子</a>以获得如何计算其导数的指导。</p><p id="5c82" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用 python 实现它:</p><figure class="mg mh mi mj gt jr"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="4367" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们定义初始特征、权重和学习速率:</p><figure class="mg mh mi mj gt jr"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="9fcd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据上面的公式，我们可以如下计算重量的变化:</p><figure class="mg mh mi mj gt jr"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="2abf" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">梯度下降通用实现</h1><p id="3fcb" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在大多数应用中，我们将有不止一个节点。因此，我们需要改变我们计算误差的方式，以避免对可能导致梯度发散的大误差求和。我们引入均方误差:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/bb4e2521dbb2d426762709877937fd99.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*sI_I60fPVbHCNZIyq6X1Ew.png"/></div></figure><ul class=""><li id="fc4b" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">m=数据中的记录数</li></ul><p id="7cd6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以将对数据集应用梯度下降的一般算法定义如下:</p><ol class=""><li id="f76b" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la nz mq mr ms bi translated">将重量步长设置为零:wi = 0</li><li id="e259" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la nz mq mr ms bi translated">对于培训数据中的每条记录:</li></ol><ul class=""><li id="a895" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">向前通过网络，并计算输出<em class="me"> ŷ </em></li><li id="31e4" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">计算输出单元的误差项<em class="me"> δ </em></li><li id="e6a5" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">更新权重步长:δwi =δwi+<em class="me">δXi</em></li></ul><p id="f78c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.更新权重:wi = wi+<em class="me">η</em>δwi/m</p><p id="156c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.重复 e 个时期</p><p id="ca7f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这一点上，我们如何初始化权重成为一个相关的问题。我们希望权重足够小，以使 sigmoid 的输入位于 0 附近的线性区域，并且不会在高端和低端受到挤压。</p><p id="d8fd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们遵循<a class="ae kc" href="http://cs231n.github.io/neural-networks-2/#init" rel="noopener ugc nofollow" target="_blank">斯坦福 CS231n </a>规定的方法，随机初始化它们，使它们都有不同的初始值并发散，打破对称性。因此，我们从以 0 为中心的正态分布初始化权重，并以 1/sqrt(n)对其进行缩放，其中 n 是输入单元的数量。</p><p id="b2da" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于具有 n _ recrods 和 n_features 的数据集，我们可以用 python 实现如下:</p><figure class="mg mh mi mj gt jr"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="46fe" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">反向传播</h1><p id="0b84" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">到目前为止，我们已经了解了如何计算输出节点中的误差并向前传播误差。我们也可以使用<a class="ae kc" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>来反向传播误差。</p><p id="ac85" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">层的输出由层之间的权重决定，因此由单元产生的误差由相应的权重来缩放。</p><p id="037d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑归因于输出单元<em class="me"> k </em>和隐藏层<em class="me"> h </em>的隐藏单元<em class="me"> j </em>的误差<em class="me"> δ </em>。我们可以这样写这个错误:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/3f3c5bbdb30af7bb1a7719f45c0f0389.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*lSxZtYia97Iwqlgd9dNkzA.png"/></div></figure><ul class=""><li id="b1e1" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">o =输出层</li><li id="e941" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">h=隐藏层</li><li id="0063" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">j =的隐藏单位</li><li id="ae6f" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">k=输出单位</li></ul><p id="8d5e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">梯度下降步骤与前面定义的步骤相同，但增加了下标以标识操作发生的位置:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ae9fc1299bd5c564f35126961bac23fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*e_UIiJ-LRxIq0rT9JBpe8Q.png"/></div></figure><ul class=""><li id="49da" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">wij=输入层和隐藏层之间的权重</li><li id="dfec" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">xi=输入单位值</li></ul><p id="a158" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，权重步长等于上面计算的步长乘以该层的输出误差乘以该层的输入值:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/62916ece3f724156e268d3e93e70b454.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*pV0-YkK0iVRiuxpJkONbKA.png"/></div></figure><ul class=""><li id="ef15" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated"><em class="me"> δ输出</em> =输出误差</li><li id="a0ca" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated"><em class="me"> V in </em> =层的输入，如输出单元的隐藏层激活</li></ul><h2 id="b17f" class="na lc iq bd ld nb nc dn lh nd ne dp ll ko nf ng lp ks nh ni lt kw nj nk lx nl bi translated">反向传播手算</h2><p id="b27c" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">如果你还没有纸和笔，现在是个好时机😉。</p><p id="ae3d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑下面从底部流向顶部的两层神经网络。我们有两个输入值，一个隐藏单元和一个输出单元。</p><p id="9864" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还在隐藏和输出单元应用了 sigmoid 激活函数。我们可以使用其他激活，<a class="ae kc" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity" rel="noopener ugc nofollow" target="_blank"> PyTorch 文档</a>列出了许多带有相应图形的公式。</p><p id="204c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，输入的相应权重用黑色写在线旁边。</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/6176e9f1cfd7d5c1f6dd8533e8b0497d.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*u3_GNEX5GGoVYKvr6A-gtA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo credit: <a class="ae kc" href="http://udacity.com" rel="noopener ugc nofollow" target="_blank">Udacity.com</a></figcaption></figure><p id="bec4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了这个例子的目的，让我们假设我们的目标输出是<em class="me"> y=1 </em>。</p><p id="7b61" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们从正向传递开始，从输入到隐藏单元:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/27eec0dcea7af0ff10774bb1b9175969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*JWV_W1MfWRFfbydTUoIeYg.png"/></div></figure><p id="e568" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">隐藏单元的输出随后被激活:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/a2866af547ba65f3adec4004f338fa39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*mNJ4IAKbmo69S7lqQUBrbQ.png"/></div></figure><p id="589a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于输出注释，我们将重量倍增与激活相结合:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/8fa9e8647b9221cd111345b5bad37d2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*plREcicJqNFebTn8LDAv3w.png"/></div></figure><p id="1d47" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回想一下，导数 sigmoid 激活函数如下:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/0c93b1bad85d8598e57f7386c0a2aef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*WSIUeiu96PECAdrwN3-EZA.png"/></div></div></figure><p id="c855" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，输出单元的误差项为:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/4d852dc3adf0518882c6b7c960550712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ieemoo-KeqCfZ5fPH8gOqg.png"/></div></div></figure><p id="9cac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">进一步通过网络，我们计算带有传播的隐藏单元的误差项。回想一下，每个隐藏层对误差的贡献与其权重成比例:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/55310a9cd49893cb180394245de28b18.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*enDgk_eZG-4OEYa58FYMng.png"/></div></figure><p id="fd7d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的等式中，我们通过将输出单元连接到隐藏单元的权重 W 来缩放来自输出单元的误差项。在我们的例子中，我们只有一个隐藏节点，可以将上面的表达式简化为:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oh"><img src="../Images/c12538774aff6c1b43e1c275f92b251f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDlkK_D1IhUjhHu34_0S7w.png"/></div></div></figure><p id="971f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">学习率乘以输出单元误差乘以隐藏单元激活值等于隐藏到输出的权重步长:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/9909b0fc39768727b565bb5861826137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*N9ukdsuYgvAN84sjclqwzA.png"/></div></figure><p id="c9aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，连接隐藏到输出层的当前权重 W=0.1 将被更新这个数量。</p><p id="8d5d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一步是输入隐藏权重。更新等于学习率乘以隐藏单元误差，再乘以输入值:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/2b3d49242e748778b8352e069ff3d489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-v142bApboXtpeGzjK9m0w.png"/></div></div></figure><p id="9694" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着权重的更新，我们可以重复上述步骤，直到我们的预测变得几乎等于地面真相。</p><p id="24bb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，如果我们有许多层，使用 sigmoid 激活函数会将输入附近层中的权重步长快速降低到微小值。这不是别人，正是<a class="ae kc" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失渐变问题</a>。</p><h2 id="070c" class="na lc iq bd ld nb nc dn lh nd ne dp ll ko nf ng lp ks nh ni lt kw nj nk lx nl bi translated">数字示例</h2><p id="e9d8" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们可以在 NumPy 中使用稍微复杂一点的网络做同样的事情:三个输入节点，两个隐藏节点，一个输出层。</p><figure class="mg mh mi mj gt jr"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="7f24" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="8138" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们已经看到学习的数学是如何为神经网络工作的。通过上面的练习，我们可以更好地理解错误、激活函数，也许还可以为神经网络构建一个 python 类。</p><p id="bdfd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有更多的资源可以探索，如斯坦福 CS231n。我也强烈推荐 Andrew Trask 的<a class="ae kc" href="https://www.manning.com/books/grokking-deep-learning" rel="noopener ugc nofollow" target="_blank">探索深度学习</a>，这是一个很好的资源，有可解释的例子。</p></div></div>    
</body>
</html>