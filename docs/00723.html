<html>
<head>
<title>Machine Learning and Particle Motion in Liquids: An Elegant Link</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和液体中的粒子运动:一个优雅的链接</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945?source=collection_archive---------13-----------------------#2019-02-03">https://towardsdatascience.com/machine-learning-and-particle-motion-in-liquids-an-elegant-link-4e7c1611c945?source=collection_archive---------13-----------------------#2019-02-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b2711673b63ec89356bb8ee63e700fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hk36T4JoPJuOqE9zebBLAA.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Image by <a class="ae jg" href="https://pixabay.com/fr/users/ronymichaud-647623/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=578897" rel="noopener ugc nofollow" target="_blank">rony michaud</a> from <a class="ae jg" href="https://pixabay.com/fr/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=578897" rel="noopener ugc nofollow" target="_blank">Pixabay</a>.</figcaption></figure><div class=""/><div class=""><h2 id="17fd" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">作为全局最小化算法的朗之万方程</h2></div><p id="dbee" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降算法</a>是机器学习中最流行的优化技术之一。它有三种风格<a class="ae jg" href="https://arxiv.org/pdf/1609.04747.pdf" rel="noopener ugc nofollow" target="_blank"/>:批量或“普通”梯度下降(GD)、随机梯度下降(SGD)和小批量梯度下降<a class="ae jg" href="https://arxiv.org/pdf/1609.04747.pdf" rel="noopener ugc nofollow" target="_blank">不同之处在于用于计算每次迭代损失函数梯度的数据量</a>。</p><p id="c2de" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文的目标是描述基于<a class="ae jg" href="https://en.wikipedia.org/wiki/Langevin_dynamics" rel="noopener ugc nofollow" target="_blank">朗之万动力学</a> (LD)的全局优化器的研究进展，朗之万动力学是一种分子运动的建模方法，它起源于<a class="ae jg" href="https://en.wikipedia.org/wiki/Albert_Einstein" rel="noopener ugc nofollow" target="_blank">阿尔伯特·爱因斯坦</a>和<a class="ae jg" href="https://en.wikipedia.org/wiki/Paul_Langevin" rel="noopener ugc nofollow" target="_blank">保罗·朗之万</a>在 20 世纪早期关于统计力学的工作。</p><p id="38b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将从<a class="ae jg" href="https://books.google.com.br/books/about/Nonequilibrium_Statistical_Mechanics.html?id=4cI5136OdoMC&amp;redir_esc=y" rel="noopener ugc nofollow" target="_blank">理论物理</a>的角度提供一个<a class="ae jg" href="https://core.ac.uk/download/pdf/4380833.pdf" rel="noopener ugc nofollow" target="_blank">优雅的解释</a>，解释为什么梯度下降的变体是高效的全局优化器。</p><h1 id="92a6" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">奇迹之年</h1><p id="e157" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">没有迹象表明一场革命即将发生。1904 年，如果阿尔伯特·爱因斯坦放弃了物理学，他的科学家同事们可能都不会注意到。幸运的是，这并没有发生。1905 年，这位年轻的专利办事员发表了四篇革新科学的论文。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><a href="https://en.wikipedia.org/wiki/Annus_Mirabilis_papers"><div class="gh gi mr"><img src="../Images/cd8eecd56542d479ae082131e54acf31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*U_tUhzPq_0aUgHIf5_COmw@2x.png"/></div></a><figcaption class="jc jd gj gh gi je jf bd b be z dk">Albert Einstein circa 1905, his miracle year (<a class="ae jg" href="https://en.wikipedia.org/wiki/Annus_Mirabilis_papers" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><h1 id="85e4" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">流体中的随机运动</h1><p id="8931" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">在其中一篇论文中<a class="ae jg" href="https://en.wikipedia.org/wiki/%C3%9Cber_die_von_der_molekularkinetischen_Theorie_der_W%C3%A4rme_geforderte_Bewegung_von_in_ruhenden_Fl%C3%BCssigkeiten_suspendierten_Teilchen" rel="noopener ugc nofollow" target="_blank">，爱因斯坦推导出了所谓的</a><a class="ae jg" href="https://en.wikipedia.org/wiki/Brownian_motion" rel="noopener ugc nofollow" target="_blank">布朗运动</a>的模型，这是液体中悬浮粒子与更小、快速移动的分子(例如在水中移动的<a class="ae jg" href="https://en.wikipedia.org/wiki/Robert_Brown_(botanist,_born_1773)" rel="noopener ugc nofollow" target="_blank">花粉粒)碰撞引起的随机运动。</a></p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><a href="https://en.wikipedia.org/wiki/Brownian_motion"><div class="gh gi mw"><img src="../Images/7a2e3e1a713960c1c58f75b8e09db61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/1*k96PH5YjCsxWV7LXAy3q_w.gif"/></div></a><figcaption class="jc jd gj gh gi je jf bd b be z dk">Brownian motion: dust particle colliding with gas molecules (<a class="ae jg" href="https://en.wikipedia.org/wiki/Brownian_motion" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><p id="9b35" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇论文中，他单枪匹马地证实了原子和分子的存在，从而诞生了一个名为<a class="ae jg" href="https://en.wikipedia.org/wiki/Molecular_dynamics" rel="noopener ugc nofollow" target="_blank">分子动力学</a>的物理学新分支，并创建了一个名为<a class="ae jg" href="https://en.wikipedia.org/wiki/Stochastic_calculus" rel="noopener ugc nofollow" target="_blank">随机微积分</a>的全新应用数学领域。</p><h1 id="7d4c" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">朗之万动力学</h1><p id="2ce0" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">1908 年，在爱因斯坦发表他的里程碑式的论文三年后，法国物理学家保罗·朗之万发表了另一篇开创性的文章，他在文章中推广了爱因斯坦的理论并发展了一个描述布朗运动的新的微分方程，今天被称为朗之万方程</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/4c660bbdaf25e88ab1009a66fecb6693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*BMFin0xhSwT4sezvj5vyMg@2x.png"/></div></figure><p id="67e5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<strong class="la jk"> <em class="my"> x </em> </strong>是运动粒子的位置，<em class="my"> m </em>是其质量，<strong class="la jk"> <em class="my"> R </em> </strong>代表与流体中更小的快速运动分子碰撞产生的(随机)力(见上面的动画)，而<strong class="la jk"> <em class="my"> F </em> </strong>代表任何其他外力。随机力<strong class="la jk"> <em class="my"> R </em> </strong>是增量相关平稳<a class="ae jg" href="https://en.wikipedia.org/wiki/Langevin_dynamics" rel="noopener ugc nofollow" target="_blank">高斯过程</a>，其均值和方差如下:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/7eaab896a0e26446cf09d3659321b588.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*1ut-iwvNz3KtAP2LiwB3CQ@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><strong class="bd na">R</strong> is a normal process.</figcaption></figure><p id="02df" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">术语“增量相关”意味着在两个不同时间的力具有零相关性。LE 是第一个描述热力学不平衡的数学方程。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><a href="https://en.wikipedia.org/wiki/Paul_Langevin"><div class="gh gi nb"><img src="../Images/44560a5161598e28c7838dad1b4c7b18.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*gWHqFZ3o5IX828WyMjZkzQ@2x.png"/></div></a><figcaption class="jc jd gj gh gi je jf bd b be z dk">The French physicist Paul Langevin (<a class="ae jg" href="https://en.wikipedia.org/wiki/Paul_Langevin" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><p id="6ae9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果粒子的质量足够小，我们可以把左边设为零。此外，我们可以将一个(<a class="ae jg" href="https://en.wikipedia.org/wiki/Conservative_force" rel="noopener ugc nofollow" target="_blank">保守</a>)力表示为某个势能的导数。我们获得:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c4f719fce90978f18b83224d1f51a3aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*T31-zfo1dZGJc9TlV-xBnA@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Langevin equation for small masses.</figcaption></figure><p id="050b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，写作</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/9bd57ee91cb51f36bf1296b78fc7e585.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*00sX60mLFrl7GzvcZAGfYA@2x.png"/></div></figure><p id="8589" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<em class="my"> δt </em>是一个小的时间间隔，移动项，我们得到小质量粒子的离散化的<a class="ae jg" href="https://en.wikipedia.org/wiki/Langevin_equation" rel="noopener ugc nofollow" target="_blank">朗之万方程</a>:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/631ac02e844ad32b85ef667319cb6861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*2ZDw5mywrj99eKxqbSy03A@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Discretized Langevin equation for particles with small inertia.</figcaption></figure><p id="b45f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用这种方式表达，朗之万方程描述了布朗运动中粒子的位移增量。</p><h2 id="88cb" class="nf lv jj bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">一小段插曲:布朗运动的 Python 代码</h2><p id="ef14" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">布朗运动的 Python 实现可以在<a class="ae jg" href="https://cyrille.rossant.net/ipython-cookbook-second-edition/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。为了模拟 2D 离散布朗过程，使用了两个 1D 过程。代码的步骤是:</p><ul class=""><li id="d01a" class="nr ns jj la b lb lc le lf lh nt ll nu lp nv lt nw nx ny nz bi translated">首先，选择时间步长的数量<code class="fe oa ob oc od b">steps</code>。</li><li id="6e7d" class="nr ns jj la b lb oe le of lh og ll oh lp oi lt nw nx ny nz bi translated">坐标<code class="fe oa ob oc od b">x</code>和<code class="fe oa ob oc od b">y</code>是随机跳跃的累积和(函数<code class="fe oa ob oc od b">np.cumsum()</code>用于计算它们)</li><li id="eb8c" class="nr ns jj la b lb oe le of lh og ll oh lp oi lt nw nx ny nz bi translated">使用<code class="fe oa ob oc od b">np.interp()</code>通过插值计算中间点<code class="fe oa ob oc od b">X</code>和<code class="fe oa ob oc od b">Y</code></li><li id="4c6b" class="nr ns jj la b lb oe le of lh og ll oh lp oi lt nw nx ny nz bi translated">然后使用<code class="fe oa ob oc od b">plot()</code>功能绘制运动</li></ul><p id="3270" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码是:</p><pre class="ms mt mu mv gt oj od ok ol aw om bi"><span id="eae0" class="nf lv jj od b gy on oo l op oq">import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="c7b5" class="nf lv jj od b gy or oo l op oq">steps = 5000<br/>random.seed(42)<br/>x,y = np.cumsum(np.random.randn(steps)), np.cumsum(np.random.randn(steps))</span><span id="194e" class="nf lv jj od b gy or oo l op oq">points = 10<br/>ip = lambda x, steps, points: np.interp(np.arange(steps*points), <br/>                                        np.arange(steps)*points, <br/>                                        x)<br/>X, Y = ip(x, steps, points), ip(y, steps, points)</span><span id="c439" class="nf lv jj od b gy or oo l op oq">fig, ax = plt.subplots(1, 1, figsize=(10, 10))<br/>ax.set_title('Brownian Motion')<br/>ax.set_xlabel('x')<br/>ax.set_ylabel('y')<br/>ax.plot(X, Y, color='blue',<br/>        marker='o',  markersize=1)</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/8550f05c43e814d39a60720f41ec8135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ulKHKtMk_MY-GpFGJjKcXQ@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Illustration of Brownian motion (<a class="ae jg" href="https://github.com/ipython-books/cookbook-2nd/blob/master/chapter13_stochastic/03_brownian.md" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><h2 id="febf" class="nf lv jj bd lw ng nh dn ma ni nj dp me lh nk nl mg ll nm nn mi lp no np mk nq bi translated">朗之万动力学和全局极小值</h2><p id="c320" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">朗之万动力学的一个<a class="ae jg" href="https://pdfs.semanticscholar.org/e86f/414f860a1a70e16d9718c887f4eb59a51f62.pdf" rel="noopener ugc nofollow" target="_blank">重要性质</a>是随机过程<strong class="la jk"><em class="my"><em class="my"/></em></strong>【其中<strong class="la jk"><em class="my">x</em><em class="my"/><em class="my"/></strong>的扩散分布<em class="my">p</em>(<strong class="la jk"><em class="my">x</em></strong>)服从上面给出的朗之万方程</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><a href="https://en.wikipedia.org/wiki/Boltzmann_distribution"><div class="gh gi ot"><img src="../Images/d472363afc9b8e5817db1a35a042eb36.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*ezQQQd8-Q5f5vXdrP863kw@2x.png"/></div></a><figcaption class="jc jd gj gh gi je jf bd b be z dk">Boltzmann distribution.</figcaption></figure><p id="5087" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其集中在势能<em class="my"> E </em> ( <strong class="la jk"> <em class="my"> x </em> </strong>)的<strong class="la jk">全局最小值</strong>(从其泛函形式中我们不难看出，BD <em class="my"> </em>峰值在势能<em class="my">E</em>(<strong class="la jk"><em class="my">x</em></strong>)的全局最小值上)。更准确地说是<em class="my">、</em>如<a class="ae jg" href="https://stuff.mit.edu/afs/athena/course/6/6.435/www/Hajek88.pdf" rel="noopener ugc nofollow" target="_blank"> Hajek </a>等所示，如果温度按照离散步骤缓慢降至零:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/6a73abdd9b3d7f4e6576ad17abde02a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*j3cshNQmsLdxTff5U1Yo9A@2x.png"/></div></figure><p id="e4ed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后<em class="my">p</em>(<strong class="la jk"><em class="my">x</em></strong>)会收敛到<em class="my"> n </em>的大值的玻尔兹曼分布(而<strong class="la jk"> <em class="my"> x </em> </strong>会收敛到<em class="my">E</em>(<strong class="la jk"><em class="my">x</em></strong>)的全局最小值)。与时间相关的温度的朗之万方程通常被解释为描述亚稳态物理状态衰变为系统的基态(这是能量的全局最小值)。因此，人们可以使用朗之万动力学来设计算法，这些算法是全局最小化器<strong class="la jk">，甚至是潜在非凸函数</strong>。</p><p id="0fe6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该原理是<a class="ae jg" href="https://en.wikipedia.org/wiki/Simulated_annealing" rel="noopener ugc nofollow" target="_blank">模拟退火技术</a>的基础，用于获得函数的近似全局最优。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/4dc9b40a9fe1ebe618fd1f52b5b5672a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*hKaZBZU3CRK2ArDVgaPwTg.gif"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Application of simulated annealing to search for maxima (<a class="ae jg" href="https://en.wikipedia.org/wiki/Simulated_annealing" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><h1 id="a90f" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">梯度下降算法</h1><p id="5325" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我现在将转向机器学习优化算法。</p><p id="1abe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">梯度下降是一种简单的迭代优化算法，用于最小化(或最大化)函数。在机器学习的背景下，这些函数是损失函数(或成本函数)。具体来说，考虑一个多元损失函数<em class="my">L</em>(<strong class="la jk"><em class="my">w</em></strong>)定义为围绕某个固定点<strong class="la jk"><em class="my">w</em></strong>p 的所有点。GD 算法基于一个简单的性质，即从任意点<strong class="la jk"> <em class="my"> p </em> </strong>开始，函数 L( <strong class="la jk"> <em class="my"> w </em> </strong>)在其负梯度方向上下降最快:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/840cd6e414f4f96563d4c468fd4348df.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*1j1ANrYJ5kTIQtvleCnlAA@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Negative gradient of the loss function.</figcaption></figure><p id="8d87" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先猜测(未知)最小值的初始值，然后计算序列</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/07a36442cdf22f59f0888dc9e99b6dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*s5b5vTBLh6bgcm1QgwvR2A@2x.png"/></div></figure><p id="4538" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在迭代过程之后</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f9e12b8b71c4a06d02a297fb728bf385.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*j_TsaG-IU4Vvue31YDjC2Q@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Gradient descent recursion.</figcaption></figure><p id="e08b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<em class="my"> γ </em>是<em class="my">学习率，</em>允许在每次迭代中改变<em class="my"> n </em>。如果损失函数<em class="my"> L </em>及其梯度<em class="my"> </em>具有<a class="ae jg" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">某些特性</a>并且学习率变化是按照<a class="ae jg" href="https://en.wikipedia.org/wiki/Line_search" rel="noopener ugc nofollow" target="_blank">某些协议</a>选择的，则<strong class="la jk">局部</strong>收敛是有保证的(只有当<em class="my"> L </em>是凸的时，收敛到<strong class="la jk">全局最小值</strong>才是有保证的，因为对于凸函数，任何局部最小值也是全局最小值)。</p><h1 id="a146" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">随机梯度下降(SGD)和小批量梯度下降</h1><p id="9d22" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">与基本 GD 算法不同，基本 GD 算法在每次迭代时扫描整个数据集，<a class="ae jg" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank"> SGD </a>和小批量 GD 只使用训练数据的一个子集。SGD 使用训练数据的单个样本来更新每次迭代中的梯度，即当它扫过训练数据时，它对每个训练示例执行上述的<strong class="la jk"> <em class="my"> w </em> </strong>的更新。小批量 GD 使用小批量训练样本执行参数更新。</p><p id="b68e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们用数学术语来描述它。对于一般的训练集</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/f711d01a2c1c0da63ca355251c612016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BXlFPXi8OaldYf-1gUsTAg@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Training set with n samples.</figcaption></figure><p id="b652" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">损失函数具有一般形式:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/515205c9f91b334f706def11b4f980ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*zwZq7-ZvILurExJHodya0Q@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">General loss function.</figcaption></figure><p id="3bec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在小批量梯度下降的情况下，总和仅超过批量内的训练样本。特别是 SGD，只使用一个样本。与普通 GD 相比，这些过程有两个主要优势:它们更快，可以处理更大的数据集(因为它们使用一个或几个样本)。</p><p id="07ce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">定义<strong class="la jk"> <em class="my"> G </em> </strong>和<strong class="la jk"> <em class="my"> g </em> </strong>如下所示，我们在这种情况下有:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/73a678545ed6316b4378e436f72dc8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nRz0wBWWxT-c4ioJcsL8uA@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Gradient in the mini-batch algorithm.</figcaption></figure><p id="a8ab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在亚历克·拉德福德的<a class="ae jg" href="https://imgur.com/s25RsOr" rel="noopener ugc nofollow" target="_blank">精彩动画</a>中，SGD 收敛与其他方法一起展示(本文中没有提到的<a class="ae jg" href="http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent" rel="noopener ugc nofollow" target="_blank">这些其他方法</a>，是 SGD 最近的改进)。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/396bc97f23c26b915580ea11f575b363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/1*RAQ9r49O26-ODWacg75ZPA.gif"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://imgur.com/s25RsOr" rel="noopener ugc nofollow" target="_blank">Great animation</a> below by Alec Radford comparing optimization methods (<a class="ae jg" href="https://imgur.com/s25RsOr" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><h1 id="a7b7" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">机器学习和物理学:作为朗之万过程的梯度下降</h1><p id="d573" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">下一步(也是最后一步)对论证至关重要。我省略了一些更严谨的方面，以便让主要思想能够被理解。</p><p id="b24f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以将小批量梯度写成全梯度和正态分布的<strong class="la jk"> <em class="my"> η: </em> </strong>之和</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/2d8d308efae1308fe2b484673b3a1701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*-D9KoGlx-b4LA4xTZ2tS9w@2x.png"/></div></figure><p id="0b9e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在将这个表达式代入 GD 迭代表达式，我们得到:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/3d95f1b39c079b23a2c4aa2af0204db6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*ACfAdjk5TU4EksW9lVtdTg@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Mini-batch gradient descent iteration step.</figcaption></figure><h1 id="2558" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">一个优雅的链接</h1><p id="ca07" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">将小批量梯度下降迭代的表达式与朗之万方程进行比较，我们可以立即注意到它们的相似性。更准确地说，它们通过以下对应关系变得相同:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/bb48845d35cd280362a5c5dae13c2fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*6fAsILWNMiECc10RMFhh7A@2x.png"/></div></figure><p id="e21a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在第二个表达式中用<em class="my"> γ </em>代替<em class="my"> δt </em>我们发现</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/26c805939d3043cdee6a52eab3bb318c.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*55s4l_btS0HLDvpCqleCBQ@2x.png"/></div></figure><p id="bc31" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，SGD 或小批量梯度下降算法在形式上类似于 Langevin 过程，这就解释了为什么如果学习速率按照前面提到的协议变化，它们有很高的概率选择全局最小值。</p><p id="0076" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个结果并不新鲜。事实上<a class="ae jg" href="https://ieeexplore.ieee.org/document/4048402" rel="noopener ugc nofollow" target="_blank">有许多证据</a>表明，在通常的梯度下降递归中加入噪声项会使算法收敛到全局最小值。必须强调的是，正是学习率的“冷却协议”提供了至关重要的“额外”随机化，使得算法能够逃离局部最小值并收敛到全局最小值。</p><h1 id="3708" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">结论</h1><p id="40ba" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">在这篇文章中，我表明，通过将随机或小批量梯度下降视为朗之万随机过程(用损失函数标识能量)，并通过学习率包括额外的随机化水平，我们可以理解为什么这些算法可以作为全局优化器工作得如此之好。这是一个优雅的结果，表明从多个角度审视一个问题通常是非常有用的。</p><p id="59f1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p><p id="76ef" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我的<a class="ae jg" href="https://github.com/marcotav" rel="noopener ugc nofollow" target="_blank"> Github </a>和我的个人网站<a class="ae jg" href="https://marcotavora.me/" rel="noopener ugc nofollow" target="_blank"> www.marcotavora.me </a>(希望)有一些关于数据科学和物理学的其他有趣的东西。</p><p id="253a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一如既往，欢迎建设性的批评和反馈！</p></div></div>    
</body>
</html>