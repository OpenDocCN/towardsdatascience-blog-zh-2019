<html>
<head>
<title>Fine-tuning BERT with Keras and tf.Module</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Keras 和 tf 微调 BERT。组件</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2?source=collection_archive---------4-----------------------#2019-11-30">https://towardsdatascience.com/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2?source=collection_archive---------4-----------------------#2019-11-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="2756" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个实验中，我们将预训练的 BERT 模型检查点转换为可训练的 Keras 层，我们使用它来解决文本分类任务。</p><p id="0c83" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们通过使用 tf 来实现这一点。模块，这是一个简洁的抽象，旨在处理预先训练好的 Tensorflow 模型。</p><p id="798a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">导出的模块可以很容易地集成到其他模型中，这有助于使用强大的神经网络架构进行实验。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/ea1e2026e79c229e9fbbb5a79500614a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zc42ldHPlG3-2R7YcKT4vQ.jpeg"/></div></div></figure><p id="2a51" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个实验的计划是:</p><ol class=""><li id="918e" class="la lb it js b jt ju jx jy kb lc kf ld kj le kn lf lg lh li bi translated">获得预训练的 BERT 模型检查点</li><li id="2a47" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated">定义 tf 的规格。组件</li><li id="b7cd" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated">导出模块</li><li id="d262" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated">构建文本预处理管道</li><li id="7ce4" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated">实现自定义 Keras 层</li><li id="ab9a" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated">训练一个 Keras 模型来解决句子对分类任务</li><li id="1a85" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated">保存和恢复</li><li id="fdd0" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated">优化用于推理的训练模型</li></ol><h1 id="f4ea" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">这本指南里有什么？</h1><p id="2d1d" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">本指南是关于将预先训练的 Tensorflow 模型与 Keras 集成。它包含两个东西的实现:一个 BERT <a class="ae mr" href="https://www.tensorflow.org/api_docs/python/tf/Module" rel="noopener ugc nofollow" target="_blank"> tf。模块</a>和构建在它上面的 Keras 层。它还包括微调(见下文)和推理的例子。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="7394" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">需要什么？</h1><p id="61ff" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">对于熟悉 TensorFlow 的读者来说，完成本指南大约需要 60 分钟。代码用 tensorflow==1.15.0 测试。</p><h1 id="4183" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">好吧，给我看看代码。</h1><p id="f42c" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">这个实验的代码可以在 T2 的实验室里找到。独立版本可以在<a class="ae mr" href="https://github.com/gaphex/bert_experimental" rel="noopener ugc nofollow" target="_blank">库</a>中找到。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="6356" class="lo lp it bd lq lr nb lt lu lv nc lx ly lz nd mb mc md ne mf mg mh nf mj mk ml bi translated">步骤 1:获得预训练模型</h1><p id="1091" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">我们从一个预先训练好的基于 BERT 的检查点开始。在这个实验中，我们将使用由谷歌预先训练的英语模型<a class="ae mr" href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank">。当然，在构建 tf.Module 时，您可以使用更适合您的用例的模型。</a></p><h1 id="cc17" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">步骤 2:构建一个 tf。组件</h1><p id="6a48" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">tf。模块旨在提供一种简单的方法来操作 Tensorflow 中预训练机器学习模型的可重用部分。谷歌在<a class="ae mr" href="https://www.tensorflow.org/hub" rel="noopener ugc nofollow" target="_blank"> tf 维护了一个此类模块的精选库。轮毂</a>。然而，在本指南中，我们将自己从头开始构建一个。</p><p id="77b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为此，我们将实现一个包含模块内部工作的完整规范的<em class="ng"> module_fn </em>。</p><p id="59b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们从定义输入占位符开始。BERT 模型图由通过<em class="ng">配置路径</em>传递的配置文件创建。然后提取 we 模型输出:最终的编码器层输出被保存到<em class="ng"> seq_output </em>并且汇集的“CLS”令牌表示被保存到<em class="ng"> pool_output。</em></p><p id="1db0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，额外的资产可以与模块捆绑在一起。在这个例子中，我们将一个包含单词表的<em class="ng"> vocab_file </em>添加到模块资产中。因此，词汇文件将与模块一起导出，这将使它成为自包含的。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="a9c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们定义了签名，它是输入到输出的特定转换，向消费者公开。人们可以把它看作是与外界的一个模块接口。</p><p id="bce2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里添加了两个签名。第一个将原始文本特征作为输入，并将计算后的文本表示作为输出返回。另一个不接受输入，返回词汇表文件的路径和小写标志。</p><h1 id="335e" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">步骤 3:导出模块</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="f211" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">既然已经定义了<em class="ng"> module_fn </em>，我们就可以用它来构建和导出模块了。将<em class="ng"> tags_and_args </em>参数传递给<em class="ng"> create_module_spec </em>将导致两个图变量被添加到模块中:用 tags<em class="ng">{“train”}</em>进行训练，以及用一组空的 tags 进行推断。这允许控制退出，这在推理时被禁用，而在训练期间被启用。</p><h1 id="9c02" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">步骤 4:构建文本预处理管道</h1><p id="0308" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">BERT 模型要求将文本表示为包含<em class="ng">input _ id</em>、<em class="ng"> input_mask </em>和<em class="ng">segment _ id</em>的 3 个矩阵。在这一步中，我们构建了一个管道，它接受一个字符串列表，并输出这三个矩阵，就这么简单。</p><p id="1216" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，原始输入文本被转换成 InputExamples。如果输入文本是由特殊的“|||”序列分隔的句子对，则句子被拆分。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="5db0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，使用来自原始存储库的<em class="ng">convert _ examples _ to _ features</em>函数，将 InputExamples 标记化并转换为 InputFeatures。之后，特征列表被转换成带有<em class="ng">特征 _ 到 _ 数组</em>的矩阵。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="6f23" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们将所有这些放在一个管道中。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="ad63" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">全部完成！</p><h1 id="6c18" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">步骤 5:实现 BERT Keras 层</h1><p id="525e" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">使用 tf 有两种方法。带有 Keras 的模块。</p><p id="e3f6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一种方法是用<a class="ae mr" href="https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer" rel="noopener ugc nofollow" target="_blank"> hub 包裹一个模块。角斗士</a>。这种方法很简单，但是不太灵活，因为它不允许在模块中加入任何定制的逻辑。</p><p id="0db4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二种方法是实现包含该模块的自定义 Keras 层。在这种情况下，我们可以完全控制可训练变量，并且可以将池化操作甚至整个文本预处理管道添加到计算图中！我们将走第二条路。</p><p id="2922" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了设计一个定制的 Keras 层，我们需要编写一个继承自 tf.keras.Layer 的类，并覆盖一些方法，最重要的是<em class="ng">构建</em>和<em class="ng">调用。</em></p><p id="8833" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ng"> build </em>方法创建模块的资产。它从实例化来自<em class="ng"> bert_path </em>的 BERT 模块开始，该路径可以是磁盘上的路径或 http 地址(例如，对于来自 tf.hub 的模块)。然后建立可训练层的列表，并填充该层的可训练权重。将可训练权重的数量限制在最后几层可以显著减少 GPU 内存占用并加快训练速度。它还可能提高模型的准确性，特别是在较小的数据集上。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="2e5b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ng"> build_preprocessor </em>方法从模块资产中检索单词表，以构建步骤 4 中定义的文本预处理管道。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="887d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ng"> initialize_module </em>方法将模块变量加载到当前的 Keras 会话中。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="bc0e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">大多数有趣的事情都发生在<em class="ng">调用</em>方法内部。作为输入，它接受一个张量<em class="ng"> tf。字符串</em>，使用我们的预处理管道将其转换成 BERT 特征。使用<em class="ng"> tf.numpy_function 将 python 代码注入到图中。</em>然后将特征传递给模块，并检索输出。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="9bf3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，根据在<em class="ng"> __init__，</em>中设置的<em class="ng">池</em>参数，额外的变换被应用于输出张量。</p><p id="2195" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果<em class="ng"> pooling==None </em>，则不应用池，输出张量具有形状<em class="ng"> [batch_size，seq_len，encoder_dim] </em>。此模式对于解决令牌级任务非常有用。</p><p id="147e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果<em class="ng"> pooling=='cls '，</em>仅检索对应于第一个<em class="ng"> 'CLS' </em>标记的向量，并且<em class="ng"> </em>输出张量具有形状<em class="ng"> [batch_size，encoder_dim] </em>。这种池类型对于解决句子对分类任务很有用。</p><p id="280c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，如果<em class="ng"> pooling=='mean '，</em>所有记号的嵌入都是平均池化的，并且<em class="ng"> </em>输出张量具有形状<em class="ng"> [batch_size，encoder_dim] </em>。这种模式对于句子表达任务特别有用。它的灵感来自<a class="ae mr" href="https://github.com/hanxiao/bert-as-service" rel="noopener ugc nofollow" target="_blank">伯特即服务</a>的 REDUCE_MEAN 池策略。</p><p id="2193" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">BERT 层的完整列表可以在<a class="ae mr" href="https://github.com/gaphex/bert_experimental/blob/master/finetuning/bert_layer.py" rel="noopener ugc nofollow" target="_blank">库</a>中找到。</p><h1 id="ef0f" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">第六步:句子对分类</h1><p id="1655" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">现在，让我们在真实数据集上尝试该图层。对于这一部分，我们将使用<a class="ae mr" href="https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs" rel="noopener ugc nofollow" target="_blank"> Quora 问题对</a>数据集，它由超过 400，000 个潜在的问题重复对组成，标记为语义等价。</p><p id="71fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">建立和训练句子对分类模型很简单:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="e165" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">顺便说一句，如果你不喜欢在图形中进行预处理，你可以通过设置<em class="ng"> do_preprocessing=False </em>来禁用它，并使用 3 个输入来构建模型。</p><p id="a385" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">仅微调最后三层就可以获得 88.3%的验证准确率。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="f469" class="nm lp it ni b gy nn no l np nq">Train on 323432 samples, validate on 80858 samples<br/>Epoch 1/5</span><span id="9d04" class="nm lp it ni b gy nr no l np nq">323432/323432 [==============================] - 3197s 10ms/sample - loss: 0.3659 - acc: 0.8255 - val_loss: 0.3198 - val_acc: 0.8551<br/>Epoch 2/5<br/>323432/323432 [==============================] - 3191s 10ms/sample - loss: 0.2898 - acc: 0.8704 - val_loss: 0.2896 - val_acc: 0.8723<br/>Epoch 3/5<br/>323432/323432 [==============================] - 3231s 10ms/sample - loss: 0.2480 - acc: 0.8920 - val_loss: 0.2833 - val_acc: 0.8765<br/>Epoch 4/5<br/>323432/323432 [==============================] - 3205s 10ms/sample - loss: 0.2083 - acc: 0.9123 - val_loss: 0.2839 - val_acc: 0.8814<br/>Epoch 5/5<br/>323432/323432 [==============================] - 3244s 10ms/sample - loss: 0.1671 - acc: 0.9325 - val_loss: 0.2957 - val_acc: 0.8831</span></pre><h1 id="90dd" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">步骤 7:保存和恢复模型</h1><p id="2167" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">模型权重可以通过常规方式保存和恢复。模型架构也可以序列化为 json 格式。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="5cfe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果到 BERT 模块的<strong class="js iu">相对</strong>路径不变，从 json 重建模型将会工作。</p><h1 id="3d98" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">步骤 8:优化推理</h1><p id="cfb6" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">在某些情况下(例如，当服务时)，人们可能想要优化训练模型以获得最大的推理吞吐量。在 TensorFlow 中，这可以通过“冻结”模型来实现。</p><p id="4a07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在“冻结”过程中，模型变量由常数代替，训练所需的节点从计算图中删除。生成的图形变得更加轻量级，需要更少的 RAM，并获得更好的性能。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="e649" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们冻结训练好的模型，并将序列化的图形写入文件。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="28eb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们还原一下冻结图，做一些推断。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="6907" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了运行推理，我们需要得到图的输入和输出张量的句柄。这部分有点棘手:我们在恢复的图中检索所有操作的列表，然后手动获取相关操作的名称。列表是排序的，所以在这种情况下，只需进行第一个和最后一个操作。</p><p id="c475" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了得到张量名称，我们将<strong class="js iu"> ":0" </strong>附加到 op 名称上。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="696f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们注入到 Keras 层中的预处理函数是不可序列化的，并且没有在新图中恢复。不过不用担心——我们可以简单地用相同的名称再次定义它。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="a071" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们得到了预测。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="a46b" class="nm lp it ni b gy nn no l np nq">array([[9.8404515e-01]], dtype=float32)</span></pre><h1 id="6d96" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">结论</h1><p id="4ee8" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">在这个实验中，我们创建了一个可训练的 BERT 模块，并用 Keras 对其进行了微调，以解决句子对分类任务。</p><p id="0bb8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过冻结训练好的模型，我们消除了它对自定义层代码的依赖性，使它变得可移植和轻量级。</p><h2 id="c1aa" class="nm lp it bd lq ns nt dn lu nu nv dp ly kb nw nx mc kf ny nz mg kj oa ob mk oc bi translated">本系列中的其他指南</h2><ol class=""><li id="9d00" class="la lb it js b jt mm jx mn kb od kf oe kj of kn lf lg lh li bi translated"><a class="ae mr" rel="noopener" target="_blank" href="/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379">用云 TPU 从头开始预训练 BERT】</a></li><li id="f87d" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated"><a class="ae mr" rel="noopener" target="_blank" href="/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a">用 BERT 和 Tensorflow 构建搜索引擎</a></li><li id="6186" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated"><a class="ae mr" rel="noopener" target="_blank" href="/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2">用 Keras 和 tf 微调 BERT。模块</a>【你在这里】</li><li id="03d5" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated"><a class="ae mr" rel="noopener" target="_blank" href="/improving-sentence-embeddings-with-bert-and-representation-learning-dfba6b444f6b">使用 BERT 和表示学习改进句子嵌入</a></li></ol></div></div>    
</body>
</html>