<html>
<head>
<title>Gentle introduction to Echo State Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回声状态网络简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gentle-introduction-to-echo-state-networks-af99e5373c68?source=collection_archive---------8-----------------------#2019-03-27">https://towardsdatascience.com/gentle-introduction-to-echo-state-networks-af99e5373c68?source=collection_archive---------8-----------------------#2019-03-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fd9e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">也是油藏计算的介绍</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f538ccb30ab1e7eb51b5cc0f7728dfff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yC5tRSLq5Qf9gC33evSLfw.jpeg"/></div></div></figure><p id="5d34" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本帖将回答以下问题:</p><ol class=""><li id="d480" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">什么是回声状态网络？</li><li id="621a" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">为什么以及何时应该使用回应状态网络？</li><li id="6650" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">python 中的简单实现示例</li></ol><p id="d3c9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下图是对论文<a class="ae mb" href="https://arxiv.org/pdf/1803.07870.pdf" rel="noopener ugc nofollow" target="_blank">多变量时间序列</a>表示和分类的储层计算方法的简化，但它很好地抓住了 ESNs 的要点。每个组件将在下面的章节中详细介绍。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mc"><img src="../Images/c3d9bdbffe3a17cef188eb82b13fc30d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LWQiAfIcVKpt6R_yb2RSnw.png"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Echo State Networks are recurrent networks. <strong class="bd mh">f</strong> is a nonlinear function (such as tanh) which makes the current state dependent on the previous state and the current input</figcaption></figure><h1 id="5c81" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">什么是回声状态网络？</h1><p id="10c4" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">回声状态网络是一种递归神经网络，是<em class="nf">油藏计算</em>框架的一部分，具有以下特殊性:</p><ul class=""><li id="4091" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ng lt lu lv bi translated">输入-隐藏层('水库'):<em class="nf"> Win </em>之间的权重以及'水库':<em class="nf"> Wr </em>之间的权重<strong class="kt ir">是随机分配的</strong>和<strong class="kt ir">是不可训练的</strong></li><li id="2c38" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">输出神经元的权重(“读出”层)是可训练的(<strong class="kt ir">,可以学习，因此网络可以复制特定的时间模式</strong></li><li id="915c" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">隐藏层(或“储层”)与输入的连接非常稀疏(通常为&lt; 10% connectivity)</li><li id="3bdb" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">the reservoir architecture creates a recurrent <strong class="kt ir">非线性</strong>嵌入(下图中的 H )),然后可以连接到所需的输出，这些最终权重是可训练的</li><li id="353c" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">可以将嵌入连接到不同的预测模型(用于分类问题的可训练神经网络或岭回归器/SVM)</li></ul><h2 id="fd80" class="nh mj iq bd mk ni nj dn mo nk nl dp ms la nm nn mu le no np mw li nq nr my ns bi translated">油藏计算</h2><p id="c864" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">储层计算是神经网络的扩展，其中输入信号连接到固定(不可训练)和随机动态系统(储层)，从而创建更高维度的表示(嵌入)。这种嵌入然后通过可训练单元连接到期望的输出。</p><p id="64f3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">储层计算的非递归等价物是<a class="ae mb" href="https://en.wikipedia.org/wiki/Extreme_learning_machine" rel="noopener ugc nofollow" target="_blank">极限学习机</a>，仅由仅具有可训练读出层的前馈网络组成。</p><h2 id="a4e0" class="nh mj iq bd mk ni nj dn mo nk nl dp ms la nm nn mu le no np mw li nq nr my ns bi translated">工作流程</h2><p id="22ef" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">对于形状为 N，T，V 的输入，其中 N 是观测值的数量，T 是时间步长的数量，V 是变量的数量，我们将:</p><ul class=""><li id="9728" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ng lt lu lv bi translated">选择储层 R 的大小和控制连接稀疏性水平的其他参数，如果我们想要对泄漏建模，维数减少后的理想组件数量等</li><li id="f5d0" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">通过从随机二项式分布中取样，生成(V，R)个输入权重<em class="nf"> Win </em></li><li id="1df2" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">通过从具有给定密度的均匀分布中取样，生成(R，R)储层权重<em class="nf"> Wr </em>，该参数设置了稀疏程度</li><li id="fe40" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">将高维状态表示 H 计算为当前时间步长(N，V)的输入乘以内部权重加上先前状态再乘以储层矩阵(R，R)的非线性函数(通常为 tanh)</li><li id="6427" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">可选地，我们可以运行降维算法，例如 PCA 到 D 分量，这将 H 带到(N，T，D)</li><li id="72e3" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">通过使用例如整个储层和训练回归器来映射状态 t 到 t+1，创建输入表示:一个表示可以是所有计算的斜率和截距的矩阵。另一种选择是使用 H 的平均值或最后值</li><li id="bd52" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">通过使用可训练的 NN 结构或通过使用其他类型的预测器，将这种嵌入连接到期望的输出。上述论文建议使用岭回归</li></ul><h2 id="1741" class="nh mj iq bd mk ni nj dn mo nk nl dp ms la nm nn mu le no np mw li nq nr my ns bi translated">为什么以及何时应该使用回声状态网络？</h2><ul class=""><li id="f953" class="ln lo iq kt b ku na kx nb la nt le nu li nv lm ng lt lu lv bi translated">传统的神经网络结构受到消失/爆炸梯度问题的困扰，因此，隐藏层中的参数要么变化不大，要么导致数值不稳定和混沌行为。回声状态网络没有这个问题</li><li id="3c96" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">传统的神经网络结构计算昂贵，回声状态网络非常快，因为在储层上没有反向传播相位</li><li id="02ee" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">传统的神经网络会被<a class="ae mb" href="http://www.scholarpedia.org/article/Bifurcation" rel="noopener ugc nofollow" target="_blank">分叉</a>破坏</li><li id="998b" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ng lt lu lv bi translated">ESN 非常适合处理混沌时间序列</li></ul><h2 id="901a" class="nh mj iq bd mk ni nj dn mo nk nl dp ms la nm nn mu le no np mw li nq nr my ns bi translated">履行</h2><p id="9aef" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">该论文提供了一个简洁的、有良好文档记录的流水线实现，提出了每一步的各种方法(权重初始化方案、维度减少、输入表示、读出层)。我通过精选步骤创建了更简化的版本，如 PCA、基于岭回归的储层表示和输出的线性回归连接，因为这些选项在我的输入中表现最佳。我在这里<a class="ae mb" href="https://github.com/ciortanmadalina/EchoStateNetwork/blob/master/EchoStateNetwork.ipynb" rel="noopener ugc nofollow" target="_blank">提供了这个版本</a>。</p><h1 id="c7a9" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">参考</h1><p id="8a0c" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated"><a class="ae mb" href="https://github.com/FilippoMB/Reservoir-Computing-framework-for-multivariate-time-series-classification" rel="noopener ugc nofollow" target="_blank">https://github . com/FilippoMB/Reservoir-Computing-framework-for-variable-time-series-class ification</a></p><p id="6eaa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae mb" href="https://www.researchgate.net/post/what_is_the_realitionship_between_deep_learning_methods_and_reservoir_computing_if_any" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/post/what _ is _ the _ realition ship _ between _ deep _ learning _ methods _ and _ reservoir _ computing _ if _ any</a></p></div></div>    
</body>
</html>