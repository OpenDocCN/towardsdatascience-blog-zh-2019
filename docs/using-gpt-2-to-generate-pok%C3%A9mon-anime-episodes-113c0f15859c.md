# 使用 GPT-2 生成神奇宝贝动画集

> 原文：<https://towardsdatascience.com/using-gpt-2-to-generate-pok%C3%A9mon-anime-episodes-113c0f15859c?source=collection_archive---------30----------------------->

从数据采集到为真正愚蠢的东西提供网络服务的旅程。

![](img/562c36532fa1b7baf76f3f6e8a6ad253.png)

> 乐天河童是萨尔萨舞大师，他会教亚什如何像神一样移动。他会取笑亚什不能快速移动，甚至会攻击他身体虚弱。

接下来的故事讲述了某人利用一项极其复杂的技术做出了一件愚蠢的事情。OpenAI 展示的 GPT-2 模型是人工智能生成文本的游戏改变者。以至于开发该模型的团队推迟了它的公开发布，以便人们可以准备好迎接一个像假新闻这样的事情可以毫不费力地产生而无需太多人工干预的世界。是的，今天我将展示如何使用这种危险的神器来制作神奇宝贝剧集。

**你可以在这里查看最终结果:**[](http://pokegen.thiagolira.com.br/)****所有代码都可以在我的**[**Github**](https://github.com/ThiagoLira/pkm-ep-generator)**资源库找到。如果你只是想看一些例子，可以跳到文章的末尾。****

**我不能保证网站会在任何时候回答所有的请求，因为服务器不能同时处理 3 个以上的请求(我将很快解释为什么),而且我已经脱离了在 AWS 上托管它的自由层。但是，如果网站产生一个错误，只需等待几秒钟，再试一次:)**

**在本文中，我将尽力解释端到端机器学习项目的主要挑战，每一步都是如此。**

# **数据**

****机器学习方法用于从数据中提取信息和推断模式。**传统的统计方法在建模阶段会有统计人员选择的许多参数和假设，而机器学习方法会让数据自己说话。**这是一个众所周知的可解释(经典)模型和精确(机器学习)模型之间的权衡**。机器学习的预测能力基本上来自于拥有大量数据和足够复杂的模型，以从中捕捉高度微妙的模式。有一个“平滑度”假设，即模型正在一个足够大的现实样本上进行训练，以推断(概括)它没有直接看到的东西，假设它接近它确实已经训练过的某个例子(因此是平滑度)。**

**最近的 NLP(自然语言处理)模型没有什么不同，它们需要大量的文本和计算能力来训练。这些新模型从零语言知识开始，到最后，它们变得非常擅长从单词序列中测量上下文信息。就理解完全相同的单词在句子的不同位置具有不同的含义而言，这是经典的 NLP 模型不太擅长实现的。**

**GPT-2 模型是在维基百科、Reddit 和许多其他地方预先训练好的。我所做的是在来自互联网的一组更具体的文本上对模型进行微调。这是互联网的子集，由神奇宝贝动画剧集的摘要组成(并且在 Bulbapedia 上)。向 [Bulbapedia](https://bulbapedia.bulbagarden.net) 大喊，因为它是一个优秀的社区驱动的神奇宝贝网站！**

**我写的爬虫下载了社区写的大约 400 个剧集摘要。以下是其中一集的样本:**

> ***亚什向自己和全世界的神奇宝贝宣布，他将成为一名神奇宝贝大师。然而，他的演讲被他的妈妈打断了，妈妈告诉他去睡觉，因为明天是他的大日子。阿什抗议说，他太兴奋了，睡不着，所以他的妈妈告诉他，如果他不睡觉，那么至少要为第二天做好准备，因为她打开了由镇上的神奇宝贝专家奥克教授主持的一个节目。Ash 看着 Oak 解释新的训练者从三个神奇宝贝中选择一个开始他们的旅程；草型妙蛙种子，火型小火龙或水型杰尼龟。***

**我的 crawler 在文件 crawler_bulbapedia.py 上，运行时会创建一个名为 data/pokeCorpusBulba 的文件夹，它会将每一集存储在一个单独的文本文件中。**

****数据尚未准备好提供给模型。另一个名为 prepare_corpus.py 的脚本将清理文本，并将它们合并到一个名为 train.txt 的文件中，准备用于 GPT-2。****

# **模型**

**GPT-2 是一种基于变压器的模型，它使用一种称为自我关注的技术，以一种令人惊讶的自然方式学习单词如何完成或继续句子。我不认为我能比这些[倍数](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04) [优秀](/transformers-141e32e69591) [来源](/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8)更好地解释这个模型的数学和内部运作。但是我可以从纯编程的角度，就如何使用这个预先训练好的模型提供一些见解，就像它是一个文本生成 API 一样。为此，我找到了一个很好的资源， [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple) python 库，它使所有 Tensorflow 的复杂性基本上不可见，并提供了一些非常简单的函数，可以从 gpt-2 模型中下载、微调和采样。**

**基本上，语言模型试图从一个句子中预测下一个单词，我们可以不断从模型中获得预测以生成新的文本，将最后的预测作为新的输入，以获得越来越多的单词。因此，作为一个例子，我们可以为我们的模型提供前缀输入“Ash 和 Pikachu were”:**

**![](img/9952c4f13298867c209731d1ee6b3cf8.png)**

**GPT-2 使用**注意力**机制，动态评估最后一个单词对预测下一个单词的重要性。在这个模型中有一个叫做“transformer cell”的东西，它计算输入序列中每个单词相对于其他每个单词的关注值。这些都被传递以生成输出，即预测句子中的下一个单词。**

**作为一个稍微简化的例子，我们可以通过注意力值的强度(紫色-er 越多的注意力)看到，显然“Ash”和“Pikachu”与确定“was”之后的内容相关。这是这个模型的一个非常好的地方，经典的“计算单词”方法，比如朴素贝叶斯方法不能做到。**

**![](img/ad1bba36e4e1bfab24b04ff196f7bfe0.png)**

****通过从语料库中删除句子上的单词并微调模型以正确预测它们来进行训练。** [最后](https://www.youtube.com/watch?v=DlkpbzXjuPM)，我们有一个检查点文件夹，这是我们从这个模型生成文本唯一需要的东西。tensorflow 创建的这个文件夹包含了用我的神奇宝贝语料库微调后模型的整个状态，gpt-2-simple 库在生成新文本时会寻找它。**

# **服务器**

**这是迄今为止最具挑战性的部分。在互联网上为这个推理模型提供服务不是一项简单的任务，因为文本生成需要大量的内存。**

**基本上，服务器结构响应指向端口 5000 的 GET 请求。它有一个函数来响应这个请求，获取参数(用户输入)，初始化模型，生成一些固定数量的文本，并返回 JSON 中的所有内容。困难的部分在于，该模型占用了高达 1GB 的内存来进行推理。因此，在做任何事情之前，我必须有一个具有相当大内存的服务器(再见 AWS 免费层！很高兴见到你)。所以我最终选择了 AWS 上的 EC2 t2-medium 实例，并在我的朋友[joo](https://github.com/jplippi)的帮助下设置了它。**

**下面这个 EC2 实例内部的 web 服务器结构完全是从我的另一个朋友 Gabriela，从她受欢迎的媒体[的帖子](https://medium.com/@gabimelo/developing-a-flask-api-in-a-docker-container-with-uwsgi-and-nginx-e089e43ed90e)中复制来的。**

**我选择在这个 EC2 实例上运行的 web 服务器是 nginx，它监听请求，然后将它们转发给 uWSGI web 服务器，后者通过 WSGI 协议与 Flask 应用程序通信。基本上我们有这样的结构:**

**![](img/b71b06b65464b4cdd2bf210ef81bdea1.png)**

**Diagram by [Gabriela Melo](https://medium.com/@gabimelo)**

**WSGI 协议的目的是为用 Python 编写的 web 应用程序创建一个公共接口。例如，我可以改变应用程序框架(Flask 到 Django)或应用程序服务器(uWSGI 到 Unicorn ),这对于其他部分来说是不可见的。**

**现在，为什么我不把 uWSGI 服务器提供给网络呢？为什么要用另一层，比如 nginx？嗯，简单的答案是 nginx 抽象出了服务器负载可能带来的一些问题，uWSGI 本身不适合处理这些问题。**

**当然，我不得不把所有这些软件打包在一个 Docker 容器里，因为这就是现在所有酷孩子做的事情。所有的代码都可以在我的 [Github](https://github.com/ThiagoLira/pkm-ep-generator) 资源库中找到，但是要阅读关于这种配置的真正深入和完善的解释，我建议访问 Gabriela 的[帖子](https://medium.com/@gabimelo/developing-a-flask-api-in-a-docker-container-with-uwsgi-and-nginx-e089e43ed90e)，因为我的设置基本上是相同的，有一些小的调整，因为我的应用程序有点不同。**

# **烧瓶应用程序**

**Flask 应用程序(模型在服务器上运行的地方)有一个请求的入口点，即生成函数:**

# **经验教训**

*   **如果你想用机器学习做出新的东西，数据是非常重要的。体面的清洁也同样重要。**
*   ****GPT-2 模型作为按需文本生成工具是不切实际的，它需要太多的内存和 CPU 来运行。拥有一个需要 1GB 内存来处理每个请求的服务是非常昂贵的。****
*   ***docker 系统修剪*是你的朋友。**
*   **web 服务器的 Python 生态系统并不那么难用，并且有大量的例子。**

# **一些输出示例**

**模型的输入信息以**粗体**显示。当然，这涉及到一些樱桃的挑选，但这就是生成模型的方式。**

> ****艾希和米丝蒂恋爱时正在约会。**当他们都回忆起各自的第一次经历时，阿什第一次与神奇宝贝擦肩而过是他所记得的一切，因为他还是个孩子。后来，在 ash 做了他的第一个扑克球后，他跳过了午餐，去追一个朋友，并换了运动鞋。这导致他们都坠入爱河，留下道恩和布洛克泪流满面。当他们外出寻找亚什的白头龙时，一只野生的 gyarados 把他们打跑了。(…)**
> 
> **皮卡丘厌倦了这一切，他害怕草地，也害怕驯兽师。杰西和詹姆斯跑出去，跑出去。(…)**
> 
> ****亚什想要成为最棒的，为了达到这个目标，他训练了所有的神奇宝贝。他告诉他的教练整个故事，并承诺成为一名伟大的教练。他告诉他的父母和朋友，他会尽最大努力训练他们。他们很惊讶，正准备放弃他时，他的父母开始哭了。他的母亲告诉他回家找他的朋友。他们别无选择，只能跟他走。****
> 
> ****皮卡丘因逃税被捕。**三人组一走出橄榄石城神奇宝贝中心，就立刻遭到一名前警官、一名警探、一名护士 joy 和一名护士 joy 的 glameow 的攻击。**