# 如何防止种族主义机器人

> 原文：<https://towardsdatascience.com/how-to-prevent-racist-robots-20a0dde6bec0?source=collection_archive---------33----------------------->

## 什么是算法公平，为什么它很重要？

你遇到过这种情况吗？

你在给你的同事讲故事，你提到了你的种族(不是白人)。突然，你的(白人)同事打断你并宣布:

![](img/e93b245a678397fd782758903b9e2e70.png)

You sure about that, Chad?

和你的老板谈论与你的女性身份有关的事情？为什么你一定要把性别带进来？如果你想被平等对待，那就不要提醒我你是女人。”

想结婚，摄影师却拒绝你服务？*“你是同性恋只是巧合！我听着不像是歧视！”*

在美国有许多受 T4 压迫的人群。系统性压迫是复杂的，但我们闭上眼睛，塞住耳朵，并不意味着不公正将不复存在。这只意味着我们站在它的中间，忽略它。

> 无知不是福。这只会让你变得无知。

尽管如此，许多人不理解或试图理解有色人种、女性、LGBTQ 人群、不同能力人群或其他任何目标人群的经历。他们选择忽略“他者”应该指出的是，这往往是善意的，但其结果却恰恰相反。机器每天做出的某些决定也是如此。

在他们的学术文章“[算法公平](https://cpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/3/1161/files/2018/07/AEA-P-and-P-algorithms-and-race-2018-suuawi.pdf)”中，作者提出，如果我们(和我们的机器)想要利用这些信息做出重要决策，我们应该*包括*种族和其他代表受压迫群体的因素。如果我们忽视这些重要的因素，我们会弊大于利。为什么？我们来分解一下文章是怎么说的。

算法是一组步骤、规则或计算，计算机通常都有自己遵循的算法。我们越来越普遍地使用算法来做决策。仅举几个常见但重要的例子:医生决定某人是否有患病风险，法官设定保释金，学校分析申请。

一次又一次的事实表明，我们的“机器人”正在接受我们人类的偏见，做出歧视性的决定。来自文章:

> 因为用于训练这些算法的数据本身带有刻板印象和过去的歧视，所以很自然地会担心偏见正在被“烤熟”

因此，一个常见的反应是排除敏感因素，训练机器人宣布:

![](img/d8b892b7b9c84de159c74a084d5235a4.png)

为什么这是个坏主意？在说之前，我们必须知道一个关键词的定义:*公平*。公平意味着公正。相等意味着“相同”因此，请记住，公平并不意味着平等。

![](img/9b3cda0bb3fc3e726779e09e0e7a980c.png)

The Difference Between Equality and Equity

当我们平等待人时，我们给予每个人同样的支持。当我们公平对待他人时，我们会给予每个人所需的支持，这样结果才会公平。

当然，我们的世界更加复杂…

![](img/bce1a7b8f5c46d4fedb75e483c4dedad.png)

Another idea: True liberation means no one cares about watching the game?

…但是这些知识足以帮助我们理解这篇文章。文章做了这样一个实验:

想象一下，我们有两个不同的大学招生顾问。一位名叫埃里卡的辅导员很有效率。Erica 的目标始终是录取在大学期间表现最好的学生。所以，她看着学生的申请，她使用机器学习模型来预测那个学生的大学平均绩点会是多少。艾丽卡所要做的就是选择最有潜力的学生。

另一个顾问法拉是公平的*和*高效的。Farrah 也想要最有潜力的学生，但她对申请者一视同仁。在实验中，他们将学生群体限定为非西班牙裔白人学生和黑人学生。因此，预测模型中唯一需要“不同”处理的变量是种族。Farrah 必须选择大学 GPA 最高的学生，确保有一定比例的黑人学生。

所以埃里卡是有效率的，她只想要在大学里表现最好的申请者。法拉是公平的，她希望申请人在大学里表现最好*结合*她想最大限度地增加黑人学生的数量。

他们使用的模型能够预测大学的 GPA，因为实验涉及到随着时间的推移跟踪申请人。所以，他们有一堆高中数据以及他们最终的大学平均绩点来显示他们在大学的真实表现。

Erica 以两种方式运行这个模型:排除种族，包括种族。不管是哪种模式，记住她总是有效地选择——预测平均绩点最高的学生。

Farrah 运行了包括种族在内的模型，她设置了不同的阈值，试图消除黑人申请者可能遇到的一些系统性偏见。例如，我们可能会担心一个黑人申请者没有参加 SAT 预备课程，而一个白人学生参加了。然后，如果这两个学生获得相同的 SAT 分数，该模型将预测黑人学生将有更高的大学 GPA。这只是 Farrah 对模型阈值进行的许多复杂调整中的一个例子。

结果非常棒:

*   最能预测申请人在大学表现的模型总是将种族因素考虑在内，即使在 Erica 的案例中，她只关心效率。这可能看起来违反直觉或令人困惑。换句话说，当人们处理申请并猜测申请人将如何上大学时，它带有我们所有的偏见，所以我们经常想从等式中去掉种族，以欺骗自己认为我们是公平的。然而，实际上由一个数学模型为我们做出预测*更有效(准确)*，因为他们的预测是基于数字，而不是基于有偏见的预感。因此，在模型中包含种族因素会更有效。
*   当 Farrah 对门槛进行公平调整时，无论她试图录取多少比例的黑人学生，使用种族意识预测器都会导致相对更多的黑人申请人被大学录取。因此，为了更加公平，有必要在模型中包含种族因素。
*   总之:永远包括种族。机器人应该能看到颜色。

> “在没有法律约束的情况下，出于公平原因，我们应该纳入性别和种族等变量……纳入这些变量可以提高公平和效率。”

他们使用各种机器学习模型进行了这项实验，但他们一致发现，“让算法盲目进行种族划分的策略无意中损害了公平性。”

虽然这篇文章有一个老生常谈的结论，但解决算法偏差并不容易。我们如何定义什么是公平的？正如[另一位名叫莎拉·谢弗勒的研究员](https://www.bu.edu/research/articles/algorithmic-fairness/)指出的:

> 有许多不同的衡量公平的标准，它们之间有所取舍。那么……系统在多大程度上符合我们想要实现的公平理念呢？

即使一些科学家同意什么是公平的，我们也经常必须确保用不同的公平阈值实现我们的算法是合法的。正如我们在[平权法案](https://www.britannica.com/topic/affirmative-action)的历史中所看到的，举个例子，对于什么应该被合法实施，经常会有不同的意见。

我对几件不同的事情充满希望:

*   我希望我们能利用与他人相处的受教育机会，向他们解释为什么正确看待一个人的整个环境是非常有价值的，而不是抱着“无知是福”的心态。
*   我也希望那些受分析推理驱动的人能够理解像我们刚刚重复的文章。我们通过在我们的科学中包括像种族这样的受保护的因素，而不是忽视它，来尽可能做出最好的预测。这有利于所有人，不管我们对“公平”的定义是什么。

我们希望这两种情况能够汇聚成一个社会，在机器的帮助下，不断地为更大的利益做出决定。