<html>
<head>
<title>What is Label Smoothing?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是标签平滑？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06?source=collection_archive---------1-----------------------#2019-12-17">https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06?source=collection_archive---------1-----------------------#2019-12-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="15c3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种让你的模型不那么过于自信的技巧</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/46e21392720936ed385754232cba8874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X1Y0JF3LRRU2Vkkw"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@xusanfeng?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Levi XU</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="61c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当使用深度学习模型进行分类任务时，我们通常会遇到以下问题:过度拟合和过度自信。过拟合得到了很好的研究，可以通过早期停止、<em class="lv"> </em>放弃、权重调整等来解决。另一方面，我们对付过度自信的工具更少了。<em class="lv">标签平滑</em>是一种解决这两个问题的正则化技术。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="8523" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">过度自信和校准</h2><p id="4a24" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">如果一个分类模型的预测结果概率反映了它们的准确性，那么它就是<em class="lv">校准的</em>。例如，考虑我们数据集中的 100 个例子，每个例子的模型预测概率为 0.9。如果我们的模型被校准，那么 90 个例子应该被正确分类。类似地，在另外 100 个预测概率为 0.6 的例子中，我们预计只有 60 个例子被正确分类。</p><p id="e0ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型校准对于以下方面非常重要</p><ul class=""><li id="2d6d" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">模型的可解释性和可靠性</li><li id="6ccc" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">决定下游应用的决策阈值</li><li id="bfbe" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">将我们的模型集成到集成或机器学习管道中</li></ul><p id="fc75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过度自信的模型没有被校准，它的预测概率总是高于准确性。例如，对于精度仅为 0.6 的输入，它可能会预测 0.9。请注意，测试误差较小的模型仍然可能过于自信，因此可以从标签平滑中受益。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="d304" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">标签平滑公式</h2><p id="24a3" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">标签平滑用<em class="lv"> y_hot </em>和均匀分布的混合代替一个热编码的标签向量<em class="lv"> y_hot </em>:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="64ce" class="md me it nq b gy nu nv l nw nx"><em class="lv">y_ls</em> = (1 - <em class="lv">α</em>) * <em class="lv">y_hot</em> + <em class="lv">α</em> / <em class="lv">K</em></span></pre><p id="2134" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> K </em>是标注类的数量，<em class="lv"> α </em>是确定平滑量的超参数。如果<em class="lv"> α </em> = 0，我们获得原始的一个热码编码的<em class="lv"> y_hot </em>。如果<em class="lv"> α </em> = 1，我们得到均匀分布。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="f082" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">标签平滑的动机</h2><p id="8050" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">当损失函数是交叉熵时，使用标签平滑，并且模型将 softmax 函数应用于倒数第二层的 logit 向量<em class="lv"> z </em>以计算其输出概率<em class="lv"> p </em>。在这种设置中，交叉熵损失函数相对于 logits 的梯度简单地为</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="8103" class="md me it nq b gy nu nv l nw nx">∇CE = <em class="lv">p</em> - <em class="lv">y = </em>softmax(<em class="lv">z</em>)<em class="lv"> - y</em></span></pre><p id="b048" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> y </em>为标签分布。特别是，我们可以看到</p><ol class=""><li id="1a9c" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ny nh ni nj bi translated">梯度下降会尽量使<em class="lv"> p </em>接近<em class="lv"> y </em>。</li><li id="660f" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ny nh ni nj bi translated">梯度限制在-1 和 1 之间。</li></ol><p id="52a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">独热编码标签鼓励将最大可能的 logit 间隙输入 softmax 函数。直觉上，较大的 logit 差距与有界梯度相结合会使模型的适应性降低，并对其预测过于自信。</p><p id="4639" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相比之下，平滑的标注鼓励小的 logit 间隙，如下例所示。[3]显示，这导致更好的模型校准，并防止过度自信的预测。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="016b" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">具体的例子</h2><p id="a72a" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">假设我们有<em class="lv"> K </em> = 3 个类，我们的标签属于第 1 类。设[ <em class="lv"> a </em>，<em class="lv"> b </em>，<em class="lv"> c </em>为我们的 logit 向量。</p><p id="8076" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们不使用标签平滑，标签向量就是独热编码向量[1，0，0]。我们的模型将使<em class="lv"> a </em> ≫ <em class="lv"> b </em>和<em class="lv"> a </em> ≫ <em class="lv"> c </em>。例如，将 softmax 应用于 logit 向量[10，0，0]会得到四舍五入到 4 位小数的[0.9999，0，0]。</p><p id="70d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们使用<em class="lv"> α </em> = 0.1 的标签平滑，平滑后的标签向量≈ [0.9333，0.0333，0.0333]。logit 向量[3.3322，0，0]将平滑后的标签向量近似到 softmax 之后的 4 位小数，并且它具有更小的间隙。这就是为什么我们称标签平滑为正则化技术，因为它限制了最大的 logit 变得比其余的大得多。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="ea73" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">履行</h2><ul class=""><li id="dabc" class="nb nc it lb b lc mw lf mx li nz lm oa lq ob lu ng nh ni nj bi translated">Tensorflow:标签平滑已经在 Tensorflow 的交叉熵损失函数中实现。参见<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy" rel="noopener ugc nofollow" target="_blank">二元交叉熵</a>和<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy" rel="noopener ugc nofollow" target="_blank">分类交叉熵</a>。</li><li id="90ee" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">PyTorch:参见 OpenNMT 中的<a class="ae ky" href="https://github.com/OpenNMT/OpenNMT-py/blob/e8622eb5c6117269bb3accd8eb6f66282b5e67d9/onmt/utils/loss.py#L186" rel="noopener ugc nofollow" target="_blank">示例</a>。</li></ul></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="a3b7" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">常见问题</h2><p id="9c55" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">问:我们什么时候使用标签平滑？</p><p id="24bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答:每当分类神经网络遭受过度拟合和/或过度自信时，我们可以尝试标签平滑。</p><p id="92e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问:我们如何选择<em class="lv"> α </em>？</p><p id="1da5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答:就像其他正则化超参数一样，没有选择<em class="lv"> α </em>的公式。通常通过试错来完成，而<em class="lv"> α = </em> 0.1 是一个很好的起点。</p><p id="f7e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问:我们可以在标签平滑中使用均匀分布以外的分布吗？</p><p id="8f41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答:技术上来说是的。在[4]中，理论基础是为任意分布开发的。也就是说，绝大多数关于标签平滑的实证研究都使用均匀分布。</p><p id="6c29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问:标签平滑在深度学习之外使用吗？</p><p id="d43c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答:不尽然。大多数流行的非深度学习方法不使用 softmax 函数。因此，标签平滑通常不适用。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="0a69" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">进一步阅读</h2><ol class=""><li id="cab9" class="nb nc it lb b lc mw lf mx li nz lm oa lq ob lu ny nh ni nj bi translated">[3]研究了标签平滑的工作方式和原因，提供了一种新的可视化方案，并分析了标签平滑对于不同任务的性能。知识蒸馏的部分尤其有趣。</li><li id="f4be" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ny nh ni nj bi translated">[5]和[4]讨论了标签平滑如何影响损失函数及其与 KL 散度的关系。</li><li id="defd" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ny nh ni nj bi translated">[1]第 7.5.1 章介绍了标签平滑如何帮助处理有噪声的标签。</li><li id="d5b0" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ny nh ni nj bi translated">[2]介绍了<em class="lv">温度缩放</em>，这是一种简单而有效的校准神经网络的方法。</li></ol></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="3524" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">参考</h2><ol class=""><li id="6aed" class="nb nc it lb b lc mw lf mx li nz lm oa lq ob lu ny nh ni nj bi translated">古德菲勒、本吉奥和库维尔。<a class="ae ky" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">深度学习</a> (2016)，麻省理工学院出版社。</li><li id="a080" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ny nh ni nj bi translated">C.郭，g .普莱斯，y .孙，k .温伯格。<a class="ae ky" href="http://proceedings.mlr.press/v70/guo17a/guo17a.pdf" rel="noopener ugc nofollow" target="_blank">关于现代神经网络的校准</a> (2017)，ICML 2017。</li><li id="0ac1" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ny nh ni nj bi translated">R.米勒、s .科恩布利斯和 g .辛顿。<a class="ae ky" href="https://papers.nips.cc/paper/8717-when-does-label-smoothing-help" rel="noopener ugc nofollow" target="_blank">标签平滑何时有帮助？</a> (2019)，NeurIPS <em class="lv"> </em> 2019。</li><li id="17b7" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ny nh ni nj bi translated">G.佩雷拉，g .塔克，j .乔洛夫斯基，凯泽和 g .辛顿。<a class="ae ky" href="https://arxiv.org/abs/1701.06548" rel="noopener ugc nofollow" target="_blank">通过惩罚置信输出分布来正则化神经网络</a> (2017)，arXiv。</li><li id="6bfd" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ny nh ni nj bi translated">C.Szegedy、V. Vanhoucke、S. Ioffe、J. Shlens 和 Z. Wojna。<a class="ae ky" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">重新思考计算机视觉的初始架构</a> (2016)，CVPR，2016。</li></ol></div></div>    
</body>
</html>