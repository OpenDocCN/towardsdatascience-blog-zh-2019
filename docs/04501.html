<html>
<head>
<title>Self Attention and Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自我关注和变形金刚</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-attention-and-transformers-882e9de5edda?source=collection_archive---------11-----------------------#2019-07-11">https://towardsdatascience.com/self-attention-and-transformers-882e9de5edda?source=collection_archive---------11-----------------------#2019-07-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="29e0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从关注到自我关注到变形金刚</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c1d6e5c29ecaf86a81ed0d3c0fbb471f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HSq6ZOP0Wb3ZBO5vjNCf5w.png"/></div></div></figure><p id="123d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这实际上是早先关于“<a class="ae lq" rel="noopener" target="_blank" href="/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda"> <strong class="kw iu">注意力</strong> </a>介绍”的帖子的延续，在那里我们看到了注意力架构所解决的一些关键挑战(并在下面的<em class="lr">图 1 </em>中提到)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ls"><img src="../Images/c79126aec36a5e197672fa31d5e7e796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BF_Pq_laV51sXbgYXoTiyQ.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Fig 1</strong>: From “<a class="ae lq" rel="noopener" target="_blank" href="/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda"><strong class="bd lx">Introduction to Attention</strong></a>”, based on <a class="ae lq" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">paper by Bahdanau et al</a>.</figcaption></figure><p id="11db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一个单独的上下文向量(编码器 RNN 中的最终隐藏状态，保存整个句子/输入序列的含义)的挑战通过用为每个解码器步骤生成的基于注意力的上下文向量来替换来解决，如<strong class="kw iu"> <em class="lr">图 1 </em> </strong>所示。</p><p id="2e80" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是它引入了为解码器的每一步计算单独的上下文向量的增加的计算复杂性的挑战。这超出了现有的与并行化相关的挑战。即 RNNs 中所需的处理的顺序性质——假设需要隐藏状态 h1 来计算下一个隐藏状态 h2，则这些操作不能并行进行。<strong class="kw iu"> <em class="lr">这两种挑战都由图 1 左半部分的红色虚线</em> </strong> <em class="lr">(在图 3 中也称为</em> <strong class="kw iu"> <em class="lr">)表示。</em>T19】</strong></p><p id="ef2a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">说到顺序处理，你可能还会想，假设我们用为每个输出步骤生成的上下文向量来替换最后一个隐藏状态，我们还需要“h”状态吗？毕竟，注意力对齐应该定义给定的输出步骤应该关注输入的哪一部分，而“h”只是“x”的间接表示。它表示所有输入步骤的上下文，直到“x ”,而不仅仅是“x”。直接用“x”不是更有意义吗？</p><p id="c81e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">事实证明确实如此。</p><p id="bc31" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">苏赫巴托尔等人提出的<a class="ae lq" href="https://arxiv.org/pdf/1503.08895.pdf" rel="noopener ugc nofollow" target="_blank">中介绍的端到端存储网络。粘贴在<strong class="kw iu"> <em class="lr">图 2 </em> </strong>只是建议模型的单层版本。所提出的模型具有代表所有输入的“输入记忆”或“关键”向量、模型需要响应的“查询”向量(像最后的解码器隐藏状态)和“值”或“输出记忆”向量——同样是输入的表示。“查询”和“关键字”之间的内积给出了“匹配”(类似于注意力)的概率。通过概率加权的“值”向量的总和给出了最终的响应。虽然产生了良好的结果，但这消除了输入的顺序处理，代之以“内存查询”范例。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ly"><img src="../Images/0ee533223b1e061d73460c6c8600a545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eemIFP2ottymY6mFJMYlJw.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Fig 2</strong>: End to End Memory Networks <a class="ae lq" href="https://arxiv.org/pdf/1503.08895.pdf" rel="noopener ugc nofollow" target="_blank">by Sukhbaatar et al</a></figcaption></figure><p id="ab7c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将此与我们之前看到的基本注意力模型进行比较，就会发现“相似之处”。虽然两者之间存在差异，但“端到端记忆网络”提出了跨句子和多次“跳跃”的记忆来生成输出，我们可以借用“键”、“查询”和“值”的概念来获得我们基本模型的概括视图。<strong class="kw iu"> <em class="lr">图 3 </em> </strong>将这些概念应用于基础模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lz"><img src="../Images/9069c4f3020fe7cbd77f5ad39bacee8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LogUEPvbQ0m-k4Ve5jGCrA.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Fig 3</strong>: Challenges in the attention model from “<a class="ae lq" rel="noopener" target="_blank" href="/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda"><strong class="bd lx">Introduction to Attention</strong></a>” based on <a class="ae lq" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">paper by Bahdanau et al</a> to Transformers</figcaption></figure><p id="2e57" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">图 3 </strong>还强调了我们想要解决的两个挑战。对于挑战#1，我们也许可以直接用输入(x)代替隐藏状态(h)作为密钥。但是如果我们直接使用单词嵌入，这就不是丰富的表示了。端到端的记忆网络对输入和输出记忆表示使用不同的嵌入矩阵，这更好，但它们仍然是单词的独立表示。将其与隐藏状态(h)进行比较，隐藏状态(h)不仅代表单词，还代表给定句子上下文中的单词。</p><p id="6290" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有没有一种方法可以消除生成隐藏状态的顺序性，但仍然产生更丰富的上下文表示向量？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ma"><img src="../Images/081ec94de6ce49340a3412adec6852f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aqz17i-Mr5zMUPB_GQyQ5Q.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Fig 4</strong>: Sequence to Sequence with self attention — Moving from being “RNN based” to “attention only”</figcaption></figure><p id="5e6c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="lr">图 4 </em> </strong>说明了这样做的一种可能方式。如果我们不使用注意力来连接编码器和解码器，而是分别在编码器和解码器内部使用注意力，会怎么样？毕竟，注意力是一种丰富的表示——因为它考虑了所有的键和值。因此，我们可以使用基于注意力的替换，而不是使用 RNN 来导出隐藏状态，其中输入(x)被用作“键”和“值”。(如下面的<strong class="kw iu"> <em class="lr">图 5 </em> </strong>所示，用“x”代替“h”)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mb"><img src="../Images/e821a80c3499bad9b87530482f106d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x-8ZifrNloIYgaSkIwOuPA.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Fig 5</strong>: Self Attention</figcaption></figure><p id="0907" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在编码器端，我们可以使用自我关注来生成给定输入步长 xi 相对于输入 x1，x2…xn 中所有其他项的更丰富的表示。与基于 RNN 的编码器中的隐藏状态生成不同，这可以对所有输入步骤并行完成。我们基本上是将<strong class="kw iu"> <em class="lr">图 4 </em> </strong>左半部分的十字交叉线向下移动，如右半部分所示，从而消除代表向量之间的红色虚线。</p><p id="afaa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在解码器方面，我们可以做一些类似的事情。我们将基于 RNN 的解码器替换为基于注意力的解码器。即不再有隐藏状态，并且不再为每个解码器步骤计算单独的上下文向量。相反，我们对迄今为止生成的所有输出进行自我关注，并随之消耗整个编码器输出。换句话说，我们正在将注意力应用到目前为止我们所知道的任何事物上。(注意，变压器中绝对不会出现这种情况，变压器中对生成输出和编码器输出的关注是在两个独立的层中一个接一个地完成的)。</p><p id="0e8a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">瓦斯瓦尼等人在论文<a class="ae lq" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的全部”中介绍的“变形金刚”模型，见下面的<strong class="kw iu"> <em class="lr">图 6 </em> </strong>，做了我们上面讨论的事情。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/cf62441706a586c9ea96b59605c730ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9nUzdaTbKzJrAsq1qqJNNA.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Fig 6:</strong> From <a class="ae lq" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">“Attention Is All You Need” by Vaswani et al.</a></figcaption></figure><p id="dd75" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Transformer 模型使用“缩放点积”注意机制。它在图 6 的右侧以及图 7 中示出。比较图 7 和图 1，感受一下这两种模型在计算注意力的“方式”上的差异。(注意:“在哪里”也不同，我们将在下一步讨论)。transformer 模型还使用了所谓的“多头注意力”(Multi-Head Attention)，而不是为给定的“xi”只计算一个“ai”(注意力)，而是使用不同的 Ws 集计算多个注意力分数“ai”。这允许模型在不同的位置关注不同的“表示子空间”，类似于使用不同的过滤器在 CNN 的单个层中创建不同的特征地图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi md"><img src="../Images/52934baff34186c0c672c4db45d7fad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a2Od2qLE757ssFGkC1ym5A.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Fig 7:</strong> Scaled Dot Product used in <a class="ae lq" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">“Attention Is All You Need” by Vaswani et al.</a></figcaption></figure><p id="44f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">编码器</strong></p><p id="b428" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所提出的变压器模型中的编码器具有多个“编码器自关注”层。每一层的构造如下:</p><ol class=""><li id="d4bb" class="me mf it kw b kx ky la lb ld mg lh mh ll mi lp mj mk ml mm bi translated">输入将是第一层的单词嵌入。对于后续层，它将是前一层的输出。</li><li id="3402" class="me mf it kw b kx mn la mo ld mp lh mq ll mr lp mj mk ml mm bi translated">在每一层中，首先使用该层的输入作为键、查询和值来计算多头自我关注。</li><li id="e65a" class="me mf it kw b kx mn la mo ld mp lh mq ll mr lp mj mk ml mm bi translated">#2 的输出被发送到前馈网络层。这里，每个位置(即每个单词表示)都通过相同的前馈来馈送，该前馈包含两个线性变换，后跟一个 ReLU(输入向量-&gt;线性变换的隐藏 1-&gt;线性变换的隐藏 2 -&gt;ReLU 输出)。</li></ol><p id="84d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">解码器</strong></p><p id="422d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">解码器也将具有多层。每一层的构造如下:</p><ol class=""><li id="6ba6" class="me mf it kw b kx ky la lb ld mg lh mh ll mi lp mj mk ml mm bi translated">输入将是到目前为止为第一层生成的单词嵌入。对于后续层，它将是前一层的输出。</li><li id="31ae" class="me mf it kw b kx mn la mo ld mp lh mq ll mr lp mj mk ml mm bi translated">在每一层内，首先使用该层的输入作为关键字、查询和值(即，到目前为止生成的解码器输出，为其余位置填充)来计算多头自我关注。</li><li id="1f4f" class="me mf it kw b kx mn la mo ld mp lh mq ll mr lp mj mk ml mm bi translated">#2 的输出被发送到“多头-编码器-解码器-注意”层。这里，使用#2 输出作为查询，使用编码器输出作为键和值，来计算另一个注意。</li><li id="89f4" class="me mf it kw b kx mn la mo ld mp lh mq ll mr lp mj mk ml mm bi translated">#3 的输出被发送到类似编码器中的位置式前馈网络层。</li></ol><p id="87bb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">虽然摆脱顺序性在很多方面都有帮助，但它有一个关键的优势——知道单词在输入序列中的顺序。如果没有它，出现在同一个句子中不同位置的同一个单词可能会以相同的输出表示结束(因为它将具有相同的键、值等)。因此，该模型使用“位置编码”——基本上是一个表示位置的向量，它被添加到编码器和解码器堆栈底部的输入嵌入中。Shaw 等人的另一篇<a class="ae lq" href="https://arxiv.org/pdf/1803.02155.pdf" rel="noopener ugc nofollow" target="_blank">论文在这里</a>提出了一种基于相对位置的替代方案，它比原始 Transformer 模型论文中建议的绝对位置编码实现了更好的结果——如果您能花时间在位置嵌入上，我建议您研究一下。</p><p id="83d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">希望这在某些方面有所帮助。</p></div></div>    
</body>
</html>