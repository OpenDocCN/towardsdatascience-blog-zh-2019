<html>
<head>
<title>Neural Networks: For beginners. By beginners.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:初学者用。由初学者。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-for-beginners-by-beginners-6bfc002e13a2?source=collection_archive---------9-----------------------#2019-10-09">https://towardsdatascience.com/neural-networks-for-beginners-by-beginners-6bfc002e13a2?source=collection_archive---------9-----------------------#2019-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8968" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个足智多谋的初学者指南神经网络的本质和实施</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/168628410b75ea1c69e1480044c78491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Fszmx--kO6gtMA5Y"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@bitcloudphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Bit Cloud</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c6a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">挺住！为什么要看一个初学者写的文章？答案很简单——我决定写一篇关于神经网络的文章，它是用一种简单的语言写的，即使像我这样的初学者也能理解，同时也足够足智多谋，可以帮助某人很好地掌握这一庞大的材料。</em></p><h2 id="e55e" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">先决条件</h2><p id="61aa" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">您需要具备以下方面的基本知识:</p><ul class=""><li id="77c1" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">线性代数</li><li id="9ea5" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">计算机编程语言</li><li id="1edb" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">NumPy</li></ul><p id="8e45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你不需要擅长这些，但是如果你以前用过的话会容易得多。</p><h2 id="62a6" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">密码</h2><p id="c427" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我在整篇文章中放了你需要的每一段代码，但是如果你想拥有整段代码，这里有<strong class="ky ir"> Jupyter 笔记本</strong>:</p><div class="nf ng gp gr nh ni"><a href="https://github.com/KrumovAI/Neural-Networks-From-Scratch/blob/master/neural_network.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">KrumovAI/从头开始的神经网络</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">github.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw kp ni"/></div></div></a></div><h2 id="0cb4" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">介绍</h2><p id="9d42" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">神经网络。这是大多数普通编码人员在听到人工智能和/或机器学习这些流行语时首先想到的。虽然不是书中最基本的材料，但如果用初学者友好的语言来解释，它实际上是一个不错的起点。</p><p id="6287" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在整篇文章中，我将带您开始一段旅程，从神经网络思想的最开始开始，带您通过使其学习的核心现代原则，最后向您展示一个从头开始的神经网络模型的一步一步的实现，其特点是完全连接、激活、扁平化、卷积和池层。这个实现很大程度上基于并受到了<a class="nx ny ep" href="https://medium.com/u/c215fdc67eb?source=post_page-----6bfc002e13a2--------------------------------" rel="noopener" target="_blank"> Omar Aflak </a>的这篇令人惊叹的文章的启发，这篇文章是每个想要了解更多关于神经网络的数学背景的人的必读之作。</p><div class="nf ng gp gr nh ni"><a href="https://medium.com/datadriveninvestor/math-neural-network-from-scratch-in-python-d6da9f29ce65" rel="noopener follow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">Python 中从头开始的数学和神经网络</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">制作自己的机器学习库。</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">medium.com</p></div></div><div class="nr l"><div class="nz l nt nu nv nr nw kp ni"/></div></div></a></div><h1 id="9e91" class="oa lu iq bd lv ob oc od ly oe of og mb jw oh jx me jz oi ka mh kc oj kd mk ok bi translated">理解神经网络</h1><p id="dce1" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">神经网络的历史可以追溯到 1943 年，当时神经生理学家沃伦·麦卡洛克和数学家沃尔特·皮茨描绘了一个带有简单电子电路的人脑神经元模型，该电路采用一组输入，将它们乘以加权值，并使它们通过一个阈值门，该阈值门根据阈值给出 0 或 1 的输出值。这个模型被称为麦卡洛克-皮茨感知器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/0a646a58d28816d1a1de7719ef06ba14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*dY1EIA2eqe0bv6XL.gif"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">McCulloch-Pitts perceptron | Source: <strong class="bd om">Wikimedia Commons</strong></figcaption></figure><p id="3d63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一位名叫罗森布拉特的心理学家进一步发展了这一想法，他创建了感知器的数学模型，并将其命名为 Mark I 感知器。它基于麦卡洛克-皮茨模型，是让机器学习的最初尝试之一。感知器模型也接受一组二进制输入，然后乘以加权值(代表突触强度)。然后添加一个值通常为 1 的偏置(该偏置确保使用相同的输入可以计算更多的函数),并再次基于阈值将输出设置为 0 或 1。上面提到的输入要么是输入数据，要么是其他感知器的输出。</p><p id="8334" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然麦卡洛克-皮茨模型在当时是一项开创性的研究，但它缺乏良好的学习机制，这使得它不适合人工智能领域。</p><p id="be9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">罗森布拉特从唐纳德·赫布(Donald Hebb)的论文中获得灵感，即学习通过神经元之间突触的形成和变化发生在人脑中，然后提出了以自己的方式复制它的想法。他想到了一种感知器，这种感知器接受一组输入-输出示例的训练集，并通过改变感知器的权重来形成(学习)一个函数。</p><p id="d67f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实施分四步进行:</p><ol class=""><li id="5f21" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr on mx my mz bi translated">用随机权重初始化感知器</li><li id="0730" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr on mx my mz bi translated">对于训练集中的每个示例，计算输出</li><li id="5f46" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr on mx my mz bi translated">如果输出应该是 1，但却是 0，则用输入 1 增加权重，反之亦然-如果输出是 1，但应该是 0，则用输入 1 减少权重。</li><li id="8790" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr on mx my mz bi translated">对每个示例重复步骤 2-4，直到感知器输出正确的值</li></ol><p id="4339" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这套指令是现代感知机的基础。然而，由于计算能力的显著提高，我们现在可以与更多的感知器一起工作，形成一个神经网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/8889f9ff17b3b94bb8177f4adcd919a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/0*TQraR6-z5XotpgT9.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Source: <strong class="bd om">mlxtend</strong></figcaption></figure><p id="abb2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，它们并不只是随机放在网络中，而是实际上是另一个构建块(层)的一部分。</p><h2 id="783e" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">层</h2><p id="27bb" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">一个层由感知器组成，这些感知器连接到前一层和下一层的感知器，如果这样的感知器确实存在的话。每一层都定义了自己的功能，因此服务于自己的目的。神经网络由输入层(获取初始数据)、输出层(返回网络的整体结果)和隐藏层(一个或多个具有不同大小(感知器数量)和功能的层)组成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/c611c8e17867db74ced0ff3bd6337770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QBtmviSCGnAUbyvF.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Source: <strong class="bd om">cs231n.github.io</strong></figcaption></figure><p id="307d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了使网络能够学习并产生结果，每一层必须实现两个功能— <strong class="ky ir">正向传播</strong>和<strong class="ky ir">反向传播</strong>(简称为<strong class="ky ir">反向传播</strong>)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Base Layer Class</figcaption></figure><p id="e955" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想象一列火车行驶在 A 点(输入)和 B 点(输出)之间，每次到达其中一个点时都会改变方向。从 A 到 B 的过程从输入层提取一个或多个样本，并将其连续通过所有隐藏层的前向传播函数，直到到达 B 点(并产生结果)。反向传播基本上是相同的事情，只是方向相反-本课程以相反的顺序通过所有图层的反向传播方法获取数据，直到数据到达点 a。但这两个课程的不同之处在于这些方法内部发生的情况。</p><p id="f119" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正向传播只负责通过函数运行输入并返回结果。没有学习，只有计算。反向传播有点棘手，因为它负责做两件事:</p><ul class=""><li id="1bbb" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">更新层的参数，以提高正向传播方法的准确性。</li><li id="237b" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">实现正向传播函数的导数并返回结果。</li></ul><p id="335b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么这到底是怎么发生的，为什么会发生。谜团在 B 点解开——在火车改变方向并通过所有层的反向传播之前。为了调整我们的模型，我们需要回答两个问题:</p><ul class=""><li id="0c02" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">模型的结果与实际产出相比有多好？</li><li id="524b" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">我们如何最小化这种差异？</li></ul><p id="9a1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回答第一个问题的过程被称为计算误差。为此，我们使用<strong class="ky ir">成本函数</strong>(与<strong class="ky ir">损失函数</strong>同义)。</p><h2 id="ec7f" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">成本函数</h2><p id="ad12" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">有不同类型的成本函数进行完全不同的计算，但都服务于相同的目的-显示我们的模型离实际结果有多远。选择成本函数与模型的目的密切相关，但在本文中，我们将只使用最流行的一种方法——均方误差(MSE)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/336481962dd3738b0d69b4a9185171bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lQR_voMCGQDQ0U_Q.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Formula for <strong class="bd om">Mean Squared Error</strong>(<strong class="bd om">MSE</strong>) | Source: <strong class="bd om">DataQuest.io</strong></figcaption></figure><p id="1a12" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个非常简单的函数——我们对实际输出和模型输出之间的差的平方求和，然后计算平均值。但是帮助我们的模型只实现 MSE 不会有任何显著的帮助。我们也必须实现它的衍生物。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="e4c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是我们为什么需要这个呢？因为臭名昭著的…</p><h2 id="7cdb" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">梯度下降</h2><p id="aa1d" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">这里我们需要做的最后一件事是向我们的模型展示如何最小化误差。为此，我们需要一个<strong class="ky ir">优化算法(optimizer) </strong>。同样，有许多种类的优化器都服务于相同的目的，但为了保持简单但仍然有意义的事情，我们将使用最广泛使用的一个，许多其他优化算法的基础。看这强大的<strong class="ky ir">梯度下降</strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/83f8c7c4994b63c24ecd8b0097998666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qWmMFtC51-XISagj.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Graphical representation of <strong class="bd om">Gradient Descent</strong> | Source: <strong class="bd om">Medium</strong></figcaption></figure><p id="25f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看起来不像听起来那么可怕，对吧？好消息，这是一个相对简单的概念。根据定义，<strong class="ky ir">梯度</strong>是一个有趣的词，表示导数，或者函数的变化率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/532916baf63d0d3512a46321e2ee23ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IjZ0Jdkw1y16THBw.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">3D representation of <strong class="bd om">Gradient</strong> | Source: <strong class="bd om">OReilly</strong></figcaption></figure><p id="cbcb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们的模型是一个球。该表面表示误差的梯度(导数)。我们希望球滚下表面(<strong class="ky ir">下降</strong>)尽可能低，以降低高度(误差)。从数学层面来说——我们需要达到一个全局(或者至少是一个足够好的局部)最小值。</p><p id="9a4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了让球移动，我们需要以一定的速率更新我们的参数——称为<strong class="ky ir">学习速率</strong>。这是一个预定义的参数，我们在运行模型之前将它传递给我们的模型。这种参数被称为超参数，在我们的模型性能中有着巨大的作用。我的意思是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/d641c047ac30b29ab524a4ab2ad1801a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gXaauM2xrsoa7vu9.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Significance of <strong class="bd om">Learning Rate</strong> | Source: <strong class="bd om">analyticsvidhya.com</strong></figcaption></figure><p id="957c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们选择一个太大的学习率，参数将会剧烈变化，我们可能会跳过最小值。另一方面，如果我们的学习率太小，那么要达到令人满意的结果将花费太多的时间和计算能力。这就是为什么通过用不同的学习率值测试模型来调整这个参数是相当重要的。强烈建议从 0.1 或 0.01 的学习率开始，并从那里开始调优。</p><h2 id="7465" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">背靠背(传播)</h2><p id="2696" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">现在，我们需要通过将适当的数据传递给反向传播方法来逐层更新模型的参数。反向传播有两个参数——输出误差和学习速率。输出误差的计算方式可以是成本函数的导数，也可以是前一层反向传播的结果(从 B 点到 A 点)，如上所述，反向传播应该给出正向传播函数的导数。通过这样做，每一层向它的前一层显示它的错误。</p><p id="624a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，如果出于某种原因，我们有一个正弦层，它看起来会像这样:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="ab23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经得到了所需的两个参数，反向传播应该更新层权重(如果存在的话)。由于每种类型的层都是不同的，因此它定义了自己的参数调整逻辑，这一点我们稍后会谈到。</p><h2 id="5dda" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">结束梯度下降</h2><p id="fa70" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">当每一层的反向传播完成，我们的列车到达 A 点时，它获取下一个样本(或样本集)，并再次通过隐藏层的前向传播函数开始它的过程——只是这一次，它们应该表现得更好一点。这个过程不断继续，直到训练完成和/或达到最小误差。</p><p id="7c5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经解释了梯度下降背后的所有理论，下面是它在代码中的样子:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="4fb1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这段代码片段能对算法本身给出更多的解释。唯一我们还没有完全覆盖的是我们可以在网络中使用什么类型的层以及如何实现它们。</p><h1 id="f76e" class="oa lu iq bd lv ob oc od ly oe of og mb jw oh jx me jz oi ka mh kc oj kd mk ok bi translated">基本层</h1><p id="dba8" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">对于初学者来说，似乎有很多种层可供选择，臭名昭著的全连接层无疑是最佳选择。</p><h2 id="8b41" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">全连接的</h2><p id="1702" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">全连接层是使用最广泛的类类型。其工作原理基于罗森布拉特模型，具体如下:</p><ol class=""><li id="d90e" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr on mx my mz bi translated"><strong class="ky ir">来自<strong class="ky ir">前</strong>层的每一个感知器</strong>都链接到<strong class="ky ir">这</strong>层的<strong class="ky ir">每一个感知器</strong>。</li><li id="943d" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr on mx my mz bi translated">每个环节都有一个<strong class="ky ir">加权值(weight)。</strong></li><li id="9622" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr on mx my mz bi translated">一个<strong class="ky ir">偏差</strong>被添加到结果中。</li><li id="0945" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr on mx my mz bi translated">该层的<strong class="ky ir">权重</strong>保存在一个 2D 数组中，大小为<strong class="ky ir"><em class="ls">【m】</em></strong><em class="ls">x</em><strong class="ky ir"><em class="ls">n</em></strong>(其中<strong class="ky ir"> <em class="ls"> m </em> </strong>是前一层的感知器数量，<strong class="ky ir"> <em class="ls"> n </em> </strong>是该层的感知器数量)。它们被初始化为随机值。</li><li id="8ad0" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr on mx my mz bi translated">该层的<strong class="ky ir">偏差</strong>保存在大小为<strong class="ky ir"> <em class="ls"> n </em> </strong>的 1D 数组中。它被初始化为随机值。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/11cfd1ef61e7c9f62b61e92dbb4275eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*gKMRrwyfSlT1Yszn.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Visual representation of <strong class="bd om">Fully-Connected(FC) Layer</strong> | Source: <strong class="bd om">cs231n.github.io</strong></figcaption></figure><p id="4951" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们来看看我们的实现:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="42dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如你所看到的，只要你知道基本的线性代数，我们的 to 方法的实现并不复杂。虽然相对简单，但这是一个完全有用且优化的层实现，我们将很容易在以后使用它。</p><p id="40ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而全连接层的唯一问题是它们是线性的。事实上，大多数层都有完全线性的逻辑。线性函数是一次多项式。仅使用这样的函数妨碍了模型学习复杂函数映射的能力，因此，学习是有限的。这就是为什么(按照惯例)在每个使用<strong class="ky ir">激活层</strong>的线性层之后添加非线性功能是好的。</p><h2 id="5f02" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">活化层</h2><p id="ca23" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">激活层与任何其他类型的层一样，只是它们没有权重，而是对输入使用非线性函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="de82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种<strong class="ky ir">激活函数</strong>的一个很好的例子是代表<strong class="ky ir">双曲正切的<strong class="ky ir"> tanh </strong>。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/68a2fa1662e96869a542da1e5fbf3321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5-8RGKbk7rYYjAk2.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd om">Tanh </strong>compared to <strong class="bd om">sinh </strong>and <strong class="bd om">cosh</strong> | Source: <strong class="bd om">Wikipedia</strong></figcaption></figure><p id="c598" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们在开始构建模型时会用到它，所以我们需要实现它:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="c4e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经实现了最重要的两层，让我们继续实现整个<strong class="ky ir">神经网络</strong>类。</p><h1 id="acf9" class="oa lu iq bd lv ob oc od ly oe of og mb jw oh jx me jz oi ka mh kc oj kd mk ok bi translated">神经网络实现</h1><p id="0445" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">有几种方法需要实现</p><ul class=""><li id="3560" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">一个构造函数——这里我们需要传递超参数(学习率和历元数——我们的网络在输入数据集上运行的次数)；必要字段的初始化</li><li id="46b7" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">添加层方法—传递层的实例；用于模型构建；可以(应该)用几次才能加几层；</li><li id="ebf7" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">使用成本函数方法-指定训练模型时要使用的成本函数</li><li id="5f6a" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">拟合方法——执行训练过程的方法的标准名称；这里是我们放置之前的渐变下降片段的地方</li><li id="b94e" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">预测方法—仅用于计算结果的方法的标准名称；一旦训练过程完成，它是有用的</li></ul><p id="ec5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代码是这样的:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="36e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可能已经注意到在每个方法的末尾都出现了<code class="fe oy oz pa pb b">return self</code>语句。我这样做的原因是它允许我们做<strong class="ky ir">方法链接</strong>。如果你不确定我在说什么，你马上就会看到一个很好的例子。</p><p id="e2da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们将它付诸实践。我们将使用<strong class="ky ir"> MNIST </strong>数据库对手写数字进行分类。可以从<a class="ae kv" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">这里</a>下载，也可以从<strong class="ky ir"> Keras </strong>轻松导入。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="8c09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为像素值表示在范围[0；255]，我们将把它缩小到范围[0.0，1.0]。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="dd4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们做的另一件事是让<strong class="ky ir"> y </strong>(结果)变得更方便一点(注意<strong class="ky ir">keras . utils . to _ categorial)</strong>。它的作用是在一个<strong class="ky ir">独热向量</strong>中表示数值结果:</p><p id="f309" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">5 =&gt;【0，0，0，0，0，1，0，0，0，0，0】</strong></p><p id="b753" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这很有帮助，因为我们网络的输出层将由 10 个节点组成，每个节点保存一个输出值。由于理想情况下的输出是正确的独热向量，因此成本函数现在更容易完成其工作。</p><p id="d385" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们通过放置一些 FC 和激活层来构建我们的第一个网络:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="bc4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们只使用前 1000 个样本的原因是，如果我们使用所有的样本，运行时间会太长。在这种情况下，使用更多的样本会得到更好的结果，所以如果有时间，可以尝试更大的范围。</p><p id="9493" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们运行这个之前，我们需要再添加一个最后的触摸——另一个预输出激活函数— <strong class="ky ir"> Softmax </strong>。它所做的是对一个概率分布数组中有<em class="ls"> n </em>个元素的数组进行归一化，该数组由与输入数字的指数成比例的<em class="ls"> n </em>个概率组成，或者简单地说，计算样本匹配某个类别的概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/9b2d7ffc0c4ac687c1bb2264b3f81b3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/0*JJyPQsmvH5nq48xx.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd om">Softmax </strong>formula | Source: <strong class="bd om">hackernoon.com</strong></figcaption></figure><p id="33f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以及实现:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="40fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们再试一次，只是这一次我们将有<strong class="ky ir"> Softmax </strong>激活作为我们的最后一层。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="5061" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经训练了我们的数据，让我们评估我们的最终模型。</p><h1 id="58d5" class="oa lu iq bd lv ob oc od ly oe of og mb jw oh jx me jz oi ka mh kc oj kd mk ok bi translated">估价</h1><p id="841c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">请记住，我们已经实现了一个用于教育目的的小型模型。它不会产生很高的结果。我强烈建议用它来玩，以获得更好的准确性。</p><p id="f671" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了评估我们的结果，我们将使用一个来自<strong class="ky ir"> sklearn </strong>的简单实用程序，它显示了我们模型的准确性。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="6a26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在你知道了神经网络构建的基础，我们可以继续学习更高级的东西…</p></div><div class="ab cl pd pe hu pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="ij ik il im in"><h1 id="1c4e" class="oa lu iq bd lv ob pk od ly oe pl og mb jw pm jx me jz pn ka mh kc po kd mk ok bi translated">卷积神经网络(CNN)</h1><p id="dc9d" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">使用 FC 层执行图像识别任务的主要问题是，它将每个像素作为一个单独的特征，这似乎不太正确，因为神经网络的目标是复制人脑。帮助我们的大脑识别视觉输入的不仅仅是随机的点，而是点的模式。这就是所谓的<strong class="ky ir">卷积</strong>和<strong class="ky ir">相关</strong>过程派上用场的地方。</p><h2 id="433d" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">卷积和相关</h2><p id="bd9e" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">图像处理中的卷积和相关几乎是相同的过程，只是在实现上有一点小小的不同。两者都取一幅图像<strong class="ky ir"> <em class="ls"> I </em> </strong>，基于另一个矩阵产生一个矩阵<strong class="ky ir"> <em class="ls"> O </em> </strong>(称为<strong class="ky ir">滤镜</strong>或<strong class="ky ir">特征图</strong> ) <strong class="ky ir"> <em class="ls"> F </em> </strong>。我的意思是</p><p id="b414" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">先说<strong class="ky ir">关联:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pp"><img src="../Images/96aac4568cdca70055b3da4a3876a9c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e141IXE2sRB8ODKzFZ4-SQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Visual representation of <strong class="bd om">Correlation</strong> | Source: <strong class="bd om">My Late Night PowerPoint Creations</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pq"><img src="../Images/e93ca6b120d90b81fa2d25c730512902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YRSq0T_0qqvSmkMUkT1XDg.png"/></div></div></figure><p id="66a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们解释一下发生了什么。过滤器就像一个<strong class="ky ir">滑动窗口</strong>，一步一步地穿过数组，向右滑动直到到达行的末尾。然后向下滑动，从行的开始处开始。在每个位置，我们对滤波器<strong class="ky ir"> <em class="ls"> F </em> </strong>和输入图像<strong class="ky ir"> <em class="ls"> I </em> </strong>的相应窗口的点积求和。每个位置代表输出<strong class="ky ir"><em class="ls"/></strong><em class="ls">中的一个单元格。</em></p><p id="c00c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么<strong class="ky ir">卷积</strong>呢？</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pr or l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Source: <strong class="ak">Giffy</strong></figcaption></figure><p id="0f99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">相关</strong>和<strong class="ky ir">卷积</strong>的唯一区别是<strong class="ky ir">卷积</strong>与同一个滤波器<strong class="ky ir"> <em class="ls"> F </em> </strong>一起工作，只是旋转了 180 度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/efcea14b46c9926eeeee7c6b24367989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*I13xX7AhVLxBsFvDSe9GWw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Visual representation of <strong class="bd om">Convolution</strong> | Source: <strong class="bd om">My Late Night PowerPoint Creations</strong></figcaption></figure><h2 id="4a7e" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">但是这对我们有什么帮助呢？</h2><p id="f23a" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">通过使用这种过滤器，通过我们的图像，我们提取的不仅仅是一个像素，而是整个像素区域，使我们的模型能够映射更复杂的图像特征，如线条和形状。</p><p id="b199" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果你给一个卷积层一个猫的图像，它将能够识别像鼻子和耳朵这样的小特征。然后通过添加更多的卷积层，它可以识别更大尺度的特征，如头部或尾部。</p><p id="dd38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其工作方式是每个卷积层有几个滤波器，都用随机数初始化。这背后的目标是让每个过滤器激活不同的特征(一个特征训练检测鼻子，另一个特征检测眼睛，等等。).</p><p id="e764" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">通常在处理</em> <strong class="ky ir"> <em class="ls">多通道图像(如 RGB) </em> </strong> <em class="ls">时，您的滤镜将是三维的，深度等于图像通道数。这样你就只关注 2D 进程了。</em></p><p id="19cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">填充和步幅</strong></p><p id="1639" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">卷积和相关的问题是角点像素没有得到太多的关注，它们只被处理一次。如果你想改变这一点，你可以添加一个<strong class="ky ir">填充。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/158ac645837ec35339bbfdff50d552b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*6dzeKWErI0zIMrkklVvkXA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Visual representation of <strong class="bd om">Padding</strong> | Source: <strong class="bd om">My Late Night PowerPoint Creations</strong></figcaption></figure><p id="e4d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，即使是角落也会像图像的其他部分一样被处理。</p><p id="5e48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，有时我们不想对每个像素都给予太多的关注(在处理较大的图像时非常适用)。让每个过滤器通过每个可能的位置有时是多余的。这就是为什么我们可以配置一个<strong class="ky ir">步幅。</strong></p><p id="78ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步幅告诉我们的过滤器在到达下一个位置时应该滑动多少。步幅为 2 将在每次移位时跳过一个位置，这减小了输出的大小并使得计算量更小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pu"><img src="../Images/799d02a411559b6cd41439a69970f67e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dfjm8E8Fvvv0khV4nYBCdQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Visual representation of <strong class="bd om">Stride</strong> | Source: <strong class="bd om">My Late Night PowerPoint Creations</strong></figcaption></figure><p id="841f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">没有步幅基本上意味着步幅为 1——过滤器覆盖了所有可能的位置而没有跳跃。</em></p><p id="cbc4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实施</strong></p><p id="4c34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先让我们列出我们的参数:</p><ul class=""><li id="b0c3" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><strong class="ky ir">滤镜数量</strong>——图层中有多少个滤镜</li><li id="99bd" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">过滤器尺寸——我们的过滤器会有多大</li><li id="1eff" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">填充</strong> —我们将有多少零填充(我们将对这个使用元组格式<strong class="ky ir"> <em class="ls"> (x，y) </em> </strong>，其中<strong class="ky ir"> <em class="ls"> x </em> </strong>代表列零填充，<strong class="ky ir"> <em class="ls"> y </em> </strong>代表行零填充)。</li><li id="cfed" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">步幅</strong> —只是一个简单的数字，显示一个滤镜每张幻灯片应该移动多少个位置</li></ul><p id="1b40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls"> NB！</em> </strong> <em class="ls">因为我们要用随机值初始化我们的过滤器，所以使用</em> <strong class="ky ir"> <em class="ls">相关性</em> </strong> <em class="ls">或</em> <strong class="ky ir"> <em class="ls">卷积</em> </strong> <em class="ls">实际上没有区别，所以为了简单起见，我们将使用相关性。</em></p><p id="2543" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里有一个非常迭代(for-loopy)的方法来实现卷积层:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="1f6a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然有点容易理解，但它太慢了，因为它缺乏<strong class="ky ir">矢量化</strong>(使用 for 循环而不是基于数组的操作)。这就是为什么我四处挖掘，找到了一些非常有用的库方法，帮助我优化我的层:</p><ul class=""><li id="256f" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><a class="ae kv" href="https://scikit-image.org/docs/dev/api/skimage.util.html#skimage.util.view_as_windows" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">skim age . util . view _ as _ windows</strong></a>—我用它来自动获取我的滤镜应该去的位置，而不是自己循环遍历它们。</li><li id="28aa" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><a class="ae kv" href="https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.tensordot.html" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">NP . tensordot</strong></a><strong class="ky ir">——</strong>我用它把窗口(我用上面提到的方法得到的)和滤镜相乘。</li></ul><p id="fc4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不幸的是，我找不到一种简单的方法来对<strong class="ky ir"> in_error </strong>的计算进行矢量化(我得到的最接近的方法是使用<a class="ae kv" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">scipy . signal . convolve _ 2d</strong></a><strong class="ky ir">，</strong>，但它不适用于 stride)，直到我意识到矢量化操作通常用于用低级语言编写的循环，所以我使用<strong class="ky ir"> Cython </strong>(一个库，它允许</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="4c60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更多关于<strong class="ky ir"> Cython </strong>的信息，请访问<a class="ae kv" href="https://cython.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">他们的文档</a>。</p><p id="49d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经有了所有的拼图，让我们组装我们的<strong class="ky ir">优化</strong>卷积层:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="201c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">说到优化，我们将看到的下一组层将帮助我们避免 GPU 融化。</p><h1 id="6217" class="oa lu iq bd lv ob oc od ly oe of og mb jw oh jx me jz oi ka mh kc oj kd mk ok bi translated">池层</h1><p id="2dfc" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">合并图层的目的是减少先前卷积图层输出的空间大小。这有助于我们的模型，原因有二:</p><ol class=""><li id="b18c" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr on mx my mz bi translated">它降低了所需的计算能力。</li><li id="1f8d" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr on mx my mz bi translated">它创建输入的较低分辨率版本。这个过程也被称为<strong class="ky ir">下采样</strong>。这有所帮助的原因是卷积层的过滤器通常被绑定到图像中的准确位置。具有小的移动或失真可能导致不期望的不同输出。下采样图像仍然包含较大的结构特征，只是排除了可能妨碍模型性能的精细细节。</li></ol><p id="1e00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">一种可以用来解决这个问题的</em> <strong class="ky ir"> <em class="ls">下采样</em> </strong> <em class="ls">的替代方法是首先在</em> <strong class="ky ir"> <em class="ls">卷积层</em> </strong> <em class="ls">上使用一个</em> <strong class="ky ir"> <em class="ls">更大的步幅</em> </strong> <em class="ls">。</em></p><p id="d377" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在大多数 CNN 模型中遇到的通常模式如下:</p><ul class=""><li id="2af5" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">卷积层</li><li id="8121" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">活化层</li><li id="c9d3" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">汇集层</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pv"><img src="../Images/450936605094bafe19fa358aa0898673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wOpHsEQs7DKTL6E7.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">A typical <strong class="bd om">CNN pattern</strong> | Source: <strong class="bd om">jefkine.com</strong></figcaption></figure><h2 id="28f9" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">工作原理</h2><p id="f831" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><strong class="ky ir">汇集</strong>的工作方式有点类似于<strong class="ky ir">卷积</strong>和<strong class="ky ir">相关</strong>。我们又一次有了一个窗口(称为<strong class="ky ir">池</strong>)滑过我们的输入。然而<strong class="ky ir">池</strong>不包含任何像<strong class="ky ir">过滤器/特征映射</strong>那样的数据。当一个池移动到一个位置时，它仅根据该位置的值计算结果(如<strong class="ky ir">平均值</strong>或<strong class="ky ir">最大值</strong>)。</p><p id="352a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了帮助您理解它是如何工作的，我们将实现最广泛使用的类型— <strong class="ky ir"> Max Pooling Layer </strong>。</p><h2 id="63a4" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">最大池层</h2><p id="66d3" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">想法很简单—当池滑动到某个位置时，该位置的最大值存储在输出中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pw"><img src="../Images/cd3ec7698ddb6716063220120ae247ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*alTwc3n9zv22uWl5VLutUw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Visual representation of <strong class="bd om">Max Pooling</strong> | Source: <strong class="bd om">My Late Night PowerPoint Creations</strong></figcaption></figure><h2 id="8177" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">因素</h2><p id="ad0e" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">这里的列表比卷积层的列表简单得多</p><ul class=""><li id="9f47" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><strong class="ky ir">池形状</strong> —池的形状，用一个元组来描述，一个好的默认值是(2，2)</li><li id="96a2" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">步距</strong> —与卷积相同，一个好的默认值是 2(大多数情况下，匹配池的尺寸是好的)</li></ul><h2 id="460e" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">履行</h2><p id="aefc" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">再次实现一个<strong class="ky ir">基类</strong>是一个好主意，因为一些操作对于所有的池层都是相同的(例如像<strong class="ky ir">初始化</strong>):</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="bd3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来是<strong class="ky ir">最大池</strong>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="5523" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我强烈建议您自己尝试实现<strong class="ky ir">平均池层</strong>。</p><p id="f233" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们建立第一个 CNN 模型之前，我们需要重新格式化我们的数据，以便它可以被其他层使用，如全连接。这是需要的，因为当我们解决分类任务时，最后的非激活层应该总是完全连接的。因为如果全连接的输入不是 1D 数组，那么它将会是一团乱麻，我们必须首先<strong class="ky ir">展平</strong>(将 N-D 数组转换为 1D 数组)数据。</p><h1 id="be15" class="oa lu iq bd lv ob oc od ly oe of og mb jw oh jx me jz oi ka mh kc oj kd mk ok bi translated">展平图层</h1><p id="d7a1" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">这可能是实现起来最简单的一层。我们的正向传播需要展平输入，反向传播需要将其重新格式化回初始形式:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="6507" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们建立我们的第一个 CNN…</p><h1 id="3f64" class="oa lu iq bd lv ob oc od ly oe of og mb jw oh jx me jz oi ka mh kc oj kd mk ok bi translated">CNN 模型</h1><p id="4b3f" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我们将使用本文中实现的所有层来构建一个简单的 CNN:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="309c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们看看它的表现如何:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="6f45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它应该比我们的普通网表现更好，尽管如果你玩它并有耐心等待训练过程结束，它实际上可以取得更高的结果。这里有一些关于如何建立一个更好的神经网络模型的有用资源…</p></div><div class="ab cl pd pe hu pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="ij ik il im in"><h1 id="1d80" class="oa lu iq bd lv ob pk od ly oe pl og mb jw pm jx me jz pn ka mh kc po kd mk ok bi translated">附加链接</h1><div class="nf ng gp gr nh ni"><a rel="noopener follow" target="_blank" href="/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">新手问“人工神经网络要用多少个隐层/神经元？”</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">人工神经网络(ann)的初学者很可能会问一些问题。这些问题包括什么…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">towardsdatascience.com</p></div></div><div class="nr l"><div class="px l nt nu nv nr nw kp ni"/></div></div></a></div><div class="nf ng gp gr nh ni"><a href="https://medium.com/@nainaakash012/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-92941c5bfb95" rel="noopener follow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">EfficientNet:重新思考卷积神经网络的模型缩放</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">自从 AlexNet 赢得了 2012 年的 ImageNet 竞赛，CNN(卷积神经网络的简称)已经成为设计者</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">medium.com</p></div></div><div class="nr l"><div class="py l nt nu nv nr nw kp ni"/></div></div></a></div></div><div class="ab cl pd pe hu pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="ij ik il im in"><h1 id="df42" class="oa lu iq bd lv ob pk od ly oe pl og mb jw pm jx me jz pn ka mh kc po kd mk ok bi translated">最后的话</h1><p id="32ac" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">感谢大家花时间阅读我的第一篇关于媒体的文章。我真的希望它能帮助你学到一些新的有用的东西，甚至可以帮助你开始学习数据科学和机器学习。</p><p id="e3bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您有任何意见、想法或建议，我会非常乐意收到您的反馈，并会尽快回复。</p></div><div class="ab cl pd pe hu pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="ij ik il im in"><h1 id="3e49" class="oa lu iq bd lv ob pk od ly oe pl og mb jw pm jx me jz pn ka mh kc po kd mk ok bi translated">图像参考</h1><p id="ebb8" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><a class="ae kv" href="https://www.researchgate.net/figure/Threshold-Logic-Unit-Source-Wikimedia-Commons_fig10_264080882" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/figure/Threshold-Logic-Unit-Source-Wikimedia-Commons _ fig 10 _ 264080882</a></p><div class="nf ng gp gr nh ni"><a href="http://rasbt.github.io/mlxtend/user_guide/classifier/Perceptron/" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">感知器</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">一种用于分类的感知器学习算法的实现。分类器导入感知器…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">rasbt.github.io</p></div></div></div></a></div><div class="nf ng gp gr nh ni"><a href="http://cs231n.github.io/neural-networks-1/" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">用于视觉识别的 CS231n 卷积神经网络</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">目录:引入神经网络而不诉诸大脑类比是可能的。在…一节中</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">cs231n.github.io</p></div></div><div class="nr l"><div class="pz l nt nu nv nr nw kp ni"/></div></div></a></div><div class="nf ng gp gr nh ni"><a href="https://www.dataquest.io/blog/understanding-regression-error-metrics/" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">教程:了解线性回归和回归误差度量</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">人类大脑的构造是为了识别我们周围世界的模式。例如，我们观察到如果我们练习…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">www.dataquest.io</p></div></div><div class="nr l"><div class="qa l nt nu nv nr nw kp ni"/></div></div></a></div><div class="nf ng gp gr nh ni"><a href="https://medium.com/bayshore-intelligence-solutions/why-is-stochastic-gradient-descent-2c17baf016de" rel="noopener follow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">为什么是随机梯度下降？</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">随机梯度下降(SGD)是数据科学中最流行和最常用的优化器之一。如果你曾经…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">medium.com</p></div></div><div class="nr l"><div class="qb l nt nu nv nr nw kp ni"/></div></div></a></div><div class="nf ng gp gr nh ni"><a href="https://www.oreilly.com/radar/the-hard-thing-about-deep-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">深度学习的难点在于</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">深度学习的核心是一个困难的优化问题。如此之难以至于在引进后的几十年里…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">www.oreilly.com</p></div></div><div class="nr l"><div class="qc l nt nu nv nr nw kp ni"/></div></div></a></div><div class="nf ng gp gr nh ni"><a href="https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">机器学习中的梯度下降算法(及其变体)简介</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">介绍优化始终是最终目标，无论你是处理一个现实生活中的问题或建立一个…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="nr l"><div class="qd l nt nu nv nr nw kp ni"/></div></div></a></div><div class="nf ng gp gr nh ni"><a href="https://bg.m.wikipedia.org/wiki/%D0%A4%D0%B0%D0%B9%D0%BB:Sinh_cosh_tanh.svg" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">уакл:Sinh cosh tanh . SVG</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk">504 × 504 пиксела. 240 × 240 пиксела | 480 × 480 пиксела | 600 × 600 пиксела | 768 × 768 пиксела | 1024 × 1024 пиксела.</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">bg.m.wikipedia.org</p></div></div><div class="nr l"><div class="qe l nt nu nv nr nw kp ni"/></div></div></a></div><div class="nf ng gp gr nh ni"><a href="https://hackernoon.com/training-an-architectural-classifier-ii-bf29eca3cfa6" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">训练建筑分类器- II</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">这是 5 篇文章系列的第 2 部分:训练架构分类器:动机训练架构分类器…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">hackernoon.com</p></div></div><div class="nr l"><div class="qf l nt nu nv nr nw kp ni"/></div></div></a></div><div class="nf ng gp gr nh ni"><a href="https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">卷积神经网络中的反向传播</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">卷积神经网络(CNN)是多层感知器(MLPs)的生物启发变体…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">www.jefkine.com</p></div></div></div></a></div></div></div>    
</body>
</html>