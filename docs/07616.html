<html>
<head>
<title>Pyspark – demand forecasting data science project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">py spark——需求预测数据科学项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-demand-forecasting-data-science-project-dae14b5319cc?source=collection_archive---------10-----------------------#2019-10-23">https://towardsdatascience.com/pyspark-demand-forecasting-data-science-project-dae14b5319cc?source=collection_archive---------10-----------------------#2019-10-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8c02" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从预处理到建模的完整指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c642dff3a6b7d10764cc9b447e82d151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jvxJNDq39isZ-Kgf9VsSAQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg" rel="noopener ugc nofollow" target="_blank">https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg</a></figcaption></figure><p id="53b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将使用 Pyspark 构建一个逐步需求预测项目。这里，任务列表:</p><ol class=""><li id="9b13" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">导入数据</strong></li><li id="c54a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">过滤数据</strong></li><li id="e193" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">功能工程(功能创建)</strong></li><li id="243d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">输入数据</strong></li><li id="6649" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">功能工程(功能转换)</strong></li><li id="1f3b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">应用梯度增强树回归器</strong></li><li id="b831" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">用 Kfold 和 GridSearch 方法优化模型</strong></li><li id="2191" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">一次性</strong></li></ol></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="89ed" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">I)导入数据</h1><p id="a27d" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">首先，我们将使用预定义的模式导入数据。我在谷歌云平台的虚拟机上工作，数据来自云存储的一个桶。还是导入吧。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="bb93" class="ns mr it no b gy nt nu l nv nw">from pyspark.sql.types import *</span><span id="e836" class="ns mr it no b gy nx nu l nv nw">schema = StructType([<br/>StructField("DATE", DateType()),<br/>StructField("STORE", IntegerType()),<br/>StructField("NUMBERS_OF_TICKETS", IntegerType()),<br/>StructField("QTY", IntegerType()),<br/>StructField("CA", DoubleType()),<br/>StructField("FORMAT", StringType())])</span><span id="71ea" class="ns mr it no b gy nx nu l nv nw">df = spark.read.csv("gs://my_bucket/my_table_in_csv_format", header = 'true', schema=schema)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b48c6bc8afa0faa3a0f12ca6920e4648.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*txWpld4-HZNDp8jmx1rzmA.png"/></div></figure><h1 id="f25c" class="mq mr it bd ms mt nz mv mw mx oa mz na jz ob ka nc kc oc kd ne kf od kg ng nh bi translated">II)过滤数据</h1><p id="3d6d" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">然后，我们将应用一些过滤器，我们将只在大型超市工作，并确保数据中没有负数量或缺少日期。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="222a" class="ns mr it no b gy nt nu l nv nw">df = df.filter(<br/> (F.col("QTY") &gt; 0) <br/>&amp;(F.col("DATE").notNull())<br/>&amp; (F.col("DATE").between("2015-01-01", "2019-01-01")) <br/>&amp; (~F.col("FORMAT").isin(["PRO", "SUP"])) <br/>)</span></pre><h1 id="fe1b" class="mq mr it bd ms mt nz mv mw mx oa mz na jz ob ka nc kc oc kd ne kf od kg ng nh bi translated">III)特征工程(特征创建)</h1><p id="cdeb" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">然后，我们将定义一些对建模有用的变量，比如日期导数。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="affd" class="ns mr it no b gy nt nu l nv nw">df = (df<br/> .withColumn('yearday', F.dayofyear(F.col("DATE")))<br/> .withColumn('Month', F.Month(F.col('DATE')))<br/> .withColumn('dayofweek', F.dayofweek(F.col('DATE')))<br/> .withColumn('YEAR', F.year(F.col('DATE')))<br/> .withColumn('QUARTER', F.q(F.col('DATE')))<br/> .withColumn('Month', F.Month(F.col('DATE')))<br/> .withColumn('WeekOfYear', F.weekofyear(F.col('DATE')))<br/> .withColumn('Week', F.date_trunc('week',F.col('DATE')))<br/> .withColumn('MonthQuarter', F.when((df[DATE] &lt;= 8), 0)                                 .otherwise(F.when((df['DATE'] &lt;= 16), 1)                                .otherwise(F.when((df['DATE'] &lt;= 24), 2)                                 .otherwise(3))))<br/>)</span></pre><p id="d190" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将计算每个商店的参考数量，即 1 年前同一天的销售量。然而，这一天可能没有销售，所以我们将在这个参考日期前后平均 7 天。这个函数会更复杂，需要 numpy。确实有可能把 numpy 和 spark 说清楚，让我们看看怎么做。</p><p id="a362" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，必须定义一个用户定义的函数，以矢量化和稀疏的方式从每个存储中提取时间序列。我们将定义一个函数来创建一个稀疏向量，该向量以一年中的天数和相关量的值为索引</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="7f9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，让我们休息一下，他有一些事情要解释。</p><ul class=""><li id="001d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu og mb mc md bi translated">输入:我们要求一个日期索引和相关数量值的列表</li><li id="0711" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu og mb mc md bi translated">输出:我们返回一个按日索引的稀疏向量，它允许我们找到与他的日索引相关的数量</li></ul><p id="afd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个过程通过一个 UDF，并期望成为一个<code class="fe oh oi oj no b"><a class="ae ky" href="https://spark.apache.org/docs/2.1.2/api/java/org/apache/spark/mllib/linalg/VectorUDT.html" rel="noopener ugc nofollow" target="_blank">VectorUDT</a></code>简洁地说，这是一种可以被 UDF 操纵的向量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="86e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，为了响应函数请求的输入，我们将每年创建一个聚合数据帧，存储并应用一个 collect_list 到日期和数量。正如您在下面看到的，我们恢复了商店当年的所有日数量值。这两个列表进入我们的 UDF 来创建想要的向量，现在我们可以在 numpy 中处理这个向量了！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/5520433a92088f035c722d535d5564e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nbsqJD6JmAx6L7A8.png"/></div></div></figure><p id="3bac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以定义一个作用于这个向量的函数，并再次把它放到 UDF 中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="81eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并应用它:</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="d9f4" class="ns mr it no b gy nt nu l nv nw">df= (df<br/>    .join(self_join<br/>        , ([self_join.p_id_store == df.STORE, self_join.year_join ==  df.year]),<br/>        how = "left"<br/>        )<br/>        .withColumn("qty_reference", getReference(F.col("yearday"), F.col("qties_vectorized")))<br/>        )</span></pre><p id="92d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们详细描述一下这个函数，首先它试图找到准确的参考日，我们已经将数据帧和它的前一年连在一起，所以我们希望当前日等于向量中的索引日并得到值。如果无法检索该值，我们将围绕该关键日期定义一个窗口，并对该窗口的值进行平均。</p><p id="565f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后的结果是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/a15a552f4c9ca0522603f0733fc06629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ddQl8FsgMBnBxuh_N_305A.png"/></div></div></figure><p id="a5f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们不会让它们像那样被硬编码，而是将所有东西都打包到一个 Spark 管道中。为此，我们将创建一个继承自 Spark Transformer 对象的类模板，如下所示，我们将为每个变量重复这个模板。我不打算在本文中写完整的功能管道，但是你可以在 github 的代码中找到它。我们将在本文的结尾看到如何将这个特性的管道放在一起，并将其插入到流程中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="146d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于这个类模板特性的更多细节，请参阅我的文章…:<a class="ae ky" rel="noopener" target="_blank" href="/pyspark-wrap-your-feature-engineering-in-a-pipeline-ee63bdb913"><strong class="lb iu"><em class="om">https://towardsdatascience . com/py spark-wrap-your-feature-engineering-in-a-a-pipeline-ee 63 BDB 913</em></strong></a></p><h1 id="77f5" class="mq mr it bd ms mt nz mv mw mx oa mz na jz ob ka nc kc oc kd ne kf od kg ng nh bi translated">IV)输入数据</h1><p id="e0f9" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">让我们检查是否有一些丢失的值。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="3307" class="ns mr it no b gy nt nu l nv nw">df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/230cb5c0b23906477e7e9223c1c4469f.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*WJwry7SEG0pfEHwt-XUkaA.png"/></div></figure><p id="0ae8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可能不会有更多，但在插入新变量后可能会有一些，因此我们将定义一个将在变量创建管道后出现的估算器。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="9d80" class="ns mr it no b gy nt nu l nv nw">from pyspark.ml.feature import Imputer<br/><br/>imputer = Imputer(<br/>    inputCols=df.columns, <br/>    outputCols=["{}_imputed".format(c) for c in df.columns]<br/>)</span><span id="9a22" class="ns mr it no b gy nx nu l nv nw">#imputer.fit(df).transform(df)</span></pre><h1 id="c395" class="mq mr it bd ms mt nz mv mw mx oa mz na jz ob ka nc kc oc kd ne kf od kg ng nh bi translated">v)特征工程(特征转换)</h1><p id="83a6" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">从下面的表格摘录中，我将向您展示在插入预测算法之前，我们将如何进行其余的数据转换。</p><blockquote class="oo op oq"><p id="2dff" class="kz la om lb b lc ld ju le lf lg jx lh or lj lk ll os ln lo lp ot lr ls lt lu im bi translated">我们将在最后一次性完成整个过程。</p></blockquote><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="73c7" class="ns mr it no b gy nt nu l nv nw"># needed import</span><span id="cab4" class="ns mr it no b gy nx nu l nv nw">from pyspark.ml import Pipeline<br/>from pyspark.ml.feature import PCA<br/>from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler</span></pre><h2 id="fe5b" class="ns mr it bd ms ou ov dn mw ow ox dp na li oy oz nc lm pa pb ne lq pc pd ng pe bi translated">索引</h2><p id="277a" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">Spark <code class="fe oh oi oj no b"><a class="ae ky" href="https://spark.apache.org/docs/latest/ml-features#stringindexer" rel="noopener ugc nofollow" target="_blank">String Indexer</a></code>将一列标签编码成一列标签索引。索引在[0，numLabels 中。映射首先由最高频率完成。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="61ee" class="ns mr it no b gy nt nu l nv nw">indexers = [ StringIndexer(inputCol=c, outputCol="{0}_indexedd".format(c), handleInvalid = 'error') for c in categorical_col]</span><span id="2765" class="ns mr it no b gy nx nu l nv nw">pip = Pipeline(stages = indexers)<br/>fitted_df =pip.fit(df)<br/>df = fitted_df.transform(df)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/f5eb3eb5c2c8c1cae308421bb3a5805d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*SBTUxfbrnPhysimpregDlA.png"/></div></figure><h2 id="ccf8" class="ns mr it bd ms ou ov dn mw ow ox dp na li oy oz nc lm pa pb ne lq pc pd ng pe bi translated">OneHotEncoding</h2><p id="8ca7" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">Spark 的<code class="fe oh oi oj no b"><a class="ae ky" href="https://spark.apache.org/docs/latest/ml-features#onehotencoder" rel="noopener ugc nofollow" target="_blank">OneHotEncoder</a></code> <a class="ae ky" href="http://en.wikipedia.org/wiki/One-hot" rel="noopener ugc nofollow" target="_blank"> One-hot 编码</a>将表示为标签索引的分类特征映射到一个二进制向量，该向量最多具有一个单值，指示所有特征值集合中特定特征值的存在。对于字符串类型的输入数据，通常首先使用<a class="ae ky" href="https://spark.apache.org/docs/latest/ml-features.html#stringindexer" rel="noopener ugc nofollow" target="_blank"> StringIndexer </a>对分类特征进行编码。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="0ee2" class="ns mr it no b gy nt nu l nv nw">indexers = [ StringIndexer(inputCol=c, outputCol="{0}_indexedd".format(c), handleInvalid = 'error') for c in categorical_col]</span><span id="a26c" class="ns mr it no b gy nx nu l nv nw">encoders = [OneHotEncoder(dropLast=True,inputCol=indexer.getOutputCol(), <br/>    outputCol="{0}_encodedd".format(indexer.getOutputCol())) for indexer in indexers]</span><span id="1b47" class="ns mr it no b gy nx nu l nv nw">pip = Pipeline(stages = indexers + encoders)<br/>fitted_df =pip.fit(df)<br/>df = fitted_df.transform(df)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/d6697a4e26260ac07d72ca8cb9a9b87a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pZh4ZtzxjroRRI4bCwc6iw.png"/></div></div></figure><h2 id="75ee" class="ns mr it bd ms ou ov dn mw ow ox dp na li oy oz nc lm pa pb ne lq pc pd ng pe bi translated">PCA 简化和向量</h2><p id="ded2" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在执行我们的管道之前，让我们检查一下在我们的流程中是否有任何令人讨厌的丢失值。在运行降维算法之前，我们必须将所有变量传递到一个向量汇编器中，该汇编器将返回所有数据的稀疏表示。然后，PCA 算法将采用这个向量来“简化”它，并在 dataframe 列中返回另一个稀疏表示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/1228d231c5ec2139745702818c5c0362.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*PuOmNWINbwpjaQLvu-6Sug.png"/></div></figure><p id="0fa8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到了如何索引、一键编码我们的数据、应用主成分分析并将所有内容放入准备建模的向量中。显然还有其他的可能性来标准化、规范化…等等。然而，全球原则保持不变。现在我们的特征已经有了很好的形状，可以通过 Pyspark 算法进行建模。</p><ul class=""><li id="270e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu og mb mc md bi translated">分割数据集</li></ul><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="e3f7" class="ns mr it no b gy nt nu l nv nw">X_train = final_dataset.filter(F.col('DATE').between("2015-01-02", "2018-06-01"))</span><span id="1b83" class="ns mr it no b gy nx nu l nv nw">X_test = final_dataset.filter(F.col('DATE') &gt; "2018-06-01")</span><span id="b728" class="ns mr it no b gy nx nu l nv nw">X_train = X_train.withColumn(target, F.log1p(F.col(target)))</span><span id="305b" class="ns mr it no b gy nx nu l nv nw">X_test = X_test.withColumn(target, F.log1p(F.col(target)))</span></pre><h1 id="2597" class="mq mr it bd ms mt nz mv mw mx oa mz na jz ob ka nc kc oc kd ne kf od kg ng nh bi translated">VI)应用梯度增强树回归器</h1><p id="e7f1" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们将训练一个梯度提升树模型，对商店每天销售的总数量进行回归。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="4147" class="ns mr it no b gy nt nu l nv nw">target = 'QTY'</span><span id="82c0" class="ns mr it no b gy nx nu l nv nw">gbt = GBTRegressor(featuresCol = 'Features', labelCol=target)</span><span id="200a" class="ns mr it no b gy nx nu l nv nw">fitted = gbt.fit(X_train)</span><span id="957e" class="ns mr it no b gy nx nu l nv nw">yhat = (fitted.transform(X_test)<br/>    .withColumn("prediction", F.expm1(F.col("prediction")))<br/>    .withColumn(target, F.expm1(F.col(target)))<br/>    ).select(F.col("prediction"), F.col("STORE").alias('SID_STORE'), F.col("DATE").alias("ID_DAY")).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/dbdd17ed9b932204d94ca279b803e749.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*EU0V3XiFcUlOtGL0EQVcNw.png"/></div></figure><h2 id="237f" class="ns mr it bd ms ou ov dn mw ow ox dp na li oy oz nc lm pa pb ne lq pc pd ng pe bi translated">计算 KPI</h2><p id="1cdf" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们将定义一个 python 对象，以此为基础在不同的指标上评估我们的模型。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="300c" class="ns mr it no b gy nt nu l nv nw">eval_ = RegressionEvaluator(labelCol= target, predictionCol= "prediction", metricName="rmse")</span><span id="fc28" class="ns mr it no b gy nx nu l nv nw">rmse = eval_.evaluate(yhat)<br/>print('rmse is %.2f', %rmse)</span><span id="6219" class="ns mr it no b gy nx nu l nv nw">mae = eval_.evaluate(yhat, {eval_.metricName: "mae"})<br/>print('mae is %.2f', %mae)</span><span id="712f" class="ns mr it no b gy nx nu l nv nw">r2 = eval_.evaluate(yhat, {eval_.metricName: "r2"})<br/>print('R² is %.2f', %r2)</span></pre><ul class=""><li id="e98d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu og mb mc md bi translated">r 是 0.84</li><li id="d06f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu og mb mc md bi translated">均方根误差为 20081.54</li><li id="c355" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu og mb mc md bi translated">mae 是 13289.10</li></ul><p id="8990" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型看起来不错，我们会努力改进它。</p><h1 id="37dc" class="mq mr it bd ms mt nz mv mw mx oa mz na jz ob ka nc kc oc kd ne kf od kg ng nh bi translated">VII)使用 Kfold 和 GridSearch 方法优化模型</h1><p id="7179" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们将尝试使用不同的参数来优化我们的 GBDT，并制作一个 kfold 以确保其鲁棒性。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="2243" class="ns mr it no b gy nt nu l nv nw">from pyspark.ml.tuning import CrossValidator, ParamGridBuilder</span><span id="01be" class="ns mr it no b gy nx nu l nv nw">paramGrid = (ParamGridBuilder()<br/>             .addGrid(gbt.maxDepth, [5, 8, 10, 12])<br/>             .addGrid(gbt.maxBins, [32, 64])<br/>             .build())</span><span id="9d62" class="ns mr it no b gy nx nu l nv nw">cv = CrossValidator(estimator=gbt,<br/>                          estimatorParamMaps=paramGrid,<br/>                          evaluator=eval_,<br/>                          numFolds=3)  </span><span id="b18d" class="ns mr it no b gy nx nu l nv nw">cvModel = cv.fit(X_train)</span><span id="7580" class="ns mr it no b gy nx nu l nv nw">yhat = (cvModel.transform(X_test)<br/>    .withColumn("prediction", F.expm1(F.col("prediction")))<br/>    .withColumn(target, F.expm1(F.col(target)))<br/>    )</span></pre><p id="287f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我只多了 0.3 分，但这已经足够了！:)</p><p id="60fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">附加:<strong class="lb iu">特性</strong>重要性:)</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="5f6b" class="ns mr it no b gy nt nu l nv nw">fi = fitted.featureImportances.toArray()</span><span id="1bf5" class="ns mr it no b gy nx nu l nv nw">import pandas as pd</span><span id="730d" class="ns mr it no b gy nx nu l nv nw">features = [encoder.getOutputCol() for encoder in encoders] + \<br/> [x +'_imputed' for x in numeric_col] + ['day', 'month', 'weekday', 'weekend', 'monthend', 'monthbegin', 'monthquarter', 'yearquarter']</span><span id="c4b9" class="ns mr it no b gy nx nu l nv nw">feat_imp = (pd.DataFrame(dict(zip(features, fi)), range(1))<br/>  .T.rename(columns={0:'Score'})<br/>  .sort_values("Score", ascending =False)<br/>  )</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/abdc556f1c8a8720e88481d7b283a2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*ck3OWKTDjO14ZIRQynwKWg.png"/></div></figure><h1 id="7431" class="mq mr it bd ms mt nz mv mw mx oa mz na jz ob ka nc kc oc kd ne kf od kg ng nh bi translated">VIII)一次性</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="d26a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">名词（noun 的缩写）b:在 features_utils 包中有所有与特性管道相关的类。</p><h1 id="68de" class="mq mr it bd ms mt nz mv mw mx oa mz na jz ob ka nc kc oc kd ne kf od kg ng nh bi translated">最后</h1><p id="f623" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">本文总结了我对一个数据科学项目的 Pyspark 各种砖块的简要介绍。我希望它们能帮助哪怕是一个人的工作。Pyspark 是一个非常强大的大容量工具。他不具备像 sklearn 那样的一系列算法，但他拥有主要的算法和许多资源。</p><p id="51cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想让我为 Pyspark 做些什么，请随时告诉我，谢谢！</p><p id="b08c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">【Y】You 可以在这里找到代码<em class="om">:</em><a class="ae ky" href="https://github.com/AlexWarembourg/Medium" rel="noopener ugc nofollow" target="_blank"><em class="om">https://github.com/AlexWarembourg/Medium</em></a></p></div></div>    
</body>
</html>