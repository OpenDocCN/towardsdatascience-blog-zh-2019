<html>
<head>
<title>Decision Boundary Visualization(A-Z)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策边界可视化(A-Z)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d?source=collection_archive---------4-----------------------#2019-01-16">https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d?source=collection_archive---------4-----------------------#2019-01-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="765b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">含义、意义、实施</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c5a5f12a8bdc9e78bcd086e15774398b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sh95piVkTAazwRrQ6gUC1A.jpeg"/></div></div></figure><p id="fd00" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">分类问题是数据科学领域中非常普遍和重要的问题。例如:糖尿病视网膜病变、情绪或情感分析、数字识别、癌症类型预测(恶性或良性)等。这些问题往往通过机器学习或者深度学习来解决。此外，在计算机视觉中，像糖尿病视网膜病变或青光眼检测这样的项目中，纹理分析现在经常使用，而不是传统图像处理或深度学习的经典机器学习。虽然根据研究论文，深度学习已经成为糖尿病视网膜病变的最先进技术:</p><p id="4c62" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lq" href="https://ieeexplore.ieee.org/document/8596839" rel="noopener ugc nofollow" target="_blank"><em class="lr"/></a>【1】一种检测糖尿病视网膜病变的深度学习方法。</p><p id="6ec3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在分类问题中，对一个特定类别的预测涉及到多个类别。换句话说，它也可以以这样的方式构成，即特定的实例(根据特征空间几何的数据点)需要保持在特定的区域(表示类)下，并且需要与其他区域分离(表示其他类)。这种与其他区域的分离可以通过被称为<strong class="kw iu">决策边界</strong>的边界来可视化。特征空间中决策边界的可视化是在散点图上完成的，其中每个点描述数据集的一个数据点，轴描述特征。决策边界将数据点分成区域，这些区域实际上是它们所属的类。</p><p id="4865" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">决策边界的重要性/显著性</strong>:</p><p id="aa1b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在使用数据集训练机器学习模型之后，通常需要可视化特征空间中数据点的分类。散点图上的决策边界用于此目的，其中散点图包含属于不同类别的数据点(用颜色或形状表示),决策边界可以按照许多不同的策略绘制:</p><ol class=""><li id="0ba3" class="ls lt it kw b kx ky la lb ld lu lh lv ll lw lp lx ly lz ma bi translated"><strong class="kw iu">单线决策边界</strong>:在散点图上绘制决策边界的基本策略是找到一条单线，将数据点分成表示不同类别的区域。现在，使用在训练模型之后获得的与机器学习算法相关的参数来找到这一单线。使用获得的参数和机器学习算法背后的直觉找到线坐标。如果不知道 ML 算法的直觉和工作机制，就不可能部署这种策略。</li><li id="ef0d" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated"><strong class="kw iu">基于轮廓的决策边界</strong>:另一种策略涉及绘制轮廓，这些轮廓是每个都用匹配或接近匹配的颜色包围数据点的区域——描绘数据点所属的类别，以及描绘预测类别的轮廓。这是最常遵循的策略，因为这不采用模型训练后获得的机器学习算法的参数和相关计算。但是另一方面，这并没有使用单线完美地分离数据点，所述单线只能由在训练和它们的坐标计算之后获得的参数给出。</li></ol><p id="91ce" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="lr">单线判定边界的范例实现:</em> </strong></p><p id="4ea8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这里，我将展示基于逻辑回归的机器学习模型的单线决策边界。</p><p id="a2dd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">进入逻辑回归假设-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/6144446e8e8c4713e0b37dbe86a3f836.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*6gKr1K-9kLhZ14aXT-qvcw.png"/></div></figure><p id="bbd8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中 z 定义为-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mh"><img src="../Images/4a3d5b3fd29629b0bb9fe583969df63e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fc3y8qC9YUS2A3RddWNWHg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk"><strong class="bd mm">theta_1</strong>, <strong class="bd mm">theta_2, theta_3</strong> , …., <strong class="bd mm">theta_n </strong>are the parameters of Logistic Regression and <strong class="bd mm">x_1</strong>, <strong class="bd mm">x_2</strong>, …, <strong class="bd mm">x_n</strong> are the features</figcaption></figure><p id="fdf7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，h(z)是一个 Sigmoid 函数，其范围是从 0 到 1 (0 和 1 包括在内)。</p><p id="9fc0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了绘制决策边界，h(z)等于逻辑回归中使用的阈值，通常为 0.5。所以，如果</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/f97b289171db19a3080d0cc88f447f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*RIo8wWJs7Nlxe8YGsifiBA.png"/></div></figure><p id="1be0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/2498f370aa6240cd2a30fcbc19eec4ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*XpDWzJg_aNvzH24ILaEgGg.png"/></div></figure><p id="8c2a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，为了绘制决策边界，需要考虑两个特征，并沿散点图的 x 轴和 y 轴绘制。所以，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/8cb25f81d93379c838198415921c3e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*KWObD9Ze7g06dpCWBhN9WQ.png"/></div></figure><p id="0b6d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在哪里，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/45b8ca5b238ebf489fb4a657cf460acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*tvo94D2MMXIHCHl5L2-0Dw.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">where <strong class="bd mm">x_1 </strong>is the original feature of the dataset</figcaption></figure><p id="4e54" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，获得了 x'_1 的 2 个值以及 2 个相应的 x'_2 值。x'_1 是单线判定边界的 x 极值，x'_2 是 y 极值。</p><p id="0d22" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">虚拟数据集上的应用:</strong></p><p id="2efc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">数据集包含 100 名学生在两次考试中获得的分数和标签(0/1)，该标签指示该学生是否将被大学录取(1 或负数)或不被大学录取(0 或正数)。该数据集位于</p><div class="mq mr gp gr ms mt"><a href="https://github.com/navoneel1092283/logistic_regression.git" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd iu gy z fp my fr fs mz fu fw is bi translated">navoneel 1092283/logistic _ 回归</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">在 GitHub 上创建一个帐户，为 navoneel 1092283/logistic _ regression 开发做出贡献。</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">github.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh ks mt"/></div></div></a></div><p id="f192" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">问题陈述</strong>:<em class="lr">给定两次考试的分数，用逻辑回归</em>预测学生是否会被大学录取</p><p id="8c7b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这里，两次考试的分数将是被考虑的两个特征。</p><p id="b04d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下是在 3 个模块中实现的逻辑回归。文中给出了具体的实现方法，</p><div class="mq mr gp gr ms mt"><a href="https://hackernoon.com/logistic-regression-in-python-from-scratch-954c0196d258" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd iu gy z fp my fr fs mz fu fw is bi translated">Python 中的逻辑回归从零开始</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">分类是机器学习问题中一个非常普遍和重要的变体。很多机器算法都有…</h3></div></div><div class="nc l"><div class="ni l ne nf ng nc nh ks mt"/></div></div></a></div><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="3ef4" class="no np it nk b gy nq nr l ns nt">import numpy as np<br/>from math import *</span><span id="73f4" class="no np it nk b gy nu nr l ns nt">def logistic_regression(X, y, alpha):<br/>    n = X.shape[1]<br/>    one_column = np.ones((X.shape[0],1))<br/>    X = np.concatenate((one_column, X), axis = 1)<br/>    theta = np.zeros(n+1)<br/>    h = hypothesis(theta, X, n)<br/>    theta, theta_history, cost = Gradient_Descent(theta, alpha<br/>                                 , 100000, h, X, y, n)<br/>    return theta, theta_history, cost<br/>def Gradient_Descent(theta, alpha, num_iters, h, X, y, n):<br/>    theta_history = np.ones((num_iters,n+1))<br/>    cost = np.ones(num_iters)<br/>    for i in range(0,num_iters):<br/>        theta[0] = theta[0] - (alpha/X.shape[0]) * sum(h - y)<br/>        for j in range(1,n+1):<br/>            theta[j] = theta[j] - (alpha/X.shape[0]) * sum((h - y) *<br/>                                   X.transpose()[j])<br/>        theta_history[i] = theta<br/>        h = hypothesis(theta, X, n)<br/>        cost[i] = (-1/X.shape[0]) * sum(y * np.log(h) + (1 - y) * <br/>                                        np.log(1 - h))<br/>    theta = theta.reshape(1,n+1)<br/>    return theta, theta_history, cost<br/>def hypothesis(theta, X, n):<br/>    h = np.ones((X.shape[0],1))<br/>    theta = theta.reshape(1,n+1)<br/>    for i in range(0,X.shape[0]):<br/>        h[i] = 1 / (1 + exp(-float(np.matmul(theta, X[i]))))<br/>    h = h.reshape(X.shape[0])<br/>    return h</span></pre><p id="b34c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对数据集执行逻辑回归:</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="2d7d" class="no np it nk b gy nq nr l ns nt">data = np.loadtxt('dataset.txt', delimiter=',')<br/>X_train = data[:,[0,1]]<br/>y_train = data[:,2]</span><span id="796d" class="no np it nk b gy nu nr l ns nt">theta, theta_history, cost = logistic_regression(X_train, y_train<br/>                                                 , 0.001)</span></pre><p id="e028" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">获得的θ(参数)向量，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/49ff4e64eef64bb41b57744674a249d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*rpioUT-0dsHLkrLteKSUbg.png"/></div></figure><p id="b469" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">获得数据点的预测或预测类别:</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="53d4" class="no np it nk b gy nq nr l ns nt">Xp=np.concatenate((np.ones((X_train.shape[0],1)), X_train),axis= 1)<br/>h=hypothesis(theta, Xp, Xp.shape[1] - 1)</span></pre><p id="1305" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">绘制单线决策边界:</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="2c02" class="no np it nk b gy nq nr l ns nt">import matplotlib.pyplot as plt</span><span id="cd63" class="no np it nk b gy nu nr l ns nt">c0 = c1 = 0 # <strong class="nk iu">Counter of label 0 and label 1 instances<br/></strong>if i in range(0, X.shape[0]):<br/>    if y_train[i] == 0:<br/>        c0 = c0 + 1<br/>    else:<br/>        c1 = c1 + 1</span><span id="d79f" class="no np it nk b gy nu nr l ns nt">x0 = np.ones((c0,2)) # <strong class="nk iu">matrix</strong> <strong class="nk iu">label 0 instances</strong><br/>x1 = np.ones((c1,2)) # <strong class="nk iu">matrix</strong> <strong class="nk iu">label 1 instances</strong></span><span id="147e" class="no np it nk b gy nu nr l ns nt">k0 = k1 = 0</span><span id="bd5c" class="no np it nk b gy nu nr l ns nt">for i in range(0,y_train.shape[0]):<br/>    if y_train[i] == 0:<br/>        x0[k0] = X_train[i]<br/>        k0 = k0 + 1<br/>    else:<br/>        x1[k1] = X_train[i]<br/>        k1 = k1 + 1</span><span id="d41c" class="no np it nk b gy nu nr l ns nt">X = [x0, x1]<br/>colors = ["green", "blue"] # <strong class="nk iu">colours for Scatter Plot</strong><br/>theta = theta.reshape(3)</span><span id="129a" class="no np it nk b gy nu nr l ns nt"># <strong class="nk iu">getting the x co-ordinates of the decision boundary<br/></strong>plot_x = np.array([min(X_train[:,0]) - 2, max(X_train[:,0]) + 2])<br/># <strong class="nk iu">getting corresponding y co-ordinates of the decision boundary<br/></strong>plot_y = (-1/theta[2]) * (theta[1] * plot_x + theta[0])</span><span id="5255" class="no np it nk b gy nu nr l ns nt"># <strong class="nk iu">Plotting the Single Line Decision Boundary</strong><br/>for x, c in zip(X, colors):<br/>    if c == "green":<br/>        plt.scatter(x[:,0], x[:,1], color = c, label = "Not<br/>                                                        Admitted")<br/>    else:<br/>        plt.scatter(x[:,0], x[:,1], color = c, label = "Admitted")<br/>plt.plot(plot_x, plot_y, label = "Decision_Boundary")<br/>plt.legend()<br/>plt.xlabel("Marks obtained in 1st Exam")<br/>plt.ylabel("Marks obtained in 2nd Exam")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ea787bccfafb36db07794a4f8b9c2991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*i_oYgWjPbXbg3Z2uQLAmtw.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Obtained <strong class="bd mm">Single Line Decision Boundary</strong></figcaption></figure><p id="d60f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这样，可以为任何基于逻辑回归的机器学习模型绘制单线决策边界。对于其他基于机器学习算法的模型，必须知道相应的假设和直觉。</p><p id="b756" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="lr">基于轮廓的判定边界的范例实现:</em> </strong></p><p id="5a21" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用相同的虚构问题、数据集和训练模型，绘制基于轮廓的决策边界。</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="a627" class="no np it nk b gy nq nr l ns nt"># Plotting decision regions<br/>x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1<br/>y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1</span><span id="a8bd" class="no np it nk b gy nu nr l ns nt">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),<br/>                     np.arange(y_min, y_max, 0.1))</span><span id="7256" class="no np it nk b gy nu nr l ns nt">X = np.concatenate((np.ones((xx.shape[0]*xx.shape[1],1))<br/>                 ,  np.c_[xx.ravel(), yy.ravel()]), axis = 1)<br/>h = hypothesis(theta, X, 2)</span><span id="2e2f" class="no np it nk b gy nu nr l ns nt">h = h.reshape(xx.shape)</span><span id="e8ed" class="no np it nk b gy nu nr l ns nt">plt.contourf(xx, yy, h)<br/>plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train,<br/>                              s=30, edgecolor='k')<br/>plt.xlabel("Marks obtained in 1st Exam")<br/>plt.ylabel("Marks obtained in 2nd Exam")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/9e2904fdfdb20c792d68bd987709e52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*jSZkObWCanLg2yFhvLzgBA.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Obtained <strong class="bd mm">Contour-Based Decision Boundary </strong>where <strong class="bd mm">yellow -&gt; Admitted </strong>and <strong class="bd mm">blue -&gt; Not Admitted</strong></figcaption></figure><p id="27b7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这种方法显然更方便，因为不需要直觉和假设或者机器学习算法背后的任何数学。所需要的，就是高级 Python 编程的诀窍！！！！</p><p id="3350" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，这是一种为任何机器学习模型绘制决策边界的通用方法。</p><p id="8d23" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在大多数实用的高级项目中，都涉及到许多特性。那么，如何在二维散点图中绘制决策边界呢？T3】</p><p id="6154" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这些情况下，有多种出路:</p><ol class=""><li id="cc39" class="ls lt it kw b kx ky la lb ld lu lh lv ll lw lp lx ly lz ma bi translated">可以使用由随机森林分类器或额外树分类器给出的特征重要性分数，以获得 2 个最重要的特征，然后可以在散点图上绘制决策边界。</li><li id="cbf6" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated">像主成分分析(PCA)或线性判别分析(LDA)这样的降维技术可用于将 N 个特征降维为 2 个特征(n_components = 2 ),因为 N 个特征的信息或解释嵌入到这 2 个特征中。然后，考虑到这两个特征，可以在散点图上绘制决策边界。</li></ol><p id="af19" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是决策边界可视化的全部内容。</p><p id="0c7a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">参考文献</strong></p><p id="cab6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[1] N. Chakrabarty，“一种检测糖尿病视网膜病变的深度学习方法”，<em class="lr"> 2018 年第 5 届 IEEE 北方邦分会国际电气、电子和计算机工程会议(UPCON) </em>，印度戈拉克普尔，2018 年，第 1–5 页。多伊指数:10.1109/升。36860.88868688666</p></div></div>    
</body>
</html>