<html>
<head>
<title>Transfer Learning : Why train when you can finetune?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">迁移学习:当你可以微调时为什么要训练？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transfer-learning-picking-the-right-pre-trained-model-for-your-problem-bac69b488d16?source=collection_archive---------5-----------------------#2019-05-06">https://towardsdatascience.com/transfer-learning-picking-the-right-pre-trained-model-for-your-problem-bac69b488d16?source=collection_archive---------5-----------------------#2019-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d7db" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PyTorch 迁移学习概述</h2></div><p id="41a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本帖中，我们将简要了解什么是迁移学习，以及如何最好地利用它提供的巨大潜力。让我们先从第一个问题开始。</p><h1 id="3520" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">什么是迁移学习？</h1><p id="8cea" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">迁移学习是使用预先训练好的模型来解决深度学习问题的艺术。预训练模型只不过是别人根据一些数据建立和训练的深度学习模型，以解决一些问题。</p><p id="c432" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">迁移学习是一种机器学习技术，在这种技术中，您使用预先训练的神经网络来解决与网络最初被训练来解决的问题类似的问题。例如，你可以重新利用为识别狗的品种而建立的深度学习模型来对狗和猫进行分类，而不是建立自己的模型。这可以省去你寻找有效的神经网络结构的痛苦、你花在训练上的时间、建立大量训练数据的麻烦，并保证良好的结果。你可以花很长时间想出一个 50 层的 CNN 来完美地区分你的猫和你的狗，或者你可以简单地重新利用在线提供的许多预先训练好的图像分类模型中的一个。现在，让我们看看这种重新定位到底包括什么。</p><h2 id="8490" class="mb lf it bd lg mc md dn lk me mf dp lo kr mg mh lq kv mi mj ls kz mk ml lu mm bi translated">使用预训练模型的三种不同方式</h2><p id="39e0" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">主要有三种不同的方式可以重新调整预训练模型。他们是，</p><ol class=""><li id="e4c9" class="mn mo it kk b kl km ko kp kr mp kv mq kz mr ld ms mt mu mv bi translated">特征提取。</li><li id="b3a6" class="mn mo it kk b kl mw ko mx kr my kv mz kz na ld ms mt mu mv bi translated">复制预训练网络的架构。</li><li id="c18a" class="mn mo it kk b kl mw ko mx kr my kv mz kz na ld ms mt mu mv bi translated">冻结一些层，训练其他层。</li></ol><p id="ca69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">特征提取</strong>:我们在这里需要做的就是改变输出层，给出猫和狗的概率(或者你的模型试图分类的类别数量)，而不是最初训练的数千个类别。当我们尝试训练模型的数据与预训练模型最初训练的数据非常相似，并且我们的数据集很小时，这是理想的使用方法。这种机制被称为固定特征提取。我们仅重新训练我们添加的新输出层，并保留所有其他层的权重。</p><p id="0130" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">复制预训练网络的架构</strong>:在这里，我们定义一个与预训练模型具有相同架构的模型，该模型在执行与我们试图实现的任务相似的任务时表现出优异的结果，并从头开始训练它。我们丢弃预训练模型中每一层的权重，并根据我们的数据重新训练整个模型。当我们有大量数据要训练，但它与预训练模型训练的数据不太相似时，我们会采用这种方法。</p><p id="e47c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">冻结一些层，训练其他层</strong>:我们可以选择冻结预训练模型的最初 k 层，只训练最上面的 n-k 层。我们保持初始模型的权重与预训练模型的权重相同且不变，并在我们的数据上重新训练更高层。当我们的数据集很小并且数据相似性也很低时，采用这种方法。较低层关注可以从数据中提取的最基本的信息，因此这可以用于另一个问题，因为基本级别的信息通常是相同的。例如，构成一只狗的图片的相同曲线和边缘可以构成一个微小的癌细胞。</p><p id="5d56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一种常见的情况是数据相似性很高，数据集也很大。在这种情况下，我们保留模型的架构和模型的初始权重。然后，我们重新训练整个模型，以更新预训练模型的权重，从而更好地适应我们的具体问题。这是使用迁移学习的理想情况。</p><p id="7fad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了随着数据集的大小和数据相似性的变化，应遵循的方法。</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/90c34ce096df311e7f90c89b2cadf53a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*PvR62SNNcJZf3uGc.jpg"/></div></figure><h1 id="fd36" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">PyTorch 中的迁移学习</h1><p id="2278" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">PyTorch 简单来说就是 Numpy 和 Keras 的私生子。即使您不熟悉 PyTorch，理解下面的代码也不会有问题。</p><p id="bd18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PyTorch 中 torchvision.models 模块下有八种不同的预训练模型。它们是:</p><ol class=""><li id="76d0" class="mn mo it kk b kl km ko kp kr mp kv mq kz mr ld ms mt mu mv bi translated"><strong class="kk iu">T3】Alex netT5】</strong></li><li id="e6bb" class="mn mo it kk b kl mw ko mx kr my kv mz kz na ld ms mt mu mv bi translated"><strong class="kk iu"> <em class="nj"> VGG </em> </strong></li><li id="0ed3" class="mn mo it kk b kl mw ko mx kr my kv mz kz na ld ms mt mu mv bi translated"><strong class="kk iu">T11】ResNetT13】</strong></li><li id="fd8e" class="mn mo it kk b kl mw ko mx kr my kv mz kz na ld ms mt mu mv bi translated"><strong class="kk iu"> <em class="nj">挤压网</em> </strong></li><li id="a865" class="mn mo it kk b kl mw ko mx kr my kv mz kz na ld ms mt mu mv bi translated"><strong class="kk iu"> <em class="nj">丹塞尼</em> </strong></li><li id="72ca" class="mn mo it kk b kl mw ko mx kr my kv mz kz na ld ms mt mu mv bi translated"><strong class="kk iu">T23】盗梦空间 v3T25】</strong></li><li id="044b" class="mn mo it kk b kl mw ko mx kr my kv mz kz na ld ms mt mu mv bi translated"><strong class="kk iu"> <em class="nj">谷歌网</em> </strong></li><li id="1493" class="mn mo it kk b kl mw ko mx kr my kv mz kz na ld ms mt mu mv bi translated"><strong class="kk iu"> <em class="nj">洗牌网 v2 </em> </strong></li></ol><p id="d535" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些都是为图像分类而构建的卷积神经网络，在 ImageNet 数据集上进行训练。ImageNet 是一个根据 WordNet 层次结构组织的图像数据库，包含 14，197，122 幅图像，分属 21841 个类别。</p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nk"><img src="../Images/f2b4da9bac6b7e5609ac12f3537faa6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vqish7Xo5BmntHymvdEpmQ.png"/></div></div></figure><p id="a25d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于 PyTorch 中所有预训练的模型都是在相同的数据集上为相同的任务训练的，所以我们选择哪一个都没有关系。让我们挑选 ResNet 网络，看看如何在我们前面讨论的不同场景中使用它。</p><p id="597e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用于图像识别的 ResNet 或深度残差学习在 pytorch、resnet-18、resnet-34、resnet-50、resnet-101 和 resnet-152 上有五个版本。</p><p id="cf39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从火炬视觉下载 ResNet-18。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="6601" class="mb lf it nq b gy nu nv l nw nx"><strong class="nq iu">import</strong> torchvision.models <strong class="nq iu">as</strong> models<br/>model <strong class="nq iu">=</strong> models<strong class="nq iu">.</strong>resnet18(pretrained=True)</span></pre><p id="fd25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我们刚刚下载的模型的样子。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="dd33" class="mb lf it nq b gy nu nv l nw nx">ResNet(<br/>  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<br/>  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>  (relu): ReLU(inplace)<br/>  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)<br/>  (layer1): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (layer2): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (downsample): Sequential(<br/>        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (layer3): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (downsample): Sequential(<br/>        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (layer4): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (downsample): Sequential(<br/>        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))<br/>  (fc): Linear(in_features=512, out_features=1000, bias=True)<br/>)</span></pre><p id="f908" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们看看如何针对四个不同的问题训练这个模型。</p><h1 id="0bd1" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">数据集很小，数据相似度很高</h1><p id="7477" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">考虑<a class="ae ny" href="https://www.kaggle.com/mriganksingh/cat-images-dataset" rel="noopener ugc nofollow" target="_blank">这个 kaggle 数据集</a>。这包括猫的图像和其他非猫的图像。它有 209 个训练和 50 个像素为 64*64*3 的测试图像。这显然是一个非常小的数据集来建立一个可靠的图像分类模型，但我们知道 ResNet 是在大量的动物和猫图像上训练的，所以我们可以使用 ResNet 作为固定的特征提取器来解决我们的猫与非猫问题。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="2122" class="mb lf it nq b gy nu nv l nw nx">num_ftrs = model.fc.in_features<br/>num_ftrs</span></pre><p id="21fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">出局:512</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="30cf" class="mb lf it nq b gy nu nv l nw nx">model.fc.out_features</span></pre><p id="8c8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">出局:1000</p><p id="b98c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要冻结所有网络，除了最后一层。我们需要设置 requires_grad = False 来冻结参数，这样梯度就不会反向计算()。默认情况下，新构造的模块的参数 requires_grad=True。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="776e" class="mb lf it nq b gy nu nv l nw nx"><strong class="nq iu">for</strong> param <strong class="nq iu">in</strong> model.parameters():</span><span id="03a3" class="mb lf it nq b gy nz nv l nw nx">     param.requires_grad = <strong class="nq iu">False</strong></span></pre><p id="1c9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们只需要最后一层给出两个概率，即图像是猫还是不是猫，所以让我们重新定义最后一层中输出特征的数量。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="7bf3" class="mb lf it nq b gy nu nv l nw nx">model.fc = nn.Linear(num_ftrs, 2)</span></pre><p id="105a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我们模型的新架构。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="0cd9" class="mb lf it nq b gy nu nv l nw nx">ResNet(<br/>  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<br/>  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>  (relu): ReLU(inplace)<br/>  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)<br/>  (layer1): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (layer2): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (downsample): Sequential(<br/>        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (layer3): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (downsample): Sequential(<br/>        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (layer4): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (downsample): Sequential(<br/>        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))<br/>  (fc): Linear(in_features=512, out_features=2, bias=True)<br/>)</span></pre><p id="bd11" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在要做的就是训练模型的最后一层，我们将能够使用我们重新设计的 vgg16 来预测一幅图像是否是一只猫，只需要很少的数据和训练时间。</p><h1 id="ba37" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated"><strong class="ak">数据量小，数据相似度很低</strong></h1><p id="ab5e" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">考虑这个来自 kaggle 的<a class="ae ny" href="https://www.kaggle.com/kvinicki/canine-coccidiosis" rel="noopener ugc nofollow" target="_blank">数据集，犬球虫病寄生虫的图像。该数据集包含犬等孢菌和等孢菌的图像和标签。卵囊，一种感染狗肠道的球虫寄生虫。它是由萨格勒布兽医学院发明的。它包含了总共 341 张这两种寄生虫的图片。</a></p><figure class="nc nd ne nf gt ng gh gi paragraph-image"><div class="ab gu cl oa"><img src="../Images/49783166850371c91500952f39a5950f.png" data-original-src="https://miro.medium.com/v2/0*GiBuW7gQXHvOAXMk"/></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">Coccidiosis in Dogs</figcaption></figure><p id="f30d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个数据集很小，并且不属于 Imagenet 中的类别，VGG16 就是在 Imagenet 上训练的。在这种情况下，我们保留预训练模型的架构，冻结较低层并保留其权重，然后训练较低层更新其权重以适应我们的问题。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="617c" class="mb lf it nq b gy nu nv l nw nx">count = 0</span><span id="fe1a" class="mb lf it nq b gy nz nv l nw nx"><strong class="nq iu">for</strong> child <strong class="nq iu">in</strong> model.children():</span><span id="454f" class="mb lf it nq b gy nz nv l nw nx">    count+=1</span><span id="9f18" class="mb lf it nq b gy nz nv l nw nx">print(count)</span></pre><p id="1bee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">出局:10</p><p id="b450" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ResNet18 总共有 10 层。让我们冻结前 6 层。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="138e" class="mb lf it nq b gy nu nv l nw nx">count = 0</span><span id="1426" class="mb lf it nq b gy nz nv l nw nx"><strong class="nq iu">for</strong> child <strong class="nq iu">in</strong> model.children():</span><span id="8ec4" class="mb lf it nq b gy nz nv l nw nx">  count+=1<br/>  <strong class="nq iu">if</strong> count <strong class="nq iu">&lt;</strong> 7:</span><span id="2e66" class="mb lf it nq b gy nz nv l nw nx"><strong class="nq iu">    for</strong> param <strong class="nq iu">in</strong> child.parameters():<br/>        param.requires_grad = <strong class="nq iu">False</strong></span></pre><p id="593a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经冻结了前 6 层，让我们重新定义最终的输出层，只给出 2 个输出而不是 1000。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="4708" class="mb lf it nq b gy nu nv l nw nx">model.fc = nn.Linear(num_ftrs, 2)</span></pre><p id="c999" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是更新后的架构。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="857b" class="mb lf it nq b gy nu nv l nw nx">ResNet(<br/>  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<br/>  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>  (relu): ReLU(inplace)<br/>  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)<br/>  (layer1): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (layer2): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (downsample): Sequential(<br/>        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (layer3): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (downsample): Sequential(<br/>        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (layer4): Sequential(<br/>    (0): BasicBlock(<br/>      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (downsample): Sequential(<br/>        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (1): BasicBlock(<br/>      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace)<br/>      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))<br/>  (fc): Linear(in_features=512, out_features=2, bias=True)<br/>)</span></pre><p id="fb5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，训练该模型将更新最后 4 层的权重，以正确区分犬等孢菌和等孢菌。卵囊寄生虫。</p><h1 id="51f8" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated"><strong class="ak">数据集规模很大，但数据相似度很低</strong></h1><p id="d9ab" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">考虑来自 kaggle，皮肤癌 MNIST: HAM10000 <br/>的这个数据集，其是色素性病变的多源皮肤镜图像的大集合，其具有属于 7 个不同类别的超过 10015 个皮肤镜图像，即光化性角化病和上皮内癌/鲍恩氏病(<strong class="kk iu"> akiec </strong>)、基底细胞癌(<strong class="kk iu"> bcc </strong>)、良性角化病样病变(<strong class="kk iu"> bkl </strong>)、皮肤纤维瘤(<strong class="kk iu"> df </strong>)、黑色素瘤(<strong class="kk iu"> mel 【T10 这不是我们能在 Imagenet 中找到的那种数据。</strong></p><p id="58e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们只保留模型架构，而不保留预训练模型的权重。让我们重新定义输出层，将项目分为 7 个类别。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="4615" class="mb lf it nq b gy nu nv l nw nx">model.fc = nn.Linear(num_ftrs, 7)</span></pre><p id="9f53" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在没有 GPU 的机器上训练这个模型需要几个小时，但是如果你运行它足够长的时间，你仍然会得到好的结果，而不需要定义你自己的模型架构。</p><h1 id="4ef0" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated"><strong class="ak">数据量大，数据相似度高</strong></h1><p id="cc44" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">考虑来自 kaggle 的<a class="ae ny" href="https://www.kaggle.com/alxmamaev/flowers-recognition" rel="noopener ugc nofollow" target="_blank"> flowers 数据集</a>。它包含了从 data flicr、google images、yandex images 等网站收集的 4242 幅花卉图片。图片分为五类:洋甘菊、郁金香、玫瑰、向日葵、蒲公英。每个班级大约有 800 张照片。动植物图像是 ImageNet 的一大部分。</p><p id="c5d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是应用迁移学习的理想情况。我们保留预训练模型的架构和每层的权重，并训练模型更新权重以匹配我们的特定问题。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="f203" class="mb lf it nq b gy nu nv l nw nx">model.fc = nn.Linear(num_ftrs, 5)<br/>best_model_wts = copy.deepcopy(model.state_dict())</span></pre><p id="b0cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们复制预训练模型的权重，并初始化我们的模型。我们使用我们的训练和测试阶段来更新这些权重。</p><pre class="nc nd ne nf gt np nq nr ns aw nt bi"><span id="dc5b" class="mb lf it nq b gy nu nv l nw nx">for epoch in range(num_epochs):<br/> <br/>      print(‘Epoch {}/{}’.format(epoch, num_epochs — 1))<br/>      print(‘-’ * 10)</span><span id="bb18" class="mb lf it nq b gy nz nv l nw nx">      for phase in [‘train’, ‘test’]:<br/> <br/>      if phase == 'train':<br/>         scheduler.step()<br/>         model.train()  <br/>      else:<br/>         model.eval()</span><span id="0a30" class="mb lf it nq b gy nz nv l nw nx">      running_loss = 0.0<br/>      running_corrects = 0</span><span id="cc3e" class="mb lf it nq b gy nz nv l nw nx">      for inputs, labels in dataloaders[phase]:<br/> <br/>         inputs = inputs.to(device)<br/>         labels = labels.to(device)<br/>         optimizer.zero_grad()</span><span id="6b95" class="mb lf it nq b gy nz nv l nw nx">         with torch.set_grad_enabled(phase == ‘train’):<br/> <br/>               outputs = model(inputs)<br/>               _, preds = torch.max(outputs, 1)<br/>              loss = criterion(outputs, labels)<br/> <br/>              if phase == ‘train’:<br/>                  loss.backward()<br/>                  optimizer.step()</span><span id="0ee7" class="mb lf it nq b gy nz nv l nw nx">         running_loss += loss.item() * inputs.size(0)<br/>         running_corrects += torch.sum(preds == labels.data)<br/>     <br/>      epoch_loss = running_loss / dataset_sizes[phase]<br/>      epoch_acc = running_corrects.double() / dataset_sizes[phase]</span><span id="1e75" class="mb lf it nq b gy nz nv l nw nx">      print(‘{} Loss: {:.4f} Acc: {:.4f}’.format(<br/>                            phase, epoch_loss, epoch_acc))<br/> <br/>      if phase == ‘test’ and epoch_acc &gt; best_acc:<br/>          best_acc = epoch_acc<br/>          best_model_wts = copy.deepcopy(model.state_dict())</span><span id="bd4f" class="mb lf it nq b gy nz nv l nw nx">print(‘Best val Acc: {:4f}’.format(best_acc))<br/>model.load_state_dict(best_model_wts)</span></pre><p id="3c07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种模型也需要几个小时来训练，但即使只训练一个时期，也会产生很好的效果。</p><p id="459f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">遵循相同的原则，您可以使用任何其他平台中的任何其他预训练网络来执行迁移学习。Resnet 和 pytorch 是为本文随机挑选的。任何其他 CNN 都会给出类似的结果。希望这能让你在用计算机视觉解决现实世界的问题时省下几个小时的痛苦。</p></div></div>    
</body>
</html>