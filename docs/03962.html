<html>
<head>
<title>Demystifying Tensorflow Time Series: Local Linear Trend</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭开张量流时间序列的神秘面纱:局部线性趋势</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/demystifying-tensorflow-time-series-local-linear-trend-9bec0802b24a?source=collection_archive---------2-----------------------#2019-06-22">https://towardsdatascience.com/demystifying-tensorflow-time-series-local-linear-trend-9bec0802b24a?source=collection_archive---------2-----------------------#2019-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="764d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内部 AI </a></h2><div class=""/><div class=""><h2 id="ebe3" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">了解 Tensorflow 如何使用线性动力系统、卡尔曼滤波器和变分推理来模拟时间序列并进行预测</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/717786250edfcc5942a83c0691080724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-JktH3u02NCNn_VtZY2Whg.png"/></div></div></figure><p id="99ce" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在过去的几年里，大公司匆忙发布他们基于机器学习的时间序列库。比如脸书发布了<a class="ae lz" href="https://facebook.github.io/prophet/" rel="noopener ugc nofollow" target="_blank">先知</a>，亚马逊发布了<a class="ae lz" href="https://aws.amazon.com/blogs/opensource/gluon-time-series-open-source-time-series-modeling-toolkit/" rel="noopener ugc nofollow" target="_blank">胶子时间序列</a>，微软发布了<a class="ae lz" href="https://azure.microsoft.com/en-gb/services/time-series-insights/" rel="noopener ugc nofollow" target="_blank">时间序列洞察</a>，谷歌发布了<a class="ae lz" href="https://medium.com/tensorflow/structural-time-series-modeling-in-tensorflow-probability-344edac24083" rel="noopener"> Tensorflow 时间序列</a>。这种流行表明，基于机器学习的时间序列预测需求很高。</p><p id="368e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">本文介绍了 Google 最近发布的<a class="ae lz" href="https://medium.com/tensorflow/structural-time-series-modeling-in-tensorflow-probability-344edac24083" rel="noopener"> Tensorflow 时间序列库</a>。这个库使用概率模型来描述时间序列。在我 7 年的算法交易经验中，我已经养成了分析时间序列库内部运作的习惯。因此，我深入研究了这个库的源代码，以了解 Tensorflow 团队对时间序列建模的看法。</p><p id="ddb9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是一次迷人的旅行。Tensorflow 库将时间序列建模为带参数的线性动态系统。为了学习模型参数，它使用了卡尔曼滤波算法和变分推理。</p><p id="d134" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">本文将解释我们为什么以及如何使用这些技术，以及它们是如何协同工作的。要阅读这篇文章，你不需要知道动态线性系统，卡尔曼滤波器或变分推理。我会边走边介绍他们。我将向你们展示，这些技术自然会出现，使我们能够学习模型参数。</p><p id="42e3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们开始吧。</p><h1 id="3182" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">一些符号</h1><p id="19ec" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">这篇文章是关于时间序列的，所以会有下标。我使用下划线“_”来引导下标。比如我写<em class="mx"> Y_1 </em>，<em class="mx"> Y_t </em>，<em class="mx"> Y_t-1 </em>，<em class="mx"> Y_T+1 </em>的意思是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi my"><img src="../Images/366e83e298fbf1eb36c79c8f009ab63c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gtpTA0U5J1f-JrdZKgiAQg.png"/></div></div></figure><p id="a500" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当我写<em class="mx"> Y_1:t </em>时，我指的是术语的序列(你可以把它理解为术语的数组):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mz"><img src="../Images/702a543d0fc83146b4ef5f76f4ef9522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ytxQPXGDNR09mdw9INO2nw.png"/></div></div></figure><p id="d3aa" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果一些数学符号在你的手机上显示为问号，请尝试从电脑上阅读这篇文章。这是某些 Unicode 呈现的已知问题。</p><h1 id="0743" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">局部线性趋势</h1><p id="267f" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">局部线性趋势是张量流时间序列中最基本的模型。它有以下定义:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi na"><img src="../Images/3fa426648b5f8faa4b42995967cd763b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ms-UkIvPdwCEODSYvUBQ4A.png"/></div></div></figure><p id="a0b9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在上面三个方程中，对于每一个时间步长<em class="mx"> t </em>、<em class="mx"> slope_t </em>、<em class="mx"> level_t </em>、<em class="mx"> y_t </em>都是随机变量。<em class="mx"> ε_1 </em>、<em class="mx"> ε_2 </em>和<em class="mx"> ε_3 </em>也是随机变量。注意<em class="mx"> slope_t </em>、<em class="mx"> level_t </em>和<em class="mx"> y_t </em>依赖于数量，例如<em class="mx"> slope_t-1 </em>，在前面的时间步<em class="mx"> t-1 </em>。但是噪声的分布<em class="mx"> ε_1 </em>、<em class="mx"> ε_2 </em>和<em class="mx"> ε_3 </em>不依赖于时间。</p><p id="6142" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们有观测值<em class="mx"> Y_1 </em>、<em class="mx"> Y_2 </em>直到并包括<em class="mx"> Y_T </em>，或者更简洁地说，<em class="mx"> Y_1:T </em>。为了对这些方程如何生成观测值进行建模，我们使用以下生成性思维:</p><ol class=""><li id="46b3" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">看第三个等式。想象在时间步<em class="mx"> t </em>，观测随机变量<em class="mx"> y_t </em>的分布描述了在这个时间步实际观测可以取什么值。而我们实际观察到的<em class="mx"> Y_t </em>就是这个分布中的一个样本。</li><li id="2251" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">看第二个等式。随机变量<em class="mx"> level_t </em>本身是由时间步<em class="mx"> t-1 </em>的随机变量<em class="mx"> level_t-1 </em>加上添加了高斯噪声的随机变量<em class="mx"> slope_t-1 </em>产生的<em class="mx"> ε_2 </em>。与<em class="mx"> y_t </em>不同，对于<em class="mx"> level_t </em>我们没有实际观测值，所以它是一个<strong class="lf jd">潜变量</strong>。</li><li id="f27d" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">看第一个方程。随机变量<em class="mx"> slope_t </em>由随机变量<em class="mx"> slope_t-1 </em>在步骤<em class="mx"> t-1 </em>产生，带有高斯噪声<em class="mx"> ε_1 </em>。<em class="mx"> slope_t </em>也是一个潜变量。</li></ol><p id="9d87" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">注意独立的高斯噪声<em class="mx"> ε_1 </em>、<em class="mx"> ε_2 </em>和<em class="mx"> ε_3 </em>具有零均值和标准差<em class="mx"> σ_level </em>、<em class="mx"> σ_slope </em>和<em class="mx"> σ_obs。</em>由于这三个量模拟标准偏差，它们只能取正的实数值，用ℝ⁺.表示它们是这个模型的参数。我们不知道他们的价值观；我们需要通过观察来了解它们。</p><p id="74f1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，一个局部线性趋势能建模什么样的时间序列？你可以理解为<em class="mx"> level_t = level_t-1 + ε_2 </em>代表某种趋势水平。这个趋势水平可以以固定的速率变化，由<em class="mx"> ε_2 </em>控制。使用<em class="mx"> slope_t </em>组件，趋势水平可以随时间以不同的速率变化。直观地说，<em class="mx"> slope_t </em>变量决定了一个新的观察值可以从当前的<em class="mx"> level_t </em>改变多少。我想这就是为什么 Tensorflow 的人把它叫做<em class="mx">斜率，</em>从直线方程中类比<em class="mx"> y=ax+b. </em>在直线方程中，<em class="mx"> a </em>叫做斜率，它决定了 y 在一步中可以改变多少<em class="mx">δx</em>。</p><p id="1990" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了获得这个模型的具体感受，我编写了<a class="ae lz" href="https://gist.github.com/jasonweiyi/f88b8d61fc72785b84d248bd6fa86a47" rel="noopener ugc nofollow" target="_blank">这个代码</a>来从中抽取时间序列。下图显示了 30 个时间序列样本。我通过将所有三种噪声的标准偏差设置为 1 来生成这些曲线。你可以看到一些曲线有明显的上升或下降趋势，而另一些曲线有改变方向的趋势。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/f2ecbaa1ff97bfcd91299c5edc8dc5ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wI-W8rbNfan_eW2Z8rowcQ.png"/></div></div></figure><p id="96ca" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果我将<em class="mx"> ε_1 </em>的标准偏差改为 0，有效地禁用<em class="mx"> slope_t </em>变量，我们会得到完全不同的结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/a5d1a21d55bd09ecefaff4b3db13eee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZofDP450U45EDM2_omjYwA.png"/></div></div></figure><p id="3f8e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">正如您所看到的，对模型参数使用不同的值允许您对不同的时间序列进行建模。</p><p id="2a0f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在的问题是，如果我有一个时间序列，比如这个:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/4b06f779c2aa0b9f88550e1cd7e74cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jy3h6yHSs7GKDjf_JV4raw.png"/></div></div></figure><p id="296e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这条曲线是特色图片中山脉的天际线(改编自插画师约书亚·琼斯的<a class="ae lz" href="https://pixabay.com/photos/lake-mcdonald-landscape-mountains-1056561/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>)。它不是你典型的“时间序列”。但我想找点乐子，看看局部线性趋势模型是如何从一座山到另一座山追踪天际线的。</p><p id="e908" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我应该为这三个标准偏差选择什么值来模拟天际线时间序列？幸运的是，我们不需要手动选择这些值。我们将应用机器学习来计算它们的最优值。使用 Tensorflow 时间序列 API，您将编写类似下面清单 1 的代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Listing 1. Using Tensorflow time series API</figcaption></figure><p id="47f6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在清单 1 中，第 9~11 行为 skyline 训练数据定义了一个局部线性趋势模型。第 13~23 行找到了模型参数的最佳值。第 25~38 行使用优化的模型预测未来值。</p><p id="dc5e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">上面的代码片段读起来不错，API 也很直观。但是你可能想知道它是如何工作的，以及这些线条背后的数学原理是什么。我也是。这是我的发现。</p><p id="9eab" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">局部线性趋势的矩阵形式<br/> </strong>我们先把上面的方程改写成矩阵形式，以简化记法:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/bc059f2f70457b7aa3afadf1f576cb8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fwr9HlwkOs3kVfOXzjwxRg.png"/></div></div></figure><p id="6fd5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mx"> w_t </em>是一个二维多元高斯变量。一个组件尺寸用于<em class="mx"> ε_1 </em>，另一个用于<em class="mx"> ε_2 </em>。这个多元高斯函数的平均值为 0。它的协方差矩阵是对角的，方差<em class="mx"> ε_1 </em>和<em class="mx"> ε_2 </em>在其主对角线上。<em class="mx"> w_t </em>均值为 0，因为随机变量<em class="mx"> ε_1 </em>和<em class="mx"> ε_2 </em>均值为 0。<em class="mx"> w_t </em>的协方差矩阵是对角的，因为<em class="mx"> ε_1 </em>和<em class="mx"> ε_2 </em>是独立的。同样，我们可以把<em class="mx"> v_t </em>看成一个只有一维的多元高斯变量。再次注意，<em class="mx"> w_t </em>和<em class="mx"> v_t </em>不依赖于前一时间步<em class="mx"> t-1 </em>的数量，不同于<em class="mx"> level_t、slope_t </em>和<em class="mx"> y_t </em>。</p><p id="4e03" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们为不同的矩阵命名:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/0518cb77092462c22a570b3d0058411c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jRMajkaw--8WywJTWVI6tg.png"/></div></div></figure><p id="f924" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后我们得到相同方程的更紧凑的符号:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/518b279e3b5e36d4dbf96fb4f16649d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GBfZAVC0ud-GeKUN_ASyiQ.png"/></div></div></figure><p id="d136" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这两个方程描述了系统如何随时间演化。我们称这个系统为线性动力系统。换句话说，Tensorflow 人所说的局部线性趋势模型，和线性动力系统是一样的。它们是同义词。</p><p id="7274" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在该系统中，<em class="mx"> x_t </em>被称为时间步<em class="mx"> t </em>的状态变量。<em class="mx"> y_t </em>称为观测变量。该系统是线性的，因为<em class="mx"> x_t </em>和<em class="mx"> y_t </em>线性依赖<strong class="lf jd">于<em class="mx"> x_t-1 </em>。</strong></p><p id="c3db" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">上述线性系统是以归纳的方式定义的——它描述了时间步长 t 的量是如何从先前时间步长<em class="mx"> t-1 </em>的量产生的。</p><p id="fd3d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，系统从哪里开始(或者这个归纳定义的基础情况是什么)？可以假设系统从任意初始状态开始，比如说<em class="mx"> level_0 = 0 </em>，<em class="mx"> slope_0 = 0。</em>换句话说，<em class="mx"> x_0 = [0，0]，</em>或者你可以选择<em class="mx"> x_0 =[3，5]。</em>无所谓。<em class="mx"> </em>即使你的初始选择相当不正确，经过几步，系统状态也会远离这个错误的初始状态，变得与实际观测值更加一致。如果你还不明白为什么，不要担心，我们以后会回到这个重要的问题上来。</p><p id="eb95" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">模型参数<br/> </strong>以上线性动力系统有三个参数<em class="mx"> σ_level </em>、<em class="mx"> σ_slope </em>和<em class="mx"> σ_obs。</em>我们称它们为<strong class="lf jd">模型参数</strong>。在我们使用这个模型预测未来的观测值之前，我们需要计算出这些模型参数的值。这叫做<strong class="lf jd">参数学习</strong>。</p><h1 id="3855" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">参数学习</h1><p id="4eb1" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">Tensorflow 时间序列将局部线性趋势视为贝叶斯模型。在贝叶斯设置中，参数学习意味着在给定观察数据的情况下计算模型参数的后验分布。定义<em class="mx"> Y </em>为观测值<em class="mx"> Y_1 </em>到<em class="mx"> Y_T </em>的向量。<em class="mx"> Y </em>是长度为<em class="mx"> T </em>的向量。定义<em class="mx"> z </em>为模型参数的向量<em class="mx"> σ_level，σ_slope，σ_obs </em>。<em class="mx"> z </em>是长度为 3 的向量。</p><p id="7f7e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们使用<em class="mx"> p(z|y) </em>来表示后验密度函数。在这个公式里面，<em class="mx"> z </em>是我们模型参数的向量。定义<em class="mx"> y </em>为第二系统方程中引入的观测变量<em class="mx"> y_1，y_2，…，y_T </em>的向量。<em class="mx"> y </em>的长度为<em class="mx"> T </em>。注意大写字母和小写字母的用法。上部<em class="mx"> Y </em>是实际观测值的矢量；而下方的<em class="mx"> y </em>是随机观测变量的向量。</p><p id="4adc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在本文中，大写字母<em class="mx"> A，H，Y </em>表示常数，小写字母<em class="mx"> x，Y，σ，</em>表示变量。除了<em class="mx">σ</em>，我用来表示噪声的协方差，比如<em class="mx">σ_ w，σ_ v</em>。</p><p id="21f3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">后验密度<em class="mx"> p(z|y) </em>通过贝叶斯法则计算:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/4a605782cad1ff8bd353f2293ed32c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7BU3uWbAksrtgQnJ8COYDw.png"/></div></div></figure><p id="f2ca" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该公式表明，计算后验<em class="mx"> p(z|y) </em>需要三条信息:</p><p id="934f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">(1)先验分布<em class="mx"> p(z) </em>，我们在看到任何观察值之前对模型参数看起来如何的假设。</p><p id="6c2c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">(2)可能性<em class="mx"> p(y|z) </em>，观察值的概率，如果它们是由我们的模型生成的话。</p><p id="5516" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">(3)边际似然，是分母中的积分。该积分将等式的整个右侧换算成积分为 1 的适当概率密度函数。</p><p id="f266" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们需要计算这三个部分。作为预览:(1)是琐碎的，(2)需要一些工作，(3)是困难的。让我们一步一步来。</p><h2 id="e130" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">先验 p(z)</h2><p id="7e1a" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">计算先验是微不足道的，因为我们完全控制指定先验。在局部线性趋势模型中，我们<strong class="lf jd">假设</strong>三个模型参数的先验来自<a class="ae lz" href="https://en.wikipedia.org/wiki/Log-normal_distribution" rel="noopener ugc nofollow" target="_blank">对数正态</a>分布。所以这些模型参数也是随机变量。这些对数正态分布有固定的自变量来决定概率密度的形状。我们不在乎这种形状。使用对数正态分布的主要原因是模拟标准偏差，在ℝ⁺.标准偏差只能取正值</p><p id="c6be" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们还<strong class="lf jd">假设</strong>三个模型参数<em class="mx"> σ_level、σ_slope </em>和<em class="mx"> σ_obs </em>相互独立。因此，先验概率 p(z) 等于三个独立对数正态概率密度的乘积:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/6549c0716c8a5827d69dce049394687d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K9mkdHMB00uJN13_2MjSLA.png"/></div></div></figure><p id="8fc0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中<em class="mx">LN(σ_ level；)</em>代表具有一些固定自变量的对数正态概率密度函数。在批注中<em class="mx">LN(σ_ level；)</em>，<em class="mx"> LN </em>是概率密度函数的名字，这里是对数正态。在括号内，分号前，<em class="mx"> σ_level </em>表示这个概率密度是针对随机变量<em class="mx"> σ_level </em>的。在分号之后，我们放入密度函数的参数。对数正态密度有两个参数。这里我们用一个点" "来表示我们不关心那些固定参数的值。</p><p id="5458" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你会注意到最后一条蓝线和它奇怪的解释“API”。这是一种明确显示函数参数的符号。<em class="mx"> λz.p(z) </em>表示<em class="mx"> p(z) </em>是一个函数(我们这里是一个概率密度函数)，这个函数取单自变量<em class="mx"> z </em>。有编程语言背景的人会立刻看出这个符号是λ演算的λ表达式。λ是微积分中定义函数的符号。点等于“≐”符号读“有 API 的”。</p><p id="1d76" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">λ符号使得函数的参数显式化。随着我们将构建越来越多的函数，簿记函数 API 帮助我们“接口检查”数学。请注意，这种簿记并没有改变数学。如果您不关心函数 API，您可以放心地忽略所有蓝色的线。</p><p id="9efa" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我强调，先验的<em class="mx"> p(z) </em>是我们的<strong class="lf jd">建模假设</strong>，即模型参数相互独立，并且都来自对数正态分布。我们为他们继续建模提供了最佳猜测，但我们不知道这是否属实。</p><h2 id="b2e6" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">可能性 p(y|z)</h2><p id="d1f1" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">可能性是在给定模型参数<em class="mx"> z </em>的情况下观察数据 y 的概率。我们将可能性表示如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/1cb2891a724e85f27c20d61b56c88898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIJioOOfi-1OdQJi07Q4Rg.png"/></div></div></figure><p id="19e7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个公式是所有概率模型的通用模板。我们需要为我们的局部线性趋势模型计算出<em class="mx"> p(y|z) </em>的解析形式。让我们通过逆向应用概率论中的<a class="ae lz" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)" rel="noopener ugc nofollow" target="_blank">乘积法则</a>(也叫链式法则)将其改写成乘积形式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/efd306dd42ed83e966b05c588622adf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FBpSJvMjpVQHfuyeq0pXxQ.png"/></div></div></figure><p id="a41c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以看到，这个产品中的所有术语都具有相同的结构。让我们来看一个任意的术语:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/745d4af422f54bddd3f76873d6974d5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NS4-WzBYAud3G36hBgsfrA.png"/></div></div></figure><p id="12be" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">此概率条件下的符号<em class="mx"> y_1:t-1 </em>表示<em class="mx"> y_1 </em>至<em class="mx"> y_t-1 </em>。我们的目标是写出这一项的解析式。如果我们能做到这一点，我们就能对可能性中的所有项做同样的处理。</p><p id="8fbe" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了推导上述表达式的解析形式，我们需要使用来自线性动力系统的两个方程。让我再次展示它的定义:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/518b279e3b5e36d4dbf96fb4f16649d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GBfZAVC0ud-GeKUN_ASyiQ.png"/></div></div></figure><p id="1920" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">系统是用归纳的方式定义的，所以我们用一些归纳的思维。</p><p id="4213" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们来看第一个方程。这个等式告诉我们<em class="mx"> x_t </em>是从<em class="mx"> x_t-1 </em>加高斯噪声<em class="mx"> w_t </em>的线性变换。如果<em class="mx"> x_t-1 </em>是一个高斯随机变量，那么高斯的一个性质告诉我们，一个加了高斯噪声的高斯变量的<strong class="lf jd">线性变换会产生另一个高斯变量</strong>。</p><p id="de50" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所以如果我们<strong class="lf jd">假设</strong>随机变量<em class="mx"> x_t-1|y_1:t-1，z </em>是一个均值为<em class="mx"> μ_t-1 </em>，方差为<em class="mx">σ_ t-1</em>的高斯随机变量，并且我们知道<em class="mx"> μ_t-1 </em>和<em class="mx">σ_ t-1</em>的解析形式(我们将在后面证明这个假设):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/56274c0a5ceabc127e3aa32600887bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Rbae7VFRnYm_Spv0XJmlA.png"/></div></div></figure><p id="152d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">那么我们就可以<strong class="lf jd">得出</strong>结论<em class="mx"> x_t | y_1:t-1，z </em>(请注意下标区别:<em class="mx"> x_t-1 </em>在假设中，<em class="mx"> x_t </em>在结论中)也是一个高斯随机变量，高斯线性变换的规则也告诉我们其均值和方差的公式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/185f4145af4a7e266b9cce26846411a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DENDslzTtxpca9HycpLM1w.png"/></div></div></figure><p id="394b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，为什么我们需要在<em class="mx"> y_1:t-1，z 上条件<em class="mx"> x_t </em>？</em>这是因为<em class="mx"> x_t </em>可以取什么值取决于我们在时间步长<em class="mx"> t </em>之前看到的观察值和模型参数。这是常识。如果前面的观察值是 100，那么<em class="mx"> x_t </em>的分布也在 100 左右是有意义的。换句话说，<em class="mx"> x_t </em>取决于随机变量<em class="mx"> y_1:t </em>和<em class="mx"> z </em>。</p><p id="f263" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，我们怎么知道<em class="mx"> x_t </em>，其中<em class="mx"> t=1，2，..，T </em>，一开始是高斯变量吗？这很简单:我们可以假设系统从某个固定的初始高斯分布开始，例如，<em class="mx">x _ 0 ~ N(</em><strong class="lf jd"><em class="mx">0</em></strong><em class="mx">，</em> <strong class="lf jd"> <em class="mx"> 1 </em> </strong> <em class="mx">)。</em> <strong class="lf jd"> <em class="mx"> 0 </em> </strong>是长度为 2 的零向量，因为<em class="mx"> x_t </em>是一个二维向量。<strong class="lf jd"> <em class="mx"> 1 </em> </strong>是大小为 2x2 <em class="mx">的单位矩阵。注意，这也是我们之前定义的系统的基本情况。然后我们知道<em class="mx"> x_1|y_1，z </em>是高斯，还有<em class="mx"> x_2|y_1:2，z </em>，…，<em class="mx"> x_t|y_1:t，z </em>。</em></p><p id="8727" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，如果<em class="mx"> x_0 </em>的均值为 0，换句话说，<em class="mx"> μ_0=0 </em>，所有的后件<em class="mx"> x_t|y_1:t-1，z </em>的均值都是前面均值的线性变换，那么<em class="mx"> x_t </em>的均值会一直为 0 吗<em class="mx"> μ_t=0 </em>？答案是否定的，因为<em class="mx"> μ_t </em>能取什么值取决于实际观测<em class="mx"> Y_1:t </em>。我们将在下一部分<em class="mx">卡尔曼滤波器</em>部分对此进行详细解释。</p><p id="065c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，让我们继续讨论第二个系统方程。既然我们已经表明<em class="mx"> x_t|y_1:t-1，z </em>是一个高斯变量，同样根据高斯线性变换的规则，<em class="mx"> y_t|y_1:t-1，z </em>也是一个高斯变量。将<em class="mx"> x_t|y_1:t-1，z </em>的均值和方差代入转换公式，得到<em class="mx"> y_t|y_1:t-1，z </em>的概率密度:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/b390e9c74bec281b59a0b44344fb6fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5CJexfFxBSm_sI3bJA1aCQ.png"/></div></div></figure><p id="a831" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是我们的可能性<em class="mx"> p(y|z) </em>中任意项的分析形式。这个公式很长，但是它是如何推导出来的很简单，我们根据线性动力系统的第一个和第二个方程应用高斯线性变换规则两次。</p><p id="a16f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个多元高斯线性变换规则在机器学习中非常重要，绝对值得你花时间去记住它。它会在很多模型中弹出，比如<a class="ae lz" rel="noopener" target="_blank" href="/understanding-gaussian-process-the-socratic-way-ba02369d804">高斯过程</a>，甚至在<a class="ae lz" rel="noopener" target="_blank" href="/where-do-confidence-interval-in-linear-regression-come-from-the-case-of-least-square-formulation-78f3d3ac7117">最小二乘线性回归</a>中对不确定性进行推理。</p><h2 id="8042" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">卡尔曼滤波器</h2><p id="6225" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">还有一件事，上述所有推导都假定我们知道下列各项的解析形式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/126aa5a7b8040a0a5da0ef8fcde37bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9TiaGOwcCQbvC8YrGnnw8A.png"/></div></div></figure><p id="63cb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们需要证明这个假设是正确的。具体来说，我们需要证明我们可以推导出从<em class="mx"> 1 </em>到<em class="mx"> T </em>的所有<em class="mx"> t </em>的<em class="mx"> x_t|y_1:t，z </em>的解析形式。我们将进行归纳证明。别担心，证明很简单。</p><p id="3e08" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们想假设我们知道:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/e2dfb977fe1f6f14b18d2e6da8f4e6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4EGXS4FhYCAEOcfEm5bFw.png"/></div></div></figure><p id="18fb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">并证明我们可以导出的解析形式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/56589df14d75679d771c8ca5c92ee82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_61n8v6aKK_5xJ-W5-BBA.png"/></div></div></figure><p id="11c3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请注意归纳假设中的下标和证明义务，以意识到我们正试图在下一个时间步中证明一些量，基于前一个时间步的量和新的观察。</p><p id="ddf7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个证明义务意味着我们要找出<em class="mx"> μ_t </em>和<em class="mx">σ_ t</em>的解析形式。不出意外的话，应该是<em class="mx"> μ_t-1 </em>和<em class="mx">σ_ t-1</em>的表达式。</p><p id="b695" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们开始证明:</p><p id="5ef4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">基步:我们知道<em class="mx"> p(x_0) = N( </em> <strong class="lf jd"> <em class="mx"> 0，1 </em> </strong> <em class="mx"> ) </em>。因此<em class="mx"> μ_0 </em>的解析形式为<strong class="lf jd"> <em class="mx"> 0 </em> </strong>，<em class="mx">σ_ 0</em>为<strong class="lf jd"> <em class="mx"> 1 </em> </strong>。</p><p id="46ac" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">归纳步骤:给定归纳假设:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/e2dfb977fe1f6f14b18d2e6da8f4e6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4EGXS4FhYCAEOcfEm5bFw.png"/></div></div></figure><p id="5cac" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">根据我们线性动力系统中的两个方程和高斯线性变换法则，我们可以推导出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/628eb8815929bf102c1c50e5b060651e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q5EfUhkSUydfbMKNQX1_PQ.png"/></div></div></figure><p id="585e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我用<em class="mx"> μ_a </em>，<em class="mx">σ_ aa，μ_b，σ_ bb</em>命名均值和方差来缩短公式，因为公式会变长。但是不要担心，公式会以一种不经大脑的方式变得更长——它们变得更长是因为我们将它们代入两个多元高斯分布规则。出于本文的目的，您可以放心地接受这些规则。它们是:联合概率密度和多元高斯的条件概率密度。</p><p id="56de" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">规则 1 </strong>:利用多元高斯的<a class="ae lz" href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf" rel="noopener ugc nofollow" target="_blank">联合概率密度规则，我们可以推导出<em class="mx"> x_t </em>和<em class="mx"> y_t </em>之间的联合概率:</a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/48cd8522039d56335910d989a1de8326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WJQncci2NTU7b_sef43rBQ.png"/></div></div></figure><p id="a644" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以对多元高斯应用联合概率密度规则，因为<em class="mx"> x_t|y_1:t-1 </em>和<em class="mx"> y_t|y_1:t-1 </em>都是多元高斯变量。</p><p id="e0ee" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在上面的公式中，名称<em class="mx">σ_ ab</em>和<em class="mx">σ_ ba</em>(它们是相互转置的)是<em class="mx"> x_t|y_1:t-1，z </em>和<em class="mx"> y_t|y_1:t-1，z </em>之间的协方差。我们也可以推导出它们的解析形式。例如:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/edcefe3b04223a760d2ef169f33498c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z96ILX06yrK-CbdHBVJz_A.png"/></div></div></figure><p id="86a4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在上面的推导中，为了节省空间，我去掉了条件部分<em class="mx"> y_1:t-1 </em>。所以请记住上面公式中的<em class="mx"> x_t </em>和<em class="mx"> y_t </em>的意思是<em class="mx"> x_t|y_1:t-1，y_t|y_1:t-1 </em>。</p><p id="10ef" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(1)行是<em class="mx"> x_t|y_1:t-1 </em>和<em class="mx"> y_t|y_1:t-1 </em>之间协方差的表示法。</p><p id="f683" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">线(2)是两个随机变量之间的协方差的定义。</p><p id="99d9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(3)行用第二个系统方程代替<em class="mx"> y_t|y_1:t_1 </em>及其均值。</p><p id="0407" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(4)行重新组织公式中的术语，并使用期望的线性属性将第(3)行中的期望分成两个期望。我们可以认识到第一个期望是<em class="mx"> x_t|y_1:t-1 </em>的方差的定义。</p><p id="e4b6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(5)行和第(6)行插入<em class="mx">σ_ aa</em>求<em class="mx"> x_t|y_1:t-1 的方差。</em></p><p id="2ecb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(7)行显示噪声的期望值<em class="mx"> v </em>为 0，因为<em class="mx"> v </em>来自 0 均值高斯分布。</p><p id="d9cf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">规则 2 </strong>:由于上述联合是二维高斯分布，我们可以使用<a class="ae lz" href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf" rel="noopener ugc nofollow" target="_blank">多元高斯条件规则</a>写下条件概率:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/11f3e156345bcb42a0adf1a8113cba70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qrg6sImOCisLxDJxRMEOvw.png"/></div></div></figure><p id="2a9f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第一行是条件概率表示。第二行显示其解析形式，即<em class="mx"> μ_t </em>和<em class="mx">σ_ t</em>的公式。它们是由多元高斯条件规则给出的。第三行重新组织了条件中的术语，以安抚您的眼睛。</p><p id="bcda" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因此我们证明了我们可以推导出<em class="mx"> x_t|y_1:t，z. </em>的概率密度函数的解析形式</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/5132d51f08b8bcd112aa6ad9f08ac89f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x4ySXHHQKx6LJvNZ3mjX5A.png"/></div></div></figure><p id="958c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">证明结束。</p><p id="fcb5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们来考察一下证明中的最后一个公式，对于<em class="mx"> p(x_t|y_1:t，z) </em>的那个公式。这个公式告诉我们一些重要的事情。</p><p id="587b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，从上面公式的第二行可以看出，<em class="mx"> μ_t </em>和<em class="mx">σ_ t</em>只依赖于随机变量<em class="mx"> y_t </em>和模型参数<em class="mx"> z </em>(因为其中的项，如，<em class="mx"> μ_a，σ_ ab</em>，都是<em class="mx"> y_1:t-1 </em>和<em class="mx"> z </em>的表达式)。换句话说，<em class="mx"> μ_t </em>和<em class="mx">σ_ t</em>是<em class="mx"> y_1:t </em>和<em class="mx"> z </em>的函数。认识到这一点后，让我们再次看看可能性中的任意项:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/f9da27bf16fb0a487ca7b3db2b6fe181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Mu1jyLlKbqA5GQMLgm6iA.png"/></div></div></figure><p id="d568" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个项依赖于<em class="mx"> μ_t-1 </em>和<em class="mx">σ_ t-1</em>，所以过渡性地依赖于<em class="mx"> y_1:t-1 </em>和<em class="mx"> z </em>。从 API 的角度来看，这个术语开始是一个带有三个参数的函数，<em class="mx"> y_t，y_1:t-1 </em>和<em class="mx"> z </em>，如第 2 行所示。我们有实际观测<em class="mx"> Y_1:t-1 </em>。在我们将观察值<em class="mx"> Y_t </em>和<em class="mx"> Y_1:t-1 </em>插入到前两个自变量中之后，整个项就变成了一个只将<em class="mx"> z </em>作为其单一自变量的函数。这显示在第 3 行。</p><p id="3778" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">将此推广到<em class="mx"> p(y|z) </em>中的所有项，我们可以看到，当插入观察值<em class="mx"> Y </em>时，可能性是一个以<em class="mx"> z </em>作为其单一自变量的函数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/6848663af074732e9b922a37657c7835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZrEGcWo5ovmhhOejkV9dKQ.png"/></div></div></figure><p id="7532" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其次，让我们了解状态变量<em class="mx"> x_t </em>的均值是如何随时间变化的。让我们一起来看看<em class="mx"> μ_t </em>的表达式以及<em class="mx"> μ_a </em>和<em class="mx"> μ_b </em>的定义:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/87fc3fb96ab9e94ceee737c09ec21eb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pIZffHDlF-P6bLSBpt9vHQ.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/629dec1194d8d3f8bffff9b2d6bf7652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qWbjd0ti6irIbHs9Sz2gBQ.png"/></div></div></figure><p id="fd29" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以看到<em class="mx"> μ_a = Aμ_t-1 </em>是系统认为在看到新的观察值<em class="mx"> Y_T </em>之前<strong class="lf jd">的状态。该算法然后使用新的观察值<em class="mx"> Y_T </em>来<strong class="lf jd">校正</strong>其关于该量的信念，从而为下一时间步<em class="mx"> t </em>产生新的校正后的信念<em class="mx"> μ_t </em>。</strong></p><p id="8e1e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个关于<em class="mx"> μ_t </em>的解释也解决了之前的一个问题，问<em class="mx"> μ_t </em>如果从 0 开始是否会一直为 0。答案是否定的。现在你明白为什么了。如果实际观测值不为 0，校正公式将快速迫使均值<em class="mx"> μ_t </em>远离 0。</p><p id="2dba" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这些证明步骤也可以理解为著名的<strong class="lf jd">卡尔曼滤波算法</strong>——一种从以前的状态和新的观测值推导出当前系统状态分布的算法。卡尔曼滤波算法有许多解释。我发现上面的版本从概率的角度来看非常直观。</p><p id="5300" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于卡尔曼滤波器是如此重要的算法，让我总结一下:对于具有高斯噪声的线性动态系统:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/518b279e3b5e36d4dbf96fb4f16649d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GBfZAVC0ud-GeKUN_ASyiQ.png"/></div></div></figure><p id="9b09" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">假设我们知道潜在变量<em class="mx"> x_t-1 </em>在时间步长<em class="mx"> t-1 </em>的分布。给定一个新的观测值<em class="mx"> y_t </em>，卡尔曼滤波算法导出潜在变量<em class="mx"> x_t </em>在下一时间步<em class="mx"> t </em>的分布。这个过程叫做过滤。其步骤是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/25e271152fa8b21cad5bf6216166a86c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jz9g2Cn3u3WscTRlajxTjg.png"/></div></div></figure><p id="eb3c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请注意这些步骤中的下标。下标的变化表明算法如何从系统状态<em class="mx"> x_t-1 </em>开始并导出系统状态<em class="mx"> x_t </em>。每一步都优雅地应用了一个变换。</p><p id="65e7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请注意，上述算法中的所有五项都代表概率密度函数。当新的观察可用时，该算法发展状态变量<em class="mx"> x </em>的概率密度。前三步告诉你线性动力系统认为下一个系统状态和观测应该是什么。最后两步使用实际观察来更新这种信念。</p><p id="6681" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是一个美丽的算法！</p><p id="a3ef" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回到清单 1，第 9~11 行描述了一个局部线性趋势模型。这个模型类有返回先验分布和似然概率的方法。</p><p id="4fb3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">唷，我们已经写下了先验<em class="mx"> p(z) </em>和似然<em class="mx"> p(y|z) </em>的解析形式。现在我们来看三者中的最后一个——边际可能性。</p><h2 id="49c1" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">边际可能性</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/c1106b4868c5e50bd229c070fa23f7d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*24klDxYF3aq_Z2itDGn3fg.png"/></div></div></figure><p id="3c97" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">边际似然是似然和先验乘积的积分。由于<em class="mx"> z </em>已经被积分出来，结果是一个以<em class="mx"> y </em>作为其单一参数的函数，如第二行所示。将<em class="mx"> Y=Y_1:t </em>代入<em class="mx"> y </em>后，整个边际似然变成一个常数，显示在第三行。常数用“λ”表示在λ之后和点“.”之前没有任何内容的部分。</p><p id="ccb9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">不幸的是，积分很难用解析的方法计算。在我们的例子中，<em class="mx"> p(y|z) </em>是许多高斯密度函数的乘积，每个观测值一个。<em class="mx"> p(z) </em>是三个对数正态概率密度函数的乘积，每个函数对应一个模型参数。即使写下他们的描述，我也能感受到其中的复杂性。</p><p id="c169" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于<em class="mx">很难解析地</em>计算边际似然，而边际似然是后验<em class="mx">、</em>的先决条件，我们也无法解析地计算后验<em class="mx"> p(z|y) </em>。我们需要在数字上近似后验<em class="mx"/>。</p><p id="a159" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">近似后验分布意味着用新的分布代替后验分布。这个新的分布应该类似于真实的后验分布。它应该有简单的解析形式。Tensorflow 时间序列使用变分推理技术来寻找这种新的分布。</p><h1 id="5069" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">近似后验概率的变分推断</h1><p id="c5aa" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">变分推断用另一个分布<em class="mx"> q(z) </em>逼近后验分布<em class="mx"> p(z|y) </em>，称为<strong class="lf jd">变分分布</strong>。我们先想想<em class="mx"> q(z) </em>应该是什么样子。</p><ol class=""><li id="1a50" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated"><em class="mx"> q(z) </em>的结构应该很简单，所以我们可以很容易地用它进行分析。</li><li id="4bcc" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><em class="mx"> q(z) </em>和<em class="mx"> p(z|y) </em>在同一组变量 z 上；每个变量取相同的一组值；并且<em class="mx"> q(z) </em>作为概率密度函数，应该积分为 1。换句话说，<em class="mx"> q(z) </em>应该和<em class="mx"> p(z|y) </em>有相同的 API。</li><li id="3af8" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><em class="mx"> q(z) </em>需要尽可能的接近<em class="mx"> p(z|y) </em>，所以近似于<em class="mx"> p(z|y) </em>。</li></ol><h2 id="6861" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">变分分布 q(z)</h2><p id="0fc4" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">对于<em class="mx"> q(z) </em>我们应该提出什么概率密度函数？Tensorflow 时间序列对<em class="mx"> q(z) </em>使用一个<a class="ae lz" href="https://arxiv.org/pdf/1601.00670.pdf" rel="noopener ugc nofollow" target="_blank">平均场变分族</a>。</p><p id="2335" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">平均场族是对<em class="mx"> z </em>中随机变量之间关系的限制——它假设所有变量彼此独立。这种限制的好处是联合概率密度<em class="mx"> q(z) </em>等于个体变量概率密度的乘积。</p><p id="cab0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了完全指定<em class="mx"> q(z) </em>，我们还需要选择<em class="mx"> z </em>中每个变量的概率密度。为此，Tensorflow 时间序列为<em class="mx"> z </em>中的每个随机变量选择高斯分布。在公式中:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/1552e81cff95400ec182d10ed5eccbf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h3pzs-avcBu4HSeV1JeDGw.png"/></div></div></figure><p id="d0b2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中<em class="mx">N(σ_ level；第 3 行的μ_l，σ_l </em> <em class="mx"> ) </em>代表变量<em class="mx"> σ_level </em>的高斯概率密度函数，均值<em class="mx"> μ_l </em>和标准差<em class="mx"> σ_l. </em>同样适用于模型参数<em class="mx"> σ_slope </em>和<em class="mx"> σ_obs。</em></p><p id="46e0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第 4 行和第 5 行显示<em class="mx"> q(z) </em>有自己的参数<em class="mx"> vp </em> =[ <em class="mx"> μ_l，μ_s，μ_o，σ_l </em>，<em class="mx"> σ_s，σ_o </em> ]，一个长度为 6 的向量。这些参数指定了三种独立的高斯分布。我们将这六个参数<strong class="lf jd">称为变分参数</strong>，以区别于我们之前介绍的局部线性趋势模型的参数<strong class="lf jd">模型参数</strong>。</p><p id="4afb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第 5 行显示<em class="mx"> q(z) </em>以<em class="mx"> z </em>和<em class="mx"> vp </em>为自变量。<em class="mx">q(z；vp) </em>，分号在内，表示变量<em class="mx"> z. </em>的一个概率密度，这个密度有参数<em class="mx"> vp </em>来控制它的形状。注意，在第 1 行，<em class="mx"> q(z) </em>也是<em class="mx"> z </em>和<em class="mx"> vp </em>的函数。这一点从第 1 行到第 5 行没有变化。我们把它写成<em class="mx"> q(z) </em>而没有明确提到<em class="mx"> vp </em>只是为了强调<em class="mx"> q(z) </em>是<em class="mx"> z </em>中随机变量的概率密度函数。</p><p id="7faf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请注意模型参数<em class="mx"> z </em>和变分参数<em class="mx"> vp: </em>之间的关系</p><ol class=""><li id="070a" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated"><em class="mx"> z </em>是我们局部线性趋势模型的参数。z 的实际取值决定了我们可以建立什么样的时间序列模型。提醒自己我们在本文开头看到的不同样本时间序列。</li><li id="15df" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><em class="mx"> vp </em>是<em class="mx"> z </em>的概率密度函数的参数。<em class="mx"> z </em>建模为随机变量，变分参数<em class="mx"> vp </em>控制<em class="mx"> q(z) </em>的形状，<em class="mx"> z </em>的概率密度函数。</li></ol><p id="d86b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回到清单 1，第 14 行构建了变分分布<em class="mx"> q(z) </em>。由于<em class="mx"> q(z) </em>具有均值场结构，所以 Python 代码中的<em class="mx"> qs </em>变量就是一个字典，每个键值对就是一个模型参数及其对应的变分分布。</p><p id="1190" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们检查一下这个<em class="mx"> q(z) </em>是否满足我们的要求:<em class="mx"> q(z) </em>具有简单的结构和解析形式。此外，<em class="mx"> q(z) </em>与<em class="mx"> p(z|y) </em>具有相同的变量集。然而，<em class="mx"> q(z) </em>中的变量与<em class="mx"> p(z|y) </em>中的变量具有不同的定义域。</p><h2 id="b41d" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">随机变量域不匹配的问题</h2><p id="99d1" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated"><em class="mx"> q(z </em>)中的变量来自高斯分布，因此它们具有全实数域，用ℝ.表示</p><p id="e3f0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">后验<em class="mx"> p(z|y) </em>变量的定义域怎么样？后<em class="mx"> p(z|y) </em>变量的定义域与前<em class="mx"> p(z) </em>变量的定义域相同。<em class="mx"> p(z) </em>是对数正态密度函数的乘积，其定义域是正数ℝ⁺.</p><p id="b7c0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mx"> q(z) </em>中的变量与<em class="mx"> p(z|y) </em>中的变量之间存在域不匹配。这种域不匹配违反了上述<em class="mx"> q(z) </em>的第二个要求。为了解决这个问题，我们将ℝ⁺域上变量的概率密度函数的后验概率 p(z|y) 转换成ℝ域上变量向量<em class="mx"> u </em>的另一个概率密度函数<em class="mx"> p(u|y) </em>。</p><p id="0335" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，我们不知道后验<em class="mx"> p(z|y) </em>的解析形式，怎么进行这种变换呢？嗯，我们知道后鼻孔的结构:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/a003646645cf0db4f34219daf11b0877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_B6fUhJCMtWNlJuGH0wOkQ.png"/></div></div></figure><p id="df44" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中<em class="mx"> m </em>代表边际可能性的倒数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/2afdcaf2b67293223fde00e6c1564a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u5IhZRA5HHoJsJyWV-9SdQ.png"/></div></div></figure><p id="6b6a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mx"> m </em>是一个不涉及<em class="mx"> z </em>的常数，因为<em class="mx"> z </em>已经被积分掉了。当插入观察值<em class="mx"> Y </em>时，后验概率就变成了一个以<em class="mx"> z </em>为唯一自变量的函数。这是因为它是一个常数和两个函数的乘积，这两个函数都将<em class="mx"> z </em>作为它们唯一的自变量。上面的蓝线清楚地显示了这个结构。</p><p id="a20d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们还知道<em class="mx"> p(y|z) </em>和<em class="mx"> p(z) </em>的解析形式，因此我们可以转换它们的乘积，常数<em class="mx"> m </em>只是缩放转换后的结果。</p><p id="cfba" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能还会问，我们为什么不把 q(z) 转换成正实域ℝ⁺上的另一个分布呢？因为这通过给它添加约束使得<em class="mx"> q(z) </em>更加复杂。我们想要一个简单的<em class="mx"> q(z) </em>，这就是变分推理的要点。</p><p id="aaaf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们使用<a class="ae lz" href="https://www.cl.cam.ac.uk/teaching/2003/Probability/prob11.pdf" rel="noopener ugc nofollow" target="_blank">概率密度变换</a>的技术。</p><h2 id="d6d8" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">概率密度变换</h2><p id="af4f" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">让我们重新陈述一下当前的目标。有时候在一个漫长的过程中，我们很容易忘记我们想要做什么。</p><p id="ed5a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">概括地说，在正ℝ⁺域上，我们有后验<em class="mx"> p(z|y) = m×p(y|z)p(z) </em>和变量<em class="mx"> z </em>。我们想把它转换成另一个分布<em class="mx">p(u | y)</em>u 中的每个变量都来自于整个ℝ域。这样，后验密度和变分分布密度在其输入变量上将具有相同的域。</p><p id="82cb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mx"> u </em>会是什么样子？由于<em class="mx"> z </em>中的三个变量<em class="mx"> σ_level、σ_slope </em>和<em class="mx"> σ_obs </em>是独立的，我们可以将它们独立变换。所以让<em class="mx"> u </em>包含三个变量<em class="mx"> u_level，u_slope，u_obs </em>是有意义的。<em class="mx"> u </em>中的每个变量负责<em class="mx"> z </em>中相应的变量。</p><p id="25ac" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">概率密度<em class="mx"> p(u|y) </em>会是什么样子？我们先来关注一下单变量对<em class="mx">σ_ level</em>∊ℝ⁺<em class="mx">t29】和<em class="mx"> u_level </em> ∊ ℝ.我们想把<em class="mx"> σ_level </em>写成<em class="mx"> u_level 的函数。</em>该功能有输入域<em class="mx"> </em> ℝ和输出范围ℝ⁺.许多函数都可以做到这一点，例如:</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/b7c9e3c54decff858402393f3e271ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpM_VpvgzaoPTStu7Je2dA.png"/></div></div></figure><p id="511f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下图显示了绿色的指数函数曲线和红色的平方函数曲线。这两个函数都将一个值从ℝ映射到ℝ⁺.</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/cbb9f39274f2a2533746c88f5f091242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eg4j_HdD_bGllCal5n6ggg.png"/></div></div></figure><p id="e792" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">两者之间，我们要选择指数函数。这是因为指数函数是单调递增的。所以在<em class="mx"> σ_level </em>和<em class="mx"> u_level 之间是一一对应的。</em>稍后我们将需要这种一对一的映射属性。</p><p id="e58f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所以我们决定使用指数作为从<em class="mx"> u </em>中的变量到<em class="mx"> z </em>中的变量的转换函数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi na"><img src="../Images/6396af6b76516f05bde4fb86643eb7b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KFhyVfsA6fZR8IeN-TZWIA.png"/></div></div></figure><p id="bcbd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这就是<a class="ae lz" href="https://en.wikipedia.org/wiki/Change_of_variables" rel="noopener ugc nofollow" target="_blank">用其他变量的函数代替变量</a>的教科书案例。现在我们需要弄清楚这个变量替换是如何改变我们的后验概率密度<em class="mx"> p(z|y) = m×p(y|z)p(z) </em>。为此，我们来看看概率密度函数的定义，它是一个非负的函数，在变量的定义域上积分为 1。所以我们的后验密度实际上意味着:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/ac37edc3105ebe400952e4c55275454c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WyxZdsJS9bAbtDvi8XQasA.png"/></div></div></figure><p id="ed80" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第一行是概率密度函数的定义。第二行明确显示了变量和积分极限。</p><p id="2c18" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们看看最内在的整合:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/e178537960ab1029e75d8e54d88b8e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QBTRqb1C-eo8JTHTyAc1WQ.png"/></div></div></figure><p id="6003" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在这个积分中，我们希望用指数函数<em class="mx"> exp(u_level) </em>替换所有出现的<em class="mx">∑level</em>。在微积分中，<a class="ae lz" href="https://en.wikipedia.org/wiki/Integration_by_substitution" rel="noopener ugc nofollow" target="_blank">换元法</a>告诉我们如何去做:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/f8740408fdbb5f6cc5ac4a05d3972589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q2GbkPYKrKGCtUtuGPhLYQ.png"/></div></div></figure><ol class=""><li id="e288" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">第一行是原始集成。</li><li id="3b2b" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">在第二行，所有与当前积分无关的变量都用点“<em class="mx">”</em>代替，以简化公式。</li><li id="4ba4" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">第三行是至关重要的一步。它应用替换积分规则:(a)在要积分的原始函数中，用变换函数替换所有出现的旧变量。(b)将原始函数乘以变换函数相对于新变量的导数。(c)改变整合的限制。<em class="mx"> log </em>函数是我们用作变换函数的指数函数的反函数。我们使用这个反函数将原来的积分极限改为新的极限。这就是为什么我们希望变换函数是单调的，所以它的逆也是一个函数。</li><li id="df52" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">第四行减少了新的极限和导数。</li></ol><p id="2d25" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们研究三重积分，得出这个公式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/157f000b719540a6658ca8deeb5b541d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6KQIII5fDNXOrGU3ZrByw.png"/></div></div></figure><ol class=""><li id="a89a" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">第一行通过替换三个变量来执行三次积分。所得积分具有从-∞到+∞的极限。</li><li id="d307" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">在第二行中，神秘的术语<em class="mx"> |J(u)| </em>是由于代换积分规则的应用。||运算符代表矩阵的行列式。而矩阵<em class="mx"> J(u) </em>称为雅可比矩阵。对于本文，你只需要知道在我们的例子中<em class="mx"> |J(u)| </em>等于<em class="mx"> e </em>的<em class="mx"> u_level + u_slope + u_obs </em>次方。这是因为应用了代换积分法则。<a class="ae lz" href="https://www.cl.cam.ac.uk/teaching/2003/Probability/prob11.pdf" rel="noopener ugc nofollow" target="_blank">本文</a>详细解释了雅可比矩阵。</li></ol><p id="d66c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第二行的公式是后验概率密度函数的定义。也就是说，<em class="mx"> p(u|y) = m p(y|u) p(u </em>)对于整个实域ℝ.上的变量集<em class="mx"> u </em></p><p id="10fd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从现在开始，我们要用<em class="mx"> p(y|u)p(u) </em>而不是<em class="mx"> p(y|z)p(z) </em>。然而，为了使事情更简单，让我们将我们的<em class="mx"> z </em>重新定义为<em class="mx"> u </em>。这意味着从现在开始:</p><ol class=""><li id="4078" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">用<em class="mx"> σ_level、σ_slope、σ_obs </em>来表示<em class="mx"> u_level </em>、u <em class="mx"> _slope </em>和<em class="mx"> u_obs </em>。换句话说，<em class="mx"> z </em>就是<em class="mx"> u </em>的意思。</li><li id="30b2" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">用<em class="mx"> p(y|z)p(z) </em>来表示<em class="mx"> p(y|u)p(u) </em>。</li><li id="b196" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><em class="mx"> q(z) </em>保持不变。</li></ol><p id="45ff" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这样，上面所有的公式保持不变，你只需要记住现在东西在变换域中。</p><h2 id="bb97" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">测量两个分布之间的相似性</h2><p id="930e" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">上面列出的对<em class="mx"> q(z) </em>的第三个要求是要求<em class="mx"> q(z) </em>尽可能接近<em class="mx"> p(z|y) </em>。从图形上看，这意味着<em class="mx"> q(z) </em>的曲线应该尽可能地与<em class="mx"> p(z|y) </em>的曲线重叠。下图展示了这个有趣的想法。我写了<a class="ae lz" href="https://gist.github.com/jasonweiyi/c5b9b2aaca3aef4279691d480db7a42a" rel="noopener ugc nofollow" target="_blank">这个代码</a>来生成图。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/a3f07bc1d19e3460b38e4ad4d3946833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f2IrViZjdc68s13UA61rrQ.png"/></div></div></figure><p id="7044" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">蓝色曲线代表我们想要计算的后验概率 p(z|y) 。它的形状不规则——注意左边比右边大。当然，我们并不知道<em class="mx"> p(z|y) </em>的形状，我只是用一个不规则的形状来传达一个想法，即<em class="mx"> p(z|y) </em>是一个复杂的东西。</p><p id="eb71" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">绿色曲线代表我们提出的变分分布<em class="mx"> q(z) </em>。我用一个规则的形状来表示<em class="mx"> q(z) </em>结构简单。</p><p id="be7a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mx"> q(z) </em>的形状由六个变分参数<em class="mx"> vp </em>控制。我们需要调整这六个值，使绿色曲线尽可能与蓝色曲线重叠。</p><p id="51bb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">记住，最终我们想要知道模型参数 z 的值。为此，我们需要通过找出变分参数<em class="mx"> vp </em>的值来确定变分分布<em class="mx"> q(z) </em>的形状。我们正在寻找<em class="mx"> vp </em>的值，使得<em class="mx"> q(z) </em>尽可能与后验<em class="mx"> p(z|y) </em>重叠。一旦我们有了<em class="mx"> q(z) </em>的形状，我们就可以从中取样以获得<em class="mx"> z </em>的实际值。</p><p id="50de" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了量化两个概率密度之间的重叠程度，我们使用<a class="ae lz" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">kull back–lei bler 散度</a>。概率密度<em class="mx"> q(z) </em>和概率密度<em class="mx"> p(z|y) </em>之间的 KL 散度为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/70a101357c17c4e07fca4ba41999dffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ulnraK2rJXh7vrnfucq1-w.png"/></div></div></figure><p id="05e9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">KL-divergence 背后的直觉很简单。它是<em class="mx"> z </em>域内每一点的两个概率密度之差的加权平均值(用<em class="mx"> q(z) </em>加权)。</p><p id="fca0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们想要为<em class="mx"> vp </em>找到一个特定值，使得<em class="mx"> q(z) </em>最小化到后面的距离<em class="mx"> p(z|y) </em>。我们可以用下面的公式来表示这种最小化:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/5abce2368924e00f25a2e05f42ed0742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CaVolTK9mg0MHBB65LHozA.png"/></div></div></figure><p id="fec9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mx">中的<em class="mx">星</em>q(z；vp)* </em>表示<em class="mx"> q(z) </em>的最佳值。这意味着我们想要找到这六个变分参数的值，使得<em class="mx"> q(z) </em>和<em class="mx"> p(z|y) </em>之间的 KL 散度最小。</p><p id="369e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">看看变分推理如何将推理问题转化为优化问题！使用贝叶斯规则计算后验概率 p(z|y) 的原始问题通常被称为推理问题。另一方面，最小化<em class="mx"> q(z) </em>和<em class="mx"> p(z|y) </em>之间的 KL 散度是一个优化问题。KL 散度是优化目标。</p><p id="aa34" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">令人惊讶的是，我们可以最小化<em class="mx"> q(z) </em>和<em class="mx"> p(z|y) </em>之间的 KL，即使我们不知道如何计算<em class="mx"> p(z|y) </em>。</p><p id="83df" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">秘密在于<a class="ae lz" href="https://arxiv.org/pdf/1601.00670.pdf" rel="noopener ugc nofollow" target="_blank">变分推理理论</a>。它告诉我们最小化 KL-divergence 等价于<strong class="lf jd">最大化</strong>另一个称为证据下限的公式，表示为<em class="mx"> ELBO(q(z)) </em>。如果你不熟悉为什么，不要担心。现在，接受这两个优化问题是等价的就足够了。</p><p id="0bef" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mx"> ELBO(q(z)) </em>定义为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/f0b21a85cf10245a535e15bd881033a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dw_jhQ3T9IpS9oO3AQAMhA.png"/></div></div></figure><p id="498f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从上面第二行可以最好地看出<em class="mx"> ELBO </em>背后的直觉。它有两项:一个期望减去另一个期望。我们想最大化减法的结果。</p><p id="75b6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第一项是对数似然的期望值。它描述了我们的模型与数据的吻合程度。为什么？因为为了使这一项变大，我们的<em class="mx"> q(z) </em>应该在<em class="mx"> p(y|z) </em>也变大时，给<em class="mx"> z </em>的值以大概率。我们希望这一项很大。</p><p id="f552" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第二项是<em class="mx"> q(z) </em>和之前的<em class="mx"> p(z) </em>之间的 KL 背离(如果你还没有意识到的话)。我们知道一个<a class="ae lz" href="https://stats.stackexchange.com/questions/335197/why-kl-divergence-is-non-negative" rel="noopener ugc nofollow" target="_blank"> KL 散度是非负的</a>。这个术语描述了我们提出的<em class="mx"> q(z) </em>与我们先前的假设<em class="mx"> p(z) </em>有多么不同。我们希望差别很小，因为我们假设我们的先验是有意义的。所以我们希望第二项小一些。</p><p id="ecfb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mx"> ELBO </em>传达了我们的愿望，即我们的优化模型很好地符合数据，同时它保持接近我们的模型假设，即先验。</p><p id="cee3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第五行和第六行显示我们将<em class="mx"> ELBO(q(z)) </em>(一个期望)扩展为一个积分。</p><p id="f5d6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第 7 行显示了积分内部的函数，开始是一个带有两个参数<em class="mx"> z </em>和<em class="mx"> vp </em>的函数(插入了观察值<em class="mx"> Y </em>)。这是因为它所包含的功能要么是<em class="mx"> z </em>的功能，要么是<em class="mx"> z </em>和<em class="mx"> vp </em>的功能。</p><p id="15e5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第 8 行显示在将<em class="mx"> z </em>积分出来后，<em class="mx"> ELBO </em>仅仅是变量参数<em class="mx"> vp </em>的函数。</p><p id="e068" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，我们到底为什么需要<em class="mx"> ELBO(q(z)) </em>？为什么我们不能直接最小化<em class="mx"> KL(q(z)||p(z|y)) </em>？要回答这个问题，我们需要注意:</p><ol class=""><li id="4cf2" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">KL 公式提到了我们无法计算的后验概率 p(z|y) 。</li><li id="3145" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">但是<em class="mx"> ELBO(q(z)) </em>公式没有提到后验<em class="mx"> p(z|y) </em>。只提到了先验<em class="mx"> p(z) </em>，似然<em class="mx"> p(y|z) </em>和变分分布<em class="mx"> q(z) </em>。我们知道这三个量的解析形式。</li></ol><p id="7650" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这就是为什么我们需要<em class="mx"> ELBO </em>以及如何最小化<em class="mx"> q(z) </em>和<em class="mx"> p(z|y) </em>之间的 KL 散度，即使我们不知道<em class="mx"> p(z|y) </em>。这个事实一开始挺让人吃惊的。但这是合理的。因为可能性<em class="mx"> p(y|z) </em>和先验<em class="mx"> p(z) </em>包含了计算后验<em class="mx"> p(z|y) </em>所需的所有信息。</p><p id="e3bb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，在<em class="mx">概率密度变换</em>部分，我们把后验变换成了ℝ域上的一个函数，从那以后，我们就需要使用变换后的后验。但是<em class="mx"> ELBO </em>根本不提后路。我们该怎么办？在<em class="mx"> ELBO </em>中，我们有<em class="mx"> p(y，z) = p(y|z)p(z) </em>。并且变换后的后验概率是<em class="mx"> mp(y|z)p(z) </em>，其中<em class="mx"> m </em>是常数。所以我们仍然可以将转换后的结果插入<em class="mx"> ELBO </em>中。<em class="mx"> m </em>是常数；它不会改变优化的解决方案。</p><p id="ebfe" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回到清单 1，第 14 行构建 ELBO 作为优化目标。</p><h2 id="4bf7" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">梯度下降以最大化<em class="pp"> ELBO(q(z)) </em></h2><p id="3e64" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">由于我们知道<em class="mx"> ELBO(q(z)) </em>的解析形式，我们可以使用<a class="ae lz" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>来最大化<em class="mx"> ELBO(q(z)) </em>关于其参数<em class="mx"> vp </em>。其实我们用梯度下降来最小化<em class="mx"> -ELBO(q(z)) </em>。</p><p id="ce3b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们需要计算<em class="mx"> ELBO(q(z)) </em>相对于变分参数<em class="mx"> vp </em>的梯度:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pq"><img src="../Images/aa612edc8119a1b496cb5774516fb94e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HF2WcCvMECVdJ-wJTgHy_w.png"/></div></div></figure><p id="3926" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中𝛻符号代表向量微分算子。它计算多参数函数相对于其参数的梯度。在我们的例子中，𝛻_ <em class="mx"> vp </em>计算<em class="mx"> ELBO(q(z)) </em>相对于<em class="mx"> vp </em>中参数向量的梯度。</p><p id="f3ac" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">微妙的一点。上式中，积分是针对模型参数<em class="mx"> z= </em> [ <em class="mx"> σ_level，σ_slope，σ_obs </em>。但是积分之外的梯度计算是关于变分参数<em class="mx"> vp </em> =[ <em class="mx"> μ_l，μ_s，μ_o，σ_l，σ_s </em>，<em class="mx"> σ_o </em>。所以整合和分化并不相互抵消。</p><p id="84ab" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于积分，解析计算<em class="mx"> ELBO(q(z)) </em>的梯度是困难的。因为积分计算的是期望值，所以我们可以用样本平均值来逼近它。在每个优化步骤中，我们可以从当前的变分分布<em class="mx"> q(z) </em> 的<strong class="lf jd">中采样一些<em class="mx"> z </em>的值。这意味着从电流<em class="mx"> q(z) </em>中采样一些值<em class="mx"> σ_level、σ_slope </em>和<em class="mx"> σ_obs </em>。并使用样本平均值来近似积分。在公式中:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/6887b1e3169329b718c01c4387297cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YFd85bexdFn6DxXaIMeM1Q.png"/></div></div></figure><p id="525b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后一行使用来自当前<em class="mx"> q(z) </em>分布的<em class="mx"> n </em>个样本<em class="mx"> Z_1 </em>到<em class="mx"> Z_n </em>来计算样本平均值。我强调，在每个优化步骤，我们需要从当前的<strong class="lf jd">q(z)中抽取新的样本。</strong>这是因为每个优化步骤通过改变变化参数的值来改变<em class="mx"> q(z) </em>分布。</p><p id="1db4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然而，当我们要应用梯度下降时，上面的公式有一个问题。为了看到这一点，让我们继续操作上面的公式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pr"><img src="../Images/1fe7a5e74cdce71b0f89f3f362452d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rtiy0EzrLAvL1YpycpRqsw.png"/></div></div></figure><p id="4a0c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在第 6 行，<em class="mx"> ELBO </em>公式的 API 是显式的。看第一个学期。插上<em class="mx"> Y </em>和<em class="mx"> Z_i </em>后，<em class="mx"> log(p(Y，Z_i)) </em>成为常数。这就是为什么我们有一个“λ”引导其 API 的符号。<em class="mx"> 𝛻_vp </em>算子计算该常数相对于<em class="mx"> vp </em>的梯度。<em class="mx"> </em>渐变为 0。这似乎表明，可能性在优化问题中不起任何作用(因为它消失了)。这说不通。</p><p id="14db" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个问题的原因是，我们从<em class="mx"> q(z) </em>中得到哪些样本<em class="mx"> Z_i </em>取决于变分参数。我们正在使用梯度下降优化这些变化的参数。但是从<em class="mx"> q(z) </em>抽取样本发生在梯度下降算法之外。这使得梯度下降算法失去了关于<em class="mx"> q(z) </em>的知识。症状是计算梯度时似然项消失了。</p><p id="b903" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">重新参数化技巧解决了这个问题。</p><h2 id="2123" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">重新参数化技巧</h2><p id="7ca2" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">在我们的例子中，由于难以计算积分，采样是不可避免的。但是采样发生在梯度下降框架之外。为了解决这个问题，我们应该从更简单的分布中取样。并且这种分布必须不依赖于变化的参数。</p><p id="5c64" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">重新参数化技巧的思想是，对于<em class="mx"> z </em>中的每个模型参数，例如<em class="mx"> σ_level </em>:</p><ol class=""><li id="094a" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">引入一个新的随机变量，<em class="mx"> θ_level~ </em> N(0，1)。所以<em class="mx"> θ_level </em>来自固定的高斯分布。它不依赖于任何变化的参数。我们称<em class="mx"> θ_level 为</em> <strong class="lf jd">重参数化变量</strong>。</li><li id="2485" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">将我们的原始模型参数<em class="mx"> σ_level </em>重写为这个新变量<em class="mx"> θ_level </em>的函数。这个函数也会提到相应的变分参数<em class="mx"> μ_l，σ_l </em>。我们把这个函数叫做<strong class="lf jd">重新参数化函数</strong>。</li><li id="9ad9" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">在作为积分的<em class="mx"> ELBO </em>公式中，用重新参数化函数<em class="mx">替换该模型参数<em class="mx"> σ_level </em>的每次出现。</em></li></ol><p id="879c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在公式中，我们继续以模型参数<em class="mx"> σ_level </em>为例:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ps"><img src="../Images/3967f0cc081b78f9f77b2bb23f87f0a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2IaBRb21Q4wfGrdHceosTw.png"/></div></div></figure><p id="89ea" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以将<em class="mx"> σ_level </em>重写为<em class="mx"> θ_level </em>的函数，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/e7ece7874221f8758029ceab6329fa70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cvev4pFvhuumMYnteV_myw.png"/></div></div></figure><p id="4646" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">注意在该函数中，<em class="mx"> μ_l </em>和<em class="mx"> σ_l </em>为变参数。<em class="mx"> </em>如果你不知道我们是怎么想出这样一个函数的，也没关系。人们一直在研究如何为不同的发行版设计这样的函数，见<a class="ae lz" href="http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/" rel="noopener ugc nofollow" target="_blank">这里</a>。很多都不是小事！</p><p id="e599" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对我们来说重要的是:<em class="mx"> σ_level </em>仍然是均值<em class="mx"> μ_l </em>方差<em class="mx"> σ_l </em>的高斯分布。这是因为等式右边的第一项</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/58531dd6cfa5eb78651c789739f149a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tWVDLIlAHVBkGCCfoR_Sqg.png"/></div></div></figure><p id="9a1b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">是高斯变量<em class="mx"> θ_level~N(0，1) </em>的线性变换，没有添加噪声。<em class="mx"> </em>我们已经知道如何写下它的分布:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/68cdc6b0fd3680f373e7727d2b44ebd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2t-CfqX2aL1wLWQeDCtow.png"/></div></div></figure><p id="3838" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在重新参数化函数将<em class="mx"> μ_l </em>加到这个高斯分布上，得到高斯分布<em class="mx"> N </em> ( <em class="mx"> μ_l，σ_l)。</em>换句话说，这种重新参数化保持<em class="mx"> σ_level </em>的分布不变<em class="mx">。</em></p><p id="fabf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们定义<em class="mx"> θ= </em> [ <em class="mx"> θ_level </em>，<em class="mx"> θ_slope </em>，<em class="mx"> θ_obs </em>为重新参数化变量的向量。设重新参数化函数为<em class="mx"> r </em>。在我们的<em class="mx"> θ_level </em>的例子中:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pv"><img src="../Images/22828167e4fe10586374e0322aa65d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAJSI2a5aCDTNvcB5nOFMQ.png"/></div></div></figure><p id="399b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个技巧把<em class="mx"> q(z) </em>变成了<em class="mx"> θ </em>和<em class="mx"> vp </em>的函数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/437b92b2c38b2cf48aadb6d889fd93de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mUuDkwljhAL59bkZpDb7IA.png"/></div></div></figure><p id="4a42" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mx"> ELBO(q(z)) </em>成为变分参数<em class="mx"> vp </em>和新引入的变量向量<em class="mx"> θ </em>的函数。我们引入了<em class="mx"> θ </em>，因为我们可以独立于变分参数对其进行采样。当<em class="mx"> θ </em>替换为样本<em class="mx">θ~ N(</em><strong class="lf jd"><em class="mx">0</em></strong><em class="mx">，</em> <strong class="lf jd"> <em class="mx"> 1 </em> </strong> <em class="mx">)时，</em>ELBO 成为<em class="mx"> vp </em>的唯一函数。在公式中:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi px"><img src="../Images/a6692e30c3b98741f37a033f891b8274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yJHZlmE2Xb9LtJ21b71XtA.png"/></div></div></figure><p id="6e52" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">哇，这太可怕了。让我解释一下:</p><ol class=""><li id="559c" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">第一行是<em class="mx"> ELBO </em>符号。</li><li id="69f5" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">第二行是<em class="mx"> ELBO </em>的原定义。</li><li id="116c" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">第三行应用替换积分规则，因为我们想在积分中用函数<em class="mx"> r(θ) </em>替换出现的<em class="mx"> z </em>。之前，我们已经在适应随机变量域的上下文中看到过这个规则。</li><li id="b654" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">第四行减少了导数，重新组织了项，使概率密度为<em class="mx">N(θ；</em> <strong class="lf jd"> <em class="mx"> 0 </em> </strong> <em class="mx">，</em><strong class="lf jd"><em class="mx">1</em></strong><em class="mx">)</em>弹出。<em class="mx">N(θ；</em> <strong class="lf jd"> <em class="mx"> 0 </em> </strong> <em class="mx">，</em><strong class="lf jd"><em class="mx">1</em></strong><em class="mx">)</em>代表多元高斯的概率密度，为<em class="mx"/>【3 维】；它有均值<strong class="lf jd"> <em class="mx"> 0 </em> </strong>和恒等方差矩阵<strong class="lf jd"> <em class="mx"> 1 </em> </strong>。你可能看不出这是如何得出的，我稍后会解释。重要的是要注意，现在我们有了关于<em class="mx"> θ </em>的期望的定义。所以我们可以用样本平均值来近似这个期望值。</li><li id="69de" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">用样本平均值逼近第 4 行的期望值。样本<em class="mx">θ_ 1:N</em>来自于<em class="mx"> N( </em> <strong class="lf jd"> <em class="mx"> 0 </em> </strong> <em class="mx">，</em><strong class="lf jd"><em class="mx">1</em></strong><em class="mx">)</em>分布。重要的是，这种分布不依赖于变化的参数<em class="mx"> vp </em>。</li><li id="9a4d" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">第六行显示插入观测值<em class="mx"> Y </em>和样本<em class="mx">θ_ I:n</em>后，<em class="mx"> ELBO </em>仅成为变分参数<em class="mx"> vp </em>的函数。所以梯度下降可以正常工作，什么都不会消失。</li></ol><p id="6f86" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，我将演示我们是如何从上面的第三行到达第四行的。为简单起见，让我将<em class="mx"> θ </em>视为一维变量，并使用符号<em class="mx"> θ </em>、<em class="mx"> μ </em>和<em class="mx"> σ </em>，而不是<em class="mx"> θ_level </em>、<em class="mx"> μ_l </em>和<em class="mx"> σ_l. </em>和<em class="mx"> q </em>成为一元高斯分布:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi py"><img src="../Images/06103b25f2386a6bbe717f99f29caece.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PeM1ZdHomJBannuoJTPd2w.png"/></div></div></figure><p id="5fa7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">解释:</p><ol class=""><li id="458d" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">第 1 行，我们想要操作的行。</li><li id="23fd" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">第 2 行，插入<em class="mx"> q </em>的定义，这是我们演示的单变量高斯分布。</li><li id="ccb2" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">第 3 行，扩展<em class="mx"> r(θ) </em>的定义。</li><li id="2a5b" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">第 4 行，通过取消项和计算导数项来简化。</li><li id="44e7" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">第 5 行，进一步简化公式。认识到积分中的第一部分是<em class="mx"> N(0，1) </em>的概率密度。</li><li id="e199" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">用概率密度函数的名字代替它。</li></ol><p id="402d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以把这种单变量情况推广到多变量情况。</p><h1 id="43aa" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">最后一件事，我保证</h1><p id="60b7" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">谢谢你看了这么久。我知道这是一段漫长的旅程。我真想说“就这样”。但是，还有一件事。</p><p id="e0e6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">梯度下降算法适用于具有全实数域ℝ.的变量它不允许您指定变量的界限。然而，如果我们再次观察变分分布<em class="mx"> q(z) </em>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pz"><img src="../Images/4795b18e250b3040f4199db736f6cd2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lPWp5EI3rbG5HIl8J6q0kQ.png"/></div></div></figure><p id="4b9b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于<em class="mx"> σ_l、σ_s </em>和<em class="mx"> σ_o </em>代表标准差，所以它们必须是非负的。到目前为止，我们应该养成用函数重新表达变量的习惯。这正是我们在这里应该做的。我们引入三个新变量<em class="mx"> ϕ_l </em>、<em class="mx"> ϕ_s </em>和<em class="mx"> ϕ_o </em>，并将<em class="mx"> σ_l、σ_s </em>和<em class="mx"> σ_o </em>改写为它们的函数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qa"><img src="../Images/07a08ae6748f7cd73421190f4265842e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P_k_vWZV-B3Mu59JH6aM_Q.png"/></div></div></figure><p id="bb66" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这样，<em class="mx"> ϕ_l、</em>ϕ_s 和<em class="mx"> ϕ_o </em>可以从全实域取值，而<em class="mx"> σ_l、σ_s </em>和<em class="mx"> σ_o </em>仅保持非负。<em class="mx"> q(z) </em>变成了:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qb"><img src="../Images/867d2219cdab93dc2ba2ae00e0fadc51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xAX84PE-do1wxuffY0cH-A.png"/></div></div></figure><p id="f798" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有了这最后一步(当然，还有上面的一切)，<em class="mx"> ELBO </em>变成了<em class="mx"> μ_l，μ_s，μ_o，ϕ_l，ϕ_s，</em>和<em class="mx"> ϕ_o </em>的函数。所有六个变量都可以取全实数域ℝ.的值梯度下降终于可以通过了。</p><p id="e53c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们在两个地方采用了从ℝ⁺到ℝ:的可变域</p><ol class=""><li id="838d" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">之前，我们修改了模型参数的域，因此<em class="mx"> q(z) </em>和<em class="mx"> p(z|y) </em>具有相同的域ℝ.</li><li id="60a0" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">这里，我们修改了与标准差相关的变分参数的域，因此所有变分参数都具有域ℝ.平均相关变分参数已经有域ℝ.</li></ol><p id="b944" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，选择<em class="mx"> q(z) </em>作为高斯的产物，我们得到了什么？为什么它比原始先验好，原始先验是 LogNormals 的产物？</p><p id="c2ef" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当我们查看优化过程时，<em class="mx"> q(z </em>)唯一能帮助我们的地方是，我们可以轻松地从<em class="mx"> q(z) </em>进行采样。更准确地说，我们需要能够重新参数化<em class="mx"> q(z) </em>以便它可以被表示为来自固定分布的函数。我们可以很容易地从固定的分布中抽取样本。我们已经表明，通过我们提出的<em class="mx"> q(z) </em>，我们可以做到这一点。但是对数正态怎么样呢？原来我们也可以重新参数化对数正态分布并从中取样，详情<a class="ae lz" href="https://stats.stackexchange.com/questions/110961/sampling-from-a-lognormal-distribution" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="c8d9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因此，要回答这个问题，在我们的特殊情况下，提出的<em class="mx"> q(z) </em>并不是一个大的救命稻草。我们也可以用对数正态先验进行优化。但是想象一下，当先验很复杂并且很难从中采样时，一个结构简单的<em class="mx"> q(z) </em>可能是一个真正的帮助。</p><p id="519f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回到清单 1，第 17~23 行使用梯度下降来寻找<em class="mx"> vp </em>的最优值。它首先创建一个以<em class="mx"> ELBO </em>为目标函数的 Adam 优化器，然后执行 200 个梯度下降步骤。</p><p id="878b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这标志着参数学习的结束。</p><p id="a19e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">顺便说一下，变分推理在现代机器学习中非常重要，是一个你无法逃脱的话题。参见其在<a class="ae lz" rel="noopener" target="_blank" href="/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4">变分高斯过程</a>和<a class="ae lz" rel="noopener" target="_blank" href="/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756">人脸生成</a>中的应用。</p><h1 id="4ac5" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">预测未来值</h1><p id="fd9c" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">梯度下降将给出六个变分参数<em class="mx"> μ_l*、σ_l*、μ_s*、ϕ_l* </em>、<em class="mx"> ϕ_s* </em>和<em class="mx"> ϕ_o* </em>的最优值。这些值完全指定了优化的变分分布<em class="mx"> q(z)* </em>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/0472e4f7ff8560124f82324e877d19db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sfpr7patAE5x8RZgN3u53Q.png"/></div></div></figure><p id="83ba" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以用<em class="mx"> q(z)* </em>来近似后验<em class="mx"> p(z|y) </em>。</p><p id="add0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">给定时间序列观测值<em class="mx"> Y_1 </em>到<em class="mx"> Y_T </em>，我们的兴趣是预测未来值，<em class="mx"> Y_T+1，Y_T+2 </em>等等。为此，我们可以从时间步长 1 到<em class="mx"> T </em>运行卡尔曼滤波算法，以计算潜在变量<em class="mx"> x_t|y_1:t，z </em>的分布。从上面复制卡尔曼滤波算法:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/25e271152fa8b21cad5bf6216166a86c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jz9g2Cn3u3WscTRlajxTjg.png"/></div></div></figure><p id="439d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们现在可以运行卡尔曼滤波器<strong class="lf jd"/>，因为现在我们可以知道其参数 z*=[ <em class="mx"> σ_level*、σ_slope*、σ_obs* </em> ]的值，例如，通过从优化的<em class="mx"> q(z)* </em>分布中采样<em class="mx"> σ_level*、σ_slope* </em>和<em class="mx"> σ_obs* </em>。</p><p id="4e52" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，为什么我们需要从<em class="mx"> q(z)* </em>取样？这是因为<em class="mx"> q(z)* </em>是一个分布，为了运行卡尔曼滤波器，我们需要三个标准偏差<em class="mx"> σ_level、σ_slope </em>和<em class="mx"> σ_obs </em>的具体值。对于每个标准差，我们需要多少个样本？由你来决定。例如，您可以对每个标准差取样 30 个并取平均值，然后将这三个平均值代入局部线性趋势。</p><p id="a9dd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">卡尔曼滤波算法结束后，结果就是系统状态变量<em class="mx"> x_t </em>的分布，换句话说就是<em class="mx"> μ_t </em>和<em class="mx">σ_ t</em>的值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qc"><img src="../Images/09aeebcbf2a9c8107ce39f3676322d1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nw6NoxaM5CwEVIMNVjwk-Q.png"/></div></div></figure><p id="ca77" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从现在开始，我们的预测开始了。我们可以将<em class="mx"> x_T|y_1:T，z </em>代入线性动力系统方程，得到状态变量<em class="mx"> x_T+1|y_1:T，z </em>和观测变量<em class="mx"> y_T+1|y_1:T，z </em>的<strong class="lf jd">预测分布。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qd"><img src="../Images/74435ea172f822edc61c72e4b9b22c07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lbk3wlPmfTe0pOHkAxSULQ.png"/></div></div></figure><p id="95de" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">事实上，你可以这样做无数次:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qe"><img src="../Images/d2d8829d943b8179e5016474758e47f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oIEpaOg-h_h2uyTJlANhXg.png"/></div></div></figure><p id="3f48" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在上图中，我们从卡尔曼滤波的结果<em class="mx"> p(x_T|y_1:T，z)开始。</em>每个右箭头“→”代表第一个系统方程的应用，以获得下一个状态变量的分布。每个向下箭头“↓”代表第二个系统方程的一个应用，以获得下一个观察值的分布。</p><p id="ec5f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这样就可以得到任意步<em class="mx"> i </em>到未来的观测变量<em class="mx"> y_T+i|y_1:T，z </em>的预测分布。请记住，您仍然需要将实际观察值<em class="mx"> Y_1:T </em>和优化的模型参数<em class="mx"> z* </em>代入公式，以获得完全指定的分布。</p><p id="f134" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">注意上图中，所有分布的条件部分里面，<em class="mx"> y_1:T </em>部分没有变化，我们没有<em class="mx"> y_1:T+1 </em>，<em class="mx"> y_1:T+2 </em>。这是因为在时间步<em class="mx"> T </em>之后，我们不再有实际的观察。我们应该认识到，预测的过程是使用卡尔曼滤波算法中 5 步中的前 3 步来导出我们对系统状态和未来观测可能性分布的信念。我们不能再使用卡尔曼滤波器的最后两步来修正我们的信念，因为我们没有新的观测值。</p><p id="a00f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">数学允许你预测无限未来的新值。但这并不意味着你应该。你对未来的预测越遥远，就越不精确。这是常识；而我们来看看这个常识是如何体现在数学上的。</p><p id="ee31" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们需要看看系统状态变量和观察变量的分布结构。让我们通过两次应用第一个系统方程，首先来看下两个时间步<em class="mx"> T+1 </em>和<em class="mx"> T+2 </em>中的系统状态变量分布:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/8e518366eddc1e6f30aaae0097e4a134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VuxYBm1j6F6miaxptNT5-Q.png"/></div></div></figure><p id="f413" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们通过从上述推导中得出一个归纳结论来研究均值和方差分量。</p><p id="f9a6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mx"> x_T+i </em>有均值<em class="mx"> μ_T+i </em>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qf"><img src="../Images/ed74d59eccc0368317606bb6acfde193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9xqiy1D6gQgy_5iKeqk5vA.png"/></div></div></figure><p id="ad31" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在上面的内容中，我在<em class="mx"> level_T </em>和<em class="mx"> slope_T </em>上方使用了一个横杠“-”来表示这些是<em class="mx"> μ_T 中的平均值成分。</em>第 5 行显示趋势水平预测(矩阵中的第一个条目)是一条始于当前趋势水平的直线。并且斜率水平预测(第二条目)保持不变。</p><p id="71fe" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们来研究变量<em class="mx"> x_T+1 </em>和<em class="mx"> x_T+2 </em>的方差<em class="mx">σ_ T+I</em>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qg"><img src="../Images/18b7058c52d07d5ba0faa85ac839b17b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j555MqRxoaMbNzeGxSkq5w.png"/></div></div></figure><p id="15b7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">公式有点复杂。但是我们可以看到，每次我们向前移动时，噪声方差<em class="mx">σ_ w</em>会被添加到系统状态变量的方差中。这导致系统状态变量<em class="mx"> x_T+i </em>的方差随着时间变得越来越大。</p><p id="8006" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们继续观察观察变量<em class="mx"> y_T+i </em>的预测分布。它是系统状态变量<em class="mx"> x_T+i </em>的线性高斯变换。所以<em class="mx"> y_T+i </em>遵循相同的特性:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qh"><img src="../Images/be9135a07724f1929ce2d642b7bf2d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5aDKJMbn1dD3Q7fyF6FmPw.png"/></div></div></figure><p id="3468" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们可以看到<em class="mx"> y_T+i </em>的预测分布的平均值与<em class="mx"> x_T+i </em>的平均值相同，这是一条直线，因为矩阵<em class="mx"> H </em>被设置为<em class="mx">【1，0】</em>。这个直线公式也提醒了我们为什么斜率变量叫斜率。<em class="mx"> y_T+i </em>的方差会将噪声<em class="mx">σ_ v</em>的方差添加到我们移动到未来的每个时间步长中。这导致<em class="mx"> y_T+i </em>的方差也越来越大。</p><p id="6bec" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回到代码清单 1，第 26 行从优化的<em class="mx"> q(z) </em>分布中抽取模型参数值。第 28~33 行通过在内部运行卡尔曼滤波算法来建立观察变量<em class="mx"> y </em>的预测分布。第 36~38 行通过报告每个未来时间步的预测分布的平均值和标准偏差来执行预测。</p><p id="2d66" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有了这个数学分析，让我们绕回我们的山脉天际线“时间序列”。天际线有 220 个观察点。我使用前 200 个观察值进行参数学习。然后使用学习过的模型来预测最后 20 个点。它看起来是这样的:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/1ba1adb48de14c7bcec97fb2edc954a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RzP3Iwha0GK67oCSL5G_kA.png"/></div></div></figure><p id="eac0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在上图中，蓝色曲线是地面真理山天际线的观测结果。垂直虚线将左边的训练和右边的预测分开。绿色虚线是预测的平均值。右侧的浅绿色锥形显示了 95%置信水平下的预测方差。</p><p id="9e47" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">预测平均值和方差证实了我们的数学分析。平均预测是一条直线。随着我们越往未来走，方差会变得越来越大。</p><p id="11d9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们还可以看到，平均预测是非常错误的(或者，如果考虑到置信区间，我们可以解释为预测是正确的)！平均预测说，山脉的天际线应该上升，但它实际上是下降的。这是因为我故意选择了一个分界点，一个山顶，来挑战算法。为了进行预测，算法只能依赖于在山向上的第 200 次观察之前的先前值。当然，它会预测山会继续上升。因此得名<strong class="lf jd">局部线性</strong>趋势。从数学上讲，算法知道斜率应该是正的。</p><p id="709e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">那我们该怎么办？我们获得新的观察值，每次当新的观察值可用时，使用所有的观察值从头开始重新训练模型，然后进行预测。如果我们用 8 个以上的观察值训练我们的模型，并在第 208 个观察值开始预测，这是一幅图:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/8091c9ec9070628978268fc6ea0a02be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*76E5vS_KeR8RfdM_LR7t8A.png"/></div></div></figure><p id="4a04" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在预测更加精确了。额外的 8 次观察显示山向下，因此算法得知斜率变量是负的。预测更精确，因为山一直在往下走，换句话说，趋势没有改变方向。</p><p id="9610" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae lz" href="https://gist.github.com/jasonweiyi/43d23da4b8c65a9a6a6aaddd883b5a52" rel="noopener ugc nofollow" target="_blank">这里</a>是天际线预测的代码。改编自<a class="ae lz" href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Structural_Time_Series_Modeling_Case_Studies_Atmospheric_CO2_and_Electricity_Demand.ipynb" rel="noopener ugc nofollow" target="_blank"> Tensorflow 时间序列实例</a>。</p><p id="854e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会好奇，或者你还记得卡尔曼滤波算法本身就可以根据观测值进行时间序列预测。在变分推理发明之前，我们不就是用这种算法登陆月球的吗？为什么我们需要变分推理？嗯，卡尔曼滤波算法只对没有任何参数的完全指定的模型有效。由于我们的局部线性模型包含参数，我们首先需要学习它们的值。使用变分推理作为推理算法的贝叶斯方法是这个 Tensorflow 时间序列库的选择。</p><h1 id="4aa2" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">结论</h1><p id="8b34" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">本文介绍了 Tensorflow 时间序列库中的局部线性趋势模型。其重点在于如何学习该模型的参数:</p><ol class=""><li id="a40e" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">我们将局部线性趋势模型定义为一个带参数的线性动力系统。</li><li id="1a38" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">然后，我们使用高斯线性变换和卡尔曼滤波算法来导出可能性<em class="mx"> p(y|z) </em>。</li><li id="9f18" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">我们使用变分推断，使用分布<em class="mx"> q(z) </em>来近似后验<em class="mx"> p(z|y) </em>。<em class="mx"> q(z) </em>是参数变化的变分分布。</li><li id="4658" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">我们使用概率密度的变换来确保<em class="mx"> p(z|y) </em>和<em class="mx"> q(z) </em>具有相同的可变域。</li><li id="f612" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">我们使用<em class="mx"> ELBO(q(z)) </em>作为优化目标，以最小化<em class="mx"> q(z) </em>和<em class="mx"> p(z|y) </em>之间的 KL 散度。梯度下降是优化算法。</li><li id="a79e" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">我们使用样本平均值来逼近<em class="mx"> ELBO(q(z)) </em>定义内的积分。样本来自于<em class="mx"> q(z) </em>分布。</li><li id="c31f" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">我们使用重新参数化技巧从更简单分布的样本中构造出<em class="mx"> q(z) </em>的样本。这个更简单的分布不依赖于我们正在优化的参数。这样梯度下降可以通过。</li></ol><p id="eab7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是很多步骤。我试图解释清楚每一步背后的直觉。我只展示了必要的数学。</p><p id="6198" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">好消息是，你只需要基本的大学数学就能理解以上所有内容。下面是提到的数学知识点列表:</p><ol class=""><li id="c2d1" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Expected_value" rel="noopener ugc nofollow" target="_blank">预期的定义</a></li><li id="3312" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">单变量高斯分布</a></li><li id="f693" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf" rel="noopener ugc nofollow" target="_blank">多元高斯分布，其联合与条件</a></li><li id="6edd" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://www.statlect.com/probability-distributions/normal-distribution-linear-combinations" rel="noopener ugc nofollow" target="_blank">高斯的线性变换</a></li><li id="f65d" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Log-normal_distribution" rel="noopener ugc nofollow" target="_blank">对数常态分布</a></li><li id="d2a1" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯法则</a></li><li id="e9d6" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)" rel="noopener ugc nofollow" target="_blank">概率中的产品/链规则</a></li><li id="a59b" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Marginal_distribution" rel="noopener ugc nofollow" target="_blank">边际分配</a></li><li id="2617" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Probability_density_function" rel="noopener ugc nofollow" target="_blank">概率密度函数的定义</a></li><li id="73cf" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Integration_by_substitution" rel="noopener ugc nofollow" target="_blank">替代积分</a></li><li id="7126" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Multiple_integral" rel="noopener ugc nofollow" target="_blank">多重整合</a></li><li id="8e99" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Change_of_variables" rel="noopener ugc nofollow" target="_blank">用函数</a>替换变量</li><li id="e1dc" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated"><a class="ae lz" href="https://arxiv.org/pdf/1601.00670.pdf" rel="noopener ugc nofollow" target="_blank">用于变分推理的 ELBO</a></li></ol><h1 id="937c" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">支持我</h1><p id="2497" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">如果你喜欢我的故事，如果你考虑支持我，通过这个链接成为灵媒会员，我将不胜感激:<a class="ae lz" href="https://jasonweiyi.medium.com/membership" rel="noopener">https://jasonweiyi.medium.com/membership</a>。</p><p id="8f53" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我会继续写这些故事。</p><h1 id="5b14" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">承认属实</h1><p id="02d5" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">我要感谢文森特·亚当和我一起阅读了 Tensorflow 时间序列的代码。感谢<a class="ae lz" href="https://www.linkedin.com/in/lucas-bordeaux-7704b42/" rel="noopener ugc nofollow" target="_blank">卢卡斯·波尔多</a>、<a class="ae lz" href="https://www.linkedin.com/in/michaeldavidpedersen/" rel="noopener ugc nofollow" target="_blank">迈克尔·彼得森</a>、<a class="ae lz" href="https://www.linkedin.com/in/elvijs-sarkans-69087a5b/" rel="noopener ugc nofollow" target="_blank">埃尔维斯·萨尔坎斯</a>、<a class="ae lz" href="https://sites.google.com/site/nicolasdurrandehomepage/" rel="noopener ugc nofollow" target="_blank">尼古拉斯·杜兰德</a>、<a class="ae lz" href="https://www.linkedin.com/in/seleftheriadis/" rel="noopener ugc nofollow" target="_blank">斯特凡诺斯·埃勒弗瑟里亚迪斯</a>在撰写本文期间提出的问题和建议。感谢<a class="ae lz" href="https://10creative.co.uk/" rel="noopener ugc nofollow" target="_blank">10 位创意人</a>的<a class="ae lz" href="https://10creative.co.uk/about10" rel="noopener ugc nofollow" target="_blank">约书亚·琼斯</a>创作出特色形象。</p><p id="882f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我决定包括一些附录。没有这个附录也能看懂整篇文章。但是这里的材料给了你一个不同的，有趣的角度。</p><h1 id="2cef" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">附录 1:通过边缘化推导可能性</h1><p id="388d" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">本节说明如何使用概率论中的规则推导线性动态系统的似然性，而不使用卡尔曼滤波算法。</p><p id="1c8c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在我们开始之前，有一个关于从公式中移除随机变量的旁注。</p><h2 id="d62d" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">如何从公式中删除随机变量</h2><p id="87bc" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">从概率论中推导似然性<em class="mx"> p(Y) </em>需要对概率密度函数进行相当多的操作。作为旁注，让我们首先考虑如何从概率密度函数中移除随机变量。通常，有四种方式:</p><ol class=""><li id="8a86" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">融入社会。例如，设<em class="mx"> p(x，y) </em>为随机变量<em class="mx"> x </em>和<em class="mx"> y </em>的联合概率密度。那么<em class="mx"> ∫ p(x，y)dy = p(x) </em>。现在<em class="mx"> p(x) </em>是随机变量 x 的概率密度函数。</li><li id="32b6" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">插入观测值:如果随机变量的观测值可用，可以将观测值插入该随机变量的位置。例如，设<em class="mx"> Y </em>为随机变量<em class="mx"> y </em>的观测值。那么<em class="mx"> p(x，Y = Y)</em>或者<em class="mx"> p(x，Y) </em>如果我们要省墨<em class="mx">，</em>就变成了只针对随机变量<em class="mx"> x </em>的概率密度函数。如果你从一个概率<em class="mx"> p(y) </em>开始，你有随机变量<em class="mx"> y </em>的观测值<em class="mx"> Y </em>，你代入<em class="mx"> Y </em>后，你得到<em class="mx"> p(y=Y) </em>，或者<em class="mx"> p(Y) </em>。注意<em class="mx"> p(Y) </em>不再是概率密度函数，它变成了一个概率数。或者你可以把<em class="mx"> p(Y) </em>看成一个没有自变量的函数。</li><li id="1525" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">丢弃不相关的条件。例如，在一个条件中，设<em class="mx"> p(x|y) </em>为随机变量<em class="mx"> x </em>和<em class="mx">y</em>的联合概率密度，如果<em class="mx"> x </em>不依赖于<em class="mx"> y </em>，则可以去掉<em class="mx"> y </em>得到<em class="mx"> p(x) </em>。现在<em class="mx">p(x)= p(x | y)</em>和<em class="mx"> p(x) </em>只是随机变量 x 的概率密度函数。</li><li id="2050" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">高斯线性变换。如果随机变量 x 是高斯分布:<em class="mx">x</em>T2】~𝒩(μ，K) ，<em class="mx"> y </em>是添加高斯白噪声的<em class="mx"> x </em>的线性变换:<em class="mx"> y=Ax + </em> ε，其中ε <em class="mx"> ~𝒩(0，σ)。</em>所以<em class="mx"> y </em>的分布是<em class="mx"> 𝒩(Ax，σ)。</em>这个分布提到了<em class="mx"> x </em>。应用高斯线性变换法则得到<em class="mx"> y </em>的分布公式，不提及<em class="mx"> x </em> : <em class="mx"> y~𝒩(Aμ，akaᵀ+σ).</em></li></ol><p id="e295" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将使用这三种机制中的一些来推导可能性<em class="mx"> p(Y) </em>。</p><h2 id="f4c2" class="oa mb it bd mc ob oc dn mg od oe dp mk lm of og mm lq oh oi mo lu oj ok mq iz bi translated">似然公式</h2><p id="3210" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">让我从上面复制线性动力系统的定义:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/ff592003253d26ab126046ed553aa13a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*B5SKA9NmTsiNkTbb.png"/></div></div></figure><p id="9307" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这里我们用<em class="mx"> p(Y) </em>代替<em class="mx"> p(Y|z) </em>来表示可能性。我决定去掉模型参数<em class="mx"> z </em>以节省空间。因为模型参数<em class="mx"> z </em>与当前截面无关。所以记住，这一节<em class="mx"> p(Y) </em>是可能性，不是边际可能性。</p><p id="429c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们用<em class="mx"> x </em>来表示每个状态的随机变量集合。所以 x={ <em class="mx"> x₁ </em>，<em class="mx"> x₂ </em>，…，到<em class="mx"> x_T} </em>。并使用<em class="mx"> y </em>来表示每次观察背后的随机变量集合。所以 y={y <em class="mx"> ₁ </em>，y <em class="mx"> ₂ </em>，…，到 y <em class="mx"> _T}。</em>而相应的观测被标注为<em class="mx"> Y={Y₁ </em>、<em class="mx"> Y₂ </em>、…，到<em class="mx"> Y_T}。</em></p><p id="2625" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">可能性<em class="mx"> p(Y) </em>在概率论中定义为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qi"><img src="../Images/8ea6d739e8360723350ea10c80596849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqeWLF0uxOwBvIVxFJfKeA.png"/></div></div></figure><p id="696f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(1)行是通过从联合分布<em class="mx"> p(Y，x) </em>中边缘化状态随机变量<em class="mx"> x </em>来导出<em class="mx"> p(y=Y) </em>的公式。我写<em class="mx"> p(Y，x) </em>的时候，意思是<em class="mx"> p(y=Y，x) </em>，读作“随机变量 Y，x 的联合概率密度并把观测值<em class="mx"> Y </em>代入<em class="mx"> y </em>”。这是一个你可以应用于任何概率系统的模板。</p><p id="8384" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该线采用上述<em class="mx">整合出</em>和<em class="mx">堵塞观察</em>机构。并且我们认识到，可能性<em class="mx"> p(Y) </em>是一个概率数，不再是一个概率密度函数。</p><p id="baea" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(2)行使用概率论中的链式法则将联合分布<em class="mx"> p(Y，x) </em>一分为二。这也是一个普遍适用的模板。</p><p id="b97f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(3)行明确列出了随机变量<em class="mx"> x₁ </em>，到<em class="mx"> x_T </em>和观测值<em class="mx"> Y₁ </em>到<em class="mx"> Y_T </em>。每个概率系统也是如此。请注意，该公式还明确写出了 x <em class="mx"> ₁ </em>到 x<em class="mx">t</em>的<em class="mx"> T </em>积分。每个积分对单个随机变量求和。</p><p id="4f2d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从第(4)行开始，我们开始推导特定于我们的线性动力系统的公式。</p><p id="c118" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(4)行将大条件句<em class="mx"> p(Y|x) </em>拆分或分解成<em class="mx"> T </em>小条件句<em class="mx">p(yᵢ|xᵢ】</em>。我们可以这样做，因为线性动力系统的第二个方程。这个等式告诉我们两件事:</p><ol class=""><li id="6c84" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">随机变量 y <em class="mx"> ₜ </em>只取决于<em class="mx"> xₜ </em>。所以调节 y <em class="mx"> ₜ </em>在其他<em class="mx"> xₖ </em>那里<em class="mx"> k≠t </em>减少到调节<em class="mx"> Yₜ </em>在<em class="mx"> xₜ </em>。这允许我们分解<em class="mx"> p(Y|x) </em>中的条件部分。单词<em class="mx">“因式分解”</em>的意思是将一个联合概率拆分成许多更小概率的乘积。</li><li id="6bd5" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">给定<em class="mx"> xₜ </em>，y <em class="mx"> ₜ </em>独立于其他随机变量 y <em class="mx"> ₖ </em>其中<em class="mx"> k≠i </em>。所以 y <em class="mx"> ₁ </em>，y <em class="mx"> ₂ </em>，…，y <em class="mx"> _T </em>之间的联合概率等于这些随机变量以相应的 x <em class="mx"> ₁ </em>，<em class="mx"> x₂ </em>，…，x <em class="mx"> _T </em>为条件的乘积。</li></ol><p id="e7ed" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该线采用下降<em class="mx">无关条件</em>机构。</p><p id="0dfa" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(5)行将<em class="mx"> p(x₁，x₂，…，x_T) </em>分解成<em class="mx"> T </em>个较小概率的乘积。我们可以这样做，因为我们可以首先应用概率中的链式法则，然后使用线性动力系统的方程 2。这个等式定义<em class="mx"> xₜ </em>只依赖于前一个状态<em class="mx"> xₜ₋₁ </em>，就像这样:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qj"><img src="../Images/b35e0053f5fbc529b4e0b697a3b9fec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNp-2dhIKl63Beo53lpzSA.png"/></div></div></figure><p id="4a90" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(6)行使用括号对 x <em class="mx"> ₁ </em>、<em class="mx"> x₂ </em>、……的积分进行分组。这样，很容易看出我们可以首先计算 x <em class="mx"> ₁ </em>上的最内层积分，然后计算<em class="mx"> x₂ </em>上的积分，依此类推。该线采用<em class="mx">整合出</em>机构。</p><p id="5837" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们演示如何计算内部最大积分。这个积分是关于随机变量<em class="mx"> x₁ </em>的。如果我们能做到这一点，我们可以用同样的方法计算其他积分。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qk"><img src="../Images/4024966b6cb5f068b2847c52b7877fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9lV5BTQFDF_rzm0WWL2I5Q.png"/></div></div></figure><p id="a081" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在积分中，有三项。它们都是提及 x <em class="mx"> ₁、</em>的可能性项，因此我们必须将它们列在关于<em class="mx"> x1 </em>的积分下。我们展示了这三部分是什么，以及如何计算这个积分。</p><p id="5288" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，我们有:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ql"><img src="../Images/50324e2901e1721b5bb25098290bf148.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2lcEHDcEAuWbm1Iifqp8A.png"/></div></div></figure><p id="0337" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是因为我们已经定义了<em class="mx"> x </em> ₀ <em class="mx"> ~N(0，I) </em>。而<em class="mx"> X₁ </em>是根据线性动力系统的第一方程从<em class="mx"> X </em> ₀的线性变换。</p><p id="beb3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第二，我们有:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qm"><img src="../Images/6359ee7bce52f5ea2303568ddbd1785a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aw_qxdZcIbxGYqbuJ30YoA.png"/></div></div></figure><p id="4621" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是因为线性动力系统的第二个方程将随机变量<em class="mx"> y₁ </em>定义为随机变量<em class="mx"> x₁ </em>的线性高斯变换，我们将观测值<em class="mx"> Y₁ </em>代入<em class="mx"> y₁ </em>。</p><p id="c49b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后，我们有:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qn"><img src="../Images/9de9463c690b1affaaf9dc8f0d95d3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gy4elTiJJalrDJmpohtqOw.png"/></div></div></figure><p id="6288" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是因为线性动力系统的第一个方程将随机变量<em class="mx"> x₂ </em>定义为随机变量<em class="mx"> x₁.的线性高斯变换</em></p><p id="da26" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在集成变成了:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qo"><img src="../Images/1d45642002dea7fee248663dcb7e834a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1vQd6E-kM1pMWOI44IH8A.png"/></div></div></figure><p id="f7a1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请注意，在积分内部，三个高斯概率密度分布在不同的随机变量 x <em class="mx"> ₁、</em> y <em class="mx"> ₁ </em>和<em class="mx"> x₂ </em>上。因为积分是关于 x <em class="mx"> ₁ </em>的，我想把三个高斯概率密度转换成同一个随机变量 x <em class="mx"> ₁ </em>。所以我可以做到以下几点:</p><ol class=""><li id="d0a1" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">我可以应用两个高斯函数的乘积法则。应用该规则两次，以获得 x <em class="mx"> ₁ </em>上的单个高斯概率密度，该概率密度前面有一个常数。</li><li id="3f0e" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">我知道对一个概率密度积分会得到 1。所以积分值是常数。</li></ol><p id="5003" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了明白我的意思，让我们执行这个计划。顺便说一下，这是积分高斯概率密度乘积的常用技巧。</p><p id="1b2a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，由于<em class="mx"> p(x₁) </em>已经是<em class="mx"> x₁ </em>的概率密度函数，我们不需要做任何事情。</p><p id="9424" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第二，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qp"><img src="../Images/c7193b2ffa4906929849544f09a90a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_FQa7njNowwJakmHsx2RXg.png"/></div></div></figure><p id="6862" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">注意 2π的幂是 1/2，因为 y <em class="mx"> ₁ </em>是一维随机变量。如果有疑问，请查看<a class="ae lz" href="http://cs229.stanford.edu/section/gaussians.pdf" rel="noopener ugc nofollow" target="_blank">的定义，多元高斯概率密度函数</a>。</p><p id="1dc4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了将这个公式转换成 x <em class="mx"> ₁ </em>的概率密度函数，我们需要提取 x <em class="mx"> ₁ </em>前面的<em class="mx"> H </em>。我们还需要反转 x <em class="mx"> ₁ </em>前面的标志。这样，我们就可以为 x <em class="mx"> ₁ </em>匹配多元高斯概率密度函数的定义。</p><p id="8deb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们在<em class="mx"> Y₁-Hx₁ </em>前面乘以<em class="mx"> HH⁻ </em>。这不会改变该项的值，因为<em class="mx"> HH⁻ </em>计算出单位矩阵。经过一些操作，我们得到了:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qq"><img src="../Images/92027567c34d8347591e2aff14b26d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fi3mkRMTCXbptgJLm5IK_Q.png"/></div></div></figure><p id="7382" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有了这个，我们就可以进行转置(<em class="mx"> Y₁-HX₁)ᵀ </em>):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qr"><img src="../Images/14976de343211097436400fb0d1874a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*75bQ4Y7vaICYQRUrw4XEOQ.png"/></div></div></figure><p id="98be" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们现在用这两个新的等价变换替换原始公式中的相应项。我们有:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qs"><img src="../Images/106da3f2512589491a5f03b2a99535b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*srYzsT_PCiNX2wHcg_F3ew.png"/></div></div></figure><p id="8d8c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(1)行和第(2)行显示原始公式；这是 Y₁的概率密度函数。</p><p id="f283" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(3)行执行我们之前执行的术语转换。现在我们可以看到，指数项是随机变量<em class="mx"> x₁ </em>的新多元高斯概率密度函数的一部分。它有均值<em class="mx"> H⁻ Y₁，</em>并且它的协方差矩阵的逆是<em class="mx">hᵀσᵥ⁻h</em>，所以它的协方差矩阵是(<em class="mx">hᵀσᵥ⁻h</em>)<em class="mx">⁻=h⁻σᵥ(h⁻)ᵀ</em>。</p><p id="747f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该指数项只是多元高斯概率密度函数的<strong class="lf jd">部分</strong>,因为完整的多元高斯概率密度函数需要指数前的归一化常数。我们可以从协方差矩阵计算这个常数，但是为了节省空间，我们称这个常数为 M₁常数。</p><p id="a9c7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第(4)行写出了这个新的高斯分布的签名。由于这个分布包含常数<em class="mx"> M₁ </em>，我们必须将分布乘以<em class="mx"> M₁⁻ </em>以保持相等。</p><p id="d896" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后，我们在 p(x₂工作；Ax₁，σ_ w)通过执行同样的步骤:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qt"><img src="../Images/641bc92c480e75ee988752b552b26f32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FU2JETU3WerVX74Rgq2Irw.png"/></div></div></figure><p id="8a39" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">注意 2π的幂是 2/2，因为我们态的维数是 2。按照与之前相同的步骤，我们已经:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qu"><img src="../Images/a46b0d7669a4736d5fa5eefdc2ae7f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NvW-V_pQEO_AenwGgwGUbw.png"/></div></div></figure><p id="fee4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">并且我们表示多元高斯分布前面的常数<em class="mx"> M₂.</em></p><p id="4ef3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在集成变成了:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qv"><img src="../Images/eecf46cf17cd58c2c1bed807977387ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6tT2Zg16W1oqj0wUCqycNg.png"/></div></div></figure><p id="e625" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第三行使用常数<em class="mx"> M </em>表示概率密度前面的所有常数。</p><p id="c562" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">上述变换的重要性在于，它让我们看到，积分中的三个概率密度函数是随机变量 x <em class="mx"> ₁.的</em>所以我们可以用下面两个高斯乘积的规则来导出一个组合概率密度:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qw"><img src="../Images/b99352829e444c7fb753447270c6b24c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5HeJjaRwjR5j3ykF0pgnw.png"/></div></div></figure><p id="d055" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">和</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qx"><img src="../Images/0afce82b7f190d8782be6ad417206ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HSSLrszFMoRqDzmqmCC1EA.png"/></div></div></figure><p id="069a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这两个高斯乘积规则表明，组合概率密度仍然是针对同一随机变量，具有不同的均值、协方差和归一化常数。</p><p id="b1a6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于积分内部有三个多元高斯密度，我们需要应用这个规则两次，才能得出最终的单一概率密度。假设最终概率密度的归一化常数为 Zₓ₁，平均值和协方差为μₓ₁和σₓ₁，则我们有:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qy"><img src="../Images/b806116c8f3098b5e4ab19a8b083ae47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1pyp58PtyRQqMwPIPlKxlg.png"/></div></div></figure><p id="e84c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第四行两次应用两个高斯乘积规则来导出最终的概率密度。注意，这个最终概率密度积分为 1，所有概率密度函数都是如此。所以整个积分评估为<em class="mx"> MZₓ₁ </em>。</p><p id="7493" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请注意，<em class="mx"> MZₓ₁ </em>是一个提到<em class="mx"> x₂ </em>的表达式，因为<em class="mx"> x₂ </em>出现在第三分量高斯分布的均值中，见第三行。实际上，<em class="mx"> MZₓ₁ </em>就是多元高斯概率密度函数<em class="mx">p(x₂】</em>。所以当你对 x₂做第二次积分时，你会得到和对₁:做第二次积分相同的结构</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qz"><img src="../Images/881b14946f9fa8e675e022a2078dd136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XlCVM_c5sQk8M25Y-4Hr5g.png"/></div></div></figure><p id="7015" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所以，你可以继续计算<em class="mx"> x₂ </em>，<em class="mx"> x₃ </em>，…一个接一个地计算整个可能性<em class="mx"> p(Y) </em>。结果将与我们在使用卡尔曼滤波算法之前展示的结果相同。</p><p id="baeb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会问，为什么我们要用不同的方法来推导可能性:</p><ol class=""><li id="8e38" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">一种方法使用卡尔曼滤波算法，如前一节所述。</li><li id="a2c0" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">一种方法依赖于对联合概率<em class="mx"> p(Y，x) </em>进行因式分解，在当前章节中描述。</li></ol><p id="410a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">实际上，知道一个就足够了，这意味着无论用哪种方法，你都可以计算出可能性。然后你可以进行参数学习，并做出预测。而是从两个方面来做:</p><ol class=""><li id="f28e" class="nb nc it lf b lg lh lj lk lm nd lq ne lu nf ly ng nh ni nj bi translated">你可以检查一下这两种方法是否一致。他们应该会给出非常接近的结果，因为他们计算的是相同的量，可能性<em class="mx"> p(Y) </em>。</li><li id="72d4" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">您可以进行分析，看看哪种方法更快。</li><li id="f562" class="nb nc it lf b lg nk lj nl lm nm lq nn lu no ly ng nh ni nj bi translated">更重要的是(至少对我来说)，这是一个非常好的练习，可以锻炼你对概率模型的理解。推导公式真的可以帮助你理解之前定义的线性动力系统的性质。你会明白这些属性在计算可能性<em class="mx"> p(Y) </em>方面有多大帮助。有了可能性，你就可以做参数学习，做预测。</li></ol></div></div>    
</body>
</html>