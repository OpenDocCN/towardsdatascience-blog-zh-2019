<html>
<head>
<title>Tips &amp; Tricks in Multiple Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">å¤šå…ƒçº¿æ€§å›å½’çš„æŠ€å·§å’Œçªé—¨</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/tips-tricks-in-multiple-linear-regression-b5e83a4e73f1?source=collection_archive---------21-----------------------#2019-02-24">https://towardsdatascience.com/tips-tricks-in-multiple-linear-regression-b5e83a4e73f1?source=collection_archive---------21-----------------------#2019-02-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5fac" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">æ”¶é›†åˆ†ææ•°æ®ã€è¯Šæ–­æ¨¡å‹å’Œå¯è§†åŒ–ç»“æœçš„æ–¹æ³•</h2></div><p id="962d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è¿™ä¸ªåˆ†ææ˜¯æˆ‘å†³å®šåœ¨å­¦æ ¡å›å½’åˆ†ææ¨¡å—ä¸­æ‰¿æ‹…çš„ä¸€ä¸ªé¡¹ç›®ã€‚æˆ‘å·²ç»å­¦ä¹ å¹¶æ”¶é›†äº†å‡ ç§æ–¹æ³•ï¼Œä½ å¯ä»¥åœ¨ R ä¸­ä½¿ç”¨ï¼Œä½¿ä½ çš„åˆ†ææ›´æ·±å…¥ã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘æ€»æ˜¯è‡ªå­¦æœ€æœ‰å‘ç°çš„ä¸œè¥¿ã€‚</p><h1 id="c7cd" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">æ•°æ®</h1><p id="85b9" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">å›åº”å˜é‡:æ‰¿è®¤çš„æœºä¼š</p><p id="cde5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">é¢„æµ‹å› ç´ :GRE æˆç»©ï¼Œæ‰˜ç¦æˆç»©ï¼Œå¤§å­¦è¯„çº§ï¼ŒSOPï¼ŒLORï¼ŒCGPAï¼Œç ”ç©¶</p><p id="b8b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae mb" href="https://github.com/jasonyip184/regression_analysis/blob/master/Admission_Data.csv" rel="noopener ugc nofollow" target="_blank">é“¾æ¥åˆ° csv </a></p><h1 id="d5b2" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">å›¾ä¹¦é¦†</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="d6fa" class="ml lf it mh b gy mm mn l mo mp">library(dplyr);<br/>library(ggplot2);<br/>library(GGally);<br/>library(vioplot);<br/>library(corpcor);<br/>library(ppcor);<br/>library(mctest);<br/>library(ggfortify);<br/>library(lmtest);<br/>library(MASS);<br/>library(car);<br/>library(DAAG);<br/>library(jtools);<br/>library(relaimpo);</span></pre><h1 id="ff93" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">æè¿°æ€§ç»Ÿè®¡</h1><p id="1cbf" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated"><code class="fe mq mr ms mh b">summary(df)</code></p><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/8499945edb003237b14c9bf9d47763a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*oo-iZ5xbCmYwSHFVsrDvoQ.png"/></div></figure><h1 id="fecd" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">åˆ†å¸ƒå›¾</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="d007" class="ml lf it mh b gy mm mn l mo mp">par(mfrow=c(4, 2))<br/>colnames = names(df)<br/>for(name in colnames) {<br/>  vioplot(df[name], horizontal=TRUE, col='gold', lineCol='gold', lty=0, colMed='floralwhite', yaxt='n',rectCol='dodgerblue4')<br/>  title(main=name)<br/>}</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/e8466263899d2b12b8ca8ec93f1240f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*8RE6Dkfm6czoYD8xXPZSQw.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">There is no extreme skew for the variables. this makes the confidence intervals for estimating parameters for our predictors and estimating the mean response more meaningful.</figcaption></figure><h1 id="0419" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">æ£€æŸ¥ 1)DV å’Œæ¯ä¸ª iv ä¹‹é—´çš„çº¿æ€§ 2)iv ä¹‹é—´çš„å¤šé‡å…±çº¿æ€§</h1><p id="b9d3" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated"><code class="fe mq mr ms mh b">ggpairs(df, progress=FALSE)</code></p><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/457d33c793747033dd0d4acc44a12fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*hhLZVMy18I0Id-T7Miq6oA.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">From the last row, we can observe that most of the IVs seem to have a linear relationship with our response variable except for the binary variable Research. Therefore the assumption for linearity between DV and each of IVs hold.</figcaption></figure><p id="f73a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">æ‰€æœ‰å˜é‡çš„æˆå¯¹ç›¸å…³æ€§éƒ½ç›¸å½“é«˜ã€‚è¿™ä¼¼ä¹è¿åäº†å¤šå…ƒçº¿æ€§å›å½’æ²¡æœ‰å¤šé‡å…±çº¿æ€§çš„å‡è®¾ã€‚</p><h1 id="9da9" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">åç›¸å…³ç³»æ•°</h1><p id="d4e1" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">è€ƒè™‘åˆ°å…¶ä»–é¢„æµ‹å› ç´ çš„æ··æ‚æ•ˆåº”ã€‚</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="34ea" class="ml lf it mh b gy mm mn l mo mp">pcorr = as.data.frame(cor2pcor(cov(df)))<br/>names(pcorr) = names(df)<br/>rownames(pcorr) = names(df)<br/>pcorr = format(pcorr, digits=1)<br/>print.data.frame(pcorr)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/22f8e80598da96e509f030debbeb4a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*1U-3bbEUV7OHshZWCLuC5w.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">The partial correlation coefficients suggest otherwise, that there is less multicollinearity with only GRE.Score &amp; TOEFL.Score having a value &gt; 0.4. Partial correlation between CGPA and our response variable Chance.of.Admit is fairly high but it does not violate the â€œNo Multicollinearity between its IVs assumptionâ€ of MLR.</figcaption></figure><h1 id="6b6b" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">ä½¿ç”¨ä¸ªåˆ«å¤šé‡å…±çº¿æ€§è¯Šæ–­æµ‹é‡</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="363a" class="ml lf it mh b gy mm mn l mo mp">imcdiag(df[,1:7],df$Chance.of.Admit)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/6a5204ea1ec01fa4dfb90335aa54df46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbI3sAlcK6yYH_qPI50e9g.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">All the predictors have a VIF (=1/(1-RÂ²)) value of &lt;5 which indicates that the multicollinearity is not so problematic.</figcaption></figure><h1 id="dbb7" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">å®‰è£… MLR</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="7cc8" class="ml lf it mh b gy mm mn l mo mp">fit = lm(Chance.of.Admit ~ ., data=df)<br/>summary(fit)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/15b091d367fb782ccb37d02f0827bae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*VlrzX75F9wyso9Bf5SYdhw.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Fit: Chance.of.Admit = -1.28 + 0.00186(GRE.Score) + 0.00278(TOEFL.Score) + 0.00594(University.Rating) + 0.00159(SOP) + 0.0169(LOR) + 0.118(CGPA) + 0.0243(Research) (3s.f.)</figcaption></figure><p id="87fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è¿™è¡¨æ˜å¹³å‡è€Œè¨€ï¼ŒGRE æ¯å¢åŠ ä¸€ä¸ªå•ä½ã€‚åˆ†æ•°/æ‰˜ç¦ã€‚åˆ†æ•°/å¤§å­¦ã€‚Rating/SOP/LOR/CGPA/Research åœ¨ä¿æŒæ‰€æœ‰å…¶ä»–å˜é‡ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå°†å½•å–æœºä¼šå¢åŠ  0.00186/0.00278/0.00594/0.00159/0.0169/0.118/0.0243ã€‚</p><p id="c911" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">F ç»Ÿè®¡é‡çš„ p å€¼æ˜¯&lt;2.2e-16, indicating that we can reject the null hypothesis that the intercept-only model is the same fit as the MLR model even at alpha=0.001. Therefore, the MLR model is highly statistically significant at the 0.01 significance level.</p><p id="e975" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">The Adjusted R-squared: 0.8194 is high which suggests that the model is a good fit.</p><p id="ddd6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">The coefficients for GRE.Score, TOEFL.Score, LOR, CGPA, Research are statistically significant at alpha=0.01 where the respective pvalues &lt; 0.01 as we reject the null that their coeffs is 0 at the 0.01 significance level.</p><p id="dafb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">The coefficients for University.Rating (0.118) and SOP (0.728263) are &gt; 0.01ï¼Œæˆ‘ä»¬æ— æ³•åœ¨ 0.01 çš„æ˜¾è‘—æ€§æ°´å¹³ä¸Šæ‹’ç»å®ƒä»¬çš„ç³»æ•°ä¸º 0 çš„ç©ºå€¼ã€‚</p><h1 id="feb3" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">æ¨¡å‹è¯Šæ–­</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="5e90" class="ml lf it mh b gy mm mn l mo mp">autoplot(fit)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/39b5441b141a3b61d362bff0b968bf62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*Glt0lchblLFJDiear-wLbQ.png"/></div></figure><p id="8c29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> (1)æ®‹å·® vs æ‹Ÿåˆ</strong></p><p id="c0d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è“çº¿(æ¯ä¸ªæ‹Ÿåˆå€¼çš„æ®‹å·®å¹³å‡å€¼)å‡ ä¹æ˜¯å¹³çš„ã€‚è¿™è¡¨æ˜æ®‹å·®æ²¡æœ‰æ˜æ˜¾çš„éçº¿æ€§è¶‹åŠ¿ã€‚æ®‹å·®çœ‹èµ·æ¥æ˜¯éšæœºåˆ†å¸ƒçš„ï¼Œä½†å½“æ¥è¿‘è¾ƒé«˜çš„æ‹Ÿåˆå€¼æ—¶ï¼Œå®ƒä¼šæ”¶æ•›ã€‚è¿™ä¼¼ä¹æ˜¯æ–¹å·®çš„å‡å°‘ï¼Œå®ƒè¿åäº† MLR çš„åŒæ–¹å·®å‡è®¾ã€‚</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="58f2" class="ml lf it mh b gy mm mn l mo mp">bptest(fit)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/61ff42cdefd88681582d0998f3a74473.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*Z5wg1bTY6u6kfcNjjnbpAw.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Using the Breusch-Pagan test, we can reject the null hypothesis at the 0.05 significance level that variance of the residuals is constant and infer that heteroscedasticity is present. Therefore, this makes our coefficient estimates less precise and increases the likelihood that the estimates are further from the true population value.</figcaption></figure><p id="3bcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> (2)æ­£å¸¸ Q-Q </strong>(åˆ†ä½æ•°-åˆ†ä½æ•°å›¾)</p><p id="944a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">æ®‹å·®ä¼¼ä¹åç¦»è¾ƒä½å°¾éƒ¨çš„å¯¹è§’çº¿å¾ˆå¤šã€‚æ®‹å·®çš„åˆ†å¸ƒå‘å·¦å€¾æ–œã€‚è¿™è¡¨æ˜ MLR æ¨¡å‹å¯¹æ®‹å·®æ­£æ€æ€§çš„å‡è®¾è¢«è¿åã€‚</p><p id="3b60" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ä½¿ç”¨ Box-Cox å¹‚å˜æ¢å˜æ¢å“åº”å˜é‡ï¼Œä½¿å…¶æ­£å¸¸å¹¶å¤„ç†å¼‚æ–¹å·®</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="1f23" class="ml lf it mh b gy mm mn l mo mp">bc = boxcox(Chance.of.Admit ~ ., data=df);</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8f4cacbf1a1870107d0feb396b9674b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*wOCMdi5zdtnisxFofE0PTg.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">The procedure identifies an appropriate exponent (Lambda = l) to use to transform data into a â€œnormal shape. The Lambda value indicates the power to which all data should be raised and it is suggested to use lambda=2.</figcaption></figure><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="1a93" class="ml lf it mh b gy mm mn l mo mp">lambda = bc$x[which.max(bc$y)]<br/>powerTransform &lt;- function(y, lambda1, lambda2 = NULL, method = "boxcox") {<br/>  boxcoxTrans &lt;- function(x, lam1, lam2 = NULL) {<br/>    # if we set lambda2 to zero, it becomes the one parameter transformation<br/>    lam2 &lt;- ifelse(is.null(lam2), 0, lam2)<br/>    if (lam1 == 0L) {<br/>      log(y + lam2)<br/>    } else {<br/>      (((y + lam2)^lam1) - 1) / lam1<br/>    }<br/>  }<br/>  switch(method<br/>         , boxcox = boxcoxTrans(y, lambda1, lambda2)<br/>         , tukey = y^lambda1<br/>  )<br/>}<br/># re-run with transformation<br/>bcfit &lt;- lm(powerTransform(Chance.of.Admit, lambda) ~ ., data=df)</span><span id="9d2f" class="ml lf it mh b gy nn mn l mo mp">summary(bcfit)</span></pre><p id="a905" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è°ƒæ•´åçš„ R å¹³æ–¹ä» 0.8194 å¢åŠ åˆ° 0.8471ï¼Œè€Œé¢„æµ‹å› å­ä»ç„¶æ˜¾è‘—ã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ¨¡å‹çš„å¯è§£é‡Šæ€§è¾ƒå·®ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹å°½å¯èƒ½ç®€æ´ã€‚æˆ‘ä»¬å°†åœ¨ä»¥åæ¢ç´¢æ›´å¤šçš„æ¨¡å‹ã€‚</p><p id="e95d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> (3)æ®‹å·®ä¸æ æ†</strong></p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="71ee" class="ml lf it mh b gy mm mn l mo mp">cooksd &lt;- cooks.distance(fit)<br/>sample_size &lt;- nrow(df)<br/>plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")<br/>abline(h = 4/sample_size, col="red")<br/>text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd&gt;4/sample_size, names(cooksd),""), col="red")</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/569923d8c635b3bf907ffd02d0a93735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*F-UFwkmzPUaWmfXIAAUJjQ.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">This helps us to find influential outliers. They are points above the dashed line which are not approximated well by the model (has high residual) and significantly influences model fit (has high leverage). By considering Cookâ€™s D &gt; 4/sample size criterion, we identify influential outliers to remove.</figcaption></figure><h1 id="2861" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">ç§»é™¤å¼‚å¸¸å€¼åé‡æ–°æ‹Ÿåˆ MLR</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="d354" class="ml lf it mh b gy mm mn l mo mp">influential = as.numeric(names(cooksd)[(cooksd &gt; (4/sample_size))])<br/>df2 = df[-influential, ]<br/>fit2 = lm(Chance.of.Admit ~ ., data=df2)<br/>summary(fit2)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi no"><img src="../Images/fdbd731714250327c4b2656cfc784d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*77kySXKIrp7ROSBQtgQUtQ.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">By removing the highly influential outliers, we refitted the model on the filtered data and the Adjusted R-squared increased to 0.8194 to 0.8916 without introducing complexity to the model.</figcaption></figure><h1 id="232a" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">ä½¿ç”¨å“åº”å˜é‡çš„å‡½æ•°æ¥æ‹Ÿåˆæ¨¡å‹</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="fce5" class="ml lf it mh b gy mm mn l mo mp">fit3 = lm(exp(Chance.of.Admit) ~ ., data=df2)<br/>summary(fit3)</span></pre><p id="3987" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">é€šè¿‡å›å½’æˆ‘ä»¬å¯¹é¢„æµ‹å› å­çš„ååº”æŒ‡æ•°ï¼Œæˆ‘ä»¬å¾—åˆ°äº†è°ƒæ•´åçš„ R å¹³æ–¹ä» 0.8916 å¢åŠ åˆ° 0.9023ï¼Œè€Œé¢„æµ‹å› å­ä»ç„¶ä¿æŒæ˜¾è‘—æ€§ã€‚</p><h1 id="84d9" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">é€šè¿‡æ·»åŠ äº¤äº’é¡¹æ¥è¯´æ˜äº¤äº’</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="b68a" class="ml lf it mh b gy mm mn l mo mp">fit4 = lm(exp(Chance.of.Admit) ~ GRE.Score*University.Rating+TOEFL.Score+Research+SOP+LOR+CGPA, data=df2)<br/>summary(fit4)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2b689f9cbe15e3d7dd029301d4e8a91b.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*UZZFJjltlkS3vy4Bz7-fCA.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Interaction arises as the relationship between Chance.of.Admit and the IVs: GRE.Score and University.Rating is affected by the interaction between the GRE.Score &amp; University.Rating. This makes it hard to predict the consequence of changing the value of GRE.Score &amp; University.Rating without controlling for this interaction.</figcaption></figure><p id="e703" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è¯¥æ¨¡å‹æ˜¾ç¤ºäº† GRE ä¹‹é—´çš„æ˜¾è‘—äº¤äº’ä½œç”¨ã€‚åˆ†æ•°&amp;å¤§å­¦ã€‚è¯„çº§ä¸º p å€¼=0.000799 &lt; 0.001 and is significant at the 0.001 significance level.</p><h1 id="fbd2" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">Comparing nested models with ANOVA</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="1645" class="ml lf it mh b gy mm mn l mo mp">anova(fit3, fit4)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/44ea447cc822907bff1cf276efaa0aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*aC-spPSLDgW1qAHzslUctQ.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">The first order model is nested within the interaction model. By using ANOVA to compare the simpler first order model vs the more complex model with interaction term, the p-value=0.0007995 is &lt;0.001. The null hypothesis that the reduced simpler model is adequate is rejected at the 0.001 significance level. Therefore, the complex model did significantly improve the fit over the simpler model.</figcaption></figure><h1 id="609b" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">Drop insignificant predictor SOP</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="4246" class="ml lf it mh b gy mm mn l mo mp">fit5 = lm(exp(Chance.of.Admit) ~ GRE.Score*University.Rating+TOEFL.Score+Research+LOR+CGPA, data=df2)<br/>summary(fit5)</span></pre><p id="dced" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Previously, SOP was insignificant at the 0.05 significance level and even after removing it, the modelâ€™s Adjusted R-squared is still 0.904.</p><h1 id="a982" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">Variable selection using stepwise model selection by AIC</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="2d69" class="ml lf it mh b gy mm mn l mo mp">step &lt;- stepAIC(fit5, direction="both")</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/87d17042e09050b9b1e0ffdf0b338908.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*f9DOD4BJ3ehNVP7EFSla8Q.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">A model with fewer parameters is to be preferred to one with more. AIC considers both the fit of the model and the number of parameters used. Having more parameters result in penalty. AIC helps to balance over- and under-fitting. The stepwise model comparison iteratively adds/removes variables one at a time and compares the AIC. The lowest AIC is selected for the final model.</figcaption></figure><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="3502" class="ml lf it mh b gy mm mn l mo mp">step$anova</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/491b84c3d691976cd1fb771a849c5320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*vRFpXPtTLbA8XC412KuQQw.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">In our case, there no further addition or removal of variables required by AIC.</figcaption></figure><h1 id="c59e" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">Relative feature importance</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="5406" class="ml lf it mh b gy mm mn l mo mp">calc.relimp(fit5,type="lmg", rela=TRUE)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b27873645c322539134ff6cec71cf1a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*wSnyDbN7IPpSM07t68wuDA.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Relative importance is measured by an algorithm by Lindemann, Merenda and Gold (lmg; 1980) which decomposes total R-squared and observe the increase in R-squared by adding the predictors sequentially. The order of adding predictors matters and therefore, the algorithm takes the average of the R-squared across all orderings.</figcaption></figure><p id="e236" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Relative importance is measured by an algorithm by Lindemann, Merenda and Gold (lmg; 1980) which decomposes total R-squared and observe the increase in R-squared by adding the predictors sequentially. The order of adding predictors matters and therefore, the algorithm takes the average of the R-squared across all orderings.</p><p id="e1da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">The features are ranked in this order with highest relative importance first: GRE.Score, CGPA, University.Rating, TOEFL.Score, LOR, Research and GRE.Score*University.Rating.</p><h1 id="06a4" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">K-Fold cross-validation results on final model</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="e8bc" class="ml lf it mh b gy mm mn l mo mp">cv_new = CVlm(data=df2, fit5, m=3, printit=FALSE)</span><span id="b006" class="ml lf it mh b gy nn mn l mo mp">attr(cv_new, "ms")</span><span id="832a" class="ml lf it mh b gy nn mn l mo mp">[1] 0.007749426</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/2bf3343242264ecea354709cbecaa153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*ROvDrNoNkA14Lf6xpQ6d4Q.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Each of the k-fold modelâ€™s prediction accuracy isnâ€™t varying too much for any one particular sample, and the lines of best fit from the k-folds donâ€™t vary too much with respect the the slope and level. The average mean square error of the predictions for 3 portions is 0.00775. The value is low and represents a good accuracy result.</figcaption></figure><h1 id="33c4" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">95% CIs for every IVâ€™s estimates</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="99c7" class="ml lf it mh b gy mm mn l mo mp">export_summs(fit5, error_format = "[{conf.low}, {conf.high}]", digits=5)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/87910667b6b15a1a4ef975b4d12d4364.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*UXmYusW4cLWe_0_40ncHkQ.png"/></div></figure><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="9129" class="ml lf it mh b gy mm mn l mo mp">plot_summs(fit5)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/77c86118defc19ac571827f1c16d2b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*_2mpeurYyPlddXdsQZ4WBw.png"/></div></figure><h1 id="92e8" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">Individual CI plots</h1><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="e5fc" class="ml lf it mh b gy mm mn l mo mp">effect_plot(fit4, pred = CGPA, interval = TRUE, plot.points = TRUE)</span></pre><figure class="mc md me mf gt mu gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/ac34ad84be22728d1611103374acefba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*M6uwwr9750iKW-bDMIIXog.png"/></div></figure></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="ee46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">I hope this has helped improve your analysis one way or another. Please do not take any of it as a perfect example or as entirely correct and accurate as I am still learning as well. This has certainly liven up my otherwise dull module ğŸ˜ƒ.</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="690c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae mb" href="https://jasonyip184.github.io/regression_analysis/" rel="noopener ugc nofollow" target="_blank">é“¾æ¥åˆ°ç¬”è®°æœ¬</a></p><p id="540b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è¯·åœ¨<a class="ae mb" href="http://linkedin.com/in/jasonyip184" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>æˆ–é€šè¿‡ jasonyip184@gmail.com ä¸æˆ‘è¿›ä¸€æ­¥è®¨è®ºï¼</p></div></div>    
</body>
</html>