<html>
<head>
<title>A Deeper Look into Gradient Based Learning for Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络基于梯度学习的深入研究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-deeper-look-into-gradient-based-learning-for-neural-networks-ad7a35b17b93?source=collection_archive---------13-----------------------#2019-09-20">https://towardsdatascience.com/a-deeper-look-into-gradient-based-learning-for-neural-networks-ad7a35b17b93?source=collection_archive---------13-----------------------#2019-09-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/5ced5286be46c059022fec7161e0f4ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*SjtKOauOXFVjWRR7iCtHiA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk"><a class="ae jy" href="http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif" rel="noopener ugc nofollow" target="_blank">http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif</a></figcaption></figure><p id="dc75" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在深度学习中，我们试图逼近输入和输出之间的函数的方法是，首先随机初始化网络的参数，然后通过最小化损失函数来逐步更新它们以找到这些参数的正确配置，该损失函数在大多数情况下本质上是非凸的(许多局部最小值而不是单个全局最小值)，因此训练神经网络是一个不确定的组合优化问题，因为我们不能保证当达到时收敛点是全局最小值或误差表面上的鞍点。</p><p id="b878" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">一大堆算法被提出来处理这种情况，其中流行的有 SGD，momentum based，NAG，RMSprop 和 ADAM。它们都是经典梯度下降算法的一些变体，而结合了 RMSprop 和 momentum 的 ADAM 被认为是当前的技术水平。</p><h1 id="be86" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">1.梯度下降更新规则</h1><p id="944f" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">考虑网络的所有权重和偏差被展开并堆叠到某个高维向量空间中的单个向量θ中，并且与之相关联的损失是ϑ(θ).θ的方向或大小的任何变化都会给我们一个新的ϑ(θ).值</p><p id="518a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在训练网络时，在每个时期，我们的目标是找到另一个向量∏θ，使得新的损失ϑ(θ+∏θ小于先前的损失ϑ(θ，因此在每个时期，目标是找到∏θ，使得以下不等式成立:</p><p id="0632" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi">ϑ(θ + ∆θ) — ϑ(θ) &lt; 0</p><p id="4761" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在我们继续寻找∏θ之前，让我们快速回顾一下<strong class="kb ir">泰勒级数</strong>，它指出任何非多项式函数都可以精确地近似为多项式项的无穷和，</p><p id="a3be" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">(x) = c0 + c1 x + c2 x + c3 x +。。。。</p><p id="2fde" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要满足的唯一条件是 LHS 和 RHS 的一阶、二阶和所有高阶导数应该完全匹配，并且(x)应该是一个无限可微的函数。</p><blockquote class="ma mb mc"><p id="3bdd" class="jz ka md kb b kc kd ke kf kg kh ki kj me kl km kn mf kp kq kr mg kt ku kv kw ij bi translated">cos(θ)在θ= 0°时，我们能否写成 cos(θ) = 1- 1/2 θ为二阶泰勒近似。因为我们只使用前三项(c1= 1，c2 = 0，c3 = 1/2)而不是所有的无限多项式项，所以这种近似只在θ接近 0°时才有效。</p></blockquote><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/958bfacb45e3849e5973bbac97b599a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/1*eEzj4aj6N1a3ihF9WukM1w.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk"><a class="ae jy" href="https://bespokeblog.files.wordpress.com/2011/07/sintay.png" rel="noopener ugc nofollow" target="_blank">https://bespokeblog.files.wordpress.com/2011/07/sintay.png</a></figcaption></figure><p id="39d9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">通常，x 处的泰勒级数是以下幂级数</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/cf4ebcf2a576fcb141c8bca4d1555ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*0vP6ejLjrDOn59bNzcm-Ng.png"/></div></figure><p id="0298" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于一个小的 x，λ(x)≈λ(x+∏x)。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/84661dcfef987d66c0c0bb7c53e81eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*0-aBrb0afdO_7XnC_twpKQ.png"/></div></figure><p id="abc5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于多元函数ф(x ),就像损耗ϑ(θ和ϑ(θ+∈θ),它依赖于向量θ和θ+∈θ，下面是一阶泰勒近似</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/3ae712cfbac8adbf9262935589c83111.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*ErD-y0KPVyUScMaiUAq9WA.png"/></div></figure><p id="c01e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这是(x+∏x)的二阶泰勒近似</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/f6c116d0e7673edd39e34a7400629691.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*Zws257BaX_YavkgaBZigAQ.png"/></div></figure><p id="1ae7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">取损失函数的一阶泰勒近似</p><p id="52da" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">ϑ(θ+∏θ)=ϑ(θ)+∏θ∇ϑ(θ)<br/>ϑ(θ+∏θ)—ϑ(θ)=∏θ∇ϑ(θ)</p><p id="8bae" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们知道，为了使损失函数最小化，每一步的损失都应该小于前一步，并且这个不等式(ϑ(θ+∏θ)-ϑ(θ)&lt; 0) should hold, and thus from the first order Taylor approximation implies that the dot product ∆θ ∇ϑ(θ) &lt; 0. <br/>因为点 B (A B cos(ϕ))只能在它们之间的角度大于 90 度时为负，当∏θ与损失梯度的方向相反时，在任何时期损失的最大减少都是可能的</p><p id="9a43" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这给了我们众所周知的<strong class="kb ir">梯度下降</strong>更新规则</p><p id="af07" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi">θ := θ — ∇ϑ(θ)</p><blockquote class="ma mb mc"><p id="bd3e" class="jz ka md kb b kc kd ke kf kg kh ki kj me kl km kn mf kp kq kr mg kt ku kv kw ij bi translated">因为我们使用的是损失函数的一阶泰勒近似，它只在θ的邻近区域有效，所以∏θ必须小是一个必要条件。所以我们乘以η &lt;&lt; 0 with ∆θ in our update rule.</p><p id="d1a8" class="jz ka md kb b kc kd ke kf kg kh ki kj me kl km kn mf kp kq kr mg kt ku kv kw ij bi">θ := θ — η ∇ƒ(θ)</p><p id="f458" class="jz ka md kb b kc kd ke kf kg kh ki kj me kl km kn mf kp kq kr mg kt ku kv kw ij bi translated">In practice, larger η also causes overshooting in machine learning and is termed as the learning rate and is a hyper parameter.</p></blockquote><h2 id="1a75" class="mq ky iq bd kz mr ms dn ld mt mu dp lh kk mv mw ll ko mx my lp ks mz na lt nb bi translated">Limitations</h2><p id="fd8b" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">One of the limitations of Vanilla Gradient Descent is that in the region of gentle slopes, computed gradients are very small resulting in a very slow convergence. One may simply increase the value of η but it may violate the assumption of first order Taylor approximation of loss function, also larger learning rate may cause overshooting.</p><p id="3b2c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">Another point to consider is that near the minima loss functions are slightly ellipsoidal and gradients in these regions from the first order Taylor approximation of loss function are almost orthogonal to the true direction of convergence, causing a zig-zag path and a slow convergence.</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/10ba9843a983cfa24575341278fa94a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*hytYy8vTsTggPFqolLt3Og.png"/></div></figure><blockquote class="ma mb mc"><p id="a250" class="jz ka md kb b kc kd ke kf kg kh ki kj me kl km kn mf kp kq kr mg kt ku kv kw ij bi translated">Although it is not very clear, but second order Taylor approximation may solve this problem. but calculation of ∇ ²ƒ(θ) (Hessian) is computationally very expensive. There are some proposed implicit ways of using hessian by setting the learning rate η as 1/ β, where β is the dominant eigenvalue of the hessian. <a class="ae jy" href="https://www.cs.princeton.edu/courses/archive/fall18/cos597G/lecnotes/lecture3.pdf" rel="noopener ugc nofollow" target="_blank">点击此处</a>阅读更多关于这个提议的解决方案。</p></blockquote><h1 id="ed70" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">2.利用动力学习</h1><p id="722d" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">为了解决梯度下降算法收敛速度慢的问题，一种方法是在前面步骤中计算的梯度的指数加权平均方向上更新θ。</p><p id="142d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">动量 _t = γ *动量 _t -1 + η ∇ϑ(θ_t) <br/> θ_t+1 := θ_t -动量 _t</p><p id="ddef" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">其中∇ϑ(θ_t)表示在步骤 t 计算的梯度</p><p id="60eb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">momentum _ 0 = 0<br/>momentum _ 1 = momentum _ 0+η∇ϑ(θ_1)= η∇ϑ(θ_1)<br/>momentum _ 2 = momentum _ 1+η∇ϑ(θ_2)=γη∇ϑ(θ_1)+η∇ϑ(θ_2)<br/>momentum _ 3 = momentum _ 2+η∇ϑ(θ_3)<br/>=&gt;γη∇ϑ(θ_1)+γη∇ϑ(θ_2)+η∏ϑ(θ_ 3)</p><p id="e6dd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">设置γ &lt;&lt; 1 insures that the higher power of γ causes decay in the gradients computed in previous steps and maximum weight is given to the last step. <br/>动量会导致快速收敛，即使在梯度下降需要大量时间收敛的缓坡区域也是如此。</p><h2 id="1e0e" class="mq ky iq bd kz mr ms dn ld mt mu dp lh kk mv mw ll ko mx my lp ks mz na lt nb bi translated">限制</h2><p id="e32c" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">虽然动量导致比梯度下降相对更快的收敛，但是它通常超过目标并导致在最小值附近振荡。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/1cc4d70db10e6b5adf82a081219cbffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/1*K3i65aLdM6wJEba0Fi1msA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">A pendulum oscillating around minima because of momentum</figcaption></figure><h1 id="7cdb" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">3.内斯特罗夫加速梯度下降法</h1><p id="aec1" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">NAG 是基于动量梯度下降算法的简单改进。正如我们已经讨论过的，在动量项中，我们也考虑了具有误差面梯度的更新规则中的历史动量向量。</p><ul class=""><li id="3478" class="nd ne iq kb b kc kd kg kh kk nf ko ng ks nh kw ni nj nk nl bi translated">初始化动量= 0</li><li id="2fd5" class="nd ne iq kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">还包括动量项的更新步骤<br/> θ_t+1 = θ_t - {γ *动量+η * ∇ϑ(θ_t)}</li><li id="ece9" class="nd ne iq kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">将当前梯度加入动量<br/>动量=动量+ η * ∇ϑ(θ_t)</li></ul><p id="913b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">想象在步骤= t，我们在误差面上的点 A。在 A 点计算的梯度会把我们带到 B 点，动量会把我们带到 C 点，两者都会把我们带到 d 点。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/03eac5b70e4c89fe887b696dd72af01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*GTkJcpA1aOnocedMDmAQ3Q.png"/></div></figure><p id="084f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这种情况下，动量本身是超调的，在 A 点计算的梯度甚至加入其中，导致更大的超调。</p><p id="3c8b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">NAG 说，不是计算 A 点的梯度并把它们加到从 A 点到 D 点着陆的动量中，而是应该计算 C 点(前瞻点)的梯度并把它加到从 A 点到 D '点着陆的动量中，因此与动量相比减少了最小值附近的振荡。</p><ul class=""><li id="f068" class="nd ne iq kb b kc kd kg kh kk nf ko ng ks nh kw ni nj nk nl bi translated">初始化动量= 0</li><li id="dd72" class="nd ne iq kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">计算前瞻点<br/> look_ahead = θ_t +动量</li><li id="566e" class="nd ne iq kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">用在 look_ahead 点计算的梯度更新 nag<br/>θ_ t+1 =θ_ t—{γ*动量+ η*∇ϑ(θ_look_ahead)}</li><li id="d1ec" class="nd ne iq kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">将前瞻处的梯度加入动量<br/>动量=动量+ η*∇ϑ(θ_look_ahead)</li></ul><h1 id="6ad1" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">4.Adagrad 和 RMSprop 用于自适应学习速率</h1><p id="19e2" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">到目前为止，我们一直在寻找更好的算法来更快地通过小梯度误差表面。训练大型神经网络的另一个问题是许多特征的不均匀稀疏性。想象与产生激活 h(w.x + b)的特征 x1 相关联的权重 w1，并且应用 L2 损失来训练网络。在大多数情况下，如果 x1 为零，则相对于 w1 的损耗梯度将非常小。</p><p id="f05a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">∂ϑ/∂w1 =(h(w . x+b)—y)* h’(w . x+b)* x1。(乘以 x1)</p><p id="f9e1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">深度神经网络中不同特征的稀疏性的这种不平衡导致所计算的梯度值的显著变化，从而导致与稀疏特征相比，非稀疏特征的权重的更大更新。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/30a432caa9d89b441a796eebf162d513.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*UBpYckScxWW86EQ3NpmPYw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">NPTEL NOC-IITM, Deep Learning CS7015</figcaption></figure><p id="ad76" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在上图中，与 w 相关的特征非常稀疏，这导致了上图中所示的收敛路径。</p><p id="4e12" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">Adagrad 的主要思想是根据特征的稀疏性将自适应学习速率分配给不同的权重，与具有较小梯度的权重相比，具有较大梯度的权重被分配较小的学习速率。</p><p id="cd39" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这可以简单地通过累积先前的梯度并从中划分学习率来实现。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c7471dd1add57e4bdbb19c98d1742db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*8vV5jVfsPTvw7tHYqpiNww.png"/></div></figure><p id="9d13" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">其中，ϵ是一个超参数，通常设置为 1e-6<br/>，这是<strong class="kb ir">自适应梯度更新规则(Adagrad) </strong>，由于某些原因，它在分母中没有平方根时将不起作用，对此没有明确的答案。<br/> v 是累积梯度的更新向量，通过这种方式，我们可以根据特征的稀疏性调整学习率。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/2da20d7a1fee7456b37f534b108639a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*m05L8s62XqzSq799Sw0aqQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">NPTEL NOC-IITM, Deep Learning CS7015</figcaption></figure><p id="cb88" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在上图中，绿色曲线代表阿达格拉德采取的收敛路径。它比 momentum 或 NAG 路径更短，在稀疏情况下速度更快，但可能不会收敛。<br/>原因是如果 v_t 的值对于不稀疏的特征来说非常大，我们只是在多次迭代中杀死梯度。</p><h2 id="b941" class="mq ky iq bd kz mr ms dn ld mt mu dp lh kk mv mw ll ko mx my lp ks mz na lt nb bi translated">RMSprop</h2><p id="0ff7" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">对于自适应学习速率，我们希望将η除以每个权重的累积梯度的平方根，但是我们不希望梯度的“过度累积”用于不太稀疏的特征，以防止步长的消失。</p><p id="f843" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">一个可能的解决方案是取梯度平方的指数加权平均值(就像动量一样)，而不是简单地相加。</p><p id="b535" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">v_t = β * v_t + (∇ w_t)并将β设置在 0 到 1 之间(β的默认设置是 0.9 或 0.95)</p><p id="b139" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">似乎累加梯度平方的加权平均的“分量”而不是整个梯度可以产生更好的结果。</p><p id="1ba4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">v_t = β * v_t + (1 — β) * (∇ w_t)其中，通过乘以 1 — β，我们仅在 v_t 中累加梯度平方的一个分量作为加权平均值</p><p id="355e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果我们用上面的等式代替 Adagrad 的累加步骤，那就变成 RMSprop(通过梯度的加权均方根的延拓来调整学习速率)。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c2f06a123c9b115a69b8764f4bdc2d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*Ju5EuLuHqpU7zl3bEvtVeQ.png"/></div></figure><p id="d1ae" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">下图显示了 RMSprop 采用的收敛路径。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi nw"><img src="../Images/b52a782917fc956ecf185ec3ef37050f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*-8Ic-2tHV5ZmYhuHOIE8KA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">NPTEL NOC-IITM, Deep Learning CS7015</figcaption></figure><h1 id="b9d2" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">亚当·乐观主义者——结合一切，获得当前的艺术状态</h1><ul class=""><li id="7f34" class="nd ne iq kb b kc lv kg lw kk ob ko oc ks od kw ni nj nk nl bi translated">Venilla 梯度下降<br/>重复直到收敛:<br/>。。。θ = θ — η * ∇θ .。。。。㈠</li><li id="7798" class="nd ne iq kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">基于动量的梯度下降<br/>初始化 m = 0 <br/>重复直到收敛:<br/>。。。m = γ * m + η * ∇θ。。。。。(二)<br/>。。。θ = θ — m。。。。㈣</li><li id="8049" class="nd ne iq kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">具有自适应学习的 RMSprop 更新规则<br/>初始化 v = 0 <br/>重复直到收敛:<br/>。。。v = β * v + (1 — β) * (∇θ)。。。。。<br/>(五)。。。θ = θ — {η / √(v + ϵ)} * ∇θ。。。。。㈥</li></ul><p id="99f3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在 ADAM 中，关键思想是结合动量和 RMSprop，即在没有η的情况下计算等式(ii)中的 m，并用等式(vi)中的∇θ代替 m。<br/>这使它成为适应性的时刻</p><p id="d706" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">初始化 m = 0，v = 0 <br/>重复直到收敛:<br/>。。。m = β1 * m + (1 — β1) * ∇θ。。。。。(七)<br/>。。。v = β2 * v + (1 — β2) * (∇θ)。。。。。(八)<br/>。。。θ = θ — {η / √(v + ϵ)} * m。。。。㈨</p><p id="2797" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这是 ADAM 的更新规则。人们应该注意到等式(vii)与等式(ii)完全相同。γ被重命名为β1，并且我们通过对自适应学习速率应用 RMSprop 的相同逻辑，仅累积梯度(1-β1)的一部分。<br/>亚当<strong class="kb ir"> </strong>的一般设置为β1 = 0.9，β2 = 0.999，η = 0.01，ϵ = 1e — 6</p><h2 id="c45e" class="mq ky iq bd kz mr ms dn ld mt mu dp lh kk mv mw ll ko mx my lp ks mz na lt nb bi translated">ADAM 中的偏差校正</h2><p id="47ea" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">Adam 的上述更新规则适用于批量梯度下降。但在小批量梯度和随机梯度下降中，上述更新规则是不正确的。背后的原因是，在小批量梯度下降和 SGD 中计算的梯度不是真正的梯度，因为我们没有使用整个训练数据来计算它们，因此在每个小批量中计算的梯度存在随机变化，并且可能指向冲突的方向。</p><p id="3c15" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了解决这个问题，应该将小批量或 SGD 中计算的梯度视为具有某种概率分布的随机变量，因此矩(m 和 v)应该等于该概率分布的期望值。</p><p id="fece" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要保持的条件是𝔼[m] = 𝔼[∇θ] <br/>但是，<br/>m0 = 0<br/>m1 =β1 * m0+(1—β1)* ∇θ_1=(1—β1)∇θ_1</p><p id="3542" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">m2 =β1 * m1+(1—β1)* ∇θ_2 =<br/>=&gt;β1 {β1 * m0+(1—β1)* ∇θ_1}+(1—β1)* ∇θ_2<br/>=&gt;β1(1—β1)* ∇θ_1+(1—β1)* ∇θ_2</p><p id="4970" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">m3 =β1(1—β1)* ∇θ_1+β1(1—β1)* ∇θ_2+(1—β1)* ∇θ_3</p><p id="fc45" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">一般情况下，<br/> mt = ∑ β1^(t-i) * {( 1 — β1) * ∇θ_i}。。。。。I 从 1 开始到 t<br/>=&gt;mt =(1—β1)*∑β1^(t-i)* ∇θ_i</p><p id="0a7e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">=&gt;𝔼[mt]= 𝔼[(1—β1)*∑β1^(t-i)* ∇θ_i]<br/>=&gt;𝔼[mt]=𝔼[∇θ]{(1—β1)*∑β1^(t-i)}<br/>=&gt;𝔼[mt]=𝔼[∇θ]*(1—β1^t)</p><p id="5bec" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">但是因为我们想让𝔼[m 等于梯度概率分布的期望值(𝔼[∇θ)，我们必须通过除以(1-β1^t).)来修正 m</p><p id="879e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">m' = m / (1 — β1^t) <br/> v' = v / (1 — β2^t)。。。。(相同逻辑适用于 v)</p><p id="7701" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">以下是带有偏差修正的 ADAM 更新规则</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/98c6598807c1dd06184dde49a697a52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*Xbhk-UPfW4yYgeTyK-d9Yg.png"/></div></figure><p id="5c2c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">ADAM 是训练神经网络的当前技术状态，并且被认为是过去四年来训练深度神经网络的首选。虽然其他算法特别是带有一些权重衰减策略的 RMSprop 工作得和 ADAM 一样好。人们可以将 RMSprop 比喻为汽车中的手动变速器，而 ADAM 是自动变速器。<br/> ADAM 广泛用于所有的计算机视觉工作，并在几乎所有流行的卷积神经网络架构上产生了非常好的结果，包括非常深的网络，如 Resent 50。</p><h1 id="4545" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">新发现</h1><p id="ab7a" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">最近有一些研究论文表明，ADAM 在某些情况下不会产生好的结果。另外，在这个领域有一些有趣的新研究。</p><ul class=""><li id="2888" class="nd ne iq kb b kc kd kg kh kk nf ko ng ks nh kw ni nj nk nl bi translated">循环学习率:<a class="ae jy" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1506.01186</a></li><li id="69d4" class="nd ne iq kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">亚当·W(解耦重量衰减):<a class="ae jy" href="https://arxiv.org/abs/1711.05101" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1711.05101</a></li></ul><p id="b98e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最近在 2019 年，谷歌大脑的研究人员提出了分层自适应矩优化器(LAMB)算法来训练他们的流行语言模型 BERT。<br/>早些时候，用亚当 W 训练伯特需要 3 天，但兰姆只用了 76 分钟。下面是<a class="ae jy" href="https://arxiv.org/pdf/1904.00962.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1904.00962.pdf</a>的论文，供进一步参考。:)</p></div></div>    
</body>
</html>