<html>
<head>
<title>How to Build Your Own PyTorch Neural Network Layer from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从零开始构建自己的 PyTorch 神经网络层</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-your-own-pytorch-neural-network-layer-from-scratch-842144d623f6?source=collection_archive---------7-----------------------#2019-11-01">https://towardsdatascience.com/how-to-build-your-own-pytorch-neural-network-layer-from-scratch-842144d623f6?source=collection_archive---------7-----------------------#2019-11-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b42f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">并了解一些关于权重初始化的知识</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4fc2d2bffd247d756fdd4f0645d164c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oLcN6Vlpa-PrxnRYJGnXDQ.png"/></div></div></figure><p id="4acc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated">这实际上是杰瑞米·霍华德第五课的作业。我已经展示了使用 PyTorch 从零开始构建卷积神经网络是多么容易。今天，让我们试着更深入地研究一下，看看我们是否可以编写自己的<code class="fe mc md me mf b">nn.Linear</code>模块。既然脸书的开发人员已经写好了 PyTorch 模块，为什么还要浪费时间写自己的呢？</p><p id="8e02" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">嗯，首先，你会对所有的部分是如何组合在一起的有更深的理解。通过将您的代码与 PyTorch 代码进行比较，您将了解为什么以及如何开发这些库。</p><p id="6fad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，一旦你完成了，你将对实现和使用所有这些库更有信心，知道事情是如何工作的。对你来说不会有神话。</p><p id="3f0b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后但同样重要的是，如果情况需要，您将能够修改/调整这些模块。这就是 noob 和 pro 的区别。</p><p id="17c9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好了，动机够了，让我们开始吧。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h2 id="4293" class="mn mo it bd mp mq mr dn ms mt mu dp mv ld mw mx my lh mz na nb ll nc nd ne nf bi translated">简单的 MNIST 一层 NN 作为背景</h2><p id="81fd" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld ni lf lg lh nj lj lk ll nk ln lo lp im bi lq translated">首先，我们需要一些“背景”代码来测试我们的模块是否执行以及执行得有多好。让我们建立一个非常简单的单层神经网络来求解古老的 MNIST 数据集。下面的代码片段(在 Jupyter 笔记本中运行):</p><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="7069" class="mn mo it mf b gy np nq l nr ns"># We'll use fast.ai to showcase how to build your own 'nn.Linear' module<br/>%matplotlib inline<br/>from fastai.basics import *<br/>import sys<br/><br/># create and download/prepare our MNIST dataset<br/>path = Config().data_path()/'mnist'<br/>path.mkdir(parents=True)<br/>!wget http://deeplearning.net/data/mnist/mnist.pkl.gz -P {path}<br/>  <br/># Get the images downloaded into data set<br/>with gzip.open(path/'mnist.pkl.gz', 'rb') as f:<br/>    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')<br/><br/># Have a look at the images and shape<br/>plt.imshow(x_train[0].reshape((28,28)), cmap="gray")<br/>x_train.shape<br/><br/># convert numpy into PyTorch tensor<br/>x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))<br/>n,c = x_train.shape<br/>x_train.shape, y_train.min(), y_train.max()<br/><br/># prepare dataset and create fast.ai DataBunch for training<br/>bs=64<br/>train_ds = TensorDataset(x_train, y_train)<br/>valid_ds = TensorDataset(x_valid, y_valid)<br/>data = DataBunch.create(train_ds, valid_ds, bs=bs)<br/><br/># create a simple MNIST logistic model with only one Linear layer<br/>class Mnist_Logistic(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.lin = nn.Linear(784, 10, bias=True)<br/><br/>    def forward(self, xb): return self.lin(xb)<br/><br/>model =Mnist_Logistic()<br/><br/>lr=2e-2<br/>loss_func = nn.CrossEntropyLoss()<br/><br/># define update function with weight decay<br/>def update(x,y,lr):<br/>    wd = 1e-5<br/>    y_hat = model(x)<br/>    # weight decay<br/>    w2 = 0.<br/>    for p in model.parameters(): w2 += (p**2).sum()<br/>    # add to regular loss<br/>    loss = loss_func(y_hat, y) + w2*wd<br/>    loss.requres_grad = True<br/>   <br/>    loss.backward()<br/>    with torch.no_grad():<br/>        for p in model.parameters():<br/>            p.sub_(lr * p.grad)<br/>            p.grad.zero_()<br/>    return loss.item()<br/><br/># iterate through one epoch and plot losses<br/>losses = [update(x,y,lr) for x,y in data.train_dl]<br/>plt.plot(losses);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/87118bc3c56d37b557aa66ccdec73a38.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*p2sKZABskEsunz6WcZdf7A.png"/></div></figure><p id="484b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些代码很容易理解。我们在这个项目中使用了<a class="ae mb" href="https://github.com/fastai" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>库。下载 MNIST pickle 文件并解压缩，将其转换为 PyTorch 张量，然后将其填充到 fast.ai DataBunch 对象中，以便进一步训练。然后我们创建了一个只有一个<code class="fe mc md me mf b">Linear</code>层的简单神经网络。我们还编写了自己的<code class="fe mc md me mf b">update</code>函数，而不是使用<code class="fe mc md me mf b">torch.optim</code>优化器，因为我们可以从头开始编写自己的优化器，作为 PyTorch 学习之旅的下一步。最后，我们遍历数据集并绘制损失图，以查看它是否有效以及效果如何。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h2 id="fb23" class="mn mo it bd mp mq mr dn ms mt mu dp mv ld mw mx my lh mz na nb ll nc nd ne nf bi translated">第一次迭代:让它工作</h2><p id="0d93" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld ni lf lg lh nj lj lk ll nk ln lo lp im bi translated">所有 PyTorch 模块/层都是从<code class="fe mc md me mf b">torch.nn.Module</code>扩展而来。</p><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="d6e0" class="mn mo it mf b gy np nq l nr ns">class myLinear(nn.Module):</span></pre><p id="554e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这个类中，我们需要一个<code class="fe mc md me mf b">__init__</code> dunder 函数来初始化我们的线性层，并需要一个<code class="fe mc md me mf b">forward</code>函数来进行正向计算。让我们先来看看<code class="fe mc md me mf b">__init__</code>函数。</p><p id="088b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将使用 PyTorch 官方文档作为构建模块的指南。从文档中可以看出，<code class="fe mc md me mf b">nn.Linear</code>模块具有以下属性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://pytorch.org/docs/stable/nn.html#linear"><div class="gh gi nu"><img src="../Images/b4ec4e8cd4a40df76e806dffba33d0a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eDRvelSa-3eugiKZ2X_fdw.png"/></div></a></figure><p id="89fa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，我们将获得这三个属性:</p><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="0192" class="mn mo it mf b gy np nq l nr ns">def __init__(self, <strong class="mf iu">in_features, out_features, bias=True</strong>):<br/>        super().__init__()<br/>       <strong class="mf iu"> self.in_features = in_features<br/>        self.out_features = out_features<br/>        self.bias = bias</strong></span></pre><p id="4a98" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该类还需要保存重量和偏差参数，以便进行训练。我们也初始化那些。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/8f25b7ddfc61ec19951380c54192835f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bxuSixoCvOkpt9HijP_4Mg.png"/></div></div></figure><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="2269" class="mn mo it mf b gy np nq l nr ns">       <strong class="mf iu"> self.weight</strong> = torch.nn.Parameter(torch.randn(out_features, in_features))<br/>       <strong class="mf iu"> self.bias</strong> = torch.nn.Parameter(torch.randn(out_features))</span></pre><p id="e563" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里我们用<code class="fe mc md me mf b">torch.nn.Parameter</code>来设置我们的<code class="fe mc md me mf b">weight</code>和<code class="fe mc md me mf b">bias</code>，否则，它不会训练。</p><p id="d431" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">另外，请注意，我们使用了<code class="fe mc md me mf b"><a class="ae mb" href="https://pytorch.org/docs/stable/torch.html#torch.randn" rel="noopener ugc nofollow" target="_blank">torch.rand</a>n</code>而不是文档中描述的来初始化参数。这不是权重初始化的最佳方式，但我们的目的是让它先工作，我们将在下一次迭代中调整它。</p><p id="10d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好了，现在<code class="fe mc md me mf b">__init__</code>部分完成了，让我们继续<code class="fe mc md me mf b">forward</code>功能。这实际上是最简单的部分:</p><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="3889" class="mn mo it mf b gy np nq l nr ns">def forward(self, input):<br/>        _, y = input.shape<br/>        if y != self.in_features:<br/>            sys.exit(f'Wrong Input Features. Please use tensor with {self.in_features} Input Features')<br/>        <strong class="mf iu">output = input @ self.weight.t() + self.bias<br/>        return output</strong></span></pre><p id="371a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们首先获得输入的形状，计算出输入中有多少列，然后检查输入大小是否匹配。然后我们做矩阵乘法(注意我们在这里做了转置来调整权重)并返回结果。我们可以通过给它一些数据来测试它是否有效:</p><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="2707" class="mn mo it mf b gy np nq l nr ns">my = myLinear(20,10)<br/>a = torch.randn(5,20)<br/>my(a)</span></pre><p id="172f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们有一个 5x20 的输入，它通过我们的层，得到一个 5x10 的输出。您应该会得到这样的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/43f920d9c13ef78c6b8d4960099481ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uz_Hb4rul6pYs0bMIcTEBQ.png"/></div></div></figure><p id="a691" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好，现在回到我们的神经网络代码，找到<code class="fe mc md me mf b">Mnist_Logistic</code>类，将<code class="fe mc md me mf b">self.lin = nn.Linear(784,10, bias=True)</code>改为<code class="fe mc md me mf b">self.lin = myLinear(784, 10, bias=True)</code>。运行代码，您应该会看到类似这样的图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/3f8165786fd3813b281e1c5eb84c436b.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*IdvjAdDwEwhgLw0hRi2zwg.png"/></div></figure><p id="527b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如你所见，它没有很好地收敛(一个时期大约 2.5 次损失)。那很可能是因为我们初始化不好。另外，我们没有注意到<code class="fe mc md me mf b">bias</code>部分。让我们在下一次迭代中解决这个问题。第一次迭代的最终代码如下所示:</p><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="c7bb" class="mn mo it mf b gy np nq l nr ns">class myLinear(nn.Module):<br/>    def __init__(self, in_features, out_features, bias=True):<br/>        super().__init__()<br/>        self.in_features = in_features<br/>        self.out_features = out_features<br/>        self.bias = bias<br/>        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))<br/>        self.bias = torch.nn.Parameter(torch.randn(out_features))<br/>       <br/>        <br/>    def forward(self, input):<br/>        x, y = input.shape<br/>        if y != self.in_features:<br/>            sys.exit(f'Wrong Input Features. Please use tensor with {self.in_features} Input Features')<br/>        output = input @ self.weight.t() + self.bias<br/>        return output</span></pre></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h2 id="c1a8" class="mn mo it bd mp mq mr dn ms mt mu dp mv ld mw mx my lh mz na nb ll nc nd ne nf bi translated">第二次迭代:正确的权重初始化和偏差处理</h2><p id="38c6" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld ni lf lg lh nj lj lk ll nk ln lo lp im bi translated">我们已经处理了<code class="fe mc md me mf b">__init__</code>和<code class="fe mc md me mf b">forward</code>，但是记住我们还有一个<code class="fe mc md me mf b">bias</code>属性，如果<code class="fe mc md me mf b">False</code>，将不会学习加法偏差。我们还没有实施。此外，我们使用<code class="fe mc md me mf b">torch.nn.randn</code>来初始化权重和偏差，这不是最佳的。让我们解决这个问题。更新后的<code class="fe mc md me mf b">__init__</code>函数如下:</p><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="ae3c" class="mn mo it mf b gy np nq l nr ns">def __init__(self, in_features, out_features, bias=True):<br/>        super().__init__()<br/>        self.in_features = in_features<br/>        self.out_features = out_features<br/>        self.bias = bias<br/>        <strong class="mf iu">self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))<br/>        if bias:<br/>            self.bias = torch.nn.Parameter(torch.Tensor(out_features))<br/>        else:<br/>            self.register_parameter('bias', None)</strong></span><span id="fd2a" class="mn mo it mf b gy nx nq l nr ns"><strong class="mf iu">        self.reset_parameters()</strong></span></pre><p id="c225" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，当我们创建<code class="fe mc md me mf b">weight</code>和<code class="fe mc md me mf b">bias</code>参数时，我们没有将它们初始化为最后一次迭代。我们只是给它分配一个规则的张量对象。实际的初始化在另一个函数<code class="fe mc md me mf b">reset_parameters</code>中完成(<em class="ny">将在后面解释</em>)。</p><p id="10bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于<code class="fe mc md me mf b">bias</code>，我们增加了一个条件，如果<code class="fe mc md me mf b">True</code>，做我们上一次迭代做的事情，但是如果<code class="fe mc md me mf b">False</code>，将使用<code class="fe mc md me mf b">register_parameter(‘bias’, None)</code>给它<code class="fe mc md me mf b">None</code>值。现在对于<code class="fe mc md me mf b">reset_parameter</code>功能，它看起来是这样的:</p><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="7e7e" class="mn mo it mf b gy np nq l nr ns">def reset_parameters(self):<br/>        <strong class="mf iu">torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))</strong><br/>        if self.bias is not None:<br/>            <strong class="mf iu">fan_in, _ torch.nn.init._calculate_fan_in_and_fan_out(self.weight)<br/>            bound = 1 / math.sqrt(fan_in)<br/>            torch.nn.init.uniform_(self.bias, -bound, bound)</strong></span></pre><p id="800c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以上代码直接取自 PyTorch 源代码。PyTorch 对权重初始化所做的称为<code class="fe mc md me mf b">kaiming_uniform_</code>。它来自一篇论文<a class="ae mb" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank">深入研究整流器:在 ImageNet 分类上超越人类水平的性能——何，k .等人(2015) </a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/6886e67fa7fd0d400f96a6562a6376eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*cq0NDktwlKPtWQ2OrhL67Q.png"/></div></figure><p id="7448" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它实际上所做的是通过用均值为 0 且方差为 T16 的正态分布<strong class="kw iu">初始化权重，它避免了<strong class="kw iu">消失/爆炸梯度</strong>的问题(<em class="ny">尽管我们在这里只有一层，当编写线性类时，我们仍然应该记住 MLN</em>)。</strong></p><p id="b30f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请注意，对于<code class="fe mc md me mf b">self.weight</code>，我们实际上给了<code class="fe mc md me mf b">a</code>一个值<code class="fe mc md me mf b">math.sqrt(5)</code>而不是<code class="fe mc md me mf b">math.sqrt(fan_in)</code>，这在 PyTorch repo 的<a class="ae mb" href="https://github.com/pytorch/pytorch/issues/15314" rel="noopener ugc nofollow" target="_blank"> this GitHub issue </a>中有所解释，可能有人对此感兴趣。</p><p id="c908" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样，我们可以在模型中添加一些<code class="fe mc md me mf b">extra_repr</code>字符串:</p><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="a325" class="mn mo it mf b gy np nq l nr ns">def extra_repr(self):<br/>        return 'in_features={}, out_features={}, bias={}'.format(<br/>            self.in_features, self.out_features, self.bias is not None<br/>        )</span></pre><p id="70ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最终的模型如下所示:</p><pre class="kj kk kl km gt nl mf nm nn aw no bi"><span id="6890" class="mn mo it mf b gy np nq l nr ns">class myLinear(nn.Module):<br/>    def __init__(self, in_features, out_features, bias=True):<br/>        super().__init__()<br/>        self.in_features = in_features<br/>        self.out_features = out_features<br/>        self.bias = bias<br/>        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))<br/>        if bias:<br/>            self.bias = torch.nn.Parameter(torch.Tensor(out_features))<br/>        else:<br/>            self.register_parameter('bias', None)<br/>        self.reset_parameters()<br/>        <br/>    def reset_parameters(self):<br/>        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))<br/>        if self.bias is not None:<br/>            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)<br/>            bound = 1 / math.sqrt(fan_in)<br/>            torch.nn.init.uniform_(self.bias, -bound, bound)<br/>        <br/>    def forward(self, input):<br/>        x, y = input.shape<br/>        if y != self.in_features:<br/>            print(f'Wrong Input Features. Please use tensor with {self.in_features} Input Features')<br/>            return 0<br/>        output = input.matmul(weight.t())<br/>        if bias is not None:<br/>            output += bias<br/>        ret = output<br/>        return ret<br/>    <br/>    def extra_repr(self):<br/>        return 'in_features={}, out_features={}, bias={}'.format(<br/>            self.in_features, self.out_features, self.bias is not None<br/>        )</span></pre><p id="05e2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">重新运行代码，您应该能够看到这个图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d515b224aa298e7ae0654cdba9dcbad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*6nUlBO7nIt9t2E0xrfgP-w.png"/></div></figure><p id="7683" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到，它在一个时期内收敛到 0.5 的损耗要快得多。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h2 id="dd5b" class="mn mo it bd mp mq mr dn ms mt mu dp mv ld mw mx my lh mz na nb ll nc nd ne nf bi translated">结论</h2><p id="8159" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld ni lf lg lh nj lj lk ll nk ln lo lp im bi translated">我希望这能帮你驱散这些 PyTorch <code class="fe mc md me mf b">nn.modules</code>上的阴霾。这可能看起来很无聊和多余，但有时最快(也是最短)的方法就是“无聊”的方法。一旦你深究此事，那种知道没有什么“更多”的感觉是无价的。你会意识到:</p><blockquote class="oa"><p id="55b6" class="ob oc it bd od oe of og oh oi oj lp dk translated">在 PyTorch 下面，没有技巧，没有神话，没有陷阱，只有坚如磐石的 Python 代码。</p></blockquote><p id="45ff" class="pw-post-body-paragraph ku kv it kw b kx ok ju kz la ol jx lc ld om lf lg lh on lj lk ll oo ln lo lp im bi translated">此外，通过编写自己的代码，然后与官方源代码进行比较，您将能够看到不同之处，并向行业中的佼佼者学习。多酷啊。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="7ac5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">觉得这篇文章有用？在 Medium 上关注我(<a class="ae mb" href="https://medium.com/u/72c98619a048?source=post_page-----dbe7106145f5----------------------" rel="noopener">李立伟</a>)或者你可以在 Twitter <a class="ae mb" href="https://twitter.com/lymenlee" rel="noopener ugc nofollow" target="_blank"> @lymenlee </a>或者我的博客网站<a class="ae mb" href="https://wayofnumbers.com/" rel="noopener ugc nofollow" target="_blank">wayofnumbers.com</a>上找到我。你也可以看看我下面最受欢迎的文章！</p><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/this-is-cs50-a-pleasant-way-to-kick-off-your-data-science-education-d6075a6e761a"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">“这是 CS50”:开始数据科学教育的愉快方式</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">为什么 CS50 特别适合巩固你的软件工程基础</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ks os"/></div></div></a></div><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/two-sides-of-the-same-coin-fast-ai-vs-deeplearning-ai-b67e9ec32133"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">一枚硬币的两面:杰瑞米·霍华德的 fast.ai vs 吴恩达的 deeplearning.ai</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">如何不通过同时参加 fast.ai 和 deeplearning.ai 课程来“过度适应”你的人工智能学习</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="ph l pd pe pf pb pg ks os"/></div></div></a></div><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/what-you-need-to-know-about-netflixs-jupyter-killer-polynote-dbe7106145f5"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">你需要了解网飞的“朱庇特黑仔”:冰穴📖</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">是时候让 Jupyter 笔记本有个有价值的竞争对手了</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pi l pd pe pf pb pg ks os"/></div></div></a></div></div></div>    
</body>
</html>