<html>
<head>
<title>BERT to the rescue!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特来救援了。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bert-to-the-rescue-17671379687f?source=collection_archive---------3-----------------------#2019-06-05">https://towardsdatascience.com/bert-to-the-rescue-17671379687f?source=collection_archive---------3-----------------------#2019-06-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/c25625f81c99bdd134ad0933b59c6e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UKUKRDNC_rK0woSeLOqKng.jpeg"/></div></div></figure><p id="ca5c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我想展示如何将 BERT 应用于一个简单的文本分类问题。我假设您或多或少地熟悉 BERT 在高层次上是什么，并通过向您展示如何在您的工作中利用它来更加关注实践方面。粗略来说，BERT 是一个知道表示文本的模型。你给它一些序列作为输入，然后它左看右看几次，产生每个单词的向量表示作为输出。在他们的论文中，作者描述了两种使用 BERT 的方法，一种是“特征提取”机制。也就是说，我们使用 BERT 的最终输出作为另一个模型的输入。通过这种方式，我们使用 BERT 从文本中“提取”特征，然后在一个单独的模型中用于手头的实际任务。另一种方法是“微调”伯特。也就是说，我们在 BERT 上添加额外的层，然后一起训练整个东西。通过这种方式，我们可以训练我们的附加层，还可以改变(微调)层的权重。在这里，我想展示第二种方法，并介绍一个非常简单和流行的文本分类任务的分步解决方案——IMDB 电影评论情感分类。这个任务可能不是最难解决的任务，将 BERT 应用于它可能有点矫枉过正，但是这里显示的大多数步骤对于几乎每个任务都是相同的，不管它有多复杂。</p><p id="de50" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在深入实际代码之前，让我们了解一下 BERT 的一般结构，以及在分类任务中使用它需要做些什么。如前所述，一般来说，BERT 的输入是一个单词序列，输出是一个向量序列。BERT 允许我们根据它的输出执行不同的任务。因此，对于不同的任务类型，我们需要稍微改变输入和/或输出。在下图中，您可以看到 4 种不同的任务类型，对于每种任务类型，我们可以看到模型的输入和输出应该是什么。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi kx"><img src="../Images/a4ceb781053bc88dffd905b358702832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eDC9QUkMHJAp-NJg.JPEG"/></div></div></figure><p id="c190" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可以看到，对于输入，在每个序列的开头总是有一个特殊的<code class="fe lc ld le lf b">[CLS]</code>标记(代表分类),还有一个特殊的<code class="fe lc ld le lf b">[SEP]</code>标记将输入分成两部分。</p><p id="f2a5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于输出，如果我们对分类感兴趣，我们需要使用第一个令牌(<code class="fe lc ld le lf b">[CLS]</code>令牌)的输出。对于更复杂的输出，我们可以使用所有其他的令牌输出。</p><p id="bb4c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们对“单句分类”感兴趣(右上)，所以我们将添加特殊的<code class="fe lc ld le lf b">[CLS]</code>标记，并将其输出作为线性层的输入，然后激活<code class="fe lc ld le lf b">sigmoid</code>，执行实际的分类。</p><p id="7bdd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在让我们来理解手头的任务:给定一个电影评论，预测它是正面的还是负面的。我们使用的数据集是 PyTorch-NLP 库中的 50，000 条 IMDB 评论(25，000 条用于训练，25，000 条用于测试)。每个评论都被标记为<code class="fe lc ld le lf b">pos</code>或<code class="fe lc ld le lf b">neg</code>。在训练集和测试集中，正面评价和负面评价各占 50%。</p><p id="90b7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以在这个笔记本里找到所有的代码。</p><h1 id="42a4" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">1.准备数据</h1><p id="6bc6" class="pw-post-body-paragraph jy jz iq ka b kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv ij bi translated">我们使用<code class="fe lc ld le lf b">pytorch-nlp</code>库加载数据:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="ef5e" class="mn lh iq lf b gy mo mp l mq mr">train_data, test_data = imdb_dataset(train=True, test=True)</span></pre><p id="01c6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个数据集中的每个实例都是一个字典，包含两个字段:<code class="fe lc ld le lf b">text</code>和<code class="fe lc ld le lf b">sentimet</code></p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="8777" class="mn lh iq lf b gy mo mp l mq mr">{<br/>    'sentiment': 'pos',  <br/>    'text': 'Having enjoyed Joyces complex nove...'<br/>}</span></pre><p id="e0b3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们为每个集合创建两个变量，一个用于文本，一个用于标签:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="ddc0" class="mn lh iq lf b gy mo mp l mq mr">train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))</span><span id="b016" class="mn lh iq lf b gy ms mp l mq mr">test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))</span></pre><p id="683c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们需要标记我们的文本。伯特是用词块符号化来训练的。这意味着一个单词可以分解成多个子单词。例如，如果我对句子“嗨，我的名字是马頔”进行分词，我会得到:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="e4d8" class="mn lh iq lf b gy mo mp l mq mr">tokenizer.tokenize('Hi my name is Dima')</span><span id="a577" class="mn lh iq lf b gy ms mp l mq mr"># OUTPUT<br/>['hi', 'my', 'name', 'is', 'dim', '##a']</span></pre><p id="9695" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种标记化在处理词汇之外的单词时是有益的，并且它可以帮助更好地表示复杂的单词。子词是在训练期间构建的，并且依赖于模型被训练的语料库。当然，我们可以使用任何其他的记号化技术，但是如果我们使用训练 BERT 模型的相同记号化器进行记号化，我们会得到最好的结果。PyTorch-Pretrained-BERT 库为我们提供了每个 BERTS 模型的标记器。这里我们使用基本的<code class="fe lc ld le lf b">bert-base-uncased</code>型号，还有其他几种型号，包括更大的型号。BERT 的最大序列长度是 512，因此我们将截断任何长于此长度的评论。</p><p id="92fa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面的代码创建标记器，标记每个评论，添加特殊的<code class="fe lc ld le lf b">[CLS]</code>标记，然后只获取训练集和测试集的前 512 个标记:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="9d6a" class="mn lh iq lf b gy mo mp l mq mr">tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)</span><span id="ad16" class="mn lh iq lf b gy ms mp l mq mr">train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))</span><span id="b332" class="mn lh iq lf b gy ms mp l mq mr">test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))</span></pre><p id="faf9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们需要将每个评论中的每个标记转换成标记化器词汇表中的<code class="fe lc ld le lf b">id</code>。如果有一个标记不在词汇表中，标记器将使用特殊的<code class="fe lc ld le lf b">[UNK]</code>标记并使用它的 id:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="4900" class="mn lh iq lf b gy mo mp l mq mr">train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))</span><span id="83cd" class="mn lh iq lf b gy ms mp l mq mr">test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens_ids))</span></pre><p id="979a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我们需要填充我们的输入，这样它将具有相同的大小 512。这意味着对于任何少于 512 个令牌的评论，我们将添加零以达到 512 个令牌:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="2efb" class="mn lh iq lf b gy mo mp l mq mr">train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating="post", padding="post", dtype="int")</span><span id="d02a" class="mn lh iq lf b gy ms mp l mq mr">test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating="post", padding="post", dtype="int")</span></pre><p id="8865" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的目标变量目前是一个由<code class="fe lc ld le lf b">neg</code>和<code class="fe lc ld le lf b">pos</code>字符串组成的列表。我们将把它转换成布尔的<code class="fe lc ld le lf b">numpy</code>数组:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="7b7c" class="mn lh iq lf b gy mo mp l mq mr">train_y = np.array(train_labels) == 'pos'<br/>test_y = np.array(test_labels) == 'pos'</span></pre><h1 id="9e25" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">2.模型结构</h1><p id="e055" class="pw-post-body-paragraph jy jz iq ka b kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv ij bi translated">我们将使用 PyTorch 和优秀的<a class="ae kw" href="https://github.com/huggingface/pytorch-pretrained-BERT" rel="noopener ugc nofollow" target="_blank">py torch-pre trained-BERT</a>库来构建模型。实际上，这个库中已经实现了一个非常相似的模型，我们可以使用这个模型。对于这篇文章，我想自己实现它，这样我们可以更好地了解正在发生的事情。</p><p id="49fe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们创建模型之前，让我们看看如何使用在 PyTorch-Pretrained-BERT 库中实现的 BERT 模型:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="9226" class="mn lh iq lf b gy mo mp l mq mr">bert = BertModel.from_pretrained('bert-base-uncased')</span><span id="c7bf" class="mn lh iq lf b gy ms mp l mq mr">x = torch.tensor(train_tokens_ids[:3])<br/>y, pooled = bert(x, output_all_encoded_layers=False)</span><span id="af76" class="mn lh iq lf b gy ms mp l mq mr">print('x shape:', x.shape)<br/>print('y shape:', y.shape)<br/>print('pooled shape:', pooled.shape)</span><span id="dfbe" class="mn lh iq lf b gy ms mp l mq mr"># OUTPUT<br/>x shape :(3, 512)<br/>y shape: (3, 512, 768)<br/>pooled shape: (3, 768)</span></pre><p id="485a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们创建 BERT 模型，然后我们创建 PyTorch 张量，其中包含来自我们训练集的前 3 条评论，并将其传递给它。输出是两个变量。让我们来理解所有的形状:<code class="fe lc ld le lf b">x</code>的大小是<code class="fe lc ld le lf b">(3, 512)</code>，我们只取了 3 个评论，每个评论有 512 个标记。<code class="fe lc ld le lf b">y</code>的大小为<code class="fe lc ld le lf b">(3, 512, 768)</code>，这是每个令牌的 BERTs 最终层输出。我们可以使用<code class="fe lc ld le lf b">output_all_encoded_layer=True</code>来获得所有 12 层的输出。使用大小为 768 的向量来表示每个评论中的每个标记。<code class="fe lc ld le lf b">pooled</code>的大小是<code class="fe lc ld le lf b">(3, 768)</code>这是我们的<code class="fe lc ld le lf b">[CLS]</code>令牌的输出，我们序列中的第一个令牌。</p><p id="f924" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的目标是采用 BERTs 池输出，应用线性层和<code class="fe lc ld le lf b">sigmoid</code>激活。我们的模型看起来是这样的:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="3250" class="mn lh iq lf b gy mo mp l mq mr">class BertBinaryClassifier(nn.Module):<br/>    def __init__(self, dropout=0.1):<br/>        super(BertBinaryClassifier, self).__init__()</span><span id="a453" class="mn lh iq lf b gy ms mp l mq mr">        self.bert = BertModel.from_pretrained('bert-base-uncased')<br/>        self.linear = nn.Linear(768, 1)<br/>        self.sigmoid = nn.Sigmoid()<br/>    <br/>    def forward(self, tokens):<br/>        _, pooled_output = self.bert(tokens, <!-- -->utput_all=<!-- -->False)<br/>        linear_output = self.linear(dropout_output)<br/>        proba = self.sigmoid(linear_output)<br/>        return proba</span></pre><p id="cdc0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">PyTorch 中的每个模型都是一个<code class="fe lc ld le lf b">nn.Module</code>对象。这意味着我们构建的每个模型都必须提供 2 个方法。<code class="fe lc ld le lf b">__init__ </code>方法声明了模型将使用的所有不同部分。在我们的例子中，我们创建了将要微调的 BERT 模型、线性层和 Sigmoid 激活。<code class="fe lc ld le lf b">forward</code>方法是正向传递期间运行的实际代码(类似于<code class="fe lc ld le lf b">sklearn</code>或<code class="fe lc ld le lf b">keras</code>中的<code class="fe lc ld le lf b">predict</code>方法)。这里我们将<code class="fe lc ld le lf b">tokens</code>输入传递给 BERT 模型。BERT 的输出是 2 个变量，正如我们之前看到的，我们只使用第二个变量(使用<code class="fe lc ld le lf b">_</code>名称是为了强调没有使用这个变量)。我们将汇集的输出传递给线性层。最后，我们使用 Sigmoid 激活来提供实际概率。</p><h1 id="554a" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">3.培训/微调</h1><p id="ba3e" class="pw-post-body-paragraph jy jz iq ka b kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv ij bi translated">训练相当标准。首先，我们准备好张量和数据加载器:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="92b1" class="mn lh iq lf b gy mo mp l mq mr">train_tokens_tensor = torch.tensor(train_tokens_ids)<br/>train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()</span><span id="2101" class="mn lh iq lf b gy ms mp l mq mr">test_tokens_tensor = torch.tensor(test_tokens_ids)<br/>test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()</span><span id="5966" class="mn lh iq lf b gy ms mp l mq mr">train_dataset = TensorDataset(train_tokens_tensor, train_y_tensor)<br/>train_sampler = RandomSampler(train_dataset)<br/>train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)</span><span id="023b" class="mn lh iq lf b gy ms mp l mq mr">test_dataset = TensorDataset(test_tokens_tensor, test_y_tensor)<br/>test_sampler = SequentialSampler(test_dataset)<br/>test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)</span></pre><p id="f0f5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将使用<code class="fe lc ld le lf b">Adam</code>优化器和二元交叉熵(<code class="fe lc ld le lf b">BCELoss</code>)损失，并训练 10 个时期的模型:</p><pre class="ky kz la lb gt mj lf mk ml aw mm bi"><span id="aa79" class="mn lh iq lf b gy mo mp l mq mr">bert_clf = BertBinaryClassifier()<br/>bert_clf = bert_clf.cuda()<br/>optimizer = Adam(bert_clf.parameters(), lr=3e-6)<br/>bert_clf.train()</span><span id="4dfa" class="mn lh iq lf b gy ms mp l mq mr">for epoch_num in range(EPOCHS):<br/>    for step_num, batch_data in enumerate(train_dataloader):<br/>        token_ids, labels = tuple(t.to(device) for t in batch_data)<br/>        probas = bert_clf(token_ids)<br/>        loss_func = nn.BCELoss()<br/>        batch_loss = loss_func(probas, labels)<br/>        bert_clf.zero_grad()<br/>        batch_loss.backward()<br/>        optimizer.step()</span></pre><p id="8054" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于那些不熟悉 PyTorch 的人，让我们一步一步地看代码。</p><p id="ec69" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们创建上面定义的<code class="fe lc ld le lf b">BertBinaryClassifier</code>。我们通过应用<code class="fe lc ld le lf b">bert_clf.cuda()</code>将其移动到 GPU。我们用我们的模型参数(优化器将会更新)和一个学习率创建了 Adam 优化器，我发现效果很好。</p><p id="106d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于每个时期中的每个步骤，我们执行以下操作:</p><ol class=""><li id="5607" class="mt mu iq ka b kb kc kf kg kj mv kn mw kr mx kv my mz na nb bi translated">通过应用<code class="fe lc ld le lf b">.to(device)</code>将我们的张量移动到 GPU</li><li id="3bb5" class="mt mu iq ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated"><code class="fe lc ld le lf b">bert_clf(token_ids)</code>给出了概率(向前传递)</li><li id="75e1" class="mt mu iq ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated">用<code class="fe lc ld le lf b">loss_func(probas, labels)</code>计算损失</li><li id="62ee" class="mt mu iq ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated">将上一步的梯度归零</li><li id="e2df" class="mt mu iq ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated">通过<code class="fe lc ld le lf b">batch_loss.backward()</code>计算并传播新的梯度</li><li id="d80c" class="mt mu iq ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated">通过<code class="fe lc ld le lf b">optimizer.step()</code>更新与梯度相关的模型参数</li></ol><p id="9e58" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">经过 10 个时代，我得到了相当不错的结果。</p><p id="aa90" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">结论</strong></p><p id="8c69" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">BERT 是一个非常强大的模型，可以应用于许多任务。对我来说，它为我的工作任务提供了一些非常好的结果。我希望这篇文章能帮助你更好地理解与 BERT 一起工作的实际方面。如前所述，您可以在本<a class="ae kw" href="https://github.com/shudima/notebooks/blob/master/BERT_to_the_rescue.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>中找到代码</p></div></div>    
</body>
</html>