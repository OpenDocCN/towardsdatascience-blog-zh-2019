<html>
<head>
<title>Predicting Hotel Reservation Cancellations with Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用机器学习预测酒店预订取消</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-hotel-cancellations-with-machine-learning-fa669f93e794?source=collection_archive---------8-----------------------#2019-12-30">https://towardsdatascience.com/predicting-hotel-cancellations-with-machine-learning-fa669f93e794?source=collection_archive---------8-----------------------#2019-12-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f41025de3866e610c6393d3558c7057e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4-CTWbkwkhVNGTqcheivQ.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Image by <a class="ae kf" href="https://pixabay.com/users/Alexas_Fotos-686414/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=897419" rel="noopener ugc nofollow" target="_blank">Alexas_Fotos</a> from <a class="ae kf" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=897419" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="eb78" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可想而知，在线预订行业的预订取消率相当高。一旦预订被取消，几乎没有什么可做的。这让许多机构感到不安，并产生了采取预防措施的愿望。因此，预测可以取消的预订并防止这些取消将为机构创造剩余价值。</p><p id="93a7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我将尝试解释如何通过机器学习方法提前预测未来取消的预订。先说预处理！</p><h1 id="2e11" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">预处理</h1><p id="d359" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">首先，我应该说，您可以访问我的存储库中使用的数据，我将在我的文章结尾分享这些数据。我还想分享一下，这是一篇论文的主题。[1]</p><p id="ebe4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有两个独立的数据集，因为我们要对它们进行预处理，所以将它们组合起来是有意义的。但是在建模阶段，我们需要分别获得这两组数据。所以，为了区分这两者，我创建了<code class="fe mh mi mj mk b">id</code>字段。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="6ed8" class="mt lf it mk b gy mu mv l mw mx">import pandas as pd</span><span id="ead6" class="mt lf it mk b gy my mv l mw mx">h1 = pd.read_csv('data/H1.csv')<br/>h2 = pd.read_csv('data/H2.csv')</span><span id="0621" class="mt lf it mk b gy my mv l mw mx">h1.loc[:, 'id'] = range(1, len(h1) + 1)<br/><br/>start = h1['id'].max() + 1<br/>stop = start + len(h2)<br/>h2.loc[:, 'id'] = range(start, stop)</span><span id="a032" class="mt lf it mk b gy my mv l mw mx">df = pd.concat([h1, h2], ignore_index=True, sort=False)</span></pre><p id="fbaf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是该项目的预处理步骤:</p><ul class=""><li id="195d" class="mz na it ki b kj kk kn ko kr nb kv nc kz nd ld ne nf ng nh bi translated">将字符串<code class="fe mh mi mj mk b">NULL</code>或<code class="fe mh mi mj mk b">Undefined </code>值转换为 np.nan</li><li id="81ab" class="mz na it ki b kj ni kn nj kr nk kv nl kz nm ld ne nf ng nh bi translated">从具有少量<code class="fe mh mi mj mk b">NULL</code>值的列中删除缺失的观察值</li><li id="a1b9" class="mz na it ki b kj ni kn nj kr nk kv nl kz nm ld ne nf ng nh bi translated">根据规则填充缺失值</li><li id="df0a" class="mz na it ki b kj ni kn nj kr nk kv nl kz nm ld ne nf ng nh bi translated">删除不正确的值</li><li id="2283" class="mz na it ki b kj ni kn nj kr nk kv nl kz nm ld ne nf ng nh bi translated">离群点检测</li></ul><h2 id="7a2b" class="mt lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated"><strong class="ak">步骤 1 —将</strong> <code class="fe mh mi mj mk b">NULL</code> <strong class="ak">或未定义的值串到</strong> <code class="fe mh mi mj mk b"><strong class="ak">np.nan</strong></code></h2><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="6351" class="mt lf it mk b gy mu mv l mw mx">import numpy as np</span><span id="72c1" class="mt lf it mk b gy my mv l mw mx">for col in df.columns:<br/>    if df[col].dtype == 'object' and col != 'country':<br/>        df.loc[df[col].str.contains('NULL'), col] = np.nan<br/>        df.loc[df[col].str.contains('Undefined', na=False), col] = np.nan</span><span id="58eb" class="mt lf it mk b gy my mv l mw mx">null_series = df.isnull().sum()<br/>print(null_series[null_series &gt; 0])</span></pre><p id="5c7b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用上面的代码，我们将字符串<code class="fe mh mi mj mk b">NULL</code>和<code class="fe mh mi mj mk b">Undefined</code>值转换为<code class="fe mh mi mj mk b">np.nan</code>值。然后，我们打印每一列的<code class="fe mh mi mj mk b">NULL</code>值的计数。这是结果的样子，</p><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/0fa9c2836b0042d580fee6bea984cc31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*Zi9pUeyt2m2eLwi762uWIg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Null values</figcaption></figure><h2 id="5386" class="mt lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">步骤 2-删除一些缺失的值</h2><p id="b4f4" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们可以删除<em class="nz">国家</em>、<em class="nz">子</em>、<em class="nz">市场 _ 细分</em>、<em class="nz">分销 _ 渠道</em>中的<code class="fe mh mi mj mk b">NULL</code>值，因为这些字段的<code class="fe mh mi mj mk b">NULL</code>值很少。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="c23b" class="mt lf it mk b gy mu mv l mw mx">subset = [<br/>    'country',      <br/>    'children',      <br/>    'market_segment',      <br/>    'distribution_channel'<br/>] <br/>df = df.dropna(subset=subset)</span></pre><h2 id="0a44" class="mt lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">步骤 3-通过规则集填充缺失值</h2><p id="4a8b" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">为数据指定了许多规则。[2]例如，值为<code class="fe mh mi mj mk b">Undefined/SC</code>意味着它们为<code class="fe mh mi mj mk b">no meal type.</code>，因为我们之前已经用<code class="fe mh mi mj mk b">NULL</code>替换了<code class="fe mh mi mj mk b">Undefined</code>值，我们可以用<code class="fe mh mi mj mk b">SC</code>填充<em class="nz">餐</em>字段中的<code class="fe mh mi mj mk b">NULL</code>值。</p><p id="9abf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">a<em class="nz">agent</em>字段为<code class="fe mh mi mj mk b">NULL</code>的事实意味着预订不是来自任何代理。因此，这些预订可以被认为是由顾客直接购买的，不需要任何中介组织，如代理等。这就是为什么我们没有删除<code class="fe mh mi mj mk b">NULL</code>值，而是抛出一个像 999 这样的随机值。这同样适用于 c<em class="nz">company</em>字段。</p><p id="08b2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更详细的信息可以在参考资料的第二个链接中找到。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="d5ba" class="mt lf it mk b gy mu mv l mw mx">df.loc[df.agent.isnull(), 'agent'] = 999 <br/>df.loc[df.company.isnull(), 'company'] = 999 df.loc[df.meal.isnull(), 'meal'] = 'SC'</span></pre><h2 id="2fbe" class="mt lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">步骤 4-删除错误的值</h2><p id="134d" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><em class="nz"> ADR </em>字段指的是预订的每晚平均价格。因此，它取小于零的值是不正常的。你可以使用<code class="fe mh mi mj mk b">df.describe().T</code>来查看这种情况。对于<em class="nz"> ADR </em>字段，我们删除小于零的值。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="dabf" class="mt lf it mk b gy mu mv l mw mx">df = df[df.adr &gt; 0]</span></pre><h2 id="b8cd" class="mt lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">步骤 5—异常值检测</h2><p id="1f10" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">对于<code class="fe mh mi mj mk b">integer</code>和<code class="fe mh mi mj mk b">float</code>字段，我们使用下面的代码来确定低点和高点。如果在低点和高点之间有等式，我们不做任何滤波。如果不相等，我们从数据集中移除大于上点的观测值和小于下点的观测值。</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Outlier detection with IQR</figcaption></figure><p id="cf17" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">田地的低点和高点似乎在下面，</p><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/99a2d0f20d120457e3951343f83dc5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0y3yDewa8w1hcKe2j4gC8A.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">IQR results</figcaption></figure><p id="6320" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们将讨论多元异常值检测。[3]这是我们多做一点工作的特殊推论，并不适用于每一个企业。一晚 5 美元或 10 美元的费用可以正常支付，但 10 晚的费用就不正常了。因此，从数据集中删除这些被认为是相反的值将有助于我们的模型进行学习。所以我已经尝试了<code class="fe mh mi mj mk b">LocalOutlierFactor</code>和<code class="fe mh mi mj mk b">EllipticEnvelope</code>，我只检查<code class="fe mh mi mj mk b">EllipticEnvelope</code>是因为它产生了更好的结果，但是如果你想检查这两个，你可以看看我的资源库。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="1fc4" class="mt lf it mk b gy mu mv l mw mx">from sklearn.covariance import EllipticEnvelope<br/>import matplotlib.pyplot as plt<br/>import numpy as np</span><span id="9da2" class="mt lf it mk b gy my mv l mw mx"># create new features: total price and total nights<br/>cleaned.loc[:, 'total_nights'] = \<br/>cleaned['stays_in_week_nights'] + cleaned['stays_in_weekend_nights']<br/>cleaned.loc[:, 'price'] = cleaned['adr'] * cleaned['total_nights']</span><span id="2a57" class="mt lf it mk b gy my mv l mw mx"># create numpy array<br/>X = np.array(cleaned[['total_nights', 'price']])</span><span id="6e02" class="mt lf it mk b gy my mv l mw mx"><em class="nz"># create model</em> <br/>ee = EllipticEnvelope(contamination=.01, random_state=0)</span><span id="20ae" class="mt lf it mk b gy my mv l mw mx"><em class="nz"># predictions</em> <br/>y_pred_ee = ee.fit_predict(X)</span><span id="253e" class="mt lf it mk b gy my mv l mw mx"><em class="nz"># predictions (-1: outlier, 1: normal)</em><br/>anomalies = X[y_pred_ee == -1]</span><span id="660a" class="mt lf it mk b gy my mv l mw mx"><em class="nz"># plot data and outliers</em><br/>plt.figure(figsize=(15, 8))<br/>plt.scatter(X[:, 0], X[:, 1], c='white', s=20, edgecolor='k')<br/>plt.scatter(anomalies[:, 0], anomalies[:, 1], c='red');</span></pre><p id="0cf4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图表如下。红点显示异常值。</p><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/c11376f70a01b334123fe89e5f0b0f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G8ItXD1j6hAEDeMIDHY7MQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">EllipticEnvelope result</figcaption></figure><p id="ef22" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如您所见，在数据集之外保留小值是有意义的，尤其是在 6 个晚上之后。通过应用这个过程，我们可以保存数据集。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="5571" class="mt lf it mk b gy mu mv l mw mx">df_cleaned = cleaned[y_pred_ee != -1].copy()</span><span id="3b70" class="mt lf it mk b gy my mv l mw mx">h1_cleaned = df_cleaned[df_cleaned.id.isin(h1.id.tolist())]<br/>h2_cleaned = df_cleaned[df_cleaned.id.isin(h2.id.tolist())]</span><span id="003a" class="mt lf it mk b gy my mv l mw mx">h1_cleaned = h1_cleaned.drop('id', axis=1)<br/>h2_cleaned = h2_cleaned.drop('id', axis=1)</span><span id="7a3b" class="mt lf it mk b gy my mv l mw mx">h1_cleaned.to_csv('data/H1_cleaned.csv', index=False)<br/>h2_cleaned.to_csv('data/H2_cleaned.csv', index=False)</span></pre><h1 id="fe9a" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">特征工程</h1><p id="af8f" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在建立模型之前，另一个重要的问题是特征工程。添加或删除功能可能对我们的模型更有效。</p><h2 id="7f1b" class="mt lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">步骤 1—相关性</h2><p id="7a34" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">首先，我将使用<code class="fe mh mi mj mk b">LabelEncoder</code>将分类数据转换为<code class="fe mh mi mj mk b">integer</code>，然后我将查看相关性。[4]下面的代码可以做到这一点，</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="f86d" class="mt lf it mk b gy mu mv l mw mx">from sklearn.preprocessing import LabelEncoder<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import seaborn as sns</span><span id="dbc6" class="mt lf it mk b gy my mv l mw mx">train = pd.read_csv('./data/H1_cleaned.csv')<br/>test = pd.read_csv('./data/H2_cleaned.csv')</span><span id="c0b7" class="mt lf it mk b gy my mv l mw mx">df_le = train.copy()<br/>le = LabelEncoder()<br/><br/>categoricals = [<br/>    'arrival_date_month',<br/>    'meal',<br/>    'country',<br/>    'market_segment',<br/>    'distribution_channel',<br/>    'reserved_room_type',<br/>    'assigned_room_type',<br/>    'deposit_type',<br/>    'agent',<br/>    'company',<br/>    'customer_type',<br/>    'reservation_status',<br/>]<br/><br/>for col in categoricals:<br/>    df_le[col] = le.fit_transform(df_le[col])</span><span id="8e41" class="mt lf it mk b gy my mv l mw mx">plt.figure(figsize=(20, 15))<br/>sns.heatmap(df_le.corr(), annot=True, fmt='.2f');</span></pre><p id="a895" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这段代码给了我们一个如下所示的相关矩阵，</p><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/d0157fcad5ab5832eb15628191c0c9c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NN-O6W3azD3D_w3Wc75RJQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Correlation matrix</figcaption></figure><p id="6fd6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在该矩阵中，在<em class="nz"> reservation_status </em>和<em class="nz">is _ cancelled</em>特征之间似乎存在负的高相关性。在<em class="nz"> total_nights </em>和<em class="nz"> stays_in_week_nights </em>和<em class="nz">stays _ in _ weekend _ nights</em>字段之间也有很高的相关性。因此，我们从数据集中删除了<em class="nz"> reservation_status </em>和<em class="nz"> total_nights </em>特性。由于<em class="nz">reservation _ status _ date</em>和<em class="nz"> reservation_status </em>之间存在关联，我们将删除此功能。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="3b30" class="mt lf it mk b gy mu mv l mw mx">columns = [<br/>    'reservation_status_date',<br/>    'total_nights',<br/>    'reservation_status',<br/>]<br/><br/>train = train.drop(columns, axis=1)<br/>test = test.drop(columns, axis=1)<br/>df_le = df_le.drop(columns, axis=1)</span></pre><h2 id="de60" class="mt lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">步骤 2—虚拟变量与标签编码器</h2><p id="e92d" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">机器学习模型需要数字数据来运行。因此，在我们可以建模之前，我们需要将分类变量转换为数字变量。我们可以使用两种方法来做到这一点:<code class="fe mh mi mj mk b">Dummy variables</code>和<code class="fe mh mi mj mk b">LabelEncoder</code>。通过下面你看到的代码，我们使用<code class="fe mh mi mj mk b">LabelEncoder</code>和<code class="fe mh mi mj mk b">dummy variables</code>创建特征。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="d0d7" class="mt lf it mk b gy mu mv l mw mx">import pandas as pd</span><span id="4d0a" class="mt lf it mk b gy my mv l mw mx">new_categoricals = [col for col in categoricals if col in train.columns]</span><span id="74f4" class="mt lf it mk b gy my mv l mw mx">df_hot = pd.get_dummies(data=train, columns=new_categoricals)<br/>test_hot = pd.get_dummies(data=test, columns=new_categoricals)</span><span id="d43b" class="mt lf it mk b gy my mv l mw mx">X_hot = df_hot.drop('is_canceled', axis=1)<br/>X_le = df_le.drop('is_canceled', axis=1)<br/>y = train['is_canceled']</span></pre><p id="18c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们用<code class="fe mh mi mj mk b">dummy variables</code>构建一个<code class="fe mh mi mj mk b">logistic regression</code>模型，并检查分类报告，作为对数据的初步观察。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="f12f" class="mt lf it mk b gy mu mv l mw mx">from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score, classification_report<br/>from sklearn.model_selection import train_test_split</span><span id="4337" class="mt lf it mk b gy my mv l mw mx">X_train, X_test, y_train, y_test = train_test_split(X_hot, y, test_size=.2, random_state=42)<br/><br/>log = LogisticRegression().fit(X_train, y_train)<br/>y_pred = log.predict(X_test)</span><span id="4868" class="mt lf it mk b gy my mv l mw mx">print(accuracy_score(y_test, y_pred))<br/>print(classification_report(y_test, y_pred))</span></pre><p id="6201" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">准确性分数看起来是 0.8584，但是当查看分类报告时，已经取消的预订的准确性非常低。因为我们的数据包含 23720 个成功案例和 8697 个取消案例。在这种情况下，优选的是稀释加权类或增加较少采样类的样本数量。我们将首先使用特征选择算法选择特征，然后使用稀释的数据比较虚拟变量和标签编码器。</p><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/5a1c6c1c2f0f0df0f46308deff84a10c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nIYT7rFYFq4p2lX0poi_pA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">First classification report</figcaption></figure><h2 id="a034" class="mt lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">步骤 3—特征选择</h2><p id="5b04" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">特征选择是特征工程中最重要的问题之一。这里我们将使用<code class="fe mh mi mj mk b">SelectKBest</code>，这是一种用于分类问题的流行特征选择算法。我们的评分函数将是<em class="nz"> chi </em>。[5]</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Feature selection</figcaption></figure><p id="0006" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用上述函数，我们为<code class="fe mh mi mj mk b">LabelEncoder</code>和<code class="fe mh mi mj mk b">dummy variables</code>选择最佳特征。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="c838" class="mt lf it mk b gy mu mv l mw mx">selects_hot = select(X_hot)<br/>selects_le = select(X_le)</span></pre><p id="ffeb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们用一种简单的方式来比较这些特征。</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Dummy variables vs label encoder</figcaption></figure><p id="82f0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">比较结果如下:</p><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/e803c38f374fbc8c0e14fda4d7b3ef74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvuREnmidn5IULHPTJeoEg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Dummy variables vs label encoder classification reports</figcaption></figure><p id="5138" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们选择这些字段是因为我们用虚拟变量创建的特征能给出更好的结果。</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="f52a" class="mt lf it mk b gy mu mv l mw mx">from sklearn.model_selection import train_test_split<br/>from sklearn.utils import resample<br/>import pandas as pd</span><span id="1e38" class="mt lf it mk b gy my mv l mw mx">last = test_hot[selects_hot + ['is_canceled']]<br/><br/>X_last = last.drop('is_canceled', axis=1)<br/>y_last = last['is_canceled']</span><span id="057d" class="mt lf it mk b gy my mv l mw mx"><em class="nz"># separate majority and minority classes</em><br/>major = selected[selected['is_canceled'] == 0]<br/>minor = selected[selected['is_canceled'] == 1]<br/><br/><em class="nz"># downsample majority class</em><br/>downsampled = resample(major, replace=False, n_samples=len(minor), random_state=123) <br/><br/><em class="nz"># combine minority class with downsampled majority class</em><br/>df_new = pd.concat([downsampled, minor])<br/><br/><em class="nz"># display new class counts</em><br/>print(df_new['is_canceled'].value_counts())</span><span id="3580" class="mt lf it mk b gy my mv l mw mx">X = df_new.drop('is_canceled', axis=1)<br/>y = df_new['is_canceled']</span><span id="ae26" class="mt lf it mk b gy my mv l mw mx">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)</span></pre><p id="dfbd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用上面的代码，我们将成功预订的数量和取消预订的数量平均化了 8697，并将数据集分为 train 和 test。然后，我们将通过创建以下类来度量模型的性能。</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Report class</figcaption></figure><p id="0461" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们到最后一步，对比一下我们的车型！</p><h1 id="3bd3" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">系统模型化</h1><p id="fc4f" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这里尝试了很多模型，你可以在我的知识库里看到。但在这里，我将分享前 2 个模型的结果和一些代码，展示我们如何做<code class="fe mh mi mj mk b">hyperparameter tuning</code>。事情是这样的，</p><pre class="ml mm mn mo gt mp mk mq mr aw ms bi"><span id="5c6f" class="mt lf it mk b gy mu mv l mw mx">from sklearn.model_selection import GridSearchCV<br/>from xgboost import XGBClassifier</span><span id="6f57" class="mt lf it mk b gy my mv l mw mx">report = Report(X_test, y_test)<br/>xgb = XGBClassifier().fit(X_train, y_train)</span><span id="bd49" class="mt lf it mk b gy my mv l mw mx">xgb_params = {<br/>    'n_estimators': [100, 500, 1000],     <br/>    'max_depth': [3, 5, 10],     <br/>    'min_samples_split': [2, 5, 10]<br/>}</span><span id="481d" class="mt lf it mk b gy my mv l mw mx">params = {<br/>    'estimator': xgb,<br/>    'param_grid': xgb_params,<br/>    'cv': 5,<br/>    'refit': False,<br/>    'n_jobs': -1,<br/>    'verbose': 2,<br/>    'scoring': 'recall',<br/>}</span><span id="ace6" class="mt lf it mk b gy my mv l mw mx">xgb_cv = GridSearchCV(**params)<br/>_ = xgb_cv.fit(X_train, y_train)</span><span id="e813" class="mt lf it mk b gy my mv l mw mx">print(xgb_cv.best_params_)<br/>xgb = XGBClassifier(**xgb_cv.best_params_).fit(X_train, y_train)</span><span id="1da7" class="mt lf it mk b gy my mv l mw mx">report.metrics(xgb)<br/>report.plot_roc_curve(xgb, save=True)</span></pre><p id="27d1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">XGBoost 结果如下:</p><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/fa1306952cd6299a2050c0dffdcae7d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rdpmRTZdZiWxtI62jJDIjA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">XGB results</figcaption></figure><p id="f39a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们使用上面的代码将 XGBoost 替换为 GBM，结果如下:</p><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/ebacdf76b4ad74d6e00b2d9261b57ba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yE8jFIQfGxUNJnTL5qAcjA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">GBM results</figcaption></figure><h1 id="c42d" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结论</h1><p id="97fc" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">首先，我想在本文中强调预处理和特征选择步骤在模型构建过程中的重要性。创建成功模型的方法是获得干净的数据。</p><p id="dde0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之后建立的模型的优化，尤其是分类问题不应该忽视召回值的重要性。分类的准确性是分类问题中最关键的问题之一。</p><p id="0cbb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望这是一篇有用的文章！</p></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><p id="a9d9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读！如果你想了解更多，想看看 H2 文件的结果，请访问我的知识库！</p><div class="oq or gp gr os ot"><a href="https://github.com/egemenzeytinci/cancellation-prediction" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd iu gy z fp oy fr fs oz fu fw is bi translated">egemenzeytinci/取消-预测</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">这个项目的目的是预测将被取消的预订。它包括两个步骤:预处理和…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">github.com</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph jz ot"/></div></div></a></div><h1 id="e66e" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">参考</h1><p id="d64b" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">[1]努诺·安东尼奥、安娜·德·阿尔梅达和路易斯·努涅斯，<a class="ae kf" href="https://pdfs.semanticscholar.org/0f5f/3a506360b9be0a7ab52d77974695f1c48a4d.pdf" rel="noopener ugc nofollow" target="_blank">预测酒店预订取消，以减少不确定性并增加收入</a> (2017)</p><p id="e3f6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]努诺·安东尼奥，安娜·德·阿尔梅达和路易斯·努内斯，<a class="ae kf" href="https://www.sciencedirect.com/science/article/pii/S2352340918315191" rel="noopener ugc nofollow" target="_blank">酒店预订需求数据集</a> (2019)</p><p id="c9ae" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]克里斯托弗·何塞，<a class="ae kf" href="https://medium.com/learningdatascience/anomaly-detection-techniques-in-python-50f650c75aaf" rel="noopener">Python 中的异常检测技术</a> (2019)</p><p id="663d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4] Vishal R，<a class="ae kf" rel="noopener" target="_blank" href="/feature-selection-correlation-and-p-value-da8921bfb3cf">特征选择—相关性和 P 值</a> (2018)</p><p id="d0b9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[5] <a class="ae kf" href="https://www.kaggle.com/jepsds/feature-selection-using-selectkbest/notebook" rel="noopener ugc nofollow" target="_blank">使用选择测试</a>进行特征选择(2018)</p></div></div>    
</body>
</html>