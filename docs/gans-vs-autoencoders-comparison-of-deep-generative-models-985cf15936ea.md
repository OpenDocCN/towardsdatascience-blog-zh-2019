# GANs ä¸è‡ªåŠ¨ç¼–ç å™¨:æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„æ¯”è¾ƒ

> åŸæ–‡ï¼š<https://towardsdatascience.com/gans-vs-autoencoders-comparison-of-deep-generative-models-985cf15936ea?source=collection_archive---------0----------------------->

## æƒ³æŠŠé©¬å˜æˆæ–‘é©¬ï¼Ÿåˆ¶ä½œ DIY åŠ¨æ¼«äººç‰©æˆ–åäººï¼Ÿç”Ÿæˆæ•Œå¯¹ç½‘ç»œæ˜¯ä½ æ–°çš„æœ€å¥½çš„æœ‹å‹ã€‚

ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ˜¯è¿‡å» 10 å¹´æœºå™¨å­¦ä¹ ä¸­æœ€æœ‰è¶£çš„æƒ³æ³•â€”â€”**æ‰¬Â·å‹’æ‘ï¼Œè„¸ä¹¦äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­å¿ƒä¸»ä»»**

æœ¬æ•™ç¨‹çš„ç¬¬ 1 éƒ¨åˆ†å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°:

[](/comprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d) [## å›¾çµå­¦ä¹ å’Œ GANs ç®€ä»‹

### æƒ³æŠŠé©¬å˜æˆæ–‘é©¬ï¼Ÿåˆ¶ä½œ DIY åŠ¨æ¼«äººç‰©æˆ–åäººï¼Ÿç”Ÿæˆæ•Œå¯¹ç½‘ç»œæ˜¯â€¦

towardsdatascience.com](/comprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d) 

æœ¬æ•™ç¨‹çš„ç¬¬ 2 éƒ¨åˆ†å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°:

[](/comprehensive-introduction-to-turing-learning-and-gans-part-2-fd8e4a70775) [## GANs ä¸­çš„é«˜çº§ä¸»é¢˜

### æƒ³æŠŠé©¬å˜æˆæ–‘é©¬ï¼Ÿåˆ¶ä½œ DIY åŠ¨æ¼«äººç‰©æˆ–åäººï¼Ÿç”Ÿæˆæ•Œå¯¹ç½‘ç»œæ˜¯â€¦

towardsdatascience.com](/comprehensive-introduction-to-turing-learning-and-gans-part-2-fd8e4a70775) 

è¿™äº›æ–‡ç« åŸºäºå“ˆä½›å¤§å­¦å…³äº [AC209b](https://harvard-iacs.github.io/2019-CS109B/) çš„è®²åº§ï¼Œä¸»è¦å½’åŠŸäºå“ˆä½›å¤§å­¦ IACS ç³»çš„è®²å¸ˆ [Pavlos Protopapas](https://iacs.seas.harvard.edu/people/pavlos-protopapas) ã€‚

è¿™æ˜¯ä¸“é—¨ä½¿ç”¨ç”Ÿæˆæ€§å¯¹æŠ—ç½‘ç»œåˆ›å»ºæ·±åº¦ç”Ÿæˆæ¨¡å‹çš„ä¸‰éƒ¨åˆ†æ•™ç¨‹çš„ç¬¬ä¸‰éƒ¨åˆ†ã€‚è¿™æ˜¯ä¸Šä¸€ä¸ªå…³äºå˜å‹è‡ªåŠ¨ç¼–ç å™¨ä¸»é¢˜çš„è‡ªç„¶å»¶ä¼¸(åœ¨è¿™é‡Œæ‰¾åˆ°)ã€‚æˆ‘ä»¬å°†çœ‹åˆ°ï¼Œä¸å¯å˜è‡ªåŠ¨ç¼–ç å™¨ç›¸æ¯”ï¼ŒGANs ä½œä¸ºæ·±åº¦ç”Ÿæˆæ¨¡å‹é€šå¸¸æ›´ä¼˜è¶Šã€‚ç„¶è€Œï¼Œä¼—æ‰€å‘¨çŸ¥ï¼Œå®ƒä»¬å¾ˆéš¾ä½¿ç”¨ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„æ•°æ®å’Œè°ƒæ•´ã€‚æˆ‘ä»¬è¿˜å°†ç ”ç©¶ä¸€ç§ç§°ä¸º VAE-GAN çš„æ··åˆ GAN æ¨¡å‹ã€‚

![](img/d56e801da7bc4f16a949aee0e9631ebd.png)

Taxonomy of deep generative models. This articleâ€™s focus is on GANs.

æ•™ç¨‹çš„è¿™ä¸€éƒ¨åˆ†å°†ä¸»è¦æ˜¯å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨(VAEs)çš„ç¼–ç å®ç°ï¼Œä¹Ÿå°†å‘è¯»è€…å±•ç¤ºå¦‚ä½•åˆ¶ä½œ VAEs ç”˜ã€‚

*   CelebA æ•°æ®é›†çš„ VAE
*   è¥¿é‡Œå·´æ•°æ®é›†çš„ DC-ç”˜
*   åŠ¨æ¼«æ•°æ®é›† DC-ç”˜
*   åŠ¨æ¼«æ•°æ®é›† VAE-ç”˜

æˆ‘å¼ºçƒˆå»ºè®®è¯»è€…åœ¨è¿›ä¸€æ­¥é˜…è¯»ä¹‹å‰ï¼Œè‡³å°‘å…ˆé˜…è¯»ä¸€ä¸‹ GAN æ•™ç¨‹çš„ç¬¬ 1 éƒ¨åˆ†ï¼Œä»¥åŠæˆ‘çš„è‡ªåŠ¨ç¼–ç å™¨å˜åŒ–æ¼”ç»ƒï¼Œå¦åˆ™ï¼Œè¯»è€…å¯èƒ½å¯¹å®ç°æ²¡æœ‰å¤ªå¤šçš„äº†è§£ã€‚

æ‰€æœ‰ç›¸å…³ä»£ç ç°åœ¨éƒ½å¯ä»¥åœ¨æˆ‘çš„ GitHub å­˜å‚¨åº“ä¸­æ‰¾åˆ°:

[](https://github.com/mrdragonbear/GAN-Tutorial) [## é¾™ç†Šå…ˆç”Ÿ/ç”˜-æ•™ç¨‹

### GitHub æ˜¯è¶…è¿‡ 5000 ä¸‡å¼€å‘äººå‘˜çš„å®¶å›­ï¼Œä»–ä»¬ä¸€èµ·å·¥ä½œæ¥æ‰˜ç®¡å’Œå®¡æŸ¥ä»£ç ã€ç®¡ç†é¡¹ç›®å’Œæ„å»ºâ€¦

github.com](https://github.com/mrdragonbear/GAN-Tutorial) 

æˆ‘ä»¬å¼€å§‹å§ï¼

# **CelebA æ•°æ®é›†çš„ VAE**

CelebFaces å±æ€§æ•°æ®é›†(CelebA)æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„äººè„¸å±æ€§æ•°æ®é›†ï¼Œæ‹¥æœ‰è¶…è¿‡ 20 ä¸‡å¼ åäººå›¾åƒï¼Œæ¯å¼ å›¾åƒéƒ½æœ‰ 40 ä¸ªå±æ€§æ³¨é‡Šã€‚è¯¥æ•°æ®é›†ä¸­çš„å›¾åƒè¦†ç›–äº†è¾ƒå¤§çš„å§¿æ€å˜åŒ–å’ŒèƒŒæ™¯æ··ä¹±ã€‚è¥¿é‡Œå·´æœ‰å¾ˆå¤§çš„å¤šæ ·æ€§ï¼Œæ•°é‡å¤§ï¼Œæ³¨é‡Šä¸°å¯Œï¼ŒåŒ…æ‹¬

*   10177 ä¸ªèº«ä»½ï¼Œ
*   202ï¼Œ599 ä¸ªé¢éƒ¨å›¾åƒï¼Œä»¥åŠ
*   5 ä¸ªæ ‡å¿—ä½ç½®ï¼Œæ¯å¹…å›¾åƒ 40 ä¸ªäºŒå…ƒå±æ€§æ³¨é‡Šã€‚

ä½ å¯ä»¥ä» Kaggle è¿™é‡Œä¸‹è½½æ•°æ®é›†:

[](https://www.kaggle.com/jessicali9530/celeba-dataset) [## åäººé¢å­”å±æ€§(CelebA)æ•°æ®é›†

### è¶…è¿‡ 200ï¼Œ000 å¼ åäººå›¾ç‰‡ï¼Œå¸¦æœ‰ 40 ä¸ªäºŒå…ƒå±æ€§æ³¨é‡Š

www.kaggle.com](https://www.kaggle.com/jessicali9530/celeba-dataset) 

ç¬¬ä¸€æ­¥æ˜¯å¯¼å…¥æ‰€æœ‰å¿…è¦çš„å‡½æ•°å¹¶æå–æ•°æ®ã€‚

**è¿›å£**

```
import shutil
import errno
import zipfile
import os
import matplotlib.pyplot as plt
```

**æå–æ•°æ®**

```
# Only run once to unzip images
zip_ref = zipfile.ZipFile('img_align_celeba.zip','r')
zip_ref.extractall()
zip_ref.close()
```

**è‡ªå®šä¹‰å›¾åƒç”Ÿæˆå™¨**

è¿™ä¸€æ­¥å¯èƒ½æ˜¯å¤§å¤šæ•°è¯»è€…ä»¥å‰æ²¡æœ‰ç”¨è¿‡çš„ã€‚ç”±äºæˆ‘ä»¬çš„æ•°æ®éå¸¸åºå¤§ï¼Œå¯èƒ½æ— æ³•å°†æ•°æ®é›†åŠ è½½åˆ° Jupyter ç¬”è®°æœ¬çš„å†…å­˜ä¸­ã€‚åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆæ­£å¸¸çš„é—®é¢˜ã€‚

è§£å†³è¿™ä¸€é—®é¢˜çš„æ–¹æ³•æ˜¯ä½¿ç”¨æµç”Ÿæˆå™¨ï¼Œå®ƒå°†æˆæ‰¹çš„æ•°æ®(æœ¬ä¾‹ä¸­æ˜¯å›¾åƒ)æŒ‰é¡ºåºæµå…¥å†…å­˜ï¼Œä»è€Œé™åˆ¶è¯¥å‡½æ•°æ‰€éœ€çš„å†…å­˜é‡ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç†è§£å’Œç¼–å†™å®ƒä»¬æœ‰ç‚¹å¤æ‚ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¯¹è®¡ç®—æœºå†…å­˜ã€GPU æ¶æ„ç­‰æœ‰åˆç†çš„ç†è§£ã€‚

```
# data generator
# source from https://medium.com/@ensembledme/writing-custom-keras-generators-fe815d992c5a
from skimage.io import imread

def get_input(path):
    """get specific image from path"""
    img = imread(path)
    return img

def get_output(path, label_file = None):
    """get all the labels relative to the image of path"""
    img_id = path.split('/')[-1]
    labels = label_file.loc[img_id].values
    return labels

def preprocess_input(img):
    # convert between 0 and 1
    return img.astype('float32') / 127.5 -1

def image_generator(files, label_file, batch_size = 32):
    while True:

        batch_paths = np.random.choice(a = files, size = batch_size)
        batch_input = []
        batch_output = []

        for input_path in batch_paths:

            input = get_input(input_path)
            input = preprocess_input(input)
            output = get_output(input_path, label_file = label_file)
            batch_input += [input]
            batch_output += [output]
        batch_x = np.array(batch_input)
        batch_y = np.array(batch_output)

        yield batch_x, batch_y

def auto_encoder_generator(files, batch_size = 32):
    while True:
        batch_paths = np.random.choice(a = files, size = batch_size)
        batch_input = []
        batch_output = []

        for input_path in batch_paths:
            input = get_input(input_path)
            input = preprocess_input(input)
            output = input
            batch_input += [input]
            batch_output += [output]
        batch_x = np.array(batch_input)
        batch_y = np.array(batch_output)

        yield batch_x, batch_y
```

å…³äºç”¨ Keras ç¼–å†™å®šåˆ¶ç”Ÿæˆå™¨çš„æ›´å¤šä¿¡æ¯ï¼Œæˆ‘åœ¨ä¸Šé¢çš„ä»£ç ä¸­å¼•ç”¨äº†ä¸€ç¯‡å¾ˆå¥½çš„æ–‡ç« :

 [## ç¼–å†™å®šåˆ¶çš„ Keras ç”Ÿæˆå™¨

### ä½¿ç”¨ Keras ç”Ÿæˆå™¨èƒŒåçš„æƒ³æ³•æ˜¯åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€åœ°è·å¾—æˆæ‰¹çš„è¾“å…¥å’Œç›¸åº”çš„è¾“å‡ºâ€¦

medium.com](https://medium.com/@ensembledme/writing-custom-keras-generators-fe815d992c5a) 

**åŠ è½½å±æ€§æ•°æ®**

æˆ‘ä»¬ä¸ä»…æœ‰è¿™ä¸ªæ•°æ®é›†çš„å›¾åƒï¼Œè€Œä¸”æ¯ä¸ªå›¾åƒè¿˜æœ‰ä¸€ä¸ªä¸åäººçš„å„ä¸ªæ–¹é¢ç›¸å¯¹åº”çš„å±æ€§åˆ—è¡¨ã€‚ä¾‹å¦‚ï¼Œæœ‰æè¿°åäººæ˜¯å¦æ¶‚å£çº¢æˆ–æˆ´å¸½å­ã€ä»–ä»¬æ˜¯å¦å¹´è½»ã€ä»–ä»¬æ˜¯å¦æœ‰é»‘å¤´å‘ç­‰çš„å±æ€§ã€‚

```
# now load attribute

# 1.A.2
import pandas as pd
attr = pd.read_csv('list_attr_celeba.csv')
attr = attr.set_index('image_id')

# check if attribute successful loaded
attr.describe()
```

**å®Œæˆå‘ç”µæœºçš„åˆ¶ä½œ**

ç°åœ¨æˆ‘ä»¬å®Œæˆäº†å‘ç”µæœºçš„åˆ¶ä½œã€‚æˆ‘ä»¬å°†å›¾åƒåç§°é•¿åº¦è®¾ç½®ä¸º 6ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ä¸­æœ‰ 6 ä½æ•°çš„å›¾åƒã€‚é˜…è¯»å®šåˆ¶ Keras ç”Ÿæˆå™¨æ–‡ç« åï¼Œè¿™éƒ¨åˆ†ä»£ç åº”è¯¥æœ‰æ„ä¹‰ã€‚

```
import numpy as np
from sklearn.model_selection import train_test_splitIMG_NAME_LENGTH = 6file_path = "img_align_celeba/"
img_id = np.arange(1,len(attr.index)+1)
img_path = []
for i in range(len(img_id)):
    img_path.append(file_path + (IMG_NAME_LENGTH - len(str(img_id[i])))*'0' + str(img_id[i]) + '.jpg')# pick 80% as training set and 20% as validation set
train_path = img_path[:int((0.8)*len(img_path))]
val_path = img_path[int((0.8)*len(img_path)):]train_generator = auto_encoder_generator(train_path,32)
val_generator = auto_encoder_generator(val_path,32)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥é€‰æ‹©ä¸‰ä¸ªå›¾åƒï¼Œå¹¶æ£€æŸ¥å±æ€§æ˜¯å¦æœ‰æ„ä¹‰ã€‚

```
fig, ax = plt.subplots(1, 3, figsize=(12, 4))
for i in range(3):    
    ax[i].imshow(get_input(img_path[i]))
    ax[i].axis('off')
    ax[i].set_title(img_path[i][-10:])
plt.show()

attr.iloc[:3]
```

![](img/0ff784df069b971d1ee23e65aff9b6ae.png)![](img/ae2752fa4f82046cf95dd17c2cce0c2f.png)

Three random images along with some of their attributes.

## å»ºç«‹å’Œè®­ç»ƒä¸€ä¸ª VAE æ¨¡å‹

é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¸ºåäººé¢å­”æ•°æ®é›†åˆ›å»ºå¹¶ç¼–è¯‘ä¸€ä¸ªå·ç§¯ VAE æ¨¡å‹(åŒ…æ‹¬ç¼–ç å™¨å’Œè§£ç å™¨)ã€‚

**æ›´å¤šè¿›å£å•†å“**

```
from keras.models import Sequential, Model
from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D, Input, Reshape, UpSampling2D, InputLayer, Lambda, ZeroPadding2D, Cropping2D, Conv2DTranspose, BatchNormalization
from keras.utils import np_utils, to_categorical
from keras.losses import binary_crossentropy
from keras import backend as K,objectives
from keras.losses import mse, binary_crossentropy
```

**æ¨¡å‹æ¶æ„**

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºå¹¶æ€»ç»“è¯¥æ¨¡å‹ã€‚

```
b_size = 128
n_size = 512
def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = K.random_normal(shape = (n_size,) , mean = 0, stddev = 1)
    return z_mean + K.exp(z_log_sigma/2) * epsilon

def build_conv_vae(input_shape, bottleneck_size, sampling, batch_size = 32):

    # ENCODER
    input = Input(shape=(input_shape[0],input_shape[1],input_shape[2]))
    x = Conv2D(32,(3,3),activation = 'relu', padding = 'same')(input)    
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2), padding ='same')(x)
    x = Conv2D(64,(3,3),activation = 'relu', padding = 'same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2), padding ='same')(x)
    x = Conv2D(128,(3,3), activation = 'relu', padding = 'same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2), padding ='same')(x)
    x = Conv2D(256,(3,3), activation = 'relu', padding = 'same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2), padding ='same')(x)

    # Latent Variable Calculation
    shape = K.int_shape(x)
    flatten_1 = Flatten()(x)
    dense_1 = Dense(bottleneck_size, name='z_mean')(flatten_1)
    z_mean = BatchNormalization()(dense_1)
    flatten_2 = Flatten()(x)
    dense_2 = Dense(bottleneck_size, name ='z_log_sigma')(flatten_2)
    z_log_sigma = BatchNormalization()(dense_2)
    z = Lambda(sampling)([z_mean, z_log_sigma])
    encoder = Model(input, [z_mean, z_log_sigma, z], name = 'encoder')

    # DECODER
    latent_input = Input(shape=(bottleneck_size,), name = 'decoder_input')
    x = Dense(shape[1]*shape[2]*shape[3])(latent_input)
    x = Reshape((shape[1],shape[2],shape[3]))(x)
    x = UpSampling2D((2,2))(x)
    x = Cropping2D([[0,0],[0,1]])(x)
    x = Conv2DTranspose(256,(3,3), activation = 'relu', padding = 'same')(x)
    x = BatchNormalization()(x)
    x = UpSampling2D((2,2))(x)
    x = Cropping2D([[0,1],[0,1]])(x)
    x = Conv2DTranspose(128,(3,3), activation = 'relu', padding = 'same')(x)
    x = BatchNormalization()(x)
    x = UpSampling2D((2,2))(x)
    x = Cropping2D([[0,1],[0,1]])(x)
    x = Conv2DTranspose(64,(3,3), activation = 'relu', padding = 'same')(x)
    x = BatchNormalization()(x)
    x = UpSampling2D((2,2))(x)
    x = Conv2DTranspose(32,(3,3), activation = 'relu', padding = 'same')(x)
    x = BatchNormalization()(x)
    output = Conv2DTranspose(3,(3,3), activation = 'tanh', padding ='same')(x)
    decoder = Model(latent_input, output, name = 'decoder')

    output_2 = decoder(encoder(input)[2])
    vae = Model(input, output_2, name ='vae')
    return vae, encoder, decoder, z_mean, z_log_sigma

vae_2, encoder, decoder, z_mean, z_log_sigma = build_conv_vae(img_sample.shape, n_size, sampling, batch_size = b_size)
print("encoder summary:")
encoder.summary()
print("decoder summary:")
decoder.summary()
print("vae summary:")
vae_2.summary()
```

**å®šä¹‰ VAE æŸå¤±**

```
def vae_loss(input_img, output):
    # Compute error in reconstruction
    reconstruction_loss = mse(K.flatten(input_img) , K.flatten(output))

    # Compute the KL Divergence regularization term
    kl_loss = - 0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis = -1)

    # Return the average loss over all images in batch
    total_loss = (reconstruction_loss + 0.0001 * kl_loss)    
    return total_loss
```

**ç¼–è¯‘æ¨¡å‹**

```
vae_2.compile(optimizer='rmsprop', loss= vae_loss)
encoder.compile(optimizer = 'rmsprop', loss = vae_loss)
decoder.compile(optimizer = 'rmsprop', loss = vae_loss)
```

**è®­ç»ƒæ¨¡å‹**

```
vae_2.fit_generator(train_generator, steps_per_epoch = 4000, validation_data = val_generator, epochs=7, validation_steps= 500)
```

æˆ‘ä»¬éšæœºé€‰æ‹©è®­ç»ƒé›†çš„ä¸€äº›å›¾åƒï¼Œé€šè¿‡ç¼–ç å™¨è¿è¡Œå®ƒä»¬ä»¥å‚æ•°åŒ–æ½œåœ¨ä»£ç ï¼Œç„¶åç”¨è§£ç å™¨é‡å»ºå›¾åƒã€‚

```
import random
x_test = []
for i in range(64):
    x_test.append(get_input(img_path[random.randint(0,len(img_id))]))
x_test = np.array(x_test)
figure_Decoded = vae_2.predict(x_test.astype('float32')/127.5 -1, batch_size = b_size)
figure_original = x_test[0]
figure_decoded = (figure_Decoded[0]+1)/2
for i in range(4):
    plt.axis('off')
    plt.subplot(2,4,1+i*2)
    plt.imshow(x_test[i])
    plt.axis('off')
    plt.subplot(2,4,2 + i*2)
    plt.imshow((figure_Decoded[i]+1)/2)
    plt.axis('off')
plt.show()
```

![](img/ca91e0117abe9411a4f86b1c5fc1149d.png)

Random samples from training set compared to their VAE reconstruction.

è¯·æ³¨æ„ï¼Œé‡å»ºçš„å›¾åƒä¸åŸå§‹ç‰ˆæœ¬æœ‰ç›¸ä¼¼ä¹‹å¤„ã€‚ç„¶è€Œï¼Œæ–°çš„å›¾åƒæœ‰ç‚¹æ¨¡ç³Šï¼Œè¿™æ˜¯ä¸€ä¸ªå·²çŸ¥çš„ VAEs ç°è±¡ã€‚è¿™è¢«å‡è®¾æ˜¯ç”±äºå˜åˆ†æ¨ç†ä¼˜åŒ–äº†ä¼¼ç„¶æ€§çš„ä¸‹é™ï¼Œè€Œä¸æ˜¯å®é™…çš„ä¼¼ç„¶æ€§æœ¬èº«ã€‚

**æ½œåœ¨ç©ºé—´è¡¨å¾**

æˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸¤ä¸ªä¸åŒå±æ€§çš„å›¾åƒï¼Œå¹¶ç»˜åˆ¶å®ƒä»¬çš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ½œåœ¨ä»£ç ä¹‹é—´çš„ä¸€äº›å·®å¼‚ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾è§£é‡ŠåŸå§‹å›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚

```
# Choose two images of different attributes, and plot the original and latent space of it

x_test1 = []
for i in range(64):
    x_test1.append(get_input(img_path[np.random.randint(0,len(img_id))]))
x_test1 = np.array(x_test)
x_test_encoded = np.array(encoder.predict(x_test1/127.5-1, batch_size = b_size))
figure_original_1 = x_test[0]
figure_original_2 = x_test[1]
Encoded1 = (x_test_encoded[0,0,:].reshape(32, 16,)+1)/2 
Encoded2 = (x_test_encoded[0,1,:].reshape(32, 16)+1)/2

plt.figure(figsize=(8, 8))
plt.subplot(2,2,1)
plt.imshow(figure_original_1)
plt.subplot(2,2,2)
plt.imshow(Encoded1)
plt.subplot(2,2,3)
plt.imshow(figure_original_2)
plt.subplot(2,2,4)
plt.imshow(Encoded2)
plt.show()
```

![](img/b674b3cf02713a90011374658f302925.png)

**ä»æ½œç©ºé—´å–æ ·**

æˆ‘ä»¬å¯ä»¥éšæœºæŠ½å– 15 ä¸ªæ½œåœ¨ä»£ç ï¼Œè§£ç åç”Ÿæˆæ–°çš„åäººé¢å­”ã€‚æˆ‘ä»¬å¯ä»¥ä»è¿™ä¸ªè¡¨ç¤ºä¸­çœ‹åˆ°ï¼Œç”±æˆ‘ä»¬çš„æ¨¡å‹ç”Ÿæˆçš„å›¾åƒä¸æˆ‘ä»¬è®­ç»ƒé›†ä¸­çš„é‚£äº›å›¾åƒå…·æœ‰éå¸¸ç›¸ä¼¼çš„é£æ ¼ï¼Œå¹¶ä¸”å®ƒä¹Ÿå…·æœ‰è‰¯å¥½çš„çœŸå®æ€§å’Œå˜åŒ–æ€§ã€‚

```
# We randomly generated 15 images from 15 series of noise informationn = 3
m = 5
digit_size1 = 218
digit_size2 = 178
figure = np.zeros((digit_size1 * n, digit_size2 * m,3))

for i in range(3):
    for j in range(5):
        z_sample = np.random.rand(1,512)
        x_decoded = decoder.predict([z_sample])
        figure[i * digit_size1: (i + 1) * digit_size1,
               j * digit_size2: (j + 1) * digit_size2,:] = (x_decoded[0]+1)/2 plt.figure(figsize=(10, 10))
plt.imshow(figure)
plt.show()
```

![](img/5437be16ebd1db85f35a0228ed23b262.png)

æ‰€ä»¥çœ‹èµ·æ¥æˆ‘ä»¬çš„ VAE æ¨¡å¼å¹¶ä¸æ˜¯ç‰¹åˆ«å¥½ã€‚å¦‚æœæœ‰æ›´å¤šçš„æ—¶é—´å’Œæ›´å¥½åœ°é€‰æ‹©è¶…å‚æ•°ç­‰ç­‰ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå–å¾—æ¯”è¿™æ›´å¥½çš„ç»“æœã€‚

ç°åœ¨è®©æˆ‘ä»¬å°†è¿™ä¸ªç»“æœä¸ç›¸åŒæ•°æ®é›†ä¸Šçš„ DC-ç”˜è¿›è¡Œæ¯”è¾ƒã€‚

# **CelebA æ•°æ®é›†ä¸Šçš„ DC-ç”˜**

å› ä¸ºæˆ‘ä»¬å·²ç»è®¾ç½®äº†æµç”Ÿæˆå™¨ï¼Œæ‰€ä»¥æ²¡æœ‰å¤ªå¤šçš„å·¥ä½œè¦åšæ¥å¯åŠ¨å’Œè¿è¡Œ DC-ç”˜æ¨¡å‹ã€‚

```
# Create and compile a DC-GAN model, and print the summary

from keras.utils import np_utils
from keras.models import Sequential, Model
from keras.layers import Input, Dense, Dropout, Activation, Flatten, LeakyReLU,\
      BatchNormalization, Conv2DTranspose, Conv2D, Reshape
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import Adam, RMSprop
from keras.initializers import RandomNormal
import numpy as np
import matplotlib.pyplot as plt
import random
from tqdm import tqdm_notebook
from scipy.misc import imresize

def generator_model(latent_dim=100, leaky_alpha=0.2, init_stddev=0.02):

    g = Sequential()
    g.add(Dense(4*4*512, input_shape=(latent_dim,),
                kernel_initializer=RandomNormal(stddev=init_stddev)))
    g.add(Reshape(target_shape=(4, 4, 512)))
    g.add(BatchNormalization())
    g.add(Activation(LeakyReLU(alpha=leaky_alpha)))
    g.add(Conv2DTranspose(256, kernel_size=5, strides=2, padding='same',
                kernel_initializer=RandomNormal(stddev=init_stddev)))
    g.add(BatchNormalization())
    g.add(Activation(LeakyReLU(alpha=leaky_alpha)))
    g.add(Conv2DTranspose(128, kernel_size=5, strides=2, padding='same', 
                kernel_initializer=RandomNormal(stddev=init_stddev)))
    g.add(BatchNormalization())
    g.add(Activation(LeakyReLU(alpha=leaky_alpha)))
    g.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', 
                kernel_initializer=RandomNormal(stddev=init_stddev)))
    g.add(Activation('tanh'))
    g.summary()
    #g.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])
    return g

def discriminator_model(leaky_alpha=0.2, init_stddev=0.02):

    d = Sequential()
    d.add(Conv2D(64, kernel_size=5, strides=2, padding='same', 
               kernel_initializer=RandomNormal(stddev=init_stddev),
               input_shape=(32, 32, 3)))
    d.add(Activation(LeakyReLU(alpha=leaky_alpha)))
    d.add(Conv2D(128, kernel_size=5, strides=2, padding='same', 
               kernel_initializer=RandomNormal(stddev=init_stddev)))
    d.add(BatchNormalization())
    d.add(Activation(LeakyReLU(alpha=leaky_alpha)))
    d.add(Conv2D(256, kernel_size=5, strides=2, padding='same', 
               kernel_initializer=RandomNormal(stddev=init_stddev)))
    d.add(BatchNormalization())
    d.add(Activation(LeakyReLU(alpha=leaky_alpha)))
    d.add(Flatten())
    d.add(Dense(1, kernel_initializer=RandomNormal(stddev=init_stddev)))
    d.add(Activation('sigmoid'))
    d.summary()
    return d

def DCGAN(sample_size=100):
    # Generator
    g = generator_model(sample_size, 0.2, 0.02)

    # Discriminator
    d = discriminator_model(0.2, 0.02)
    d.compile(optimizer=Adam(lr=0.001, beta_1=0.5), loss='binary_crossentropy')
    d.trainable = False
    # GAN
    gan = Sequential([g, d])
    gan.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')

    return gan, g, d
```

ä»¥ä¸Šä»£ç åªæ˜¯é’ˆå¯¹å‘ç”Ÿå™¨å’Œé‰´åˆ«å™¨ç½‘ç»œçš„æ¶æ„ã€‚å°†è¿™ç§ç¼–ç  GAN çš„æ–¹æ³•ä¸æˆ‘åœ¨ç¬¬ 2 éƒ¨åˆ†ä¸­ä½¿ç”¨çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒæ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°è¿™ç§æ–¹æ³•ä¸å¤ªæ¸…æ™°ï¼Œå¹¶ä¸”æˆ‘ä»¬æ²¡æœ‰å®šä¹‰å…¨å±€å‚æ•°ï¼Œå› æ­¤æœ‰è®¸å¤šåœ°æ–¹æˆ‘ä»¬å¯èƒ½ä¼šæœ‰æ½œåœ¨çš„é”™è¯¯ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€äº›å‡½æ•°æ¥ç®€åŒ–æˆ‘ä»¬çš„å·¥ä½œï¼Œè¿™äº›å‡½æ•°ä¸»è¦ç”¨äºå›¾åƒçš„é¢„å¤„ç†å’Œç»˜å›¾ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬åˆ†æç½‘ç»œè¾“å‡ºã€‚

```
def load_image(filename, size=(32, 32)):
    img = plt.imread(filename)
    # crop
    rows, cols = img.shape[:2]
    crop_r, crop_c = 150, 150
    start_row, start_col = (rows - crop_r) // 2, (cols - crop_c) // 2
    end_row, end_col = rows - start_row, cols - start_row
    img = img[start_row:end_row, start_col:end_col, :]
    # resize
    img = imresize(img, size)
    return img

def preprocess(x):
    return (x/255)*2-1

def deprocess(x):
    return np.uint8((x+1)/2*255)

def make_labels(size):
    return np.ones([size, 1]), np.zeros([size, 1])  

def show_losses(losses):
    losses = np.array(losses)

    fig, ax = plt.subplots()
    plt.plot(losses.T[0], label='Discriminator')
    plt.plot(losses.T[1], label='Generator')
    plt.title("Validation Losses")
    plt.legend()
    plt.show()

def show_images(generated_images):
    n_images = len(generated_images)
    cols = 5
    rows = n_images//cols

    plt.figure(figsize=(8, 6))
    for i in range(n_images):
        img = deprocess(generated_images[i])
        ax = plt.subplot(rows, cols, i+1)
        plt.imshow(img)
        plt.xticks([])
        plt.yticks([])
    plt.tight_layout()
    plt.show()
```

**è®­ç»ƒæ¨¡å‹**

æˆ‘ä»¬ç°åœ¨å®šä¹‰è®­ç»ƒå‡½æ•°ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„ï¼Œè¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨å°†é‰´åˆ«å™¨è®¾ç½®ä¸ºå¯è®­ç»ƒå’Œä¸å¯è®­ç»ƒä¹‹é—´è¿›è¡Œäº†åˆ‡æ¢(æˆ‘ä»¬åœ¨ç¬¬ 2 éƒ¨åˆ†ä¸­éšå¼åœ°è¿™æ ·åšäº†)ã€‚

```
def train(sample_size=100, epochs=3, batch_size=128, eval_size=16, smooth=0.1): batchCount=len(train_path)//batch_size
    y_train_real, y_train_fake = make_labels(batch_size)
    y_eval_real,  y_eval_fake  = make_labels(eval_size)

    # create a GAN, a generator and a discriminator
    gan, g, d = DCGAN(sample_size)

    losses = [] for e in range(epochs):
        print('-'*15, 'Epoch %d' % (e+1), '-'*15)
        for i in tqdm_notebook(range(batchCount)):

            path_batch = train_path[i*batch_size:(i+1)*batch_size]
            image_batch = np.array([preprocess(load_image(filename)) for filename in path_batch])

            noise = np.random.normal(0, 1, size=(batch_size, noise_dim))
            generated_images = g.predict_on_batch(noise) # Train discriminator on generated images
            d.trainable = True
            d.train_on_batch(image_batch, y_train_real*(1-smooth))
            d.train_on_batch(generated_images, y_train_fake) # Train generator
            d.trainable = False
            g_loss=gan.train_on_batch(noise, y_train_real)

        # evaluate
        test_path = np.array(val_path)[np.random.choice(len(val_path), eval_size, replace=False)]
        x_eval_real = np.array([preprocess(load_image(filename)) for filename in test_path]) noise = np.random.normal(loc=0, scale=1, size=(eval_size, sample_size))
        x_eval_fake = g.predict_on_batch(noise)

        d_loss  = d.test_on_batch(x_eval_real, y_eval_real)
        d_loss += d.test_on_batch(x_eval_fake, y_eval_fake)
        g_loss  = gan.test_on_batch(noise, y_eval_real)

        losses.append((d_loss/2, g_loss))

        print("Epoch: {:>3}/{} Discriminator Loss: {:>6.4f} Generator Loss: {:>6.4f}".format(
            e+1, epochs, d_loss, g_loss))  

        show_images(x_eval_fake[:10])

    # show the result
    show_losses(losses)
    show_images(g.predict(np.random.normal(loc=0, scale=1, size=(15, sample_size))))    
    return gnoise_dim=100
train()
```

è¯¥å‡½æ•°çš„è¾“å‡ºå°†ä¸ºæˆ‘ä»¬æä¾›æ¯ä¸ªæ—¶æœŸçš„ä»¥ä¸‹è¾“å‡º:

![](img/f9d52f41c44602678485d737545c40ba.png)

å®ƒè¿˜å°†ç»˜åˆ¶é‰´åˆ«å™¨å’Œå‘ç”Ÿå™¨çš„éªŒè¯æŸå¤±ã€‚

![](img/be7461d47103c59924a5ebbe26a9eba1.png)

ç”Ÿæˆçš„å›¾åƒçœ‹èµ·æ¥å¾ˆåˆç†ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å¾—è¶³å¤Ÿå¥½ï¼Œå°½ç®¡å›¾åƒè´¨é‡ä¸å¦‚è®­ç»ƒé›†ä¸­çš„å›¾åƒè´¨é‡å¥½(å› ä¸ºæˆ‘ä»¬å¯¹å›¾åƒè¿›è¡Œäº†æ•´å½¢ï¼Œä½¿å…¶å˜å¾—æ›´å°ï¼Œå¹¶ä½¿å®ƒä»¬æ¯”åŸå§‹å›¾åƒæ›´æ¨¡ç³Š)ã€‚ä½†æ˜¯ï¼Œå®ƒä»¬è¶³å¤Ÿç”ŸåŠ¨ï¼Œå¯ä»¥åˆ›å»ºæœ‰æ•ˆçš„äººè„¸ï¼Œå¹¶ä¸”è¿™äº›äººè„¸è¶³å¤Ÿæ¥è¿‘ç°å®ã€‚æ­¤å¤–ï¼Œä¸ VAE åˆ¶ä½œçš„å›¾åƒç›¸æ¯”ï¼Œè¿™äº›å›¾åƒæ›´æœ‰åˆ›æ„ï¼Œçœ‹èµ·æ¥æ›´çœŸå®ã€‚

æ‰€ä»¥çœ‹èµ·æ¥ GAN åœ¨è¿™ç§æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œçœ‹çœ‹ GAN ä¸æ··åˆå˜ä½“ VAE-GAN ç›¸æ¯”è¡¨ç°å¦‚ä½•ã€‚

# **åŠ¨æ¼«æ•°æ®é›†**

åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ GAN ä»¥åŠå¦ä¸€ç§ç‰¹æ®Šå½¢å¼çš„ GANï¼Œå³ VAE-GANï¼Œæ¥ç”Ÿæˆä¸åŠ¨ç”»æ•°æ®é›†é£æ ¼ç›¸åŒçš„äººè„¸ã€‚æœ¯è¯­ VAE-ç”˜é¦–å…ˆç”± Larsen ç­‰äººä½¿ç”¨ã€‚al åœ¨ä»–ä»¬çš„è®ºæ–‡[â€œä½¿ç”¨å­¦ä¹ çš„ç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œåƒç´ ä»¥å¤–çš„è‡ªåŠ¨ç¼–ç â€](https://arxiv.org/abs/1512.09300)ã€‚VAE-ç”˜æ¨¡å‹ä¸ç”˜æ¨¡å‹çš„åŒºåˆ«åœ¨äºå®ƒä»¬çš„**ç”Ÿæˆå™¨æ˜¯å˜å¼‚è‡ªåŠ¨ç¼–ç å™¨**ã€‚

![](img/f45b973004a186700c45f332f71c45e1.png)

VAE-GAN architecture. Source: [https://arxiv.org/abs/1512.09300](https://arxiv.org/abs/1512.09300)

é¦–å…ˆï¼Œæˆ‘ä»¬å°†é‡ç‚¹æ”¾åœ¨ DC-ç”˜ã€‚åŠ¨æ¼«æ•°æ®é›†ç”±è¶…è¿‡ 20K å¼  64x64 å›¾åƒå½¢å¼çš„åŠ¨æ¼«å¤´åƒç»„æˆã€‚æˆ‘ä»¬è¿˜éœ€è¦åˆ›å»ºå¦ä¸€ä¸ª [Keras å®šåˆ¶æ•°æ®ç”Ÿæˆå™¨](https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43)ã€‚è¯¥æ•°æ®é›†çš„é“¾æ¥å¯åœ¨æ­¤å¤„æ‰¾åˆ°:

[](https://github.com/Mckinsey666/Anime-Face-Dataset) [## McKinsey 666/åŠ¨æ¼«äººè„¸æ•°æ®é›†

### ğŸ–¼æ”¶é›†äº†é«˜è´¨é‡çš„åŠ¨æ¼«é¢å­”ã€‚ä¸º Mckinsey666/Anime-Face-Dataset å¼€å‘åšå‡ºè´¡çŒ®ï¼Œåˆ›å»ºä¸€ä¸ªâ€¦

github.com](https://github.com/Mckinsey666/Anime-Face-Dataset) 

# **åŠ¨æ¼«æ•°æ®é›†ä¸Šçš„ç”˜**

æˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯åˆ›å»ºåŠ¨æ¼«ç›®å½•å¹¶ä¸‹è½½æ•°æ®ã€‚è¿™å¯ä»¥é€šè¿‡ä¸Šé¢çš„é“¾æ¥æ¥å®Œæˆã€‚åœ¨ç»§ç»­ä¹‹å‰æ£€æŸ¥æ•°æ®æ€»æ˜¯å¥½çš„åšæ³•ï¼Œæ‰€ä»¥æˆ‘ä»¬ç°åœ¨å°±è¿™æ ·åšã€‚

```
from skimage import io
import matplotlib.pyplot as plt

filePath='anime-faces/data/'
imgSets=[]

for i in range(1,20001):
    imgName=filePath+str(i)+'.png'
    imgSets.append(io.imread(imgName))

plt.imshow(imgSets[1234])
plt.axis('off')
plt.show()
```

æˆ‘ä»¬ç°åœ¨åˆ›å»ºå¹¶ç¼–è¯‘æˆ‘ä»¬çš„ DC-ç”˜æ¨¡å‹ã€‚

```
# Create and compile a DC-GAN modelfrom keras.models import Sequential, Model
from keras.layers import Input, Dense, Dropout, Activation, \
    Flatten, LeakyReLU, BatchNormalization, Conv2DTranspose, Conv2D, Reshape
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import UpSampling2D
from keras.optimizers import Adam, RMSprop,SGD
from keras.initializers import RandomNormalimport numpy as np
import matplotlib.pyplot as plt
import os, glob
from PIL import Image
from tqdm import tqdm_notebook image_shape = (64, 64, 3)
#noise_shape = (100,)
Noise_dim = 128
img_rows = 64
img_cols = 64
channels = 3def generator_model(latent_dim=100, leaky_alpha=0.2):
    model = Sequential()

    # layer1 (None,500)>>(None,128*16*16)
    model.add(Dense(128 * 16 * 16, activation="relu", input_shape=(Noise_dim,)))

    # (None,16*16*128)>>(None,16,16,128)
    model.add(Reshape((16, 16, 128)))

   # (None,16,16,128)>>(None,32,32,128)
    model.add(UpSampling2D())
    model.add(Conv2D(256, kernel_size=3, padding="same"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Activation("relu")) #(None,32,32,128)>>(None,64,64,128)
    model.add(UpSampling2D())

    # (None,64,64,128)>>(None,64,64,64)
    model.add(Conv2D(128, kernel_size=3, padding="same"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Activation("relu")) # (None,64,64,128)>>(None,64,64,32) model.add(Conv2D(32, kernel_size=3, padding="same"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Activation("relu"))

    # (None,64,64,32)>>(None,64,64,3)
    model.add(Conv2D(channels, kernel_size=3, padding="same"))
    model.add(Activation("tanh")) model.summary()
    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])
    return model def discriminator_model(leaky_alpha=0.2, dropRate=0.3):
    model = Sequential()

    # layer1 (None,64,64,3)>>(None,32,32,32)
    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, padding="same"))
    model.add(LeakyReLU(alpha=leaky_alpha))
    model.add(Dropout(dropRate)) # layer2 (None,32,32,32)>>(None,16,16,64)
    model.add(Conv2D(64, kernel_size=3, strides=2, padding="same")) # model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=leaky_alpha))
    model.add(Dropout(dropRate)) # (None,16,16,64)>>(None,8,8,128)
    model.add(Conv2D(128, kernel_size=3, strides=2, padding="same"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(dropRate)) # (None,8,8,128)>>(None,8,8,256)
    model.add(Conv2D(256, kernel_size=3, strides=1, padding="same"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(dropRate)) # (None,8,8,256)>>(None,8,8,64)
    model.add(Conv2D(64, kernel_size=3, strides=1, padding="same"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(dropRate))

    # (None,8,8,64)
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid')) model.summary() sgd=SGD(lr=0.0002)
    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])
    return model def DCGAN(sample_size=Noise_dim):
    # generator
    g = generator_model(sample_size, 0.2) # discriminator
    d = discriminator_model(0.2)
    d.trainable = False
    # GAN
    gan = Sequential([g, d])

    sgd=SGD()
    gan.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')
    return gan, g, d def get_image(image_path, width, height, mode):
    image = Image.open(image_path)
    #print(image.size) return np.array(image.convert(mode)) def get_batch(image_files, width, height, mode):
    data_batch = np.array([get_image(sample_file, width, height, mode) \
                           for sample_file in image_files])
    return data_batch def show_imgs(generator,epoch):
    row=3
    col = 5
    noise = np.random.normal(0, 1, (row * col, Noise_dim))
    gen_imgs = generator.predict(noise) # Rescale images 0 - 1
    gen_imgs = 0.5 * gen_imgs + 0.5 fig, axs = plt.subplots(row, col)
    #fig.suptitle("DCGAN: Generated digits", fontsize=12)
    cnt = 0 for i in range(row):
        for j in range(col):
            axs[i, j].imshow(gen_imgs[cnt, :, :, :])
            axs[i, j].axis('off')
            cnt += 1 #plt.close()
    plt.show()
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨åŠ¨ç”»æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬å°†ä»¥ä¸¤ç§ä¸åŒçš„æ–¹å¼æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œç¬¬ä¸€ç§æ–¹å¼å°†æ¶‰åŠä»¥ 1:1 çš„è®­ç»ƒæ—¶é—´æ¯”ä¾‹æ¥è®­ç»ƒé‰´åˆ«å™¨å’Œç”Ÿæˆå™¨ã€‚

```
# Training the discriminator and generator with the 1:1 proportion of training timesdef train(epochs=30, batchSize=128):
    filePath = r'anime-faces/data/' X_train = get_batch(glob.glob(os.path.join(filePath, '*.png'))[:20000], 64, 64, 'RGB')
    X_train = (X_train.astype(np.float32) - 127.5) / 127.5 halfSize = int(batchSize / 2)
    batchCount=int(len(X_train)/batchSize) dLossReal = []
    dLossFake = []
    gLossLogs = [] gan, generator, discriminator = DCGAN(Noise_dim) for e in range(epochs):
        for i in tqdm_notebook(range(batchCount)):
            index = np.random.randint(0, X_train.shape[0], halfSize)
            images = X_train[index] noise = np.random.normal(0, 1, (halfSize, Noise_dim))
            genImages = generator.predict(noise) # one-sided labels
            discriminator.trainable = True
            dLossR = discriminator.train_on_batch(images, np.ones([halfSize, 1]))
            dLossF = discriminator.train_on_batch(genImages, np.zeros([halfSize, 1]))
            dLoss = np.add(dLossF, dLossR) * 0.5
            discriminator.trainable = False noise = np.random.normal(0, 1, (batchSize, Noise_dim))
            gLoss = gan.train_on_batch(noise, np.ones([batchSize, 1])) dLossReal.append([e, dLoss[0]])
        dLossFake.append([e, dLoss[1]])
        gLossLogs.append([e, gLoss]) dLossRealArr = np.array(dLossReal)
        dLossFakeArr = np.array(dLossFake)
        gLossLogsArr = np.array(gLossLogs)        # At the end of training plot the losses vs epochs
        show_imgs(generator, e) plt.plot(dLossRealArr[:, 0], dLossRealArr[:, 1], label="Discriminator Loss - Real")
    plt.plot(dLossFakeArr[:, 0], dLossFakeArr[:, 1], label="Discriminator Loss - Fake")
    plt.plot(gLossLogsArr[:, 0], gLossLogsArr[:, 1], label="Generator Loss")
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('GAN')
    plt.grid(True)
    plt.show()

    return gan, generator, discriminator GAN,Generator,Discriminator=train(epochs=20, batchSize=128)  
train(epochs=1000, batchSize=128, plotInternal=200)
```

è¾“å‡ºç°åœ¨å°†å¼€å§‹æ‰“å°ä¸€ç³»åˆ—çš„åŠ¨æ¼«äººç‰©ã€‚å®ƒä»¬èµ·åˆéå¸¸ç²—ç³™ï¼Œéšç€æ—¶é—´çš„æ¨ç§»é€æ¸å˜å¾—è¶Šæ¥è¶Šæ˜æ˜¾ã€‚

![](img/735ed6f8251508b90e13ea4aef36f4aa.png)

æˆ‘ä»¬è¿˜å°†å¾—åˆ°å‘ç”µæœºå’Œé‰´é¢‘å™¨æŸè€—å‡½æ•°çš„æ›²çº¿å›¾ã€‚

![](img/4c7bd2ea43fa1c085a898f9f8fb186fb.png)

ç°åœ¨æˆ‘ä»¬å°†åšåŒæ ·çš„äº‹æƒ…ï¼Œä½†æ˜¯ç”¨ä¸åŒçš„è®­ç»ƒæ—¶é—´æ¥è®­ç»ƒé‰´åˆ«å™¨å’Œç”Ÿæˆå™¨ï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚

åœ¨ç»§ç»­ä¹‹å‰ï¼Œæœ€å¥½å°†æ¨¡å‹çš„æƒé‡ä¿å­˜åœ¨æŸä¸ªåœ°æ–¹ï¼Œè¿™æ ·æ‚¨å°±ä¸éœ€è¦å†æ¬¡è¿è¡Œæ•´ä¸ªè®­ç»ƒï¼Œè€Œæ˜¯å¯ä»¥å°†æƒé‡åŠ è½½åˆ°ç½‘ç»œä¸­ã€‚

ä¸ºäº†èŠ‚çœé‡é‡:

```
discriminator.save_weights('/content/gdrive/My Drive/discriminator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')
gan.save_weights('/content/gdrive/My Drive/gan_DCGAN_lr0.0001_deepgenerator+proportion2.h5')
generator.save_weights('/content/gdrive/My Drive/generator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')
```

è¦åŠ è½½ç ç :

```
discriminator.load_weights('/content/gdrive/My Drive/discriminator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')
gan.load_weights('/content/gdrive/My Drive/gan_DCGAN_lr0.0001_deepgenerator+proportion2.h5')
generator.load_weights('/content/gdrive/My Drive/generator_DCGAN_lr0.0001_deepgenerator+proportion2.h5')
```

ç°åœ¨ï¼Œæˆ‘ä»¬è½¬åˆ°ç¬¬äºŒä¸ªç½‘ç»œå®æ–½ï¼Œè€Œä¸ç”¨æ‹…å¿ƒæ¯”ä¹‹å‰çš„ç½‘ç»œèŠ‚çœæˆæœ¬ã€‚

```
# Train the discriminator and generator separately and with different training timesdef train(epochs=300, batchSize=128, plotInternal=50):
    gLoss = 1
    filePath = r'anime-faces/data/'

    X_train = get_batch(glob.glob(os.path.join(filePath,'*.png'))[:20000],64,64,'RGB')
    X_train=(X_train.astype(np.float32)-127.5)/127.5
    halfSize= int (batchSize/2) dLossReal=[]
    dLossFake=[]
    gLossLogs=[]    for e in range(epochs):
        index=np.random.randint(0,X_train.shape[0],halfSize)
        images=X_train[index] noise=np.random.normal(0,1,(halfSize,Noise_dim))
        genImages=generator.predict(noise)

        if e < int(epochs*0.5):    
            #one-sided labels
            discriminator.trainable=True
            dLossR=discriminator.train_on_batch(images,np.ones([halfSize,1]))
            dLossF=discriminator.train_on_batch(genImages,np.zeros([halfSize,1]))
            dLoss=np.add(dLossF,dLossR)*0.5
            discriminator.trainable=False cnt = e while cnt > 3:
                cnt = cnt - 4 if cnt == 0:
                noise=np.random.normal(0,1,(batchSize,Noise_dim))
                gLoss=gan.train_on_batch(noise,np.ones([batchSize,1]))

        elif e>= int(epochs*0.5) :
            cnt = e while cnt > 3:
                cnt = cnt - 4 if cnt == 0:
                #one-sided labels
                discriminator.trainable=True
                dLossR=discriminator.train_on_batch(images,np.ones([halfSize,1]))
                dLossF=discriminator.train_on_batch(genImages,np.zeros([halfSize,1]))
                dLoss=np.add(dLossF,dLossR)*0.5
                discriminator.trainable=False noise=np.random.normal(0,1,(batchSize,Noise_dim))
            gLoss=gan.train_on_batch(noise,np.ones([batchSize,1])) if e % 20 == 0:
           print("epochï¼š %d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (e, dLoss[0], 100 * dLoss[1], gLoss)) dLossReal.append([e,dLoss[0]])
        dLossFake.append([e,dLoss[1]])
        gLossLogs.append([e,gLoss]) if e % plotInternal == 0 and e!=0:
            show_imgs(generator, e)

        dLossRealArr= np.array(dLossReal)
        dLossFakeArr = np.array(dLossFake)
        gLossLogsArr = np.array(gLossLogs)

        chk = e while chk > 50:
            chk = chk - 51 if chk == 0:
            discriminator.save_weights('/content/gdrive/My Drive/discriminator_DCGAN_lr=0.0001,proportion2,deepgenerator_Fake.h5')
            gan.save_weights('/content/gdrive/My Drive/gan_DCGAN_lr=0.0001,proportion2,deepgenerator_Fake.h5')
            generator.save_weights('/content/gdrive/My Drive/generator_DCGAN_lr=0.0001,proportion2,deepgenerator_Fake.h5')
        # At the end of training plot the losses vs epochs
    plt.plot(dLossRealArr[:, 0], dLossRealArr[:, 1], label="Discriminator Loss - Real")
    plt.plot(dLossFakeArr[:, 0], dLossFakeArr[:, 1], label="Discriminator Loss - Fake")
    plt.plot(gLossLogsArr[:, 0], gLossLogsArr[:, 1], label="Generator Loss")
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('GAN')
    plt.grid(True)
    plt.show()

    return gan, generator, discriminatorgan, generator, discriminator = DCGAN(Noise_dim)
train(epochs=4000, batchSize=128, plotInternal=200)
```

è®©æˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹è¿™ä¸¤ä¸ªç½‘ç»œçš„è¾“å‡ºã€‚é€šè¿‡è¿è¡Œè¯¥è¡Œ:

```
show_imgs(Generator)
```

ç½‘ç»œå°†ä»ç”Ÿæˆå™¨è¾“å‡ºä¸€äº›å›¾åƒ(è¿™æ˜¯æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„å‡½æ•°ä¹‹ä¸€)ã€‚

![](img/6cc4033c4a44d571e8c5b34b431292a3.png)

Generated images from 1:1 training of discriminator vs. generator.

ç°åœ¨è®©æˆ‘ä»¬æ£€æŸ¥ç¬¬äºŒä¸ªæ¨¡å‹ã€‚

![](img/5353ccde8122af5660467cb96f754037.png)

Generated images from the second network with different training times for the discriminator and generator.

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç”Ÿæˆçš„å›¾åƒçš„ç»†èŠ‚å¾—åˆ°äº†æ”¹å–„ï¼Œå®ƒä»¬çš„çº¹ç†ç¨å¾®æ›´åŠ è¯¦ç»†ã€‚ç„¶è€Œï¼Œä¸è®­ç»ƒå›¾åƒç›¸æ¯”ï¼Œå®ƒä»¬ä»ç„¶æ˜¯ä¸åˆæ ¼çš„ã€‚

![](img/a4209a9809e24ec6a6e1fb5217fcf38c.png)

Training images from Anime dataset.

ä¹Ÿè®¸ VAE-ç”˜ä¼šè¡¨ç°å¾—æ›´å¥½ï¼Ÿ

# **åŠ¨æ¼«æ•°æ®é›†ä¸Šçš„ç”˜**

é‡ç”³ä¸€ä¸‹æˆ‘ä¹‹å‰è¯´è¿‡çš„å…³äº VAE-ç”˜çš„è¯ï¼Œæœ¯è¯­ VAE-ç”˜é¦–å…ˆæ˜¯ç”± Larsen ç­‰äººä½¿ç”¨çš„ã€‚al åœ¨ä»–ä»¬çš„è®ºæ–‡[â€œä½¿ç”¨å­¦ä¹ çš„ç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œåƒç´ ä»¥å¤–çš„è‡ªåŠ¨ç¼–ç â€](https://arxiv.org/abs/1512.09300)ã€‚VAE-ç”˜æ¨¡å‹ä¸ç”˜æ¨¡å‹çš„åŒºåˆ«åœ¨äºå®ƒä»¬çš„**ç”Ÿæˆå™¨æ˜¯å˜å¼‚è‡ªåŠ¨ç¼–ç å™¨**ã€‚

![](img/f45b973004a186700c45f332f71c45e1.png)

VAE-GAN architecture. Source: [https://arxiv.org/abs/1512.09300](https://arxiv.org/abs/1512.09300)

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºå¹¶ç¼–è¯‘ VAE-GANï¼Œå¹¶å¯¹æ¯ä¸ªç½‘ç»œè¿›è¡Œæ€»ç»“(è¿™æ˜¯ç®€å•æ£€æŸ¥æ¶æ„çš„å¥½æ–¹æ³•)ã€‚

```
# Create and compile a VAE-GAN, and make a summary for themfrom keras.models import Sequential, Model
from keras.layers import Input, Dense, Dropout, Activation, \
    Flatten, LeakyReLU, BatchNormalization, Conv2DTranspose, Conv2D, Reshape,MaxPooling2D,UpSampling2D,InputLayer, Lambda
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import UpSampling2D
from keras.optimizers import Adam, RMSprop,SGD
from keras.initializers import RandomNormal
import numpy as np
import matplotlib.pyplot as plt
import os, glob
from PIL import Image
import pandas as pd
from scipy.stats import norm
import keras
from keras.utils import np_utils, to_categorical
from keras import backend as K
import random
from keras import metrics
from tqdm import tqdm # plotInternal
plotInternal = 50#######
latent_dim = 256
batch_size = 256
rows = 64
columns = 64
channel = 3
epochs = 4000
# datasize = len(dataset)# optimizers
SGDop = SGD(lr=0.0003)
ADAMop = Adam(lr=0.0002)
# filters
filter_of_dis = 16
filter_of_decgen = 16
filter_of_encoder = 16 def sampling(args):
    mean, logsigma = args
    epsilon = K.random_normal(shape=(K.shape(mean)[0], latent_dim), mean=0., stddev=1.0)
    return mean + K.exp(logsigma / 2) * epsilondef vae_loss(X , output , E_mean, E_logsigma):
	# compute the average MSE error, then scale it up, ie. simply sum on all axes
  reconstruction_loss = 2 * metrics.mse(K.flatten(X), K.flatten(output))

	# compute the KL loss
  kl_loss = - 0.5 * K.sum(1 + E_logsigma - K.square(E_mean) - K.exp(E_logsigma), axis=-1) total_loss = K.mean(reconstruction_loss + kl_loss)    

  return total_loss def encoder(kernel, filter, rows, columns, channel):
    X = Input(shape=(rows, columns, channel))
    model = Conv2D(filters=filter, kernel_size=kernel, strides=2, padding='same')(X)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = LeakyReLU(alpha=0.2)(model) model = Conv2D(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(model)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = LeakyReLU(alpha=0.2)(model) model = Conv2D(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = LeakyReLU(alpha=0.2)(model) model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = LeakyReLU(alpha=0.2)(model) model = Flatten()(model) mean = Dense(latent_dim)(model)
    logsigma = Dense(latent_dim, activation='tanh')(model)
    latent = Lambda(sampling, output_shape=(latent_dim,))([mean, logsigma])
    meansigma = Model([X], [mean, logsigma, latent])
    meansigma.compile(optimizer=SGDop, loss='mse')
    return meansigma def decgen(kernel, filter, rows, columns, channel):
    X = Input(shape=(latent_dim,)) model = Dense(2*2*256)(X)
    model = Reshape((2, 2, 256))(model)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = Activation('relu')(model) model = Conv2DTranspose(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = Activation('relu')(model)

    model = Conv2DTranspose(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = Activation('relu')(model) model = Conv2DTranspose(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(model)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = Activation('relu')(model) model = Conv2DTranspose(filters=filter, kernel_size=kernel, strides=2, padding='same')(model)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = Activation('relu')(model) model = Conv2DTranspose(filters=channel, kernel_size=kernel, strides=2, padding='same')(model)
    model = Activation('tanh')(model) model = Model(X, model)
    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, beta_1=0.5), metrics=['accuracy'])
    return model def discriminator(kernel, filter, rows, columns, channel):
    X = Input(shape=(rows, columns, channel)) model = Conv2D(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(X)
    model = LeakyReLU(alpha=0.2)(model) model = Conv2D(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = LeakyReLU(alpha=0.2)(model) model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)
    model = BatchNormalization(epsilon=1e-5)(model)
    model = LeakyReLU(alpha=0.2)(model) model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model) dec = BatchNormalization(epsilon=1e-5)(model)
    dec = LeakyReLU(alpha=0.2)(dec)
    dec = Flatten()(dec)
    dec = Dense(1, activation='sigmoid')(dec) output = Model(X, dec)
    output.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])

    return output def VAEGAN(decgen,discriminator):
    # generator
    g = decgen # discriminator
    d = discriminator
    d.trainable = False
    # GAN
    gan = Sequential([g, d])

#     sgd=SGD()
    gan.compile(optimizer=Adam(lr=0.0001, beta_1=0.5), loss='binary_crossentropy')
    return g, d, gan
```

æˆ‘ä»¬å†æ¬¡å®šä¹‰äº†ä¸€äº›å‡½æ•°ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ‰“å°æ¥è‡ªç”Ÿæˆå™¨çš„å›¾åƒã€‚

```
def get_image(image_path, width, height, mode):
    image = Image.open(image_path)
    #print(image.size)

    return np.array(image.convert(mode))

def show_imgs(generator):
    row=3
    col = 5
    noise = np.random.normal(0, 1, (row*col, latent_dim))
    gen_imgs = generator.predict(noise)

    # Rescale images 0 - 1
    gen_imgs = 0.5 * gen_imgs + 0.5

    fig, axs = plt.subplots(row, col)
    #fig.suptitle("DCGAN: Generated digits", fontsize=12)
    cnt = 0

    for i in range(row):
        for j in range(col):
            axs[i, j].imshow(gen_imgs[cnt, :, :, :])
            axs[i, j].axis('off')
            cnt += 1

    #plt.close()
    plt.show()
```

å‘ç”Ÿå™¨çš„å‚æ•°å°†å—åˆ° GAN å’Œ VAE è®­ç»ƒçš„å½±å“ã€‚

```
# note: The parameters of the generator will be affected by both the GAN and VAE training

G, D, GAN = VAEGAN(decgen(5, filter_of_decgen, rows, columns, channel),discriminator(5, filter_of_dis, rows, columns, channel))

# encoder
E = encoder(5, filter_of_encoder, rows, columns, channel)
print("This is the summary for encoder:")
E.summary()

# generator/decoder
# G = decgen(5, filter_of_decgen, rows, columns, channel)
print("This is the summary for dencoder/generator:")
G.summary()

# discriminator
# D = discriminator(5, filter_of_dis, rows, columns, channel)
print("This is the summary for discriminator:")
D.summary()

D_fixed = discriminator(5, filter_of_dis, rows, columns, channel)
D_fixed.compile(optimizer=SGDop, loss='mse')

# gan
print("This is the summary for GAN:")
GAN.summary()

# VAE
X = Input(shape=(rows, columns, channel))

E_mean, E_logsigma, Z = E(X)

output = G(Z)
# G_dec = G(E_mean + E_logsigma)
# D_fake, F_fake = D(output)
# D_fromGen, F_fromGen = D(G_dec)
# D_true, F_true = D(X)

# print("type(E)",type(E))
# print("type(output)",type(output))
# print("type(D_fake)",type(D_fake))

VAE = Model(X, output)
VAE.add_loss(vae_loss(X, output, E_mean, E_logsigma))
VAE.compile(optimizer=SGDop)

print("This is the summary for vae:")
VAE.summary()
```

åœ¨ä¸‹é¢çš„å•å…ƒæ ¼ä¸­ï¼Œæˆ‘ä»¬å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨å‰é¢çš„æ–¹æ³•æ¥è®­ç»ƒé‰´é¢‘å™¨ä»¥åŠ GAN å’Œ VAE ä¸åŒçš„æ—¶é—´é•¿åº¦ã€‚æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹çš„å‰åŠéƒ¨åˆ†å¼ºè°ƒé‰´åˆ«å™¨çš„è®­ç»ƒï¼Œåœ¨ååŠéƒ¨åˆ†æˆ‘ä»¬æ›´å¤šåœ°è®­ç»ƒå‘ç”Ÿå™¨ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³æé«˜è¾“å‡ºå›¾åƒçš„è´¨é‡ã€‚

```
# We train our model in this cell

dLoss=[]
gLoss=[]
GLoss = 1
GlossEnc = 1
GlossGen = 1
Eloss = 1

halfbatch_size = int(batch_size*0.5)

for epoch in tqdm(range(epochs)):
    if epoch < int(epochs*0.5):
        noise = np.random.normal(0, 1, (halfbatch_size, latent_dim))
        index = np.random.randint(0,dataset.shape[0], halfbatch_size)
        images = dataset[index]  

        latent_vect = E.predict(images)[0]
        encImg = G.predict(latent_vect)
        fakeImg = G.predict(noise)

        D.Trainable = True
        DlossTrue = D.train_on_batch(images, np.ones((halfbatch_size, 1)))
        DlossEnc = D.train_on_batch(encImg, np.ones((halfbatch_size, 1)))       
        DlossFake = D.train_on_batch(fakeImg, np.zeros((halfbatch_size, 1)))

#         DLoss=np.add(DlossTrue,DlossFake)*0.5

        DLoss=np.add(DlossTrue,DlossEnc)
        DLoss=np.add(DLoss,DlossFake)*0.33
        D.Trainable = False

        cnt = epoch

        while cnt > 3:
            cnt = cnt - 4

        if cnt == 0:
            noise = np.random.normal(0, 1, (batch_size, latent_dim))
            index = np.random.randint(0,dataset.shape[0], batch_size)
            images = dataset[index]  
            latent_vect = E.predict(images)[0]     

            GlossEnc = GAN.train_on_batch(latent_vect, np.ones((batch_size, 1)))
            GlossGen = GAN.train_on_batch(noise, np.ones((batch_size, 1)))
            Eloss = VAE.train_on_batch(images, None)   
            GLoss=np.add(GlossEnc,GlossGen)
            GLoss=np.add(GLoss,Eloss)*0.33
        dLoss.append([epoch,DLoss[0]]) 
        gLoss.append([epoch,GLoss])

    elif epoch >= int(epochs*0.5):
        cnt = epoch
        while cnt > 3:
            cnt = cnt - 4

        if cnt == 0:
            noise = np.random.normal(0, 1, (halfbatch_size, latent_dim))
            index = np.random.randint(0,dataset.shape[0], halfbatch_size)
            images = dataset[index]  

            latent_vect = E.predict(images)[0]
            encImg = G.predict(latent_vect)
            fakeImg = G.predict(noise)

            D.Trainable = True
            DlossTrue = D.train_on_batch(images, np.ones((halfbatch_size, 1)))
        #     DlossEnc = D.train_on_batch(encImg, np.ones((halfbatch_size, 1)))       
            DlossFake = D.train_on_batch(fakeImg, np.zeros((halfbatch_size, 1)))

            DLoss=np.add(DlossTrue,DlossFake)*0.5

#             DLoss=np.add(DlossTrue,DlossEnc)
#             DLoss=np.add(DLoss,DlossFake)*0.33
            D.Trainable = False

        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        index = np.random.randint(0,dataset.shape[0], batch_size)
        images = dataset[index]  
        latent_vect = E.predict(images)[0]

        GlossEnc = GAN.train_on_batch(latent_vect, np.ones((batch_size, 1)))
        GlossGen = GAN.train_on_batch(noise, np.ones((batch_size, 1)))
        Eloss = VAE.train_on_batch(images, None)   
        GLoss=np.add(GlossEnc,GlossGen)
        GLoss=np.add(GLoss,Eloss)*0.33

        dLoss.append([epoch,DLoss[0]]) 
        gLoss.append([epoch,GLoss])

    if epoch % plotInternal == 0 and epoch!=0:
        show_imgs(G)

    dLossArr= np.array(dLoss)
    gLossArr = np.array(gLoss)

#     print("dLossArr.shape:",dLossArr.shape)
#     print("gLossArr.shape:",gLossArr.shape)

    chk = epoch

    while chk > 50:
        chk = chk - 51

    if chk == 0:
        D.save_weights('/content/gdrive/My Drive/VAE discriminator_kernalsize5_proportion_32.h5')
        G.save_weights('/content/gdrive/My Drive/VAE generator_kernalsize5_proportion_32.h5')
        E.save_weights('/content/gdrive/My Drive/VAE encoder_kernalsize5_proportion_32.h5')

    if epoch%20 == 0:    
        print("epoch:", epoch + 1,"  ", "DislossTrue loss:",DlossTrue[0],"D accuracyï¼š",100* DlossTrue[1], "DlossFake loss:", DlossFake[0],"GlossEnc loss:",
          GlossEnc, "GlossGen loss:",GlossGen, "Eloss loss:",Eloss)
#     print("loss:")
#     print("D:", DlossTrue, DlossEnc, DlossFake)
#     print("G:", GlossEnc, GlossGen)
#     print("VAE:", Eloss)

print('Training done,saving weights')
D.save_weights('/content/gdrive/My Drive/VAE discriminator_kernalsize5_proportion_32.h5')
G.save_weights('/content/gdrive/My Drive/VAE generator_kernalsize5_proportion_32.h5')
E.save_weights('/content/gdrive/My Drive/VAE encoder_kernalsize5_proportion_32.h5')

print('painting losses')
# At the end of training plot the losses vs epochs
plt.plot(dLossArr[:, 0], dLossArr[:, 1], label="Discriminator Loss")
plt.plot(gLossArr[:, 0], gLossArr[:, 1], label="Generator Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('GAN')
plt.grid(True)
plt.show()
print('end')
```

å¦‚æœä½ è®¡åˆ’è¿è¡Œè¿™ä¸ªç½‘ç»œï¼Œè¯·æ³¨æ„åŸ¹è®­è¿‡ç¨‹éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚æˆ‘ä¸ä¼šå°è¯•è¿™æ ·åšï¼Œé™¤éä½ æœ‰ä¸€äº›å¼ºå¤§çš„å›¾å½¢å¤„ç†å™¨ï¼Œæˆ–è€…æ„¿æ„è¿è¡Œä¸€æ•´å¤©çš„æ¨¡å‹ã€‚

ç°åœ¨æˆ‘ä»¬çš„ VAE-GAN è®­ç»ƒå·²ç»å®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥çœ‹çœ‹æˆ‘ä»¬çš„è¾“å‡ºå›¾åƒçœ‹èµ·æ¥å¦‚ä½•ï¼Œå¹¶ä¸æˆ‘ä»¬ä»¥å‰çš„ GAN è¿›è¡Œæ¯”è¾ƒã€‚

```
# In this cell, we generate and visualize 15 images. 

show_imgs(G)
```

![](img/f11fcca1614b9ce1255c0040905a542c.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ VAE-ç”˜çš„å®ç°ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªå¾ˆå¥½çš„æ¨¡å‹ï¼Œå®ƒå¯ä»¥ç”Ÿæˆæ¸…æ™°çš„å›¾åƒï¼Œå¹¶ä¸”å…·æœ‰ä¸åŸå§‹å›¾åƒç›¸ä¼¼çš„é£æ ¼ã€‚æˆ‘ä»¬çš„ VAE-ç”˜å¯ä»¥åˆ›å»ºæ›´å¥å£®çš„å›¾åƒï¼Œè¿™å¯ä»¥åœ¨æ²¡æœ‰é¢å¤–çš„åŠ¨ç”»è„¸å™ªå£°çš„æƒ…å†µä¸‹å®Œæˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ¨¡å‹çš„æ¦‚æ‹¬èƒ½åŠ›ä¸æ˜¯å¾ˆå¥½ï¼Œå®ƒå¾ˆå°‘æ”¹å˜è§’è‰²çš„æ–¹å¼æˆ–æ€§åˆ«ï¼Œæ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬å¯ä»¥å°è¯•æ”¹è¿›çš„ä¸€ç‚¹ã€‚

# **æœ€ç»ˆç‚¹è¯„**

ä¸ä¸€å®šæ¸…æ¥šå“ªä¸ªæ¨¡å‹æ¯”å…¶ä»–æ¨¡å‹æ›´å¥½ï¼Œè€Œä¸”è¿™äº›æ–¹æ³•éƒ½æ²¡æœ‰ç»è¿‡é€‚å½“çš„ä¼˜åŒ–ï¼Œå› æ­¤å¾ˆéš¾è¿›è¡Œæ¯”è¾ƒã€‚

è¿™ä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œæ‰€ä»¥å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œæˆ‘å»ºè®®ä½ æŠ•å…¥è¿›å»ï¼Œåœ¨ä½ è‡ªå·±çš„å·¥ä½œä¸­å°è¯•ä½¿ç”¨ GANsï¼Œçœ‹çœ‹ä½ èƒ½æƒ³å‡ºä»€ä¹ˆã€‚

æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªå…³äº GANs çš„æ–‡ç« ä¸‰éƒ¨æ›²ï¼Œå¹¶ä¸”ç°åœ¨å¯¹å®ƒä»¬æ˜¯ä»€ä¹ˆã€å®ƒä»¬èƒ½åšä»€ä¹ˆä»¥åŠå¦‚ä½•åˆ¶ä½œä½ è‡ªå·±çš„æœ‰äº†æ›´å¥½çš„äº†è§£ã€‚

æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼

## æ—¶äº‹é€šè®¯

å…³äºæ–°åšå®¢æ–‡ç« å’Œé¢å¤–å†…å®¹çš„æ›´æ–°ï¼Œè¯·æ³¨å†Œæˆ‘çš„æ—¶äº‹é€šè®¯ã€‚

[](https://mailchi.mp/6304809e49e7/matthew-stewart) [## æ—¶äº‹é€šè®¯è®¢é˜…

### ä¸°å¯Œæ‚¨çš„å­¦æœ¯ä¹‹æ—…ï¼ŒåŠ å…¥ä¸€ä¸ªç”±ç§‘å­¦å®¶ï¼Œç ”ç©¶äººå‘˜å’Œè¡Œä¸šä¸“ä¸šäººå£«ç»„æˆçš„ç¤¾åŒºï¼Œä»¥è·å¾—â€¦

mailchi.mp](https://mailchi.mp/6304809e49e7/matthew-stewart) 

# è¿›ä¸€æ­¥é˜…è¯»

**åœ¨ COLAB ä¸­è¿è¡Œ BigGAN:**

*   [https://colab . research . Google . com/github/tensor flow/hub/blob/master/examples/colab/biggan _ generation _ with _ TF _ hub . ipynb](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb)

**æ›´å¤šä»£ç å¸®åŠ©+ç¤ºä¾‹:**

*   [https://www . jessicayung . com/explaining-tensor flow-code-for-a-å·ç§¯ç¥ç»ç½‘ç»œ/](https://www.jessicayung.com/explaining-tensorflow-code-for-a-convolutional-neural-network/)
*   [https://lilian Weng . github . io/lil-log/2017/08/20/from-GAN-to-wgan . html](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)
*   [https://py torch . org/tutorials/åˆå­¦è€…/dcgan_faces_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
*   [https://github.com/tensorlayer/srgan](https://github.com/tensorlayer/srgan)
*   [https://junyanz.github.io/CycleGAN/](https://junyanz.github.io/CycleGAN/)https://affinelayer.com/pixsrv/
*   [https://tcwang0509.github.io/pix2pixHD/](https://tcwang0509.github.io/pix2pixHD/)

**æœ‰å½±å“åŠ›çš„è®ºæ–‡:**

*   https://arxiv.org/pdf/1511.06434v2.pdf
*   ç“¦ç‘Ÿæ–¯å¦ç”˜[https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf)
*   æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘(CGAN)[https://arxiv.org/pdf/1411.1784v1.pdf](https://arxiv.org/pdf/1411.1784v1.pdf)
*   ä½¿ç”¨å¯¹æŠ—ç½‘ç»œçš„æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”çš„æ·±åº¦ç”Ÿæˆå›¾åƒæ¨¡å‹(æ‹‰æ™®æ ¹)[https://arxiv.org/pdf/1506.05751.pdf](https://arxiv.org/pdf/1506.05751.pdf)
*   ä½¿ç”¨ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œ(SRGAN)çš„ç…§ç‰‡çº§å•å¹…å›¾åƒè¶…åˆ†è¾¨ç‡[https://arxiv.org/pdf/1609.04802.pdf](https://arxiv.org/pdf/1609.04802.pdf)
*   ä½¿ç”¨å¾ªç¯ä¸€è‡´å¯¹æŠ—ç½‘ç»œçš„ä¸æˆå¯¹å›¾åƒåˆ°å›¾åƒç¿»è¯‘[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)
*   InfoGAN:é€šè¿‡ä¿¡æ¯æœ€å¤§åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„å¯è§£é‡Šè¡¨ç¤ºå­¦ä¹ [https://arxiv.org/pdf/1606.03657](https://arxiv.org/pdf/1606.03657)
*   https://arxiv.org/pdf/1704.00028.pdf çš„ DCGAN
*   ç“¦ç‘Ÿæ–¯å¦Â·ç”˜æ–¯çš„å¼ºåŒ–è®­ç»ƒ(WGAN-GP)ã€https://arxiv.org/pdf/1701.07875.pdf T4
*   åŸºäºèƒ½é‡çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(EBGAN)[https://arxiv.org/pdf/1609.03126.pdf](https://arxiv.org/pdf/1609.03126.pdf)
*   ä½¿ç”¨å­¦ä¹ çš„ç›¸ä¼¼æ€§åº¦é‡(VAE-ç”˜)å¯¹åƒç´ ä¹‹å¤–çš„å†…å®¹è¿›è¡Œè‡ªåŠ¨ç¼–ç [https://arxiv.org/pdf/1512.09300.pdf](https://arxiv.org/pdf/1512.09300.pdf)
*   å¯¹æŠ—æ€§ç‰¹å¾å­¦ä¹ (https://arxiv.org/pdf/1605.09782v6.pdf[ç”˜æ¯”](https://arxiv.org/pdf/1605.09782v6.pdf)
*   å †å ç”Ÿæˆæ•Œå¯¹ç½‘ç»œ(SGAN)[https://arxiv.org/pdf/1612.04357.pdf](https://arxiv.org/pdf/1612.04357.pdf)
*   StackGAN++ä½¿ç”¨å †å å¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œè¿›è¡Œç°å®å›¾åƒåˆæˆ[https://arxiv.org/pdf/1710.10916.pdf](https://arxiv.org/pdf/1710.10916.pdf)
*   é€šè¿‡å¯¹æŠ—è®­ç»ƒ(SimGAN)ä»æ¨¡æ‹Ÿå’Œæ— ç›‘ç£å›¾åƒä¸­å­¦ä¹ [https://arxiv.org/pdf/1612.07828v1.pdf](https://arxiv.org/pdf/1612.07828v1.pdf)