<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-fine-tune-bert-with-pytorch-lightning-ba3ad2f928d2?source=collection_archive---------10-----------------------#2019-11-17">https://towardsdatascience.com/how-to-fine-tune-bert-with-pytorch-lightning-ba3ad2f928d2?source=collection_archive---------10-----------------------#2019-11-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/458612abdd93d7f19829d110f2438164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HE7fjnXaUkFFSEwU"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@rayhennessy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ray Hennessy</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a89c" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">如何用 pytorch-lightning 微调 BERT</p><p id="a40c" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">怎么了，世界！</p><p id="c13a" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">我希望您喜欢在您感兴趣的任务上微调基于 transformer 的语言模型，并获得不错的结果。</p><p id="26ca" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">我假设你们中的很多人使用 huggingface 的这个神奇的变形金刚库来微调预先训练好的语言模型。这是一个库，它允许您为 PyTorch 和 Tensorflow 使用最先进的通用(预训练)语言模型。这个库使得下载预训练模型的过程变得非常容易，另外这个库还为<a class="ae jd" href="https://github.com/huggingface/transformers/tree/master/examples" rel="noopener ugc nofollow" target="_blank">提供了一组 python 脚本</a>，所以你可以在你感兴趣的任务上对模型进行微调。</p><p id="3475" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">运行提供的脚本非常容易。然而，当我们想要对那些脚本做一些改变时，可能会变得有点棘手。</p><p id="f835" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">但是不要担心，我们很幸运，有其他惊人的库可以帮助您实现干净漂亮的脚本来进行微调。</p><p id="69ca" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated"><a class="ae jd" href="https://pytorch-lightning.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> pytorch-lightning </a>是一个轻量级 pytorch 包装器，让你从编写无聊的训练循环中解脱出来。稍后我们将在本教程中看到我们需要的最小函数。要了解这方面的细节，我会让你参考<a class="ae jd" href="https://pytorch-lightning.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank">的文档</a>。</p><p id="aea6" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">对于数据管道，我们将使用<a class="ae jd" href="https://github.com/tofunlp/lineflow" rel="noopener ugc nofollow" target="_blank"> tofunlp/lineflow </a>，这是一个用于深度学习框架的数据加载器库。这个库提供了许多函数来简化 NLP 数据集的数据处理。此外，它还为我们提供了通用的 NLP 数据集下载工具。所以我们再也不需要自己下载数据集了！</p><p id="100d" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">使用这些工具，我们将在本教程中完成以下项目。</p><ul class=""><li id="3a4b" class="kd ke jg jh b ji jj jm jn jq kf ju kg jy kh kc ki kj kk kl bi translated">预处理一个著名的复述检测数据集。</li><li id="4e14" class="kd ke jg jh b ji km jm kn jq ko ju kp jy kq kc ki kj kk kl bi translated">准备一个预先训练好的强语言模型(<a class="ae jd" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>)。</li><li id="cc7f" class="kd ke jg jh b ji km jm kn jq ko ju kp jy kq kc ki kj kk kl bi translated">最后，用<a class="ae jd" href="https://github.com/williamFalcon/pytorch-lightning" rel="noopener ugc nofollow" target="_blank"> pytorch-lightning </a>微调释义数据集上的 BERT。</li></ul><p id="bd11" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">那么让我们开始吧！</p><p id="7119" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">如果你没有时间通读这篇文章，可以直接去<a class="ae jd" href="https://github.com/sobamchan/pytorch-lightning-transformers" rel="noopener ugc nofollow" target="_blank">我的 GitHub 库</a>，克隆它，为它设置，运行它。</p></div><div class="ab cl kr ks hu kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ij ik il im in"><p id="e63b" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">首先，我们将看看我们今天正在处理的任务，<a class="ae jd" href="https://www.microsoft.com/en-us/download/details.aspx?id=52398" rel="noopener ugc nofollow" target="_blank">微软研究释义语料库</a>，一个给定两个文档的任务，要求模型预测它们是否具有相同的含义。例如，像下面这样的两个句子，“这是一个野餐的好日子！”和“在这样的日子里，我想去野餐！”，有不同的表面，但概念上的意义相同。所以我们希望我们的模型用这一对预测“真”。</p><p id="8f8b" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">要使用这个数据集，我们所要做的就是使用我上面提到的<a class="ae jd" href="https://github.com/tofunlp/lineflow#paraphrase" rel="noopener ugc nofollow" target="_blank"> lineflow </a>库。</p><figure class="ky kz la lb gt is"><div class="bz fp l di"><div class="lc ld l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Using dataset via lineflow.</figcaption></figure><p id="6f99" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">调用<em class="le"> lineflow.datasets </em>模块中的<em class="le"> MsrParphrase </em>类，它从网上下载数据并给你一个迭代器。在上面的示例中，可以看到两个句子“<em class="le"> sentence1 </em>”和“<em class="le"> sentence2 </em>”，以及<em class="le"> quality </em>(即标签)。当<em class="le">质量</em>为“1”时，这一对是意译。如果是“0”，这一对就不是意译。</p><p id="9563" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">在我们得到这个原始数据集之后，我们希望将这个数据集转换成 BERT 可以处理的格式。由于 BERT 要求文本由<a class="ae jd" href="https://en.wikipedia.org/wiki/Byte_pair_encoding" rel="noopener ugc nofollow" target="_blank"> BPE </a>处理，我们需要使用 BERT 在预训练时使用的相同的记号赋予器。不过不用担心，<a class="ae jd" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>也为它提供了简单的接口。</p><pre class="ky kz la lb gt lf lg lh li aw lj bi"><span id="ff5a" class="lk ll jg lg b gy lm ln l lo lp">from transformers import BertTokenizer</span><span id="7097" class="lk ll jg lg b gy lq ln l lo lp">tokenizer = BertTokenizer.from_pretrained("bert-base-uncased", do_lower_case=True)</span><span id="cb47" class="lk ll jg lg b gy lq ln l lo lp">text = "Hello NLP lovers!"<br/>inputs = tokenizer.encode_plus(text, add_special_tokens=True, max_length=MAX_LEN)</span><span id="c84d" class="lk ll jg lg b gy lq ln l lo lp">input_ids, token_type_ids = inputs["input_ids"], inputs["token_type_ids"]</span></pre><p id="40a6" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">通过使用<em class="le"> tokenizer 的</em> <em class="le"> encode_plus </em>函数，我们可以 1)对原始文本进行标记，2)用相应的 id 替换标记，3)为 BERT 插入特殊标记。酷！我们还可以将一对文本传递给这个函数，这样它就可以被转换成我们的任务——释义识别——的完美格式。</p><pre class="ky kz la lb gt lf lg lh li aw lj bi"><span id="8fd3" class="lk ll jg lg b gy lm ln l lo lp">sent1 = "It is an excellent day for a picnic!"<br/>sent2 = "In a day like this, I want to go for a picnic!"<br/>inputs = tokenizer.encode_plus(sent1, sent2, add_special_token=True, max_length=MAX_LEN)</span><span id="b84d" class="lk ll jg lg b gy lq ln l lo lp">input_ids, token_type_ids = inputs["input_ids"], inputs["token_type_ids"]</span></pre><p id="8333" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">现在我们知道了如何将输入字符串编码成 BERT ready 格式。接下来，让我们看看我为本文编写的实际代码。</p><figure class="ky kz la lb gt is"><div class="bz fp l di"><div class="lc ld l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Preprocessing code</figcaption></figure><p id="fb5b" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">在此，有两个主要功能。首先，<em class="le">预处理</em>:取一个数据实例，编码成 BERT 格式，填充序列。第二，<em class="le"> get_dataloader </em>:将<em class="le">预处理</em>应用于数据集中的所有实例，生成 PyTorch DataLoader。这个主旨有点长，但这只是因为我添加了一些评论行。</p></div><div class="ab cl kr ks hu kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ij ik il im in"><p id="2714" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">用<a class="ae jd" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>让你的伯特准备好非常容易。您只需要选择您想要的基于 transformer 的语言模型。</p><pre class="ky kz la lb gt lf lg lh li aw lj bi"><span id="66cc" class="lk ll jg lg b gy lm ln l lo lp">from transformers import BertForSequenceClassification</span><span id="19fd" class="lk ll jg lg b gy lq ln l lo lp">NUM_LABELS = 2  # For paraphrase identification, labels are binary, "paraphrase" or "not paraphrase".</span><span id="7a44" class="lk ll jg lg b gy lq ln l lo lp">model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=NUM_LABELS)</span></pre><p id="587d" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated"><a class="ae jd" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>提供了 BertModel，它只是一个预训练的 BERT，这里我们可以用 BertForSequenceClassification 来代替。这是一个 PyTorch 的 nn。模块类包含预训练的 BERT <strong class="jh lr">加上顶部的</strong>初始化分类层。</p><p id="29fd" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">对，就是这个！非常容易，不是吗？</p></div><div class="ab cl kr ks hu kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ij ik il im in"><p id="b490" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">我们现在已经准备好了数据和模型，让我们将它们放在一起形成 pytorch-lightning 格式，这样我们就可以轻松简单地运行微调过程。</p><p id="7bf4" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">如<a class="ae jd" href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/" rel="noopener ugc nofollow" target="_blank">官方文件</a>所示，要使用 pytorch-lightning 的 LightningModule 类，至少需要实现三种方法，1) train_dataloader，2) training_step，3)configure _ optimizer。下面我们来一个一个的检查一下这些微调的方法是怎么写的。</p><h1 id="0188" class="ls ll jg bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">train_dataloader</h1><p id="6a83" class="pw-post-body-paragraph je jf jg jh b ji mp jk jl jm mq jo jp jq mr js jt ju ms jw jx jy mt ka kb kc ij bi translated">在这个函数中，我们只需要返回我们在预处理部分实现的 pytorch 数据加载器。我将跳过这里的代码，如果你不明白，请查看<a class="ae jd" href="https://github.com/sobamchan/pytorch-lightning-transformers/blob/master/mrpc.py#L230" rel="noopener ugc nofollow" target="_blank">库</a>。</p><h1 id="e7c3" class="ls ll jg bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">训练 _ 步骤</h1><p id="93a2" class="pw-post-body-paragraph je jf jg jh b ji mp jk jl jm mq jo jp jq mr js jt ju ms jw jx jy mt ka kb kc ij bi translated">如<a class="ae jd" href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#training_step" rel="noopener ugc nofollow" target="_blank"> pytorch-lighting docs </a>所示，该函数采用我们实现的 dataloader 生成的批处理。然后，将输入批量传递给 BertForSequenceClassification 实例。因为我们通过输入传递正确的标签，所以这个模型可以返回损失值。在这种情况下，我们甚至不关心损失的计算。从模型中获得损失后，我们只需遵循<a class="ae jd" href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#training_step" rel="noopener ugc nofollow" target="_blank"> pytorch-lightning 格式</a>，创建一个包含损失的字典，用于更新模型的参数。</p><figure class="ky kz la lb gt is"><div class="bz fp l di"><div class="lc ld l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">training_step function for pytorch-lightning.LightningModule.</figcaption></figure><h1 id="66dd" class="ls ll jg bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">配置优化器</h1><p id="25fc" class="pw-post-body-paragraph je jf jg jh b ji mp jk jl jm mq jo jp jq mr js jt ju ms jw jx jy mt ka kb kc ij bi translated">这也很容易实现。在<a class="ae jd" href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#configure_optimizers" rel="noopener ugc nofollow" target="_blank">官方文档</a>中我们可以看到，我们只需要初始化 PyTorch 的优化器并返回。对于优化器的配置，我们简单地使用来自<a class="ae jd" href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py#L97" rel="noopener ugc nofollow" target="_blank"> huggingface/transformers 的示例脚本</a>的配置。实际的代码块如下所示。</p><figure class="ky kz la lb gt is"><div class="bz fp l di"><div class="lc ld l"/></div></figure></div><div class="ab cl kr ks hu kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ij ik il im in"><p id="8535" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">现在，我们一切都准备好了。对于完整的代码，你可以查看我的 GitHub 库<a class="ae jd" href="https://github.com/sobamchan/pytorch-lightning-transformers" rel="noopener ugc nofollow" target="_blank">这里</a>。在这里，实际上有两个脚本，<a class="ae jd" href="https://github.com/sobamchan/pytorch-lightning-transformers/blob/master/mrpc.py" rel="noopener ugc nofollow" target="_blank">一个</a>用于我们刚刚经历的释义检测，<a class="ae jd" href="https://github.com/sobamchan/pytorch-lightning-transformers/blob/master/csqa.py" rel="noopener ugc nofollow" target="_blank">另一个</a>用于<a class="ae jd" href="https://www.tau-nlp.org/commonsenseqa" rel="noopener ugc nofollow" target="_blank"> CommonsenseQA </a>。</p><p id="61b8" class="pw-post-body-paragraph je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ij bi translated">在本文中，我们检查如何使用<a class="ae jd" href="https://github.com/tofunlp/lineflow" rel="noopener ugc nofollow" target="_blank"> lineflow </a>下载和预处理数据集。然后，使用<a class="ae jd" href="https://github.com/williamFalcon/pytorch-lightning" rel="noopener ugc nofollow" target="_blank"> pytorch-lightning </a>微调由<a class="ae jd" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变压器</a>提供的预训练 BERT。我希望您喜欢阅读这篇文章，并实际尝试运行代码！</p></div></div>    
</body>
</html>