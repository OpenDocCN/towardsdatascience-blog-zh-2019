<html>
<head>
<title>Two Weird Ways to Regularize Your Neural Network [ Manual Back-Propagation in Tensorflow ]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">调整神经网络的两种奇怪方法 Tensorflow 中的手动反向传播]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/two-weird-ways-to-regularize-your-neural-network-manual-back-propagation-in-tensorflow-f4e63c41bd95?source=collection_archive---------27-----------------------#2019-02-04">https://towardsdatascience.com/two-weird-ways-to-regularize-your-neural-network-manual-back-propagation-in-tensorflow-f4e63c41bd95?source=collection_archive---------27-----------------------#2019-02-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ca25" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">两个奇怪的想法，实际上调整了一个神经网络。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/14157902a0e2b3a1143cc14de9824fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8SjtkPp6PLAWGnOXY-xxhg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image from this <a class="ae ky" href="https://pixabay.com/en/milky-way-universe-person-stars-1023340/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="cf28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个帖子纯粹是为了满足我的好奇心。看了多篇论文，了解到在深度神经网络的训练过程中注入噪声，会产生更好的泛化能力。(与这个概念相关的一些论文是<a class="ae ky" href="https://arxiv.org/abs/1705.07485" rel="noopener ugc nofollow" target="_blank">摇动-摇动正则化</a>和<a class="ae ky" href="https://arxiv.org/abs/1511.06807" rel="noopener ugc nofollow" target="_blank">添加梯度噪声</a>)</p><p id="40bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是有一天，我想，与其注入噪音，不如我们采样会发生什么？此外，可以使用激活函数作为正则化，旨在创建稀疏的内部数据表示？</p><p id="dd62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我感谢我的导师<a class="ae ky" href="http://www.scs.ryerson.ca/~bruce/" rel="noopener ugc nofollow" target="_blank">Bruce</a>博士的有益讨论。此外，我还要感谢我在<a class="ae ky" href="https://ryersonvisionlab.github.io/" rel="noopener ugc nofollow" target="_blank">瑞尔森视觉实验室</a>、<a class="ae ky" href="https://www.eecs.yorku.ca/~jjyu/" rel="noopener ugc nofollow" target="_blank"> Jason </a>的一名实验室成员的有益讨论。更多文章请访问我的网站，<a class="ae ky" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank"> jaedukseo.me </a>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="87b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">简介</strong></p><p id="3c4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正则化深度神经网络是一项棘手的任务，一方面，我们不希望网络简单地记住所有的输入和输出映射。而另一方面，我们希望网络达到最高的性能。</p><p id="4eb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天，我想尝试两种非常规方法，看看这些方法是否可以执行正则化。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="c15a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">方法 1:随机抽样反向传播</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/682713546e6ed2fc5f4dd7c58841368e.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*xCcjX0j26cjln4mCT3FnTA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image from this <a class="ae ky" href="https://www.alanzucconi.com/2015/09/16/how-to-sample-from-a-gaussian-distribution/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="0b6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个想法很简单，我们不是直接采用梯度，而是对高斯分布进行采样，将计算出的梯度视为该分布的平均值。</p><p id="ac71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们可以在三个不同的地方进行这种采样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi md"><img src="../Images/60e2aa1dd18a8d9b9645b030ec702115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lWf_wi2dpn3PzXRqFNwXbg.png"/></div></div></figure><p id="d636" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">蓝色球体</strong> →每层渐变<br/> <strong class="lb iu">绿色球体</strong> →权重动量<br/> <strong class="lb iu">黄色球体</strong> →权重速度<br/> <strong class="lb iu">红色球体</strong> →计算出的亚当更新</p><p id="78ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述过程显示了 Adam optimizer 如何使用梯度信息来计算权重的更新值。从上面的图像中，我们可以看到，我们可以从红色球体或蓝色球体中取样，或者从两者中取样！我对所有情况都进行了实验，所以我们将看到每种情况的表现。最后，我们可以执行采样的最后一个地方是前馈过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi me"><img src="../Images/0df8b7ea820dfe03fbd7a376927c92f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1soebMgQtyr3o2ZyrguZ8g.png"/></div></div></figure><p id="f48e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">蓝色球体</strong> →每层输入<br/> <strong class="lb iu">绿色球体</strong> →每层权重<br/> <strong class="lb iu">红色球体</strong> →每层卷积运算<br/> <strong class="lb iu">黄色球体</strong> →每层激活运算</p><p id="857c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我执行采样的下一个区域是权重本身，在训练期间，模型使用权重值，然而，在测试期间，模型执行采样以获得权重值。</p><p id="7b53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这种抽样方法的一个缺点是，我们需要选择一些标准偏差。对于所有的实验，我将标准偏差设为 0.00005。这种方法的一个优点是这种方法可以克服消失梯度，因为即使梯度为零，它也会重新采样并得到一个小的值。(布鲁斯博士已经向我提到了这个优点。).最后，合并了每种情况的卷积层可以在下面看到。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf mg l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="3a9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">方法 2:作为正则化激活</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/1961dd1adde1b7f9d29418bd94ef2702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*RZMPRrX6OBSoBpsPKyicog.png"/></div></figure><p id="6de0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">蓝色球体</strong> →卷积层<br/> <strong class="lb iu">红色球体</strong> →非线性激活</p><p id="f6f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该方法背后的思想是划分稀疏约束和特征提取部分。因此，我们不打算在权重上添加稀疏性约束，而是添加激活权重。</p><p id="30a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如何实现这一点很简单，简单地说 ReLU layer 只让正值通过。这可以被认为是一个掩码，其中这个掩码被乘以数据的内部表示。</p><p id="3144" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以通过拥有与内部数据维度完全相同的权重来放松 ReLU 激活的零和一约束，并执行元素级乘法。我们可以在这些权重上添加稀疏约束。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf mg l"/></div></figure><p id="08bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所示，我们有一个称为 ReLU 的正则化层，这个层简单地执行元素乘法。然而，在执行反向传播时，我们添加了稀疏项，最后为了实现 ReLU 的软化版本，我们应该将该值从 0 剪切到 1。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="7b84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实验设置</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl mi"><img src="../Images/bbcb078bd05ba19f5b2dad815b9d680e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*TWgff1glXxfhDa2sU6hdQg.png"/></div></figure><p id="3234" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">蓝色球体</strong> →输入图像数据<a class="ae ky" href="https://cs.stanford.edu/~acoates/stl10/" rel="noopener ugc nofollow" target="_blank"> STL10 </a> <br/> <strong class="lb iu">黄色矩形</strong> →卷积层<br/> <strong class="lb iu">红色方块</strong> →交叉熵损失项用于分类</p><p id="e2da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网络架构相当简单，六个卷积神经网络。此外，我在多集上训练了 200 个时期的网络，以研究该方法是否能重复产生稳定的结果。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="df1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果 1)随机抽样反向传播</strong></p><div class="kj kk kl km gt ab cb"><figure class="mj kn mk ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/f26a7aa538d915b2b217eff8228a555a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*fMUED9fNO4jP_73buTfeYA.png"/></div></figure><figure class="mj kn mk ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/b0d69893072ce5a4f38e263c2a4eee32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*5iDRGl8rEWVL3OYHDQpzKw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk mp di mq mr">Training Accuracy Plot (left) Testing Accuracy Plot (right) over 10 episodes</figcaption></figure></div><p id="cba0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">情况 A </strong> →基础情况无随机抽样<br/> <strong class="lb iu">情况 B </strong> →退出<br/> <strong class="lb iu">情况 C </strong> →在 Adam 更新中随机抽样<br/> <strong class="lb iu">情况 D </strong> →在 Adam 更新/梯度中随机抽样<br/> <strong class="lb iu">情况 E </strong> →在前馈期间在 Adam 更新/梯度/权重中随机抽样</p><p id="56cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">粗线代表 10 集的平均准确度，最小-最大范围以透明色显示。我们可以直接观察到情况 E 在训练和测试图像上都达到了最高的准确度。它也胜过辍学。</p><p id="486a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我们应该注意案例 C 能够跑赢其他案例的情况。当我们绘制一段时间内精度的标准偏差时，我们会得到如下图。</p><div class="kj kk kl km gt ab cb"><figure class="mj kn mk ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/d8eb63af835bfa3641a735d9ec805594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*gY9VWJKbEQ7FGurgRlYkGw.png"/></div></figure><figure class="mj kn mk ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/a6d246906141da93880788a0e294f45d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*q3hbMQFq5Zhw8wGeRIPmDA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk mp di mq mr">STD of Training Accuracy Plot (left) STD of Testing Accuracy Plot (right) over 10 episodes</figcaption></figure></div><p id="d9e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">令人惊讶的是，当我们增加随机抽样的区域时，每集之间的方差减少了，这意味着我们得到了一致的结果。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="4121" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果 2)作为正则化/随机采样反向传播的激活</strong></p><blockquote class="ms mt mu"><p id="de2c" class="kz la mv lb b lc ld ju le lf lg jx lh mw lj lk ll mx ln lo lp my lr ls lt lu im bi translated"><strong class="lb iu">对于以下所有实验，随机采样仅在 Adam 更新和梯度上执行。(案例 D)。</strong></p></blockquote><div class="kj kk kl km gt ab cb"><figure class="mj kn mz ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/4d150e6421e375cceec7a13bea089235.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*660Ps8slGyOFa5PXBL39vA.png"/></div></figure><figure class="mj kn na ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/5a012b8cff780d97c31276c7a6b8e06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*vNztDzfI-s8xiMYLxnWzCg.png"/></div></figure><figure class="mj kn nb ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/65b263917bd291e91cf73c23ca6a070b.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*86griZWsXNO-WMR3z48PJQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk nc di nd mr">Training Accuracy Plot (left) Testing Accuracy Plot (right) over 5 episodes</figcaption></figure></div><blockquote class="ms mt mu"><p id="fc9d" class="kz la mv lb b lc ld ju le lf lg jx lh mw lj lk ll mx ln lo lp my lr ls lt lu im bi translated">左边显示了所有不同的正则项及其情况。</p></blockquote><p id="abb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以注意到，每种方法都给出相似的结果。</p><div class="kj kk kl km gt ab cb"><figure class="mj kn ne ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/833f5b1478df8df13d16fdcb17038ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*545VfCTMoudSK9soFhKj5A.png"/></div></figure><figure class="mj kn nf ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/8a0568198e0c2a3b1326986f0de1bab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*EIlLJLcL4aWxharQaMJ9IA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk ng di nh mr">STD of Training Accuracy Plot (left) STD of Testing Accuracy Plot (right) over 5 episodes</figcaption></figure></div><p id="9564" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制每集的标准差变得极其混乱，对于这个实验，我不认为它增加了任何有价值的信息。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="221d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果 3)测试集的最高准确度</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1e488775244713c16b76b1d13c8795d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*HG4Y3DGxhrFZmBwIaVVX0w.png"/></div></figure><p id="0e6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，实现最高测试精度的方法是来自随机采样反向传播的情况 E。这是我们随机采样的情况，不仅是梯度，还有 Adam 更新的值和用于测试图像的过滤器的权重。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="4404" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">讨论</strong></p><p id="8138" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多名研究人员发现，注入某种随机噪声最有可能提高泛化能力，因此这多少有些出乎意料的结果。我确实想联系的一件事是，从论文“<a class="ae ky" href="https://arxiv.org/abs/1406.2572" rel="noopener ugc nofollow" target="_blank">识别和攻击高维非凸优化中的鞍点问题</a>”可以看出，鞍点是深度神经网络训练时的问题。因为随机采样反向传播总是确保到处都有一点梯度，所以它在避开那些区域方面可能非常有效。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/73243618ecc0848d7594a936f9741ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*suYr3XEjzFVgA4R-epGZZg.png"/></div></figure><p id="0780" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的下降来看，由于随机性，网络可能会逃离其中一个区域。(不是 100%确定)。</p><p id="152b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于将激活视为规则，在计划这项研究时，我认为这是一个好主意，并将提高泛化能力，但将特征选择和稀疏性约束分开似乎真的不是一个好主意。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="4478" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结论/代码</strong></p><p id="e03d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，添加随机噪声，无论是以添加还是采样的形式，似乎都有正则化的效果。此外，复合噪声因子似乎是一个好主意，这意味着在几个不同的地方添加噪声。最后，划分特征学习和稀疏性似乎不是一个好主意。(针对泛化和网络性能)。</p><p id="62df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要访问随机抽样反向传播代码，<a class="ae ky" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Class%20Stuff/Three%20Werid%20Ways/a%20things%20to%20compare%20(%20batch%20size%20100)-Copy1.ipynb" rel="noopener ugc nofollow" target="_blank">请点击此处</a>。<br/>要访问代码激活为正规化，<a class="ae ky" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Class%20Stuff/Three%20Werid%20Ways/b%20reg%20as%20activation.ipynb" rel="noopener ugc nofollow" target="_blank">请点击这里</a>。<br/>要访问创建图的代码，<a class="ae ky" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Class%20Stuff/Three%20Werid%20Ways/z%20viz.ipynb" rel="noopener ugc nofollow" target="_blank">请点击此处</a>。</p><p id="9bd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这一切都有些令人惊讶，但在我的目标范围内，没有什么接近的泛化能力。</p><blockquote class="ms mt mu"><p id="6201" class="kz la mv lb b lc ld ju le lf lg jx lh mw lj lk ll mx ln lo lp my lr ls lt lu im bi translated">最后，我想提一个事实，这些技术都没有数学证明。只有实证结果。所以请对这一切有所保留。</p></blockquote></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="a7e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考</strong></p><ol class=""><li id="bef9" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated">Pixabay 上的自由图像——银河系，宇宙，人，星星。(2019).Pixabay.com。检索于 2019 年 2 月 3 日，来自<a class="ae ky" href="https://pixabay.com/en/milky-way-universe-person-stars-1023340/" rel="noopener ugc nofollow" target="_blank">https://pix abay . com/en/milky-way-universe-person-stars-1023340/</a></li><li id="7848" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">Gastaldi，X. (2017)。抖抖正则化。arXiv.org。检索于 2019 年 2 月 3 日，来自<a class="ae ky" href="https://arxiv.org/abs/1705.07485" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1705.07485</a></li><li id="1abe" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">Neelakantan，a .，Vilnis，l .，Le，q .，Sutskever，I .，Kaiser，l .，Kurach，k .，和 Martens，J. (2015 年)。添加梯度噪声改善了对非常深的网络的学习。arXiv.org。检索于 2019 年 2 月 3 日，来自<a class="ae ky" href="https://arxiv.org/abs/1511.06807" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1511.06807</a></li><li id="742b" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">Zucconi，a .，Zucconi，a .，和 Zucconi，A. (2015 年)。如何生成高斯分布数？艾伦·祖科尼。检索于 2019 年 2 月 3 日，来自<a class="ae ky" href="https://www.alanzucconi.com/2015/09/16/how-to-sample-from-a-gaussian-distribution/" rel="noopener ugc nofollow" target="_blank">https://www . alanzucconi . com/2015/09/16/how-to-sample-from-a-Gaussian-distribution/</a></li><li id="b03f" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">RELU 市(2019)。要点。检索于 2019 年 2 月 4 日，来自<a class="ae ky" href="https://gist.github.com/JaeDukSeo/239890e40ac72705ed0c82fe25b39680" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/JaeDukSeo/239890 e 40 AC 72705 ed 0 c 82 Fe 25 b 39680</a></li><li id="d15f" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">JaeDukSeo/每日神经网络实践 2。(2019).GitHub。2019 年 2 月 4 日检索，来自<a class="ae ky" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Class%20Stuff/Three%20Werid%20Ways/a%20things%20to%20compare%20(%20batch%20size%20100)-Copy1.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Class % 20 stuff/Three % 20 werid % 20 ways/a % 20 things % 20 to % 20 compare % 20(% 20 batch % 20 size % 20100)-copy 1 . ipynb</a></li><li id="7d53" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">权重观点中的正则化[张量流中的手动反向传播]。(2019).走向数据科学。检索于 2019 年 2 月 4 日，来自<a class="ae ky" rel="noopener" target="_blank" href="/regularization-in-weights-point-of-view-manual-back-propagation-in-tensorflow-4fdc7b389257">https://towards data science . com/regulation-in-weights-point-of-view-manual-back-propagation-in-tensor flow-4 FDC 7b 389257</a></li><li id="bf40" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">道芬、y、帕斯卡努、r、古尔切雷、c、乔、k、甘古利、s 和本吉奥(2014 年)。高维非凸优化中鞍点问题的识别与攻击。arXiv.org。检索于 2019 年 2 月 4 日，来自<a class="ae ky" href="https://arxiv.org/abs/1406.2572" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1406.2572</a></li></ol></div></div>    
</body>
</html>