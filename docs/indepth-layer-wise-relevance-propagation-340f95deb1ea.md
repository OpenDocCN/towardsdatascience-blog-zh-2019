# 深度:逐层相关性传播

> 原文：<https://towardsdatascience.com/indepth-layer-wise-relevance-propagation-340f95deb1ea?source=collection_archive---------3----------------------->

## 深入 LRP 解释神经网络预测。

![](img/b8b20d215c794c285dd3c8433b9a410c.png)

Predicting 5 seems to be related to an open loop at the bottom and a straight stroke at the top.

分层相关传播(LRP)是可解释机器学习(XML)中最重要的方法之一。这篇文章将会给你一个关于 LRP 的细节和一些实现它的技巧的好主意。内容很大程度上是基于本书第章[。](https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10)

为了让你对 LRP 的潜力垂涎三尺，先来看看这个[互动演示](https://lrpserver.hhi.fraunhofer.de/image-classification)。

LRP 的目的是为任何神经网络在其输入域中的输出提供一个解释。例如，如果你的网络[从乳房 x 光片](https://news.mit.edu/2019/using-ai-predict-breast-cancer-and-personalize-care-0507)(乳房组织的图像)中预测出癌症诊断，那么 LRP 给出的解释将是原始图像中的哪些像素有助于诊断以及在多大程度上有助于诊断。这种方法不与网络的训练相互作用，因此可以很容易地将其应用于已经训练好的分类器。

XML 方法在安全关键领域特别有用，在这些领域中，从业者必须确切地知道网络在关注什么。其他用例是网络(错误)行为的诊断、科学发现和改进网络架构。你可以在 [my short primer](/why-how-interpretable-ml-7288c5aa55e4) 中阅读 XML 的高级介绍以及它为什么有用。

# 逐层相关性传播

直观地说，LRP 所做的是，它使用网络权重和前向传递创建的神经激活，通过网络将输出向上传播到输入层。在那里，我们可以看到哪些像素真正影响了输出。我们将每个像素或中间神经元的贡献大小称为“相关性”值 *R* 。

LRP 是一种*保守*技术，这意味着任何输出 *y* 的幅度在反向传播过程中都是保守的，并且等于输入层的关联图 *R* 的总和。该属性适用于任何连续的层 *j* 和 *k* ，并且适用于输入和输出层的传递性。

记住那个属性，让我们一步一步来。假设我们的网络是一个分类器，它的输出是一个大小为 *M* 的向量，其中每个条目对应于 *M* 个类中的一个。在输出层，我们选择一个我们想要解释的神经元或类。对于这个神经元，相关性等于它的激活，输出层中所有其他神经元的相关性为零。例如，如果我们想使用 LRP 找出网络神经元和输入与预测类别 *c* 的相关性，我们从类别 *c* 的输出神经元开始，只查看网络如何得出该神经元的值。

从那里开始，我们通过遵循这个基本的 LPR 规则来回溯网络:

![](img/b09f64e8ffa4bde77dad835be14ef2d7.png)

LRP-0

这里， *j* 和 *k* 是任意连续层的两个神经元。我们已经知道输出层中的相关性 *R* ，因此我们将从那里开始，并使用该公式迭代计算前一层的每个神经元的 *R* 。 *a* 表示各自神经元的激活， *w* 是两个神经元之间的权重。

这是最简单的 LRP 法则。根据您的应用程序，您可能想要使用不同的规则，这将在后面讨论。所有这些都遵循相同的基本原则。

这个公式是什么意思？分数的分子是神经元 *j* 对神经元 *k* 的影响量(对于有效 ReLU 的线性情况也是如此)。为了加强上述守恒性质，这必须除以下层所有神经元贡献的总和。我们通常在分母上加一个非常小的常数ϵ，使分数更稳定。外和超过 *k* 意味着神经元 *j* 的相关性由其对下一层所有神经元 *k* 的影响之和乘以这些神经元的相关性决定。

我们可以简单地从最后一层一直到第一层。取决于我们选择从什么输出神经元开始，我们可以得到任何类的解释。因此，我们甚至可以检查网络认为什么与预测任何类别 *A* 相关，即使它实际上预测了类别 *B* ！

可以这样分析的网络结构没有很强的限制。例如，LRP 也非常适合 CNN，而[可以用于 LSTMs](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_11) 。但是，要求网络只包含 ReLU 激活功能。

希望你已经看到，基本原理很简单。为了让你更好地理解我们应用这个公式时的过程，我将把它解释为一个 4 步的过程——遵循下表中原始作者的过程。

![](img/f8d92ac8522746d3c1fc6ef57cca1fed.png)![](img/51cbe799d1683af3508d27819f9e18b9.png)

Table from “[Layer-Wise Relevance Propagation: An Overview](https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10)”

*第一步*确定较高层中每个神经元的影响总和，类似于改进的正向传递。请注意，对于 ReLU 层，它与通过网络的正常正向传递相同，只是我们添加了一个小的常数ϵ，并在我们的权重周围包裹了一个函数ρ。这个ρ只是为了使公式更通用，包括我们后面会看到的所有可能的 LRP 规则。在我们之前的例子中，ρ是恒等函数，这意味着我们可以忽略它。

请注意，求和会在较低层的每个神经元 j 上进行，也会在偏置神经元上进行。在接下来的所有步骤中，偏差将被忽略，因为我们希望相关性只流向输入神经元，而不是在静态偏差神经元中结束。

在*的第二步*中，我们简单地用之前计算出的 *z* 值除以更高层中每个神经元的相关性。这确保守恒性质成立。

在*第三步*中，我们为前一层中的每个神经元计算一个量 *c* ，因此它可以被看作是一个反向传递。这个 *c* 可以粗略地看作有多少相关性从后续层向下流到神经元 *j* 。

最后，在*第四步*中，来自上面的相关性与神经元的激活相乘，以计算其自身的相关性。直觉上，如果 1)一个神经元具有高激活，并且 2)它对更高层的相关神经元贡献很多，则该神经元是相关的。

现在你应该已经能够自己实现 LRP 了！下一节将对此有所帮助。如果您只是想开箱即用，请注意已经有了[优秀的实现](https://github.com/albermax/innvestigate)。

# 实现技巧

当第一次介绍时，LRP 的作者提供了一个伪代码片段，它将通过网络一层一层地计算相关性，类似于我们前面看到的。(查看[这个简短的教程](http://heatmapping.org/tutorial/)来实现这个基础版本。)随着他们自己应用 LRP 的经验越来越丰富，他们出版了一个更有效的方法。

关键部分是利用高效的自动签名框架，如 pytorch 或 tensorflow 来为我们做反向传递。

![](img/fa6e7f7d6b6d90a0b23d5d84bdcca060.png)

From “[Layer-Wise Relevance Propagation: An Overview](https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10)”

与我们上面看到的公式的最大区别是，我们将计算 *c* 值作为梯度计算。

![](img/5240d2bf82123f1cd2470c4ec94209a8.png)

这允许我们利用高效的自动反向操作。为了解决这个问题并给出正确的结果，*我们必须将 s 视为常数*。这由。上面代码中的数据。

**不同的规则**

需要注意的是，LRP 有不同的传播规则，您可能希望将其中几种规则结合起来以获得最佳结果。幸运的是，这里有一个最重要的规则的简洁列表，以及何时使用它们的建议。

**输入层:**对于图像，LRP 的作者在[深度泰勒分解论文](https://www.sciencedirect.com/science/article/pii/S0031320316303582)中介绍了选择规则，并采用以下形式:

![](img/f80fca570a5d673cb97ba48d992575d2.png)

LRP-z

这里 *l* 和 *h* 分别是最低和最高的容许像素值。

**较低层:**这里我们希望解释更流畅，噪音更少，因为这些层已经非常接近我们人类将看到的并且必须理解的关联图。出于这个目的，我们可以使用 LRP-γ规则，该规则不成比例地倾向于正面证据而不是负面证据:

![](img/fe2b019c8d4d2093df732444257074b5.png)

LRP-γ

**更高层:**上面的 LRP-γ或者下面的 LRP-ϵ规则在这里都可以很好地工作。它们从相关性图中去除了一些噪声。特别是，ϵ将吸收一些小的或矛盾的证据。

![](img/79dd09b05c597806f0b6aeb44fea0114.png)

LRP-ϵ

输出层:这里我们理想地使用未修改的 LRP-0 规则。

规则甚至比这些更多，但是您在这里看到的规则对于大多数应用程序来说已经足够了。如果选择正确的规则对你来说过于复杂，不要担心！仅仅 LRP-0 和 LRP-ϵ应该可以让你走得很远，一旦你得到了这些规则的解释，你就可以决定是否花时间去试验其他的规则，让解释更漂亮。为了对不同的规则有一点直觉，我建议用互动演示[来体验一下。](https://lrpserver.hhi.fraunhofer.de/image-classification)

有这么多可能的规则也意味着你应该对 LRP 和类似技术之间的比较持保留态度，因为这些通常只根据基本的 LRP-0 规则进行。

# 应用程序

本节旨在简要给出一些成功应用 LRP 的例子。

[Lapuschkin 等人](https://arxiv.org/abs/1708.07689)利用该技术研究了从图像数据预测性别和年龄的网络。

[Thomas 等人](https://arxiv.org/abs/1810.09945)将该技术应用于大量 fMRI 神经成像数据，从 3D 数据中解释大脑状态。

[Arras 等人](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181142)将 LRP 应用于文本，研究神经网络如何将文本分类为属于一个或另一个主题。

霍斯特等人用 LRP 分析人类的步态模式。

[Srinivasan 等人](https://ieeexplore.ieee.org/document/7952445)使用 LRP 找出视频中的哪些部分被分类器用于人体动作识别。

[蒙塔冯等人在。](https://www.nature.com/articles/s41467-019-08987-4)将 LRP 应用于雅达利游戏和图像分类。

# 相关著作

既然您已经对 LRP 的内部运作有了很好的了解，那么提到相关的技术也是公平的。总括术语可解释的人工智能或可解释的人工智能可用于许多不同的技术。在这里，我将只指出一些与 LRP 密切相关的问题。

[泽勒和弗格斯](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53)在 2014 年发表了一篇开创性的论文，用去进化网络可视化神经元激活。

[Simonyan 等人](https://arxiv.org/pdf/1312.6034.pdf)在 2014 年发表了另一篇重要论文，基于泰勒分解计算显著图。

[Springenberg 等人](https://arxiv.org/abs/1412.6806)很快发表了导向反向传播。

张等()引入了激励反向传播，将反向传播视为一个概率过程。

[Ramprasaath 等人](https://arxiv.org/abs/1610.02391)为 CNN 引入了 Grad-Cam。

[Sundararajan 等人](https://arxiv.org/abs/1703.01365)介绍了积分梯度法。

在 LRP 的基础上由[巴赫等人](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140)、[蒙塔冯等人](https://www.sciencedirect.com/science/article/pii/S0031320316303582?via%3Dihub)创立了深度泰勒分解。

感谢您的阅读！希望你学到了有用的东西。