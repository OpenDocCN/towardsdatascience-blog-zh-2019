<html>
<head>
<title>An “Equation-to-Code” Machine Learning Project Walk-Through in Python— Part 1 Linear Separable Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的“等式到代码”机器学习项目演练—第 1 部分线性可分问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-equation-to-code-machine-learning-project-walk-through-in-python-part-1-linear-separable-fd0e19ed2d7?source=collection_archive---------13-----------------------#2019-06-10">https://towardsdatascience.com/an-equation-to-code-machine-learning-project-walk-through-in-python-part-1-linear-separable-fd0e19ed2d7?source=collection_archive---------13-----------------------#2019-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9372" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">数学方程式背后的详细解释，为您的机器学习或深度学习之旅奠定实用的数学基础</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/1cb250d58d84bd641b389fcb7b501d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/0*m3vW3ew-2Hk3uNYv.jpg"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Photo: Halfpoint/Shutterstock</figcaption></figure><p id="5633" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从工程师到机器学习工程师的一大差距是将数学方程转换为真实代码的能力。有时我们真的需要从头实现一些基本概念，以更好地理解幕后的魔力，而不是在没有进一步理解的情况下只导入库。</p><p id="ab1f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以我决定写一些文章来解释如何将数学方程式转换成真正的代码。这是第 1 部分，我将给出一个使用逻辑回归对一个线性可分问题进行分类的例子。我会尽可能简单地解释。</p><p id="6b03" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里是<a class="ae lq" href="https://gist.github.com/BrambleXu/738812287e7900428478c9035157db22#file-linear_data-csv" rel="noopener ugc nofollow" target="_blank">数据</a>和<a class="ae lq" href="https://gist.github.com/BrambleXu/2640af09b1f43b93c2d951ba91ca3d5c" rel="noopener ugc nofollow" target="_blank">代码</a>。</p><p id="ae6b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">内容结构如下。看起来有点长，</p><ol class=""><li id="5549" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">看数据</li><li id="7803" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">线性可分问题</li><li id="000d" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">向量表示法</li><li id="84ac" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">标准化</li><li id="73b3" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">添加偏差</li><li id="39e9" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">Sigmoid 函数</li><li id="fb0f" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">似然函数</li><li id="0b0e" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">更新参数θ</li><li id="afe3" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">绘制直线</li><li id="d52a" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">摘要</li></ol><h1 id="6dd1" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">1 看数据</h1><p id="e791" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">下面是数据，<a class="ae lq" href="https://gist.github.com/BrambleXu/738812287e7900428478c9035157db22#file-linear_data-csv" rel="noopener ugc nofollow" target="_blank"> linear_data.csv </a></p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="5f7b" class="nh mg it nd b gy ni nj l nk nl">x1,x2,y<br/>153,432,0<br/>220,262,0<br/>118,214,0<br/>474,384,1<br/>485,411,1<br/>233,430,0<br/>396,321,1<br/>484,349,1<br/>429,259,1<br/>286,220,1<br/>399,433,0<br/>403,300,1<br/>252,34,1<br/>497,372,1<br/>379,416,0<br/>76,163,0<br/>263,112,1<br/>26,193,0<br/>61,473,0<br/>420,253,1</span></pre><p id="df2b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，我们需要绘制这些数据，看看它是什么样子的。我们创建一个 Python 文件，并将其命名为 logistic_regression.py。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="95a4" class="nh mg it nd b gy ni nj l nk nl">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="5346" class="nh mg it nd b gy nm nj l nk nl"># read data<br/>data = np.loadtxt("linear_data.csv", <em class="nn">delimiter</em>=',', <em class="nn">skiprows</em>=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="0da8" class="nh mg it nd b gy nm nj l nk nl"># plot<br/>plt.plot(train_x[train_y == 1, 0], train_x[train_y == 1, 1], 'o')<br/>plt.plot(train_x[train_y == 0, 0], train_x[train_y == 0, 1], 'x')<br/>plt.show()</span></pre><p id="e63d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">运行上面的脚本后，您应该会看到下图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b629defe8bf338343cf864c526c86a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*MZpMz8qmsMZWyrwnz8poYQ.png"/></div></figure><p id="d59e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可能认为一条直线应该能很好地把 X 和 O 分开。而这是一个<a class="ae lq" href="http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html" rel="noopener ugc nofollow" target="_blank">线性可分问题</a>。</p><h1 id="906f" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">2 线性可分问题</h1><p id="b844" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们需要为这样的问题找到一个模型。最简单的情况是使用<a class="ae lq" href="https://en.wikipedia.org/wiki/Linear_function_(calculus)?oldformat=true#Properties" rel="noopener ugc nofollow" target="_blank">线性函数</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/8a7f6b94f05f89b47964230a43fb407f.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*vw5nuLbIaL11kdYkCRBHqQ.png"/></div></figure><p id="2a7d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们用θ来表示参数。左边的θ标记表示函数 f(x)有参数θ。右边的θ表示有两个参数。</p><p id="a771" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以把它写成代码</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="d931" class="nh mg it nd b gy ni nj l nk nl">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="c601" class="nh mg it nd b gy nm nj l nk nl"># read data<br/>data = np.loadtxt("linear_data.csv", <em class="nn">delimiter</em>=',', <em class="nn">skiprows</em>=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="f096" class="nh mg it nd b gy nm nj l nk nl">theta = np.random.randn(2)</span><span id="fd09" class="nh mg it nd b gy nm nj l nk nl"><strong class="nd iu">def f(x):<br/>    return theta[0] + theta[1] * x</strong></span></pre><h1 id="ee23" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">3 矢量表示法</h1><p id="f81f" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们也可以把线性函数改写成更简单的方式，向量方式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ec094bb58596f362b9205f22912ca839.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*ZbR92e2XSDsj_zlGnac9Ag.png"/></div></figure><p id="d9dd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里的θ和 x 都是列向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/24dfdfa6900a0fdf473cd4dd67c94259.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*RNofwa8uuvaR_I2rzE_XRQ.png"/></div></figure><p id="a8b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">之所以用θ的转置，是因为可以用矩阵乘法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a63d9131d91fa66e0bf7ed1aaa9174fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*my087iPrjggCjN__kKQi-g.png"/></div></figure><p id="6c4d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以写下面的代码</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="c1cc" class="nh mg it nd b gy ni nj l nk nl">import numpy as np<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="fc51" class="nh mg it nd b gy nm nj l nk nl"># read data<br/>data = np.loadtxt("linear_data.csv", <em class="nn">delimiter</em>=',', <em class="nn">skiprows</em>=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="fcd4" class="nh mg it nd b gy nm nj l nk nl"># initialize parameter<br/>theta = np.random.randn(2)</span><span id="bf8e" class="nh mg it nd b gy nm nj l nk nl"><strong class="nd iu"># dot product<br/>def f(x):<br/>    return np.dot(theta, x)</strong></span></pre><p id="080f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可能想知道为什么我们不写<code class="fe nt nu nv nd b">np.dot(theta.T, x)</code>？因为<a class="ae lq" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html" rel="noopener ugc nofollow" target="_blank">文档</a>说<strong class="kw iu">如果两个<em class="nn">向量</em>都是一维数组，那么就是向量的内积(没有复共轭)</strong>。所以<code class="fe nt nu nv nd b">np.dot(theta, x)</code>做和<code class="fe nt nu nv nd b">np.dot(theta.T, x)</code>一样的事情。</p><h1 id="1468" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">4 标准化</h1><p id="c71b" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">为了使训练快速收敛，我们使用<a class="ae lq" href="https://stats.stackexchange.com/a/10298/116970" rel="noopener ugc nofollow" target="_blank">标准化</a>，也叫<strong class="kw iu"> z </strong> - <strong class="kw iu">评分。我们是按列来做的。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/5c3f709c7b442c54fde9bf1fc63508c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*kFMKX970oeEgG6cM7pRkLg.png"/></div></figure><ul class=""><li id="596e" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp nx lx ly lz bi translated">𝜇在每一栏都很刻薄</li><li id="d12e" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp nx lx ly lz bi translated">𝜎是每列的标准偏差</li></ul><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="9a4b" class="nh mg it nd b gy ni nj l nk nl">import numpy as np<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="5318" class="nh mg it nd b gy nm nj l nk nl"># read data<br/>data = np.loadtxt("linear_data.csv", <em class="nn">delimiter</em>=',', <em class="nn">skiprows</em>=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="9351" class="nh mg it nd b gy nm nj l nk nl"># initialize parameter<br/>theta = np.random.randn(2)</span><span id="6b77" class="nh mg it nd b gy nm nj l nk nl"><strong class="nd iu"># standardization<br/>mu = train_x.mean(axis=0)<br/>sigma = train_x.std(axis=0)</strong></span><span id="9d09" class="nh mg it nd b gy nm nj l nk nl"><strong class="nd iu">def standardizer(x):<br/>    return (x - mu) / sigma<br/>std_x = standardizer(train_x)</strong></span><span id="bcb9" class="nh mg it nd b gy nm nj l nk nl"># dot product<br/>def f(x):<br/>    return np.dot(theta, x)</span></pre><h1 id="20fb" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">5 添加偏差</h1><p id="96ae" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们需要在函数中加入一个偏差项，使我们的模型具有更好的泛化能力。所以我们把参数从 2 增加到 3。并且添加常数 x0=1，以便对齐矢量表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/16fd761140a8162a584bb5b602c2e8ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*XHD87vg4hGRRUPhkez-jtg.png"/></div></figure><p id="3c58" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了使计算更简单，我们把 x 转换成矩阵。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="7c3e" class="nh mg it nd b gy ni nj l nk nl">import numpy as np<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="6387" class="nh mg it nd b gy nm nj l nk nl"># read data<br/>data = np.loadtxt("linear_data.csv", <em class="nn">delimiter</em>=',', <em class="nn">skiprows</em>=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="4ecf" class="nh mg it nd b gy nm nj l nk nl"><strong class="nd iu"># initialize parameter<br/>theta = np.random.randn(3)</strong></span><span id="ee6d" class="nh mg it nd b gy nm nj l nk nl"># standardization<br/>mu = train_x.mean(axis=0)<br/>sigma = train_x.std(axis=0)</span><span id="e4b8" class="nh mg it nd b gy nm nj l nk nl">def standardizer(x):<br/>    return (x - mu) / sigma<br/>std_x = standardizer(train_x)</span><span id="00fa" class="nh mg it nd b gy nm nj l nk nl"><strong class="nd iu"># get matrix<br/>def to_matrix(std_x):<br/>    return np.array([[1, x1, x2] for x1, x2 in std_x])<br/>mat_x = to_matrix(std_x)</strong></span><span id="d4a1" class="nh mg it nd b gy nm nj l nk nl"># dot product<br/>def f(x):<br/>    return np.dot<strong class="nd iu">(x, theta)</strong></span></pre><p id="a030" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe nt nu nv nd b">std_x</code>的尺寸为<code class="fe nt nu nv nd b">(20, 2)</code>。<code class="fe nt nu nv nd b">to_matrix(std_x)</code>之后<code class="fe nt nu nv nd b">mat_x</code>的尺寸为<code class="fe nt nu nv nd b">(20, 3)</code>。至于点积部分，注意这里我们改变了 x 和θ的位置，θ的量纲是<code class="fe nt nu nv nd b">(3,)</code>。所以点生成的结果应该是<code class="fe nt nu nv nd b">(20,3) x (3,)-&gt;(20,)</code>，这是一个包含 20 个样本预测的一维数组。</p><h1 id="8e13" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">6 Sigmoid 函数</h1><p id="9977" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">下面是我们到目前为止讲过的线性函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ec094bb58596f362b9205f22912ca839.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*ZbR92e2XSDsj_zlGnac9Ag.png"/></div></figure><p id="d1a0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">熟悉了线性函数之后。我们将在此基础上构建一个更强大的预测函数，sigmoid 函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/44394e6ff94cf4e874f2d9253a6f806f.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*DHpc4ZODxLgFnUS--g72Qw.png"/></div></figure><p id="f380" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们用 z 来表示线性函数，并将其传递给 sigmoid 函数。sigmoid 函数将给出每个数据样本的概率。我们的数据中有两个类，一个是<code class="fe nt nu nv nd b">1</code>，另一个是<code class="fe nt nu nv nd b">0</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/598ca2a33e63ef98b71a31237199f730.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*4h5rdzqS4bctbo7ixZpHfA.png"/></div></figure><p id="2f91" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到模型基于线性函数部分预测样本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/76e6302b06c1921c101316eb5574518b.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*uXvvRDXyzKCP7Oh0uRArMw.png"/></div></figure><p id="c092" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以写下面的代码</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="cd98" class="nh mg it nd b gy ni nj l nk nl">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="26e1" class="nh mg it nd b gy nm nj l nk nl"># read data<br/>data = np.loadtxt("linear_data.csv", delimiter=',', skiprows=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="cea2" class="nh mg it nd b gy nm nj l nk nl"># initialize parameter<br/>theta = np.random.randn(3)</span><span id="59e2" class="nh mg it nd b gy nm nj l nk nl"># standardization<br/>mu = train_x.mean(axis=0)<br/>sigma = train_x.std(axis=0)<br/>def standardizer(x):<br/>    return (x - mu) / sigma<br/>std_x = standardizer(train_x)</span><span id="e14b" class="nh mg it nd b gy nm nj l nk nl"># get matrix<br/>def to_matrix(std_x):<br/>    return np.array([[1, x1, x2] for x1, x2 in std_x])<br/>mat_x = to_matrix(std_x)</span><span id="9e6b" class="nh mg it nd b gy nm nj l nk nl"><strong class="nd iu"># sigmoid function<br/>def f(x):<br/>    return 1 / (1 + np.exp(-np.dot(x, theta)))</strong></span></pre><h1 id="87e4" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">7 似然函数</h1><blockquote class="oc od oe"><p id="9328" class="ku kv nn kw b kx ky ju kz la lb jx lc of le lf lg og li lj lk oh lm ln lo lp im bi translated">如果你对方程式的解释不感兴趣，你可以直接跳到第 7 步的最后一部分。</p></blockquote><p id="1358" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好了，我们准备了数据、模型(sigmoid ),还需要什么？是的，一个目标函数。<strong class="kw iu">目标函数可以指导我们如何以正确的方式更新参数。</strong>对于 sigmoid(逻辑回归)，我们通常使用<a class="ae lq" href="https://www.wikiwand.com/en/Likelihood_function#/Log-likelihood" rel="noopener ugc nofollow" target="_blank">对数似然</a>作为目标函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/3de81529a87d49a1798074dc8c709b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*B4kyEas04xPWZVarH56faA.png"/></div></figure><p id="81e0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">等等，等等…这些东西到底是怎么回事！</p><p id="ae28" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">不要慌。冷静点。</strong></p><p id="c1ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们把它拆开。</p><ul class=""><li id="f32d" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp nx lx ly lz bi translated">1-&gt;2(如何从第 1 行到第 2 行):<code class="fe nt nu nv nd b">log(ab) = log a + log b</code></li><li id="b683" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp nx lx ly lz bi translated">2-&gt;3: <code class="fe nt nu nv nd b">log(a)^b = b * log a</code></li><li id="40db" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp nx lx ly lz bi translated">3-&gt;4:由于我们只有两个类，y=0 和 y=1，所以我们可以使用下面的等式:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/7a8e5fda818e6cb17e20c614240fd8d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*6I_D7MH3ArrhI4vkdLE_GA.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">3-&gt;4</figcaption></figure><ul class=""><li id="29b8" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp nx lx ly lz bi translated">4-&gt;5:我们使用下面的变换使等式更具可读性</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/d710cf2c40d17501bede7769043057d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*bLJ9l-I2i2znZDigyve00w.png"/></div></figure><p id="5423" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以我们得到了最后一部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/203b7f77765d76082d62d215395437a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*SLJUZ9eJ5tcKzNFdPQTiiA.png"/></div></figure><p id="9e09" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">别忘了我们为什么开始这个。<strong class="kw iu">目标函数可以指导我们如何以正确的方式更新参数。</strong></p><p id="2d80" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们需要用这个来计算损耗，以更新参数。更具体地说，我们需要计算对数似然函数的<strong class="kw iu">导数</strong>。这里我直接给出最后的更新方程式。(如果你对如何得到这个方程感兴趣，这个<a class="ae lq" href="https://www.youtube.com/watch?v=SB2vz57eKgc" rel="noopener ugc nofollow" target="_blank">视频</a>应该会有帮助)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/92da534e72a3ead43346ba4017d5b243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*4SKBVfcX3OQ2uLqgnTgzTw.png"/></div></figure><p id="e778" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">第六步，最重要的方程就是这个。如果你不明白如何做到这一点，这是完全可以的。我们需要做的就是把它写成真正的代码。</strong></p><h1 id="7d02" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">8 更新参数θ</h1><p id="ea91" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">第八步稍微长一点，但是很重要。<strong class="kw iu">别慌</strong>。我们会破解它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/92da534e72a3ead43346ba4017d5b243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*4SKBVfcX3OQ2uLqgnTgzTw.png"/></div></figure><p id="cf39" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">θj 是第 j 个参数。</p><ul class=""><li id="85a4" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp nx lx ly lz bi translated">η是学习率，我们设为 0.001 (1e-3)。</li><li id="9822" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp nx lx ly lz bi translated">n 是数据样本的数量，在我们的例子中，我们有 20 个。</li><li id="9b50" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp nx lx ly lz bi translated">I 是第 I 个数据样本</li></ul><p id="16ab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为我们有三个参数，所以可以写成三个方程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/7a9e965a97b40a8ba3c400ce39ffbf6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*zSRupSQnBotCegWIB69GYA.png"/></div></figure><p id="e9bc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe nt nu nv nd b">:=</code>符号就像<code class="fe nt nu nv nd b">=</code>。你可以在这里找到解释<a class="ae lq" href="https://math.stackexchange.com/questions/25214/what-does-mean" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="ac73" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最难的部分是σ(求和符号)，所以为了更好地理解，我扩展了σ。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oo op di oq bf or"><div class="gh gi on"><img src="../Images/7a7baf204f5c9c3b1b98e1d2ee82a4c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uS9DUD32Bm4S5pnp6bKnVQ.png"/></div></div></figure><p id="a0db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">仔细看。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oo op di oq bf or"><div class="gh gi os"><img src="../Images/355c4a8cdd9a21d1188add3cdc42ffa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mDx6mDNRsLF2ozKi7zps5w.png"/></div></div></figure><p id="62f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我给等式中的三个部分涂上颜色，因为我们可以用矩阵来表示它们。看第一行红色和蓝色的部分，我们更新了θ0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oo op di oq bf or"><div class="gh gi os"><img src="../Images/db5e3e16ff2281e3fa518c2470f6e0b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*447tLj7UZp5FrtdzajAQMQ.png"/></div></div></figure><p id="336a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们把红色部分和蓝色部分写成列向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/5eba89b5540ee2ca291c4f4d071858f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*k6oWPBRvYCehNdWiuKUOGA.png"/></div></figure><p id="14d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为我们有 20 个数据样本，所以<code class="fe nt nu nv nd b">f</code>的维数是<code class="fe nt nu nv nd b">(20,1)</code>。<code class="fe nt nu nv nd b">x0</code>的尺寸为<code class="fe nt nu nv nd b">(20,1)</code>。我们可以用转置写矩阵乘法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oo op di oq bf or"><div class="gh gi ou"><img src="../Images/0cf3d8cd3910b07662571919f704f771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CeoMiq-8V77id3NmvaTQiQ.png"/></div></div></figure><p id="0e83" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以尺寸应该是<code class="fe nt nu nv nd b">(1, 20) x (20, 1) -&gt; (1,)</code>。我们得到一个标度来更新θ0。</p><p id="8b46" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe nt nu nv nd b">x1</code>和<code class="fe nt nu nv nd b">x2</code>也是列向量。我们可以把它们写成一个<strong class="kw iu"> X </strong>矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/a0f08b53e8aeba7abfd865b533cbe659.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*8V-rKp-LWaksiLxatzxoOQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/9aa651712deb36f2d95eba3ae6ab24b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*n6qCkZw-nMrwBWvqJhz4Eg.png"/></div></figure><p id="f344" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">θ是一个行向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/0b453a82676456e6ce95b9f33493b9fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*_O6aK80Nol_TKsEj5hQgmA.png"/></div></figure><p id="6dad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">回到等式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oo op di oq bf or"><div class="gh gi os"><img src="../Images/355c4a8cdd9a21d1188add3cdc42ffa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mDx6mDNRsLF2ozKi7zps5w.png"/></div></div></figure><p id="b671" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以写为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/e7d9398e8018be0e822d43685ebaa7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*8a6_K07W85lOSu8vUkektg.png"/></div></figure><p id="3327" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">写作是一个等式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/cba3402100ab21cdce3de0cf1ab08c27.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*zppuuAQ87FhuDO5hR8hCKA.png"/></div></figure><p id="c4ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">类似 Numpy 数组的版本可能容易理解。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oo op di oq bf or"><div class="gh gi oz"><img src="../Images/4d4440cecbd94b183434d49bde879e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y5IKjksRDNbsgMu8s3PO6Q.png"/></div></div></figure><p id="069b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们做一点计算，以确保尺寸是正确的。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="c770" class="nh mg it nd b gy ni nj l nk nl">θ: (1, 3) <br/>f^T: (1, 20) <br/>x: (20, 3)</span><span id="6d4c" class="nh mg it nd b gy nm nj l nk nl">dot production: (1, 20) x (20, 3) -&gt; (1, 3)</span></pre><p id="8736" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一切看起来都那么正确。让我们写代码。实际上，只有两行。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="f101" class="nh mg it nd b gy ni nj l nk nl">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="ca6e" class="nh mg it nd b gy nm nj l nk nl"># read data<br/>data = np.loadtxt("linear_data.csv", delimiter=',', skiprows=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="3c73" class="nh mg it nd b gy nm nj l nk nl"># initialize parameter<br/>theta = np.random.randn(3)</span><span id="ea7a" class="nh mg it nd b gy nm nj l nk nl"># standardization<br/>mu = train_x.mean(axis=0)<br/>sigma = train_x.std(axis=0)<br/>def standardizer(x):<br/>    return (x - mu) / sigma<br/>std_x = standardizer(train_x)</span><span id="55e0" class="nh mg it nd b gy nm nj l nk nl"># get matrix<br/>def to_matrix(std_x):<br/>    return np.array([[1, x1, x2] for x1, x2 in std_x])<br/>mat_x = to_matrix(std_x)</span><span id="fe55" class="nh mg it nd b gy nm nj l nk nl"># dot product<br/>def f(x):<br/>    return np.dot(x, theta)</span><span id="9120" class="nh mg it nd b gy nm nj l nk nl"># sigmoid function<br/>def f(x):<br/>    return 1 / (1 + np.exp(-np.dot(x, theta)))</span><span id="e563" class="nh mg it nd b gy nm nj l nk nl"># update times<br/>epoch = 2000</span><span id="e0df" class="nh mg it nd b gy nm nj l nk nl"># learning rate<br/>ETA = 1e-3</span><span id="49f7" class="nh mg it nd b gy nm nj l nk nl"># update parameter<br/><strong class="nd iu">for _ in range(epoch):<br/></strong>    """<br/>    f(mat_x) - train_y: (20,)<br/>    mat_x: (20, 3)<br/>    theta: (3,)<br/>    <br/>    dot production: (20,) x (20, 3) -&gt; (3,)<br/>    """<strong class="nd iu"><br/>    theta = theta - ETA * np.dot(f(X) - train_y, mat_x)</strong></span></pre><p id="bc42" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">奇怪的事？还记得我们在代码前写了什么吗？</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="4288" class="nh mg it nd b gy ni nj l nk nl">dot production: (1, 20) x (20, 3) -&gt; (1, 3)</span><span id="a98b" class="nh mg it nd b gy nm nj l nk nl">The dimension changes make sense here.</span></pre><p id="fdb2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是为什么我们写代码的时候要用<code class="fe nt nu nv nd b">(20,) x (20, 3) -&gt; (3,)</code>？</p><p id="729f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">实际上，这不是真正的数学符号，这是 Numpy 符号。而且如果你用的是 TensorFlow 或者 PyTroch 的话，应该很熟悉。</p><p id="fc92" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe nt nu nv nd b">(20,)</code>表示这是一个包含 20 个数字的一维数组。它可以是行向量，也可以是列向量，因为它只有一维。如果我们将其设置为二维数组，像<code class="fe nt nu nv nd b">(20, 1)</code>或<code class="fe nt nu nv nd b">(1, 20)</code>，我们可以很容易地确定<code class="fe nt nu nv nd b">(20, 1)</code>是一个列向量而<code class="fe nt nu nv nd b">(1, 20)</code>是一个行向量。</p><p id="5372" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">但是为什么不显式设置维度来消除歧义呢？</strong></p><p id="3602" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好吧。相信我，我第一次看到这个的时候就有接缝问题。但是经过一些编码实践，我想我知道原因了。</p><p id="58ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为它可以节省我们的时间！</p><p id="e937" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们以<code class="fe nt nu nv nd b">(20,) x (20, 3) -&gt; (3,)</code>为例。如果我们想得到<code class="fe nt nu nv nd b">(1, 20) x (20, 3) -&gt; (1, 3)</code>，我们需要用<code class="fe nt nu nv nd b">(20,) x (20, 3) -&gt; (3,)</code>做什么？</p><ul class=""><li id="3158" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp nx lx ly lz bi translated">将(20，)转换为(1，20)</li><li id="9023" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp nx lx ly lz bi translated">计算(1，20) x (20，3) -&gt; (1，3)</li><li id="acc9" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp nx lx ly lz bi translated">因为(1，3)是一个二维列向量，我们需要将其转换为一维数组。(1,3) -&gt; (3,)</li></ul><p id="291c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">老实说，这很令人沮丧。为什么我们不能一步到位？</p><p id="67be" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对，所以我们才能写<code class="fe nt nu nv nd b">(20,) x (20, 3) -&gt; (3,)</code>。</p><p id="d7df" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好了，我们来看看<a class="ae lq" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html" rel="noopener ugc nofollow" target="_blank"> numpy.dot() </a> doc 是怎么说的。</p><blockquote class="oc od oe"><p id="cc32" class="ku kv nn kw b kx ky ju kz la lb jx lc of le lf lg og li lj lk oh lm ln lo lp im bi translated"><a class="ae lq" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html" rel="noopener ugc nofollow" target="_blank"> numpy.dot() </a>:如果<em class="it"> a </em>是一个 N 维数组，<em class="it"> b </em>是一个 1 维数组，那么它就是<em class="it"> a </em>和<em class="it"> b </em>最后一个轴上的和积。</p></blockquote><p id="a467" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">嗯，事实上我不明白。但是<a class="ae lq" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html#numpy.matmul" rel="noopener ugc nofollow" target="_blank"> np.matmul() </a>描述了与(20，1)或(1，20)的整形类似的计算，以执行标准的 2d 矩阵乘积。也许我们能得到一些灵感。</p><blockquote class="oc od oe"><p id="ff5f" class="ku kv nn kw b kx ky ju kz la lb jx lc of le lf lg og li lj lk oh lm ln lo lp im bi translated"><a class="ae lq" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html#numpy.matmul" rel="noopener ugc nofollow" target="_blank"> np.matmul() </a>:如果第一个参数是 1-D，则通过在它的维数前加上 1 来将其提升为矩阵。在矩阵乘法之后，前置的 1 被移除。</p></blockquote><p id="bfc7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">哈，这就是缺失的部分！所以在我们的例子中，<code class="fe nt nu nv nd b">(20,)</code>变成了<code class="fe nt nu nv nd b">(1, 20)</code>，因为<code class="fe nt nu nv nd b">(20,3)</code>的第一维度是 20。还有<code class="fe nt nu nv nd b">(1, 20) * (20, 3) -&gt; (1, 3)</code>。然后前置 1 被删除，所以我们得到<code class="fe nt nu nv nd b">(3,)</code>。一步到位。</p><h1 id="05ae" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">9 画出这条线</h1><p id="7a6e" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">在更新参数 2000 次后，我们应该绘制结果来查看我们的模型的性能。</p><p id="d94b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将一些数据点做为 x1，根据我们所学的参数计算 x2。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/95ff60843ac84bb3b2f3aa7e562cb166.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*jTKtzLNkfVlZ6B2g7kCiwQ.png"/></div></figure><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="4771" class="nh mg it nd b gy ni nj l nk nl"># plot line<br/>x1 = np.linspace(-2, 2, 100)<br/><strong class="nd iu">x2 = - (theta[0] + x1 * theta[1]) / theta[2]</strong></span><span id="de5a" class="nh mg it nd b gy nm nj l nk nl">plt.plot(std_x[train_y == 1, 0], std_x[train_y == 1, 1], 'o') # train data of class 1<br/>plt.plot(std_x[train_y == 0, 0], std_x[train_y == 0, 1], 'x') # train data of class 0<br/><strong class="nd iu">plt.plot(x1, x2, linestyle='dashed') # plot the line we learned<br/></strong>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/de0927daf1dc6705710da9eaf535da4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*ufypOLQuHx6diuVew-MmrA.png"/></div></figure><h1 id="d646" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">10 摘要</h1><p id="2346" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">恭喜你！我很高兴你能来。希望我的文章对你有帮助。你可以在下面找到完整的代码。留下评论让我知道我的文章是否易懂。请继续关注我的下一篇关于非线性可分性问题的文章。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div></figure><blockquote class="oc od oe"><p id="df1e" class="ku kv nn kw b kx ky ju kz la lb jx lc of le lf lg og li lj lk oh lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">查看我的其他帖子</em> </strong> <a class="ae lq" href="https://medium.com/@bramblexu" rel="noopener"> <strong class="kw iu"> <em class="it">中等</em> </strong> </a> <strong class="kw iu"> <em class="it">同</em> </strong> <a class="ae lq" href="https://bramblexu.com/posts/eb7bd472/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> <em class="it">分类查看</em> </strong> </a> <strong class="kw iu"> <em class="it">！<br/>GitHub:</em></strong><a class="ae lq" href="https://github.com/BrambleXu" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"><em class="it">bramble Xu</em></strong></a><strong class="kw iu"><em class="it"><br/>LinkedIn:</em></strong><a class="ae lq" href="https://www.linkedin.com/in/xu-liang-99356891/" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"><em class="it">徐亮</em> </strong> </a> <strong class="kw iu"> <em class="it"> <br/>博客:</em></strong><a class="ae lq" href="https://bramblexu.com" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"><em class="it">bramble Xu</em></strong></a></p></blockquote></div></div>    
</body>
</html>