<html>
<head>
<title>Getting Started with Natural Language Processing: US Airline Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理入门:美国航空公司情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-natural-language-processing-us-airline-sentiment-analysis-24f9d8f7500d?source=collection_archive---------19-----------------------#2019-09-04">https://towardsdatascience.com/getting-started-with-natural-language-processing-us-airline-sentiment-analysis-24f9d8f7500d?source=collection_archive---------19-----------------------#2019-09-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7655b4f3fe4f8ce80b6e6336b07c0395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*P9yKKneZWxNBJPnk.png"/></div></div></figure><h1 id="5393" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">部分</h1><ol class=""><li id="ded8" class="kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">自然语言处理简介</li><li id="1253" class="kz la it lb b lc lr le ls lg lt li lu lk lv lm ln lo lp lq bi translated">数据集探索</li><li id="d105" class="kz la it lb b lc lr le ls lg lt li lu lk lv lm ln lo lp lq bi translated">自然语言处理</li><li id="c99c" class="kz la it lb b lc lr le ls lg lt li lu lk lv lm ln lo lp lq bi translated">培养</li><li id="f4c6" class="kz la it lb b lc lr le ls lg lt li lu lk lv lm ln lo lp lq bi translated">超参数优化</li><li id="9880" class="kz la it lb b lc lr le ls lg lt li lu lk lv lm ln lo lp lq bi translated">未来学习的资源</li></ol><h1 id="a156" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">自然语言处理简介</h1><p id="5994" class="pw-post-body-paragraph lw lx it lb b lc ld ly lz le lf ma mb lg mc md me li mf mg mh lk mi mj mk lm im bi translated">自然语言处理(NLP)是机器学习的一个子领域，涉及处理和分析自然语言数据，通常以文本或音频的形式。NLP 中的一些常见挑战包括语音识别、文本生成和情感分析，而一些部署 NLP 模型的高调产品包括苹果的 Siri、亚马逊的 Alexa 和许多可能会在线互动的聊天机器人。</p><p id="f97f" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">为了开始学习 NLP 并介绍该领域的一些核心概念，我们将使用流行的<a class="ae mq" href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment" rel="noopener ugc nofollow" target="_blank"> Twitter 美国航空公司情绪数据集</a>建立一个模型，试图预测与美国航空公司相关的推文的情绪(积极、中立或消极)。</p><p id="6cd5" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">代码片段将包含在这篇文章中，但是对于完全可复制的笔记本和脚本，请在其 Comet project <a class="ae mq" href="https://www.comet.ml/demo/nlp-airline/files" rel="noopener ugc nofollow" target="_blank">页面</a>上查看与该项目相关的所有笔记本和脚本。</p><h1 id="26cc" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">数据集探索</h1><p id="b753" class="pw-post-body-paragraph lw lx it lb b lc ld ly lz le lf ma mb lg mc md me li mf mg mh lk mi mj mk lm im bi translated">让我们从导入一些库开始。确保安装<a class="ae mq" href="http://comet.ml/" rel="noopener ugc nofollow" target="_blank">慧星</a>用于实验管理、可视化、代码跟踪和超参数优化。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="21b8" class="na kc it mw b gy nb nc l nd ne"># Comet<br/>from comet_ml import Experiment</span></pre><p id="57f9" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">几个标准包:pandas，numpy，matplotlib 等。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="a5c0" class="na kc it mw b gy nb nc l nd ne"># Standard packages<br/>import os<br/>import pickle<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span></pre><p id="1b03" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated"><a class="ae mq" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> Nltk </a>用于自然语言处理功能:</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="53a0" class="na kc it mw b gy nb nc l nd ne"># nltk<br/>import nltk<br/>from nltk.tokenize import sent_tokenize, word_tokenize<br/>from nltk.corpus import stopwords<br/>from nltk.stem.snowball import SnowballStemmer</span></pre><p id="1850" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated"><a class="ae mq" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank">机器学习模型的 Sklearn </a>和<a class="ae mq" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> keras </a>:</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="f65b" class="na kc it mw b gy nb nc l nd ne"># sklearn for preprocessing and machine learning models</span><span id="1f58" class="na kc it mw b gy nf nc l nd ne">from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.utils import shuffle<br/>from sklearn.preprocessing import OneHotEncoder<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="ad5d" class="na kc it mw b gy nf nc l nd ne"># Keras for neural networks<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, BatchNormalization, Flatten<br/>from keras.layers.embeddings import Embedding<br/>from keras.preprocessing import sequence<br/>from keras.utils import to_categorical<br/>from keras.callbacks import EarlyStopping</span></pre><p id="9a6b" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">现在我们将加载数据:</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="d4ce" class="na kc it mw b gy nb nc l nd ne">raw_df = pd.read_csv('twitter-airline-sentiment/Tweets.csv')</span></pre><p id="bb7c" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">让我们检查数据帧的形状:</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="16fa" class="na kc it mw b gy nb nc l nd ne">raw_df.shape()<br/>&gt;&gt;&gt; (14640, 15)</span></pre><p id="d53c" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">所以我们有 14640 个样本(推文)，每个样本有 15 个特征。让我们来看看这个数据集包含哪些要素。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="e097" class="na kc it mw b gy nb nc l nd ne">raw_df.columns</span></pre><p id="0ef7" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated"><code class="fe ng nh ni mw b">'tweet_id'</code>、<code class="fe ng nh ni mw b">'airline_sentiment'</code>、<code class="fe ng nh ni mw b">'airline_sentiment_confidence'</code>、<code class="fe ng nh ni mw b">'negativereason'</code>、<code class="fe ng nh ni mw b">'negativereason_confidence'</code>、<code class="fe ng nh ni mw b">'airline'</code>、<code class="fe ng nh ni mw b">'airline_sentiment_gold'</code>、<code class="fe ng nh ni mw b">'name'</code>、<code class="fe ng nh ni mw b">'negativereason_gold'</code>、<code class="fe ng nh ni mw b">'retweet_count'</code>、<code class="fe ng nh ni mw b">'text'</code>、<code class="fe ng nh ni mw b">'tweet_coord'</code>、<code class="fe ng nh ni mw b">'tweet_created'</code>、<code class="fe ng nh ni mw b">'tweet_location'</code>、<code class="fe ng nh ni mw b">'user_timezone'</code></p><p id="eb8e" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">让我们也来看看每个航空公司的航空公司情绪(代码可以在<a class="ae mq" href="https://www.comet.ml/demo/nlp-airline/99bcfee71c74405c84d2da1766ee4374?experiment-tab=code" rel="noopener ugc nofollow" target="_blank">彗星</a>上找到):</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="c70e" class="na kc it mw b gy nb nc l nd ne"># Create a Comet experiment to start tracking our work</span><span id="8dca" class="na kc it mw b gy nf nc l nd ne">experiment = Experiment(<br/>    api_key='&lt;HIDDEN&gt;', <br/>    project_name='nlp-airline', <br/>    workspace='demo')</span><span id="6b5f" class="na kc it mw b gy nf nc l nd ne">experiment.add_tag('plotting')</span><span id="5dd6" class="na kc it mw b gy nf nc l nd ne">airlines= ['US Airways', <br/>           'United', <br/>           'American', <br/>           'Southwest', <br/>           'Delta', <br/>           'Virgin America']</span><span id="b8e6" class="na kc it mw b gy nf nc l nd ne"><strong class="mw iu">for</strong> i <strong class="mw iu">in</strong> airlines:</span><span id="232a" class="na kc it mw b gy nf nc l nd ne">     indices = airlines.index(i)<br/>     new_df=raw_df[raw_df['airline']==i]<br/>     count=new_df['airline_sentiment'].value_counts()<br/>     experiment.log_metric('{} negative'.format(i), count[0])<br/>     experiment.log_metric('{} neutral'.format(i), count[1])<br/>     experiment.log_metric('{} positive'.format(i), count[2])<br/>experiment.end()</span></pre><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/7983abd4d3b2a8ff64e0e2b4d3d27a42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tXHIz894TQFOU6CN.jpeg"/></div></div></figure><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/9d12bd2cc531e972d1442dad12ebdafa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O30m5VxNp2RMhluR.jpeg"/></div></div></figure><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/c0b15da1c0a7ec7ee1aa08e9122b6ce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pfkWnovII714sIHJ.jpeg"/></div></div></figure><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/8942304660ef40ef8f2c7c024b287490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aUayVc2I_8TrNf1s.jpeg"/></div></div></figure><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/07d4d143061bd737b29b7d1e4b7590b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hoq2lTf04PH9wSCr.jpeg"/></div></div></figure><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/f1bcb8a09a861ea11c999e57c409d7c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*X7v3Hor4LdHJB10o.jpeg"/></div></div></figure><p id="c9b9" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">每家航空公司的负面推文都比中性或正面推文多，维珍美国航空公司在所有美国航空公司中获得了最平衡的正面、中性和负面传播。虽然在这篇文章中我们将重点放在特定于 NLP 的分析上，但是也有更深入的特性工程和探索性数据分析的极好来源。Kaggle 内核<a class="ae mq" href="https://www.kaggle.com/parthsharma5795/comprehensive-twitter-airline-sentiment-analysis" rel="noopener ugc nofollow" target="_blank">这里的</a>和<a class="ae mq" href="https://www.kaggle.com/mrisdal/exploring-audience-text-length" rel="noopener ugc nofollow" target="_blank">这里的</a>在分析与情绪相关的受众和推文长度等特征时特别有指导意义。</p><p id="41df" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">让我们创建一个只有<code class="fe ng nh ni mw b">tweet_id</code>、<code class="fe ng nh ni mw b">text</code>和<code class="fe ng nh ni mw b">airline_sentiment</code>特性的新数据帧。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="09f4" class="na kc it mw b gy nb nc l nd ne">df = raw_df[['tweet_id', 'text', 'airline_sentiment']]</span></pre><p id="3d0f" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">现在让我们来看看一些推文本身。数据是什么样的？</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="5735" class="na kc it mw b gy nb nc l nd ne">df['text'][1]</span><span id="aad9" class="na kc it mw b gy nf nc l nd ne">&gt; "<a class="ae mq" href="http://twitter.com/VirginAmerica" rel="noopener ugc nofollow" target="_blank">@VirginAmerica</a> plus you've added commercials to the experience... tacky."</span><span id="05e9" class="na kc it mw b gy nf nc l nd ne">df['text'][750]</span><span id="d03d" class="na kc it mw b gy nf nc l nd ne">&gt; "<a class="ae mq" href="http://twitter.com/united" rel="noopener ugc nofollow" target="_blank">@united</a> you are offering us 8 rooms for 32 people #FAIL"</span><span id="91e6" class="na kc it mw b gy nf nc l nd ne">df['text'][5800]</span><span id="9e65" class="na kc it mw b gy nf nc l nd ne">&gt; "<a class="ae mq" href="http://twitter.com/SouthwestAir" rel="noopener ugc nofollow" target="_blank">@SouthwestAir</a> Your #Android Wi-Fi experience is terrible! $8 is a ripoff! I can't get to <a class="ae mq" href="http://twitter.com/NASCAR" rel="noopener ugc nofollow" target="_blank">@NASCAR</a> or MRN for <a class="ae mq" href="http://twitter.com/DISupdates" rel="noopener ugc nofollow" target="_blank">@DISupdates</a> #BudweiserDuels"</span></pre><p id="63ca" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">接下来，我们将执行一些标准的 NLP 预处理技术，为训练准备好数据集。</p><h1 id="9dea" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">自然语言处理</h1><p id="835d" class="pw-post-body-paragraph lw lx it lb b lc ld ly lz le lf ma mb lg mc md me li mf mg mh lk mi mj mk lm im bi translated">为了构建自然语言处理模型，必须进行一些基本的文本预处理步骤，以便将文本从人类语言转换成机器可读的格式，以便进一步处理。这里我们将介绍一些标准实践:<em class="nk">标记化、停用词移除和词干化</em>。你可以参考这篇文章来学习额外的文本预处理技术。</p><h1 id="8e00" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">标记化</h1><p id="2365" class="pw-post-body-paragraph lw lx it lb b lc ld ly lz le lf ma mb lg mc md me li mf mg mh lk mi mj mk lm im bi translated">给定一个字符序列和一个已定义的文档单元，标记化就是将它分割成称为<em class="nk">标记</em>的离散片段的任务。在分割文本的过程中，标记化通常还包括丢弃某些字符，比如标点符号。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b8ed7f6a93265ef6da592fc1cc864e97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/0*XoPbISXyueo75xIJ.png"/></div></figure><p id="a580" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">将记号简单地认为是单词是简单的(并且经常是有用的),但是为了更好地理解 NLP 记号化的特定术语，斯坦福大学 NLP 小组的概述非常有用。</p><p id="3af3" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">NLTK 库有一个内置的<a class="ae mq" href="https://www.nltk.org/api/nltk.tokenize.html" rel="noopener ugc nofollow" target="_blank">标记器</a>，我们将使用它来标记美国航空公司的推文。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="c75a" class="na kc it mw b gy nb nc l nd ne">from nltk.tokenize import word_tokenize</span><span id="219c" class="na kc it mw b gy nf nc l nd ne">def tokenize(sentence):<br/>    tokenized_sentence = word_tokenize(sentence)<br/>    return tokenized_sentence</span></pre><h1 id="2eea" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">停用词删除</h1><p id="b308" class="pw-post-body-paragraph lw lx it lb b lc ld ly lz le lf ma mb lg mc md me li mf mg mh lk mi mj mk lm im bi translated">有时，可能对确定文档的语义质量没有什么价值的常用词被完全排除在词汇表之外。这些被称为<em class="nk">停止字</em>。确定停用词列表的一般策略是根据<em class="nk">收集频率</em>(每个术语在文档中出现的总次数)对术语进行排序，然后过滤出最频繁的术语作为停用词列表——根据语义内容进行手动过滤。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/5a974ea3a40989a204c7cac179f5cdf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/0*CsDYvwOYnJCfSjKz.png"/></div></figure><p id="0b95" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">NLTK 有一个标准的停用词表，我们将在这里采用。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="0864" class="na kc it mw b gy nb nc l nd ne">from nltk.corpus import stopwords<br/>class PreProcessor:<br/>    def __init__(self, df, column_name):<br/>        self.stopwords = set(stopwords.words('english'))</span><span id="3597" class="na kc it mw b gy nf nc l nd ne">    def remove_stopwords(self, sentence):<br/>        filtered_sentence = []<br/>        for w in sentence:<br/>            if ((w not in self.stopwords) and <br/>                (len(w) &gt; 1) and <br/>                (w[:2] != '//') and <br/>                (w != 'https')):<br/>                filtered_sentence.append(w)<br/>        return filtered_sentenceStemming</span></pre><p id="9c74" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">出于语法目的，文档使用不同形式的单词(看，看，看，看，看),这些单词在许多情况下具有非常相似的语义特征。词干化是一个粗略的过程，通过这个过程，一个单词的变体或相关形式被简化(词干化)为一个常见的基本形式。因为词干化是从单词中移除前缀或后缀字母，所以输出可能是也可能不是属于语言语料库的单词。<em class="nk">词汇化</em>是一个更精确的过程，通过这个过程，单词被适当地简化为它们所来自的基本单词。</p><p id="ac0c" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">示例:</p><p id="0ce7" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated"><strong class="lb iu"> <em class="nk">词干</em> </strong>:汽车，汽车，汽车的，汽车的’<em class="nk">变成</em>汽车</p><p id="063a" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated"><strong class="lb iu"> <em class="nk">单义化</em> </strong> : am，are is <em class="nk">成为</em> be</p><p id="2b33" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated"><strong class="lb iu"> <em class="nk">词干化单句</em> </strong>:“男孩的汽车是不同颜色的”<em class="nk">变成了</em>“男孩的汽车是不同颜色的”</p><p id="470f" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">最常见的英文文本词干算法是【波特算法】(TO DO)。<a class="ae mq" href="http://snowball.tartarus.org/texts/introduction.html" rel="noopener ugc nofollow" target="_blank"> Snowball </a>，一种用于词干算法的语言，由 Porter 在 2001 年开发，是其 SnowballStemmer 的 NLTK 实现的基础，我们将在这里使用它。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="7b91" class="na kc it mw b gy nb nc l nd ne">from nltk.stem.snowball import SnowballStemmer<br/>class PreProcessor:<br/>    <br/>    def __init__(self, df, column_name):<br/>        self.stemmer = SnowballStemmer('english')</span><span id="63d4" class="na kc it mw b gy nf nc l nd ne">    def stem(self, sentence):<br/>        return [self.stemmer.stem(word) for word in sentence]</span></pre><p id="c9e1" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">这些预处理步骤的代码可以在<a class="ae mq" href="https://www.comet.ml/demo/nlp-airline/ed77f2a005a740b09fc50f02c326f080?experiment-tab=code" rel="noopener ugc nofollow" target="_blank"> Comet </a>上找到。</p><p id="a2c5" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">接下来，我们将创建一个预处理器对象，包含每个步骤的方法，并在我们的数据框的<code class="fe ng nh ni mw b">text</code>列上运行它，以对 tweets 进行分词、词干提取和删除停用词。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="2a7c" class="na kc it mw b gy nb nc l nd ne">preprocessor = PreProcessor(df, 'text')<br/>df['cleaned text'] = preprocessor.full_preprocess()</span></pre><p id="56b2" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">现在，我们将把数据分成训练集、验证集和测试集。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="15b7" class="na kc it mw b gy nb nc l nd ne">df = shuffle(df, random_state=seed)</span><span id="893a" class="na kc it mw b gy nf nc l nd ne"><em class="nk"># Keep 1000 samples of the data as test set</em></span><span id="781a" class="na kc it mw b gy nf nc l nd ne">test_set = df[:1000]</span><span id="4794" class="na kc it mw b gy nf nc l nd ne"><em class="nk"># Get training and validation data</em></span><span id="74f1" class="na kc it mw b gy nf nc l nd ne">X_train, X_val, y_train, y_val = train_test_split(df['cleaned_text'][1000:], df['airline_sentiment'][1000:], test_size=0.2, random_state=seed)</span><span id="ec55" class="na kc it mw b gy nf nc l nd ne"><em class="nk"># Get sentiment labels for test set</em></span><span id="0151" class="na kc it mw b gy nf nc l nd ne">y_test = test_set['airline_sentiment']</span></pre><p id="e834" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">既然我们已经将数据分成了训练集、验证集和测试集，我们将对它们进行 TF-IDF 矢量化处理</p><h1 id="0551" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">TF-IDF 矢量化</h1><p id="f7a7" class="pw-post-body-paragraph lw lx it lb b lc ld ly lz le lf ma mb lg mc md me li mf mg mh lk mi mj mk lm im bi translated">TFIDF，或<em class="nk">词频——逆文档频率</em>，是一种数字统计，反映了一个词对集合或语料库中的文档有多重要。它通常用于产生与单词相关联的权重，这些权重在信息检索或文本挖掘的搜索中是有用的。单词的 tf-idf 值与单词在文档中出现的次数成比例地增加，并被包含该单词的语料库中的文档数量所抵消。这种偏移有助于调整某些词通常出现得更频繁的事实(想想看，如果没有偏移，像' a '，' the '，' to '这样的停用词可能会有非常高的 tf-idf 值)。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e42d4d72b5c025decc0f3ab17b5a89dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:60/0*CHVABA5lHJBfVbQJ"/></div></figure><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/977e66744034c0dc15dd19d50a6e36a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZFZv2LaXvLghZI9W.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Source: <a class="ae mq" href="https://becominghuman.ai/word-vectorizing-and-statistical-meaning-of-tf-idf-d45f3142be63" rel="noopener ugc nofollow" target="_blank">https://becominghuman.ai/word-vectorizing-and-statistical-meaning-of-tf-idf-d45f3142be63</a></figcaption></figure><p id="bf5f" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">我们将使用 scikit-learn 实现的<a class="ae mq" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> TfidfVectorizer </a>，它将一组原始文档(我们的 twitter 数据集)转换成一个 TF-IDF 特性矩阵。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="c61f" class="na kc it mw b gy nb nc l nd ne">vectorizer = TfidVectorizer()<br/>X_train = vectorizer.fit_transform(X_train)<br/>X_val = vectorizer.transform(X_val)<br/>X_test = vectorizer.transform(test_set['cleaned_text'])</span></pre><h1 id="5b64" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">培养</h1><p id="683d" class="pw-post-body-paragraph lw lx it lb b lc ld ly lz le lf ma mb lg mc md me li mf mg mh lk mi mj mk lm im bi translated">我们准备开始训练我们的模型。我们要做的第一件事是创建一个彗星实验对象:</p><p id="cddd" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated"><code class="fe ng nh ni mw b">experiment = Experiment(api_key='your-personal-key', project_name='nlp-airline', workspace='demo')</code></p><p id="ad20" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">接下来，我们将使用 keras 构建一个<a class="ae mq" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">光梯度增强分类器(LGBM) </a>、一个<a class="ae mq" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost 分类器</a>和一个相对简单的<a class="ae mq" href="https://keras.io/models/sequential/" rel="noopener ugc nofollow" target="_blank">神经网络，并比较这些模型的性能。通常，如果不进行测试，很难判断哪种架构的性能最好。Comet 的项目级视图有助于轻松比较不同实验的执行情况，并让您轻松地从模型选择转移到模型调整。</a></p><h1 id="10e9" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">LGBM</h1><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="1ff9" class="na kc it mw b gy nb nc l nd ne"># sklearn's Gradient Boosting Classifier (GBM)</span><span id="e337" class="na kc it mw b gy nf nc l nd ne">gbm = GradientBoostingClassifier(n_estimators=200, max_depth=6, random_state=seed)</span><span id="16f2" class="na kc it mw b gy nf nc l nd ne">gbm.fit(X_train, y_train)</span><span id="1cd3" class="na kc it mw b gy nf nc l nd ne"># Check results</span><span id="77cb" class="na kc it mw b gy nf nc l nd ne">train_pred = gbm.predict(X_train)</span><span id="198b" class="na kc it mw b gy nf nc l nd ne">val_pred = gbm.predict(X_val)</span><span id="4cfb" class="na kc it mw b gy nf nc l nd ne">val_accuracy = round(accuracy_score(y_val,val_pred), 4)</span><span id="7d87" class="na kc it mw b gy nf nc l nd ne">train_accuracy = round(accuracy_score(y_train, train_pred), 4)</span><span id="47d1" class="na kc it mw b gy nf nc l nd ne"># log to comet</span><span id="2538" class="na kc it mw b gy nf nc l nd ne">experiment.log_metric('val_acc', val_accuracy)</span><span id="c6c0" class="na kc it mw b gy nf nc l nd ne">experiment.log_metric('Accuracy', train_accuracy)XGBOOST</span></pre><h1 id="b68b" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak"> XGBoost </strong></h1><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="2e75" class="na kc it mw b gy nb nc l nd ne">xgb_params = {'objective' : 'multi:softmax',<br/>    'eval_metric' : 'mlogloss',</span><span id="4068" class="na kc it mw b gy nf nc l nd ne">    'eta' : 0.1,</span><span id="a023" class="na kc it mw b gy nf nc l nd ne">    'max_depth' : 6,</span><span id="4757" class="na kc it mw b gy nf nc l nd ne">    'num_class' : 3,</span><span id="c25c" class="na kc it mw b gy nf nc l nd ne">    'lambda' : 0.8,</span><span id="0bd4" class="na kc it mw b gy nf nc l nd ne">    'estimators' : 200,</span><span id="3fda" class="na kc it mw b gy nf nc l nd ne">    'seed' : seed</span><span id="e816" class="na kc it mw b gy nf nc l nd ne">}</span><span id="e520" class="na kc it mw b gy nf nc l nd ne">target_train = y_train.astype('category').cat.codes</span><span id="146f" class="na kc it mw b gy nf nc l nd ne">target_val = y_val.astype('category').cat.codes</span><span id="62eb" class="na kc it mw b gy nf nc l nd ne"># Transform data into a matrix so that we can use XGBoost</span><span id="38cc" class="na kc it mw b gy nf nc l nd ne">d_train = xgb.DMatrix(X_train, label = target_train)</span><span id="4d48" class="na kc it mw b gy nf nc l nd ne">d_val = xgb.DMatrix(X_val, label = target_val)</span><span id="b544" class="na kc it mw b gy nf nc l nd ne"># Fit XGBoost</span><span id="27e3" class="na kc it mw b gy nf nc l nd ne">watchlist = [(d_train, 'train'), (d_val, 'validation')]</span><span id="5cc4" class="na kc it mw b gy nf nc l nd ne">bst = xgb.train(xgb_params, d_train, 400, watchlist, </span><span id="e7cb" class="na kc it mw b gy nf nc l nd ne">early_stopping_rounds = 50, verbose_eval = 0)</span><span id="d462" class="na kc it mw b gy nf nc l nd ne"># Check results for XGBoost</span><span id="1743" class="na kc it mw b gy nf nc l nd ne">train_pred = bst.predict(d_train)</span><span id="27e6" class="na kc it mw b gy nf nc l nd ne">val_pred = bst.predict(d_val)</span><span id="216b" class="na kc it mw b gy nf nc l nd ne">experiment.log_metric('val_acc', round(accuracy_score(target_val, val_pred)*100, 4))</span><span id="6bc5" class="na kc it mw b gy nf nc l nd ne">experiment.log_metric('Accuracy', round(accuracy_score(target_train, train_pred)*100, 4))</span></pre><h1 id="d251" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">神经网络</h1><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="19d2" class="na kc it mw b gy nb nc l nd ne"><em class="nk"># Generator so we can easily feed batches of data to the neural network</em></span><span id="98d0" class="na kc it mw b gy nf nc l nd ne"><strong class="mw iu">def</strong> <strong class="mw iu">batch_generator</strong>(X, y, batch_size, shuffle):<br/>    number_of_batches = X.shape[0]/batch_size<br/>    counter = 0<br/>    sample_index = np.arange(X.shape[0])<br/>    <br/>    <strong class="mw iu">if</strong> shuffle:<br/>        np.random.shuffle(sample_index)<br/>    <strong class="mw iu">while</strong> True:<br/>        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]<br/>        X_batch = X[batch_index,:].toarray()<br/>        y_batch = y[batch_index]<br/>        counter += 1<strong class="mw iu">        yield</strong> X_batch, y_batch<br/>        <strong class="mw iu">if</strong> (counter == number_of_batches):<br/>            <strong class="mw iu">if</strong> shuffle:<br/>                np.random.shuffle(sample_index)<br/>            counter = 0</span><span id="c3c0" class="na kc it mw b gy nf nc l nd ne"><em class="nk"># Initialize sklearn's one-hot encoder class</em></span><span id="7a03" class="na kc it mw b gy nf nc l nd ne">onehot_encoder = OneHotEncoder(sparse=False)<br/>integer_encoded_train = np.array(y_train).reshape(len(y_train), 1)<br/>onehot_encoded_train = onehot_encoder.fit_transform(integer_encoded_train)<br/>integer_encoded_val = np.array(y_val).reshape(len(y_val), 1)<br/>onehot_encoded_val = onehot_encoder.fit_transform(integer_encoded_val)<br/>experiment.add_tag('NN')</span><span id="115d" class="na kc it mw b gy nf nc l nd ne"><em class="nk"># Neural network architecture</em></span><span id="d28f" class="na kc it mw b gy nf nc l nd ne">initializer = keras.initializers.he_normal(seed=seed)<br/>activation = keras.activations.elu<br/>optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-8)<br/>es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)</span><span id="9cf4" class="na kc it mw b gy nf nc l nd ne"><em class="nk"># Build model architecture</em></span><span id="09e6" class="na kc it mw b gy nf nc l nd ne">model = Sequential()<br/>model.add(Dense(20, activation=activation, kernel_initializer=initializer, input_dim=X_train.shape[1]))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(3, activation='softmax', kernel_initializer=initializer))<br/>model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])</span><span id="7eec" class="na kc it mw b gy nf nc l nd ne"><em class="nk"># Hyperparameters</em></span><span id="c10c" class="na kc it mw b gy nf nc l nd ne">epochs = 15<br/>batch_size = 32</span><span id="893e" class="na kc it mw b gy nf nc l nd ne"><em class="nk"># Fit the model using the batch_generator</em></span><span id="5380" class="na kc it mw b gy nf nc l nd ne">hist = model.fit_generator(generator=batch_generator(X_train, onehot_encoded_train, batch_size=batch_size, shuffle=True), epochs=epochs, validation_data=(X_val, onehot_encoded_val), steps_per_epoch=X_train.shape[0]/batch_size, callbacks=[es])</span></pre><p id="6726" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">使用 Comet 的 project view 比较我们的模型，我们可以看到我们的神经网络模型比 XGBoost 和 LGBM 实验表现出了相当大的优势。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/52a22b8cecacdb8f15bed289986cc5e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PGLJjba1F6qsnLGe.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk"><a class="ae mq" href="https://www.comet.ml/demo/nlp-airline/view/j1ZRx1zuXUmju7PBvRKlZEzlV" rel="noopener ugc nofollow" target="_blank">Comet Experiment List View</a></figcaption></figure><p id="35d6" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">现在让我们选择神经网络架构，并对其进行微调。<em class="nk">注意</em>，因为我们已经存储了我们所有的实验——包括我们现在不打算使用的 XGBoost 和 LGBM 运行——如果我们决定在未来重新访问这些架构，我们所要做的就是在 Comet 项目页面中查看这些实验，我们将能够立即重现它们。</p><p id="287d" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated"><strong class="lb iu">超参数优化</strong></p><p id="de5d" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">既然我们已经从 XGBoost、LGBM 和神经网络的简单 keras 实现的初始搜索中选择了我们的架构，我们将需要进行超参数优化来微调我们的模型。对于复杂的建模任务来说，超参数优化可能是一个极其困难、计算量大且缓慢的过程。Comet 已经建立了一个<a class="ae mq" href="https://www.comet.ml/docs/python-sdk/introduction-optimizer/" rel="noopener ugc nofollow" target="_blank">优化服务</a>，可以为你进行这种搜索。只需传入您想要扫描超参数空间的算法、要搜索的超参数和范围，以及最小化或最大化的度量，Comet 可以为您处理建模过程的这一部分。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="3bc0" class="na kc it mw b gy nb nc l nd ne">from comet_ml import Optimizer<br/>config = {<br/>    "algorithm": "bayes",<br/>    "parameters": {<br/>        "batch_size": {"type": "integer", "min": 16, "max": 128},<br/>        "dropout": {"type": "float", "min": 0.1, "max": 0.5},<br/>        "lr": {"type": "float", "min": 0.0001, "max": 0.001},<br/>    },<br/>    "spec": {<br/>        "metric": "loss",<br/>        "objective": "minimize",<br/>    },<br/>}<br/>opt = Optimizer(config, api_key="&lt;HIDDEN&gt;", project_name="nlp-airline", workspace="demo")<br/>for experiment in opt.get_experiments():<br/>    experiment.add_tag('LR-Optimizer')</span><span id="fdcb" class="na kc it mw b gy nf nc l nd ne">    # Neural network architecture</span><span id="0b41" class="na kc it mw b gy nf nc l nd ne">    initializer = keras.initializers.he_normal(seed=seed)   <br/>    activation = keras.activations.elu<br/>    optimizer = keras.optimizers.Adam(<br/>         lr=experiment.get_parameter("lr"), <br/>         beta_1=0.99, <br/>         beta_2=0.999, <br/>         epsilon=1e-8)</span><span id="b967" class="na kc it mw b gy nf nc l nd ne">    es = EarlyStopping(monitor='val_acc', <br/>                       mode='max', <br/>                       verbose=1, <br/>                       patience=4)<br/>    batch_size = experiment.get_parameter("batch_size")</span><span id="5cdb" class="na kc it mw b gy nf nc l nd ne">    # Build model architecture</span><span id="48ae" class="na kc it mw b gy nf nc l nd ne">    model = Sequential(# Build model like above)<br/>    score = model.evaluate(X_test, onehot_encoded_val, verbose=0)<br/>    logging.info("Score %s", score)</span></pre><p id="d541" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">运行我们的优化后，很容易选择能够产生最高精度、最低损耗或您想要优化的任何性能的超参数配置。这里我们保持优化问题相当简单:我们只搜索 epoch、batch_size 和 dropout。下面显示的平行坐标图是 Comet 的另一个固有特性，它提供了我们的优化器遍历的底层超参数空间的有用可视化:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nu"><img src="../Images/dcd40332e4d7fad37eb12441300da074.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RwDCfRXL3_l5kezf.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk"><a class="ae mq" href="https://www.comet.ml/demo/nlp-airline/view/j1ZRx1zuXUmju7PBvRKlZEzlV" rel="noopener ugc nofollow" target="_blank">Comet Visualizations Dashboard</a></figcaption></figure><p id="00db" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">让我们运行另一次优化扫描，这次包括要测试的一系列学习率。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/a2b801a268f945498317dc17c3d346bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*m4bUEfB3kYqqc4Uc.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk"><a class="ae mq" href="https://www.comet.ml/demo/nlp-airline/view/j1ZRx1zuXUmju7PBvRKlZEzlV" rel="noopener ugc nofollow" target="_blank">Comet Visualizations Dashboard</a></figcaption></figure><p id="c9d5" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">我们再次看到产生更高<code class="fe ng nh ni mw b">val_acc</code>值的潜在超参数空间的区域。</p><p id="b2eb" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">比方说，现在我们想比较两款更好的型号的性能，以保持微调。只需从您的列表中选择两个实验并单击<code class="fe ng nh ni mw b">Diff</code>按钮，Comet 将允许您直观地检查每个代码和超参数的变化，以及两个实验的并排可视化。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/43ac357bf28c655ea2211d8cb7fb6d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U53KfapPIcLgGyIz.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk"><a class="ae mq" href="https://www.comet.ml/demo/nlp-airline/258a9e3df84346e3bb503aff758cb134/ee2949dac5d74dc789103f03b986ff80/compare" rel="noopener ugc nofollow" target="_blank">Comet Experiment Diff View</a></figcaption></figure><p id="8723" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">从这里，您可以继续您的模型建设。微调我们从架构比较和参数优化扫描中抽出的一个模型，或者回到起点，将新架构与我们的基线模型进行比较。你所有的工作都保存在你的彗星项目空间。</p><h1 id="7b38" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">未来学习的资源</h1><p id="2edf" class="pw-post-body-paragraph lw lx it lb b lc ld ly lz le lf ma mb lg mc md me li mf mg mh lk mi mj mk lm im bi translated">有关 NLP 的其他学习资源，请查看 fastai 的新的<a class="ae mq" href="https://www.fast.ai/2019/07/08/fastai-nlp/" rel="noopener ugc nofollow" target="_blank"> NLP 课程</a>或由 Hugging Face 发布的这篇<a class="ae mq" href="https://medium.com/huggingface/the-best-and-most-current-of-modern-natural-language-processing-5055f409a1d1" rel="noopener">博客文章</a>，其中涵盖了 NLP 中一些最好的最新论文和趋势。</p><p id="41fa" class="pw-post-body-paragraph lw lx it lb b lc ml ly lz le mm ma mb lg mn md me li mo mg mh lk mp mj mk lm im bi translated">多亏了道格拉斯·布兰克。</p></div></div>    
</body>
</html>