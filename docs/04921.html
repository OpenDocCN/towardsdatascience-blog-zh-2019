<html>
<head>
<title>Implementation of RNN, LSTM, and GRU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RNN、LSTM 和 GRU 的实施情况</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementation-of-rnn-lstm-and-gru-a4250bf6c090?source=collection_archive---------4-----------------------#2019-07-25">https://towardsdatascience.com/implementation-of-rnn-lstm-and-gru-a4250bf6c090?source=collection_archive---------4-----------------------#2019-07-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/265f2a81b60c927bd9a24643d1409e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K6s4Li0fTl1pSX4-WPBMMA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Recurrent Neural Network</figcaption></figure><p id="4c3d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">递归神经网络是一类人工神经网络，其中节点之间的连接沿着时间序列形成有向图。与前馈神经网络不同，递归神经网络使用其内部状态存储器来处理序列。递归神经网络的这种动态行为使得它们非常有用，并且可应用于音频分析、手写识别和一些这样的应用。</p><h2 id="a80a" class="la lb iq bd lc ld le dn lf lg lh dp li kn lj lk ll kr lm ln lo kv lp lq lr ls bi translated">Keras 中简单的 RNN 实现。</h2><p id="211f" class="pw-post-body-paragraph kc kd iq ke b kf lt kh ki kj lu kl km kn lv kp kq kr lw kt ku kv lx kx ky kz ij bi translated">数学上，简单的 RNN 可以用公式表示如下:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ly"><img src="../Images/9e3eead133d19c179a863a0525f7af65.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*4nCIhMDt-MQgHfIqOQMXaQ.png"/></div></div></figure><p id="6da3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">其中 x(t)和 y(t)是输入和输出向量，Wᵢₕ、Wₕₕ和 Wₕₒ是权重矩阵，fₕ和 fₒ是隐藏和输出单元激活函数。</p><p id="235c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">具有 2 个简单 RNN 层的 RNN 的实现可以如下所示，每个层具有 32 个 RNN 单元，随后是用于 10 个类别分类的时间分布密集层:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="5d0a" class="la lb iq me b gy mi mj l mk ml">def get_model(_rnn_nb, _fc_nb):</span><span id="e188" class="la lb iq me b gy mm mj l mk ml">spec_start = Input((256,256))<br/> spec_x = spec_start<br/> for _r in _rnn_nb:<br/> spec_x = SimpleRNN(_r, activation=’tanh’, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)(spec_x)</span><span id="0033" class="la lb iq me b gy mm mj l mk ml">for _f in _fc_nb:<br/> spec_x = TimeDistributed(Dense(_f))(spec_x)<br/> spec_x = Dropout(dropout_rate)(spec_x)</span><span id="1e9b" class="la lb iq me b gy mm mj l mk ml">spec_x = TimeDistributed(Dense(10))(spec_x)<br/> out = Activation(‘sigmoid’, name=’strong_out’)(spec_x)</span><span id="a327" class="la lb iq me b gy mm mj l mk ml">_model = Model(inputs=spec_start, outputs=out)<br/> _model.compile(optimizer=’Adam’, loss=’binary_crossentropy’,metrics = [‘accuracy’])<br/> _model.summary()<br/> return _model</span></pre></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><p id="a601" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">参数:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="608d" class="la lb iq me b gy mi mj l mk ml">rnn_nb = [32, 32] # Number of RNN nodes. Length of rnn_nb = number of RNN layers<br/>fc_nb = [32] # Number of FC nodes. Length of fc_nb = number of FC layers<br/>dropout_rate = 0.5 # Dropout after each layer</span></pre><p id="c3cb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">模型总结如下:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/a2b581f55153483553ddd3c34574b92b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*fL36hbRuN7UxQt_NsdNQDw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Summary.</figcaption></figure><h2 id="067b" class="la lb iq bd lc ld le dn lf lg lh dp li kn lj lk ll kr lm ln lo kv lp lq lr ls bi translated">LSTM 在喀拉斯的实施。</h2><p id="443b" class="pw-post-body-paragraph kc kd iq ke b kf lt kh ki kj lu kl km kn lv kp kq kr lw kt ku kv lx kx ky kz ij bi translated">LSTM，也被称为长短期记忆，是一种具有反馈连接的 RNN 架构，这使得它能够执行或计算图灵机可以执行的任何事情。</p><p id="d0c8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">单个 LSTM 单元由一个单元、一个输入门、一个输出门和一个遗忘门组成，这有助于单元记忆任意时间的值。这些门控制着进出 LSTM 细胞的信息流。</p><p id="6b11" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">LSTM 单元的隐藏状态 hₜ可以计算如下:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/b4b2f1585cfd4c3466468ca4b6f1f543.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*PcX0vEEvir-zHdq5x4EPiw.png"/></div></figure><p id="3d06" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这里，<strong class="ke ir"> <em class="mw"> i，</em>f，o</strong>分别称为输入、遗忘和输出门。注意，它们具有完全相同的方程，只是具有不同的参数矩阵(<strong class="ke ir"> <em class="mw"> W </em> </strong>是在前一隐藏层和当前隐藏层的递归连接，<strong class="ke ir"> <em class="mw"> U </em> </strong>是将输入连接到当前隐藏层的权重矩阵)。</p><p id="f7ed" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">LSTM 的 Keras 实现具有 2 层 32 个 LSTM 单元，每层用于上述 10 个类别分类的任务，可以说明如下:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="abbb" class="la lb iq me b gy mi mj l mk ml">def get_model(_rnn_nb, _fc_nb):</span><span id="a588" class="la lb iq me b gy mm mj l mk ml">spec_start = Input((256,256))<br/> spec_x = spec_start<br/> for _r in _rnn_nb:<br/> spec_x = LSTM(_r, activation=’tanh’, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)(spec_x)</span><span id="4b47" class="la lb iq me b gy mm mj l mk ml">for _f in _fc_nb:<br/> spec_x = TimeDistributed(Dense(_f))(spec_x)<br/> spec_x = Dropout(dropout_rate)(spec_x)</span><span id="db1a" class="la lb iq me b gy mm mj l mk ml">spec_x = TimeDistributed(Dense(10))(spec_x)<br/> out = Activation(‘sigmoid’, name=’strong_out’)(spec_x)</span><span id="09ac" class="la lb iq me b gy mm mj l mk ml">_model = Model(inputs=spec_start, outputs=out)<br/> _model.compile(optimizer=’Adam’, loss=’binary_crossentropy’,metrics = [‘accuracy’])<br/> _model.summary()<br/> return _model</span></pre></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><p id="995b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">参数:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="1dde" class="la lb iq me b gy mi mj l mk ml">rnn_nb = [32, 32] # Number of RNN nodes. Length of rnn_nb = number of RNN layers<br/>fc_nb = [32] # Number of FC nodes. Length of fc_nb = number of FC layers<br/>dropout_rate = 0.5 # Dropout after each layer</span></pre><p id="efbc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">模型总结如下:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/a43fe1ca67b471ac0a982609aa681130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*P7NRdWciqzjifOiKpuYovg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Summary.</figcaption></figure><h2 id="de5a" class="la lb iq bd lc ld le dn lf lg lh dp li kn lj lk ll kr lm ln lo kv lp lq lr ls bi translated">GRU 在喀拉斯的实施。</h2><p id="a009" class="pw-post-body-paragraph kc kd iq ke b kf lt kh ki kj lu kl km kn lv kp kq kr lw kt ku kv lx kx ky kz ij bi translated">GRU 被称为门控循环单元，是一种 RNN 建筑，类似于 LSTM 单元。GRU 由复位门和更新门组成，而不是 LSTM 的输入、输出和遗忘门。</p><p id="c6d6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">复位门决定如何将新的输入与先前的存储器相结合，而更新门定义要保留多少先前的存储器。如果我们将 reset 设置为全 1，将 update gate 设置为全 0，我们又会得到简单的 RNN 模型。</p><p id="8159" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于 GRU，隐藏状态 hₜ可以计算如下:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6c2adbc63d7d81cd7dae42b70e57c2a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*VCgvWe30mFdSvK93p7zUJg.png"/></div></figure><p id="d21a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这里<strong class="ke ir"> <em class="mw"> r </em> </strong>是复位门，<strong class="ke ir"> <em class="mw"> z </em> </strong>是更新门。</p><p id="8eaa" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">GRU 的实施情况可以说明如下:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="234c" class="la lb iq me b gy mi mj l mk ml">def get_model(_rnn_nb, _fc_nb):</span><span id="8eb3" class="la lb iq me b gy mm mj l mk ml">spec_start = Input((256,256))<br/> spec_x = spec_start<br/> for _r in _rnn_nb:<br/> spec_x = GRU(_r, activation=’tanh’, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)(spec_x)</span><span id="2457" class="la lb iq me b gy mm mj l mk ml">for _f in _fc_nb:<br/> spec_x = TimeDistributed(Dense(_f))(spec_x)<br/> spec_x = Dropout(dropout_rate)(spec_x)</span><span id="f395" class="la lb iq me b gy mm mj l mk ml">spec_x = TimeDistributed(Dense(10))(spec_x)<br/> out = Activation(‘sigmoid’, name=’strong_out’)(spec_x)</span><span id="6a6a" class="la lb iq me b gy mm mj l mk ml">_model = Model(inputs=spec_start, outputs=out)<br/> _model.compile(optimizer=’Adam’, loss=’binary_crossentropy’,metrics = [‘accuracy’])<br/> _model.summary()<br/> return _model</span></pre></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><p id="9f35" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">参数:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="a587" class="la lb iq me b gy mi mj l mk ml">rnn_nb = [32, 32] # Number of RNN nodes. Length of rnn_nb = number of RNN layers<br/>fc_nb = [32] # Number of FC nodes. Length of fc_nb = number of FC layers<br/>dropout_rate = 0.5 # Dropout after each layer</span></pre><p id="c069" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">模型摘要:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ebea300e2e17a5aa25fbf9f9802c8ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*PE0Mehy9aZc0bSpISj5y6A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Summary.</figcaption></figure><h2 id="6e92" class="la lb iq bd lc ld le dn lf lg lh dp li kn lj lk ll kr lm ln lo kv lp lq lr ls bi translated">网络的比较。</h2><ul class=""><li id="242d" class="na nb iq ke b kf lt kj lu kn nc kr nd kv ne kz nf ng nh ni bi translated">从我的经验来看，如果你在做语言建模(不确定其他任务)，GRUs 在<strong class="ke ir">较少的训练数据</strong>上比 LSTMs 训练得更快和表现更好。</li><li id="538b" class="na nb iq ke b kf nj kj nk kn nl kr nm kv nn kz nf ng nh ni bi translated"><strong class="ke ir">gru 更简单</strong>，因此更容易修改，例如在网络有额外输入的情况下增加新的门。一般来说代码更少。</li><li id="326d" class="na nb iq ke b kf nj kj nk kn nl kr nm kv nn kz nf ng nh ni bi translated">理论上，lstm 应该<strong class="ke ir">比 GRUs 记住更长的序列</strong>，并且在需要模拟远距离关系的任务中胜过它们。</li><li id="9945" class="na nb iq ke b kf nj kj nk kn nl kr nm kv nn kz nf ng nh ni bi translated">从上面的模型总结中可以看出，gru<strong class="ke ir">的参数复杂度也比 LSTM 低。</strong></li><li id="7778" class="na nb iq ke b kf nj kj nk kn nl kr nm kv nn kz nf ng nh ni bi translated">简单的 rnn 只有简单的循环操作，没有任何门来控制信元之间的信息流。</li></ul></div></div>    
</body>
</html>