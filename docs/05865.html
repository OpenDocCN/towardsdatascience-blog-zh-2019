<html>
<head>
<title>Encode Smarter: How to Easily Integrate Categorical Encoding into Your Machine Learning Pipeline</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更聪明地编码:如何轻松地将分类编码集成到您的机器学习管道中</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/encode-smarter-how-to-easily-integrate-categorical-encoding-into-your-machine-learning-pipeline-89debe3ce24c?source=collection_archive---------26-----------------------#2019-08-26">https://towardsdatascience.com/encode-smarter-how-to-easily-integrate-categorical-encoding-into-your-machine-learning-pipeline-89debe3ce24c?source=collection_archive---------26-----------------------#2019-08-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/2ba85bb01d5049860929c173534d3e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfDSiFJCGNy0tgBOKzg6qg.png"/></div></div></figure><p id="b552" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">自动化特征工程通过简化 ML 管道中一个关键的、但手动密集的步骤，解决了应用机器学习中最大的问题之一。然而，即使在特征工程之后，处理非数字数据以供机器学习算法使用也是不可避免的，并且提出了自己的一系列独特挑战。</p><p id="d47e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这篇文章既是分类编码的指南，也是对用于自动分类编码的 Python 库的介绍，Python 库的设计考虑到了易于集成到 T2 特征工程工作流程中。</p><h1 id="aa92" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">分类编码:是什么/为什么</h1><p id="1a2c" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">当谈到自动机器学习时，传统上主要关注的是选择特定的算法或细化模型的参数，但应用机器学习的大部分工作往往源于过程中的其他步骤。例如，必须首先从原始数据中提取相关的预测变量、特征——这一步被称为<a class="ae kz" href="https://blog.featurelabs.com/secret-to-data-science-success/" rel="noopener ugc nofollow" target="_blank">特征工程，是必不可少的，但它也可能是应用 ML 过程中最需要手动操作的步骤之一。</a></p><p id="9b6e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">类似地，处理非数字数据几乎是每个机器学习过程的关键组成部分。在应用机器学习中，两种最常见的数据类型是数字数据(如年龄:5、17、35)和分类数据(如颜色:红、蓝、绿)。处理数字数据通常比处理分类数据更容易，因为机器学习算法对不知道上下文的数学向量进行操作。</p><p id="4f4d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">相比之下，许多机器学习模型无法直接处理分类数据。分类值没有任何内在的数学关系，因此我们必须先对数据做一些工作，然后才能将其输入到机器学习模型中。将分类数据转化为可用的、机器学习就绪的数学数据的过程称为分类编码。</p><h1 id="ec27" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">分类编码 API:在哪里</h1><p id="b48f" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">为了创建普遍适用于任何机器学习流水线的编码器，必须考虑需要对数据进行分类编码的三种不同情况:训练数据、测试数据和我们训练的 ML 模型将对其执行预测的新输入数据。</p><p id="ba8f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">分类编码库旨在处理所有三种情况，以便与其相邻的特征工程和机器学习模型训练步骤无缝集成。下面的流程图说明了在机器学习管道中使用分类编码的典型工作流程。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/fdecf482a766c2f84cd83b62c67ae1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xAueYPpFjMdOpf8r.png"/></div></div></figure><p id="f838" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在标准的机器学习管道中，在对数据集执行特征工程后，您会得到一个数据表，称为特征矩阵，其中包含与手头的预测问题相关的特征。在机器学习管道的这一点上，我们必须首先对我们的数据执行训练测试分割——我们需要训练数据来训练我们的模型，并测试数据来验证它。</p><p id="c9e1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面是我们代码的基本设置。你可以在这个<a class="ae kz" href="https://github.com/FeatureLabs/categorical_encoding/blob/master/guides/notebooks/categorical-encoding-DEMO.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本上获得完整的代码。</a></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1a4b" class="mn lb it mj b gy mo mp l mq mr">import categorical_encoding as ce<br/>import featuretools as ft<br/>from featuretools.tests.testing_utils import make_ecommerce_entityset</span><span id="cc2f" class="mn lb it mj b gy ms mp l mq mr">es = make_ecommerce_entityset()<br/>f1 = ft.Feature(es["log"]["product_id"])<br/>f2 = ft.Feature(es["log"]["purchased"])<br/>f3 = ft.Feature(es["log"]["value"])<br/>f4 = ft.Feature(es["log"]["countrycode"])</span><span id="8e34" class="mn lb it mj b gy ms mp l mq mr">features = [f1, f2, f3, f4]<br/>ids = [0, 1, 2, 3, 4, 5]<br/>feature_matrix = ft.calculate_feature_matrix(features, es,<br/>                                             instance_ids=ids)</span><span id="a54d" class="mn lb it mj b gy ms mp l mq mr">print(feature_matrix)</span><span id="9f18" class="mn lb it mj b gy ms mp l mq mr">product_id  purchased  value countrycode<br/>id                                          <br/>0    coke zero       True    0.0          US<br/>1    coke zero       True    5.0          US<br/>2    coke zero       True   10.0          US<br/>3          car       True   15.0          US<br/>4          car       True   20.0          US<br/>5   toothpaste       True    0.0          AL</span><span id="f11b" class="mn lb it mj b gy ms mp l mq mr">train_data = feature_matrix.iloc[[0, 1, 4, 5]]<br/>print(train_data)</span><span id="9cc6" class="mn lb it mj b gy ms mp l mq mr">product_id  purchased  value countrycode<br/>id                                          <br/>0    coke zero       True    0.0          US<br/>1    coke zero       True    5.0          US<br/>4          car       True   20.0          US<br/>5   toothpaste       True    0.0          AL</span><span id="20ae" class="mn lb it mj b gy ms mp l mq mr">test_data = feature_matrix.iloc[[2, 3]]<br/>print(test_data)</span><span id="3a2a" class="mn lb it mj b gy ms mp l mq mr">product_id  purchased  value countrycode<br/>id                                         <br/>2   coke zero       True   10.0          US<br/>3         car       True   15.0          US</span></pre><p id="dc44" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们必须分别处理这两组而不是直接编码整个特征矩阵的原因是因为在转换数据之前必须训练许多编码器。例如，目标编码器依赖于某个类别的平均值——将测试数据的值合并到我们的编码中会导致标签泄漏。</p><p id="6af0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，在用我们选择的方法初始化我们的编码器之后(见上面如何决定选择哪个编码器)，我们拟合(和转换)我们的训练数据，并且我们只转换而不拟合我们的测试数据。在我们对数据进行编码后，我们当然会得到编码后的数据，我们还会生成特征。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d46b" class="mn lb it mj b gy mo mp l mq mr">enc = ce.Encoder(method='leave_one_out')<br/><br/>train_enc = enc.fit_transform(train_data, features, train_data['value'])<br/><br/>test_enc = enc.transform(test_data)</span><span id="bbf2" class="mn lb it mj b gy ms mp l mq mr">print(train_enc)</span><span id="0892" class="mn lb it mj b gy ms mp l mq mr">PRODUCT_ID_leave_one_out  purchased  value  COUNTRYCODE_leave_one_out<br/>id                                                                       <br/>0                       5.00       True    0.0                      12.50<br/>1                       0.00       True    5.0                      10.00<br/>4                       6.25       True   20.0                       2.50<br/>5                       6.25       True    0.0                       6.25</span><span id="de72" class="mn lb it mj b gy ms mp l mq mr">print(test_enc)</span><span id="3608" class="mn lb it mj b gy ms mp l mq mr">PRODUCT_ID_leave_one_out  purchased  value  COUNTRYCODE_leave_one_out<br/>id                                                                       <br/>2                       2.50       True   10.0                   8.333333<br/>3                       6.25       True   15.0                   8.333333</span></pre><p id="0428" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在机器学习管道的最后一步，我们将获得新的数据，我们现在使用我们训练的模型来执行预测。为此，我们利用上一步生成的特征，利用 Featuretools 的<code class="fe mt mu mv mj b">calculate_feature_matrix()</code>功能立即创建编码数据——我们不必创建单独的编码器，也不必再次经历整个编码过程。现在，我们有数据可以立即应用到我们的机器学习模型中。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0ee9" class="mn lb it mj b gy mo mp l mq mr">fm2 = ft.calculate_feature_matrix(features, es, instance_ids=[6,7])<br/>print(fm2)</span><span id="6f7e" class="mn lb it mj b gy ms mp l mq mr">product_id  purchased  value countrycode<br/>id                                          <br/>6   toothpaste       True    1.0          AL<br/>7   toothpaste       True    2.0          AL</span><span id="977c" class="mn lb it mj b gy ms mp l mq mr">features_encoded = enc.get_features()<br/><br/>fm2_encoded = ft.calculate_feature_matrix(features_encoded, es, instance_ids=[6,7])<br/><br/>print(fm2_encoded)</span><span id="edcd" class="mn lb it mj b gy ms mp l mq mr">PRODUCT_ID_leave_one_out  purchased  value  COUNTRYCODE_leave_one_out<br/>id                                                                       <br/>6                       6.25       True    1.0                       6.25<br/>7                       6.25       True    2.0                       6.25</span></pre><h1 id="06c9" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">分类编码:何时/如何</h1><p id="5a05" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">分类编码最难的部分有时可能是找到正确的分类编码方法——有许多研究论文和研究致力于分析分类编码方法对不同数据集的性能。通过汇编使用相同编码方法的数据集所共有的所有公共因素，我创建了以下流程图，作为找到最适合您的数据的方法的指南。请注意，这个流程图是一个起点，您可以随意尝试不同的编码方法，甚至是这里没有列出的方法，看看哪种方法最适合您的特定数据和机器学习问题。</p><p id="af9a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">也就是说，这个流程图反映了常用分类编码方法的基本原则。此外，如果你想要一个更全面的分类编码指南，这里有一个<a class="ae kz" href="https://github.com/FeatureLabs/categorical_encoding/blob/master/guides/notebooks/categorical-encoding-guide.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>，它涵盖了每种分类编码方法及其更详细的应用场合。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/7e14bdaa8d19ce005558577e488af46a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZnwpdOcsdqzUbycG.png"/></div></div></figure><h1 id="739b" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">分类编码方法</h1><p id="e239" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">编码方法可以大致分为几类。</p><h2 id="fe5a" class="mn lb it bd lc mx my dn lg mz na dp lk km nb nc lo kq nd ne ls ku nf ng lw nh bi translated">经典编码器</h2><p id="20f2" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">经典编码器是最简单易懂的，这使得它们在 ML 从业者中非常有用和受欢迎。</p><p id="289f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果您不确定使用哪种编码方法，一键编码几乎总是一个好的起点。由于其易于使用和理解、通用性和准确性，它是分类编码的首选方法。</p><p id="62b4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">已经位于分类编码库中的经典编码器有:顺序编码器、一键编码器、二进制编码器和散列编码器。下面是代码示例，详细说明了每一个是如何实现的。如果你想继续学习，这些例子也可以在<a class="ae kz" href="https://github.com/FeatureLabs/categorical_encoding/blob/master/guides/notebooks/categorical-encoding-DEMO.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>中找到。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="fb16" class="mn lb it mj b gy mo mp l mq mr"># Creates a new column for each unique value. <br/>enc_one_hot = ce.Encoder(method='one_hot')<br/>fm_enc_one_hot = enc_one_hot.fit_transform(feature_matrix, features)<br/>print(fm_enc_one_hot)</span><span id="410c" class="mn lb it mj b gy ms mp l mq mr">product_id = coke zero  product_id = car  product_id = toothpaste  \<br/>id                                                                      <br/>0                        1                 0                        0   <br/>1                        1                 0                        0   <br/>2                        1                 0                        0   <br/>3                        0                 1                        0   <br/>4                        0                 1                        0   <br/>5                        0                 0                        1   <br/><br/>    purchased  value  countrycode = US  countrycode = AL  <br/>id                                                        <br/>0        True    0.0                 1                 0  <br/>1        True    5.0                 1                 0  <br/>2        True   10.0                 1                 0  <br/>3        True   15.0                 1                 0  <br/>4        True   20.0                 1                 0  <br/>5        True    0.0                 0                 1</span><span id="3972" class="mn lb it mj b gy ms mp l mq mr"># Each unique string value is assigned a counting number specific to that value.<br/>enc_ord = ce.Encoder(method='ordinal')<br/>fm_enc_ord = enc_ord.fit_transform(feature_matrix, features)<br/>fm_enc_ord</span><span id="4086" class="mn lb it mj b gy ms mp l mq mr">PRODUCT_ID_ordinal  purchased  value  COUNTRYCODE_ordinal<br/>id                                                           <br/>0                    1       True    0.0                    1<br/>1                    1       True    5.0                    1<br/>2                    1       True   10.0                    1<br/>3                    2       True   15.0                    1<br/>4                    2       True   20.0                    1<br/>5                    3       True    0.0                    2</span><span id="4e63" class="mn lb it mj b gy ms mp l mq mr"># The categories' values are first Ordinal encoded,<br/># the resulting integers are converted to binary,<br/># then the resulting digits are split into columns.<br/>enc_bin = ce.Encoder(method='binary')<br/>fm_enc_bin = enc_bin.fit_transform(feature_matrix, features)<br/>fm_enc_bin</span><span id="148a" class="mn lb it mj b gy ms mp l mq mr">PRODUCT_ID_binary[0]  PRODUCT_ID_binary[1]  PRODUCT_ID_binary[2]  \<br/>id                                                                     <br/>0                      0                     0                     1   <br/>1                      0                     0                     1   <br/>2                      0                     0                     1   <br/>3                      0                     1                     0   <br/>4                      0                     1                     0   <br/>5                      0                     1                     1   <br/><br/>    purchased  value  COUNTRYCODE_binary[0]  COUNTRYCODE_binary[1]  <br/>id                                                                  <br/>0        True    0.0                      0                      1  <br/>1        True    5.0                      0                      1  <br/>2        True   10.0                      0                      1  <br/>3        True   15.0                      0                      1  <br/>4        True   20.0                      0                      1  <br/>5        True    0.0                      1                      0</span><span id="e5d9" class="mn lb it mj b gy ms mp l mq mr"># Use a hashing algorithm to map category values to corresponding columns<br/>enc_hash = ce.Encoder(method='hashing')<br/>fm_enc_hash = enc_hash.fit_transform(feature_matrix, features)<br/>fm_enc_hash</span><span id="2d3a" class="mn lb it mj b gy ms mp l mq mr">PRODUCT_ID_hashing[0]  PRODUCT_ID_hashing[1]  PRODUCT_ID_hashing[2]  \<br/>id                                                                        <br/>0                       0                      0                      0   <br/>1                       0                      0                      0   <br/>2                       0                      0                      0   <br/>3                       0                      1                      0   <br/>4                       0                      1                      0   <br/>5                       0                      0                      0   <br/><br/>    PRODUCT_ID_hashing[3]  PRODUCT_ID_hashing[4]  PRODUCT_ID_hashing[5]  \<br/>id                                                                        <br/>0                       0                      1                      0   <br/>1                       0                      1                      0   <br/>2                       0                      1                      0   <br/>3                       0                      0                      0   <br/>4                       0                      0                      0   <br/>5                       1                      0                      0   <br/><br/>    PRODUCT_ID_hashing[6]  PRODUCT_ID_hashing[7]  purchased  value  \<br/>id                                                                   <br/>0                       0                      0       True    0.0   <br/>1                       0                      0       True    5.0   <br/>2                       0                      0       True   10.0   <br/>3                       0                      0       True   15.0   <br/>4                       0                      0       True   20.0   <br/>5                       0                      0       True    0.0   <br/><br/>    COUNTRYCODE_hashing[0]  COUNTRYCODE_hashing[1]  COUNTRYCODE_hashing[2]  \<br/>id                                                                           <br/>0                        0                       0                       1   <br/>1                        0                       0                       1   <br/>2                        0                       0                       1   <br/>3                        0                       0                       1   <br/>4                        0                       0                       1   <br/>5                        0                       1                       0   <br/><br/>    COUNTRYCODE_hashing[3]  COUNTRYCODE_hashing[4]  COUNTRYCODE_hashing[5]  \<br/>id                                                                           <br/>0                        0                       0                       0   <br/>1                        0                       0                       0   <br/>2                        0                       0                       0   <br/>3                        0                       0                       0   <br/>4                        0                       0                       0   <br/>5                        0                       0                       0   <br/><br/>    COUNTRYCODE_hashing[6]  COUNTRYCODE_hashing[7]  <br/>id                                                  <br/>0                        0                       0  <br/>1                        0                       0  <br/>2                        0                       0  <br/>3                        0                       0  <br/>4                        0                       0  <br/>5                        0                       0</span></pre><h2 id="a46a" class="mn lb it bd lc mx my dn lg mz na dp lk km nb nc lo kq nd ne ls ku nf ng lw nh bi translated">贝叶斯编码器</h2><p id="aa30" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">贝叶斯编码器和经典编码器之间最显著的区别在于，贝叶斯编码器除了使用分类变量之外，还使用因变量的信息。此外，它们只输出一列，这消除了有时会影响其他编码器的任何高维度问题。</p><p id="662f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">除了 One-Hot 编码，LeaveOneOut 编码是我强烈推荐的另一种分类编码方法。</p><p id="3622" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">已经在分类编码库中实现的贝叶斯编码器有:Target 和 LeaveOneOut。我在下面提供了它们的代码示例:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="c6d2" class="mn lb it mj b gy mo mp l mq mr"># Replaces each specific category value with a weighted average of the dependent variable.<br/>enc_targ = ce.Encoder(method='target')<br/>fm_enc_targ = enc_targ.fit_transform(feature_matrix, features, feature_matrix['value'])<br/>print(fm_enc_targ)</span><span id="cd4b" class="mn lb it mj b gy ms mp l mq mr">PRODUCT_ID_target  purchased  value  COUNTRYCODE_target<br/>id                                                         <br/>0            5.397343       True    0.0            9.970023<br/>1            5.397343       True    5.0            9.970023<br/>2            5.397343       True   10.0            9.970023<br/>3           15.034704       True   15.0            9.970023<br/>4           15.034704       True   20.0            9.970023<br/>5            8.333333       True    0.0            8.333333</span><span id="3b49" class="mn lb it mj b gy ms mp l mq mr"># Identical to target except leaves own row out when calculating average<br/>enc_leave = ce.Encoder(method='leave_one_out')<br/>fm_enc_leave = enc_leave.fit_transform(feature_matrix, features, feature_matrix['value'])<br/>fm_enc_leave</span><span id="64b6" class="mn lb it mj b gy ms mp l mq mr">PRODUCT_ID_leave_one_out  purchased  value  COUNTRYCODE_leave_one_out<br/>id                                                                       <br/>0                   7.500000       True    0.0                  12.500000<br/>1                   5.000000       True    5.0                  11.250000<br/>2                   2.500000       True   10.0                  10.000000<br/>3                  20.000000       True   15.0                   8.750000<br/>4                  15.000000       True   20.0                   7.500000<br/>5                   8.333333       True    0.0                   8.333333</span></pre><p id="576a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">您可能会发现，在某些情况下，您会更喜欢当前位于库中的编码器的不同实现(例如，对于我的目标编码实现，我有一个用于平均值计算的特定权重)。如果您希望进行修改，比如采用不同的权重来计算平均值，请随意创建一个新的编码器类，进行修改，并使用编码器 API 调用您的新类。</p><h2 id="4608" class="mn lb it bd lc mx my dn lg mz na dp lk km nb nc lo kq nd ne ls ku nf ng lw nh bi translated">替代编码器</h2><p id="f36f" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">第三组编码器是对比编码器——它们通过在类别中寻找数学模式来工作。您可以考虑常见的对比度编码方法，如赫尔默特、求和、后向差分或多项式编码。</p><p id="7129" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此外，还存在其他贝叶斯编码器，例如 James-Stein、M-Estimator 或证据权重。请随时尝试编码方法，找到最适合您的数据的方法。然而，如果你需要一个好的起点，看看库中已经实现的默认经典/贝叶斯编码器。One-Hot 和 LeaveOneOut 编码是最流行的编码器，这是有原因的。</p><h1 id="31a9" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">摘要</h1><p id="cdbf" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">分类编码是特征工程和机器学习中的一个重要步骤，但处理分类数据可能会很棘手且耗时。利用上述指南来确定如何开始更智能地编码，以及如何将<a class="ae kz" href="https://github.com/FeatureLabs/categorical_encoding" rel="noopener ugc nofollow" target="_blank">分类编码库</a>应用于任何<a class="ae kz" href="https://github.com/Featuretools/featuretools" rel="noopener ugc nofollow" target="_blank">功能工具</a>机器学习管道。请随时尝试多种分类编码方法 API 为您提供了灵活性，让您可以调整自己版本的通用编码器，使其最适合您的特定数据集。</p><p id="cf96" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最终，分类编码通过与特征工程以及模型创建、训练和应用的无缝集成，使您更容易处理机器学习管道中的分类数据。</p><p id="3714" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">试试分类编码库，并在<a class="ae kz" href="https://github.com/FeatureLabs/categorical_encoding" rel="noopener ugc nofollow" target="_blank"> Github </a>上留下任何反馈！我们总是欢迎任何新的编码方法实现。有关更多信息和说明，请查看<a class="ae kz" href="https://github.com/FeatureLabs/categorical_encoding/blob/master/README.md" rel="noopener ugc nofollow" target="_blank">自述文件</a>和<a class="ae kz" href="https://github.com/FeatureLabs/categorical_encoding/tree/master/guides" rel="noopener ugc nofollow" target="_blank">指南</a>。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="e782" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="np">原载于 2019 年 8 月 26 日</em><a class="ae kz" href="https://blog.featurelabs.com/encode-smarter/" rel="noopener ugc nofollow" target="_blank"><em class="np">https://blog.featurelabs.com</em></a><em class="np">。</em></p></div></div>    
</body>
</html>