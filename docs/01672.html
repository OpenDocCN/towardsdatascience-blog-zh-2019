<html>
<head>
<title>Deep learning for Arabic part-of-speech tagging</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阿拉伯语词性标注的深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-for-arabic-part-of-speech-tagging-810be7278353?source=collection_archive---------21-----------------------#2019-03-18">https://towardsdatascience.com/deep-learning-for-arabic-part-of-speech-tagging-810be7278353?source=collection_archive---------21-----------------------#2019-03-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bc29" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用 LSTM 和 Keras 构建 postagger 的一种简单方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b22f95603747cac096f36aea69c67c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yhdYZsZaaZY2tteB_rj3Hw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://pixabay.com/photos/books-students-library-university-1281581/" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><h1 id="63cc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="e401" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本帖中，我将解释<strong class="lt iu"> <em class="mn">长短期记忆网络</em> </strong>(又名。<strong class="lt iu"><em class="mn">【LSTM】</em></strong>)以及如何将其用于<strong class="lt iu"><em class="mn"/></strong>中解决<strong class="lt iu"> <em class="mn">序列建模任务</em> </strong>同时建立基于<strong class="lt iu"> <em class="mn">通用依存树库</em> </strong>的阿拉伯语词性标注器。本文是构建用于阿拉伯语自然语言处理的 python 包系列文章的一部分。你可以点击查看之前的帖子<a class="ae ky" href="https://adhaamehab.me/2019/02/01/gp-docs.html" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="f548" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">问题</h1><h2 id="b28b" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">语境无知</h2><p id="a9bf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">处理文本数据时，文本的上下文很重要，不能忽略。事实上，单词根据上下文有不同的意思。如果我们看看机器翻译的任务。上下文在这里很重要，而传统方法会忽略它。</p><p id="4ef3" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">如果我们要写一个翻译方法，把一个英语句子翻译成阿拉伯语再返回。天真的方法是从原句中提取每个单词，并将其转换为目标句。这种方法是可行的，但是它不考虑任何语法和上下文。</p><h2 id="e309" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">词性标注</h2><p id="4187" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">词性标注的任务是用标签来标记句子中的每个单词，该标签定义了该单词在该句子中的语法标注或词类消歧。</p><p id="69a7" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这里的问题是确定一个句子中一个单词的特定实例的 POS 标签。</p><p id="bb7d" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这个标签可以用来解决 NLP 中更高级的问题，比如</p><ul class=""><li id="015b" class="nf ng it lt b lu na lx nb ma nh me ni mi nj mm nk nl nm nn bi translated">语言理解，因为知道标签意味着帮助获得对文本的更好理解，因为不同的单词基于它们在句子中的位置可以具有不同的意思。</li><li id="1118" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated">文本到语音和自动语音音调控制。</li></ul><p id="1184" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">当标记单词时忽略上下文将仅导致接受的基线，因为该方法将使用与训练集中的单词相关联的最常见标签来标记每个单词。</p><p id="574c" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">因此，我们在这里试图完成的是克服这个问题，并找到一种不忽略数据背景的方法。</p><h1 id="74d7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">深度学习方法</h1><h2 id="6f33" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">序列建模和 RNNs</h2><p id="9272" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">更具体地说，我们试图解决的问题被称为序列建模。我们试图对文本或声音等连续数据进行建模，并学习如何建模。</p><p id="8010" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">序列建模或序列到序列建模首先由 Google 翻译团队引入。</p><p id="979f" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">一般来说，神经网络是模式识别模型，它通过迭代数据集来学习和增强，并在识别数据中的模式方面变得更好。</p><p id="fa98" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">递归神经网络被设计成通过使用反馈回路来防止神经网络衰退。正是这些反馈回路让 RNN 更擅长解决序列学习任务。</p><p id="c960" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">与前馈网络相比，RNNs 的工作方式更类似于人脑。因为人脑是坚持思考的。所以，当你读这篇文章的时候，每个单词都会影响你对文章的理解，你不会为了读一个新单词而抛弃以前的单词。</p><h2 id="3765" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">递归神经网络的机理</h2><p id="022e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">rnn 通过使用存储单元来处理上下文的上下文。RNN 在步骤产生的输出受该步骤输出的影响。所以总的来说，RNN 有两个输入来源。一个是实际输入，两个是来自先前输入的上下文(内存)单元。</p><p id="c5bc" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">数学上，</p><p id="7d50" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><em class="mn">前馈</em>网络由公式定义。</p><p id="147e" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">新状态是<em class="mn">权重矩阵</em>乘以<em class="mn">输入向量</em>的函数</p><p id="8bf0" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">该乘法的结果然后被传递给称为<em class="mn">激活函数</em>的方法。产生最终结果。</p><p id="9809" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">相同的过程应用于<em class="mn">循环网络</em>中，只需简单修改。</p><p id="4c8a" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">先前的状态首先乘以一个矩阵</p><p id="d716" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">名为<strong class="lt iu"> <em class="mn">的隐藏状态到隐藏状态</em> </strong>矩阵然后添加到激活函数的输入端</p><p id="fce2" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">所以不会只受之前所有隐藏状态的影响，这将确保记忆的持久性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/b2f6e36b76ce642a5391a9134791f871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z-qfB3rPDusHnUHt.png"/></div></div></figure><p id="9ee2" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">一个正常的 RNN 网络将包含一个<em class="mn">单神经网络层</em>，这使得它不能学习连接长信息。这个问题也被称为长期依赖。</p><h2 id="a9a1" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">长短期记忆结构</h2><p id="5856" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">rnn 很棒。他们在解决 NLP 中的许多任务时帮了大忙。然而，他们有长期依赖的问题。</p><blockquote class="nu nv nw"><p id="7c4d" class="lr ls mn lt b lu na ju lw lx nb jx lz nx nc mc md ny nd mg mh nz ne mk ml mm im bi translated">你可以在 Colah 的博客文章<a class="ae ky" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">中读到更多关于 LTD 的信息</a></p></blockquote><p id="cc67" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">LSTMs 通过比 RNN 记忆信息更长的时间来解决 LTD 问题。</p><p id="2c0e" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">RNN 和 LSTM 的唯一区别是 RNN 没有单一的神经网络层。我们在 LSTM 有 4 层以一种特殊的方式相互作用。</p><h2 id="49d7" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">循序渐进的 LSTM</h2><p id="473f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">LSTM 层由一系列单元状态组成，其中每个状态由 4 个主层和 3 个栅极组成。</p><p id="d2ae" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">现在让我们一步一步地走过一个 LSTM 细胞状态</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/c891c7a71ef84056f49a1ec786b97524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SaSJSbs8GWJna62b.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: Google photos</figcaption></figure><p id="2946" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">一个<em class="mn">单元状态</em>的核心是连接和之间的水平线。这一行是数据流抛出<em class="mn">单元状态链</em>的地方。数据很容易以最小的线性运算或不变的方式流动，整个过程由<em class="mn">门</em>控制。</p><p id="6587" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><em class="mn">门</em>是控制数据变化的。他们用一个<em class="mn"> sigmoid 神经层</em>和一个<em class="mn">向量</em> <em class="mn">乘法运算</em>可选地改变数据。</p><p id="32ad" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><em class="mn">Sigmoid layer</em>是一种生成浮点值的方法，该值控制有多少数据将通过 gate。表示<em class="mn">无</em>而$ 1 $表示<em class="mn">全部</em>。</p><p id="c2c1" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">第一步是<em class="mn"> sigmoid 层</em>决定从乘以的结果中传递什么信息。我们可以用数学方法表示这种操作，如下所示:</p><p id="8c07" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">前一步确实决定了哪些数据将被遗忘，哪些数据将被保留。第二步，我们需要真正做到这一点。</p><p id="4a32" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">我们将之前的结果乘以旧状态。</p><p id="8d45" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">使用<em class="mn"> Tanh </em>激活功能计算旧状态。用∑乘以∑将决定网络忘记了哪些信息，得到了哪些信息。数学上:</p><p id="660c" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">最后，我们需要决定的价值是什么</p><p id="5e6f" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这些操作顺序应用于单元状态链。查看 LSTM 的数学模型可能会令人望而生畏，因此我们将转到应用部分，并使用 Keras 为阿拉伯语实现一个 LSTM 模型。</p><h1 id="79c8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">构建阿拉伯语词性标记器</h1><p id="988a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如我们所知，我们走过了序列建模的深度学习方法背后的想法。我们将应用它来构建一个阿拉伯语词性标记器。</p><p id="2a2d" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">对于英语来说，词性标注是一个已经解决的问题。像阿拉伯语这样的形态学语言。这个问题仍然存在，并且没有任何基于深度学习的开源阿拉伯语词性标注器。我们现在的目标是使用关于 LSTMs 的知识，并构建一个开源标记器。</p><h2 id="2e72" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">数据</h2><p id="2602" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">缺少开源 tagger 是缺少阿拉伯语树库数据集的明显结果。</p><p id="aff3" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">众所周知的<strong class="lt iu"> Penn TreeBank </strong>成本约为<strong class="lt iu">3000 美元</strong>而<strong class="lt iu">Quran tree bank</strong>非常经典，在日常词汇上表现不佳。</p><p id="0310" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><a class="ae ky" href="https://universaldependencies.org/" rel="noopener ugc nofollow" target="_blank">Universal Dependencies</a>(UD)是一个跨语言一致的语法注释框架，是一个开放的社区，有 200 多名贡献者，用 70 多种语言创建了 100 多个树库，其中包括<strong class="lt iu">阿拉伯语</strong></p><p id="9163" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">UD 提供了三种不同的树库。根据评论的建议，我们将使用 PADT 树木银行。</p><h2 id="8d64" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">数据预处理</h2><p id="b2df" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">UD 以<a class="ae ky" href="http://universaldependencies.org/conll17/" rel="noopener ugc nofollow" target="_blank"> CoNLL </a>的形式提供数据集。我们可以使用<a class="ae ky" href="https://github.com/pyconll/pyconll" rel="noopener ugc nofollow" target="_blank"> pyconll </a>将数据集从 conll 格式转换成 pandas 数据框架。</p><p id="7bbc" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">我们需要我们的数据按顺序组织。所以我们需要它的形状如下</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="8a50" class="mo la it oc b gy og oh l oi oj">[<br/>    [(Word1, T1), (Word2, T2), ...., (WordN, TM)], # Sentence one<br/>    [(Word1, T1), (Word2, T2), ...., (WordN, TM)], # Sentence two<br/>    .<br/>    .<br/>    .<br/>    [(Word1, T1), (Word2, T2), ...., (WordN, TM)] # Sentence M<br/>]</span></pre><p id="6d08" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">现在数据集以一种有用的方式构建。我们希望将文本数据编码成数值。</p><p id="aa2e" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这个过程就是所谓的<strong class="lt iu"> <em class="mn">嵌入</em> </strong>。</p><p id="52f6" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这个想法很简单，我们给数据中的每个单词一个唯一的整数值。并用数据集中的这个值替换，这样我们就可以对数据进行逐点操作。</p><p id="5a3a" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">所以在实现这个模型之前，我们有两个单词嵌入<code class="fe ok ol om oc b">word2index</code>和<code class="fe ok ol om oc b">tag2index</code>。它们对单词和词性标签进行编码</p><p id="87fc" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">因为我们正在做的任务要求输入和输出是固定的。应该转换数据，使得每个序列具有相同的长度。这种矢量化允许 LSTM 模型高效地执行批量矩阵运算。</p><p id="f50e" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">在 Keras 中，我们可以通过在较短的句子中添加<code class="fe ok ol om oc b">0</code>来做到这一点，直到我们所有的句子都具有相同的长度。</p><p id="1767" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">问题是该模型将能够很容易地预测这些值。因此，即使模型没有正确预测任何标签，准确率也会非常高。</p><p id="33fd" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">因此，我们需要编写自己的精确度指标，忽略那些 paddings 预测。</p><h1 id="cfaf" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">LSTM 的 Keras 实施</h1><p id="ed07" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在将数据转换成合适的形状之后。下一步是设计和实现实际的模型。</p><h2 id="f9ab" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">模型</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/1d490d3beab41253299321ca05f2d590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/0*e1I7-NN8Is7s8vF8.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">source: Google photos</figcaption></figure><p id="532b" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">模型结构</p><p id="e799" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">现在让我们彻底解释这个模型，</p><p id="92ab" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><code class="fe ok ol om oc b">InputLayer</code>是模型的第一层。如预处理部分所解释的，Keras 具有固定大小的层，所以我们用训练集中序列的最大长度来定义该层。</p><p id="408a" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><code class="fe ok ol om oc b">Embedding</code>嵌入层要求对输入数据进行整数编码，使得每个单词都由一个唯一的整数表示。它用随机权重初始化，然后它将学习数据集中的每个单词</p><p id="a94c" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><code class="fe ok ol om oc b">LSTM</code>LSTM 编码器层是一个序列到序列层。它提供序列输出，而不是整数值。并且<code class="fe ok ol om oc b">return_sequences</code>强制该层为下一层进行前一序列输入。</p><p id="63fa" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><code class="fe ok ol om oc b">Bidirectional</code>双向包装器复制了 LSTM 层，因此我们有两个并排的层，将结果序列传输到输入序列。实际上，这种方法对长短期记忆有很大的影响。我为<code class="fe ok ol om oc b">Bidirectional</code>层使用了默认的合并模式【串联】。</p><p id="8a20" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><code class="fe ok ol om oc b">TimeDistributed Dense</code>层用于保持输入层和输出层的一一对应关系。它对 3D 张量的每个时间步长应用相同的密集(全连接)运算。</p><p id="e535" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><code class="fe ok ol om oc b">Activation</code>激活<code class="fe ok ol om oc b">Dense</code>层的方法。我们可以将激活方法定义为<code class="fe ok ol om oc b">Dense</code>层中的一个参数，但第二个是更好的方法<a class="ae ky" href="https://stackoverflow.com/a/40870126" rel="noopener ugc nofollow" target="_blank"> * </a></p><p id="633c" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">用 Keras 实现这种设计非常简单。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="b77d" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">我们使用<code class="fe ok ol om oc b">categorical_cross_entropy</code>作为损失函数，因为我们有一个多对多的标记问题。</p><p id="4477" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">以及用于训练的<code class="fe ok ol om oc b">Adam</code>优化器(<em class="mn">自适应矩估计</em>)。</p><p id="9118" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><code class="fe ok ol om oc b">ignore_class_accuracy</code>是忽略填充后重新计算精度的方法<code class="fe ok ol om oc b">&lt;PAD&gt;</code></p><h1 id="141c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结果</h1><p id="9eaa" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">训练步骤在 2017 年的 MacBook Pro 上进行，耗时约 40 分钟，配有 2.5 GHz CPU 和 8 GB Ram。</p><p id="8623" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">在我们训练我们的模型之后，我们评估并可视化训练过程。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/482dabc1d850a1741efe49531b11a58c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/0*NqXKesNVL5tZQoI3.png"/></div></figure><p id="9e44" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">模型损失</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/b1a231008fc1183555fd49290cf00c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CcFsvxS0KJ_nJu2Y.png"/></div></div></figure><p id="4e17" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">模型已经到达<code class="fe ok ol om oc b">0.916</code>并在<code class="fe ok ol om oc b">30 epochs</code>后开始收敛</p><p id="ba59" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">在评估中，我们使用了<code class="fe ok ol om oc b">stochastic gradient descnet</code>，因为它在评估<a class="ae ky" href="https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/" rel="noopener ugc nofollow" target="_blank"> * </a>中比<code class="fe ok ol om oc b">Adam</code>表现得更好</p><p id="8e16" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">尽管与斯坦福 CoreNLP 模型相比，结果看起来不错，但它还可以改进。但作为第一次尝试，还不错。</p><p id="01a4" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">原帖放在<a class="ae ky" href="https://adhaamehab.me/2019/03/01/lstm-for-pos-tagging.html" rel="noopener ugc nofollow" target="_blank">这里</a></p><h1 id="8be5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><ul class=""><li id="0504" class="nf ng it lt b lu lv lx ly ma os me ot mi ou mm nk nl nm nn bi translated"><a class="ae ky" href="https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/" rel="noopener ugc nofollow" target="_blank">https://Shao anlu . WordPress . com/2017/05/29/SGD-all-the-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/</a></li><li id="6fa3" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><a class="ae ky" href="https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/data-preparation-variable-length-input-sequences-sequence-prediction/</a></li><li id="cfe1" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated">http://colah.github.io/posts/2015-08-Understanding-LSTMs/<a class="ae ky" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"/></li><li id="3787" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated">【https://skymind.ai/wiki/lstm T4】</li><li id="f5f0" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><a class="ae ky" href="https://kevinzakka.github.io/2017/07/20/rnn/" rel="noopener ugc nofollow" target="_blank">https://kevinzakka.github.io/2017/07/20/rnn/</a></li><li id="50fa" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><a class="ae ky" href="https://developer.nvidia.com/discover/lstm" rel="noopener ugc nofollow" target="_blank">https://developer.nvidia.com/discover/lstm</a></li><li id="8385" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><a class="ae ky" href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/" rel="noopener ugc nofollow" target="_blank">https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/</a></li><li id="9f8a" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><a class="ae ky" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li><li id="9ca9" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated"><a class="ae ky" href="https://universaldependencies.org/" rel="noopener ugc nofollow" target="_blank">https://universaldependencies.org/</a></li></ul></div></div>    
</body>
</html>