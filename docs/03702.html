<html>
<head>
<title>19 entities for 104 languages: A new era of NER with the DeepPavlov multilingual BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">19 个实体对应 104 种语言:NER 的新时代</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/19-entities-for-104-languages-a-new-era-of-ner-with-the-deeppavlov-multilingual-bert-1bfa6d413ea6?source=collection_archive---------4-----------------------#2019-06-12">https://towardsdatascience.com/19-entities-for-104-languages-a-new-era-of-ner-with-the-deeppavlov-multilingual-bert-1bfa6d413ea6?source=collection_archive---------4-----------------------#2019-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/e6a2e8063876dccd24f808ad4b91ab3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Po8-9A-EQnZa5c7CH8-Rw.jpeg"/></div></div></figure><p id="821a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">世界数据科学界几乎没有人不同意<a class="ae kz" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>的发布是 NLP 领域最激动人心的事件。</p><p id="8eab" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于那些还没有听说过的人来说:BERT 是一种基于 transformer 的技术，用于预处理上下文单词表示，能够在各种自然语言处理任务中实现最先进的结果。伯特论文被公认为<a class="ae kz" href="https://syncedreview.com/2019/04/11/naacl-2019-google-bert-wins-best-long-paper/" rel="noopener ugc nofollow" target="_blank">最好的长篇论文</a>👏计算语言学协会北美分会颁发的年度大奖。Google Research】发布了几个预训练的 BERT 模型，包括多语言、中文和英文的 BERT。</p><p id="742a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们开发了<a class="ae kz" href="https://deeppavlov.ai/?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=bert" rel="noopener ugc nofollow" target="_blank">DeepPavlov</a>——一个对话式人工智能框架，包含了构建对话系统所需的所有组件。在 BERT 发布之后，我们惊讶于它可以解决的各种各样的任务。</p><p id="5910" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们决定将 BERT 集成到三个流行的 NLP 任务的解决方案中:<a class="ae kz" rel="noopener" target="_blank" href="/the-bert-based-text-classification-models-of-deeppavlov-a85892f14d61">文本分类</a>，标记和<a class="ae kz" href="https://medium.com/towards-data-science/bert-based-cross-lingual-question-answering-with-deeppavlov-704242c2ac6f" rel="noopener">问题回答</a>。在本文中，我们将详细告诉你如何在 DeepPavlov 中使用基于 BERT 的命名实体识别(NER)。</p><h1 id="b45f" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">NER 简介</h1><p id="20be" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated"><strong class="kd iu">命名实体识别</strong> ( <strong class="kd iu"> NER) </strong>是自然语言处理中最常见的任务之一，我们可以把它表述为:</p><blockquote class="md me mf"><p id="9b64" class="kb kc mg kd b ke kf kg kh ki kj kk kl mh kn ko kp mi kr ks kt mj kv kw kx ky im bi translated">给定一个标记序列(单词，可能还有标点符号)，为序列中的每个标记提供预定义标记集中的一个标记。</p></blockquote><p id="98fe" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面带标签的句子是我们的<a class="ae kz" href="https://demo.ipavlov.ai/" rel="noopener ugc nofollow" target="_blank">演示</a>中 NER 的输出，其中蓝色代表人物标签，绿色代表位置，黄色代表地理位置，灰色代表日期时间。</p><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mk"><img src="../Images/03eec058d4e0e379c2a3604e79705b8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5eliFTrwNEblK7KGY5_ByA.png"/></div></div></figure><p id="8f83" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">DeepPavlov NER 模型支持 19 个标签:ORG(组织)、GPE(国家、城市、州)、LOC(位置)、EVENT(命名的飓风、战役、战争、体育赛事)、DATE、CARDINAL、MONEY、PERSON 等。迪普巴洛夫的 NER 模型在命令行中处理的句子如下所示。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="4e37" class="mu lb it mq b gy mv mw l mx my">&gt;&gt; Amtech , which also provides technical temporary employment services to aerospace , defense , computer and high - tech companies in the Southwest and Baltimore - Washington areas , said its final audited results are due in late November .<br/>['B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-GPE', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O']</span></pre><p id="f8d3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了区分具有相同标签的相邻实体，使用生物标记方案，其中“B”表示实体的开始，“I”代表“内部”并且用于组成该实体的除第一个词之外的所有词，而“O”表示不存在实体。</p><p id="2f25" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">NER 有各种各样的商业应用📈。例如，NER 可以通过从简历中提取重要信息来帮助人力资源部门评估简历。此外，NER 可用于识别客户请求中的相关实体，如产品规格、部门或公司分支机构的详细信息，以便对请求进行相应分类并转发给相关部门。</p><h1 id="2c3e" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">如何在 DeepPavlov 中使用基于 BERT 的 NER 模型</h1><p id="d551" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">任何预先训练的模型都可以通过命令行界面(CLI)和 Python 进行推理。在使用该模型之前，请确保使用以下命令安装了所有必需的软件包:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="dfca" class="mu lb it mq b gy mv mw l mx my">python -m deeppavlov install ner_ontonotes_bert_mult</span><span id="6e5b" class="mu lb it mq b gy mz mw l mx my">python -m deeppavlov interact ner_ontonotes_bert_mult [-d]</span></pre><p id="4134" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中<strong class="kd iu"> ner_ontonotes_bert_mult </strong>表示配置文件的名称。</p><p id="7168" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">您可以通过 Python 代码与模型进行交互。</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="233c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此外，DeepPavlov 包含一个基于<a class="ae kz" href="https://github.com/deepmipt/DeepPavlov/blob/0.3.0/deeppavlov/configs/ner/ner_rus_bert.json" rel="noopener ugc nofollow" target="_blank"> RuBERT 的模型</a>，用于处理俄语数据。总的来说，基于 BERT 的模型比<a class="ae kz" href="https://arxiv.org/abs/1709.09686" rel="noopener ugc nofollow" target="_blank">基于 bi-LSTM-CRF 的模型</a>有实质性的改进。在这里，您可以看到基于英语和俄语的模型的性能。</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">The performance of the DeepPavlov’s NER models.</figcaption></figure><h1 id="ee74" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">多语言零镜头传输</h1><p id="1ffb" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">多语言 BERT 模型允许执行从一种语言到另一种语言的零转换。模型<a class="ae kz" href="https://github.com/deepmipt/DeepPavlov/blob/0.3.1/deeppavlov/configs/ner/ner_ontonotes_bert_mult.json" rel="noopener ugc nofollow" target="_blank"> ner_ontonotes_bert_mult </a>在 ontonotes 语料库(英语)上训练，该语料库在标记模式中具有 19 种类型。您可以在不同的语言上测试模型。</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ng"><img src="../Images/63de069edbe62c7709ebc1bed0890fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X-OvVtAK8U44LgeQABqgTw.png"/></div></div></figure><p id="fd4f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在德语、俄语、汉语和越南语的四个 NER 测试集上评估了多语言模型的性能。这些是转让的结果。</p><figure class="ml mm mn mo gt ju"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">The performance of the Multilingual model while tested on four test sets</figcaption></figure><h1 id="dc53" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">如何为 NER 配置 BERT</h1><p id="abeb" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">DeepPavlov NLP 管道在 config/faq 文件夹下的单独的<a class="ae kz" href="https://medium.com/deeppavlov/simple-intent-recognition-and-question-answering-with-deeppavlov-c54ccf5339a9" rel="noopener">配置文件</a>中定义。配置文件由四个主要部分组成:<strong class="kd iu">数据集 _ 读取器</strong>、<strong class="kd iu">数据集 _ 迭代器</strong>、<strong class="kd iu">链接器</strong>和<strong class="kd iu">训练器</strong>。</p><p id="445a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所有基于 BERT 的模型的公共元素是配置文件的<strong class="kd iu">链接器</strong>部分中的 BERT 预处理器(在 ner 的情况下是<strong class="kd iu"> bert_ner_preprocessor </strong>类)块。未处理的文本(即<em class="mg">“Alex goes to Atlanta”</em>)应该被传递到<strong class="kd iu"> bert_ner_preprocessor </strong>用于标记化成子标记，用它们的索引编码子标记，并创建标记和段掩码。<strong class="kd iu"> tokens </strong>参数本身包含一个语句记号列表(<em class="mg"> ['Alex '，' goes '，' to '，' Atlanta ']【T11])，<strong class="kd iu"> subword_tokens </strong>是<strong class="kd iu"> tokens </strong>带特殊记号<strong class="kd iu"> </strong> ( <em class="mg"> ['[CLS]'，' Alex '，' goes '，' to '，' Atlanta '，'[SEP]'] 【T19)。<strong class="kd iu"> subword_tok_ids </strong>包含记号 id，<strong class="kd iu"> subword_masks </strong>是一个列表，0 表示特殊记号，1 表示句子的记号(<em class="mg">[0 1 1 1 1 0】</em>)。</em></em></p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="5182" class="mu lb it mq b gy mv mw l mx my">{<br/> "class_name": "bert_ner_preprocessor",<br/> "vocab_file": "{BERT_PATH}/vocab.txt",<br/> "do_lower_case": false,<br/> "max_seq_length": 512,<br/> "max_subword_length": 15,<br/> "token_maksing_prob": 0.0,<br/> "in": ["x"],<br/> "out": ["x_tokens", "x_subword_tokens", <br/>         "x_subword_tok_ids", "pred_subword_mask"]<br/> }</span></pre><p id="f330" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以，这就是我们想告诉你的关于我们多语言的伯特 NER 的一切。我们希望这是有帮助的，你会渴望使用迪普帕洛夫 NER 模型😃你可以在这里阅读更多关于他们的信息。也可以使用我们的<a class="ae kz" href="http://demo.ipavlov.ai" rel="noopener ugc nofollow" target="_blank">演示</a>来测试我们基于 BERT 的模型。别忘了 DeepPavlov 有一个<a class="ae kz" href="https://forum.ipavlov.ai/?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=bert" rel="noopener ugc nofollow" target="_blank">论坛</a>——只要在这里问我们任何关于框架和模型的问题，我们会尽快与您联系。敬请期待！</p><figure class="ml mm mn mo gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/0d9d8da999a9ce115062d6ad28e8fdc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*PcRERLPsuW7CS70WvSbBPg.png"/></div></figure></div></div>    
</body>
</html>