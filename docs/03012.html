<html>
<head>
<title>Learning Parameters, Part 2: Momentum-Based &amp; Nesterov Accelerated Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习参数，第 2 部分:基于动量和内斯特罗夫加速梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12?source=collection_archive---------9-----------------------#2019-05-15">https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12?source=collection_archive---------9-----------------------#2019-05-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c741" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/learning-parameters/latest" rel="noopener">学习参数</a></h2><div class=""/><div class=""><h2 id="55de" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">让我们看看梯度下降的两个简单但非常有用的变体。</h2></div><p id="a306" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这篇文章中，我们看看如何在一定程度上利用动量的概念来克服梯度下降的平缓表面的限制。如果你不清楚这是关于什么的，一定要看看我的博客文章——<a class="ae lk" rel="noopener" target="_blank" href="/learning-parameters-part-1-eb3e8bb9ffbb">学习参数，第一部分:梯度下降</a>。在整篇博文中，我们都在解决第 1 部分中介绍的玩具问题。你可以点击这个帖子顶部的踢球者标签查看<strong class="kq ja"> <em class="ll">学习参数</em> </strong>系列的所有帖子。</p><p id="8bf5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在第 1 部分中，我们清楚地看到了一条曲线，在误差曲面的平缓区域，梯度可能很小，这可能会减慢速度。让我们看看什么样的动力可以克服这个缺点。</p><blockquote class="lm ln lo"><p id="211c" class="ko kp ll kq b kr ks ka kt ku kv kd kw lp ky kz la lq lc ld le lr lg lh li lj ij bi translated">引用说明:本博客中的大部分内容和图表直接取自 IIT 马德拉斯大学教授 Mitesh Khapra 提供的深度学习课程第 5 讲。</p></blockquote><h1 id="da5a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated">基于动量的梯度下降</h1><p id="81df" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">从登山运动员的角度来看，MBGD 背后的直觉(是的，这是我们在第 1 部分中使用的老掉牙的比喻)是</p><blockquote class="lm ln lo"><p id="2195" class="ko kp ll kq b kr ks ka kt ku kv kd kw lp ky kz la lq lc ld le lr lg lh li lj ij bi translated">如果我被反复要求朝同一个方向前进，那么我可能应该获得一些信心，开始朝那个方向迈出更大的步伐。就像一个球滚下斜坡时获得动量一样。</p></blockquote><h2 id="ab4f" class="mp lt iq bd lu mq mr dn ly ms mt dp mc kx mu mv me lb mw mx mg lf my mz mi iw bi translated">动量更新规则</h2><p id="fd3f" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">我们在梯度更新规则中容纳动量概念如下:</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/b8d8a0691b901f2927b1e614c0de36ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*NosPqDgX13dZbvs8E-a7FA.png"/></div></figure><p id="b501" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">除了当前的更新，我们还会查看更新的历史。我鼓励你花点时间来处理新的更新规则，并尝试将<em class="ll">更新</em>期限在每个步骤中的变化写在纸上。还是继续看。分解它，我们得到</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/c4e8fce34552a060f4b4ecbed8643aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDBsBanwX5jSFse4m2fxMQ.png"/></div></div></figure><p id="9b70" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">您可以看到，当前更新不仅与当前梯度成比例，还与先前步骤的梯度成比例，尽管它们的贡献在每个时间步骤减少<strong class="kq ja"><em class="ll">γ</em></strong>(γ)<em class="ll"/>倍。这就是我们如何在平缓区域提高更新的幅度。</p><h1 id="ffd7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated">行动中的动力</h1><p id="4bac" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">我们将普通梯度下降代码(如第 1 部分所示&amp;也可从<a class="ae lk" href="https://gist.github.com/akshaychandra21/703ecc8949a01f472d17db3359d56985" rel="noopener ugc nofollow" target="_blank">这里</a>获得)稍微修改如下:</p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="1a78" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从现在开始，我们将只使用等高线地图。将事物可视化为三维可能会很麻烦，因此等值线图可以作为一种便捷的替代方法来表示具有二维输入和一维输出的函数。如果你没有意识到它们/对它们感到不舒服，请阅读我的基本材料博客文章-<a class="ae lk" href="http://asasasasas" rel="noopener ugc nofollow" target="_blank"><em class="ll">学习参数，第 0 部分:基本材料</em> </a>的<strong class="kq ja">第 5 节</strong>(甚至有一个自我测试，你可以更好地理解它们)。让我们看看 MBGD 使用我们在第 1 部分中介绍的相同的玩具神经网络有多有效。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi np"><img src="../Images/d9633294da9e99ce48952b18b741d6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/1*sbUtb2ySE2DUSBYHhQJRaw.gif"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Momentum-Based Gradient Descent. 100 iterations of vanilla gradient descent make the black patch.</figcaption></figure><p id="02f4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它工作了。普通梯度下降的 100 次迭代形成黑色斑块，很明显，即使在缓坡区域，基于动量的梯度下降也可以迈出实质性的步伐，因为动量带着它前进。</p><p id="5615" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">重要的是，快速行动总是好的吗？会不会有这样一种情况，动量会导致我们超越目标？让我们通过改变我们的输入数据来测试 MBGD，这样我们最终得到一个不同的误差表面。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/585334d8cf620fab32ab3638a5127199.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*sVpzlIuRspAhsS7Pnv0pWA.png"/></div></figure><p id="66bb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">比如上面显示的这个，误差在最小值谷的两边都很高。在这种情况下，冲力还能发挥作用吗？或者相反，冲力可能是有害的？让我们找出答案。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1370c8eafa2a4219a24049b04e581a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/1*t-kykynrtQ0olmFeNgIB0w.gif"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">100 iterations of vanilla gradient descent make the black patch.</figcaption></figure><p id="d055" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以观察到，以动量为基础的梯度下降，随着动量带着它离开极小值谷，在极小值谷内外振荡。这使得我们在最终收敛之前做了很多 U 形转弯。尽管有这些 U 形转弯，它仍然比普通梯度下降收敛得更快。在 100 次迭代之后，基于动量的方法已经达到 0.00001 的误差，而普通梯度下降仍然停留在 0.36 的误差。</p><p id="d222" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们能做些什么来减少振荡/U 形转弯吗？是的，内斯特罗夫加速梯度下降帮助我们做到了这一点。</p><h1 id="ef44" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated">内斯特罗夫加速梯度下降</h1><p id="588e" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">NAG 背后的直觉可以用一句话来概括:</p><blockquote class="lm ln lo"><p id="9389" class="ko kp ll kq b kr ks ka kt ku kv kd kw lp ky kz la lq lc ld le lr lg lh li lj ij bi translated">三思而后行！</p></blockquote><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/aea1ecd389f763f9f8c568c30d299fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*zjEZHrbahO4RSA7HHgsL6w.png"/></div></figure><h2 id="cab0" class="mp lt iq bd lu mq mr dn ly ms mt dp mc kx mu mv me lb mw mx mg lf my mz mi iw bi translated">NAG 更新规则</h2><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/844f07cceb4da4737563bbe594f0546c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*ewQ9mtcJW00Dgp0ZJFNKmg.png"/></div></figure><p id="b849" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是为什么向前看能帮助我们避免过度呢？我敦促你们停下来思考一下。如果还不清楚，我相信几分钟后就会清楚了。看一下这个图。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ny"><img src="../Images/659fec5826238d4f608639e85358d08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6MEi74EMyPERHlAX-x2Slw.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">A toy illustration.</figcaption></figure><p id="7c30" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在图(a)中，更新 1 为正，即梯度为负，因为随着<strong class="kq ja"> <em class="ll"> w_0 </em> </strong>增加<strong class="kq ja"> <em class="ll"> L </em> </strong>减少。甚至更新 2 也是积极的，你可以看到更新比更新 1 稍大，这要归功于动量。到目前为止，你应该确信更新 3 将比更新 1 和更新 2 更大，仅仅是因为动力和积极的更新历史。更新 4 是事情变得有趣的地方。在普通动量情况下，由于正历史，更新过冲，下降通过进行负更新来恢复。</p><p id="1e67" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是在 NAG 的情况下，每次更新分两步进行——首先，部分更新，我们到达<em class="ll"> look_ahead </em>点，然后是最终更新(参见 NAG 更新规则)，参见图(b)。NAG 的前 3 次更新非常类似于基于动量的方法，因为在那些情况下两次更新(部分和最终)都是正的。但是真正的区别在 update 4 中变得很明显。通常，每次更新分两个阶段进行，部分更新(4a)为正，但最终更新(4b)将为负，因为在<strong class="kq ja"> <em class="ll"> w_lookahead </em> </strong>计算的梯度将为负(通过观察图表说服自己)。这个负的最终更新稍微降低了更新的总幅度，仍然导致过冲，但是与普通的基于动量的梯度下降相比，过冲较小。我的朋友，这就是 NAG 如何帮助我们减少超调，即让我们采取更短的 U 形转弯。</p><h1 id="9e46" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated">唠叨在行动</h1><p id="207b" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">通过稍微更新 momentum 代码来进行部分更新和完全更新，我们得到了 NAG 的代码。</p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="5132" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们用同样的玩具例子和同样的误差表面来比较基于动量的方法和 NAG 的收敛性。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/23e835f9a3c90cd86ce0d87a31b565c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/1*_Q1plMUkXfLPTRCCTRg36g.gif"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">You can see that (I hope you can) NAG (blue) is taking smaller U-turns compared to vanilla momentum (red).</figcaption></figure><p id="6365" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">即使接近误差面上的极小值谷，NAG 肯定也在进行较小的振荡/较短的 U 形转弯。与基于动量的梯度下降相比，向前看有助于 NAG 更快地修正其路线。因此，振荡更小，逃离极小值谷的机会也更小。之前，我们证明了回顾过去有助于<em class="ll">*咳咳动力咳咳* </em>，现在我们证明了展望未来也有帮助。</p><h1 id="b932" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated">结论</h1><p id="3019" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">在这篇博客文章中，我们看了两个简单的混合版本的梯度下降，帮助我们更快地收敛——<em class="ll">基于动量的梯度下降</em>和<em class="ll">内斯特罗夫加速梯度下降(NAG) </em>，还讨论了 NAG 击败普通动量法的原因和位置。我们研究了它们的更新规则、方法的 python 代码实现中的细微差别，并在一个玩具示例中用图形展示了它们的收敛性。在下一篇文章中，我们将稍微偏离主题，谈谈这些算法的随机版本。</p><p id="100d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">请阅读本系列的下一篇文章，网址是:</p><ul class=""><li id="a1b7" class="nz oa iq kq b kr ks ku kv kx ob lb oc lf od lj oe of og oh bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/ee8558f65dd7">学习参数，第 3 部分:随机&amp;小批量梯度下降</a></li></ul><h1 id="16a0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated">承认</h1><p id="4a3b" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">IIT·马德拉斯教授的<a class="ae lk" href="https://www.cse.iitm.ac.in/~miteshk/" rel="noopener ugc nofollow" target="_blank"><strong class="kq ja"/></a>和<a class="ae lk" href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> CS7015:深度学习</strong> </a> <strong class="kq ja"> </strong>课程如此丰富的内容和创造性的可视化，这要归功于很多。我只是简单地整理了提供的课堂讲稿和视频。</p></div></div>    
</body>
</html>