<html>
<head>
<title>Using TensorFlow to conduct simple Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TensorFlow 进行简单的线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-using-tensorflow-fd732e1b690d?source=collection_archive---------22-----------------------#2019-06-05">https://towardsdatascience.com/linear-regression-using-tensorflow-fd732e1b690d?source=collection_archive---------22-----------------------#2019-06-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="925c" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">目标和重要概念</h1><p id="0ed3" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在本文结束时，您应该能够成功理解在 TensorFlow 中执行简单线性回归的过程，以获得并绘制由线性关系描述的某些数据集的最佳拟合线。此外，获得预测的解释将在最后给出，以阐明最佳拟合线的使用。</p><h2 id="2f89" class="lm jr it bd js ln lo dn jw lp lq dp ka kz lr ls ke ld lt lu ki lh lv lw km lx bi translated"><strong class="ak">线性回归</strong></h2><p id="77e9" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">线性回归是通过将线性方程拟合到数据点来描述两个变量之间关系的过程。这条线通常被称为“最佳拟合线”，是通过最小二乘回归方法获得的。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/2fdb30fa6c443abd752cd1719d982ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*KsEqnI9pITu20-sRyUOZJw.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk"><a class="ae mk" href="https://www.google.com/search?q=linear+regression+in+tensorflow&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwjx1IKJhNPiAhXSs1kKHQ6RBNMQ_AUIESgC&amp;biw=1680&amp;bih=939&amp;dpr=2#imgrc=_xHshSPnhNG1oM:" rel="noopener ugc nofollow" target="_blank">https://www.google.com/search?q=linear+regression+in+tensorflow&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwjx1IKJhNPiAhXSs1kKHQ6RBNMQ_AUIESgC&amp;biw=1680&amp;bih=939&amp;dpr=2#imgrc=_xHshSPnhNG1oM:</a></figcaption></figure><h2 id="ea02" class="lm jr it bd js ln lo dn jw lp lq dp ka kz lr ls ke ld lt lu ki lh lv lw km lx bi translated">最小二乘法</h2><p id="711d" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最小二乘回归线或方法是最小化回归线和数据集中每个数据点之间垂直距离的线。这条线最小化了数据中的方差，也就是所谓的损失。在 TensorFlow 甚至其他环境中，最小二乘回归线是通过两个过程确定的:代价函数和梯度下降法。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/5089630ef24b0c61c4699a1c110e4adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DlIuT4IrWrq8mRns9W3skg.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk"><a class="ae mk" href="https://www.google.com/search?q=least+squares+regression&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwjZrNXGhNPiAhUOZN8KHc9FA_wQ_AUIESgC&amp;biw=1680&amp;bih=890&amp;dpr=2#imgdii=Jr3voN86OP8v2M:&amp;imgrc=y54-2EX4O_2KQM:" rel="noopener ugc nofollow" target="_blank">https://www.google.com/search?q=least+squares+regression&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwjZrNXGhNPiAhUOZN8KHc9FA_wQ_AUIESgC&amp;biw=1680&amp;bih=890&amp;dpr=2#imgdii=Jr3voN86OP8v2M:&amp;imgrc=y54-2EX4O_2KQM:</a></figcaption></figure><h2 id="fae0" class="lm jr it bd js ln lo dn jw lp lq dp ka kz lr ls ke ld lt lu ki lh lv lw km lx bi translated">成本函数和梯度下降</h2><p id="1546" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">TensorFlow 中线性回归的工作方式是选择一个占位符斜率和 y 截距来启动该过程。在迭代 n 的循环中，计算成本函数或误差平方和，如果没有最小化，则调整最佳拟合线的斜率和 y 截距，并再次计算成本函数，直到其最小。梯度下降或梯度下降算法是计算和调整成本函数、斜率和 y 截距值的整个过程。幸运的是，TensorFlow 有预先制作的函数来运行梯度下降过程，并在每次迭代时计算成本函数。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mq"><img src="../Images/1a0317f3aa303440ad15910a7d251784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zhO9yh9Mulo_58HuAy9XJA.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk"><a class="ae mk" href="https://www.google.com/search?q=linreg1d&amp;tbm=isch&amp;source=iu&amp;ictx=1&amp;fir=zIsDdzISOI_EYM%253A%252Cm3PAo_HTKbKCsM%252C_&amp;vet=1&amp;usg=AI4_-kSxBE2bzXqM-Fx0jSGtMmW8VmRzZw&amp;sa=X&amp;ved=2ahUKEwic7J_UhdPiAhURWN8KHQuDDH8Q9QEwBHoECAMQBA#imgrc=zIsDdzISOI_EYM:" rel="noopener ugc nofollow" target="_blank">https://www.google.com/search?q=linreg1d&amp;tbm=isch&amp;source=iu&amp;ictx=1&amp;fir=zIsDdzISOI_EYM%253A%252Cm3PAo_HTKbKCsM%252C_&amp;vet=1&amp;usg=AI4_-kSxBE2bzXqM-Fx0jSGtMmW8VmRzZw&amp;sa=X&amp;ved=2ahUKEwic7J_UhdPiAhURWN8KHQuDDH8Q9QEwBHoECAMQBA#imgrc=zIsDdzISOI_EYM:</a></figcaption></figure><h1 id="a3ee" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">进入线性回归过程</h1><h2 id="dc3e" class="lm jr it bd js ln lo dn jw lp lq dp ka kz lr ls ke ld lt lu ki lh lv lw km lx bi translated">数据</h2><p id="791b" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">用于通过 TensorFlow 进行线性回归的数据集是研究生入学数据。该数据集有许多重要的参数，这些参数在申请硕士课程时非常重要。这些参数是:</p><ol class=""><li id="bf33" class="mr ms it kq b kr mt kv mu kz mv ld mw lh mx ll my mz na nb bi translated">GRE 成绩(满分 340 分)</li><li id="9add" class="mr ms it kq b kr nc kv nd kz ne ld nf lh ng ll my mz na nb bi translated">托福成绩(满分 120 分)</li><li id="1c39" class="mr ms it kq b kr nc kv nd kz ne ld nf lh ng ll my mz na nb bi translated">大学评级(满分 5 分)</li><li id="44b9" class="mr ms it kq b kr nc kv nd kz ne ld nf lh ng ll my mz na nb bi translated">目的陈述和推荐信强度(满分 5 分)</li><li id="6dd9" class="mr ms it kq b kr nc kv nd kz ne ld nf lh ng ll my mz na nb bi translated">本科 GPA(满分 10 分)</li><li id="6825" class="mr ms it kq b kr nc kv nd kz ne ld nf lh ng ll my mz na nb bi translated">研究经验(0 或 1)</li><li id="1ce1" class="mr ms it kq b kr nc kv nd kz ne ld nf lh ng ll my mz na nb bi translated">录取机会(范围从 0 到 1)</li></ol><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nh"><img src="../Images/bcc4222a5fa2b47c68cfddec4b5cfa64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ur3k8P5w-opxtURRavtIxg.png"/></div></div></figure><p id="bf47" class="pw-post-body-paragraph ko kp it kq b kr mt kt ku kv mu kx ky kz ni lb lc ld nj lf lg lh nk lj lk ll im bi translated">就本文的目的而言，被认为可以得出线性回归线的两个因素是 GRE 分数和录取机会。</p><h2 id="286d" class="lm jr it bd js ln lo dn jw lp lq dp ka kz lr ls ke ld lt lu ki lh lv lw km lx bi translated">数据探索</h2><p id="edcf" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在选择 GRE 分数作为线性回归的变量之前，我分析了数据集中提供的所有要素之间的关系和相关性。以下是数据集中所有要素的配对图:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nl"><img src="../Images/37b6f502fba80a514761b6d965d42a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WvLesKE2VHQbk8FJXGcYPw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 1: The relationships between GRE_Score, TOEFL_Score, CGPA, and Chance_Of_Admit are all linear</figcaption></figure><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="b471" class="pw-post-body-paragraph ko kp it kq b kr mt kt ku kv mu kx ky kz ni lb lc ld nj lf lg lh nk lj lk ll im bi translated">为了进一步挖掘 GRE_Score、TOEFL_Score 和 Chance _ Of _ admission 之间的关系，我们使用了一个三维散点图来对此进行可视化:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi no"><img src="../Images/118893efc75b31ba1335a601c88bc597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xPs7rPlw8B6banfWgJpPLw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 2: This scatterplot shows that an increase in GRE_Scores and TOEFL_Scores lead to relatively linear increases in Chance_Of_Admit</figcaption></figure><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="397c" class="pw-post-body-paragraph ko kp it kq b kr mt kt ku kv mu kx ky kz ni lb lc ld nj lf lg lh nk lj lk ll im bi translated">最后，为了巩固我使用 GRE_Scores 作为线性回归独立变量的选择，我们创建了一个关联热图来可视化关联:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi np"><img src="../Images/3073f066370320da67b8a7d724028325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YS85pLlGLWfSv8AoQyG_yQ.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 3: between GRE_Score and TOEFL_Score, GRE_Score has a higher correlation to Chance_Of_Admit</figcaption></figure><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="99db" class="lm jr it bd js ln lo dn jw lp lq dp ka kz lr ls ke ld lt lu ki lh lv lw km lx bi translated">使用张量流的线性回归</h2><p id="547f" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">线性回归的第一步是将数据集上传到文件中。其代码如下所示:</p><p id="f622" class="pw-post-body-paragraph ko kp it kq b kr mt kt ku kv mu kx ky kz ni lb lc ld nj lf lg lh nk lj lk ll im bi translated">并非数据集中的所有列都具有相同的数据类型。GRE_Score、TOEFL_Score 和 University_Rating 都是字符串数据类型。尽管 TOEFL_Score 和 University_Rating 不会用于线性回归，但保持要素的数据类型不变是一种很好的做法。下面显示了数据类型转换代码:</p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="2c02" class="pw-post-body-paragraph ko kp it kq b kr mt kt ku kv mu kx ky kz ni lb lc ld nj lf lg lh nk lj lk ll im bi translated">最后，我们可以开始线性回归过程的核心。首先，自变量和因变量的列需要放入一个列表中。在这个例子中，我们将 GRE_Score 转换为 x 列表，将 Chance_Of_Admit 转换为 y 列表。这方面的代码如下所示:</p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="2ec8" class="pw-post-body-paragraph ko kp it kq b kr mt kt ku kv mu kx ky kz ni lb lc ld nj lf lg lh nk lj lk ll im bi translated">接下来的几个步骤都被打包成一个函数，我称之为 linearregression()。在这个函数中，我们首先定义将被输入到模型中的两组数据。这些集合最初用随机的浮点数填充，但是当输入到模型中时，它们被上面声明的 x 和 y 数据占据。接下来，我们声明两个变量 W 和 b，并给它们分配随机数。w 用作最佳拟合线的斜率，而 b 用作最佳拟合线的 y 截距。最后，y_pred 和 loss 也在这个函数中定义。y_pred 的值是通过将 W 乘以 x1，然后加上 b 得到的，就像数学课上学过的等式一样。这个函数的代码如下所示。</p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="f297" class="pw-post-body-paragraph ko kp it kq b kr mt kt ku kv mu kx ky kz ni lb lc ld nj lf lg lh nk lj lk ll im bi translated">最后的步骤包括在设计的 run()方法中运行梯度下降函数。GradientDescentOptimizer()函数用于创建步长为 0.000005 的优化器对象。然后，在优化器对象上使用 minimize()函数来返回成本函数的最小值。最后，将数据输入模型，并在散点图上绘制最佳拟合线。run()方法的代码如下所示:</p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="0f8e" class="lm jr it bd js ln lo dn jw lp lq dp ka kz lr ls ke ld lt lu ki lh lv lw km lx bi translated">结果</h2><p id="f571" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">线性回归函数的最终结果给了我们一条最佳拟合线，它与 Seaborn 上的 regplot 生成的线极其相似。两者如下所示:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nq"><img src="../Images/ee5720207225c30fd0b0f5cddc5b6e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-kweA08BXIAxTKvz9EpiQ.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 4: Loss value for this line of best fit is 0.0068259086</figcaption></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nr"><img src="../Images/a22a902f82cd677ef0685edaf0d03ed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ohK2CaJgPwy2NxPw6LbIsQ.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 5: This is the regplot that was generated with the Seaborn package in python</figcaption></figure><h1 id="e09d" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">结论</h1><p id="05e1" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这篇文章是关于在 TensorFlow 中进行线性回归的。执行此操作的主要步骤是使用 TensorFlow 提供的梯度下降函数建立一个优化器，然后使用 minimize 函数确保返回最低成本函数值。要完全理解线性回归，必须了解梯度下降过程和成本函数。梯度下降是通过一个步长值调整函数的斜率和 y 截距，直到找到最小成本函数的渐进过程。成本函数本质上是绘制的每条最佳拟合线的最小平方和。有了这两个工具和上面的文章，你现在应该能够制作自己的线性回归函数了！</p></div></div>    
</body>
</html>