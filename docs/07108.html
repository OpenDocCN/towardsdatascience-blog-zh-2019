<html>
<head>
<title>Neural Networks | Fundamentals</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络|基础</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-fundamentals-1b4c46e7dbfe?source=collection_archive---------7-----------------------#2019-10-08">https://towardsdatascience.com/neural-networks-fundamentals-1b4c46e7dbfe?source=collection_archive---------7-----------------------#2019-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="af95" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这篇文章中，我将试图强调一些与人工神经网络相关的基础知识和基本概念。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/175d1f51abec0f7ddb3ac442bea2953f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5pQvYqNzgfYxrYVP.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" href="https://www.shutterstock.com/g/Liu+zishan" rel="noopener ugc nofollow" target="_blank">Liu Zishan</a></figcaption></figure><h1 id="aca7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">定义</h1><p id="f4ee" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">人工神经网络(ANN) </strong>是一系列算法，旨在通过模拟人脑运作方式的过程来识别一组数据中的潜在关系。这种系统通过分析例子来“学习”执行任务，通常不需要用特定于任务的规则来编程。</p><h1 id="409d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">全球建筑</h1><p id="48c5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">神经网络分为不同的层次:</p><ul class=""><li id="9af1" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><strong class="lt iu">输入层</strong>:输入层神经元接收应该解释待分析问题的信息；</li><li id="debc" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu">隐藏层</strong>:隐藏层是一个中间层，允许神经网络模拟非线性现象。这被说成是“隐藏的”，因为没有与外界的直接联系。每个隐含层的输出是下一层单元的输入；</li><li id="9d84" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu">输出层</strong>:输出层是网络的最后一层；它产生了结果，预测。</li></ul><h1 id="7d9b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">感知器</h1><p id="4ef4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">感知器是第一个也是最简单的神经网络模型，是人工智能领域著名的心理学家 Frank Rosenblatt 在 1957 年发明的监督学习算法。</p><p id="3d7c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">这个网络之所以简单，是因为它只有两层:一个输入层和一个输出层。这种结构只涉及一个权重矩阵，输入层的所有单元都连接到输出层的。</p><p id="cd9e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">感知器是用于二元预测的线性分类器，换句话说，它可以将数据分类或分成两类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/fda4b6f9804fb61183985bfd979ae5cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/0*olusX8DF2vBwJGQ8.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">3D representation of linearly separable data</figcaption></figure><p id="2f30" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">感知器操作</strong></p><p id="9414" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">首先，简单感知器取<em class="nh"> n 个</em>输入值(<em class="nh"> x1 </em>，<em class="nh"> x2 </em>，…，<em class="nh"> xn </em>)，同样由<em class="nh"> n+1 </em>常量定义:</p><ul class=""><li id="f2f5" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><em class="nh"> n </em>突触系数(或权重:<em class="nh"> w1 </em>，<em class="nh"> w2 </em>，…，<em class="nh">wn</em>)；</li><li id="7167" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">偏置:激活函数等于 1 的神经元。像其他神经元一样，偏差通过一个权重(通常称为阈值)将其自身连接到上一层神经元。</li></ul><p id="f917" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">然后，每个输入值必须乘以其各自的权重(<em class="nh">至</em>)，并且这些乘积的结果必须相加，以获得<strong class="lt iu">加权和。</strong>然后，神经元将生成两个可能值中的一个，这由总和的结果低于或高于阈值<em class="nh"> θ </em>这一事实来确定。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/dfe3137abbae14e2e8cbe266df9259ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*a9eP5cnBK4RtydaLddGx5A.png"/></div></figure><p id="43df" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">加权和可以转化为两个向量<em class="nh"> w </em>(权重)<em class="nh"> </em>和<em class="nh"> x </em>(输入)的点积，其中<em class="nh"> w⋅ x </em> = ∑ <em class="nh"> wixi </em>，那么不等式可以通过将<em class="nh"> θ </em>(阈值)移到另一边来解决。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/c4a6fd5ac379e75107ac805a217337e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*8NOzUVRYPR8Qd7lDNw-j5w.png"/></div></figure><p id="3ac7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">此外，请记住，偏差就像一个神经元，其中的激活函数等于 1，因此这最后一个与其权重(阈值)的乘积意味着阈值乘以 1。因此，常用的符号包括用变量<em class="nh"> b </em>(用于偏置)代替阈值，其中<em class="nh">b</em>=<em class="nh">θ</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/c5413f9b6c3800a2b6f99073b7647423.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*GR9FJwx9Jw-vCNbgvBSdkg.png"/></div></figure><p id="abac" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">完成所有这些步骤会产生以下架构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/abe99771aada70386c17737cef558d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7JGazjl_fdfSVs2w3YF--A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Schematic representation of the simple perceptron</figcaption></figure><p id="294f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">一旦获得加权和，有必要应用一个<strong class="lt iu">激活函数。</strong>简单的感知器使用 Heaviside 阶跃函数将结果值转换为二进制输出，将输入值分类为 0 或 1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/b8f65f69bb44f2715f503767895f0b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*ajOqUnSu24sQS3A2PhbZ3Q.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Graphical representation of the Heaviside step function</figcaption></figure><p id="0760" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">Heaviside 阶跃函数在输入数据可线性分离的分类任务中特别有用。</p><h1 id="ccf7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">多层感知器</h1><p id="c4e1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">多层感知器(MLP)由一个输入层、一个输出层和一个或多个隐藏层组成。所以，它不再是简单感知器那样的神经网络，而是复数形式的<strong class="lt iu">神经网络</strong>。<strong class="lt iu"> </strong>如果 MLP 有<em class="nh"> n </em>层，那么它有<em class="nh"> n-1 </em>个权重矩阵。</p><p id="bbcd" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">理论上，向隐藏层添加足够数量的神经元可能足以逼近任何非线性函数。</p><p id="34c2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">多层感知器架构:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/88eea9a255bcd1468f5bc6b05d003d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B0oORrFqaQFjmZ133F6yfA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Graphical representation of the Multi-Layer Perceptron</figcaption></figure><p id="0e48" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">神经网络在将所有输入传递到所有层(直到输出层)后生成预测。这个过程称为<strong class="lt iu">正向传播。</strong></p><p id="0b84" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">神经网络的工作方式与感知器相同。所以，为了理解神经网络，必须理解感知机！</p><h1 id="4c6c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">激活功能</h1><p id="6304" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">激活函数，也称为传递函数，是神经网络的重要组成部分。除了将非线性概念引入网络之外，<strong class="lt iu">旨在将进入一个单元(神经元)的信号转换成输出信号(响应)。</strong></p><p id="01cf" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">该函数的名称来自其生物学等价物“动作电位”:<strong class="lt iu">一旦达到，就会导致神经元反应的兴奋阈值。</strong></p><p id="ba98" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">值得注意的是，由于偏置，有可能将激活函数曲线上移或下移<strong class="lt iu">、</strong>，这意味着网络有更大的学习机会。</p><p id="98f4" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">激活函数有两种:<strong class="lt iu">线性和非线性。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/6f2ce6576314e38a0eab8fd571b53e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aAcXjmkITo8ijuBMCR606Q.png"/></div></div></figure><h2 id="b24c" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">线性激活函数</h2><p id="328f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这是一个简单的函数，其形式为:<em class="nh"> f(x) = x </em>。输入经过很少或没有修改就传递到输出。这完全是一个相称的问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/014ca541840ad8016a28672f44604678.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*T2ZW6P6hx5urXLLLf_qbgA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Graphical representation of a linear function</figcaption></figure><h2 id="9fcd" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">非线性激活函数</h2><p id="e655" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">非线性函数是最常用的，它可以分离不可线性分离的数据。非线性方程决定了输入和输出之间的对应关系。</p><p id="97cc" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">主要非线性函数:</strong></p><ul class=""><li id="4e33" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><strong class="lt iu">乙状结肠:</strong></li></ul><p id="5883" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">sigmoid 函数(或逻辑函数)是一条“S”曲线，它产生 0 到 1 之间的输出，用概率表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/de18d4b791776d657e217e7679cdcee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*qwsWMI7onEm63z6MH2Kk9Q.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Sigmoid function definition</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/2f61ef91afa52893f8a9d104c61f0d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*ttakDNg-YmN-HoLeuiSPIg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Graphical representation of the sigmoid function</figcaption></figure><p id="8049" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">作为一个“更平滑”的版本，它优于 Heaviside 阶跃函数，但并非没有缺陷。事实上，sigmoid 函数不是以零为中心的，因此负输入可能会产生正输出。此外，它对神经元的影响相对较低，结果通常非常接近 0 或 1，因此它会导致其中一些神经元饱和。最后，指数函数使得该过程作为计算是昂贵的。</p><ul class=""><li id="e549" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><strong class="lt iu">双曲正切值:</strong></li></ul><p id="e56e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">双曲正切像前一个一样是一个 s 形函数；然而，由于其对称性，它通常比逻辑函数产生更好的结果。实际上，区别在于 TanH 函数的结果映射在-1 和 1 之间。它通常优于 Sigmoid 函数，因为它以零为中心。这个函数非常适合多层感知器，尤其是隐藏层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/b66c7fbc1b5f29b32ab0d25336ef2833.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*mQgejtM72pWAGMaUzOhAIA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Hyperbolic tangent definition</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f636ea74095af192a3b221bc5bebcadd.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*9NKSe_dlQPFeb_2ZzuA-iw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Graphical representation of the TanH function</figcaption></figure><p id="5c23" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">除此之外，双曲正切函数也有与 Sigmoid 函数相同的缺点。</p><ul class=""><li id="6c59" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><strong class="lt iu">整流线性单元(ReLU) : </strong></li></ul><p id="9562" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">ReLU 函数有助于解决上述函数的饱和问题。它是最常用的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/82360abe9be83c838c1ffad07e01c6aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*emeslahXBLX59VhhBZstlQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">ReLU function definition</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/fc9a5e4253a341e360c65432b0b9f6d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*i-o6xNCaexEMzODqlvsdkg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Graphical representation of the ReLU function</figcaption></figure><p id="7a34" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如果输入为负，则输出为 0，而如果输入为正，则输出为<em class="nh"> z </em>。该激活函数显著增加了网络收敛，并且不会饱和。</p><p id="6d52" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">但是，ReLU 函数也并不完美。如果输入值为负，神经元可能保持不活动，因此权重不会更新，网络不再学习。</p><h2 id="d0cf" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">为什么需要这个激活功能？</h2><p id="d7f1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">没有非线性激活函数，人工神经网络无论有多少层，都将表现为简单的感知器，因为对其层求和只会产生另一个线性函数。</p><h2 id="f63d" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">应该使用哪个函数？</h2><p id="ea70" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">是回归还是分类问题？第一种情况，是二元分类情况吗？最终，没有更好的激活函数，<strong class="lt iu">这取决于要处理的任务。</strong></p><h1 id="93bd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">价值函数</h1><p id="ec14" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了学习，感知机必须知道它犯了一个错误，以及它应该给出的答案。这是监督学习。为此，需要使用一个成本函数，其目的是计算误差，换句话说，量化预测<em class="nh"> y_hat </em>和期望值<em class="nh"> y </em>之间的差距。因此，有必要最小化成本函数，直到最优:<strong class="lt iu">这是神经网络训练。</strong></p><p id="66c1" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">为了定义成本函数<em class="nh"> J </em>，可以使用均方误差:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/bd796412bb8022548bbcf2dcc79fe99b.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*_AjuIofpCjglYLctQGrSkQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Mean squared error (MSE)</figcaption></figure><p id="dc84" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">其中:</p><ul class=""><li id="a91e" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><em class="nh"> m </em> <strong class="lt iu"> </strong>是训练样本数；</li><li id="6586" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><em class="nh"> y </em> <strong class="lt iu"> </strong>是期望值；</li><li id="a56d" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><em class="nh"> y_hat </em> <strong class="lt iu"> </strong>是预测值。</li></ul><p id="493c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">一旦对预测值和期望值进行了比较，信息必须返回到神经网络，因此它返回到突触并更新权重。这不多也不少于前面提到的正向传播的反向路径。它被称为<strong class="lt iu">反向传播。</strong></p><p id="4f63" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如前所述，<strong class="lt iu">机器学习算法的目的是找到一个权重组合，以便最小化成本函数</strong></p><blockquote class="oi oj ok"><p id="24b9" class="lr ls nh lt b lu mp ju lw lx mq jx lz ol nd mc md om ne mg mh on nf mk ml mm im bi translated"><strong class="lt iu">延伸阅读:</strong></p></blockquote><div class="oo op gp gr oq or"><a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener follow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">理解神经网络中的激活函数</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">最近，我的一个同事问了我几个类似“为什么我们有这么多激活功能？”，“为什么是…</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">medium.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf ks or"/></div></div></a></div><h1 id="64db" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">梯度下降</h1><p id="de13" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在拥有更多权重从而拥有高维空间的情况下，就出现了一个问题:<strong class="lt iu">维度的诅咒。像蛮力这样幼稚的方法不能再用了。因此，有必要使用可行的方法来计算成本函数:<strong class="lt iu">梯度下降</strong>，<strong class="lt iu">，</strong>最流行的算法之一来执行优化。</strong></p><p id="d302" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">让我们假设成本函数<em class="nh"> J </em>是凸的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/542f448ff7a24753fef929006a3468eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*4jTMJEa4afrZrw4xfST3DA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Tridimensional representation of a convex function</figcaption></figure><p id="da94" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">横轴代表参数、权重和偏差的空间，而代价函数<em class="nh"> J </em>是横轴上方的<strong class="lt iu">误差面</strong>。蓝色圆圈是初始成本值。剩下的就是往下走，但是从这里走哪条路最好呢？</p><p id="1449" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">要回答这个问题，必须改变一些参数，即权重和偏差。然后，它将涉及成本函数的梯度，因为梯度向量将自然地指示最陡的斜率。<strong class="lt iu">输入值是固定的，这一点很重要，因此权重(和偏差)是唯一可以控制的调整变量。</strong></p><p id="c1c7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">现在，想象一个球被扔进一个圆形的桶里(凸函数)，它必须到达桶的底部。<strong class="lt iu">这就是优化。</strong>在梯度下降的情况下，它必须从左向右移动，以优化其位置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/fb26c0ed7beef534676b7fc648a270c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R-TKQB8QSyhMg4SGcB15MA.png"/></div></div></figure><p id="7acb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">从一个初始位置开始，观察倾斜角，以便画出该点的切线:这意味着计算导数。如果斜率是负的，球向右，如果斜率是正的，球向左。</p><p id="6535" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">但是缺少了一些东西，至少因为它是一个超参数:<strong class="lt iu">学习率(<em class="nh"> α </em>)。</strong>坡度表示要走的方向，但并没有告诉球在那个方向应该走多远。<strong class="lt iu">这就是学习率的作用，它决定了每一步达到最小值的大小。</strong></p><p id="a7aa" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">综上所述，梯度下降可定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/47fadc82d9b9f7f14978abb34c25a944.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*fnjWPoc6ceS0koDEZ1AcDQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Canonical formula for gradient descent</figcaption></figure><p id="b547" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">其中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/2e2d2e830bc9f64175ec7530e6bf0e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*OnlKj1rtj0a4uTY2F90AvQ.png"/></div></figure><ul class=""><li id="6bc1" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><em class="nh"> θ </em> <strong class="lt iu"> </strong>是模型的参数(权重向量)；</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/e84a5b60d0142dffc9fe0b79fbdf32a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*HMFaQVhUMm_yH2yMjVFGXA.png"/></div></figure><ul class=""><li id="c76a" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">∇ <em class="nh"> J(θ) </em> <strong class="lt iu"> </strong>是代价函数<em class="nh"> J </em>的梯度，换句话说，这是包含每一个偏导数的向量。我们将通过对函数求导一次得到它；</li><li id="298c" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><em class="nh"> α </em> <strong class="lt iu"> </strong>是<strong class="lt iu"> </strong>的学习速率(步长)，设定在学习过程之前。</li></ul><p id="54f1" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">学习率的选择仍然是一个开放性的问题，对于给定数据集上的给定模型，最优值无法通过分析计算得到。相反，足够好的学习率必须通过反复试验来发现，但是要注意，它必须随着时间的推移而降低。</p><p id="0e00" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">为学习速率选择正确的值很重要，因为它一方面会影响学习速度，另一方面会影响找到局部最优(收敛)的机会。如果选择不当，该值可能会导致预测模型性能不佳的两个主要原因:</p><ul class=""><li id="92a9" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><strong class="lt iu">过拟合</strong>:算法很好地适应了训练数据集，在现实中太好了，这是一个问题，因为它将不再能够概括数据。随着学习率的提高，每一步可以走更远的距离，但是会有超过最低点的风险，因为斜率是不断变化的。简单来说，损失函数在最小值附近波动，甚至可能发散；</li><li id="c274" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">设定一个非常低的学习率可以让你自信地走向负梯度。较低的<em class="nh"> α </em>更精确，但是计算斜率需要很多时间，并且导致收敛速度非常慢。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/08182bba08a146f06267d45813460ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*A-O-4CupxfW1TXoA.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Left: α too small; Middle: decent α; Right: α too big.</figcaption></figure><p id="c020" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">有了好的学习率，经过几次迭代，应该会找到一个合适的最小值，然后球就再也下不去了。</p><p id="06b7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">最后，确定优化网络的最佳权重。</p><p id="9e10" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">这就是梯度下降的全部内容:<strong class="lt iu">了解每个权重在网络总误差中的贡献，从而收敛到优化的权重配置。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/a468e616762ff0bc55d256cc55fa1fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*laN3aseisIU3T9QTIlob4Q.gif"/></div></figure><h2 id="aa37" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">随机梯度下降</h2><p id="27e8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">然而，还有一个问题:<strong class="lt iu">梯度下降需要成本函数是凸的。</strong>换句话说，曲线完全在它的每条切线之上，并且这样一个函数的导数在其区间上增加。如前所述，它采用以下形式:∨。但是一个非凸的函数呢？</p><p id="8380" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">这一次，让我们假设成本函数<em class="nh"> J </em>是非凸的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/8a53b828db035bc4020dc87f6a694e28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/0*lrjtIe22h-jNxyrk.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Tridimensional representation of a non-convex function</figcaption></figure><p id="0a4b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">在这种情况下，采取最陡的坡度是不够的。误差表面在视觉上变得更加复杂，难以理解，并且具有特定的特征，例如<strong class="lt iu">局部最小值</strong>和可能的<strong class="lt iu">鞍点。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/020430a26219839bf40592e66a5f5a0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/0*CP5qYQ83Of9jEUz6.png"/></div></figure><p id="5802" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">因此，风险是导致一些迭代算法的阻塞，大大减慢反向传播，并且<strong class="lt iu">落在不是最小整体值(全局最小值)的位置上。</strong></p><p id="ddb2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">为了克服这个问题，可以使用<strong class="lt iu">随机梯度下降(SGD)。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/c1311e08de78c1fa90e8e42523f0a58d.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*axpCBP0-vos3hj3WXXdJ8w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Mathematical formula for SGD</figcaption></figure><p id="4ec7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">除了外观，这一个更快。它提供了更多的权重波动，这增加了检测到全局最小值而不停留在局部最小值的机会。</p><p id="0b00" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">事实上，没有必要测试并在内存中加载整个数据集以仅在最后调整权重。<strong class="lt iu">随机梯度下降将在每次迭代后完成，使得该过程作为计算变得更轻。</strong>此外，<strong class="lt iu">标准(或批次)梯度下降是确定性的:</strong>如果起始条件(权重)始终相同，那么结果也将始终相同。</p><blockquote class="oi oj ok"><p id="f2aa" class="lr ls nh lt b lu mp ju lw lx mq jx lz ol nd mc md om ne mg mh on nf mk ml mm im bi translated"><strong class="lt iu">进一步阅读:</strong></p></blockquote><div class="oo op gp gr oq or"><a href="http://www.deeplearning.ai/ai-notes/optimization/" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">AI 笔记:神经网络中的参数优化- deeplearning.ai</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">在机器学习中，你从定义一个任务和一个模型开始。该模型由架构和参数组成…</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">www.deeplearning.ai</p></div></div><div class="pa l"><div class="po l pc pd pe pa pf ks or"/></div></div></a></div><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">理解梯度下降背后的数学。</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">机器学习中一种常用优化算法背后的简单数学直觉。</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pp l pc pd pe pa pf ks or"/></div></div></a></div></div><div class="ab cl pq pr hx ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="im in io ip iq"><h1 id="3022" class="kz la it bd lb lc px le lf lg py li lj jz pz ka ll kc qa kd ln kf qb kg lp lq bi translated">神经网络训练</h1><ol class=""><li id="3451" class="mn mo it lt b lu lv lx ly ma qc me qd mi qe mm qf mv mw mx bi translated">用接近(但不同于)0 的值初始化权重；</li><li id="cb41" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm qf mv mw mx bi translated">发送输入层中的第一个观察值，每个神经元一个变量；</li><li id="c268" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm qf mv mw mx bi translated">正向传播:神经元被激活的方式取决于它们被赋予的权重。传播激活直到获得<em class="nh"> y_hat </em>预测；</li><li id="f65c" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm qf mv mw mx bi translated">将预测值与期望值进行比较，用代价函数度量误差；</li><li id="acf6" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm qf mv mw mx bi translated">反向传播:错误在网络中再次传播。根据它们在误差中的责任更新权重，调整学习率；</li><li id="062d" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm qf mv mw mx bi translated">重复步骤 1 至 5，并在每次观察后调整权重，或者在一批观察后调整权重(批量学习)；</li><li id="07e0" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm qf mv mw mx bi translated">当所有的数据集都通过了神经网络，它被称为一个时期。重复更多的时代。</li></ol><blockquote class="oi oj ok"><p id="d4d9" class="lr ls nh lt b lu mp ju lw lx mq jx lz ol nd mc md om ne mg mh on nf mk ml mm im bi translated"><strong class="lt iu">延伸阅读:</strong></p></blockquote><div class="oo op gp gr oq or"><a href="http://www.deeplearning.ai/ai-notes/initialization/" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">AI 笔记:初始化神经网络- deeplearning.ai</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">为了建立一个机器学习算法，通常你要定义一个架构(例如，逻辑回归，支持向量…</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">www.deeplearning.ai</p></div></div><div class="pa l"><div class="qg l pc pd pe pa pf ks or"/></div></div></a></div></div><div class="ab cl pq pr hx ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="im in io ip iq"><h1 id="142c" class="kz la it bd lb lc px le lf lg py li lj jz pz ka ll kc qa kd ln kf qb kg lp lq bi translated">关键要点</h1><ul class=""><li id="574e" class="mn mo it lt b lu lv lx ly ma qc me qd mi qe mm mu mv mw mx bi translated">感知器是第一个也是最简单的人工神经网络模型。人工神经网络以同样的方式工作；</li><li id="dbe9" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">神经网络由一个输入层、一个或多个隐藏层以及最后一个输出层组成；</li><li id="f61d" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">输入值是固定的，突触系数(权重)和偏差是唯一可以控制的参数；</li><li id="3ff2" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">神经网络的层数越多，就越深，但将它们相乘可能会适得其反；</li><li id="78af" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">还有其他类型的人工神经网络，仅举几个例子，例如卷积神经网络(CNN 或 ConvNet)或递归神经网络(RNN)；</li><li id="fd05" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">成本函数用于量化预测值和期望值之间的差距。由于权重的优化组合(神经网络训练)，这必须被最小化；</li><li id="8d43" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">梯度下降是一种优化算法，也是迄今为止训练神经网络最常用的方法；</li><li id="c04c" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">知道方向和该方向上每一步的大小(学习率)是执行梯度下降的关键。最后一个必须仔细设置；</li><li id="d613" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">过拟合和欠拟合是预测模型性能差的两个主要原因。</li></ul></div><div class="ab cl pq pr hx ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="im in io ip iq"><h1 id="f57d" class="kz la it bd lb lc px le lf lg py li lj jz pz ka ll kc qa kd ln kf qb kg lp lq bi translated">更多资源:</h1><h2 id="cf69" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">课程</h2><ul class=""><li id="e25f" class="mn mo it lt b lu lv lx ly ma qc me qd mi qe mm mu mv mw mx bi translated"><a class="ae ky" href="https://www.deeplearning.ai/" rel="noopener ugc nofollow" target="_blank"> deeplearning.ai </a>，吴恩达入门深度学习课程；</li><li id="af39" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><a class="ae ky" href="http://cs231n.stanford.edu/syllabus.html" rel="noopener ugc nofollow" target="_blank"> CS231n:用于视觉识别的卷积神经网络</a>，斯坦福的深度学习课程；</li><li id="d0a1" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><a class="ae ky" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>，动手项目课程。</li></ul><h2 id="cef9" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">阅读</h2><ul class=""><li id="53ee" class="mn mo it lt b lu lv lx ly ma qc me qd mi qe mm mu mv mw mx bi translated"><a class="ae ky" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">深度学习</a>，这本书的在线版本被广泛认为是深度学习的“圣经”，作者是伊恩·古德菲勒、约舒阿·本吉奥和亚伦·库维尔；</li><li id="ddd3" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><a class="ae ky" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习</a>，迈克尔·尼尔森的免费、清晰、易懂的教材；</li><li id="1440" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><a class="ae ky" href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap" rel="noopener ugc nofollow" target="_blank">深度学习论文阅读路线图</a>，按时间顺序和研究领域组织的关键论文汇编。</li></ul></div></div>    
</body>
</html>