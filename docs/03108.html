<html>
<head>
<title>Review: MobileNetV2 — Light Weight Model (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:MobileNetV2 —轻量模型(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c?source=collection_archive---------1-----------------------#2019-05-19">https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c?source=collection_archive---------1-----------------------#2019-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="87b1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">胜过<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>、<a class="ae kf" href="https://medium.com/@sh.tsang/review-nasnet-neural-architecture-search-network-image-classification-23139ea0425d" rel="noopener">纳斯网</a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f"> ShuffleNet V1 </a></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/5e967ade9c23314fdcf1c09935a6683b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DmbUhqkK3ApsSxYC"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">MobileNetV2 for Mobile Devices</strong></figcaption></figure><p id="7db9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di">在</span>这个故事中，对<strong class="kz ir"> Google </strong>的<strong class="kz ir"> MobileNetV2 </strong>做一个简单的回顾。在前一版本<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>中，引入了<strong class="kz ir">深度方向可分离卷积</strong>，大大降低了网络的复杂度成本和模型规模，适用于移动设备或任何计算能力较低的设备。在 MobileNetV2 中，引入了一个更好的模块，带有<strong class="kz ir">反向剩余结构</strong>。<strong class="kz ir">此次去除了窄层中的非线性</strong>。使用 MobileNetV2 作为特征提取的主干，还可以在对象检测和语义分割方面实现一流的性能。这是一篇<strong class="kz ir"> 2018 CVPR </strong>的论文，引用超过<strong class="kz ir"> 200 次</strong>。(<a class="mc md ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----8febb490e61c--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="1bb2" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">概述</h1><ol class=""><li id="ceb5" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls nk nl nm nn bi translated"><strong class="kz ir"> MobileNetV2 卷积块</strong></li><li id="3f97" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">整体架构</strong></li><li id="570c" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">消融研究</strong></li><li id="ea63" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">实验结果</strong></li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="27d8" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 1。MobileNetV2 卷积块</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nt"><img src="../Images/f28b79205bec7b5efbee0f308e6231e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqE59FvgpvoAQUMQ0WEoUA.png"/></div></div></figure><h2 id="bb9c" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lg nz oa mx lk ob oc mz lo od oe nb of bi translated">1.1.<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a></h2><ul class=""><li id="86ff" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls og nl nm nn bi translated">在<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>中，有 2 层。</li><li id="a6c1" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated"><strong class="kz ir">第一层</strong>被称为<strong class="kz ir">深度方向卷积</strong>，它通过对每个输入通道应用单个卷积滤波器来执行轻量级滤波。</li><li id="bcb5" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated"><strong class="kz ir">第二层</strong>是一个<strong class="kz ir"> 1×1 卷积</strong>，称为<strong class="kz ir">逐点卷积</strong>，负责通过计算输入通道的线性组合来构建新的特征。</li><li id="5711" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated"><strong class="kz ir"> ReLU6 </strong>此处用于对比。(实际上，在<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>的技术报告中，我找不到他们使用 ReLU6 的任何暗示……也许我们需要检查 Github 中的代码……)，即<strong class="kz ir"> min(max( <em class="oh"> x </em>，0)，6) </strong>如下:</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/256cfa938fa05f4dd2e259709abad16e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*M1--hwMDlc8trpvT.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">ReLU6</strong></figcaption></figure><ul class=""><li id="23c9" class="nd ne iq kz b la lb ld le lg oj lk ok lo ol ls og nl nm nn bi translated">基于[27] MobileNetV1，ReLU6 因其在用于低精度计算时的稳健性而被使用。</li></ul><h2 id="4962" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lg nz oa mx lk ob oc mz lo od oe nb of bi translated">1.2.MobileNetV2</h2><ul class=""><li id="fea6" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls og nl nm nn bi translated">在 MobileNetV2 中，有两种类型的块。一个是步长为 1 的剩余块。另一个是步长为 2 的块，用于缩小尺寸。</li><li id="3b5c" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">两种类型的砌块都有 3 层。</li><li id="e69f" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">这次的<strong class="kz ir">第一层</strong>是和 ReLU6 的<strong class="kz ir"> 1×1 卷积。</strong></li><li id="bde0" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated"><strong class="kz ir">第二层</strong>为<strong class="kz ir">深度方向卷积</strong>。</li><li id="c443" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated"><strong class="kz ir">第三层</strong>是另一个<strong class="kz ir"> 1×1 卷积，但是没有任何非线性。</strong>据称，如果再次使用 ReLU，深度网络仅在输出域的非零体积部分上具有线性分类器的能力。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8afcf8d8ed1cc7d2661b9f1b828d32fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*b_eYtG_Vmkr1Sz4nT_OQDA.png"/></div></figure><ul class=""><li id="c902" class="nd ne iq kz b la lb ld le lg oj lk ok lo ol ls og nl nm nn bi translated">而且还有一个膨胀系数<em class="oh"> t </em>。对于所有主要实验，<em class="oh"> t </em> =6。</li><li id="8c55" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">如果输入得到 64 个通道，内部输出将得到 64× <em class="oh"> t </em> =64×6=384 个通道。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="e8a8" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">2.整体架构</h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi on"><img src="../Images/018d3c20190876668e983d6e7ea2b63f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*5iA55983nBMlQn9f6ICxKg.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">MobileNetV2 Overall Architecture</strong></figcaption></figure><ul class=""><li id="60cf" class="nd ne iq kz b la lb ld le lg oj lk ok lo ol ls og nl nm nn bi translated">其中<em class="oh"> t </em>:扩展因子，<em class="oh"> c </em>:输出通道数，<em class="oh"> n </em>:重复次数，s:步距。3×3 核用于空间卷积。</li><li id="9faf" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">通常情况下，<strong class="kz ir">主网络</strong>(宽度乘数 1，<strong class="kz ir"> 224×224 </strong>)，计算成本为<strong class="kz ir"> 3 亿次乘加</strong>，使用<strong class="kz ir">340 万个参数</strong>。(宽度乘数在<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>中介绍。)</li><li id="7ef1" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">对于从 96 到 224 的<strong class="kz ir">输入分辨率和从 0.35 到 1.4 </strong>的<strong class="kz ir">宽度乘数，进一步探索了性能折衷。</strong></li><li id="1f52" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">网络计算成本高达 585M MAdds，而模型大小在 1.7M 和 6.9M 参数之间变化。</li><li id="7f6b" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">为了训练网络，使用 16 个 GPU，批量大小为 96。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/f195b7a98864c0cb517a48697fccfea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*5bJL6MG0r2-oQUa7M9zUtQ.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Number of Maximum Channels/Memory in Kb) at Each Spatial Resolution for Different Architecture with 16-bit floats for activation</strong></figcaption></figure></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="7133" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">3.<strong class="ak">消融研究</strong></h1><h2 id="e7e9" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lg nz oa mx lk ob oc mz lo od oe nb of bi translated">3.1.线性瓶颈的影响</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi op"><img src="../Images/4bf2201bb8e0bc6f149fe5a75d838a4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*EbFlMpgElRsWzD6Grp1S6w.png"/></div></figure><ul class=""><li id="7d6d" class="nd ne iq kz b la lb ld le lg oj lk ok lo ol ls og nl nm nn bi translated">随着每个瓶颈模块输出端 ReLU6 的移除，精确度得到了提高。</li></ul><h2 id="1df2" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lg nz oa mx lk ob oc mz lo od oe nb of bi translated">3.2.捷径的影响</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/145d0072ee47a7ad831d64e08e101819.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*HSGxRJq2b5YbIC5_5bLEJQ.png"/></div></figure><ul class=""><li id="663f" class="nd ne iq kz b la lb ld le lg oj lk ok lo ol ls og nl nm nn bi translated">瓶颈之间的快捷方式，它优于扩展之间的快捷方式和没有任何剩余连接的快捷方式。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="51fe" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">4.实验结果</h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/c59e48028e5bf984c14d0ddf915c0538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bLh_FRCym1UWnbpN.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">MobileNetV2 for Classification, Detection and Segmentation (From </strong><a class="ae kf" href="https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html" rel="noopener ugc nofollow" target="_blank"><strong class="bd kw">https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html</strong></a><strong class="bd kw">)</strong></figcaption></figure><h2 id="8133" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lg nz oa mx lk ob oc mz lo od oe nb of bi translated">4.1.ImageNet 分类</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi or"><img src="../Images/0e552383d640727a6d369cd7eaab3ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*BKOaDAMOxS7T71UKcsD8gA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">ImageNet Top-1 Accuracy</strong></figcaption></figure><ul class=""><li id="7056" class="nd ne iq kz b la lb ld le lg oj lk ok lo ol ls og nl nm nn bi translated">在模型大小和计算成本相当的情况下，MobileNetV2 优于<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f"> ShuffleNet </a> (1.5)。</li><li id="1d6d" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">宽度乘数为 1.4，MobileNetV2 (1.4)比<a class="ae kf" rel="noopener" target="_blank" href="/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f"> ShuffleNet </a> (×2)，和<a class="ae kf" href="https://medium.com/@sh.tsang/review-nasnet-neural-architecture-search-network-image-classification-23139ea0425d" rel="noopener"> NASNet </a>推理时间更快。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi os"><img src="../Images/932564a7c9a225e1dfc49c2e54e036d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*r_0i5QgbeTMFedSTL5yrfg.png"/></div></figure><ul class=""><li id="16d6" class="nd ne iq kz b la lb ld le lg oj lk ok lo ol ls og nl nm nn bi translated">如上所示，使用了不同的输入分辨率和宽度乘数。它的表现一直优于 MobileNetV1。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="ce8e" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lg nz oa mx lk ob oc mz lo od oe nb of bi translated">4.2.MS COCO 对象检测</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/fb7bd2b64da1fe840ec292e00335202d.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*VOGgkCrxTAojI9zDZxAkQg.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">SSDLite</strong></figcaption></figure><ul class=""><li id="b281" class="nd ne iq kz b la lb ld le lg oj lk ok lo ol ls og nl nm nn bi translated">首先，SSDLite 是通过用深度方向可分离的卷积(<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a> one)修改<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>中的规则卷积而引入的。</li><li id="010e" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">SSDLite 极大地减少了参数数量和计算成本。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/dad618c37354f964509809522cbbea41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*hU_B89lM7JDaXTnw5RQ_aA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">MS COCO Object Detection</strong></figcaption></figure><ul class=""><li id="737a" class="nd ne iq kz b la lb ld le lg oj lk ok lo ol ls og nl nm nn bi translated">MobileNetV2 + SSDLite 以明显更少的参数和更小的计算复杂度实现了具有竞争力的准确性。</li><li id="8f36" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">并且推理时间比<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>快。</li><li id="ccfb" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">值得注意的是，MobileNetV2 + SSDLite 的效率提高了 20 倍，体积缩小了 10 倍，但在 COCO 数据集上仍优于 YOLOv2。</li></ul><h2 id="f1c2" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lg nz oa mx lk ob oc mz lo od oe nb of bi translated">4.3.<strong class="ak"> PASCAL VOC 2012 </strong>语义分割</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/47d3793eb316b1174ba666beb6dce078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*Wj_vHqlj8lRhTZS_7fDbwA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">PASCAL VOC 2012 Validation Set</strong></figcaption></figure><ul class=""><li id="6968" class="nd ne iq kz b la lb ld le lg oj lk ok lo ol ls og nl nm nn bi translated">这里，MobileNetV2 用作<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74"> DeepLabv3 </a>的特征提取器。</li><li id="089b" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls og nl nm nn bi translated">通过禁用阿特鲁空间金字塔池(ASPP)以及多尺度和翻转(MP)，也将输出步幅从 8 改变为 16，获得了 75.32%的 mIOU，模型大小和计算成本低得多。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="259d" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lg nz oa mx lk ob oc mz lo od oe nb of bi translated">参考</h2><p id="f3d9" class="pw-post-body-paragraph kx ky iq kz b la nf jr lc ld ng ju lf lg ow li lj lk ox lm ln lo oy lq lr ls ij bi translated">【2018 CVPR】【MobileNetV2】<br/><a class="ae kf" href="https://arxiv.org/abs/1801.04381" rel="noopener ugc nofollow" target="_blank">MobileNetV2:逆残差和线性瓶颈</a></p><h2 id="6708" class="nu mm iq bd mn nv nw dn mr nx ny dp mv lg nz oa mx lk ob oc mz lo od oe nb of bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kx ky iq kz b la nf jr lc ld ng ju lf lg ow li lj lk ox lm ln lo oy lq lr ls ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(是)(这)(些)(人)(,)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(但)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(想)(要)(让)(这)(些)(人)(都)(有)(这)(些)(情)(况)(,)(我)(们)(还)(不)(想)(要)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(就)(是)(这)(些)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(都)(是)(很)(强)(的)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(起)(来)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(</p><p id="e0d8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">物体检测</strong> [ <a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener"> R-CNN </a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae kf" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4">G-RMI</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolo v1</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">yolo v2/yolo 9000</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">yolo v3</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a></p><p id="7bff" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">语义切分</strong>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-gcn-global-convolutional-network-large-kernel-matters-semantic-segmentation-c830073492d2"/></p><p id="fc65" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生物医学图像分割</strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumevision 1</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumevision 2/DCAN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1">M FCN</a>]</p><p id="3134" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">实例分段 </strong> <a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979"> Hypercolumn </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413"> MultiPathNet </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> InstanceFCN </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></p><p id="58de" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">人体姿态估计</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">深度姿态</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">汤普森·尼普斯 14</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">汤普森·CVPR 15</a><a class="ae kf" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac" rel="noopener">CPM</a></p></div></div>    
</body>
</html>