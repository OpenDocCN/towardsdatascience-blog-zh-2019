<html>
<head>
<title>Comparison of Activation Functions for Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度神经网络激活函数的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a?source=collection_archive---------0-----------------------#2019-05-09">https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a?source=collection_archive---------0-----------------------#2019-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f7cb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">阶跃函数、线性函数、Sigmoid 函数、双曲正切函数、Softmax 函数、ReLU 函数、Leaky ReLU 函数和 Swish 函数都有手把手的讲解！</h2></div><p id="f5a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🔥<strong class="kk iu"> <em class="le">激活函数在神经网络中起着关键作用，因此了解其优缺点以获得更好的性能至关重要。</em>T3】</strong></p><p id="aa0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有必要从引入非线性激活函数开始，它是最著名的<strong class="kk iu">s 形函数</strong>的替代函数。重要的是要记住，在评估激活功能的最终性能时，许多不同的条件都很重要。在这一点上，有必要提请注意数学和导数过程的重要性。因此，如果你准备好了，让我们卷起袖子，动手干一场吧！🙌🏻</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/2041c1b7ec3a46be48884bf1222781b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*m9pKSSP3ObV0l7GG.gif"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><a class="ae lr" href="https://www.reactiongifs.us/im-ready-game-of-thrones/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="251f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">什么是人工神经网络？让我们先记住这个:</strong>基于通过生物的神经系统对学习结构进行建模/模仿的人工学习技术。该神经结构通过分级电流感测过程来实现。从感受器获取的电脉冲使我们能够学习、记忆和记住我们自出生以来所看到、听到、感受到和想到的一切。神经科学是一个非常深入和有趣的研究领域。</p><h2 id="3a21" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">为什么我们需要激活功能？</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/3f274727a4544f3a279bd5c237dda0b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/0*ULAZWFmhB8hOMGBy.jpeg"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><a class="ae lr" href="http://cs231n.stanford.edu/index.html" rel="noopener ugc nofollow" target="_blank">CS231n: Convolutional Neural Networks for Visual Recognition</a></figcaption></figure><p id="0210" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要激活函数将非线性真实世界的属性引入人工神经网络。基本上，在一个简单的神经网络中，x 被定义为输入，w 是权重，我们传递 f (x)是传递给网络输出的值。这将是另一层的最终输出或输入。</p><h2 id="08cc" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">为什么不激活就不能把这个信号切换到输出？</h2><p id="ffcc" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">如果不应用激活函数，输出信号变成简单的线性函数。线性函数只是单级多项式。<strong class="kk iu">未激活的神经网络将充当学习能力有限的线性回归。</strong>但是我们也希望我们的神经网络学习非线性状态。<strong class="kk iu">因为我们会给你图像、视频、文本、声音等复杂的现实世界信息，让你学习到我们的神经网络。</strong>多层深度神经网络可以从数据中学习有意义的特征。</p><h2 id="3090" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">那么为什么非线性函数需要？</h2><p id="ef32" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">多阶函数称为非线性函数。人工神经网络被设计为<strong class="kk iu">通用函数</strong> <strong class="kk iu">近似器</strong>并用于此目标。这意味着他们必须具备计算和学习任何函数的能力。由于非线性激活函数，可以实现更强的网络学习。<strong class="kk iu">这篇文章已经和这个问题完全相关了</strong>😇</p><p id="975e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了计算与权重相关的误差值，应用人工神经网络的反向传播算法。有必要确定优化策略并最小化错误率。选择合适的优化算法也是一个单独的问题。</p><blockquote class="mr"><p id="3a1e" class="ms mt it bd mu mv mw mx my mz na ld dk translated"><a class="ae lr" href="https://twitter.com/sirajraval/status/930876397987549185" rel="noopener ugc nofollow" target="_blank">投入。乘以重量。加个偏向。激活！</a></p></blockquote><h1 id="0db1" class="nb lt it bd lu nc nd ne lx nf ng nh ma jz ni ka md kc nj kd mg kf nk kg mj nl bi translated">激活功能</h1><h2 id="9092" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">阶跃函数</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/13b4b1624883c4a58446ae36db332efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*QXjftnJ9y5pFwajiB3CG-g.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Step Function and Derivative</figcaption></figure><p id="e5bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它是一个接受二进制值的函数，用作二进制分类器。因此，在输出层中通常是优选的。不建议在隐藏层使用，因为它不代表衍生学习值，未来也不会出现。然后，我们来想一个导函数，线性函数马上就想到了。</p><h2 id="d7c2" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">线性函数</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/7648fc3058ac0445d03f55ba43494dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*NOWmZpY6va1QWRpI6MPYwQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Linear Function and Derivative</figcaption></figure><p id="14eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它生成一系列激活值，这些值不是二进制值，就像阶跃函数一样。它当然允许你将几个神经元(神经细胞)连接在一起。但是这个功能有一个重大问题！修正了导数。为什么我们需要它的导数，它被固定的负面影响是什么？我们所说的；通过反向传播算法，我们完成了神经元的学习过程。这个算法由一个导数系统组成。当 A = c.x 由 x 导出，我们就到了 c，这就意味着与 x 没有关系，好吧，如果导数始终是一个常数值，我们能说学习过程正在进行吗？可惜没有！</p><p id="f908" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有一个问题！当在所有图层中使用线性函数时，在输入图层和输出图层之间会达到相同的线性结果。<strong class="kk iu">线性函数的线性组合是另一个线性函数。</strong>这意味着我们最开始所说的神经元可以干扰互连层！🙄</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="8aa2" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">Sigmoid 函数</h2><p id="5db9" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">假设自然界中的大多数问题都不是线性的，sigmoid 函数的组合也不是线性的。答对了。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/581221ea1e99636a861e8f07066bfbd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*Myto4ZQagAOoyom4tqkaRQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Sigmoid Function and Derivative</figcaption></figure><p id="bb29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们可以对这些层进行分类😃所以让我们考虑非二元函数。它也是可导的，因为它不同于阶跃函数。这意味着学习是可以发生的。如果我们检查图形 x 在-2 和+2 之间，y 值变化很快。x 中的小变化在 y 中会很大。这意味着它可以用作一个好的分类器。此函数的另一个优点是，当遇到线性函数中的(- infinite，+ infinite)时，它会产生(0，1)范围内的值。所以激活值没有消失，这是个好消息！🎈</p><blockquote class="nv nw nx"><p id="ccb2" class="ki kj le kk b kl km ju kn ko kp jx kq ny ks kt ku nz kw kx ky oa la lb lc ld im bi translated"><strong class="kk iu">sigmoid 函数是最常用的激活函数，但还有许多其他更有效的替代函数。</strong></p><p id="33f5" class="ki kj le kk b kl km ju kn ko kp jx kq ny ks kt ku nz kw kx ky oa la lb lc ld im bi translated"><strong class="kk iu">那么乙状结肠功能</strong>有什么问题呢？</p></blockquote><p id="2416" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们仔细观察函数两端的图形，y 值对 x 的变化反应很小，让我们想想这是什么问题！🤔这些区域的导数值非常小，收敛到 0。这被称为<strong class="kk iu">消失梯度</strong>并且学习是最小的。如果为 0，则没有任何学习！当缓慢学习发生时，使误差最小化的优化算法可以附加到局部最小值，并且不能从人工神经网络模型获得最大性能。所以让我们继续寻找另一种激活功能！🔎</p><h2 id="9b26" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">双曲正切函数</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/2a179708e64e7513d0d3b6346c7687dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*51Q7QouspCkOvENni2RwfQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Hyperbolic Tangent and Derivative</figcaption></figure><p id="9ee3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它的结构非常类似于 Sigmoid 函数。但是，这一次函数被定义为(-1，+ 1)。相对于 sigmoid 函数的优势在于它的导数更陡，这意味着它可以获得更多的值。这意味着它的效率会更高，因为它的学习和评分范围更广。但是，函数两端的梯度问题仍然存在。虽然我们有一个非常普通的激活功能，但我们将继续寻找最好的一个！</p><h2 id="1183" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">ReLU(校正线性单位)功能</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="ab gu cl ob"><img src="../Images/d501a4d2758b50618aef61705837c225.png" data-original-src="https://miro.medium.com/v2/format:webp/1*m_0v2nY5upLmCU-0SuGZXg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">ReLU Function and Derivative</figcaption></figure><p id="fc24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">乍一看，它似乎与正轴上的线性函数具有相同的特征。但最重要的是，ReLU 本质上不是线性的。事实上，一个好的评估者。也有可能通过 ReLU 的组合与任何其他函数收敛。太好了！这意味着我们仍然可以(再次)在我们的人工神经网络中对层进行排序😄</p><p id="a490" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> ReLU 估值在[0，+G0]，但是回报是什么，有什么好处？</strong>让我们想象一个拥有太多神经元的大型神经网络。Sigmoid 和双曲线正切导致几乎所有的神经元都以相同的方式被激活。这意味着激活非常密集。网络中的一些神经元是活跃的，并且激活是不频繁的，所以我们想要有效的计算负载。我们用 ReLU 得到它。负轴上的值为 0 意味着网络将运行得更快。<strong class="kk iu">计算负荷小于 sigmoid 和双曲线正切函数的事实导致了多层网络的更高优先性。</strong>超级！😎但是即使是 ReLU 也不是很好，为什么？因为这个零值区域给了我们这个过程的速度！所以学习不是发生在那个区域。😕那你就需要找一个新的激活功能，有窍门的。</p><h2 id="9e5a" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">漏流函数</h2><p id="9c36" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">💧你能看到负平面上的漏洞吗？😲</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/95f6404554ccaa9d99d618e9c607d5bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*gDIUV3yonKbIWh_9Kl4ShQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Leaky ReLU Function and Derivative</figcaption></figure><p id="9b0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个泄漏值被给定为 0.01。如果给定一个接近零的不同值，函数的名称随机地改变为 Leaky ReLU。(没有，没有新功能？！😱)泄漏 ReLU 的定义范围仍然是负无穷大。这是接近于 0，但 0 与非生活梯度的价值在 RELU 生活在负区域的学习提供的价值。多聪明啊。🤓</p><h2 id="778c" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">Softmax 函数</h2><p id="0b55" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">它的结构非常类似于 Sigmoid 函数。与同一个 Sigmoid 一样，它在用作分类器时表现相当好。<strong class="kk iu">最重要的区别是在深度学习模型的输出层优先，尤其是需要分类两个以上的时候。</strong> I <a class="ae lr" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" rel="noopener ugc nofollow" target="_blank"> t 允许通过产生 0-1 范围内的值来确定输入属于特定类的概率。</a>所以它执行概率解释。</p><h2 id="07fc" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">Swish(自门控)功能</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/5040b33ec2f34092ffbdc9e8befd6532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*kFWX1dWUtLWo3LJLKsqhwA.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Swish Function and Derivative</figcaption></figure><p id="a43e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与 ReLU 最重要的区别是在负区域。<strong class="kk iu">ReLU 里 Leaky 的值一样，有什么区别吗？</strong>其他所有激活功能都很单调。请注意，即使输入增加，swish 函数的输出也可能下降。这是一个有趣的 swish 特有的特性。</p><p id="175b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> f(x)=2x*sigmoid(beta*x) </strong></p><p id="2dab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们认为<strong class="kk iu"> beta=0 </strong>是 Swish 的简单版本，是一个可学习的参数，那么 sigmoid 部分总是 1/2，f (x)是线性的。另一方面，如果β是一个非常大的值，sigmoid 就变成了一个接近两位数的函数(0 代表 x &lt; 0，1 代表 x &gt; 0)。因此 f (x)收敛于 ReLU 函数。<strong class="kk iu">因此，选择标准的 Swish 函数为β= 1。</strong>通过这种方式，提供了软插值(将变量值集与给定范围和期望精度内的函数相关联)。太棒了。已经找到了解决梯度消失问题的方法。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi oc"><img src="../Images/70aa8fa6cb715f032d3465f67a1b67aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HFI_us_0ooxn62ewEUCrvw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Mathematical Expressions of Activation Functions</figcaption></figure></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><blockquote class="nv nw nx"><p id="1d62" class="ki kj le kk b kl km ju kn ko kp jx kq ny ks kt ku nz kw kx ky oa la lb lc ld im bi translated"><a class="ae lr" href="https://www.youtube.com/watch?v=GkB4vW16QHI" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">点击此处</strong> </a> <strong class="kk iu">查看梯度和偏导数可视化！</strong></p></blockquote></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="2ecc" class="nb lt it bd lu nc oh ne lx nf oi nh ma jz oj ka md kc ok kd mg kf ol kg mj nl bi translated">应该优先选择哪种激活功能？</h1><p id="741f" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">当然，我不会说你会用它或者那个。因为我已经列出了每个激活功能的独特优缺点。<strong class="kk iu"> sigmoid </strong>函数可以用如果你说<strong class="kk iu">双曲正切</strong>或者模型因为激活函数范围广可以学的稍微慢一点。但是如果你的网络太深，计算量是主要问题，可以首选<strong class="kk iu"> ReLU </strong>。你可以决定使用<strong class="kk iu"> Leaky ReLU </strong>作为 ReLU 中渐变消失问题的解决方案。但是你做的计算比 ReLU 多。</p><blockquote class="nv nw nx"><p id="8aea" class="ki kj le kk b kl km ju kn ko kp jx kq ny ks kt ku nz kw kx ky oa la lb lc ld im bi translated"><strong class="kk iu">所以激活函数是一个关键的优化问题，你需要根据所有这些信息和你的深度学习模型的要求来决定。</strong></p></blockquote><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><a class="ae lr" href="https://giphy.com/gifs/cheezburger-meryl-streep-movies-and-tv-ZjAPnic8kycM0" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><ul class=""><li id="1184" class="oo op it kk b kl km ko kp kr oq kv or kz os ld ot ou ov ow bi translated">网络的简单和快速收敛可以是第一标准。</li><li id="4f50" class="oo op it kk b kl ox ko oy kr oz kv pa kz pb ld ot ou ov ow bi translated">ReLU 在速度方面会有优势。你必须让渐变消失。它通常用于中间层，而不是输出。</li><li id="d220" class="oo op it kk b kl ox ko oy kr oz kv pa kz pb ld ot ou ov ow bi translated">漏 ReLU 可以是梯度消失问题的第一个解决方案。</li><li id="0188" class="oo op it kk b kl ox ko oy kr oz kv pa kz pb ld ot ou ov ow bi translated">对于深度学习模型，用 ReLU 开始实验是可取的。</li><li id="cff3" class="oo op it kk b kl ox ko oy kr oz kv pa kz pb ld ot ou ov ow bi translated">Softmax 通常用于输出图层。</li></ul><p id="fd2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以找到无数的文章来评价他们的比较。我最好的建议是把手弄脏！所以，测试你自己，如果你准备好了… </p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="e229" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">激活函数的定义和绘制</h2><p id="0980" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">首先，让我们看看激活函数的识别和绘制:</p><p id="1a43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🌈<a class="ae lr" href="https://colab.research.google.com/drive/1JGRVpj0-0uaDrUqBpjIw9LT9us48DC89" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">你可以在 Google Colab 里找到代码。</strong> </a></p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pc on l"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/30895db616fd1678a11a1a256d7bb8d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*UN79Z_9jSAEqvd5jiov32Q.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Demonstration of Activation Functions</figcaption></figure></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="cdc0" class="nb lt it bd lu nc oh ne lx nf oi nh ma jz oj ka md kc ok kd mg kf ol kg mj nl bi translated">激活功能的性能评估</h1><blockquote class="nv nw nx"><p id="1d8a" class="ki kj le kk b kl km ju kn ko kp jx kq ny ks kt ku nz kw kx ky oa la lb lc ld im bi translated">让我们来看看卷积神经网络模型在经典 MNIST 数据集上的激活函数的比较，我们称之为最先进的</p></blockquote><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/9b9282a3ad1f8762fc586c93cdec7a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*A3ShQ6aar5WaGEaLjNposw.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Summary of Deep Learning Model Used</figcaption></figure><p id="8353" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当具有两个卷积层的模型应用于 Sigmoid、双曲线正切、ReLU、Leaky ReLU 和 Swish 函数时，您可以观察到一些函数是如何领先于其他函数的，以及一些函数是如何接近的。您可以测试不同的数据集。此外，其他的，epoch，batch-size，dropout 等参数的影响也可以考察。也许我下一篇文章的主题可以是其中之一！</p><blockquote class="nv nw nx"><p id="a34a" class="ki kj le kk b kl km ju kn ko kp jx kq ny ks kt ku nz kw kx ky oa la lb lc ld im bi translated"><strong class="kk iu">给出了 20 个历元的样本验证、训练精度和损失值的结果。测试结果也如下表所示。</strong></p></blockquote><div class="lg lh li lj gt ab cb"><figure class="pf lk pg ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><img src="../Images/bdd84a0c82c9beb1586f22b7aac40c33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Kae-q8k2ylOV05gsLqbMtg.png"/></div></figure><figure class="pf lk pg ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><img src="../Images/161300f95ece965fd58cab8018722225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*9oIKvA0aKwXVuJZXLpDNxQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk pl di pm pn">Comparison of Validation and Training for Different Activation Functions (TRAINING)</figcaption></figure></div><div class="ab cb"><figure class="pf lk pg ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><img src="../Images/3574139f502a388e63f9e64c8ad778b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*zdjuydyYjFonUWKbssVr4A.png"/></div></figure><figure class="pf lk pg ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><img src="../Images/b4ea70c3670de24d3bc41e667bf36e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*N5Gea7Qee8FZJaDOJigIdw.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk pl di pm pn">Comparison of Validation and Training for Different Activation Functions (LOSS)</figcaption></figure></div><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi po"><img src="../Images/5c17b683611bf5b3ae27e3d11a9d6a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dmOQkB7se7VQc8gHKkWzYw.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/9956b4589ff422fc48d8b3b0f92453ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:176/format:webp/0*XY5A0f00D03pncCk.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Comparison of Activation Functions for Deep Neural Networks by Merve Ayyüce Kızrak is licensed under a <a class="ae lr" href="http://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</figcaption></figure><p id="107d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🌈你可以在谷歌实验室找到代码👇🏻</p><div class="pq pr gp gr ps pt"><a href="https://colab.research.google.com/drive/1fCVQ8JFiuyVWs6BoCIw0UqWPZPshU25h" rel="noopener  ugc nofollow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">激活函数比较 _ 源代码 Google Colab</h2><div class="qa l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">colab.research.google.com</p></div></div><div class="qb l"><div class="qc l qd qe qf qb qg ll pt"/></div></div></a></div><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="qh on l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><a class="ae lr" href="https://giphy.com/gifs/party-excited-birthday-YTbZzCkRQCEJa/links" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="7654" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">🌈激活函数如何帮助创建非线性决策极限？<a class="ae lr" href="https://github.com/snnclsr/neural_nets_from_scratch/blob/master/why_need_activations.ipynb" rel="noopener ugc nofollow" target="_blank">您可以在此处获得其他申请！</a></h2></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="cbab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">所以，欢迎随时关注我关于</strong> <a class="ae lr" href="https://medium.com/@ayyucekizrak" rel="noopener"> <strong class="kk iu">中</strong></a><strong class="kk iu"/><a class="ae lr" href="https://twitter.com/ayyucekizrak" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">推特</strong> </a> <strong class="kk iu">，以及</strong><a class="ae lr" href="https://www.linkedin.com/in/merve-ayyuce-kizrak/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">LinkedIn</strong></a><strong class="kk iu">👽</strong></p><p id="e627" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想用土耳其语阅读这篇文章，点击<a class="ae lr" href="https://medium.com/deep-learning-turkiye/derin-%C3%B6%C4%9Frenme-i%C3%A7in-aktivasyon-fonksiyonlar%C4%B1n%C4%B1n-kar%C5%9F%C4%B1la%C5%9Ft%C4%B1r%C4%B1lmas%C4%B1-cee17fd1d9cd" rel="noopener">这里</a>！</p><p id="895a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">❓ <em class="le">如果有任何意见或问题，请不要犹豫让我知道。</em> ❗️</p><h2 id="fce8" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">🎉我之前的一些博文！干杯！</h2><div class="pq pr gp gr ps pt"><a href="https://heartbeat.fritz.ai/capsule-networks-a-new-and-attractive-ai-architecture-bd1198cc8ad4" rel="noopener  ugc nofollow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">胶囊网络:一种新的有吸引力的人工智能架构🚨</h2><div class="qa l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">heartbeat.fritz.ai</p></div></div><div class="qb l"><div class="qi l qd qe qf qb qg ll pt"/></div></div></a></div><div class="pq pr gp gr ps pt"><a href="https://interestingengineering.com/what-is-explainable-artificial-intelligence-and-is-it-needed" rel="noopener  ugc nofollow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">什么是可解释的人工智能，是否需要？</h2><div class="qj l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">可解释的人工智能-XAI 是一个近年来经常被争论的话题，也是一个…</h3></div><div class="qa l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">interestingengineering.com</p></div></div><div class="qb l"><div class="qk l qd qe qf qb qg ll pt"/></div></div></a></div><div class="pq pr gp gr ps pt"><a href="https://interestingengineering.com/ai-vs-lawyers-the-future-of-artificial-intelligence-and-law" rel="noopener  ugc nofollow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">人工智能与律师:人工智能和法律的未来</h2><div class="qj l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">“机器会思考吗？”让我们扩展一下艾伦·图灵在 50 年代提出的这个问题。无数的灾难场景，在…</h3></div><div class="qa l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">interestingengineering.com</p></div></div><div class="qb l"><div class="ql l qd qe qf qb qg ll pt"/></div></div></a></div></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="5bf1" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">参考</h2><ul class=""><li id="97c7" class="oo op it kk b kl mm ko mn kr qm kv qn kz qo ld ot ou ov ow bi translated"><a class="ae lr" href="https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/" rel="noopener ugc nofollow" target="_blank">深度学习的基础——激活函数以及何时使用它们？</a></li><li id="766e" class="oo op it kk b kl ox ko oy kr oz kv pa kz pb ld ot ou ov ow bi translated"><a class="ae lr" href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener">理解神经网络中的激活功能</a></li><li id="0920" class="oo op it kk b kl ox ko oy kr oz kv pa kz pb ld ot ou ov ow bi translated"><a class="ae lr" href="https://arxiv.org/pdf/1804.02763.pdf" rel="noopener ugc nofollow" target="_blank">深度神经网络的非线性激活函数在 MNIST 分类任务上的比较</a></li><li id="ae2e" class="oo op it kk b kl ox ko oy kr oz kv pa kz pb ld ot ou ov ow bi translated"><a class="ae lr" href="https://arxiv.org/pdf/1710.05941v1.pdf" rel="noopener ugc nofollow" target="_blank"> SWISH:自门控激活功能</a></li><li id="90ec" class="oo op it kk b kl ox ko oy kr oz kv pa kz pb ld ot ou ov ow bi translated"><a class="ae lr" href="https://codeodysseys.com/posts/activation-functions/" rel="noopener ugc nofollow" target="_blank">开创性的激活功能</a></li><li id="bf64" class="oo op it kk b kl ox ko oy kr oz kv pa kz pb ld ot ou ov ow bi translated"><a class="ae lr" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" rel="noopener ugc nofollow" target="_blank">soft max 函数及其导数</a></li><li id="903f" class="oo op it kk b kl ox ko oy kr oz kv pa kz pb ld ot ou ov ow bi translated"><a class="ae lr" href="https://medium.com/@jaiyamsharma/experiments-with-swish-activation-function-on-mnist-dataset-fc89a8c79ff7" rel="noopener">在 MNIST 数据集上使用 SWISH 激活函数的实验</a></li></ul></div></div>    
</body>
</html>