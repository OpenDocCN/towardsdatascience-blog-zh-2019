<html>
<head>
<title>Sentiment Analysis: a practical benchmark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">情感分析:一个实用的基准</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-analysis-a-benchmark-903279cab44a?source=collection_archive---------18-----------------------#2019-07-25">https://towardsdatascience.com/sentiment-analysis-a-benchmark-903279cab44a?source=collection_archive---------18-----------------------#2019-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="be91" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 FCNNs、CNN、RNNs 和 Python 中的嵌入对客户评论进行分类。</h2></div><p id="b1e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过动手实践 Python 代码，我们展示了简单递归神经网络的局限性，并展示了嵌入如何改进用于情感分类的全连接神经网络和卷积神经网络。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/0423472f7e2d098ae770434fb2bdf832.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/0*-zb3l3GpryrHDaKb.png"/></div></figure><p id="5b9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们通过对电影评论数据集进行情感分类，展示了如何处理序列数据。情感基本上是感情，包括用自然语言写的情感、态度和观点。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/328447afe8e889b9688cebd84e78c3df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*2I5LTTZYpMrMSpNX.jpg"/></div></figure><h1 id="52ed" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">IMDB 电影评论数据集</h1><p id="e270" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">我们首先使用 Keras API 加载 IMDB 数据集。评论已经被符号化了。我们希望有一个有限的词汇表，以确保我们的单词矩阵不是任意小的。我们还希望有一个有限长度的评论，而不是必须处理非常长的句子。我们的训练数据集有 25，000 条客户评论，以及它们的正确标签(正面或负面)。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mk ml l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/aea8d3fcfeeda035b7f67d1008c4a8c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGPdcdo8pxIoheubaChN9Q.png"/></div></div></figure><p id="98f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面你可以看到一个例子，是一个数字列表，也叫做单词向量。每个数字代表一个单词在词汇表中的索引。如果您需要更深入地了解机器学习的文本表示，您可能想在继续之前看看下面的文章。</p><div class="mr ms gp gr mt mu"><a rel="noopener follow" target="_blank" href="/representing-text-in-natural-language-processing-1eead30e57d8"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd iu gy z fp mz fr fs na fu fw is bi translated">自然语言处理中的文本表示</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">理解书面单词:温习 Word2vec、GloVe、TF-IDF、单词袋、N-grams、1-hot 编码…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni lk mu"/></div></div></a></div><p id="562f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过将单词 vectors 转换回文本，下面的代码可以用来以纯文本显示评论。我们运行一个样本正面和一个样本负面审查的代码。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mk ml l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nj"><img src="../Images/a34f49a9446709e1e6326bfa7b81b4fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABHQUiFXK_kyTEFDjjTg1g.png"/></div></div></figure><h1 id="cf08" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">FCNN、CNN、RNN 的情感分析基准</h1><p id="c998" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">一个句子可以被认为是一系列具有跨时间语义联系的单词。就语义联系而言，我们的意思是，出现在句子前面的词在句子的后半部分影响句子的结构和意义。在理想情况下，一个句子中还有向后的语义联系。它们通常是通过将句子以相反的顺序输入到一个模型中来捕获的。</p><p id="0086" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">递归神经网络</strong>可用于提取、识别或表征文本的情感内容，并将其分类为正面或负面。</p><div class="mr ms gp gr mt mu"><a rel="noopener follow" target="_blank" href="/recurrent-neural-networks-explained-ffb9f94c5e09"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd iu gy z fp mz fr fs na fu fw is bi translated">递归神经网络解释</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">一个有趣的和有插图的指南来理解直觉。</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="nk l nf ng nh nd ni lk mu"/></div></div></a></div><p id="f1a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将这种方法与全连接神经网络、卷积神经网络以及两者的组合进行比较。</p><p id="a1ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的代码运行 ca。在 Google Cloud 的 GPU 实例上运行 2 小时。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mk ml l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5e065d33e03b477761dedcf7787ae336.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*j72ATJic4S9VmWucYNkFZQ.png"/></div></figure><p id="29b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">未看过的电影评论(测试数据集)的分类精度如上表所示。令人惊讶的是，具有嵌入的全连接神经网络优于其余的网络。这是一个 250 个节点完全连接的单层网络。卷积神经网络具有类似的精度。虽然 CNN 被设计为尊重图像数据中的空间结构，同时对场景中学习到的对象的位置和方向具有鲁棒性，但这一相同的原理也可以用于序列，例如电影评论中的一维单词序列。使 CNN 模型对于学习识别图像中的对象有吸引力的相同属性可以帮助学习单词段落中的结构，即对于特征的特定位置的技术不变性。在研究社区中，关于 CNNs vs RNNs 仍然存在公开的争论。在电影评论分类的情况下，与没有嵌入的全连接网络相比，简单的 RNN 在处理序列数据中是有用的。</p><h1 id="e6cb" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">自由文本上的情感预测</h1><p id="cd67" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">下面我们在自由文本评论上测试我们的模型。每个评论首先被标记化，然后被转换成用于预测的词向量。正面评论的概率被选择为大于 0.5。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mk ml l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/7d18755ac0644ee9b10b3dc0c5f67fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*T5RefctyJoDUawUXlpaI8Q.png"/></div></figure><p id="c5f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">带有嵌入的完全连接的神经网络对所有四种情绪都是正确的，而其余的模型有假阳性和假阴性。我们的 RNN 和 CNN 的性能肯定可以通过在更强的 GPU 机器上增加训练时段的数量来提高。</p><h1 id="47a0" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">结论</h1><p id="4da5" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">在本文中，我们展示了神经网络如何解决电影评论的分类问题，同时对卷积神经网络的全连接方法进行了基准测试。简单的 RNN 是一个“非常深”的前馈网络(当在时间上展开时)。除了爆炸和消失梯度的问题，在实践中，他们不能学习长期的依赖关系。LSTMs 的明确设计是为了避免长期依赖问题。这里没有涉及它们，也没有涉及朴素贝叶斯，朴素贝叶斯是一种概率性的情感分析方法。</p><p id="efb8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有助于理解 RNN 和 LSTM 运作的两个最好的博客是:</p><ol class=""><li id="2c17" class="nn no it kk b kl km ko kp kr np kv nq kz nr ld ns nt nu nv bi translated"><a class="ae nw" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li><li id="aae0" class="nn no it kk b kl nx ko ny kr nz kv oa kz ob ld ns nt nu nv bi translated">http://colah.github.io/posts/2015-08-Understanding-LSTMs/<a class="ae nw" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"/></li></ol><p id="b2e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你也可以看看我的其他文章，详细解释 LSTMs 和一般的深度学习。</p><div class="mr ms gp gr mt mu"><a rel="noopener follow" target="_blank" href="/lstm-based-african-language-classification-e4f644c0f29e"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd iu gy z fp mz fr fs na fu fw is bi translated">基于 LSTM 的非洲语言分类</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">厌倦了德法数据集？看看 Yemba，脱颖而出。力学的 LSTM，GRU 解释和应用，与…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="oc l nf ng nh nd ni lk mu"/></div></div></a></div><div class="mr ms gp gr mt mu"><a rel="noopener follow" target="_blank" href="/why-deep-learning-works-289f17cab01a"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd iu gy z fp mz fr fs na fu fw is bi translated">深度学习为什么有效:解决一个农民的问题</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">在开始是神经元:梯度下降，反向传播，回归，自动编码器，细胞神经网络…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="od l nf ng nh nd ni lk mu"/></div></div></a></div></div></div>    
</body>
</html>