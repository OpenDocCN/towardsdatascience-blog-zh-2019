<html>
<head>
<title>Why Is Machine Learning Feasible?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习为什么可行？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-learning-feasible-8e9c18b08a3c?source=collection_archive---------20-----------------------#2019-11-29">https://towardsdatascience.com/is-learning-feasible-8e9c18b08a3c?source=collection_archive---------20-----------------------#2019-11-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/53b19c491eb7e48fab1e6f2dff71fb5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTToiWrCVLw2oEbjz0Yzbw.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@rocknrollmonkey?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Rock’n Roll Monkey</a> on <a class="ae jg" href="https://unsplash.com/s/photos/robot?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="3f8e" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">对学习基础的快速探索</h2></div><p id="026e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated">现在是 2019 年末，围绕机器学习的炒作已经发展到了不合理的程度。似乎每周都有新的艺术成果被报道，一个更光滑的深度学习库出现在 GitHub 上，OpenAI 发布了一个具有更多参数的 GPT-2 模型。迄今为止，我们已经看到了令人难以置信的结果，很难不被炒作所吸引。</p><p id="db46" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，其他人警告说，机器学习承诺过多，交付不足。他们担心这种持续的行动可能会导致研究资金枯竭，导致另一个人工智能冬天。这确实是个坏消息。因此，为了遏制围绕机器学习的热情，并单枪匹马地阻止不可避免的 AI 寒冬，我将说服你<strong class="la jk">学习不可行</strong>。</p><p id="5cfd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="md">本文改编自《从数据中学习》一书[1]。</em></p><h1 id="e8ec" class="me mf jj bd mg mh mi mj mk ml mm mn mo kp mp kq mq ks mr kt ms kv mt kw mu mv bi translated"><span class="l lv lw lx bm ly lz ma mb mc di"> T </span>何的学习问题</h1><p id="e9a1" class="pw-post-body-paragraph ky kz jj la b lb mw kk ld le mx kn lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">从根本上说，机器学习的目标是找到一个函数<em class="md"> g </em>，它最接近地逼近某个未知的目标函数<em class="md"> f </em>。</p><p id="d7be" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，在监督学习中，我们在某些点上被给定了<em class="md"> f </em>的值<em class="md"> X </em>，我们使用这些值来帮助我们找到<em class="md"> g </em>。更正式的说法是，给我们一个数据集<em class="md"> D = {(x₁，y₁)，(x₂，y₂)，……，(xₙ，yₙ)} </em>其中<em class="md">yᵢ=f(xᵢ)</em>for<em class="md">x∈x</em>。我们可以通过找到一个函数<em class="md"> g </em>使得<em class="md"> g(x) ≈ f(x) </em>在<em class="md"> D </em>上，使用这个数据集来近似<em class="md"> f </em>。然而，学习的目标不是简单地在<em class="md"> D，</em>上很好地近似<em class="md"> f </em>，而是在任何地方都很好地近似<em class="md"> f </em>。也就是我们要<em class="md">一般化</em>。为了说明这一点，请看下图。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/16c85a069f435f367591e6e7afde49ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*iehVjXsBaUAqwZOB7WrB2A.jpeg"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Two different approximations to the function f.</figcaption></figure><p id="bb51" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="md"> g </em>和<em class="md">g’</em>在训练数据上完全匹配<em class="md"> f </em>(图中用“x”s 表示)。然而，<em class="md"> g </em>显然是<em class="md"> f </em>比<em class="md">g’</em>更好的近似值。我们要的是找到一个类似<em class="md"> g </em>的函数，而不是<em class="md">g’。</em></p><h1 id="8c7c" class="me mf jj bd mg mh mi mj mk ml mm mn mo kp mp kq mq ks mr kt ms kv mt kw mu mv bi translated">为什么学习不可行</h1><p id="5f87" class="pw-post-body-paragraph ky kz jj la b lb mw kk ld le mx kn lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">既然我们已经设置了学习问题，值得强调的是目标函数<em class="md"> f </em>是<em class="md">真正的</em> <em class="md">未知的</em>。如果我们知道目标函数，我们根本不需要做任何学习，而是直接使用它。而且，由于我们不知道什么是<em class="md"> f </em>，不管我们最终选择什么<em class="md"> g </em>，我们都没有<em class="md">方法</em>来验证它有多接近<em class="md"> f </em>。这看起来似乎是一个微不足道的观察，但是本文的其余部分将有望展示它的分支。</p><p id="39ac" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设目标函数<em class="md"> f </em>是一个具有三维输入空间的布尔函数，即<em class="md"> f: X → {0，1}，X = {0，1} </em>。这是一个便于我们分析的设置，因为很容易列举出空间中所有可能的函数[2]。我们想利用下面的训练数据，用函数<em class="md"> g </em>来逼近<em class="md"> f </em>。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/91a8ca586d6f05dd2d97dfd2b960aad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*pODTl73xi7ywnZXasQDDcw.jpeg"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Training data available to approximate f. Here, x is the input and y = f(x). For clarity, we use ○ to indicate an output of 0 and ● to indicate an output of 1.</figcaption></figure><p id="4de9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为我们想要找到对<em class="md"> f </em>的最佳可能近似，所以让我们只保留与训练数据一致的空间中的函数，并去掉所有其他的。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/7fd568da823b0f579a988daaec3c03d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cv6u4VNjgzvaa8ZG0B4zKQ.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">All possible target functions which agree with the training data.</figcaption></figure><p id="88ba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">很好，现在我们只剩下 8 个可能的目标函数，在上图中标记为<em class="md"> f₁ </em>到<em class="md"> f₈ </em>。为了方便起见，我们还保留了训练数据的标签。请注意，我们无法访问样本外标签，因为我们不知道目标函数是什么。</p><p id="ae09" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在问题变成了，我们选择哪个函数作为<em class="md"> g？由于我们不知道哪个<em class="md"> f </em>是真正的目标函数，也许我们可以通过选择一个与最有可能的目标函数相一致的<em class="md"> g </em>来尝试对冲我们的赌注。我们甚至可以用训练数据来指导我们的选择(学习！).这听起来是一个有前途的方法。但是首先，让我们定义一下“符合最有潜力的目标函数”是什么意思</em></p><h2 id="c79d" class="ni mf jj bd mg nj nk dn mk nl nm dp mo lh nn no mq ll np nq ms lp nr ns mu nt bi translated">学习最佳假设</h2><p id="201a" class="pw-post-body-paragraph ky kz jj la b lb mw kk ld le mx kn lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">为了确定一个假设(即选择<em class="md"> g </em>)有多好，我们首先需要定义一个目标。一个直截了当的目标是，每当假设与样本外输入的<em class="md"> fᵢ </em>之一一致时，给假设一分。例如，如果假设在所有输入上与<em class="md"> f₁ </em>一致，它得到 3 分，如果它在 2 个输入上一致，它得到 2 分，对于所有<em class="md"> fᵢ </em>以此类推。很容易看出，一个假设最多可以得 24 分。然而，要得到 24 分，假设必须与所有可能输入的所有可能目标函数一致。当然，这是不可能的，因为假设对于相同的输入必须输出 0 和 1。如果忽略不可能的场景，那么最多只能得 12 分。</p><p id="edd0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在唯一要做的就是选择一个假设并评估它。既然我们是聪明人，我们可以查看我们的训练数据，并“学习”是否有某种模式。XOR 函数似乎是一个很好的选择:如果输入有奇数个 1，则输出 1，否则输出 0。它与训练数据完全一致，所以让我们看看它在目标上做得有多好。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/f8d92041560502feedff25adcb627216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZCU73pDf396UQjyL2kAVKg.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Computing the objective value for the XOR hypothesis.</figcaption></figure><p id="f33d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们进行练习，我们会看到，在样本数据外，XOR 与一个函数完全一致(+3 分)，与两个输入的三个函数一致(+6 分)，与一个输入的三个函数一致(+3 分)，与一个函数完全不一致(+0 分)，总共 12 分。完美！我们能够找到一个可能得分最高的假设。可能还有其他同样好的假设，但是我们保证没有比异或更好的假设。甚至可能有更差的假设，得到的分数少于 12 分。事实上，让我们找一个这样的坏假设，这样我们就可以检查 XOR 确实是目标函数的一个很好的候选。</p><h2 id="35cd" class="ni mf jj bd mg nj nk dn mk nl nm dp mo lh nn no mq ll np nq ms lp nr ns mu nt bi translated">最坏的假设？</h2><p id="1caa" class="pw-post-body-paragraph ky kz jj la b lb mw kk ld le mx kn lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">如果 XOR 是最好的假设之一，那么一个很明显的坏假设的候选就是它的对立面，XOR。对于每个输入，如果 XOR 输出 1，则 XOR 输出 0，反之亦然。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/5a8f2950df48e0c7daa7457747e2616b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vYr9kTF6_Z5N6EjBER5GwQ.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Computing the objective value for the ¬XOR hypothesis.</figcaption></figure><p id="0f9f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">再次回顾练习，我们现在看到，在样本数据外，XOR 与一个函数完全一致(+3 分)，与两个输入端的三个函数一致(+6 分)，与一个输入端的三个函数一致(+3 分)，与一个函数完全不一致(+0 分)，再次得到满分。事实上，<em class="md">任何我们选择作为假设的</em>函数都会得到满分。这是因为任何函数<em class="md">必须</em>与可能的目标函数中的一个——且仅一个——完全一致。从那里，很容易看出任何<em class="md"> fᵢ </em>将在两个输入上匹配其他<em class="md"> fᵢ </em>中的三个，三个在一个输入上，一个在没有输入上。</p><p id="7d8c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">令人震惊的是，我们甚至可以不看训练数据就选择一个完美的假设。更糟糕的是，我们可以选择一个与训练数据完全不一致的假设，但它仍然可以获得满分。事实上，这正是 XOR 所做的。</p><blockquote class="nw nx ny"><p id="023a" class="ky kz md la b lb lc kk ld le lf kn lg nz li lj lk oa lm ln lo ob lq lr ls lt im bi translated">这意味着所有函数在样本外数据上与目标函数一致的可能性是相等的，不管它们是否同意训练。</p></blockquote><p id="acb7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，函数与训练数据一致的事实没有给<em class="md">任何关于它与样本外的目标函数一致程度的信息</em>。<em class="md">既然我们真正关心的是样本外的表现，为什么我们还需要学习呢？</em>这不仅适用于布尔函数，也适用于任何可能的目标函数。这里有一张图可以更清楚地说明这一点。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/b112d764d2bdc8552d04a6291af444ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5BRKFpj4zOeKlG4-6XpyUQ.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Valid values the function f could take for input &gt; n.</figcaption></figure><p id="3d1c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">知道<em class="md"> (-∞，n) </em>上的<em class="md"> f </em>并不能告诉我们它在[n，∞)上的行为。任何虚线都可能是<em class="md"> f </em>的有效延续。</p><h1 id="1382" class="me mf jj bd mg mh mi mj mk ml mm mn mo kp mp kq mq ks mr kt ms kv mt kw mu mv bi translated">概率，可取之处</h1><p id="f6e5" class="pw-post-body-paragraph ky kz jj la b lb mw kk ld le mx kn lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">也许这是你希望我说的那部分，这一切都是一个巨大的笑话，并解释我如何欺骗你的眼睛。嗯，这不是开玩笑，但显然机器学习在实践中是可行的，所以我们肯定遗漏了什么。</p><p id="e460" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">学习“起作用”的原因是由于一个重要的基本假设，即所谓的<a class="ae jg" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank">独立同分布假设</a> : <em class="md">训练数据<em class="md">和</em>样本外数据<strong class="la jk">和</strong>是独立的，并且<strong class="la jk">和</strong>是同分布的。这仅仅意味着我们的训练数据应该代表样本数据。这是一个合理的假设:如果样本外的数据差异很大，我们不可能期望我们从训练数据中学习到的东西能够推广到样本外。更简单的说，如果我们所有的训练数据都来自<em class="md"> y = 2x </em>，那么我们就不可能学到任何关于<em class="md"> y = -9x + 5 </em>的东西。</em></p><p id="c038" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">i.i.d .的假设在实践中也大致成立——或者至少不会被打破。例如，如果一家银行想要使用来自过去客户的信息(训练数据)建立一个模型，他们可以合理地假设他们的新客户(来自样本数据)不会太不同。我们在上面的布尔函数例子中没有做这个假设，所以我们不能排除任何<em class="md"> fᵢ </em>是真正的目标函数。然而，如果我们假设样本外数据的目标函数的输出类似于训练数据，那么显然 f₁或 f₂最有可能是目标函数。但是当然，我们永远无法确定😉。</p><p id="0845" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">欧根·霍塔吉<br/>2019 年 11 月 29 日</p><p id="753b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你喜欢这篇文章，请在 Medium 上关注我，以获得关于新帖子的通知。</p><h1 id="c0f0" class="me mf jj bd mg mh mi mj mk ml mm mn mo kp mp kq mq ks mr kt ms kv mt kw mu mv bi translated">脚注</h1><p id="9719" class="pw-post-body-paragraph ky kz jj la b lb mw kk ld le mx kn lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">[1]阿布-穆斯塔法等人。al，<a class="ae jg" href="https://amzn.com/1600490069" rel="noopener ugc nofollow" target="_blank">《从数据中学习》</a>(2012)——如果你想深入学习理论的基础，这是一本很棒的书。它写得很好，作者有天赋用简单直观的方式解释复杂的话题。阿布-穆斯塔法教授也有一个免费的在线课程,随书而来，我<em class="md">强烈推荐这个课程。</em></p><p id="afcd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]整个空间仅由 256 个函数组成。每个输入可以是 0 或 1，给出 2 = 8 种可能的输入组合。此外，对于每个输入组合，函数只能返回 0 或 1。因此，有 2⁸ = 256 个可能的函数。</p></div></div>    
</body>
</html>