<html>
<head>
<title>Kernel Support Vector Machines from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的核支持向量机</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-learning-data-science-step-by-step-f2a569d90f76?source=collection_archive---------26-----------------------#2019-12-13">https://towardsdatascience.com/support-vector-machines-learning-data-science-step-by-step-f2a569d90f76?source=collection_archive---------26-----------------------#2019-12-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ac7a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从最大间距分隔符到内核技巧的逐步数学和实现</h2></div><p id="728e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从 20 世纪 90 年代末，直到深度学习的兴起，具有非线性核的支持向量机(SVM)一直是领先的算法。他们能够解决像逻辑回归这样的线性分类器无法解决的许多非线性问题。最简单和最著名的例子是一个数据集，其标签分布类似于异或(XOR)真值表。</p><p id="563e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SVM 是由机器学习的主要理论家之一 Vladimir Vapnik [1]于 1992 年至 1995 年提出的。他与人合著了 Vapnik-Chervonenkis 机器学习理论[2]。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/02c14a2ef79d2d1ff2c12cf150287b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6x-Kfl48mw94qNRp5pjwjg.jpeg"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><a class="ae lr" href="https://www.pexels.com/photo/landscape-photography-of-green-and-gray-mountain-2125075/" rel="noopener ugc nofollow" target="_blank">© Jason Steffan, Pexel</a></figcaption></figure><p id="c9ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SVM 背后有三个主要想法:</p><ul class=""><li id="dd1f" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated"><strong class="kh ir">最大空白分隔符</strong>:画一条线或超平面，使分隔符和训练数据之间的距离最大化，从而引入一个空白层</li><li id="a09c" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated"><strong class="kh ir">软页边距分隔符</strong>:当不同标签的数据混淆时，考虑页边距内的样本，画出最佳分隔线</li><li id="51ab" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated"><strong class="kh ir">内核技巧</strong>:对于数据分离边界不是线性的更复杂的模型，允许使用高阶多项式甚至非多项式函数</li></ul><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mg"><img src="../Images/0feafdb1890b5c810cb7465b29d4d667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0JoJPidOa0nxEgyUv9cnNA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Dataset looking like the exclusive-or (XOR) truth table © the author</figcaption></figure><p id="ae5b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了更好地理解这些想法，我们将按顺序逐一解释。每次我们都会提供支持数学的摘要。<strong class="kh ir">完整的 Python 笔记本在 Github 上有</strong> <a class="ae lr" href="https://tonio73.github.io/data-science/classification/ClassificationSVM.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> HTML </strong> </a> <strong class="kh ir">或者</strong> <a class="ae lr" href="https://tonio73.github.io/data-science/classification/ClassificationSVM.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">木星</strong> </a> <strong class="kh ir">。</strong></p><p id="b3c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有两种方法来安装 SVM:</p><ul class=""><li id="353c" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated">通过使用更新规则的梯度下降</li><li id="31a0" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">使用求解器和拉格朗日原始或对偶形式的问题陈述</li></ul><p id="076b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，使用的是第二种解决方案。</p><h1 id="0655" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">1.最大边距分隔符</h1><p id="f553" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">使用几何解释可以更好地理解 SVM。给定一个 p 维的向量空间，我们想用一个超平面(如果 p=2，则是一条直线，即 2D 平面)来分隔它，使得标签为 1 的训练数据在超平面的一侧，标签为-1 的训练数据在超平面的另一侧。</p><p id="4318" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">超平面方程是:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ne"><img src="../Images/e7460b87e3fa3c33b272cf2828503a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*ZmmFYwAXLbUPmQ6tw4uPuQ.png"/></div></div></figure><p id="d5fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中 w 是与超平面正交的向量，b 是相对于原点定位超平面的偏差。</p><p id="3fd2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定一个向量 x 和{-1，1}中相应的二进制标号 y，到超平面的有符号距离是:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/0d9b0b5e0898323e39167d03cec99959.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*W4Q4nCXj1amSCDE0FFhcWA.png"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ng"><img src="../Images/d5998c16ae54c4afdcea9f79ef5e0a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aOO8CDL_OLh1Yi2fFKaVjQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">© the author</figcaption></figure><p id="4fed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">宽度<em class="nh"> M </em>的最大边距分隔符的表达式为:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/64b2c8015d679081664f9c0b2a0d5b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*3DHrwWJwDB77YF3HLUJPlA.png"/></div></figure><p id="35aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个优化问题可以被重写为对偶拉格朗日问题，并且由求解器提供解决方案。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nj"><img src="../Images/bcc77b5f3b69fbe8c43ea066da5871ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wB6IzNvKVel-4c5lJsDz_A.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Maximum margin separator © the author</figcaption></figure><p id="8b0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">边距边界上的点称为支持向量。</p><p id="9b57" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种分类器非常接近于逻辑回归，但是不能处理由于噪声(一种无法解释的量)而导致数据类混合的数据集。这导致了以下改进。</p><h1 id="c7e0" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">2.软利润 SVM</h1><p id="9960" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">为了克服当数据类混合时的优化问题，添加了一个新的约束来容忍误分类的数据点:位于分离超平面的错误侧的点，在边缘板之内或之外。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/15aefd5818e40df5afe6ef51119a92cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*_Go1MpJmWE2-VqD-fVDJQA.png"/></div></figure><p id="7e4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下图中，我们看到分隔线现在位于两个类的数据点内。支持向量现在包括边缘板内的点。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nl"><img src="../Images/f745bbb828aae3cfffe491d9e980e4da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViGgT8YvtwwdR9dQE4hkEQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Soft margin linear SVM © the author</figcaption></figure><p id="028d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个分类器分离超平面非常接近逻辑回归或线性判别分析(LDA)的分类器分离超平面。即使决策函数看起来不同，性能也是一样的。</p><p id="81ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们仍然无法解决异或数据集。需要第三个也是最后一个技巧。</p><h1 id="559f" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">3.内核技巧</h1><p id="9161" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">到目前为止，SVM 与其他线性分类器如逻辑回归或线性判别分析没有太大的不同。</p><p id="9f36" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，观察到 SVM 的拟合和预测仅依赖于<em class="nh"> x </em>样本的内积，出现了使用替代产品的新想法，好像通过变换<em class="nh"> h(x) </em>对<em class="nh"> x </em>进行了预处理。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/f0ddddcb0ef73a62a8ca3466552a1c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35E89AV1ZO5mvOOGeZpDDQ.png"/></div></div></figure><p id="4526" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下产品功能应是对称的和正的:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a840a5c772a5e49dc30255189ceeba02.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*FsJCkCGQVc6GwsEQFNEGuQ.png"/></div></figure><p id="8e9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其范围从简单的二次函数到更复杂的函数，如高斯径向核函数(RBF):</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b0661f7a5d145930ccde347dc9cf3f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*DtZeFs5vppvr75HF-d1WVA.png"/></div></figure><p id="f07a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于需要新的<em class="nh"> x </em>值与训练样本的乘积，预测变得更加复杂。然而，我们可以将这种计算限制到支持向量。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nl"><img src="../Images/c168076a635829e89b782ccb9894f7e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PYrDh20CXwcOo6C_jmwlvg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">XOR problem solved by an SVM with Gaussian RBF kernel © the author</figcaption></figure><p id="2886" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了具有非线性内核的 SVM，我们已经能够解决 XOR 问题，这是在 60 年代感知机发明之后第一个人工智能冬天[3]的原因之一。</p><p id="5abe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">完整的 Python 笔记本在 Github 上有</strong><a class="ae lr" href="https://tonio73.github.io/data-science/classification/ClassificationSVM.html" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">HTML</strong></a><strong class="kh ir">或者</strong><a class="ae lr" href="https://tonio73.github.io/data-science/classification/ClassificationSVM.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">Jupyter</strong></a><strong class="kh ir">的版本。</strong>它包含了<strong class="kh ir">更详细的数学，一个用 Python 定制的实现</strong>使用<strong class="kh ir"> Scipy </strong>通用解算器<strong class="kh ir">，</strong>与实现<strong class="kh ir"> Scikit 的比较学习，</strong>与<strong class="kh ir">与逻辑回归和线性判别分析的比较</strong></p><p id="6e8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇帖子是 Github“<a class="ae lr" href="https://tonio73.github.io/data-science/" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">循序渐进学习数据科学</strong> </a>”中开发的系列文章的一部分。如果你喜欢它，请访问我们的知识库，并在项目中添加一颗星来提高它的可见性。</p><p id="03e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参考资料:</p><ul class=""><li id="3da1" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated">[1]支持向量网络，Cortes，C. &amp; Vapnik，载于 v . Mach Learn(1995)20:273—【https://doi.org/10.1007/BF00994018 T4】</li><li id="769b" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">[2]维基百科上的 Vapnik Chervnonenkis 机器学习理论—<a class="ae lr" href="https://en.wikipedia.org/wiki/Vapnik–Chervonenkis_theory" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Vapnik–Chervonenkis_theory</a></li><li id="4ba7" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">[3]艾冬天上维基—<a class="ae lr" href="https://en.wikipedia.org/wiki/AI_winter" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/AI_winter</a></li><li id="7c4b" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">[4]朱庇特本帖笔记本—<a class="ae lr" href="https://tonio73.github.io/data-science/classification/ClassificationSVM.ipynb" rel="noopener ugc nofollow" target="_blank">https://medium.com/r/?URL = https % 3A % 2F %2F nio 73 . github . io % 2f 数据-科学% 2f 分类% 2f 分类 SVM.ipynb </a></li></ul></div></div>    
</body>
</html>