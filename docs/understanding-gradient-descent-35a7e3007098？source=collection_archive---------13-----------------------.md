# 了解梯度下降

> 原文：<https://towardsdatascience.com/understanding-gradient-descent-35a7e3007098?source=collection_archive---------13----------------------->

## 这一关键数据科学工具的基础

![](img/fe32c125dfb8e4a4fcb8e42822c48370.png)

最小化模型和训练数据集之间的误差是数据科学中的一项基本实践。毕竟，如果一个模型与训练数据集不匹配，那么它显然不会有任何预测能力。

关于这个概念的例子，请参见[了解线性回归的基本原理](/understanding-the-fundamentals-of-linear-regression-7e64afd614e1)和[了解多元回归](/understanding-multiple-regression-249b16bde83e)。在这两种情况下，特定类型的模型(线性回归或多元回归)通过最小化误差来拟合数据集。

为了最小化模型中的误差，还需要能够计算误差。这样做时，既要抓住情况的复杂性，又要对数据科学家的全局关注保持敏感。这需要一个写得很好的成本函数，正如在[成本函数:机器学习的基础](/cost-functions-the-underpinnings-of-machine-learning-549ac5edb211)中所描述的。

一旦选择了模型类型并编写了成本函数，最后一步就是实现一个工具来最小化成本函数，从而使模型符合数据。成本函数将根据需要改变模型中的参数，以找到最佳拟合。

梯度下降是这个角色的有力工具。

## 什么是梯度下降？

让我们通过一个例子来解释梯度下降。想象一条抛物线。抛物线既有不断变化的斜率，也有一个极小点。最小点出现在斜率为零的地方，因为这也是另一侧斜率开始增加的地方。

对于熟悉微积分的人来说，这相当于说最小值出现在方程的导数等于零的点上。

现在想象我们从某个点开始，想要到达最小值。我们现在不需要具体说明某一点，因为这都是概念性的。我们可以从起点计算任意方向的斜率，确定哪个斜率是正的，哪个是负的，然后向负斜率的方向移动。这将自动使我们更接近最小值。

逐步反复朝那个方向走，最终会找到最小点。

这就是梯度下降的过程。梯度下降是一系列函数，1)自动识别任意给定点在所有方向上的斜率，2)调整方程的参数以向负斜率的方向移动。这逐渐把你带到一个最低点。

用梯度下降法求一条抛物线的极小点有点傻；对于这个简单的例子来说，这是一个非常强大的工具。但是，当有许多变量，在许多不同的方向上有许多不同的斜率时，会发生什么呢？这就是自动化过程的力量变得清晰的地方。

## 梯度下降是如何工作的？

G 梯度下降自动执行我们之前抛物线示例中描述的所有步骤。特别是它:

*   计算每个变量在该点的偏导数，
*   结合关于每个变量的偏导数以识别向量空间中最负斜率的方向，
*   朝着最负斜率的方向前进，
*   重复该过程，直到找到最小点。

这个过程不会立即找到最小点。偏导数中没有足够的信息来告诉梯度下降算法最小点在哪里，相反它只知道向哪个方向移动。因此，梯度下降必须估计最佳步长，向那个方向移动，重新计算偏导数，估计最佳步长，向那个方向移动…等等，直到它最终找到最小值。

这回避了关于步长的问题。梯度下降算法如何选择步长？

## 我如何选择步长？

事实上，这可能是一个棘手的选择。它通常被描述为一门艺术而不是一门科学。

小步走是最不可能超调和错过最小点的。这很好。小步走最准。另一方面，它们会导致极其耗时的搜索。

想象一下，运行梯度下降，并希望如此确定，你准确地找到了最小值，你使用一个微小的步长，算法需要迭代十亿次。这可不好。

但是，与此同时，您确实希望保持步长足够小，以确保不会超调。

这种困境的一些可能的解决方案包括:

*   随着时间的推移，你可以逐渐减小步长。这是可行的，因为在开始时超过最小点的风险很小。您可以使用较大的步长对整个参数空间进行粗略搜索。然后，随着算法对最小值的大致位置越来越有信心，它开始使用较小的步长来缩小特定点的范围。
*   您还可以计算在该方向上产生最小点的步长，并执行该步长。以这种方式，模型识别移动的方向，然后识别在该方向上移动的最佳距离，并且这样做。一旦到达那里，它就搜索新的最佳方向和距离。这个过程大大减少了寻找最小值所需的迭代次数。

不幸的是，这两种方法都不是明显的赢家。

逐渐减小步长的效果并不理想，因为步长减小可能太慢或太快。步长的快速减小导致该算法试图避免的问题，微小的步长和许多迭代。缓慢减小步长会导致超过最小值点的另一个问题。

计算最佳步长非常有效，但是计算量非常大。它确实减少了迭代次数，但每次迭代花费的时间太长，不会显著减少计算时间。

一个很好的解决方案是使用一个计算最优步长的伪版本。这可以通过为算法提供一系列可能的步长来实现。然后，该算法遍历这些可能的步长，找到具有最佳结果的步长，并执行该步长。这比随时间自动改变步长更精确，并且在每一点计算最佳步长更有时间效率。

它提供了灵活的解决方案，在每一点上都能很好地工作，而不需要太大的计算开销。

## 梯度下降的局限性是什么？

梯度下降法的最大限制是计算时间。对大型数据集中的复杂模型执行此过程可能需要很长时间。这部分是因为梯度必须在每一步对整个数据集进行计算。

这个问题最常见的解决方案是随机梯度下降。

## 什么是随机梯度下降？

随机梯度下降基于参数空间中每个点的误差是可加的假设。第一点的误差可以加到第二点的误差上，第二点的误差可以加到第三点的误差上，对于所有点都是如此。

这意味着模型不需要同时计算所有点的误差(一个计算量很大的过程)。相反，它可以单独计算每个点的误差，并将它们相加。

顺便说一下，这个假设几乎总是正确的。最好的做法是思考一会儿，以确保它在你的问题中是正确的，而不是简单地假设它是正确的，但你会发现它在大多数情况下是正确的。

通过使用该假设，随机梯度下降能够依次计算每个点的误差。这大大节省了计算时间。

这极大地提高了梯度下降过程中每一步的速度。随机梯度下降仍然是一个迭代的过程，确定一个方向，一步一步向最小值前进。

随机梯度下降的缺点是它不能可靠地找到真正的最小值。相反，它倾向于非常接近，然后永远围绕最小值。为了避免这种情况，设置一个改进阈值是很重要的。如果通过梯度下降函数的重复迭代没有在[成本函数](/cost-functions-the-underpinnings-of-machine-learning-549ac5edb211?source=your_stories_page---------------------------)中产生显著的减少，那么你仅仅是在围绕最小点。是时候退出梯度下降过程，接受可用的结果了。

## 将这一切结合在一起

梯度下降是生成数据科学预测模型的重要工具。这是一种常见的优化方法，用于最小化[成本函数](/cost-functions-the-underpinnings-of-machine-learning-549ac5edb211)，从而找到模型中的最佳参数。例如，它通常用于将[线性回归](/understanding-the-fundamentals-of-linear-regression-7e64afd614e1)或[多元回归](/understanding-multiple-regression-249b16bde83e)模型拟合到训练数据集。

该算法的工作原理是计算曲线在数据空间中的点在每个方向上的斜率，并调整参数以向最负斜率的方向移动。对于那些熟悉微积分的人来说，它是通过偏导数来实现的。

确定步长是使用梯度下降的一个基本挑战。小的步长确保找到真正的最小值，但是也产生许多许多迭代和长的计算时间。大步长运行得更快，但更有可能超过最小点。常见的解决方案是为模型提供一些不同的步长供选择。然后，它计算每一步长的改进，并移动到那个距离。这比使用大的步长提高了精度，同时避免了小步长的计算时间挑战。

使用梯度下降优化参数集可能是一个非常耗时的过程。这是因为它必须同时计算整个数据集的误差。随机梯度下降通过假设误差是可加的来避免这个问题。这样，它可以依次计算每一点的误差，减少对计算机的计算要求。它产生明显更快的结果。

[1]对于没学过微积分的人来说，偏导数就是单变量的方程的斜率。比如 A = 0.25 + 0.9 * B + B * C + 1.2 C 对 B 的偏导数等于 0.9 + C。

[2]“最负斜率”是指大概会有很多负斜率的方向。为了尽可能快地找到最小值，梯度下降使用负斜率绝对值最高的方向。

[3]它能走多远？它如何知道要使用的步长？好问题。请继续阅读，这些问题很快就会得到解决。