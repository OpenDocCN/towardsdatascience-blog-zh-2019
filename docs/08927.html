<html>
<head>
<title>Do you know K_Nearest_Neighbors can also be used for regression tasks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你知道 K_Nearest_Neighbors 也可以用于回归任务吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/do-you-know-k-nearest-neighbors-can-also-be-used-for-regression-tasks-117da22bcac3?source=collection_archive---------18-----------------------#2019-11-28">https://towardsdatascience.com/do-you-know-k-nearest-neighbors-can-also-be-used-for-regression-tasks-117da22bcac3?source=collection_archive---------18-----------------------#2019-11-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="5e18" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">是的，它是！让我们来理解这个简单的算法</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/503afecfd06648a7d94a0c90f84c8e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*gC4Cr1MkXBMRgJSbHwHOpw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">source: researchgate.net</figcaption></figure><p id="dfb6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你有什么好处？</p><p id="b6bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文将揭示最简单易懂的最大似然算法——K 近邻算法。它可用于分类和回归任务，但在分类中更常见，因此我们将重点放在分类上，并了解如何将其用作回归变量。不过，这些原则在这两种情况下都适用。</p><p id="043c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">我们来谈谈数据</strong></p><p id="8a58" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一次，我不会像在我的其他文章中通常所做的那样挑选任何数据集。如果数据集能给我一个机会来解释一个概念、一个陷阱或一种方法，那么我会选择数据集。就像我承认的，这是最简单的 ML 算法。我将使用与 sklearn 捆绑在一起的乳腺癌数据集。</p><p id="b843" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">就这么简单吗？</strong></p><p id="d0e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">基本上，算法如下:</p><ol class=""><li id="a52e" class="la lb it js b jt ju jx jy kb lc kf ld kj le kn lf lg lh li bi translated">定义𝑘</li><li id="87c7" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated">定义距离度量，通常是欧几里德距离(2 范数距离)</li><li id="64ba" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lf lg lh li bi translated">对于新的数据点，找到𝑘最近的训练点，并以某种方式(通常是投票)组合它们的类，以获得预测的类</li></ol><p id="61e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就是这样！</p><p id="9fc3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">KNN 的好处</strong></p><ul class=""><li id="d70c" class="la lb it js b jt ju jx jy kb lc kf ld kj le kn lo lg lh li bi translated">它实际上不需要任何传统意义上的训练。你只需要一个快速的方法来找到最近的邻居。</li><li id="45a5" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lo lg lh li bi translated">容易理解</li></ul><p id="1e15" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">KNN 的弊端</strong></p><ul class=""><li id="5f7d" class="la lb it js b jt ju jx jy kb lc kf ld kj le kn lo lg lh li bi translated">我们需要定义 k，这是一个超参数，因此它可以通过交叉验证进行调整。k 值越高，偏差越大，k 值越小，方差越大。偏差和方差总是在跷跷板上，所以从技术上来说，你不可能一箭双雕。必须有所取舍。</li></ul><p id="2300" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">专业提示:</strong>在一个模型中，如果你必须减少方差，然后引入大的训练集，这不是在所有情况下都可行的选择。如果您必须减少偏差，那么添加特征(预测值)会有所帮助，但代价是引入额外的方差。我的<a class="ae lp" href="https://medium.com/analytics-vidhya/linear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323/#822e" rel="noopener">文章</a>中有更多提示</p><ul class=""><li id="29ec" class="la lb it js b jt ju jx jy kb lc kf ld kj le kn lo lg lh li bi translated">必须选择一个距离度量，根据度量的不同，可能会得到非常不同的结果。同样，你可以使用交叉验证。</li><li id="d307" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lo lg lh li bi translated">它并没有真正提供哪些特性可能是重要的见解。</li><li id="c8eb" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lo lg lh li bi translated">由于维数灾难，它可能会受到高维数据的影响。</li></ul><p id="a1fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">基本假设:</strong></p><ul class=""><li id="960b" class="la lb it js b jt ju jx jy kb lc kf ld kj le kn lo lg lh li bi translated">对于我们的目标来说，接近的数据点是相似的</li></ul><p id="57d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">什么是维度的诅咒？</strong></p><p id="cf19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一种忧郁的表达，但它意味着数据中更高的维度顺序。</p><p id="c2ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为什么这是一个问题？我听到了。我们无法将数据可视化。可视化就像第一世界国家的问题，有更多的噪音，但实际上非常琐碎。</p><p id="c7a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更大的问题是，当你增加维度时，代表你的数据的数据点也会增加，数据变得更加分散。很可能近点并不比平均距离近多少，也就是说近并没有多大意义。KNN 在数据点之间的距离上工作，因此它将使这个算法低效。</p><p id="cc0f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你很好奇，想直观的知道为什么会这样。<a class="ae lp" href="https://blog.galvanize.com/how-to-manage-dimensionality/" rel="noopener ugc nofollow" target="_blank">去这里</a></p><h2 id="4f81" class="lq lr it bd ls lt lu dn lv lw lx dp ly kb lz ma mb kf mc md me kj mf mg mh mi bi translated">欧几里得距离</h2><p id="26ef" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi translated">对于被比较的向量 q 和 p(这些将是我们的特征向量):</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/bec33d0e94a4c6dbc13a07eb372943a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*F507EfvD_1GAA42iLcCANw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Source: Wiki</figcaption></figure><p id="aae3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这不是结局。还有许多其他距离度量。我将在本文后面讨论术语。该算法最优选的距离范数是曼哈顿(1 范数)或欧几里德(2 范数)。</p><h1 id="c73b" class="mp lr it bd ls mq mr ms lv mt mu mv ly mw mx my mb mz na nb me nc nd ne mh nf bi translated"><strong class="ak">型号</strong></h1><p id="b4eb" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi translated">你可以在这里跟随代码<a class="ae lp" href="https://github.com/hdev7/medium-KNN-follow-up-code" rel="noopener ugc nofollow" target="_blank"/></p><p id="f4fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你如何确定 K 值？</p><p id="d1f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">没有科学的方法来确定一个。我们只需要反复试验。网格搜索为了解决这个问题，我将 KNNClassifier 管道化到网格搜索中，以了解最佳超参数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/5c8e2754cb03474a7eaf61961e1bdf07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MaWcXIQaGwT6W_vtL5XP2w.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">grid search report</figcaption></figure><p id="f890" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看起来在这种情况下，K = 5 的均匀权重方法效果最好。</p><p id="b7b3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">了解关于分类指标的更多信息。跟进我的<a class="ae lp" href="https://medium.com/analytics-vidhya/logistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be/#805e" rel="noopener">文章</a></p><h1 id="3ecf" class="mp lr it bd ls mq mr ms lv mt mu mv ly mw mx my mb mz na nb me nc nd ne mh nf bi translated">投票方法的类型</h1><ul class=""><li id="6e20" class="la lb it js b jt mj jx mk kb nl kf nm kj nn kn lo lg lh li bi translated">多数投票:在你选择了𝑘最近的邻居后，你对这些邻居的班级进行了“投票”。新的数据点被分类为邻居的大多数类。如果您正在进行二元分类，建议您使用奇数个邻居，以避免票数相等。然而，在一个多类问题中，更难避免平局。对此的常见解决方案是降低𝑘，直到平局被打破。</li><li id="3df0" class="la lb it js b jt lj jx lk kb ll kf lm kj ln kn lo lg lh li bi translated">距离加权:不是直接对最近的邻居进行投票，而是根据该实例与新数据点的距离对每一票进行加权。一种常见的加权方法是</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi no"><img src="../Images/cb2f25bb9f135ff6ee9b270529a6669a.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*hPKdWHZNOTDNEKeRwimC6w.png"/></div></div></figure><p id="9dd0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">或者是新数据点和训练点之间的距离。新的数据点被添加到具有最大增加权重的类别中。这不仅减少了平局的机会，而且还减少了数据失真的影响。</p><h1 id="6e2f" class="mp lr it bd ls mq mr ms lv mt mu mv ly mw mx my mb mz na nb me nc nd ne mh nf bi translated">距离度量</h1><p id="ed1b" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi translated">欧几里德距离或 2 范数是一种非常常用的𝑘-nearest 邻居距离度量。任何𝑝-norm 都可以使用。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3bc17565ccccf9c8a388203b3eb89aba.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*8Q34CStgbEFnEGg68ZMX6A.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">p-norm</figcaption></figure><p id="5e7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，对于分类数据，这可能是一个问题。例如，如果我们将汽车颜色的特征从红色、蓝色和绿色编码为 0、1、2，如何测量绿色和红色之间的“距离”？您可以创建虚拟变量，但是如果一个特性有 15 个可能的类别，这意味着您的特性集要增加 14 个变量，我们会遇到维数灾难。但是要注意分类特征对最近邻分类器的影响。</p><h1 id="df10" class="mp lr it bd ls mq mr ms lv mt mu mv ly mw mx my mb mz na nb me nc nd ne mh nf bi translated">搜索算法</h1><p id="7fb4" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi translated">假设数据集包含 2000 个点。对一个点的 3 个最近邻居的强力搜索不需要很长时间。但是如果数据集包含 2000000 个点，强力搜索可能会变得非常昂贵，尤其是在数据的维度很大的情况下。其他搜索算法为了更快的运行时间而牺牲了穷举搜索。像<a class="ae lp" href="https://en.wikipedia.org/wiki/K-d_tree" rel="noopener ugc nofollow" target="_blank"> KDTrees </a>或<a class="ae lp" href="https://en.wikipedia.org/wiki/Ball_tree" rel="noopener ugc nofollow" target="_blank"> Ball trees </a>这样的结构用于更快的运行时间。虽然我们不会深入研究这些结构的细节，但请注意它们以及它们如何优化您的运行时间(尽管训练时间确实会增加)。</p><h1 id="9808" class="mp lr it bd ls mq mr ms lv mt mu mv ly mw mx my mb mz na nb me nc nd ne mh nf bi translated">另一种类型:半径邻居分类器</h1><p id="4f08" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi translated">这与𝑘最近邻分类器的想法相同，但不是查找𝑘最近邻，而是查找给定半径内的所有邻居。设置半径需要一些领域知识；如果您的点紧密地聚集在一起，您可能希望使用较小的半径来避免几乎每个点都投票。</p><h1 id="cd67" class="mp lr it bd ls mq mr ms lv mt mu mv ly mw mx my mb mz na nb me nc nd ne mh nf bi translated">KNN 回归量</h1><p id="4eca" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi translated">要将我们的问题从分类转变为回归，我们所要做的就是找到𝑘最近邻的加权平均值。我们使用与上述相同的加权方法，计算这些最接近值的加权平均值，而不是取多数类。</p><p id="d4b3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们尝试使用 KNN 回归器根据其他特征来预测组织的面积。跟随代码<a class="ae lp" href="https://github.com/hdev7/medium-KNN-follow-up-code" rel="noopener ugc nofollow" target="_blank">这里</a></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nq"><img src="../Images/29d46107f482a08b1fdb0fd6ad0c3573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5bhkOAtJkj_OW5ZiXJGtgA.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Now you know why nobody uses it for regression tasks</figcaption></figure><h1 id="6e12" class="mp lr it bd ls mq mr ms lv mt mu mv ly mw mx my mb mz na nb me nc nd ne mh nf bi translated">一个应用:异常检测</h1><p id="29ba" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi translated">在𝑘k-nearest 的邻居中，数据自然是聚集的。在这些聚类中，我们可以找到点之间的平均距离(或者穷尽地或者从聚类的质心开始)。如果我们发现几个点比到其他点或质心的平均距离远得多，那么有理由(但不总是正确的)认为它们可能是异常值。我们也可以在新的数据点上使用这个过程。</p></div></div>    
</body>
</html>