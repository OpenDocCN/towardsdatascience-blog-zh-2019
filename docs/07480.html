<html>
<head>
<title>Deep CNN-Based Blind Image Quality Predictor in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中基于深度 CNN 的图像质量盲预测器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-image-quality-assessment-with-tensorflow-2-0-69ed8c32f195?source=collection_archive---------10-----------------------#2019-10-19">https://towardsdatascience.com/deep-image-quality-assessment-with-tensorflow-2-0-69ed8c32f195?source=collection_archive---------10-----------------------#2019-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="aa11" class="jr js it bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">介绍</h1><p id="33af" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本教程中，我们将实现 Jongio Kim、Anh-Duc Nguyen 和 Sanghoon Lee [1]提出的<em class="ln">基于深度 CNN 的盲图像质量预测器(DIQA) </em>方法。此外，我将介绍以下 TensorFlow 2.0 概念:</p><ul class=""><li id="a359" class="lo lp it kr b ks lq kw lr la ls le lt li lu lm lv lw lx ly bi translated">使用<em class="ln"> tf.data.Dataset </em>构建器下载并准备数据集。</li><li id="216b" class="lo lp it kr b ks lz kw ma la mb le mc li md lm lv lw lx ly bi translated">使用<em class="ln"> tf.data </em> API 定义一个 TensorFlow 输入管道来预处理数据集记录。</li><li id="fe9e" class="lo lp it kr b ks lz kw ma la mb le mc li md lm lv lw lx ly bi translated">使用<em class="ln"> tf.keras </em>函数 API 创建 CNN 模型。</li><li id="8fab" class="lo lp it kr b ks lz kw ma la mb le mc li md lm lv lw lx ly bi translated">为客观误差图模型定义自定义训练循环。</li><li id="5709" class="lo lp it kr b ks lz kw ma la mb le mc li md lm lv lw lx ly bi translated">训练客观误差图和主观评分模型。</li><li id="b3f4" class="lo lp it kr b ks lz kw ma la mb le mc li md lm lv lw lx ly bi translated">使用训练好的主观评分模型进行预测。</li></ul><p id="6f7d" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated"><em class="ln">注:部分功能在</em><a class="ae mh" href="https://github.com/ocampor/image-quality/blob/master/notebooks/utils.py" rel="noopener ugc nofollow" target="_blank"><em class="ln">utils . py</em></a><em class="ln">中实现，不在本指南讨论范围内。</em></p><h2 id="494a" class="mi js it bd jt mj mk dn jx ml mm dp kb la mn mo kf le mp mq kj li mr ms kn mt bi translated">迪迦是什么？</h2><p id="ead1" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">DIQA 是一项原创提案，专注于解决将深度学习应用于图像质量评估(IQA)的一些最受关注的挑战。相对于其他方法的优势是:</p><ul class=""><li id="2d41" class="lo lp it kr b ks lq kw lr la ls le lt li lu lm lv lw lx ly bi translated">该模型不限于专门处理自然场景统计(NSS)图像[1]。</li><li id="64f2" class="lo lp it kr b ks lz kw ma la mb le mc li md lm lv lw lx ly bi translated">通过将训练分为两个阶段(1)特征学习和(2)将学习到的特征映射到主观分数，防止过度拟合。</li></ul><h2 id="06b8" class="mi js it bd jt mj mk dn jx ml mm dp kb la mn mo kf le mp mq kj li mr ms kn mt bi translated">问题</h2><p id="ff2d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">生成 IQA 数据集的成本很高，因为它需要专家的监督。因此，基本的 IQA 基准仅由几千条记录组成。后者使深度学习模型的创建变得复杂，因为它们需要大量的训练样本来进行归纳。</p><p id="c7fe" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">例如，让我们考虑最频繁使用的数据集来训练和评估 IQA 方法<a class="ae mh" href="https://live.ece.utexas.edu/research/quality/subjective.htm" rel="noopener ugc nofollow" target="_blank"> Live </a>，<a class="ae mh" href="http://www.ponomarenko.info/tid2008.htm" rel="noopener ugc nofollow" target="_blank"> TID2008 </a>，<a class="ae mh" href="http://www.ponomarenko.info/tid2013.htm" rel="noopener ugc nofollow" target="_blank"> TID2013 </a>，<a class="ae mh" href="http://vision.eng.shizuoka.ac.jp/mod/page/view.php?id=23" rel="noopener ugc nofollow" target="_blank"> CSIQ </a>。下表包含每个数据集的总体摘要:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mu"><img src="../Images/b57d90bf5738105afb0394c3e64c4fe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qedqmQJDYJ1om2qaSiXAoA.png"/></div></div></figure><p id="29af" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">其中任何一个样本的总量都不超过 4，000 条记录。</p><h1 id="6530" class="jr js it bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">资料组</h1><p id="068a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">IQA 基准只包含有限数量的记录，可能不足以训练 CNN。然而，出于本指南的目的，我们将使用<a class="ae mh" href="https://live.ece.utexas.edu/research/quality/subjective.htm" rel="noopener ugc nofollow" target="_blank">实时</a>数据集。它由 29 幅参考图像和 5 种不同的失真组成，每种失真有 5 种严重程度。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ng"><img src="../Images/c3f60949c7e22dcbdd8690c5ade4d638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nehRT1u1lD19hZTzQj96yA.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk"><strong class="bd nl">Fig 1. </strong>An example of a reference image in Live dataset.</figcaption></figure><p id="e68d" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">第一项任务是下载和准备数据集。我为图像质量评估创建了几个 TensorFlow 数据集构建器，并在<a class="ae mh" href="https://github.com/ocampor/image-quality" rel="noopener ugc nofollow" target="_blank">图像质量</a>包中发布了它们。构建器是由<a class="ae mh" href="https://www.tensorflow.org/datasets" rel="noopener ugc nofollow" target="_blank"> tensorflow-datasets </a>定义的接口。</p><p id="6f66" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated"><em class="ln">注意:由于数据集的大小(700 兆字节)，这个过程可能需要几分钟。</em></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nm"><img src="../Images/7a1997c1dd4036af0dcc7c315706e6d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p-fbSRin-IwIutHaqJV2tQ.png"/></div></div></figure><p id="3f53" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">下载并准备好数据后，将构建器转换成数据集，并对其进行洗牌。请注意，批次等于 1。原因是每个图像都有不同的形状。增加批量大小将导致错误。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nn"><img src="../Images/adc3ddb7aba12c4555604bb1e093c485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3seeSXOpdmaZmALwdHNf7A.png"/></div></div></figure><p id="4d7f" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">输出是发电机；因此，使用括号运算符访问样本会导致错误。有两种方法可以访问发生器中的图像。第一种方法是将生成器转换成迭代器，并使用<em class="ln"> next </em>函数提取单个样本。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nm"><img src="../Images/0849bd2603925e632f0c076f909c7135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UapblemcLA6ozMA4pKloZQ.png"/></div></div></figure><p id="fbac" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">输出是包含失真图像、参考图像和主观分数(dmos)的张量表示的字典。另一种方法是通过 for 循环从生成器中提取样本:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi no"><img src="../Images/d2e7f160a636892eb69b2f806025e9e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*US9XMOvSN8k0PWg6cLOyFw.png"/></div></div></figure><h1 id="548f" class="jr js it bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">方法学</h1><h2 id="b0ca" class="mi js it bd jt mj mk dn jx ml mm dp kb la mn mo kf le mp mq kj li mr ms kn mt bi translated">图像标准化</h2><p id="f243" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">DIQA 的第一步是预处理图像。图像被转换成灰度，然后应用低通滤波器。低通滤波器定义为:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a3d7f9f9df75590b8501d934db6a5a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*lmZeLaRYH8j1_P4Zxm2-ZA.png"/></div></figure><p id="488b" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">其中低频图像是以下算法的结果:</p><ol class=""><li id="0921" class="lo lp it kr b ks lq kw lr la ls le lt li lu lm nq lw lx ly bi translated">模糊灰度图像。</li><li id="9604" class="lo lp it kr b ks lz kw ma la mb le mc li md lm nq lw lx ly bi translated">将它缩小 1 / 4。</li><li id="5486" class="lo lp it kr b ks lz kw ma la mb le mc li md lm nq lw lx ly bi translated">把它放大到原来的大小。</li></ol><p id="4555" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">这种标准化的主要原因是(1)人类视觉系统(HVS)对低频带的变化不敏感，以及(2)图像失真几乎不影响图像的低频分量。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nr"><img src="../Images/f1804a0857b309a2c566ad77a32fcbed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tCuTOV-Wp8bB3QNkEKB5HQ.png"/></div></div></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ns"><img src="../Images/3f037a679834cce850c8c7859a403b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ZpfNC_FbxJuqOpTwsHdSg.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk"><strong class="bd nl">Fig 2.</strong> On the left, the original image. On the right, the image after applying the low-pass filter.</figcaption></figure><h2 id="4dac" class="mi js it bd jt mj mk dn jx ml mm dp kb la mn mo kf le mp mq kj li mr ms kn mt bi translated">客观误差图</h2><p id="f6b8" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于第一个模型，使用客观误差作为代理，以利用数据增加的影响。损失函数由预测误差图和地面真实误差图之间的均方误差定义。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi np"><img src="../Images/639cc1619bb215605ea0b3169e1c235d.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*y0wjGlwOW0ObX0rGMiu1gQ.png"/></div></div></figure><p id="063e" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">而<em class="ln"> err( ) </em>可以是任何误差函数。对于这种实现，作者建议使用</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d89a5157a54e1f7f90fc1f04ca1e9682.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*-Yw1YK9U1r33-6RuQiDVAQ.png"/></div></figure><p id="0555" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">用<em class="ln"> p=0.2 </em>。后者是为了防止误差图中的值很小或接近零。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nu"><img src="../Images/95a635872004e0df16e5607412a549ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTX0Fx-RwFnC_uQAYmv8iw.png"/></div></div></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nv"><img src="../Images/044554738202749530cb2048483f5966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ZGQo_S8iSnOJm49uV92Rg.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk"><strong class="bd nl">Fig 3.</strong> On the left, the original image. In the middle, the pre-processed image, and finally, the image representation of the error map.</figcaption></figure><h2 id="6a64" class="mi js it bd jt mj mk dn jx ml mm dp kb la mn mo kf le mp mq kj li mr ms kn mt bi translated">可靠性地图</h2><p id="1a9d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">根据作者的说法，该模型很可能无法预测具有同质区域的图像。为了防止它，他们提出了一个可靠性函数。假设模糊区域比纹理区域可靠性低。可靠性函数定义为</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/657537dc02baeca576380ab54018ff34.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*7kitfDF8yEoND3j0ur6LvQ.png"/></div></figure><p id="0671" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">其中α控制可靠性图的饱和特性。sigmoid 的正部分用于将足够大的值分配给低亮度的像素。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nx"><img src="../Images/87d7343ac17b405604ab4e2365e36a5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lw4QdTJ2ZZsnX0a5ZiGtQg.png"/></div></div></figure><p id="2cab" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">之前的定义可能会直接影响预测得分。因此，使用平均可靠性图来代替。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/fcc951cd90ada05d6cf9b1a6d02a9996.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*M1E3aosA-gc_KcMgLJ8zbw.png"/></div></figure><p id="ac0a" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">对于 Tensorflow 函数，我们只需计算可靠性图，并除以其均值。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nz"><img src="../Images/debca080364d85d3238d60ee901e340d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-aS3Lm4lc3Fj6s8xOt8g1g.png"/></div></div></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oa"><img src="../Images/b2957514ad0842dd2ac8140c90ea6f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xcIzprlHthKS_7KNUlWddg.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk"><strong class="bd nl">Fig 4.</strong> On the left, the original image, and on the right, its average reliability map.</figcaption></figure><h2 id="3b8e" class="mi js it bd jt mj mk dn jx ml mm dp kb la mn mo kf le mp mq kj li mr ms kn mt bi translated">损失函数</h2><p id="0dea" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">损失函数被定义为可靠性图和客观误差图之间的乘积的均方误差。误差是预测误差图和地面实况误差图之间的差异。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ob"><img src="../Images/7496b5f893b0a7fbd1048626af69c3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYdIG6YiLzgIiourioCrfA.png"/></div></div></figure><p id="76ff" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">损失函数要求将误差乘以可靠性图；因此，我们不能使用默认的 loss 实现<em class="ln">TF . loss . meansquaerror</em>。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nn"><img src="../Images/4536129cbfb69376d194f3cbf8042b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VJNc_41pdmbyMF-cLpA4hw.png"/></div></div></figure><p id="6954" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">创建自定义损耗后，我们需要告诉 TensorFlow 如何区分它。好的一面是我们可以利用<a class="ae mh" href="https://www.tensorflow.org/tutorials/customization/autodiff" rel="noopener ugc nofollow" target="_blank">自动微分</a>使用<em class="ln"> tf。梯度胶带</em>。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nz"><img src="../Images/74cb361828112016ab69c7217c5de016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOtAyt2-2DeHeEzfLZiPyw.png"/></div></div></figure><h2 id="72b6" class="mi js it bd jt mj mk dn jx ml mm dp kb la mn mo kf le mp mq kj li mr ms kn mt bi translated">【计算机】优化程序</h2><p id="f80c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">作者建议使用学习速率为<em class="ln"> 2e-4 </em>的那达慕优化器。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi no"><img src="../Images/d96bd1b82c49ab1aa70094a16cd8e5d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gQDh1hiPiSg1VQHrVN1pg.png"/></div></div></figure><h1 id="dc29" class="jr js it bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">培养</h1><h2 id="1987" class="mi js it bd jt mj mk dn jx ml mm dp kb la mn mo kf le mp mq kj li mr ms kn mt bi translated">客观误差模型</h2><p id="db12" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于训练阶段，利用<em class="ln"> tf.data </em>输入管道来生成更清晰易读的代码是很方便的。唯一的要求是创建应用于输入的函数。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nn"><img src="../Images/60d720d3e873ef4694c1974269c231fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aRJXgsIUNSKbH-yqJN-5IQ.png"/></div></div></figure><p id="f8d5" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">然后，将<em class="ln"> tf.data.Dataset </em>映射到<em class="ln"> calculate_error_map </em>函数。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nm"><img src="../Images/70b0802753d2aafbad583df465de09fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ic54orMkxALEyW6aq9rfdw.png"/></div></div></figure><p id="07dd" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">几乎很快就可以应用转换。原因是处理器还没有对数据执行任何操作，它是按需发生的。这个概念通常被称为<a class="ae mh" href="https://wiki.python.org/moin/Generators" rel="noopener ugc nofollow" target="_blank">懒惰评估</a>。</p><p id="e26d" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">到目前为止，已经实施了以下组件:</p><ol class=""><li id="df2d" class="lo lp it kr b ks lq kw lr la ls le lt li lu lm nq lw lx ly bi translated">预处理输入并计算目标的生成器。</li><li id="6b55" class="lo lp it kr b ks lz kw ma la mb le mc li md lm nq lw lx ly bi translated">定制训练循环所需的损失和梯度函数。</li><li id="7cbf" class="lo lp it kr b ks lz kw ma la mb le mc li md lm nq lw lx ly bi translated">优化器功能。</li></ol><p id="acc0" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">唯一缺少的是模型的定义。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ca"><img src="../Images/7773195ee83e718f6026202f6c87456d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F2WD-jQPbj1XW6rc.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk"><strong class="bd nl">Fig 5.</strong> The architecture for the objective error map prediction. The red and blue arrows indicate the flows of the first and stage. Source: <a class="ae mh" href="http://bit.ly/2Ldw4PZ" rel="noopener ugc nofollow" target="_blank">http://bit.ly/2Ldw4PZ</a></figcaption></figure><p id="743a" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">在上图中，描述了如何:</p><ul class=""><li id="7324" class="lo lp it kr b ks lq kw lr la ls le lt li lu lm lv lw lx ly bi translated">预处理后的图像进入卷积神经网络(CNN)。</li><li id="e618" class="lo lp it kr b ks lz kw ma la mb le mc li md lm lv lw lx ly bi translated">它用 Relu 激活函数和“相同”填充进行 8 次卷积变换。这被定义为 f()。</li><li id="57e5" class="lo lp it kr b ks lz kw ma la mb le mc li md lm lv lw lx ly bi translated">f()的输出通过与线性激活函数的最后卷积来处理。这被定义为 g()。</li></ul><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oc"><img src="../Images/e98349e7d78f5ba90e07bdb3d8165081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CbbzgEgQ_9EB1Y426IRg4w.png"/></div></div></figure><p id="c45e" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">对于自定义训练循环，有必要:</p><ol class=""><li id="aa9e" class="lo lp it kr b ks lq kw lr la ls le lt li lu lm nq lw lx ly bi translated">定义度量标准来衡量模型的性能。</li><li id="86e3" class="lo lp it kr b ks lz kw ma la mb le mc li md lm nq lw lx ly bi translated">计算损耗和梯度。</li><li id="7cea" class="lo lp it kr b ks lz kw ma la mb le mc li md lm nq lw lx ly bi translated">使用优化器更新权重。</li><li id="969e" class="lo lp it kr b ks lz kw ma la mb le mc li md lm nq lw lx ly bi translated">印刷精度。</li></ol><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi od"><img src="../Images/8451546e3807f73fc14cf8b9221842e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wacc56dUJH7C3S7B2bb2zA.png"/></div></div></figure><p id="8521" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated"><em class="ln">注意:使用斯皮尔曼的等级相关系数(SRCC)或皮尔逊的线性相关系数(PLCC)作为准确性指标是一个好主意。</em></p><h2 id="5cb0" class="mi js it bd jt mj mk dn jx ml mm dp kb la mn mo kf le mp mq kj li mr ms kn mt bi translated">主观评分模型</h2><p id="fb44" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了创建主观评分模型，让我们使用 f()的输出来训练回归变量。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi no"><img src="../Images/f11b0d5b4ee4206bd7019a9aa8e04169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mv5ebqNR37zjHdyeYWLyjA.png"/></div></div></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oe"><img src="../Images/871173546774360135fa21388b41059d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zUqTT0dTouDy_g7B8X8P9g.png"/></div></div></figure><p id="89e5" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">使用<em class="ln"> tf.keras.Model </em>的 fit 方法训练模型需要返回两个参数的数据集。第一个是输入，第二个是目标。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nz"><img src="../Images/c64897257968c5504b7576d0ec341ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1dtDnEcCUbl8_R2opfqtzQ.png"/></div></div></figure><p id="d76d" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">然后，<em class="ln">拟合</em>主观评分模型。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi no"><img src="../Images/faf73a0cd48e126dab9726095b6810fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5962BuZC3FT7lhmit_vzUg.png"/></div></div></figure><h1 id="227b" class="jr js it bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">预言；预测；预告</h1><p id="37d1" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用已经训练好的模型进行预测很简单。在模型中使用<em class="ln">预测</em>方法即可。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nx"><img src="../Images/e2a02f6208b3845aef8c9f8022614754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nUxQgwSCByGZQ2lyVGgkWw.png"/></div></div></figure><h1 id="39cc" class="jr js it bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">结论</h1><p id="dee6" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，我们学习了如何利用 tf.data 模块来创建易于阅读且节省内存的数据管道。此外，我们使用功能性 Keras API 实现了基于深度 CNN 的盲图像质量预测器(DIQA)模型。该模型通过使用 TensorFlow 的自动微分功能的自定义训练循环进行训练。</p><p id="a37c" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">下一步是找到最大化 PLCC 或 SRCC 准确性度量的超参数，并评估模型相对于其他方法的整体性能。</p><p id="ceba" class="pw-post-body-paragraph kp kq it kr b ks lq ku kv kw lr ky kz la me lc ld le mf lg lh li mg lk ll lm im bi translated">另一个想法是使用更大的数据集来训练客观误差图模型，并查看最终的整体性能。</p><h1 id="55fb" class="jr js it bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">Jupyter 笔记本</h1><p id="cfd4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr iu"> <em class="ln">更新 2020/04/15: </em> </strong> <em class="ln">包图像质量和笔记本更新修复了 LiveIQA 和 Tid2013 TensorFlow 数据集的一个问题。现在一切正常，检查一下！</em></p><div class="of og gp gr oh oi"><a href="https://github.com/ocampor/image-quality.git" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd iu gy z fp on fr fs oo fu fw is bi translated">o import/图像质量</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">Image quality 是一个用于自动图像质量评估(IQA)的开源软件库。该包是公共的…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">github.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow ne oi"/></div></div></a></div><h1 id="ab04" class="jr js it bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">文献学</h1><p id="afe7" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[1] Kim，j .，Nguyen，A. D .，&amp; Lee，S. (2019)。基于深度 CNN 的图像质量盲预测器。神经网络和学习系统汇刊。<a class="ae mh" href="https://doi.org/10.1109/TNNLS.2018.2829819" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/TNNLS.2018.2829819</a></p></div></div>    
</body>
</html>