<html>
<head>
<title>Using RNNs for Machine Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 RNNs 进行机器翻译</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-rnns-for-machine-translation-11ddded78ddf?source=collection_archive---------8-----------------------#2019-03-10">https://towardsdatascience.com/using-rnns-for-machine-translation-11ddded78ddf?source=collection_archive---------8-----------------------#2019-03-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bab1b3a022ddad6abbfad6b4c7aa8ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XWibQn7P3bndw6X4QV-hfw.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@atharva_tulsi" rel="noopener ugc nofollow" target="_blank">Atharva Tulsi</a> on <a class="ae jg" href="https://unsplash.com/photos/eQ7j390QAEs" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="65d5" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">介绍 RNN 和 LSTM 网络及其应用。</h2></div><p id="9f2b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">故事是人类文化的精髓。它们包含了我们过去的信息和未来的理论。它们让我们能够深入人类思维的内部运作和微妙之处，发现传统上不可能分析的方面。</p><p id="15e2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然过去故事总是由我们人类撰写，但随着深度学习领域的发展和研究，我们看到计算机程序能够编写故事，并像人类一样使用语言，但如何做到呢？</p><p id="ef4f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当你读最后两段时，你对每个单词的理解都是基于前面的句子和单词。你用以前的数据来理解现在的数据。传统的神经网络会查看循环中的每个单词，并且只理解每个单词的字面意思，而不考虑其上下文或之前可能添加了信息的任何单词。这使得它们在试图理解上下文(和过去的信息)重要的信息时非常糟糕。那么我们如何解决这个问题呢？</p><h1 id="4ff2" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">递归神经网络</h1><p id="9c5b" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">RNNs 是一种特殊类型的神经网络，具有<strong class="la jk">循环</strong>，允许信息在网络的不同步骤中保持不变。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl mv"><img src="../Images/019744560b8d3385567f86cddf9b17b5.png" data-original-src="https://miro.medium.com/v2/format:webp/1*FKOv5k_7x0kW5lXbltMiCQ.png"/></div></figure><p id="26b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该循环使神经网络在决定当前单词的实际意思之前，返回并检查所有先前单词中发生的事情。RNN 可以被认为是一次又一次地复制粘贴同一个网络，每一次新的复制粘贴都会比前一次添加更多的信息。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl mv"><img src="../Images/12764b220dd5076f0a685c0c70eb5f2a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*xLcQd_xeBWHeC6CeYSJ9bA.png"/></div></figure><p id="9a97" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">RNN 神经网络的应用与传统神经网络有很大的不同，因为它们没有一个输出和输入集作为一个具体的值，相反，它们把序列作为输入或输出。</p><p id="864e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">那么我们可以用 RNNs 做什么呢？</strong></p><ul class=""><li id="429e" class="mw mx jj la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated">自然语言处理</li><li id="c3d4" class="mw mx jj la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated">股票市场数据(时间序列分析)</li><li id="e517" class="mw mx jj la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated">图像/视频字幕</li><li id="f1dd" class="mw mx jj la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated">翻译</li><li id="8dc9" class="mw mx jj la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated">还有更多</li></ul><h2 id="74f1" class="nk lv jj bd lw nl nm dn ma nn no dp me lh np nq mg ll nr ns mi lp nt nu mk nv bi translated">RNN 也有不同的模式可以效仿。</h2><ol class=""><li id="a574" class="mw mx jj la b lb mm le mn lh nw ll nx lp ny lt nz nc nd ne bi translated"><strong class="la jk">固定到序列</strong></li></ol><p id="9b42" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">RNN 接受固定大小的输入，并输出一个序列。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl mv"><img src="../Images/18fb4bf900c507880a5e8a30bea877b0.png" data-original-src="https://miro.medium.com/v2/format:webp/1*gUhBxl-0nkQOF9heEo6J0g.jpeg"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Image captioning takes an image and outputs a sentence of words</figcaption></figure><p id="5b99" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.<strong class="la jk">序列到固定</strong></p><p id="2c55" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">RNN 接受一个输入序列并输出一个固定的大小。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl mv"><img src="../Images/7b1ee5fe26134e75e88c96b44c266880.png" data-original-src="https://miro.medium.com/v2/format:webp/1*EczR_-elNTREdwIa0Js08A.jpeg"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Sentiment analysis where a given sentence is classified as expressing positive or negative sentiment</figcaption></figure><p id="bbe8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.<strong class="la jk">序列到序列</strong></p><p id="276a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">RNN 接收一个输入序列并输出一个序列。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl mv"><img src="../Images/5d24b671db4e8b43b5e502e517539e02.png" data-original-src="https://miro.medium.com/v2/format:webp/1*UUIlFslG1i-noShS1VLuJA.jpeg"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Machine Translation: an RNN reads a sentence in one language and then outputs it in another</figcaption></figure><p id="bc2e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这应该有助于你对 RNNs 有一个高层次的理解，如果你想了解更多关于 RNN 运算背后的数学知识，我推荐你查看这个<a class="ae jg" href="https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>！</p><h1 id="c35b" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">消失梯度问题</h1><p id="33eb" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">这个问题发生在任何使用基于梯度的优化技术的网络中。当计算反向传播时(计算相对于权重的损失梯度)，随着反向传播算法在网络中移动，梯度变得非常小。这导致早期层的学习速度比后期层慢。这降低了 rnn 的有效性，因为它们通常不能完全考虑长序列。随着所需信息之间的差距越来越大，RNN 变得越来越低效。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="ab gu cl mv"><img src="../Images/9844be126ac1e3858aada391d49eb09a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*FWy4STsp8k0M5Yd8LifG_Q.png"/></div></figure><p id="ba4d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，这个问题的一个常见解决方案是使用不会导致渐变消失的激活函数，如 RELU，而不是其他激活函数，如 sigmoid 或双曲线正切。一个更好的解决方法是使用长短期记忆网络！</p><h1 id="cfd1" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">长短期记忆</h1><p id="929d" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">LSTM 网络的建立只有一个目的——解决传统无线网络的长期依赖性问题。记住很久以前的事情是他们与生俱来的天性，他们根本不会为此而挣扎。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/7f679cd776f4a5542bbd5dcb26e5b2f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wf-4tXIWLR0kldb5.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">The repeating module in an LSTM contains 4 layers, as opposed to only one in an RNN.</figcaption></figure><p id="09b7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">LSTMs 工作良好的原因主要是由于网络中存在的小区状态。这是您在图中看到的穿过网络顶部的线。信息很容易在细胞状态中流动而不会改变。连接到单元状态的门能够在需要时添加或删除信息。有一堆更复杂的数学材料，你可以在这里阅读更多关于<a class="ae jg" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">的内容</a>，但总的来说，LSTMs 真的很擅长记忆旧信息。</p><h1 id="7506" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">神经机器翻译</h1><p id="dc11" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">既然我们对 LSTMs 和 RNNs 有了基本的了解，让我们试着用 Keras 开发一个机器翻译模型。</p><p id="4378" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将演示一个序列到序列 LSTM 网络如何被用来将文本从英语翻译成法语。数据可以在<a class="ae jg" href="http://www.manythings.org/anki/fra-eng.zip" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="mr ms mt mu gt iv"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="3bce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从 Keras 导入 LSTM 层，并在这里设置几个重要的变量。</p><figure class="mr ms mt mu gt iv"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="9c1b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们需要<strong class="la jk">将</strong>我们的文本矢量化成数字格式，这使得神经网络更容易处理。你可以在这里阅读更多关于这是如何工作的<a class="ae jg" href="https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7" rel="noopener">。</a></p><figure class="mr ms mt mu gt iv"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="e2bd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们需要创建我们的模型。因为这是一个序列到序列模型，我们需要两个部分——编码器和解码器。编码器抛出一个隐藏状态，它包含了作为输入获得的所有信息。然后，解码器获取这些信息，然后生成指定格式的输出(在本例中是法语)。</p><p id="6c76" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这之后，就像训练模型和使用它一样简单。所有代码的详细回顾可以在<a class="ae jg" href="https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。在我的<a class="ae jg" href="https://aryanmisra.com/translate/" rel="noopener ugc nofollow" target="_blank">网站</a>上查看现场演示。</p><h2 id="1708" class="nk lv jj bd lw nl nm dn ma nn no dp me lh np nq mg ll nr ns mi lp nt nu mk nv bi translated">TL；速度三角形定位法(dead reckoning)</h2><p id="59e2" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">RNNs 和 LSTM 网络今天正在提供非常酷的用例，我们看到它在大量技术中得到应用。</p><p id="fa72" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">递归神经网络不同于传统的人工神经网络，因为它们不需要固定的输入/输出大小，并且它们还使用以前的数据来进行预测。</p><p id="67ce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">LSTM 网络是专门为解决 rnn 中的长期依赖性问题而制造的，并且它们超级擅长通过使用小区状态来利用来自前一段时间的数据(这是其本质所固有的)。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="b4e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">如果你喜欢我的文章或者学到了新东西，请务必:</strong></p><ul class=""><li id="ca05" class="mw mx jj la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated">在<a class="ae jg" href="https://www.linkedin.com/in/aryan-misra/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上和我联系。</li><li id="7cf0" class="mw mx jj la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated">给我发一些反馈和评论(aryanmisra@outlook.com)。</li><li id="1634" class="mw mx jj la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated">在我的网站上查看现场演示:<a class="ae jg" href="https://aryanmisra.com/translate/" rel="noopener ugc nofollow" target="_blank">https://aryanmisra.com/translate/</a></li></ul></div></div>    
</body>
</html>