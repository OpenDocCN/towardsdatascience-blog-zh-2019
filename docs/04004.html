<html>
<head>
<title>Bias and variance in linear models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性模型中的偏差和方差</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bias-and-variance-in-linear-models-e772546e0c30?source=collection_archive---------7-----------------------#2019-06-24">https://towardsdatascience.com/bias-and-variance-in-linear-models-e772546e0c30?source=collection_archive---------7-----------------------#2019-06-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6d5b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">线性模型的偏差和方差权衡</h2></div><p id="6285" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我相信每个人过去都见过这个图表:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/6b882c4baced2eab498f043ab550600a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CgIdnlB6JK8orFKPXpc7Rg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">First figure in Scott Fortmann-Roe’s <a class="ae lr" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">Understanding the Bias-Variance Tradeoff</a>.</figcaption></figure><p id="47ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lr" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">了解偏差-方差权衡</a>详细介绍了权衡和误差，我强烈推荐。上图是帖子上的第一个图，显示了多个模型在不同偏差和方差误差下的预测。靶心是我们想要预测的真实值，蓝点是模型实际预测的值。在这篇文章中，我想尝试并直观地展示线性模型中偏差和方差的权衡。</p><h2 id="1a7f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">为什么是线性模型？</h2><p id="0c6f" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">因为它们很容易理解，并且提供了一种非常简单的方法来控制这些误差——通过正则化。众所周知，与非线性模型相比，普通最小二乘(OLS)回归可以给出方差较低的无偏结果。<strong class="kh ir">岭</strong> (OLS 带 L2 点球)和<strong class="kh ir">拉索</strong> (OLS 带 L1 点球)给出了有偏差的结果，与 OLS 相比方差低得多。惩罚程度由正则化系数λ控制。这反过来控制两个误差，我们将在下面看到。Lasso 实际上是一个特例，因为它积极地将系数估计值推至零，但有助于保持事物的前瞻性。你可以在这里阅读更多关于正规化<a class="ae lr" rel="noopener" target="_blank" href="/regularization-in-machine-learning-76441ddcf99a">的内容。</a></p><h2 id="433d" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">程序</h2><p id="ead0" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">我将坚持使用 Scott 的文章中用来从概念上描述错误的方法。我在这里任意挑选所有固定的数字。</p><ol class=""><li id="c883" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">从<em class="mz"> y = α+ βx + ϵ </em>模拟 500 个数据点，其中<em class="mz"> ϵ ~ N(0，8)，x ~ U(-2，2)，α = 2 </em>和<em class="mz"> β = 3。</em></li><li id="a5e1" class="mq mr iq kh b ki na kl nb ko nc ks nd kw ne la mv mw mx my bi translated">重复第一步 1000 次，收集所有数据集。</li><li id="5c27" class="mq mr iq kh b ki na kl nb ko nc ks nd kw ne la mv mw mx my bi translated">对于每个集合，用固定的λ拟合 OLS、脊和套索模型，以预测<em class="mz"> x = 3 时的<em class="mz"> y </em>。</em>预期预测应该是<em class="mz"> 2 + 3 </em> x <em class="mz"> 3 = 11 </em></li></ol><p id="709e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有 3000 个(1000 个 OLS + 1000 个山脊+ 1000 个套索)预测，我们可以看看这些模型的真正“本质”。你可以在我的 GitHub 页面<a class="ae lr" href="https://github.com/DarkestFloyd/blog_files/blob/master/bias_variance/bias_variance.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到所有代码。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><p id="e69d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于如何阅读情节的注释。我要你注意两件事。真实值(显示为黑色虚线)和模型的平均预测值(显示为相同颜色的虚线)之间的距离。这个距离就是模型的<strong class="kh ir">偏差</strong>(或偏差的平方)。与真实值(11)的大偏移是大偏差。<br/> 2。直方图的宽度是模型的<strong class="kh ir">方差</strong>。宽度越大，方差越大。</p><h2 id="8b31" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi">λ ~ 0</h2><p id="d7a9" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">从一个非常小的λ值开始。这相当于没有罚分，因此我们可以预期在 OLS 对山脊和套索的结果是一样的。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/970ba164473b9454641d99da594f9ab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KReR8DpkNN6XsH2-UzDDw.png"/></div></div></figure><p id="1a9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">情节没有给人惊喜。所有这三种分布都与真实值周围的平均值重叠。请注意分布是如何分散的。从 9 到 13 的预测中有很大的差异。</p><h2 id="e53f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi">λ = 0.01</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/37fedfe48e93b3255702c3d38e26dfe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jh0TiQqgDDRNwKNIuRXPMw.png"/></div></div></figure><p id="fed3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过(非常)小的代价，很容易看到正则化的效果。分布已经向左移动(从平均值可以明显看出)。在山脊观察到一个小的偏差，在套索观察到一个相对较大的偏差。不清楚方差是否已经改变。</p><h2 id="4d85" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi">λ = 0.05</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/985dff96c70ab8e786aacd5edd1acb80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xvkz19A4pUlhvWRczHZJbA.png"/></div></div></figure><p id="f620" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在λ = 0.05 时，Lasso 已经过于激进，偏差为 3 个单位。岭是足够接近，但看起来它有相同的方差。因此，对于这个数据来说，山脊还没有优势。</p><h2 id="cacf" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi">λ = 0.1</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/df53f151de771ea5e8ec6dc1b171827a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OPFkP0hDRWjGqUMPqRhVqQ.png"/></div></div></figure><p id="7e47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与上面几乎相似的结果。目前还很难注意到差异有任何变化。</p><h2 id="cf9a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi">λ = 0.5</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/fe3b8c16b2a0bca5bf643b556b934c3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HtWTFsII4xD1ZYytPv_ivA.png"/></div></div></figure><p id="651a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更高的惩罚提供了一些(合理的)令人满意的线索。山脊上的偏差增加了近三个单位，但方差较小。Lasso 非常积极地推动β的零系数估计，导致结果偏差很大，但方差很小。</p><h2 id="ff9f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">λ = 1 —一些好结果！</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/64a2af2618c95954c0483882baddb338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vo9X1ThVpkhi3cUH3qtmhw.png"/></div></div></figure><p id="28e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，权衡显然已经改变了立场。以较高的偏置为代价，脊的方差较小。</p><h2 id="5773" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi">λ = 5</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/427336836bcd45af8977887fcf0b2972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yG1jC5K_38ZNtOrJWPmK5A.png"/></div></div></figure><p id="566f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了让大家真正理解这一点，这里有一个非常大的惩罚。以更高的偏差为代价，脊上的方差很小。你可能永远不会需要这么大的罚款。但事实很清楚，较低的方差是以较高的偏差为代价的。</p><h2 id="5100" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">各种正则化值的偏差和方差</h2><p id="ba7f" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">对一系列正则化值重复上述操作，可以得到清晰的图像。</p><ul class=""><li id="e0cb" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la nn mw mx my bi translated">偏差计算为平均预测值和真实值之间的距离-真实值减去平均值(预测值)</li><li id="5fde" class="mq mr iq kh b ki na kl nb ko nc ks nd kw ne la nn mw mx my bi translated">方差是平均预测值与平均值(预测值减去平均值(预测值))的平均偏差</li></ul><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/d78736ce76e7513b4642d0f10d41b849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*irhDXKSfyiHPmAHL06Z3bQ.png"/></div></div></figure><p id="34ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些图给出了相同的观察结果。OLS 的偏差最小，但方差最大。在λ = 0.2 左右(β变为 0，因此对于<em class="mz"> x </em> ) <em class="mz">的所有值，预测<em class="mz"> y = α </em>)后，山脊看起来像平滑移动，套索是恒定的。</em></p><h2 id="8632" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">理想分布</h2><p id="cc50" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">更好的数据选择可以给我们一个理想的预测抽样分布图。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/452acd847b6c6e7b7e207fed394c8aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQwtseFFbteSLy-qKmq8qQ.png"/></div></div></figure><p id="dbfb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于重叠分布，山脊提供的优势在这里非常明显。岭给出一个稍有偏差的预测，但会给出一个比 OLS 更接近的预测。这才是脊的真正价值。一个小偏差，但更一致的预测。OLS 给出了一个公正的结果，但不是很一致。这很关键，OLS 给出了一个无偏的结果<strong class="kh ir"> <em class="mz">平均，而不是一直</em> </strong> <em class="mz">。</em>这就是线性模型中的偏差和方差权衡取<em class="mz">形</em>。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><p id="01d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以在这里找到我在这篇文章中使用的所有代码。我建议您针对不同的λ值运行它，亲自查看变化。甚至可以在不同的数据集上使用它，看看是否可以看到一些重叠。如果你对这篇文章有任何建议，请随时和我打招呼。</p><p id="a1a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我只想花点时间感谢让这篇文章成为可能的每一个人。一定要花点时间分享并表达你的感激之情。:)感谢阅读！</p></div></div>    
</body>
</html>