<html>
<head>
<title>Review: 3D U-Net+ResNet — Volumetric Convolutions + Long &amp; Short Residual Connections (Biomedical Image Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:3D U-Net+ResNet —体积卷积+长短残差连接(生物医学图像分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-3d-u-net-resnet-volumetric-convolutions-long-short-residual-connections-biomedical-3a7da3f98dae?source=collection_archive---------13-----------------------#2019-05-06">https://towardsdatascience.com/review-3d-u-net-resnet-volumetric-convolutions-long-short-residual-connections-biomedical-3a7da3f98dae?source=collection_archive---------13-----------------------#2019-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b447" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">胜过类似 V-Net 的网络</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/cf8326c2723438c6a53fa64b6c65cf3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GmLE6aGWHWmrT__iobUqJQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Example of prostate MR images displaying large variations (Only centre part)</strong></figcaption></figure><p id="cecc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di">在</span>这个故事中，回顾了一篇论文“<strong class="kz ir">使用混合残差连接的体积转换，用于从 3D MR 图像中自动分割前列腺</strong>”。这是一个使用<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1"><strong class="kz ir">3D U-Net</strong></a><strong class="kz ir">+</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kz ir">ResNet</strong></a>概念的网络。对于 3D 磁共振(MR)图像，<strong class="kz ir">从 3D MR 图像中手动分割耗时且主观，可重复性有限</strong>。它严重依赖于经验，并且在观察者之间和观察者内部有很大的差异。另一方面，自动分段非常具有挑战性:</p><ul class=""><li id="5d9b" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">首先，<strong class="kz ir">由于不同的 MR 扫描协议，不同的 MR 图像具有全局的扫描间可变性和扫描内强度变化</strong>，例如有/没有直肠内线圈。</li><li id="98d8" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">第二，<strong class="kz ir">缺乏清晰的前列腺边界</strong>由于前列腺和周围组织的外观相似。</li><li id="9b65" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">第三，<strong class="kz ir">由于病变或图像分辨率不同，前列腺在不同受试者之间的大小和形状差异很大</strong>。</li></ul><p id="dbdf" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这项工作中，混合使用长短剩余连接，提出了具有体积卷积的类 U-Net 网络。这是由<strong class="kz ir"> CUMED </strong>团队在<strong class="kz ir">香港中文大学(CUHK) </strong>的<strong class="kz ir"> MICCAI 前列腺 MR 图像分割(PROMISE12)挑战数据集</strong>上所做的工作。这是一篇<strong class="kz ir"> 2017 AAAI </strong>论文，引用次数超过<strong class="kz ir"> 90 次</strong>。(<a class="mq mr ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----3a7da3f98dae--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="b4f2" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">概述</h1><ol class=""><li id="3394" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls nw mi mj mk bi translated"><strong class="kz ir">网络架构</strong></li><li id="81ec" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls nw mi mj mk bi translated"><strong class="kz ir">混合短长剩余连接</strong></li><li id="5eeb" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls nw mi mj mk bi translated"><strong class="kz ir">消融研究</strong></li><li id="0e0e" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls nw mi mj mk bi translated"><strong class="kz ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="de45" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated"><strong class="ak"> 1。网络架构和混合长短残留连接</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nx"><img src="../Images/0ef57bf1642c863c12970a1e0a07ac73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F5v0i_jPPw-UJPf1TWmHIA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">(a) Network Structure, (b) ResBlock</strong></figcaption></figure><h2 id="57c6" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">1.1.基本体积转换网络</h2><ul class=""><li id="c7d3" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">2D 完全转换网络(FCN)扩展为<strong class="kz ir">体积转换网络</strong>以实现体积到体积的预测。</li><li id="c10f" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">从<strong class="kz ir">下采样路径</strong>中，我们只能获得粗略的预测，这对于一些检测和分类任务来说是足够的，但是<strong class="kz ir">不适用于基于体素的语义分割</strong>。</li><li id="ac4f" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">在 ResBlocks 之间应用三个<strong class="kz ir"> 2×2×2 最大池层，跨距为 2 </strong>。</li><li id="814e" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">由<strong class="kz ir">去卷积和卷积层</strong>组成的<strong class="kz ir">上采样路径</strong>被实现为<strong class="kz ir">生成具有更高分辨率的密集预测</strong>。</li><li id="55c5" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">通过 3D 方式的卷积、反卷积和汇集工作，网络可以在提取要素和进行预测时完全保留和利用 3D 空间信息。</li></ul><h2 id="e544" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">1.2.深度监督机制</h2><ul class=""><li id="7ee7" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">利用网络中的深度监督机制来加快收敛速度。</li><li id="6eee" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">在网络末端增加一个卷积层(核大小 1×1×1)，生成<strong class="kz ir">主预测</strong>。</li><li id="3f94" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">此外，采用几个卷积层(核大小 1×1×1)，然后在上采样路径中使用隐藏特征映射来获得<strong class="kz ir">辅助粗略预测</strong>，然后使用解卷积层来获得具有相同输入大小的辅助密集预测。</li><li id="74de" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated"><strong class="kz ir">当训练体积 ConvNet 时，主预测和辅助预测的交叉熵损失的加权和被最小化</strong>。</li><li id="eb01" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">原则上，深度监督机制可以在训练期间起到强有力的“<strong class="kz ir">规则化</strong>的作用，因此对于用有限的训练数据训练 ConvNet 是重要的。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="180e" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated"><strong class="ak"> 2。混合短长</strong>剩余<strong class="ak">连接</strong></h1><ul class=""><li id="e423" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">长和短剩余连接的使用就像<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener"> U-Net+ResNet </a>一样。</li></ul><h2 id="3a2d" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">2.1.短剩余连接</h2><ul class=""><li id="775f" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">第一种残差连接用于<strong class="kz ir">构建局部残差块(ResBlocks) </strong>，如图的(b)部分所示。</li><li id="ca82" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">在 ResBlock 中，它由两个卷积层和两个校正线性单元(ReLUs)组成。</li></ul><h2 id="082a" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">2.2.长剩余连接</h2><ul class=""><li id="db70" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated"><strong class="kz ir">长残差连接:</strong>第二种残差连接用于<strong class="kz ir">连接下采样和上采样路径</strong>中具有相同分辨率的残差块，如图(a)部分所示。</li><li id="d958" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">这些剩余连接可以在 ConvNet 中显式传播两种重要信息。</li><li id="945d" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">首先，它们可以<strong class="kz ir">将空间位置信息向前传播到上采样路径</strong>，以便恢复由下采样操作引起的空间信息丢失，从而进行更精确的分割。</li><li id="8fa9" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">第二，使用求和操作来构建剩余连接，该架构可以更平滑地将梯度流向后传播，从而提高训练效率和网络性能。</li><li id="a6d8" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">因此，这些连接可以<strong class="kz ir">在端到端训练过程</strong>期间有效地向前和向后传播上下文和梯度信息。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="aa87" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">3.消融研究</h1><h2 id="f766" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">3.1.资料组</h2><ul class=""><li id="91b2" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated"><strong class="kz ir">使用 MICCAI 前列腺 MR 图像分割(PROMISE12)挑战数据集</strong>。</li><li id="9ebe" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated"><strong class="kz ir">训练</strong> <strong class="kz ir">数据集</strong>包含<strong class="kz ir">前列腺</strong>的 50 个横向 T2 加权 MR 图像和相应的分割基础事实。</li><li id="8eaa" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated"><strong class="kz ir">测试</strong> <strong class="kz ir">数据集</strong>由<strong class="kz ir"> 30 幅 MR 图像</strong>组成，地面实况由组织者拿出进行独立评估。</li><li id="c3c9" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">将所有 MR 体积转换为 0.625×0.625×1.5 mm 的固定分辨率，然后将其归一化为零均值和单位方差。</li><li id="b6b2" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">增强操作包括旋转(90 度、180 度和 270 度)和轴向翻转。</li></ul><h2 id="4914" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">3.2.培训和测试</h2><ul class=""><li id="ae2a" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">用的是 Caffe。</li><li id="e958" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">使用 NVIDIA TITAN X GPU，由于有限的 GPU 内存，在训练网络时，从每个样本中随机裁剪<strong class="kz ir"> 64×64×16 个子体积作为输入。</strong></li><li id="dd58" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">在测试过程中，<strong class="kz ir">重叠滑动窗口策略</strong>用于裁剪子体积。并且<strong class="kz ir">这些子体积</strong>的概率图的平均值被用于获得整个体积预测。</li><li id="4b3d" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">子体积大小也是 64×64×16，步距是 50×50×12。一般来说，训练网络大约需要<strong class="kz ir"> 4 小时，处理一张 320×320×60 </strong>大小的 MR 图像大约需要<strong class="kz ir"> 12 秒。</strong></li><li id="a4f7" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">消融研究采用 10 倍交叉验证。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/dfed063ed49b3e2c76208bf830281c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*6BYpUu9KKz7h-bUTpScdhQ.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Training and validation loss of networks with and without mixed residual connections</strong></figcaption></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/0abc55d11eeca8e3209e65877d306680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*BhA5TSmhI_2T5OZ2qjFOtA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Cross validation performance with different configurations.</strong></figcaption></figure><ul class=""><li id="974b" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">当然，无论是长剩余连接还是短剩余连接，Dice 系数都是最高的。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="102f" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated"><strong class="ak"> 4。与最先进方法的比较</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi om"><img src="../Images/82a54c69d63f301fafa305a3dfac7371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCXO-jcksMYS0GpLyBVmTQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Quantitative comparison between the proposed method and other methods</strong></figcaption></figure><ul class=""><li id="d1d9" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">PROMISE12 挑战中使用的评估指标包括 Dice 系数(DSC)、体积之间绝对差异的百分比(aRVD)、体积边界点之间最短距离的平均值(ABD)和 95% Hausdorff 距离(95HD)。然后主办方算了一个<strong class="kz ir">总分</strong>，如上图。</li><li id="6ff3" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">在论文提交之前，共有 21 个团队提交了他们的结果，只有前 10 个团队列在表格中。</li><li id="ba94" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">前十名中的七个团队采用了各种手工制作的功能。除了 team (CUMED)之外，使用 ConvNet 的另外两个小组是 SIATMIDS 和 CAMP-TUM2。</li><li id="3b19" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">团队 CAMP-TUM2 有一个<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974"> V 网</a>般的网络。</li><li id="2c38" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">再次，当然是 CUMED 队，凭借使用<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1"> 3D U-Net </a>般的网络和<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener"> U-Net+ResNet </a>般的长短剩余连接，以 86.65 分的成绩遥遥领先。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi on"><img src="../Images/240b6c7851583f53fc12b9d04ee92e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CVurq9Vpzeiy2lMc8_0npg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Qualitative segmentation results of case 4 (first row) and case 22 (second row) at the apex(left), center (middle) and base (right) of the prostate in testing dataset.</strong></figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h2 id="62f2" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">参考</h2><p id="1570" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg oo li lj lk op lm ln lo oq lq lr ls ij bi translated">【2017 AAAI】【3D U-Net+ResNet】<br/><a class="ae kf" href="https://appsrv.cse.cuhk.edu.hk/~lqyu/papers/AAAI17_Prostate.pdf" rel="noopener ugc nofollow" target="_blank">具有混合残差连接的体积 ConvNets，用于从 3D MR 图像中自动分割前列腺</a></p><h2 id="53d0" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg oo li lj lk op lm ln lo oq lq lr ls ij bi translated">)(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(?)(我)(们)(都)(不)(在)(这)(些)(情)(况)(上)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(况)(。 <a class="ae kf" href="https://medium.com/@sh.tsang/review-pyramidnet-deep-pyramidal-residual-networks-image-classification-85a87b60ae78" rel="noopener"> PyramidNet </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5"> DRN </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-dpn-dual-path-networks-image-classification-d0135dce8817"> DPN </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-residual-attention-network-attention-aware-features-image-classification-7ae44c4f4b8"> 残留注意网络 </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-msdnet-multi-scale-dense-networks-image-classification-4d949955f6d5"> MSDNet </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f"> ShuffleNet V1 </a></p><p id="8b77" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">物体检测<br/></strong><a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae kf" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4">G-RMI</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a></p><p id="6582" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">语义切分<br/></strong><a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae kf" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生物医学图像分割<br/></strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1">M FCN<br/></a></p><p id="3134" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">实例分割<br/> </strong> [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></p><p id="58de" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"/><br/><a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">【DeepPose】</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">【汤普森 NIPS'14】</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">【汤普森 CVPR'15】</a></p></div></div>    
</body>
</html>