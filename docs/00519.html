<html>
<head>
<title>Review: RetinaNet — Focal Loss (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:RetinaNet —焦点丢失(物体检测)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=collection_archive---------0-----------------------#2019-01-24">https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=collection_archive---------0-----------------------#2019-01-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1289" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 ResNet+FPN 的具有焦点损失和视网膜网络的一级检测器，超越了两级检测器的准确性，更快的 R-CNN</h2></div><p id="32eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事里，<strong class="kh ir"> RetinaNet </strong>，由<strong class="kh ir">脸书艾研究(FAIR) </strong>进行点评。发现<strong class="kh ir">在一级检测器</strong>中存在极端的前景-背景类别不平衡问题。并且据信这是使一级检测器<strong class="kh ir">的性能不如两级检测器</strong>的主要原因。</p><p id="f823" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 RetinaNet(一级检测器)中，<strong class="kh ir">通过使用焦点损失，较低的损失是由“容易”的负样本造成的，因此损失集中在“硬”样本上</strong>，这提高了预测精度。以<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">【ResNet】</strong></a><strong class="kh ir">+</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"><strong class="kh ir">【FPN】</strong></a><strong class="kh ir">为骨干</strong>进行特征提取，<strong class="kh ir">加上两个特定任务的子网络进行分类和包围盒回归</strong>，形成了<strong class="kh ir"> RetinaNet </strong>，达到了最先进的性能，<strong class="kh ir">优于</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <strong class="kh ir">更快的 R-CNN </strong> </a>是一篇<strong class="kh ir"> 2017 ICCV </strong> <strong class="kh ir">最佳学生论文奖</strong>论文，引用量超过<strong class="kh ir"> 500 篇</strong>。(第一作者宗林逸在 2017 年 ICCV 展示 RetinaNet 时，已经成为谷歌大脑的研究科学家。)(<a class="ll lm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----38fba6afabe4--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中型)</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/3319351e45f9f0cb8af5bb4a099b6eb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*E30eIZ5aCGjhCz9E.gif"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="bd lz">A Demo of RetinaNet on Parking Lot Entrance Video (</strong><a class="ae lk" href="https://www.youtube.com/watch?v=51ujDJ-01oc" rel="noopener ugc nofollow" target="_blank"><strong class="bd lz">https://www.youtube.com/watch?v=51ujDJ-01oc</strong></a><strong class="bd lz">)</strong></figcaption></figure><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="ma mb l"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="ak">Another Demo of RetinaNet on Car Camera Video</strong></figcaption></figure></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="0190" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">概述</h1><ol class=""><li id="40fb" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la ni nj nk nl bi translated"><strong class="kh ir">一级检测器的类不平衡问题</strong></li><li id="274a" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated"><strong class="kh ir">焦损失</strong></li><li id="9f5e" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated"><strong class="kh ir">视网膜检测器</strong></li><li id="c95a" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated"><strong class="kh ir">消融研究</strong></li><li id="9b07" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated"><strong class="kh ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="ee45" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><strong class="ak"> 1。一级检测器的类别不平衡问题</strong></h1><h2 id="4af7" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">1.1.两级检测器</h2><ul class=""><li id="f788" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated">在两级检测器中，例如<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>、<strong class="kh ir">第一级，区域建议网络(RPN) </strong>将候选物体位置的<strong class="kh ir">数量缩小到一个小的数量(例如 1–2k)</strong>，过滤掉大部分背景样本。</li><li id="892e" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">在<strong class="kh ir">第二阶段</strong>，对每个候选对象位置执行<strong class="kh ir">分类</strong>。<strong class="kh ir">采样试探法</strong>使用固定的前景与背景比率(1:3)<strong class="kh ir">或在线硬示例挖掘(OHEM) </strong>为每个迷你批次选择一小组锚点(例如，256)。</li><li id="c2af" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">因此，在前景和背景之间有一个可管理的类平衡。</li></ul><h2 id="1053" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">1.2.一级检测器</h2><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/d83afb61d86c87baa808bc64551d8702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*zZJOspTU1DfHQLXZ2bQoXA.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="bd lz">Many negative background examples, Few positive foreground examples</strong></figcaption></figure><ul class=""><li id="358e" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated">一个更大的候选物体位置集合在一幅图像上被有规律地采样(大约 100k 个位置)，这些位置密集地覆盖了空间位置、比例和纵横比。</li><li id="e72a" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">训练过程仍然由容易分类的背景例子主导。它通常通过引导或硬示例挖掘来解决。但是它们的效率不够高。</li></ul><h2 id="64f1" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">1.3.箱子数量比较</h2><ul class=""><li id="b631" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89"> YOLOv1 </a> : 98 盒</li><li id="87b4" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">约洛夫 2  : ~1k</li><li id="3caa" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过吃</a>:~ 1–2k</li><li id="e4da" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">固态硬盘</a>:~ 8–26k</li><li id="47c8" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><strong class="kh ir"> RetinaNet: ~100k </strong>。RetinaNet 可以有大约 100k 个盒子，使用焦点损失解决等级不平衡问题。</li></ul><h1 id="f696" class="mj mk iq bd ml mm oi mo mp mq oj ms mt jw ok jx mv jz ol ka mx kc om kd mz na bi translated">2.焦点损失</h1><h2 id="691c" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">2.1.交叉熵损失</h2><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a4464c2f7df3adcf66e3ded3432ee8a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*rt5o8yRz7qMWkwxWNM1ZLg.png"/></div></figure><ul class=""><li id="6400" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated">上面的等式是二元分类的 CE 损失。<em class="oo"> y </em> ∈{ 1}是地面实况类，而<em class="oo"> p </em> ∈[0，1]是模型的估计概率。很容易将其扩展到多类情况。为了便于标注，定义了<em class="oo"> pt </em>，ce 改写如下:</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi op"><img src="../Images/3ecbcf60b13835c66eda634991aebfb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*RJPQLSgqu4AtKkAr4icaHQ.png"/></div></figure><ul class=""><li id="cb50" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated"><strong class="kh ir">当对大量简单的例子求和时，这些小的损失值可以压倒罕见的类。</strong>下面是例子:</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/e042351caeb43b4742ff0bf0113e6cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*b8Z0SprNLpNRLv8-8lzpdQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="bd lz">Example</strong></figcaption></figure><ul class=""><li id="5d65" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated">让我们以上图为例。如果我们有 100000 个简单的例子(每个 0.1)和 100 个困难的例子(每个 2.3)。当我们需要求和来估算 CE 损失时。</li><li id="f84d" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">简单例子的损失= 100000×0.1 = 10000</li><li id="cba4" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">硬例损失= 100×2.3 = 230</li><li id="e6a0" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">10000 / 230 = 43.从简单的例子来看，损失大约大 40 倍。</li><li id="8811" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">因此，当存在极端的阶级不平衡时，CE 损失不是一个好的选择。</li></ul><h2 id="dac2" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated"><strong class="ak"> 2.2。</strong> α- <strong class="ak">平衡 CE 损失</strong></h2><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi or"><img src="../Images/f551d479f1eeb075d9ca94423bd7bd58.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*AR6jsJX5ihtNni78p5kr9A.png"/></div></figure><ul class=""><li id="6e12" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated">为了解决类别不平衡，一种方法是为类别 1 添加加权因子<em class="oo"> α </em>，为类别 1 添加 1 - <em class="oo"> α </em>。</li><li id="2668" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><em class="oo"> α </em>可以通过逆类频率设置，也可以作为超参数交叉验证设置。</li><li id="72a1" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">如在两级检测器处看到的，通过选择 1∶3 的前景与背景比率来隐含地实现<em class="oo"> α </em>。</li></ul><h2 id="e5f2" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">2.3.焦点损失</h2><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi os"><img src="../Images/76be56aa2b5a5d1f857b5907522bc86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*gO_nxGFmpAelOrU_D9O5-Q.png"/></div></figure><ul class=""><li id="e112" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated">损失函数被重塑为降低简单例子的权重，从而将训练集中在硬负面上。一个调制因子(1- <em class="oo"> pt </em> )^ <em class="oo"> γ </em>被添加到交叉熵损失中，其中<em class="oo"> γ </em>在实验中从[0，5]开始被测试。</li><li id="b2ba" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">FL 有两个属性:</li></ul><ol class=""><li id="24f4" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la ni nj nk nl bi translated">当一个例子被错误分类并且 pt 很小时，调制因子接近 1，并且损失不受影响。当<em class="oo"> pt </em> →1 时，该因子变为 0，并且良好分类示例的损失被向下加权。</li><li id="e96e" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">聚焦参数<em class="oo"> γ </em>平滑地调整简单示例向下加权的速率。当<em class="oo"> γ </em> = 0 时，FL 相当于 CE。当<em class="oo"> γ </em>增加时，调制因子的效果同样增加。(<em class="oo"> γ </em> =2 在实验中效果最好。)</li></ol><ul class=""><li id="456f" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated">例如，在<em class="oo"> γ </em> = 2 的情况下，与 ce 相比，分类为<em class="oo"> pt </em> = 0.9 的示例将具有低 100 的损耗，而分类为<em class="oo"> pt </em> = 0.968 的示例将具有低 1000 的损耗。这反过来增加了纠正错误分类的例子的重要性。</li><li id="9647" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">当<em class="oo"> pt </em> ≤ 0.5 且γ = 2 时，损耗最多降低 4 倍。</li></ul><h2 id="ba0f" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">2.4.FL 的α平衡变体</h2><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/365a25bd38be1b8831469041c07ff164.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*Wa6UX2I9AEtBrj5focAETA.png"/></div></figure><ul class=""><li id="6265" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated">上述形式用于实践中的实验，其中将<em class="oo"> α </em>添加到方程中，这产生了比没有<em class="oo"> α </em>的方程略微提高的精度。并使用<strong class="kh ir"> sigmoid 激活函数计算<em class="oo"> p </em> </strong> <em class="oo"> </em>导致<strong class="kh ir">更大的数值稳定性</strong>。</li><li id="299c" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><strong class="kh ir"> <em class="oo"> γ </em>:多关注硬例。</strong></li><li id="dac9" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><strong class="kh ir"> <em class="oo"> α </em>:抵销类实例数的不平衡。</strong></li></ul><h2 id="d7f4" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">2.5.模型初始化</h2><ul class=""><li id="7bf4" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated"><strong class="kh ir">在训练</strong>开始时，为<em class="oo"> p </em>的值设置一个先验π，使得模型对于稀有类的估计<em class="oo"> p </em>较低，例如<strong class="kh ir"> 0.01 </strong>，以提高严重类不平衡情况下的训练稳定性。</li><li id="f7fc" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">发现训练 RetinaNet 使用标准 CE 损耗<strong class="kh ir">而没有使用先验π </strong>进行初始化，导致<strong class="kh ir">网络在训练</strong>时发散，最终失败。</li><li id="6614" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">并且结果对π的精确值不敏感。π = 0.01 用于所有实验。</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="81e5" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">3.<strong class="ak">视网膜网探测器</strong></h1><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi ou"><img src="../Images/40bb4ecf6c20f3d79e784dc6cf7e10bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0-GVAp6WCzPMR6puuaYQTQ.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="bd lz">RetinaNet Detector Architecture</strong></figcaption></figure><h2 id="2c25" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">3.1.(a)和(b)主干</h2><ul class=""><li id="ef91" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kh ir"> ResNet </strong> </a> <strong class="kh ir"> </strong>用于深度特征提取。</li><li id="9ad0" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> <strong class="kh ir">特征金字塔网络(FPN) </strong> </a>在<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>之上使用，用于从一个单一分辨率输入图像构建丰富的多尺度特征金字塔。(最初，<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a>是一个两级探测器，具有最先进的结果。如果有兴趣，请阅读我关于 FPN 的评论。)</li><li id="a347" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a>是多尺度的，在所有尺度上语义都很强，并且计算速度很快。</li><li id="32f0" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">这里有一些对 FPN 的适度改变。从 P3 到 P7 生成一个金字塔。一些主要的变化是:由于计算的原因，现在不使用 P2。(ii)通过步长卷积而不是下采样来计算 P6。(iii)额外包括 P7 以提高大物体检测的精度。</li></ul><h2 id="2dc1" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">3.2.锚</h2><ul class=""><li id="0553" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated">主播分别在从 P3 到 P7 的金字塔等级上有 32 到 512 的<strong class="kh ir">区域。</strong></li><li id="41d2" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><strong class="kh ir">使用三种长宽比{1:2，1:1，2:1} </strong>。</li><li id="7b2d" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">对于更密集的规模覆盖，在每个金字塔等级添加<strong class="kh ir">大小为{2⁰、2^(1/3、</strong>的锚。</li><li id="c781" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">总共，<strong class="kh ir">每级 9 个锚</strong>。</li><li id="aa38" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><strong class="kh ir">跨等级，比例覆盖从 32 到 813 像素</strong>。</li><li id="cda1" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><strong class="kh ir">每个锚</strong>，都有<strong class="kh ir">一个长度<em class="oo"> K </em>一个分类目标的热向量</strong> (K:类数)<strong class="kh ir">一个盒回归目标的 4 向量</strong>。</li><li id="4b3d" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><strong class="kh ir">使用 IoU 阈值 0.5 </strong>将锚点分配给地面实况对象框，如果 IoU 在[0，0.4】中，则将其分配给<strong class="kh ir">背景。每个锚点最多被分配一个对象框，并在那个<em class="oo"> K </em> one-hot vector 中设置相应的类条目为 1，所有其他条目为 0。如果锚<strong class="kh ir">未赋值如果 IoU 在【0.4，0.5】</strong>中并且在训练期间被忽略。</strong></li><li id="09ae" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">长方体回归计算为锚点和指定对象长方体之间的偏移，如果没有指定，则忽略该值。</li></ul><h2 id="c4cd" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">3.3.(c)分类子网</h2><ul class=""><li id="dff7" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated">这个分类子网<strong class="kh ir">为每个<em class="oo"> A </em>锚和<em class="oo"> K </em>物体类别预测在每个空间位置</strong>物体存在的概率。</li><li id="1f8d" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">子网是一个<strong class="kh ir"> FCN </strong>，它应用了四个 3×3 conv 层，每个层都有<em class="oo"> C </em>滤波器，每个层后面都有 ReLU 激活，后面是一个 3×3 conv 层，有<em class="oo"> KA </em>滤波器。(<em class="oo"> K </em>类，<em class="oo"> A </em> =9 个锚点，<em class="oo"> C </em> = 256 个过滤器)</li></ul><h2 id="4c21" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">3.4.(d)箱式回归子网</h2><ul class=""><li id="9441" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated">该子网是每个金字塔等级的<strong class="kh ir"> FCN </strong>,用于回归从每个锚框到附近地面实况对象(如果存在)的偏移。</li><li id="344a" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">它与分类子网相同，除了它终止于每个空间位置的<strong class="kh ir"> 4 <em class="oo"> A </em>线性输出。</strong></li><li id="cd24" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">它是一个<strong class="kh ir">类别不可知的包围盒回归器</strong>，使用较少的参数，被发现同样有效。</li></ul><h2 id="9265" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">3.5.推理</h2><ul class=""><li id="a701" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated">在阈值检测器置信度为 0.05 之后，网络仅解码来自<strong class="kh ir">的每个</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"><strong class="kh ir">FPN</strong></a><strong class="kh ir">等级的最多 1k 最高得分预测的框预测。</strong></li><li id="8f71" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated"><strong class="kh ir">来自所有级别的顶部预测被合并</strong>并且<strong class="kh ir">阈值为 0.5 的非最大抑制(NMS)被应用</strong>以产生最终检测。</li></ul><h2 id="15ff" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">3.6.培养</h2><ul class=""><li id="c0ea" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated">因此，在训练期间，图像的<strong class="kh ir">总聚焦损失</strong>被计算为所有 100k 个锚的聚焦损失的总和<strong class="kh ir">，由分配给地面实况框的锚的数量归一化。</strong></li><li id="67b6" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">使用 ImageNet1K 预训练的<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> ResNet-50-FPN </a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> ResNet-101-FPN </a>。</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="334e" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">4.消融研究</h1><ul class=""><li id="17ce" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated"><strong class="kh ir">使用 COCO 数据集</strong>。COCO <strong class="kh ir"> trainval35k </strong>分体用于<strong class="kh ir">训练</strong>。并且<strong class="kh ir"> minival (5k) </strong> split 用于<strong class="kh ir">验证</strong>。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/01e0ef894ce358451a9278e1a5b2f717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*FuhGUeyfJBUb565roMGMrg.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="bd lz">α for CE loss (Left), γ for FL (Right)</strong></figcaption></figure><h2 id="54d7" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">4.1.<em class="pa"> α为</em>α-平衡 CE 损耗</h2><ul class=""><li id="56dd" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated">使用 ResNet-50 。</li><li id="56a6" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">首先，测试不同α下的α平衡 CE 损耗。</li><li id="22e4" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">α = 0.75 时，增益为 0.9 AP。</li></ul><h2 id="ec0d" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">4.2.<em class="pa"> γ </em>为 FL</h2><ul class=""><li id="8427" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated">γ=0 是α平衡的 CE 损耗。</li><li id="45fc" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">当γ增加时，简单的例子会被贴现到损失中。</li><li id="435e" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">γ=2，α=0.25，比α平衡 CE 损耗(α=0.75)提高了 2.9 AP。</li><li id="66c5" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">据观察，较低的α选择较高的γ</li><li id="2220" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">改变的好处要大得多，事实上，最佳α的范围仅为[0.25，0.75]，α∈[:01；:999]已测试。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi pb"><img src="../Images/24057b0cf991bbb6d0576489630e9639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_jD7K0G3bTrP9V3vhtDapg.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="bd lz">Cumulative distribution functions of the normalized loss for positive and negative samples</strong></figcaption></figure><h2 id="96db" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">4.3.前景和背景样本分析</h2><h2 id="1f10" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated"><strong class="ak">前景样本</strong></h2><ul class=""><li id="2241" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated">对从最低到最高的损失进行排序，并绘制正负样本和不同γ设置的累积分布函数(CDF)。</li><li id="1a29" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">大约 20%的最难阳性样本占阳性损失的大约一半。</li><li id="709d" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">随着γ的增加，更多的损失集中在前 20%的例子中，但是影响很小。</li></ul><h2 id="6618" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated"><strong class="ak">背景样本</strong></h2><ul class=""><li id="daa8" class="nb nc iq kh b ki nd kl ne ko nf ks ng kw nh la od nj nk nl bi translated">随着γ的增加，更多的重量集中在硬反例上。</li><li id="f0f8" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">绝大多数损失来自一小部分样本。</li><li id="83c0" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">FL 可以有效地降低容易否定的影响，将所有注意力集中在难否定的例子上。</li></ul><h2 id="02da" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">4.4.锚密度</h2><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/cc3600ce082817af85c10cdda850de93.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*_Db6bz0kPw6sNeC5yLfhoQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="bd lz">Different Number of Scales (#sc) and Aspect Ratios (#ar)</strong></figcaption></figure><ul class=""><li id="64c6" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated">使用一个方形锚(#sc=1，#ar=1)实现了 30.3%的 AP，这还不错。</li><li id="8be7" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">使用 3 个尺度和 3 个长宽比，AP 可以提高近 4 个点(34.0)。</li><li id="bbb9" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">增加超过 6-9 个锚不会显示进一步的收益。</li></ul><h2 id="b436" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">4.5.佛罗里达州对 OHEM(在线硬示例挖掘)</h2><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/0a29ac4a1d7787d8764c12ee1496f34e.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*xAUjeIijE6fzGECGsQNVcQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="bd lz">FL vs OHEM (Online Hard Example Mining)</strong></figcaption></figure><ul class=""><li id="ec9e" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated">这里使用<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-101 </a>。</li><li id="4633" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">在 OHEM，每个示例根据其损失评分，然后应用非最大抑制(NMS ),并使用最高损失示例构建一个小批次。</li><li id="ec02" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">像焦点损失一样，OHEM 更加强调错误分类的例子。</li><li id="3fdb" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">但是与 FL 不同，OHEM 完全抛弃了简单的例子。</li><li id="106a" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">在将 nms 应用于所有示例后，构建迷你批处理以强制正负比例为 1:3。</li><li id="f0e9" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">OHEM 的最佳设置(比例不为 1:3，批量为 128，NMS 为 0.5)达到 32.8%的 AP。</li><li id="5565" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">而 FL 获得 36.0% AP，即差距 3.2 AP，证明了 FL 的有效性。</li><li id="2305" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">注:作者还测试了铰链损耗，其中损耗在 pt 的某个值以上设置为 0。但是，训练是不稳定的。</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="660f" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><strong class="ak"> 5。与最先进方法的比较</strong></h1><h2 id="67ba" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">5.1.速度与精度的权衡</h2><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi pe"><img src="../Images/f80eccc6013166b0c079037e1578277c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFIPWaRP7ULSmMKq2gHwOQ.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="bd lz">Speed versus Accuracy</strong></figcaption></figure><ul class=""><li id="5b6b" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated"><strong class="kh ir">retina net-101–600</strong>:retina net 配有<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> ResNet-101-FPN </a>和 600 像素的图像比例，与最近发布的<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> ResNet-101-FPN 更快的 R-CNN (FPN) </a>的精确度相当，同时每张图像的运行时间为 122 毫秒，而不是 172 毫秒(均在 Nvidia M40 GPU 上测量)。</li><li id="a7ee" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">更大的主干网络产生更高的准确性，但也降低了推理速度。</li><li id="9c98" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">培训时间从 10 小时到 35 小时不等。</li><li id="01cb" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">使用更大的规模允许 RetinaNet 超越所有两阶段方法的准确性，同时仍然更快。</li><li id="b753" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">除了<a class="ae lk" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65"> YOLOv2 </a>(目标是极高的帧率)，RetinaNet 的表现优于<a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>、<a class="ae lk" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a>、<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a>。</li><li id="41b2" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">对于更快的运行时间，只有一个工作点(500 像素输入)，在这个点上使用<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> ResNet-50-FPN </a>的 RetinaNet 比使用<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> ResNet-101-FPN </a>的 retina net 有所改进。</li></ul><h2 id="44af" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">5.2.最先进的精确度</h2><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi pf"><img src="../Images/11cffbb356309ede238fb6f2028cbb26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQ0RWNKloCGWZ9l6oLu2SA.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk"><strong class="bd lz">Object detection single-model results (bounding box AP), vs. state-of-the-art on COCO test-dev</strong></figcaption></figure><ul class=""><li id="b6b2" class="nb nc iq kh b ki kj kl km ko of ks og kw oh la od nj nk nl bi translated"><strong class="kh ir"> RetinaNet 使用</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"><strong class="kh ir">ResNet-101-FPN</strong></a><strong class="kh ir">:retina net-101–800</strong>模型使用<strong class="kh ir">比例抖动</strong>训练，比表(5.1)中的模型长 1.5 倍。</li><li id="9db1" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">与现有的单级检测器相比，它与最接近的竞争对手<a class="ae lk" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a>的 AP 差距为 5.9 点(39.1 对 33.2)。</li><li id="cacd" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">与最近的两阶段方法相比，RetinaNet 比基于<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet-v2</a>-<a class="ae lk" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>的表现最好的<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>模型高出 2.3 个点。(如果有兴趣，请阅读我关于<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>和<a class="ae lk" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener"> TDM </a>的评测。)</li><li id="1f4f" class="nb nc iq kh b ki nm kl nn ko no ks np kw nq la od nj nk nl bi translated">RetinaNet 使用<a class="ae lk" href="http://ResNeXt" rel="noopener ugc nofollow" target="_blank">ResNeXt-101</a>-<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>:插上<a class="ae lk" href="http://ResNeXt" rel="noopener ugc nofollow" target="_blank">ResNeXt-32x8d-101</a>-<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>【38】作为 RetinaNet 主干，成绩再提升 1.7 AP，超过 COCO 上的 40 AP。(如果有兴趣，请阅读我关于<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>的评论。)</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="16bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过使用焦点损耗，可以在简单样本和困难样本之间自适应地平衡总损耗。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h2 id="8b3d" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">参考</h2><p id="9cb3" class="pw-post-body-paragraph kf kg iq kh b ki nd jr kk kl ne ju kn ko pg kq kr ks ph ku kv kw pi ky kz la ij bi translated">【2017 ICCV】【retina net】<br/><a class="ae lk" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank">密集物体探测的焦损失</a></p><h2 id="205f" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ko nw nx mv ks ny nz mx kw oa ob mz oc bi translated">我的相关评论</h2><p id="4da9" class="pw-post-body-paragraph kf kg iq kh b ki nd jr kk kl ne ju kn ko pg kq kr ks ph ku kv kw pi ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a></p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a></p><p id="fc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong>[<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a></p><p id="3134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割<br/>T32】[<a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></strong></p><p id="58de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">超分辨率<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">Sr CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4">fsr CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f">VDSR</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener">ESPCN</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>】</p></div></div>    
</body>
</html>