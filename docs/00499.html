<html>
<head>
<title>Parallelize Processing a Large AWS S3 File</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">å¹¶è¡Œå¤„ç†å¤§å‹ AWS S3 æ–‡ä»¶</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/parallelize-processing-a-large-aws-s3-file-d43a580cea3?source=collection_archive---------19-----------------------#2019-01-22">https://towardsdatascience.com/parallelize-processing-a-large-aws-s3-file-d43a580cea3?source=collection_archive---------19-----------------------#2019-01-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="20e0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">è¿™ç¯‡æ–‡ç« å±•ç¤ºäº†ä½¿ç”¨ AWS S3 é€‰æ‹©å°†ä¸€ä¸ªå¤§çš„ AWS S3 æ–‡ä»¶(å¯èƒ½æœ‰æ•°ç™¾ä¸‡æ¡è®°å½•)å¤„ç†æˆå¯ç®¡ç†çš„å¹¶è¡Œå—çš„æ–¹æ³•</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/501c1e9666e0960690c2d12d195eb454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9TZemgXW6-VOWQ3dli9LMA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Parallel Processing S3 File Workflow | </strong>Image created by Author</figcaption></figure><p id="a64b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">åœ¨æˆ‘çš„<a class="ae ls" rel="noopener" target="_blank" href="/efficiently-streaming-a-large-aws-s3-file-via-s3-select-85f7fbe22e46">ä¸Šä¸€ç¯‡æ–‡ç« </a>ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†é€šè¿‡ S3 é€‰æ‹©æ¥æé«˜å¤„ç†å¤§å‹ AWS S3 æ–‡ä»¶çš„æ•ˆç‡ã€‚å¤„ç†è¿‡ç¨‹æœ‰ç‚¹é¡ºåºï¼Œå¯¹äºä¸€ä¸ªå¤§æ–‡ä»¶æ¥è¯´å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•åœ¨å¤šä¸ªå•å…ƒä¹‹é—´å¹¶è¡Œå¤„ç†å‘¢ï¼ŸğŸ¤”å—¯ï¼Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°å®ƒï¼Œå¹¶çœ‹åˆ°å®ƒçš„å·¥ä½œï¼</p><p id="9cd7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ğŸ“<em class="lt">æˆ‘å¼ºçƒˆæ¨èé€šè¿‡ S3 æŸ¥çœ‹æˆ‘åœ¨</em> <a class="ae ls" rel="noopener" target="_blank" href="/efficiently-streaming-a-large-aws-s3-file-via-s3-select-85f7fbe22e46"> <em class="lt">ä¸Šçš„ä¸Šä¸€ç¯‡æ–‡ç« â€”â€”é€‰æ‹©</em> </a> <em class="lt">æ¥è®¾ç½®è¿™ç¯‡æ–‡ç« çš„èƒŒæ™¯ã€‚</em></p><p id="24b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æˆ‘æ€»æ˜¯å–œæ¬¢æŠŠä¸€ä¸ªé—®é¢˜åˆ†è§£æˆè§£å†³å®ƒæ‰€å¿…éœ€çš„å°éƒ¨åˆ†(åˆ†ææ–¹æ³•)ã€‚è®©æˆ‘ä»¬è¯•ç€ç”¨ä¸‰ä¸ªç®€å•çš„æ­¥éª¤æ¥è§£å†³è¿™ä¸ªé—®é¢˜:</p><h1 id="e630" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">1.æ‰¾å‡º S3 æ–‡ä»¶çš„æ€»å­—èŠ‚æ•°</h1><p id="3c23" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">ä¸æˆ‘ä»¬ä¸Šä¸€ç¯‡æ–‡ç« çš„ç¬¬ä¸€æ­¥éå¸¸ç›¸ä¼¼ï¼Œè¿™é‡Œæˆ‘ä»¬ä¹Ÿå°è¯•å…ˆæ‰¾åˆ°æ–‡ä»¶å¤§å°ã€‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µå±•ç¤ºäº†å°†å¯¹æˆ‘ä»¬çš„ S3 æ–‡ä»¶æ‰§è¡Œ HEAD è¯·æ±‚å¹¶ç¡®å®šæ–‡ä»¶å¤§å°(ä»¥å­—èŠ‚ä¸ºå•ä½)çš„å‡½æ•°ã€‚</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="91d3" class="mw lv iq ms b gy mx my l mz na"># core/utils.py<br/><br/>def get_s3_file_size(bucket: str, key: str) -&gt; int:<br/>    """Gets the file size of S3 object by a HEAD request<br/><br/>    Args:<br/>        bucket (str): S3 bucket<br/>        key (str): S3 object path<br/><br/>    Returns:<br/>        int: File size in bytes. Defaults to 0 if any error.<br/>    """<br/>    aws_profile = current_app.config.get('AWS_PROFILE_NAME')<br/>    s3_client = boto3.session.Session(profile_name=aws_profile).client('s3')<br/>    file_size = 0<br/>    try:<br/>        response = s3_client.head_object(Bucket=bucket, Key=key)<br/>        if response:<br/>            file_size = int(response.get('ResponseMetadata').get('HTTPHeaders').get('content-length'))<br/>    except ClientError:<br/>        logger.exception(f'Client error reading S3 file {bucket} : {key}')<br/>    return file_size</span></pre><h1 id="39b3" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">2.åˆ›å»ºä¸€ä¸ªèŠ¹èœä»»åŠ¡æ¥å¤„ç†ä¸€ä¸ªå—</h1><p id="5cfe" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">è¿™é‡Œï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ª celery ä»»åŠ¡æ¥å¤„ç†ä¸€ä¸ªæ–‡ä»¶å—(ç¨åå°†å¹¶è¡Œæ‰§è¡Œ)ã€‚è¿™é‡Œçš„æ•´ä¸ªå¤„ç†è¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤º:</p><ul class=""><li id="dfad" class="nb nc iq ky b kz la lc ld lf nd lj ne ln nf lr ng nh ni nj bi translated">æ¥æ”¶è¿™ä¸ªå—çš„<code class="fe nk nl nm ms b">start</code>å’Œ<code class="fe nk nl nm ms b">end bytes</code>ä½œä¸ºå‚æ•°</li><li id="b66a" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">é€šè¿‡ S3 è·å– S3 æ–‡ä»¶çš„è¿™ä¸€éƒ¨åˆ†â€”â€”é€‰æ‹©å¹¶å°†å…¶å­˜å‚¨åœ¨æœ¬åœ°çš„ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ä¸­(åœ¨æœ¬ä¾‹ä¸­ä¸º CSV)</li><li id="af0e" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">è¯»å–è¿™ä¸ªä¸´æ—¶æ–‡ä»¶å¹¶æ‰§è¡Œä»»ä½•éœ€è¦çš„å¤„ç†</li><li id="4954" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">åˆ é™¤è¿™ä¸ªä¸´æ—¶æ–‡ä»¶</li></ul><p id="93b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ğŸ“æˆ‘å°†è¿™ä¸ªä»»åŠ¡ç§°ä¸ºæ–‡ä»¶å—å¤„ç†å™¨ã€‚å®ƒå¤„ç†æ–‡ä»¶ä¸­çš„ä¸€ä¸ªå—ã€‚è¿è¡Œå¤šä¸ªè¿™æ ·çš„ä»»åŠ¡å¯ä»¥å®Œæˆæ•´ä¸ªæ–‡ä»¶çš„å¤„ç†ã€‚</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="e2c7" class="mw lv iq ms b gy mx my l mz na"># core/tasks.py<br/><br/>@celery.task(name='core.tasks.chunk_file_processor', bind=True)<br/>def chunk_file_processor(self, **kwargs):<br/>    """ Creates and process a single file chunk based on S3 Select ScanRange start and end bytes<br/>    """<br/>    bucket = kwargs.get('bucket')<br/>    key = kwargs.get('key')<br/>    filename = kwargs.get('filename')<br/>    start_byte_range = kwargs.get('start_byte_range')<br/>    end_byte_range = kwargs.get('end_byte_range')<br/>    header_row_str = kwargs.get('header_row_str')<br/>    local_file = filename.replace('.csv', f'.{start_byte_range}.csv')<br/>    file_path = path.join(current_app.config.get('BASE_DIR'), 'temp', local_file)<br/><br/>    logger.info(f'Processing {filename} chunk range {start_byte_range} -&gt; {end_byte_range}')<br/>    try:<br/>        # 1. fetch data from S3 and store it in a file<br/>        store_scrm_file_s3_content_in_local_file(<br/>            bucket=bucket, key=key, file_path=file_path, start_range=start_byte_range,<br/>            end_range=end_byte_range, delimiter=S3_FILE_DELIMITER, header_row=header_row_str)<br/><br/>        # 2. Process the chunk file in temp folder<br/>        id_set = set()<br/>        with open(file_path) as csv_file:<br/>            csv_reader = csv.DictReader(csv_file, delimiter=S3_FILE_DELIMITER)<br/>            for row in csv_reader:<br/>                # perform any other processing here<br/>                id_set.add(int(row.get('id')))<br/>        logger.info(f'{min(id_set)} --&gt; {max(id_set)}')<br/><br/>        # 3. delete local file<br/>        if path.exists(file_path):<br/>            unlink(file_path)<br/>    except Exception:<br/>        logger.exception(f'Error in file processor: {filename}')</span></pre><h1 id="e368" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">3.å¹¶è¡Œæ‰§è¡Œå¤šä¸ª celery ä»»åŠ¡</h1><p id="c259" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">è¿™æ˜¯è¿™ä¸ªæµç¨‹ä¸­æœ€æœ‰è¶£çš„ä¸€æ­¥ã€‚æˆ‘ä»¬å°†é€šè¿‡<a class="ae ls" href="https://docs.celeryproject.org/en/stable/userguide/canvas.html#groups" rel="noopener ugc nofollow" target="_blank"> celery Group </a>åˆ›å»ºå¤šä¸ªå¹¶è¡Œè¿è¡Œçš„ Celery ä»»åŠ¡ã€‚<br/>ä¸€æ—¦æˆ‘ä»¬çŸ¥é“äº† S3 ä¸­ä¸€ä¸ªæ–‡ä»¶çš„æ€»å­—èŠ‚æ•°(æ¥è‡ªæ­¥éª¤ 1)ï¼Œæˆ‘ä»¬å°±ä¸ºè¿™ä¸ªå—è®¡ç®—<code class="fe nk nl nm ms b">start</code>å’Œ<code class="fe nk nl nm ms b">end bytes</code>ï¼Œå¹¶é€šè¿‡ celery ç»„è°ƒç”¨æˆ‘ä»¬åœ¨æ­¥éª¤ 2 ä¸­åˆ›å»ºçš„ä»»åŠ¡ã€‚<code class="fe nk nl nm ms b">start</code>å’Œ<code class="fe nk nl nm ms b">end bytes</code>èŒƒå›´æ˜¯æ–‡ä»¶å¤§å°çš„è¿ç»­èŒƒå›´ã€‚å¯é€‰åœ°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨æ‰€æœ‰å¤„ç†ä»»åŠ¡å®Œæˆåè°ƒç”¨å›è°ƒ(ç»“æœ)ä»»åŠ¡ã€‚</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="4cbf" class="mw lv iq ms b gy mx my l mz na"># core/tasks.py<br/><br/>@celery.task(name='core.tasks.s3_parallel_file_processing', bind=True)<br/>def s3_parallel_file_processing_task(self, **kwargs):<br/>    """ Creates celery tasks to process chunks of file in parallel<br/>    """<br/>    bucket = kwargs.get('bucket')<br/>    key = kwargs.get('key')<br/>    try:<br/>        filename = key<br/>        # 1. Check file headers for validity -&gt; if failed, stop processing<br/>        desired_row_headers = (<br/>            'id',<br/>            'name',<br/>            'age',<br/>            'latitude',<br/>            'longitude',<br/>            'monthly_income',<br/>            'experienced'<br/>        )<br/>        is_headers_valid, header_row_str = validate_scrm_file_headers_via_s3_select(<br/>            bucket=bucket,<br/>            key=key,<br/>            delimiter=S3_FILE_DELIMITER,<br/>            desired_headers=desired_row_headers)<br/>        if not is_headers_valid:<br/>            logger.error(f'{filename} file headers validation failed')<br/>            return False<br/>        logger.info(f'{filename} file headers validation successful')<br/><br/>        # 2. fetch file size via S3 HEAD<br/>        file_size = get_s3_file_size(bucket=bucket, key=key)<br/>        if not file_size:<br/>            logger.error(f'{filename} file size invalid {file_size}')<br/>            return False<br/>        logger.info(f'We are processing {filename} file about {file_size} bytes :-o')<br/><br/>        # 2. Create celery group tasks for chunk of this file size for parallel processing<br/>        start_range = 0<br/>        end_range = min(S3_FILE_PROCESSING_CHUNK_SIZE, file_size)<br/>        tasks = []<br/>        while start_range &lt; file_size:<br/>            tasks.append(<br/>                chunk_file_processor.signature(<br/>                    kwargs={<br/>                        'bucket': bucket,<br/>                        'key': key,<br/>                        'filename': filename,<br/>                        'start_byte_range': start_range,<br/>                        'end_byte_range': end_range,<br/>                        'header_row_str': header_row_str<br/>                    }<br/>                )<br/>            )<br/>            start_range = end_range<br/>            end_range = end_range + min(S3_FILE_PROCESSING_CHUNK_SIZE, file_size - end_range)<br/>        job = (group(tasks) | chunk_file_processor_callback.s(data={'filename': filename}))<br/>        _ = job.apply_async()<br/>    except Exception:<br/>        logger.exception(f'Error processing file: {filename}')<br/><br/><br/>@celery.task(name='core.tasks.chunk_file_processor_callback', bind=True, ignore_result=False)<br/>def chunk_file_processor_callback(self, *args, **kwargs):<br/>    """ Callback task called post chunk_file_processor()<br/>    """<br/>    logger.info('Callback called')</span><span id="8ee5" class="mw lv iq ms b gy ns my l mz na"><br/># core/utils.py<br/><br/>def store_scrm_file_s3_content_in_local_file(bucket: str, key: str, file_path: str, start_range: int, end_range: int,<br/>                                             delimiter: str, header_row: str):<br/>    """Retrieves S3 file content via S3 Select ScanRange and store it in a local file.<br/>       Make sure the header validation is done before calling this.<br/><br/>    Args:<br/>        bucket (str): S3 bucket<br/>        key (str): S3 key<br/>        file_path (str): Local file path to store the contents<br/>        start_range (int): Start range of ScanRange parameter of S3 Select<br/>        end_range (int): End range of ScanRange parameter of S3 Select<br/>        delimiter (str): S3 file delimiter<br/>        header_row (str): Header row of the local file. This will be inserted as first line in local file.<br/>    """<br/>    aws_profile = current_app.config.get('AWS_PROFILE_NAME')<br/>    s3_client = boto3.session.Session(profile_name=aws_profile).client('s3')<br/>    expression = 'SELECT * FROM S3Object'<br/>    try:<br/>        response = s3_client.select_object_content(<br/>            Bucket=bucket,<br/>            Key=key,<br/>            ExpressionType='SQL',<br/>            Expression=expression,<br/>            InputSerialization={<br/>                'CSV': {<br/>                    'FileHeaderInfo': 'USE',<br/>                    'FieldDelimiter': delimiter,<br/>                    'RecordDelimiter': '\n'<br/>                }<br/>            },<br/>            OutputSerialization={<br/>                'CSV': {<br/>                    'FieldDelimiter': delimiter,<br/>                    'RecordDelimiter': '\n',<br/>                },<br/>            },<br/>            ScanRange={<br/>                'Start': start_range,<br/>                'End': end_range<br/>            },<br/>        )<br/><br/>        """<br/>        select_object_content() response is an event stream that can be looped to concatenate the overall result set<br/>        """<br/>        f = open(file_path, 'wb')  # we receive data in bytes and hence opening file in bytes<br/>        f.write(header_row.encode())<br/>        f.write('\n'.encode())<br/>        for event in response['Payload']:<br/>            if records := event.get('Records'):<br/>                f.write(records['Payload'])<br/>        f.close()<br/>    except ClientError:<br/>        logger.exception(f'Client error reading S3 file {bucket} : {key}')<br/>    except Exception:<br/>        logger.exception(f'Error reading S3 file {bucket} : {key}')</span></pre><p id="a6d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">å°±æ˜¯è¿™æ ·ï¼ğŸ˜ç°åœ¨ï¼Œæˆ‘ä»¬ä¸æ˜¯ä¸€ä¸ªå­—èŠ‚ä¸€ä¸ªå­—èŠ‚åœ°ä¼ è¾“ S3 æ–‡ä»¶ï¼Œè€Œæ˜¯é€šè¿‡å¹¶å‘å¤„ç†æ•°æ®å—æ¥å®ç°å¹¶è¡Œå¤„ç†ã€‚æ²¡é‚£ä¹ˆéš¾ï¼Œä¸æ˜¯å—ï¼ŸğŸ˜…</p><p id="d06d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ğŸ“Œæ‚¨å¯ä»¥<a class="ae ls" href="https://github.com/idris-rampurawala/s3-select-demo" rel="noopener ugc nofollow" target="_blank">æŸ¥çœ‹æˆ‘çš„ GitHub åº“</a>ä»¥è·å¾—è¿™ç§æ–¹æ³•çš„å®Œæ•´å·¥ä½œç¤ºä¾‹ã€‚</p></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="82e0" class="lu lv iq bd lw lx oa lz ma mb ob md me jw oc jx mg jz od ka mi kc oe kd mk ml bi translated">ğŸ”æ¯”è¾ƒå¤„ç†æ—¶é—´</h1><p id="54f4" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">å¦‚æœæˆ‘ä»¬ç”¨è¿™ç§æ–¹æ³•æ¯”è¾ƒæˆ‘ä»¬åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­å¤„ç†çš„åŒä¸€ä¸ªæ–‡ä»¶çš„å¤„ç†æ—¶é—´ï¼Œå¤„ç†é€Ÿåº¦å¤§çº¦æ¯”<strong class="ky ir">å¿« 68%</strong>(ä½¿ç”¨ç›¸åŒçš„ç¡¬ä»¶å’Œé…ç½®)ã€‚ğŸ˜†</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="3a85" class="mw lv iq ms b gy mx my l mz na">â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—<br/>â•‘                 â•‘ <strong class="ms ir">Streaming S3 File </strong>â•‘ <strong class="ms ir">Parallel Processing S3 File</strong>â•‘<br/>â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£<br/>â•‘ <strong class="ms ir">File size       </strong>â•‘ 4.8MB             â•‘ 4.8MB                      â•‘<br/>â•‘ <strong class="ms ir">Processing time</strong> â•‘ ~37 seconds       â•‘ ~12 seconds                â•‘<br/>â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/0193f332707396b6048acd2a2001764f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sNkoN3dVowDmGdGE.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Streaming S3 File</strong> <strong class="bd kv">Logs </strong>| Image by the Author</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/60aa39cc9e6bad315a14b04265b71fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WevibFiMZYZk1skA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Parallel Processing S3 File Logs | </strong>Image by the Author</figcaption></figure></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="5cf0" class="lu lv iq bd lw lx oa lz ma mb ob md me jw oc jx mg jz od ka mi kc oe kd mk ml bi translated">âœ”ï¸è¿™ç§æ–¹æ³•çš„å¥½å¤„</h1><ul class=""><li id="8040" class="nb nc iq ky b kz mm lc mn lf og lj oh ln oi lr ng nh ni nj bi translated">åŒ…å«æ•°ç™¾ä¸‡æ¡è®°å½•çš„éå¸¸å¤§çš„æ–‡ä»¶å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å¾—åˆ°å¤„ç†ã€‚æˆ‘åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨è¿™ç§æ–¹æ³•å·²ç»æœ‰ä¸€æ®µæ—¶é—´äº†ï¼Œå®ƒéå¸¸ä»¤äººæ„‰å¿«</li><li id="64bb" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">è®¡ç®—å’Œå¤„ç†åˆ†å¸ƒåœ¨åˆ†å¸ƒçš„å·¥ä½œäººå‘˜ä¸­</li><li id="f67c" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">å·¥ä½œæ± çš„å¯ç”¨æ€§å¯ä»¥è°ƒæ•´å¤„ç†é€Ÿåº¦</li><li id="0ae9" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">ä¸å†æœ‰å†…å­˜é—®é¢˜</li></ul></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="dd8c" class="lu lv iq bd lw lx oa lz ma mb ob md me jw oc jx mg jz od ka mi kc oe kd mk ml bi translated">ğŸ“‘èµ„æº</h1><ul class=""><li id="e388" class="nb nc iq ky b kz mm lc mn lf og lj oh ln oi lr ng nh ni nj bi translated"><a class="ae ls" href="https://github.com/idris-rampurawala/s3-select-demo" rel="noopener ugc nofollow" target="_blank">æˆ‘çš„ GitHub åº“å±•ç¤ºäº†ä¸Šè¿°æ–¹æ³•</a></li><li id="b67e" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated"><a class="ae ls" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.select_object_content" rel="noopener ugc nofollow" target="_blank"> AWS S3 é€‰æ‹© boto3 å‚è€ƒå€¼</a></li><li id="79c7" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated"><a class="ae ls" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html" rel="noopener ugc nofollow" target="_blank"> AWS S3 é€‰æ‹©ç”¨æˆ·æŒ‡å—</a></li></ul></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><p id="05e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt">åŸå‘å¸ƒäº 2019 å¹´ 1 æœˆ 22 æ—¥</em><a class="ae ls" href="https://dev.to/idrisrampurawala/parallelize-processing-a-large-aws-s3-file-8eh" rel="noopener ugc nofollow" target="_blank"><em class="lt">https://dev . to</em></a><em class="lt">ã€‚</em></p></div></div>    
</body>
</html>