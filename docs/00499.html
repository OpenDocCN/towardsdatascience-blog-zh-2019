<html>
<head>
<title>Parallelize Processing a Large AWS S3 File</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">并行处理大型 AWS S3 文件</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/parallelize-processing-a-large-aws-s3-file-d43a580cea3?source=collection_archive---------19-----------------------#2019-01-22">https://towardsdatascience.com/parallelize-processing-a-large-aws-s3-file-d43a580cea3?source=collection_archive---------19-----------------------#2019-01-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="20e0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这篇文章展示了使用 AWS S3 选择将一个大的 AWS S3 文件(可能有数百万条记录)处理成可管理的并行块的方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/501c1e9666e0960690c2d12d195eb454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9TZemgXW6-VOWQ3dli9LMA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Parallel Processing S3 File Workflow | </strong>Image created by Author</figcaption></figure><p id="a64b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我的<a class="ae ls" rel="noopener" target="_blank" href="/efficiently-streaming-a-large-aws-s3-file-via-s3-select-85f7fbe22e46">上一篇文章</a>中，我们讨论了通过 S3 选择来提高处理大型 AWS S3 文件的效率。处理过程有点顺序，对于一个大文件来说可能需要很长时间。那么，我们如何在多个单元之间并行处理呢？🤔嗯，在这篇文章中，我们将实现它，并看到它的工作！</p><p id="9cd7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">📝<em class="lt">我强烈推荐通过 S3 查看我在</em> <a class="ae ls" rel="noopener" target="_blank" href="/efficiently-streaming-a-large-aws-s3-file-via-s3-select-85f7fbe22e46"> <em class="lt">上的上一篇文章——选择</em> </a> <em class="lt">来设置这篇文章的背景。</em></p><p id="24b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我总是喜欢把一个问题分解成解决它所必需的小部分(分析方法)。让我们试着用三个简单的步骤来解决这个问题:</p><h1 id="e630" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">1.找出 S3 文件的总字节数</h1><p id="3c23" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">与我们上一篇文章的第一步非常相似，这里我们也尝试先找到文件大小。下面的代码片段展示了将对我们的 S3 文件执行 HEAD 请求并确定文件大小(以字节为单位)的函数。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="91d3" class="mw lv iq ms b gy mx my l mz na"># core/utils.py<br/><br/>def get_s3_file_size(bucket: str, key: str) -&gt; int:<br/>    """Gets the file size of S3 object by a HEAD request<br/><br/>    Args:<br/>        bucket (str): S3 bucket<br/>        key (str): S3 object path<br/><br/>    Returns:<br/>        int: File size in bytes. Defaults to 0 if any error.<br/>    """<br/>    aws_profile = current_app.config.get('AWS_PROFILE_NAME')<br/>    s3_client = boto3.session.Session(profile_name=aws_profile).client('s3')<br/>    file_size = 0<br/>    try:<br/>        response = s3_client.head_object(Bucket=bucket, Key=key)<br/>        if response:<br/>            file_size = int(response.get('ResponseMetadata').get('HTTPHeaders').get('content-length'))<br/>    except ClientError:<br/>        logger.exception(f'Client error reading S3 file {bucket} : {key}')<br/>    return file_size</span></pre><h1 id="39b3" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">2.创建一个芹菜任务来处理一个块</h1><p id="5cfe" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">这里，我们将定义一个 celery 任务来处理一个文件块(稍后将并行执行)。这里的整个处理过程如下所示:</p><ul class=""><li id="dfad" class="nb nc iq ky b kz la lc ld lf nd lj ne ln nf lr ng nh ni nj bi translated">接收这个块的<code class="fe nk nl nm ms b">start</code>和<code class="fe nk nl nm ms b">end bytes</code>作为参数</li><li id="b66a" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">通过 S3 获取 S3 文件的这一部分——选择并将其存储在本地的一个临时文件中(在本例中为 CSV)</li><li id="af0e" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">读取这个临时文件并执行任何需要的处理</li><li id="4954" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">删除这个临时文件</li></ul><p id="93b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">📝我将这个任务称为文件块处理器。它处理文件中的一个块。运行多个这样的任务可以完成整个文件的处理。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="e2c7" class="mw lv iq ms b gy mx my l mz na"># core/tasks.py<br/><br/>@celery.task(name='core.tasks.chunk_file_processor', bind=True)<br/>def chunk_file_processor(self, **kwargs):<br/>    """ Creates and process a single file chunk based on S3 Select ScanRange start and end bytes<br/>    """<br/>    bucket = kwargs.get('bucket')<br/>    key = kwargs.get('key')<br/>    filename = kwargs.get('filename')<br/>    start_byte_range = kwargs.get('start_byte_range')<br/>    end_byte_range = kwargs.get('end_byte_range')<br/>    header_row_str = kwargs.get('header_row_str')<br/>    local_file = filename.replace('.csv', f'.{start_byte_range}.csv')<br/>    file_path = path.join(current_app.config.get('BASE_DIR'), 'temp', local_file)<br/><br/>    logger.info(f'Processing {filename} chunk range {start_byte_range} -&gt; {end_byte_range}')<br/>    try:<br/>        # 1. fetch data from S3 and store it in a file<br/>        store_scrm_file_s3_content_in_local_file(<br/>            bucket=bucket, key=key, file_path=file_path, start_range=start_byte_range,<br/>            end_range=end_byte_range, delimiter=S3_FILE_DELIMITER, header_row=header_row_str)<br/><br/>        # 2. Process the chunk file in temp folder<br/>        id_set = set()<br/>        with open(file_path) as csv_file:<br/>            csv_reader = csv.DictReader(csv_file, delimiter=S3_FILE_DELIMITER)<br/>            for row in csv_reader:<br/>                # perform any other processing here<br/>                id_set.add(int(row.get('id')))<br/>        logger.info(f'{min(id_set)} --&gt; {max(id_set)}')<br/><br/>        # 3. delete local file<br/>        if path.exists(file_path):<br/>            unlink(file_path)<br/>    except Exception:<br/>        logger.exception(f'Error in file processor: {filename}')</span></pre><h1 id="e368" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">3.并行执行多个 celery 任务</h1><p id="c259" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">这是这个流程中最有趣的一步。我们将通过<a class="ae ls" href="https://docs.celeryproject.org/en/stable/userguide/canvas.html#groups" rel="noopener ugc nofollow" target="_blank"> celery Group </a>创建多个并行运行的 Celery 任务。<br/>一旦我们知道了 S3 中一个文件的总字节数(来自步骤 1)，我们就为这个块计算<code class="fe nk nl nm ms b">start</code>和<code class="fe nk nl nm ms b">end bytes</code>，并通过 celery 组调用我们在步骤 2 中创建的任务。<code class="fe nk nl nm ms b">start</code>和<code class="fe nk nl nm ms b">end bytes</code>范围是文件大小的连续范围。可选地，我们也可以在所有处理任务完成后调用回调(结果)任务。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="4cbf" class="mw lv iq ms b gy mx my l mz na"># core/tasks.py<br/><br/>@celery.task(name='core.tasks.s3_parallel_file_processing', bind=True)<br/>def s3_parallel_file_processing_task(self, **kwargs):<br/>    """ Creates celery tasks to process chunks of file in parallel<br/>    """<br/>    bucket = kwargs.get('bucket')<br/>    key = kwargs.get('key')<br/>    try:<br/>        filename = key<br/>        # 1. Check file headers for validity -&gt; if failed, stop processing<br/>        desired_row_headers = (<br/>            'id',<br/>            'name',<br/>            'age',<br/>            'latitude',<br/>            'longitude',<br/>            'monthly_income',<br/>            'experienced'<br/>        )<br/>        is_headers_valid, header_row_str = validate_scrm_file_headers_via_s3_select(<br/>            bucket=bucket,<br/>            key=key,<br/>            delimiter=S3_FILE_DELIMITER,<br/>            desired_headers=desired_row_headers)<br/>        if not is_headers_valid:<br/>            logger.error(f'{filename} file headers validation failed')<br/>            return False<br/>        logger.info(f'{filename} file headers validation successful')<br/><br/>        # 2. fetch file size via S3 HEAD<br/>        file_size = get_s3_file_size(bucket=bucket, key=key)<br/>        if not file_size:<br/>            logger.error(f'{filename} file size invalid {file_size}')<br/>            return False<br/>        logger.info(f'We are processing {filename} file about {file_size} bytes :-o')<br/><br/>        # 2. Create celery group tasks for chunk of this file size for parallel processing<br/>        start_range = 0<br/>        end_range = min(S3_FILE_PROCESSING_CHUNK_SIZE, file_size)<br/>        tasks = []<br/>        while start_range &lt; file_size:<br/>            tasks.append(<br/>                chunk_file_processor.signature(<br/>                    kwargs={<br/>                        'bucket': bucket,<br/>                        'key': key,<br/>                        'filename': filename,<br/>                        'start_byte_range': start_range,<br/>                        'end_byte_range': end_range,<br/>                        'header_row_str': header_row_str<br/>                    }<br/>                )<br/>            )<br/>            start_range = end_range<br/>            end_range = end_range + min(S3_FILE_PROCESSING_CHUNK_SIZE, file_size - end_range)<br/>        job = (group(tasks) | chunk_file_processor_callback.s(data={'filename': filename}))<br/>        _ = job.apply_async()<br/>    except Exception:<br/>        logger.exception(f'Error processing file: {filename}')<br/><br/><br/>@celery.task(name='core.tasks.chunk_file_processor_callback', bind=True, ignore_result=False)<br/>def chunk_file_processor_callback(self, *args, **kwargs):<br/>    """ Callback task called post chunk_file_processor()<br/>    """<br/>    logger.info('Callback called')</span><span id="8ee5" class="mw lv iq ms b gy ns my l mz na"><br/># core/utils.py<br/><br/>def store_scrm_file_s3_content_in_local_file(bucket: str, key: str, file_path: str, start_range: int, end_range: int,<br/>                                             delimiter: str, header_row: str):<br/>    """Retrieves S3 file content via S3 Select ScanRange and store it in a local file.<br/>       Make sure the header validation is done before calling this.<br/><br/>    Args:<br/>        bucket (str): S3 bucket<br/>        key (str): S3 key<br/>        file_path (str): Local file path to store the contents<br/>        start_range (int): Start range of ScanRange parameter of S3 Select<br/>        end_range (int): End range of ScanRange parameter of S3 Select<br/>        delimiter (str): S3 file delimiter<br/>        header_row (str): Header row of the local file. This will be inserted as first line in local file.<br/>    """<br/>    aws_profile = current_app.config.get('AWS_PROFILE_NAME')<br/>    s3_client = boto3.session.Session(profile_name=aws_profile).client('s3')<br/>    expression = 'SELECT * FROM S3Object'<br/>    try:<br/>        response = s3_client.select_object_content(<br/>            Bucket=bucket,<br/>            Key=key,<br/>            ExpressionType='SQL',<br/>            Expression=expression,<br/>            InputSerialization={<br/>                'CSV': {<br/>                    'FileHeaderInfo': 'USE',<br/>                    'FieldDelimiter': delimiter,<br/>                    'RecordDelimiter': '\n'<br/>                }<br/>            },<br/>            OutputSerialization={<br/>                'CSV': {<br/>                    'FieldDelimiter': delimiter,<br/>                    'RecordDelimiter': '\n',<br/>                },<br/>            },<br/>            ScanRange={<br/>                'Start': start_range,<br/>                'End': end_range<br/>            },<br/>        )<br/><br/>        """<br/>        select_object_content() response is an event stream that can be looped to concatenate the overall result set<br/>        """<br/>        f = open(file_path, 'wb')  # we receive data in bytes and hence opening file in bytes<br/>        f.write(header_row.encode())<br/>        f.write('\n'.encode())<br/>        for event in response['Payload']:<br/>            if records := event.get('Records'):<br/>                f.write(records['Payload'])<br/>        f.close()<br/>    except ClientError:<br/>        logger.exception(f'Client error reading S3 file {bucket} : {key}')<br/>    except Exception:<br/>        logger.exception(f'Error reading S3 file {bucket} : {key}')</span></pre><p id="a6d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！😎现在，我们不是一个字节一个字节地传输 S3 文件，而是通过并发处理数据块来实现并行处理。没那么难，不是吗？😅</p><p id="d06d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">📌您可以<a class="ae ls" href="https://github.com/idris-rampurawala/s3-select-demo" rel="noopener ugc nofollow" target="_blank">查看我的 GitHub 库</a>以获得这种方法的完整工作示例。</p></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="82e0" class="lu lv iq bd lw lx oa lz ma mb ob md me jw oc jx mg jz od ka mi kc oe kd mk ml bi translated">🔍比较处理时间</h1><p id="54f4" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">如果我们用这种方法比较我们在上一篇文章中处理的同一个文件的处理时间，处理速度大约比<strong class="ky ir">快 68%</strong>(使用相同的硬件和配置)。😆</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="3a85" class="mw lv iq ms b gy mx my l mz na">╔═════════════════╦═══════════════════╦════════════════════════════╗<br/>║                 ║ <strong class="ms ir">Streaming S3 File </strong>║ <strong class="ms ir">Parallel Processing S3 File</strong>║<br/>╠═════════════════╬═══════════════════╬════════════════════════════╣<br/>║ <strong class="ms ir">File size       </strong>║ 4.8MB             ║ 4.8MB                      ║<br/>║ <strong class="ms ir">Processing time</strong> ║ ~37 seconds       ║ ~12 seconds                ║<br/>╚═════════════════╩═══════════════════╩════════════════════════════╝</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/0193f332707396b6048acd2a2001764f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sNkoN3dVowDmGdGE.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Streaming S3 File</strong> <strong class="bd kv">Logs </strong>| Image by the Author</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/60aa39cc9e6bad315a14b04265b71fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WevibFiMZYZk1skA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Parallel Processing S3 File Logs | </strong>Image by the Author</figcaption></figure></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="5cf0" class="lu lv iq bd lw lx oa lz ma mb ob md me jw oc jx mg jz od ka mi kc oe kd mk ml bi translated">✔️这种方法的好处</h1><ul class=""><li id="8040" class="nb nc iq ky b kz mm lc mn lf og lj oh ln oi lr ng nh ni nj bi translated">包含数百万条记录的非常大的文件可以在几分钟内得到处理。我在生产环境中使用这种方法已经有一段时间了，它非常令人愉快</li><li id="64bb" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">计算和处理分布在分布的工作人员中</li><li id="f67c" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">工作池的可用性可以调整处理速度</li><li id="0ae9" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated">不再有内存问题</li></ul></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="dd8c" class="lu lv iq bd lw lx oa lz ma mb ob md me jw oc jx mg jz od ka mi kc oe kd mk ml bi translated">📑资源</h1><ul class=""><li id="e388" class="nb nc iq ky b kz mm lc mn lf og lj oh ln oi lr ng nh ni nj bi translated"><a class="ae ls" href="https://github.com/idris-rampurawala/s3-select-demo" rel="noopener ugc nofollow" target="_blank">我的 GitHub 库展示了上述方法</a></li><li id="b67e" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated"><a class="ae ls" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.select_object_content" rel="noopener ugc nofollow" target="_blank"> AWS S3 选择 boto3 参考值</a></li><li id="79c7" class="nb nc iq ky b kz nn lc no lf np lj nq ln nr lr ng nh ni nj bi translated"><a class="ae ls" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html" rel="noopener ugc nofollow" target="_blank"> AWS S3 选择用户指南</a></li></ul></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><p id="05e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt">原发布于 2019 年 1 月 22 日</em><a class="ae ls" href="https://dev.to/idrisrampurawala/parallelize-processing-a-large-aws-s3-file-8eh" rel="noopener ugc nofollow" target="_blank"><em class="lt">https://dev . to</em></a><em class="lt">。</em></p></div></div>    
</body>
</html>