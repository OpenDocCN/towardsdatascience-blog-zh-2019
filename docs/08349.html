<html>
<head>
<title>Glimpse into Apache Spark 3.0 [Early Access]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark 3.0 一瞥[早期访问]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/glimpse-into-spark-3-0-early-access-c1854327d6c?source=collection_archive---------14-----------------------#2019-11-13">https://towardsdatascience.com/glimpse-into-spark-3-0-early-access-c1854327d6c?source=collection_archive---------14-----------------------#2019-11-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/fe71eee7ca577b6cc74fa7ec1f5a6955.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*086vlztqY36cSugGTRARcw.png"/></div></div></figure><p id="aae0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://www.tusharck.com/" rel="noopener ugc nofollow" target="_blank">图沙尔·卡普尔</a>:(【https://www.tusharck.com/】T2</p><p id="3e70" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://www.apache.org" rel="noopener ugc nofollow" target="_blank"> Apache </a>继续保持强势地位，展示了其针对大数据科学的<a class="ae kz" href="https://spark.apache.org/news/spark-3.0.0-preview.html" rel="noopener ugc nofollow" target="_blank"> Spark 3.0 </a>预览版。根据预告，<a class="ae kz" href="https://spark.apache.org" rel="noopener ugc nofollow" target="_blank"> Spark </a>即将推出几大重要功能。</p><p id="00a8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以从这个链接下载预览版:<a class="ae kz" href="https://archive.apache.org/dist/spark/spark-3.0.0-preview/" rel="noopener ugc nofollow" target="_blank">https://archive.apache.org/dist/spark/spark-3.0.0-preview/</a></p><p id="ff01" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们看看它的一些主要功能，这些功能为其大数据统一分析的目标注入了活力。</p><h1 id="e7e5" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">火花图:</strong>密码脚本&amp;属性图</h1><p id="e02c" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">Spark 3.0 中加入了流行的图查询语言 Cypher，它通过属性图模型 a directed multigraph 进行耦合。图形查询将遵循与 SparkSQL 相似的原则，它自己的催化剂为数据帧提供完全支持。</p><p id="f0d8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://databricks.com/session_eu19/graph-features-in-spark-3-0-integrating-graph-querying-and-algorithms-in-spark-graph" rel="noopener ugc nofollow" target="_blank">点击查看 Databricks 在 Spark 3.0 图形 API 上的会话</a>。</p><h1 id="3cb5" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">Python 3，Scala 2.12 和 JDK 11</h1><ul class=""><li id="2c16" class="md me it kd b ke ly ki lz km mf kq mg ku mh ky mi mj mk ml bi translated">Spark 3.0 有望完全迁移到 Python3。</li><li id="485c" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">Scala 版本升级到 2.12。</li><li id="3d50" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">它将全力支持 JDK 11 号。</li></ul><h1 id="8aca" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">深度学习:增加 GPU 支持</h1><p id="01b9" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">这是每个数据工程师和科学家都在寻找的东西，而 Spark 3.0 符合他们的期望。带<a class="ae kz" href="https://www.nvidia.com/en-us/" rel="noopener ugc nofollow" target="_blank"> NVIDIA </a>的 Spark 3.0 提供 GPU 加速，可以跨多个 GPU 运行。它支持 AMD、Intel 和 Nvidia 等异构 GPU。对于 Kubernetes，它提供了执行程序 pod 级别的 GPU 隔离。除此之外，我们还得到了:</p><ul class=""><li id="610f" class="md me it kd b ke kf ki kj km mr kq ms ku mt ky mi mj mk ml bi translated">熊猫 UDF 的 GPU 加速。</li><li id="569f" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">您可以指定 RDD 操作中的 GPU 数量。</li><li id="ed27" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">为了轻松指定深度学习环境，有 YARN 和 Docker 支持推出带 GPU 的 Spark 3.0。</li></ul><h2 id="13ec" class="mu lb it bd lc mv mw dn lg mx my dp lk km mz na lo kq nb nc ls ku nd ne lw nf bi translated">日志丢失:支持</h2><p id="0ae1" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">您还可以设置您的指标的物流损耗。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="714b" class="mu lb it nl b gy np nq l nr ns">val evaluator = new MulticlassClassificationEvaluator() .setMetricName("<strong class="nl iu">logLoss</strong>")</span></pre><h1 id="8259" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">二进制文件</h1><p id="fe5e" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">您可以使用二进制文件作为 spark 数据帧的数据源，但是，现在不允许二进制的写操作。我们可以期待在未来的版本中。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="7cff" class="mu lb it nl b gy np nq l nr ns">val df = spark.read.format(<strong class="nl iu">BINARY_FILE</strong>).load(dir.getPath)</span></pre><h1 id="b2bb" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">库伯内特斯</h1><p id="2dbc" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">现在，在最新 Kubernetes 版本的支持下，您将能够通过 Kubernetes 托管集群。它在运行时提供 spark-submit 变化的 web-hooks 配置修改窗格。它还改进了 Kubernetes 的动态分配。此外，我们得到:</p><ul class=""><li id="9265" class="md me it kd b ke kf ki kj km mr kq ms ku mt ky mi mj mk ml bi translated">支持 GPU 调度。</li><li id="1382" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">Kubernetes 后端的 spark.authenticate secret 支持。</li><li id="88ae" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">Kubernetes 资源管理器现在支持 Kerberos 身份验证协议。</li></ul><h1 id="617f" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">考拉:熊猫的星火尺度</h1><p id="5f3b" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated"><a class="ae kz" href="https://github.com/databricks/koalas" rel="noopener ugc nofollow" target="_blank">考拉</a>是 Apache Spark 上的熊猫 API，它使数据工程师和科学家在与大数据交互时更有效率。随着 3.0 功能发布，考拉现在可以扩展到分布式环境，而不需要单独读取 Spark 3.0 数据帧，这与以前的单节点环境不同。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="b1e4" class="mu lb it nl b gy np nq l nr ns">import databricks.koalas as ks<br/>import pandas as pd<br/><br/>pdf = pd.DataFrame({'x':range(3), 'y':['a','b','b'], 'z':['a','b','b']})<br/><br/># Create a Koalas DataFrame from pandas DataFrame<br/>df = ks.from_pandas(pdf)<br/><br/># Rename the columns<br/>df.columns = ['x', 'y', 'z1']<br/><br/># Do some operations in place:<br/>df['x2'] = df.x * df.x</span></pre><p id="6ed1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以上例子摘自<a class="ae kz" href="https://github.com/databricks/koalas" rel="noopener ugc nofollow" target="_blank">考拉吉特回购</a>。</p><h1 id="973e" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">卡夫卡流:包括磁头</h1><p id="07b8" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">现在你可以在<a class="ae kz" href="https://kafka.apache.org" rel="noopener ugc nofollow" target="_blank">Kafka</a>streaming(<a class="ae kz" href="https://github.com/apache/spark/blob/master/docs/structured-streaming-kafka-integration.md" rel="noopener ugc nofollow" target="_blank">git</a>)中读取标题了。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="f989" class="mu lb it nl b gy np nq l nr ns">val df = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", "host1:port1,host2:port2") .option("subscribe", "topic1")<br/><strong class="nl iu">.option("includeHeaders", "true")</strong><br/>.load()</span><span id="594e" class="mu lb it nl b gy nt nq l nr ns">df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)", <strong class="nl iu">"headers"</strong>) .as[(String, String, Map)]</span></pre><h1 id="84e3" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">纱线特征</h1><p id="45d9" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">纱线还获得了一组新特征，主要是:</p><ul class=""><li id="c469" class="md me it kd b ke kf ki kj km mr kq ms ku mt ky mi mj mk ml bi translated">Spark 3.0 框架现在可以自动发现集群或系统上的 GPU。</li><li id="a788" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">可以调度 GPU。</li></ul><p id="6993" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以在这里查看 YARN 的 GPU 配置:<a class="ae kz" href="https://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/UsingGpus.html" rel="noopener ugc nofollow" target="_blank">https://Hadoop . Apache . org/docs/r 3 . 1 . 0/Hadoop-YARN/Hadoop-YARN-site/using GPU . html</a></p><h1 id="9cbf" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">分析缓存数据</h1><p id="26bc" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">现在您可以分析 Spark 3.0 中的缓存数据，这是 Spark ( <a class="ae kz" href="https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala" rel="noopener ugc nofollow" target="_blank"> git </a>)最想要的特性之一。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="10d2" class="mu lb it nl b gy np nq l nr ns">withTempView("cachedQuery") {<br/>  sql("""CACHE TABLE cachedQuery AS<br/>          |  SELECT c0, avg(c1) AS v1, avg(c2) AS v2<br/>          |  FROM (SELECT id % 3 AS c0, id % 5 AS c1, 2 AS c2 FROM range(1, 30))<br/>          |  GROUP BY c0<br/>       """.stripMargin)<br/><strong class="nl iu">// Analyzes one column in the cached logical plan</strong><br/>sql("<strong class="nl iu">ANALYZE</strong> TABLE cachedQuery COMPUTE STATISTICS FOR COLUMNS v1")      </span><span id="4792" class="mu lb it nl b gy nt nq l nr ns"><strong class="nl iu">// Analyzes two more columns</strong><br/>sql("<strong class="nl iu">ANALYZE</strong> TABLE cachedQuery COMPUTE STATISTICS FOR COLUMNS c0, v2")</span></pre><h1 id="3d7f" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">动态分区剪枝</h1><p id="9a1c" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">它通过在散列连接中重用维度表广播结果，在运行时提供优化的执行。这有助于 Spark 3.0 更有效地处理基于星型模式的查询，从而消除了 ETL 非规范化表的需要。</p><p id="7b0f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://databricks.com/session_eu19/dynamic-partition-pruning-in-apache-spark" rel="noopener ugc nofollow" target="_blank">点击查看 Databricks 关于 Apache Spark 中动态分区修剪的会话</a>。</p><h1 id="2fca" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">三角洲湖</h1><p id="5099" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated"><a class="ae kz" href="https://delta.io/" rel="noopener ugc nofollow" target="_blank"> Delta Lake </a>是一个开源存储层，为 Apache Spark 3.0 带来了 ACID 事务，并且由于其易于实施和升级到任何现有 Spark 应用程序，它为数据湖带来了可靠性。</p><p id="9066" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">参见三角洲湖泊快速启动:【https://docs.databricks.com/delta/quick-start.html T2】</p><h1 id="4fd7" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">更多功能:</h1><ul class=""><li id="9eba" class="md me it kd b ke ly ki lz km mf kq mg ku mh ky mi mj mk ml bi translated">SparkML 中的决策树。</li><li id="409e" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">改进了查询执行期间的优化器。</li><li id="4b78" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">可插拔目录集成。</li><li id="b0dd" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">执行器内存的度量。</li><li id="8ee0" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">动态分配。</li></ul><p id="ba3b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">上面提到的功能并不是 Spark 3.0 的唯一功能。毫无疑问，Spark 3.0 正在帮助数据科学家利用附加功能做更多事情，等待 Spark 3.0 的最终发布。</p></div></div>    
</body>
</html>