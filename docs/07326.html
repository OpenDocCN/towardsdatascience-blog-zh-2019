<html>
<head>
<title>What is Teacher Forcing?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是老师逼？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c?source=collection_archive---------5-----------------------#2019-10-15">https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c?source=collection_archive---------5-----------------------#2019-10-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a119" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">训练<em class="ki">递归神经网络</em>的常用技术</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/e6737a0b9d74d474dd4f91669e7eac46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8CSDr3i8V6nR6T4-"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Photo by <a class="ae kz" href="https://unsplash.com/@jerry_318?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jerry Wang</a> on <a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6027" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">大量的<em class="lw">递归神经网络</em>在<em class="lw">自然语言处理</em>(例如在图像字幕、机器翻译中)在训练过程中使用<em class="lw">教师强制</em>。尽管<em class="lw">老师逼</em>盛行，但大部分文章只是简单描述了它是如何运作的。比如 TensorFlow 关于<a class="ae kz" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention#training" rel="noopener ugc nofollow" target="_blank">神经机器翻译的教程关注</a>只说“<em class="lw">老师强制</em>是将<em class="lw">目标词</em>作为<em class="lw">下一个输入</em>传递给解码器的技术。”在这篇文章中，我们将回顾一下<em class="lw">老师强迫</em>的细节，并回答一些常见的问题。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="809f" class="me mf it bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">教师强制是如何起作用的？</h2><p id="7353" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">你是否遇到过由多个部分组成的数学考试题，其中(b)部分的计算需要(a)部分的答案，而(c)部分需要(b)部分的答案，等等？我总是格外关注这些问题，因为如果我们(a)部分出错，那么所有后续部分也很可能出错，即使公式和计算是正确的。老师强迫补救如下:在我们获得(a)部分的答案后，老师会将我们的答案与正确答案进行比较，记录(a)部分的分数，并告诉我们正确答案，以便我们可以用它来做(b)部分。</p><p id="6d1b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">输出序列的<em class="lw">递归神经网络</em>的情况非常相似。假设我们想要训练一个图像标题模型，上面图像的真实标题是“两个人在看书”。我们的模型在预测第二个单词时出错，我们在第一个和第二个预测中分别有“Two”和“birds”。</p><ul class=""><li id="d5fc" class="nc nd it lc b ld le lg lh lj ne ln nf lr ng lv nh ni nj nk bi translated">如果没有老师的强迫，我们会把“鸟”送回我们的 RNN 去预测第三个单词。假设第三个预测是“飞行”。尽管我们的模型预测“飞行”是有意义的，因为输入是“鸟”，但这与地面事实不同。</li><li id="5660" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nh ni nj nk bi translated">另一方面，如果我们使用<em class="lw">教师强制</em>，我们将在计算并记录第二次预测的损失后，为第三次预测将“人”喂给我们的 RNN。</li></ul><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/59fc92d935a9e33dcc3d1bfd75bac254.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*U3d8D_GnfW13Y3nDgvwJSw.png"/></div></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="c99a" class="me mf it bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">教师强制的利弊</h2><h2 id="6789" class="me mf it bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">优点:</h2><p id="8607" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">用<em class="lw">老师逼</em>训练收敛更快。在训练的早期阶段，模型的预测非常糟糕。如果我们不使用<em class="lw">老师强制</em>，模型的隐藏状态会被一系列错误预测更新，误差会累积，模型很难从中学习。</p><h2 id="223a" class="me mf it bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">缺点:</h2><p id="01ea" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">在推断过程中，由于通常没有实际情况可用，RNN 模型需要将自己之前的预测反馈给自己，以便进行下一次预测。因此，训练和推理之间存在差异，这可能导致模型性能差和不稳定。这在文献中被称为<em class="lw">曝光偏差</em>。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="8a9d" class="me mf it bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">实现示例</h2><ul class=""><li id="eb3f" class="nc nd it lc b ld mx lg my lj nr ln ns lr nt lv nh ni nj nk bi translated">TensorFlow:注意看<a class="ae kz" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention#training" rel="noopener ugc nofollow" target="_blank">神经机器翻译的“训练”环节</a></li><li id="23b9" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nh ni nj nk bi translated">PyTorch:参见<a class="ae kz" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#training-the-model" rel="noopener ugc nofollow" target="_blank"> NLP 的“训练模型”环节从无到有:从序列到序列网络的翻译和注意力</a></li></ul></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="466c" class="me mf it bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">常见问题</h2><p id="ea8e" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">问:既然我们通过 RNN 模型传递整个基本事实序列，那么该模型有可能通过简单地记忆基本事实来“作弊”吗？</p><p id="c747" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">答:没有。在时间步<em class="lw"> t </em>时，模型的输入是时间步<em class="lw"> t - 1 </em>的地面真值，模型的隐藏状态已经被时间步<em class="lw"> 1 </em>到<em class="lw"> t - 2 </em>的地面真值更新。模型永远无法窥视未来。</p><p id="626a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">问:有必要在每个时间步更新损失吗？</p><p id="130b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">答:不。另一种方法是将所有时间步长的预测存储在一个 Python 列表中，然后一次性计算所有损失。</p><p id="7c24" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">问:<em class="lw">老师强制</em>是用在<em class="lw">自然语言处理</em>之外吗？</p><p id="4052" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">答:是的。它可以用于任何输出序列的模型，例如时间序列预测。</p><p id="7189" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">问:<em class="lw">老师强制</em>是在<em class="lw">递归神经网络</em>之外使用的吗？</p><p id="fcc1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">答:是的。它用于其他自回归模型，如 Transformer。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="0afb" class="me mf it bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">进一步阅读</h2><ol class=""><li id="0dfa" class="nc nd it lc b ld mx lg my lj nr ln ns lr nt lv nu ni nj nk bi translated">已经发明了许多算法来减轻<em class="lw">曝光偏差</em>，例如<em class="lw">预定采样</em> [1】和<em class="lw">并行预定采样</em> [3】，<em class="lw">教授强制</em> [5】，以及<em class="lw">光束搜索</em> [2]，[6]。</li><li id="36e2" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nu ni nj nk bi translated">[4]的结果表明<em class="lw">暴露偏差</em>可能不像假设的那样显著。</li></ol></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="3eaf" class="me mf it bd mg mh mi dn mj mk ml dp mm lj mn mo mp ln mq mr ms lr mt mu mv mw bi translated">参考</h2><ol class=""><li id="6694" class="nc nd it lc b ld mx lg my lj nr ln ns lr nt lv nu ni nj nk bi translated">南本吉奥、o .维尼亚尔斯、n .贾伊特利和 n .沙泽尔。<a class="ae kz" href="http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank">递归神经网络序列预测的预定采样</a> (2015)，NeurIPS <em class="lw"> </em> 2015。</li><li id="dfc5" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nu ni nj nk bi translated">R.科洛伯特、a .汉南和 g .辛纳伊夫。<a class="ae kz" href="http://proceedings.mlr.press/v97/collobert19a/collobert19a.pdf" rel="noopener ugc nofollow" target="_blank">一种全微分波束搜索解码器</a> (2019)，ICML 2019。</li><li id="78f0" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nu ni nj nk bi translated">D.杜克沃斯，a .尼拉坎坦，b .古德里奇，l .凯泽和 s .本吉奥。<a class="ae kz" href="https://arxiv.org/abs/1906.04331" rel="noopener ugc nofollow" target="_blank">平行预定抽样</a> (2019)，arXiv。</li><li id="4e79" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nu ni nj nk bi translated">T.何，张军，周，格拉斯。<a class="ae kz" href="https://arxiv.org/abs/1905.10617" rel="noopener ugc nofollow" target="_blank">量化神经语言生成的暴露偏差</a> (2019)，arXiv。</li><li id="03d7" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nu ni nj nk bi translated">A.Lamb、A. Goyal、Y. Zhang、S. Zhang、和 Y. Bengio。<a class="ae kz" href="https://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks" rel="noopener ugc nofollow" target="_blank">强迫教授:训练递归网络的新算法</a> (2016)，NeurIPS <em class="lw"> </em> 2016。</li><li id="a113" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nu ni nj nk bi translated">南怀斯曼和拉什。<a class="ae kz" href="https://www.aclweb.org/anthology/D16-1137/" rel="noopener ugc nofollow" target="_blank">作为波束搜索优化的序列间学习</a> (2016)，EMNLP 2016。</li></ol></div></div>    
</body>
</html>