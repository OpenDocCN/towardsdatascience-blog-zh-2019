<html>
<head>
<title>How is the Quiet Revolution in Semi-Supervised Learning Changing the Industry?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">半监督学习中静悄悄的革命是如何改变行业的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-is-the-quiet-revolution-in-semi-supervised-learning-changing-the-industry-4a25f211ce1f?source=collection_archive---------18-----------------------#2019-07-13">https://towardsdatascience.com/how-is-the-quiet-revolution-in-semi-supervised-learning-changing-the-industry-4a25f211ce1f?source=collection_archive---------18-----------------------#2019-07-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/9a6014d6bde153e9b7c998c44bb4baab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCmNT_a0Tjc2qFvdXuYMNw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@penguinuhh" rel="noopener ugc nofollow" target="_blank">@penguinuhh</a></figcaption></figure><div class=""/><div class=""><h2 id="d537" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">混合匹配、无监督数据扩充和 PATE 方法</h2></div><p id="cecb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">作为一名辅修计算机科学的人类学学生，我想尽我最大的努力去理解这一发展，以及当它实现时会有什么样的后果。然而，首先，我们必须运行的变化和技术，使一个可行的半监督学习方法的实际方面。然后，我将跳转到一些技术，这些技术结合起来，可能会改变我们在机器学习中处理这一领域的方式。</p><p id="5dd2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我三天看三个问题的最后一天。</p><p id="98ea" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/google-ai-and-developments-in-semi-supervised-learning-5b1a4ad29d67?source=friends_link&amp;sk=ddc7b1d106980109b8699b187ee39191">第一天:谷歌如何成为人工智能领域的领跑者？</a>(完成)</p><p id="6593" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/advancements-in-semi-supervised-learning-with-unsupervised-data-augmentation-fc1fc0be3182?source=friends_link&amp;sk=cbac033d5f98b5ff54daf4547a3d0ace">第二天:无监督数据增强(UDA)的半监督学习(SSL)取得了哪些进展，为什么它对人工智能领域很重要？</a>(完成)</p><p id="0b64" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">第三天:SSL 的悄然革命如何改变行业？</strong></p><p id="d5e4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">今天是最后一天，我将写下这场静悄悄的半监督革命。我将从这个术语是如何产生的开始；以前的惯例；以及 SSL 格局是如何变化的。之后我将很快结束发言。T11】</p><h2 id="0eea" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">谁创造了安静的半监督革命？</h2><p id="6d30" class="pw-post-body-paragraph kv kw jg kx b ky ml kh la lb mm kk ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">5 月 15 日，谷歌首席科学家 Vincent Vanhoucke 发表了一篇名为半监督革命的文章。据我所知，这是第一次提到以这种方式使用 SSL 的变化。</p><p id="f261" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">他首先谈到了半监督学习(SSL)之前的问题。由于要访问大量数据、有限的监督数据和大量未标记的数据，SSL 似乎是一个显而易见的解决方案。他在图表上展示了他的观点，这些图表通常来自监督和半监督实验。</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="ab gu cl mw"><img src="../Images/935d9ea58f3ef83b24448e14f2f5a116.png" data-original-src="https://miro.medium.com/v2/0*bij0UqwjAgLXNJRG"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Illustrative graph by <a class="mq mr ep" href="https://medium.com/u/2879ca55026a?source=post_page-----4a25f211ce1f--------------------------------" rel="noopener" target="_blank">Vincent Vanhoucke</a></figcaption></figure><p id="1e40" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">根据 Vanhoucke 的说法，一名机器学习工程师经历了一段旅程，最终回到了图中所示的监督学习。</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="ab gu cl mw"><img src="../Images/e2bbfc54aa46aa05cab9f6dfef0588eb.png" data-original-src="https://miro.medium.com/v2/0*kE-Y3bRoik6zQ984"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Illustrative graph by <a class="mq mr ep" href="https://medium.com/u/2879ca55026a?source=post_page-----4a25f211ce1f--------------------------------" rel="noopener" target="_blank">Vincent Vanhoucke</a></figcaption></figure><p id="3d50" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是他接着说:</p><blockquote class="mx"><p id="f0b5" class="my mz jg bd na nb nc nd ne nf ng lq dk translated">一个有趣的趋势是，半监督学习的前景可能会变得更像这样:</p></blockquote><figure class="nh ni nj nk nl is gh gi paragraph-image"><div class="ab gu cl mw"><img src="../Images/316032a233e613946ecddc2471acc720.png" data-original-src="https://miro.medium.com/v2/0*Jc3Z7DWU_eYlhy81"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Illustrative graph by <a class="mq mr ep" href="https://medium.com/u/2879ca55026a?source=post_page-----4a25f211ce1f--------------------------------" rel="noopener" target="_blank">Vincent Vanhoucke</a></figcaption></figure><h2 id="94f4" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">以前的惯例是什么？</h2><p id="ef62" class="pw-post-body-paragraph kv kw jg kx b ky ml kh la lb mm kk ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">对于工程师来说，SSL 被描述为一个兔子洞，几乎是一种成年仪式，最终只能回到数据标记上来。根据 Vanhoucke 的说法，以前的惯例是:</p><blockquote class="nm nn no"><p id="8ee8" class="kv kw lr kx b ky kz kh la lb lc kk ld np lf lg lh nq lj lk ll nr ln lo lp lq ij bi translated">…首先学习未标记数据的自动编码器，然后对标记数据进行微调。几乎没有人再这样做了，因为通过自动编码学习到的表示往往会在经验上限制微调的渐近性能。</p></blockquote><p id="a836" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么什么是自动编码器呢？我们来分析一下。</p><p id="3118" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh"> Autoencoder </strong>是一种人工神经网络，用于以无监督的方式学习高效的数据编码。自动编码器的目的是通过训练网络忽略信号“噪声”来学习一组数据的表示(编码)，通常用于<em class="lr">降维</em>。随着缩减侧，学习重构侧，其中自动编码器试图从缩减的编码中生成尽可能接近其原始输入的表示，因此得名。</p><ul class=""><li id="6b4c" class="ns nt jg kx b ky kz lb lc le nu li nv lm nw lq nx ny nz oa bi translated"><strong class="kx jh">输入空间的维度</strong>。高维空间(100 或 1000)。空间的体积增加太多，数据变得稀疏。例如，计算优化问题中的每个值的组合。如果你想要一个神秘的倾斜，这一点可以被称为<a class="ae jd" href="https://en.m.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">维度诅咒</a>。</li></ul><p id="f07a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">渐近线</strong>在数学中用来指在无穷远处与曲线相切的直线。<strong class="kx jh"> <em class="lr">计算复杂性中的渐近符号</em> </strong>是指定义域和值域为 Z+的函数的极限行为，对于定义域大于特定阈值的值有效。因此，这里我们用曲线来近似曲线。通常，优选地，我们寻找紧密跟踪原始曲线的曲线。</p><ul class=""><li id="8b04" class="ns nt jg kx b ky kz lb lc le nu li nv lm nw lq nx ny nz oa bi translated"><strong class="kx jh">渐近性能</strong>是一种比较算法性能的方法。你可以抽象出底层的细节(例如精确的汇编代码)；调查缩放行为(对于非常大的输入，哪种方式更好？).渐近性能:随着输入大小的增长，执行时间如何增长？</li></ul><p id="d7fb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Vanhoucke 声称:</p><blockquote class="nm nn no"><p id="7cf5" class="kv kw lr kx b ky kz kh la lb lc kk ld np lf lg lh nq lj lk ll nr ln lo lp lq ij bi translated">…即使是大幅改进的现代生成方法也没有改善这种情况，可能是因为好的生成模型不一定就是好的分类器。因此，今天当你看到工程师微调模型时，通常是从在监督数据上学习的表示开始的…</p></blockquote><p id="3a5c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">什么是生成方法？</p><p id="02a2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">生成学习</strong>是一种理论，涉及到新思想与学习者现有图式的积极整合。<strong class="kx jh">生成性学习</strong>的主要思想是，为了理解地学习，学习者必须主动构建意义。生成模型仅适用于概率方法。在统计分类中，包括机器学习，两种主要的方法被称为<strong class="kx jh">生成</strong>方法和鉴别方法。下面显示了生成分类器(联合分布):</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/90b395b8d63682f76634d3f2b4a37b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8OA8Gz4HGMaDc1y8.png"/></div></div></figure><h2 id="1378" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">是什么改变了 SSL 的前景？</h2><p id="3645" class="pw-post-body-paragraph kv kw jg kx b ky ml kh la lb mm kk ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">昨天我写了一篇文章，名为谷歌人工智能和半监督学习的发展。文章首先经过了讲解<em class="lr">无监督学习、监督学习和强化学习</em>。然后，它继续解释半监督学习(SSL)以及如何利用无监督数据增强(UDA)对 SSL 进行研究。因此，如果你不熟悉这些术语，最好跳回那篇文章。</p><p id="a9e6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">无论如何，有一些进步提到了向 SSL 的可用性增加的转变。你可能想看看这三个流行的:</p><ul class=""><li id="a3b5" class="ns nt jg kx b ky kz lb lc le nu li nv lm nw lq nx ny nz oa bi translated"><a class="ae jd" href="https://arxiv.org/abs/1905.02249" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh">提高精度的组合方法</strong>。MixMatch:半监督学习的整体方法</a></li><li id="f776" class="ns nt jg kx b ky oc lb od le oe li of lm og lq nx ny nz oa bi translated"><a class="ae jd" href="https://arxiv.org/abs/1904.12848" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh">利用无监督数据扩充更好地处理未标记数据</strong></a></li><li id="a73d" class="ns nt jg kx b ky oc lb od le oe li of lm og lq nx ny nz oa bi translated"><strong class="kx jh">维护隐私</strong>。PATE 方法(<a class="ae jd" href="https://arxiv.org/abs/1610.05755" rel="noopener ugc nofollow" target="_blank">从私人训练数据进行深度学习的半监督知识转移</a>，<a class="ae jd" href="https://arxiv.org/abs/1802.08908" rel="noopener ugc nofollow" target="_blank">利用 PATE 进行可扩展的私人学习</a></li></ul><p id="9882" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有一些新的聪明的方法来给数据贴上自我标签，并表达损失，这些方法与自我标签的噪音和潜在偏见更加兼容。与前两点相匹配的两个最近的工作例证了最近的进展并指向相关文献:<a class="ae jd" href="https://arxiv.org/abs/1905.02249" rel="noopener ugc nofollow" target="_blank"> MixMatch:半监督学习的整体方法</a>和<a class="ae jd" href="https://arxiv.org/abs/1904.12848" rel="noopener ugc nofollow" target="_blank">非监督数据</a>增强。</p><p id="ca2a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在 MixMatch 论文中，他们介绍了<em class="lr"> MixMatch </em>，这是一种 SSL 算法，它提出了一种单一损失，统一了半监督学习的主流方法。与以前的方法不同，MixMatch 同时针对所有属性，我们发现它有以下好处:</p><ul class=""><li id="412d" class="ns nt jg kx b ky kz lb lc le nu li nv lm nw lq nx ny nz oa bi translated">在一项实验中，他们表明 MixMatch 在所有标准<br/>图像基准上获得了最先进的结果(第 4.2 节)，例如在具有 250 个标签的 CIFAR-10 <br/>上获得了 11.08%的错误率(相比之下，次优方法获得了 38%)。</li><li id="7f60" class="ns nt jg kx b ky oc lb od le oe li of lm og lq nx ny nz oa bi translated">他们在消融研究中表明，MixMatch 大于其各部分的总和。</li><li id="863a" class="ns nt jg kx b ky oc lb od le oe li of lm og lq nx ny nz oa bi translated">他们证明了 MixMatch 对于不同的私人学习是有用的，使 PATE 框架中的学生能够获得新的最先进的结果，<br/>同时加强了提供的隐私保证和实现的准确性。</li></ul><p id="4c4f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">一致性正则化</strong>通过利用分类器即使在被增强后也应该为未标记的示例输出相同的类分布的思想，将数据增强应用于半监督学习。MixMatch 通过对图像使用标准数据扩充(随机水平翻转和裁剪)来利用一致性正则化的形式。<br/> <br/> <strong class="kx jh"> MixMatch </strong>是一种“整体”方法，它结合了主流 SSL 范例的思想和组件。</p><p id="3367" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">MixMatch 是由 Google Brain 团队的成员作为半监督学习方法引入的，它结合了当前半监督学习的主流范式的思想和组件。</p><p id="0e0e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过对半监督和隐私保护学习的大量实验，我们发现在他们研究的所有设置中，MixMatch 与其他方法相比表现出显著改善的性能，错误率通常降低两倍或更多。</p><p id="1bd5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在未来的工作中，他们有兴趣将来自半监督学习文献的额外想法融入混合方法，并继续探索哪些组件会产生有效的算法。</p><p id="9330" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另外，大多数关于半监督学习算法的现代工作都是在图像基准上进行评估的；他们对探索 MixMatch 在其他领域的有效性很感兴趣。</p><p id="ecf6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">使用 UDA 的 SSL。</strong>由于获得无标签数据比获得有标签数据容易得多，所以在实际操作中，我们经常会遇到<br/>的情况，即无标签数据的数量与有标签数据的数量差距很大。</p><p id="1723" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了使 UDA 能够利用尽可能多的未标记数据，他们通常需要一个足够大的<br/>模型，但是一个大模型很容易使有限大小的监督数据过拟合。</p><p id="75ab" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了解决这个困难，他们引入了一种新的训练技术，叫做训练信号退火。TSA 背后的主要直觉是，随着模型在越来越多的未标记示例上进行训练，逐渐释放已标记示例的训练信号，而不会过度拟合它们。</p><p id="e67b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">锐化预测</strong>。我们观察到，在问题很难并且标记的<br/> <br/>例子的数量非常少的情况下，在未标记的例子和增强的未标记的例子<br/> <br/>上的预测分布在类别间趋于过度平坦。</p><p id="df93" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">基于置信度的屏蔽</strong>。屏蔽掉模型不确定的例子。</p><p id="5214" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1802.08908.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh"> <em class="lr">【可扩展私学】同 PATE </em> </strong> </a> <em class="lr">。我将摘录 2018 年 2 月 24 日发布的论文摘要:</em></p><blockquote class="nm nn no"><p id="2c15" class="kv kw lr kx b ky kz kh la lb lc kk ld np lf lg lh nq lj lk ll nr ln lo lp lq ij bi translated">机器学习的快速采用增加了对基于敏感数据(如医疗记录或其他个人信息)训练的机器学习模型的隐私影响的担忧。为了解决这些问题，一种有前途的方法是教师集合的私人聚合，或 PATE，它将“教师”模型集合的知识转移到“学生”模型，通过在不相交的数据上训练教师来提供直观的隐私，并通过教师答案的嘈杂聚合来保证强隐私。</p></blockquote><h2 id="c6f2" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">结论</h2><p id="2ffd" class="pw-post-body-paragraph kv kw jg kx b ky ml kh la lb mm kk ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">使用 UDA 的 SSL 非常类似于根据您刚刚看到的内容重新创建，以便在计算的意义上理解视觉印象。MixMatch 结合了许多方法来使 SSL 更好地工作。PATE 是维护隐私所必需的。当学习必须在需要知道的基础上进行时，SSL 还可以提议保护隐私，这些数据可能是您事先不知道(或不允许知道)的。因此，在这种情况下，提高准确性非常重要，并且可能会使行业变得更好。</p><p id="ef7f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是#500daysofAI 的第 41 天。</p><p id="41c3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">希望你喜欢这篇文章，如果有机会记得给我反馈。正如我在引言中提到的，我尽我所能去理解，我写作是为了学习。</p><p id="556d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">祝你一切顺利。</p><blockquote class="mx"><p id="1ce4" class="my mz jg bd na nb nc nd ne nf ng lq dk translated"><em class="oh">什么是#500daysofAI？<br/>我在挑战自己，用#500daysofAI 来写下并思考未来 500 天的人工智能话题。一起学习是最大的快乐，所以如果你觉得一篇文章引起了共鸣，请给我反馈。</em></p></blockquote></div></div>    
</body>
</html>