<html>
<head>
<title>What is Two-Stream Self-Attention in XLNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XLNet 中的双流自我关注是什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-two-stream-self-attention-in-xlnet-ebfe013a0cf3?source=collection_archive---------6-----------------------#2019-07-29">https://towardsdatascience.com/what-is-two-stream-self-attention-in-xlnet-ebfe013a0cf3?source=collection_archive---------6-----------------------#2019-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8ada" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">直观理解 XLNet 中的双流自我关注</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9819c18a1796ea6eb4624b75199207df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I3lIfJ7LFzRpfk0hdAbsww.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@the_bracketeer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Hendrik Cornelissen</a> on <a class="ae ky" href="https://unsplash.com/search/photos/stream?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0cc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我之前的帖子<a class="ae ky" rel="noopener" target="_blank" href="/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335">什么是 XLNet，为什么它优于 BERT </a>中，我主要讲了<strong class="lb iu">XLNet(AR 语言模型)和 BERT (AE 语言模型)的区别</strong>和<strong class="lb iu">置换语言建模。</strong></p><p id="f6b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我相信对 XLNet 有一个直观的理解远比实现细节重要，所以我只解释了<strong class="lb iu">置换语言建模</strong>而没有提到另一个重要的部分<strong class="lb iu">双流自关注</strong>架构。但是正如陈家明在<a class="ae ky" href="https://link.medium.com/OXsOM5ZlcY" rel="noopener">评论</a>、<strong class="lb iu">中提到的，双流自我关注是 XLNet 论文中的另一个亮点，</strong>所以我写了这篇文章来尽可能清晰地解释双流自我关注。</p><p id="b0f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">内容结构如下。</p><ul class=""><li id="bf2c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">置换语言建模快速回顾</li><li id="4d11" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">排列带来了哪些问题？</li><li id="b394" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">伯特有这样的问题吗？</li><li id="83fe" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">XLNet 如何解决这个问题？</li><li id="1e06" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">注意屏蔽:XLNet 如何实现置换？</li></ul><h1 id="fa26" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">快速回顾<strong class="ak">置换语言建模</strong></h1><p id="333f" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">特殊术语:</p><ul class=""><li id="2ccc" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">AR 语言模型:自回归语言模型</li><li id="a767" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">AE 语言模型:自动编码器语言模型</li></ul><p id="978d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了让本帖更加独立，这里我简单总结一下<a class="ae ky" rel="noopener" target="_blank" href="/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335">什么是 XLNet，为什么它的表现优于 BERT </a>。</p><p id="a531" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XLNet 提出使用<strong class="lb iu">置换语言建模</strong>到<strong class="lb iu">使 AR 语言模型从双向上下文中学习。</strong>通过这种方式，可以避免 AE 语言模型中 MASK 方法带来的弊端。</p><p id="ccf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">排列方法是得到一个序列的排列，并使用前面的<code class="fe ng nh ni nj b">t-1</code>记号作为上下文来预测第 t 个位置记号。例如，我们有一个句子<code class="fe ng nh ni nj b">[x1, x2, x3, x4]</code>，<code class="fe ng nh ni nj b">x3</code>是我们想要预测的第 t 个位置标记。首先，我们得到句子的排列。</p><pre class="kj kk kl km gt nk nj nl nm aw nn bi"><span id="2df6" class="no mk it nj b gy np nq l nr ns">[('x1', 'x2', 'x3', 'x4'),<br/> ('x1', 'x2', 'x4', 'x3'),<br/> ('x1', 'x3', 'x2', 'x4'),<br/> ('x1', 'x3', 'x4', 'x2'),<br/> ('x1', 'x4', 'x2', 'x3'),<br/> ('x1', 'x4', 'x3', 'x2'),<br/> ('x2', 'x1', 'x3', 'x4'),<br/> ('x2', 'x1', 'x4', 'x3'),<br/> ('x2', 'x3', 'x1', 'x4'),<br/> ('x2', 'x3', 'x4', 'x1'),<br/> ('x2', 'x4', 'x1', 'x3'),<br/> ('x2', 'x4', 'x3', 'x1'),<br/> ('x3', 'x1', 'x2', 'x4'),<br/> ('x3', 'x1', 'x4', 'x2'),<br/> ('x3', 'x2', 'x1', 'x4'),<br/> ('x3', 'x2', 'x4', 'x1'),<br/> ('x3', 'x4', 'x1', 'x2'),<br/> ('x3', 'x4', 'x2', 'x1'),<br/> ('x4', 'x1', 'x2', 'x3'),<br/> ('x4', 'x1', 'x3', 'x2'),<br/> ('x4', 'x2', 'x1', 'x3'),<br/> ('x4', 'x2', 'x3', 'x1'),<br/> ('x4', 'x3', 'x1', 'x2'),<br/> ('x4', 'x3', 'x2', 'x1')]</span></pre><p id="b57a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们选择一些样本作为训练数据。(在论文中，它从排列中随机选择样本)</p><pre class="kj kk kl km gt nk nj nl nm aw nn bi"><span id="d51c" class="no mk it nj b gy np nq l nr ns">('x1', 'x2', 'x4', 'x3'),<br/>('x1', 'x4', 'x3', 'x2'),<br/>('x2', 'x3', 'x4', 'x1'),<br/>('x4', 'x2', 'x3', 'x1'),<br/>('x3', 'x2', 'x4', 'x1'),</span></pre><p id="f5cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到<strong class="lb iu">每一个令牌都有机会出现在 x3 </strong>之前。因此 AR 模型可以从这些上下文标记中学习双向信息。如果你仍然不清楚排列，你可以阅读以前的帖子。</p><h1 id="ddfb" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">排列带来了哪些问题？</h1><p id="5dcf" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">排列可以让 AR 模型从两个方向看到上下文，但也带来了原 transformer 无法解决的<strong class="lb iu">问题。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/cc33b9332a7127b53dbc62afa8a195b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORISPXAdhjVTBFxYWyEG1A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">permutation language modeling objective</figcaption></figure><ul class=""><li id="af64" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu"> Z </strong>:因式分解顺序</li><li id="4c65" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">p_θ:似然函数</li><li id="773a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">x_zt:因子分解顺序中的第 t 个记号</li><li id="4597" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">x_t 的 x_z <t: the="" tokens="" before="" t-th="" token=""/></li></ul><p id="e08f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">This is the objective function for permutation language modeling, which means takes t-1 tokens as the context and to predict the t-th token.</p><p id="8fd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">There are two requirements that a standard Transformer cannot do:</p><ol class=""><li id="7d1f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nu mb mc md bi translated">to predict the token x_t, the model should only see the <strong class="lb iu">位置</strong>，而不是 x_t 的<strong class="lb iu">内容</strong>(我会在下一节解释什么是<strong class="lb iu">内容</strong>)</li><li id="42a7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nu mb mc md bi translated">为了预测标记 x_t，模型应该将 x_t 之前的所有标记编码为<strong class="lb iu">内容</strong></li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/39da8dfd36652bad50b3f8410cb0cb41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAQ96S9-GMZnVAY1esW7fA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">graph from <a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">Illustrated Transformer</a></figcaption></figure><p id="fe70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尤其是第一个要求，transformer 将位置编码合并到令牌嵌入中。因此它不能将位置信息从令牌嵌入中分离出来。</p><h1 id="4eb6" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">伯特有这样的问题吗？</h1><p id="073f" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">BERT 是一个 AE 语言模型，它不像 AR 语言模型那样需要单独的位置信息。与 XLNet 需要位置信息来预测第 t 个令牌不同，BERT 使用[MASK]来表示要预测哪个令牌(我们可以认为[MASK]只是一个占位符)。例如，如果 BERT 使用 x2，x1 和 x4 来预测 x3，则 x2，x1，x4 的嵌入包含位置信息和其他与[MASK]相关的信息。所以模型有很大几率预测到[MASK]是 x3。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/fdc283dd382105541b8db6024a8ae353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kkksKomsJFW6ilEcrUfVNQ.png"/></div></div></figure><p id="b86c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我将对<strong class="lb iu">信息</strong>进行更详细的解释。BERT 嵌入(BERT 学习的信息)包含两种信息，<strong class="lb iu">位置信息，和内容信息</strong>(我只是为了简单起见把它分成两部分)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/21a53c766a5485549e38b9e925d221b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RTYVkLcnzDFbMYtbx7GUIg.png"/></div></div></figure><p id="0f48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">位置信息很容易理解，它告诉模型当前令牌的位置。内容信息(语义和语法)包含当前令牌的“含义”。你见过的一个直观的例子是<code class="fe ng nh ni nj b">kind — man + woman = queen</code>。</p><h1 id="c641" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">XLNet 如何解决这个问题？</h1><p id="ca71" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">XLNet 提出<strong class="lb iu">双流自关注</strong>解决问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/8720dd576038553e0fbd8704dc7f581d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ueo2JzpFNUXCA6zfmy8-qQ.png"/></div></div></figure><p id="883a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顾名思义，包含两种自我关注。一个是<strong class="lb iu">内容流关注</strong>，也就是《变形金刚》里的<a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">标准自关注</a>。另一个是<strong class="lb iu">查询流关注。XLNet 引入它来代替 BERT 中的[MASK]标记。</strong></p><p id="eefa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果 BERT 想用上下文单词 x1 和 x2 的知识来预测 x3，它可以使用[MASK]来表示 x3 标记。[掩码]只是一个占位符。而 x1 和 x2 的嵌入包含了帮助模型“知道”[MASK]是 x3 的位置信息。</p><p id="b121" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来 XLNet 事情就不一样了。一个令牌 x3 将服务于两种角色。当它被用作内容来预测其他令牌时，我们可以使用内容表示(通过内容流注意力学习)来表示 x3。<strong class="lb iu">但如果要预测 x3，应该只知道它的位置，不知道它的内容。</strong>这就是为什么 XLNet 使用<strong class="lb iu">查询表示</strong>(通过查询流注意学习到的)来<strong class="lb iu">保留 x3 之前的上下文信息，只保留 x3 的位置信息。</strong></p><p id="d279" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了直观地理解双流自我关注，我们可以只认为<strong class="lb iu"> XLNet 用查询表示代替了 BERT 中的【MASK】。他们只是选择不同的方法来做同一件事。</strong></p><h1 id="5a1e" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">注意屏蔽:XLNet 如何实现置换？</h1><p id="f97d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">当我第一次读到这篇论文时，我无法停止对训练中排列的实现细节的好奇。所以如果你感兴趣的话，我会说一点。</p><p id="4aaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第一部分“排列语言建模的快速回顾”中，我给出了一个例子，一个句子<code class="fe ng nh ni nj b">[x1, x2, x3, x4]</code>的排列如下所示。</p><pre class="kj kk kl km gt nk nj nl nm aw nn bi"><span id="70c8" class="no mk it nj b gy np nq l nr ns">[('x1', 'x2', 'x3', 'x4'),<br/> ('x1', 'x2', 'x4', 'x3'),<br/> ('x1', 'x3', 'x2', 'x4'),<br/> ('x1', 'x3', 'x4', 'x2'),<br/> ('x1', 'x4', 'x2', 'x3'),<br/> ('x1', 'x4', 'x3', 'x2'),<br/> ('x2', 'x1', 'x3', 'x4'),<br/> ('x2', 'x1', 'x4', 'x3'),<br/> ('x2', 'x3', 'x1', 'x4'),<br/> ('x2', 'x3', 'x4', 'x1'),<br/> ('x2', 'x4', 'x1', 'x3'),<br/> ('x2', 'x4', 'x3', 'x1'),<br/> ('x3', 'x1', 'x2', 'x4'),<br/> ('x3', 'x1', 'x4', 'x2'),<br/> ('x3', 'x2', 'x1', 'x4'),<br/> ('x3', 'x2', 'x4', 'x1'),<br/> ('x3', 'x4', 'x1', 'x2'),<br/> ('x3', 'x4', 'x2', 'x1'),<br/> ('x4', 'x1', 'x2', 'x3'),<br/> ('x4', 'x1', 'x3', 'x2'),<br/> ('x4', 'x2', 'x1', 'x3'),<br/> ('x4', 'x2', 'x3', 'x1'),<br/> ('x4', 'x3', 'x1', 'x2'),<br/> ('x4', 'x3', 'x2', 'x1')]</span></pre><p id="425d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常容易让人误解我们需要获取一个句子的随机顺序，并输入到模型中。但这不是真的。输入句子的顺序是<code class="fe ng nh ni nj b">[x1, x2, x3, x4]</code>，而<strong class="lb iu"> XLNet 使用注意掩码来置换因式分解的顺序。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/56ff0964a124a8b79194b5f2abf63d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dWkbvnTg15KV46U-xPOcjA.png"/></div></div></figure><p id="2464" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">句子的原顺序是<code class="fe ng nh ni nj b">[x1, x2, x3, x4]</code>。并且我们随机得到一个因式分解顺序为<code class="fe ng nh ni nj b">[x3, x2, x4, x1]</code>。</p><p id="2ed8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">左上角是内容表示的计算。如果我们想要预测 x1 的<strong class="lb iu">内容表示，我们应该拥有所有 4 个令牌内容信息。<code class="fe ng nh ni nj b">KV = [h1, h2, h3, h4]</code>和<code class="fe ng nh ni nj b">Q = h1</code>。</strong></p><p id="f154" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">左下角是查询表示的计算。如果我们想预测 x1 的<strong class="lb iu">查询表示，我们无法看到 x1 本身的内容表示。<code class="fe ng nh ni nj b">KV = [h2, h3, h4]</code>和<code class="fe ng nh ni nj b">Q = g1</code>。</strong></p><p id="d5e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">右角是整个计算过程。我自下而上解释。首先将<code class="fe ng nh ni nj b">h</code>和<code class="fe ng nh ni nj b">g</code>初始化为<code class="fe ng nh ni nj b">e(xi)</code>和<code class="fe ng nh ni nj b">w</code>。并且在内容屏蔽和查询屏蔽之后，双流关注会输出第一层输出<code class="fe ng nh ni nj b">h^(1)</code>和<code class="fe ng nh ni nj b">g^(1)</code>然后计算第二层。</p><p id="545b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意右边的内容掩码和查询掩码。</strong>两者都是矩阵。在内容蒙版中，第一行有 4 个红点。这意味着第一个令牌(x1)可以看到(注意到)所有其他令牌，包括它自己(x3- &gt; x2- &gt; x4- &gt; x1)。第二排有 2 个红点。意味着第二个令牌(x2)可以看到(注意到)两个令牌(x3- &gt; x2)。其他行依此类推。</p><p id="3361" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">内容掩码<strong class="lb iu"> </strong>和查询掩码<strong class="lb iu">的唯一区别</strong>是查询掩码中的那些<strong class="lb iu">对角线元素为 0，这意味着令牌看不到它们自己。</strong></p><p id="d49f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们总结一下。输入的句子只有一个顺序。但是我们可以使用不同注意力掩模来实现不同的分解顺序。</p><h1 id="ad7c" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">摘要</h1><p id="0a1a" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在这篇文章中，我主要解释了 XLNet 面临的问题是什么，以及如何使用双流自我关注来解决它。我还提到了一些关于置换的注意屏蔽的实现细节。至于结果对比，可以找 XLNet 团队的<a class="ae ky" href="https://medium.com/@xlnet.team/a-fair-comparison-study-of-xlnet-and-bert-with-large-models-5a4257f59dc0" rel="noopener">最新帖子</a>，比论文做了更公正的对比。</p><blockquote class="oa ob oc"><p id="eac0" class="kz la od lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">查看我的其他帖子</em> </strong> <a class="ae ky" href="https://medium.com/@bramblexu" rel="noopener"> <strong class="lb iu"> <em class="it">中等</em> </strong> </a> <strong class="lb iu"> <em class="it">同</em> </strong> <a class="ae ky" href="https://bramblexu.com/posts/eb7bd472/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="it">一个分类查看</em> </strong> </a> <strong class="lb iu"> <em class="it">！<br/>GitHub:</em></strong><a class="ae ky" href="https://github.com/BrambleXu" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="it">bramble Xu</em></strong></a><strong class="lb iu"><em class="it"><br/>LinkedIn:</em></strong><a class="ae ky" href="https://www.linkedin.com/in/xu-liang-99356891/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="it">徐亮</em> </strong> </a> <strong class="lb iu"> <em class="it"> <br/>博客:</em></strong><a class="ae ky" href="https://bramblexu.com" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="it">bramble Xu</em></strong></a></p></blockquote><h1 id="d21b" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><ul class=""><li id="1b10" class="lv lw it lb b lc nb lf nc li oh lm oi lq oj lu ma mb mc md bi translated">论文:<a class="ae ky" href="https://arxiv.org/abs/1906.08237?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1906.08237</a></li><li id="aeae" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">实现:<a class="ae ky" href="https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/modeling_xlnet.py" rel="noopener ugc nofollow" target="_blank">py torch _ transformers/modeling _ xlnet . py</a></li><li id="0491" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335">什么是 XLNet，为什么它的表现优于 BERT </a></li><li id="561c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/" rel="noopener ugc nofollow" target="_blank">论文解析:【XLNet:面向语言理解的广义自回归预训练】讲解</a>，这主要讲的是 transformer-xl。</li><li id="2126" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/@xlnet.team/a-fair-comparison-study-of-xlnet-and-bert-with-large-models-5a4257f59dc0" rel="noopener">XLNet 和 BERT 与大型模型的公平比较研究</a></li><li id="4249" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi"><a class="ae ky" href="https://zhuanlan.zhihu.com/p/70257427" rel="noopener ugc nofollow" target="_blank">XLNet:运行机制及和 Bert 的异同比较</a></li><li id="502d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi"><a class="ae ky" href="http://fancyerii.github.io/2019/06/30/xlnet-theory/" rel="noopener ugc nofollow" target="_blank">XLNet 原理</a></li></ul></div></div>    
</body>
</html>