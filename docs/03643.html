<html>
<head>
<title>BERT Classifier: Just Another Pytorch Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特分类器:只是另一个 Pytorch 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bert-classifier-just-another-pytorch-model-881b3cf05784?source=collection_archive---------9-----------------------#2019-06-10">https://towardsdatascience.com/bert-classifier-just-another-pytorch-model-881b3cf05784?source=collection_archive---------9-----------------------#2019-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f0a56f8d4410608bb0d4e67de6c3940b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wgzXx_W_Njh4g3kXHQEN6A.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Valencia, Spain. Whenever I don’t do projects with image outputs I just use parts of my photo portfolio…. Per usual FRIEND LINK <a class="ae jg" rel="noopener" target="_blank" href="/bert-classifier-just-another-pytorch-model-881b3cf05784?source=friends_link&amp;sk=6600cc7ff8762bff146ebbda1dfab54d">here</a></figcaption></figure><div class=""/><p id="ff29" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2018 年底，谷歌发布了<a class="ae jg" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank"> BERT </a>，它本质上是一个基于所有维基百科训练的 12 层网络。训练协议很有趣，因为与最近的其他语言模型不同，BERT 被训练为从两个方向考虑语言上下文，而不仅仅是单词左侧的内容。在<a class="ae jg" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">的预训练</a>中，BERT 屏蔽掉给定句子中的随机单词，并使用句子的其余部分来预测丢失的单词。Google 还通过在与其他语言模型规模相当的数据集上训练 BERT 来对其进行基准测试，并显示出更强的性能。</p><p id="1b8e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">NLP 是一个我有点熟悉的领域，但是看到 NLP 领域拥有它的“ImageNet”时刻是很酷的，该领域的从业者现在可以相当容易地将最先进的模型应用于他们自己的问题。简单回顾一下，ImageNet 是一个大型开源数据集，在其上训练的模型通常可以在 Tensorflow、Pytorch 等库中找到。这些熟练的预训练模型让数据科学家花更多时间解决有趣的问题，而不是重新发明轮子，专注于数据集的监管(尽管数据集监管仍然非常重要)。你现在需要数千而不是数百万的数据集来开始深度学习。</p><p id="78d1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了工作，我在有限的能力范围内使用过几次 BERT，主要是基于我找到的其他教程。然而，我一直在推迟深入剖析管道并以我更熟悉的方式重建它…在这篇文章中，我只想更好地理解如何以我习惯的方式创建 BERT 管道，以便我可以开始在更复杂的用例中使用 BERT。主要是我对将 BERT 集成到各种网络的多任务集合中感兴趣。</p><p id="b127" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过这个学习过程，我的希望是展示虽然 BERT 是一个推动 NLP 边界的艺术模型，但它就像任何其他 Pytorch 模型一样，并且通过理解它的不同组件，我们可以使用它来创建其他有趣的东西。我真正想要的是克服我对使用 BERT 的恐惧/恐吓，并像使用其他预训练模型一样自由地使用 BERT。</p><h1 id="09b4" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">资料组</h1><p id="6e3a" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">所以在这篇文章中，我使用了经典的 IMDB 电影评论数据集。这个数据集有 50，000 条电影评论，每条评论都标有“正面”或“负面”的情绪。与我的其他帖子不同，我没有构建自定义数据集，部分原因是我不知道快速构建文本数据集的方法，我不想在这上面花太多时间，这一个很容易在互联网上找到。</p><p id="a11e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总的来说，我同意这不是我能做的最有趣的事情，但是在这篇文章中，我更关注如何使用 BERT 构建一个管道。一旦管道到位，我们就可以根据自己的选择交换出数据集，用于更多样/有趣的任务。</p><h1 id="498c" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">分类架构</h1><p id="0fa5" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在这篇文章中，我将使用一个名为<a class="ae jg" href="https://github.com/huggingface/pytorch-pretrained-BERT" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>的团体的伯特 Pytorch port(很酷的团体，奇怪的名字…让我想到半条命拥抱脸)。通常最好是使用任何内置的网络来避免新移植实现的准确性损失…但谷歌对拥抱脸的移植竖起了大拇指，这很酷。</p><p id="5f34" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">无论如何…继续…</p><p id="cec7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我要做的第一件事是建立一个模型架构。为此我主要从拥抱脸的例子中拿出一个例子，叫做<strong class="ki jk"><em class="mh"/></strong>。目前，这个类在文档中看起来已经过时了，但是它是如何构建 BERT 分类器的一个很好的例子。基本上，您可以使用 BertModel 类初始化一个 BERT 预训练模型。然后，您可以根据需要添加额外的层作为分类器头。这与创建其他定制 Pytorch 架构的方式相同。</p><p id="ead8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">像其他 Pytorch 模型一样，您有两个主要部分。首先，您有 init，其中您定义了架构的各个部分，在这种情况下，它是 Bert 模型核心(在这种情况下，它是较小的小写模型，大约 110M 参数和 12 层)，要应用的 dropout，以及分类器层。第二部分是前面的部分，在这里我们定义了如何将架构的各个部分整合到一个完整的管道中。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="abec" class="mr lf jj mn b gy ms mt l mu mv"><strong class="mn jk">class</strong> <strong class="mn jk">BertForSequenceClassification</strong>(nn.Module):<br/>  <br/>    <strong class="mn jk">def</strong> __init__(self, num_labels=2):<br/>        super(BertForSequenceClassification, self).__init__()</span><span id="1817" class="mr lf jj mn b gy mw mt l mu mv">        self.num_labels = num_labels</span><span id="53d1" class="mr lf jj mn b gy mw mt l mu mv">        self.bert = BertModel.from_pretrained('bert-base-uncased')</span><span id="8030" class="mr lf jj mn b gy mw mt l mu mv">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><span id="b96a" class="mr lf jj mn b gy mw mt l mu mv">        self.classifier = nn.Linear(config.hidden_size, num_labels)</span><span id="3923" class="mr lf jj mn b gy mw mt l mu mv">        nn.init.xavier_normal_(self.classifier.weight)</span><span id="b305" class="mr lf jj mn b gy mw mt l mu mv">    <strong class="mn jk">def</strong> forward(self, input_ids, token_type_ids=<strong class="mn jk">None</strong>, attention_mask=<strong class="mn jk">None</strong>, labels=<strong class="mn jk">None</strong>):</span><span id="ad2b" class="mr lf jj mn b gy mw mt l mu mv">        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=<strong class="mn jk">False</strong>)</span><span id="4717" class="mr lf jj mn b gy mw mt l mu mv">        pooled_output = self.dropout(pooled_output)</span><span id="81d5" class="mr lf jj mn b gy mw mt l mu mv">        logits = self.classifier(pooled_output)<br/><br/>        <strong class="mn jk">return</strong> logits</span></pre><p id="ce98" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然已经定义了模型，我们只需要弄清楚如何组织我们的数据，这样我们就可以输入数据并优化权重。在图像的情况下，这通常只是计算出我们需要应用什么样的转换，并确保我们得到正确的格式。对于 BERT，我们需要能够将字符串标记化，并将它们转换成映射到 BERT 词汇表中单词的 id。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/85321531e5d48019d353eef02b19ab3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NkTwBCmA-nUAsssQZwTnNg.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Mendoza, Argentina. Lots of good wine!</figcaption></figure><h1 id="ff62" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">BERT 数据预处理</h1><p id="9053" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">使用 BERT 进行数据准备时，我们需要的主要功能是如何标记输入，并将其转换为 BERT 词汇表中相应的 id。拥抱脸为 BertModel 和 BertTokenizer 类增加了非常好的功能，你可以输入你想要使用的模型的名字，在这篇文章中，它是“bert-base-uncased”模型。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="c748" class="mr lf jj mn b gy ms mt l mu mv">tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')</span></pre><p id="39a9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要对文本进行标记，您只需调用标记化器类的标记化函数。见下文。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="1c69" class="mr lf jj mn b gy ms mt l mu mv">tokenized_text = tokenizer.tokenize(some_text)</span></pre><p id="deeb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，一旦你把一个字符串转换成一个记号列表，你就必须把它转换成一个匹配 BERT 词汇表中单词的 id 列表。这一次，您只需对之前标记化的文本调用 convert_tokens_to_ids 函数。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="545b" class="mr lf jj mn b gy ms mt l mu mv">tokenizer.convert_tokens_to_ids(tokenized_text)</span></pre><p id="10c2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了这些基础知识，我们就可以组装数据集生成器，它总是像流水线中的无名英雄一样，这样我们就可以避免将整个东西加载到内存中，这是一种痛苦，会使在大型数据集上学习变得不合理。</p><h1 id="8dea" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">自定义 BERT 数据集类</h1><p id="fde5" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">一般来说，Pytorch 数据集类是基本数据集类的扩展，您可以在其中指定如何获取下一个项目以及该项目的返回内容，在本例中，它是一个长度为 256 的 id 张量和一个热编码目标值。从技术上来说，你可以做长度为 512 的序列，但我需要一个更大的显卡。我目前正在一台配有 11GB GPU RAM 的 GTX 2080ti 上进行训练。在我之前的 1080 卡上，我只能舒服地使用 128 的序列。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="ba40" class="mr lf jj mn b gy ms mt l mu mv">max_seq_length = 256<br/><strong class="mn jk">class</strong> <strong class="mn jk">text_dataset</strong>(Dataset):<br/>    <strong class="mn jk">def</strong> __init__(self,x_y_list):</span><span id="6625" class="mr lf jj mn b gy mw mt l mu mv">self.x_y_list = x_y_list<br/>        <br/>    <strong class="mn jk">def</strong> __getitem__(self,index):<br/>        <br/>        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])<br/>        <br/>        <strong class="mn jk">if</strong> len(tokenized_review) &gt; max_seq_length:<br/>            tokenized_review = tokenized_review[:max_seq_length]<br/>            <br/>        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)<br/><br/>        padding = [0] * (max_seq_length - len(ids_review))<br/>        <br/>        ids_review += padding<br/>        <br/>        <strong class="mn jk">assert</strong> len(ids_review) == max_seq_length<br/>        <br/>        <em class="mh">#print(ids_review)</em><br/>        ids_review = torch.tensor(ids_review)<br/>        <br/>        sentiment = self.x_y_list[1][index] <em class="mh"># color        </em><br/>        list_of_labels = [torch.from_numpy(np.array(sentiment))]<br/>        <br/>        <br/>        <strong class="mn jk">return</strong> ids_review, list_of_labels[0]<br/>    <br/>    <strong class="mn jk">def</strong> __len__(self):<br/>        <strong class="mn jk">return</strong> len(self.x_y_list[0])</span></pre><p id="1e87" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为这是一段相当不错的未加注释的代码…让我们把它分解一下！</p><p id="33a0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于变量 x_y_list 的第一位。这是我在建立数据集时经常做的事情…它基本上只是一个 x 和 y 的列表，不管它们有多少。然后，我对特定的列表进行索引，根据需要检索特定的 x 或 y 元素。</p><p id="5418" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果有人看过我的其他图像管道，我基本上总是有这个，它通常是对应于测试或训练集的图像 URL 列表。在这种情况下，它是训练电影评论文本的测试，第二个元素是这些电影评论文本的标签。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="e473" class="mr lf jj mn b gy ms mt l mu mv"><strong class="mn jk">class</strong> <strong class="mn jk">text_dataset</strong>(Dataset):<br/>    <strong class="mn jk">def</strong> __init__(self,x_y_list):<br/>        self.x_y_list = x_y_list</span></pre><p id="55cc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以不要再说了！其中最重要的部分是数据集类如何定义给定样本的预处理。</p><ol class=""><li id="d928" class="my mz jj ki b kj kk kn ko kr na kv nb kz nc ld nd ne nf ng bi translated">对于这个 BERT 用例，我们在“self.x_y_list[0][index]”检索给定的评论</li><li id="9f1b" class="my mz jj ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated">然后如上所述用“tokenizer.tokenize”对该评论进行标记。</li><li id="2f97" class="my mz jj ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated">所有的序列需要长度一致，因此，如果序列长于最大长度 256，它将被截短为 256。</li><li id="a99a" class="my mz jj ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated">然后通过“tokenizer . convert _ tokens _ to _ IDs”将标记化和截断的序列转换成 BERT 词汇表 id</li><li id="bb73" class="my mz jj ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated">在序列短于 256 的情况下，现在用 0 填充直到 256。</li><li id="86ea" class="my mz jj ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated">该评论被转换成 torch 张量。</li><li id="48bc" class="my mz jj ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated">然后，该函数返回评论的张量及其一个热编码的正或负标签。</li></ol><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="0a1e" class="mr lf jj mn b gy ms mt l mu mv"><br/>    <strong class="mn jk">def</strong> __getitem__(self,index):<br/>        <br/>        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])<br/>        <br/>        <strong class="mn jk">if</strong> len(tokenized_review) &gt; max_seq_length:<br/>            tokenized_review = tokenized_review[:max_seq_length]<br/>            <br/>        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)<br/><br/>        padding = [0] * (max_seq_length - len(ids_review))<br/>        <br/>        ids_review += padding<br/>        <br/>        <strong class="mn jk">assert</strong> len(ids_review) == max_seq_length<br/>        <br/>        <em class="mh">#print(ids_review)</em><br/>        ids_review = torch.tensor(ids_review)<br/>        <br/>        sentiment = self.x_y_list[1][index]<em class="mh">      </em><br/>        list_of_labels = [torch.from_numpy(np.array(sentiment))]<br/>        <br/>        <br/>        <strong class="mn jk">return</strong> ids_review, list_of_labels[0]<br/>    <br/>    <strong class="mn jk">def</strong> __len__(self):<br/>        <strong class="mn jk">return</strong> len(self.x_y_list[0])</span></pre><h1 id="6071" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">培养</h1><p id="6d95" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在这一点上，培训管道相当标准(现在 BERT 只是另一个 Pytorch 模型)。如果你想检查<a class="ae jg" href="https://github.com/sugi-chan/custom_bert_pipeline/blob/master/bert_pipeline.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>的第 21 块，我可以使用普通的循环训练。这款笔记本和我的其他笔记本之间唯一真正的区别是风格上的，我在网络本身之外使用了最终分类器层的 softmax。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="7c9e" class="mr lf jj mn b gy ms mt l mu mv">outputs = F.softmax(outputs,dim=1)</span></pre><p id="cd63" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后一个有趣的部分是，我给网络的不同部分分配了特定的学习率。几个月前，当我浏览 fastai 视频时，我对这样做产生了兴趣，并发现它很有用。</p><p id="ea14" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个部分做的第一件事是分配两个学习率值，称为<em class="mh"> lrlast </em>和<em class="mh"> lrmain。</em> lrlast 相当标准，为 0.001，而 lrmain 则低得多，为 0.00001。其思想是，当网络的部分被随机初始化，而其他部分已经被训练时，你不需要对预训练的部分应用激进的学习速率而没有破坏速率的风险，然而，新的随机初始化的部分可能不会覆盖，如果它们处于超低的学习速率…因此，对网络的不同部分应用更高或更低的学习速率有助于使每个部分适当地学习。下一部分可以是积极的，而预训练部分可以进行逐步调整。</p><p id="3e1a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">应用这一点的机制出现在字典列表中，您可以在其中指定应用于优化器(在本例中为 Adam 优化器)中网络不同部分的学习率。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="8604" class="mr lf jj mn b gy ms mt l mu mv">lrlast = .001<br/>lrmain = .00001<br/>optim1 = optim.Adam(<br/>    [<br/>        {"params":model.bert.parameters(),"lr": lrmain},<br/>        {"params":model.classifier.parameters(), "lr": lrlast},       <br/>   ])<br/><br/>optimizer_ft = optim1</span></pre><p id="b4d1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">设置好学习速率后，我让它运行 10 个周期，每 3 个周期降低学习速率。网络从一个非常强的点开始…</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="6fd0" class="mr lf jj mn b gy ms mt l mu mv">Epoch 0/9<br/>----------<br/>train total loss: 0.4340 <br/>train sentiment_acc: 0.8728<br/>val total loss: 0.4089 <br/>val sentiment_acc: 0.8992</span></pre><p id="47a2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基本上，用 Bert 预先训练的权重初始化网络意味着它已经对语言有了很好的理解。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="33ec" class="mr lf jj mn b gy ms mt l mu mv">Epoch 9/9<br/>----------<br/>train total loss: 0.3629 <br/>train sentiment_acc: 0.9493<br/>val total loss: 0.3953 <br/>val sentiment_acc: 0.9160</span></pre><p id="d406" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个过程结束时，精确度提高了几个点，损失略有减少…我还没有真正看到模型通常如何在这个数据集上得分，但我认为这是合理的，现在足以表明网络正在进行一些学习。</p><p id="7ad8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我的新 2080ti 卡上，这个数据集上的 10 个时代花了 243m 48s 才完成。顺便提一下，让卡和 Pytorch 一起工作有很多麻烦…主要是更新各种版本的东西。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1f066760e55d502d681f3f820869e0f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmMcAa0eJQmZ6fKUcX3GAA.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Buenos Aires Metropolitan Cathedral</figcaption></figure><h1 id="a4f8" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结束语</h1><p id="7a63" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">对我来说，这样做很重要，可以向自己表明，虽然 BERT 是最先进的，但当我试图将它应用于我自己的问题时，我不应该被吓倒。由于人们付出了大量努力将 BERT 移植到 Pytorch 上，以至于谷歌对其性能表示赞赏，这意味着 BERT 现在只是数据科学家在 NLP 盒子中的另一个工具，就像 Inception 或 Resnet 对于计算机视觉一样。</p><p id="b7ec" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就性能而言，我认为我可以通过在最终分类器之前添加额外的层来挤出几个额外的百分点。这将允许在这个特定的任务中有更多的层。我也不太精通如何在 NLP 领域进行数据扩充，所以这将是其他需要检查的东西，可能使用其他经过训练的语言模型来生成合成文本...但是现在我有了一个 BERT 管道，并且知道我可以像对任何其他模型那样在其上构建自定义分类器…谁知道呢…这里有很多令人兴奋的可能性。</p><blockquote class="nm nn no"><p id="72c1" class="kg kh mh ki b kj kk kl km kn ko kp kq np ks kt ku nq kw kx ky nr la lb lc ld im bi translated">像往常一样，你可以在这里随意查看笔记本<a class="ae jg" href="https://github.com/sugi-chan/custom_bert_pipeline" rel="noopener ugc nofollow" target="_blank">。为了简单起见，数据集也在 repo 中，所以如果你安装了 pytorch 和 pytorch-pretrained-bert 库，你应该可以使用了。</a></p></blockquote></div></div>    
</body>
</html>