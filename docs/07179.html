<html>
<head>
<title>Pyspark – Import any data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">py spark–导入任何数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-import-any-data-f2856cda45fd?source=collection_archive---------8-----------------------#2019-10-10">https://towardsdatascience.com/pyspark-import-any-data-f2856cda45fd?source=collection_archive---------8-----------------------#2019-10-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="aabb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Spark 导入数据的简要指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c642dff3a6b7d10764cc9b447e82d151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jvxJNDq39isZ-Kgf9VsSAQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg" rel="noopener ugc nofollow" target="_blank">https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg</a></figcaption></figure><p id="4ede" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这篇文章，我将开始一系列关于 Pyspark 的简短教程，从数据预处理到建模。第一个将处理任何类型的数据的导入和导出，CSV，文本文件，Avro，Json…等等。我在谷歌云平台上的一台虚拟机上工作，数据来自云存储上的一个桶。让我们导入它们。</p><h2 id="f003" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">导入 CSV</h2><p id="5ec8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Spark 具有读取 csv 的集成功能，非常简单:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="1032" class="lv lw it mu b gy my mz l na nb">csv_2_df = spark.read.csv("gs://my_buckets/poland_ks")</span><span id="17f3" class="lv lw it mu b gy nc mz l na nb">#print it<br/>csv_2_df.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/c5b3b4c27fdf79e3095518acc39ff9a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ni2qFERGCeMLBlJQQpYRQQ.png"/></div></div></figure><p id="b4be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据以正确的列数加载，数据中似乎没有任何问题，但是标题不固定。我们需要设置 header = True 参数。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d5dd" class="lv lw it mu b gy my mz l na nb">csv_2_df = spark.read.csv("gs://my_buckets/poland_ks", header = "true")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/4ec4487f35a22d3748046fdd401e7b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L0aP7SuKQWGkEFlkgt2Q5w.png"/></div></div></figure><p id="0d84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有其他可能的语法。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d92f" class="lv lw it mu b gy my mz l na nb">csv_2_df= spark.read.load("gs://my_buckets/poland_ks", format="csv", header="true")</span></pre><p id="cb00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以及 sep 这样的参数来指定分隔符或者 inferSchema 来推断数据的类型，我们顺便来看看 Schema。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="acb3" class="lv lw it mu b gy my mz l na nb">csv_2_df.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/580bdd03aa4335c83f681faba74be3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*4Rr6HKarrPdaYY9_HeNM7g.png"/></div></figure><p id="a5cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的 dataframe 在 string 中有所有类型的数据集，让我们尝试推断模式。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5a60" class="lv lw it mu b gy my mz l na nb">csv_2_df = spark.read.csv("gs://my_buckets/poland_ks", header =True, inferSchema=True)</span><span id="8bae" class="lv lw it mu b gy nc mz l na nb">csv_2_df.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/797c3626f47ea5b33d8a6533dea69fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*OQ8t77dSHQbql1P6hSHE1Q.png"/></div></figure><p id="db0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以手动指定我们的模式</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="e88a" class="lv lw it mu b gy my mz l na nb">from pyspark.sql.types import *</span><span id="12e4" class="lv lw it mu b gy nc mz l na nb">schema = StructType([<br/>    StructField("ID_DAY", DateType()),<br/>    StructField("SID_STORE", IntegerType()),<br/>    StructField("NB_TICKET", IntegerType()),<br/>    StructField("F_TOTAL_QTY", IntegerType()),<br/>    StructField("F_VAL_SALES_AMT_TCK", DoubleType()),<br/>    StructField("SITE_FORMAT", StringType())])</span><span id="0546" class="lv lw it mu b gy nc mz l na nb">csv_2_df = spark.read.csv("gs://alex_precopro/poland_ks", header = 'true', schema=schema)</span></pre><h2 id="2156" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">导入 JSON</h2><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="ff3d" class="lv lw it mu b gy my mz l na nb">json_to_df = spark.read.json("gs://my_bucket/poland_ks_json")</span></pre><h2 id="6050" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">进口拼花地板</h2><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="52a8" class="lv lw it mu b gy my mz l na nb">parquet_to_df = spark.read.parquet("gs://my_bucket/poland_ks_parquet")</span></pre><h2 id="e6e5" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">导入 AVRO</h2><p id="e6a9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在 Avro 的情况下，我们需要调用外部数据块包来读取它们。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="7706" class="lv lw it mu b gy my mz l na nb">df = spark.read.format("com.databricks.spark.avro").load("gs://alex_precopro/poland_ks_avro", header = 'true')</span></pre><h2 id="193b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">导入文本文件</h2><p id="15aa" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">同样，spark 也有一个内置功能</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="193c" class="lv lw it mu b gy my mz l na nb">textFile = spark.read.text('path/file.txt')</span></pre><p id="1e62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您也可以将文本文件读取为 rdd</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="3d0c" class="lv lw it mu b gy my mz l na nb"># read input text file to RDD<br/>lines = sc.textFile('path/file.txt')<br/># collect the RDD to a list<br/>list = lines.collect()</span></pre><h2 id="d029" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">出口任何东西</h2><p id="7c83" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">要导出数据，你必须适应你想要的输出，如果你写在 parquet，avro 或任何分区文件没有问题。如果我们想以 CSV 格式编写，我们必须将分散在不同工作器上的分区分组，以编写我们的 CSV 文件</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="fcbe" class="lv lw it mu b gy my mz l na nb">#partitioned file<br/>output.write.parquet(“gs://my_bucket/my_output")</span><span id="e63e" class="lv lw it mu b gy nc mz l na nb">#csv </span><span id="8768" class="lv lw it mu b gy nc mz l na nb">partitioned_output.coalesce(1).write.mode("overwrite")\<br/>.format("com.databricks.spark.csv")\<br/>.option("header", "true")\<br/>.option("sep", "|")\<br/>.save('gs://my_bucket/my_output_csv')</span></pre><p id="c052" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用 coalesce(1)或 repartition(1 ),数据帧的所有分区被组合到一个块中。</p><h2 id="4671" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">最后</h2><p id="64b9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们看到了如何导入我们的文件并编写它。让我们来看我的下一篇文章，学习如何过滤我们的数据框架。<strong class="lb iu">感谢</strong>。【Y】You 可以在这里找到代码<em class="nh">:</em><a class="ae ky" href="https://github.com/AlexWarembourg/Medium" rel="noopener ugc nofollow" target="_blank"><em class="nh">https://github.com/AlexWarembourg/Medium</em></a></p></div></div>    
</body>
</html>