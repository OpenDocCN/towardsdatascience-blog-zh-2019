<html>
<head>
<title>Can a Robot Make You Laugh? — Teaching an AI to Tell Jokes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器人能让你笑吗？—教人工智能讲笑话</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-a-robot-make-you-laugh-teaching-an-ai-to-tell-jokes-815f1e1e689c?source=collection_archive---------8-----------------------#2019-12-26">https://towardsdatascience.com/can-a-robot-make-you-laugh-teaching-an-ai-to-tell-jokes-815f1e1e689c?source=collection_archive---------8-----------------------#2019-12-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fde0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">是的，他几乎和我一样有趣！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/fc2a4ea5c73af9d5a9818b28dd1f4aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*rbMFAaMy0kfl5Exhnezdsg.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">A robot laughing at a friend’s joke (<a class="ae ku" href="https://www.google.com/search?q=robot+jokes+comics&amp;rlz=1C5CHFA_enPH839PH839&amp;sxsrf=ACYBGNR-oCXliQuf1GWRcuRm2q3vgfQb-w:1577114175318&amp;tbm=isch&amp;source=iu&amp;ictx=1&amp;fir=7Q2pU2RK6szL2M%253A%252CvxeVVXMCg9fblM%252C_&amp;vet=1&amp;usg=AI4_-kSsnrtxqZtASeALJ49CEgTkRUomWA&amp;sa=X&amp;ved=2ahUKEwi_rsKRiMzmAhVJfXAKHYIqCDkQ9QEwAHoECAkQBg#imgrc=eCDVGiE98kZ_QM" rel="noopener ugc nofollow" target="_blank">Pinterest</a>)</figcaption></figure><p id="d254" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><a class="ae ku" href="https://github.com/enzoampil/tito-joker" rel="noopener ugc nofollow" target="_blank"> Tito Joker </a>是一个幽默的 AI，它使用最先进的深度学习来讲笑话。他的目标是充分理解幽默，讲出真正有趣的笑话。</p><p id="975e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为什么他被命名为提托小丑？因为在菲律宾语中，“tito”翻译成英语就是“叔叔”的意思，而在菲律宾，我们都有那个说最幼稚笑话的叔叔！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/17eb1bae4c65b51ab9f6e45af768eaa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*gwA44t8iPCne4jgKfxTmvA.gif"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk"><a class="ae ku" href="https://github.com/enzoampil/tito-joker" rel="noopener ugc nofollow" target="_blank">Tito Joker</a> tells riddle type jokes based on custom inputs typed in by users</figcaption></figure><p id="fd25" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">欢迎在这个<a class="ae ku" href="http://35.225.94.177:8501/" rel="noopener ugc nofollow" target="_blank">网站</a>上与蒂托·小丑互动。</p><p id="ec69" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如上面的 gif 图所示，他不仅能够讲语法正确的笑话，而且还能讲出正确的笑话。</p><p id="eb00" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">例如，他理解“小鸡为什么过马路？”，用一个在鸡过马路的上下文中有意义的地点或原因来回答。当然，他并不是一直都这样，但他经常这样(如下图)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/3b58500b58f78ae4c8ab16ac2160f217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ENrlDxo3ea5GFCF1jnNbBg.gif"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Tito Joker can tell as many jokes as you want given your input question</figcaption></figure><p id="1669" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了让他更有趣，我决定教他如何使用互联网上最流行的交流方式之一——gif！这个想法是，一旦他讲了一个笑话，他也试图展示一个与这个笑话相关的 GIF。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/96f57f315f3dd391fa9b51e418c1eb11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qnKiaHvk9OxOIO9_6qkrCQ.gif"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Tito Joker is able to show a GIF that matches the meaning of the joke</figcaption></figure><h1 id="47c2" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">这怎么可能？</strong></h1><p id="33dd" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">今年 2019 年，我在深度学习的自然语言处理(NLP)应用方面做了很多工作，特别关注去年发布的基于<a class="ae ku" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> Transformer </a>的预训练模型——<a class="ae ku" href="https://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">BERT</a>、<a class="ae ku" href="https://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank"> OpenAI GPT-2 </a>和<a class="ae ku" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNET </a>。</p><p id="f900" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">别担心，我不会在这里详细讨论 Transformer 架构，但下面是来自 Jay Allamar 的惊人的<a class="ae ku" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">博客文章</a>的解释图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mv"><img src="../Images/ddfb829dc25eebd0fee19023de770d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*clcynXp3sQGj_ov2.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Transformer Architecture Illustration by <a class="ae ku" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">Jay Alammar</a></figcaption></figure><p id="0931" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">作为一个爱开玩笑的人，我可以说我对幽默很有热情，所以模拟其复杂性的想法真的让我很兴奋。我们能否利用深度学习来建立一个实际上自己很有趣的 AI？也许，我们甚至可以把它带到一个可以做单口喜剧的地步！(见下文)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/cd0c600a6e032da9aaca22d72d3af54f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*MzKEdDDJhdrmRP6ssOuleQ.jpeg"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">C-3PO telling a joke as a standup comedian</figcaption></figure><h2 id="c0d3" class="mx lz it bd ma my mz dn me na nb dp mi le nc nd mk li ne nf mm lm ng nh mo ni bi translated">1.利用开放式 GPT-2 进行迁移学习</h2><p id="d8e7" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">鉴于它在语言生成任务上的顶级性能，我决定使用 OpenAI GPT-2(简称 GPT2)作为 Tito Joker 的主干模型。GPT2 的目标很简单——给定所有前面的单词，预测语句的下一个单词(如下所示)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nj"><img src="../Images/927d8717ee1801138383fb24ce14bcd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w4UGcVmuNuceEUyy.gif"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Language modelling illustration by <a class="ae ku" href="https://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">Jay Alammar</a></figcaption></figure><p id="a4ee" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">请注意，GPT2 被训练使用从 800 万个网页中抓取的 40GB 文本来完成这一任务。这是一个很大的文本！</p><p id="785c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，这一切都很有趣，但我们如何使用这个模型来创建一个讲笑话的人工智能？<em class="nk">迁移学习</em> —使用预训练模型(例如 GPT2)并在另一个数据集上“微调”它的过程，该数据集包含您希望模型学习的信息。</p><p id="f3e6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">从这里开始，很明显，方法将是收集包含幽默的文本数据集。通过在幽默数据集上微调 GPT2，我们可以创建一个新的人工智能模型，它理解幽默，并因此可以讲笑话——<strong class="kx iu">蒂托·小丑</strong>。</p><h2 id="aaeb" class="mx lz it bd ma my mz dn me na nb dp mi le nc nd mk li ne nf mm lm ng nh mo ni bi translated">2.带有谜语式笑话的幽默数据集创建</h2><p id="fd81" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">来自 Kaggle 的笑话数据集被用于微调。它包含了 231，657 个不同格式的笑话，包括“Yo Mama”和“它需要多少个”各种笑话。</p><p id="b891" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="nk">警告:该数据集包含 NSFW 笑话，因此 Tito Joker 的幽默也将反映这种性质的笑话。我计划在 Tito Joker 的未来版本中增加过滤这些内容的功能。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nl"><img src="../Images/c953916112ed5148ef32a187bbbc6e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b_2hrOY6pafFVSeK.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Kaggle is a platform that gives free access to data science competitions and open-sourced datasets</figcaption></figure><p id="bfba" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了让 Tito Joker 更容易理解一个笑话的“概念”，我决定把这个笑话的范围限定为谜语类型的笑话。换句话说，我过滤了以“什么”、“如何”、“何时”或“为什么”开头的笑话。这使得笑话的数量下降到 65394 个。</p><p id="1678" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">除此之外，我还添加了一些特殊的标记，让模型能够理解一个谜语式笑话的“问题”和“答案”之间的区别。下表总结了这些情况:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/096b926234524eb033a4def2c79b622e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*2-qWWgGg3n6gfjZNamP1qQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Special tokens help Tito Joker understand the structure of a riddle type joke</figcaption></figure><p id="17a1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">带有特殊标志的笑话示例:</p><blockquote class="nn"><p id="cba2" class="no np it bd nq nr ns nt nu nv nw lq dk translated"><soq>小鸡为什么要过马路？<eoq>到另一边去。<eoa>T13】</eoa></eoq></soq></p></blockquote><p id="f409" class="pw-post-body-paragraph kv kw it kx b ky nx ju la lb ny jx ld le nz lg lh li oa lk ll lm ob lo lp lq im bi translated">更多详情请参考 Tito Joker 的<a class="ae ku" href="https://github.com/enzoampil/tito-joker/blob/master/experiments/process_jokes.py" rel="noopener ugc nofollow" target="_blank">预处理脚本</a>。</p><h2 id="6fa4" class="mx lz it bd ma my mz dn me na nb dp mi le nc nd mk li ne nf mm lm ng nh mo ni bi translated">3.用 GPT2 +幽默数据集进行 Tito Joker 训练</h2><p id="b469" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">既然我们已经有了预训练的模型和幽默的数据集，我们现在可以训练 Tito Joker 了！Tito Joker 是通过对上一节的幽默数据集进行微调而创建的。通过这个过程，Tito Joker 有效地从幽默数据集中“学习”了幽默的概念。</p><p id="becc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">Tito Joker 的端到端培训工作流程总结如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oc"><img src="../Images/6f65a38024fdf18ebf4dbf7e6c1409de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*32c3FvziXwzMt8K-akf5RA.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Tito Joker’s Training Workflow</figcaption></figure><p id="db51" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在一台配有一个 T4 GPU、批量为 2 的 Google Colab 笔记本上，微调大约需要 30 分钟。另外，请注意，培训过程是使用与<a class="ae ku" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">原始 GPT2 论文</a>相同的语言建模目标和超参数执行的。</p><p id="2966" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">更多信息请参考铁托小丑<a class="ae ku" href="https://github.com/enzoampil/tito-joker/blob/master/experiments/Jokes_GPT2_Finetuning.ipynb" rel="noopener ugc nofollow" target="_blank">训练脚本</a>。</p><h2 id="4bd4" class="mx lz it bd ma my mz dn me na nb dp mi le nc nd mk li ne nf mm lm ng nh mo ni bi translated">4.使用词性标注和 GIPHY 生成 GIF</h2><p id="41d8" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">词性标注被用来检测蒂托·乔克讲的笑话中的名词。一旦常见名词被识别出来，它们就被用来搜索<a class="ae ku" href="https://developers.giphy.com/docs/sdk/" rel="noopener ugc nofollow" target="_blank"> GIPHY API </a>，然后返回一个相关的 GIF。</p><p id="b07a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">比如，如果输入的段子是“为什么<em class="nk">鸡</em>要过马路？”，常见名词，<em class="nk">鸡</em>，将被检测并用于从 GIPHY 返回一个 GIF。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi od"><img src="../Images/32cf07dfc0a02707c132891d7701afed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bhR5vvwqjhya3SAH.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">POS tagging illustration from <a class="ae ku" href="https://nlpforhackers.io/wp-content/uploads/2016/08/Intro-POS-Tagging.png" rel="noopener ugc nofollow" target="_blank">nlpforhackers</a></figcaption></figure><p id="ed3b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">请注意，我最初计划使用命名实体识别(NER)，但决定从名词开始，因为“名称”在 GIPHY 上更难匹配。例如，“Lorenzo”(指我自己)不太可能返回我自己的相关 GIF，相比之下，常见的名词“boy”在 GIPHY API 上会很容易有匹配。</p><h1 id="4db7" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">提托小丑怎么学才能更搞笑？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oe"><img src="../Images/c6aa6112f9bd0bef5c2601c08474e98c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_zRzfQ4uLtH3o7SO"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Tito Joker still has a lot to learn if he wants to be as funny as me :)</figcaption></figure><h2 id="3eb5" class="mx lz it bd ma my mz dn me na nb dp mi le nc nd mk li ne nf mm lm ng nh mo ni bi translated">1.对笑话的滑稽程度进行“评级”</h2><p id="d2d0" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">一个<em class="nk">反馈系统</em>将允许用户“评价”蒂托·乔克讲的笑话。这些反馈将被储存起来，并可用于随着时间的推移不断提高 Tito Joker 的“滑稽度”。</p><p id="fe98" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">从建模的角度来看，这可能意味着必须训练一个单独的“滑稽”模型，用于过滤产生的笑话。强力方法的一个例子是生成 100 个笑话，然后只返回这 100 个笑话中最有趣的一个。</p><h2 id="e0db" class="mx lz it bd ma my mz dn me na nb dp mi le nc nd mk li ne nf mm lm ng nh mo ni bi translated">2.控制要讲的笑话类型</h2><p id="d709" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated"><em class="nk">语义控件</em>将允许用户配置他们想从 Tito Joker 中体验的幽默类型。例如，我们可能想要明确地告诉 Tito Joker 产生<em class="nk"> Yo Mama </em>类型的笑话，或者我们甚至可能想要明确地设置情绪，并最小化所讲笑话的毒性。</p><p id="3367" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这将要求我们在培训和部署时考虑这些玩笑维度(例如玩笑类型、情绪和毒性)。他们的梦想是拥有类似于关的 TL-GAN 模型的东西，这种模型可以根据年龄、发际线和性别(以及其他因素)轻松配置模型生成的人脸。</p><h2 id="76f2" class="mx lz it bd ma my mz dn me na nb dp mi le nc nd mk li ne nf mm lm ng nh mo ni bi translated">3.给 Tito Joker 一个主题的上下文</h2><p id="130c" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated"><em class="nk">上下文输入</em>将允许用户给提托小丑<em class="nk">上下文信息</em>，他们希望提托小丑在讲笑话时考虑这些信息。这是基于一个想法，即语境是一个笑话的<em class="nk">智慧</em>的来源。例如，单口喜剧演员被认为是有趣的，因为他们能够在他们讲的笑话中嵌入相关的概念——政治、文化、时事。</p><p id="e8e3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">实现这一点的一种方法是使用类似的规范，用<a class="ae ku" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT 来表示 Q &amp; A </a>，其中问题和上下文段落都被用作模型的输入。想象一下，能够输入一篇关于唐纳德·特朗普的文章，以向蒂托·小丑提供在讲笑话时要考虑哪种信息的背景(例如，墨西哥边境上的墙、移民、与中国的贸易战等)。).</p><h1 id="0efd" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">结论</h1><p id="ea02" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">用人工智能模拟幽默仍然是一项正在进行的工作，但还有很多实验有待进行。如果我们继续朝着这个目标努力，我相信很快我们就能训练提托·小丑自己变得有趣。</p><p id="6fbf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果你想试用 Tito Joker，了解更多的方法，甚至为改进模型做出贡献，请随时查看<a class="ae ku" href="http://35.225.94.177:8501/" rel="noopener ugc nofollow" target="_blank">网站</a>和<a class="ae ku" href="https://github.com/enzoampil/tito-joker" rel="noopener ugc nofollow" target="_blank"> github repo </a>。</p><p id="dedb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果还有任何问题，请不要犹豫，在下面评论，给我发电子邮件(<em class="nk">lorenzo.ampil@gmail.com)</em>，或者通过<a class="ae ku" href="https://www.linkedin.com/in/lorenzoampil/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>或<a class="ae ku" href="https://twitter.com/AND__SO" rel="noopener ugc nofollow" target="_blank"> Twitter </a>给我发信息。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h2 id="caa4" class="mx lz it bd ma my mz dn me na nb dp mi le nc nd mk li ne nf mm lm ng nh mo ni bi translated">承认</h2><p id="017d" class="pw-post-body-paragraph kv kw it kx b ky mq ju la lb mr jx ld le ms lg lh li mt lk ll lm mu lo lp lq im bi translated">特别感谢:</p><ul class=""><li id="6065" class="om on it kx b ky kz lb lc le oo li op lm oq lq or os ot ou bi translated"><a class="ae ku" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>因为他们使用 PyTorch 实现了 OpenAI GPT-2</li><li id="1682" class="om on it kx b ky ov lb ow le ox li oy lm oz lq or os ot ou bi translated">Streamlit 让部署 Tito Joker 成为一个网络应用变得非常简单</li><li id="67f4" class="om on it kx b ky ov lb ow le ox li oy lm oz lq or os ot ou bi translated">思维机器数据科学公司赞助了我运行 Tito Joker 的服务器</li><li id="b44f" class="om on it kx b ky ov lb ow le ox li oy lm oz lq or os ot ou bi translated">我的 TM 同事在这篇文章上给了我大量建设性的反馈。</li><li id="1451" class="om on it kx b ky ov lb ow le ox li oy lm oz lq or os ot ou bi translated"><a class="ae ku" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">空间</a>让他们的 POS 和 NER 模型变得非常简单易用</li></ul><h2 id="9c6b" class="mx lz it bd ma my mz dn me na nb dp mi le nc nd mk li ne nf mm lm ng nh mo ni bi translated">参考</h2><ul class=""><li id="9961" class="om on it kx b ky mq lb mr le pa li pb lm pc lq or os ot ou bi translated">拉德福德等人(2018)“语言模型是无监督的多任务学习者”来自<a class="ae ku" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">https://cdn . open ai . com/better-Language-Models/Language _ Models _ are _ Unsupervised _ multishop _ Learners . pdf</a></li><li id="3b10" class="om on it kx b ky ov lb ow le ox li oy lm oz lq or os ot ou bi translated">OpenAI (2019)“更好的语言模型及其含义”，来自 https://openai.com/blog/better-language-models/#fn1<a class="ae ku" href="https://openai.com/blog/better-language-models/#fn1" rel="noopener ugc nofollow" target="_blank"/></li><li id="9621" class="om on it kx b ky ov lb ow le ox li oy lm oz lq or os ot ou bi translated">来自 https://jalammar.github.io/illustrated-gpt2/<a class="ae ku" href="https://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank"/>的 Allamar，J. (2019)“图解 GPT-2(可视化变压器语言模型)”</li><li id="971c" class="om on it kx b ky ov lb ow le ox li oy lm oz lq or os ot ou bi translated">关，S. (2018)“使用人工智能生成定制的照片级真实感人脸”来自<a class="ae ku" href="https://blog.insightdatascience.com/generating-custom-photo-realistic-faces-using-ai-d170b1b59255" rel="noopener ugc nofollow" target="_blank">https://blog . insightdatascience . com/Generating-custom-photo-realistic-faces-using-AI-d 170 B1 b 59255</a></li><li id="6914" class="om on it kx b ky ov lb ow le ox li oy lm oz lq or os ot ou bi translated">Devlin 等人(2018)“BERT:用于语言理解的深度双向转换器的预训练”，来自<a class="ae ku" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.04805.pdf</a></li></ul></div></div>    
</body>
</html>