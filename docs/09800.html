<html>
<head>
<title>Running StormCrawler continuously in local mode without a Storm cluster</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在本地模式下连续运行 StormCrawler，不使用 Storm cluster</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/running-stormcrawler-continuously-in-local-mode-without-a-storm-cluster-22e1aef72198?source=collection_archive---------24-----------------------#2019-12-23">https://towardsdatascience.com/running-stormcrawler-continuously-in-local-mode-without-a-storm-cluster-22e1aef72198?source=collection_archive---------24-----------------------#2019-12-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/0696b8b72e33f30ad8c18aff975abf5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8f_Py2iFdMrWwFhc0J1ZA.png"/></div></div></figure><p id="09c6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在之前的<a class="ae kz" href="https://medium.com/@cnf271/web-scraping-and-indexing-with-stormcrawler-and-elasticsearch-a105cb9c02ca" rel="noopener">文章</a>中，我分享了我如何使用<a class="ae kz" href="http://stormcrawler.net/" rel="noopener ugc nofollow" target="_blank"> StormCrawler </a>抓取网页并将其索引到 Elasticsearch 服务器的经验。然而，我使用了<a class="ae kz" href="https://storm.apache.org/releases/2.0.0/flux.html" rel="noopener ugc nofollow" target="_blank"> Apache Flux </a>在本地模式下运行注入器和爬虫拓扑。运行这两种拓扑的缺点是，flux 使用 60 秒的 TTL，我们必须重复运行注入器和爬虫。此外，我们使用一个<em class="la"> FileSpout </em>从注入器拓扑中的一个文本文件中读取预定义的 URL。</p><p id="2b9e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在本文中，我将解释如何在没有 Storm 集群的情况下，在本地模式下同时运行注入器和爬虫拓扑。此外，我将使用 AWS 的简单队列服务(SQS)向注入器提供 URL，而不是从文本文件中读取 URL。</p><p id="cd2b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我将跳过最初的步骤，因为我在上一篇文章中简要地解释了它们。此外，我将本文分为两个部分。</p><h1 id="360b" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">第 1 部分— StormCrawler 基本配置</h1><p id="49da" class="pw-post-body-paragraph kb kc it kd b ke lz kg kh ki ma kk kl km mb ko kp kq mc ks kt ku md kw kx ky im bi translated"><strong class="kd iu">第一步</strong></p><p id="c7f0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用以下命令创建 maven 项目。假设你已经在你的电脑上安装了<a class="ae kz" href="https://maven.apache.org/" rel="noopener ugc nofollow" target="_blank"> maven </a>。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="bed4" class="mn lc it mj b gy mo mp l mq mr">mvn archetype:generate -DarchetypeGroupId=com.digitalpebble.stormcrawler -DarchetypeArtifactId=storm-crawler-archetype -DarchetypeVersion=1.15</span></pre><p id="b932" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我已经给了<em class="la"> com.cnf271 </em>和<em class="la"> stormcrawlersqs </em>分别作为 groupId 和 artifactId。</p><p id="4051" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第二步</strong></p><p id="85b3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">删除<em class="la"> crawler.flux </em>文件，并将以下文件从 StormCrawler GitHub <a class="ae kz" href="https://github.com/DigitalPebble/storm-crawler" rel="noopener ugc nofollow" target="_blank">资源库</a>的<em class="la">/external/elastic search</em>文件夹添加到 source 文件夹。</p><ul class=""><li id="d044" class="ms mt it kd b ke kf ki kj km mu kq mv ku mw ky mx my mz na bi translated"><em class="la"> ES_IndexInit.sh </em></li><li id="6d01" class="ms mt it kd b ke nb ki nc km nd kq ne ku nf ky mx my mz na bi translated"><em class="la"> es-conf.yaml </em></li></ul><p id="0ecf" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">另外，删除<em class="la">src/main/Java/com/{ your-group-name }</em>文件夹中的以下文件</p><ul class=""><li id="461d" class="ms mt it kd b ke kf ki kj km mu kq mv ku mw ky mx my mz na bi translated"><em class="la">爬虫拓扑</em></li></ul><p id="7460" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第三步</strong></p><p id="d853" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">将<em class="la">storm crawler elastic search</em>依赖项和<em class="la"> AWS Java SQS SDK </em>添加到<em class="la"> pom.xml </em>文件中。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0e83" class="mn lc it mj b gy mo mp l mq mr">&lt;dependency&gt;<br/>   &lt;groupId&gt;com.digitalpebble.stormcrawler&lt;/groupId&gt;<br/>   &lt;artifactId&gt;storm-crawler-elasticsearch&lt;/artifactId&gt;<br/>   &lt;version&gt;1.15&lt;/version&gt;<br/>&lt;/dependency&gt;</span><span id="f1bf" class="mn lc it mj b gy ng mp l mq mr">&lt;dependency&gt;<br/>   &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;<br/>   &lt;artifactId&gt;aws-java-sdk-sqs&lt;/artifactId&gt;<br/>   &lt;version&gt;LATEST&lt;/version&gt;<br/>&lt;/dependency&gt;</span></pre><p id="dbf6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第四步</strong></p><p id="ece0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">更改<em class="la"> ES_IndexInit.sh </em> bash 脚本中的配置以启用内容存储。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a66e" class="mn lc it mj b gy mo mp l mq mr">...</span><span id="96db" class="mn lc it mj b gy ng mp l mq mr">"_source": {  <br/>  "enabled": true  <br/>    }, <br/>  "properties": { <br/>        "content": {  <br/>            "type": "text",  <br/>            "index": "true",  <br/>            "store": true  <br/>        }<br/>    }<br/>}</span><span id="203f" class="mn lc it mj b gy ng mp l mq mr">...</span></pre><p id="6ee5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第五步</strong></p><p id="5ad6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">执行<em class="la"> ES_IndexInit.sh </em> bash 脚本在 Elasticsearch 服务器中创建索引。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b374" class="mn lc it mj b gy mo mp l mq mr">E:\stormcrawlersqs\stormcrawlersqs&gt;ES_IndexInit.sh</span></pre><p id="0ca1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">既然基本配置已经完成，我将继续讨论本文的实际目标。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="6a4a" class="lb lc it bd ld le no lg lh li np lk ll lm nq lo lp lq nr ls lt lu ns lw lx ly bi translated">第 2 部分-为 URL 注入和 web 爬行创建单独的实例</h1><p id="4778" class="pw-post-body-paragraph kb kc it kd b ke lz kg kh ki ma kk kl km mb ko kp kq mc ks kt ku md kw kx ky im bi translated"><strong class="kd iu">在没有风暴集群的情况下，在本地模式下连续运行 StormCrawler，实际上是什么意思？T3】</strong></p><p id="d1d0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我将解释我是如何设法在没有任何风暴集群在后台运行的情况下连续运行爬虫的。为了做到这一点，我创建了两个单独的 StormCrawler 实例，Injector 和 Crawler 分别执行 URL 注入和 web 抓取。此外，我还创建了一个<em class="la"> SQSSpout </em>来读取来自 AWS SQS 的队列消息。下图给出了它是如何发生的大致情况。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/484ba1af259c557647fc6bf60168f1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HkgI3MVjXzmWRFjoadwnYQ.png"/></div></div></figure><p id="3491" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从上图中可以明显看出，AWS SQS 已被用于向<em class="la">注入器实例</em>发送队列消息，注入器实例用状态索引的域细节更新 Elasticsearch 服务器。</p><p id="7c5d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="la"> Crawler 实例，</em>另一方面，持续寻找新的 URL(处于<em class="la"> DISCOVERED </em>状态的 URL ),这些 URL 已经由 Injector 实例发送到 ES 服务器。Crawler 实例将从相关页面中抓取数据，并用内容数据更新 Elasticsearch 服务器。</p><p id="5ed4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在源文件夹中为 Injector 和 Crawler 以及 SQSSpout 类创建两个单独的类。</p><ul class=""><li id="4146" class="ms mt it kd b ke kf ki kj km mu kq mv ku mw ky mx my mz na bi translated"><em class="la">注射拓扑</em></li><li id="7cbd" class="ms mt it kd b ke nb ki nc km nd kq ne ku nf ky mx my mz na bi translated"><em class="la">爬虫拓扑</em></li><li id="7a73" class="ms mt it kd b ke nb ki nc km nd kq ne ku nf ky mx my mz na bi translated"><em class="la">风暴 sqsqqueuespout</em></li></ul><p id="5593" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">注射器实例(</strong> <em class="la">注射器拓扑</em> <strong class="kd iu"> ) </strong></p><p id="d268" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如前所述，注入器实例将使用<em class="la">stormsqqueuespout</em>从 AWS 的 SQS 读取消息，我已经实现了类似于<a class="ae kz" href="https://github.com/DigitalPebble/storm-crawler/blob/master/core/src/main/java/com/digitalpebble/stormcrawler/spout/FileSpout.java" rel="noopener ugc nofollow" target="_blank"><em class="la">FileSpout</em></a><em class="la"/>的<a class="ae kz" href="https://mvnrepository.com/artifact/com.digitalpebble.stormcrawler/storm-crawler-core/1.15" rel="noopener ugc nofollow" target="_blank"> StormCrawler 核心</a>库。</p><p id="d56b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，Injector 实例包含一个 spout 和两个 bolts，用于从队列中读取消息、过滤 URL 和更新爬行状态。</p><p id="7763" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以下要点显示了使用<em class="la">stormsqsquespout</em>从 SQS 队列中读取数据的注入器实例。我使用了一个<em class="la"> config.properties </em>文件来存储和读取 AWS 凭证以访问 SQS 消息。</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="6da7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="la">保存在 config.properties </em>文件中的 AWS 信息如下<em class="la">、</em></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="8cd7" class="mn lc it mj b gy mo mp l mq mr"># AWS SQS CREDENTIALS<br/>aws.sqs.followerQueueUrl={queueUrl}<br/>aws.sqs.accessKey={accessKey}<br/>aws.sqs.secretAccessKey={secretAccessKey}<br/>aws.sqs.region={Region}</span></pre><p id="e033" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> SQS 壶嘴(</strong><em class="la">stormsqqueuespout</em><strong class="kd iu">)</strong></p><p id="da55" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">stormsqsquespout 是本文的重点之一。当注入器实例启动并运行时，它使用 StormSqsQueueSpout 来读取由 SQS 发送的要被爬网的 URL。</p><p id="d86b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意，我使用了下面的 json 格式来发送<em class="la"> url </em>到使用 SQS 的注射器实例。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="600d" class="mn lc it mj b gy mo mp l mq mr">{"url":"<a class="ae kz" href="https://www.bbc.co.uk/" rel="noopener ugc nofollow" target="_blank">https://www.bbc.co.uk/</a>"}</span></pre><p id="3066" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以下要点显示了如何使用 ISpout 接口的<em class="la"> nextTuple() </em>来接收和处理发送到消息队列的 SQS 消息。</p><blockquote class="nw nx ny"><p id="5d59" class="kb kc la kd b ke kf kg kh ki kj kk kl nz kn ko kp oa kr ks kt ob kv kw kx ky im bi translated"><a class="ae kz" href="https://storm.apache.org/releases/current/javadocs/org/apache/storm/spout/ISpout.html" rel="noopener ugc nofollow" target="_blank"><strong class="kd iu">next tuple()</strong></a><strong class="kd iu">-</strong>调用此方法时，Storm 请求 Spout 向输出收集器发出元组。此方法应该是非阻塞的，因此如果 Spout 没有要发出的元组，此方法应该返回。nextTuple、ack 和 fail 都是在 spout 任务的单线程中的一个紧循环中调用的。当没有元组要发出时，礼貌的做法是让 nextTuple 休眠一小段时间(比如一毫秒)，以免浪费太多 CPU。</p></blockquote><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="c63a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">爬虫实例(</strong> <em class="la">爬虫拓扑</em> <strong class="kd iu"> ) </strong></p><p id="3efb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">顾名思义，Crawler 实例使用单个喷口和几个螺栓来完成实际的爬行部分。</p><p id="3b21" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以下要点是如何使用<a class="ae kz" href="https://github.com/DigitalPebble/storm-crawler/blob/master/external/elasticsearch/src/main/java/com/digitalpebble/stormcrawler/elasticsearch/persistence/AggregationSpout.java" rel="noopener ugc nofollow" target="_blank"><em class="la">aggregation spout</em></a><em class="la"/>从 ES 服务器检索 URL，以及如何使用其他 bolts 获取、过滤、抓取和索引抓取的内容的示例。</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="d999" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一旦正确配置了所有必需的类，就可以构建 maven 项目。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e155" class="mn lc it mj b gy mo mp l mq mr">mvn clean package</span></pre><p id="f41b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在项目已经打包，您可以使用下面的命令启动两个独立的注入器和爬虫实例。打开两个终端并运行以下命令。</p><p id="1ce5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">喷油器</strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="fbf7" class="mn lc it mj b gy mo mp l mq mr">java -cp target\stormcrawlersqs-1.0-SNAPSHOT.jar com.cnf271.InjectorTopology -conf es-conf.yaml -local</span></pre><p id="9507" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">履带</strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b045" class="mn lc it mj b gy mo mp l mq mr">java -cp target\stormcrawlersqs-1.0-SNAPSHOT.jar com.cnf271.CrawlerTopology -conf es-conf.yaml -local</span></pre><p id="4bb1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">继续发送 SQS 消息，并检查 Elasticsearch 服务器不断更新。</p><h2 id="2014" class="mn lc it bd ld oc od dn lh oe of dp ll km og oh lp kq oi oj lt ku ok ol lx om bi translated">使用 ExecutorService 在一个实例中同时运行注入器和爬虫</h2><p id="7a5c" class="pw-post-body-paragraph kb kc it kd b ke lz kg kh ki ma kk kl km mb ko kp kq mc ks kt ku md kw kx ky im bi translated">您可以使用<em class="la"> util.concurrent </em>包中的<a class="ae kz" href="https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ExecutorService.html" rel="noopener ugc nofollow" target="_blank"> ExecutorService </a>来运行单个实例，而不是分别运行两个实例。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9e3c" class="mn lc it mj b gy mo mp l mq mr">ExecutorService executorService = Executors.<em class="la">newFixedThreadPool</em>(2);<br/>executorService.execute(new InjectorTopology());<br/>executorService.execute(new CrawlerTopology());</span></pre><p id="4ba8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我已经将示例项目添加到我的<a class="ae kz" href="https://github.com/cnf271/stormcrawlersqs" rel="noopener ugc nofollow" target="_blank"> GitHub </a>资源库中，供您参考。</p></div></div>    
</body>
</html>