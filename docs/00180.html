<html>
<head>
<title>Andrew Ng’s Machine Learning Course in Python (Kmeans-Clustering, PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">吴恩达的 Python (Kmeans-Clustering，PCA)机器学习课程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-kmeans-clustering-pca-b7ba6fafa74?source=collection_archive---------12-----------------------#2019-01-08">https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-kmeans-clustering-pca-b7ba6fafa74?source=collection_archive---------12-----------------------#2019-01-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/dc64f9320e20b742a636c2ce90b428f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0p2WOAM3CGCZBy6lKhM7g.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Machine Learning — Andrew Ng</figcaption></figure><p id="8dba" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">本系列的倒数第二部分，我们来看看由无标签数据组成的无监督学习算法。让我们直接进入作业，因为我们今天要学习两个算法。</p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="3b75" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi lk translated"><span class="l ll lm ln bm lo lp lq lr ls di">K</span>-意思是聚类是一种聚类分析技术，允许将数据分组到称为聚类的组中。因为没有为每个训练数据提供标签，所以通过数据彼此的相似性来确定聚类。</p><p id="63a2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们将从实现 K-means 算法开始。由于 K-means 是一个迭代过程，它将训练样本分配给它们最近的质心，然后重新计算质心，我们需要两个主要函数来完成这一任务。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="ee14" class="mc md it ly b gy me mf l mg mh">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from scipy.io import loadmat</span><span id="a27c" class="mc md it ly b gy mi mf l mg mh">mat = loadmat("ex7data2.mat")<br/>X = mat["X"]</span></pre><p id="bbd0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><code class="fe mj mk ml ly b">findClosestCentroids</code>通过评估训练样本与每个质心之间的距离来找到最近的质心，并将质心分配给具有最小距离的训练样本。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="7413" class="mc md it ly b gy me mf l mg mh">def findClosestCentroids(X, centroids):<br/>    """<br/>    Returns the closest centroids in idx for a dataset X where each row is a single example.<br/>    """<br/>    K = centroids.shape[0]<br/>    idx = np.zeros((X.shape[0],1))<br/>    temp = np.zeros((centroids.shape[0],1))<br/>    <br/>    for i in range(X.shape[0]):<br/>        for j in range(K):<br/>            dist = X[i,:] - centroids[j,:]<br/>            length = np.sum(dist**2)<br/>            temp[j] = length<br/>        idx[i] = np.argmin(temp)+1<br/>    return idx</span><span id="7df1" class="mc md it ly b gy mi mf l mg mh"># Select an initial set of centroids<br/>K = 3<br/>initial_centroids = np.array([[3,3],[6,2],[8,5]])<br/>idx = findClosestCentroids(X, initial_centroids)<br/>print("Closest centroids for the first 3 examples:\n",idx[0:3])</span></pre><p id="c49a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><code class="fe mj mk ml ly b">np.argmin</code>找出距离最小的指标，赋给训练样本。这里使用+1 从 1 而不是 0 开始对质心进行编号。</p><p id="391a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">print 语句将打印:</p><p id="cc47" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">前三个例子的最近质心:<br/> [[1。】<br/>【3。】<br/>【2。]]</p><p id="bca8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了计算分配后的质心平均值，我们将分配给特定质心的训练样本相加，然后除以每个质心中的样本数。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="89d7" class="mc md it ly b gy me mf l mg mh">def computeCentroids(X, idx, K):<br/>    """<br/>    returns the new centroids by computing the means of the data points assigned to each centroid.<br/>    """<br/>    m, n = X.shape[0],X.shape[1]<br/>    centroids = np.zeros((K,n))<br/>    count = np.zeros((K,1))<br/>    <br/>    for i in range(m):<br/>        index = int((idx[i]-1)[0])<br/>        centroids[index,:]+=X[i,:]<br/>        count[index]+=1<br/>    <br/>    return centroids/count</span><span id="97ff" class="mc md it ly b gy mi mf l mg mh">centroids = computeCentroids(X, idx, K)<br/>print("Centroids computed after initial finding of closest centroids:\n", centroids)</span></pre><p id="add7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">print 语句将打印:</p><p id="9f68" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在最初找到最近的质心之后计算的质心:<br/>[[2.42830111 3.15792418]<br/>[5.81350331 2.63365645]<br/>[7.11938687 3.6166844]]</p><p id="599f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，为了可视化整个过程，我为算法的每次迭代创建了一个子图，以监控质心的移动和训练示例的分配。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="5f09" class="mc md it ly b gy me mf l mg mh">def plotKmeans(X, centroids, idx, K, num_iters):<br/>    """<br/>    plots the data points with colors assigned to each centroid<br/>    """<br/>    m,n = X.shape[0],X.shape[1]<br/>    <br/>    fig, ax = plt.subplots(nrows=num_iters,ncols=1,figsize=(6,36))<br/>    <br/>    for i in range(num_iters):    <br/>        # Visualisation of data<br/>        color = "rgb"<br/>        for k in range(1,K+1):<br/>            grp = (idx==k).reshape(m,1)<br/>            ax[i].scatter(X[grp[:,0],0],X[grp[:,0],1],c=color[k-1],s=15)</span><span id="7af7" class="mc md it ly b gy mi mf l mg mh"># visualize the new centroids<br/>        ax[i].scatter(centroids[:,0],centroids[:,1],s=120,marker="x",c="black",linewidth=3)<br/>        title = "Iteration Number " + str(i)<br/>        ax[i].set_title(title)<br/>        <br/>        # Compute the centroids mean<br/>        centroids = computeCentroids(X, idx, K)<br/>        <br/>        # assign each training example to the nearest centroid<br/>        idx = findClosestCentroids(X, centroids)<br/>    <br/>    plt.tight_layout()</span><span id="e625" class="mc md it ly b gy mi mf l mg mh">m,n = X.shape[0],X.shape[1]<br/>plotKmeans(X, initial_centroids,idx, K,10)</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mm"><img src="../Images/f304540e9c86c1bfff6307bdc1c759bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYZWG7A8bhYT9vR-oexahg.png"/></div></div></figure><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mn"><img src="../Images/207ea8169a4a4b57297c4c01581c2717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pnqCDoNxSONyg95grMviPw.png"/></div></div></figure><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/e0f198bdcd5a76c23dced9e4a6ac280a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6_gfl7zpQsnFPso6R5VCg.png"/></div></div></figure><p id="d53b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">因为 K-means 算法并不总是给出最优解，所以随机初始化很重要。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="b9d1" class="mc md it ly b gy me mf l mg mh">def kMeansInitCentroids(X, K):<br/>    """<br/>    This function initializes K centroids that are to beused in K-Means on the dataset X<br/>    """<br/>    m,n = X.shape[0], X.shape[1]<br/>    centroids = np.zeros((K,n))<br/>    <br/>    for i in range(K):<br/>        centroids[i] = X[np.random.randint(0,m+1),:]<br/>        <br/>    return centroids</span><span id="a65d" class="mc md it ly b gy mi mf l mg mh">centroids = kMeansInitCentroids(X, K)<br/>idx = findClosestCentroids(X, centroids)<br/>plotKmeans(X, centroids,idx, K,10)</span></pre><p id="c847" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的代码将再次运行可视化，但会进行随机初始化。您可以多次运行代码来查看随机初始质心的影响。</p><p id="7b7f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在我们完成了算法的编码，我们可以开始用其他数据集实现它了。在本练习中，我们将使用该算法选择 16 个聚类来代表图像(从数千种颜色中选择)以压缩图像。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="9073" class="mc md it ly b gy me mf l mg mh">mat2 = loadmat("bird_small.mat")<br/>A = mat2["A"]</span><span id="7c1f" class="mc md it ly b gy mi mf l mg mh"># preprocess and reshape the image<br/>X2 = (A/255).reshape(128*128,3)</span><span id="7769" class="mc md it ly b gy mi mf l mg mh">def runKmeans(X, initial_centroids,num_iters,K):<br/>    <br/>    idx = findClosestCentroids(X, initial_centroids)<br/>    <br/>    for i in range(num_iters):<br/>        <br/>        # Compute the centroids mean<br/>        centroids = computeCentroids(X, idx, K)</span><span id="e59a" class="mc md it ly b gy mi mf l mg mh"># assign each training example to the nearest centroid<br/>        idx = findClosestCentroids(X, initial_centroids)</span><span id="c08a" class="mc md it ly b gy mi mf l mg mh">return centroids, idx</span></pre><p id="1b9b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在对数据集运行 k-means 算法</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="a82f" class="mc md it ly b gy me mf l mg mh">K2 = 16<br/>num_iters = 10<br/>initial_centroids2 = kMeansInitCentroids(X2, K2)<br/>centroids2, idx2 = runKmeans(X2, initial_centroids2, num_iters,K2)</span><span id="15d1" class="mc md it ly b gy mi mf l mg mh">m2,n2 = X.shape[0],X.shape[1]<br/>X2_recovered = X2.copy()<br/>for i in range(1,K2+1):<br/>    X2_recovered[(idx2==i).ravel(),:] = centroids2[i-1]</span><span id="4087" class="mc md it ly b gy mi mf l mg mh"># Reshape the recovered image into proper dimensions<br/>X2_recovered = X2_recovered.reshape(128,128,3)</span><span id="6205" class="mc md it ly b gy mi mf l mg mh"># Display the image<br/>import matplotlib.image as mpimg<br/>fig, ax = plt.subplots(1,2)<br/>ax[0].imshow(X2.reshape(128,128,3))<br/>ax[1].imshow(X2_recovered)</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/a2de8cf80bb2d3ea0496f3c3a012772a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*oMext6iL8HrKH7_kY85cWQ.png"/></div></figure><p id="7634" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这是原始图像和只有 16 种颜色的压缩图像的并排比较。</p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/4b812fefac52242ce7f6c984b95cb0ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*veYE3uCdBRFY_Npbey_APg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Machine Learning — Andrew Ng</figcaption></figure><p id="d2a5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">作业的下一部分利用 2D 数据集来获得对主成分分析(PCA)过程的直觉，然后在人脸图像数据集上进行 PCA 以执行降维。</p><p id="1ef7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">加载并可视化 2D 数据集</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="4873" class="mc md it ly b gy me mf l mg mh">mat3 = loadmat("ex7data1.mat")<br/>X3 = mat3["X"]</span><span id="9131" class="mc md it ly b gy mi mf l mg mh">plt.scatter(X3[:,0],X3[:,1],marker="o",facecolors="none",edgecolors="b")</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/e282cfec163409d943ab73e2fc1cae43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*yimLrR4HhdticjXmlOuZzg.png"/></div></figure><p id="d408" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了实现 PCA 算法，PCA 还包括两个计算步骤，我们将对其中一个步骤进行编码以计算协方差矩阵，并对另一个步骤使用 numpy 库以获得特征向量。</p><p id="ada0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在此之前，需要进行特征归一化，以确保数据在同一范围内。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="9677" class="mc md it ly b gy me mf l mg mh">def featureNormalize(X):<br/>    """<br/>    Returns a normalized version of X where the mean value of each feature is 0 and the standard deviation is 1.<br/>    """<br/>    mu = np.mean(X,axis=0)<br/>    sigma = np.std(X,axis=0)<br/>    <br/>    X_norm = (X - mu)/sigma<br/>    <br/>    return X_norm, mu , sigma</span><span id="51c6" class="mc md it ly b gy mi mf l mg mh">def pca(X):<br/>    """<br/>    Computes eigenvectors of the covariance matrix of X<br/>    """<br/>    m,n = X.shape[0], X.shape[1]<br/>    <br/>    sigma = 1/m * X.T @ X<br/>    <br/>    U,S,V = svd(sigma)<br/>    <br/>    return U,S,V</span></pre><p id="0271" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><code class="fe mj mk ml ly b">np.linalg.svd</code>类似于 matlab 中的<code class="fe mj mk ml ly b">svd</code>函数，返回相同的 U、S、V 矩阵。官方文档可以在<a class="ae ms" href="https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.svd.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="7a6a" class="mc md it ly b gy me mf l mg mh">from numpy.linalg import svd<br/>X_norm,mu,std = featureNormalize(X3)<br/>U,S = pca(X_norm)[:2]</span><span id="9f3c" class="mc md it ly b gy mi mf l mg mh">plt.scatter(X3[:,0],X3[:,1],marker="o",facecolors="none",edgecolors="b")<br/>plt.plot([mu[0],(mu+1.5*S[0]*U[:,0].T)[0]],[mu[1],(mu+1.5*S[0]*U[:,0].T)[1]],color="black",linewidth=3)<br/>plt.plot([mu[0],(mu+1.5*S[1]*U[:,1].T)[0]],[mu[1],(mu+1.5*S[1]*U[:,1].T)[1]],color="black",linewidth=3)<br/>plt.xlim(-1,7)<br/>plt.ylim(2,8)</span></pre><p id="aa7a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的代码块在数据集上实现 PCA，并可视化数据上的特征向量。我发现<a class="ae ms" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">维基百科</a>为大多数学习算法提供了很好的信息来源，如果你想更深入地研究算法，绝对值得一看。</p><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/77e71d522e607ac797c470c22a87ad57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*w2_FUuFASMm-n__YXZoctg.png"/></div></figure><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="d233" class="mc md it ly b gy me mf l mg mh">print("Top eigenvector U(:,1) =:",U[:,0])`</span></pre><p id="f33c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印报表打印:<code class="fe mj mk ml ly b">Top eigenvector U(:,1) =: [-0.70710678 -0.70710678]</code></p><p id="666e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了减少数据集的维度，我们将数据投影到找到的主成分(特征向量)上。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="f7ce" class="mc md it ly b gy me mf l mg mh">def projectData(X, U, K):<br/>    """<br/>    Computes the reduced data representation when projecting only on to the top k eigenvectors<br/>    """<br/>    m = X.shape[0]<br/>    U_reduced = U[:,:K]<br/>    Z = np.zeros((m,K))<br/>    <br/>    for i in range(m):<br/>        for j in range(K):<br/>            Z[i,j] = X[i,:] @ U_reduced[:,j]<br/>    <br/>    return Z</span><span id="51a7" class="mc md it ly b gy mi mf l mg mh"># Project the data onto K=1 dimension<br/>K=1<br/>Z = projectData(X_norm, U, K)<br/>print("Projection of the first example:",Z[0][0])</span></pre><p id="e77d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印语句将打印:<code class="fe mj mk ml ly b">Projection of the first example: 1.4963126084578515</code></p><p id="83a7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">也可以通过将数据投影回原始维度空间来近似重构数据。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="8dc1" class="mc md it ly b gy me mf l mg mh">def recoverData(Z, U, K):<br/>    """<br/>    Recovers an approximation of the original data when using the projected data<br/>    """<br/>    m,n = Z.shape[0],U.shape[0]<br/>    X_rec = np.zeros((m,n))<br/>    U_reduced = U[:,:K]<br/>    <br/>    for i in range(m):<br/>        X_rec[i,:] = Z[i,:] @ U_reduced.T<br/>    <br/>    return X_rec</span><span id="0435" class="mc md it ly b gy mi mf l mg mh">X_rec  = recoverData(Z, U, K)<br/>print("Approximation of the first example:",X_rec[0,:])</span></pre><p id="99ed" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印语句将打印:<code class="fe mj mk ml ly b">Approximation of the first example: [-1.05805279 -1.05805279]</code></p><p id="f267" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了可视化整个过程，</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="341a" class="mc md it ly b gy me mf l mg mh">plt.scatter(X_norm[:,0],X_norm[:,1],marker="o",label="Original",facecolors="none",edgecolors="b",s=15)<br/>plt.scatter(X_rec[:,0],X_rec[:,1],marker="o",label="Approximation",facecolors="none",edgecolors="r",s=15)<br/>plt.title("The Normalized and Projected Data after PCA")<br/>plt.legend()</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/d6e473bbdb421dfaea1ce0eb9bd36659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*Ito95Z7pedFgHkxIISY2Ug.png"/></div></figure></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="1358" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">最后，我们转向更复杂的数据集——人脸图像数据集。为了加载和可视化数据，</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="ba2c" class="mc md it ly b gy me mf l mg mh">mat4 = loadmat("ex7faces.mat")<br/>X4 = mat4["X"]</span><span id="5988" class="mc md it ly b gy mi mf l mg mh">fig, ax = plt.subplots(nrows=10,ncols=10,figsize=(8,8))<br/>for i in range(0,100,10):<br/>    for j in range(10):<br/>        ax[int(i/10),j].imshow(X4[i+j,:].reshape(32,32,order="F"),cmap="gray")<br/>        ax[int(i/10),j].axis("off")</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/226d421f1167d316a2804235bc908b45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*Gp0MBfNKixLGpinGKa3pPg.png"/></div></figure><p id="e44c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这些图像包含 32 X 32 像素的灰度，导致 1，024 个特征的维度，我们的任务是将维度减少到大约 100 个最能描述我们数据的主成分。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="78f4" class="mc md it ly b gy me mf l mg mh">X_norm2 = featureNormalize(X4)[0]</span><span id="bb88" class="mc md it ly b gy mi mf l mg mh"># Run PCA<br/>U2 =pca(X_norm2)[0]</span><span id="17df" class="mc md it ly b gy mi mf l mg mh">#Visualize the top 36 eigenvectors found<br/>U_reduced = U2[:,:36].T<br/>fig2, ax2 = plt.subplots(6,6,figsize=(8,8))<br/>for i in range(0,36,6):<br/>    for j in range(6):<br/>        ax2[int(i/6),j].imshow(U_reduced[i+j,:].reshape(32,32,order="F"),cmap="gray")<br/>        ax2[int(i/6),j].axis("off")</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/f7d71cba6d79c16968560c8cc131994a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*LUWIPa1wzV6i9LC38aQGZw.png"/></div></figure><p id="855c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面是描述数据集中最大变化的 36 个主成分的可视化。</p><p id="6c1f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我们将数据投射到前 100 个主成分上，有效地将维度降低到 100，恢复数据并尝试理解在维度降低过程中丢失了什么。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="35a3" class="mc md it ly b gy me mf l mg mh">K2 = 100<br/>Z2 = projectData(X_norm2, U2, K2)<br/>print("The projected data Z has a size of:",Z2.shape)</span><span id="33d1" class="mc md it ly b gy mi mf l mg mh"># Data reconstruction<br/>X_rec2  = recoverData(Z2, U2, K2)</span><span id="a599" class="mc md it ly b gy mi mf l mg mh"># Visualize the reconstructed data<br/>fig3, ax3 = plt.subplots(10,10,figsize=(8,8))<br/>for i in range(0,100,10):<br/>    for j in range(10):<br/>        ax3[int(i/10),j].imshow(X_rec2[i+j,:].reshape(32,32,order="F"),cmap="gray")<br/>        ax3[int(i/10),j].axis("off")</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/10913bcd0f814b78ad9103b64e2cc762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*F0qDNJSsX7T5qqX0EpcndA.png"/></div></figure></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="d174" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这是无监督学习的终结。请期待该系列的最后一篇文章。Jupyter 笔记本会上传到我的 GitHub 上(<a class="ae ms" href="https://github.com/Benlau93/Machine-Learning-by-Andrew-Ng-in-Python" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/Ben lau 93/Machine-Learning-by-Andrew-Ng-in-Python</a>)。</p><p id="5d56" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于本系列中的其他 python 实现，</p><ul class=""><li id="4ab9" class="my mz it kh b ki kj km kn kq na ku nb ky nc lc nd ne nf ng bi translated"><a class="ae ms" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137" rel="noopener">线性回归</a></li><li id="71e4" class="my mz it kh b ki nh km ni kq nj ku nk ky nl lc nd ne nf ng bi translated"><a class="ae ms" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-logistic-regression-c0ae25509feb" rel="noopener">逻辑回归</a></li><li id="acb3" class="my mz it kh b ki nh km ni kq nj ku nk ky nl lc nd ne nf ng bi translated"><a class="ae ms" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-regularized-logistic-regression-lasso-regression-721f311130fb" rel="noopener">正则化逻辑回归</a></li><li id="d3d9" class="my mz it kh b ki nh km ni kq nj ku nk ky nl lc nd ne nf ng bi translated"><a class="ae ms" rel="noopener" target="_blank" href="/andrew-ngs-machine-learning-course-in-python-neural-networks-e526b41fdcd9">神经网络</a></li><li id="8969" class="my mz it kh b ki nh km ni kq nj ku nk ky nl lc nd ne nf ng bi translated"><a class="ae ms" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-support-vector-machines-435fc34b7bf9" rel="noopener">支持向量机</a></li><li id="9322" class="my mz it kh b ki nh km ni kq nj ku nk ky nl lc nd ne nf ng bi translated"><a class="ae ms" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-anomaly-detection-1233d23dba95" rel="noopener">异常检测</a></li></ul><p id="a72e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">感谢您的阅读。</p></div></div>    
</body>
</html>