# 衡量绩效:准确性

> 原文：<https://towardsdatascience.com/measuring-performance-accuracy-db54b6dca250?source=collection_archive---------14----------------------->

![](img/53d27b697ed9b1f537ee18d0df08641d.png)

这篇文章开始了一个关于性能指标的新系列，并简要讨论了最广为人知的方法:准确度= #正确/总数。要点:当数据集平衡时，准确性是最有用的。

考虑一组猫和狗的照片。您想要建立一个模型来确定照片是猫还是狗。假设你有一个总是猜“猫”的天真模型。下面是准确性如何根据数据的偏斜而变化:

正如你所看到的，如果数据集包含的猫的照片比狗的照片多得多，一个总是猜测“猫”的愚蠢模型的准确性仍然很高。这个模型对猫长什么样、狗长什么样一无所知，但如果只看精度，我们可能会认为模型表现不错。

在不平衡数据集上训练机器学习模型的一个风险是，该模型可能会学习总是输出多数类作为其预测。这不是一个有用的模型，但它会达到很高的精度。

以下是关于如何使用准确性来判断机器学习模型的提示:

*   如果你的类是平衡的(相同数量的猫和狗),那么准确度是一个有用的性能指标
*   如果你的类是不平衡的，那么计算“朴素模型准确度”为“多数类中的实例数/总实例数”然后，当你查看你的机器学习模型的准确性时，你可以将其与天真模型的准确性进行比较。
*   如果你有 80 张猫的照片，20 张狗的照片，你的天真准确率是 80%；因此，在这个数据集上达到 80%的机器学习模型并不比天真的模型做得更好
*   另一方面，一个达到 95%的机器学习模型学到了东西！
*   如果您的类别不平衡，您还应该计算比准确度更能提供信息的性能指标，例如接收器操作特征下的面积(也称为 AUROC、AUC、c-statistic)或精确度-召回曲线下的面积(AUPRC)

为了准确就是这样！本系列关于性能指标的后续文章将讨论 AUROC 和 AUPRC。

图片来源:[小猫](https://en.wikipedia.org/wiki/File:Red_Kitten_01.jpg)、[小狗](https://commons.wikimedia.org/wiki/File:Golde33443.jpg)

*原载于 2019 年 2 月 16 日*[*http://glassboxmedicine.com*](https://glassboxmedicine.com/2019/02/16/measuring-performance-accuracy/)*。*