<html>
<head>
<title>Backpropagation in simple terms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单来说就是反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/backpropagation-in-simple-terms-8df312471d32?source=collection_archive---------18-----------------------#2019-05-03">https://towardsdatascience.com/backpropagation-in-simple-terms-8df312471d32?source=collection_archive---------18-----------------------#2019-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="dc40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着我对神经网络(NN)的了解，我努力理解反向传播在做什么以及为什么它有意义。这篇文章是<a class="ae kl" href="https://medium.com/@fjulozada/neural-networks-demystified-487a54d2a8db" rel="noopener">神经网络揭秘</a>故事的后续。所以，如果你不知道什么是梯度下降，什么是正向传播，我建议你去查一下。</p><h2 id="087e" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">为什么我们关心反向传播？</h2><p id="1372" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">我们关心反向传播，因为这是神经网络学习的方式！具体来说，这就是我们如何更新神经网络<strong class="jp ir"> w </strong>和<strong class="jp ir"> b </strong>的权重:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/fb2b8b922130f44278bb85b1c29ba7b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*t_p-vUrNz5Nq2C6X46i0DA.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Rule for updating weights</figcaption></figure><p id="3cac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> α </strong>是某个被称为学习率的正实数，它表示<strong class="jp ir"> w </strong>和<strong class="jp ir"> b </strong>的更新幅度，而<strong class="jp ir"> dw、db </strong>分别是成本函数相对于<strong class="jp ir"> w </strong>和<strong class="jp ir"> b </strong>的斜率。</p><h2 id="434d" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">为什么有效？</h2><p id="16d5" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">为了建立我们的直觉，假设我们想要找到绝对值函数的最小值:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/77737bf3f7eb4e5cb07b345beb3300c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/0*SUEI4n5U637jj_GZ.gif"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Absolute value function</figcaption></figure><p id="30e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于那些微积分新手来说，为了找到一个函数在给定点的斜率，只要问自己:响应于<strong class="jp ir"> x </strong>的变化，<strong class="jp ir"> f(x) </strong>变化了多少？注意，对于所有小于零的<strong class="jp ir"> x </strong>值，斜率为<strong class="jp ir"> -1 </strong>，对于所有大于零的<strong class="jp ir"> x </strong>值，斜率为<strong class="jp ir"> +1 </strong>，当<strong class="jp ir"> x </strong>为零时，斜率未定义。思考一下更新<strong class="jp ir"> x </strong>的值以最小化<strong class="jp ir"> f(x) = |x| </strong>的规则。保持这种想法，现在让我们使用反向传播中使用的相同更新规则来更新<strong class="jp ir"> x </strong>:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/bdb3bbebaa90cce5a745b1a0a28db581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*R8utyg-WWgsX1BH8o44kcg.png"/></div></figure><p id="de90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据反向传播更新规则，如果<strong class="jp ir"> x </strong>为负(即<strong class="jp ir"> x + α </strong>)，我们希望将<strong class="jp ir"> x </strong>向右移动；如果<strong class="jp ir"> x </strong>为正(即<strong class="jp ir"> x — α </strong>)，我们希望将<strong class="jp ir"> x </strong>向左移动。希望这也是你自己想出的最小化<strong class="jp ir"> f(x) </strong>的策略！</p><h2 id="d459" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">我们到底是怎么计算 dw 和 db 的？</h2><p id="44b1" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">假设我们有一个用于分类的简单 NN，只有一个隐藏层，我们的模型成本函数是每个训练示例的平均损失，我们的损失函数是用于逻辑回归的损失函数:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi gj"><img src="../Images/497f0ea862d30b93e563dfba246a3f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gb98HH4SfilQJ59hQ0fSNA.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Cost and Loss functions</figcaption></figure><p id="e1bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<strong class="jp ir"> yʲ </strong>是<strong class="jp ir"> jᵗʰ </strong>例子的观测值，<strong class="jp ir"> aʲ </strong>是我们的 NN 为<strong class="jp ir"> jᵗʰ </strong>例子产生的输出。此外，我们模型的输出是 sigmoid 函数的结果，sigmoid 函数使用输入要素与权重的线性组合:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/94bbca03459da17b74bb72bb48d44c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*HcZHEYtDnenY3t8wQYsARQ.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Activation (sigmoid) and linear combination functions</figcaption></figure><p id="1783" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最终，我们想要的是成本函数相对于权重的斜率，因为权重是我们唯一可以改变以最小化成本的东西。用数学术语来说，我们可以把它表达为:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi md"><img src="../Images/cc3263321deaafeb2d256e2b027da658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*0PJuyZYAm1EU4k08GNdhEA.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Slope of the cost function with regards to the weights and the intercept</figcaption></figure><p id="a30f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同时计算<strong class="jp ir"> dw </strong>和<strong class="jp ir"> db </strong>可能会令人望而生畏，因此我们将使用一种众所周知的技术，称为<strong class="jp ir">链式法则</strong>。</p><h2 id="5921" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">什么是链式法则，在 NNs 中是如何使用的？</h2><p id="6527" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">一个例子可以清楚地说明这一点:假设我们有一些初始值、中间值和最终值，我们想通过只改变 w₁和 w₂来最小化 l</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi me"><img src="../Images/b5f5d21f201fba144248a1d48ed4bad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*TZb5fZJ7GDCV3ASt7N9nPw.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Example set-up</figcaption></figure><p id="72a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先我们需要分别计算出<strong class="jp ir"> L </strong>相对于<strong class="jp ir"> w₁ </strong>和<strong class="jp ir">w₂</strong>的斜率，从而找到<strong class="jp ir"> dw₁ </strong>和<strong class="jp ir"> dw₂ </strong>。然后，我们可以将反向传播更新规则应用于<strong class="jp ir"> w₁ </strong>和<strong class="jp ir"> w₂ </strong>。只看这个例子，我们知道<strong class="jp ir"> w₁ </strong>每改变一个单位，<strong class="jp ir"> L </strong>就会改变 2.5 个单位。同样，<strong class="jp ir"> w₂ </strong>每改变一个单位，<strong class="jp ir"> L </strong>将改变 1 个单位。如果你不相信我，试着自己做出改变。事实证明，同样的计算可以表示为中间偏导数的乘法:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/29a9422b57e3a743be979dc909169c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*VuJFQTqU8_VxREGUuppq3Q.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Chain rule</figcaption></figure><p id="0580" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以现在，回到 NN 的，我们可以将<strong class="jp ir"> dw </strong>和<strong class="jp ir"> db </strong>表示为导致<strong class="jp ir"> w </strong>和<strong class="jp ir"> b </strong>的偏导数的乘积:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/223f422de7f42c5f09cc1e9ff8e1c048.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*q-Na6PxUTwrfNDOfXBbO8Q.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Chain rule in NN's</figcaption></figure><p id="d689" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们可以计算每一项，并用链式法则将它们相乘，得到<strong class="jp ir"> dw </strong>和<strong class="jp ir"> db </strong>。我知道每一项的实际解看起来很难看，但至少比试图做一次大规模求导要好(如果你有兴趣看每一项的求导，请向下滚动)。</p><p id="db75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望这篇文章有助于驱散有时围绕反向传播的迷雾。请记住，我们所做的一切都是应用链式法则来计算我们应该如何更新我们的权重，这是神经网络学习过程的核心。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mh"><img src="../Images/d16d9f9f1a1a10c9f6aea0ebddcab1d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0_cHAb1VyjsesfvHNOOkw.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Derivation of each of the partial derivatives</figcaption></figure></div></div>    
</body>
</html>