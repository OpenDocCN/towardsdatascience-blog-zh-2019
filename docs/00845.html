<html>
<head>
<title>Supervised Machine Learning: Feature Engineering and Hyper Parameter Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督机器学习:特征工程和超参数调整</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/supervised-machine-learning-feature-engineering-and-hyper-parameter-tuning-a3da583dd7b9?source=collection_archive---------16-----------------------#2019-02-08">https://towardsdatascience.com/supervised-machine-learning-feature-engineering-and-hyper-parameter-tuning-a3da583dd7b9?source=collection_archive---------16-----------------------#2019-02-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="45da" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">特征工程</strong>是使用数据的领域知识来创建使机器学习算法工作的特征的过程。<strong class="ak">超参数优化</strong>或调谐是为学习算法选择一组最佳超参数的问题。与选择特定的模型相比，这些对模型验证的影响更大。在这里，我提供了一步一步的特征工程和超参数调整概述。</h2></div><p id="2f68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们深入探讨特征工程和超参数调整步骤。在我们开始更好地理解模型验证之前，看看我以前的博客。<a class="ae lb" rel="noopener" target="_blank" href="/supervised-machine-learning-model-validation-a-step-by-step-approach-771109ae0253"> <em class="lc">监督机器学习:模型验证，一步一步逼近</em> </a> <em class="lc">。</em></p><p id="b0be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将使用来自<strong class="kh ir">驱动数据</strong>的数据集。数据集用于从<strong class="kh ir"> Taarifa </strong>和<strong class="kh ir">坦桑尼亚水利部</strong>的 水位表中<a class="ae lb" href="https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/" rel="noopener ugc nofollow" target="_blank"> <em class="lc">数据挖掘。这是一个具有高基数特征的多类分类问题。</em></a></p><h2 id="40d9" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated"><strong class="ak">特色工程</strong></h2><p id="2f71" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">正如我之前的<a class="ae lb" rel="noopener" target="_blank" href="/supervised-machine-learning-model-validation-a-step-by-step-approach-771109ae0253"> <em class="lc">博客</em> </a>中提到的。<strong class="kh ir">特征工程</strong>是指识别独立特征和从属特征之间的关系。然后，我们可以将识别的关系添加为多项式或交互特征。</p><blockquote class="mb mc md"><p id="5665" class="kf kg lc kh b ki kj jr kk kl km ju kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated">特征工程步骤是连续迭代的入口点。这是一个关键步骤，与模型验证相比，它在预测中起着更大的作用。</p></blockquote><p id="4987" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">手头问题的领域知识将对特征工程有很大的用处。在这里，我将为新的识别特性提供一些思想模型。</p><p id="3302" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们先看一下数据。似乎存在具有高基数和 NaN 值的要素。</p><div class="mh mi mj mk gt ab cb"><figure class="ml mm mn mo mp mq mr paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><img src="../Images/d908f670f77f7ec25eb1cc8cd2db925d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*khAdVtgIuw001ITwItCOyg.png"/></div></figure><figure class="ml mm my mo mp mq mr paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><img src="../Images/e895bc4014951f73ccc3e22667bac4fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*Bcg13Bmnrdzr3CwViLqQwg.png"/></div></figure></div><p id="06ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果需要，我们可以删除具有高基数和 NaN 值的列。同样对于数字特征，我们可以用其平均值、中值或众数来填充 NaN 值，而不是丢弃它。同样，对于分类特征，我们可以将 NaN 归类为一个单独的类别。</p><p id="5ffa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样基于地下水位数据集，我们可以设计一些特征，如:</p><ul class=""><li id="73d9" class="mz na iq kh b ki kj kl km ko nb ks nc kw nd la ne nf ng nh bi translated"><strong class="kh ir">经度</strong>和<strong class="kh ir">纬度</strong>中的 NaN 值用其平均值更新。</li></ul><figure class="mh mi mj mk gt mm"><div class="bz fp l di"><div class="ni nj l"/></div></figure><ul class=""><li id="cd45" class="mz na iq kh b ki kj kl km ko nb ks nc kw nd la ne nf ng nh bi translated">识别<strong class="kh ir">出资方</strong>是否至少出资了 5 台泵的二元特征。同样确定<strong class="kh ir">安装人员</strong>是否安装了至少 5 台泵。</li></ul><figure class="mh mi mj mk gt mm"><div class="bz fp l di"><div class="ni nj l"/></div></figure><ul class=""><li id="5a0a" class="mz na iq kh b ki kj kl km ko nb ks nc kw nd la ne nf ng nh bi translated">基于前缀对高基数的分类特征数据进行分组。更好的方法是详细分析数据集，并尝试根据某种逻辑结构对它们进行分组。</li></ul><figure class="mh mi mj mk gt mm"><div class="bz fp l di"><div class="ni nj l"/></div></figure><ul class=""><li id="b7a6" class="mz na iq kh b ki kj kl km ko nb ks nc kw nd la ne nf ng nh bi translated">根据同一病房其他泵的中值更新<strong class="kh ir">建造 _ 年份</strong>。</li></ul><figure class="mh mi mj mk gt mm"><div class="bz fp l di"><div class="ni nj l"/></div></figure><ul class=""><li id="a45d" class="mz na iq kh b ki kj kl km ko nb ks nc kw nd la ne nf ng nh bi translated">将<strong class="kh ir">日期记录</strong>拆分为<strong class="kh ir">年记录</strong>和<strong class="kh ir">月记录</strong>。甚至分组在不同箱中。</li></ul><figure class="mh mi mj mk gt mm"><div class="bz fp l di"><div class="ni nj l"/></div></figure><ul class=""><li id="0b47" class="mz na iq kh b ki kj kl km ko nb ks nc kw nd la ne nf ng nh bi translated">根据<strong class="kh ir">建造年份</strong>和<strong class="kh ir">记录年份</strong>计算泵的年龄。</li></ul><figure class="mh mi mj mk gt mm"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h2 id="96f0" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">超参数调谐</h2><p id="51ed" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated"><strong class="kh ir">超参数优化</strong>或<strong class="kh ir">调整</strong>是为学习算法选择一组最优超参数的问题。同一种机器学习模型可能需要不同的约束、权重或学习速率来概括不同的数据模式。这些措施被称为超参数，必须进行调整，以便模型可以最优地解决机器学习问题。超参数优化找到超参数元组，该元组产生最佳模型，该模型最小化给定独立数据上的预定义损失函数。目标函数采用一组超参数并返回相关损失。交叉验证通常用于评估这种泛化性能。</p><p id="fff0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来源:<a class="ae lb" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" rel="noopener ugc nofollow" target="_blank"> <em class="lc">维基百科</em> </a></p><blockquote class="mb mc md"><p id="7667" class="kf kg lc kh b ki kj jr kk kl km ju kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated">简而言之，超参数调优意味着找到参数的最佳值。作为其中的一部分，我们选择了一系列值来调整参数。然后根据交叉验证结果确定哪个值更合适。然后进入下一组模型参数。超参数调整是一个微妙而繁琐的比较和排除过程。</p></blockquote><p id="ec16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们用<a class="ae lb" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" rel="noopener ugc nofollow" target="_blank"><em class="lc">xgb classifier</em></a>进行分类。<a class="ae lb" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn-model-selection-gridsearchcv" rel="noopener ugc nofollow" target="_blank"> <em class="lc"> GridSearchCV </em> </a>用于与独立测试数据集的交叉验证。为了简洁起见，让我们跳过管道和 param_grid 创建的细节。详情参考之前的博客。这里我来举例说明<strong class="kh ir"> XGBClassifier </strong>的参数<strong class="kh ir"> max_depth </strong>和<strong class="kh ir"> min_child_weight </strong>的调整。</p><p id="a3eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> max_depth (int) — </strong>基础学习者的最大树深度。</p><ul class=""><li id="b497" class="mz na iq kh b ki kj kl km ko nb ks nc kw nd la ne nf ng nh bi translated">用于控制过度拟合，因为较高的深度将允许模型学习特定样本的特定关系。</li><li id="45d0" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la ne nf ng nh bi translated">典型值:3–9</li></ul><p id="f35f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> min_child_weight (int) </strong> —一个孩子所需实例重量的最小总和(hessian)。</p><ul class=""><li id="86fc" class="mz na iq kh b ki kj kl km ko nb ks nc kw nd la ne nf ng nh bi translated">用于控制过度拟合。较高的值会阻止模型学习可能高度特定于为树选择的特定样本的关系。</li><li id="ee31" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la ne nf ng nh bi translated">过高的值会导致拟合不足，因此应该使用 CV 进行调整。典型值:1–5</li></ul><figure class="mh mi mj mk gt mm"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="5917" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参数调整的第一次迭代的输出:</p><ul class=""><li id="3afe" class="mz na iq kh b ki kj kl km ko nb ks nc kw nd la ne nf ng nh bi translated"><strong class="kh ir">交叉验证得分:</strong>0。46860 . 66866868661</li><li id="b690" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la ne nf ng nh bi translated"><strong class="kh ir">最佳参数:</strong>{ ' xgb classifier _ _ max _ depth ':5，' xgb classifier _ _ min _ child _ weight ':1 }</li></ul><p id="2360" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第一次迭代中，我们以两步递增参数值。因此，在第二次迭代中，让我们进一步缩小参数范围并进行检查。</p><figure class="mh mi mj mk gt mm"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="5297" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第二次迭代期间，<strong class="kh ir"> max_depth 的最佳值是 5 </strong>。现在是<strong class="kh ir"> 6 </strong>。所以在第三次迭代中，让我们再次确认 max_depth 的值。现在让我们将<strong class="kh ir">最小 _ 子 _ 重量</strong>参数固定为 1。</p><figure class="mh mi mj mk gt mm"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="d708" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参数调整的第三次迭代的输出:</p><ul class=""><li id="36dd" class="mz na iq kh b ki kj kl km ko nb ks nc kw nd la ne nf ng nh bi translated">交叉验证分数:0。46860 . 68686868661</li><li id="282d" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la ne nf ng nh bi translated"><strong class="kh ir">最佳参数:</strong>{ ' xgb classifier _ _ max _ depth ':6，' xgb classifier _ _ min _ child _ weight ':1 }</li></ul><p id="9666" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从第三次迭代中，我们可以确定<strong class="kh ir">最大深度</strong>的值为<strong class="kh ir"> 6 </strong>。以同样的方式，我们可以选择其他参数并调整它们的优化值。</p><figure class="mh mi mj mk gt mm gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7b4a9483fb1e01e4c2b32b9cafc21f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*cHT34IMn9igbAEGLPluO0Q.png"/></div></figure><p id="642e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注:本博客旨在提供关于特征工程和超参数调整的快速介绍。这里的想法是理解过程，所以代码没有优化。使用这种方法来设置基线指标得分。提高我们每次迭代的模型验证分数。</p><p id="9632" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在此 获取完整笔记本<a class="ae lb" href="https://github.com/ShreyasJothish/blognotebooks/blob/master/DS1_Predictive_Modeling_Challenge_Parameter_Tuning.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="lc">。</em></a></p><p id="46dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可能会觉得我的其他博客很有趣。一定要去看看。</p><p id="851a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/analyse-retweet-ratio-to-determine-social-influence-d83bda0559d"> <em class="lc">分析转发率，确定社会影响力。</em>T29】</a></p><p id="ba66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/why-are-we-letting-trumps-tweet-affect-our-mood-7f9baafae3a7"> <em class="lc">为什么我们要让特朗普的推文影响我们的心情？</em> </a></p><h2 id="ca40" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">参考资料:</h2><div class="nq nr gp gr ns nt"><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd ir gy z fp ny fr fs nz fu fw ip bi translated">XGBoost 中参数调整的完整指南(带 Python 代码)</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">如果在预测建模中事情没有按照你的方式发展，使用 XGboost。XGBoost 算法已经成为终极…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh mw nt"/></div></div></a></div></div></div>    
</body>
</html>