<html>
<head>
<title>ESPNetv2 for Semantic Segmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于语义分段的 ESPNetv2</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/espnetv2-for-semantic-segmentation-9e80f155d522?source=collection_archive---------15-----------------------#2019-06-07">https://towardsdatascience.com/espnetv2-for-semantic-segmentation-9e80f155d522?source=collection_archive---------15-----------------------#2019-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="62d4" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">如今，许多现实世界的应用，如自动驾驶汽车，都涉及到视觉场景理解。语义分割是为视觉场景理解开辟道路的主要任务之一。然而，这是计算机视觉中计算量最大的任务之一。本文概述了在<a class="ae ks" href="https://arxiv.org/abs/1811.11431" rel="noopener ugc nofollow" target="_blank"> ESPNetv2 </a>论文中使用的高效语义分割网络。</p></blockquote><p id="8e08" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated"><strong class="jw iu">ESPNet vs ESPNet v2:</strong>ESPNet v2(在 CVPR 19 上被接受)是一个通用架构，可以用于建模<em class="jv">可视化</em>和<em class="jv">顺序</em>数据。<a class="ae ks" href="https://arxiv.org/pdf/1811.11431.pdf" rel="noopener ugc nofollow" target="_blank"> ESPNetv2 </a>用深度扩展的可分离卷积扩展了<a class="ae ks" href="https://arxiv.org/abs/1803.06815" rel="noopener ugc nofollow" target="_blank"> ESPNet </a>(在 ECCV'18 接受)并将其推广到不同的任务，包括<em class="jv">图像分类、对象检测、语义分割、</em>和<em class="jv">语言建模</em>。</p><p id="0f8e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated"><strong class="jw iu">源代码:</strong>我们的源代码以及不同数据集上的预训练模型可在<strong class="jw iu"><em class="jv"/></strong><a class="ae ks" href="https://github.com/sacmehta/EdgeNets" rel="noopener ugc nofollow" target="_blank"><strong class="jw iu"><em class="jv">Github</em></strong></a>上获得。</p></div><div class="ab cl kw kx hx ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="im in io ip iq"><h1 id="6dee" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">语义分割</h1><p id="287f" class="pw-post-body-paragraph jt ju it jw b jx mb jz ka kb mc kd ke kt md kh ki ku me kl km kv mf kp kq kr im bi translated">语义分割是一个细粒度的推理任务，它预测图像中每个像素的标签。<em class="jv">前景-背景</em>和<em class="jv">全场景分割</em>任务示例如下。</p><div class="mg mh mi mj gt ab cb"><figure class="mk ml mm mn mo mp mq paragraph-image"><img src="../Images/f1f81ae47f8f3eca7f7456138ae21743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*-gfo7jybQLx6qYAKeaOMrA.jpeg"/></figure><figure class="mk ml mm mn mo mp mq paragraph-image"><img src="../Images/87d3d71ff5180d00977960b3b54823e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*OOWlND11F9vhhMw0d5RCCQ.png"/></figure></div><div class="ab cb"><figure class="mk ml mm mn mo mp mq paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><img src="../Images/38cb2818230cc26ce4b38fbca829a160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Qey6q3SCllXbSny1A1fSwA.jpeg"/></div></figure><figure class="mk ml mm mn mo mp mq paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><img src="../Images/6be6b0b2a7a92e72b302ba149be03ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*WGke38syTRv_w_6XGclQqg.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk nb di nc nd"><strong class="bd ne">Figure 1:</strong> Top row visualizes a foreground-background segmentation task (e.g. The <a class="ae ks" href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="noopener ugc nofollow" target="_blank">PASCAL VOC</a> dateset) while the bottom row visualizes a full scene segmentation task (e.g. The <a class="ae ks" href="https://www.cityscapes-dataset.com/" rel="noopener ugc nofollow" target="_blank">Cityscapes</a> dataset).</figcaption></figure></div></div><div class="ab cl kw kx hx ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="im in io ip iq"><h1 id="f793" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">编码器-解码器网络概述</h1><p id="2db6" class="pw-post-body-paragraph jt ju it jw b jx mb jz ka kb mc kd ke kt md kh ki ku me kl km kv mf kp kq kr im bi translated">大部分高效的分割网络，包括<a class="ae ks" href="https://arxiv.org/abs/1606.02147" rel="noopener ugc nofollow" target="_blank"> ENet </a>和<a class="ae ks" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U-Net </a>，都采用了编解码结构。简而言之，编码器-解码器结构包括两个组件:(1)编码器和(2)解码器。编码器将 RGB 图像作为输入，并通过执行卷积和下采样操作来学习多尺度的表示。作为下采样操作的结果，空间分辨率和精细细节丢失了。解码器通过执行上采样和卷积运算来逆转这种损失。下图显示了一个普通的编码器-解码器网络。</p><figure class="mg mh mi mj gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nf"><img src="../Images/f4358bd8902b0beac2f47f984fed2bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ongboC_C4b6bZPgBnykNzQ.png"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk"><strong class="bd ne">Figure 2:</strong> A vanilla encoder-decoder network. The green boxes in encoder and the decoder represents convolutional layers while the red and orange boxes represent down-sampling and up-sampling layers respectively.</figcaption></figure><p id="1096" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">按照惯例，编码器和解码器使用跳跃连接共享信息。这些跳跃连接已经被证明非常有效。更多详情见<a class="ae ks" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U-Net </a>。</p></div><div class="ab cl kw kx hx ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="im in io ip iq"><h1 id="8c23" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">ESPNetv2 中的分段架构</h1><p id="c528" class="pw-post-body-paragraph jt ju it jw b jx mb jz ka kb mc kd ke kt md kh ki ku me kl km kv mf kp kq kr im bi translated">像 ESPNet 一样，ESPNetv2 也使用编码器-解码器架构来进行语义分段，但是，它使用更强大和高效的编码和解码块:(1)用于编码器的极其高效的扩展卷积空间金字塔(EESP)模块，以及(2)用于解码器的高效金字塔池(EPP)模块。</p><p id="9367" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated"><strong class="jw iu">编码器的 EESP 模块</strong>:为了提高计算效率，EESP 模块用高效的深度扩展卷积替换了 ESP 模块中计算量大的标准卷积层。图 3 比较了电潜泵和 e ESP 模块。有关这些区块的更多详细信息，请参阅我们的论文 ESPNetv2。</p><figure class="mg mh mi mj gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ng"><img src="../Images/fb4a2557f7dcf1023523e0973ca03356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iokrY1or22Rg6oeaKKAAPA.png"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk"><strong class="bd ne">Figure 3:</strong> Comparison between the ESP module and the EESP module. Each convolutional layer (Conv-n: n×n standard convolution, GConv-n:n×n group convolution, DConv-n: n×n dilated convolution, DDConv-n: n×n depth-wise dilated convolution) is denoted by (# input channels, # output channels, and dilation rate). HFF denotes hierarchical feature fusion. See ESPNet and ESPNetv2 papers for more details.</figcaption></figure><p id="c6ee" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated"><strong class="jw iu">解码器的 EPP 模块:</strong>子采样允许学习尺度不变表示。这些操作非常有效，是不同(和流行的)计算机视觉算法的关键组件，包括 SIFT 和卷积神经网络。为了让 ESPNetv2 能够有效地学习比例不变表示，我们引入了一个有效的金字塔池(EPP)模块，如图 4 所示。为了高效和有效，EPP 将<em class="jv"> N </em>维特征映射到低维空间，比如说<em class="jv"> M </em>维(<em class="jv">N&gt;T33】M</em>)，然后使用<em class="jv">深度卷积</em>学习<strong class="jw iu">和<em class="jv">不同尺度</em>和</strong>的表示。让我们假设我们有 b 分支。我们连接这些<em class="jv"> b </em>分支的输出，以产生一个<em class="jv"> bM </em>维空间的输出。为了便于学习更丰富的尺度间表示，我们首先打乱这些<em class="jv"> bM </em>维特征图，然后使用<em class="jv">组卷积</em>将它们融合。然后应用逐点卷积来学习在组卷积之后获得的特征图之间的线性组合。</p><p id="8061" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">请注意，EESP 和 EPP 中的基本操作是相同的，即重新采样。在 EESP 中，使用扩张卷积来实现重采样，而在 EPP 中，使用上采样和下采样操作来实现重采样。</p><figure class="mg mh mi mj gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nh"><img src="../Images/c165754b67892ef7de93dc076f115cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u_VmX_D0jPDsZLsCFW9GbQ.png"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk"><strong class="bd ne">Figure 4:</strong> EPP module allows to learn scale-invariant representations efficiently. Point-wise, depth-wise, and group-wise convolutions are represented in blue, green, and purple respectively.</figcaption></figure></div><div class="ab cl kw kx hx ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="im in io ip iq"><h1 id="0b4d" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">ESPNet 和 ESPNetv2 之间的比较</h1><p id="6097" class="pw-post-body-paragraph jt ju it jw b jx mb jz ka kb mc kd ke kt md kh ki ku me kl km kv mf kp kq kr im bi translated">表 1 给出了使用两个广泛使用的数据集(Cityscapes 和 PASCAL VOC 2012)的在线服务器对<strong class="jw iu"> <em class="jv">私有测试集</em> </strong>进行的定性性能比较。我们可以清楚地看到，ESPNetv2 比 ESPNet 更高效、更准确。</p><p id="0d5b" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">请注意，ESPNetv2 在图像大小为 384x384 的情况下获得了 68 的显著平均交集(mIOU)分数；为许多深度和重量级细分架构提供了具有竞争力的性能(更多详细信息，请参见<a class="ae ks" href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6" rel="noopener ugc nofollow" target="_blank">PASCAL VOC 2012 排行榜</a>)。PASCAL 数据集上广泛使用的图像大小是 512x512(或 500x500)。</p><figure class="mg mh mi mj gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ni"><img src="../Images/b5f3f514fa9fc5658a995d0427fc7d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHy3i8eMzdn3rNh6IF1fMw.png"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk"><strong class="bd ne">Table 1:</strong> The performance of the ESPNet and the ESPNetv2 is compared in terms of FLOPs and the accuracy (in terms of mean intersection over union) on the private test set of two widely used datasets.</figcaption></figure></div></div>    
</body>
</html>