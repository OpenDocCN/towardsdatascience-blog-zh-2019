<html>
<head>
<title>Deep Dive into the Computer Vision World: Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入计算机视觉世界:第 2 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-into-the-computer-vision-world-part-2-7a24efdb1a14?source=collection_archive---------21-----------------------#2019-07-16">https://towardsdatascience.com/deep-dive-into-the-computer-vision-world-part-2-7a24efdb1a14?source=collection_archive---------21-----------------------#2019-07-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="82b4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">有 CNN 的地区，让我们开始物体检测！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e623a7be1881b73190835d0d3ba99578.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*82g9vFAyax8QnkkgP4y2VQ.jpeg"/></div></div></figure><p id="f830" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是“深入计算机视觉世界”的第二个故事，该系列的完整集如下:</p><ol class=""><li id="5be2" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/deep-dive-into-the-computer-vision-world-f35cd7349e16?source=friends_link&amp;sk=449ea5da20c884dadca23b907efb7e13">从 VGG 开始，ResNet，Inception Network 和 MobileNet </a></li><li id="c6ec" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp lv lw lx ly bi translated">CNN 地区，让我们开始目标检测</li><li id="ee3b" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp lv lw lx ly bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/deep-dive-into-the-computer-vision-world-part-3-abd7fd2c64ef?source=friends_link&amp;sk=876a90f05dcef8f9f0546b42adaec42d"> YOLO，SSD 和 RetinaNet，比较统一的那些</a></li><li id="6cd2" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp lv lw lx ly bi translated">从对象检测到实例分割(TBU)</li></ol><p id="2e22" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在前一篇文章中，我们讨论了五个流行的网络，VGG、ResNet、Inception Network、Xception 和 MobileNet。这些网络现在是高级网络的主要组成部分，因此有必要了解它们的架构。</p><p id="66bf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们进入下一页。从图像分类到目标检测。到目前为止，我们讨论的是一个大型卷积神经网络。虽然它们有很多层，但最终是一个网络。但是 R-CNN 和它的变体将是我们的焦点，这个网络是它的一部分。CNN 现在是整个“系统”的一部分，处理更复杂的任务，如对象检测和图像分割。从 R-CNN 开始，我们将看到这些网络是如何转变的，以及这些变化背后的想法。</p><h1 id="aae8" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">从图像分类到目标检测</h1><p id="3fbf" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">在我们直接进入 R-CNN“家族”之前，让我们简单检查一下图像分类和图像检测的基本思想。两者有什么区别？为了检测图像中的物体，我们还需要做哪些额外的工作？</p><p id="451e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，无论我们有多少个班级，都会有一个额外的班级-背景。需要一个对象检测器来回答这个问题，“有对象吗？”，这不是图像分类的情况。第二，当有一个对象时，仅仅说“是的，有”还是不够的(想象起来相当可笑..😅)它还应该告诉我们，“物体位于哪里？”我们需要探测到的物体的位置。这听起来可能很简单，但实现起来并不容易。当我们考虑速度和效率挑战时，事情变得更加复杂。</p><p id="b1fc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，物体检测就像..寻找一个物体的区域，定位它并对它进行分类。有了这个基本概念，我们现在准备开始 R-CNN 的第二个话题。</p><h1 id="7784" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">R-CNN</h1><p id="c2e2" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">R-CNN 回答问题，<em class="nc">“在 ImageNet 上的 CNN 分类结果能在多大程度上概括为物体检测结果？”</em>所以这个网络可以说是物体检测<em class="nc">家谱</em>的开始，在神经网络的应用中有很大的重要性。基本结构由三个步骤组成:提取区域建议、计算 CNN 和分类。</p><p id="bf94" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，我们从输入图像中提取一些看起来有希望有物体的区域。R-CNN 使用选择性搜索来获得那些感兴趣的区域(ROI)。选择性搜索是一种基于像素强度分割图像的区域提议算法。如果你想深入了解选择性搜索，这里有<a class="ae lz" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">原文</strong> </a>。基本思路如下图。它首先从过度分割的图片开始，并在每个片段周围绘制一个边界框。并且基于它们在颜色、纹理、大小和形状兼容性方面的相似性，它不断将相邻的分组并形成更大的片段。R-CNN 通过这种方法提取了大约 2000 个地区提案，并将它们提供给 CNN。这也是它被命名为 R-CNN 的原因，<em class="nc">具有 CNN 特色的区域</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/694e30f138d3540607c4392e24071270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6UfdQmJE78IdEs6q41zdXg.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><a class="ae lz" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" rel="noopener ugc nofollow" target="_blank">Selective Search for Object Recognition</a></figcaption></figure><p id="3921" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">得到候选后，第二步是进入一个大型卷积神经网络。每个提议都被调整为固定大小，并分别输入 CNN。R-CNN 用的是 AlexNet(那是 2014 年，当时还没有 ResNet 和 InceptionNet)我们从每一个提案中得到 4096 维的特征向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/f4d2e3b1fd35c0770a5c4f589bdf799c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1zG21rqVp-0Sg6U-uSE4bQ.png"/></div></div></figure><p id="9f94" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并且在最后一步，从 CNN 提取的输出被馈送到一组<em class="nc">类特定的</em>线性 SVM 模型。我们为每个类优化一个线性 SVM，并且我们得到具有得分高于阈值的边界框的输出图像。在具有重叠框的情况下，通过应用非最大抑制，只需要一个。</p><p id="7162" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">值得一提的一个有趣部分是它如何克服数据稀缺问题。研究人员面临的挑战是，仅用少量的标记数据训练如此庞大的网络。解决方案是用不同的标签数据对 CNN 进行预训练(这是监督学习)，然后用原始数据集进行微调。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi gj"><img src="../Images/2f4be67064f7363d3a8bb43e1500a8b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p_pmv_WO2y_m97BPVgEZfA.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><a class="ae lz" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" rel="noopener ugc nofollow" target="_blank">R-CNN architecture</a></figcaption></figure><p id="ccee" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您可能已经注意到了一些可以改进的低效部分。选择性搜索是计算密集型的。并且为每个 ROI 处理 CNN 是重复的工作，这再次需要大量的计算成本。对预训练过程和分离的分类器的需要是没有吸引力的。而且这些机型的存储量太大。虽然 R-CNN 是一个里程碑式的成就，但它有几个缺点需要改进。</p><h1 id="5f29" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">快速 R-CNN</h1><p id="c92d" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">快速 R-CNN 是上一部作品的下一个版本。这里发生了什么变化？重复处理卷积映射得到了改进。第一个变化发生在重复卷积层。</p><p id="2389" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设计算一个卷积网络需要<em class="nc"> N 秒。由于 R-CNN 分别向网络输入 2000 个 RoI，总处理时间将为 2000*N 秒。现在，我们不再单独处理 CNN，而是通过与所有建议共享卷积，只处理一次。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/d64b30fb244e2963c1f7271d692ddb28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uaNOCXYBcN2uD451BpZK_g.png"/></div></div></figure><p id="b0f1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如您在上面看到的，这个网络接受两个数据输入，一个原始图像和一组区域建议作为输入。整个图像通过网络前馈产生特征图。有了这个特征地图，每个区域提议通过一个汇集层和完全连接的层来创建一个特征向量。因此，卷积计算是一次完成的，而不是针对每个建议。</p><p id="91a6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并且用两个兄弟层替换多个分类器。一个是 softmax 函数，用于利用每一类的可能性估计对对象进行分类，另一个是边界框回归器，用于返回检测到的对象的坐标。所以得到的特征向量被送入这两层，我们从这两层得到结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/4e341fe52bb37238c1294ed4fb03087c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*AzkmdVnpfXfhoHKXAUviBA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk"><a class="ae lz" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">Fast R-CNN architecture</a></figcaption></figure><p id="926b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，这个模型进行了更改，以共享卷积并分离附加的分类器。通过将“多个主体”合并为一个并去掉“沉重的尾巴”，我们可以实现更少的计算和存储。这种架构允许我们一起训练所有权重，甚至包括 softmax 分类器和多元回归器的权重。这意味着传播将来回更新所有的权重。惊人的进步！然而，我们仍然有机会获得更好的表现。</p><h1 id="ba4c" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">更快的 R-CNN</h1><p id="72ae" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">然而，快速 R-CNN 很难用于实时检测。造成这种时间延迟的主要原因是选择性搜索。它存在计算瓶颈，需要用更有效的方法来替代。<em class="nc">但是如何？如果没有选择性搜索，我们如何获得区域提案？如何最大限度地利用 ConvNet 呢？</em>研究人员发现，Fast R-CNN 中的特征地图也可用于生成区域提议。因此，通过避免选择性搜索，可以开发更高效的网络。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/a43fe319f3993768feeb862bdefb4eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rP2RwzHNROX8vwVBbMEt9Q.png"/></div></div></figure><p id="1c88" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">快速 R-CNN 由两个模块组成，区域提议网络(RPN)和快速 R-CNN。我们首先输入一幅图像到一个“迷你网络”,它将输出特征图。通过在特征图上滑动小窗口，我们提取区域提议。并且每个提议被馈送到两个兄弟层，softmax 分类器和包围盒回归器。</p><p id="903b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这两层可能看起来类似于快速 R-CNN 的最后几层，但它们是为了不同的目的而衍生的。对于每个提议，分类器估计图像中对象存在的概率。回归器返回边界框的坐标。所以它们是用来产生候选者的，而不是预测实际的物体。</p><p id="12e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们只取得分高于某个阈值的建议，将它们与特征图一起输入到快速 R-CNN。以下步骤相同。它们被输入到卷积网络和 RoI 池层。最后一层将是分类器和回归器，最终预测图像中的真实对象。</p><div class="kj kk kl km gt ab cb"><figure class="nm kn nn no np nq nr paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/69bcf0acb8ed21783eced96a32e5618a.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*XyQa5bD1oT7YdDrOkU6p8Q.png"/></div></figure><figure class="nm kn ns no np nq nr paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/e7c0b61683de9b183a9a3ba1bc5452ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*lVE5BICv4IvRERUptLj3DA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk nt di nu nv"><a class="ae lz" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">Faster R-CNN architecture (left) and Region Proposal Network (right)</a></figcaption></figure></div><p id="af2f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个网络的一个重要属性是<em class="nc">平移不变量</em>，这是通过锚和它计算相对于锚的提议的方式<em class="nc">来实现的。</em> <a class="ae lz" href="https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo" rel="noopener ugc nofollow" target="_blank"> <em class="nc">什么是平移不变？</em> </a>简单来说，就是不管物体旋转、移位、大小变化什么的，我们都能检测到。图像中的对象可以位于中心或左上角。根据视角不同，同一物体可以是宽的或长的。为了防止模型因为平移而无法定位物体，我们制作了多种比例和纵横比的锚框，如上图所示。这些盒子放在推拉窗的中央。所以如果在某个位置有<strong class="kw iu"> K </strong>个盒子，我们得到<strong class="kw iu"> 2K </strong>个盒子的分数和<strong class="kw iu"> 4K </strong>坐标。</p><p id="b6f9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">总之，在区域提议网络，我们在特征图上滑动具有多个锚框的窗口，并通过分类器和回归器评估每个框。低于阈值的建议被拒绝，因此，只有有希望的建议进入下一步。</p><p id="f525" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">不仅如此，该模型通过分别微调 RPN 和 Fast R-CNN 特有的层，同时修复共享层，优化了 4 步交替训练。这允许模型共享权重，形成统一的网络，并在效率和准确性方面带来更高的性能。为了比较性能，建议更快的 R-CNN 的时间是每幅图像 10 毫秒(整个过程每秒 5 帧)，而使用 CPU 进行选择性搜索的时间是 2 秒。</p><h1 id="a675" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">减去</h1><p id="9be3" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">从 R-CNN 到更快的 R-CNN，网络得到了转变，不再依赖其他组件。R-CNN 可以通过丢弃线性支持向量机和共享卷积计算来增强。而快速 R-CNN 也改为共享卷积去除选择性搜索。通过将整个过程集成到一个网络中，我们可以实现更高的精度和更快的速度。</p><h1 id="5058" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">参考</h1><ul class=""><li id="abcf" class="lq lr it kw b kx mx la my ld nw lh nx ll ny lp nz lw lx ly bi translated">J.R.R. Uijlings 等人，<a class="ae lz" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nc">物体识别的选择性搜索</em> </a>，2012</li><li id="6de9" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp nz lw lx ly bi translated">Ross Girshick 等人，<a class="ae lz" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nc">用于精确对象检测和语义分割的丰富特征层次</em> </a>，2014 年</li><li id="23a6" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp nz lw lx ly bi translated">何等，<a class="ae lz" href="http://Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition" rel="noopener ugc nofollow" target="_blank"> <em class="nc">视觉识别深度卷积网络中的空间金字塔池</em> </a>，2015</li><li id="db25" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp nz lw lx ly bi translated">罗斯·吉希克，<a class="ae lz" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nc">快速 R-CNN </em> </a>，2015</li><li id="1576" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp nz lw lx ly bi translated">任等，<a class="ae lz" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nc">更快的 R-CNN:面向实时目标检测的区域提议网络</em> </a>，2015</li><li id="e0e6" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp nz lw lx ly bi translated">何等<a class="ae lz" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank"> <em class="nc">面具 R-CNN </em> </a>，2018</li></ul></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><p id="c007" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个故事引起你的共鸣了吗？请与我们分享您的见解。我总是乐于交谈，所以请在下面留下评论，分享你的想法。我还在 LinkedIn<a class="ae lz" href="https://www.linkedin.com/in/jiwon-jeong/" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu">上分享有趣和有用的资源，所以请随时关注并联系我。下次我会带来另一个有趣的故事。一如既往，敬请期待！</strong></a></p></div></div>    
</body>
</html>