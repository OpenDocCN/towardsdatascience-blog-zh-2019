<html>
<head>
<title>Feature Variation Explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征变化解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explain-feature-variation-employing-pca-in-scikit-learn-6711e0a5c0b7?source=collection_archive---------19-----------------------#2019-12-18">https://towardsdatascience.com/explain-feature-variation-employing-pca-in-scikit-learn-6711e0a5c0b7?source=collection_archive---------19-----------------------#2019-12-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9a9b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在 Scikit-Learn 中使用 PCA</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/a22f7f35e5a438a43bc0799abd4fdd31.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*7pvzx6eCdHFoXomqE0IuhQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Visualization of PCA results[1]</figcaption></figure><p id="d024" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">作为一种多元排序技术，主成分分析(PCA)可以对多元数据集中的因变量进行分析，以探索它们之间的关系。这导致以更少的维度显示数据点的相对位置，同时保留尽可能多的信息[2]。</p><p id="c54b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本文中，我将使用 Scikit-learn 库对 auto-mpg 数据集(来自 GitHub)应用 PCA。本文分为数据预处理、主成分分析和主成分可视化三个部分。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h2 id="7f8c" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">a)数据预处理</h2><p id="7c05" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">我们首先需要导入数据集预处理所需的库:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="531d" class="lx ly it mw b gy na nb l nc nd">import tensorflow as tf<br/>from tensorflow import keras</span><span id="860c" class="lx ly it mw b gy ne nb l nc nd">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns</span><span id="5ca6" class="lx ly it mw b gy ne nb l nc nd">!pip install -q git+https://github.com/tensorflow/docs<br/>import tensorflow_docs as tfdocs<br/>import tensorflow_docs.plots<br/>import tensorflow_docs.modeling</span></pre><p id="9be4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们可以导入数据:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="a4e6" class="lx ly it mw b gy na nb l nc nd">datapath = keras.utils.get_file(“auto-mpg.data”, “http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data")</span><span id="9b8c" class="lx ly it mw b gy ne nb l nc nd">datapath</span></pre><p id="65e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并更改列的标题:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="eb3c" class="lx ly it mw b gy na nb l nc nd">columnTitles = [‘MPG’,’Cylinders’,’Displacement’,’Horsepower’,’Weight’, ‘Acceleration’, ‘Model Year’, ‘Origin’]</span><span id="73ec" class="lx ly it mw b gy ne nb l nc nd">rawData = pd.read_csv(dataPath, names=columnTitles, na_values = “?”, comment=’\t’, sep=” “, skipinitialspace=True)</span></pre><p id="6ecf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">数据如下所示:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="28e0" class="lx ly it mw b gy na nb l nc nd">data = rawData.copy()<br/>data.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nf"><img src="../Images/2133d9e851f6d0aaf8a353f7e74ddd95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XntVT3RWnUp8TeP1jwQ-8w.png"/></div></div></figure><p id="a3c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们需要清理数据:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="35b3" class="lx ly it mw b gy na nb l nc nd">data.isna().sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nk"><img src="../Images/265a3dcb93c0b8261ab6010e0aeaed18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wXYxPGue4zFwJj8dor9aSw.png"/></div></div></figure><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="4eb2" class="lx ly it mw b gy na nb l nc nd">data = data.dropna()</span></pre><p id="559b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下一步是定义特征，将它们从响应变量中分离出来，并将其标准化为 PCA 的输入:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="e903" class="lx ly it mw b gy na nb l nc nd">from sklearn.preprocessing import StandardScaler</span><span id="3490" class="lx ly it mw b gy ne nb l nc nd">df = pd.DataFrame(data)</span><span id="d8b0" class="lx ly it mw b gy ne nb l nc nd">features = [‘Cylinders’, ‘Displacement’, ‘Horsepower’, ‘Weight’, ‘Acceleration’]</span><span id="9fc9" class="lx ly it mw b gy ne nb l nc nd">X = df.loc[:, features].values</span><span id="6f2c" class="lx ly it mw b gy ne nb l nc nd">y = df.loc[:,[‘MPG’]].values</span><span id="fa88" class="lx ly it mw b gy ne nb l nc nd">X = StandardScaler().fit_transform(X)</span></pre><h2 id="ed02" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">b)五氯苯甲醚</h2><p id="afe6" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">在 PCA 中，我们首先需要知道需要多少成分来解释至少 90%的特征变化:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="a9d6" class="lx ly it mw b gy na nb l nc nd">from sklearn.decomposition import PCA</span><span id="1c51" class="lx ly it mw b gy ne nb l nc nd">pca = PCA().fit(X)</span><span id="86a9" class="lx ly it mw b gy ne nb l nc nd">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span><span id="2af7" class="lx ly it mw b gy ne nb l nc nd">plt.xlabel(‘number of components’)</span><span id="df36" class="lx ly it mw b gy ne nb l nc nd">plt.ylabel(‘cumulative explained variance’)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nl"><img src="../Images/ec1e76e165ab1c1d2d3e139ceceb0f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ErfXyzDLIDEPomYfxZRwBA.png"/></div></div></figure><p id="c037" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本案例研究中，选择了两个组件作为组件的最佳数量。然后，我们可以开始进行 PCA:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="b66d" class="lx ly it mw b gy na nb l nc nd">from sklearn.decomposition import PCA</span><span id="7187" class="lx ly it mw b gy ne nb l nc nd">pca = PCA(n_components=2)</span><span id="d8f2" class="lx ly it mw b gy ne nb l nc nd">principalComponents = pca.fit_transform(X)</span></pre><p id="1144" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如下所示，总体而言，这两个组件解释了数据集约 95%的特征变化:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="a5d8" class="lx ly it mw b gy na nb l nc nd">pca.explained_variance_ratio_</span><span id="bf18" class="lx ly it mw b gy ne nb l nc nd">array([0.81437196, 0.13877225])</span></pre><h2 id="9aa8" class="lx ly it bd lz ma mb dn mc md me dp mf ld mg mh mi lh mj mk ml ll mm mn mo mp bi translated">c)主成分可视化</h2><p id="f28b" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">给定响应变量值(当前数据集中的 MPG ),两个主要成分可以如下所示:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="5507" class="lx ly it mw b gy na nb l nc nd">plt.scatter(principalComponents[:, 0], principalComponents[:, 1],</span><span id="56b8" class="lx ly it mw b gy ne nb l nc nd">c=data.MPG, edgecolor=’none’, alpha=0.5,</span><span id="bc88" class="lx ly it mw b gy ne nb l nc nd">cmap=plt.cm.get_cmap(‘Spectral’, 10))</span><span id="9b2b" class="lx ly it mw b gy ne nb l nc nd">plt.xlabel(‘Principal component 1’)</span><span id="f399" class="lx ly it mw b gy ne nb l nc nd">plt.ylabel(‘Pricipal component 2’)</span><span id="b82a" class="lx ly it mw b gy ne nb l nc nd">plt.colorbar()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nm"><img src="../Images/12897dc8bb22524b64375628e6c88537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQXXqiMIJvnVp-LP45w3RQ.png"/></div></div></figure></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h1 id="1d10" class="nn ly it bd lz no np nq mc nr ns nt mf jz nu ka mi kc nv kd ml kf nw kg mo nx bi translated">结论</h1><p id="555e" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">主成分分析(PCA)降低了大数据集的维数，增加了数据的可解释性，同时最小化了信息损失。它通过创建新的不相关成分来实现方差的最大化[3]。</p><h1 id="2cd7" class="nn ly it bd lz no ny nq mc nr nz nt mf jz oa ka mi kc ob kd ml kf oc kg mo nx bi translated"><strong class="ak">参考文献</strong></h1><p id="09e1" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">[1]<a class="ae od" href="https://commons.wikimedia.org/wiki/File:PCA_vs_Linear_Autoencoder.png" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:PCA _ vs _ Linear _ auto encoder . png</a></p><p id="14b8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[2] C. Syms。主成分分析。(2008)，<em class="oe">生态百科</em>，2940–2949。doi:10.1016/b978–008045405–4.00538–3。</p><p id="dd5f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[3] T. I .乔利夫和 j .卡迪马。"主成分分析:回顾与最新发展."(2016)，<em class="oe">英国皇家学会哲学汇刊 A:数学、物理与工程科学</em>，第 374 卷，第 2065 期，第 2015–2020 页，doi:10.1098/rsta.2015.0202。</p></div></div>    
</body>
</html>