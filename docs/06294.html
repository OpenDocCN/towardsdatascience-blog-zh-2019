<html>
<head>
<title>Feature Engineering- Translating the dataset to a machine ready format</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征工程-将数据集转换成机器可用的格式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-engineering-translating-the-dataset-to-a-machine-ready-format-af4788d15d6c?source=collection_archive---------23-----------------------#2019-09-10">https://towardsdatascience.com/feature-engineering-translating-the-dataset-to-a-machine-ready-format-af4788d15d6c?source=collection_archive---------23-----------------------#2019-09-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="36a9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">创建可以帮助机器学习算法工作的功能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0d03ea2c322c690c8c1006a18a3da925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t8d6pcrhaZ3GZn31Y-U0pA.png"/></div></div></figure><p id="9dc8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这一步使您的数据为最终训练做好准备。特性工程的方法基于对数据的良好理解，正如我在之前关于<a class="ae lq" rel="noopener" target="_blank" href="/exploratory-data-analysis-unravelling-a-story-with-data-b01d70069035"> EDA </a>的文章中提到的。我将继续使用 Kaggle 上的住房数据集(在上一篇文章中用于解释 EDA)，来说明我是如何对它进行特征工程的。</p><p id="7457" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">测试和训练数据集都被合并，以从特征工程开始。我按照以下步骤得出最终数据集:</p><h2 id="2156" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated"><strong class="ak"> <em class="mk">剔除异常值</em> </strong></h2><p id="8148" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">在上一篇文章中，我举例说明了如何识别异常值。第一步是删除所有带有离群值的索引，因为离群值往往会向机器传递关于数据集的不准确信息。</p><h2 id="a2e6" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">将分类变量转换为数字变量</h2><p id="99d7" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">其次，分类变量需要转换为数值变量，因为大多数机器学习算法只能读取数值。</p><p id="a611" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请注意，一些分类变量(如 Alley、GarageQual、BsmtQual)缺少一些值，而其他变量则由“Po”、“Gd”、“TA”等值填充。数据描述文件有助于解释这些变量的含义。在这种情况下，缺失的数据仅仅意味着房子可能没有小巷、车库或地下室。因此，我使用下面的代码来估算缺失值，并将现有值转换为数字值:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="5118" class="lr ls it mr b gy mv mw l mx my">combined.Alley = combined.Alley.map({np.nan:0, 'Grvl':1, 'Pave':2})<br/>combined.BsmtCond = combined.BsmtCond.map({np.nan:0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})</span><span id="9cd4" class="lr ls it mr b gy mz mw l mx my">*combined is the name of the merged dataset of train and test</span></pre><p id="2cce" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是可能的，因为赋值就像等级变量，例如“Fa”比“Po”好，“TA”比“Fa”好。</p><p id="8ef0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是对于那些值不表示等级的分类变量，pandas 的 get_dummies 函数将行转换为列。</p><h2 id="99b7" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">输入分类和数值特征的缺失值</h2><p id="ef22" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">但在此之前，重要的是估算所有分类变量的缺失值。例如，mischaracter 和 MasVnrType 缺少值，这可能表明它们不存在。有一个简单的方法来交叉检查这个。如果 MiscVal 和 MasVnrArea 是对应于缺失值的零，则假设是正确的，可以估算如下:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="c0d9" class="lr ls it mr b gy mv mw l mx my">combined.MiscFeature.fillna('NA', inplace=True)<br/>combined.MasVnrType.fillna('None', inplace=True)</span></pre><p id="5f05" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于 KitchenQual 或 SaleType 等其他特性，使用值的模式对它们进行估算更安全。</p><p id="03a7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">数字特征中缺失的值可以通过放置零来估算，具体取决于特征。例如，GarageCars 列中缺少值可能表示根本没有车库，因此零是最合适的值。</p><p id="4b66" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">GarageYrBlt 也有缺失值，我用零来估算，因为不可能猜出缺失的年份！</p><h2 id="9de9" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">处理时间变量</h2><p id="1844" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">数据集中有 5 个时间变量。将它们转换成年龄变量似乎更有意义，因为它们提供了关于特征寿命的更多信息。类似于这样的事实，陈述“X 先生在 66 岁时去世”比陈述“X 先生在 2019 年去世”包含更多的信息。因此引入了三个年龄特征:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="782f" class="lr ls it mr b gy mv mw l mx my">combined['Age'] = combined.YrSold - combined.YearBuilt<br/>combined['AgeRemod'] = combined.YrSold - combined.YearRemodAdd<br/>combined['AgeGarage'] = combined.YrSold - combined.GarageYrBlt </span></pre><p id="dce3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">“AgeGarage”列的值似乎很少，都在 2000 左右！这是因为 GarageYrBlt 的缺失值被估算为零，如前一节所述。根据以下代码，所有这些异常值都被替换为低于某个阈值的最大可能值:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="6818" class="lr ls it mr b gy mv mw l mx my">max_AgeGarage = np.max(combined.AgeGarage[combined.AgeGarage &lt; 1000])<br/>combined['AgeGarage'] = combined['AgeGarage'].map(lambda x: max_AgeGarage if x &gt; 1000 else x)</span></pre><p id="4497" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">负值，如果有的话，应该被替换为零，并且原始时间特征现在可以被丢弃。</p><h2 id="be7a" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">将分类列转换为虚拟列</h2><p id="fd9d" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">分类特征现在可以用示例代码转换成虚拟列:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="3180" class="lr ls it mr b gy mv mw l mx my">Foundation_dummies=pd.get_dummies(combined['Foundation'], prefix='Foundation_', drop_first=True)<br/>combined=pd.concat([combined, Foundation_dummies], axis=1)<br/>combined.drop('Foundation', axis=1, inplace=True)</span></pre><p id="9c79" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">prefix 参数有助于以后识别列。我们只需要保留 k 列中的 k-1 列，因此 drop_first 被设置为 True。要了解更多关于这些论点的信息，请阅读此处的。</p><p id="486c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">虚拟数据集与原始数据集连接在一起，实际的列被删除。</p><p id="e7cf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">数据集几乎建立起来了，但还没有。</p><p id="b23e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">完整的数据集现在有 209 列！数据太多，机器消化不了。更多的特征会导致更多的噪声，并最终导致过度拟合。</p><h2 id="1702" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">删除不重要的功能</h2><p id="7807" class="pw-post-body-paragraph ku kv it kw b kx ml ju kz la mm jx lc ld mn lf lg lh mo lj lk ll mp ln lo lp im bi translated">问题是如何决定放弃哪个特性？幸运的是 XGBoost 有一个答案。</p><p id="5bd6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">组合的数据集被切片以获得训练集，并且 xgboost 对象被拟合在其上。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="03f2" class="lr ls it mr b gy mv mw l mx my">X = combined[:-1459]<br/>y = targets<br/>y = y.drop(index_drop).reset_index(drop=True)</span><span id="ede2" class="lr ls it mr b gy mz mw l mx my">xgb = XGBRegressor()<br/>xgb.fit(X, y)</span></pre><p id="b761" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后计算特征重要性并排序:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="0931" class="lr ls it mr b gy mv mw l mx my">imp = pd.DataFrame(xgb.feature_importances_ ,columns = ['Importance'],index = X.columns)<br/>imp = imp.sort_values(['Importance'], ascending = False)<br/>print(imp)</span></pre><p id="16f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">结果是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/c80ddaac3f99b4704341552a7013ba7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*TbSNHhviUx0bAXe6uEaaZA.png"/></div></figure><p id="ed64" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有很多功能的重要性为零。为了确定到底应该保留多少特性，我使用了一个名为<a class="ae lq" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" rel="noopener ugc nofollow" target="_blank"> RFECV </a>的函数。</p><p id="8973" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">RFECV 告诉我们应该保留多少和哪些特性。它有一个评分参数，我用 RMSE(均方根误差)来表示，因为 Kaggle 练习也将根据相同的条件进行评估。以下函数定义 rmse，并使其成为计分器:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="5b33" class="lr ls it mr b gy mv mw l mx my">def rmse(y_true, y_pred):<br/>    return np.sqrt(np.mean((y_true-y_pred)**2))</span><span id="68d8" class="lr ls it mr b gy mz mw l mx my">rmse = make_scorer(rmse, greater_is_better=False)</span></pre><p id="a4c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为目标是减少 RMSE，所以“越大越好”的参数被设置为假。最后，RFECV 在 X 和 y 数据集上拟合，给出了最佳特征数 67！</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="8d5c" class="lr ls it mr b gy mv mw l mx my">rfecv = RFECV(estimator=xgb, step=1, cv=3, n_jobs=-1, scoring=rmse)<br/>rfecv = rfecv.fit(X, y)<br/>print("Optimal number of features : %d" % rfecv.n_features_)</span></pre><p id="b476" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">绘制特征与交叉验证分数的关系为我们提供了下图:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="e619" class="lr ls it mr b gy mv mw l mx my">plt.figure()<br/>plt.xlabel("Number of features selected")<br/>plt.ylabel("Cross validation score")<br/>plt.xticks(np.arange(0,200,10))<br/>plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ca6e565ed3026052a59cc126be2b3fbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*ATwJ_DS5ix0UMJJfq5mLiA.png"/></div></figure><p id="e0b8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在大约 67 个特征之后，交叉验证分数几乎没有改善。这里的下一步是检查函数建议哪些列。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="8ea8" class="lr ls it mr b gy mv mw l mx my">features_kept = X.columns.values[rfecv.support_] <br/>X = X[features_kept]</span></pre><p id="e10b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">X 的特点是:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="35f3" class="lr ls it mr b gy mv mw l mx my">LotFrontage<br/>LotArea<br/>Alley<br/>OverallQual<br/>OverallCond<br/>MasVnrArea<br/>ExterQual<br/>ExterCond<br/>BsmtQual<br/>BsmtCond<br/>BsmtExposure<br/>BsmtFinType1<br/>BsmtFinSF1<br/>BsmtFinSF2<br/>BsmtUnfSF<br/>TotalBsmtSF<br/>HeatingQC<br/>CentralAir<br/>1stFlrSF<br/>2ndFlrSF<br/>GrLivArea<br/>BsmtFullBath<br/>FullBath<br/>HalfBath<br/>KitchenAbvGr<br/>KitchenQual<br/>Functional<br/>FireplaceQu<br/>GarageType<br/>GarageFinish<br/>GarageCars<br/>GarageArea<br/>GarageQual<br/>GarageCond<br/>PavedDrive<br/>WoodDeckSF<br/>OpenPorchSF<br/>EnclosedPorch<br/>ScreenPorch<br/>PoolArea <br/>Fence<br/>Age<br/>AgeRemod<br/>AgeGarage<br/>MSZoning__FV<br/>MSZoning__RL<br/>MSZoning__RM<br/>LotConfig__CulDSac<br/>Street__Pave<br/>Foundation__CBlock<br/>Neighborhood__BrkSide<br/>Neighborhood__ClearCr<br/>Neighborhood__Crawfor<br/>Neighborhood__OldTown<br/>Neighborhood__Sawyer<br/>Neighborhood__StoneBr<br/>Condition1__Norm<br/>Exterior1st__BrkComm<br/>Exterior1st__BrkFace<br/>Exterior1st__HdBoard<br/>Exterior2nd__BrkFace<br/>Exterior2nd__Wd Shng<br/>HouseStyle__SLvl<br/>SaleType__New<br/>SaleCondition__Family<br/>SaleCondition__Normal<br/>MSSubClass__class2</span></pre><p id="f28b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里有一些功能，根据我们在<a class="ae lq" rel="noopener" target="_blank" href="/exploratory-data-analysis-unravelling-a-story-with-data-b01d70069035"> EDA </a>中的分析，这些功能本不应该出现在这里。例如，我们已经推断出公摊面积不是预测销售价格的重要特征。</p><p id="1e61" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此可以删除 PoolArea。</p><p id="a8af" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其次，有一些变量是相互关联的。因此，只保留其中一个是有意义的。运行命令时(参考上一篇关于<a class="ae lq" rel="noopener" target="_blank" href="/exploratory-data-analysis-unravelling-a-story-with-data-b01d70069035"> EDA </a>的文章):</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="18df" class="lr ls it mr b gy mv mw l mx my">print(corr['SalePrice'].sort_values(ascending=False))</span><span id="b205" class="lr ls it mr b gy mz mw l mx my">SalePrice        1.000000<br/>OverallQual      0.790982<br/>GrLivArea        0.708624<br/>GarageCars       0.640409<br/>GarageArea       0.623431<br/>TotalBsmtSF      0.613581<br/>1stFlrSF         0.605852<br/>FullBath         0.560664<br/>TotRmsAbvGrd     0.533723<br/>YearBuilt        0.522897<br/>YearRemodAdd     0.507101<br/>GarageYrBlt      0.486362<br/>MasVnrArea       0.477493<br/>Fireplaces       0.466929<br/>BsmtFinSF1       0.386420<br/>LotFrontage      0.351799<br/>WoodDeckSF       0.324413<br/>2ndFlrSF         0.319334<br/>OpenPorchSF      0.315856<br/>HalfBath         0.284108<br/>LotArea          0.263843<br/>BsmtFullBath     0.227122<br/>BsmtUnfSF        0.214479<br/>BedroomAbvGr     0.168213<br/>ScreenPorch      0.111447<br/>PoolArea         0.092404<br/>MoSold           0.046432<br/>3SsnPorch        0.044584<br/>BsmtFinSF2      -0.011378<br/>BsmtHalfBath    -0.016844<br/>MiscVal         -0.021190<br/>Id              -0.021917<br/>LowQualFinSF    -0.025606<br/>YrSold          -0.028923<br/>OverallCond     -0.077856<br/>MSSubClass      -0.084284<br/>EnclosedPorch   -0.128578<br/>KitchenAbvGr    -0.135907</span></pre><p id="acd7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，GarageArea，1stFlrSF 可能会被删除，我们减少到 64 个功能。</p><p id="e8f7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里的结论是，一个人不应该盲从任何函数的结果。我们应该每时每刻问自己，这是否有意义。因此，据说通过机器学习实现高精度不仅是一门科学，也是一门艺术。特征工程通常是一个迭代过程，在实施模型时，可能需要进一步修改数据集以实现更高的精度。</p><p id="308f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我继续使用这个数据集，并用不同的算法进行训练，这将在我的下一篇博客中介绍。</p><p id="dd47" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">欢迎提出进一步改进功能工程的建议！</p></div></div>    
</body>
</html>