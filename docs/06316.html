<html>
<head>
<title>Math for Data Science: Collaborative Filtering on Utility Matrices</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学的数学:效用矩阵上的协同过滤</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/math-for-data-science-collaborative-filtering-on-utility-matrices-e62fa9badaab?source=collection_archive---------16-----------------------#2019-09-11">https://towardsdatascience.com/math-for-data-science-collaborative-filtering-on-utility-matrices-e62fa9badaab?source=collection_archive---------16-----------------------#2019-09-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/42b66992c617c33ab1c74d07bffb399b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eySK-x0O6u4Ro3EIPP-_WA.png"/></div></div></figure><div class=""/><div class=""><h2 id="bed0" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">理解推荐引擎协同过滤模型背后的数学原理</h2></div><p id="13f8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我强烈<em class="lp">推荐</em>看看我的另一篇文章，作为推荐引擎的介绍:</p><div class="is it gp gr iu lq"><a rel="noopener follow" target="_blank" href="/a-primer-to-recommendation-engines-49bd12ed849f"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jf gy z fp lv fr fs lw fu fw jd bi translated">推荐引擎入门</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">它们是什么，它们是如何工作的，以及它们为什么伟大。</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">towardsdatascience.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me ja lq"/></div></div></a></div><h2 id="0b4f" class="mf mg je bd mh mi mj dn mk ml mm dp mn lc mo mp mq lg mr ms mt lk mu mv mw mx bi translated">概述:什么是协同过滤？</h2><p id="8bbf" class="pw-post-body-paragraph kt ku je kv b kw my kf ky kz mz ki lb lc na le lf lg nb li lj lk nc lm ln lo im bi translated">协同过滤是一种使用用户和项目数据的推荐引擎。更具体地说，是个人用户对单个商品的<strong class="kv jf">评分</strong>。这样，基于来自其他用户的评级来推荐项目，因此，<em class="lp">协作</em>。这些数据可以用<strong class="kv jf">效用矩阵</strong>来表示，其中一个轴是用户，另一个轴是项目。协同过滤推荐引擎的目标是<strong class="kv jf">填补效用矩阵</strong>中的空白，因为不是每个用户都对每个项目进行了评级，然后输出评级最高的、先前未评级的项目作为推荐。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/9f461bea1708769126dc89d2417519a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*gBSm6E1nmPOmIbBf96eb2A.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">A simple utility matrix, with 4 users (columns) and 4 items (rows).</figcaption></figure><p id="b11e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">填充效用矩阵有三种主要技术:用户-用户、项目-项目和 SVD。我们将使用上面的简单效用矩阵逐一分析，尝试并<strong class="kv jf">预测用户 1 对项目 3 </strong>的评价。</p><h1 id="eeaf" class="nm mg je bd mh nn no np mk nq nr ns mn kk nt kl mq kn nu ko mt kq nv kr mw nw bi translated">用户对用户</h1><p id="0a16" class="pw-post-body-paragraph kt ku je kv b kw my kf ky kz mz ki lb lc na le lf lg nb li lj lk nc lm ln lo im bi translated">计算效用矩阵的缺失值有两个主要步骤:</p><ol class=""><li id="3858" class="nx ny je kv b kw kx kz la lc nz lg oa lk ob lo oc od oe of bi translated">计算 U1 和所有其他用户之间的<strong class="kv jf">相似度</strong></li><li id="ff60" class="nx ny je kv b kw og kz oh lc oi lg oj lk ok lo oc od oe of bi translated">通过对其他用户的 I3 评分取平均值<strong class="kv jf">，计算 U1 对 I3 的评分，根据用户与 U1 的相似度对每个用户的评分进行加权</strong></li></ol></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="3d0a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这里，我们快速讨论一下<strong class="kv jf">相似性度量</strong>。常见的有:<code class="fe os ot ou ov b">euclidean distance</code>(T1 的一种具体形式)<code class="fe os ot ou ov b">cosine similarity</code><code class="fe os ot ou ov b">Pearson correlation</code><code class="fe os ot ou ov b">Jaccard index</code>等。从实验上来看，皮尔逊相关性被证明是最好的。对于这篇文章中的例子，我们将使用余弦相似度。</p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/881cfa794248d9a80051f40e336d3561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eCcWFuWhEnTuGb3-5_6BGQ.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Left: cosine similarity of U1 to all other users; Right: weighted average of ratings for I3</figcaption></figure><p id="4ca7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因此，我们对 U1 和 I3 的预测评级是 4.34！另请注意，不同的相似性度量会给出略微不同的结果。</p><h1 id="bbb4" class="nm mg je bd mh nn no np mk nq nr ns mn kk nt kl mq kn nu ko mt kq nv kr mw nw bi translated">项目对项目</h1><p id="85b5" class="pw-post-body-paragraph kt ku je kv b kw my kf ky kz mz ki lb lc na le lf lg nb li lj lk nc lm ln lo im bi translated">项目-项目协同过滤与用户-用户非常相似，但不是计算用户之间的相似性，而是计算项目之间的相似性。你想要的最终值是 U1 其他评分的平均值，<strong class="kv jf">用 I3 和其他项目的相似度加权</strong>。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/aaf0799e6404acd102f54ca016f5eae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fD-_IpjAm4Co0jWzl-dmHA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Left: cosine similarity of I3 to all other items; Right: weighted average of ratings for U1</figcaption></figure><p id="f3c0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">采用这种逐项计算的方法，我们最终得到的预测值为 3.31——与我们之前的预测值 4.34 大相径庭。</p><p id="7406" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">需要注意几件事:</p><ul class=""><li id="7275" class="nx ny je kv b kw kx kz la lc nz lg oa lk ob lo oy od oe of bi translated">当计算相似性时，一些来源说将缺失值视为 0，而一些来源在计算相似性时简单地忽略缺失值的整个行/列。</li><li id="819f" class="nx ny je kv b kw og kz oh lc oi lg oj lk ok lo oy od oe of bi translated">一般来说，由于用户的独特品味，逐项方法更有效。</li><li id="146a" class="nx ny je kv b kw og kz oh lc oi lg oj lk ok lo oy od oe of bi translated">在决定是使用用户-用户还是项目-项目时，您可能要考虑算法的复杂性。如果你有<strong class="kv jf"> m </strong>个用户和<strong class="kv jf"> n </strong>个项目，那么用户-用户的时间复杂度是 O(m ^ n ),项目-项目的时间复杂度是 O(m n)。如果你有更多的用户，你可能会选择<strong class="kv jf">条目，反之亦然。</strong></li></ul><h1 id="b32e" class="nm mg je bd mh nn no np mk nq nr ns mn kk nt kl mq kn nu ko mt kq nv kr mw nw bi translated">奇异值分解</h1><p id="be02" class="pw-post-body-paragraph kt ku je kv b kw my kf ky kz mz ki lb lc na le lf lg nb li lj lk nc lm ln lo im bi translated">我们必须从一点点理论开始，来理解这些概念从何而来。奇异值分解(SVD)是矩阵分解的一种形式。矩阵分解是将一个矩阵分解成(通常是三个)矩阵的乘积。如果你还记得代数的话，当我们把二次方程分解成它们的线性部分(即<em class="lp"> x + 2x + 1 = (x+1)(x+1) </em>)时，也是类似的思路。</p><p id="8cfc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">SVD 是由西蒙·芬克在 2007 年 Netflix 奖竞赛中一举成名的型号。如果你学过线性代数课程，奇异值分解通常是利用矩阵的特征值和特征向量将其分解为三个分量矩阵。Python 库中用于解决推荐引擎的算法名为<em class="lp">SVD</em>，但它并不完全分解你的效用矩阵。取而代之的是，它在做 SVD 的<strong class="kv jf">逆</strong>，并试图<strong class="kv jf">使用两个分量矩阵</strong>而不是三个分量矩阵<strong class="kv jf">来重建</strong>你的效用矩阵。如下所示，这两个矩阵可以被解释为项目矩阵和用户矩阵。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/8097dc134159ea2a9249d53ba7ab7361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zNsxs-n-JAYS36HLH6Qx3g.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Decomposing the utility matrix into an item matrix and a user matrix.</figcaption></figure><p id="e744" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">潜在特征指的是所有物品或用户特征的某种抽象。只要你有<strong class="kv jf">相同数量的物品和用户的潜在特征</strong>，你就可以将矩阵相乘，得到一个与你的效用矩阵维数相同的矩阵。潜在特征的数量是一个可以在模型中调整的超参数。基于矩阵乘法，我们还可以看到，U1 对 I3 的评级值受到项目矩阵的 I3 行和用户矩阵的 U1 列的影响。</p><p id="ab50" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因为我们不能分解有缺失值的矩阵，所以我们必须采取另一种方法。这就是机器学习的用武之地。如前所述，我们现在将尝试<strong class="kv jf">用我们的项目矩阵和用户矩阵</strong>重新创建效用矩阵。这是使用梯度下降的方法完成的:<strong class="kv jf">交替最小二乘法</strong>。</p><h2 id="5cb6" class="mf mg je bd mh mi mj dn mk ml mm dp mn lc mo mp mq lg mr ms mt lk mu mv mw mx bi translated">1.初始化</h2><p id="509b" class="pw-post-body-paragraph kt ku je kv b kw my kf ky kz mz ki lb lc na le lf lg nb li lj lk nc lm ln lo im bi translated">与所有使用梯度下降的模型一样，您必须从一些初始值开始。我们正在初始化两个分量矩阵。在这个例子中，我已经用全 1 初始化了我们的分量矩阵。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/54b31c2fbe71b113224bc85529609e19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Atw5ebV-UsZSjKMsFZ1NqA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">In blue: After initializing the component matrices with 1s, the <strong class="bd pc">recreated utility matrix</strong> is all 2s. In grey: the original utility matrix for comparison.</figcaption></figure><h2 id="ca35" class="mf mg je bd mh mi mj dn mk ml mm dp mn lc mo mp mq lg mr ms mt lk mu mv mw mx bi translated">2.价值函数</h2><p id="7f86" class="pw-post-body-paragraph kt ku je kv b kw my kf ky kz mz ki lb lc na le lf lg nb li lj lk nc lm ln lo im bi translated">在这个模型中，<strong class="kv jf">成本函数</strong>是允许我们<strong class="kv jf">比较我们原始效用矩阵和我们重新创建的效用矩阵中的相应值</strong>的任何度量。这意味着，我正在比较我的原始效用矩阵 4 中的<strong class="kv jf"> U1-I1 的评级与我重新创建的矩阵</strong>2 中的<strong class="kv jf"> U1-I1 的评级，并对矩阵中的所有值进行同样的操作。在这里，我使用均方根误差(<em class="lp"> RMSE:对每个个体的差值求平方，取平均值，然后求平方根</em>)并在计算 RMSE 时将缺失值视为 0。</strong></p><h2 id="196d" class="mf mg je bd mh mi mj dn mk ml mm dp mn lc mo mp mq lg mr ms mt lk mu mv mw mx bi translated">3.交替最小二乘法梯度下降</h2><p id="caf1" class="pw-post-body-paragraph kt ku je kv b kw my kf ky kz mz ki lb lc na le lf lg nb li lj lk nc lm ln lo im bi translated">这种梯度下降的工作原理是通过一次改变一个分量矩阵中的一个值来尝试最小化成本函数(RMSE)。让我们从寻找项目矩阵中 I1 的第一个潜在特征的最优值开始。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/66a3fb4fb2d93071e897d44e8ab905b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wf3MMqaYKgCmMZo-60EYDw.png"/></div></div></figure><p id="d2ae" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如上图所示，通过改变这个第一个值(现在表示为未知的<em class="lp"> x </em>，我们<strong class="kv jf">更新了我们重新创建的效用矩阵的整个第一行</strong>。通过矩阵乘法，这整个第一行变成<em class="lp"> x </em> + 1。因为矩阵的其余部分是静态的，我们可以通过第一行最小化我们的成本函数。因此，我们只是最小化那个二次方程，以得到 2.5 的最优<em class="lp"> x </em>！</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/1f3d9664aeb34eda2680d8350d63a8bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FuZF-m0Sx9fU7MNC1dQuEA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Our RMSE goes down!</figcaption></figure><p id="89f4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">用 2.5 替换<em class="lp"> x </em>，我们重新创建的效用矩阵的第一行变成了 3.5，我们的 RMSE 从 1.75 下降到 1.58！这个过程一遍又一遍地重复，直到 RMSE 再也好不起来。需要注意的是，改变项目或用户矩阵中的一个值，<strong class="kv jf">会改变重新创建的效用矩阵的整个行或列</strong>。这维护了用户和项目之间的关系，这个过程被称为<strong class="kv jf">并行化</strong>。一遍又一遍地做这个过程(我是用 Python 库<strong class="kv jf"> Surprise </strong>做的，我将在另一篇文章中介绍)，我们最终得到 1.15 的 RMSE，这就是我们重新创建的效用矩阵的样子:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/654f0b1f23ef65f3592736f80440db65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ar0t5KIh0NUWvf6lN71obw.png"/></div></div></figure><h2 id="ed6b" class="mf mg je bd mh mi mj dn mk ml mm dp mn lc mo mp mq lg mr ms mt lk mu mv mw mx bi translated">4.估价</h2><p id="d191" class="pw-post-body-paragraph kt ku je kv b kw my kf ky kz mz ki lb lc na le lf lg nb li lj lk nc lm ln lo im bi translated">基于此，我们可以猜测 U1 对 I3 的评分是 3.7！在一个更稀疏的矩阵中，每个用户有多个未知的评分，然后你会推荐以前最高的未评分项目。有趣的是，与我们的用户-用户(4.34)和项目-项目(3.31)预测相比，我们的 SVD 值 3.7 介于使用两个不同轴的相似性之间。</p><p id="17c2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在实际操作中，如果有更多的数据，你可以对你的已知评分做一个<code class="fe os ot ou ov b">train-test-split</code>，然后测量<strong class="kv jf"> RMSE 在你的测试集中的实际评分值和它们在模型</strong>中的预测值。RMSE 将用于评估模型，<strong class="kv jf">不要与在执行此交替最小二乘梯度下降时用作成本函数的 RMSE</strong>混淆。RMSE 可以解释为你的预测评级与实际评级的平均偏差，即你的预测平均偏离多少颗星。</p><h1 id="acd7" class="nm mg je bd mh nn no np mk nq nr ns mn kk nt kl mq kn nu ko mt kq nv kr mw nw bi translated">我喜欢的资源:</h1><ul class=""><li id="1dc4" class="nx ny je kv b kw my kz mz lc pg lg ph lk pi lo oy od oe of bi translated">推荐系统的矩阵分解技术:<a class="ae oz" href="https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf" rel="noopener ugc nofollow" target="_blank">https://data jobs . com/data-science-repo/Recommender-Systems-[网飞]。pdf </a></li><li id="af51" class="nx ny je kv b kw og kz oh lc oi lg oj lk ok lo oy od oe of bi translated">给你推荐:Netflix 奖和哈利南与 Striphas 制作的算法文化:<a class="ae oz" href="https://journals.sagepub.com/doi/pdf/10.1177/1461444814538646" rel="noopener ugc nofollow" target="_blank">https://journals . sage pub . com/doi/pdf/10.1177/1461444814538646</a></li><li id="6e37" class="nx ny je kv b kw og kz oh lc oi lg oj lk ok lo oy od oe of bi translated">莱斯科维克、拉贾拉曼和乌尔曼对大规模数据集的挖掘:<a class="ae oz" href="http://infolab.stanford.edu/~ullman/mmds/book.pdf" rel="noopener ugc nofollow" target="_blank">http://infolab.stanford.edu/~ullman/mmds/book.pdf</a>(特别是第 9 章对理解奇异值分解非常有帮助)</li><li id="58fc" class="nx ny je kv b kw og kz oh lc oi lg oj lk ok lo oy od oe of bi translated">给<em class="lp">惊喜</em>的文档，我用于 SVD 的库:<a class="ae oz" href="https://surprise.readthedocs.io/en/stable/index.html" rel="noopener ugc nofollow" target="_blank">https://surprise.readthedocs.io/en/stable/index.html</a></li></ul></div></div>    
</body>
</html>