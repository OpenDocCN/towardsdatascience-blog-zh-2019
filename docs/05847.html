<html>
<head>
<title>All the Annoying Assumptions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">所有恼人的假设</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-the-annoying-assumptions-31b55df246c3?source=collection_archive---------8-----------------------#2019-08-26">https://towardsdatascience.com/all-the-annoying-assumptions-31b55df246c3?source=collection_archive---------8-----------------------#2019-08-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4822" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这篇文章中，我试图收集和映射常见数据科学算法的假设。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3419608aa2810e3c197e90e02db9b65d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZcJKz3761vChU1VFHfzkw.jpeg"/></div></div></figure><p id="099c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设是恼人的小事，如果你要成功地实现任何模型，就需要坚持这些假设。大多数模型，无论多么基本，都有一套假设，在本文中，我试图将它们映射成一种模式，以便于理解。</p><p id="3537" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我从一张我尝试构建的假设图开始。从这里开始，我们可以轻松地浏览与每个部分相关的假设。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lq"><img src="../Images/9564b017eba97c1947b992cc852704d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kO2xE3tQFxg9xp0ZSKWOIQ.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Assumption Map</figcaption></figure><p id="2e47" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们开始吧..</p><h1 id="2281" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">回归</h1><p id="20d0" class="pw-post-body-paragraph ku kv it kw b kx mn ju kz la mo jx lc ld mp lf lg lh mq lj lk ll mr ln lo lp im bi translated">线性回归和多项式回归都有一组共同的假设，如果它们的实现是有益的，就需要满足这些假设。</p><ol class=""><li id="2360" class="ms mt it kw b kx ky la lb ld mu lh mv ll mw lp mx my mz na bi translated"><strong class="kw iu">线性关系:</strong>线性回归需要<strong class="kw iu">自变量和因变量之间的关系是线性的。</strong>这种线性假设<strong class="kw iu">最好用散点图来检验。</strong>然而，显然<strong class="kw iu">线性假设对多项式回归无效。</strong></li><li id="9199" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated"><strong class="kw iu">多元正态:</strong>回归分析要求所有变量都是多元正态。这种假设最好用<strong class="kw iu">直方图或 Q-Q 图来检验。</strong> <strong class="kw iu">正态性可通过拟合优度检验进行检查，如 Kolmogorov-Smirnov 检验。当数据不是正态分布时，非线性变换(例如，对数变换)可以解决这个问题。</strong></li><li id="bee0" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated"><strong class="kw iu">多重共线性:</strong>回归假设数据中很少或没有多重共线性。<strong class="kw iu">当自变量之间的相关性过高时，会出现多重共线性。</strong>您可以使用相关矩阵来检查变量之间是否相关。您可以删除一组相关的列，同时保留另一组。</li><li id="adfc" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated"><strong class="kw iu">自相关:R </strong>回归分析要求数据中很少或没有自相关。<strong class="kw iu">当残差不是相互独立的时候，就会出现自相关。</strong>虽然散点图可以让你检查自相关，你可以用<strong class="kw iu">德宾-沃森测试来测试自相关的线性回归模型。</strong></li><li id="086c" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated"><strong class="kw iu">同方差:</strong>同方差描述了一种情况，其中<strong class="kw iu">误差项在独立变量的所有值上都是相同的。</strong>散点图是检查数据是否同方差的好方法(意味着回归线上的残差相等)。如果存在同方差，非线性校正可能会解决问题。</li><li id="ff74" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated">误差项必须是<strong class="kw iu">正态分布</strong>。</li></ol><blockquote class="ng nh ni"><p id="df82" class="ku kv nj kw b kx ky ju kz la lb jx lc nk le lf lg nl li lj lk nm lm ln lo lp im bi translated">像套索、脊、弹性网等分离方法不过是试图平衡偏差与方差的正则化技术。因此，这些技术没有任何独立的假设。</p><p id="c0b6" class="ku kv nj kw b kx ky ju kz la lb jx lc nk le lf lg nl li lj lk nm lm ln lo lp im bi translated">SVR 对输入数据也相当宽容，并且没有任何独立的假设。</p></blockquote><h1 id="1434" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">使聚集</h1><p id="d335" class="pw-post-body-paragraph ku kv it kw b kx mn ju kz la mo jx lc ld mp lf lg lh mq lj lk ll mr ln lo lp im bi translated">广泛使用的聚类算法有四种:<strong class="kw iu"><em class="nj"/><em class="nj">k-means 聚类分析、潜在类分析、</em>和<em class="nj">自组织映射。</em> </strong></p><ol class=""><li id="81f6" class="ms mt it kw b kx ky la lb ld mu lh mv ll mw lp mx my mz na bi translated"><strong class="kw iu"> K 均值聚类</strong>方法考虑了关于聚类<strong class="kw iu">的两个假设——首先，聚类是球形的，其次，聚类具有相似的大小。</strong>当算法处理数据并形成聚类时，球形假设有助于分离聚类。如果违背了这一假设，所形成的簇可能不是人们所期望的。另一方面，对群集大小的假设有助于决定群集的边界。</li><li id="4f25" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated">某些相似性度量(例如欧几里德距离)假设变量在聚类内是不相关的。</li><li id="a65d" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated">在<strong class="kw iu">谱聚类</strong>中，数据点被视为图形的节点。因此，聚类被视为一个图划分问题。然后，节点被映射到一个低维空间，该空间可以很容易地被分离以形成集群。<strong class="kw iu">需要注意的重要一点是，没有对集群的形状/形式进行假设。</strong></li><li id="a9f1" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated"><strong class="kw iu">层次聚类没有自己独立的假设。</strong></li><li id="3a47" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated">与其他聚类算法类似，<strong class="kw iu"> GMM 对数据的格式和形状有一些假设。如果不满足这些标准，绩效可能会显著下降。</strong></li><li id="8844" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated">LCA 以<strong class="kw iu">“条件独立性”为标准定义潜在类别这意味着，在每个潜在类别中，每个变量在统计上独立于其他变量。例如，在对应于不同医学综合征的潜在类别中，一种症状的存在/不存在被视为与所有其他症状的存在/不存在无关。对于某些应用程序，条件独立性可能是一个不合适的假设。例如，一个人可能有两个非常相似的项目，因此对它们的响应可能总是相关联的。对于这种情况和某些相关情况，潜在类模型的扩展是存在的。</strong></li><li id="83a7" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated">与多变量技术相比，<strong class="kw iu">非参数 SOM </strong>程序有许多优点。<strong class="kw iu">首先，它们不假设变量的分布，也不要求变量之间的独立性。</strong>其次，它们更容易实现，能够解决非常复杂的非线性问题。最后，它们可以更有效地处理噪音和缺失数据、非常小的维度和无限大小的样本。</li></ol><h1 id="d6a4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">分类</h1><p id="4a58" class="pw-post-body-paragraph ku kv it kw b kx mn ju kz la mo jx lc ld mp lf lg lh mq lj lk ll mr ln lo lp im bi translated">常见的分类器类型有:<strong class="kw iu"> <em class="nj">逻辑回归、朴素贝叶斯分类器、KNN、随机森林</em> </strong></p><ol class=""><li id="54d8" class="ms mt it kw b kx ky la lb ld mu lh mv ll mw lp mx my mz na bi translated"><strong class="kw iu">逻辑回归</strong>用于预测从属变量，这些变量属于有限数量的类别之一(将二项式情况下的从属变量视为<a class="ae nn" href="https://en.wikipedia.org/wiki/Bernoulli_trial" rel="noopener ugc nofollow" target="_blank">伯努利试验</a>的结果)，而非连续结果。<strong class="kw iu">鉴于这种差异，违反了线性回归的假设。特别是残差不能正态分布。</strong></li><li id="451a" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated">结果的 logit 和每个预测变量之间存在<strong class="kw iu">线性关系。回想一下，logit 函数是<code class="fe no np nq nr b">logit(p) = log(p/(1-p))</code>，其中 p 是结果的概率。这可以通过目视检查每个预测值和 logit 值之间的散点图来检查。</strong></li><li id="2da5" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated">预测值之间没有高度相关性(即<strong class="kw iu">多重共线性</strong>)。这可以通过可视化厨师的距离值来检查。</li><li id="cf8d" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated">在<strong class="kw iu">朴素贝叶斯分类器</strong>的情况下，假设<strong class="kw iu">预测器/特征是独立的</strong>。这里做的另一个假设是所有的<strong class="kw iu">预测因素对结果有相同的影响。</strong></li><li id="2ee2" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated"><strong class="kw iu"> KNN 是一种<em class="nj">非参数懒惰学习</em>算法。那是一个非常简洁的陈述。当你说一项技术是非参数化的，这意味着它没有对底层数据分布做任何假设。</strong></li><li id="aa32" class="ms mt it kw b kx nb la nc ld nd lh ne ll nf lp mx my mz na bi translated">在<strong class="kw iu">决策树</strong>中，由于我们没有概率模型，只有二分分裂，我们根本不需要<strong class="kw iu">做任何假设</strong>。那是关于决策树的，但它也适用于<strong class="kw iu">随机森林。</strong>不同之处在于，对于随机森林，我们使用引导聚合。它下面没有模型，唯一依赖的假设是<strong class="kw iu">采样具有代表性</strong>。但这通常是一个常见的假设。</li></ol><blockquote class="ng nh ni"><p id="1c3b" class="ku kv nj kw b kx ky ju kz la lb jx lc nk le lf lg nl li lj lk nm lm ln lo lp im bi translated">集成方法是一种元算法，它将几种机器学习技术结合到一个预测模型中，以便<strong class="kw iu">减少</strong> <strong class="kw iu">方差</strong>(bagging)<strong class="kw iu">偏差</strong> (boosting)，或者<strong class="kw iu">提高预测</strong> (stacking)。因此，这些也没有任何单独的假设。</p></blockquote><h1 id="128a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">时间序列分析</h1><p id="2ccb" class="pw-post-body-paragraph ku kv it kw b kx mn ju kz la mo jx lc ld mp lf lg lh mq lj lk ll mr ln lo lp im bi translated">ARIMA 模型基于<strong class="kw iu">平稳性</strong>的假设工作(即它们必须具有恒定的方差和<a class="ae nn" href="https://www.statisticshowto.datasciencecentral.com/mean/" rel="noopener ugc nofollow" target="_blank">均值</a>)。如果你的模型是不稳定的，你需要在使用 ARIMA 之前对它进行变换。</p><p id="e66b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">随着我对数据科学的了解越来越多，我会继续更新这篇文章，直到那时，干杯。</p><h1 id="13c9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考资料:</h1><ol class=""><li id="3db0" class="ms mt it kw b kx mn la mo ld ns lh nt ll nu lp mx my mz na bi translated"><a class="ae nn" href="https://www.researchgate.net/publication/263084866_An_Introduction_to_Self-Organizing_Maps" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/publication/263084866 _ An _ Introduction _ to _ Self-Organizing _ Maps</a></li></ol><p id="315c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">2.<a class="ae nn" href="https://cdn1.sph.harvard.edu/wp-content/uploads/sites/59/2016/10/harvard-lecture-series-session-5_LCA.pdf" rel="noopener ugc nofollow" target="_blank">https://cdn1 . SPH . Harvard . edu/WP-content/uploads/sites/59/2016/10/Harvard-lecture-series-session-5 _ LCA . pdf</a></p><p id="bb5f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">3.<a class="ae nn" href="https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2015/12/complete-tutorial-time-series-modeling/</a></p><p id="659b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">4.维基百科(一个基于 wiki 技术的多语言的百科全书协作计划ˌ也是一部用不同语言写成的网络百科全书ˌ 其目标及宗旨是为全人类提供自由的百科全书)ˌ开放性的百科全书</p><p id="e540" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">5.<a class="ae nn" href="https://www.statisticssolutions.com/assumptions-of-linear-regression/" rel="noopener ugc nofollow" target="_blank">https://www . statistics solutions . com/assumptions-of-linear-regression/</a></p><p id="0218" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">6.大量堆栈溢出问题</p><p id="c912" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">7.<a class="ae nn" href="https://cdn1.sph.harvard.edu/wp-content/uploads/sites/59/2016/10/harvard-lecture-series-session-5_LCA.pdf" rel="noopener ugc nofollow" target="_blank">https://cdn1 . SPH . Harvard . edu/WP-content/uploads/sites/59/2016/10/Harvard-lecture-series-session-5 _ LCA . pdf</a></p><p id="d3d0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">8.<a class="ae nn" rel="noopener" target="_blank" href="/spectral-clustering-aba2640c0d5b">https://towards data science . com/spectral-clustering-ABA 2640 CD 5b</a></p><p id="2800" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">9.<a class="ae nn" rel="noopener" target="_blank" href="/naive-bayes-classifier-81d512f50a7c">https://towards data science . com/naive-Bayes-classifier-81d 512 f 50 a7c</a></p><p id="d66c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">10.<a class="ae nn" href="https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/" rel="noopener ugc nofollow" target="_blank">https://saravananthirumuruganathan . WordPress . com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-KNN-algorithm/</a></p><p id="a64c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">11.<a class="ae nn" href="https://www.statisticssolutions.com/assumptions-of-logistic-regression/" rel="noopener ugc nofollow" target="_blank">https://www . statistics solutions . com/assumptions-of-logistic-regression/</a></p></div></div>    
</body>
</html>