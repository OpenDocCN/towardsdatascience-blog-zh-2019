<html>
<head>
<title>Time Series Machine Learning Regression Framework</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时间序列机器学习回归框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/time-series-machine-learning-regression-framework-9ea33929009a?source=collection_archive---------1-----------------------#2019-04-06">https://towardsdatascience.com/time-series-machine-learning-regression-framework-9ea33929009a?source=collection_archive---------1-----------------------#2019-04-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ae5f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">构建时间序列预测管道来预测每周销售交易</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3818b0ffe81654b605da9a4b990ee821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4A6pdVwoD5-GR9J3g-G53Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Most commonly, a <strong class="bd ky">time</strong> series is a <strong class="bd ky">sequence</strong> taken at successive equally spaced points in <strong class="bd ky">time</strong>. Thus it is a <strong class="bd ky">sequence</strong> of discrete-<strong class="bd ky">time</strong> data. Photo by <a class="ae kz" href="https://unsplash.com/photos/2bIQ87pf6DM?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Justin Campbell</a> on <a class="ae kz" href="https://unsplash.com/search/photos/sequence?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="f298" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">介绍</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ls lt l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig.1) Let’s borrow the concept of time from Entropy &amp; thermodynamics: <strong class="ak">Entropy</strong> is the only quantity in the physical sciences that seems to imply a particular direction of progress, sometimes called an arrow of <strong class="ak">time</strong>. As <strong class="ak">time </strong>progresses, the second law of <strong class="ak">thermodynamics </strong>states that the <strong class="ak">entropy</strong> of an isolated system never decreases in large systems over significant periods of <strong class="ak">time</strong>. Can we consider our Universe as an isolated large system? Then what?</figcaption></figure><p id="d015" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">时间！宇宙中最具挑战性的概念之一。我研究物理学多年，看到大多数杰出的科学家都在努力处理时间的概念。在机器学习中，即使我们远离那些复杂的物理理论，在这些理论中，对时间概念的共同理解会改变，时间的存在和 ML 问题中的观察序列会使问题变得更加复杂。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="0946" class="la lb it bd lc ld mx lf lg lh my lj lk jz mz ka lm kc na kd lo kf nb kg lq lr bi translated">时间序列的机器学习；</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/47449d654714deb8dd5740da4e6a090c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScwIEwLmXPFhBP46QMpy_A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig.2) In time series forecasting, we use historical data to forecast the future. George Santayana: Those Who Do Not Learn History Are Doomed To Repeat It. The right figure is taken from <a class="ae kz" href="https://www.kdnuggets.com/2018/02/cartoon-valentine-machine-learning.html" rel="noopener ugc nofollow" target="_blank">https://www.kdnuggets.com/2018/02/cartoon-valentine-machine-learning.html</a></figcaption></figure><p id="1289" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">时间序列是按时间顺序进行的一系列观察。时间序列预测包括采用模型，然后根据历史数据进行拟合，然后用它们来预测未来的观测值。因此，例如，测量的分钟(秒)、天(秒)、月(秒)、前(秒)被用作预测</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/3cb2054943622882c607ecfffeac894b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBdDmb7p5sZokWmAvgVrCw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig.3) Transform Time Series to Supervised Machine Learning.</figcaption></figure><p id="6246" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">接下来的分钟、天、月。被认为是在时间(序列)上向后移动数据的步骤，称为滞后时间或滞后。因此，通过添加测量的滞后作为监督 ML 的输入，可以将时间序列问题转化为监督 ML。见图 3 右侧。一般来说，探索滞后的数量作为一个超参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/de1be58d057faa48677a1977af171407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j895yH6Twy6PYK7vy6YJGQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig.4) Transform the time series to supervised machine learning by adding lags. Lags are basically the shift of the data one step or more backward in the time.</figcaption></figure><h1 id="1617" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">时间序列的交叉验证</h1><p id="170c" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">时间序列的交叉验证不同于不涉及时间或序列的机器学习问题。在没有时间的情况下，我们选择一个随机的数据子集作为验证集来估计测量的准确性。在时间序列中，我们经常预测未来的某个值。因此，验证数据总是必须在训练数据的之后<em class="nk">出现。有两种模式<strong class="lw iu">滑动窗口</strong>和<strong class="lw iu">前向链接验证方法</strong>，可用于时间序列 CV。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/f4be857523496549c28d39f73f55884c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6HaTi1DKsNqEUjdECQV0A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 5) Basically, there are two kinds of cross-validation for the time series sliding window and forward chaining. In this post, we will consider forward chaining cross-validation method</figcaption></figure><p id="bb64" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">图 5 顶部显示了滑动窗口方法。对于这种方法，我们在 n 个数据点上训练，并在接下来的 n 个数据点上验证预测，为下一步及时滑动 2n 训练/验证窗口。图 5 底部显示了正向链接方法。对于这种方法，我们在最后 n 个数据点上训练，并在接下来的 m 个数据点上验证预测，在时间上滑动 n+m 训练/验证窗口。这样，我们就可以估计我们模型的参数了。为了测试模型的有效性，我们可能会在时间序列的末尾使用一个数据块，该数据块是为用学习到的参数测试模型而保留的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/99d73d502090fc3c13e0b1bd5fe8f8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PojrnEYHLULn18uSrN78qg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig.6) Forward chaining cross-validation.</figcaption></figure><p id="1a50" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">图六。显示了正向链接 CV 的工作方式。在这里，有一个滞后。因此，我们从第一秒到第三秒/分钟/小时/天等来训练模型。然后验证第四个，以此类推。既然现在我们已经熟悉了 TS 问题，让我们选择一个时间序列问题并建立一个预测模型。</p><h1 id="10f6" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">预测每周销售交易</h1><p id="03fb" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">假设一家商店的经理要求我们建立一个 ML 模型来预测下周的销售额。该模型必须每周日运行，预测结果必须在每周一上午报告。然后，经理可以决定一周的订单数量。经理给我们提供了 52 周 811 产品的销售数据。销售数据可以在<a class="ae kz" href="https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly" rel="noopener ugc nofollow" target="_blank"> <em class="nk"> UCI 仓库</em> </a>或<a class="ae kz" href="https://www.kaggle.com/crawford/weekly-sales-transactions" rel="noopener ugc nofollow" target="_blank"> <em class="nk"> kaggle </em> </a>中找到。</p><p id="ad60" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们来看数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/2d3f0f888d1fd549d89cbc64d16f28c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfNiaQabjto_MQYjqvDN1Q.png"/></div></div></figure><p id="6e29" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">许多数据科学家可能会为每种产品创建一个模型来预测销售数量。虽然这可以很好地工作，但我们可能会有问题，因为每个模型只有 52 个数据点，这真的很低！尽管这种方法是可行的，但它可能不是最佳解决方案。此外，如果两种或更多产品的销售数量之间存在交互，我们可能会因为为每种产品构建一个模型而错过它们的交互。因此，在这篇文章中，我们将探讨如何建立一个多时间序列预测模型。</p><h1 id="302f" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">数据准备</h1><p id="88c3" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">原始数据有一个产品代码列和 52 周销售额列。首先，我们将通过融合周数据来创建一个新的数据框架。因此，新的数据框架有三列，产品代码、周和销售额。此外，“W”和“P”分别从星期和产品中删除。那么，让我们看看新数据帧的头部和尾部</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/2e9e3eaa6bd070a91ae22f788fd481ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*auCR3nYHBI770hG4o0mS7A.png"/></div></div></figure><p id="6310" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">为了熟悉数据集，销售分布绘制在图 7 中。可以看出，有大量产品的销售额很低，数据也向左倾斜。这个问题对建模的影响将在后面讨论。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/807cd285cd4479531bbb03adede57972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*a58zHt6n8FV4n-zvv1alJQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 7) Sales distribution. There are many product sales items with very low sales.</figcaption></figure><h1 id="a2b7" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">基础特征工程</h1><p id="4b2a" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">因为这篇文章的目标不是 TS 的特性工程，我们将尽量保持这部分简单。让我们创建两个通常用于时间序列的特征。时间上后退一步，1-lag(shift =1)和一周前(W 1)的购买数量与其前一周的购买数量之差，意味着，两周前(W2)。此后，由于 lag 和 diff 导致数据集中的值为空，参见图 4，我们将其删除。因此，当我们查看数据帧的头部时，它从 week = 2 开始。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/8eb70e9a14696fa124a04d2118978419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jGmRonHpD9b5jD-ZYgLONw.png"/></div></div></figure><p id="0158" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">“ToSupervised”和“ToSupervisedDiff”类，代码 1 和代码 2，如编码部分所示，用于通过简单的管道获得新的数据帧:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="a33e" class="nw lb it ns b gy nx ny l nz oa">steps = [('1_step',<br/>          ToSupervised('Sales','Product_Code',1)),<br/>         ('1_step_diff',<br/>          ToSupervisedDiff('1_Week_Ago_Sales',<br/>          'Product_Code',1,dropna=True))]</span><span id="3f4d" class="nw lb it ns b gy ob ny l nz oa">super_1 = Pipeline(steps).fit_transform(df)</span></pre><p id="2a6d" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">现在，数据有了适当的形状，可以在受监督的 ML 中使用。</p><h1 id="e02d" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">正向链接交叉验证:</h1><p id="9b12" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">另一个问题是，当我们处理时间序列时，我们必须处理时间序列的 CV。我们选择前向链接进行模型验证。为了避免在很少的几周内有一个非常好的模型，我们将使用从 40 到 52 的每一周，每次重复一个过程，并计算分数。因此，这个模式下的 k 倍代码可以在 C. 3 中找到。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="a9b9" class="nw lb it ns b gy nx ny l nz oa">kf = Kfold_time(target='Sales',date_col = 'Week', <br/>                   date_init=40, date_final=52)</span></pre><p id="0e9e" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">因为这篇文章只是一个演示，所以我不打算分离一个测试数据集。在实际项目中，总是保留一些时间段作为测试数据集，以根据看不见的数据评估模型。</p><h1 id="b608" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">公制的</h1><p id="bebe" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">由于问题是回归，有几个众所周知的指标来评估模型，如均方误差(MSE)，平均绝对误差(MAE)，均方根误差(RMSE)，均方根对数误差(RMSLE)，R 平方，等等。这些度量标准中的每一个都有自己的用例，它们以不同的方式惩罚错误，但它们之间也有一些相似之处。在本文中，我们选择 RMSLE 来评估这个模型。</p><h1 id="40dc" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">基线:</h1><p id="9ed9" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">通常，当我们构建一个模型时，我们可能会提出一个非常简单的假设，认为使用 ML 可以改进它。在这里，让我们假设每种产品的数量在本周售出，下周也是如此。这意味着，如果产品-1 在第 1 周销售了 10 次，那么它在第 2 周的销售数字也将是相同的。这通常是一个不错的假设。所以，让我们把这个假设当作我们的基线模型。</p><p id="9200" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">基线模型用 C. 5 编码，让我们看看基线模型是如何工作的</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="5e17" class="nw lb it ns b gy nx ny l nz oa">base_model = BaseEstimator('1_Week_Ago_Sales')<br/>errors = []<br/>for indx,fold in enumerate(kf.split(super_1)):<br/>    X_train, X_test, y_train, y_test = fold<br/>    error = base_model.score(X_test,y_test,rmsle)<br/>    errors.append(error)<br/>    print("Fold: {}, Error: {:.3f}".format(indx,error))<br/>    <br/>print('Total Error {:.3f}'.format(np.mean(errors)))</span></pre><blockquote class="oc od oe"><p id="4918" class="lu lv nk lw b lx ly ju lz ma mb jx mc of me mf mg og mi mj mk oh mm mn mo mp im bi translated">折:0，误差:0.520 <br/>折:1，误差:0.517 <br/>折:2，误差:0.510 <br/>折:3，误差:0.508 <br/>折:4，误差:0.534 <br/>折:5，误差:0.523 <br/>折:6，误差:0.500 <br/>折:7，误差:0.491 <br/>折:8，误差:0.506 【T8</p></blockquote><p id="217e" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这里，折叠 0 到 11 表示周= 40 到周= 52。基线模型在这 12 周内的 RMSLE 平均值为 0.51。这可以被认为是一个很大的错误，这可能是由于图 7 所示的大量商品的销售量很少造成的</p><h1 id="9f1c" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">机器学习模型:</h1><p id="14c4" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">现在，我们将应用 ML 来改进基线预测。让我们定义一个时间序列回归器类 C. 5，它与我们的时间序列交叉验证一起工作。该类获取 cv 和模型，并返回模型预测及其得分。有很多种最大似然算法可以用作估计器。这里，我们选择一个随机的森林。简单地说，RF 可以被认为是装袋和在决策树顶部随机选择特征列的组合。因此，它减少了决策树模型预测的方差。因此，它通常比单个树具有更好的性能，但比旨在减少决策树模型的偏差误差的集成方法具有更弱的性能。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="d7f5" class="nw lb it ns b gy nx ny l nz oa">model = RandomForestRegressor(n_estimators=1000,<br/>                               n_jobs=-1,<br/>                                random_state=0)</span><span id="16ac" class="nw lb it ns b gy ob ny l nz oa">steps_1 = [('1_step',<br/>              ToSupervised('Sales','Product_Code',1)),<br/>           ('1_step_diff',<br/>              ToSupervisedDiff('1_Week_Ago_Sales',<br/>                       'Product_Code',1,dropna=True)),<br/>           ('predic_1',<br/>              TimeSeriesRegressor(model=model,cv=kf))]</span><span id="d378" class="nw lb it ns b gy ob ny l nz oa">super_1_p = Pipeline(steps_1).fit(df)<br/>Model_1_Error = super_1_p.score(df)</span></pre><p id="9d74" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们得到了</p><blockquote class="oc od oe"><p id="5762" class="lu lv nk lw b lx ly ju lz ma mb jx mc of me mf mg og mi mj mk oh mm mn mo mp im bi translated">倍:0，误差:0.4624 <br/>倍:1，误差:0.4596 <br/>倍:2，误差:0.4617 <br/>倍:3，误差:0.4666 <br/>倍:4，误差:0.4712 <br/>倍:5，误差:0.4310 <br/>倍:6，误差:0.4718 <br/>倍:7，误差:0.4494 <br/>倍:8，误差:</p></blockquote><p id="90af" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">似乎模型起作用了，误差减小了。让我们添加更多的滞后，并再次评估模型。因为我们构建了管道，所以添加更多的 lag 会非常简单。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="7e20" class="nw lb it ns b gy nx ny l nz oa">steps_3 = [('1_step',<br/>            ToSupervised('Sales','Product_Code',3)),<br/>           ('1_step_diff',<br/>            ToSupervisedDiff('1_Week_Ago_Sales','Product_Code',1)),<br/>           ('2_step_diff',<br/>            ToSupervisedDiff('2_Week_Ago_Sales','Product_Code',1)),<br/>           ('3_step_diff',<br/>            ToSupervisedDiff('3_Week_Ago_Sales',<br/>                  'Product_Code',1,dropna=True)),<br/>           ('predic_3',<br/>            TimeSeriesRegressor(model=model,cv=kf,scoring=rmsle))]<br/>super_3_p = Pipeline(steps_3).fit(df)</span></pre><blockquote class="oc od oe"><p id="6368" class="lu lv nk lw b lx ly ju lz ma mb jx mc of me mf mg og mi mj mk oh mm mn mo mp im bi translated">倍:0，误差:0.4312 <br/>倍:1，误差:0.4385 <br/>倍:2，误差:0.4274 <br/>倍:3，误差:0.4194 <br/>倍:4，误差:0.4479 <br/>倍:5，误差:0.4070 <br/>倍:6，误差:0.4395 <br/>倍:7，误差:0.4333【T15</p></blockquote><p id="44f9" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">似乎预测的误差再次减小，并且模型学习得更多。我们可以继续添加滞后，看看模型的性能如何变化；然而，我们将推迟这个过程，直到我们使用 LGBM 作为一个估计。</p><h1 id="d205" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">统计转换:</h1><p id="51cc" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">销售的分布，图 7 显示数据向低销售数字或左边倾斜。通常，对数变换在应用于偏斜分布时很有用，因为它们倾向于扩展较低幅度范围内的值，并倾向于压缩或减少较高幅度范围内的值。当我们进行统计变换时，模型的可解释性会发生变化，因为系数不再告诉我们原始特征，而是变换后的特征。因此，当我们对销售数字应用 np.log1p 来转换其分布以使其更接近正态分布时，我们也对预测结果应用 np.expm1，参见 C. 6，TimeSeriesRegressorLog。现在，我们用提到的变换重复计算</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="7ec4" class="nw lb it ns b gy nx ny l nz oa">steps_3_log = [('1_step',<br/>                 ToSupervised('Sales','Product_Code',3)),<br/>               ('1_step_diff',<br/>                 ToSupervisedDiff('1_Week_Ago_Sales',<br/>                                    'Product_Code',1)),<br/>               ('2_step_diff',<br/>                 ToSupervisedDiff('2_Week_Ago_Sales',<br/>                                    'Product_Code',1)),<br/>               ('3_step_diff',<br/>                 ToSupervisedDiff('3_Week_Ago_Sales',<br/>                                    'Product_Code',1,dropna=True)),<br/>               ('predic_3',<br/>                 TimeSeriesRegressorLog(model=model,<br/>                                     cv=kf,scoring=rmsle))]<br/>super_3_p_log = Pipeline(steps_3_log).fit(df)</span></pre><p id="03c2" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">所以我们有</p><blockquote class="oc od oe"><p id="5811" class="lu lv nk lw b lx ly ju lz ma mb jx mc of me mf mg og mi mj mk oh mm mn mo mp im bi translated">倍:0，误差:0.4168 <br/>倍:1，误差:0.4221 <br/>倍:2，误差:0.4125 <br/>倍:3，误差:0.4035 <br/>倍:4，误差:0.4332 <br/>倍:5，误差:0.3977 <br/>倍:6，误差:0.4263 <br/>倍:7，误差:0.4122【T33</p></blockquote><p id="6ad3" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这表明模型的性能提高了，误差又减小了。</p><h1 id="a1d5" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">套装 ML:</h1><p id="b00c" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">现在，是时候使用更强的 ML 估计量来改进预测了。我们选择 LightGBM 作为新的估计器。所以让我们重复计算</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="745a" class="nw lb it ns b gy nx ny l nz oa">model_lgb = LGBMRegressor(n_estimators=1000, learning_rate=0.01)</span><span id="2ca1" class="nw lb it ns b gy ob ny l nz oa">steps_3_log_lgbm = [('1_step',<br/>                       ToSupervised('Sales','Product_Code',3)),<br/>                    ('1_step_diff',<br/>                       ToSupervisedDiff('1_Week_Ago_Sales',<br/>                                          'Product_Code',1)),<br/>                    ('2_step_diff',<br/>                       ToSupervisedDiff('2_Week_Ago_Sales',<br/>                                          'Product_Code',1)),<br/>                    ('3_step_diff',<br/>                       ToSupervisedDiff('3_Week_Ago_Sales',<br/>                                          'Product_Code',1,<br/>                                                dropna=True)),<br/>                   ('predic_3',<br/>                       TimeSeriesRegressorLog(model=model_lgb, <br/>                                              cv=kf,scoring=rmsle))]</span><span id="f5da" class="nw lb it ns b gy ob ny l nz oa"><br/>super_3_p_log_lgbm = Pipeline(steps_3_log_lgbm).fit(df)</span></pre><p id="a604" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">然后，</p><blockquote class="oc od oe"><p id="bc6f" class="lu lv nk lw b lx ly ju lz ma mb jx mc of me mf mg og mi mj mk oh mm mn mo mp im bi translated">倍:0，误差:0.4081 <br/>倍:1，误差:0.3980 <br/>倍:2，误差:0.3953 <br/>倍:3，误差:0.3949 <br/>倍:4，误差:0.4202 <br/>倍:5，误差:0.3768 <br/>倍:6，误差:0.4039 <br/>倍:7，误差:0.3868 【T43</p></blockquote><p id="2348" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们再次成功地改进了预测。</p><h1 id="7f81" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">调整步骤数:</h1><p id="c915" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">在这一部分，我们将调整步数(滞后/差异)。在使用 LGBM 作为回归变量后，我有意推迟了本节的步骤，因为它比 RF 更快。图 8 清楚地显示，通过向模型添加更多的步骤，误差减小；然而，正如我们预期的那样，在超过步长= 14 左右的阈值后，增加更多的步长不会显著降低误差。您可能有兴趣定义一个错误阈值来停止这个过程。剩余的计算选择步长= 20。请检查代码 C 7。a 和 B 用于调谐。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="f2f1" class="nw lb it ns b gy nx ny l nz oa">model_lgbm = LGBMRegressor(n_estimators=1000, learning_rate=0.01)</span><span id="c0da" class="nw lb it ns b gy ob ny l nz oa">list_scores2 = stepsTune(df,TimeSeriesRegressorLog(model=model_lgbm,<br/>                           scoring=rmsle,cv=kf,verbosity=False),20)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/6e592776c61b06e6f6dc2b6ac12a180a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*VmTkleLgTzKDfVZKBhFF5w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig 8) Tuning the number of lags/diff is shown. The x-axis shows the RMSLE error as a function of the steps (number of lags/diffs). The model improves by adding more steps; however, steps more than 14 do not improve the model significantly.</figcaption></figure><h1 id="fcff" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">调整超参数:</h1><p id="d560" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">在这一部分，我们将实现网格搜索方法，我们可以通过管道应用它，参见代码 8 A 和 B. C.8.A 代码是从 Sklearn 库借来的。这一部分的目的不是构建一个完全调优的模型。我们试图展示工作流程。稍加调整后，误差变为</p><blockquote class="oc od oe"><p id="f571" class="lu lv nk lw b lx ly ju lz ma mb jx mc of me mf mg og mi mj mk oh mm mn mo mp im bi translated">RMSLE= 0.3868</p></blockquote><p id="3284" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">对于这两个超参数{'learning_rate': 0.005，' n_estimators': 1500}。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="dc74" class="nw lb it ns b gy nx ny l nz oa">params = {'n_estimators':[100,500,1000,1500,2000],<br/>         'learning_rate':[0.005,.01,.1]}</span><span id="4ded" class="nw lb it ns b gy ob ny l nz oa">steps_20 = getDataFramePipeline(20)<br/>super_20 = Pipeline(steps_20).fit_transform(df)</span><span id="1e8d" class="nw lb it ns b gy ob ny l nz oa">model_lgbm2 = LGBMRegressor(random_state=0)</span><span id="63a8" class="nw lb it ns b gy ob ny l nz oa">tune_lgbm =TimeSeriesGridSearch(model = model_lgbm2, cv = kf,<br/>                  param_grid=params,verbosity=False,scoring=rmsle)</span></pre><p id="e87d" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">当最佳调整超参数处于调整参数的边缘时，意味着我们必须重新考虑超参数的范围并重新计算模型，尽管在本文中我们不会这样做。</p><h1 id="4385" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">预测与实际销售</h1><p id="15a1" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">图九。显示第 52 周的预测值与销售额。可以看出，该模型对于高达 15 的销售数字工作良好；然而，它对 30 左右的销量预测很差。正如我们在图 7 中所讨论的，我们可能会为不同的销售范围建立不同的模型来克服这个问题，并拥有一个更强大的预测模型，尽管进一步的建模超出了本文的范围，并且本文已经很长了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/ea2c07040dea89e92d4cfa26f8c66138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*kzmuH1RLfNX3gMT0mxxNoQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 9) Prediction of sales vs. real sales number. It is seen that the model works properly for the low number of sales (less than 15); however, it does not work well for a large number of sales. Therefore, this might be a good motivation to build two models for low and high sales items.</figcaption></figure><p id="6583" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">最后，图 10 显示了我们预测销售的所有尝试。我们从一个非常简单的假设作为基线开始，并试图通过使用不同的 lags/diff、统计转换和应用不同的机器学习算法来改进预测。基线误差为 0.516，调整后的模型误差为 0.3868，这意味着误差减少了 25%。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/603dd9b1a78b9332e7fba6ece7337e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*l0LbsLO-Y9uMER-jtNgR6A.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 10) Our different models score are shown. We could reduce the error of the baseline by 25%.</figcaption></figure><p id="5bbc" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">仍然有许多方法来改进所提出的模型，例如，适当地将产品作为分类变量来处理，更广泛的特征工程，调整超参数，使用各种机器学习算法以及混合和堆叠。</p><h1 id="c413" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">结论:</h1><p id="57d4" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">我们建立了一个时间序列预测管道来预测每周的销售交易。我们从一个简单的逻辑假设作为基线模型开始；然后，我们可以通过构建一个包括基本特征工程、统计转换和应用随机森林和 LGBM 并最终对其进行调优的管道，将基线误差降低 25%。此外，我们还讨论了不同的时间序列交叉验证方法。此外，我们展示了如何使用 Sklearn 基类来构建管道。</p><h1 id="eb6a" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">编码:</h1><p id="b318" class="pw-post-body-paragraph lu lv it lw b lx nf ju lz ma ng jx mc md nh mf mg mh ni mj mk ml nj mn mo mp im bi translated">这篇帖子的完整代码可以在我的<a class="ae kz" href="https://github.com/pourya-ir/Medium/blob/master/Time%20Series%20Machine%20Learning%20Regression%20Framework.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><p id="756a" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">代码 1。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="c7e2" class="nw lb it ns b gy nx ny l nz oa">class ToSupervised(base.BaseEstimator,base.TransformerMixin):<br/>    <br/>    def __init__(self,col,groupCol,numLags,dropna=False):<br/>        <br/>        self.col = col<br/>        self.groupCol = groupCol<br/>        self.numLags = numLags<br/>        self.dropna = dropna<br/>        <br/>    def fit(self,X,y=None):<br/>        self.X = X<br/>        return self<br/>    <br/>    def transform(self,X):</span><span id="377c" class="nw lb it ns b gy ob ny l nz oa">        tmp = self.X.copy()<br/>        for i in range(1,self.numLags+1):<br/>            tmp[str(i)+'_Week_Ago'+"_"+self.col] =<br/>              tmp.groupby([self.groupCol])[self.col].shift(i) <br/>            <br/>        if self.dropna:<br/>            tmp = tmp.dropna()<br/>            tmp = tmp.reset_index(drop=True)<br/>            <br/>        <br/>            <br/>        return tmp</span></pre><p id="5476" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">代码 2。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="4748" class="nw lb it ns b gy nx ny l nz oa">class ToSupervisedDiff(base.BaseEstimator,base.TransformerMixin):<br/>    <br/>    def __init__(self,col,groupCol,numLags,dropna=False):<br/>        <br/>        self.col = col<br/>        self.groupCol = groupCol<br/>        self.numLags = numLags<br/>        self.dropna = dropna<br/>        <br/>    def fit(self,X,y=None):<br/>        self.X = X<br/>        return self<br/>    <br/>    def transform(self,X):<br/>        tmp = self.X.copy()<br/>        for i in range(1,self.numLags+1):<br/>            tmp[str(i)+'_Week_Ago_Diff_'+"_"+self.col] = <br/>               tmp.groupby([self.groupCol])[self.col].diff(i) <br/>            <br/>        if self.dropna:<br/>            tmp = tmp.dropna()<br/>            tmp = tmp.reset_index(drop=True)<br/>            <br/>        return tmp</span></pre><p id="da7e" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">代码 3。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="74a6" class="nw lb it ns b gy nx ny l nz oa">from itertools import chain<br/>class Kfold_time(object):<br/>    <br/>    def __init__(self,**options):<br/>        <br/>        <br/>        self.target     = options.pop('target', None)<br/>        self.date_col   = options.pop('date_col', None)<br/>        self.date_init  = options.pop('date_init', None)<br/>        self.date_final = options.pop('date_final', None)</span><span id="624a" class="nw lb it ns b gy ob ny l nz oa">        if options:<br/>            raise TypeError("Invalid parameters passed: %s" %<br/>                               str(options))<br/>            <br/>        if ((self.target==None )|(self.date_col==None )|<br/>            (self.date_init==None )|(self.date_final==None )):<br/>             <br/>             raise TypeError("Incomplete inputs")<br/>    <br/>    def _train_test_split_time(self,X):<br/>        n_arrays = len(X)<br/>        if n_arrays == 0:<br/>            raise ValueError("At least one array required as input")</span><span id="a1c7" class="nw lb it ns b gy ob ny l nz oa">         for i in range(self.date_init,self.date_final):</span><span id="4d1d" class="nw lb it ns b gy ob ny l nz oa">            train = X[X[self.date_col] &lt; i]<br/>            val   = X[X[self.date_col] == i]</span><span id="3d6c" class="nw lb it ns b gy ob ny l nz oa">             X_train, X_test = train.drop([self.target], axis=1),<br/>                                val.drop([self.target], axis=1)</span><span id="9196" class="nw lb it ns b gy ob ny l nz oa">            y_train, y_test = train[self.target].values,<br/>                               val[self.target].values</span><span id="a443" class="nw lb it ns b gy ob ny l nz oa">            yield X_train, X_test, y_train, y_test</span><span id="b585" class="nw lb it ns b gy ob ny l nz oa">     def split(self,X):<br/>        cv_t = self._train_test_split_time(X)<br/>        return chain(cv_t)</span></pre><p id="6eee" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">代码 4。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="1228" class="nw lb it ns b gy nx ny l nz oa">class BaseEstimator(base.BaseEstimator, base.RegressorMixin):<br/>    def __init__(self, predCol):<br/>        """<br/>            As a base model we assume the number of sales <br/>            last week and this week are the same<br/>            Input: <br/>                    predCol: l-week ago sales<br/>        """<br/>        self.predCol = predCol</span><span id="8e60" class="nw lb it ns b gy ob ny l nz oa">    def fit(self, X, y):<br/>        return self</span><span id="ccc1" class="nw lb it ns b gy ob ny l nz oa">    def predict(self, X):<br/>        prediction = X[self.predCol].values<br/>        return prediction</span><span id="3d6a" class="nw lb it ns b gy ob ny l nz oa">    def score(self, X, y,scoring):<br/>        <br/>        prediction = self.predict(X)<br/>    <br/>        error =scoring(y, prediction)</span><span id="c8e4" class="nw lb it ns b gy ob ny l nz oa">        return error</span></pre><p id="ae1d" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">代码 5。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="c3b9" class="nw lb it ns b gy nx ny l nz oa">class TimeSeriesRegressor(base.BaseEstimator, base.RegressorMixin):<br/>    <br/>    def __init__(self,model,cv,scoring,verbosity=True):<br/>        self.model = model<br/>        self.cv = cv<br/>        self.verbosity = verbosity<br/>        self.scoring = scoring <br/>        <br/>            <br/>    def fit(self,X,y=None):<br/>        return self<br/>        <br/>    <br/>    def predict(self,X=None):<br/>        <br/>        pred = {}<br/>        for indx,fold in enumerate(self.cv.split(X)):</span><span id="1aab" class="nw lb it ns b gy ob ny l nz oa">            X_train, X_test, y_train, y_test = fold    <br/>            self.model.fit(X_train, y_train)<br/>            pred[str(indx)+'_fold'] = self.model.predict(X_test)<br/>            <br/>        prediction = pd.DataFrame(pred)<br/>    <br/>        return prediction</span><span id="7b36" class="nw lb it ns b gy ob ny l nz oa">    def score(self,X,y=None):</span><span id="5aa8" class="nw lb it ns b gy ob ny l nz oa">        errors = []<br/>        for indx,fold in enumerate(self.cv.split(X)):</span><span id="3c9e" class="nw lb it ns b gy ob ny l nz oa">            X_train, X_test, y_train, y_test = fold    <br/>            self.model.fit(X_train, y_train)<br/>            prediction = self.model.predict(X_test)<br/>            error = self.scoring(y_test, prediction)<br/>            errors.append(error)</span><span id="63ef" class="nw lb it ns b gy ob ny l nz oa">            if self.verbosity:<br/>                print("Fold: {}, Error: {:.4f}".format(indx,error))</span><span id="3d9a" class="nw lb it ns b gy ob ny l nz oa">         if self.verbosity:<br/>            print('Total Error {:.4f}'.format(np.mean(errors)))</span><span id="7dfd" class="nw lb it ns b gy ob ny l nz oa">        return errors</span></pre><p id="f9f7" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">代码 6。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="c434" class="nw lb it ns b gy nx ny l nz oa">class TimeSeriesRegressorLog(base.BaseEstimator,<br/>                                 base.RegressorMixin):<br/>    <br/>    def __init__(self,model,cv,scoring,verbosity=True):<br/>        self.model = model<br/>        self.cv = cv<br/>        self.verbosity = verbosity<br/>        self.scoring = scoring<br/>        <br/>            <br/>    def fit(self,X,y=None):<br/>        return self<br/>        <br/>    <br/>    def predict(self,X=None):<br/>        <br/>        pred = {}<br/>        for indx,fold in enumerate(self.cv.split(X)):</span><span id="0624" class="nw lb it ns b gy ob ny l nz oa">            X_train, X_test, y_train, y_test = fold    <br/>            self.model.fit(X_train, y_train)<br/>            pred[str(indx)+'_fold'] = self.model.predict(X_test)<br/>            <br/>        prediction = pd.DataFrame(pred)<br/>    <br/>        return prediction</span><span id="eb6d" class="nw lb it ns b gy ob ny l nz oa">    def score(self,X,y=None):#**options):</span><span id="6402" class="nw lb it ns b gy ob ny l nz oa">        errors = []<br/>        for indx,fold in enumerate(self.cv.split(X)):</span><span id="8239" class="nw lb it ns b gy ob ny l nz oa">            X_train, X_test, y_train, y_test = fold    <br/>            self.model.fit(X_train, np.log1p(y_train))<br/>            prediction = np.expm1(self.model.predict(X_test))<br/>            error = self.scoring(y_test, prediction)<br/>            errors.append(error)</span><span id="eb1f" class="nw lb it ns b gy ob ny l nz oa">            if self.verbosity:<br/>                print("Fold: {}, Error: {:.4f}".format(indx,error))</span><span id="be8d" class="nw lb it ns b gy ob ny l nz oa">        if self.verbosity:<br/>            print('Total Error {:.4f}'.format(np.mean(errors)))</span><span id="9b28" class="nw lb it ns b gy ob ny l nz oa">        return errors</span></pre><p id="fb69" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">代码 7。</p><p id="f8c1" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">答:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="e8dd" class="nw lb it ns b gy nx ny l nz oa">def getDataFramePipeline(i):<br/>    steps = [(str(i)+'_step',<br/>              ToSupervised('Sales','Product_Code',i))]<br/>    for j in range(1,i+1):<br/>        if i==j:</span><span id="26a5" class="nw lb it ns b gy ob ny l nz oa">            pp = (str(j)+'_step_diff',<br/>                  ToSupervisedDiff(str(i)+'_Week_Ago_Sales',<br/>                                   'Product_Code',1,dropna=True))</span><span id="9c38" class="nw lb it ns b gy ob ny l nz oa">            steps.append(pp)<br/>        else:</span><span id="1bd4" class="nw lb it ns b gy ob ny l nz oa">            pp = (str(j)+'_step_diff',  <br/>                  ToSupervisedDiff(str(i)+'_Week_Ago_Sales',<br/>                                   'Product_Code',1))</span><span id="f0b6" class="nw lb it ns b gy ob ny l nz oa">            steps.append(pp)<br/>            <br/>    return steps</span></pre><p id="1643" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">乙:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="f736" class="nw lb it ns b gy nx ny l nz oa">from tqdm import tqdm<br/>def stepsTune(X,model,num_steps,init=1):<br/>    scores = []<br/>    for i in tqdm(range(init,num_steps+1)):<br/>        steps = []<br/>        steps.extend(getDataFramePipeline(i))<br/>        steps.append(('predic_1',model))<br/>        super_ = Pipeline(steps).fit(X)<br/>        score_ = np.mean(super_.score(X))<br/>        scores.append((i,score_))<br/>        <br/>    return scores</span></pre><p id="7376" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">代码 8。</p><p id="80f1" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">答:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="4ca2" class="nw lb it ns b gy nx ny l nz oa">from collections.abc import Mapping, Sequence, Iterable<br/>from itertools import product<br/>from functools import partial, reduce<br/>import operator</span><span id="cefd" class="nw lb it ns b gy ob ny l nz oa">class TimeGridBasic(base.BaseEstimator, base.RegressorMixin):<br/>    <br/>    def __init__(self,param_grid):<br/>        <br/>    <br/>        if not isinstance(param_grid, (Mapping, Iterable)):<br/>                raise TypeError('Parameter grid is not a dict or '<br/>                                'a list ({!r})'.format(param_grid))</span><span id="e68f" class="nw lb it ns b gy ob ny l nz oa">        if isinstance(param_grid, Mapping):<br/>                # wrap dictionary in a singleton list to support<br/>                  either dict<br/>                # or list of dicts<br/>                param_grid = [param_grid]</span><span id="fbdc" class="nw lb it ns b gy ob ny l nz oa">        if isinstance(param_grid, Mapping):<br/>                # wrap dictionary in a singleton list to support<br/>                  either dict<br/>                # or list of dicts<br/>                param_grid = [param_grid]</span><span id="6fdb" class="nw lb it ns b gy ob ny l nz oa">        # check if all entries are dictionaries of lists<br/>        for grid in param_grid:<br/>            if not isinstance(grid, dict):<br/>                raise TypeError('Parameter grid is not a '<br/>                                'dict ({!r})'.format(grid))<br/>            for key in grid:<br/>                if not isinstance(grid[key], Iterable):<br/>                    raise TypeError('Parameter grid value is not<br/>                                     iterable '<br/>                                    '(key={!r}, value={!r})'<br/>                                    .format(key, grid[key]))</span><span id="c3d4" class="nw lb it ns b gy ob ny l nz oa">        self.param_grid = param_grid<br/>                <br/>    def __iter__(self):<br/>        """Iterate over the points in the grid.<br/>        Returns<br/>        -------<br/>        params : iterator over dict of string to any<br/>            Yields dictionaries mapping each estimator parameter to<br/>            one of its<br/>            allowed values.<br/>        """<br/>        for p in self.param_grid:<br/>            # Always sort the keys of a dictionary, for<br/>             reproducibility<br/>            items = sorted(p.items())<br/>            if not items:<br/>                yield {}<br/>            else:<br/>                keys, values = zip(*items)<br/>                for v in product(*values):<br/>                    params = dict(zip(keys, v))<br/>                    yield params</span></pre><p id="f99c" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">乙:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="33f0" class="nw lb it ns b gy nx ny l nz oa">class TimeSeriesGridSearch(TimeGridBasic,base.BaseEstimator,<br/>                              base.RegressorMixin):<br/>    <br/>    <br/>    def __init__(self,**options):<br/>        <br/>        self.model      = options.pop('model', None)<br/>        self.cv         = options.pop('cv', None)<br/>        self.verbosity  = options.pop('verbosity', False)<br/>        self.scoring    = options.pop('scoring', None)<br/>        param_grid      = options.pop('param_grid', None)<br/>        self.param_grid = TimeGridBasic(param_grid)<br/>        <br/>        if options:<br/>            raise TypeError("Invalid parameters passed: %s" %<br/>                              str(options))</span><span id="ecb5" class="nw lb it ns b gy ob ny l nz oa">        if ((self.model==None )| (self.cv==None)):<br/>            raise TypeError("Incomplete inputs")<br/>            <br/>            <br/>    def fit(self,X,y=None):<br/>        self.X = X<br/>        return self</span><span id="048c" class="nw lb it ns b gy ob ny l nz oa">    def _get_score(self,param):</span><span id="400d" class="nw lb it ns b gy ob ny l nz oa">        errors = []<br/>        for indx,fold in enumerate(self.cv.split(self.X)):</span><span id="2ebb" class="nw lb it ns b gy ob ny l nz oa">            X_train, X_test, y_train, y_test = fold    <br/>            self.model.set_params(**param).fit(X_train, y_train)<br/>            prediction = self.model.predict(X_test)<br/>            error = self.scoring(y_test, prediction)<br/>            errors.append(error)</span><span id="3aab" class="nw lb it ns b gy ob ny l nz oa">            if self.verbosity:<br/>                print("Fold: {}, Error: {:.4f}".format(indx,error))</span><span id="a76e" class="nw lb it ns b gy ob ny l nz oa">        if self.verbosity:<br/>            print('Total Error {:.4f}'.format(np.mean(errors)))<br/>                <br/>        <br/>        return errors</span><span id="649b" class="nw lb it ns b gy ob ny l nz oa">     def score(self):</span><span id="f58e" class="nw lb it ns b gy ob ny l nz oa">        errors=[]<br/>        get_param = []<br/>        for param in self.param_grid:<br/>            <br/>            if self.verbosity:<br/>                print(param)<br/>                <br/>            errors.append(np.mean(self._get_score(param)))<br/>            get_param.append(param)</span><span id="a94e" class="nw lb it ns b gy ob ny l nz oa">        self.sorted_errors,self.sorted_params = <br/>          (list(t) for t in zip(*sorted(zip(errors,get_param))))<br/>        <br/>        return self.sorted_errors,self.sorted_params<br/>    <br/>    <br/>    def best_estimator(self,verbosity=False):</span><span id="2d9d" class="nw lb it ns b gy ob ny l nz oa">        if verbosity:<br/>            print('error: {:.4f} \n'.format(self.sorted_errors[0]))<br/>            print('Best params:')<br/>            print(self.sorted_params[0])</span><span id="da64" class="nw lb it ns b gy ob ny l nz oa">        return self.sorted_params[0]</span></pre></div></div>    
</body>
</html>