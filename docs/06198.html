<html>
<head>
<title>All About Backup Diagram</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于备份图表的所有信息</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-about-backup-diagram-fefb25aaf804?source=collection_archive---------14-----------------------#2019-09-07">https://towardsdatascience.com/all-about-backup-diagram-fefb25aaf804?source=collection_archive---------14-----------------------#2019-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/8a718e7d3372cd6d20902ff5fccb1c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPcVxHzEKJfDaOR2TGnOpA.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="239b" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">解释强化学习算法的图表</h2></div><p id="5c2c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">众所周知，一幅画胜过千言万语；备份图给出了强化学习中不同算法和模型的可视化表示。</p><p id="792c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">备份过程(<strong class="kv jf">更新操作</strong>)通过表示状态、动作、状态转换、奖励等，是算法的图形化表示..价值函数(状态或状态动作)从它的后继状态或状态动作被<strong class="kv jf">转移回</strong>一个状态(或状态动作)。</p><p id="ec4b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在备份图中，状态值由空心圆表示，而状态-动作值或动作值由实心圆表示。动作由从状态开始的箭头表示。奖励通常显示在行动值之后。产生最大动作值动作显示为从一个状态开始的弧线。状态值、动作值、动作、最大动作值、状态转换的标准表示见下图。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lp"><img src="../Images/f04c4557bb9463b4d4e7f43abf6c85b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8ey1DgP4IDXyoqKQ.png"/></div></div></figure><h1 id="a6be" class="lu lv je bd lw lx ly lz ma mb mc md me kk mf kl mg kn mh ko mi kq mj kr mk ml bi translated">随机策略π下的状态值函数</h1><p id="e917" class="pw-post-body-paragraph kt ku je kv b kw mm kf ky kz mn ki lb lc mo le lf lg mp li lj lk mq lm ln lo im bi translated">现在我将介绍如何使用备份图显示状态值。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lp"><img src="../Images/81c5674c60f1e12f7c3f914f839ffe7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*r0lQMY-ateUvUV9p.png"/></div></div></figure><ol class=""><li id="b641" class="mr ms je kv b kw kx kz la lc mt lg mu lk mv lo mw mx my mz bi translated">s 是起始状态，它是根节点</li><li id="188e" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">从状态 s 可以有三个动作，如箭头所示，代理按照策略π采取动作</li><li id="0d2e" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">状态动作值或动作用实线圆圈表示(惯例是从状态采取动作，采取动作后领取奖励)。</li><li id="710c" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">一旦采取行动，如果是具有一定状态转移概率的随机环境，则代理可以结束于不同的状态(在确定性环境中，代理结束于特定行动的特定状态)。我已经展示了在采取最正确的行动后，代理可以进入的 3 种可能状态。3 个转换显示在蓝色箭头中，转换概率为 p。获得的回报为 r，这也取决于转换动态和采取的行动。代理转换到新状态 s’。</li></ol><h1 id="c89b" class="lu lv je bd lw lx ly lz ma mb mc md me kk mf kl mg kn mh ko mi kq mj kr mk ml bi translated">随机政策π下的国家行为价值函数</h1><p id="2923" class="pw-post-body-paragraph kt ku je kv b kw mm kf ky kz mn ki lb lc mo le lf lg mp li lj lk mq lm ln lo im bi translated">类似于状态值函数，我们可以为动作值函数或状态-动作值函数创建备份图。在这种情况下，根节点是实心圆，因为它是来自特定状态的特定动作。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lp"><img src="../Images/1101e5badec15e22ef15f3ee2ad07c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tPOQ54cl3ANz-jsv.png"/></div></div></figure><p id="26b9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在下面的备份图中，我展示了每个组件是如何连接的，以便更直观。这让我们对 MDP 有了更好的理解，因为我们可以把这个图扩展到整个状态空间。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lp"><img src="../Images/a4f0da4510efe8b1b82808352a344924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iVkEz0D0s1-Ks8nw.png"/></div></div></figure><ol class=""><li id="b0c5" class="mr ms je kv b kw kx kz la lc mt lg mu lk mv lo mw mx my mz bi translated">在状态 s 下，状态值为 vπ(s)</li><li id="975d" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">从状态 s，代理可以采取 3 个动作(a1，a2，a3)</li><li id="35c4" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">对于所采取的动作，动作值为 qπ(s，a ),其中 a = {a1，a2，a3}</li><li id="a12c" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">这里，代理采取了行动 a3。它可以分别以转移概率 p1、p2 或 p3 到达状态 s’1、s’2 或 s’3(注意，如果代理选择动作 a2 或 a3，将会有不同的状态，并且相应的转移概率将是适用的。</li><li id="14fe" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">收集的奖励根据其落地状态显示为 r1、r2 或 r3。</li></ol><h1 id="c5b1" class="lu lv je bd lw lx ly lz ma mb mc md me kk mf kl mg kn mh ko mi kq mj kr mk ml bi translated">最佳状态值和最佳动作值</h1><p id="6df1" class="pw-post-body-paragraph kt ku je kv b kw mm kf ky kz mn ki lb lc mo le lf lg mp li lj lk mq lm ln lo im bi translated">下图显示了特定状态 s 的状态值的贝尔曼最优性方程，以及从状态 s 采取的动作 a 的状态动作值的贝尔曼最优性方程。根据贝尔曼最优性方程，这是确保最优性的状态的最大动作(在后续状态中产生最大状态值的动作)。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lp"><img src="../Images/899a2f165055389e14a42debcab49617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6UMWl8MxHQ071yxF.png"/></div></div></figure><p id="35b7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">备份图可用于显示在等式中使用值函数的 RL 算法的图形表示。下面是几个更广为人知的算法，当我们参考备份图时很容易理解。</p><h1 id="18ac" class="lu lv je bd lw lx ly lz ma mb mc md me kk mf kl mg kn mh ko mi kq mj kr mk ml bi translated">蒙特卡洛状态值</h1><p id="26f4" class="pw-post-body-paragraph kt ku je kv b kw mm kf ky kz mn ki lb lc mo le lf lg mp li lj lk mq lm ln lo im bi translated">蒙特卡洛方法是一个非常简单的概念，当智能体与环境交互时，智能体学习状态和奖励。在这种方法中，代理生成经验样本，然后基于平均回报，为一个<strong class="kv jf">状态</strong>计算值。下面是蒙特卡罗(MC)方法的主要特征:</p><ol class=""><li id="be04" class="mr ms je kv b kw kx kz la lc mt lg mu lk mv lo mw mx my mz bi translated">没有模型(代理不知道状态 MDP 转换)</li><li id="b172" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">经纪人<strong class="kv jf">向<strong class="kv jf">学习</strong>被取样</strong>的经验</li><li id="2319" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">通过体验来自所有采样剧集的<strong class="kv jf">平均</strong>回报，学习策略π下的状态值 vπ(s )(值=平均回报)</li><li id="7eb7" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">仅在<strong class="kv jf">完成一集</strong>后，值被更新(因为该算法收敛缓慢，并且更新发生在<strong class="kv jf">集完成</strong>后)</li><li id="36bc" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">没有自举</li><li id="596a" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">仅可用于<strong class="kv jf">偶发性问题</strong></li></ol><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lp"><img src="../Images/5f1b06308cea630452d80800bfba2d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bHA7zkNNlus2GFQ5.png"/></div></div></figure><h1 id="65cd" class="lu lv je bd lw lx ly lz ma mb mc md me kk mf kl mg kn mh ko mi kq mj kr mk ml bi translated">蒙特卡洛状态作用值</h1><p id="819f" class="pw-post-body-paragraph kt ku je kv b kw mm kf ky kz mn ki lb lc mo le lf lg mp li lj lk mq lm ln lo im bi translated">在该方法中，代理生成经验样本，然后基于平均回报，为一个<strong class="kv jf">状态-动作</strong>计算值。所以这里的起点是状态-行动，终点是状态。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lp"><img src="../Images/4bbd62c87cf73c08cb3aa6591cac6c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fCtbYDSZ7JFJ_7BM.png"/></div></div></figure><h1 id="5473" class="lu lv je bd lw lx ly lz ma mb mc md me kk mf kl mg kn mh ko mi kq mj kr mk ml bi translated">时间差 TD(0)</h1><p id="bfcb" class="pw-post-body-paragraph kt ku je kv b kw mm kf ky kz mn ki lb lc mo le lf lg mp li lj lk mq lm ln lo im bi translated">时域差分法是蒙特卡罗法和动态规划法的结合。</p><p id="2aa9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下面是蒙特卡罗(MC)方法的主要特征:</p><ol class=""><li id="06e0" class="mr ms je kv b kw kx kz la lc mt lg mu lk mv lo mw mx my mz bi translated">没有模型(代理不知道状态 MDP 转换)</li><li id="d826" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">代理人<strong class="kv jf">从<strong class="kv jf">采样的</strong>经验中学习</strong>(类似于 MC)</li><li id="ec0d" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">像 DP 一样，TD 方法部分基于其他<strong class="kv jf">学习估计</strong>更新估计，而不等待最终结果(它们<strong class="kv jf">像 DP 一样引导</strong>)。</li><li id="1408" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">它可以从<strong class="kv jf">不完整事件</strong>中学习，因此该方法也可以用于连续问题</li><li id="0e39" class="mr ms je kv b kw na kz nb lc nc lg nd lk ne lo mw mx my mz bi translated">TD 将猜测更新为猜测，并根据实际经验修改猜测</li></ol><p id="d409" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">TD(0)是 TD 学习的最简单形式。在这种形式的 TD 学习中，在每一步之后，用下一个状态的值来更新值函数，并且沿途获得奖励。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lp"><img src="../Images/ca99068813a9869120afeaec07ae20da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uYvVjE__r4tsAz8E.png"/></div></div></figure><h1 id="8cbc" class="lu lv je bd lw lx ly lz ma mb mc md me kk mf kl mg kn mh ko mi kq mj kr mk ml bi translated">萨尔萨</h1><p id="6f8d" class="pw-post-body-paragraph kt ku je kv b kw mm kf ky kz mn ki lb lc mo le lf lg mp li lj lk mq lm ln lo im bi translated">用于控制或改进的 TD 算法之一是 SARSA。SARSA 的名字来源于这样一个事实，即代理从一个状态-动作值对向另一个状态-动作值对迈出一步，并在此过程中收集奖励 R(因此是 S( t)，A (t)，R( t+1)，S(t+1)和 A (t+1)元组创建了术语<strong class="kv jf"> S，A，R，S，A </strong>)。SARSA 是<strong class="kv jf"> on-policy </strong>法。SARSA 使用动作值函数 Q 并遵循策略π。SARSA 备份图如下所示。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lp"><img src="../Images/5c9eaa74615a1a86cd4bf3426855066a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eCxNf0x_6EQbrL-2.png"/></div></div></figure><h1 id="9cc7" class="lu lv je bd lw lx ly lz ma mb mc md me kk mf kl mg kn mh ko mi kq mj kr mk ml bi translated">结论</h1><p id="2673" class="pw-post-body-paragraph kt ku je kv b kw mm kf ky kz mn ki lb lc mo le lf lg mp li lj lk mq lm ln lo im bi translated">备份图对于传达 RL 算法步骤非常有用。这提供了一种不用复杂的数学符号就能理解算法的方法。</p><h2 id="2e90" class="nf lv je bd lw ng nh dn ma ni nj dp me lc nk nl mg lg nm nn mi lk no np mk nq bi translated">感谢阅读。可以联系我@ <a class="ae nr" href="http://www.linkedin.com/in/baijayantaroy" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>。</h2></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="54a6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">只需每月 5 美元，就可以无限制地获取最鼓舞人心的内容…点击下面的链接，成为媒体会员，支持我的写作。谢谢大家！<br/><a class="ae nr" href="https://baijayanta.medium.com/membership" rel="noopener"><strong class="kv jf"><em class="nz">https://baijayanta.medium.com/membership</em></strong></a></p></div></div>    
</body>
</html>