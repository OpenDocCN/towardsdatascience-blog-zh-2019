<html>
<head>
<title>[ML] Feature Transformation, NLP, and Classification Modeling on Hotel Scoring</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[ML]酒店评分的特征转换、自然语言处理和分类建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ml-feature-transformation-nlp-and-classification-modeling-on-hotel-scoring-94107353a015?source=collection_archive---------18-----------------------#2019-02-15">https://towardsdatascience.com/ml-feature-transformation-nlp-and-classification-modeling-on-hotel-scoring-94107353a015?source=collection_archive---------18-----------------------#2019-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f322" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">结合数字特征和文本评论来建立分类模型(神经网络、Xgboost、逻辑)</h2></div><h1 id="4fa3" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">0.摘要</h1><h2 id="a820" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated"><strong class="ak"> 0.1 奖励</strong></h2><p id="541e" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">这篇文章的目的是，在我之前的尝试中，我试图为每家酒店的点评者评分建立一个预测模型。事实证明，预测能力并不令人满意，T2 R T3(决定系数)徘徊在 T4 0.4 到 0.5 T5 之间。因此，在这项任务中，我将尝试一种不同的方法，首先将评论者的分数离散化为三个有序类别—低、中、高，然后建立一个分类模型。看看在简化分类方法下是否达到更高的预测能力。该模型应该更容易将评审者的分数分为三个级别，而不是要求得出精确的预测分数。</p><h2 id="6c4a" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated"><strong class="ak"> 0.2 数据集的信息</strong></h2><p id="1a67" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">原始数据集从 Booking.com 收集，并在[ <a class="ae mf" href="https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe" rel="noopener ugc nofollow" target="_blank"> Kaggle </a> ]平台上共享。该数据集包含 50 万条酒店评论，收集了 14 个特征，包括数字评分和文本评论。</p><h2 id="3fe2" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated"><strong class="ak"> 0.3 车型性能快速汇总</strong></h2><p id="bcaf" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">在比较了 MLP、Xgboost 和 Logistic 的模型性能后，我最终选择了<strong class="lo ir"> Logistic 模型</strong>。结果表明，逻辑斯蒂模型不仅比其他模型表现更好，尽管差距很小，而且在训练上也有相当好的效率。在此任务中还实施了超参数微调，但是结果显示默认配置和优化参数设置之间没有显著的性能差异。</p><p id="7bb1" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><strong class="lo ir">最终模型准确率达到 0.62，F1 评分达到 0.61。</strong>如果我们看得更深一点，每一个阶层在人口中所占的份额分别是 35%、29%、36%，与每一个阶层的预测结果的精度分别是 68%、47%、65%相比，我们可以说<strong class="lo ir"> Logistic 模型的精度一般是</strong>的两倍。</p><h2 id="5401" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated"><strong class="ak"> 0.4 报告的结构</strong></h2><blockquote class="ml mm mn"><p id="46ac" class="lm ln mo lo b lp mg jr lr ls mh ju lu mp mi lw lx mq mj lz ma mr mk mc md me ij bi translated">1.<a class="ae mf" href="#e0d7" rel="noopener ugc nofollow">数据预处理</a></p></blockquote><p id="a9dc" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">在这一部分，原始数据将经过一系列预处理，以便成为高质量的、随时可用的数据集。它包括缺失值移除、重复移除和异常值移除的实现。</p><blockquote class="ml mm mn"><p id="7c98" class="lm ln mo lo b lp mg jr lr ls mh ju lu mp mi lw lx mq mj lz ma mr mk mc md me ij bi translated">2.<a class="ae mf" href="#ab05" rel="noopener ugc nofollow">探索性可视化</a></p></blockquote><p id="5d37" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">探索性分析包括目标变量和所有特征的统计汇总。可视化数据并观察要素的分布情况。</p><blockquote class="ml mm mn"><p id="6573" class="lm ln mo lo b lp mg jr lr ls mh ju lu mp mi lw lx mq mj lz ma mr mk mc md me ij bi translated">3.<a class="ae mf" href="#283e" rel="noopener ugc nofollow">特征工程—离散化</a></p></blockquote><p id="9293" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">在更加熟悉手头的数据后，我将首先<strong class="lo ir">把目标变量分成三组——低、中、高</strong>。基于目标变量的分布来定制每个箱的范围，因此，目标变量具有不相等的箱宽度。</p><blockquote class="ml mm mn"><p id="ecbd" class="lm ln mo lo b lp mg jr lr ls mh ju lu mp mi lw lx mq mj lz ma mr mk mc md me ij bi translated">4.<a class="ae mf" href="#6de8" rel="noopener ugc nofollow">特征工程—日志转换</a></p></blockquote><p id="c13d" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">从分布图中可以看出，这些特征是高度倾斜的。在模型中输入时，最好使用正态分布特征。因此，大多数数字特征将具有<strong class="lo ir">对数变换</strong>，并且<strong class="lo ir">也被缩放以具有相同的单位</strong>，为稍后的建模做准备。</p><blockquote class="ml mm mn"><p id="eb72" class="lm ln mo lo b lp mg jr lr ls mh ju lu mp mi lw lx mq mj lz ma mr mk mc md me ij bi translated">5.<a class="ae mf" href="#0ddc" rel="noopener ugc nofollow">特征工程——词语标记化</a></p></blockquote><p id="b02a" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">这个数据集的优势在于它包含酒店客人的文本评论以及他们的评分。因此，我们实际上可以利用文本提供的信息来提高预测模型的准确性。在本节中，<strong class="lo ir">文本将通过词条化和标记化</strong>进行预处理。然后给单词分配根据<strong class="lo ir"> TF-IDF 公式</strong>计算的值。</p><blockquote class="ml mm mn"><p id="cd84" class="lm ln mo lo b lp mg jr lr ls mh ju lu mp mi lw lx mq mj lz ma mr mk mc md me ij bi translated">6.<a class="ae mf" href="#02b9" rel="noopener ugc nofollow">相关性分析</a></p></blockquote><p id="6de6" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">在这一部分，我将检查特征工程是否增加了目标变量(评审者的分数)和输入特征之间的相关性。结果显示在相关矩阵中。</p><blockquote class="ml mm mn"><p id="057a" class="lm ln mo lo b lp mg jr lr ls mh ju lu mp mi lw lx mq mj lz ma mr mk mc md me ij bi translated">7.<a class="ae mf" href="#a399" rel="noopener ugc nofollow">不同型号的试验</a></p></blockquote><p id="350b" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">有许多模型非常适合分类案例。在这里，在我的第一次尝试中，我将比较三种模式的性能— <strong class="lo ir">逻辑回归、Xgboost 决策树和神经网络</strong>。超参数设置为默认值。</p><blockquote class="ml mm mn"><p id="e5b4" class="lm ln mo lo b lp mg jr lr ls mh ju lu mp mi lw lx mq mj lz ma mr mk mc md me ij bi translated">8.<a class="ae mf" href="#85c9" rel="noopener ugc nofollow">微调造型</a></p></blockquote><p id="d8a8" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">从上述模型中，选择性能最好的模型作为最终模型，然后通过网格搜索对模型的超参数进行微调，以进一步提高其精度。最终模型将在测试数据集上进行测试，并与默认配置的模型进行比较。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="6ae1" class="kf kg iq bd kh ki mz kk kl km na ko kp jw nb jx kr jz nc ka kt kc nd kd kv kw bi translated">1.预处理数据</h1><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ne"><img src="../Images/f0a0d9e0837cb68aacc27138c7c89f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FW5lnCylSs-T2k6xEmiDiQ.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Columns in Dataset</figcaption></figure><p id="89d3" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><a class="ae mf" href="#7bb1" rel="noopener ugc nofollow"> ︽ </a>检查数据集行中是否有缺失值或重复值。未完成的行和重复的行将从数据集中删除。</p><p id="df25" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">总共有 3268 行包含缺失值和 526 个重复行。删除这些记录后，最终数据集仍有 511，944 条记录。</p><p id="ab05" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">对数据的样子感兴趣，可以查看链接[ <a class="ae mf" href="https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe" rel="noopener ugc nofollow" target="_blank"> Kaggle </a> ]。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="b6a7" class="kf kg iq bd kh ki mz kk kl km na ko kp jw nb jx kr jz nc ka kt kc nd kd kv kw bi translated">2.探索可视化</h1><p id="34b3" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated"><a class="ae mf" href="#a9dc" rel="noopener ugc nofollow"> ︽ </a>在此部分，将分别显示目标变量和所有其他数值特征。</p><h2 id="bed8" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">2.1 目标变量的分布(reviewer_score)</h2><p id="3b72" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">事先，我为下面的可视化定义了一个绘图函数。</p><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Function — Plot Histogram and Mean/Median Line</figcaption></figure><p id="9774" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><code class="fe nw nx ny nz b"><strong class="lo ir">Result:</strong></code></p><p id="f919" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">很明显，评论者的分数分布是<strong class="lo ir">左倾</strong>。有很大一部分分数分布在 9.5 分及以上。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oa"><img src="../Images/1058a110591e0a19f00066911299b2ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bVFQC8hsY_AM9FMKQmXGfw.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Distribution of reviewer_score</figcaption></figure><h2 id="f3d7" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">2.2 特征分布</h2><ul class=""><li id="fe1e" class="ob oc iq lo b lp lq ls lt lc od lf oe li of me og oh oi oj bi translated">平均分数</li><li id="3aeb" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me og oh oi oj bi translated">评论总数</li><li id="2983" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me og oh oi oj bi translated">附加得分次数</li><li id="3473" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me og oh oi oj bi translated">复习 _ 总计 _ 负数 _ 字数</li><li id="787a" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me og oh oi oj bi translated">复习总字数</li></ul><p id="8e44" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">以下几组网格图显示了每个数字特征的 QQ 图和直方图。<strong class="lo ir"> QQ-plot </strong>增加可读性，以检查特征是否遵循正态分布。此外，在内核分布旁边增加了一行<strong class="lo ir">理论正态分布</strong>，用于直方图比较。下面是创建 QQ-plot 和直方图并排的定义函数。</p><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Function — Plot QQ-Plot and Histogram Alongside</figcaption></figure><p id="9d99" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><code class="fe nw nx ny nz b"><strong class="lo ir">Result:</strong></code></p><p id="8b9a" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">我们可以看出，除了平均分数<em class="mo">和平均分数</em>之外，几乎所有的特征都是<strong class="lo ir">左偏或右偏</strong>。根据显示的证据，需要对这些特征进行转换和归一化，以便在使用这些特征作为输入时建模可以达到更高的精度。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi op"><img src="../Images/8e092c076c8abc35c2f61353f59f9b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3XcCsbzDJq9HgB8yDqvjw.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">QQ-Plot and Histogram over Each Numerical Feature (Before Transformation)</figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="8a29" class="kf kg iq bd kh ki mz kk kl km na ko kp jw nb jx kr jz nc ka kt kc nd kd kv kw bi translated">3.特征工程—离散化</h1><h2 id="37d2" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">3.1 目标变量</h2><p id="76b9" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated"><a class="ae mf" href="#5d37" rel="noopener ugc nofollow"> ︽ </a>关注目标变量— <strong class="lo ir">审核人 _ 分数</strong>，将分为三组。绑定间隔如下，</p><ol class=""><li id="f0df" class="ob oc iq lo b lp mg ls mh lc oq lf or li os me ot oh oi oj bi translated"><em class="mo">低:数值&lt; = 8.0。</em></li><li id="c22f" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me ot oh oi oj bi translated"><em class="mo">中:8.0 &lt;值&lt; = 9.5。</em></li><li id="d83b" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me ot oh oi oj bi translated"><em class="mo">高:9.5 &lt;值&lt; = 10.0。</em></li></ol><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Function — Bin up Variable</figcaption></figure><p id="4008" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><code class="fe nw nx ny nz b"><strong class="lo ir">Result:</strong></code></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/d9b0f741c7bd56f71674cebf63042b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*E2oV1FhqZtQmPDqDOa244A.png"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Bin Size and Share of Target Variable</figcaption></figure><p id="8b15" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">离散化后，目标变量分成三组，每组分别为<strong class="lo ir"> 35%、29%、36% </strong>。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="e44b" class="kf kg iq bd kh ki mz kk kl km na ko kp jw nb jx kr jz nc ka kt kc nd kd kv kw bi translated">4.特征工程—测井转换</h1><h2 id="f14d" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">4.1 功能</h2><p id="ca33" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated"><a class="ae mf" href="#9293" rel="noopener ugc nofollow"> <strong class="lo ir"> ︽ </strong> </a> <strong class="lo ir">数值特征首先进行对数变换，然后缩放到范围(0，1)内。</strong>通过这种方式，数值将具有带有分类特征的相同单位，稍后也将被热编码为(0，1)虚拟值。</p><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Log Transformation and MinMax Scaling</figcaption></figure><p id="ff7b" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><code class="fe nw nx ny nz b"><strong class="lo ir">Result:</strong></code></p><p id="1f48" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">从 QQ 图和直方图中，我们可以看出，即使经过变换，特征仍然不太符合正态分布，但至少它们现在不那么偏斜了。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ov"><img src="../Images/cbe57ecd6daa9fa9403414fa5de33413.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iSxHoI2fnXGfJJ7Pt1Q_VQ.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">QQ-Plot and Histogram over Each Numerical Feature (After Transformation)</figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="bbef" class="kf kg iq bd kh ki mz kk kl km na ko kp jw nb jx kr jz nc ka kt kc nd kd kv kw bi translated">5.特征工程——单词标记化</h1><h2 id="2018" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">5.1 整理文本</h2><p id="e853" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated"><a class="ae mf" href="#c13d" rel="noopener ugc nofollow"> ︽ </a>在清理文本数据时，我首先将两个独立的评论(一个负面，一个正面)连接成一个，然后让它去掉空白和其他标点符号。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ow"><img src="../Images/e4244e565c0974dfa2e1639b95e4c080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IaT2pT2DQrNkVbVfDx-UCQ.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Preview of Text Review (Before Concatenated and After)</figcaption></figure><h2 id="5845" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">5.2 词汇化和标记化</h2><p id="df55" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">在对文本进行分词之前，我使用<strong class="lo ir">分词器</strong>进一步提取每个单词的词根，从而剔除不同形式单词的派生词。</p><p id="bafa" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">然后<strong class="lo ir">停止字</strong>被忽略，并通过<strong class="lo ir"> max_df </strong>和<strong class="lo ir"> min_df </strong>指定字选择阈值。</p><p id="3604" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><strong class="lo ir">二元语法</strong>用于标记化，这样我们可以考虑短语中相反的意思，例如<em class="mo">非常好</em>和<em class="mo">不好</em>。</p><p id="4bf4" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">随后选择第一个<strong class="lo ir"> 2500 </strong>记号作为模型输入。</p><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Lemmatize and Tokenize</figcaption></figure><p id="608e" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><code class="fe nw nx ny nz b"><strong class="lo ir">Result:</strong></code></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ox"><img src="../Images/b3992f62112faffd00b1a9454cf9440e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WFAehQGRPmgso-2wTKFq4Q.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Samples of Tokenized Word (Both Uni-gram and Bi-gram)</figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="0d1d" class="kf kg iq bd kh ki mz kk kl km na ko kp jw nb jx kr jz nc ka kt kc nd kd kv kw bi translated">6.相关性检验</h1><p id="4eda" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated"><a class="ae mf" href="#b02a" rel="noopener ugc nofollow"> ︽ </a>下面，我将呈现 target (reviewer_score)和所有其他特征之间的相关性，包括特征转换之前和之后。看看相关性是因为变换而被放大了还是保持不变。</p><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Function — Plot Correlation Matrix</figcaption></figure><p id="8ad0" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><code class="fe nw nx ny nz b"><strong class="lo ir">Result:</strong></code></p><p id="9b2e" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">通过观察这种相关性，我得出了一些有趣的发现。</p><ol class=""><li id="5ac4" class="ob oc iq lo b lp mg ls mh lc oq lf or li os me ot oh oi oj bi translated">对于离散化的目标变量(reviewer_score_class ),所有特征的相关性(无论是否经过变换)似乎稍弱。比方说(平均分数与审阅者分数)的相关性为<strong class="lo ir"> 0.36 </strong>，而(平均分数与审阅者分数类)的相关性为<strong class="lo ir"> 0.34 </strong>。</li><li id="77da" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me ot oh oi oj bi translated">转换后的特征与目标变量有更强的相关性。实际上，当这些转换后的特征作为输入时，这对于后续的建模是一个好兆头。</li></ol><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oy"><img src="../Images/cf592c8efb9b293c8cb2b422022b8f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6kwqvoDGyhsTuMir2oh2g.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Correlation Matrix (Target, Discretized Target, and Standardized Features)</figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="3193" class="kf kg iq bd kh ki mz kk kl km na ko kp jw nb jx kr jz nc ka kt kc nd kd kv kw bi translated">7.不同模型的试验</h1><p id="adbb" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">︽:现在是整个任务的关键部分——建模。我将首先在三个不同的模型上进行实验，<strong class="lo ir">多感知机、Xgboost 和逻辑回归</strong>。</p><p id="0f3e" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">这些模型将基于它们在测试数据集上的<strong class="lo ir">性能(F1 分数)</strong>进行比较。最佳模型将进行到下一部分，即超参数微调。这样我们可以进一步提高它的准确性。</p><h2 id="6a86" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">7.1 MLP 模式—培训</h2><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="d86c" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">7.2 Xgboost 模型—培训</h2><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="9d89" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">7.3 物流模型——培训</h2><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="93bd" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">7.4 功能重要性</h2><p id="3a6f" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">在拟合 Xgboost 模型时，我们从中获得了额外的好处。Xgboost 可以为我们提供特征重要性的图表，这样我们就可以告诉我们什么样的特征对数据分类更有帮助和影响。</p><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Plot Top Feature Importance From Xgboost Model</figcaption></figure><p id="d4aa" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><code class="fe nw nx ny nz b"><strong class="lo ir">Result:</strong></code></p><p id="dd32" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">快速排序在这里，我们可以从特性重要性图(Xgboost)中看出，确定的前 5 个重要特性是，</p><ol class=""><li id="ed15" class="ob oc iq lo b lp mg ls mh lc oq lf or li os me ot oh oi oj bi translated">review _ total _ negative _ words _ count _ STD(数字)</li><li id="33fa" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me ot oh oi oj bi translated">review _ total _ positive _ words _ count _ STD(数字)</li><li id="b65b" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me ot oh oi oj bi translated">average_score_std(数字)</li><li id="0ef4" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me ot oh oi oj bi translated">位置(单词)</li><li id="026c" class="ob oc iq lo b lp ok ls ol lc om lf on li oo me ot oh oi oj bi translated">五线谱(word)</li></ol><p id="f531" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">值得注意的一点是，这里我选择同时使用单字和双字作为输入特征，似乎最重要的标记是<strong class="lo ir">单字</strong>，而不是一个双字排在前 20 个重要特征中。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oz"><img src="../Images/cb04e5639064822896972d10336fd60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3DM9NfyF0RAJgVWNJsv1Q.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Feature Importance Ranking</figcaption></figure><h2 id="9ba6" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">7.5 最终选择的模型:逻辑回归</h2><p id="6417" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">结果是，<strong class="lo ir">逻辑回归</strong>在测试数据集上达到了<strong class="lo ir">最佳性能(F1 分数)</strong>，尽管只是很小的差距。但考虑到它在如此大量的数据和特征(超过 2000 个特征)上的训练效率，这确实令人印象深刻。</p><p id="1170" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">因此，在下一节中，我将进一步微调逻辑回归的超参数。但总的来说，三个模型在分类数据集方面都表现得很好，准确率都在<strong class="lo ir"> 60% </strong>以上，相比之下<strong class="lo ir">随机猜测</strong>的准确率在<strong class="lo ir"> 33% </strong>左右，这是一个相当大的预测能力增益。下表列出了更多详细信息。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi pa"><img src="../Images/db2324999dd73ddd71cc20ee474f67e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOxCPwv3wLDVZIbKJV1j-Q.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Table of Performance over Models</figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="13fa" class="kf kg iq bd kh ki mz kk kl km na ko kp jw nb jx kr jz nc ka kt kc nd kd kv kw bi translated">8.微调选择的模型</h1><p id="3b27" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated"><a class="ae mf" href="#350b" rel="noopener ugc nofollow"> ︽ </a>在这一部分，我将进一步微调逻辑模型的超参数，看看它是否能进一步提高模型的性能。</p><p id="bdd4" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">被微调(网格搜索)的超参数是<strong class="lo ir"> C </strong>和<strong class="lo ir">惩罚</strong>。这两个参数都负责模型的正则化。</p><h2 id="758c" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">8.1 微调模型-培训</h2><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="f362" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">通过热图可视化，我们甚至可以更好地掌握不同超参数组合的整体性能。让我们看看下面的图表。</p><p id="8c51" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">在热图中绘制网格搜索分数的函数参考了<a class="ae mf" href="https://github.com/amueller/introduction_to_ml_with_python/blob/master/05-model-evaluation-and-improvement.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="mo">Python 机器学习入门</em> </a> <em class="mo">，</em>初学者必备的 ML 书。</p><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Function — Plot Grid Search Scores in Heat map</figcaption></figure><p id="0174" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><code class="fe nw nx ny nz b"><strong class="lo ir">Result:</strong></code></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/fdadcf8e1299685ab8204e43394a6aa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*qneXUMTROtGnOMYaS7lOnA.png"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Score Heatmap over Parameters</figcaption></figure><p id="69c5" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">选择的最佳参数是<strong class="lo ir"> C </strong>的<strong class="lo ir"> 0.5 </strong>和<strong class="lo ir"> L1 </strong>的<strong class="lo ir">罚</strong>。</p><p id="9e8a" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">似乎一旦 C 超过 0.5，模型的性能就达到了平稳状态。精度没有提高。</p><p id="e644" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">此外，微调带来的好处似乎微不足道，没有显著的改善。因为右上角的大多数分数始终保持在 0.61。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h2 id="824b" class="kx kg iq bd kh ky kz dn kl la lb dp kp lc ld le kr lf lg lh kt li lj lk kv ll bi translated">8.2 网格搜索前后的性能</h2><p id="1a38" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">现在，让我们检查一下，相对于没有超参数微调，它获得了多少改进。</p><p id="e2a0" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><code class="fe nw nx ny nz b"><strong class="lo ir">Result:</strong></code></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi pc"><img src="../Images/367a38bfeb6b4b2380eeef31fcf8a760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1az4T3IROEFbx9K5cnun5A.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Score of Before and After Grid Search on Test Data</figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="1cc0" class="kf kg iq bd kh ki mz kk kl km na ko kp jw nb jx kr jz nc ka kt kc nd kd kv kw bi translated">结论</h1><p id="f409" class="pw-post-body-paragraph lm ln iq lo b lp lq jr lr ls lt ju lu lc lv lw lx lf ly lz ma li mb mc md me ij bi translated">在对评论者的评分进行清理、转换、训练和微调分类模型的所有努力之后。该模型在识别低分和高分方面表现很好，但在识别处于中间的 T2 分数方面表现不佳。</p><p id="91d4" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">参见<strong class="lo ir">回忆</strong>，该模型实际上只能识别出所有得分中的<strong class="lo ir"> 31% </strong>，这意味着其余的<strong class="lo ir"> 69% </strong>被错误地归类为低水平或高水平。</p><p id="b0c3" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">尽管如此，如果我们看看<strong class="lo ir">精度</strong>，那么至少模型达到了<strong class="lo ir"> 47% </strong>的准确度，这意味着对于每个被标记为中等的样本，有 47%的几率是正确的。与随机猜测的结果相比，<strong class="lo ir"> 29% </strong>。这仍然被认为是一个重大的收获。</p><p id="fae5" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">综合 jupyter 笔记本可以在这个链接找到:【<a class="ae mf" href="https://github.com/TomLin/Playground/blob/master/01-Classification-Modeling-on-Hotel-Scoring.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>】。如果你喜欢这篇文章，欢迎在下面留下你的反馈。让我们一起把这个模型改进得更好！！！</p><p id="0fec" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated"><strong class="lo ir">参考:</strong></p><p id="c9b3" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">[1] <a class="ae mf" href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction" rel="noopener ugc nofollow" target="_blank">定制矢量器类</a>(版本 0.20.2)，sckite-learn</p><p id="c1de" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">[2] A. Muller，<a class="ae mf" href="https://github.com/amueller/introduction_to_ml_with_python/blob/master/05-model-evaluation-and-improvement.ipynb" rel="noopener ugc nofollow" target="_blank">用 Python 进行机器学习的介绍</a> (2018)，github</p><p id="1b1a" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">[3] S. Hallows，<a class="ae mf" href="https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn" rel="noopener ugc nofollow" target="_blank">探索 XGBoost 的使用及其与 Scikit 的集成-Learn </a> (2018)，kaggle</p><p id="9a0e" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">[4] S. Miller，<a class="ae mf" href="https://www.datasciencecentral.com/profiles/blogs/xgboost-with-python-part-0" rel="noopener ugc nofollow" target="_blank">XGBoost with Python</a>(2018)，数据科学中心</p><p id="d90f" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">[5] J. Brownlee，<a class="ae mf" href="https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/" rel="noopener ugc nofollow" target="_blank">Python 中 XGBoost 的特征重要性和特征选择</a> (2016)，机器学习掌握</p><p id="b85e" class="pw-post-body-paragraph lm ln iq lo b lp mg jr lr ls mh ju lu lc mi lw lx lf mj lz ma li mk mc md me ij bi translated">[6] J. Brownlee，<a class="ae mf" href="https://machinelearningmastery.com/save-gradient-boosting-models-xgboost-python/" rel="noopener ugc nofollow" target="_blank">Python 中如何用 XGBoost 保存梯度增强模型</a> (2016)，机器学习掌握</p></div></div>    
</body>
</html>