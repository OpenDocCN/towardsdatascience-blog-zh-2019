<html>
<head>
<title>Neural Networks with Sine Basis Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正弦基函数神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-with-sine-basis-function-c5c13fd63513?source=collection_archive---------14-----------------------#2019-09-13">https://towardsdatascience.com/neural-networks-with-sine-basis-function-c5c13fd63513?source=collection_archive---------14-----------------------#2019-09-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="11ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于在各个领域的成功，神经网络证明了它们是我们这个世纪的趋势主题。今天我要讲的是用正弦函数代替线性函数作为基的神经网络。我们将实现一个使用正弦函数作为基础的神经网络，我们将评估结果。</p><h1 id="2b14" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">这个想法是从哪里来的？</h1><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/afd8b8ede1b749a3257ca9e0d132cec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5FcQj7yuqBGy5XJo"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Photo by <a class="ae mc" href="https://unsplash.com/@benji3pr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Benjamin Lizardo</a> on <a class="ae mc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1d79" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用正弦函数作为神经网络基础的想法来自于我的信号处理背景。如果你以前听说过傅里叶变换，你知道你可以用余弦和正弦函数来表示任何函数，并且你可以在频域中清楚地显示求和函数。“你可以用余弦和正弦函数来表示任何函数”这句话打动了我。我把这个和神经网络联系起来，因为我们也可以用神经网络来表示任何函数。</p><h2 id="bdb2" class="md kp it bd kq me mf dn ku mg mh dp ky kb mi mj lc kf mk ml lg kj mm mn lk mo bi translated">说得够多了，我知道你想看到背后的数学，实现和结果。那么，让我们继续……</h2><h1 id="a49c" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">神经网络中的正弦基函数</h1><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/f98c9f80a1b9a17b6c5ed480a013247d.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*SAky9oYKOrplgkxI2Lb_Gw.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Sine Basis Function for Neural Networks</figcaption></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/e80c619a98910297ccc05ba8f6c936af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*-PWdlcGzSivWSWF4tD8pPQ.png"/></div></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/7551a012056c7eaa6c688888ab9cb9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*EGwTNaRc8SSjI0H2W1Wj9Q.jpeg"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Basic Structure</figcaption></figure><h1 id="9503" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">履行</h1><p id="f782" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">我使用了 Python 中<a class="ae mc" rel="noopener" target="_blank" href="/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6">如何从零开始构建自己的神经网络中的类和代码结构作为我的基础。</a></p><pre class="ln lo lp lq gt mx my mz na aw nb bi"><span id="c758" class="md kp it my b gy nc nd l ne nf">import numpy as np</span><span id="ee40" class="md kp it my b gy ng nd l ne nf">class NeuralNetwork:<br/>    #DEFINE INITIALS<br/>    def __init__(self, x, y):<br/>        self.input      = x<br/>        self.y          = y<br/>        self.weights_in = np.random.uniform(-1, 1,(self.y.shape[1],self.input.shape[1]))<br/>        self.weights_out = np.random.uniform(-1, 1,(self.y.shape[1],self.input.shape[1]))     <br/>        self.output     = np.random.uniform(-1, 1,self.y.shape)<br/>        print('Output:',self.output)<br/>        print('Y:',self.y)</span><span id="7623" class="md kp it my b gy ng nd l ne nf">#DEFINE FEEDFORWARD<br/>    def feedforward(self,cnt):<br/>        self.output[cnt] = np.sum(self.weights_out*self.input[cnt]*np.sin(np.pi*self.input[cnt]*self.weights_in),axis = 1)<br/>        <br/>    #DEFINE BACKPROPAGATION<br/>    def backprop(self,cnt):<br/>        error = np.square(self.y[cnt]-self.output[cnt])<br/>        derror_doutput = self.y[cnt]-self.output[cnt]<br/>        doutput_dweights_in = self.weights_out * np.square(self.input[cnt]) * np.pi * np.cos(np.pi*self.input[cnt]*self.weights_in)<br/>        doutput_dweights_out = self.input[cnt]*np.sin(np.pi*self.input[cnt]*self.weights_in)<br/>        dweights_in = np.dot(derror_doutput,doutput_dweights_in)<br/>        dweights_out = np.dot(derror_doutput,doutput_dweights_out)<br/>        self.weights_in += dweights_in*0.05<br/>        self.weights_out += dweights_out*0.05</span><span id="db4a" class="md kp it my b gy ng nd l ne nf">#PREDICT THE TEST DATA<br/>    def predict(self,input_):<br/>        predictions = []<br/>        for elm in input_:<br/>            predictions.append(np.sum(self.weights_out*elm*np.sin(np.pi*elm*self.weights_in),axis = 1).tolist())<br/>        return np.array(predictions)<br/>    <br/>    #SAVE WEIGHTS<br/>    def save_weights(self,dir_in = './weights_in.npy',dir_out = './weights_out.npy'):<br/>        np.save(dir_in,self.weights_in)<br/>        np.save(dir_out,self.weights_out)<br/>        <br/>    #IMPORT WEIGHTS<br/>    def import_weights(self,dir_in = './weights_in.npy',dir_out = './weights_out.npy'):<br/>        self.weights_in = np.load(dir_in)<br/>        self.weights_out = np.load(dir_out)</span></pre><p id="e5e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们定义了我们的阶级。测试时间到了。</p><p id="e6f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将用我们的神经网络来表示逻辑门(AND、OR、XOR)(这意味着我们将从神经网络为每个逻辑门获得 3 个输出)。我将为每个输入样本给出 3 个值。输入不是整数，而是浮点数，比如 0.01 而不是 0，或者 0.99 而不是 1。</p><h2 id="1e3e" class="md kp it bd kq me mf dn ku mg mh dp ky kb mi mj lc kf mk ml lg kj mm mn lk mo bi translated">示例:</h2><p id="9ac9" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">输入:[0.01，0.01，0.99]</p><p id="ff5c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与:0.01，或:0.99，异或:0.99</p><pre class="ln lo lp lq gt mx my mz na aw nb bi"><span id="93cf" class="md kp it my b gy nc nd l ne nf">x = np.array([[0.01,0.1,0.01],[0.01,0.99,0.99],[0.99,0.99,0.01],[0.99,0.99,0.01],[0.9,0.9,0.9]])<br/>y = np.array([[0.01,0.01,0.01],[0.01,0.99,0.99],[0.01,0.99,0.99],[0.01,0.99,0.99],[0.99,0.99,0.01]])</span><span id="bd33" class="md kp it my b gy ng nd l ne nf">nn = NeuralNetwork(x,y)<br/>#TRAIN NETWORK FOR 2000 TIMES.<br/>for gen_cnt in range(2000):<br/>    for cnt in range(5):<br/>        nn.feedforward(cnt)<br/>        nn.backprop(cnt)</span><span id="5ba2" class="md kp it my b gy ng nd l ne nf">#PREDICT THE TEST DATA<br/>predictions = nn.predict(np.array([[0.01,0.2,0.2],[0.9,0.1,0.95]]))<br/>print('Predictions:\n',np.around(predictions),'\nExpected:\n',[[0,0,0],[0,1,1]])</span></pre><p id="09c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我给出的测试数据是[[0.01，0.2，0.2]，[0.9，0.1，0.95]]，而不是[[0.01，0.01，0.01]，[0.99，0.01，0.99]]，看看网络是否做出了概括。</p><h1 id="6e4d" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">结果</h1><p id="fa8f" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">预言:<br/> [[ 0。0.-0.][ 0.1.1.]] <br/>预期:<br/> [[0，0，0]，[0，1，1]]</p><h2 id="3ee4" class="md kp it bd kq me mf dn ku mg mh dp ky kb mi mj lc kf mk ml lg kj mm mn lk mo bi translated">在我训练了几次之后，我发现最好的重量是:</h2><p id="5093" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated"><strong class="js iu">权重 _ 输入:</strong></p><p id="644c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">0.793539-0.866737 0.883148<br/>1.32484-0.20837 0.214222<br/>0.359695-1.21818-0.329374</p><p id="a953" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">权重 _ 出:</strong></p><ul class=""><li id="9d77" class="nh ni it js b jt ju jx jy kb nj kf nk kj nl kn nm nn no np bi translated">0.0841964-0.147961 0.854469<br/>-0.958252 0.0361619 1.2046<br/>0.73270.0986183-0.305971</li></ul><h1 id="54f4" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">结论</h1><p id="9b74" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">正如我们所看到的，“你可以用正弦函数表示任何函数”的概念也适用于神经网络。即使我们创建了一个没有任何隐藏层的神经网络，我们也证明了正弦函数可以代替线性函数作为基函数。因为它们是动态函数，我们不需要添加任何激活函数。由于正弦函数的性质，我们可以很容易地捕捉到非线性。我的下一步是实现深度神经网络的正弦基函数。这是一个复杂的问题，再次感谢正弦函数的性质，但挑战总是受欢迎的…</p></div></div>    
</body>
</html>