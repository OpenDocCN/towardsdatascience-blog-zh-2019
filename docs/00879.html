<html>
<head>
<title>Introduction to Augmented Random Search.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">增强随机搜索简介。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-augmented-random-search-d8d7b55309bd?source=collection_archive---------10-----------------------#2019-02-10">https://towardsdatascience.com/introduction-to-augmented-random-search-d8d7b55309bd?source=collection_archive---------10-----------------------#2019-02-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2106" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让 MuJoCo 学习快速有趣的方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/217b77d13479ac7b398d90291e0c5ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aEAysVNBdkNSCM9b"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@jwimmerli?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">jean wimmerlin</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0103" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">更新</strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae kv" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="00cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文基于加州大学伯克利分校的 Horia Mania、Aurelia Guy 和 Benjamin Recht 于 2018 年 3 月发表的一篇<a class="ae kv" href="https://arxiv.org/pdf/1803.07055.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><p id="56dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者声称，他们已经建立了一种算法，在 MuJoCo 运动基准上，该算法比最快的竞争对手无模型方法至少高效 15 倍。</p><p id="4adf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们将这种算法命名为增强随机搜索，简称 ARS。</p><h2 id="9847" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">问题是</h2><p id="44ca" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">正如在每个 RL 问题中一样，目标是找到一个策略来最大化代理在给定环境中遵循该策略时可能获得的期望回报。</p><h2 id="558f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">方法</h2><p id="b25c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">本文提出的解决方案是增强一种称为基本随机搜索的现有算法。</p><h2 id="6f3b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">基本随机搜索</h2><p id="d4eb" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">基本随机搜索的想法是挑选一个参数化的政策𝜋𝜃，通过对所有𝜹应用+𝛎𝜹和-𝛎𝜹(其中𝛎&lt; 1 is a constant noise and 𝜹 is a random number generate from a normal distribution).</p><p id="92a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Then apply the actions based on 𝜋(𝜃+𝛎𝜹) and 𝜋(𝜃-𝛎𝜹) then collect the rewards r(𝜃+𝛎𝜹) and r(𝜃-𝛎𝜹) resulting from those actions.</p><p id="5901" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Now that we have the rewards of the perturbed 𝜃, compute the average <br/>δ= 1/n *σ[r(𝜃+𝛎𝜹)-r(𝜃-𝛎𝜹)]𝜹来冲击(或扰动)参数𝜃，并且我们使用δ和学习率来更新参数。𝜃ʲ⁺=𝜃ʲ+𝝰.δ</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/bd49518497f364a1eec1521aa538bb11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lIL0rhIlEhT0dv0CA582Vw.png"/></div></div></figure><h1 id="37dd" class="mr lt iq bd lu ms mt mu lx mv mw mx ma jw my jx md jz mz ka mg kc na kd mj nb bi translated">增强随机搜索(ARS)</h1><p id="b3f9" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">ARS 是 BRS 的改进版本，它包含三个方面的增强功能，使其性能更高。</p><h2 id="f19b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">除以标准偏差𝞼ᵣ</h2><p id="917a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">随着迭代的进行，r(𝜃+𝛎𝜹和 r(𝜃-𝛎𝜹之间的差异可以显著变化，学习率𝝰固定，更新𝜃ʲ⁺=𝜃ʲ+𝝰.δ可能大幅振荡。例如，如果𝝰 = 0.01，δ= 10，那么𝝰.δ就是 0.1，但是如果δ变成 1000，𝝰.δ就变成 10。这种残酷的变化伤害了更新。请记住，我们的目标是让𝜃向回报最大化的价值观靠拢。<br/>为了避免这种类型的差异，我们将𝝰.δ除以𝞼ᵣ(所收集奖励的标准差)。</p><h2 id="9e0f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">使状态正常化</h2><p id="af8d" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">国家的正常化确保政策对国家的不同组成部分给予同等的重视。例如，假设一个状态分量取值范围为[90，100]，而另一个状态分量取值范围为[1，1]。然后，第一个状态分量将支配计算，而第二个不会有任何影响。为了获得一个直觉，考虑一个简单的平均值，假设 C1 = 91，C2 = 1，平均值将是(C1 + C2) / 2 = 92 / 2 = 46。现在假设 C2 急剧下降到最小值，C2 = -1。平均值将是(91–1)/2 = 45。<br/>注意，相对于 C2 的大幅下降，它几乎没有移动。<br/>现在让我们使用规范化。对于 C1 = 91，NC1 =(91–90)/(100–90)= 0.1，<br/>对于 C2 = 1，NC2 = (1 - (-1))/(1-(-1)) = 2/2 =1。<br/>归一化平均值将为(0.1 + 1)/2 = 0.55。<br/>现在如果 C2 下降到-1，NC2 = (-1-(-1))/2 = 0，归一化的平均值变成(0.1 + 0)/2 = 0.05。<br/>如你所见，平均值受到了 C2 急剧变化的极大影响。</p><p id="249f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ARS 中使用的标准化技术包括从状态输入中减去状态的当前观察平均值，然后除以状态的标准偏差:<br/>(state _ input-state _ observed _ average)/state _ STD</p><h2 id="aa9b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">使用最佳表现方向</h2><p id="9d31" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">记住我们的目标是最大化收集到的奖励是很有用的。然而，我们在每次迭代中计算平均奖励，这意味着在每次迭代中，我们在𝜋(𝜃+𝛎𝜹和𝜋(𝜃-𝛎𝜹之后计算 2N 集，然后我们对所有 2N 集收集的奖励 r(𝜃+𝛎𝜹和 r(𝜃-𝛎𝜹进行平均。这就带来了一些隐患，因为如果一些奖励相对于其他奖励来说很小，它们就会压低平均值。<br/>解决这个问题的一个方法是根据 max(r(𝜃+𝛎𝜹、r(𝜃-𝛎𝜹)).键将奖励按降序排列然后只使用排名前<strong class="ky ir">的<em class="nc"> b </em>和</strong>的奖励(以及它们各自的摄𝜹)来计算平均奖励。<br/>注意，当<strong class="ky ir"> <em class="nc"> b </em> = N </strong>时，算法将与没有此增强的算法相同。<br/>例如，假设我们有以下扰动及其各自的回报，如下表所列。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/27e48f4b70ae1a09d165e84b92a106bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*3LEE4bBWp2UHVNWAr0ra4Q.png"/></div></div></figure><p id="9e67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们按照键 max([r(𝜹i)、r(-𝜹i)])的降序对表进行排序，结果如下表所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/8832cb371ba4f518a6874fab4767ce0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*pmGUvRmzKz6sLKHcuvMaeQ.png"/></div></figure><p id="c596" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们假设<strong class="ky ir"> <em class="nc"> b </em> </strong> = 3，那么我们在平均计算中考虑这些数字:<br/>【𝜹9，r(𝜹9】，r(-𝜹9)]，【𝜹5，r(𝜹5】，r(-𝜹5)]，【𝜹1，r(𝜹1】，r(-1)]</p><h2 id="212c" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">ARS 算法</h2><p id="5949" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">最后，ARS 算法变成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/1b488e85a3cbc370ccad0de497a2c6be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RU9-goz_MC5YsW6VYjVcmQ.png"/></div></div></figure><p id="33c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单来说，它会变成如下形式:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="88be" class="ls lt iq nh b gy nl nm l nn no">Let 𝛎 a positive constant &lt; 1<br/>Let 𝝰 be the learning rate<br/>Let N the number of perturbations<br/>Let 𝜃 a (p x n) matrix representing the parameters of the policy 𝜋<br/>Let 𝜹i a (p x n) matrix representing the ith perturbation</span><span id="4b51" class="ls lt iq nh b gy np nm l nn no">1. While end condition not satisfied do:<br/>2. Generate N perturbations 𝜹 from a normal distribution<br/>3. Normalize 𝜋i+ = (𝜃+𝛎𝜹i)ᵀx and 𝜋i- = (𝜃-𝛎𝜹i)ᵀx for i = 1 to N<br/>4. Generate 2N episodes and their 2N rewards using 𝜋i+ and 𝜋i- and collect the rewards ri+ and ri-<br/>5. Sort all 𝜹 by max(ri+, ri-)<br/>6. Update 𝜃 = 𝜃 + (𝝰/(b*𝞼ᵣ)) Σ(ri+ - ri-)𝜹i (where i = 1 to b)<br/>7. End While</span></pre><h2 id="a81f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">实施和演示</h2><p id="483f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">下面是标准的 ARS 实现，很容易在互联网上找到。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="ae25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">运行代码将产生不同的学习阶段。</p><p id="c974" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在最初的几次迭代之后</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nr l"/></div></figure><p id="5dd4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第 100 次迭代之后</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nr l"/></div></figure><p id="607d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第 300 次迭代之后</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nr l"/></div></figure></div></div>    
</body>
</html>