<html>
<head>
<title>Deep Multi-Task Learning — 3 Lessons Learned</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度多任务学习— 3 个经验教训</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-multi-task-learning-3-lessons-learned-7d0193d71fd6?source=collection_archive---------19-----------------------#2019-01-06">https://towardsdatascience.com/deep-multi-task-learning-3-lessons-learned-7d0193d71fd6?source=collection_archive---------19-----------------------#2019-01-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="0e61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在过去的一年里，我和我的团队一直致力于在<a class="ae kl" href="https://blog.taboola.com/taboola-feed/" rel="noopener ugc nofollow" target="_blank"> Taboola feed </a>中实现个性化的用户体验。我们使用<a class="ae kl" href="http://ruder.io/multi-task" rel="noopener ugc nofollow" target="_blank">多任务学习</a> (MTL)来预测同一组输入特征的多个关键性能指标(KPI)，并在 TensorFlow 中实现了一个深度学习(DL)模型来实现这一点。回到我们开始的时候，MTL 对我们来说似乎比现在复杂得多，所以我想分享一些经验教训。</p><p id="8d5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">已经有不少关于用 DL 模式实现 MTL 的帖子(<a class="ae kl" href="https://jg8610.github.io/Multi-Task" rel="noopener ugc nofollow" target="_blank"> 1 </a>、<a class="ae kl" href="https://medium.com/@kajalgupta/multi-task-learning-with-deep-neural-networks-7544f8b7b4e3" rel="noopener"> 2 </a>、<a class="ae kl" href="https://hanxiao.github.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing" rel="noopener ugc nofollow" target="_blank"> 3 </a>)。在这篇文章中，我将分享一些在神经网络(NN)中实现 MTL 时需要考虑的具体问题。我也将提出简单的张量流解决方案来克服所讨论的问题。</p><h2 id="766e" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">分享是关怀</h2><p id="8128" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">我们想从<a class="ae kl" href="http://ruder.io/multi-task/index.html#hardparametersharing" rel="noopener ugc nofollow" target="_blank">硬参数共享</a>的基本方法开始。硬共享意味着我们有一个共享子网，后面是特定于任务的子网。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/e46c3690c4540f2fe872757e79fa2ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*twD0MQrd2Zf6iHjl.png"/></div></figure><p id="9233" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 TensorFlow 中开始使用这种模型的一个简单方法是使用带有多个头部的估算器。因为它看起来和其他神经网络架构没有什么不同，你可能会问自己哪里出错了？</p><h2 id="4443" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">第 1 课—组合损失</h2><p id="9451" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">我们在 MTL 模型中遇到的第一个挑战是为多个任务定义一个损失函数。虽然单个任务有明确定义的损失函数，但多个任务会带来多个损失。</p><p id="a6f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们尝试的第一件事就是简单地将不同的损失相加。很快我们可以看到，当一个任务收敛到好的结果时，其他的看起来很糟糕。当仔细观察时，我们很容易明白为什么。损失的规模如此不同，以至于一项任务主导了整体损失，而其余的任务没有机会影响共享层的学习过程。</p><p id="2e31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个快速解决方案是用加权总和代替损失总和，这样所有损失的规模大致相同。然而，这个解决方案涉及到另一个可能需要偶尔调整的超参数。</p><p id="05e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">幸运的是，我们发现了一篇很棒的论文，提出用不确定性来衡量 MTL 的损失。其实现方式是通过学习另一个噪声参数，该参数被整合到每个任务的损失函数中。这允许有多个任务，可能的回归和分类，并使所有损失达到相同的规模。现在我们可以回到简单地总结我们的损失。</p><p id="1686" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们不仅得到了比加权总和更好的结果，还可以忘记额外的权重超参数。<a class="ae kl" href="https://github.com/yaringal/multi-task-learning-example/blob/master/multi-task-learning-example.ipynb" rel="noopener ugc nofollow" target="_blank">这里的</a>是本文作者提供的一个 Keras 实现。</p><h2 id="0dbf" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">第 2 课—调整学习率</h2><p id="adc2" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">学习率是<a class="ae kl" href="https://engineering.taboola.com/hitchhikers-guide-hyperparameter-tuning/" rel="noopener ugc nofollow" target="_blank">调节神经网络</a>的最重要的超参数之一，这是一个普遍的约定。所以我们尝试调整，发现一个学习率看起来非常适合任务 A，另一个非常适合任务 b。选择较高的学习率会导致其中一个任务的<a class="ae kl" href="https://www.quora.com/What-is-the-dying-ReLU-problem-in-neural-networks" rel="noopener ugc nofollow" target="_blank">死亡，而使用较低的学习率会导致另一个任务收敛缓慢。那我们能做什么？我们可以为每个“头”(特定于任务的子网)调整一个单独的学习速率，为共享子网调整另一个速率。</a></p><p id="2937" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然听起来很复杂，但实际上很简单。通常，在 TensorFlow 中训练神经网络时，您会使用类似以下的内容:</p><pre class="ll lm ln lo gt ls lt lu lv aw lw bi"><span id="cb3f" class="km kn iq lt b gy lx ly l lz ma">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)</span></pre><p id="e517" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mb"> AdamOptimizer </em>定义如何应用渐变，而<em class="mb"> minimize </em>计算并应用渐变。我们可以将<em class="mb">最小化</em>替换为我们自己的实现，在应用梯度时，我们将对计算图中的每个变量使用适当的学习率:</p><pre class="ll lm ln lo gt ls lt lu lv aw lw bi"><span id="e63c" class="km kn iq lt b gy lx ly l lz ma">all_variables = shared_vars + a_vars + b_vars<br/>all_gradients = tf.gradients(loss, all_variables)<br/><br/>shared_subnet_gradients = all_gradients[:len(shared_vars)]<br/>a_gradients = all_gradients[len(shared_vars):len(shared_vars + a_vars)]<br/>b_gradients = all_gradients[len(shared_vars + a_vars):]<br/><br/>shared_subnet_optimizer = tf.train.AdamOptimizer(shared_learning_rate)<br/>a_optimizer = tf.train.AdamOptimizer(a_learning_rate)<br/>b_optimizer = tf.train.AdamOptimizer(b_learning_rate)<br/><br/>train_shared_op = shared_subnet_optimizer.apply_gradients(zip(shared_subnet_gradients, shared_vars))<br/>train_a_op = a_optimizer.apply_gradients(zip(a_gradients, a_vars))<br/>train_b_op = b_optimizer.apply_gradients(zip(b_gradients, b_vars))<br/><br/>train_op = tf.group(train_shared_op, train_a_op, train_b_op)</span></pre><p id="8092" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">顺便说一下，这个技巧实际上对单任务网络也很有用。</p><h2 id="b2e3" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">第 3 课—使用评估作为特征</h2><p id="2e3e" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">一旦我们通过了创建一个预测多项任务的神经网络的第一阶段，我们可能会希望使用我们对一项任务的估计作为另一项任务的特征。在向前传球时，这真的很容易。估计是一个张量，所以我们可以像任何其他层的输出一样连接它。但是在 backprop 中会发生什么呢？</p><p id="2430" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设任务 A 的估计值作为一个特征传递给任务 B。我们可能不想将渐变从任务 B 传播回任务 A，因为我们已经有了一个 A 的标签。<br/>别担心，TensorFlow 的 API 有<a class="ae kl" href="https://www.tensorflow.org/api_docs/python/tf/stop_gradient" rel="noopener ugc nofollow" target="_blank"> tf.stop_gradient </a>就是因为这个原因。当计算梯度时，它让你传递一个你希望当作常数的张量列表，这正是我们所需要的。</p><pre class="ll lm ln lo gt ls lt lu lv aw lw bi"><span id="39ce" class="km kn iq lt b gy lx ly l lz ma">all_gradients = tf.gradients(loss, all_variables, stop_gradients=stop_tensors)</span></pre><p id="c6de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样，这在 MTL 网络中是有用的，但不是唯一的。每当您想用 TensorFlow 计算一个值，并需要假设该值是一个常数时，都可以使用这种技术。例如，当训练生成性对抗网络(GANs)时，你不希望通过对抗例子的生成过程反向推进。</p><h2 id="a121" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">那么，下一步是什么？</h2><p id="91a3" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">我们的模型已经启动并运行，Taboola feed 正在被个性化。然而，仍有很大的改进空间，还有许多有趣的架构有待探索。在我们的用例中，预测多个任务也意味着我们基于多个 KPI 做出决策。这可能比使用单一的 KPI 要复杂一些…但这已经是一个全新的话题了。</p><p id="9658" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读，希望这篇文章对你有用！</p></div></div>    
</body>
</html>