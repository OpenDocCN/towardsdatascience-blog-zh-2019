<html>
<head>
<title>Data: the Predicament and Opportunity in the Deep Learning Era</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据:深度学习时代的困境与机遇</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-the-predicament-and-opportunity-in-the-deep-learning-era-256f4b4fef?source=collection_archive---------24-----------------------#2019-10-07">https://towardsdatascience.com/data-the-predicament-and-opportunity-in-the-deep-learning-era-256f4b4fef?source=collection_archive---------24-----------------------#2019-10-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="ec0b" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">介绍</h1><p id="48f2" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">随着深度学习已经渗透到几乎每一个学术学科，并催生了无数的初创企业和新的工业应用，很多人可能会问:所谓的“深度学习”的极限在哪里，深度学习的未来会是什么样子？而下面是我个人的回答:<strong class="kx iu">数据</strong>。</p><p id="ffe8" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">正如我们所知，深度学习的兴起有三个支柱:强大的计算能力、大模型容量和大数据。然而，随着计算能力和模型容量的稳步增长，最大数据集的大小似乎保持不变。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/d34fdf72d8323a6ff4221909f7540c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqyrnzV8RwfQe4Sc70y8Rg.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">[1]</figcaption></figure><p id="3993" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">这主要是由于标注足够大、足够多样且足够干净(即具有高标签质量)的数据集的高成本和难度。此外，还有一些标注成本极高的任务，比如医学。这也为域数据集的快速增长设置了限制。</p><p id="e079" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">然而，在传统的(强)监督学习设置中，训练一个具有足够好性能的模型需要大量的标记数据。这让我们陷入了数据匮乏的困境。</p><p id="35e0" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">在我看来，主要有三个方向可以解决数据匮乏的困境:</p><ol class=""><li id="3e33" class="mo mp it kx b ky lt lc lu lg mq lk mr lo ms ls mt mu mv mw bi translated">充分利用给定的数据集:更好的数据采样策略[2]，更大的模型容量[3]…</li><li id="cb68" class="mo mp it kx b ky mx lc my lg mz lk na lo nb ls mt mu mv mw bi translated">放大标注数据集量:数据标注；</li><li id="ed52" class="mo mp it kx b ky mx lc my lg mz lk na lo nb ls mt mu mv mw bi translated">利用更多的知识:自我监督学习/预训练，弱监督学习/预训练，迁移学习(预训练)。</li></ol><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nc"><img src="../Images/e91feed7b5d199a9ecfe37a6664ec3ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UdLHp3orz4VOyZPsejZSnw.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Modified from Stanford AI lab <a class="ae nd" href="https://ai.stanford.edu/blog//assets/img/posts/2019-03-03-weak_supervision/WS_mapping.png" rel="noopener ugc nofollow" target="_blank">blog pic</a></figcaption></figure><p id="b33f" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">正如你将看到的，它们中的许多与其他的交织在一起。虽然强(传统)监督学习仍然是主流，但在不久的将来，所有方法都将变得越来越重要。在这篇文章中，我将简要概述这些方法，并对更有前途的方向提供一些我的看法。</p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="8acf" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">明智地使用数据</h1><p id="774e" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这种类型的方法并不试图增加数据集的大小，而是试图充分利用我们已经拥有的数据。我不会对此进行太详细的描述，因为一般来说，几乎所有当前的论文都属于这一类别:模型架构的改进、损失设计、优化算法……这些都是试图充分利用给定数据集的各种努力——提取其中的所有知识，并学习良好的表示或模型。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ne"><img src="../Images/17a0c20af77792ea47462456d9b9124a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hfxhfvnIToq4kWkNBqc1rw.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">From [4]</figcaption></figure><p id="40bb" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">然而，有一个有趣的研究方向，旨在“在数据集层面”更好地利用数据集。也就是说，他们试图使用不同的数据采样策略来帮助训练一个更少偏差、更稳健的模型。例如，[2] [4]试图处理数据集的长尾分布，这是现实世界分布的一种典型形式。这些方法通常采用不同的再平衡策略，例如基于每个类别的观察值数量或特征空间几何形状(例如，聚类结果)的再采样和再加权。</p><p id="4926" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">在许多数据集固定的实际应用程序中，这种类型的方法是可以帮助您解决问题的唯一且最适用的方法。</p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="9423" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">扩大数据集大小</h1><p id="85cd" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">当我们想要在一个新的应用场景中训练一个模型时，总是不可避免地要先构造一个带标签的数据集；当我们无法获得满意的结果时，最直接的方法就是扩大标记数据集的大小。为了构建或扩大带标签的数据集，我们需要数据标注。</p><p id="15ae" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">自从机器学习的引入，数据标注就出现了。多年来，各种大型数据集被构建:Pascal-VOC[5]，ImageNet[6]，LSUN[7]，COCO[8]，mega face[9]…许多在线注释服务也是可用的，如<a class="ae nd" href="http://labelme.csail.mit.edu/Release3.0/" rel="noopener ugc nofollow" target="_blank"> LabelMe </a>，<a class="ae nd" href="https://supervise.ly/" rel="noopener ugc nofollow" target="_blank">supervisor . ly</a>等，尽管它们通常不节省成本。同时，也有许多新的数据注释方法被提出[10][11][12]。这些最新的方法通常将机器作为注释的一部分，或者使用它来自动化注释的一部分，或者帮助人类注释者。这些方法不仅大大加速了标注过程(实例分割速度提高了 3-6 倍[10][12])，而且还提高了传统标注的质量。</p><p id="df8e" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">然而，尽管数据标注方法取得了很大进展，但目前还没有对其进行系统的学习或研究，也没有可靠的基准。这使得不同的数据标注方法难以相互比较，也使得研究者无法系统地研究如何改进数据标注方法。我将在下一篇文章中讨论更多，因为这是我们正在努力做的。</p><p id="0c48" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">扩大数据集的另一个方向是用较低质量或较高抽象级别的数据进行标注——弱标注。例如见下图。对于像素级实例分割的任务，有三个较低级别的注释可能也有帮助:人级别、点级别和边界框级别。较低级别的注释节省了大量时间。换句话说，给定相同的预算，我们可能希望获得更多的低层次注释数据，并获得更好的最终结果。然而，这种直觉仍然需要一个评估标准或基准来阐明。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nf"><img src="../Images/6b9f91be6be73edfb71bb357fe9bbbdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qwmyA-Wh1wByR4j_choYdA.png"/></div></div></figure></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="55fb" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">利用更多的知识</h1><p id="f5ae" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这是一个活跃的研究领域，每天都有新的有趣的方法被开发出来。实际上来说，<strong class="kx iu">迁移学习</strong>已经证明了它在计算机视觉和自然语言处理中的重要性，因为现在不使用 ImageNet 或类似 BERT 的预训练模型听起来有些鲁莽[13]。优势是显而易见的:你用更少的数据训练得更快。例如，在 ULMFit[14]中，作者发现预训练可以节省达到相同性能所需的大约 10 倍的数据。</p><p id="fb26" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">然而，仍然有许多问题是目标语言无法解决的，如细粒度分类、自然语言生成等。此外，当适应的数据集规模较大时，迁移学习的好处变得小得多[14]。毕竟，在一个新的应用环境中，迁移学习是不适用的。然而，考虑到它的简单性和实用性，迁移学习无疑是利用其他数据源知识的最常用方法。</p><p id="7a85" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">也有一些研究人员旨在利用大量未标记的数据集。<strong class="kx iu">半监督学习</strong>是一个非常活跃的领域。其思想是使用结构假设(聚类假设和流形假设)来自动利用未标记的数据。在最近的一篇文章[15]中，作者表明半监督学习有可能取代监督学习，并大大减少需要完全注释的数据量。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ng"><img src="../Images/39fd105037c4c17dedbdcdab560c1bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Rc_jl69da6-k5WpLeMkqQ.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">The hope of the future of semi-supervised learning [15]</figcaption></figure><p id="c50c" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">然而，目前，在 SSL 实用化之前仍然存在一些挑战。第一个是未标记数据的不可控偏差，这将在[16]的第 4 节中详细讨论。另一个问题是 SSL 深受类别分布不匹配之苦，有标签和无标签的数据应该有相同的标签空间，否则准确率会大大下降。这极大地限制了 SSL 的使用范围。此外，据报道[17]在许多设置中，在不同的标记数据集上预训练分类器，然后仅在感兴趣的数据集的标记数据上重新训练，可以胜过所有 SSL 算法。这意味着 SSL 方法要变得实用还有很长的路要走。</p><p id="b936" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">另一种试图利用未标记数据集的方法叫做<strong class="kx iu">自我监督学习</strong>。其核心思想是在没有明确的人工监督的情况下，从数据本身学习表示。它与 SSL 的不同之处在于，它不需要标记数据，而是使用各种假设来创建监管信号[18][19]。在[18]中，作者表明，通过扩大预训练数据、模型容量和问题复杂性，人们可以在很大程度上匹配甚至超过监督预训练在各种任务上的性能，例如对象检测、表面法线估计(3D)和使用强化学习的视觉导航。这表明自我监督学习是一个很有前途的研究方向。然而，在图像分类和低镜头分类任务中，监督预训练模型仍然是更好的。从作者的角度来看，当前的自我监督方法不够“硬”，不足以充分利用大规模数据，并且似乎没有学习有效的高级语义表示。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nh"><img src="../Images/d4ed38a56db77b707c95354ce8aa974f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y38nzQYPK_DXy1eHBxXZkg.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">From [18]</figcaption></figure><p id="6234" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">自我监督学习的另一个缺点是，它比其他形式的训练需要更多的领域知识，因为它需要专家来设计自我监督的方式，这可能因任务而异。这似乎让我们离真正的智能学习形式越来越远。</p><p id="76de" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">最后，我想讨论一下<strong class="kx iu">弱监督学习</strong>【20】【21】和预训练【22】。如前所述，在某些情况下，微弱的监控信号也能产生良好的效果。数据集的大尺寸似乎可以补偿噪声或其标签的低质量。另一个优势是，可以设计标记函数并自动获得大量弱监督数据，尽管这类似于自我监督方法的方式——使用领域专业知识来设计自己的规则！即使没有完全自动化，弱监督的数据注释也可以节省大量时间，这可以转化为更大量的弱标记数据，并可能有利于前面提到的最终性能。</p><p id="2699" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">然而，像自监督学习一样，设计弱监督学习及其标注算法需要大量的领域专业知识。此外，该方法本身仍然需要在更实际的任务中证明其有效性，特别是在噪声/低质量/抽象如何影响性能，以及数据集的大小可以足够大方面，尽管它们在[22]的分类设置中有部分讨论。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ni"><img src="../Images/7b8e4cdc3fce00a6d31498a38d5cd84c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3NROdqNmgRZmoo7ZplPlHA.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">From [22]</figcaption></figure><p id="f97a" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">弱监督预训练并不是什么新东西，而是在其他任务中使用弱监督预训练模型。在图像分类和对象检测中，利用这种思想的模型已经显示出超过监督预训练的 ImageNet 模型[22]。可以预期，在不久的将来，这个领域的研究，连同相应的数据标注研究，将会腾飞。</p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="9d08" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">结论</h1><p id="a000" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在本帖中，我们将讨论解决数据匮乏困境的不同方法。对于每种方法，我们讨论了它的优点和缺点，并介绍了一些最近的研究和讨论。</p><p id="42f6" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">虽然最后一部分中的算法和方法听起来可能更吸引人，但从实际的角度来看，数据注释，无论是强还是弱，都是大多数情况下最有效的解决方案。并且数据注释基准的引入和数据注释方法的开发可以将成本降低到可负担的规模，以便很快注释非常大的数据集。因此，我认为，在不久的将来，社区要做的最重要的事情是在<strong class="kx iu">数据注释研究</strong>以及如何利用不同监管级别中注释的数据集<strong class="kx iu">方面投入更多精力。</strong></p><p id="6289" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated"><strong class="kx iu"> <em class="nj">要有数据！</em> </strong></p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="1052" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">参考资料:</h1><p id="8cae" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">[1]孙辰、阿比纳夫·什里瓦斯塔瓦、绍拉布·辛格和阿比纳夫·古普塔，重新审视深度学习时代数据的不合理有效性</p><p id="0cc9" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[2]基于有效样本数的类别平衡损失</p><p id="aff2" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[3]用于图像识别的深度残差学习</p><p id="6ddc" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[4]小张，，严东文，，于乔，程损为深度人脸识别用的长尾</p><p id="e0d0" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated"><a class="ae nd" href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="noopener ugc nofollow" target="_blank">http://host.robots.ox.ac.uk/pascal/VOC/</a></p><p id="cab4" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated"><a class="ae nd" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank">http://www.image-net.org/</a></p><p id="d4dc" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated"><a class="ae nd" href="https://www.yf.io/p/lsun" rel="noopener ugc nofollow" target="_blank">https://www.yf.io/p/lsun</a></p><p id="5705" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated"><a class="ae nd" href="http://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">http://cocodataset.org/#home</a></p><p id="4c9b" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[9]<a class="ae nd" href="http://megaface.cs.washington.edu/" rel="noopener ugc nofollow" target="_blank">http://megaface.cs.washington.edu/</a></p><p id="2f91" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[10]Rodrigo Benenson，Stefan Popov，Vittorio Ferrari，使用人类注释器的大规模交互式对象分割</p><p id="c008" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[11]Olga Russakovsky，李-，，两全其美:<br/>面向对象标注的人机协作</p><p id="ea18" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[12]大卫·阿库纳、黄玲、阿姆兰·卡尔、桑佳·菲德勒,“用多边形有效标注分割数据集——RNN ++</p><p id="0eb0" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[13] NLP 的 ImageNet 时刻已经到来<a class="ae nd" href="http://ruder.io/nlp-imagenet/" rel="noopener ugc nofollow" target="_blank">http://ruder.io/nlp-imagenet/</a></p><p id="f45a" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[14]杰瑞米·霍华德，塞巴斯蒂安·鲁德，用于文本分类的通用语言模型微调</p><p id="d7fe" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[15]安静的半监督革命<a class="ae nd" rel="noopener" target="_blank" href="/the-quiet-semi-supervised-revolution-edec1e9ad8c">https://towards data science . com/The-Quiet-Semi-Supervised-Revolution-edec 1 e 9 ad 8c</a></p><p id="5416" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[16]半监督学习，Olivier Chapelle，Bernhard schlkopf 和 Alexander Zien，麻省理工学院出版社</p><p id="ee17" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[17] Avital Oliver，Augustus Odena，Colin Raffel，Ekin D. Cubuk &amp; Ian J. Goodfellow，深度半监督学习算法的现实评估</p><p id="cd2a" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[18] Priya Goyal，Dhruv Mahajan，Abhinav Gupta，Ishan Misra，自我监督视觉表征学习的标度和基准</p><p id="77f8" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[19]亚历山大·科列斯尼科夫，翟晓华，卢卡斯·拜尔，再论自我监督的视觉表征学习</p><p id="7df1" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[20]Armand Joulin，Laurens van der Maaten，Allan Jabri，Nicolas Vasilach，来自大量弱监督数据的电子学习视觉特征</p><p id="1fd0" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[21]阿历克斯·拉特纳、帕罗马·瓦尔马、布雷登·汉考克、克里斯·雷和哈兹实验室的其他成员，弱监督:机器学习的新编程范例<a class="ae nd" href="https://ai.stanford.edu/blog/weak-supervision/" rel="noopener ugc nofollow" target="_blank">https://ai.stanford.edu/blog/weak-supervision/</a></p><p id="86cb" class="pw-post-body-paragraph kv kw it kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls im bi translated">[22]Dhruv Mahajan，Ross Girshick，Vignesh Ramanathan，，Manohar Paluri，，Ashwin Bharambe，Laurens van der Maaten，探索弱监督预审的局限性</p></div></div>    
</body>
</html>