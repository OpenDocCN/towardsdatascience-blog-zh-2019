<html>
<head>
<title>Hand tracking with Turi Create and Core ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Turi Create 和 Core ML 进行手动跟踪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hand-tracking-with-turi-create-and-core-ml-f3f9d3b60f7a?source=collection_archive---------14-----------------------#2019-01-19">https://towardsdatascience.com/hand-tracking-with-turi-create-and-core-ml-f3f9d3b60f7a?source=collection_archive---------14-----------------------#2019-01-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a311" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在移动设备上进行实时手部跟踪的任务既有趣又具有挑战性。手是身体中较难检测和跟踪的部分之一。原因是手可能看起来非常不同，无论是形状(尽管这也适用于身体的其他部位)还是手指的位置。一手牌可以在几分钟内从出拳变成击掌。这意味着很难从不同的手状态、不同的角度收集带有适当注释的数据集。在这篇文章中，我们将看到如何使用 Turi Create 实现这一点的方法，Turi Create 是苹果用于创建核心 ML 模型的框架。</p><p id="e771" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://www.youtube.com/watch?v=q7cBgyssAg8" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=q7cBgyssAg8</a></p><h1 id="1284" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">目标跟踪</h1><p id="a0ab" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">这里我们需要的机器学习任务是物体检测。对象检测使您不仅可以检测当前相机帧中是否存在对象，还可以检测该对象的位置。这使您能够绘制一些视觉指示(如矩形)或在检测到的地方呈现一个虚拟模型。一旦找到该对象，您就可以使用视觉的跟踪功能，在感兴趣的对象移动时更新视觉指示。</p><p id="c0dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你只想知道物体是否在图片上，你需要图像分类。为此，您可以使用 Create ML，这是苹果公司用于创建机器学习模型的另一个框架。</p><h1 id="7745" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">Turi 创建</h1><p id="9b92" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">Turi Create 是一个简化定制机器学习模型创建的工具，可以轻松导出为苹果的核心 ML 格式。这意味着，你不必成为机器学习专家，就可以在你的应用程序中添加一些智能。Turi Create 支持的任务有推荐、图像分类、对象检测、风格转换、活动分类等等。虽然它需要一点 Python 代码，但它仍然很容易上手，正如我们在这篇文章中看到的。</p><p id="6afd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以在他们的<a class="ae kl" href="https://github.com/apple/turicreate" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>上找到如何安装 Turi Create 的细节。基本上，您需要从终端执行以下操作:</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="f28c" class="ly kn iq lu b gy lz ma l mb mc">pip install -U turicreate</span></pre><p id="5af1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦安装了 Turi Create，我们的下一个任务就是找到并准备数据。</p><h1 id="4733" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">准备数据</h1><p id="ca89" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">机器学习的最大挑战之一是找到足够的数据来训练机器学习模型。正如我们在开始时讨论的，检测一手牌的正确界限可能有点棘手。这就是为什么我们需要非常好的数据集。让我们看看我发现了什么。</p><p id="0cc4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用的数据集可以在找到<a class="ae kl" href="http://cvrr.ucsd.edu/vivachallenge/index.php/hands/hand-detection/" rel="noopener ugc nofollow" target="_blank">。我试过几种选择，这一种被证明是最好的。它相当大，大约 6 GB，对于没有 GPU 的 Mac 机器来说，训练模型是相当具有挑战性的。我发现的其他选项有</a><a class="ae kl" href="http://vision.soic.indiana.edu/projects/egohands/" rel="noopener ugc nofollow" target="_blank"> EgoHands </a>和<a class="ae kl" href="http://www.robots.ox.ac.uk/~vgg/data/hands/" rel="noopener ugc nofollow" target="_blank"> Oxford Hand </a>数据集。我在考虑将这三者结合起来，建立一个更好的机器学习模型，但我的 Mac 无法处理这一点。</p><p id="bbe3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们看看数据。我使用的 VIVA 手部检测挑战的数据被分成两个文件夹，pos 和 posGt，都在 train 文件夹下。pos 文件夹包含所有图像，而 posGt 包含 csv 格式的所有注释，以及关于手(左手或右手)的信息。每个 csv 条目包含关于手的边界框的信息，使用 2D 图像平面中的左上角点、宽度和高度[x y w h]来描述。</p><h1 id="5e9d" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">Turi Create 期望什么？</h1><p id="ba70" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">另一方面，Turi Create 需要 SFrame，这是一种表格数据结构，可以将图像和相应的注释放入其中。注释是 JSON 格式的。每个图像都有一个对象数组，其中有键坐标和标签。坐标值包含边界框的信息，而标签则包含什么是边界框。在我们的例子中，不是左手就是右手。</p><p id="efe5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe md me mf lu b">[ {'coordinates': {'height': 104, 'width': 110, 'x': 115, 'y': 216},<br/> 'label': 'left'}, ...]</code></p><p id="7999" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">坐标代表矩形的中心，以及宽度和高度，这与手部数据集中的数据组织方式不同(这里是左上角，而不是矩形的中心)。</p><p id="9811" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了创建该数据结构，我们将进行一些 Python 编码。以下脚本将把数据集的图像和 csv 文件转换成 SFrame，然后可用于创建 Turi Create 模型。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="71cf" class="ly kn iq lu b gy lz ma l mb mc">import turicreate as tc<br/>import os<br/>from os import listdir<br/>from os.path import isfile, join</span><span id="8ed3" class="ly kn iq lu b gy mg ma l mb mc">path = 'train/posGt'<br/>imagesDir = "train/pos"<br/>files = [f for f in listdir(path) if isfile(join(path, f))]<br/>annotations = []<br/>labels = []<br/>for fname in files:<br/> if fname != ".DS_Store":<br/>  lines = tuple(open(path + "/" + fname, 'r'))<br/>  count = 0<br/>  entries = []<br/>  for line in lines:<br/>   if count &gt; 0:<br/>    words = line.split()<br/>    passengerLabel = words[0]<br/>    label = "left"<br/>    if passengerLabel.find("left") == -1:<br/>     label = "right"<br/>    x = int(words[1])<br/>    y = int(words[2])<br/>    width = int(words[3])<br/>    height = int(words[4])<br/>    xCenter = x + width / 2<br/>    yCenter = y + height / 2<br/>    coordinates = {'height': height, 'width': width, 'x': xCenter, 'y': yCenter}<br/>    entry = { 'coordinates' : coordinates, 'label' : label }<br/>    entries.append(entry)<br/>   count = count + 1<br/>  annotations.append(entries)<br/>sf_images = tc.image_analysis.load_images(imagesDir, random_order=False, with_path=False)<br/>sf_images["annotations"] = annotations<br/>sf_images['image_with_ground_truth'] = \<br/>    tc.object_detector.util.draw_bounding_boxes(sf_images['image'], sf_images['annotations'])<br/>sf_images.save('handsFrame.sframe')</span></pre><p id="37f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了实现这一点，我们首先浏览注释，解析 CSV 并创建坐标 JSON，同时还将左上坐标转换为中心坐标。接下来，我们使用 turicreate 包中的辅助函数加载图像。然后我们简单地放置注释，同时保持顺序。然后，将 SFrame 保存到手帧数据结构中。</p><p id="c32f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还可以调用 sf_images.explore()来可视化边界框和图像。然而，你应该用少量的图片来测试，否则它会永远加载。</p><figure class="lp lq lr ls gt mi gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/5b1890f910a0e594ac303a482cf32360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/0*wU4tZNO2MxMwU_BA"/></div></figure><p id="64e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一步是使用 SFrame 创建核心 ML 模型。这意味着我们应该进行另一轮 Python 编码。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="4a52" class="ly kn iq lu b gy lz ma l mb mc">import turicreate as tc</span><span id="e5ef" class="ly kn iq lu b gy mg ma l mb mc"># Load the data<br/>data =  tc.SFrame('handsFrame.sframe')</span><span id="09e4" class="ly kn iq lu b gy mg ma l mb mc"># Make a train-test split<br/>train_data, test_data = data.random_split(0.8)</span><span id="f099" class="ly kn iq lu b gy mg ma l mb mc"># Create a model<br/>model = tc.object_detector.create(train_data, feature='image', max_iterations=120)</span><span id="4821" class="ly kn iq lu b gy mg ma l mb mc"># Save predictions to an SArray<br/>predictions = model.predict(test_data)</span><span id="c155" class="ly kn iq lu b gy mg ma l mb mc"># Evaluate the model and save the results into a dictionary<br/>metrics = model.evaluate(test_data)</span><span id="9575" class="ly kn iq lu b gy mg ma l mb mc"># Export for use in Core ML<br/>model.export_coreml('Hands.mlmodel')</span></pre><p id="f809" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们在这里做的是首先加载我们用第一个脚本创建的 SFrame。然后，我们创建训练和测试数据，以 80–20%的比例随机分割。然后，使用 Turi Create 中的 object_detector.create 方法，我们用训练数据创建模型。您可以使用 max_iterations 属性(我的机器在 150 时崩溃，所以 120 是我能做的最好的)。之后，我们进行预测并评估模型。在最后一步中，我们以核心 ML 格式导出模型。</p><h1 id="8a9d" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">iOS 实施</h1><p id="d0ce" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">现在我们有了核心 ML 模型，很容易将其集成到 iOS 应用程序中，只需将其拖放到 Xcode 项目中。</p><p id="0f42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们检查一下创建的模型。这种模式被称为 Pipeline，它与苹果从 iOS 12 开始的愿景非常契合。它接受 416×416 大小的图像作为输入。作为输出，它提供了两个 MLMultiArrays，其中包含关于检测到的对象的置信度和坐标。</p><figure class="lp lq lr ls gt mi gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/d469f30e4dfc8d21037c50f763bf4888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/0*TlwuJA4qxnpkmSrr"/></div></figure><p id="4061" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好消息是，您不必处理这些复杂的多维数组 Vision 框架会自动为您完成这项工作(针对使用 Turi Create 创建的管道模型),并为您提供一个<a class="ae kl" href="https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation" rel="noopener ugc nofollow" target="_blank">VNRecognizedObjectObservation</a>。该类型包含关于边界框的信息(作为一个 CGRect ),以及置信度。现在，当您运行 Vision 会话时，您只需要检查结果是否属于该类型，并绘制适当的边界框。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="e46a" class="ly kn iq lu b gy lz ma l mb mc">func handleNewHands(request: VNRequest, error: Error?) {<br/>        DispatchQueue.main.async {<br/>            //perform all the UI updates on the main queue<br/>            guard let results = request.results as? [VNRecognizedObjectObservation] else { return }<br/>            for result in results {<br/>                print("confidence=\(result.confidence)")<br/>                if result.confidence &gt;= self.confidence {<br/>                    self.shouldScanNewHands = false<br/>                    let trackingRequest = VNTrackObjectRequest(detectedObjectObservation: result, completionHandler: self.handleHand)<br/>                    trackingRequest.trackingLevel = .accurate<br/>                    self.trackingRequests.append(trackingRequest)<br/>                }<br/>                <br/>            }<br/>        }<br/>    }</span></pre><p id="447b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦物体被检测到，我们可以告诉视觉跟踪它。为此，我们正在创建类型为<a class="ae kl" href="https://developer.apple.com/documentation/vision/vntrackobjectrequest" rel="noopener ugc nofollow" target="_blank"> VNTrackObjectRequest </a>的对象，在这里我们通过已识别的对象观察并开始跟踪。每次调用完成处理程序 handleHand 时，我们都会更新跟踪矩形。</p><h1 id="391f" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">源代码</h1><p id="3cd7" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">这是 iOS 实施中最重要的部分。你可以在这里找到完整的源代码<a class="ae kl" href="https://github.com/martinmitrevski/HandTracking" rel="noopener ugc nofollow" target="_blank">，以及所有的视觉检测和跟踪细节。</a></p><h1 id="8fd7" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">结论</h1><p id="1c0c" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">对我来说，这是一个非常有趣的机器学习练习。Turi Create 是一个非常强大的创建机器学习模型的工具。它创建了与 iOS 应用程序无缝协作的模型。</p><p id="b3ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个项目有很大的改进空间。首先，该模型应该用更多的数据进行训练，这样它就可以在所有光线条件和手的位置下正确地工作。此外，可以改进 iOS 代码，以更好地处理跟踪请求。现在，同一只手可能会识别出多个矩形。</p><p id="6595" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一件很酷的事情是不仅跟踪整只手，还跟踪手指。</p><p id="ed57" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是这篇文章的全部内容。你认为检测身体部位对我们未来的应用程序有用吗？一般来说机器学习怎么样？在下面的部分省去任何注释。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><p id="d467" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ms">原载于 2019 年 1 月 19 日</em><a class="ae kl" href="https://martinmitrevski.com/2019/01/19/hand-tracking-with-turi-create-and-core-ml/" rel="noopener ugc nofollow" target="_blank"><em class="ms">【martinmitrevski.com】</em></a><em class="ms">。</em></p></div></div>    
</body>
</html>