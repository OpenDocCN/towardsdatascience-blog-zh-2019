<html>
<head>
<title>Sinusoidal Neural Networks for Digit Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于数字分类的正弦神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sinusoidal-neural-networks-for-digit-classification-bd2b14e57ad8?source=collection_archive---------23-----------------------#2019-09-23">https://towardsdatascience.com/sinusoidal-neural-networks-for-digit-classification-bd2b14e57ad8?source=collection_archive---------23-----------------------#2019-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e938" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种正弦基函数的隐层神经网络</h2></div><p id="7b7e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在之前的帖子里讲过正弦基函数的神经网络(<a class="ae le" rel="noopener" target="_blank" href="/neural-networks-with-sine-basis-function-c5c13fd63513">正弦基函数的神经网络</a>)。这是具有正弦基函数的神经网络的基本实现。此后，我将把具有正弦基函数的神经网络称为“正弦神经网络(SNN)”。这篇文章假设你了解神经网络及其工作原理。如果你不知道，有惊人的故事来学习什么是媒体中的神经网络。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/aab744554834e3d719fd4bdddd7853ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BEr8NymsoTrXG688"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">A fancy image by <a class="ae le" href="https://unsplash.com/@skraidantisdrambliukas?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Gertrūda Valasevičiūtė</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e4f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">今天，我将介绍为数字分类而实现和训练的 SNNs。数字分类是神经网络应用的一个众所周知的领域。但是，对新结构进行首次评估是一个很好的主题。</p><p id="0186" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个不同的主题，我也将谈到的是实现 SNN 与隐藏层。当我从零开始实现神经网络架构时，我严重地遇到了很多矩阵维数的问题，尤其是哪些矩阵应该在哪些维数上才能进行正确的计算。每当我试图实现带有隐藏层的神经网络时，我都会对此感到困惑。既然我终于自己说清楚了，那我也帮你理解一下 NNs 中的维数，矩阵及其乘法。您还将认识到，snn 不需要任何激活函数来创建线性，因为它们本身就是非线性函数。</p><h2 id="1d9f" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">Python 代码中的架构、维度和公式</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mo"><img src="../Images/f524e96196c4d59ae0c0f2cc2e6cad0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LSvESuj1SXjApDo4P0706Q.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Sinusoidal Neural Networks — Structure and Formulas</figcaption></figure><p id="f3fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">前馈操作非常简单。我们只是将矩阵相乘并求和(点积。)</p><p id="c1ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">反向传播是令人头疼的问题。你可能也猜到了，我们将在反向传播中使用链式法则。问题是矩阵的维数。每个输出应该有哪个维度？我一时无法回答这个问题。我终于明白了，恭喜我，:D</p><h2 id="d1b3" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">矩阵的维数</h2><p id="029c" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated"><em class="mu">在这一节中，dy_dx 表示 y 相对于 x 的导数。因此，要注意变量的命名，以理解正在发生的事情。</em></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/4576b81c0b7203376e3647c4586b4913.png" data-original-src="https://miro.medium.com/v2/resize:fit:116/format:webp/1*t_MRxfiK2zjIe4zBZJLuiw.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">derivative of y with respect to x</figcaption></figure><p id="5f7a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">derror_doutput 具有我们的输出变量的维数，(1xK)。</p><p id="7e54" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">doutput_dlayer 通常需要与我们的隐藏层具有相同的维度。但是，我们不会让它发生。我们将把它保留为 KxN，因为这个变量将进入一个带有 derror_doutput 的操作。此操作的输出应该与隐藏层具有相同的维度。</p><p id="5949" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">derror_dlayer 具有我们隐藏层的维度。(详见上文。)</p><p id="7319" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">dlayer _ dweights_in_1 将与 weights _ in _ 1 具有相同的尺寸。因为 weights_in_1 矩阵中的每个变量都应该变化。</p><p id="7c69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">derror_dweights_in_1 将具有 weights_in_1 矩阵的转置维数。当我们更新权重时，我们把它固定回来(转置它)。</p><p id="75f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">doutput _ dweights_in_2 将与 weights _ in _ 2 具有相同的维度。因为 weights_in_2 矩阵中的每个变量都应该改变。</p><p id="76b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">derror_dweights_in_2 将具有 weights_in_2 矩阵的转置维数。当我们更新权重时，我们把它固定回来(转置它)。</p><p id="e198" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我没有写 weights_out_1 和 weights_out_2，因为它们的维数与 weights_in_1 和 weights_in_2 相同。</p><h1 id="bb0d" class="mw lw it bd lx mx my mz ma na nb nc md jz nd ka mg kc ne kd mj kf nf kg mm ng bi translated">哦，终于实现了…</h1><h2 id="f891" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">构建神经网络类:</h2><pre class="lg lh li lj gt nh ni nj nk aw nl bi"><span id="ef90" class="lv lw it ni b gy nm nn l no np">import numpy as np</span><span id="8a3f" class="lv lw it ni b gy nq nn l no np">class NeuralNetwork:<br/>    #DEFINE INITIALS<br/>    def __init__(self, x, y):<br/>        self.input      = x<br/>        self.y          = y<br/>        self.layer = np.zeros((1,32))<br/>        <br/>        #weights connecting to hidden layer<br/>        self.weights_in_1 = np.random.uniform(-1, 1,(self.layer.shape[1],self.input.shape[1]))<br/>        self.weights_out_1 = np.random.uniform(-1, 1,(self.layer.shape[1],self.input.shape[1]))<br/>        <br/>        #weights connecting to output.<br/>        self.weights_in_2 = np.random.uniform(-1, 1,(self.y.shape[1],self.layer.shape[1]))<br/>        self.weights_out_2 = np.random.uniform(-1, 1,(self.y.shape[1],self.layer.shape[1]))<br/>        <br/>        self.output     = np.random.uniform(-1, 1,self.y.shape)<br/>        print('Output:',self.output)<br/>        print('Y:',self.y)<br/>        #DEFINE FEEDFORWARD<br/>    def feedforward(self,cnt):<br/>        self.layer = np.sum(self.weights_out_1*self.input[cnt]*np.sin(np.pi*self.input[cnt]*self.weights_in_1),axis = 1) #forwarding to hidden layer<br/><br/>        self.output[cnt] = np.sum(self.weights_out_2*self.layer*np.sin(np.pi*self.layer*self.weights_in_2),axis = 1) #forwarding to output<br/>    <br/>    #function for derivative of output with respect to hidden layer.<br/>    def doutput_dlayer_func(self): <br/>        return self.weights_out_2*np.sin(np.pi*self.layer*self.weights_in_2)+np.pi*self.layer*self.weights_in_2*self.weights_out_2*np.cos(np.pi*self.layer*self.weights_in_2)<br/>    <br/>    #function for derivative of hidden layer with respect to weights on first level.<br/>    def dlayer_d_weights_func(self,cnt):<br/>        doutput_dweights_in = self.weights_out_1 * np.square(self.input[cnt]) * np.pi * np.cos(np.pi*self.input[cnt]*self.weights_in_1)<br/>        doutput_dweights_out = self.input[cnt]*np.sin(np.pi*self.input[cnt]*self.weights_in_1)<br/>        return doutput_dweights_in,doutput_dweights_out</span><span id="8e6c" class="lv lw it ni b gy nq nn l no np">    #function for derivative of output with respect to weights on second level.<br/>    def doutput_d_weights_func(self):<br/>        doutput_dweights_in = self.weights_out_2 * np.square(self.layer) * np.pi * np.cos(np.pi*self.layer*self.weights_in_2)<br/>        doutput_dweights_out = self.layer*np.sin(np.pi*self.layer*self.weights_in_2)<br/>        return doutput_dweights_in,doutput_dweights_out</span><span id="66e2" class="lv lw it ni b gy nq nn l no np">#DEFINE BACKPROPAGATION<br/>    def backprop(self,cnt):<br/>        error = np.square(self.y[cnt]-self.output[cnt])<br/>        print(cnt,'___',np.sum(error))<br/>        derror_doutput = self.y[cnt]-self.output[cnt]<br/>        #print('___',cnt,'___',derror_doutput)<br/>        <br/>        #calculate update amount for weights_in_1 and weights_out_1<br/>        #application of chain rule for weights on first level.<br/>        doutput_dlayer = self.doutput_dlayer_func()<br/>        derror_dlayer = np.dot(derror_doutput,doutput_dlayer)<br/>        dlayer_dweights_in_1,dlayer_dweights_out_1 = self.dlayer_d_weights_func(cnt)<br/>        derror_dweights_in_1 = derror_dlayer*dlayer_dweights_in_1.T<br/>        derror_dweights_out_1 = derror_dlayer*dlayer_dweights_out_1.T<br/>        <br/>        #application of chain rule for weights on first level.<br/>        doutput_dweights_in_2, doutput_dweights_out_2 = self.doutput_d_weights_func()<br/>        derror_dweights_in_2 = derror_doutput*doutput_dweights_in_2.T<br/>        derror_dweights_out_2 = derror_doutput*doutput_dweights_out_2.T<br/>        <br/>        self.weights_in_1 += derror_dweights_in_1.T*0.0001<br/>        self.weights_out_1 += derror_dweights_out_1.T*0.0001<br/>        self.weights_in_2 += derror_dweights_in_2.T*0.0001<br/>        self.weights_out_2 += derror_dweights_out_2.T*0.0001<br/>        <br/>    #PREDICT THE TEST DATA<br/>    def feedforward_test(self,test_data):<br/>        self.layer = np.sum(self.weights_out_1*test_data*np.sin(np.pi*test_data*self.weights_in_1),axis = 1)<br/>        test_output = np.sum(self.weights_out_2*self.layer*np.sin(np.pi*self.layer*self.weights_in_2),axis = 1)<br/>        return test_output<br/>        <br/>    def predict(self,input_):<br/>        predictions = []<br/>        for elm in input_:<br/>            #print('___',elm)<br/>            predictions.append(self.feedforward_test(elm).tolist())<br/>        return np.array(predictions)<br/>    <br/>    #SAVE WEIGHTS<br/>    def save_weights(self,dir_in_1 = './weights_in_1.npy',dir_out_1 = './weights_out_1.npy',<br/>                     dir_in_2 = './weights_in_2.npy',dir_out_2 = './weights_out_2.npy'):<br/>        np.save(dir_in_1,self.weights_in_1)<br/>        np.save(dir_out_1,self.weights_out_1)<br/>        np.save(dir_in_2,self.weights_in_2)<br/>        np.save(dir_out_2,self.weights_out_2)<br/>        <br/>    #IMPORT WEIGHTS<br/>    def import_weights(self,dir_in_1 = './weights_in_1.npy',dir_out_1 = './weights_out_1.npy',<br/>                       dir_in_2 = './weights_in_2.npy',dir_out_2 = './weights_out_2.npy'):<br/>        self.weights_in_1 = np.load(dir_in_1)<br/>        self.weights_out_1 = np.load(dir_out_1)<br/>        self.weights_in_2 = np.load(dir_in_2)<br/>        self.weights_out_2 = np.load(dir_out_2)</span></pre><h2 id="b1da" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">测试:</h2><pre class="lg lh li lj gt nh ni nj nk aw nl bi"><span id="c538" class="lv lw it ni b gy nm nn l no np">from sklearn.datasets import load_digits<br/>from sklearn import preprocessing</span><span id="c16d" class="lv lw it ni b gy nq nn l no np">lb = preprocessing.LabelBinarizer() #create label binarizer<br/>digits_object = load_digits()<br/>images = digits_object.data<br/>images = (images/np.max(images))+0.01<br/>labels = digits_object.target.reshape((1797,1))<br/>lb.fit(labels) # fit your labels to a binarizing map.<br/>labels = lb.transform(labels) #binarize your labels (example: from [0,3,2] to [[1,0,0,0,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0,0]])</span><span id="daee" class="lv lw it ni b gy nq nn l no np">#split your data-set<br/>x = images[0:1500]<br/>test_x = images[1500:-1]<br/>y = labels[0:1500]<br/>test_y = labels[1500:-1]</span><span id="3480" class="lv lw it ni b gy nq nn l no np">#create neural network instance.<br/>nn = NeuralNetwork(x,y)</span><span id="3a72" class="lv lw it ni b gy nq nn l no np">#TRAIN NETWORK FOR 1000 TIMES (EPOCH = 1000).<br/>for gen_cnt in range(1000):<br/>    for cnt in range(1500):<br/>        print('Epoch: {}'.format(gen_cnt))<br/>        nn.feedforward(cnt)<br/>        nn.backprop(cnt)</span><span id="f397" class="lv lw it ni b gy nq nn l no np">#PREDICT THE TEST DATA<br/>predictions = nn.predict(test_x)<br/>error = np.around(predictions,decimals = 2)-test_y</span><span id="3478" class="lv lw it ni b gy nq nn l no np">#CHECK BELOW</span></pre><h2 id="a3d5" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">结果和结论:</h2><p id="8553" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">我不知道为什么在神经网络中不常用正弦基函数。因为结果其实是令人满意的。我刚刚使用了 1 个隐藏层，得到了相当好的结果。我们不需要为选择不同类型的激活函数而感到困惑。非线性已经位于正弦基函数之下。我们的神经网络应该做的唯一一件事就是选择要求和的正弦函数。</p><p id="a12e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">编辑:我修改了预测函数，以便能够看到准确性。您可以在下面找到新功能:</p><pre class="lg lh li lj gt nh ni nj nk aw nl bi"><span id="51b7" class="lv lw it ni b gy nm nn l no np">def predict(self,input_):<br/>        predictions = []<br/>        for elm in input_:<br/>            #print('___',elm)<br/>            result_local = self.feedforward_test(elm)<br/>            result_temp = np.zeros(result_local.shape)<br/>            result_temp[np.argmax(result_local)] = 1<br/>            predictions.append(result_temp.tolist())<br/>        return np.array(predictions)</span></pre><p id="c808" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在运行预测函数后也添加这个脚本。</p><pre class="lg lh li lj gt nh ni nj nk aw nl bi"><span id="4103" class="lv lw it ni b gy nm nn l no np">accuracy = 1-((np.count_nonzero(predictions-test_y)/2)/predictions.shape[0])<br/>print('Accuracy is {}'.format(accuracy*100))</span></pre><h2 id="819b" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">我在 1000 次迭代后检查了准确度。准确率为 86.82%。这可以通过添加更多的隐藏层或通过用更多的纪元训练网络来改善。(训练集:[0，1500]，测试集:[1500，1797])</h2></div></div>    
</body>
</html>