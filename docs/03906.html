<html>
<head>
<title>Optimizing Individual-Level Models for Group-Level Predictions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为群体水平预测优化个体水平模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizing-individual-level-models-for-group-level-predictions-a66e675138ff?source=collection_archive---------25-----------------------#2019-06-19">https://towardsdatascience.com/optimizing-individual-level-models-for-group-level-predictions-a66e675138ff?source=collection_archive---------25-----------------------#2019-06-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9dc3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第一部分——偏倚分析</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d95f50b19bb46591a065b69abcdb9ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KPQIGBvWkeeqfeWAIa5vag.png"/></div></div></figure><h1 id="4f3b" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">介绍</h1><p id="ff9f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">这篇文章的目的是提出我对机器学习(特别是医疗保健)中一个非常普遍但很少解决的问题的一些想法:如何在只给定个体水平数据的情况下，以优化预定损失函数(例如，MAE 或 MSE)的方式构建群体水平的预测？<br/> <br/>如果您首先创建一个针对某个损失函数进行优化的个人级模型，然后取预测值的平均值，那么您是否会自动针对群体级的相同损失函数进行优化？事实证明，虽然 MSE 的答案恰好是“是”,但 MAE 的答案却是响亮的“不是”!<br/> <br/>在这篇文章中，我解释了为什么在个体层面使用相同损失函数的下意识方法是错误的。<br/> <br/>这是一个关于揭开这个问题真正本质的故事。作为这个探索的一部分，我使用了彼得·霍尔的一个有趣的定理，我发现这个定理的证明有点难以理解。作为对社区的服务，在<a class="ae mf" href="https://medium.com/@hilafhasson/optimizing-individual-level-models-for-group-level-predictions-f48f491363f8" rel="noopener">第 2 部分</a>中，我将展示该定理的背景故事，并提供一个比他在论文中提供的稍微更自然的证明，并填充原始论文中遗漏的所有关键细节。</p><p id="a059" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">这篇文章中看到的所有情节代码都可以在<a class="ae mf" href="https://github.com/lumiata/tech_blog/blob/master/Individual_Model_Optimization/Individual_Model_Blog_Code.ipynb" rel="noopener ugc nofollow" target="_blank">这个 GitHub repo </a>中找到。</p><h1 id="e4e2" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">示例和假设</h1><p id="08ef" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">为了使这种探索更容易理解，我将使用一个具体的例子并修正我的假设。</p><ul class=""><li id="1b50" class="ml mm iq ll b lm mg lp mh ls mn lw mo ma mp me mq mr ms mt bi translated"><em class="mu">示例</em>:一家医疗保险公司希望估算下一年雇主团体的人均成本(即团体的总成本除以成员数量)，其中每个雇主团体由一定数量的个人成员组成。</li><li id="cd1a" class="ml mm iq ll b lm mv lp mw ls mx lw my ma mz me mq mr ms mt bi translated"><em class="mu">基本假设</em>:给你训练和测试数据，包括个人层面的数据和成员到团队的映射。训练数据具有由每个人在下一年发生的成本表示的目标值，而测试数据没有目标值。测试数据和训练数据可能具有不同的组。</li></ul><p id="ca28" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">剩下的工作就是指定在集团层面上与什么“真实值”进行比较。为了说明为什么在个体层面优化与在群体层面优化相同的损失函数是错误的，最简单的方法是使用以下假设:</p><ul class=""><li id="3a5c" class="ml mm iq ll b lm mg lp mh ls mn lw mo ma mp me mq mr ms mt bi translated"><em class="mu">假设 A </em>:对于每个组，将其真值设为其成员的平均目标值。</li></ul><p id="89be" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">我们稍后还将考虑另一个假设，如果你允许成员的变化，这个假设会更自然地出现。</p><h1 id="24bc" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">为什么要有一个独立的模型呢？</h1><p id="e592" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">在组级优化特定损失函数的一种方法是简单地创建优化该损失函数的组级模型，其特征是工程聚合的个体级特征。但是群体的数量大概比个体的数量小得多，所以人们会认为这样的模型会有很大的差异。个人层面和团体层面相结合的方法将是最明智的。<br/> <br/>“为什么不用<a class="ae mf" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a>？这样，您就可以创建一个使用所有数据的组级模型！”，我听到你哭了。嗯……这将使用所有个体水平的特征，而不是未聚集的目标值，除非使用特殊的损失函数。即使这样，RNN 的是顺序依赖的，而组成员不是！所以让我们继续称之为“实验性的”。<br/> <br/>无论哪种方式，你的群体层面的预测只能从做好个体层面的模型中获益。所以让我们开始吧！</p><h1 id="71e6" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">关于 MAE/MSE 优化的直觉</h1><p id="aec1" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">底线是:为 MSE 优化意味着你在估计平均值；针对 MAE 进行优化意味着您正在估计中值。<br/> <br/>这到底是什么意思？设<strong class="ll ir"> <em class="mu"> Y </em> </strong>为目标值，设<strong class="ll ir"> <em class="mu"> X_1，…，X_n </em> </strong>为特征。如果你的特征值是<strong class="ll ir"> <em class="mu"> X_1=x_1，…，X_n=x_n </em> </strong>，那么给定那些特征的目标值<strong class="ll ir"> <em class="mu"> Y|X_1=x_1，…，X_n=x_n </em> </strong>是一个随机变量，而不是一个常数。换句话说，你的特征值并不能决定目标。例如，如果你正在预测成本，那么完全可以想象两个具有相同特征值的个体具有不同的成本；尽管知道这些特征值确实会改变成本的分布。</p><p id="23be" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">如果你是针对 MSE 进行优化，那么在你的模型中插入<strong class="ll ir"> <em class="mu"> (x_1，…，x_n) </em> </strong>会试图预测<strong class="ll ir"> <em class="mu"> E(Y|X_1=x_1，…，X _ n = X _ n)</em></strong>；然而，如果您正在为 MAE 进行优化，您的模型将尝试预测<strong class="ll ir"> <em class="mu">中值(Y|X_1=x_1，…，X_n=x_n) </em> </strong>。</p><p id="4b12" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">事实上，如果<strong class="ll ir"> <em class="mu"> f(x_1，…，x_n) </em> </strong>是损失函数为 MSE 的机器学习模型在这些特征值处的模型预测，那么它将试图近似使以下各项最小化的<strong class="ll ir"> <em class="mu"> a </em> </strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/2dbd573995b91f691a12eaa1c244a8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*uxVAl_7cedK7TKINMt-GcA.png"/></div></figure><p id="baa0" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">这最后一个表达式是<strong class="ll ir"> <em class="mu"> a </em> </strong>中具有全局最小值<strong class="ll ir"> <em class="mu"> E(Y|X_1=x_1，…，X_n=x_n) </em> </strong>的抛物线。</p><p id="9707" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">类似地，如果<strong class="ll ir"> <em class="mu"> f(x_1，…，x_n) </em> </strong>是损失函数为 MAE 的机器学习模型在这些特征值处的模型预测，那么它将尝试近似将以下各项最小化的<strong class="ll ir"> <em class="mu"> a </em> </strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/bdf818ea68a98ca8780654a787228bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*UnjgWAnoNwb3jEXqfNhDcw.png"/></div></figure><p id="11d3" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">最小化该表达式的<strong class="ll ir"> <em class="mu"> a </em> </strong>为<strong class="ll ir"> <em class="mu"> median(Y|X_1=x_1，…，X_n=x_n) </em> </strong>。(要看到这一点，你必须摆弄积分；这是一个简单但令人讨厌的练习。)</p><p id="5f4e" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">技术提示:给定一个事件的随机变量的条件概率，只有当你设定的事件有正概率时才有意义。试图将定义扩展到零概率事件注定是不明确的，除非我们指定一个限制程序；参见<a class="ae mf" href="https://en.wikipedia.org/wiki/Borel%E2%80%93Kolmogorov_paradox" rel="noopener ugc nofollow" target="_blank">Borel-Kolmogorov 悖论</a>。如果<strong class="ll ir"> <em class="mu"> X_i </em> </strong>中至少有一个是连续的，那么<strong class="ll ir"> <em class="mu"> P(X_1=x_1，…，X_n=x_n)=0 </em> </strong>，这就暗示<strong class="ll ir"> <em class="mu"> Y|X_1=x_1，…，X_n=x_n </em> </strong>没有任何意义。(本科教材中常见的定义为<strong class="ll ir"> <em class="mu"> Y|X_1=x_1，…，X_n=x_n </em> </strong>不是坐标不变的。对于更高级的读者来说:对次∑代数而不是事件进行调节会产生相同的问题，因为最终的随机变量在“几乎确定相等”之前是唯一的，这意味着零概率事件可以是例外。)我们可以也将会通过将特性的值限制为计算机能够表示的值来优雅地避免这个问题。这样，即使是“连续的”变量实际上也是离散的，一切都是定义明确的。</p><h1 id="2873" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">问题是</h1><p id="3af8" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">固定一组尺寸为<strong class="ll ir"> <em class="mu"> m </em> </strong>的，设</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/e857f0264a5398c49467736e277cabae.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*iKNWPrA3Y7SEZTxMlwMA4w.png"/></div></div></figure><p id="5668" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">成为第<strong class="ll ir"> <em class="mu"> i </em> </strong>人的特征。如果我们正在创建一个优化 MSE 的个体水平模型，那么结果的平均值就是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/68c2c0a0c3793a3195cee1ed2009b606.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*r234M_QAsBTnOd71PiE1mQ.png"/></div></figure><p id="1f92" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">根据期望的线性度，这等于</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/7a8b1f46f3dcb04f52ce4f05d0a01b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*99brRrWLfKhLvyeZ887fPw.png"/></div></figure><p id="e134" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">太好了！换句话说，通过在个人层面优化 MSE，您也在团队层面优化 MSE！<br/> <br/>现在让我们对梅做同样的分析。如果我们正在创建一个优化 MAE 的个体水平模型，那么结果的集合就是对</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/5adbaed2589de7d84a6bf30e3ed8f281.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*Halc8LkVhTlQOGDjn3KxrQ.png"/></div></figure><p id="7889" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">但那是非常非常非常遥远的事</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/662e0645fabdcc916ae11c6d74430336.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*oJ4ehRqMWcyqsRQnaYx7iA.png"/></div></figure><p id="10f5" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">这是一个简单的例子，说明总和的中位数与中位数之和相差甚远，即使随机变量都是独立同分布的:</p><p id="828b" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">为了设置这个例子，我将使用比例为<strong class="ll ir"> <em class="mu"> 200 </em> </strong>和形状为<strong class="ll ir"> <em class="mu"> 1 </em> </strong>的伽玛分布。这个分布看起来是这样的:</p><p id="15f7" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">(本帖中看到的剧情代码可以在<a class="ae mf" href="https://github.com/lumiata/tech_blog/blob/master/Individual_Model_Optimization/Individual_Model_Blog_Code.ipynb" rel="noopener ugc nofollow" target="_blank"> this GitHub repo </a>中找到。)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/14aa4ed7131f7f1906c12b2353b9d0c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H0Chxz5HvPsIkQDo0hk0EQ.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd nm">Figure 1</strong></figcaption></figure><p id="1d72" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">现在将<strong class="ll ir"> <em class="mu"> X_i </em> </strong>作为遵循该分布的 i.i.d。下面是他们的中位数与平均值的中位数的比较:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/973b89987f48caa988c1af8a3e233ea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMSkEedlffsXw55v8-xMBA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd nm">Figure 2</strong></figcaption></figure><p id="e290" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">你也看到了，这个差距挺大的！</p><h1 id="8298" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">对于群体级 MAE，优化个体级 MSE 优于优化个体级 MAE</h1><p id="db8c" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">如果一个群体很大，那么可以合理地假设</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/6de567b2dd4f5b86a218b8c792346d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*p9wCJirewUjIRPm2l-BCyA.png"/></div></figure><p id="44fc" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">是近似正常的；正态分布的中值就是它的平均值。因此，如果您在个人层面优化 MSE，然后取预测的平均值，您将近似估计大群体的中值！乍一看，这似乎是反直觉的:不仅对于组级 MSE，而且对于组级 MAE，在个体级优化 MSE 都优于在个体级优化 MAE。</p><p id="171e" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">但是，我们应该期望对小群体的偏见有多严重呢？(事实将会证明，秘密地关键是使用中心极限定理的误差估计。)</p><h1 id="b195" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">彼得·霍尔的一个结果</h1><p id="2b36" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">这激起了我的兴趣。所以我开始寻找关于 i.i.d .总和的中位数的结果。我找到了彼得·霍尔的《关于独立变量和的众数和中位数的极限行为》，在那里他证明了下面的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/8f6ff28d7795f322e40d5572690b9490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2er_PCMOnS8mUX3I4Rd6Uw.png"/></div></div></figure><p id="3959" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">(见本帖第二部分更深入的理解为什么这个定理是真的。)由于<strong class="ll ir"> <em class="mu"> X_i </em> </strong>都是同分布的，为了便于标注，我们就简单地让<strong class="ll ir"> <em class="mu"> X := X_1 </em> </strong>吧。如果我们不假设<strong class="ll ir"><em class="mu">X _ I</em></strong>s 有均值<strong class="ll ir"> <em class="mu"> 0 </em> </strong>和方差<strong class="ll ir"> <em class="mu"> 1 </em> </strong>的话，一个快速的回包络计算，还原到归一化的情况下，表明这还原到如下的近似值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/a1134d2c21dd2dedf2f2870b01fe389e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VNuKZvTxYVFF_6BGhpzbug.png"/></div></div></figure><p id="0525" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">请注意，这是一个渐近结果！为了能够在上面讨论的机器学习环境中使用它，我们必须首先确保这个近似对于小的<strong class="ll ir"> <em class="mu"> n </em> </strong>是合理的。让我们做一个快速的概念验证:</p><p id="59e3" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">(本帖中看到的剧情代码可以在<a class="ae mf" href="https://github.com/lumiata/tech_blog/blob/master/Individual_Model_Optimization/Individual_Model_Blog_Code.ipynb" rel="noopener ugc nofollow" target="_blank">这个 GitHub repo </a>中找到。)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/ce8c085ed7f1c23f6528ccef4ff29594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6oLh8YJ8asaolI55cTWzxA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd nm">Figure 3</strong></figcaption></figure><p id="7e60" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">这是相当准确的！</p><h1 id="7400" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">机器学习环境中的偏差估计</h1><p id="dbb9" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">对于每一组，我们将尝试估计</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/93fa5b58eb856d5b4bf009a57968ada4.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*kgKiCtZub7kf8Hr3BCgovA.png"/></div></figure><p id="772d" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">为了那个团体。被加数不是独立同分布的，所以霍尔的结果不能直接应用。为此，我们可以尝试将假设 A 替换为:</p><ul class=""><li id="beb0" class="ml mm iq ll b lm mg lp mh ls mn lw mo ma mp me mq mr ms mt bi translated"><em class="mu">假设 B: </em>对于每一组，假设其“真值”是其成员<em class="mu">真值的<strong class="ll ir"> m </strong>个样本重复的均值的期望值。</em></li></ul><p id="1e36" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">(可以说，假设 B 在现实生活中出现得更多。在组级预测中，成员通常不是固定的，但是当前成员的目标值表示未来成员的分布目标值。)<br/> <br/>既然目标值是 i.i.d .的平均值(因为采样是重复进行的)，我们可以采用霍尔近似值。固定一个组，设<strong class="ll ir"> <em class="mu"> T_1，…，T_m </em> </strong>为采样，重复，如假设 b 中所述。<strong class="ll ir"> <em class="mu"> T_i </em> </strong>现在是 I . I . d,<strong class="ll ir"><em class="mu">T:= T _ 1</em></strong>。我们现在面临的挑战是为每个组逼近<strong class="ll ir"><em class="mu">V(T)</em></strong><strong class="ll ir"><em class="mu">E((T-μ))</em></strong>。这可能很困难，因为我们最想纠正偏差的群体是小群体。<br/> <br/>一种简单化的做法是将<strong class="ll ir"><em class="mu"/></strong>估计为<strong class="ll ir"><em class="mu">【Y】</em></strong>，将<strong class="ll ir"> <em class="mu"> E((T-μ) ) </em> </strong>估计为<strong class="ll ir"> <em class="mu"> E((Y-E(Y)) ) </em> </strong>。进而，通过取我们数据中目标值的平均值来估计<strong class="ll ir"><em class="mu">【Y】</em></strong>和<strong class="ll ir"> <em class="mu"> E((Y-E(Y)) ) </em> </strong>，并将这些估计值分别表示为<strong class="ll ir"><em class="mu">ˇ</em></strong>和<strong class="ll ir"><em class="mu">ζζζ_ 3</em></strong>。这些估计是否有效取决于各组之间的差异——它们越相似，偏差校正就越好。无论哪种方式，这些近似值足以获得一个相对清晰的图片，说明一个小组需要有多小，才能使偏差超出你的舒适水平。也就是说，粗略的估计是，如果一个组的大小是<strong class="ll ir"> <em class="mu"> m </em> </strong>，那么对使用 MSE 训练的单个模型进行平均应该具有大约</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/3bef6391d32840029551cb0a6ddb3443.png" data-original-src="https://miro.medium.com/v2/resize:fit:112/format:webp/1*WjQIV8hlvOQlWiXGTDivPA.png"/></div></figure><p id="6b31" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">对于这个数字大得令人无法忍受的组，我建议格外小心:<strong class="ll ir"><em class="mu"/></strong>和<strong class="ll ir"><em class="mu">ζζ_ 3</em></strong>可能不是足够好的估计，并且可能导致糟糕的偏差校正。我只是建议将这种规模的组标记为需要偏差校正，并根据数据和您想到的特定应用对所需的偏差校正进行更多研究。<br/> <br/>(请注意，虽然使用贝叶斯方法使您能够从<strong class="ll ir"> <em class="mu"> Y|X_1=x_{i，1}，…，X_n=x_{i，n} </em> </strong>的分布中进行采样很有吸引力，但这些方法的普通版本假设<strong class="ll ir"> <em class="mu"> Y|X_1=x_{i，1}，…，X_n=x_{i，n} </em> </strong>的分布为如果是这样的话，就不会有任何偏差需要校正……这也适用于在神经网络中使用预测时的漏失，根据亚林·加尔和邹斌·加赫拉马尼的工作<a class="ae mf" href="http://proceedings.mlr.press/v48/gal16.pdf" rel="noopener ugc nofollow" target="_blank">,这大致相当于在适当定义的高斯过程设置中对后验样本进行采样——但该设置也假设误差呈正态分布。)</a></p><p id="a312" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><em class="mu">原贴于</em> <a class="ae mf" href="https://medium.com/lumiata" rel="noopener"> <em class="mu">卢米亚塔</em> </a></p><p id="b7b1" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">如果这个帖子引起了你的兴趣，你想用类似的问题挑战自己，Lumiata 正在招聘！请检查 Lumiata 的<a class="ae mf" href="https://www.lumiata.com/careers.html" rel="noopener ugc nofollow" target="_blank">打开位置</a>。</p><p id="76c8" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">GitHub:<a class="ae mf" href="https://github.com/lumiata/tech_blog" rel="noopener ugc nofollow" target="_blank">https://github.com/lumiata/tech_blog</a></p><p id="f583" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在 www.lumiata.com<a class="ae mf" href="http://www.lumiata.com/" rel="noopener ugc nofollow" target="_blank">拜访卢米娅塔</a>，并通过<a class="ae mf" href="http://www.twitter.com/lumiata" rel="noopener ugc nofollow" target="_blank">@卢米娅塔</a>在推特上关注。</p><p id="5523" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在 LinkedIn 上找到卢米娅塔:<a class="ae mf" href="https://www.linkedin.com/company/lumiata/" rel="noopener ugc nofollow" target="_blank">www.linkedin.com/company/lumiata</a></p><p id="dc32" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><strong class="ll ir"> <em class="mu">引文</em> </strong> <em class="mu"> : </em></p><p id="6bc1" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><em class="mu"> 1。Hall，P. (1980)关于独立随机变量和的众数和中位数的极限行为。安。概率 8 419–430。</em></p><p id="b327" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><em class="mu"> 2。Gal，y .和 Ghahramani，Z. (2016)辍学作为贝叶斯近似:表示深度学习中的模型不确定性。<br/> ICML。</em></p></div></div>    
</body>
</html>