<html>
<head>
<title>Logistic Regression on MNIST with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 对 MNIST 的 Logistic 回归分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19?source=collection_archive---------2-----------------------#2019-04-27">https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19?source=collection_archive---------2-----------------------#2019-04-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f372" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">逻辑回归</strong>用于描述数据，解释<strong class="jp ir">一个因变量</strong>与一个或多个名义变量、序数变量、区间变量或比率水平自变量之间的关系【1】。下图显示了<strong class="jp ir">逻辑</strong>和<strong class="jp ir">线性</strong>回归的区别。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/df7b65e6d7ed5fc50d569c868c9883b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*xFhICZgdr2VEZQ-C4FLUEA.jpeg"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Taken from <a class="ae kx" href="https://www.sciencedirect.com/topics/nursing-and-health-professions/logistic-regression-analysis" rel="noopener ugc nofollow" target="_blank">https://www.sciencedirect.com/topics/nursing-and-health-professions/logistic-regression-analysis</a></figcaption></figure><p id="0464" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将展示如何用 PyTorch 编写一个逻辑回归模型。</p><p id="d7f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将尝试解决 MNIST 数据集的分类问题。首先，让我们导入所有我们需要的库。</p><pre class="km kn ko kp gt ky kz la lb aw lc bi"><span id="cfdb" class="ld le iq kz b gy lf lg l lh li"><strong class="kz ir">import </strong>torch<br/><strong class="kz ir">from </strong>torch.autograd <strong class="kz ir">import </strong>Variable<br/><strong class="kz ir">import </strong>torchvision.transforms <strong class="kz ir">as </strong>transforms<br/><strong class="kz ir">import </strong>torchvision.datasets <strong class="kz ir">as </strong>dsets</span></pre><p id="357d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我创建一个模型时，我喜欢把下面的步骤列表放在我面前。这份名单公布在 PyTorch 网站上[2]。</p><pre class="km kn ko kp gt ky kz la lb aw lc bi"><span id="2857" class="ld le iq kz b gy lf lg l lh li"><em class="lj"># Step 1. Load Dataset<br/># Step 2. Make Dataset Iterable<br/># Step 3. Create Model Class<br/># Step 4. Instantiate Model Class<br/># Step 5. Instantiate Loss Class<br/># Step 6. Instantiate Optimizer Class<br/># Step 7. Train Model</em></span></pre><p id="ce14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以让我们一个接一个地经历这些步骤。</p><h2 id="51a6" class="ld le iq bd lk ll lm dn ln lo lp dp lq jy lr ls lt kc lu lv lw kg lx ly lz ma bi translated">加载数据集</h2><p id="1de3" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">为了加载数据集，我们使用了<strong class="jp ir"> torchvision.datasets，</strong>这个库几乎包含了机器学习中使用的所有流行数据集。你可以在[3]查看完整的数据集列表。</p><pre class="km kn ko kp gt ky kz la lb aw lc bi"><span id="1560" class="ld le iq kz b gy lf lg l lh li">train_dataset = dsets.MNIST(root=<strong class="kz ir">'./data'</strong>, train=<strong class="kz ir">True</strong>, transform=transforms.ToTensor(), download=<strong class="kz ir">False</strong>)<br/>test_dataset = dsets.MNIST(root=<strong class="kz ir">'./data'</strong>, train=<strong class="kz ir">False</strong>, transform=transforms.ToTensor())</span></pre><h2 id="7951" class="ld le iq bd lk ll lm dn ln lo lp dp lq jy lr ls lt kc lu lv lw kg lx ly lz ma bi translated">使数据集可迭代</h2><p id="d941" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">我们将使用 DataLoader 类，通过下面几行代码使我们的数据集可迭代。</p><pre class="km kn ko kp gt ky kz la lb aw lc bi"><span id="75b2" class="ld le iq kz b gy lf lg l lh li">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=<strong class="kz ir">True</strong>)<br/>test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=<strong class="kz ir">False</strong>)</span></pre><h2 id="8e96" class="ld le iq bd lk ll lm dn ln lo lp dp lq jy lr ls lt kc lu lv lw kg lx ly lz ma bi translated">创建模型类</h2><p id="e636" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">现在，我们将创建一个定义逻辑回归架构的类。</p><pre class="km kn ko kp gt ky kz la lb aw lc bi"><span id="7f0d" class="ld le iq kz b gy lf lg l lh li"><strong class="kz ir">class </strong>LogisticRegression(torch.nn.Module):<br/>    <strong class="kz ir">def </strong>__init__(self, input_dim, output_dim):<br/>        super(LogisticRegression, self).__init__()<br/>        self.linear = torch.nn.Linear(input_dim, output_dim)<br/><br/>    <strong class="kz ir">def </strong>forward(self, x):<br/>        outputs = self.linear(x)<br/>        <strong class="kz ir">return </strong>outputs</span></pre><h2 id="95c6" class="ld le iq bd lk ll lm dn ln lo lp dp lq jy lr ls lt kc lu lv lw kg lx ly lz ma bi translated">实例化模型类</h2><p id="616d" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">在实例化之前，我们将初始化如下一些参数。</p><pre class="km kn ko kp gt ky kz la lb aw lc bi"><span id="348d" class="ld le iq kz b gy lf lg l lh li">batch_size = 100<br/>n_iters = 3000<br/>epochs = n_iters / (len(train_dataset) / batch_size)<br/>input_dim = 784<br/>output_dim = 10<br/>lr_rate = 0.001</span></pre><p id="e718" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们初始化我们的逻辑回归模型。</p><pre class="km kn ko kp gt ky kz la lb aw lc bi"><span id="7e19" class="ld le iq kz b gy lf lg l lh li">model = LogisticRegression(input_dim, output_dim)</span></pre><h2 id="3b6d" class="ld le iq bd lk ll lm dn ln lo lp dp lq jy lr ls lt kc lu lv lw kg lx ly lz ma bi translated">实例化损失类</h2><p id="0f66" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">我们使用交叉熵来计算损失。</p><pre class="km kn ko kp gt ky kz la lb aw lc bi"><span id="bed2" class="ld le iq kz b gy lf lg l lh li">criterion = torch.nn.CrossEntropyLoss() <em class="lj"># computes softmax and then the cross entropy</em></span></pre><h2 id="e25c" class="ld le iq bd lk ll lm dn ln lo lp dp lq jy lr ls lt kc lu lv lw kg lx ly lz ma bi translated">实例化优化器类</h2><p id="c61f" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">优化器将是我们使用的学习算法。在这种情况下，我们将使用随机梯度下降。</p><pre class="km kn ko kp gt ky kz la lb aw lc bi"><span id="5d39" class="ld le iq kz b gy lf lg l lh li">optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)</span></pre><h2 id="c4c9" class="ld le iq bd lk ll lm dn ln lo lp dp lq jy lr ls lt kc lu lv lw kg lx ly lz ma bi translated">训练模型</h2><p id="b357" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">现在，在最后一步中，我们将使用以下代码来训练模型。</p><pre class="km kn ko kp gt ky kz la lb aw lc bi"><span id="2e5f" class="ld le iq kz b gy lf lg l lh li">iter = 0<br/><strong class="kz ir">for </strong>epoch <strong class="kz ir">in </strong>range(int(epochs)):<br/>    <strong class="kz ir">for </strong>i, (images, labels) <strong class="kz ir">in </strong>enumerate(train_loader):<br/>        images = Variable(images.view(-1, 28 * 28))<br/>        labels = Variable(labels)<br/><br/>        optimizer.zero_grad()<br/>        outputs = model(images)<br/>        loss = criterion(outputs, labels)<br/>        loss.backward()<br/>        optimizer.step()<br/><br/>        iter+=1<br/>        <strong class="kz ir">if </strong>iter%500==0:<br/>            <em class="lj"># calculate Accuracy<br/>            </em>correct = 0<br/>            total = 0<br/>            <strong class="kz ir">for </strong>images, labels <strong class="kz ir">in </strong>test_loader:<br/>                images = Variable(images.view(-1, 28*28))<br/>                outputs = model(images)<br/>                _, predicted = torch.max(outputs.data, 1)<br/>                total+= labels.size(0)<br/>                <em class="lj"># for gpu, bring the predicted and labels back to cpu fro python operations to work<br/>                </em>correct+= (predicted == labels).sum()<br/>            accuracy = 100 * correct/total<br/>            print(<strong class="kz ir">"Iteration: {}. Loss: {}. Accuracy: {}."</strong>.format(iter, loss.item(), accuracy))</span></pre><p id="36e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练，这个模型仅仅 3000 次迭代就给出了 82% 的<strong class="jp ir">准确率。您可以稍微调整一下参数，看看精度是否会提高。</strong></p><p id="07a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了更深入地理解 PyTorch 中的逻辑回归模型，一个很好的练习是将它应用于你能想到的任何分类问题。例如，你可以训练一个逻辑回归模型来对你最喜欢的<strong class="jp ir">漫威超级英雄</strong>的图像进行分类(应该不会很难，因为他们中的一半已经消失了:)。</p><h2 id="6f4e" class="ld le iq bd lk ll lm dn ln lo lp dp lq jy lr ls lt kc lu lv lw kg lx ly lz ma bi translated">参考</h2><p id="ee65" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">[1]<a class="ae kx" href="https://www.statisticssolutions.com/what-is-logistic-regression/" rel="noopener ugc nofollow" target="_blank">https://www . statistics solutions . com/what-is-logistic-regression/</a></p><p id="862b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]<a class="ae kx" href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/beginner/blitz/neural _ networks _ tutorial . html # sphx-glr-beginner-blitz-neural-networks-tutorial-py</a></p><p id="4363" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kx" href="https://pytorch.org/docs/stable/torchvision/datasets.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/torchvision/datasets.html</a></p></div></div>    
</body>
</html>