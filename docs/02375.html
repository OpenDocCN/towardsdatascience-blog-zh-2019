<html>
<head>
<title>Of Suppandi, Regularization and Lasso the feature selector!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">的支持，正则化和套索的特征选择器！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/of-suppandi-regularization-and-lasso-the-feature-selector-2a09acfdbc1b?source=collection_archive---------18-----------------------#2019-04-18">https://towardsdatascience.com/of-suppandi-regularization-and-lasso-the-feature-selector-2a09acfdbc1b?source=collection_archive---------18-----------------------#2019-04-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><a href="http://www.tinkleonline.com/char_info.php?cid=12&amp;button=1"><div class="gh gi jn"><img src="../Images/3abfb04ece0047ac251c22019faba7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*2PI9ulyxzY010wGIvECFYA.png"/></div></a><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Suppandi</figcaption></figure><p id="6101" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可以说，没有与印度常青树漫画英雄苏潘迪的会面，任何印度童年都是不完整的。苏潘迪是典型主角的反面角色，几乎每一项分配给他的任务都搞砸了。大多数关于苏潘迪的故事都是从他得到一条建议开始的，然后苏潘迪只是把这条建议应用到一个例子中，他把事情搞砸了，看起来很愚蠢，尽管他在故事的开始学到了新的知识。我认为没有什么比 Suppandi 的故事更好地解释了机器学习算法中的过度拟合问题。</p><p id="8375" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Suppandi 的故事的第一部分几乎相当于机器学习算法的训练阶段，第二部分是测试阶段，他试图应用他的学习，但却完全错误。这就是当 ML 算法试图从每一个数据点学习而不进行归纳时会发生的情况。</p><h2 id="22b5" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">过度拟合</h2><p id="12ab" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">在我的<a class="ae kw" href="https://www.analyticsindiamag.com/the-world-of-cost-functions%e2%80%8a-%e2%80%8ainclusivity-majoritarianism-and-oligarchy/" rel="noopener ugc nofollow" target="_blank">上一篇文章</a>中，我谈到了曲线拟合问题中代价函数的选择。当代表性曲线复杂且曲线参数的数量/值较大时，成本函数往往会过度拟合训练样本，如下图所示。那么，我们如何制作“拟合曲线”,既能从训练数据中学习广泛的模式，又能稳健地应用于看不见的数据呢？实现这个看似难以置信的任务的技术被称为<a class="ae kw" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正则化</a>。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/faebeefb22e17a224c6277672ddee5fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*5KizicdVpwvNCAo2Zy4hFQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">The green line represents the Overfitting Problem (<a class="ae kw" href="https://commons.wikimedia.org/wiki/File:Overfitting.svg" rel="noopener ugc nofollow" target="_blank">Image credits</a>)</figcaption></figure><p id="ee97" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">文献中谈到了三种正则化技术，它们与我们在上一篇文章中看到的<a class="ae kw" href="https://www.analyticsindiamag.com/the-world-of-cost-functions%e2%80%8a-%e2%80%8ainclusivity-majoritarianism-and-oligarchy/" rel="noopener ugc nofollow" target="_blank">Lp-范数</a>有着很深的联系。我们将看到这些正则化函数及其性质背后的直觉，甚至看到它们如何成立的一些数学证明。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="ab gu cl ma"><img src="../Images/b97b2a46cd893a9e8fb22fde95b3985b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*-q1diXztodH0wIEN8NDQRA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">How many features are needed for humans to predict price of a house?</figcaption></figure><p id="f74a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所有的正则化技术都是为了减少对太多变量的依赖。回到<a class="ae kw" href="https://www.kaggle.com/alphaepsilon/housing-prices-dataset#test.csv" rel="noopener ugc nofollow" target="_blank">经典住房问题</a>，输入因子或变量在 80 左右。我们知道，使用所有 80 个特征来预测房价是一种矫枉过正。作为人类，我们最多会考虑 10-20 个因素来估算价格。正则化的作用完全相同-尝试将观察到的样本拟合到曲线，但使曲线函数依赖于较少数量的变量。和前面一样，<strong class="ka ir">我们将使用线性回归的例子来更好地理解这一点</strong>。</p><h1 id="692a" class="mb ky iq bd kz mc md me lc mf mg mh lf mi mj mk li ml mm mn ll mo mp mq lo mr bi translated">岭正则化</h1><p id="6a4c" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">实现这一点最简单的方法是干预成本函数。请记住，在上一篇文章中，我们了解到，由于 L₂范数的包容性和可微性，它可能是成本函数的最佳选择。让我们回忆一下 L₂范数成本函数是什么样子的:</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d2f41d5e60c6cbec861609d2c33af093.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/0*-fr3fN6z468HPl6l.png"/></div></figure><p id="439e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">m 指的是我们观察到的总数。还记得 h(x)是基于各种输入因子 x 的线性函数。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/463ba1f9821bb0218e6f642382fc8c5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/0*5vmOOaJyDBFf3nYx.png"/></div></figure><p id="76ae" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，我们希望曲线受尽可能少的因素影响，而不出现误差激增。因此，我们可以修改成本函数，称为岭成本函数，如下所示:</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/a0d468ef6cf93ecfe6957ee72d7167c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*P265na-rbZdhfjn4.png"/></div></figure><p id="ef68" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这个节骨眼上，你们很多人可能还记得，我们在上一篇文章中看到的新求和项的 L₂范数性质。这样做的目的是尽可能降低 xⱼ的权重或系数，并在此过程中降低这些变量的影响。λ的值越高，对正则化的关注就越大。</p><p id="9da9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">看待这个问题的另一种方法是最小化以下成本</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d2f41d5e60c6cbec861609d2c33af093.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/0*-fr3fN6z468HPl6l.png"/></div></figure><p id="3dc2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">符合以下标准(山脊函数)</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/3898ff7ecc2ceac2a572b74b2162eb68.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/0*Y3bzMZp6JQpfVV9L.png"/></div></figure><p id="dd4c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">换句话说，在上述项中，系数取值有上限。</p><p id="e68b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">一个有趣的问题出现了，这是否会减少无关因素的影响或完全消除无关变量</strong>。为了回答这个问题，我还想介绍另一种正则化技术和对比，正如你将看到的，将更好地揭示这个问题。</p><h2 id="c204" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">套索正则化</h2><p id="9f02" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">这种正则化技术被隐晦地称为 Lasso(“最小绝对收缩和选择算子”)，它将成本函数修改如下:</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/1a57caf31cfd3c2879e7b8b9dbda86a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/0*lhSwuBfgVtjDX2Vj.png"/></div></figure><p id="dde0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如你所见，山脊和套索的区别就是 L2 范数和 L1 范数的区别。</p><p id="2c77" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个问题可以重新表述为最小化下列成本</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d2f41d5e60c6cbec861609d2c33af093.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/0*-fr3fN6z468HPl6l.png"/></div></figure><p id="4b77" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">符合以下标准(套索功能)</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/414109d90de52ca3cb692feac3324bd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/0*_GvJ-yHC8hnM7NE4.png"/></div></figure><h2 id="c168" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">哪个更好？</h2><p id="e995" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">原来 Lasso 顾名思义是一个选择操作符。因此，从减少对太多因素的依赖和聚焦于最重要信号的角度来看，这样做更好。事实上，当脊函数降低了不相关信号的系数时，Lasso 将它们归零。这是如何发生的——让我们试着看看背后的数学原理。</p><h2 id="3cf0" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">套索和山脊功能的可视化</h2><p id="1b79" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">让我们先画出这些函数，以便更好地形象化。假设只有两个参数 w1 和 w2。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/02619dabceb31937898aede3e18079a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l2I_35rA64KHwIIKVW3LiA.png"/></div></div></figure><p id="4b12" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">山脊函数显示为由圆形包围的区域，而套索函数显示为由菱形包围的区域。为了清楚起见，脊函数如下:</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/3898ff7ecc2ceac2a572b74b2162eb68.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/0*Y3bzMZp6JQpfVV9L.png"/></div></figure><p id="dd9c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这种情况下(两个变量)</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/7b63da8554cc66dcc8ca6e4fb608d1df.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/0*85CFMYaDvZigObP9.png"/></div></figure><p id="cf61" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意，r 是脊圆的半径。</p><p id="8d77" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">套索函数的公式如下:</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/414109d90de52ca3cb692feac3324bd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/0*_GvJ-yHC8hnM7NE4.png"/></div></figure><p id="7596" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们两个变量的例子中，套索图基于以下函数:</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/ba78ffc7aabfdd4a95682325d1cc89a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/0*ayAqQ4jQ9BCQXB0Y.png"/></div></figure><p id="9050" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如我们将要看到的，这两个函数的几何在它们的特征选择能力中起着重要作用。</p><p id="b5ab" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此时，需要注意的是<strong class="ka ir"> <em class="nf">圆和菱形与轴的交点是导致特征选择的点。</em>T3】</strong></p><h2 id="d257" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">成本函数的等高线</h2><p id="f5dc" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">现在让我们画出成本函数的等高线——共享相同成本的点(w1，w2)。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ng"><img src="../Images/307261a4676e8fe0fefb98caf329c7b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4CFOX3zRgr4y0hrk4s2LfA.png"/></div></div></figure><p id="134d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">绿色椭圆是特定成本值的成本函数的轮廓。回想一下，成本函数如下:</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d2f41d5e60c6cbec861609d2c33af093.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/0*-fr3fN6z468HPl6l.png"/></div></figure><p id="b195" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">根据轮廓的定义，椭圆边界中的所有点将具有相同的成本值。</p><p id="e3ad" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在较高的成本值下，轮廓椭圆将会扩展并向山脊/套索功能移动。等值线椭圆与山脊/套索函数的交点将是满足山脊/套索条件的最低成本值。</p><h2 id="e3a8" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">特征选择-动画</h2><p id="a9e9" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">请花一分钟时间观看下面的视频，看看等高线如何与山脊和套索图相交。</p><figure class="lw lx ly lz gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="abe6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如视频所示，轮廓在轴外的一点接触脊圆。然而<strong class="ka ir">轮廓在轴上接触套索菱形，导致特征选择发生！</strong></p><p id="ae4a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这就把我们带到了一个问题——我的十字路口视频只是一个“被操纵的用例”来突出套索的特征选择吗？这种可能性有多大？</p><h2 id="dc9a" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">Lasso 多久做一次特征选择？</h2><p id="a75c" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">假设一个随机的椭圆轮廓函数有一个概率 p 在一个特定的轴上相切于脊圆。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0797afa34b70603b47924d7e16db8f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*5d3HgZwaBxYD78NWbJIaUA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Let probability of ellipse tangential to Ridge circle be “p”</figcaption></figure><p id="090c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这表明脊函数已经完成了特征选择。换句话说，让脊函数的特征选择概率(P_r)为 P。</p><p id="6634" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">根据套索函数的几何结构，套索函数的特征选择概率(P_l)</p><blockquote class="nk"><p id="99ff" class="nl nm iq bd nn no np nq nr ns nt kv dk translated">P_l &gt; P_r =&gt; P_l &gt; p</p></blockquote><p id="31dc" class="pw-post-body-paragraph jy jz iq ka b kb nu kd ke kf nv kh ki kj nw kl km kn nx kp kq kr ny kt ku kv ij bi translated">(套索正方形完全位于脊圆内，因此与圆相切的椭圆也将与正方形相切)</p><p id="347f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们考虑在轴上与脊圆相交但不相切的椭圆。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nz"><img src="../Images/dab1b38b5f4b9bc8acb2d2ee578d9417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbUKarlbVuzFCYetXUERVQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Elliptical countours that intersect at the axes</figcaption></figure><p id="f58d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">椭圆轮廓与脊线圆以非相切方式相交的概率= 1 - p</p><p id="0ac6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请注意，非相切椭圆可能会像上图中的绿色椭圆或黑色椭圆一样相交。</p><p id="798a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">绿色椭圆仍然是套索正方形的切线，但黑色椭圆不是！这意味着除了 p，Lasso 有一个额外的概率来自这样的椭圆。所以问题如下:</p><blockquote class="nk"><p id="14cd" class="nl nm iq bd nn no np nq nr ns nt kv dk translated"><em class="oa">椭圆与正方形相切但不与脊线圆相切的概率是多少？</em></p></blockquote><p id="05a7" class="pw-post-body-paragraph jy jz iq ka b kb nu kd ke kf nv kh ki kj nw kl km kn nx kp kq kr ny kt ku kv ij bi translated">这取决于椭圆的任何部分是否落在蓝色区域内。</p><p id="74bb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">蓝色区域的面积由下式给出</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ob"><img src="../Images/725a50f22210eb69b4972eec54142cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:164/format:webp/0*8C_74Mr87AncsEEc.png"/></div></div></figure><p id="5da9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因为蓝色区域由围绕原点的 4 个直角三角形组成。</p><p id="b68e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">圆的面积由下式给出</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/2db641ff081199aaed7a64b4609b65db.png" data-original-src="https://miro.medium.com/v2/resize:fit:132/format:webp/0*6aAsnib3W9SjmwS4.png"/></div></figure><p id="265a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随机椭圆的一部分落入蓝色区域的概率由下式给出</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/fd266b9282b8350eebccee3320222185.png" data-original-src="https://miro.medium.com/v2/resize:fit:170/format:webp/0*ISRr_nQPS6G8TtPp.png"/></div></figure><p id="ea28" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这等于</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/3cb78e184277106c9de87001ae2f3ba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:50/format:webp/0*N08EpZq3ddCP9oto.png"/></div></figure><p id="e846" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">大约是 0.6366</p><p id="3195" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这意味着与脊圆相交的非相切椭圆与套索正方形相切的概率为= 1–0.6366 = 0.3634</p><blockquote class="nk"><p id="140d" class="nl nm iq bd nn no np nq nr ns nt kv dk translated">P_l = p + 0.3634 (1 — p)</p></blockquote><p id="dab5" class="pw-post-body-paragraph jy jz iq ka b kb nu kd ke kf nv kh ki kj nw kl km kn nx kp kq kr ny kt ku kv ij bi translated">这意味着，即使 P 是零，P_l 也是 0.3634。换句话说，套索方法保证至少在 36%的情况下进行特征选择！</p><p id="29bc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以给定 Lasso 和 Ridge 是 L-p 范数，学习可以更一般化。我们看到，在删除要素时，Ridge 比 Lasso 更保守一些。随着 p 的降低，特征选择发生了——不太重要的特征开始被丢弃。还想提到的是，有一些技术如<a class="ae kw" href="https://en.wikipedia.org/wiki/Elastic_net_regularization" rel="noopener ugc nofollow" target="_blank">弹性网</a>试图在这两者之间取得平衡，我不会在本文中详细讨论。</p><p id="576a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在已经学会了套索，我们不禁想，如果我们能给可怜的苏潘迪配备一个套索特征选择器，并帮助他避免在应用阶段的那些尴尬！唉，那样的话，印度不仅会失去一个童年时代的漫画英雄，还会失去一个在我们生命的早期就教导我们需要正规化的人，而我们会从他的错误中吸取教训！苏潘迪虽然成了嘲笑的对象，但毕竟是我们的老师。</p><p id="e5ba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首次出现在<a class="ae kw" href="https://www.analyticsindiamag.com/childhood-comic-hero-suppandi-meets-machine-learning-applying-lessons-to-regularisation-functions/" rel="noopener ugc nofollow" target="_blank">印度分析杂志</a>上。</p></div></div>    
</body>
</html>