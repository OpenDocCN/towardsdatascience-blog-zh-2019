<html>
<head>
<title>Review: RefineNet — Multi-path Refinement Network (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:RefineNet——多路径细化网络(语义分段)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=collection_archive---------15-----------------------#2019-04-06">https://towardsdatascience.com/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=collection_archive---------15-----------------------#2019-04-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d688" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在七个数据集上优于 FCN、DeconvNet、SegNet、CRF-RNN、DilatedNet、DeepLab-v1、DeepLab-v2</h2></div><p id="b84c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事中，由<strong class="kh ir">阿德莱德大学</strong>和<strong class="kh ir">澳大利亚机器人视觉中心</strong>开发的<strong class="kh ir">refinent</strong>被评论。通用<strong class="kh ir">多路径细化网络</strong>，明确利用下采样过程中的所有可用信息，使用长距离残差连接实现高分辨率预测。捕捉高级语义特征的更深层次可以使用来自早期卷积的细粒度特征直接细化。还引入了<strong class="kh ir">链式剩余池</strong>，它以高效的方式捕获丰富的背景上下文。这是一篇<strong class="kh ir"> 2017 CVPR </strong>论文，引用<strong class="kh ir"> 400 多次</strong>。(<a class="lk ll ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----5763d9da47c1--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="6307" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">概述</h1><ol class=""><li id="37b1" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la ms mt mu mv bi translated"><strong class="kh ir">问题之</strong><a class="ae mw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet</strong></a><strong class="kh ir">和散瞳卷积</strong></li><li id="4d36" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ms mt mu mv bi translated"><strong class="kh ir"> RefineNet </strong></li><li id="0bb2" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ms mt mu mv bi translated"><strong class="kh ir">消融研究</strong></li><li id="d427" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ms mt mu mv bi translated"><strong class="kh ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="149d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak"> 1。ResNet 和扩张卷积的问题</strong></h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/a8b6f1440ff89a331c8135fe3331759e.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*kk6593SvCBfiIS_nl_cahQ.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">(a) ResNet (b) Dilated (Atrous) Convolution</strong></figcaption></figure><ul class=""><li id="0139" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated"><strong class="kh ir">(a)</strong><a class="ae mw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet</strong></a>:It<strong class="kh ir">遭遇特征图</strong>的降尺度，不利于语义分割。</li><li id="4190" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated"><strong class="kh ir"> (b)扩张(阿特鲁)卷积</strong>:在<a class="ae mw" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">深度实验室</a>和<a class="ae mw" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">扩张网</a>中介绍。虽然它有助于保持输出特征图的分辨率更大，但 atrous 过滤器训练的计算成本很高，甚至在现代 GPU 上也很快达到内存限制。</li></ul></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="0e83" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.<strong class="ak"> RefineNet </strong></h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi nt"><img src="../Images/0425fa8ef2d475cb5b355e1f19edcbeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y2LgyBC184ChEqkYUbv62Q.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">(a) Overall Architecture, (b) RCU, (c) Fusion, (d) Chained Residual Pooling</strong></figcaption></figure><ul class=""><li id="10b4" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated"><strong class="kh ir"> (a) </strong>:图的左上方，是<a class="ae mw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>主干。沿着<a class="ae mw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>，不同分辨率的特征地图经过残差 Conv 单元(RCU)。<a class="ae mw" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">使用预激活 ResNet </a>。</li><li id="60e1" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated"><strong class="kh ir"> (b) RCU </strong>:使用剩余块，但去除了<a class="ae mw" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">批量归一化</a>。</li><li id="8118" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated"><strong class="kh ir"> (c)融合</strong>:然后多分辨率融合被用于使用逐元素求和来合并特征图。</li><li id="6f3a" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated"><strong class="kh ir"> (d)链式残差池</strong>:通过残差连接求和，将所有池块的输出特征图与输入特征图融合在一起。它<strong class="kh ir">的目的是从一个大的图像区域中捕获背景上下文。</strong></li><li id="75d0" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated"><strong class="kh ir"> (a)输出 Conv </strong>:在图的右边，最后，另一个 RCU 被放置在这里，以在多路径融合的特征图上使用非线性操作来生成用于进一步处理或用于最终预测的特征。</li></ul></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="906d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">3.<strong class="ak">消融研究</strong></h1><h2 id="093d" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">3.1.主干、链式剩余汇集和多尺度评估</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/3219660f7001073a42ca927ec76de0e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*lC2avQ1D_nMULJLjLiwp6A.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Backbones, Chained Residual Pooling, and Multi-Scale Evaluation</strong></figcaption></figure><ul class=""><li id="104f" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated">借助更深入的<a class="ae mw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-152 </a>、链式剩余池和测试时多尺度评估，两个数据集一致地获得了更高的 IoU。</li></ul><h2 id="9452" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">3.2.不同的 RefineNet 变体</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi ol"><img src="../Images/931960639cffb44e88c2d619cb784f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qTv7zNMvKMiWlBWnXN1YAA.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Different RefineNet Variants</strong></figcaption></figure><ul class=""><li id="d550" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated"><strong class="kh ir"> (a)单个 RefineNet 模型</strong>:它从<a class="ae mw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>的四个模块中获取所有四个输入，并在单个过程中融合所有分辨率的特征图。</li><li id="bb42" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated"><strong class="kh ir"> (b) 2 级 RefineNet </strong>:仅采用两个 RefineNet 模块，而不是四个。底部的一个 RefineNet-2 有来自<a class="ae mw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>模块 3 和 4 的两个输入，另一个有三个输入，两个来自剩余的<a class="ae mw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>模块，一个来自 RefineNet-2。</li><li id="8984" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated"><strong class="kh ir"> (c) 4 级级联 2 尺度细化</strong>:图像的 2 个尺度作为输入，分别用 2 个<a class="ae mw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">resnet</a>生成特征图。输入图像被缩放到 1.2 和 0.6 倍，并被送入两个独立的<a class="ae mw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">结果网</a>。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8ff4d96468b4377755968a226be29864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*Eu7zhLGESiIi74aLVT3OCA.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Different RefineNet Variants</strong></figcaption></figure><ul class=""><li id="a235" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated">由于网络容量更大，4 级联 2 级 RefineNet 具有最佳结果，但它也导致训练时间更长。</li><li id="f9d4" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">因此，<strong class="kh ir"> 4 级联 RefineNet 用于与最先进的方法进行比较。</strong></li></ul></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="1636" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">4.<strong class="ak">与最先进方法的比较</strong></h1><h2 id="340b" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">4.1.人物角色</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/615a98d8bb0005d2dded3001265888c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*x1c3sFnWXdO_QWT3pEFtBg.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Person-Part</strong></figcaption></figure><ul class=""><li id="5ee6" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated">人体部分数据集提供了六个人体部分的像素级标签，包括头部、躯干、上臂/下臂和上/小腿。剩下的都是背景。</li><li id="23d5" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">有 1717 幅训练图像和 1818 幅测试图像。</li><li id="7349" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">RefineNet 大幅度超过<a class="ae mw" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deep lab v1&amp;deep lab v2</a>。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/bc4a7c6e43ade02e98b20f7914d4e342.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*_PC_X4y92TD9fu7qSeeg2g.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Some Examples</strong></figcaption></figure><h2 id="0b26" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">4.2.NYUD-v2</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi op"><img src="../Images/23cc198079ae854a7c835772f4458b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*gxYzAn2TJOq4YSZ0QovVNA.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">NYUD-v2</strong></figcaption></figure><ul class=""><li id="b721" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated">它由 1449 幅显示室内场景的 RGB-D 图像组成，共有 40 个类别。</li><li id="aa9e" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">使用具有 795 和 654 个图像的标准训练/测试分割。</li><li id="d2dd" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">在不使用深度信息进行训练的情况下，RefineNet 优于<a class="ae mw" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN-32s </a>。</li></ul><h2 id="042d" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">4.3.帕斯卡 VOC 2012</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi oq"><img src="../Images/742e2ce7f97d81853a0b367ff56c63a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EjyauBqqpD0TlOqjTnhfmA.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">PASCAL VOC 2012 Test Set</strong></figcaption></figure><ul class=""><li id="18ae" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated">它包括 20 个对象类别和一个背景类。</li><li id="e55d" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">它被分成训练集、验证集和测试集，每个都有 1464、1449 和 1456 个图像。</li><li id="a45e" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">尝试了在<a class="ae mw" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deep lab v1&amp;deep lab v2</a>中用于进一步细化的条件随机场(CRF)方法，但在验证集上仅有 0.1%的边际改善。因此，通用报告格式不用于 RefineNet。</li><li id="ac79" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">明显优于<a class="ae mw" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN-8s </a>、<a class="ae mw" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">德孔内</a>、<a class="ae mw" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c"> CRF-RNN </a>和<a class="ae mw" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">DeepLabv1&amp;DeepLabv2</a>。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi or"><img src="../Images/1a123e4d52f61632c36f56a5175af040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*k8iBIeQJGp9WvHh4QtaT7Q.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Some Examples</strong></figcaption></figure><h2 id="ec1c" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">4.4.城市景观</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi os"><img src="../Images/2da31d91f274f9d94e59c0b0c7e9b22e.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*erVSAprazAJcUSzqF93XSg.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Cityscapes Test Set</strong></figcaption></figure><ul class=""><li id="d9e5" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated">这是一个来自 50 个不同欧洲城市的街景图像数据集。该数据集提供了道路、汽车、行人、自行车、天空等的细粒度像素级注释。</li><li id="d2f4" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">所提供的训练集具有 2975 幅图像，而验证集具有 500 幅图像。</li><li id="d254" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">考虑对 19 个班级进行培训和评估。</li><li id="c687" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">同样，RefineNet 的表现优于<a class="ae mw" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN-8s </a>、<a class="ae mw" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">去配置网</a>和<a class="ae mw" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">深度实验室 v1 &amp;深度实验室 v2 </a>，以及<a class="ae mw" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">扩展网</a>。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/bc3b4215af4c2c7a9c15eea9714930d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*YLttbS_qaGM9fcAoZAOFvg.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">Some Examples</strong></figcaption></figure><h2 id="d2e0" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">4.5.PASCAL 上下文</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi ou"><img src="../Images/2be7694db6f71134d4ebf479e0f887ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*nSAF2rP7Fbie8mmdd-k0fw.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">PASCAL-Context</strong></figcaption></figure><ul class=""><li id="141e" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated">它为 PASCAL VOC 图像提供了整个场景的分割标签，共有 60 类(1 为背景)。</li><li id="8817" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">训练集包含 4998 幅图像，测试集包含 5105 幅图像。</li><li id="714c" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">同样，RefineNet 的表现优于<a class="ae mw" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN-8s </a>和<a class="ae mw" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv2 </a>。</li></ul><h2 id="4b52" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">4.6.太阳-RGBD</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/a5cc8358c15212c1ed1c059409b01bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*f3BEzu8jF1mhn7TF1s5VFQ.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">SUN-RGBD</strong></figcaption></figure><ul class=""><li id="a8a5" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated">它包含大约 10，000 幅 RGB-D 室内图像，并为 37 个类别提供像素标记遮罩。</li><li id="32ce" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">在不使用深度信息进行训练的情况下，RefineNet 仍然是所有方法中最好的。</li></ul><h2 id="c61c" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">4.7.ADE20K MIT</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/0076e7332448c3b1cd626a869088ea03.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*ptWaayP8-FlZc7jrMOgS2w.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk"><strong class="bd no">ADE20K dataset (150 classes) val set.</strong></figcaption></figure><ul class=""><li id="ba7a" class="ml mm iq kh b ki kj kl km ko np ks nq kw nr la ns mt mu mv bi translated">这是一个场景解析数据集，在超过 20K 个场景图像上提供了 150 个类别的密集标签。</li><li id="b156" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">这些类别包括各种各样的对象(例如，人、汽车等。)和东西(如天空、道路等。).所提供的由 2000 幅图像组成的验证集用于定量评估。</li><li id="23c6" class="ml mm iq kh b ki mx kl my ko mz ks na kw nb la ns mt mu mv bi translated">还是那句话，RefineNet 比<a class="ae mw" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN-8s </a>、<a class="ae mw" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96"> SegNet </a>和<a class="ae mw" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5"> DilatedNet </a>，甚至是<a class="ae mw" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96"> SegNet </a>和<a class="ae mw" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5"> DilatedNet </a>的级联版。</li></ul></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h2 id="8c50" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">参考</h2><p id="5731" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko ox kq kr ks oy ku kv kw oz ky kz la ij bi translated">【2017 CVPR】【RefineNet】<br/><a class="ae mw" href="https://arxiv.org/abs/1611.06612" rel="noopener ugc nofollow" target="_blank">RefineNet:用于高分辨率语义分割的多路径细化网络</a></p><h2 id="fde8" class="ny lu iq bd lv nz oa dn lz ob oc dp md ko od oe mf ks of og mh kw oh oi mj oj bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko ox kq kr ks oy ku kv kw oz ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae mw" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae mw" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae mw" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae mw" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae mw" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae mw" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae mw" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae mw" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae mw" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae mw" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae mw" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolo v1</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">yolo v2/yolo 9000</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">yolo v3</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net</a>[<a class="ae mw" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a>]</p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong><a class="ae mw" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae mw" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae mw" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae mw" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae mw" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae mw" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae mw" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae mw" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a><a class="ae mw" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a></p><p id="fc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong>[<a class="ae mw" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae mw" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae mw" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae mw" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>[<a class="ae mw" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>]</p><p id="3134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割<br/></strong>[<a class="ae mw" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS</a>[<a class="ae mw" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae mw" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae mw" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae mw" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">Instance fcn</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS</a></p><p id="58de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">超分辨率<br/></strong>[<a class="ae mw" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">Sr CNN</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4">fsr CNN</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f">VDSR</a>][<a class="ae mw" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener">ESPCN</a>][<a class="ae mw" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>][<a class="ae mw" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener">DRCN</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994">DRRN</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8">LapSRN&amp;MS-LapSRN</a>][<a class="ae mw" rel="noopener" target="_blank" href="/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8">srdensenenet</a></p><p id="f29d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">人体姿态估计</strong><br/><a class="ae mw" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">深度姿态</a><a class="ae mw" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">汤普逊·尼普斯 14 </a></p></div></div>    
</body>
</html>