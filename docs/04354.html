<html>
<head>
<title>Support Vector Machines for Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-for-classification-fc7c1565e3?source=collection_archive---------4-----------------------#2019-07-07">https://towardsdatascience.com/support-vector-machines-for-classification-fc7c1565e3?source=collection_archive---------4-----------------------#2019-07-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d3a3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解支持向量机(SVM)，从直觉到实现</h2></div><p id="c2e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习中的分类是学习区分数据集中属于两个或更多类别的点的任务。在几何术语中，将一组点与某个类别相关联涉及找到这些点之间的最佳可能分离。假设我们有一个类似这样的数据集:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/4374d4f99257574581a2d655e04d34de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*6U9NrruycDBsPOyivpn8UQ.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Figure 1. Dataset representation and margin</figcaption></figure><p id="c85f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们可以清楚地区分两类，分别用<strong class="kk iu"> C1 = 1 </strong>和<strong class="kk iu"> C2 = -1 </strong>来标识。我们也将这定义为一个<em class="lq">二元分类问题</em>。例如，如果我们的每个数据点都代表电子邮件文本，并且我们希望将这些分类为垃圾邮件和非垃圾邮件，那么我们将每封电子邮件都是一个数据点<strong class="kk iu"> x </strong>，而<strong class="kk iu"> y = 1 </strong>将识别垃圾邮件，<strong class="kk iu"> y = -1 </strong>非垃圾邮件。同样，我们还可以有其他类似的场景，比如猫狗分类、信用/非信用批准等。</p><p id="03b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在图 1 中，我们还可以看到，中间的线性函数确保了两个类别之间的最佳分离。我们故意称之为<em class="lq">超平面</em>的原因是因为我们这里的描述可以扩展到多维场景。例如，如果我们在 3D 空间中工作，我们将有一个分割平面而不是一条线。</p><p id="818b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从现在起，我们将分类数据集之间的分离标识为<em class="lq">边缘</em>、<em class="lq"> </em>，并且它将由最近点和分离超平面之间形成的距离来管理。因此，立即出现的问题是，我们如何确保这个最佳超平面确保两个类别的最佳可能余量？这正是支持向量机，或简称 SVM 将为我们做的。</p><p id="33c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在继续之前，值得指出的是，支持向量机是用于分类任务的最强大的机器学习算法之一，广泛用于从计算机视觉到 NLP 的应用。让我们进一步探讨这个方法。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="8f5d" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">导出原始问题</h2><p id="d4aa" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">首先，假设我们的数据集由<em class="lq"> N </em>个点组成。在图 1 中，我们可以看到，通过使用由下式定义的线性函数(即超平面),可以很好地分离这两个类别:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/1a67e47bf1808484f676db481d63e430.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*a6aRrNwOgLt7TashQdxEtA.png"/></div></figure><p id="e78f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<strong class="kk iu"> x </strong> <em class="lq"> n </em>表示一个数据点，<strong class="kk iu"> w </strong>是一个权重向量，<em class="lq"> b </em>是偏差，<strong class="kk iu"> <em class="lq"> y </em> </strong> <em class="lq"> n </em>是模型的预测。在这种情况下，对于位于超平面两侧的点，每个预测可以是 1 或-1，对于位于超平面本身上的点，每个预测可以是 0。给定一个包含几个<strong class="kk iu"> x </strong>及其对应的<strong class="kk iu"> y </strong>的数据集，我们想要做的是找到给我们最好的可能余量的<strong class="kk iu"> w </strong>和<em class="lq"> b </em>的值。</p><p id="c931" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了更清楚，让我们用几何图形来说明我们的模型变量:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/44903ac7b12a975ceceed96f60112689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*bfpvUbN2Zn9XQgaenGIXxg.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Figure 2. Geometric representation of variables</figcaption></figure><p id="b33b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<strong class="kk iu">x<em class="lq">n</em>T25】是最接近超平面的点之一，并且它形成了源自该点的正交向量<strong class="kk iu"> d </strong>。我们可以看到，矢量<strong class="kk iu"> d </strong>与<strong class="kk iu"> w </strong>方向相同。此外，位于超平面上的任何一点<strong class="kk iu"> x0 </strong>都会与<strong class="kk iu">x<em class="lq">n</em>T39】形成一个矢量<strong class="kk iu"> r </strong>。根据这些定义，我们可以清楚地看到<strong class="kk iu"> d </strong>是<strong class="kk iu"> r </strong>在<strong class="kk iu"> w </strong>上的投影，由下式给出:</strong></strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi my"><img src="../Images/57094d0bf36392ddd99b63b119a91318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*IF26ISLrItP_hLomDgH9bg.png"/></div></figure><p id="e3e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们把<em class="lq"> b </em>和- <em class="lq"> b </em>加到这个等式的分子上。<strong class="kk iu"> d </strong>的大小将给出<strong class="kk iu"> x </strong>到超平面的距离，定义如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/0aa1cafb937af08a1f64337dbdf681cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*3NeWzHSpkKBOQUP8P1bMmg.png"/></div></figure><p id="51a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，对于位于超平面上的<strong class="kk iu"> x0 </strong>，其对应的预测<strong class="kk iu"> y0 </strong>为零，这就是括号内的表达式消失的原因。所以为了找到最优余量，我们要做的就是最大化<em class="lq"> d </em>。有了这些考虑，我们最终可以陈述模型的优化问题:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi na"><img src="../Images/f13f75eea6f927448fd0ecebc87bcfe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rWQzhnzVngmJYHvpmt-g5w.png"/></div></figure><p id="4bec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，从微积分我们知道，为了找到最大值，我们需要使用导数。在这种情况下，我们使用不涉及对等术语的表达会更方便。那么，我们怎样才能使 d<em class="lq">d</em>方程更容易优化呢？是的，你猜对了！将括号内的倒数项倒置，然后将其转化为最小化问题。让我们这样做:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/30ac2a5440bd93f47e59ec81eae1439e.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*ZQe1xXX9N3u6ciud0eL55g.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Equation 1. Optimal distance expression</figcaption></figure><p id="f30e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以现在你可能会问，为什么我们把 w 的幂提高到 2，然后除以 2？这只是我们在计算导数时，为了让事情变得简单而应用的一个数学技巧。基本上，主要原因是这个表达式的导数就是<strong class="kk iu"> w </strong>。然而，最重要的是，这将帮助我们定义一个二次规划问题，我们将能够使用流行的优化库来解决这个问题，我们将在后面看到。</p><p id="a619" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，现在我们已经定义了优化问题的一部分。但是，让我们更深入地看看这个限制:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/bb571df4b9cd0cc6a480238936a40a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*SLIPyfNi1RG8Eo_-DapTww.png"/></div></figure><p id="b940" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，这涉及到一个绝对值。看起来不太好，对吧？如果我们能摆脱它就好了。怎么才能做到呢？让我们稍微想想我们预测的正确性。我们希望我们的模型尽最大努力对所有点进行正确分类。但是我们如何用数学术语定义一个正确分类的点呢？让我们首先介绍一些有用的符号。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b2babf21d658f2c7e451f7e1013193a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*kFAqPvyJHWHKnU4tXjMvng.png"/></div></figure><p id="7bb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以我们希望我们的模型做出与正确答案<strong class="kk iu"> <em class="lq"> yn </em> </strong>相等的预测。当给出的答案和预测的答案符号相同时，就会发生这种情况。也就是说，当两者的乘积至少为 1 时。让我们用数学表达式来表述:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/59743fc154ef59ed590758d16e6befad.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*MBPV1xMZy6YsmnhTl4LUnQ.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Equation 2. Modified restriction</figcaption></figure><p id="62e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太好了！有了这个限制，以及等式 1，我们的优化问题将如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8fbdf9ba184a9a373af0a190bf282f3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*L0l8cVX-bgsr9voLqToIew.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Equation 3. Expression for the primal problem</figcaption></figure><p id="76c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个表达式定义了支持向量机的主要问题。请注意，对于由(<strong class="kk iu"> x </strong> <em class="lq"> n </em>，<em class="lq"> yn </em>)形成的每个限制，都有一个关联的 Langrange 乘数<em class="lq"> α </em>。阿尔法值大于零的数据点<strong class="kk iu">x</strong>n 也被称为<em class="lq">支持向量</em>，因为它们影响分离超平面的行为。</p><p id="a59a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，如果我们看一下等式 3，我们可以看到它在<strong class="kk iu"> w </strong>上有一个二次型。因此我们可以说这是一个二次规划问题。然而，我们这里有一个问题，这个问题取决于三个变量，而不是一个。让它只依赖于一个参数集肯定会让事情变得更简单，这将激发我们对下一节讨论的对偶问题的推导。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="a28a" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">定义对偶问题</h2><p id="25a3" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">在上一节中，我们阐述了 SVM 的原始问题。然而，正如我们之前讨论的，让它依赖于一个参数会容易得多。让我们现在做那件事。</p><p id="2a6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们计算等式 3 <em class="lq"> </em>中的<em class="lq"> L </em>相对于<strong class="kk iu"> w </strong>和<em class="lq"> b </em>的导数，然后使导数等于零，我们得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/5a05bbaca177d064cbf7990b7b29d4a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*AMPyySXbQ3m3kAuobLefyQ.png"/></div></figure><p id="2577" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这里，我们得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/fe62cc8b0100dbd6dfd1db42e0ad6d7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*pvxaWzgMnJQsvkbOnmxtOw.png"/></div></figure><p id="4330" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">替换等式 3 中的这两个表达式，我们得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/ab3d44c190f4e4c40e0a4bf2fc1fc255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yUrbqDeRoQpsB4WqjsAMjw.png"/></div></div></figure><p id="c2f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">重新排列:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi my"><img src="../Images/36a6ddc7e048b8cb3f76a686bdadef07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*dZDt1nn5YW2DC4dYQSIp2w.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Equation 4</figcaption></figure><p id="55d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，正如我们所看到的，这已经成为一个二次规划问题，它只取决于拉格朗日乘数<em class="lq"> α </em>，而不再取决于<strong class="kk iu">w</strong><em class="lq">T5】和<em class="lq"> b </em>。这肯定比我们之前讨论的原始问题要简单得多。除此之外，关于等式 4 中<strong class="kk iu">x</strong>T10】m</em>和<em class="lq"> </em> <strong class="kk iu"> x </strong> <em class="lq"> n </em>的产生还有一个有趣的方面。原来这个产品其实可以表述为一个<em class="lq">内核函数</em>。一般来说，核函数允许我们将非线性可分空间转换为线性可分空间，并且当我们试图执行不同的分类任务时，它是一个有用的工具。通常使用的核包括但不限于线性核和径向基函数(RBF)。更多关于内核方法的信息可以在【2】这里找到。</p><p id="bc7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，对偶问题可以表述如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi no"><img src="../Images/80a13610858f200e59249595feecd0d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*yRSzCvyos95QYJYXOSmfrQ.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Equation 5. Dual problem for SVM</figcaption></figure><p id="5f71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个最大化的二次规划问题。这里，<strong class="kk iu"> k </strong>是我们定义的一个核函数。另外，请注意第一个限制，它强制所有字母大于或等于零。如前所述，阿尔法值大于零的数据点是<em class="lq">支持向量</em>，并影响分离超平面的行为。此外，如果我们将此转化为最小化问题，并将这些表达式转换为矩阵形式，我们可以很容易地使用 CVXOPT 等优化库来求解最佳参数。我们将在后面看到如何做到这一点。</p><p id="08b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们对对偶问题的定义在这里有一个重要的限制。只有当类别可以完全分离时，它才能很好地工作。这就是为什么这个默认版本也被称为<em class="lq">硬边际 SVM </em>。但是你可以想象，我们不太可能在现实生活中找到完全分离的数据集。因此，我们接下来将讨论一种变体，它允许我们将支持向量机扩展到一些点可能被错误分类的场景，也称为<em class="lq">软裕度 SVM </em>。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="c378" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">软利润 SVM</h2><p id="a9f2" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">如前所述，硬边界支持向量机在实际应用中用途有限。在这里，我们将表明，通过对原始对偶问题做一点小小的改变，我们可以将支持向量机扩展到一些点可能被错误分类的场景。让我们用图表来说明我们的想法:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi np"><img src="../Images/46dd0ee0e23c621402ae3322a2b18db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*Cc2YUbhIDrsTbixk0kajFg.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Figure 3. Misclassification scenarios for the soft-margin SVM</figcaption></figure><p id="8164" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，我们定义了一个新的公差变量ε。位于边缘的点的ε = 0。另一方面，我们离正确的边界越远，ε的值就越大。最终，对于在超平面错误一侧的点，我们期望ε大于 1。我们现在需要做的是，把这个新变量插入到原始问题的初始定义中。因此，如果我们将ε加入等式 2 中定义的限制条件，我们会得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4d83d31d61827e79c4b79469561b1a8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*aCakxlEZUNECqNDeGd-hMg.png"/></div></figure><p id="4f8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着我们的限制现在允许一些点离边界更远一点。ε本身现在成为另一个参数，我们必须在我们的主要问题中考虑，我们必须努力使它最小化。在这种情况下，我们的目标是:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/fd771c35a024682e429563d816e910d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*_JKyikmkspqBLU4-X4NDHA.png"/></div></figure><p id="3598" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如我们所见，这里的第二项是我们定义的一个新变量<em class="lq"> C </em>和所有ε之和的乘积。这将迫使当<em class="lq"> C </em>较大时，由ε值给出的总体错误分类将较小。这意味着，<em class="lq"> C </em>越大，保证金就越严格。通过考虑原始问题中的这些新变量，我们得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ns"><img src="../Images/98aa262b1e39c2fd4bd527483e026118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EWWKD7Si9dc-3X3lqKTCbw.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Equation 6</figcaption></figure><p id="da31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，μ是我们为公差值ε定义的拉格朗日乘数。我们期望每个μ大于或等于零。现在让我们相对于<strong class="kk iu"> w </strong>、<em class="lq"> b </em>和ε来区分<em class="lq"> L </em>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7d8650bc14c60d2ad766643cdc7f410a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*KqHSqU9neXp4INMI37J0Ww.png"/></div></figure><p id="5bfa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这里，我们得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bea9c5e5a314b4468043871b275d3532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*oolfRhznW2P8A5SNDVfTvw.png"/></div></figure><p id="d237" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们把注意力稍微转向第三个表达。我们之前说过，μ和<em class="lq"> α </em>都可以大于或等于零，这意味着:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/0bad198c0ca3432569ad29c9351cd7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*w0b2SBsRcBMLVG2Bas4mmg.png"/></div></figure><p id="788a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，通过替换等式 6 中的所有这些表达式，并像我们对硬边界 SVM 情况所做的那样重新排列，我们得到以下结果:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi no"><img src="../Images/eaf0085e0ff498528a81eea276d86857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*YNtivE3wVi2OsyO10DddlA.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Equation 7. Dual problem for the soft-margin SVM</figcaption></figure><p id="691a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太好了！在这一点上，我们可以看到，除了现在考虑到<em class="lq"> C </em>的第一个限制之外，一切都与硬边际 SVM 的情况相同。在下一节中，我们将通过使用一个玩具数据集来实现一个软边界 SVM，我们将使用 CVXOPT 进行优化。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="8c7d" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">用 CVXOPT 实现软利润 SVM</h2><p id="7e38" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">注意:完整的实现可以在<a class="ae nn" href="https://bit.ly/2xBDJ2Q" rel="noopener ugc nofollow" target="_blank">https://bit.ly/2xBDJ2Q</a>的 Jupyter 笔记本上获得</p><p id="e6f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CVXOPT [3]是一个用于凸优化的 Python 库。我们将用它来解决软间隔支持向量机的对偶问题。然而，在此之前，我们需要将这样一个对偶问题转化为一个最小化目标，然后将所有变量转化为矩阵。为了将对偶问题转化为最小化问题，我们只需反转等式 7 中拉格朗日量的符号，然后将其表示为矩阵形式:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ff6e5743935ba1115f10a7aedb73c0c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*8v4BTPoU_bK-R3QlkR6qVQ.png"/></div></figure><p id="603f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在哪里</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/930c6cd427df14c02804c446e313979b.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*ZcuEVGYZrp71DJ5XxvCdLg.png"/></div></figure><p id="542f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，我们使用了*操作符来表示 Python 中的逐元素乘法。此外，<em class="lq"> K </em>是通过计算整个数据集<strong class="kk iu"> X </strong>的核值而得到的 Gram 矩阵。</p><p id="d3ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在以矩阵形式表示的限制如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ny"><img src="../Images/48ef16a9c42c6c1ea240a5ecfea34e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*agcqwymKJOLy7A63vqfu6g.png"/></div></div></figure><p id="a059" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，CVXOPT 将需要一个特殊的符号来配置我们的对偶问题，以及我们的限制。本质上，我们需要做的是配置以下矩阵:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/fa29dc12506332beb3e2de6e959cb621.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*ZK9GtQxh1oTO43huSVWZAw.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/a830947051b8ba5464fc8c8b1eb4c25c.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*l657kvgsPtmD4RkXLGfsvg.png"/></div></figure><p id="f914" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 CVXOPT 的相应 Python 代码如下:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="5546" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后的容差参数定义了在声明收敛之前我们允许的变化量。如你所见，<strong class="kk iu"> solvers.qp </strong>调用接收我们配置的所有矩阵，它将打印出每个时期的成本值，直到收敛。</p><p id="6fa8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们的 SVM 被完全训练，我们可以通过使用下面的表达式容易地获得参数<strong class="kk iu"> w </strong>和<em class="lq"> b </em>:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/87e4e1b9923e9c2866e6b44a7b49402c.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*WtFFXRHzTfwOogLPqxhTBQ.png"/></div></figure><p id="4160" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="kk iu"> S </strong>是支持向量的子集。相应的实现如下:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="d1cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了便于说明，我们使用两个自己生成的数据集测试了这个实现。第一个数据集是线性可分的，第二个数据集包含可以用圆圈分隔的类别。这是两个数据集的图表:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oe"><img src="../Images/dcd66d4d01b8ac0047f1e18fc85a7b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFJLohsaLdivrpLT2vq0DA.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Figure 4. Dataset graphs</figcaption></figure><p id="2cf8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这两种情况下，我们可以看到属于第一类和第二类的点分别被染成红色和蓝色。同样，黑色的点是我们找到的支持向量。通过适当配置<em class="lq"> C </em>参数，我们应该可以看到不同的裕量行为。</p><p id="feb4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，第二数据集不是线性可分的。在这些情况下，我们所做的就是使用<em class="lq">内核技巧</em>。也就是说，我们使用一个核函数，将我们从某个空间中的非线性可分数据集带到另一个空间中的线性可分数据集。特别是，我们在本例中处理第二个数据集的方法是使用 RBF 而不是线性核。</p><p id="5367" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性核和 RBF 可以分别通过这些表达式来计算:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi of"><img src="../Images/598b7c8edf1e217e6dcc87979a674532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*idGesu_M7W2M_Vm-__5D3g.png"/></div></figure><p id="e55c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中γ是我们为 RBF 情况设置的超参数。这些内核的相应实现如下所示:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ob oc l"/></div></figure></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="7f04" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">结束语</h2><p id="1420" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">支持向量机是一种非常强大的机器学习模型。尽管我们的注意力主要集中在用于二元分类的支持向量机上，但我们可以通过使用一对一或一对一等技术将它们的使用扩展到多类场景，这涉及到为每一对类创建一个 SVM。我强烈建议您进一步研究实现，以及我们如何使用流行的库来训练 SVM，例如<em class="lq"> sklearn。</em>在这里，您需要为<em class="lq"> C </em>选择合适的值，以及一个合适的内核以获得更好的分类结果。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="5a72" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">参考</h2><p id="da2b" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">[1] Bishop，Christopher M. <em class="lq">模式识别和机器学习</em> (2006)施普林格出版社柏林，海德堡。</p><p id="8909" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]托马斯·霍夫曼。伯恩哈德·肖科普夫。《机器学习中的核心方法》 (2008)统计年鉴。第 36 卷，第 3 卷，1171-1220 页。</p><p id="81cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3] CVXOPT。<em class="lq">解一个二次规划</em>。在 https://cvxopt.org/examples/tutorial/qp.html<a class="ae nn" href="https://cvxopt.org/examples/tutorial/qp.html" rel="noopener ugc nofollow" target="_blank">有售</a></p><p id="a863" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4]加州理工学院。<em class="lq">从数据中学习</em> (2012)。在<a class="ae nn" href="https://work.caltech.edu/telecourse" rel="noopener ugc nofollow" target="_blank">https://work.caltech.edu/telecourse</a>上市</p></div></div>    
</body>
</html>