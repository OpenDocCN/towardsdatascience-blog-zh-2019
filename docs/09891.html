<html>
<head>
<title>Introducing Hoeffding’s Inequality for creating Storage-less Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">引入 Hoeffding 不等式生成无存储决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introducing-hoeffdings-inequality-for-creating-storage-less-decision-trees-b5135e65e51e?source=collection_archive---------15-----------------------#2019-12-27">https://towardsdatascience.com/introducing-hoeffdings-inequality-for-creating-storage-less-decision-trees-b5135e65e51e?source=collection_archive---------15-----------------------#2019-12-27</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="323d" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">探索 Hoeffding 不等式陈述了什么，以及如何使用它来创建一类特殊的不使用存储的决策树:Hoeffding 树。</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/1c3c0eef4bf6b68c3488fb579f05841c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ta2dSdHMn0xqTklF6_tmyA.jpeg"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">© by my lovely wife <a class="ae kz" href="https://dribbble.com/tinati" rel="noopener ugc nofollow" target="_blank">Tinati Kübler</a></figcaption></figure><h1 id="990f" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">任务</h1><p id="deaf" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi mo translated">我想象你有一个巨大的带标签的数据集，你想为一个预测任务建立一个模型。例如，这可以是 Twitter，在那里你有比你可以计算的包括相应的喜欢(标签)数量更多的推文(功能)。现在你想要建立一个模型，可以预测一条推文是否会被喜欢超过 100 次，对去年写的所有推文进行训练，也就是说，我们想要解决一个<strong class="lu iv">分类任务</strong>。标签为 1 意味着该推文有超过 100 个赞，否则为 0。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj mx"><img src="../Images/2b28a6f4e4e4765f3c4bb904cdfe655e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vZI1Avy11lt3CaRZ"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Photo by <a class="ae kz" href="https://unsplash.com/@stereophototyp?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sara Kurfeß</a> on <a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4cd7" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">你决定使用决策树，甚至是从它衍生出来的智能工具，比如随机森林或梯度推进。但是决策树、基于决策树的模型、甚至其他模型都有以下缺点:</p><blockquote class="nd"><p id="2d00" class="ne nf iu bd ng nh ni nj nk nl nm mn dk translated">您需要在内存中随时可用的训练数据。</p></blockquote><h1 id="842e" class="la lb iu bd lc ld le lf lg lh li lj lk ka nn kb lm kd no ke lo kg np kh lq lr bi translated">问题是</h1><p id="4d75" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在最好的情况下，完整的训练数据可以放入本地机器的内存中。但是，您意识到 tweets 数据集大于 8–32GB，所以您运气不好。也许您可以访问具有 512 GB RAM 的集群，但这也不够大。</p></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><p id="4388" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">数据集实际上有多大？让我们做一个粗略的估计。<a class="ae kz" href="https://blog.twitter.com/engineering/en_us/a/2013/new-tweets-per-second-record-and-how.html" rel="noopener ugc nofollow" target="_blank">推特</a>本身和其他几个来源(这里是<a class="ae kz" href="https://www.internetlivestats.com/twitter-statistics/" rel="noopener ugc nofollow" target="_blank">这里是</a>和<a class="ae kz" href="https://www.dsayce.com/social-media/tweets-day/" rel="noopener ugc nofollow" target="_blank">这里是</a>)报告称，每秒大约有 6000 条推文。这意味着每年大约有 6000 * 60 * 24 * 365 =189,216,000,000‬推文。让我们假设每条 tweet 的大小为 140 字节，每个字节对应于<a class="ae kz" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">单词包编码</a>中的一个字符。<em class="nx">我们忽略了每条 tweet 可能附带了一些元数据，我们也可以使用二元模型、三元模型等。</em>这是一个巨大的 140 *189,216,000,000‬/10⁹=<strong class="lu iv">26，490 GB 的推文数据</strong>！</p><p id="5c8c" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">因此，使用 RAM 不是一个选项。即使你碰巧拥有一个足够容纳整个数据集的硬盘，从其中读取数据也会使训练非常缓慢，正如你在这里看到的<a class="ae kz" href="https://queue.acm.org/detail.cfm?id=1563874" rel="noopener ugc nofollow" target="_blank">(图 3) </a>。</p><p id="e2d0" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">嗯，怎么办呢？</p><h1 id="f120" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">解决方案</h1><p id="cf78" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">来自华盛顿大学计算机科学与工程系的 Pedro Domingos 和 Geoff Hulten 介绍了决策树的一种变体，称为<em class="nx">Hoeffding Trees</em>【1】，可以用于<em class="nx">流</em>方式。这意味着我们只需<strong class="lu iv">解析一次大的训练数据集</strong>，并在此过程中构建树。</p><p id="9288" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">我们甚至不必存储所使用的样本:我们可以直接从 Twitter(或任何大型数据库)中获取它们，通过增加一些计数器来处理它们，然后再次忘记它们。为什么这是可能的可以用<em class="nx">赫夫丁不等式</em>来解释，这就是赫夫丁树的名字。</p><p id="553b" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">高层次的想法是，我们不必查看所有的样本，而只需查看决策树算法中每个分裂点的<strong class="lu iv">足够大的随机子集</strong>。这个子集有多大是下一节的主题。</p><p id="f182" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">我写这篇文章是因为 Domingos 和 Hulten 的论文非常专业(因此也非常精确),我想提出一个高级的、易于理解的观点来解释作者的方法为什么有效。</p></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><p id="9289" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">此外，如果您不想在接下来的部分中处理数学问题，至少可以看看最后一部分的一些代码！在那里，我使用 scikit-multiflow 软件包来使用 Hoeffding 树。</p><h1 id="79b0" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">赫夫丁不等式</h1><p id="9773" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">让我们研究一下霍夫丁不等式的内容，以及如何利用它来解决存储问题。</p><h2 id="cf64" class="ny lb iu bd lc nz oa dn lg ob oc dp lk mb od oe lm mf of og lo mj oh oi lq oj bi translated">介绍</h2><p id="1d31" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated"><a class="ae kz" href="https://en.wikipedia.org/wiki/Wassily_Hoeffding" rel="noopener ugc nofollow" target="_blank">芬兰统计学家、非参数统计的创始人之一瓦西里·赫夫丁</a>(基于<a class="ae kz" href="https://en.wikipedia.org/wiki/Herman_Chernoff" rel="noopener ugc nofollow" target="_blank">赫尔曼·切诺夫</a>的思想)发现了一个不等式[2]，量化了以下陈述:</p><blockquote class="nd"><p id="b7bf" class="ne nf iu bd ng nh ni nj nk nl nm mn dk translated">有界随机变量的和(也是均值)紧紧围绕其期望值。</p></blockquote><p id="2b6f" class="pw-post-body-paragraph ls lt iu lu b lv ok jv lx ly ol jy ma mb om md me mf on mh mi mj oo ml mm mn in bi translated">以一枚公平的硬币(看到正面的概率= 0.5)为例，我们掷 1000 次。定义随机变量<em class="nx"> X₁，…，X₁₀₀₀ </em>用</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj op"><img src="../Images/c93997ab0ddb01b5d7735dcf0258b060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*ivy89QYzW2wR-i2F4qdk2A.png"/></div></figure><p id="3cc8" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">那么人头的数量——让我们称之为<em class="nx">x</em>——正好是<em class="nx"> Xᵢ </em>的总和。我们已经知道，从那以后我们可以预期 500 次人头</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oq"><img src="../Images/5e35b1673e4341f8cbf9cde0905b01fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i2BuJlLtIGUeum_a5nh9vg.png"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">The expectation of the <em class="or">Xᵢ</em>’s is the probability of them being equal to 1, which is 0.5 in our case. Furthermore, the <a class="ae kz" href="https://en.wikipedia.org/wiki/Expected_value" rel="noopener ugc nofollow" target="_blank">expected value is linear</a>, justifying the first equation.</figcaption></figure><p id="9185" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">现在赫夫丁不等式把我们覆盖了:我们也知道<em class="nx"> X </em>大概率不会偏离<em class="nx">500<em class="nx">太多</em>，</em>见下图<em class="nx">。</em>我们一会儿就知道“多”和“概率大”是什么意思了。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj os"><img src="../Images/a9aa9cad140f4b494b5732cd51702eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qr213CUf2UIulR3_3_CKGg.png"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">You can see how concentrated the probability mass is around 500. The probability of seeing exactly 500 times heads is about 0.025.</figcaption></figure><h2 id="ed43" class="ny lb iu bd lc nz oa dn lg ob oc dp lk mb od oe lm mf of og lo mj oh oi lq oj bi translated">简化的定理</h2><blockquote class="nd"><p id="7174" class="ne nf iu bd ng nh ni nj nk nl nm mn dk translated">设 X₁，…，Xₙ是独立的伯努利随机变量，即它们中的每一个都取值 0 或 1。设 X = X₁ + … + Xₙ是他们的和。然后</p></blockquote><figure class="ou ov ow ox oy ko gi gj paragraph-image"><div class="gi gj ot"><img src="../Images/8d082b92ad58792707b0373d10e7c7c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*cUAT_EYdOQfC04hnckD2Ig.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Hoeffding’s Inequality for sums.</figcaption></figure><p id="8077" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated"><strong class="lu iv">注意:</strong>一个更一般的公式成立，其中<em class="nx"> Xᵢ </em>可以在任意实数<em class="nx"> a </em>和<em class="nx"> b </em>之间有界，甚至不必随机独立。但是我们使用这里给出的版本。</p><p id="6a3b" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">直觉上，这个不等式是有意义的:t<em class="nx">变得越大，也就是说，我们允许误差变得越大，落在区间或长度 2 <em class="nx"> t </em>内的概率就越大。</em></p><p id="7a36" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">但是真正有趣的事情是这样的:等式的右边只包括随机变量的数量<em class="nx"> n </em>和允许误差<em class="nx"> t. </em> <strong class="lu iv">最重要的事情，Xᵢ等于 1 的概率，在不等式的右边是找不到的。概率对我们来说可以是已知的，也可以是未知的，对所有人来说都是一样的，这并不重要。</strong>这就是为什么这种不平等如此多才多艺，如此适合研究。</p><p id="f960" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">具有这种性质的其他不等式是更容易的<a class="ae kz" href="https://en.wikipedia.org/wiki/Markov%27s_inequality" rel="noopener ugc nofollow" target="_blank">马尔科夫不等式</a>和<a class="ae kz" href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality" rel="noopener ugc nofollow" target="_blank">切比雪夫不等式</a>，但是它们给我们的右边的下界要差得多。平心而论，这两个不等式不需要我们的随机变量有界。然而，赫夫丁不等式利用了这样一个事实:Xᵢ在 0 和 1 之间有界。</p></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><p id="7f62" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">取该不等式，用<em class="nx"> nt、</em>代替<em class="nx"> t </em>，并将 P(…)内的不等式除以<em class="nx"> n </em>得到平均值的 Hoeffding 不等式:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oz"><img src="../Images/d4aaba5e51e2282d469b8dee642c8e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*ddQ661O4bB3Y0I64Spt8Ig.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Hoeffding’s Inequality for the mean.</figcaption></figure><p id="7391" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">在哪里</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pa"><img src="../Images/6d5bfaeb70d7dabd6a650ef2cc2e6a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*BWaUpzJ4vUG5uPS1uM6-2Q.png"/></div></figure><h1 id="0572" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">应用程序</h1><p id="74e6" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">现在让我们来看看不平等在起作用。首先，我们将谈论一个非常基本的硬币例子。然后，我们会去第二个非常重要的例子，估计股份，我们需要了解为什么赫夫丁树工作。</p><h2 id="1ff9" class="ny lb iu bd lc nz oa dn lg ob oc dp lk mb od oe lm mf of og lo mj oh oi lq oj bi translated">回到硬币的例子(总和)</h2><p id="dee0" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">假设我们要计算得到小于等于 450 或者大于等于 550 人头的概率。由于𝔼( <em class="nx"> X </em> ) = 500，我们可以设置<em class="nx"> t </em> =50，最后得到</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pb"><img src="../Images/4026874d309faeeb0e91a42b870cca98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pc3wQyeD6wD28VhOg4esBA.png"/></div></div></figure><p id="576b" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">这意味着偏离期望值小于 50 的概率非常高。头数极度集中在 500 左右。</p><h2 id="ff6e" class="ny lb iu bd lc nz oa dn lg ob oc dp lk mb od oe lm mf of og lo mj oh oi lq oj bi translated">估计份额(平均值)</h2><p id="1b77" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">假设有一大池<em class="nx"> N </em>个球，其中未知份额<em class="nx"> p </em>为白色，其余为黑色。一个自然的问题可能是:白球的份额<em class="nx"> p </em>有多大？如果池大小<em class="nx"> N </em>足够大(想想几十亿)，捡起每个球并检查它不是一个选项。</p><p id="9362" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">当然，自然要做的事情是均匀地抽取几个样本——比如说有替换的<em class="nx"> n — </em>球，然后检查它们。如果其中的<em class="nx"> w </em>是白色的，我们就可以断定<em class="nx"> p </em> ≈ <em class="nx"> w/n </em>。但是我们的子样本应该有多大呢？什么尺寸适合<em class="nx"> n </em>？10 可以吗？100?1000?</p><p id="7a6b" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">嗯，这取决于<strong class="lu iv">与<em class="nx"> p </em> </strong>的偏差有多大，以及<strong class="lu iv">在这个偏差范围内，你需要多大的信心</strong>。您可以设置两个参数:</p><ul class=""><li id="d35f" class="pc pd iu lu b lv my ly mz mb pe mf pf mj pg mn ph pi pj pk bi translated">误差的上界在<em class="nx"> p，</em>让我们再把它叫做<em class="nx"> t </em></li><li id="e665" class="pc pd iu lu b lv pl ly pm mb pn mf po mj pp mn ph pi pj pk bi translated">在以长度为 2 的<em class="nx"> p </em>为中心的区间内概率的一个下界姑且称之为 1- <em class="nx"> ɛ </em>对于小的<em class="nx"> ɛ &gt; 0 </em>。</li></ul><blockquote class="nd"><p id="5796" class="ne nf iu bd ng nh pq pr ps pt pu mn dk translated">这意味着 t 和ɛ是固定的，我们想找到 n 个例子，这样至少在概率为 1-ɛ的情况下，分数 w/n 与 p 的差异不超过 t</p></blockquote></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><p id="f843" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">我们现在要正式确定这件事。让我们定义<em class="nx"> n 个</em>随机变量<em class="nx"> X₁，…，Xₙ </em>为</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pv"><img src="../Images/90a1faa34e4a37cd012cb4dd300a6521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*KvY7-D4QDti5qpt6LrhBYA.png"/></div></figure><p id="982d" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">由于我们随机抽取有替换的球，<em class="nx"> Xᵢ </em>为 1 的概率正好是<em class="nx"> p </em>，未知份额。这也正是<em class="nx"> Xᵢ.的期望值</em></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pw"><img src="../Images/8bf11e075385e7db764048224324b810.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*-_hDueEXotrfHeqoFfBmXA.png"/></div></figure><p id="680b" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">让<em class="nx"> X̅=w/n </em>再次成为这些伯努利变量的平均值。然后我们还有</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj px"><img src="../Images/70fc91f8660b70221b0c4a871f6d6792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*471RTqjzk0ceRsMQkR9a4Q.png"/></div></div></figure><p id="2556" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">赫夫丁不等式给了我们</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj py"><img src="../Images/d9aa43cb04f282be676480bb976c0008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HpKh6Ht35Z9lybh3urNsHA.png"/></div></div></figure><p id="b134" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">现在我们希望右手边大于 1- <em class="nx"> ɛ，</em>，这给我们一个关于<em class="nx"> X̅=w/n </em>偏离<em class="nx"> p </em>不超过<em class="nx"> t </em>的下限，其概率至少为<em class="nx"> </em> 1- <em class="nx"> ɛ.</em>因此，我们必须解决这个不等式</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pz"><img src="../Images/7f5271b93fc556bad7a1b6112df84184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*rXYMZMFEyA0ib-PWS5BhZQ.png"/></div></figure><p id="cdf2" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">对于<em class="nx"> n </em>。使用一些基本运算，我们得到</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj qa"><img src="../Images/243f2e5717e247e3e20440eaf2632aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*9NCYg_n6Guki_Xj0TkLmZw.png"/></div></figure></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><p id="fbaa" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">如果我们容忍概率至少为 99%的<em class="nx"> p </em>的绝对误差至多为 1%，我们必须将<em class="nx"> t </em> =0.01，将<em class="nx"> ɛ= </em>设为 0.01，这样我们得到的所需子样本大小至少为</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj qb"><img src="../Images/680935ea97f9ed7de036e724913da2b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*lgLxYLJOCPy1C2dJTblFGQ.png"/></div></figure><p id="ad2c" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">让我们用 Python 来试试吧。我们考虑 30，000，000 个球的池，其中 10，000，000 个球是白色的(标记为 1)，20，000，000 个球是黑色的(标记为 0)。所以白球的真实份额是 0.3333……我们假装不知道。</p><pre class="kk kl km kn gu qc qd qe qf aw qg bi"><span id="6f4e" class="ny lb iu qd b gz qh qi l qj qk">import numpy as np</span><span id="aa61" class="ny lb iu qd b gz ql qi l qj qk">np.random.seed(0)</span><span id="d3b4" class="ny lb iu qd b gz ql qi l qj qk"># Create a pool with 30 million entries, 10 million being a one (white ball) and 20 million being a zero (black ball).<br/># This means that we have a share of 1/3=0.3333... white balls. <br/>pool = 200000000 * [0] + 100000000 * [1]</span><span id="fae8" class="ny lb iu qd b gz ql qi l qj qk"># Set Hoeffding parameters.<br/>t = 0.01<br/>eps = 0.01<br/>hoeffding_amount = int(np.ceil(np.log(2 / eps) / (2 * t ** 2)))</span><span id="cc75" class="ny lb iu qd b gz ql qi l qj qk">subsample = np.random.choice(pool, size=hoeffding_amount, replace=True)<br/>print(subsample.mean())<br/># Result: 0.32975992752529065</span></pre><p id="1a68" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">看起来不错！误差只有 0.0035 &lt; 0.01 = <em class="nx"> t </em>左右。但是这种情况发生的概率至少是 99%,所以我们为什么要庆祝呢？；)</p><blockquote class="qm qn qo"><p id="714f" class="ls lt nx lu b lv my jv lx ly mz jy ma qp na md me qq nb mh mi qr nc ml mm mn in bi translated">注意，不是采样、保存，然后取 26，492 个样本的平均值，我们可以仅仅采样，每当我们画出一个白球(1)时增加一个计数器，然后再次忘记该样本。通过这种方式，我们只需要跟踪计数器和我们查看的球的总数，这使得这成为一种<strong class="lu iv">非常高效的内存</strong>算法(在我们的子样本大小中为对数<em class="iu"> n </em>)。</p></blockquote></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><p id="276f" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">总的来说，我们可以说一个 26500 左右大小的子样本就足以高置信度地确定白球的份额。</p><h1 id="018a" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">回到机器学习</h1><p id="ba0a" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在最后一个例子中，我们已经看到了如何在一个巨大的黑白球池中计算白球的份额，而无需检查整个池，也不会有很大的误差。</p><p id="3f74" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">我们将使用这些知识来训练决策树，而不需要在内存中有大量可用的训练数据，这是我们首先要解决的问题。</p><p id="83fa" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">但是首先，我们必须重复一遍决策树是如何按照通常的方式构建的。</p><h2 id="f61f" class="ny lb iu bd lc nz oa dn lg ob oc dp lk mb od oe lm mf of og lo mj oh oi lq oj bi translated">以传统方式构建决策树</h2><p id="2d65" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在这里，我不会详细讨论决策树算法是如何工作的。在网上和媒体上有很多很棒的视频和文章。</p><p id="e7b8" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">但是，如果完整的训练数据集适合我们的内存，通常这是算法的第一步:</p><p id="659f" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">我们计算完整数据集的标签的杂质<em class="nx"> I₁ </em>的度量，例如用于分类任务的<a class="ae kz" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank">香农熵</a>，我们现在也将使用它。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qs"><img src="../Images/b642e132160fb498fb6c94d8fe5b8949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2gGIXkPwoUGwPGYlBxrT7w.png"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">The formula of the Shannon Entropy. p is the share of samples labeled 1. If p is 0 or 1, <strong class="bd qt">meaning that the samples all have the same label</strong>, the impurity is 0. The entropy function (=impurity) takes its maximum when exactly half of the samples are labeled 1 and the other half labeled 0, which corresponds to p=0.5. As you can see, the formula is symmetric, i.e. I(p)=I(1-p), which means that we can also consider the share of black balls instead of white balls.</figcaption></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj qu"><img src="../Images/15013021d14a7eb09ee312ed9727072c.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*K_NoYZ9QAE5ChBcfjNv4wA.png"/></div></figure><p id="5ace" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">然后，我们采用特征<em class="nx"> f </em>和切割点<em class="nx"> c. </em>使用这些，我们将整个数据集划分成两个不相交的子集:</p><ul class=""><li id="c16e" class="pc pd iu lu b lv my ly mz mb pe mf pf mj pg mn ph pi pj pk bi translated">其中特征<em class="nx"> f </em>的值小于(或等于)c<em class="nx"/>的所有样本</li><li id="cee6" class="pc pd iu lu b lv pl ly pm mb pn mf po mj pp mn ph pi pj pk bi translated">另一个包含所有样本，其中特征<em class="nx"> f </em>的值严格大于<em class="nx"> c. </em></li></ul><p id="6e6f" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">再次测量这两组的杂质，并合并成一个加权平均值，给出两组的单个杂质测量值<em class="nx"> I₂ </em>。则<em class="nx">信息增益</em>计算为<em class="nx"> G </em> = <em class="nx"> I₁ — I₂.</em></p><p id="e0a7" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">考虑以下一个拆分示例:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qv"><img src="../Images/789df6a46b0edf25777097a82106aade.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TvZswZEIR19qeWBQwXQS5Q.png"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">In this small example, we start with five samples with three features each. We start by measuring the impurity, namely <em class="or">I₁</em>. Then we choose Feature 3 and a cut point of 4 to split the node. Doing this, the original five samples are distributed in the two children nodes. We measure the impurities of both nodes and combine them into a single number <em class="or">I₂. The Information Gain is then computed.</em></figcaption></figure></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><p id="ad0e" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated"><em class="nx">对所有特征和所有可能的切割点进行上述操作，选择具有最大信息增益 G 的切割点作为树中的第一个分割点。我们现在对树中的所有叶子递归地重复这一过程，直到满足某个停止标准(节点的杂质小于阈值，树已经达到最大深度，节点中的样本太少等等)。</em></p><h2 id="a770" class="ny lb iu bd lc nz oa dn lg ob oc dp lk mb od oe lm mf of og lo mj oh oi lq oj bi translated">消除内存需求</h2><p id="62ee" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">现在，我们必须使用更少的样本来模拟行为。正如我们在普通决策树中看到的:</p><blockquote class="nd"><p id="8bf5" class="ne nf iu bd ng nh ni nj nk nl nm mn dk translated">只有标签为 1 的样品的份额 p 与计算杂质有关！</p></blockquote><p id="39aa" class="pw-post-body-paragraph ls lt iu lu b lv ok jv lx ly ol jy ma mb om md me mf on mh mi mj oo ml mm mn in bi translated">幸运的是，我们可以使用原始训练集的子样本来近似这个份额<em class="nx"> p </em>和<strong class="lu iv">因此杂质</strong>，正如在关于赫夫丁不等式的部分中所讨论的。在球示例的语言中，带有标签 1 的标签是白球，带有标签 0 的样本是黑球。</p><p id="d1ae" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">考虑到单次分裂，我们仅使用大约 26，500 个样本就可以近似得到<em class="nx"> I₁ </em>。然而，为了估计<em class="nx"> I₂ </em>，我们在每个子节点中需要<strong class="lu iv"> 26，500。如果幸运的话，样本被平均分配到两个节点上，那么 2*26，500=53，000 个样本就足够了。否则，我们可能需要更多，但只要我们需要的样本少于几百万，我们就比以前更好了。即使我们需要一百万个:因为我们可以将它们流式传输到树中并跟踪一些计数器，所以我们不会遇到内存问题。</strong></p><p id="1adb" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">这样我们可以安全地训练我们的模型，即使是在 Twitter 上的每条推文中。快乐大练兵！</p></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><p id="ef41" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">如果你想详细了解这是如何做到的，请阅读论文[1]。作者用伪代码描述了他们的算法以及所有必要的计数器，并且他们还给出通过流式传输数据来构建多少 Hoeffding 树的证明，并且相应的正常决策树将会偏离。</p><h1 id="40f1" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">结论</h1><p id="be79" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我们已经看到，在极大的训练集上训练决策树是不可行的。克服这个问题需要对正确数量的训练样本进行子采样，并用不完整但相当准确的信息构建树。这种二次抽样是合理的赫夫丁不等式，给这些特殊的决策树也有他们的名字:赫夫丁树。</p><p id="b94d" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">此外，我们<strong class="lu iv">甚至不必存储子样本</strong>，我们只需在扫描训练数据时跟踪我们已经看到了多少标签为 1(白色球)的样本，这是一种简单有效的方法，可以进一步降低存储复杂度。</p></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><p id="651f" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">在本文中，我们只看到了用于分类任务的香草胡夫丁树。还存在用于回归的算法，甚至是对抗所谓的<em class="nx">概念转移</em>的方法，即当训练分布随时间变化时<em class="nx">。</em></p><p id="bc34" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">幸运的是，这些算法都是由 Python 的<a class="ae kz" href="https://scikit-multiflow.github.io/scikit-multiflow/index.html" rel="noopener ugc nofollow" target="_blank"> scikit-multiflow </a>的开发者实现的！让我们做一个快速测试。</p><h1 id="729b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">四处玩耍</h1><p id="3568" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">首先，做一个禁食</p><pre class="kk kl km kn gu qc qd qe qf aw qg bi"><span id="0997" class="ny lb iu qd b gz qh qi l qj qk">pip install scikit-multiflow</span></pre><p id="173d" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">然后，让我们比较在完整的训练集上拟合 Hoeffding 树(=足够的可用 RAM)与逐个传递每个样本。更详细的测试也可以在[1]中找到。</p><pre class="kk kl km kn gu qc qd qe qf aw qg bi"><span id="52ea" class="ny lb iu qd b gz qh qi l qj qk">from sklearn.datasets import make_classification<br/>from sklearn.model_selection import train_test_split<br/>from skmultiflow.trees import HoeffdingTree<br/>import matplotlib.pyplot as plt</span><span id="c7d4" class="ny lb iu qd b gz ql qi l qj qk">res = []</span><span id="37de" class="ny lb iu qd b gz ql qi l qj qk"># Create a dataset.<br/>X, y = make_classification(10000, random_state=123)<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)</span><span id="5b6e" class="ny lb iu qd b gz ql qi l qj qk"># Define a tree for fitting the complete dataset and one for streaming.<br/>ht_complete = HoeffdingTree()<br/>ht_partial = HoeffdingTree()</span><span id="110c" class="ny lb iu qd b gz ql qi l qj qk"># Fit the complete dataset.<br/>ht_complete.fit(X_train, y_train)<br/>ht_complete_score = ht_complete.score(X_test, y_test)<br/>print(f'Score when fitting at once: {ht_complete_score}')</span><span id="bcd8" class="ny lb iu qd b gz ql qi l qj qk"># Streaming samples one after another.<br/>timer = False<br/>j = 0<br/>for i in range(len(X_train)):<br/>    ht_partial.partial_fit(X_train[i].reshape(1, -1), np.array([y_train[i]]))<br/>    res.append(ht_partial.score(X_test, y_test))<br/>    print(f'Score when streaming after {i} samples: {res[-1]}')<br/>    if res[-1] &gt;= ht_complete_score - 0.01:<br/>        print(f'(Almost) full score reached! Continue for another {20 - j} samples.')<br/>        timer = True<br/>    if timer:<br/>        j += 1<br/>        if j == 20:<br/>            break</span><span id="6f55" class="ny lb iu qd b gz ql qi l qj qk"># Plot the scores after each sample.<br/>plt.figure(figsize=(12, 6))<br/>plt.plot([0, i], [ht_complete_score, ht_complete_score], '--', label='Hoeffding Tree built at once')<br/>plt.plot(res, label='Incrementally built Hoeffding Tree')<br/>plt.xlabel('Number of Samples', fontsize=15)<br/>plt.ylabel('Accuracy', fontsize=15)<br/>plt.title('Fitting a Hoeffding Tree at once (enough Memory available) vs fitting it via Streaming', fontsize=20)<br/>plt.legend()</span></pre><p id="097a" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">生成的图形可能如下所示:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qw"><img src="../Images/227078fb7ce5814b7811ebc3e9e3bcc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t59dxwe1XueNSsuphy6OqQ.png"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">We can see that building a Hoeffding Tree H directly yields an accuracy of about 91% (on a test set). If we build another Hoeffding Tree by feeding in each sample one after another, we can see that the performance approaches the performance of H. After about 50 samples, our streaming Hoeffding Tree has an accuracy of about 88% already. If we pass in more samples, 91% is reached at some point (for me it was after about 4000 samples out of the 8000 training samples).</figcaption></figure><h1 id="0477" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">参考</h1><p id="0d9d" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">[1] P. Domingos 和 G. Hulten，<a class="ae kz" href="https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf" rel="noopener ugc nofollow" target="_blank">挖掘高速数据流</a> (2000)，第六届 ACM SIGKDD 知识发现和数据挖掘国际会议论文集</p><p id="1b1d" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">[2] W. Hoeffding，<a class="ae kz" href="http://repository.lib.ncsu.edu/bitstream/1840.4/2170/1/ISMS_1962_326.pdf" rel="noopener ugc nofollow" target="_blank">有界随机变量和的概率不等式</a> (1962)，美国统计协会杂志。58 (301)</p></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><p id="a91c" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="5551" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated"><strong class="lu iv">作为最后一点，如果你</strong></p><ol class=""><li id="b61f" class="pc pd iu lu b lv my ly mz mb pe mf pf mj pg mn qx pi pj pk bi translated"><strong class="lu iv">想支持我多写点机器学习和</strong></li><li id="49b7" class="pc pd iu lu b lv pl ly pm mb pn mf po mj pp mn qx pi pj pk bi translated"><strong class="lu iv">无论如何，计划获得一个中等订阅，</strong></li></ol><p id="1faf" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated"><strong class="lu iv">为什么不通过此链接</strong><a class="ae kz" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"><strong class="lu iv"/></a><strong class="lu iv">？这将对我帮助很大！😊</strong></p><p id="c85e" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated"><em class="nx">说白了，给你的价格不变，但大约一半的订阅费直接归我。</em></p><p id="cf46" class="pw-post-body-paragraph ls lt iu lu b lv my jv lx ly mz jy ma mb na md me mf nb mh mi mj nc ml mm mn in bi translated">非常感谢，如果你考虑支持我的话！</p><blockquote class="nd"><p id="d255" class="ne nf iu bd ng nh ni nj nk nl nm mn dk translated"><em class="or">有问题就在</em><a class="ae kz" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"><em class="or">LinkedIn</em></a><em class="or">上写我！</em></p></blockquote></div></div>    
</body>
</html>