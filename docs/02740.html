<html>
<head>
<title>Reinforcement Learning — Implement Grid World</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——实现网格世界</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff?source=collection_archive---------4-----------------------#2019-05-04">https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff?source=collection_archive---------4-----------------------#2019-05-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ec77" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">引入值迭代</h2></div><p id="8b37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当你试图着手强化学习时，很可能<strong class="kh ir">网格世界游戏</strong>是你遇到的第一个问题。这是强化学习中最基本也是最经典的问题，我相信，通过自己实现它，是理解强化学习基础的最好方法。与此同时，实现你自己的游戏，看看一个机器人如何自己学习也是非常有趣的！</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi li"><img src="../Images/1f965d1c877a20bbf8b619726a12e247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9gFX8Yf2H36Qebj2.jpg"/></div></div></figure><h1 id="4283" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">规则</h1><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mm"><img src="../Images/fc23a3fb53c600e6060f2d5d0e3d1bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3oszDubmqzfuu2vRnn3yw.png"/></div></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Grid Board</figcaption></figure><p id="3c9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">规则很简单。你的代理人/机器人从左下角(“开始”符号)开始，并以相应的奖励+1 或-1 结束。在每一步，代理人有 4 个可能的动作，包括上、下、左、右，而黑色的方块是一堵墙，代理人无法穿过。为了使它更直接，我们的第一个实现<strong class="kh ir">假设每个动作都是确定性的</strong>，也就是说，代理将去它打算去的地方。例如，当代理决定在(2，0)采取行动时，它将在(1，0)而不是(2，1)或其他地方着陆。(我们将在我们的第二个实现中增加不确定性)然而，如果代理碰到墙，它将保持在相同的位置。</p><h1 id="0f01" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">板</h1><p id="1148" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">所以让我们开始破解代码吧！首先，让我们建立一些董事会的全局规则。(<a class="ae mw" href="https://github.com/MJeremy2017/RL/blob/master/GridWorld/gridWorld.py" rel="noopener ugc nofollow" target="_blank">全码</a>)</p><figure class="lj lk ll lm gt ln"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="44f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而作为一个网格游戏，它需要一个状态来为我们代理人的每个状态(位置)正名，根据其状态给予奖励。</p><figure class="lj lk ll lm gt ln"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="453f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们的代理人采取行动时，国家应该有一个功能来接受行动并返回下一个国家的法律地位。</p><figure class="lj lk ll lm gt ln"><div class="bz fp l di"><div class="mx my l"/></div></figure><h1 id="24a8" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">代理人</h1><p id="fdaf" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">这是人工智能部分，因为我们的代理应该能够从过程中学习，并像人类一样思考。魔术的关键是价值迭代。</p><h2 id="93e3" class="mz lv iq bd lw na nb dn ma nc nd dp me ko ne nf mg ks ng nh mi kw ni nj mk nk bi translated"><strong class="ak">值迭代</strong></h2><p id="e0e9" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们的代理最终将学习的是一个策略，而策略是从状态到动作的映射，简单地指示代理在每个状态下应该做什么。在我们的例子中，不是学习从状态到行动的映射，我们将利用价值迭代来首先学习状态到价值的映射(这是估计的回报)，并且基于该估计，在每个状态，我们的代理将选择给出最高估计回报的最佳行动。</p><p id="2358" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不会有任何古怪、令人挠头的数学问题，因为价值迭代的核心非常简洁。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/540870a3ae0b81e836eb4e8c3ac0d5cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*DZcvRVaNyQ34pAhk4wwUDQ.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">value iteration</figcaption></figure><p id="b524" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是价值迭代的精髓，超级利落吧？这个公式几乎适用于所有的强化学习问题，让我解释一下我们的智能体是如何基于这个公式从一个婴儿进化成专家的。价值迭代，顾名思义，在每次迭代(游戏结束)时更新其价值(预估奖励)。</p><p id="2ef7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">起初，我们的代理对网格世界(环境)一无所知，所以<strong class="kh ir">它会简单地将所有奖励初始化为 0。然后，它开始通过随意走动来探索世界，当然它一开始会经历很多失败，但这完全没关系。一旦它到达游戏结束，奖励+1 或奖励-1，整个游戏重置，并且<strong class="kh ir">奖励以向后的方式传播，最终沿途所有状态的估计值将基于上面的公式更新。</strong></strong></p><p id="fa82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们仔细看看这个公式。<strong class="kh ir">左边的<em class="nm"> V(St) </em>是该状态的更新值，右边的是当前未更新值，</strong> α <strong class="kh ir">是学习率。</strong>该公式简单地说，状态的更新值等于当前值加上一个时间差，这是代理从玩游戏的迭代中学习到的减去先前的估计。例如，假设有两个状态，<code class="fe nn no np nq b">S1</code>和<code class="fe nn no np nq b">S2</code>，它们都有一个估计值 0，在这一轮游戏中，我们的代理从<code class="fe nn no np nq b">S1</code>移动到<code class="fe nn no np nq b">S2</code>并获得奖励 1，然后是<code class="fe nn no np nq b">S1 = S1 + α(S2 - S1)</code>的新估计值，即<code class="fe nn no np nq b">0 + 0.1(1-0) = 0</code>(假设α为 0.1，在<code class="fe nn no np nq b">S1</code>的奖励为 0)</p><figure class="lj lk ll lm gt ln"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="0640" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们记录代理的所有状态，在游戏结束时，我们将以<code class="fe nn no np nq b">reversed</code>的方式更新评估。</p><h2 id="02e7" class="mz lv iq bd lw na nb dn ma nc nd dp me ko ne nf mg ks ng nh mi kw ni nj mk nk bi translated"><strong class="ak">勘探&amp;开采</strong></h2><p id="864e" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们还有最后一件事要谈。一旦我们的代理发现了一条获得+1 奖励的路径，它应该坚持下去并永远沿着这条路径走下去(剥削)还是应该给其他路径一个机会(探索)并期待一条更短的路径？实际上，我们将平衡探索和开发，以避免我们的代理陷入局部最优。在这里，我们的代理将基于某些<code class="fe nn no np nq b">exploration_rate</code>选择操作</p><figure class="lj lk ll lm gt ln"><div class="bz fp l di"><div class="mx my l"/></div></figure><h2 id="b8a0" class="mz lv iq bd lw na nb dn ma nc nd dp me ko ne nf mg ks ng nh mi kw ni nj mk nk bi translated">玩</h2><p id="2829" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">就是这样！这些都是我们玩网格世界游戏所需要的。我们可以开始，让我们的代理人玩游戏！</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1e89681394a24f2e4e0d359c62da25f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*OvEIlYIkC-V5xZ9BaoeQFA.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">50 round of playing</figcaption></figure><p id="86f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是每个州打完 50 轮游戏后的预估。由于我们的行动是确定性的，我们可以通过遵循最高的估计在每个状态下获得最佳行动！完整的代码是<a class="ae mw" href="https://github.com/MJeremy2017/RL/blob/master/GridWorld/gridWorld.py" rel="noopener ugc nofollow" target="_blank">这里</a>，去玩吧，如果你发现有什么需要更新的，欢迎投稿！</p><p id="fad1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目前，我们都在关注价值迭代和确定性游戏世界。然而，在实际情况中，代理并不总是到达它希望到达的位置。让我们更深入地探索<a class="ae mw" href="https://medium.com/@zhangyue9306/implement-grid-world-with-q-learning-51151747b455" rel="noopener">非确定性游戏和 Q-learning </a>。</p><p id="1eac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献</strong></p><p id="603c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1]<a class="ae mw" href="https://www.cs.swarthmore.edu/~bryce/cs63/s16/slides/3-21_value_iteration.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . Swarthmore . edu/~ bryce/cs63/s16/slides/3-21 _ value _ iteration . pdf</a></p><p id="669e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]<a class="ae mw" href="https://github.com/JaeDukSeo/reinforcement-learning-an-introduction" rel="noopener ugc nofollow" target="_blank">https://github . com/JaeDukSeo/reinforcement-learning-an-introduction</a></p></div></div>    
</body>
</html>