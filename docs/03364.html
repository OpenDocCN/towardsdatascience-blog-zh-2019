<html>
<head>
<title>How To Be Confident In Your Neural Network Confidence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何对自己的神经网络自信</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-be-confident-in-your-neural-network-confidence-10d10dcf8003?source=collection_archive---------17-----------------------#2019-05-29">https://towardsdatascience.com/how-to-be-confident-in-your-neural-network-confidence-10d10dcf8003?source=collection_archive---------17-----------------------#2019-05-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="23e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些笔记基于<a class="ae kl" href="https://arxiv.org/abs/1706.04599" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">关于现代神经网络</strong>校准的研究论文(郭等，2017。)</a>。</p><p id="fa8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 CIFAR100 等计算机视觉数据集上，ResNet 等非常大而深的模型远比 LeNet 等较老的模型准确。<strong class="jp ir">然而，尽管他们更擅长对图像进行分类，我们对他们自己的信心却不太有信心！</strong></p><p id="a348" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大多数用于分类的神经网络使用 softmax 作为最后一次激活:它产生每个目标(猫、狗、船等)的概率分布。).这些概率总计为 1。我们可以预期，如果对于给定的图像，我们的模型将 0.8 的分数与目标“船”相关联，我们的模型有 80%的把握这是正确的目标。</p><p id="ff31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">超过 100 个图像被检测为船，我们可以预期大约 80 个图像确实是真实的船，而剩余的 20 个是假阳性。</p><p id="6fcf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 LeNet 这样的浅层模型来说是正确的，但是随着新模型精度的提高<strong class="jp ir">，它们的置信度变得与“真实置信度”不相关</strong>。</p><p id="d38c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这对深度神经网络不再有效:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/a2e6917d088d770712af75ce7c41ae13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fhvkdS0OebldzKps2soGxA.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Figure 1: <em class="lc">Miscalibration in modern neural network [</em><a class="ae kl" href="https://arxiv.org/abs/1706.04599" rel="noopener ugc nofollow" target="_blank"><em class="lc">source</em></a><em class="lc">]</em></figcaption></figure><p id="6274" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如您所见，LeNet 等较老的网络准确度较低(55%)，但它们的可信度实际上与准确度相当！ResNet 等现代网络具有更高的准确性(69%)，但如图 1 所示，它们过于自信。</p><p id="df87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型置信度和实际准确度之间的差异被称为<strong class="jp ir">校准错误</strong>。</p><h1 id="a811" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">为什么它很重要</h1><p id="efb8" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">除了学院中使用的玩具数据集，了解我们的模型有多少可信度也是有用的。</p><p id="53a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想象一下，我们有一个预测欺诈的模型。我们希望根据模型置信度将某个交易标记为可疑，因为它是一个欺诈。我们可以明确地计算出验证集的最佳阈值，然后超过该阈值的所有置信度都将被标记为欺诈。然而，这个计算的阈值可以是 0.2 或 0.9，但是对人类来说可能更有意义。</p><p id="7e8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">没有错误校准的模型将有助于用户更好地解释预测。</p><h1 id="15d3" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">为什么会这样</h1><p id="2a0e" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">作者从经验上探索了现代网络中这种失调的原因。</p><p id="1652" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">他们用<strong class="jp ir">预期校准误差</strong> (ECE)来测量误校准:置信度和准确度之间的平均差值。这个指标应该最小化。</p><h1 id="3b1b" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">更高的容量和交叉熵</h1><p id="6a2d" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">错误校准的最可解释的原因是容量的增加和交叉熵损失。</p><p id="4c7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型容量可以被看作是一个模型可以记忆多少的度量。由于容量无限，该模型可以简单地记住整个训练数据集。必须在低容量和高容量之间进行权衡。如果太低，模型将无法学习数据的基本特征。如果它太高，模型将学习太多和过度拟合，而不是一般化。实际上，理解就是压缩:通过留下足够少的容量，模型必须挑选出最有代表性的特征(非常类似于 PCA 的工作方式)，然后将更好地概括(但是容量太少&amp;将不会发生学习！).</p><p id="36e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ResNet 之类的新体系结构比老的 LeNet 具有更大的容量(前者 25M 参数，后者 20k 参数)。这种高容量导致了更好的准确性:训练集几乎可以被记住。</p><p id="5449" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，模型优化了交叉熵损失，这迫使他们正确并且非常自信。更高的容量有助于降低交叉熵损失，从而鼓励深度神经网络过于自信。正如您在图 1 中看到的，新模型现在过于自信了。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mg"><img src="../Images/4d41446cd186b16deeb6e2547e0d0afd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CqCjSOcOXXOftlVY_bcZqg.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk"><em class="lc">Figure 2: More capacity (in depth or width) increases the miscalibration [</em><a class="ae kl" href="https://arxiv.org/abs/1706.04599" rel="noopener ugc nofollow" target="_blank"><em class="lc">source</em></a><em class="lc">]</em></figcaption></figure><h1 id="04a3" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">神秘的批量标准化</h1><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mh"><img src="../Images/d24e3afa48ff9f573df8612a29db3696.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*vf2vyUpz2AAByaHHHBRRpA.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Figure 3: <em class="lc">Batch Normalization increases the miscalibration. [</em><a class="ae kl" href="https://arxiv.org/abs/1706.04599" rel="noopener ugc nofollow" target="_blank"><em class="lc">source</em></a><em class="lc">]</em></figcaption></figure><p id="0d14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">批量归一化</strong>归一化网络中的张量。这大大提高了训练收敛&amp;的最终表现。为什么它能如此有效还不太清楚(<a class="ae kl" href="https://arthurdouillard.com/posts/normalization" rel="noopener ugc nofollow" target="_blank">见更多</a>)。</p><p id="908c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者凭经验评论说，使用批量标准化增加了错误校准，但找不到确切的原因。</p><p id="c267" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种方法在训练中的帮助会促进过度自信吗？</p><h1 id="f334" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">正规化</h1><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/d59e2e9e2671a2333adb91d75847dbdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*ozRPfA0vFuDWCmz0nqDZxw.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Figure 4: <em class="lc">More regularization decreases the miscalibration. [</em><a class="ae kl" href="https://arxiv.org/abs/1706.04599" rel="noopener ugc nofollow" target="_blank"><em class="lc">source</em></a><em class="lc">]</em></figcaption></figure><p id="e0c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">重量衰减</strong>是一种额外的损失，它损害了重量的 L2 标准。权重越大，标准越大，损失也就越大。通过约束权重的大小，它避免了模型寻找可能使其过度拟合的极端权重值。</p><p id="d2ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者发现，正如预期的那样，增加正则化会降低模型精度。</p><p id="b3b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，它也减少了误校准！答案还是因为正则化避免了过度拟合&amp;从而避免了过度自信。</p><h1 id="37b5" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">如何修复校准错误</h1><p id="823b" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">这篇文章的标题，“如何对你的神经网络自信”，让你相信你会发现如何减少错误校准。</p><p id="1f37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您不会减少容量、取消批处理规范化并增加正则化:您会过多地损害您宝贵的准确性。</p><p id="9aa4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">幸运的是，有后处理解决方案。作者描述了几种，但最有效也是最简单的一种:<strong class="jp ir">温度标度</strong>。</p><p id="3149" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不是像这样计算 softmax:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mk"><img src="../Images/c6de61febcebc7c75c687b23cd97443b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOCoyILq7QvRHKIULSsGGQ.png"/></div></div></figure><p id="5f37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所有 logits(最终激活前的值，此处为 softmax)除以相同的温度值:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ml"><img src="../Images/71828a0aaa847c4308ba126143e33697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f9xj-mEI9ApU9I8UPaG6IA.png"/></div></div></figure><p id="876a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似(<a class="ae kl" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank"> Hinton 等人，2015。</a>)，这个温度<em class="mj">软化了概率</em>。</p><p id="5f5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">极端概率(高置信度)比较小概率(低置信度)减少得更多。作者通过最小化验证集上的预期校准误差来找到最佳温度。</p><p id="f1a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">校准错误几乎完全得到纠正:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mm"><img src="../Images/df70cd424b10dfae35771da0e6038295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RyhxpIdY3UjT34vfwmrIJA.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk"><em class="lc">Figure 5: Temperature Scaling fixes the miscalibration. [</em><a class="ae kl" href="https://arxiv.org/abs/1706.04599" rel="noopener ugc nofollow" target="_blank"><em class="lc">source</em></a><em class="lc">]</em></figcaption></figure><p id="31e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">温度缩放的另一个很酷的特性:因为所有的 logits 都除以相同的值，并且 softmax 是一个<a class="ae kl" href="https://en.wikipedia.org/wiki/Monotonic_function" rel="noopener ugc nofollow" target="_blank">单调函数</a>，精度保持不变！</p></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><p id="edc7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mj">原载于 2019 年 5 月 29 日</em><a class="ae kl" href="https://arthurdouillard.com/post/miscalibration/" rel="noopener ugc nofollow" target="_blank"><em class="mj">https://arthurdouillard.com</em></a><em class="mj">。</em></p></div></div>    
</body>
</html>