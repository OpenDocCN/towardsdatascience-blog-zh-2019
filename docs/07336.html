<html>
<head>
<title>Linear Regression with Gradient Descent from Scratch in Numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Numpy 中从零开始的梯度下降线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-with-gradient-descent-from-scratch-in-numpy-d894a800a2ca?source=collection_archive---------15-----------------------#2019-10-15">https://towardsdatascience.com/linear-regression-with-gradient-descent-from-scratch-in-numpy-d894a800a2ca?source=collection_archive---------15-----------------------#2019-10-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d1e2000c3a0e530de6bec12a64355a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dFtR1Ky64NftNWjLYBj_6w.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Image by <a class="ae jg" href="https://pixabay.com/users/EliasSch-3372715/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2127669" rel="noopener ugc nofollow" target="_blank">Elias Sch.</a> from <a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2127669" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><div class=""/><p id="e861" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">几天前，我写了一篇介绍梯度下降的文章，介绍了一些基本的数学和逻辑知识，在文章的最后，我要求你尝试用一个简单的线性回归来实现它。</p><div class="is it gp gr iu le"><a rel="noopener follow" target="_blank" href="/gradient-descent-demystified-in-5-minutes-f02966704e35"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd jk gy z fp lj fr fs lk fu fw ji bi translated">梯度下降在 5 分钟内揭开神秘面纱</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">学习梯度下降是如何工作的，只需要一点点数学知识和很多常识。</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">towardsdatascience.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls ja le"/></div></div></a></div><p id="9ed0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我承认——这个要求有点过分，尤其是如果那篇文章是你第一次接触梯度下降的话。这就是为什么今天我想自己从头开始实现<strong class="ki jk"/>，先借助一些数学，再借助 Python。读完这篇文章后，你会完全理解梯度下降，并能够用它来解决任何线性回归问题。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="db92" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">手动梯度下降</h1><p id="f846" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">我强烈建议你阅读上面链接的文章。它将为这个主题奠定基础，另外一些数学知识已经在这里讨论过了。</p><p id="fcea" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我将定义我的数据集—只有三个呈线性关系的点。我选择这么少的点只是因为<strong class="ki jk">数学将会更短</strong>——不用说，对于更长的数据集，数学不会更复杂，它只会更长，我不想犯一些愚蠢的算术错误。</p><p id="33d3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我将系数<em class="nd">β0</em>和<em class="nd">β1</em>设置为某个常数，并将成本函数定义为<em class="nd">残差平方和(SSR/SSE) </em>。最后，我将把<strong class="ki jk">学习率</strong>设置为一个小值，比如 0.001:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/04ec02ce3a0ac46256682a534a764073.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*Pf02igLQ2KTLizhVyVj1UA.png"/></div></figure><p id="c3fc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过使用<strong class="ki jk">线方程</strong>，很容易计算模型的预测值:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/92fc042dcbe2feb6ad6e02f7d7426873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*sW3YMPBSabMK32tdS0bLNw.png"/></div></figure><p id="98a7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如前所述，成本函数将是<em class="nd">残差平方和</em>:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b421378f24649948743b2d769fca883c.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*zdeYs_hx6cydo1fU2y-XkA.png"/></div></figure><p id="b381" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你懂一些微积分，你可以计算出成本函数相对于<em class="nd"> beta 0 </em>和<em class="nd"> beta 1 </em>的<strong class="ki jk">偏导数</strong>。如果没有，就拿着这些方程，试着在网上搜索“多元微分”和“链式法则”:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/3ad452a77f153a49de4e137c185ee5d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ZeFNhurnifFwUnHU8HmDkQ.png"/></div></figure><p id="2da3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来要做的是计算β0 和β1 的成本。这归结为基本的算术，很容易手工完成。</p><p id="4540" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">beta 0 的成本:</strong></p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/d78fb0f56092fa549edffc8af3d41442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L6UEllJwhG6MUDWmBv5bCg.png"/></div></div></figure><p id="0063" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">beta 1 的成本:</strong></p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/38dfa638941e7077fba50331e019061a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NgjmyZmm_UE_2nvZ-huKXg.png"/></div></div></figure><p id="a3c8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦获得这两个数字，就可以通过将计算出的成本乘以学习率来计算这两个系数的步长:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/6d99c6bdcbd1ad8dcb06bb527090c878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*EwySTP5oxXI_P4qU5BfunQ.png"/></div></figure><p id="7d01" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后一步是通过从旧值中减去各自的步长来计算新的<em class="nd">β0</em>和<em class="nd">β1</em>:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/28087324745317eacd9b7d77ccf30c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GfodMotQ8T6n4FW_RLWEHg.png"/></div></div></figure><p id="7f07" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在你可以使用这些新的系数值，重复整个过程大约 10000 次。出于明显的原因<strong class="ki jk">，你不应该手动操作</strong>——然而，知道算法是如何工作的是很好的。</p><p id="e1a2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">差不多就是这样，现在你可以简单地重复同样的逻辑，不断更新你的系数。让我们看看如何在 Python 中实现梯度下降。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="4ad9" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">Python 中的梯度下降</h1><p id="9ee2" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">从实现开始，让我们首先定义成本函数，并使用<a class="ae jg" href="https://www.sympy.org/" rel="noopener ugc nofollow" target="_blank"><strong class="ki jk"><em class="nd">Sympy</em></strong></a><strong class="ki jk"><em class="nd"/></strong>获取导数:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3adba8e56858bb9aa9bb97cd32484e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*A00wlTrGlNkpqH6iR0yJ7w.png"/></div></figure><p id="d5cf" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不知道怎么用 Sympy？查看这篇文章:</p><div class="is it gp gr iu le"><a rel="noopener follow" target="_blank" href="/taking-derivatives-in-python-d6229ba72c64"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd jk gy z fp lj fr fs lk fu fw ji bi translated">在 Python 中取导数</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">学习如何处理机器学习中的微积分部分</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">towardsdatascience.com</p></div></div><div class="ln l"><div class="nr l lp lq lr ln ls ja le"/></div></div></a></div><p id="aea5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以看到计算出的导数与之前手动计算出的是一样的——Sympy 只做了乘法来去掉括号(<em class="nd"> ish </em>)。</p><p id="3b50" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在您可以将<strong class="ki jk"> <em class="nd"> x </em> </strong>和<strong class="ki jk"> <em class="nd"> y </em> </strong>都定义为变量并绘制它们:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/ce6cab733366437884e43867d2aab124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_9auEYmeUrfPorO5dVhM8g.png"/></div></figure><p id="2d2d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你拥有了所需的一切——这意味着接下来是梯度下降。因为我已经在数学部分解释了发生了什么，所以我不会在这里做太多的细节，我将只概述这个过程:</p><ol class=""><li id="3da3" class="nt nu jj ki b kj kk kn ko kr nv kv nw kz nx ld ny nz oa ob bi translated">将<em class="nd">β0</em>和<em class="nd">β1</em>系数初始化为某个值</li><li id="3e1c" class="nt nu jj ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">初始化学习率和所需的周期数</li><li id="9660" class="nt nu jj ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">制作一个循环的<em class="nd">，它将运行 n 次，其中 n 是历元数</em></li><li id="18f2" class="nt nu jj ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">初始化保存当前时期误差的变量</li><li id="f27e" class="nt nu jj ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">使用直线方程进行预测</li><li id="b7f2" class="nt nu jj ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">将平方差追加到误差数组</li><li id="b36d" class="nt nu jj ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">计算数据集中当前行的两个系数的偏导数</li><li id="36cb" class="nt nu jj ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">增加系数的成本</li><li id="bce3" class="nt nu jj ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">重新计算系数值</li></ol><p id="5143" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这听起来像是很多步骤，确实如此，但是如果你已经阅读了我以前的文章，并遵循了数学，你可以看到这里没有发生任何复杂的事情。下面是 Python 实现的代码:</p><figure class="nf ng nh ni gt iv"><div class="bz fp l di"><div class="oh oi l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Gradient Descent — <a class="ae jg" href="https://gist.github.com/dradecic/cb1a3b0a68f8b8e0307dba754de08113" rel="noopener ugc nofollow" target="_blank">https://gist.github.com/dradecic/cb1a3b0a68f8b8e0307dba754de08113</a></figcaption></figure><p id="71fb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦代码单元执行完毕，你就可以检查你的系数的最终值，并用它们来做预测:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/f60290416f01bb8e711e8455bd8e5422.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*xhCFts_jxRTy2k6vUa_seg.png"/></div></figure><p id="2e13" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在使用<strong class="ki jk"> <em class="nd"> y_preds </em> </strong>你可以添加一条回归线到先前绘制的图中:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/cde2a0635beb61f6f6a09724b3cca9c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6jpbbtNhFmByhOSMP3Ouw.png"/></div></div></figure><p id="4449" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还记得你是如何记录纪元误差的吗？</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/7d472ec16ac0f0d62d1f2de2ea26d42f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EFzjGQ0j3myBwpttjAk5xA.png"/></div></div></figure><p id="49ce" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看起来 10000 个时期有点太多了，但是尽管如此，在大约 1000 个时期之后，误差被最小化，并且在剩余的运行中保持不变。</p><p id="4446" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在梯度下降的代码单元中添加了一串打印语句，所以每次迭代都被打印出来(<em class="nd">我只运行了 1 个时期来验证数学工作</em>)，下面是我得到的结果:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/7b89ba50e07331969c8a67c30d9d722e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*zuu2um37KwfRR6_cBhbduQ.jpeg"/></div></figure><p id="a5da" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这只是一次迭代，但它是数学如前所述起作用的决定性证据。你现在可以在更大的数据集上使用相同的逻辑，只是为了好玩。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="fe15" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">最后的话</h1><p id="ddbb" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">梯度下降一开始可能看起来像一个令人生畏的算法，但它背后的逻辑相当简单，数学也没有你以前可能认为的那么复杂。</p><p id="a490" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你现在能从理论、数学和代码上看到清晰的画面。我如何实现梯度下降只是一个想法，肯定还有改进的空间-您可以将系数值保存在列表中-如果您的数据集中有多个要素，这将非常方便。现在，您已经拥有了自己进一步探索所需的所有工具。</p><p id="1771" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请随意分享您的想法，如果有任何不清楚的地方，请随时联系我。T3】</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="dce6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nd">喜欢这篇文章吗？成为</em> <a class="ae jg" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="nd">中等会员</em> </a> <em class="nd">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="is it gp gr iu le"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd jk gy z fp lj fr fs lk fu fw ji bi translated">通过我的推荐链接加入 Medium-Dario rade ci</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">medium.com</p></div></div><div class="ln l"><div class="on l lp lq lr ln ls ja le"/></div></div></a></div></div></div>    
</body>
</html>