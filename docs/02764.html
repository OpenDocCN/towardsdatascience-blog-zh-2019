<html>
<head>
<title>Visualizing Loss Landscape of Deep Neural Networks…..but can we Trust them?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可视化深度神经网络的损失情况…..但是我们能相信他们吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualizing-loss-landscape-of-deep-neural-networks-but-can-we-trust-them-3d3ae0cff46e?source=collection_archive---------11-----------------------#2019-05-05">https://towardsdatascience.com/visualizing-loss-landscape-of-deep-neural-networks-but-can-we-trust-them-3d3ae0cff46e?source=collection_archive---------11-----------------------#2019-05-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a4b9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak"> <em class="kf">我们能相信深度神经网络的损失景观可视化吗？</em>T3】</strong></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi kg"><img src="../Images/8edbcdce9c49678516fb6be13f0c30cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*T61E1R-G0BATUuphZ-bttQ.png"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk">Landscape from this <a class="ae ks" href="https://pixabay.com/illustrations/evening-sun-sunset-backlighting-55067/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="c731" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">简介</strong></p><p id="db25" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">最近<a class="ae ks" href="https://arxiv.org/abs/1712.09913" rel="noopener ugc nofollow" target="_blank">开发了一种方法</a>来可视化深度神经网络的损失情况。我个人认为这是一个巨大的突破，然而，我对创建的可视化的有效性感到有点怀疑。今天，我将研究作者的可视化方法，并介绍一些我认为非常酷的其他方法。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="5fd1" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">方法</strong></p><p id="6020" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">创建亏损景观的整个过程非常简单直接。</p><ol class=""><li id="252a" class="lw lx iq kv b kw kx kz la lc ly lg lz lk ma lo mb mc md me bi translated">训练网络</li><li id="5fd8" class="lw lx iq kv b kw mf kz mg lc mh lg mi lk mj lo mb mc md me bi translated">创建随机方向</li><li id="69ce" class="lw lx iq kv b kw mf kz mg lc mh lg mi lk mj lo mb mc md me bi translated">给固定重量加上不同的扰动量，看看损失值是如何变化的。</li></ol><p id="b77e" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">唯一需要注意的是这些随机方向是如何产生的。我们来看看作者的方法。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/0b309f9cfeeac8c7b6c335aa84ec801c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*Q5GKxEaNAPc5zQ0NtfjBbg.png"/></div></div></figure><p id="b882" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">他们的方法被称为“过滤器标准化”,非常容易理解。(这里是<a class="ae ks" href="https://github.com/tomgoldstein/loss-landscape" rel="noopener ugc nofollow" target="_blank">链接</a>到作者的代码)。基本上，对于四维张量如(64，3，3，3)，我们将匹配关于第一维的范数，因此(64，1，1，1)，在权重的范数和随机方向的范数之间。(用一个更简单的术语来说，我们可以将这理解为匹配权重和随机方向之间的比例)。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mp"><img src="../Images/d05273a81471b4d14834d3ee6fb1bfff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I9AZM-SV5cj6o2897xbrew.png"/></div></div></figure><p id="7048" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">以上是运行作者代码时的部分结果。现在我们可以利用张量运算来简化整个过程。(我稍后会展示)</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="6e47" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">网络</strong></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mq"><img src="../Images/3ad6b173f68701bac9a349d700b8d0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7a_7McIqZDlZQRBdWngcQ.png"/></div></div></figure><p id="251c" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">绿球</strong> →输入图像(64，64，3) <br/> <strong class="kv ir">蓝色矩形</strong> →卷积+ ReLU 激活<br/> <strong class="kv ir">红色矩形</strong> →软最大输出</p><p id="7852" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">对于这篇文章，我在 CIFAR 10 数据集上训练了三个九层完全卷积神经网络(如上所示)。无任何归一化、批量归一化和<a class="ae ks" href="https://prateekvjoshi.com/2016/04/05/what-is-local-response-normalization-in-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">局部响应归一化。</a></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mr"><img src="../Images/db9c4b6fa2287989afe1768d79902738.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*971ietpiFTps3_BdI67V_Q.png"/></div></div></figure><p id="0519" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">并且从上面的图中我们可以看到，批量归一化的网络取得了最高的性能。</p><p id="6a75" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">从现在起我将把每个网络称为如下的<br/> <strong class="kv ir">正常</strong>:没有任何归一化层的网络<br/> <strong class="kv ir">批量规范</strong>:具有批量归一化层的网络<br/> <strong class="kv ir">局部规范</strong>:具有局部响应归一化层的网络</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="1a7b" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">滤波归一化</strong></p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="bce9" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">上面的代码片段展示了如何使用张量运算进行过滤器标准化。</p><div class="kh ki kj kk gt ab cb"><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/65a9bacc1dada369965f2a894b9e8457.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*scJztY-7_bqnvPtjZ__Fpw.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/f6dcbe18cfed6c1e729cf7aebdc32d5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*sd1EMxNhM7VKIgXqAtUSUQ.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/c8a56d1b4e8a9fcff3f43d0b4e827db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*-MFMID48d0wxUEh7e8A66Q.gif"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk na di nb nc">Normal, Batch Norm, Local Norm</figcaption></figure></div><div class="ab cb"><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/07ed1ac7d222277b92da801ae26e8ce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*3KClLrr_yr3vLgMMwWaecg.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/c87441dc38c3af48405689899beac751.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*DZ90qfzC3KBw4S_XHDX-Ag.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/d91c4c1ea5588e83be3210167061cf8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*JbcS-XMWBSCAw-caH4wAOw.gif"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk na di nb nc">Normal, Batch Norm, Local Norm — Log Scale</figcaption></figure></div><p id="050b" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">当我们使用滤波归一化方法来可视化损失景观时，我们可以看到每个景观看起来并没有太大的不同。只有在我们以对数比例显示景观的情况下，我们才能看到，事实上，局部响应归一化的景观要清晰得多。</p><div class="kh ki kj kk gt ab cb"><figure class="mu kl nd mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/c1ae3244bc6949ce8a22cff031609571.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*Saqxkt8L0nPZBKmKGQQ04A.gif"/></div></figure><figure class="mu kl nd mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/252e130e2346346f6aae216b9522762d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*ZlAK017qEMWniMS2WKJLcQ.gif"/></div></figure></div><p id="df8f" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">当我们将这三幅图按原始比例叠加在一起时，我们可以看到它们看起来是多么相似。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="e5e7" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">滤波器正交化</strong></p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="7788" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">上述方法只是作者方法的简单修改，我们从简单的高斯分布生成随机方向，但是通过<a class="ae ks" href="https://www.tensorflow.org/api_docs/python/tf/linalg/qr" rel="noopener ugc nofollow" target="_blank"> QR 分解</a>我们使方向正交化。</p><div class="kh ki kj kk gt ab cb"><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/7be765b078eebaf7aade07a87858b5b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*qVYrsggu_pS8Lmo3Ye9LbA.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/0b963b2c8fe5cbca3fb30eb51d00a00b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*enKqefTepxnh58TqdrzB9Q.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/00e047266930b9061a5a7ea1db948305.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*017yFiZS-atIA7BMRmOq2g.gif"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk na di nb nc">Normal, Batch Norm, Local Norm</figcaption></figure></div><div class="ab cb"><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/57a033b1f02cf300f96037a3e11e988a.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*eLrv8NxojHH_1a4RdvFbrg.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/4bb1e6b25c2601622647ae6f1ab4068c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*44lwpJ7vnLX0sa4zNIfFFw.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/c2e9170220096ef4b8b97acf81c92d6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*JtxCDPNC_IsPKkb_JRvYSA.gif"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk na di nb nc">Normal, Batch Norm, Local Norm — Log Scale</figcaption></figure></div><p id="200e" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">当我们将不同维度的方向正交化时，我们可以立即看到创建的损失景观是如何彼此不同的。与作者的方法相比，我们可以看到三个网络之间的损耗情况有所不同。</p><div class="kh ki kj kk gt ab cb"><figure class="mu kl nd mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/289b01893f59610c12d2460bba23b19e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*YN_6DEXamtn7aH6XnuULYw.gif"/></div></figure><figure class="mu kl nd mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/ee4b4fd733e875df4ea877d0130d684f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*aIc1F8N9kn9hpd5b2fwaUg.gif"/></div></figure></div></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="126e" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">正交权重投影</strong></p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="d713" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">这与滤波器正交化基本相同，唯一的区别在于，对不同维度的收敛权重执行 ZCA 白化，而不是从高斯分布生成。</p><div class="kh ki kj kk gt ab cb"><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/d63d1725e4fe4af000bfc87b62e37b74.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*GI-sY9-93C8SyXMVv7517g.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/a5ffd45e9b9d08d04fa9e103db206f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*_OjWhdXANZ3nSsA0u0hkJg.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/90d6d2954394e5ad935e8d0c1f1e99a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*yVRiAKwQadWEumbC1LA5iw.gif"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk na di nb nc">Normal, Batch Norm, Local Norm</figcaption></figure></div><div class="ab cb"><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/435a64cf7eae5954dc7b25b7f71e07b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*JJAqa_8FI8kkZPB1Gqd30w.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/8b0a78407fc4c2fa52edafe19a90a930.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*B37PkuAkYx6QJhEN9qR45A.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/9d3abebf8c8140428729e6e545b0b185.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*0j98frW033ZSjW_StOC8FA.gif"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk na di nb nc">Normal, Batch Norm, Local Norm — Log Scale</figcaption></figure></div><p id="c3f1" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">类似于滤波器正交化，我们可以看到生成的可视化之间的一些差异。</p><div class="kh ki kj kk gt ab cb"><figure class="mu kl nd mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/b82317236484b6882d34a4aff7a3c5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*FmWjNnbYQsMhGNLUM_Dl0g.gif"/></div></figure><figure class="mu kl nd mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/63c0da285d40ab9b79bcce5df5ab591e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*VOiKQlOSrl58vVORbObfjA.gif"/></div></figure></div></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="eead" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">重量的主要方向</strong></p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="a452" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">最后一种方法是在不同的维度之间，在它们的第一主方向上扰动权重。</p><div class="kh ki kj kk gt ab cb"><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/9e2a4a27f0dd997a0d347f63839307e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*EPdnK2l7RZayVncGTAgxlw.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/aab3d57d59799d4df1f3103f29b9aee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*bPhhhQGLH3XRYELPyrDqxA.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/e570ec55b50669ea18cbd809a0c75f89.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*Bi6ae0lra_6ux5c6-eT0SA.gif"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk na di nb nc">Normal, Batch Norm, Local Norm</figcaption></figure></div><div class="ab cb"><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/a3ef906e368dcf89bdfb8c8fc70db84e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*ZwHae0WmJn9aGYB3NBWQMw.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/cb8482f24d34600d9f2b4cd0dda9bbe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*lyinApacJ6j3FE08IOM9hA.gif"/></div></figure><figure class="mu kl mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/6e17ee42251b99f47e209e637574760c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*7tgu4nkH4SxlKZFmTRucFw.gif"/></div><figcaption class="ko kp gj gh gi kq kr bd b be z dk na di nb nc">Normal, Batch Norm, Local Norm — Log Scale</figcaption></figure></div><p id="e8f4" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">我们可以清楚地看到所产生的损失情况之间的差异。</p><div class="kh ki kj kk gt ab cb"><figure class="mu kl nd mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/d82ca61e664d0243098c1405cf34ebe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*ATlXisfczVncY_nUK_FkMQ.gif"/></div></figure><figure class="mu kl nd mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><img src="../Images/0d3ee67b9fc437e8e57186a898f3d6ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*37wf-gCOFMK8ka0GIcBocw.gif"/></div></figure></div></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="9852" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">讨论</strong></p><p id="2f69" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">我写这篇文章的唯一原因是为了表明，根据我们使用的方向，创造的损失景观可以发生巨大的变化。因此，我们需要质疑生成的损失景观的有效性，它们是否真正反映了训练网络的特征。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="004d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">代码</strong></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ne"><img src="../Images/1a246fa37decad2506bf48a7f31667c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EAWClpVN6is6goRLLMNgDw.png"/></div></div></figure><p id="546c" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">要访问创建可视化效果的代码，请<a class="ae ks" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-3/blob/master/Loss%20LanScape/0%20create%20viz.ipynb" rel="noopener ugc nofollow" target="_blank">点击此处。</a> <br/>要查看整篇博文的代码，请<a class="ae ks" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-3/tree/master/Loss%20LanScape" rel="noopener ugc nofollow" target="_blank">点击此处</a>。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="fc83" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">遗言</strong></p><p id="654f" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">我不想做任何大胆的断言，但似乎不同的方向选择会产生不同的视觉效果。问题仍然存在，哪个方向是最‘正确’的？可有正确的，哪一个揭示了真相？此外，我想提一下名为“<a class="ae ks" href="https://arxiv.org/abs/1703.04933" rel="noopener ugc nofollow" target="_blank">尖锐极小值可以推广到深度网络</a>的论文，该论文表明已经收敛到尖锐极小值的深度神经网络可以很好地推广，并且该理论不适用于具有 ReLU 激活的网络。就像那篇论文如何证明我们的观察可以根据我们的定义而改变一样，我们应该致力于创造反映真理的定义。</p><p id="13e3" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">还有更多研究要做，我很期待。如果你希望看到更多这样的帖子，请访问我的<a class="ae ks" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">网站</a>。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="aec4" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">参考</strong></p><ol class=""><li id="da23" class="lw lx iq kv b kw kx kz la lc ly lg lz lk ma lo mb mc md me bi translated">李，h，徐，z，泰勒，g，斯图德，c，&amp;戈尔茨坦，T. (2017)。可视化神经网络的损失景观。arXiv.org。2019 年 5 月 3 日检索，来自<a class="ae ks" href="https://arxiv.org/abs/1712.09913" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1712.09913</a></li><li id="5aeb" class="lw lx iq kv b kw mf kz mg lc mh lg mi lk mj lo mb mc md me bi translated">tomgoldstein/loss-landscape。(2019).GitHub。检索于 2019 年 5 月 3 日，来自<a class="ae ks" href="https://github.com/tomgoldstein/loss-landscape" rel="noopener ugc nofollow" target="_blank">https://github.com/tomgoldstein/loss-landscape</a></li><li id="28b6" class="lw lx iq kv b kw mf kz mg lc mh lg mi lk mj lo mb mc md me bi translated"><a class="ae ks" href="https://prateekvjoshi.com/2016/04/05/what-is-local-response-normalization-in-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://prateekvjoshi . com/2016/04/05/what-is-local-response-normalization-in-convolutionary-neural-networks/</a></li></ol></div></div>    
</body>
</html>