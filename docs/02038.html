<html>
<head>
<title>Home Value Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">家庭价值预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/home-value-prediction-2de1c293853c?source=collection_archive---------10-----------------------#2019-04-04">https://towardsdatascience.com/home-value-prediction-2de1c293853c?source=collection_archive---------10-----------------------#2019-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="67d8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用机器学习算法预测房地产价值</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d1b131277e50a1f48bec845ad6c132d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eO3CIaFBe7LMRePApDwSYA.jpeg"/></div></div></figure><p id="c2b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">像 Zillow 这样的公司是如何为不出售的房屋提供估价的？他们收集每个属性的特征数据，并使用机器学习算法进行预测。在本文中，我将使用 Kaggle 的“房价”竞赛中包含的数据集来演示类似的分析。</p><h1 id="d57d" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated"><strong class="ak">探索性数据分析</strong></h1><p id="e42b" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">首先，让我们看看响应变量“销售价格”。它是正向倾斜的。大多数房子的售价在 10 万至 25 万美元之间，但有些房子的售价要高得多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/bd7fb2d3263eafbebe4f9a66d506523f.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*UFfnwgu61KA87sQwlo4lmw.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Figure 1: Observed sale price</figcaption></figure><p id="e4cf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该数据集包含 80 个描述房产特征的特征，包括浴室数量、地下室面积、建造年份、车库面积等。热图(图 2)显示了每个特性和响应变量“销售价格”之间的相关性。这为我们提供了有关预测销售价格的特征重要性的信息，并指出哪里可能存在多重共线性。毫不奇怪，房屋的整体质量“总体质量”与销售价格高度相关。相比之下，房屋出售的年份“YrSold”与销售价格几乎没有关联。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/87b9cd1174c165b7569bb588977a4c64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TATCpuA1RwNg984SMPUJIQ.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Figure 2: Heat map showing the correlation among features and sale price</figcaption></figure><h1 id="6d45" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated"><strong class="ak">数据清理</strong></h1><h2 id="0628" class="mt lr it bd ls mu mv dn lw mw mx dp ma ld my mz mc lh na nb me ll nc nd mg ne bi translated">应对 NAs</h2><p id="7758" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">此数据集中有许多 NAs 有些功能几乎全是 NAs，而许多功能只有少数几个。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/3c156288bb35c4e2606ca71536493c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*FTIDGfclLboipNDpKdLpiA.png"/></div></figure><p id="f125" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以删除提供很少信息的功能，如实用程序。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="0b7c" class="mt lr it nh b gy nl nm l nn no">df.Utilities.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/af65430830e91ed2418fcff818ff558b.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*gGp_u3D_Q0sr1v97V_Mbiw.png"/></div></figure><p id="e844" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">除了一个属性之外，所有属性都被指定为“Allpub”类别，因此我们可以删除该特性。由于缺乏变化，该特性与我们的响应销售价格几乎没有关联(图 2)，所以我们并不担心会失去它。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="2d5b" class="mt lr it nh b gy nl nm l nn no">df=df.drop([‘Utilities’], axis=1)</span></pre><p id="e7c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">很少有 NAs 是随机的，因为缺少信息通常与记录本身有关，而不仅仅是因为收集错误。例如，GarageType 记录的 NA 可能意味着该物业上没有车库。在该数据集中，有与车库相关的分类和连续特征。我们可以相应地用 0 和“无”来填充那些具有 NAs 功能的属性，表示缺少车库空间。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="a537" class="mt lr it nh b gy nl nm l nn no"># Garage categorical features to none<br/>for i in (‘GarageType’, ‘GarageFinish’, ‘GarageQual’, ‘GarageCond’):<br/> df[i] = df[i].fillna(‘None’)</span><span id="534b" class="mt lr it nh b gy nq nm l nn no"># Garage continuous features to 0<br/>for i in (‘GarageYrBlt’, ‘GarageArea’, ‘GarageCars’):<br/> df[i] = df[i].fillna(0)</span></pre><p id="6fe2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">NAs 对于其他功能没有明确的解释，与信息的缺乏有关。在这种情况下，我们可以观察每个记录的出现频率，并选择最可能的值。让我们看看描述分区分类的特性“MSZoning”的频率分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ea18a570bdc1352cd55dcc4cd451d7c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*9BZ9uDi65RDAlrZZPGYREg.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Figure 3: Frequency of zoning classification</figcaption></figure><p id="c9da" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">到目前为止，住宅低密度(RL)是最常见的分类。解决此功能中四个 NAs 问题的实用方法是简单地用“RL”替换 NAs。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="6875" class="mt lr it nh b gy nl nm l nn no">df.MSZoning=df[‘MSZoning’].fillna(df[‘MSZoning’].mode()[0])</span></pre><h2 id="fd8b" class="mt lr it bd ls mu mv dn lw mw mx dp ma ld my mz mc lh na nb me ll nc nd mg ne bi translated">数据转换</h2><p id="de3d" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">为了最大化我们的模型的性能，我们想要标准化我们的特征和响应变量。正如我们在图 1 中看到的，我们的响应变量是正偏的。通过应用对数转换，销售价格现在类似于正态分布(图 4)。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="d38d" class="mt lr it nh b gy nl nm l nn no">resp=np.log1p(resp) # transform by log(1+x)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/cb6c706f17429ea9724ebd7f24aff6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*Dcxnts8S843CMbqLtCtq1g.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Figure 4: Log transformed response variable Sale Price</figcaption></figure><p id="c90a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们还必须检查所有连续要素的倾斜情况。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="bfab" class="mt lr it nh b gy nl nm l nn no"># identify numerical features<br/>num_feats=df.dtypes[df.dtypes!=’object’].index</span><span id="2baf" class="mt lr it nh b gy nq nm l nn no"># quantify skew<br/>skew_feats=df[num_feats].skew().sort_values(ascending=False)<br/>skewness=pd.DataFrame({‘Skew’:skew_feats})<br/>skewness=skewness[abs(skewness)&gt;0.75].dropna()<br/>skewed_features=skewness.index</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/649f7f7ea3ebf28b2e3032b13009b60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*zL78XvD5ZfKl34O2shxWIQ.png"/></div></figure><p id="4595" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们想要转换的所有这些特性之间，偏斜度会有很大的变化。box cox 变换提供了一种灵活的变换特征的方法，每个特征可能都需要另一种方法。函数 boxcox 将估计最佳 lambda 值(变换中的一个参数)并返回变换后的要素。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="8c3e" class="mt lr it nh b gy nl nm l nn no"># add one to all skewed features, so we can log transform if needed<br/>df[skewed_features]+=1<br/># conduct boxcox transformation<br/>from scipy.stats import boxcox</span><span id="d1d0" class="mt lr it nh b gy nq nm l nn no"># apply to each of the skewed features<br/>for i in skewed_features:<br/> df[i],lmbda=boxcox(df[i], lmbda=None)</span></pre><h2 id="3d95" class="mt lr it bd ls mu mv dn lw mw mx dp ma ld my mz mc lh na nb me ll nc nd mg ne bi translated">一键编码</h2><p id="3297" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">最后，我们需要对我们的分类变量进行一次性编码(或伪代码),以便它们可以被模型解释。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="5e73" class="mt lr it nh b gy nl nm l nn no">df=pd.get_dummies(df)</span></pre><h1 id="6247" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">建模</h1><p id="ba16" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">我们将把两个广泛应用的机器学习模型拟合到训练数据中，并使用交叉验证来评估它们的相对性能。</p><h2 id="c4fc" class="mt lr it bd ls mu mv dn lw mw mx dp ma ld my mz mc lh na nb me ll nc nd mg ne bi translated">随机森林回归量</h2><p id="b413" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">为了确保我们的随机森林回归模型具有最大化其预测能力的属性，我们将优化超参数值。我们希望估计以下各项的最佳值:</p><p id="34da" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"><em class="nu">n _ 估计量:</em> </strong>森林中的树木数量</p><p id="8988" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"><em class="nu">max _ features</em></strong><em class="nu">:</em>每次分割要考虑的最大特征数</p><p id="4592" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="nu"> max_depth: </em> </strong>任意树的最大分裂数</p><p id="7c95" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"><em class="nu">min _ samples _ split:</em></strong>拆分一个节点所需的最小样本数</p><p id="80fb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"><em class="nu">min _ samples _ leaf:</em></strong>每个叶节点所需的最小样本数</p><p id="104b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="nu">自举:</em> </strong>数据集是自举还是整个数据集用于每棵树</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="8736" class="mt lr it nh b gy nl nm l nn no">n_estimators=[int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]<br/>max_features = [‘auto’, ‘sqrt’, ‘log2’]<br/>max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]<br/>max_depth.append(None)<br/>min_samples_split = [2, 5, 10]<br/>min_samples_leaf = [1, 2, 4]<br/>bootstrap = [True, False]<br/>grid_param = {‘n_estimators’: n_estimators,<br/> ‘max_features’: max_features,<br/> ‘max_depth’: max_depth,<br/> ‘min_samples_split’: min_samples_split,<br/> ‘min_samples_leaf’: min_samples_leaf,<br/> ‘bootstrap’: bootstrap}</span></pre><p id="3cc0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们使用 sci-kit learn 的 GridSearchCV 来确定最佳超参数，我们将评估 6，480 个候选模型和 32，400 个符合五重交叉验证的模型。这在计算上将非常昂贵，因此我们将使用 RandomizedSearchCV，它使用从我们定义的参数空间中随机选择的超参数来评估指定数量的候选模型(n_iter)。我们将使用五个折叠进行 k 折叠交叉验证。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="3c3f" class="mt lr it nh b gy nl nm l nn no">from sklearn.ensemble import RandomForestRegressor</span><span id="d05a" class="mt lr it nh b gy nq nm l nn no"># the model prior to hyperparameter optimization<br/>RFR=RandomForestRegressor(random_state=1)</span><span id="cf02" class="mt lr it nh b gy nq nm l nn no">from sklearn.model_selection import RandomizedSearchCV<br/>RFR_random = RandomizedSearchCV(estimator = RFR, param_distributions = grid_param, n_iter = 500, cv = 5, verbose=2, random_state=42, n_jobs = -1)</span><span id="8577" class="mt lr it nh b gy nq nm l nn no">RFR_random.fit(train, resp) <br/>print(RFR_random.best_params_)</span></pre><p id="1fff" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们有了一个模型，它的属性最适合我们的数据。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="60e8" class="mt lr it nh b gy nl nm l nn no">Best_RFR = RandomForestRegressor(n_estimators=1000, min_samples_split=2, min_samples_leaf=1,max_features=’sqrt’, max_depth=30, bootstrap=False)</span></pre><p id="7b5d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们希望精确测量模型预测的房价与售出房屋的实际价格之间的差异。我们将通过 k 倍交叉验证来计算模型的均方根误差(RMSE)。给定五个折叠，我们将使用五组模型拟合中每一组的平均 RMSE 值。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="c403" class="mt lr it nh b gy nl nm l nn no">from sklearn.model_selection import KFold, cross_val_score<br/>n_folds=5<br/>def rmse_cv(model):<br/>    kf = KFold(n_folds,shuffle=True,random_state=42)<br/>.get_n_splits(train)<br/>    rmse= np.sqrt(-cross_val_score(model, train, resp, scoring=”neg_mean_squared_error”, cv = kf))<br/>    return(rmse.mean())<br/> <br/>rmse_cv(Best_RFR)</span></pre><p id="abe9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">随机森林模型表现相当好，平均 RMSE 为 0.149。</p><p id="bee5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们尝试另一个模型，看看我们是否能获得更好的预测。</p><h2 id="bec4" class="mt lr it bd ls mu mv dn lw mw mx dp ma ld my mz mc lh na nb me ll nc nd mg ne bi translated">梯度推进回归器</h2><p id="92af" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">我们将使用 RandomizedSearchCV 进行相同的评估，以确定最佳超参数。我们将从“xgboost”中使用的梯度推进回归器具有以下我们想要优化的超参数:</p><p id="d0f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"><em class="nu">n _ 估计量:</em> </strong>树木数量</p><p id="f9f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="nu">子样本:</em> </strong>每棵树样本百分比</p><p id="e1b6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="nu"> max_depth: </em> </strong>每棵树的最大层数</p><p id="3dd1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"><em class="nu">min _ child _ weight:</em></strong>一个子代中需要的所有观察值的最小权重之和</p><p id="4451" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"><em class="nu">col sample _ bytree:</em></strong>每棵树使用的特征百分比</p><p id="141f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="nu">学习率:</em> </strong>学习率或步长收缩</p><p id="a8e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="nu">伽玛:</em> </strong>进行拆分所需的最小缩减成本函数</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="0c74" class="mt lr it nh b gy nl nm l nn no">n_estimators=[int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]<br/>subsample = [.6,.7,.8,.9,1]<br/>max_depth = [int(x) for x in np.linspace(10, 50, num = 10)]<br/>min_child_weight = [1,3,5,7]<br/>colsample_bytree=[.6,.7,.8,.9,1]<br/>learning_rate=[.01,.015,.025,.05,.1]<br/>gamma = [.05,.08,.1,.3,.5,.7,.9,1]<br/>rand_param = {‘n_estimators’: n_estimators,<br/> ‘subsample’: subsample,<br/> ‘max_depth’: max_depth,<br/> ‘colsample_bytree’: colsample_bytree,<br/> ‘min_child_weight’: min_child_weight,<br/> ‘learning_rate’: learning_rate,<br/> ‘gamma’: gamma}</span></pre><p id="27bc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用与随机森林模型相同的方法，我们将使用 k-fold 交叉验证运行随机超参数搜索。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="25f3" class="mt lr it nh b gy nl nm l nn no">Boost_random = RandomizedSearchCV(estimator = Boost, param_distributions = rand_param, n_iter = 500,<br/> cv = 5, verbose=2, random_state=42, n_jobs = -1)</span><span id="67fe" class="mt lr it nh b gy nq nm l nn no">Boost_random.fit(train, resp)</span></pre><p id="93bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们现在可以计算优化模型的 RMSE，并将 xgboost 的性能与随机森林模型进行比较。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="9664" class="mt lr it nh b gy nl nm l nn no">Best_Boost = XGBRegressor(subsample=.7, n_estimators=1600, min_child_weight=3, max_depth=41,learning_rate=.025, gamma=.05, colsample_bytree=.6)</span><span id="d8d8" class="mt lr it nh b gy nq nm l nn no"># evaluate rmse<br/>rmse_cv(Best_Boost)</span></pre><p id="0f95" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的梯度推进回归模型表现出优于随机森林模型的性能，RMSE 值为 0.131。</p><h2 id="0cf6" class="mt lr it bd ls mu mv dn lw mw mx dp ma ld my mz mc lh na nb me ll nc nd mg ne bi translated">做最后的预测</h2><p id="72b4" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">在这个分析中，我采用了一种实用的建模方法；还有其他建模技术可以略微提高预测精度，如堆叠或应用一套替代模型(如 Lasso、ElasticNet、KernalRidge)。</p><p id="1b1a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将把这个分析中的最佳模型(梯度推进回归)应用于测试集，并评估它的性能。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="710f" class="mt lr it nh b gy nl nm l nn no"># fit to the training data<br/>Best_Boost.fit(train,resp)<br/># transform predictions using exponential function<br/>ypred=np.expm1(Best_Boost.predict(test))</span><span id="17a2" class="mt lr it nh b gy nq nm l nn no"># make a data frame to hold predictions, and submit to Kaggle<br/>sub=pd.DataFrame()<br/>sub['Id']=test['Id']<br/>sub['SalePrice']=ypred<br/>sub.to_csv('KaggleSub.csv', index=False)</span></pre><p id="9a52" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">梯度推进回归模型在测试集上以 0.1308 的 RMSE 值执行，不错！</p><h1 id="11b4" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">结论</h1><p id="cbc9" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">我们可以根据房产的特点对房子的售价做出合理的预测。关键步骤包括为 NAs 分配适当的值、标准化变量、优化候选模型的超参数以及选择最佳模型。</p><p id="15aa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我感谢任何反馈和建设性的批评。与该分析相关的代码可在<a class="ae nv" href="https://github.com/njermain" rel="noopener ugc nofollow" target="_blank">github.com/njermain</a>上找到</p></div></div>    
</body>
</html>