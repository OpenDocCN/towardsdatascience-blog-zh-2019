<html>
<head>
<title>Natural Language Processing for Automated Feature Engineering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向自动化特征工程的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-processing-for-automated-feature-engineering-46a61a3930b1?source=collection_archive---------21-----------------------#2019-08-23">https://towardsdatascience.com/natural-language-processing-for-automated-feature-engineering-46a61a3930b1?source=collection_archive---------21-----------------------#2019-08-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/5fc0fa21e13dba97bfaaf3876bd98319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hcLZsBcc_M9kx7ptdqMLsQ.png"/></div></div></figure><div class=""/><p id="c286" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">如何使用 Featuretools 应用 nlp-primitives 库。</em></p><p id="23a3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当试图在机器学习管道中利用真实世界的数据时，经常会遇到文本。文本数据可以包含许多有价值的信息，但经常被忽略，因为很难将文本翻译成算法可以解释的有意义的数字。</p><p id="5770" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在<a class="ae kx" href="https://github.com/Featuretools/featuretools" rel="noopener ugc nofollow" target="_blank"> Featuretools </a>、<a class="ae kx" href="https://docs.featuretools.com/automated_feature_engineering/primitives.html" rel="noopener ugc nofollow" target="_blank">、<em class="kw">原语函数</em>、</a>用于为不同类型的数据自动创建<em class="kw">特征</em>，使机器学习模型能够充分利用你的数据集中的数据。</p><p id="dd76" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本文中，我探索了使用<a class="ae kx" href="https://github.com/FeatureLabs/nlp_primitives" rel="noopener ugc nofollow" target="_blank"> nlp-primitives 库</a>从文本数据中创建特征，通过使用一个示例数据集来研究机器学习模型中的这些附加特征。然后，我解释为什么这些原语对模型的准确性有如此大的影响。</p><h2 id="22d5" class="ky kz jb bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">一个使用 nlp 原语的机器学习演示</h2><p id="de52" class="pw-post-body-paragraph jy jz jb ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">在这个演示中，我将使用来自<a class="ae kx" href="https://www.kaggle.com/jkgatt/restaurant-data-with-100-trip-advisor-reviews-each" rel="noopener ugc nofollow" target="_blank">这个 Kaggle 数据集</a>的数据，该数据集包含旧金山地区 57 家餐馆的 100 条评论。数据集相对较小，但足以用 Featuretools 和 nlp-primitives 库演示特征工程。您可以使用这个库中的<a class="ae kx" href="https://github.com/FeatureLabs/predict-restaurant-rating" rel="noopener ugc nofollow" target="_blank">Jupyter 笔记本继续学习。</a></p><p id="d5ba" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，让我们确定问题:根据每个评论者的反馈，确定他们对餐厅的看法。该模型的成功与否将取决于它能在多大程度上准确预测每个评审者根据给定数据和评审给出的评级。在数据集中，评论有 5 个可能值，从 1 星到 5 星，所以问题是一个 5 类分类问题。</p><p id="4dfa" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了评估 NLP 原语的有效性，我还使用没有这些新原语函数的<a class="ae kx" href="https://blog.featurelabs.com/deep-feature-synthesis/" rel="noopener ugc nofollow" target="_blank">深度特征合成</a> (DFS)创建了一个基线特征矩阵，以便我可以看到这些新原语的有效性。利用这个基线特征矩阵，我创建了一个机器学习模型，准确率大约为 50%。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="1934" class="ky kz jb mb b gy mf mg l mh mi">baseline_feature_matrix, baseline_features = ft.dfs(entityset=es,<br/>                                            target_entity='reviews',<br/>                                            verbose=True,<br/>                                            ignore_variables=ignore)</span><span id="74a7" class="ky kz jb mb b gy mj mg l mh mi"><strong class="mb jc">built 33 features</strong></span><span id="5db7" class="ky kz jb mb b gy mj mg l mh mi">base_rfc = RandomForestClassifier(n_estimators=100,<br/>                                  class_weight = "balanced", <br/>                                  n_jobs=-1)<br/>base_rfc.fit(baseline_train_fm, baseline_y_train)<br/>base_rfc.score(baseline_test_fm, baseline_y_test)</span><span id="7db6" class="ky kz jb mb b gy mj mg l mh mi"><strong class="mb jc">0.5156462585034014</strong></span></pre><p id="c2de" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个模型预测大多数评论属于最常见的评论类别，所以它的预测不是很准确，正如你在下面的混淆矩阵中看到的。</p><p id="5298" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在基线模型和使用 nlp 原语的模型之间，只有一个因素发生了变化:NLP 原语库的使用。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="ee1d" class="ky kz jb mb b gy mf mg l mh mi">trans = [DiversityScore, LSA, MeanCharactersPerWord, <br/>         PartOfSpeechCount, PolarityScore, PunctuationCount, <br/>         StopwordCount, TitleWordCount, UniversalSentenceEncoder,      <br/>         UpperCaseCount]<br/><br/>features = ft.dfs(entityset=es, target_entity='reviews', <br/>                  trans_primitives=trans, verbose=True,<br/>                  features_only=True, ignore_variables=ignore,<br/>                  drop_contains=drop_contains, max_depth=4)</span><span id="8b5e" class="ky kz jb mb b gy mj mg l mh mi"><strong class="mb jc">Built 333 features</strong></span></pre><p id="ca54" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过 DFS 调用中的这个小变化，生成的特性数量<strong class="ka jc">增加了 10 倍。</strong></p><p id="9608" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个库非常容易合并，只需要几行额外的代码就可以导入库和原语，然后将这些原语添加到<code class="fe mk ml mm mb b">ft.dfs</code>函数用来创建特征的默认原语中。在基线模型和 nlp-图元模型中，DFS 用于查找特征，尽管 NLP-图元具有修改的深度场，允许 DFS 创建堆叠在 NLP 特征之上的图元。</p><p id="0603" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在运行 DFS 并创建结果特征矩阵后，我们可以将数据分成训练集和测试集，并在<a class="ae kx" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> sklearn 机器学习模型</a>中使用这些集来测试它们的准确性。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="5516" class="ky kz jb mb b gy mf mg l mh mi">vot = VotingClassifier(voting='soft', <br/>                       estimators=[('lgr', lgr),('rfc', rfc),   <br/>                                  ('hgbc', hgbc)],<br/>                       weights=[3, 1, 6])<br/>vot.fit(train_feature_matrix, y_train)<br/>vot.score(test_feature_matrix, y_test)</span><span id="ee29" class="ky kz jb mb b gy mj mg l mh mi"><strong class="mb jc">0.6925170068027211</strong></span></pre><p id="38eb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当使用 nlp-primitives 库时，模型能够实现大约 70%的准确性，混淆矩阵被准确地分布(深蓝色表示猜测)，大多数不正确的猜测非常接近实际答案(1-由更明显的对角线上的深蓝色表示)(完美的算法将在向下的对角线上有 1，表示预测和真实标签一致，在每个其他类别中有 0-了解更多关于混淆矩阵的信息<a class="ae kx" rel="noopener" target="_blank" href="/understanding-confusion-matrix-a9ad42dcfd62">此处</a>)。</p><p id="fbe3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这两个模型使用相似的训练和测试步骤(基线模型使用稍微不太复杂的函数，因为更复杂的函数不会改变准确性)，但是具有 NLP 功能的模型的准确性比基线高大约 40%。由于其他一切都保持不变，很明显 NLP 原语库是准确性大幅提高的原因。此外，当我们检查特性重要性时，我们看到使用 NLP 原语的特性排名最高(更多详细信息，请参见<a class="ae kx" href="https://github.com/FeatureLabs/predict-restaurant-rating" rel="noopener ugc nofollow" target="_blank">笔记本</a>)。</p><h2 id="3e6d" class="ky kz jb bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">为什么这些原语会有所不同？</h2><p id="1b86" class="pw-post-body-paragraph jy jz jb ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">数据必须格式化为数字，以便机器学习模型从中“学习”。文字很难放进数字，或者至少很难放进数字而不失去很多意义。例如，获得一段文本的字数是相当简单的，但是，通常这并不是一个足够的意义度量。尽管这有时可能是一个有用的功能，但文本的意义远不止它包含的字数。</p><p id="c04f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">那么，解决办法是什么呢？如何以有意义的方式将文本编码成数字？一种解决方案是将文本的含义矢量化。NLP 原语，如 UniversalSentenceEncoder、LSA(潜在语义分析)和 PartOfSpeechCount 都使用这种方法。它们都是多输出原语，这意味着它们接受一个字段并创建一个具有多个字段的要素。在这种情况下，这些字段表示向量的维数。在下面的例子中，每个文本字符串对应两个输出，因为<a class="ae kx" href="https://docs.featuretools.com/generated/nlp_primitives.LSA.html" rel="noopener ugc nofollow" target="_blank"> LSA(潜在语义分析)</a>为每个给定的字符串创建一个长度为 2 的向量。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="5ff8" class="ky kz jb mb b gy mf mg l mh mi">from nlp_primitives import LSA<br/>import pandas as pd<br/>data = ["hello, this is a new featuretools library",<br/>        "this will add new natural language primitives",<br/>        "we hope you like it!"]<br/>lsa = LSA()<br/>pd.DataFrame(lsa(data).tolist()).T</span></pre><figure class="lw lx ly lz gt is gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/04686075a2a78b96b49454766bcb79dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*ILtzC0fYCxoT8MeKvchl3g.png"/></div></figure><p id="89ef" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在下一个示例中，原语 PartOfSpeechCount 为每个输入生成 15 个值。这个向量的每个维度代表一个词性，以及该词性在输入文本字段中出现的次数。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="144f" class="ky kz jb mb b gy mf mg l mh mi">from nlp_primitives import PartOfSpeechCount<br/>data = ["hello, this is a new featuretools library",<br/>        "this will add new natural language primitives",<br/>        "we hope you like it!"]<br/>pscount = PartOfSpeechCount()<br/>pd.DataFrame(pscount(data).tolist()).T</span></pre><figure class="lw lx ly lz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mo"><img src="../Images/04b97b00e477ff83314ad64157d728f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b9PE-g6rAi6bvjCatNhtnA.png"/></div></div></figure><p id="5c00" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些原语以这样一种方式对向量中文本字段的含义进行编码，即具有相似含义的两个文本字段具有相似的向量，即使它们由不同的单词组成。这使得这些方法特别有用，因为以类似方式编码类似含义的能力允许机器学习模型学习特定向量的结果，并将该向量的结果与类似向量相关联。</p><p id="e252" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然而，处理许多输出常常是具有挑战性的，尤其是当试图堆叠原语时——将一些原语的输出用作其他原语的输入。当存在许多实体或数据源时，这会产生更多的信息。Featuretools 很好地处理了这一点，这使用户能够跨实体以及在实体内收集信息，以最大限度地利用当前数据。Featuretools 自动<a class="ae kx" href="https://docs.featuretools.com/automated_feature_engineering/afe.html#creating-deep-features" rel="noopener ugc nofollow" target="_blank">【堆叠】图元的能力进一步增强了这一点</a>，甚至在特征工程步骤中进一步扩展了任何单个图元转化特征的能力。</p><h1 id="2abf" class="mp kz jb bd la mq mr ms ld mt mu mv lg mw mx my lj mz na nb lm nc nd ne lp nf bi translated">关键要点</h1><ol class=""><li id="7d61" class="ng nh jb ka b kb lr kf ls kj ni kn nj kr nk kv nl nm nn no bi translated"><strong class="ka jc">NLP-primitives 库在处理文本数据时增强了模型的准确性。</strong>这种额外的准确性源于对文本含义的编码，而不仅仅是计算简单的描述性指标。</li><li id="84f3" class="ng nh jb ka b kb np kf nq kj nr kn ns kr nt kv nl nm nn no bi translated"><strong class="ka jc">使用正确的工具使机器学习过程变得更容易。</strong>当我们修改了几行代码来合并 nlp-primitives 库时，准确性提高了，而代码的复杂性保持不变。</li><li id="48ae" class="ng nh jb ka b kb np kf nq kj nr kn ns kr nt kv nl nm nn no bi translated"><strong class="ka jc">正确性可以通过多种方式进行评估。</strong>当一个模型不准确的时候，理解哪里出了问题是很重要的——它仅仅是错误的(像基线模型)还是错误的，但接近正确的(像 nlp-primitives 模型)？</li></ol><h1 id="3ff6" class="mp kz jb bd la mq mr ms ld mt mu mv lg mw mx my lj mz na nb lm nc nd ne lp nf bi translated">最后的想法</h1><p id="07fe" class="pw-post-body-paragraph jy jz jb ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">这个库已经发布，<a class="ae kx" href="https://github.com/FeatureLabs/nlp_primitives/" rel="noopener ugc nofollow" target="_blank">可以独立安装</a>或者通过使用<code class="fe mk ml mm mb b">pip</code>的功能工具安装。但是，它仍处于发展的初级阶段，我们欢迎任何关于它的反馈。此外，自然语言处理领域是一个快速发展的领域，所以如果您看到一个添加新原语的机会，请作为<a class="ae kx" href="https://github.com/FeatureLabs/nlp_primitives/issues" rel="noopener ugc nofollow" target="_blank"> GitHub 问题</a>提出建议，或者尝试自己创建——该库是开源的，因此任何人都可以做出贡献！</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="8d74" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">原载于 2019 年 8 月 23 日</em><a class="ae kx" href="https://blog.featurelabs.com/natural-language-processing-featuretools/" rel="noopener ugc nofollow" target="_blank"><em class="kw">https://blog.featurelabs.com</em></a><em class="kw">。</em></p></div></div>    
</body>
</html>