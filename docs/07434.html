<html>
<head>
<title>Step-by-Step R-CNN Implementation From Scratch In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 从头开始一步一步实现 R-CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/step-by-step-r-cnn-implementation-from-scratch-in-python-e97101ccde55?source=collection_archive---------2-----------------------#2019-10-18">https://towardsdatascience.com/step-by-step-r-cnn-implementation-from-scratch-in-python-e97101ccde55?source=collection_archive---------2-----------------------#2019-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5ae3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">分类和目标检测是计算机视觉的主要部分。分类是找到图像中的内容，而对象检测和定位是找到该对象在图像中的位置。检测是一个更复杂的问题，因为我们需要找到对象在图像中的坐标。</p><p id="39c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了解决这个问题，2014 年 Ross Girshick、Jeff Donahue、Trevor Darrell 和 Jitendra Malik 引入了 R-CNN。R-CNN 代表有 CNN 的地区。在 R-CNN 中，我们通过选择性搜索传递图像，并从结果中选择第一个 2000 区域建议，并在此基础上运行分类，而不是在大量区域上运行分类。这样，我们只需要对前 2000 个区域进行分类，而不是对大量区域进行分类。这使得该算法比以前的对象检测技术更快。R-CNN 有 4 个步骤。它们如下:</p><ol class=""><li id="d6a0" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">通过选择性搜索图像并生成区域建议。</li><li id="b792" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">使用地面真实数据计算建议区域的 IOU(交集/并集),并向建议区域添加标签。</li><li id="1ffc" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">使用带标签的建议区域进行迁移学习。</li><li id="d500" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">将测试图像传递给选择性搜索，然后传递来自训练模型的前 2000 个建议区域，并预测这些区域的类别。</li></ol><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi kz"><img src="../Images/324030bf167b3fcd67fba2b1462fa63c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GPBlt7i3IY1MWSTwSHodHA.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">R-CNN</figcaption></figure><p id="8739" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我将使用来自<a class="ae lp" href="http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html" rel="noopener ugc nofollow" target="_blank">http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html</a>的飞机数据集在 Keras 从头开始实现完整的 R-CNN。要获得带注释的数据集，你可以从下面的链接下载。实现 RCNN 的代码也可以在下面提到的资源库中找到。</p><blockquote class="lq lr ls"><p id="2ba7" class="jn jo lt jp b jq jr js jt ju jv jw jx lu jz ka kb lv kd ke kf lw kh ki kj kk ij bi translated">https://github.com/1297rohit/RCNN.git<a class="ae lp" href="https://github.com/1297rohit/RCNN.git" rel="noopener ugc nofollow" target="_blank"/></p></blockquote><p id="fcf1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下载完数据集后，您可以继续执行下面的步骤。</p><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="e0f2" class="mc md iq ly b gy me mf l mg mh">import os,cv2,keras<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import tensorflow as tf</span></pre><p id="144a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一步是导入实现 R-CNN 所需的所有库。我们需要 cv2 对图像进行选择性搜索。要使用选择性搜索，我们需要下载 opencv-contrib-python。要下载刚刚运行的<strong class="jp ir"> pip，请在终端中安装 opencv-contrib-python </strong>，并从 pypi 安装它。</p><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="8d99" class="mc md iq ly b gy me mf l mg mh">ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()</span></pre><p id="8bbb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下载 opencv-contrib 后，我们需要初始化选择性搜索。为此，我们增加了上述步骤。</p><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="fecd" class="mc md iq ly b gy me mf l mg mh">def get_iou(bb1, bb2):<br/>    assert bb1['x1'] &lt; bb1['x2']<br/>    assert bb1['y1'] &lt; bb1['y2']<br/>    assert bb2['x1'] &lt; bb2['x2']<br/>    assert bb2['y1'] &lt; bb2['y2']</span><span id="7001" class="mc md iq ly b gy mi mf l mg mh">    x_left = max(bb1['x1'], bb2['x1'])<br/>    y_top = max(bb1['y1'], bb2['y1'])<br/>    x_right = min(bb1['x2'], bb2['x2'])<br/>    y_bottom = min(bb1['y2'], bb2['y2'])</span><span id="ed80" class="mc md iq ly b gy mi mf l mg mh">    if x_right &lt; x_left or y_bottom &lt; y_top:<br/>        return 0.0</span><span id="0d57" class="mc md iq ly b gy mi mf l mg mh">    intersection_area = (x_right - x_left) * (y_bottom - y_top)</span><span id="5049" class="mc md iq ly b gy mi mf l mg mh">    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])<br/>    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])</span><span id="7765" class="mc md iq ly b gy mi mf l mg mh">    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)<br/>    assert iou &gt;= 0.0<br/>    assert iou &lt;= 1.0<br/>    return iou</span></pre><p id="9d86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们正在初始化函数，从通过选择性搜索计算出的盒子中计算出基础真值盒子的 IOU(交集/并集)。要了解更多关于计算欠条，你可以参考下面的链接。</p><blockquote class="lq lr ls"><p id="1d4c" class="jn jo lt jp b jq jr js jt ju jv jw jx lu jz ka kb lv kd ke kf lw kh ki kj kk ij bi translated"><a class="ae lp" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2016/11/07/intersection-over-union-iou-for-object-detection/</a></p></blockquote><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="301f" class="mc md iq ly b gy me mf l mg mh">train_images=[]<br/>train_labels=[]<br/>for e,i in enumerate(os.listdir(annot)):<br/>    try:<br/>        if i.startswith("airplane"):<br/>            filename = i.split(".")[0]+".jpg"<br/>            print(e,filename)<br/>            image = cv2.imread(os.path.join(path,filename))<br/>            df = pd.read_csv(os.path.join(annot,i))<br/>            gtvalues=[]<br/>            for row in df.iterrows():<br/>                x1 = int(row[1][0].split(" ")[0])<br/>                y1 = int(row[1][0].split(" ")[1])<br/>                x2 = int(row[1][0].split(" ")[2])<br/>                y2 = int(row[1][0].split(" ")[3])<br/>                gtvalues.append({"x1":x1,"x2":x2,"y1":y1,"y2":y2})<br/>            ss.setBaseImage(image)<br/>            ss.switchToSelectiveSearchFast()<br/>            ssresults = ss.process()<br/>            imout = image.copy()<br/>            counter = 0<br/>            falsecounter = 0<br/>            flag = 0<br/>            fflag = 0<br/>            bflag = 0<br/>            for e,result in enumerate(ssresults):<br/>                if e &lt; 2000 and flag == 0:<br/>                    for gtval in gtvalues:<br/>                        x,y,w,h = result<br/>                        iou = get_iou(gtval,{"x1":x,"x2":x+w,"y1":y,"y2":y+h})<br/>                        if counter &lt; 30:<br/>                            if iou &gt; 0.70:<br/>                                timage = imout[y:y+h,x:x+w]<br/>                                resized = cv2.resize(timage, (224,224), interpolation = cv2.INTER_AREA)<br/>                                train_images.append(resized)<br/>                                train_labels.append(1)<br/>                                counter += 1<br/>                        else :<br/>                            fflag =1<br/>                        if falsecounter &lt;30:<br/>                            if iou &lt; 0.3:<br/>                                timage = imout[y:y+h,x:x+w]<br/>                                resized = cv2.resize(timage, (224,224), interpolation = cv2.INTER_AREA)<br/>                                train_images.append(resized)<br/>                                train_labels.append(0)<br/>                                falsecounter += 1<br/>                        else :<br/>                            bflag = 1<br/>                    if fflag == 1 and bflag == 1:<br/>                        print("inside")<br/>                        flag = 1<br/>    except Exception as e:<br/>        print(e)<br/>        print("error in "+filename)<br/>        continue</span></pre><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/17750987d9a8c0a0c8ff5ff6c930f66c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*vYlZHF09gD-Ungd_-rSg9g.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Running selective search on an image and getting proposed regions</figcaption></figure><p id="2369" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的代码是预处理和创建要传递给模型的数据集。在这种情况下，我们可以有两个类。这些类别是提议的区域可以是前景(即飞机)还是背景。所以我们将前景(即飞机)的标签设为 1，背景的标签设为 0。在上面的代码块中执行以下步骤。</p><ol class=""><li id="ea1a" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">循环浏览图像文件夹，使用代码<strong class="jp ir"> ss.setBaseImage(image) </strong>将每个图像逐一设置为选择性搜索的基础</li><li id="68ed" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">使用代码<strong class="jp ir">ss . switchtoselectvesearchfast()</strong>和<strong class="jp ir"> ssresults = ss.process() </strong>初始化快速选择性搜索并获得建议区域</li><li id="e85d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">迭代通过选择性搜索传递的所有前 2000 个结果，并使用上面创建的<strong class="jp ir"> get_iou() </strong>函数计算建议区域和注释区域的 IOU。</li><li id="95bc" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">现在，由于一个图像可以有许多负样本(即背景)和一些正样本(即飞机)，所以我们需要确保我们有良好的正样本和负样本的比例来训练我们的模型。因此，我们设定从一幅图像中最多收集 30 个阴性样本(即背景)和阳性样本(即飞机)。</li></ol><p id="6d20" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行上面的代码片段后，我们的训练数据就准备好了。List <strong class="jp ir"> train_images=[] </strong>将包含所有图像，而<strong class="jp ir"> train_labels=[] </strong>将包含所有标记飞机图像为 1、非飞机图像(即背景图像)为 0 的标签。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/48cb3e957d74199fb55d8ba3a765717e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*jghiCMSgk_lSo9q-lBc2Vw.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Positive sample on right, Negative sample on left</figcaption></figure><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="bac4" class="mc md iq ly b gy me mf l mg mh">X_new = np.array(train_images)<br/>y_new = np.array(train_labels)</span></pre><p id="f034" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在完成创建数据集的过程后，我们将把数组转换成 numpy 数组，这样我们就可以轻松地遍历它，并以有效的方式将 datatset 传递给模型。</p><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="510f" class="mc md iq ly b gy me mf l mg mh">from keras.layers import Dense<br/>from keras import Model<br/>from keras import optimizers<br/>from keras.preprocessing.image import ImageDataGenerator<br/>from keras.optimizers import Adam</span><span id="b7a3" class="mc md iq ly b gy mi mf l mg mh">from keras.applications.vgg16 import VGG16<br/>vggmodel = VGG16(weights='imagenet', include_top=True)</span></pre><p id="39ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们将对 imagenet 权重进行迁移学习。我们将导入 VGG16 模型，并将图像净重放入模型中。要了解更多关于迁移学习的内容，你可以参考下面链接上的文章。</p><blockquote class="lq lr ls"><p id="9011" class="jn jo lt jp b jq jr js jt ju jv jw jx lu jz ka kb lv kd ke kf lw kh ki kj kk ij bi translated"><a class="ae lp" href="https://medium.com/@1297rohit/transfer-learning-from-scratch-using-keras-339834b153b9" rel="noopener">https://medium . com/@ 1297 rohit/transfer-learning-from-scratch-using-keras-339834 b 153 b 9</a></p></blockquote><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="2b8d" class="mc md iq ly b gy me mf l mg mh">for layers in (vggmodel.layers)[:15]:<br/>    print(layers)<br/>    layers.trainable = False</span><span id="d6c3" class="mc md iq ly b gy mi mf l mg mh">X= vggmodel.layers[-2].output<br/>predictions = Dense(2, activation="softmax")(X)<br/>model_final = Model(input = vggmodel.input, output = predictions)<br/>opt = Adam(lr=0.0001)<br/>model_final.compile(loss = keras.losses.categorical_crossentropy, optimizer = opt, metrics=["accuracy"])<br/>model_final.summary()</span></pre><p id="236d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在循环的这一部分，我们冻结了模型的前 15 层。之后，我们取出模型的倒数第二层，然后添加一个<strong class="jp ir"> 2 单位的 softmax 密集层</strong>，因为我们只有 2 个类要预测，即前景或背景。之后，我们使用<strong class="jp ir"> Adam 优化器编译模型，学习率为 0.001 </strong>。我们使用<strong class="jp ir">分类交叉熵</strong>作为损失，因为模型的输出是分类的。最后，将使用 model_final.summary()打印模型摘要。下面附上总结的图片。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/378d5347099e1f42ec0f86ac37bef416.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*LDudqcvKXv6jw-rca70Frg.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Model Summary</figcaption></figure><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="7867" class="mc md iq ly b gy me mf l mg mh">from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelBinarizer</span><span id="a666" class="mc md iq ly b gy mi mf l mg mh">class MyLabelBinarizer(LabelBinarizer):<br/>    def transform(self, y):<br/>        Y = super().transform(y)<br/>        if self.y_type_ == 'binary':<br/>            return np.hstack((Y, 1-Y))<br/>        else:<br/>            return Y<br/>    def inverse_transform(self, Y, threshold=None):<br/>        if self.y_type_ == 'binary':<br/>            return super().inverse_transform(Y[:, 0], threshold)<br/>        else:<br/>            return super().inverse_transform(Y, threshold)</span><span id="f41e" class="mc md iq ly b gy mi mf l mg mh">lenc = MyLabelBinarizer()<br/>Y =  lenc.fit_transform(y_new)</span><span id="ae4d" class="mc md iq ly b gy mi mf l mg mh">X_train, X_test , y_train, y_test = train_test_split(X_new,Y,test_size=0.10)</span></pre><p id="89ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">创建模型后，现在我们需要将数据集分为训练集和测试集。在此之前，我们需要一次性编码标签。为此，我们使用了<strong class="jp ir"> MyLabelBinarizer() </strong>并对数据集进行编码。然后我们使用 sklearn 的<strong class="jp ir"> train_test_split </strong>来拆分数据集。我们保留 10%的数据集作为测试集，90%作为训练集。</p><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="31d7" class="mc md iq ly b gy me mf l mg mh">trdata = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)<br/>traindata = trdata.flow(x=X_train, y=y_train)<br/>tsdata = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)<br/>testdata = tsdata.flow(x=X_test, y=y_test)</span></pre><p id="56e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们将使用 Keras <strong class="jp ir"> ImageDataGenerator </strong>将数据集传递给模型。我们将对数据集进行一些增强，如水平翻转、垂直翻转和旋转，以增加数据集。</p><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="ecf0" class="mc md iq ly b gy me mf l mg mh">from keras.callbacks import ModelCheckpoint, EarlyStopping</span><span id="8d95" class="mc md iq ly b gy mi mf l mg mh">checkpoint = ModelCheckpoint("ieeercnn_vgg16_1.h5", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)</span><span id="9135" class="mc md iq ly b gy mi mf l mg mh">early = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')</span><span id="709c" class="mc md iq ly b gy mi mf l mg mh">hist = model_final.fit_generator(generator= traindata, steps_per_epoch= 10, epochs= 1000, validation_data= testdata, validation_steps=2, callbacks=[checkpoint,early])</span></pre><p id="dd19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们使用<strong class="jp ir"> fit_generator </strong>开始模型的训练。</p><pre class="la lb lc ld gt lx ly lz ma aw mb bi"><span id="28eb" class="mc md iq ly b gy me mf l mg mh">z=0<br/>for e,i in enumerate(os.listdir(path)):<br/>    if i.startswith("4"):<br/>        z += 1<br/>        img = cv2.imread(os.path.join(path,i))<br/>        ss.setBaseImage(img)<br/>        ss.switchToSelectiveSearchFast()<br/>        ssresults = ss.process()<br/>        imout = img.copy()<br/>        for e,result in enumerate(ssresults):<br/>            if e &lt; 2000:<br/>                x,y,w,h = result<br/>                timage = imout[y:y+h,x:x+w]<br/>                resized = cv2.resize(timage, (224,224), interpolation = cv2.INTER_AREA)<br/>                img = np.expand_dims(resized, axis=0)<br/>                out= model_final.predict(img)<br/>                if out[0][0] &gt; 0.70:<br/>                    cv2.rectangle(imout, (x, y), (x+w, y+h), (0, 255, 0), 1, cv2.LINE_AA)<br/>        plt.figure()<br/>        plt.imshow(imout)<br/>        break</span></pre><p id="6c9c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在一旦我们创建了模型。我们需要对那个模型做预测。为此，我们需要遵循以下步骤:</p><ol class=""><li id="7692" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">传递选择性搜索的图像。</li><li id="b99e" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">使用<strong class="jp ir"> model_final.predict(img) </strong>将选择性搜索的所有结果作为输入传递给模型。</li><li id="e5c1" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果模型的输出表明该区域是前景图像(即飞机图像),并且如果置信度高于定义的阈值，则在建议区域的坐标上在原始图像上创建边界框。</li></ol><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/3b136165cc281d8a1e3714e93958ffcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*PQi5b_so_1J86G_xfJT45Q.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Output of the model</figcaption></figure><p id="5e36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如你在上面看到的，我们在建议的区域创建了一个盒子，在这个盒子里，模型的精度超过了 0.70。这样，我们可以在图像上进行定位，并使用 R-CNN 执行对象检测。这就是我们如何使用 keras 从零开始实现 R-CNN 架构。</p><p id="7d54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以从下面提供的链接中获得完全实现的 R-CNN。</p><blockquote class="lq lr ls"><p id="4e16" class="jn jo lt jp b jq jr js jt ju jv jw jx lu jz ka kb lv kd ke kf lw kh ki kj kk ij bi translated"><a class="ae lp" href="https://github.com/1297rohit/RCNN.git" rel="noopener ugc nofollow" target="_blank">https://github.com/1297rohit/RCNN.git</a></p></blockquote><p id="d030" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想从头开始一步一步地学习<strong class="jp ir">人脸检测和人脸识别</strong>，那么你可以点击链接阅读我的文章:<a class="ae lp" href="https://medium.com/@1297rohit/step-by-step-face-recognition-code-implementation-from-scratch-in-python-cc95fa041120" rel="noopener">https://medium . com/@ 1297 rohit/step-step-step-Face-Recognition-code-implementation-from-scratch-in-python-cc 95 fa 041120</a></p><p id="b3f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">享受 R-CNN！</p></div></div>    
</body>
</html>