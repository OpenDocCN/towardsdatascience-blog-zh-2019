<html>
<head>
<title>Monte Carlo Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">蒙特卡洛学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/monte-carlo-learning-b83f75233f92?source=collection_archive---------5-----------------------#2019-09-12">https://towardsdatascience.com/monte-carlo-learning-b83f75233f92?source=collection_archive---------5-----------------------#2019-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d0e03e0ade3e74c434c3e7ef18ea5e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*61rQGzv3je-93T5_XyHd-w.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="e192" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">使用蒙特卡罗方法的强化学习</h2></div><p id="f3da" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这篇文章中，我将介绍强化学习的蒙特卡罗方法。我在之前的文章中简要介绍了动态编程(值迭代和策略迭代)方法。在动态编程中，我们需要一个模型(代理知道 MDP 转换和奖励)，代理做<strong class="kv jf">计划</strong>(一旦模型可用，代理需要计划它在每个状态中的动作)。在动态规划方法中，主体没有真正的学习。</p><p id="f6b6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">另一方面，蒙特卡罗方法是一个非常简单的概念，当代理与环境交互时，代理学习状态和奖励。在这种方法中，代理生成经验样本，然后基于平均回报，为状态或状态-动作计算值。下面是蒙特卡罗(MC)方法的主要特征:</p><ol class=""><li id="be04" class="lp lq je kv b kw kx kz la lc lr lg ls lk lt lo lu lv lw lx bi translated">没有模型(代理不知道状态 MDP 转换)</li><li id="b172" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">经纪人<strong class="kv jf">从<strong class="kv jf">那里学到</strong>被取样的</strong>经验</li><li id="2319" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">通过体验来自所有采样剧集的<strong class="kv jf">平均</strong>回报，学习策略π下的状态值 vπ(s )(值=平均回报)</li><li id="7eb7" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">仅在<strong class="kv jf">完成一集</strong>之后，值才被更新(因为该算法收敛缓慢，并且更新发生在一集完成之后)</li><li id="36bc" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">没有自举</li><li id="596a" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">仅可用于<strong class="kv jf">偶发性问题</strong></li></ol><p id="0d05" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">考虑一个真实生活的类比；蒙特卡洛学习就像年度考试，学生在年底完成它的一集。在这里，年度考试的结果就像是学生获得的回报。现在，如果问题的目标是找出学生在一个日历年(这是一个插曲)内对一个班级的分数，我们可以从一些学生的样本结果中提取，然后计算平均结果来找出一个班级的分数(不要逐点进行类比，但在整体水平上，我认为你可以获得 MC 学习的本质)。类似地，我们有 TD 学习或时间差异学习(TD 学习就像在每个时间步骤中更新值，不需要等到一集结束时更新值)，我们将在未来的博客中介绍，可以认为就像每周或每月的考试(学生可以在每个小间隔后根据这个分数(收到的奖励)调整他们的表现，最终分数是所有每周测试(总奖励)的累积)。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/4a2404c2ee49c7fe8c295413d518f31b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-asPt2CRBCi80NwL.png"/></div></div></figure><p id="6eea" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">价值函数=预期<strong class="kv jf">返回</strong></p><p id="f314" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">预期回报等于所有奖励的贴现总额。</p><p id="11e6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在蒙特卡罗方法中，我们使用代理人根据政策抽样的经验回报，而不是预期回报。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/0a8a3679f5847b1c2b7e1d64437e6986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/0*Mx-nOpbgmkXDoKmO.png"/></div></figure><p id="f682" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果我们回到我们收集宝石的第一个例子，代理遵循政策并完成一集，在每一步中它收集宝石形式的奖励。为了获得状态值，代理从该状态开始，在每一集后汇总收集的所有宝石。参考下图，其中从状态 S 05 开始收集了 3 个样本。每集收集的总奖励(为简单起见，折扣系数被视为 1)如下:</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/839733e352e19acbd7ff302774cdacb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8H_U9U2SYes8MhlA.png"/></div></div></figure><p id="280f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Return(样本 01) = 2 + 1 + 2 + 2 + 1 + 5 = 13 颗宝石</p><p id="6eda" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Return(样本 02) = 2 + 3 + 1 + 3 + 1 + 5 = 15 颗宝石</p><p id="5e0d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Return(样本 03) = 2 + 3 + 1 + 3 + 1 + 5 = 15 颗宝石</p><p id="c085" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">观察到的平均回报(基于 3 个样本)= (13 + 15 + 15)/3 = 14.33 宝石</p><p id="9b95" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因此，按照蒙特卡罗方法，v π(S 05)的值是 14.33 宝石，基于遵循政策π的 3 个样本。</p><h1 id="dc2b" class="mj mk je bd ml mm mn mo mp mq mr ms mt kk mu kl mv kn mw ko mx kq my kr mz na bi translated">蒙特卡洛备份图</h1><p id="8525" class="pw-post-body-paragraph kt ku je kv b kw nb kf ky kz nc ki lb lc nd le lf lg ne li lj lk nf lm ln lo im bi translated">蒙特卡洛备份图如下所示(参考<a class="ae ng" href="https://baijayantaroy.github.io/baijayantaroy.github.io/Reinforcement_Learning_Series_03_backup_diagram/" rel="noopener ugc nofollow" target="_blank">第三篇博客</a>帖子了解更多备份图)。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/ecc61e45f09a386670e2dc789b1dc201.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_BZ7de2MQZHLUGud.png"/></div></div></figure><p id="0938" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">有两种类型的 MC 学习策略评估(预测)方法:</p><h1 id="d67e" class="mj mk je bd ml mm mn mo mp mq mr ms mt kk mu kl mv kn mw ko mx kq my kr mz na bi translated">首次访问蒙特卡罗方法</h1><p id="dbbc" class="pw-post-body-paragraph kt ku je kv b kw nb kf ky kz nc ki lb lc nd le lf lg ne li lj lk nf lm ln lo im bi translated">在这种情况下，在一个事件中，对该状态的首次访问进行计数(即使代理在该事件中多次回到同一状态，也只对首次访问进行计数)。详细步骤如下:</p><ol class=""><li id="1844" class="lp lq je kv b kw kx kz la lc lr lg ls lk lt lo lu lv lw lx bi translated">为了评估状态 s，首先我们设置访问次数，N(s) = 0，总回报 TR(s) = 0(这些值在每集中更新)</li><li id="9a52" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">状态 s 在一集内被访问的第一个<strong class="kv jf">时间步长 t，递增计数器 N(s) = N(s) + 1</strong></li><li id="f3b7" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">增量总回报 TR(s) = TR(s) + Gt</li><li id="cc10" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">价值通过平均回报 V(s) = TR(s)/N(s)来估算</li><li id="47dd" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">根据大数定律，当 N(s)接近无穷大时，V(s) -&gt; vπ(s)(这在策略π下称为真值)</li></ol><p id="2719" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">请参考下图，以便更好地理解计数器增量。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/5b6cde0f2377f576ef2c84a246d8c845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7OfkMiUYKF-YehrC.png"/></div></div></figure><h1 id="51d4" class="mj mk je bd ml mm mn mo mp mq mr ms mt kk mu kl mv kn mw ko mx kq my kr mz na bi translated">每次访问蒙特卡罗方法</h1><p id="b3e4" class="pw-post-body-paragraph kt ku je kv b kw nb kf ky kz nc ki lb lc nd le lf lg ne li lj lk nf lm ln lo im bi translated">在这种情况下，在一个事件中，国家的每次访问都被计算在内。详细步骤如下:</p><ol class=""><li id="c6b3" class="lp lq je kv b kw kx kz la lc lr lg ls lk lt lo lu lv lw lx bi translated">为了评估状态 s，首先我们设置访问次数，N(s) = 0，总回报 TR(s) = 0(这些值在每集中更新)</li><li id="9cd8" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated"><strong class="kv jf">每个</strong>时间步长 t，状态 s 在一个情节中被访问，递增计数器 N(s) = N(s) + 1</li><li id="b5a3" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">增量总回报 TR(s) = TR(s) + Gt</li><li id="518c" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">价值通过平均回报 V(s) = TR(s)/N(s)来估算</li><li id="ce83" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">根据大数定律，当 N(s)接近无穷大时，V(s) -&gt; vπ(s)(这在策略π下称为真值)</li></ol><p id="e20b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">请参考下图，以便更好地理解计数器增量。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/372acc1be5fd488ffa210ef299bd1e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dzOaC4UyiB_X0Vf2.png"/></div></div></figure><p id="a780" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">通常 MC 在每次发作后递增更新(不需要存储旧的发作值，它可以是每次发作后更新的状态的运行平均值)。</p><p id="97b5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在第一集，第二集，第三集，…，S t 为每个状态 S T，返回 G t</p><p id="a9fe" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">通常用一个恒定的学习速率(α)来代替 1/N(S t ),上述等式变为:</p><p id="b022" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">对于策略改进，使用广义策略改进概念，通过蒙特卡罗方法的动作值函数来更新策略。</p><p id="26e1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">蒙特卡罗方法有以下<strong class="kv jf">个优点</strong>:</p><ul class=""><li id="050d" class="lp lq je kv b kw kx kz la lc lr lg ls lk lt lo ni lv lw lx bi translated">零偏差</li><li id="2ade" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo ni lv lw lx bi translated">良好的收敛特性(即使是函数逼近)</li><li id="0229" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo ni lv lw lx bi translated">对初始值不太敏感</li><li id="ce1a" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo ni lv lw lx bi translated">非常容易理解和使用</li></ul><p id="fddc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">但是它也有下面的<strong class="kv jf">限制</strong>:</p><ul class=""><li id="d230" class="lp lq je kv b kw kx kz la lc lr lg ls lk lt lo ni lv lw lx bi translated">MC 必须等到剧集结束后才知道回归</li><li id="a651" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo ni lv lw lx bi translated">MC 具有高方差</li><li id="eb35" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo ni lv lw lx bi translated">MC 只能从完整的序列中学习</li><li id="1087" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo ni lv lw lx bi translated">MC 仅适用于偶发(终止)环境</li></ul><p id="ebe5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">尽管 MC 方法需要时间，但对于任何强化学习实践者来说，它都是一个重要的工具。</p><h2 id="6f5e" class="nj mk je bd ml nk nl dn mp nm nn dp mt lc no np mv lg nq nr mx lk ns nt mz nu bi translated">感谢阅读。可以联系我@ <a class="ae ng" href="http://www.linkedin.com/in/baijayantaroy" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>。</h2></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="6476" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">只需每月 5 美元，就可以无限制地获取最鼓舞人心的内容…点击下面的链接，成为媒体会员，支持我的写作。谢谢大家！<br/><a class="ae ng" href="https://baijayanta.medium.com/membership" rel="noopener"><strong class="kv jf"><em class="oc">https://baijayanta.medium.com/membership</em></strong></a></p></div></div>    
</body>
</html>