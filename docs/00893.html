<html>
<head>
<title>Explaining Feature Importance by example of a Random Forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">以随机森林为例解释特征的重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e?source=collection_archive---------3-----------------------#2019-02-11">https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e?source=collection_archive---------3-----------------------#2019-02-11</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><figure class="iu iv gp gr iw ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi it"><img src="../Images/6a4e0d6e0d5fce749da21fc8d9806224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1CAaJystqYg4Z-2R3F6GfA.jpeg"/></div></div><figcaption class="je jf gj gh gi jg jh bd b be z dk">Source: <a class="ae ji" href="https://unsplash.com/photos/BPbIWva9Bgo" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/BPbIWva9Bgo</a></figcaption></figure><div class=""/><div class=""><h2 id="73fc" class="pw-subtitle-paragraph ki jk jl bd b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dk translated">了解 Python 中最流行的确定要素重要性的方法</h2></div><p id="18dd" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">在许多(商业)案例中，不仅有一个准确的，而且有一个可解释的模型是同样重要的。通常，除了想知道我们的模型的房价预测是什么，我们还想知道为什么会这么高/低，以及在确定预测时哪些特征是最重要的。另一个例子可能是预测客户流失——拥有一个成功预测哪些客户容易流失的模型是非常好的，但是确定哪些变量是重要的可以帮助我们及早发现，甚至改进产品/服务！</p><p id="cc63" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">了解由机器学习模型指示的特征重要性可以在多个方面使您受益，例如:</p><ul class=""><li id="fb17" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">通过更好地理解模型的逻辑，你不仅可以验证它是正确的，还可以通过只关注重要的变量来改进模型</li><li id="6bc9" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">以上内容可用于变量选择——您可以删除不太重要的<em class="mk"> x </em>变量，这些变量在更短的训练时间内具有相似或更好的性能</li><li id="881a" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">在一些商业案例中，为了可解释性而牺牲一些准确性是有意义的。例如，当一家银行拒绝一项贷款申请时，它也必须有一个决策背后的理由，这个理由也可以呈现给客户</li></ul><p id="b6bd" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">这就是为什么在这篇文章中，我想通过一个随机森林模型的例子来探索解释特性重要性的不同方法。它们中的大部分也适用于不同的模型，从线性回归开始，到黑盒(如 XGBoost)结束。</p><p id="178e" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">需要注意的一点是，我们的模型越精确，我们就越能相信特征重要性度量和其他解释。我假设我们构建的模型相当准确(因为每个数据科学家都会努力拥有这样的模型)，在本文中，我将重点关注重要性度量。</p><h1 id="7a50" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">数据</h1><p id="a103" class="pw-post-body-paragraph la lb jl lc b ld nd km lf lg ne kp li lj nf ll lm ln ng lp lq lr nh lt lu lv io bi translated">对于这个例子，我将使用波士顿房价数据集(因此是一个回归问题)。但是本文描述的方法同样适用于分类问题，唯一的区别是用于评估的度量标准。</p><figure class="ni nj nk nl gt ix"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="1a33" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">准备数据时唯一不标准的事情是向数据集添加一个随机列。从逻辑上讲，它对因变量(以 1000 美元为单位的自有住房的中值)没有预测能力，因此它不应该是模型中的一个重要特征。让我们看看结果如何。</p><p id="2cd3" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">下面我考察随机特征和目标变量之间的关系。可以观察到，散点图上没有模式，相关性几乎为 0。</p><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi no"><img src="../Images/baccdf68ac5d0829e1606c8f000c4cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3_0NELtdjq4AkQRhvSGRkQ.png"/></div></div></figure><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi np"><img src="../Images/592639aabd8dddef0cd3d13ef6cfbd27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WDXbi5bCd2vykeBXjj91sA.png"/></div></div></figure><p id="f596" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">这里需要注意的一点是，解释<code class="fe nq nr ns nt b">CHAS</code>的相关性没有多大意义，因为它是一个二元变量，应该对它使用不同的方法。</p><h1 id="941b" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">基准模型</h1><p id="cd56" class="pw-post-body-paragraph la lb jl lc b ld nd km lf lg ne kp li lj nf ll lm ln ng lp lq lr nh lt lu lv io bi translated">我训练了一个简单的随机森林模型来建立一个基准。我设置了一个<code class="fe nq nr ns nt b">random_state</code>来确保结果的可比性。我还使用了 bootstrap 并设置了<code class="fe nq nr ns nt b">oob_score = True</code>,这样我以后就可以使用 out-of-bag 错误。</p><p id="3fe1" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">简而言之，关于出袋误差的主题，随机森林中的每棵树都在不同的数据集上训练，用原始数据的替换进行采样。这导致每个训练集中大约 2/3 的不同观察。在所有的观察值上计算出袋外误差，但是为了计算每行的误差，模型只考虑在训练期间没有看到该行的树。这类似于在验证集上评估模型。你可以在这里阅读更多<a class="ae ji" href="https://stackoverflow.com/questions/18541923/what-is-out-of-bag-error-in-random-forests" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="ni nj nk nl gt ix"><div class="bz fp l di"><div class="nm nn l"/></div></figure><pre class="ni nj nk nl gt nu nt nv nw aw nx bi"><span id="83db" class="ny mm jl nt b gy nz oa l ob oc">R^2 Training Score: 0.93 <br/>OOB Score: 0.58 <br/>R^2 Validation Score: 0.76</span></pre><p id="0989" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">嗯，模型中有一些过度拟合，因为它在 OOB 样本和验证集上表现得更差。但是，让我们说它足够好，并前进到特征重要性(根据训练集性能来衡量)。一些方法也可以用于验证/OOB 集，以获得对未知数据的进一步解释能力。</p><h1 id="39b9" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">1.总体功能重要性</h1><p id="4f70" class="pw-post-body-paragraph la lb jl lc b ld nd km lf lg ne kp li lj nf ll lm ln ng lp lq lr nh lt lu lv io bi translated">我所说的总体特征重要性是指在模型层次上得到的特征，<em class="mk">即</em>，也就是说在一个给定的模型中，这些特征在解释目标变量时是最重要的。</p><h1 id="776d" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">1.1.默认 Scikit-learn 的功能重要性</h1><p id="0a9e" class="pw-post-body-paragraph la lb jl lc b ld nd km lf lg ne kp li lj nf ll lm ln ng lp lq lr nh lt lu lv io bi translated">让我们从决策树开始，建立一些直觉。在决策树中，每个节点都是如何在单个特征中分割值的条件，以便因变量的相似值在分割后最终出现在同一集合中。该条件基于杂质，在分类问题的情况下是基尼杂质/信息增益(熵)，而对于回归树是其方差。因此，当训练一棵树时，我们可以计算每个特征对减少加权杂质的贡献。<code class="fe nq nr ns nt b">feature_importances_</code>在 Scikit 中——Learn 是基于这种逻辑，但是在随机森林的情况下，我们讨论的是对树中杂质的减少进行平均。</p><p id="1865" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">优点:</p><ul class=""><li id="a99b" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">快速计算</li><li id="e239" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">易于检索—一个命令</li></ul><p id="c42f" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">缺点:</p><ul class=""><li id="2ce6" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">有偏见的方法，因为它倾向于夸大连续特征或高基数分类变量的重要性</li></ul><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi od"><img src="../Images/bb683a234f7b4e5a4ee822894e9fdf35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pgPE9pfZhsSiGY8CtANGhw.png"/></div></div></figure><p id="d31b" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">似乎前 3 个最重要的特性是:</p><ul class=""><li id="6e4b" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">房间的平均数量</li><li id="82ad" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">%较低的人口地位</li><li id="146f" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">到五个波士顿就业中心的加权距离</li></ul><p id="34fd" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">然而令人惊讶的是，一列随机值竟然比:</p><ul class=""><li id="9ba8" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">每个城镇非零售商业用地的比例</li><li id="f6e6" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">放射状公路可达性指数</li><li id="ebfe" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">面积超过 25，000 平方英尺的住宅用地比例</li><li id="7e5f" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">查尔斯河虚拟变量(= 1，如果区域边界为河流；否则为 0)</li></ul><p id="f494" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">直觉上，这个特征对目标变量的重要性应该为零。让我们看看不同的方法是如何评价它的。</p><h1 id="26b5" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">1.2.置换特征重要性</h1><p id="a95a" class="pw-post-body-paragraph la lb jl lc b ld nd km lf lg ne kp li lj nf ll lm ln ng lp lq lr nh lt lu lv io bi translated">这种方法通过观察每个预测器的随机重排(从而保持变量的分布)如何影响模型性能来直接测量特征重要性。</p><p id="13f5" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">该方法可以用以下步骤来描述:</p><ol class=""><li id="fef3" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv oe mc md me bi translated">训练基线模型，并通过验证集(或随机森林情况下的 OOB 集)记录分数(准确性/R/任何重要性度量)。这也可以在训练集上完成，代价是牺牲关于泛化的信息。</li><li id="5c01" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv oe mc md me bi translated">重新排列所选数据集中一个要素的值，将数据集再次传递给模型以获得预测，并计算此修改后的数据集的度量。特征重要性是基准分数和来自修改(置换)数据集的分数之间的差异。</li><li id="b9b8" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv oe mc md me bi translated">重复 2。对于数据集中的所有要素。</li></ol><p id="deca" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">优点:</p><ul class=""><li id="3340" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">适用于任何型号</li><li id="732c" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">相当有效</li><li id="d6f0" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">可靠的技术</li><li id="a710" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">不需要在每次修改数据集时重新训练模型</li></ul><p id="6e5b" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">缺点:</p><ul class=""><li id="1bc8" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">比默认的<code class="fe nq nr ns nt b">feature_importances</code>计算量更大</li><li id="33ee" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">排列重要性高估了相关预测值的重要性——施特罗布尔<em class="mk">等</em> (2008)</li></ul><p id="076e" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">至于这个方法的第二个问题，我已经在上面画出了相关矩阵。然而，我将使用我使用的库中的一个函数来可视化 Spearman 的相关性。标准皮尔逊相关的区别在于，它首先将变量转换为等级，然后才对等级进行皮尔逊相关。</p><p id="9459" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">斯皮尔曼关联:</p><ul class=""><li id="32e9" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">是非参数的</li><li id="9bbc" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">不假设变量之间的线性关系</li><li id="0a8f" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">它寻找单调的关系。</li></ul><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi of"><img src="../Images/4b4c515999b97de73c21073fe1873065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f3kSY7QiJFHEPfD4sLz9DQ.png"/></div></div></figure><p id="b185" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">我找到了两个具有这种功能的库，并不是说它很难编码。让我们来看一下这两种，因为它们有一些独特的特点。</p><p id="a31a" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated"><code class="fe nq nr ns nt b">rfpimp</code></p><p id="7c53" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">关于这个库需要注意的一点是，我们必须提供一个度量作为表单<code class="fe nq nr ns nt b">metric(model, X, y)</code>的函数。这样，我们可以使用更高级的方法，如使用随机森林的 OOB 分数。该库已经包含该功能(<code class="fe nq nr ns nt b">oob_regression_r2_score)</code>)。但是为了保持方法的一致性，我将在训练集上计算度量(丢失关于泛化的信息)。</p><figure class="ni nj nk nl gt ix"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi og"><img src="../Images/ab11714699e762f702fac04b7027a3e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Z2nFsfF_H1ah71fL8e-1g.png"/></div></div></figure><p id="2ee2" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">这个图证实了我们在上面看到的，4 个变量没有一个随机变量重要！令人惊讶的是…前四名仍然保持不变。关于<code class="fe nq nr ns nt b">rfpimp</code>的一个更好的特性是它包含了处理共线特性问题的功能(这是展示 Spearman 相关矩阵背后的想法)。为了简洁，我不会在这里展示这个案例，但是你可以在这个库的作者写的这篇伟大的文章中读到更多。</p><p id="07a6" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated"><code class="fe nq nr ns nt b">eli5</code></p><p id="51ed" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">与<code class="fe nq nr ns nt b">rfpimp</code>的基本方法和<code class="fe nq nr ns nt b">eli5</code>中使用的方法有一些不同。其中一些是:</p><ul class=""><li id="2942" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">有与使用交叉验证相关的参数<code class="fe nq nr ns nt b">cv</code>和<code class="fe nq nr ns nt b">refit</code>。在本例中，我将它们设置为<code class="fe nq nr ns nt b">None</code>，因为我并不使用它，但在某些情况下它可能会派上用场。</li><li id="bf99" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">有一个<code class="fe nq nr ns nt b">metric</code>参数，它像在<code class="fe nq nr ns nt b">rfpimp</code>中一样接受一个<code class="fe nq nr ns nt b">metric(model, X, y)</code>形式的函数。如果未指定该参数，该函数将使用估计器的默认<code class="fe nq nr ns nt b">score</code>方法。</li><li id="c8f2" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe nq nr ns nt b">n_iter</code> -随机洗牌迭代次数，最终得分为平均值</li></ul><figure class="ni nj nk nl gt ix"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi og"><img src="../Images/6b6cd2c22751ce448cb3a887f9047145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AwQyZhUqWAb7TBndrdqnYA.png"/></div></div></figure><p id="f1b9" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">结果与前面的非常相似，尽管这些结果来自每列的多次重新排列。关于<code class="fe nq nr ns nt b">eli5</code>的一个额外的好处是，通过使用 Scikit-learn 的<code class="fe nq nr ns nt b">SelectFromModel</code>或<code class="fe nq nr ns nt b">RFE</code>，使用置换方法的结果来执行特征选择真的很容易。</p><h1 id="cfb0" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">1.3.删除列功能重要性</h1><p id="70f1" class="pw-post-body-paragraph la lb jl lc b ld nd km lf lg ne kp li lj nf ll lm ln ng lp lq lr nh lt lu lv io bi translated">这种方法非常直观，因为我们通过比较具有所有特性的模型和不具有该特性的模型来研究特性的重要性。</p><p id="c004" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">我为下面的方法创建了一个函数(基于<code class="fe nq nr ns nt b">rfpimp</code>的实现),展示了底层逻辑。</p><p id="f31b" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">优点:</p><ul class=""><li id="1ed9" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">最准确的特征重要性</li></ul><p id="fbf3" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">缺点:</p><ul class=""><li id="005d" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">由于为数据集的每个变量重新训练模型(删除单个要素列后),潜在的高计算成本</li></ul><figure class="ni nj nk nl gt ix"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi oh"><img src="../Images/a68c5238cce257eed9e124164abf72cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UWexfBetrMD9XLD3yDTdXg.png"/></div></div></figure><p id="147c" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">这里变得有趣了。首先，负重要性，在这种情况下，意味着从模型中删除给定的特性实际上提高了性能。这在我们的随机变量中是很好的例子。</p><p id="b23e" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">或者，代替拟合模型的默认<code class="fe nq nr ns nt b">score</code>方法，我们可以使用出袋误差来评估特征重要性。为此，我们需要将上述要点中的<code class="fe nq nr ns nt b">score</code>方法替换为<code class="fe nq nr ns nt b">model.oob_score_</code>(记住对循环中的基准和模型都要这样做)。</p><h1 id="f495" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">2.观察级别特征重要性</h1><p id="eba0" class="pw-post-body-paragraph la lb jl lc b ld nd km lf lg ne kp li lj nf ll lm ln ng lp lq lr nh lt lu lv io bi translated">我所说的观察水平特征重要性是指对解释输入到模型中的特定观察有最大影响的特征。例如，在信用评分的情况下，我们可以说这些特征对确定客户的信用评分影响最大。</p><h1 id="abdf" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">2.1.树解释器</h1><p id="4025" class="pw-post-body-paragraph la lb jl lc b ld nd km lf lg ne kp li lj nf ll lm ln ng lp lq lr nh lt lu lv io bi translated"><code class="fe nq nr ns nt b">treeinterpreter</code>的主要思想是，它使用随机森林中的底层树来解释每个特征如何对最终值做出贡献。我们可以观察预测值(定义为每个特征贡献的总和+由基于整个训练集的初始节点给出的平均值)如何沿着决策树内的预测路径变化(在每次分割之后)，以及哪些特征导致了分割的信息(预测的变化也是如此)。</p><p id="be03" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">预测函数(f(x))的公式可以写为:</p><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/5c57081d5d2e745fe1bec106c11420b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*HCDEiJg3c4zMbZ1W1LPtFg.png"/></div></figure><p id="ddd1" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">其中 c_full 是整个数据集(初始结点)的平均值，K 是要素的总数。</p><p id="de83" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">这听起来可能很复杂，但是请看库的作者提供的一个例子:</p><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi oj"><img src="../Images/7b61b9c287490ad3b4bb22c712a39849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L_tT7cy_XulQUlqliZxwlg.png"/></div></div><figcaption class="je jf gj gh gi jg jh bd b be z dk">source: <a class="ae ji" href="http://blog.datadive.net/interpreting-random-forests/" rel="noopener ugc nofollow" target="_blank">http://blog.datadive.net/interpreting-random-forests/</a></figcaption></figure><p id="a94a" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">由于随机森林的预测是这些树的平均值，因此平均值预测的公式如下:</p><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi ok"><img src="../Images/ac193d62e408e2763a5c472fc865500e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cBaV_bLVW-gT6PkG5ve26A.png"/></div></div></figure><p id="1836" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">其中 J 是森林中树木的数量。</p><p id="4f52" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">我从识别具有最低和最高绝对预测误差的行开始，并试图找出造成差异的原因。</p><pre class="ni nj nk nl gt nu nt nv nw aw nx bi"><span id="a9e1" class="ny mm jl nt b gy nz oa l ob oc">Index with smallest error: 31<br/>Index with largest error: 85</span></pre><p id="abb9" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">使用<code class="fe nq nr ns nt b">treeintrerpreter</code>,我获得了 3 个对象:预测、偏差(数据集的平均值)和贡献。</p><figure class="ni nj nk nl gt ix"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="4fb4" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">对于误差最小的观察结果，主要贡献者是<code class="fe nq nr ns nt b">LSTAT</code>和<code class="fe nq nr ns nt b">RM</code>(在以前的案例中，它们被证明是最重要的变量)。在最高误差的情况下，最高贡献来自<code class="fe nq nr ns nt b">DIS</code>变量，克服了在第一种情况下起最重要作用的两个变量。</p><pre class="ni nj nk nl gt nu nt nv nw aw nx bi"><span id="c4c3" class="ny mm jl nt b gy nz oa l ob oc"><br/>Row 31<br/>Prediction: 21.996 Actual Value: 22.0<br/>Bias (trainset mean) 22.544297029702978<br/>Feature contributions:<br/>LSTAT 3.02<br/>RM -3.01<br/>PTRATIO 0.36<br/>AGE -0.29<br/>DIS -0.21<br/>random 0.18<br/>RAD -0.17<br/>NOX -0.16<br/>TAX -0.11<br/>CRIM -0.07<br/>B -0.05<br/>INDUS -0.02<br/>ZN -0.01<br/>CHAS -0.01<br/>--------------------<br/>Row 85<br/>Prediction: 36.816 Actual Value: 50.0<br/>Bias (trainset mean) 22.544297029702978<br/>Feature contributions:<br/>DIS 7.7<br/>LSTAT 3.33<br/>RM -1.88<br/>CRIM 1.87<br/>TAX 1.32<br/>NOX 1.02<br/>B 0.54<br/>CHAS 0.36<br/>PTRATIO -0.25<br/>RAD 0.17<br/>AGE 0.13<br/>INDUS -0.03<br/>random -0.01<br/>ZN 0.0<br/>---------------------</span></pre><p id="a6e2" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">为了更深入，我们可能还对许多变量的联合贡献感兴趣(如 XOR <a class="ae ji" href="http://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/" rel="noopener ugc nofollow" target="_blank">和这里的</a>的例子中所解释的)。我将直接看示例，更多信息可以在链接下找到。</p><figure class="ni nj nk nl gt ix"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="b856" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">最佳和最差预测案例之间的大部分差异来自房间数量(<code class="fe nq nr ns nt b">RM</code>)特征，以及到五个波士顿就业中心的加权距离(<code class="fe nq nr ns nt b">DIS</code>)。</p><h1 id="45df" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">2.2.石灰</h1><p id="7d2e" class="pw-post-body-paragraph la lb jl lc b ld nd km lf lg ne kp li lj nf ll lm ln ng lp lq lr nh lt lu lv io bi translated">LIME(局部可解释模型不可知解释)是一种以可解释和忠实的方式解释任何分类器/回归器预测的技术。为此，通过用一个可解释的模型(如带有正则化的线性模型或决策树)局部逼近所选模型来获得解释。可解释的模型是在原始观测值(表格数据情况下的 row)的小扰动(添加噪声)上训练的，因此它们仅提供良好的局部近似。</p><p id="6f23" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">需要注意的一些缺点:</p><ul class=""><li id="73eb" class="lw lx jl lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">只有线性模型用于近似局部行为</li><li id="a476" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">为了获得正确的解释，需要对数据进行的扰动类型通常是特定于用例的</li><li id="f8f4" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">简单的(默认)扰动通常是不够的。在理想情况下，修改将由数据集中观察到的变化驱动</li></ul><p id="8989" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">下面你可以看到 LIME 解释的输出。</p><p id="96d5" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">输出有 3 部分:<br/> 1。预测值<br/> 2。特征重要性-在回归的情况下，它显示它对预测有负面影响还是正面影响，按绝对影响降序排序。<br/> 3。所解释行的这些特性的实际值。</p><p id="c1b6" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">请注意，LIME 在解释中离散化了特征。这是因为在上面的构造函数中设置了<code class="fe nq nr ns nt b">discretize_continuous=True</code>。之所以离散化，是因为它给了连续的特征更直观的解释。</p><figure class="ni nj nk nl gt ix"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="ni nj nk nl gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi ol"><img src="../Images/7c53c548225a94043afc1272823b4f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nES1iEKEh84MOJoHfe9tcw.png"/></div></div></figure><p id="2436" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">LIME 解释同意，对于这两种观测，最重要的特征是<code class="fe nq nr ns nt b">RM</code>和<code class="fe nq nr ns nt b">LSTAT</code>，这也是之前的方法所指出的。</p><p id="92b9" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">我收到了一个有趣的问题:我们应该相信哪种观察水平的方法，因为结果可能会有所不同？这是一个没有明确答案的难题，因为这两种方法在概念上是不同的，因此很难直接比较。我建议你参考<a class="ae ji" href="https://stackoverflow.com/questions/48909418/lime-vs-treeinterpreter-for-interpreting-decision-tree/48975492" rel="noopener ugc nofollow" target="_blank">这个答案</a>，其中有一个类似的问题得到了解决并得到了很好的解释。</p><h1 id="79a2" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">结论</h1><p id="ae22" class="pw-post-body-paragraph la lb jl lc b ld nd km lf lg ne kp li lj nf ll lm ln ng lp lq lr nh lt lu lv io bi translated">在本文中，我展示了几种从机器学习模型(不限于随机森林)中获得特征重要性的方法。我认为理解结果通常与获得好的结果一样重要，因此每个数据科学家都应该尽最大努力理解哪些变量对模型最重要，以及为什么。这不仅有助于获得更好的业务理解，还可以导致模型的进一步改进。</p><p id="77bd" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">您可以在我的<a class="ae ji" href="https://github.com/erykml/medium_articles/blob/master/Machine%20Learning/feature_importance.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到本文使用的代码。一如既往，我们欢迎任何建设性的反馈。你可以在<a class="ae ji" href="https://twitter.com/erykml1?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">推特</a>或评论中联系我。</p><p id="c7a0" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">喜欢这篇文章吗？成为一个媒介成员，通过无限制的阅读继续学习。如果你使用<a class="ae ji" href="https://eryk-lewinson.medium.com/membership" rel="noopener">这个链接</a>成为会员，你将支持我，而不需要额外的费用。提前感谢，再见！</p><h1 id="a868" class="ml mm jl bd mn mo mp mq mr ms mt mu mv kr mw ks mx ku my kv mz kx na ky nb nc bi translated">参考</h1><ul class=""><li id="0afb" class="lw lx jl lc b ld nd lg ne lj om ln on lr oo lv mb mc md me bi translated"><a class="ae ji" href="https://explained.ai/rf-importance/index.html" rel="noopener ugc nofollow" target="_blank">小心默认随机森林重要性</a></li><li id="c411" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><a class="ae ji" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-307" rel="noopener ugc nofollow" target="_blank">随机森林的条件变量重要性</a></li><li id="75ea" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><a class="ae ji" href="http://blog.datadive.net/interpreting-random-forests/" rel="noopener ugc nofollow" target="_blank">解读随机森林</a></li><li id="7b54" class="lw lx jl lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><a class="ae ji" href="http://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/" rel="noopener ugc nofollow" target="_blank">随机森林解释—条件特征贡献</a></li></ul></div><div class="ab cl op oq hz or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="io ip iq ir is"><p id="4977" class="pw-post-body-paragraph la lb jl lc b ld le km lf lg lh kp li lj lk ll lm ln lo lp lq lr ls lt lu lv io bi translated">我最近出版了一本关于使用 Python 解决金融领域实际任务的书。如果你有兴趣，我贴了一篇文章<a class="ae ji" rel="noopener" target="_blank" href="/introducing-my-book-python-for-finance-cookbook-de219ca0d612">介绍这本书的内容。你可以在亚马逊或者 Packt 的网站上买到这本书。</a></p></div></div>    
</body>
</html>