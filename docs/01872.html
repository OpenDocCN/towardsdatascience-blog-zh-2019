<html>
<head>
<title>Word Vectors and Lexical Semantics — Introduction to Count-based Vectors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">词向量和词汇语义——基于计数的向量介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-vectors-and-lexical-semantics-part-1-c9dbc8932c1d?source=collection_archive---------23-----------------------#2019-03-27">https://towardsdatascience.com/word-vectors-and-lexical-semantics-part-1-c9dbc8932c1d?source=collection_archive---------23-----------------------#2019-03-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/c486882a82596ae9da0ed6a97db0e6ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0o1-LRO6ma12xDHr7nAgNg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Image from Ryan Haggins via Unsplash</figcaption></figure><div class=""/></div><div class="ab cl kf kg hx kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="im in io ip iq"><p id="00ea" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated"><em class="lk">以下是我基于 2017 年牛津大学举办的深度 NLP 课程的个人笔记。该材料可从[1]处获得。</em></p></div><div class="ab cl kf kg hx kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="im in io ip iq"><h1 id="ce6f" class="ll lm ji bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">介绍</h1><p id="b60f" class="pw-post-body-paragraph km kn ji ko b kp mj kr ks kt mk kv kw kx ml kz la lb mm ld le lf mn lh li lj im bi translated"><strong class="ko jj">单词向量</strong> : <em class="lk">用向量格式表示单词。</em></p><p id="75c7" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated"><strong class="ko jj">词汇语义学</strong> : <em class="lk">分析词义以及词义之间的关系。</em></p><p id="9aab" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">神经网络需要矢量表示作为输入。因此，需要将单词或句子转换成向量。</p><h1 id="0f4b" class="ll lm ji bd ln lo mo lq lr ls mp lu lv lw mq ly lz ma mr mc md me ms mg mh mi bi translated">代表单词</h1><p id="7fe8" class="pw-post-body-paragraph km kn ji ko b kp mj kr ks kt mk kv kw kx ml kz la lb mm ld le lf mn lh li lj im bi translated">文本仅仅是离散符号(即单词)的序列。一种简单的表示方法是对句子中的每个单词进行一次性编码。然而，这样做需要大量的内存/空间，因为向量空间(由一个热点编码向量组成)实际上就是你的词汇表的大小。</p><p id="06fe" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">这种方法的问题是，由于每个向量都被定义为一个单词，因此每个向量都以一种相当正交的方式表示，在语义上彼此没有明确(弱)的关系。他们彼此之间也很稀疏。因此，需要能够表达语义相似性的更丰富的表示。</p><h1 id="1359" class="ll lm ji bd ln lo mo lq lr ls mp lu lv lw mq ly lz ma mr mc md me ms mg mh mi bi translated">分布语义学</h1><p id="08c2" class="pw-post-body-paragraph km kn ji ko b kp mj kr ks kt mk kv kw kx ml kz la lb mm ld le lf mn lh li lj im bi translated"><strong class="ko jj">分布语义学</strong> : <em class="lk">发展和研究理论和方法的研究领域，这些理论和方法基于语言项在大量语言数据样本中的分布特性，对语言项之间的语义相似性进行量化和分类</em>【2】。</p><blockquote class="mt mu mv"><p id="7352" class="km kn lk ko b kp kq kr ks kt ku kv kw mw ky kz la mx lc ld le my lg lh li lj im bi translated"><em class="ji">“从一个人所交往的人身上，你就可以知道他所说的话”——J . R .弗斯(1957) </em></p></blockquote><p id="5425" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">上面的引用和其他类似的类比指出，可以通过观察人们如何使用单词来理解单词的意思。</p><p id="9c7b" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">同时，我们也对减少向量空间的大小感兴趣。这可以通过产生密集矢量表示(与稀疏相反)来实现。</p><p id="33d0" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">从计算角度来看，有 3 种主要方法可以做到这一点。</p><ol class=""><li id="08a8" class="mz na ji ko b kp kq kt ku kx nb lb nc lf nd lj ne nf ng nh bi translated">基于计数的</li><li id="55c9" class="mz na ji ko b kp ni kt nj kx nk lb nl lf nm lj ne nf ng nh bi translated">预言性的</li><li id="9804" class="mz na ji ko b kp ni kt nj kx nk lb nl lf nm lj ne nf ng nh bi translated">基于任务的</li></ol><p id="44ad" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">能够将单词指定为向量的优点是，可以开始客观地测量和比较单词向量，或者计算相似性、距离等。</p><p id="dbbd" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">让我们先介绍一下基于计数的方法。</p><h2 id="1ca0" class="nn lm ji bd ln no np dn lr nq nr dp lv kx ns nt lz lb nu nv md lf nw nx mh ny bi translated">计数法</h2><p id="b27b" class="pw-post-body-paragraph km kn ji ko b kp mj kr ks kt mk kv kw kx ml kz la lb mm ld le lf mn lh li lj im bi translated">定义要使用的基本词汇。通常它们是基于我们自己的经验/直觉或语料库的统计数据来选择的。这些词汇最好是信息丰富且有意义的。</p><p id="dc2a" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">通常词汇量是有限的。停用词通常被排除在外，因为它们在大多数可用的语料库中出现得很多。如果我们要包含它们，那么我们将很难确定它们之间的关系，因为停用词到处都在同时出现。</p><p id="4e2d" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">让我们看一个例子:</p><figure class="oa ob oc od gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/fc6d106dfd04e080a722830c44416693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Acp5YyF1l_FUg9fLdZw5XQ.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Credits to [1]</figcaption></figure><p id="7cb5" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">在上面的例子中，我们可以看到某些单词被选为我们感兴趣的基本词汇。还要注意像<em class="lk">、</em>、<em class="lk">、</em>这样的停用词不是基础词汇的一部分。</p><p id="a31d" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">确定了目标单词及其上下文之后，我们现在可以将其表示为一个向量(如下所示)。</p><figure class="oa ob oc od gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/903b6d307f00022aeee51d983987b3e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*wjQDx3xky_UbDWbf0eelJw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Example. Credits to 1. Note that the value doesn’t have to ONLY be 1.</figcaption></figure><p id="0a2d" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">作为向量，现在可以分析单词，可能通过相似性(计算相似性的最流行的方法是余弦距离)、向量空间中的距离等。</p><p id="0c21" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">然而，仍然有一些缺点:</p><ul class=""><li id="652a" class="mz na ji ko b kp kq kt ku kx nb lb nc lf nd lj of nf ng nh bi translated">不是所有的单词都具有同等的信息量，因为有些单词在不同的文本中出现的频率更高；并且由于这一点，不再能够唯一地与特定的上下文相关联。</li><li id="9f65" class="mz na ji ko b kp ni kt nj kx nk lb nl lf nm lj of nf ng nh bi translated">例如，在描述各种四条腿动物的文本中，单词<em class="lk">跑</em>或<em class="lk">四条腿</em>将无法区分语料库中的动物类型。</li><li id="9608" class="mz na ji ko b kp ni kt nj kx nk lb nl lf nm lj of nf ng nh bi translated">然而，它们是克服这一点的方法，如 TF-IDF 或 PMI。</li></ul><p id="b320" class="pw-post-body-paragraph km kn ji ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj im bi translated">在我的下一篇文章中，我们将探索一种更简单的方法来解决这些问题。</p></div><div class="ab cl kf kg hx kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="im in io ip iq"><h1 id="a355" class="ll lm ji bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">参考</h1><ol class=""><li id="0bbc" class="mz na ji ko b kp mj kt mk kx og lb oh lf oi lj ne nf ng nh bi translated"><a class="ae oj" href="https://github.com/oxford-cs-deepnlp-2017/lectures" rel="noopener ugc nofollow" target="_blank"><em class="lk">https://github.com/oxford-cs-deepnlp-2017/lectures</em></a></li><li id="c5e8" class="mz na ji ko b kp ni kt nj kx nk lb nl lf nm lj ne nf ng nh bi translated"><a class="ae oj" href="https://en.wikipedia.org/wiki/Distributional_semantics" rel="noopener ugc nofollow" target="_blank"><em class="lk">https://en.wikipedia.org/wiki/Distributional_semantics</em></a></li></ol></div></div>    
</body>
</html>