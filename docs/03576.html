<html>
<head>
<title>Different techniques to represent words as vectors (Word Embeddings)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将单词表示为向量的不同技术(单词嵌入)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/different-techniques-to-represent-words-as-vectors-word-embeddings-3e4b9ab7ceb4?source=collection_archive---------4-----------------------#2019-06-07">https://towardsdatascience.com/different-techniques-to-represent-words-as-vectors-word-embeddings-3e4b9ab7ceb4?source=collection_archive---------4-----------------------#2019-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1d09" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从计数矢量器到 Word2Vec</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2227c940131a50f555b18d01c59e718b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*plRZbBJ610ZTWkkh"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@rvignes?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Romain Vignes</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4ff4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目前，我正在做一个 Twitter 情感分析项目。在阅读如何向我的神经网络输入文本时，我发现我必须将每条推文的文本转换成指定长度的向量。这将允许神经网络对推文进行训练，并正确地学习情感分类。</p><p id="9dc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我简要地分析了将文本转换成向量的各种方法——通常称为单词嵌入。</p><blockquote class="lv lw lx"><p id="bd52" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">单词嵌入</strong>是自然语言处理(NLP)中一套语言建模和特征学习技术的统称，其中词汇表中的单词或短语被映射到实数向量。— <a class="ae ky" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></blockquote><p id="51a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将探索以下单词嵌入技术:</p><ol class=""><li id="5a37" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">计数矢量器</li><li id="02b1" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">TF-IDF 矢量器</li><li id="eccd" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">哈希矢量器</li><li id="9ee5" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">Word2Vec</li></ol><h1 id="d88a" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">示例文本数据</h1><p id="cdb1" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我正在创造 4 个句子，我们将在上面应用这些技术并理解它们是如何工作的。对于每种技术，我将只使用小写单词。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><h1 id="1367" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">计数矢量器</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/16e28dcfb94d282d534f373d6468876f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_TcP9GHAUJ-z_P2P"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@steve_j?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Steve Johnson</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7142" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将文本转换为矢量的最基本方法是通过计数矢量器。</p><p id="46e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 1: </strong>在完整的文本数据中识别唯一的单词。在我们的例子中，列表如下(17 个单词):</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="23fe" class="nv mr it nr b gy nw nx l ny nz">['ended', 'everyone', 'field', 'football', 'game', 'he', 'in', 'is', 'it', 'playing', 'raining', 'running', 'started', 'the', 'towards', 'was', 'while']</span></pre><p id="0adb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤 2: 对于每个句子，我们将创建一个长度与上面(17)相同的零数组</p><p id="dcaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第三步:一次看一个句子，我们将阅读第一个单词，找出它在句子中的总出现次数。一旦我们知道了这个词在句子中出现的次数，我们就可以确定这个词在上面列表中的位置，并在那个位置用这个计数替换同一个零。对所有的单词和句子重复这一过程</p><h2 id="37b0" class="nv mr it bd ms oa ob dn mw oc od dp na li oe of nc lm og oh ne lq oi oj ng ok bi translated">例子</h2><p id="97d2" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">就拿第一句话来说吧，<strong class="lb iu"> <em class="ly">他在打野战。</em> </strong>其向量为<code class="fe ol om on nr b">[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</code>。</p><p id="da95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个词是<code class="fe ol om on nr b">He</code>。它在句子中的总数是 1。还有，在上面的单词列表中，它的位置是从开始算起的第 6 位(都是小写)。我将更新它的向量，现在它将是:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6c39" class="nv mr it nr b gy nw nx l ny nz">[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span></pre><p id="e47c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑第二个词，也就是<code class="fe ol om on nr b">is</code>，向量变成:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9334" class="nv mr it nr b gy nw nx l ny nz">[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span></pre><p id="f838" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，我也将更新其余的单词，第一句话的向量表示将是:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="19a4" class="nv mr it nr b gy nw nx l ny nz">[0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]</span></pre><p id="ad9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样的情况也会在其他句子中重复出现。</p><h2 id="7d01" class="nv mr it bd ms oa ob dn mw oc od dp na li oe of nc lm og oh ne lq oi oj ng ok bi translated">密码</h2><p id="6f69" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated"><code class="fe ol om on nr b">sklearn</code>提供了 CountVectorizer()方法来创建这些单词嵌入。导入包后，我们只需要在完整的句子列表上应用<code class="fe ol om on nr b">fit_transform()</code>,就可以得到每个句子的向量数组。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="e5c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述要点中的输出显示了每个句子的向量表示。</p><h1 id="c166" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">TF-IDF 矢量器</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/f83c95e80a636d3fc034184dafb2d1c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*H4FH0aHqNTSBKqhq"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@officialdavery?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Devin Avery</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="08b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然计数矢量器将每个句子转换成它自己的矢量，但它不考虑单词在整个句子列表中的重要性。例如，<code class="fe ol om on nr b">He</code>出现在两个句子中，它不能提供区分这两个句子的有用信息。因此，它在句子的整个向量中应该具有较低的权重。这就是 TF-IDF 矢量器的用武之地。</p><p id="06d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TF-IDF 由两部分组成:</p><ol class=""><li id="cb60" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu"> TF(词频)</strong> —定义为一个词在给定句子中出现的次数。</li><li id="47de" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu"> IDF(逆文档频率)</strong> —它被定义为总文档数除以单词出现的文档数的以 e 为底的对数。</li></ol><p id="a55e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 1: </strong>识别完整文本数据中的唯一单词。在我们的例子中，列表如下(17 个单词):</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a26b" class="nv mr it nr b gy nw nx l ny nz">['ended', 'everyone', 'field', 'football', 'game', 'he', 'in', 'is', 'it', 'playing', 'raining', 'running', 'started', 'the', 'towards', 'was', 'while']</span></pre><p id="175b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 2: </strong>对于每个句子，我们将创建一个长度与上面(17)相同的零数组</p><p id="1267" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 3: </strong>对于每个句子中的每个单词，我们将计算 TF-IDF 值，并更新该句子的向量中的相应值</p><h2 id="ccce" class="nv mr it bd ms oa ob dn mw oc od dp na li oe of nc lm og oh ne lq oi oj ng ok bi translated">例子</h2><p id="7602" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们将首先为所有句子组合中的所有 17 个唯一单词定义一个零数组。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="32c4" class="nv mr it nr b gy nw nx l ny nz">[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span></pre><p id="266c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我就拿第一句的<code class="fe ol om on nr b">he</code>这个词，<strong class="lb iu"> <em class="ly">他在打野战</em> </strong>为它申请 TF-IDF。然后，该值将在句子的数组中更新，并对所有单词重复。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="294a" class="nv mr it nr b gy nw nx l ny nz">Total documents (N): 4<br/>Documents in which <!-- -->the word <!-- -->appears (n): 2<br/>Number of times the <!-- -->word<!-- --> appears in the first sentence: 1<br/>Number of words in the first sentence: 6</span><span id="7109" class="nv mr it nr b gy op nx l ny nz">Term Frequency(TF) = 1</span><span id="3875" class="nv mr it nr b gy op nx l ny nz">Inverse Document Frequency(IDF) = log(N/n)<br/>                                = log(4/2)<br/>                                = log(2)</span><span id="6d52" class="nv mr it nr b gy op nx l ny nz">TF-IDF value = 1 * log(2)<br/>             = 0.69314718</span></pre><p id="6111" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更新的向量:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="78b2" class="nv mr it nr b gy nw nx l ny nz">[0, 0, 0, 0, 0, 0<!-- -->.69314718<!-- -->, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span></pre><p id="5ef3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于所有其他单词，情况也是如此。但是，有些库可能使用不同的方法来计算该值。例如，<code class="fe ol om on nr b">sklearn</code>，计算逆文档频率为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3fae" class="nv mr it nr b gy nw nx l ny nz">IDF = (log(N/n)) + 1</span></pre><p id="5e95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，TF-IDF 值为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d719" class="nv mr it nr b gy nw nx l ny nz">TF-IDF value = 1 * (log(4/2) + 1)<br/>             = 1 * (log(2) + 1)<br/>             = 1.69314718</span></pre><p id="6f0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重复时，该过程将第一句的向量表示为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4536" class="nv mr it nr b gy nw nx l ny nz">[0, 0, 1.69314718, 0, 0, 1.69314718, 1.69314718, 1.69314718, 0, 1.69314718, 0, 0, 0, 1, 0, 0, 0]</span></pre><h2 id="0f2b" class="nv mr it bd ms oa ob dn mw oc od dp na li oe of nc lm og oh ne lq oi oj ng ok bi translated">密码</h2><p id="f871" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated"><code class="fe ol om on nr b">sklearn</code>提供了计算 TF-IDF 值的方法<code class="fe ol om on nr b">TfidfVectorizer</code>。但是，它对其应用了<code class="fe ol om on nr b">l2</code>规范化，我会忽略使用标志值<code class="fe ol om on nr b">None</code>并保持<code class="fe ol om on nr b">smooth_idf</code>标志为假，因此上面的方法被它用于 IDF 计算。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="080b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述要点中的输出显示了每个句子的向量表示。</p><h1 id="646c" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">哈希矢量器</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/0d32e9625dab497a209a10f0e51458e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XjxkuNOE_wGmbHEY"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@nhillier?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nick Hillier</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5c6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个矢量器非常有用，因为它允许我们将任何单词转换成它的散列，并且不需要生成任何词汇。</p><p id="066b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第一步:</strong>定义为每个句子创建的向量的大小</p><p id="1282" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 2: </strong>对句子应用哈希算法(比如 MurmurHash)</p><p id="0124" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 3: </strong>对所有句子重复步骤 2</p><h2 id="65e4" class="nv mr it bd ms oa ob dn mw oc od dp na li oe of nc lm og oh ne lq oi oj ng ok bi translated">密码</h2><p id="b221" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">由于这个过程只是一个散列函数的应用，我们可以简单地看一下代码。我将使用来自<code class="fe ol om on nr b">sklearn</code>的<code class="fe ol om on nr b">HashingVectorizer</code>方法。将规范化设置为“无”会将其删除。鉴于以上讨论的两种矢量化技术，我们在每个矢量中都有 17 列，我也将这里的要素数量设置为 17。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="18f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将生成必要的散列值向量。</p><h1 id="99ac" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">Word2Vec</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/7e511f1f6fc2c91fdb62bb10d6403e90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hyMWq_HRbljl2cvY"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@mahesh_ranaweera?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mahesh Ranaweera</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f681" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是一组神经网络模型，其目的是在向量空间中表示单词。这些模型在理解语境和词与词之间的关系方面是高效的。相似的单词在向量空间中被放置得很近，而不相似的单词被放置得很远。</p><p id="33c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">描述单词是如此令人惊奇，它甚至能够识别关键关系，例如:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="270a" class="nv mr it nr b gy nw nx l ny nz">King - Man + Woman = Queen</span></pre><p id="fdb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它能够解释男人对于国王，女人对于王后的意义。通过这些模型可以确定各自的关系。</p><p id="dd66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该课程有两种模式:</p><ol class=""><li id="3415" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu"> CBOW(连续单词包):</strong>神经网络查看周围的单词(比如左边 2 个和右边 2 个)，并预测中间的单词</li><li id="3399" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">跳跃图:</strong>神经网络接受一个单词，然后尝试预测周围的单词</li></ol><p id="e370" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络有一个输入层、一个隐藏层和一个输出层，用于对数据进行训练和构建向量。由于这是神经网络如何工作的基本功能，我将跳过一步一步的过程。</p><h2 id="417c" class="nv mr it bd ms oa ob dn mw oc od dp na li oe of nc lm og oh ne lq oi oj ng ok bi translated">密码</h2><p id="f2e5" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">为了实现<code class="fe ol om on nr b">word2vec</code>模型，我将使用<code class="fe ol om on nr b">gensim</code>库，它提供了模型中的许多特性，比如找出奇怪的一个，最相似的单词等等。然而，它没有小写/标记句子，所以我也这样做了。然后将标记化的句子传递给模型。我已经将 vector 的<code class="fe ol om on nr b">size</code>设置为 2，<code class="fe ol om on nr b">window</code>设置为 3，这定义了要查看的距离，<code class="fe ol om on nr b">sg</code> = 0 使用 CBOW 模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="b1d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我用<code class="fe ol om on nr b">most_similar</code>的方法找到所有和<code class="fe ol om on nr b">football</code>这个词相似的词，然后打印出最相似的。对于不同的培训，我们会得到不同的结果，但在我尝试的最后一个案例中，我得到了最相似的单词<code class="fe ol om on nr b">game</code>。这里的数据集只有 4 个句子。如果我们同样增加，神经网络将能够更好地找到关系。</p><h1 id="3ce6" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">结论</h1><p id="96d5" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们找到了。我们已经了解了单词嵌入的 4 种方式，以及如何使用代码来实现这一点。如果你有任何想法，想法和建议，请分享并告诉我。感谢阅读！</p></div></div>    
</body>
</html>