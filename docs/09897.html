<html>
<head>
<title>Dating Texts with Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用决策树确定文本的日期</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dating-texts-with-decision-trees-c716841a33f1?source=collection_archive---------21-----------------------#2019-12-27">https://towardsdatascience.com/dating-texts-with-decision-trees-c716841a33f1?source=collection_archive---------21-----------------------#2019-12-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3ce8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用决策树来估计一本书的写作年份</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6aceb0693c98959ef2460c26d705ea78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jNuxayEAWMyDS28mUAd4tA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@kiwihug?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Kiwihug</a> on <a class="ae kv" href="https://unsplash.com/s/photos/manuscripts?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="d113" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">介绍</h1><blockquote class="lv lw lx"><p id="d29e" class="ly lz ma mb b mc md jr me mf mg ju mh mi mj mk ml mm mn mo mp mq mr ms mt mu ij bi translated">如果我们使用机器学习来估计这本书的写作时间会怎么样？</p></blockquote><p id="20d9" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">想象你自己在你祖父母的旧阁楼里，仔细阅读布满灰尘的旧书架，这时一本不熟悉的大部头吸引了你的目光。这似乎是一部小说。你不知道它是什么时候写的，这个答案似乎连谷歌自己都不知道。你拍下其中一页的照片，然后使用文本提取器，获得文本的数字表示。如果我们使用机器学习来估计这本书的写作时间会怎么样？</p><p id="37e6" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">在这篇文章中，我们将讨论和实现一些方法来尝试确定文本的日期。除了所述的例子，这种类型的模型在一系列领域中有应用，从检测假新闻，到打击历史修正主义，到改善历史上准确的小说，到确定死后文学作品的真实性。</p><p id="f399" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">这项任务最容易解释的模型之一是决策树，它可以提供关于哪些类型的单词可以用来区分文本日期的有趣推论。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="858f" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">资料组</h1><p id="cb3c" class="pw-post-body-paragraph ly lz iq mb b mc my jr me mf mz ju mh mv na mk ml mw nb mo mp mx nc ms mt mu ij bi translated">我们的首要任务是收集数据进行训练。我们将要使用的数据集包含大约 200 个文本，按时间分布如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/4afdcd20460a0ed14dd2fb463c8e01e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wuoD4zqDvQmrM52cZe1VkA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Time spread of the dataset</figcaption></figure><p id="e211" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">我们将自己限制在这个时间段内，因为在一个更长的时间段内，英语的不同状态之间的绝对语言距离太大了(对于一个说现代英语的人来说，14 世纪的英语是难以理解的)。查找较旧的文本数据也是乏味的，因为许多较旧文本的可用版本只是用更近的、可理解的英语转录的(使用这些版本而没有适当的谨慎相当于给数据贴错标签)。我们的优先任务之一是确保在每个时间段，文本包括不同的主题，以防止我们的模型识别文本主题或格式，而不是纪元。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="670d" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">特征选择</h1><p id="5d0f" class="pw-post-body-paragraph ly lz iq mb b mc my jr me mf mz ju mh mv na mk ml mw nb mo mp mx nc ms mt mu ij bi translated">作为健全性检查，为了确保文本可以可行地聚类，我们可以执行<a class="ae kv" href="https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0" rel="noopener">主成分分析</a> (PCA):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/f907fe84f91eb2d2d7c341b8fc4f330c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvPJbqYgTlD-KuJMUfllkg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">PCA on the dataset</figcaption></figure><p id="93a1" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">我们有相当多的聚类，然而，蓝点的聚类(17 世纪晚期)可能是因为这一时期的文本处理了类似的主题。这凸显了从多个来源获取数据的重要性，因为它显示了模型倾向于选择最简单的要素。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="d73d" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">表示文本</h1><p id="3f3d" class="pw-post-body-paragraph ly lz iq mb b mc my jr me mf mz ju mh mv na mk ml mw nb mo mp mx nc ms mt mu ij bi translated">有无数种方式来表示文本数据，最常见的一种是<strong class="mb ir"> word2vec，</strong>将所有的单词放在一个 n 维向量空间中，相似的单词在空间上很接近<strong class="mb ir">。</strong>然而，出于我们的目的，我们注意到基于 word2vec 的模型对于决策树来说既慢又低效。</p><p id="75a8" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">相反，让我们使用一种<strong class="mb ir">单词袋</strong>方法，其中每本书是一个向量，其索引代表一个唯一的单词，包含该单词在书中的实例数量。向量的大小将取决于词汇的大小(即，如果你的书中有 1000 个独特的单词，则输入的大小将是 1000)。</p><p id="233a" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">例如，假设您的词汇量为 5，表示为列表</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/491b438c352d6ac1fb21d91e0cce560b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*XVo0LfpDJ1D3OerLzRdb2g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Vocabulary for the example</figcaption></figure><p id="2316" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">我们想把“我饿了”这句话编码产生的向量将简单地是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/a7187402351e403a540491c16d6ea31f.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*0RYMDgMuLoaA5SQSb3qiag.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Encoding for “I am hungry” with the given vocabulary</figcaption></figure><p id="e075" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">相反，如果是“我饿了，我”，我们就会</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/fda3db834fe119a03e612f13d374199b.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*cIl-mqazogO41O4tyZfHsw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Encoding for “I am hungry, I” with the given vocabulary</figcaption></figure><p id="9fc5" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">既然我们有了对每本书进行编码的方法，那么让我们定义一个准确性度量。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="576b" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">定义准确性</h1><p id="c036" class="pw-post-body-paragraph ly lz iq mb b mc my jr me mf mz ju mh mv na mk ml mw nb mo mp mx nc ms mt mu ij bi translated">假设我们的模型声称一本书写于 1527 年，但它实际上写于 1531 年。模型准确吗？</p><p id="9696" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">尽管均方误差(MSE)是确定模型准确性的显而易见的方法，但它不是很直观。为了产生更简单的度量，让我们定义<strong class="mb ir"> 𝛿 </strong>、容差(以年为单位)，以获得以下精度函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/7139d0b0cc2bc0124fd199b9a9c866b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*w7IT9HBOv_zQAH47h5JFkw.png"/></div></figure><p id="bf3b" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">换句话说，如果模型在<strong class="mb ir"> 𝛿 </strong>年内猜测正确，那么我们说它是准确的；否则就是不准确的。在下文中，我们将使用𝛿的<strong class="mb ir">= 50 年和𝛿的<strong class="mb ir">= 100 年。</strong></strong></p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="32e6" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">什么是决策树？</h1><p id="e1fd" class="pw-post-body-paragraph ly lz iq mb b mc my jr me mf mz ju mh mv na mk ml mw nb mo mp mx nc ms mt mu ij bi translated">让我们来看看我们将要用来学习将文本与日期相关联的模型，决策树。</p><p id="9922" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">决策树分类器类似于流程图，终端节点代表分类输出/决策。从一个数据集开始，测量熵以找到一种分割数据集的方法，直到所有数据都属于同一个类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/97a50c5226104e653fec8b9a5587a950.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*3L1-LxytBXu_26s4Bk4dFg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Basic example of a decision tree¹</figcaption></figure><p id="8d76" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">点击<a class="ae kv" href="https://medium.com/@chiragsehra42/decision-trees-explained-easily-28f23241248" rel="noopener">此处</a>查看更多关于决策树的文章。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="938a" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">结果和解释</h1><h2 id="43e5" class="nk le iq bd lf nl nm dn lj nn no dp ln mv np nq lp mw nr ns lr mx nt nu lt nv bi translated">模型的准确性</h2><p id="accf" class="pw-post-body-paragraph ly lz iq mb b mc my jr me mf mz ju mh mv na mk ml mw nb mo mp mx nc ms mt mu ij bi translated">一旦对决策树进行了训练，我们就可以在测试集上绘制一些预测:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/b62748fe7d148d01d8a19e5d181dc1a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*elbXfNB-2hfzyiiFIuFWVg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Results for a decision tree with pruning</figcaption></figure><p id="7aed" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">上面的图是线性的，表明决策树的性能很好。事实上，在我们的测试集上，我们获得了 84%的准确率(对于𝛿100 年)和 82%的准确率(对于𝛿50 年)。</p><h2 id="c362" class="nk le iq bd lf nl nm dn lj nn no dp ln mv np nq lp mw nr ns lr mx nt nu lt nv bi translated">解释和可视化</h2><p id="236f" class="pw-post-body-paragraph ly lz iq mb b mc my jr me mf mz ju mh mv na mk ml mw nb mo mp mx nc ms mt mu ij bi translated">在这里，我们可以看到树的顶部本身的可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/7eb820afb09a9b4b9d6cf9347a2c38fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q9AUa8NQPlOgezEQilBzOA.png"/></div></div></figure><p id="f4f1" class="pw-post-body-paragraph ly lz iq mb b mc md jr me mf mg ju mh mv mj mk ml mw mn mo mp mx mr ms mt mu ij bi translated">树的许多分裂决定是智能的，对我们人类来说是有意义的。例如，电话在 1900 年开始流行，珀西这个名字的流行程度在 19 世纪晚期达到顶峰，从那以后稳步急剧下降。这里有一个表格，包含一些有趣的分裂，可以在树中找到。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/a0a6702418dd10169c60e8b556ecf04a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*O_dYubUSB4uiB4xX_8wp-Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Some interesting splits in the decision tree</figcaption></figure><h1 id="2906" class="ld le iq bd lf lg nz li lj lk oa lm ln jw ob jx lp jz oc ka lr kc od kd lt lu bi translated">结论</h1><p id="b6d6" class="pw-post-body-paragraph ly lz iq mb b mc my jr me mf mz ju mh mv na mk ml mw nb mo mp mx nc ms mt mu ij bi translated">我希望您现在对如何使用决策树来估算一本书的写作日期有了更好的了解。</p><h1 id="9ec8" class="ld le iq bd lf lg nz li lj lk oa lm ln jw ob jx lp jz oc ka lr kc od kd lt lu bi translated">感谢</h1><p id="153a" class="pw-post-body-paragraph ly lz iq mb b mc my jr me mf mz ju mh mv na mk ml mw nb mo mp mx nc ms mt mu ij bi translated">我要感谢保罗·塞格斯和法布里斯·海底平顶山-西奥内斯对本文的贡献。</p><h1 id="4b41" class="ld le iq bd lf lg nz li lj lk oa lm ln jw ob jx lp jz oc ka lr kc od kd lt lu bi translated">参考</h1><p id="b937" class="pw-post-body-paragraph ly lz iq mb b mc my jr me mf mz ju mh mv na mk ml mw nb mo mp mx nc ms mt mu ij bi translated">[1] Chirag Sehra，决策树易解释(2018)，<a class="ae kv" href="https://medium.com/@chiragsehra42/decision-trees-explained-easily-28f23241248" rel="noopener">https://medium . com/@ Chirag se HRA 42/Decision-Trees-Explained-Easily-28f 23241248</a></p></div></div>    
</body>
</html>