<html>
<head>
<title>Decision Tree in Layman’s Terms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通俗地说就是决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-in-laymans-terms-part-1-76e1f1a6b672?source=collection_archive---------20-----------------------#2019-07-08">https://towardsdatascience.com/decision-tree-in-laymans-terms-part-1-76e1f1a6b672?source=collection_archive---------20-----------------------#2019-07-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/c4908332012a2b747549658af40f21cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qML7jVWx5N9O6UjpWPr4cg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Image by <a class="ae kf" href="https://pixabay.com/users/jplenio-7645255/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3097419" rel="noopener ugc nofollow" target="_blank">Johannes Plenio</a> from <a class="ae kf" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3097419" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><h2 id="1a7d" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">什么是决策树？</h2><blockquote class="lc ld le"><p id="510e" class="lf lg lh li b lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">决策树是基于特定条件的决策的所有可能解决方案的图形表示。目标变量可以取一组有限值的树模型称为<strong class="li iu">分类树</strong>，目标变量可以取连续值(数字)的树模型称为<strong class="li iu">回归树</strong>。</p></blockquote><p id="5981" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">让我们举一个现实生活中的例子，</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="mi mj l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Toll-Free</figcaption></figure><p id="65d9" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">每当你拨打银行的免费电话时，它会将你转接到他们的智能电脑助手那里，询问你一系列问题，如英语请按 1，西班牙语请按 2 等。一旦你选择了你想要的，它会再次将你重定向到一系列特定的问题，比如贷款请按 1，储蓄账户请按 2，信用卡请按 3 等等。这样不断重复，直到你最终找到合适的人或服务。你可能认为这只是一个语音邮件流程，但实际上，银行实施决策树是为了让你进入正确的产品或服务。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mk"><img src="../Images/82b2e6c0a03248af14f927664251e225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMOtF7bwKPHl1Pg52xN7fg.png"/></div></div></figure><p id="a388" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">考虑一下上面的图片，我是否应该接受一份新的工作邀请？为此，我们需要创建一个决策树，从基本条件或根节点(蓝色)开始，即最低工资应为 100，000 美元，如果没有 100，000 美元，则您不会接受该提议。所以，如果你的薪水高于 10 万英镑，那么你会进一步检查公司是否给你一个月的假期？如果他们不给，那么你就是在拒绝这个提议。如果他们给你一个假期，那么你会进一步检查该公司是否提供免费健身房？如果他们不提供免费健身房，那么你是在拒绝这个提议。如果他们提供免费健身房，那么你很乐意接受这个提议。这只是决策树的一个例子。</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="ml mj l"/></div></figure><h1 id="952b" class="mm kh it bd ki mn mo mp kl mq mr ms ko mt mu mv ks mw mx my kw mz na nb la nc bi translated">好了，怎么建树？</h1><p id="4f01" class="pw-post-body-paragraph lf lg it li b lj nd ll lm ln ne lp lq kp nf lt lu kt ng lx ly kx nh mb mc md im bi translated">有许多具体的决策树算法可用。值得注意的包括:</p><ul class=""><li id="6c03" class="ni nj it li b lj lk ln lo kp nk kt nl kx nm md nn no np nq bi translated">ID3(迭代二分法 3)</li><li id="2e39" class="ni nj it li b lj nr ln ns kp nt kt nu kx nv md nn no np nq bi translated">分类和回归树</li><li id="6334" class="ni nj it li b lj nr ln ns kp nt kt nu kx nv md nn no np nq bi translated">卡方自动交互检测(CHAID)。计算分类树时执行多级拆分。</li></ul><p id="fd35" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">在这个博客中，我们将看到 ID3。决策树常用的杂质度量有三种:<strong class="li iu">熵</strong>、<strong class="li iu">基尼指数</strong>、<strong class="li iu">分类误差</strong>。决策树算法使用<strong class="li iu">信息增益</strong>来分割节点。基尼指数或熵是计算信息增益的标准。CART 算法使用的基尼指数和 ID3 算法使用的熵。在进入细节之前，我们先来看看杂质。</p><h2 id="cc97" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">什么是杂质？</h2><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/25df373365478590d39f959edf1d7516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-FhFdm9qPVZTJdzRY0wREg.png"/></div></div></figure><p id="f682" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">假设你有一个装满苹果的篮子，而另一个碗里装满了同样的苹果标签。如果你被要求从每个篮子和碗中挑选一件物品，那么得到苹果及其正确标签的概率是 1，所以在这种情况下，你可以说杂质是 0</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/ea76b48a7f545d8d34986810dc3e70b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b83tDX-Y2XSkoKyx1o4f8Q.png"/></div></div></figure><p id="a9db" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">假设现在篮子里有三种不同的水果，碗里有三种不同的标签，那么水果与标签匹配的概率显然不是 1，而是小于 1。如果我们从篮子里拿一根香蕉，然后从碗里随机拿一个标签，上面写着葡萄，这是有可能的。所以在这里，任何随机的排列组合都是可能的。在这种情况下，我们可以说杂质不为零。</p><h2 id="b3df" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">熵</h2><p id="2354" class="pw-post-body-paragraph lf lg it li b lj nd ll lm ln ne lp lq kp nf lt lu kt ng lx ly kx nh mb mc md im bi translated">熵是你的数据有多杂乱的一个指标。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ny"><img src="../Images/299d4c2ed5ea6e6593ab9237d8cec378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tzaI426weie1rY16fFNflQ.png"/></div></div></figure><p id="3352" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">熵是对数据集中随机性或不可预测性的度量。换句话说，它控制决策树决定如何分割数据。熵是数据同质性的度量。它的值范围从 0 到 1。如果一个节点的所有样本都属于同一个类，则熵为 0(这对训练数据集不利)，如果我们具有均匀的类分布，则熵最大(对训练数据集有利)。熵的等式是</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nz"><img src="../Images/5c5f4d7f65443b80712f8b269c6aa1dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mxl-DmuM2e2O_H97Rr6G5A.png"/></div></div></figure><h2 id="8758" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated"><strong class="ak">信息增益</strong></h2><p id="996a" class="pw-post-body-paragraph lf lg it li b lj nd ll lm ln ne lp lq kp nf lt lu kt ng lx ly kx nh mb mc md im bi translated"><strong class="li iu"> <em class="lh">信息增益(IG) </em> </strong> <em class="lh">度量一个特征给我们多少关于类的“信息”。</em>信息增益基于数据集在属性上拆分后熵的减少。它是用于构建决策树的主要参数。<strong class="li iu">具有最高信息增益的属性将首先被测试/分割。</strong></p><p id="ac57" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated"><strong class="li iu">信息增益=基础熵—新熵</strong></p><p id="3b50" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">让我们以下面的卡通数据集为例。感谢 Minsuk Heo 分享这个例子。你可以点击查看他的 youtube 频道<a class="ae kf" href="https://www.youtube.com/user/TheEasyoung/videos" rel="noopener ugc nofollow" target="_blank"/></p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/47020c1d6fa8fd3ba7d601355a3e03f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lT_sio4ua--vCF6FJ8Zebw.png"/></div></div></figure><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/156662d3acdbf5ced82ce5b504e80df2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*0buB74R8RHS3XwDSCf0gIg.png"/></div></figure><p id="a468" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">数据集有一个卡通，冬天，&gt; 1 属性。家庭冬季照是我们的目标。共 8 张图片。我们需要教宝宝挑选正确的冬季家庭度假照片。</p><h2 id="3b1c" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">如何拆分数据？</h2><p id="19eb" class="pw-post-body-paragraph lf lg it li b lj nd ll lm ln ne lp lq kp nf lt lu kt ng lx ly kx nh mb mc md im bi translated">我们必须以信息增益最高的方式构建分割数据的条件。请注意，增益是分裂后熵减少的量度。首先，将计算上述数据集的熵。</p><p id="b3d1" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">共 8 张照片。冬季全家福— 1(是)，现在冬季全家福— 7(否)。如果我们代入上面的熵公式，</p><p id="90f8" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">=-(1/8)* log2(1/8)——(7/8)* log2(7/8)</p><p id="39d1" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated"><strong class="li iu">熵= 0.543 </strong></p><p id="9340" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">我们得到了三个属性，即卡通、冬天和&gt; 1。那么哪种属性最适合构建决策树呢？我们需要计算所有三个属性的信息增益，以便选择最佳的一个或根节点。我们的基本熵是 0.543</p><p id="5424" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">卡通片《冬天》的信息增益，</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/daf2e6babb52a4a574007270b35db790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o9YLm3BJ_JJkLVl3WAjefw.png"/></div></div></figure><p id="d96c" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">卡通的信息增益高，所以根节点是一个卡通人物。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/845674fe101d90d0c7935a9d07f28117.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UsI1kMbEKmXF5zqwbFiMaw.png"/></div></div></figure><p id="7cf0" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">根节点是一个卡通人物。我们需要根据另外两个属性 winter 或&gt; 1 再次进行拆分。再次计算信息增益并选择最高的一个用于选择下一次分裂。</p><p id="76eb" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">&gt; 1 属性具有高信息增益，相应地拆分树。最终的树如下</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/58f06836c4093da7c88ee7555c935322.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*Ab9rdVK65yCrtFNZsR_iZw.png"/></div></figure><h1 id="8120" class="mm kh it bd ki mn mo mp kl mq mr ms ko mt mu mv ks mw mx my kw mz na nb la nc bi translated">决策树的优点</h1><ol class=""><li id="c96a" class="ni nj it li b lj nd ln ne kp of kt og kx oh md oi no np nq bi translated">决策树易于可视化和解释。</li><li id="af22" class="ni nj it li b lj nr ln ns kp nt kt nu kx nv md oi no np nq bi translated">它可以很容易地捕捉非线性模式。</li><li id="1054" class="ni nj it li b lj nr ln ns kp nt kt nu kx nv md oi no np nq bi translated">它可以处理数字和分类数据。</li><li id="c34d" class="ni nj it li b lj nr ln ns kp nt kt nu kx nv md oi no np nq bi translated">数据准备所需的工作量很小。(例如，不需要标准化数据)</li></ol><h1 id="96d4" class="mm kh it bd ki mn mo mp kl mq mr ms ko mt mu mv ks mw mx my kw mz na nb la nc bi translated">决策树的缺点</h1><ol class=""><li id="4391" class="ni nj it li b lj nd ln ne kp of kt og kx oh md oi no np nq bi translated">过拟合是决策树模型最实际的困难之一。</li><li id="9e28" class="ni nj it li b lj nr ln ns kp nt kt nu kx nv md oi no np nq bi translated">连续变量精度低:在处理连续数值变量时，决策树在对不同类别的变量进行分类时会丢失信息。</li><li id="2c4b" class="ni nj it li b lj nr ln ns kp nt kt nu kx nv md oi no np nq bi translated">它是不稳定的，意味着数据的微小变化会导致最优决策树结构的巨大变化。</li><li id="8fcd" class="ni nj it li b lj nr ln ns kp nt kt nu kx nv md oi no np nq bi translated">决策树偏向于不平衡数据集，因此建议在创建决策树之前平衡数据集。</li></ol><p id="9b69" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">我将在下一篇文章中使用 Python 解释 CART 算法和过度拟合问题。</p><p id="e902" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">请继续学习，并关注更多内容！</p><p id="e275" class="pw-post-body-paragraph lf lg it li b lj lk ll lm ln lo lp lq kp ls lt lu kt lw lx ly kx ma mb mc md im bi translated">如果您发现任何错误或需要改进的地方，请随时在下面发表评论。</p></div></div>    
</body>
</html>