<html>
<head>
<title>Trust and interpretability in machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的信任和可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/trust-and-interpretability-in-machine-learning-b7be41f01704?source=collection_archive---------12-----------------------#2019-03-30">https://towardsdatascience.com/trust-and-interpretability-in-machine-learning-b7be41f01704?source=collection_archive---------12-----------------------#2019-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/9f57720d84f6d0b1633e8e2d62285752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LI9TzwDU1l6IyJFBRcULw.jpeg"/></div></div></figure><p id="a7a2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">机器学习模型总是需要可解释的吗？如果要在不准确的可解释模型和准确的不可解释模型之间做出选择，你会选择不可解释但准确的模型吗？换句话说，在可解释性的祭坛上牺牲准确性有任何理由吗？</p><p id="8a61" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在进一步探讨之前，我们应该试着澄清是什么使得一个模型是可解释的。通常，可解释性等同于简单性。这个定义明显模糊不清；对一个人来说简单的事情对另一个人来说可能不简单。更重要的是，高级机器学习对复杂系统建模最有用。这些系统被称为复杂是有原因的——它们并不简单！无论如何，要求这种系统的有用模型应该简单是没有意义的。</p><p id="168f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">也许，如果我们考虑如何在传统的建模环境中思考可解释性，我们可以走得更远。考虑汽车内燃机的数学模型。我怀疑是否有人会认为这很简单。但是，另一方面，我也怀疑是否有人会认为内燃机的模型是不可解释的。这主要是因为我们可以从热力学和流体动力学等成熟的物理理论中推导出这个模型。</p><p id="e646" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为什么不用这个作为我们对可解释性的定义呢？如果一个模型可以从一个值得信赖的理论中推导出来(或者至少有动机),那么它应该被认为是可解释的。这个可解释性的定义服务于理解和信任的双重目的。它帮助我们理解模型，因为我们倾向于以演绎的方式理解事物——从已知到未知。同样，有了这样的定义，模型中的信任来源于我们对基础理论的信任。</p><p id="4b54" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">事实上，有些情况下理解和信任都是必要的——在这些情况下，我们有兴趣确定系统行为背后的因果因素。在这样的场景中，我们必须坚持相应的模型必须根据上述定义是可解释的。物理科学领域的大多数模型都属于这一类。有人可能会说，纯粹的感应黑盒模型不适合这种情况。</p><p id="2ecc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然而，在许多其他情况下，理解可能是很好的，但绝不是必须的。在这些情况下，真正重要的是做出可信预测的能力。在这些情况下，如果我们能够提供一个替代的信任源，那么我们的模型就不需要被上面给出的可解释性定义所束缚。这是一个常见的论点，也有可取之处。请记住，机器学习是一种使用归纳推理从(最好是)大量数据中系统地建立模型的方法。限制这些模型以演绎的方式解释会严重限制它们的准确性。</p><p id="8971" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，接下来的问题是，我们如何在一个我们对其内部运作知之甚少甚至一无所知的黑盒模型中产生信任。可信的信任基础可能是<a class="ae kw" href="https://hackernoon.com/explainable-ai-wont-deliver-here-s-why-6738f54216be" rel="noopener ugc nofollow" target="_blank">测试</a>。毕竟，测试是我们信任常规软件的基础。但是为了测试一个模型，我们需要能够形式化我们对它的期望。如果我们能够完全形式化我们的期望，那么这将对应于模型本身的完整规范。在这种情况下，我们真的不需要机器学习或任何其他建模方法。我们真正需要做的是将我们对模型中我们认为重要的方面的期望正式化。这也不容易，因为我们关心的许多概念，如<a class="ae kw" href="https://vimeo.com/248490141" rel="noopener ugc nofollow" target="_blank">公平</a>，并不适合于方便的数学处理。</p><p id="fc1c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">值得指出的是，在开发测试机器学习模型的测试方法方面已经取得了重大进展。我个人认为使用<a class="ae kw" href="https://ieeexplore.ieee.org/document/8493260" rel="noopener ugc nofollow" target="_blank">变形关系来形式化期望</a>的想法特别有前途。但是，我们距离拥有允许我们对黑盒模型进行全面测试的具体方法还有很长的路要走，我们的这种无能导致了黑盒模型中的信任赤字。</p><p id="a9c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">人们可能会质疑这种基于预期的全面测试的有效性。毕竟，机器学习的目标是在数据中发现未被发现的模式。坚持认为模型符合我们的期望相当于预先定义了模型，这违背了整个目的。按照这种推理方式，人们会认为，只要数据具有代表性，并且我们的算法足够强大，能够捕捉到模式，就没有理由不相信模型——我们应该期望模型结果能够推广到总体，并且我们应该期望它们推广的程度包含在模型的性能(准确性)得分中。</p><p id="91d3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，本质上，我们被要求将我们的信任委托给数据、算法和性能分数这三者。我们首先需要说服自己不要认为一个单一的性能(准确性)分数就可以构成信任模型的充分基础。性能分数通常是在给定当前数据的情况下，模型在总体上的平均推广程度的点估计值。另一方面，信任是一个微妙的多维度概念，无法用这么一个粗粒度的分数来概括。人们可以想象定义更精细的绩效分数，例如通过人口细分。但是，这需要对人口有一定程度的了解，并确定我们认为什么是重要的——这与形成预期没有太大区别。</p><p id="1091" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们检查这个论点的数据方面。事实上，很容易说服自己，如果数据代表我们感兴趣的人群，那么它应该包含所有相关的模式，而不是虚假的。不幸的是，情况很少如此。数据的无代表性程度取决于具体情况。尽管如此，我们可以确定某些高级场景。</p><p id="16f9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在第一个场景中，我们将对人口有很好的了解，并完全控制数据收集机制。在这种情况下，我们可以选择具有代表性的数据，并且有很高的可信度，我们可以预期我们的结果模型的预测适用于总体人口。但是，请注意，对总体有足够好的了解，能够为手头的任务绘制代表性样本，这意味着我们已经对哪些特征对预测很重要有了一些了解。因此，在这种情况下，黑盒模型是否非常有用是有争议的。预测选举结果的民意调查就是这种情况的一个很好的例子。</p><p id="354c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在第二种情况下，我们不能完全控制数据收集，但是我们的预测不会影响收集的数据。在这种情况下，如果我们假设数据收集机制是无偏的，那么如果我们等待足够长的时间，我们将得到一个具有代表性的总体样本。当然，这个假设有很多如果和但是。首先，人们不知道多长时间才算够长。因此，人们需要假设收集数据的时间尺度足够长，足以产生代表性样本。此外，在此期间，人口本身也可能发生变化。因此，另一个假设是，人口变化的时间尺度比产生代表性样本的时间尺度长得多。只要我们能够证明这些假设，那么估计的性能将是可靠的。预测股票价格的模型就是这种情况的一个例子——只要我们没有进行大到足以影响整个市场的投资，我们根据预测结果做出的决策就不会影响股票价格。</p><p id="d77f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第三种情况是，数据收集受到预测的影响，但我们对错误预测的风险偏好从中度到高度不等。这方面的一个例子是产品推荐系统。推荐系统的模型将在由不同用户已经购买/点击的产品的有序列表组成的数据上被训练。基于这些数据，模型将预测用户最有可能购买/点击什么，并且基于模型的预测，系统将决定用户可以看到什么，这限制了他可以购买/点击什么。因此，预测会使数据收集产生偏差。在产品推荐系统中，人们可以通过保留一个探索预算来在某种程度上规避这个问题——对于一小部分情况，系统向用户显示一组随机的产品，而不管模型的预测。从这些随机预测中得到的观察结果可以用来估计模型的性能。为了获得这些估计的可靠性，人们仍然必须解决上述第二种情况的问题。</p><p id="7607" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在第四个也是最后一个场景中，数据收集受到预测的影响，但是我们对错误的预测几乎没有风险偏好。例如，假设我们必须建立一个模型来预测某人是否会拖欠抵押贷款。抵押贷款将根据预测批准或不批准。如果预测是这个人会违约，那么贷款将不会被批准，在这种情况下，没有办法知道这个人是否真的会违约。很难想象一家机构会为了数据探索而随机批准(或以其他方式)一笔贷款。在这些情况下，在没有额外信息的情况下，很难衡量所得模型的估计性能的可靠性。</p><p id="89c7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，盲目地期望数据能够代表总体并不是一个好主意。在大多数情况下，考虑到手头问题的限制，可能根本不可能获得无偏见的代表性样本。了解一个人的数据收集机制的局限性，能够推断出这些局限性的含义，并诚实地将这些作为模型结果的一部分进行报告，这对建立信任大有帮助。</p><p id="1b2e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在让我们考虑一下论证的算法方面。人们普遍认为算法越灵活越好，因为灵活性使算法能够捕捉更复杂的模式。但是，如果机器学习的实际成功应用的历史可以借鉴的话，那么这种信念似乎是完全错误的。在计算机视觉中，当我们能够以卷积神经网络的形式将图片中的对称性编码到模型中时，成功就来了。在自然语言处理中，我们现在能够构建极其精确的跨用途语言模型，因为我们可以将我们关于语言的知识，包括结构和单词上下文，编码到这些模型中。在推荐系统中——包括矩阵分解方法在内的大多数协同过滤算法，对用户对某个项目的亲和力做出了强有力的假设。</p><p id="7e92" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">不管我们愿不愿意给这些模型贴上可解释性的标签，当我们理解了模型需要运作的领域和环境时，我们会构建更好的模型，这是一个客观事实。最好的模型不是来自最灵活的算法，而是来自受到领域知识很好约束的算法，这些算法具有恰到好处的灵活性来捕捉数据中的相关模式。</p><p id="11c3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上面的讨论中，我们已经多次看到理解这个词。我们现在应该已经意识到，没有理解就很难建立信任。最终，它归结为一个人如何看待机器学习。是的，机器学习是一种非常强大的归纳建模技术。当与大数据和大计算相结合时，它允许我们对系统进行建模，并解决以前我们力所不及的问题。但是机器学习的进入不应该意味着其他一切的退出，包括常识。机器学习是更广泛的建模家族中的一个元素，包括演绎建模以及领域知识。我们越好地理解和利用这些元素之间的相互联系，我们就能更好地进行健壮的复杂系统建模。</p><p id="1a68" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">信任是有背景的，信任可以有多种来源，但最终它来自知识和诚信；特别是我们对构建模型的个人的知识和诚信的信任。在我看来，只有当更广泛的受众相信建模者有知识理解他们的模型的局限性(机器学习或其他)，并诚实地报告它们时，信任以及模型的采用才会到来。</p></div></div>    
</body>
</html>