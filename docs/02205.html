<html>
<head>
<title>ML Intro 7: Local Connections and Spatial Parameter Sharing (Aka Convolutional Layers)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML Intro 7:本地连接和空间参数共享(又名卷积层)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ml-intro-7-local-connections-and-spatial-parameter-sharing-abbreviated-convolutional-layers-b419e629d2d0?source=collection_archive---------11-----------------------#2019-04-11">https://towardsdatascience.com/ml-intro-7-local-connections-and-spatial-parameter-sharing-abbreviated-convolutional-layers-b419e629d2d0?source=collection_archive---------11-----------------------#2019-04-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="b9b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章紧随<a class="ae kl" rel="noopener" target="_blank" href="/ml-intro-3-logistic-output-units-ec42cc576634"> ML intro 3 </a>或<a class="ae kl" rel="noopener" target="_blank" href="/ml-intro-6-reinforcement-learning-for-non-differentiable-functions-c75e1464c6b9"> ML intro 6 </a>之后。我们假设您直观地理解了堆叠线性回归图层和交替非线性函数来构建神经网络的强大功能，并理解了自定义输出单元，如 logistic 或 softmax 函数。</p><p id="11ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个演讲是我在哥伦比亚工程【https://bootcamp.cvn.columbia.edu/data/ T4】的系列演讲的一部分，旨在适用于技术和企业观众。</p><h2 id="eac2" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">学习目标</h2><p id="4b05" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">在这篇文章中，我们建立了一个数学工具集，让我们处理图像、语言、视频、图形和其他 ML 问题。我们将设计自定义的数学计算层，这些计算层采用本地连接来共享本地信息和空间参数，从而共享情报。最后，我们构建卷积层，并使用一个包(在这种情况下，我们选择 PyTorch)从图像数据中学习。</p><p id="2a8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这次讲座之后，你应该能够熟练地使用图像完成任务，例如物体识别，并且能够熟练地设计数学表示，以及将适当的参数共享应用于定制的商业和学术场合。</p><h1 id="db74" class="lk kn iq bd ko ll lm ln kr lo lp lq ku lr ls lt kx lu lv lw la lx ly lz ld ma bi translated">问题设置</h1><p id="f8cb" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">眼前的问题是依靠 softmax 输出单元进行多类分类。正如在<a class="ae kl" rel="noopener" target="_blank" href="/ml-intro-3-logistic-output-units-ec42cc576634"> ML intro 3 </a>中所讨论的，多类分类是将一个数据点分类为多个类型中的一个。正如在<a class="ae kl" rel="noopener" target="_blank" href="/ml-intro-3-logistic-output-units-ec42cc576634"> ML intro 3 </a>中，我们使用 softmax 输出单位来分配每种类型(或类别)的可能性。但是在这个分析中，我们使用图像作为输入数据。</p><p id="bfeb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不要跑去谷歌“ML for images”或“如何卷积张量流”。相反，让我们想想我们想要从我们的机器学习系统中得到什么，并适当地设计它。</p><h2 id="cd41" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">代码</h2><p id="b29d" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">在这里跟随尚未解决的部分代码:<a class="ae kl" href="https://github.com/leedtan/LeeTanData/blob/master/ML_Intro_7_Convolutions/Unsolved.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/leedtan/LeeTanData/blob/master/ML _ Intro _ 7 _ Convolutions/unsolved . ipynb</a></p><p id="ab50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并在此查看填写的解决方案:<a class="ae kl" href="https://github.com/leedtan/LeeTanData/blob/master/ML_Intro_7_Convolutions/Solution.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/leedtan/LeeTanData/blob/master/ML _ Intro _ 7 _ Convolutions/solution . ipynb</a></p><h2 id="e514" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">原始数据</h2><p id="ac74" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">过去，我们需要学习数据点。数据集可以像我们的第一个例子一样在 2d 网格中可视化，其中每行代表一个数据点，每列代表一个要预测的特征或目标。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/c887c24a3b20cc5d9b3300bd90f572e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*sIUzNY6za4A5_qlye-ffdA.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Table Visualizing Sales</figcaption></figure><p id="f38f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图像也是如此。每个图像可以是一行，图像的每个值可以是我们机器学习模型中的一个特征。但是，请注意，图像显然具有网格结构。图像通常被理解为像素的 2d 网格，或者是 NxN 灰度图像(每个像素只是一个值)，或者是 Nx3 RGB 图像(每个像素包含 3 个值，红绿蓝)。这些可以被认为是具有 1 个过滤器或 3 个过滤器的 2d 网格，其中过滤器在该空间中被定义为数值层，图像中的每个空间区域一个值。在图像的神经网络中，我们计算由许多过滤器组成的神经网络层，而不仅仅是许多密集的节点。</p><p id="a36e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们想象一下我们的一幅灰度图像:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/7792dd81ccccf30551f1b47be88fb546.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*9me9JXbY4_9p1Am59SQdYg.png"/></div></figure><h1 id="9f8a" class="lk kn iq bd ko ll lm ln kr lo lp lq ku lr ls lt kx lu lv lw la lx ly lz ld ma bi translated">建模图像数据</h1><p id="66de" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">所以现在我们已经讨论了图像具有结构，但我们仍然会用我们的神经网络来处理它们。那看起来是不是…乱七八糟？</p><h2 id="0106" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">概念检查:神经网络图像数学</h2><p id="458a" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">假设我们有一个 1024x1024 的图像，我们想计算一个同样大小的隐藏层。对于这一层，我们需要学习多少个标量乘数？</p><h2 id="a2ef" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">解决方案:神经网络图像数学</h2><p id="2d6a" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">1024*1024*1024*1024 标量，因为每百万个输入单元都需要连接到每百万个输出单元。一个隐藏层就有十亿个学习参数。似乎有点过分，对吧？</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h2 id="6d60" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">引入本地连接</h2><p id="4f20" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">与其将所有的输入单元连接到所有的隐藏单元，不如将隐藏单元构造成一幅图像。这样，我们可以认为每个隐藏单元不需要计算关于图像的特征，它只需要计算关于其区域的特征。</p><p id="6fc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们称之为局部连接层。它是保持上一层结构的一层，每个单元只接受来自自身周围一个小的局部区域的输入。我们把它接收输入的区域的宽度和高度叫做它的“内核大小”。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/8ab5a78f9c0c6ede257843b6696ede38.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/0*Bg6EfwhYGwkIuJf0.png"/></div></figure><p id="79c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面，内核大小没有标注，但是你可以看到蓝色的盒子正在计算红色图像的隐藏表示。蓝框的每个位置都接收来自其周围一个小区域的输入。</p><p id="1452" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以看到在蓝色立方体中图像的相同位置有许多球。这是因为我们创建了多个隐藏单元(就像隐藏层中的节点)来计算不同的函数。然后，蓝色立方体就像图像一样被处理，并用于进一步处理表示，就像神经网络中的层一样。</p><p id="e7be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">查看基本的本地连接图层，并注意生成输出图层的计算。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/8e6b6bfd0936a8c59989e452dd61f558.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/0*rgP2WAh0kvTn8bG8.gif"/></div></figure><p id="0a82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，上面的输出(右边的绿色)过滤器比左边的过滤器小。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="ab gu cl mx"><img src="../Images/3f3600e3e9fd956c3078a115ea31b07c.png" data-original-src="https://miro.medium.com/v2/1*1okwhewf5KCtIPaFib4XaA.gif"/></div></figure><p id="4c45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在请注意，蓝色立方体和红色图像，或者蓝色正方形和绿色正方形的高度和宽度是相同的。看到绿色方块的每个位置是如何由一个 3×3 的网格计算出来的吗？还要注意，边节点是通过观察蓝色网格外的区域来计算的。这些用来保持图像大小和形状的空白单元格被称为<strong class="jp ir">填充</strong>。因此，5x5 内核需要在所有边上填充 2，以保持图像大小。</p><h2 id="31fc" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">概念检查:神经网络本地连接</h2><p id="1315" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">如果是在课堂上，与同伴讨论。假设我们有一个 5x5x1 的灰度图像，我们想计算一个没有填充的 3x3x2 的灰度图像。我们需要多少内核？我们将如何使用它们？使用 python 随机生成内核[-1，0，1]并计算以下输入图像的隐藏层值:</p><p id="6198" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[[2，1，0，0，1]，<br/>，【0，0，0，1，0】，<br/>，<br/>，【1，2，0，1，0】，<br/>，【0，1，0，0，1】]</p><h2 id="9100" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">解决方案:神经网络本地连接</h2><p id="f94c" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">我们需要 18 个内核，每个空间位置 2 个，总共 9 个空间位置。</p><p id="f69c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面我们随机生成 18 个内核，</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="3846" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">乘以输入</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="4620" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并将负值修剪为 0</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="7699" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">创建:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="f23d" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">概念检查:局部连接的参数数量</h2><p id="c612" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">利用上面的公式，我们现在需要多少个参数来从 1024×1024 灰度图像学习隐藏表示？现在让我们假设我们的隐藏层有 16 个隐藏单元，每个单元有一个 3x3 内核。</p><h2 id="cb8b" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">解决方案:局部连接的参数数量</h2><p id="73a1" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">我们需要大约 1024×1024 个位置来计算，对于每个位置，我们只需要学习 16×9 个参数(与全连接层的 16×1024 个参数相反)。这使我们的参数减少了 100 倍！我们现在只需要 10⁸参数，这听起来仍然很多。</p><h2 id="303e" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">包装本地连接:</h2><p id="1705" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">本地连接的网络在图像处理之外还有很多应用。它们可以应用于处理语言数据，以及处理图形数据，例如社会或地理信息。本练习的目标是学习识别何时需要处理局部信息来计算局部表示。</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h2 id="b3de" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">引入空间参数共享</h2><p id="cba3" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">在传统的机器学习中，我们首先手工制作内核来检测图像中的对象，如蓝线或黄色斑点。</p><p id="c5e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于本地连接，我们有一个内核来处理图像的左上角。也许该特征擅长检测图像中的黄色斑点，这是全局图像分类之前的重要局部任务。如果内核对图像的左上角有用，那么检测图像的中间和底部可能也很重要。为什么不只是定义相同的一组内核，然后将它们应用到图像中所有不同的空间区域？通过这种方式，我们不会学习不同的函数来处理图像的右侧和左侧，我们学习的参数数量会急剧下降。</p><h2 id="ece6" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">概念检查:空间参数共享多个参数</h2><p id="c018" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">利用上面的公式，我们现在需要多少个参数来从 1024×1024 灰度图像学习隐藏表示？同样，我们的隐藏层有两个隐藏单元，它们有相同的 3x3 内核。</p><p id="5039" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">之后，使用与之前相同的图像，定义适当数量的内核，并利用参数共享计算隐藏层。</p><p id="7bba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[[2，1，0，0，1]，<br/>，【0，0，0，1，0】，<br/>，【0，0，1，2，0】，<br/>，【1，2，0，1，0】，<br/>，【0，1，0，0，1】]</p><h2 id="8e2b" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">解决方案:空间参数共享多个参数</h2><p id="f5d8" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">嗯，在每个图像位置，我们仍然需要 2*9 个参数来学习 2 个过滤器，但现在我们在整个图像上应用同样的 2 个过滤器！现在我们只需要 2*9 ~ 18 个参数就可以学习整个滤波器了！</p><p id="392b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面我们随机生成两个内核，</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="7594" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">乘以输入</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="81de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并将负值修剪为 0</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="b94e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">创建:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h2 id="01af" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">创造术语卷积层</h2><p id="6e14" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">本地连接层的参数共享组合如此强大，以至于它有了自己的名字，卷积层，它构建成一个卷积网络。</p><p id="b54a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参数共享及其对 2D 卷积层的应用是使 Yann Lecun(脸书人工智能研究负责人)出名的关键见解之一，也是我们日常接触到的大部分人工智能得以实现的原因之一。</p><p id="30b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于下面的编码活动，我们可以依赖卷积层，但我也应该先向您介绍一些其他概念:</p><h2 id="1981" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">池层:</h2><p id="075a" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">池是压缩层。假设我有一张 1024x1024x1 的灰度图像，我不想在计算上处理这么大的图像。嗯，对于每个 4x4 的立方体，我可以取最大值(或最小值，或平均值)，这将把图像压缩成 256x256x1 的灰度图像。(一层所需的处理能力降低了 16 倍！)</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi na"><img src="../Images/58e4c856594bd8a2186fecb111806df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/0*Lm_TvoQvJc-x0eBn.png"/></div></figure><p id="85e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相反，如你所料，取消 pooling 会在空间上扩展图像。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/657438c5a8db6eee7cc6056eb1904e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*yAtDUTxOBSn6NL_d_Z0kXw.png"/></div></figure><p id="92ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">传统上，我们交替使用卷积层、ReLU 层和(最大)池层来降低数据的维度，同时为每个空间位置计算越来越多的要素</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/4df3d555397c1c86797179a387ed765d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*E1kyEHziIYea0Rl5.png"/></div></figure><p id="2a28" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不过，我们不太喜欢池层不能完全区分所有的输出单元。对于最大池，只有最大值可以摆动以影响输出，因此其他值不会得到导数。因此，现在我们经常使用步长卷积进行降维。</p><h2 id="acb8" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">交错盘旋；</h2><p id="9bed" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">卷积的跨度是压缩到单个输出位置的输入空间位置的数量。</p><p id="a8b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，步幅为 2 会将图像大小减少一半，类似于 2x2 池。此外，0.5 的步幅(也称为转置卷积)会将图像的大小扩大 2 倍。</p><p id="13b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意下面的步幅 2 是图像形状的一半，而步幅 0.5 是图像宽度和高度的两倍。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/387144d543373ea4afae81613782ccac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MN0gWvDpIthuBZqT.gif"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">In the transposed convolution images above, blank pixels are inserted between the blue pixels to show that the 2x2 or 3x3 blue image is expanded and the output green image is much wider</figcaption></figure><p id="eb02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还要注意，网络中大于 1 或小于 1 的跨距是一种通用数学工具，用于网络层以扩展或压缩卷积信息，并且比仅应用于图像操作更通用。</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><p id="a957" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是定制机器学习数学的一个案例！如果你在 ML 方面有所进步，你会发现理解核心操作背后的这些概念是令人难以置信的强大，因此你可以用它们来为你手头的问题构建定制的操作，而不是试图将你的问题放入现成的系统中。</p><h1 id="7155" class="lk kn iq bd ko ll lm ln kr lo lp lq ku lr ls lt kx lu lv lw la lx ly lz ld ma bi translated">现在让我们在 Pytorch 中实现一个神经网络</h1><p id="c255" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">对于这个练习，您应该使用 nn。新罕布什尔州 Conv2d。LeakyReLU 和 nn。MaxPool2d 构建神经网络层，将图像转换为窄的类似图像的表示，然后将内容重塑为单个向量，并执行标准分类。</p><p id="f5a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:这只是真实数据集的一个样本，所以我们不能获得很高的准确性，而只是演示概念。</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h2 id="2856" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">首先导入许多包</h2><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="7ce9" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">编码活动:读入数据标签</h2><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="35e2" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">解决方案:读入数据标签</h2><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="8a09" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">编码活动:将数据文本标签映射到数字</h2><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="d989" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">解决方案:将数据文本标签映射到数字</h2><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="66f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">读入图像。如果之前读入，则从缓存加载。</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="f277" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">以下是一些对所有 PyTorch 项目有用的函数</h2><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="b126" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">填写你的模型的待办事项</h2><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="23ed" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">并通过填写这些待办事项来训练模型:</h2><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="a2c6" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">创建并训练模型，绘制学习曲线</h2><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ec34cc4d4fa663b7df79f647a8895655.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*LNM0HZoh-609H9HNM7Wgjg.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Learning curve of training and validation losses (notice the model improves over time!)</figcaption></figure><h2 id="0bd1" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">想象你的第一层(或第二层！)</h2><p id="4ecd" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">最后，我们很好奇我们的第一层正在学习什么类型的形状，所以我们想象下面的层权重</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nj"><img src="../Images/5c7d022e4a0b04ecbadc859ae39e0a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lnh2lelFvATHPlG-39iTyg.png"/></div></div></figure><p id="fe60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们训练我们的模型更长时间，如果我们有一个更大的数据集，我们可能会学习关于线和斑点的低级特征，以及关于曲率的中级特征，这些特征可能如下所示:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/dcb02211fbee0790042c04095c88d06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*XpwN4bxsuaZNdd4Mn2DR6w.jpeg"/></div></figure><h2 id="5216" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">包裹</h2><p id="f996" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">不幸的是，我们只在一些图像的小样本上运行我们的代码，所以我们的第一层看起来不太有意义，我们的精确度也不是很好。不过没关系，我们学习了代码。如果我们有一个强大的 GPU 来运行大型数据集上的代码，这个模型可能会做得更好。</p><p id="035b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的例子使用了一个小数据集，并且没有训练太长时间，所以你们都可以获得舒适的学习体验，而不用等待很长的训练时间。</p><p id="2d95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">反正就是这样！当我们处理图像时，我们希望在计算最终预测之前使用空间参数共享来处理局部特征，因此我们可以使用简单的神经网络卷积层，但我们认识到为什么我们使用那些类型的卷积，以便我们可以在解决其他类似问题时使用其他类似的操作。</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="dc5b" class="lk kn iq bd ko ll nl ln kr lo nm lq ku lr nn lt kx lu no lw la lx np lz ld ma bi translated">总结我们所讲的内容:</h1><p id="b63e" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">在整个会议期间，我们</p><ul class=""><li id="c3bb" class="nq nr iq jp b jq jr ju jv jy ns kc nt kg nu kk nv nw nx ny bi translated">开始用<strong class="jp ir">神经网络</strong>来处理<strong class="jp ir">图像</strong>的像素特征。</li><li id="f3ae" class="nq nr iq jp b jq nz ju oa jy ob kc oc kg od kk nv nw nx ny bi translated">计算出的<strong class="jp ir">局部</strong>特征而不是<strong class="jp ir">密集</strong>特征(所有节点都连接到所有其他节点)以减少计算次数。</li><li id="322c" class="nq nr iq jp b jq nz ju oa jy ob kc oc kg od kk nv nw nx ny bi translated">了解了术语<strong class="jp ir">内核</strong>，它是一个占用空间很小的矩阵，可以让我们基于前一层的局部补丁计算一个新的隐藏值。<strong class="jp ir">内核的高度</strong>和<strong class="jp ir">宽度</strong>很重要，因为它们决定了每个输出单元有多少单元。</li><li id="3e27" class="nq nr iq jp b jq nz ju oa jy ob kc oc kg od kk nv nw nx ny bi translated">利用<strong class="jp ir">参数共享</strong>在我们图像的各个区域共享内核矩阵。</li><li id="5ff4" class="nq nr iq jp b jq nz ju oa jy ob kc oc kg od kk nv nw nx ny bi translated">为上述过程的组合创造了术语<strong class="jp ir">卷积</strong>。</li><li id="b5bf" class="nq nr iq jp b jq nz ju oa jy ob kc oc kg od kk nv nw nx ny bi translated">了解了<strong class="jp ir">池化</strong>和<strong class="jp ir">跨步</strong>，两种减少图像空间宽度的方法。</li><li id="d3cd" class="nq nr iq jp b jq nz ju oa jy ob kc oc kg od kk nv nw nx ny bi translated">利用医学图像数据集上的卷积和池实现网络。</li></ul><p id="3342" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">软件工程师可选后续岗位:<br/> </strong>重数学、重 CS 的详解(上):<a class="ae kl" href="https://medium.com/@leetandata/neural-network-introduction-for-software-engineers-1611d382c6aa" rel="noopener">https://medium . com/@ leetandata/neural-network-introduction-for-Software-Engineers-1611d 382 C6 aa</a></p><p id="c78e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">数学-CS-重详解(下):</strong><a class="ae kl" href="https://medium.com/@leetandata/neural-network-for-software-engineers-2-mini-batch-training-and-validation-46ee0a1269a0" rel="noopener">https://medium . com/@ leetandata/neural-network-for-software-engineers-2-mini-batch-training-and-validation-46ee 0a 1269 a 0</a></p><p id="fc27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">Cho 教授可选数学笔记:</strong><br/><a class="ae kl" href="https://github.com/nyu-dl/Intro_to_ML_Lecture_Note/raw/master/lecture_note.pdf" rel="noopener ugc nofollow" target="_blank">https://github . com/NYU-dl/Intro _ to _ ML _ Lecture _ Note/raw/master/Lecture _ Note . pdf</a></p></div></div>    
</body>
</html>