<html>
<head>
<title>eXtreme Deep Factorization Machine(xDeepFM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">极端深度因子分解机器(xDeepFM)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/extreme-deep-factorization-machine-xdeepfm-1ba180a6de78?source=collection_archive---------5-----------------------#2019-12-12">https://towardsdatascience.com/extreme-deep-factorization-machine-xdeepfm-1ba180a6de78?source=collection_archive---------5-----------------------#2019-12-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bd84" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">推荐系统领域的新热点</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/cca50f60532c119f06acbecfb05304bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-s4dxT3_XxKZ2fK-8eG3Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@nate_dumlao?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Nathan Dumlao</a> on <a class="ae kv" href="https://unsplash.com/s/photos/choice?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5648" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们生活在一个被选择宠坏的时代。无论是食物、音乐还是娱乐，我们拥有的选择之多简直令人难以置信。但是由于推荐引擎为这些应用/网站提供了燃料，这些备选方案以排名列表的形式提供给我们。在这篇博客中，我们将讨论一种新的推荐算法，称为极端深度分解机器(xDeepFM)。</p><p id="c393" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本博客组织如下:</p><ol class=""><li id="faf8" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><em class="mb">当前生产中的推荐系统</em></li><li id="ecd8" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><em class="mb">介绍 xDeepFM </em></li><li id="5865" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><em class="mb">介绍 CIN(xDeepFM 的核心支柱)<br/> 3.1 CIN 特性<br/> 3.2 CIN 隐层<br/> 3.3 与 RNN 相似<br/> 3.4 与 CNN 相似<br/> 3.5 隐层的池化与串接<br/>3.6 xDeepFM 的输出</em></li><li id="0681" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><em class="mb"> CIN 复杂度<br/> 4.1 空间复杂度<br/> 4.2 时间复杂度</em></li></ol><p id="7554" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb"> 5。使用 deepCTR 库的编码示例</em></p><p id="6004" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb"> 6。参考文献</em></p><p id="38e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">7。概要</p><p id="2c63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">开始吧！！</p><h1 id="4530" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">1.当前推荐系统</h1><p id="f426" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">当前的推荐格局由基于 FM/DNN 的模型主导。但是也出现了一些融合 FM 和 DNN 系统的混合架构。</p><p id="f93e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb"> 1。基于因式分解机(FM)的方法</em></p><ul class=""><li id="8b73" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr ne ly lz ma bi translated">+ve:自动学习组合特征的模式</li><li id="3c0f" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">+ve:很好地概括看不见的特征</li><li id="bf85" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">-ve:试图捕获所有导致无用交互学习的特征交互。这可能会引入噪声。</li><li id="a46d" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">示例:推荐领域最受信任的主力</li></ul><p id="bef5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb"> 2。基于深度神经网络(DNN)的方法</em></p><ul class=""><li id="b5de" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr ne ly lz ma bi translated">+ve:学习复杂和选择性的功能交互</li><li id="f3b8" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">-ve:功能交互是在元素级别上建模的。一种热编码用于分类变量，以在维度 d 中表示它们。这将被馈送到完全连接的层中。这与基于 FM 的方法形成鲜明对比，基于 FM 的方法在向量级别(用户向量*项目向量)对特征交互进行建模。</li><li id="ca9a" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">例如:神经协同过滤的 DNN 部分(NCF)，Youtube 推荐的 DNN</li></ul><p id="77a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb"> 3。DNN +调频(混合)方法</em></p><ul class=""><li id="34f6" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr ne ly lz ma bi translated">+ve:学习低/高阶特征交互</li><li id="5f3e" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">例如:宽深度网络(Youtube)、神经协同过滤(NCF)、深度和交叉网络(DCN)、深度分解机(DeepFM)和极端深度分解机(xDeepFM)</li></ul><p id="41b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以看看这篇文章，了解一下神经协同过滤(NCF)的概况。这是所有混合动力车型中被引用最多的。</p><div class="nf ng gp gr nh ni"><a href="https://medium.com/@abhisheksharma_57055/neural-collaborative-filtering-96cef1009401" rel="noopener follow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">神经协同过滤</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">基于神经网络的增压协同过滤</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">medium.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw kp ni"/></div></div></a></div><p id="bea6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有混合类别的例子都使用 DNN 来学习隐式的按位特征交互。它们的不同之处在于如何学习高阶特征相互作用。</p><h1 id="1963" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">2.xDeepFM 简介</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/93c5bf974825cadcdbb61b40f6416011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v1laVsJx-PuVTq09S_5GWQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 1: The architecture of xDeepFM</figcaption></figure><p id="369b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">xDeepFM 由 3 部分组成:</p><ol class=""><li id="adcf" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">线性模型(直接在原始输入要素上工作)</li><li id="08c9" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">普通 DNN(在密集要素嵌入的基础上工作)</li><li id="6367" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">CIN(在密集特征嵌入之上工作)</li></ol><blockquote class="nx ny nz"><p id="99e5" class="kw kx mb ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated">在这三个国家中，CIN 是 xDeepFM 独有的。</p></blockquote><h1 id="442d" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">3.介绍<strong class="ak"><em class="od">【CIN】</em></strong>压缩互动网络</h1><p id="cbe8" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">基于 DNN 的系统的问题是它们隐含地学习高阶交互。在 xDeepFM 中，通过压缩交互网络(CIN)学习显式高阶特征交互。</p><h2 id="b614" class="oe mi iq bd mj of og dn mn oh oi dp mr lf oj ok mt lj ol om mv ln on oo mx op bi translated">3.1 CIN 的特点</h2><p id="a507" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">xDeepFM 推荐 CIN 是因为以下好处:</p><ul class=""><li id="8834" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr ne ly lz ma bi translated">它在向量层次上学习功能交互，而不是在逐位层次上。</li><li id="b062" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">它明确地测量高阶特征相互作用。</li><li id="aaa7" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">它的复杂性不会随着相互作用的程度呈指数增长。</li></ul><h2 id="08cb" class="oe mi iq bd mj of og dn mn oh oi dp mr lf oj ok mt lj ol om mv ln on oo mx op bi translated">3.2 CIN 隐藏层</h2><p id="98ee" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">CIN 通过其隐藏层(图 4 中的每个 x 是一个隐藏层)学习显式特征交互，其定义如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/0389260547832ddaee8ac7d5739e5364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*tPW2MbTBJLPFgG-3HTy2Mw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation 1</figcaption></figure><p id="e590" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> X(k-1，i) </strong> <em class="mb"> : </em>第 k 层(k-1 隐层)第 I 个场的嵌入向量<br/> <strong class="ky ir"> <em class="mb"> X(0，j) </em> </strong>:第 k 层第 j 个场的嵌入向量(基嵌入/原始特征嵌入)<br/> <strong class="ky ir"> <em class="mb"> W(k，H，I，j) </em> </strong>:维数为 m * H(k-1)的可学习参数</p><blockquote class="nx ny nz"><p id="015f" class="kw kx mb ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated"><em class="iq"> X(k) </em>中的每一行仅用 W(k，h)来区分。与 DNN 相反，没有激活来转换隐藏层。</p></blockquote><h2 id="2e7d" class="oe mi iq bd mj of og dn mn oh oi dp mr lf oj ok mt lj ol om mv ln on oo mx op bi translated">3.3 与 RNN 相似</h2><p id="3687" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">等式 1 类似于基于 RNN 的模型，因为 CIN 的输出取决于最后一个隐藏层和附加输入。</p><h2 id="2f11" class="oe mi iq bd mj of og dn mn oh oi dp mr lf oj ok mt lj ol om mv ln on oo mx op bi translated"><strong class="ak"> <em class="od"> 3.4 与 CNN </em> </strong>相似</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e7f552ff5cd454f1dc1d7a35d019f831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*EpgHdXQWg9vXNCeflHzVgg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 2: Outer products along each dimension for feature interactions</figcaption></figure><p id="c8d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> m: </strong> <em class="mb"> </em>原始特征矩阵的行数<br/> <strong class="ky ir"> H(k): </strong>隐藏层 k 的特征矩阵的行数</p><blockquote class="nx ny nz"><p id="e304" class="kw kx mb ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated">Z(k+1)是一个中间张量，它是隐藏层 X(k)和原始特征矩阵 X(0)的外积。Z(k+1)可以看作一种特殊类型的图像，W(k，h)可以看作一种滤波器。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/baf8bafb01e1fdac380d0b5889c6b2b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*v_rjF0CPaFDy3PobgYIt5w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 3: The k-th layer of CIN. It compresses the intermediate tensor from H(K)*m to H(k+1)</figcaption></figure><p id="80c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当你沿着维度 D(每个嵌入特征的大小)滑动权重矩阵时，你得到一个隐藏向量 X(k+1，I)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/09cee77fac728e887bad0bd7b49cf310.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*fOS_xmim9ZhNQYhC4BIkVg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 4: This is the expansion of CIN component of Figure 1</figcaption></figure><p id="ed7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> T: </strong>网络的深度</p><h2 id="4cc1" class="oe mi iq bd mj of og dn mn oh oi dp mr lf oj ok mt lj ol om mv ln on oo mx op bi translated">3.5 隐藏层的汇集和连接</h2><p id="625a" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">隐藏层的结果按行相加，然后在馈送到输出层之前连接在一起。</p><ol class=""><li id="c873" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">每个隐藏层都与输出单元有联系</li><li id="0cc8" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">总和池应用于每个隐藏层，以创建一个池向量，如下所示:</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/21a153067439dc2c23108ae7a98a02f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*L94H51ebnpFZCZxJveM4mA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation 2:</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/defb4699e8b9c143ab48c76cffdd01b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*dBq4OfEOD5Re-_85jCmSZg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Pooling Vector of length H(k) at layer k</figcaption></figure><p id="d846" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">隐藏层中的所有池向量在连接到输出单元之前被连接:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/df60726e0981ca87137cdac2e70150f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*F4uUeOAIm93nj_YJ94R4Qg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Concatenation of pooling vectors of all layers</figcaption></figure><h2 id="ccd1" class="oe mi iq bd mj of og dn mn oh oi dp mr lf oj ok mt lj ol om mv ln on oo mx op bi translated">3.6 xDeepFM 的输出方程</h2><p id="7133" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">线性、CIN 和 DNN 都是用如下输出等式并行训练的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/c140972462ea175e9b00e0aeffc0702d.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*4aHqU-JarYtx-xi5Hf6nng.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation 3: Output Function of CIN</figcaption></figure><p id="b867" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mb">适马:</em></strong>s 形函数<br/> <strong class="ky ir"> <em class="mb"> a: </em> </strong>原始特征<br/> <strong class="ky ir"> W，b: </strong>可学参数</p><blockquote class="nx ny nz"><p id="5a77" class="kw kx mb ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated">基于 DNN 和 CIN 的层可以很好地互相补充，一起学习</p><p id="fc71" class="kw kx mb ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated">1.隐式(DNN)和显式(CIN)特征交互。</p><p id="6717" class="kw kx mb ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated">2.低阶(DNN)和高阶(两者)特征相互作用。</p></blockquote><h1 id="6b74" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">4.CIN 空间和时间复杂性</h1><h2 id="1f0b" class="oe mi iq bd mj of og dn mn oh oi dp mr lf oj ok mt lj ol om mv ln on oo mx op bi translated">4.1 <strong class="ak"> <em class="od">空间复杂度</em> </strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f380eb094752625c791f0f38daf668a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*RJDPzQLrLnONtiF6gVc75Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Space Complexity of CIN</figcaption></figure><p id="4b75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一项表示输出层的参数数量<br/>第二项表示每层 k <br/>的参数数量，如果我们假设所有隐藏层具有相同数量的特征向量 H。参数数量可以表示为 O(m*H*H*T)</p><h2 id="fd79" class="oe mi iq bd mj of og dn mn oh oi dp mr lf oj ok mt lj ol om mv ln on oo mx op bi translated">4.2 <strong class="ak"> <em class="od">时间复杂度</em> </strong></h2><p id="a7fe" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">时间复杂度相当于 O(mHD * H * T)。<br/> <strong class="ky ir"> <em class="mb"> mhd: </em> </strong>计算 1 行 Z(k+1，H)的成本<br/><strong class="ky ir"><em class="mb">H:</em></strong>H 层的特征向量(行数)<br/><strong class="ky ir"><em class="mb">T:</em></strong>CIN 的隐藏层数</p><p id="94b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">至此，我们完成了 xdeepfm 的理论。是时候看看它的实际效果了。</p><h1 id="6268" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">5.编码示例</h1><p id="851f" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">我使用了 deepCTR 库中的 xDeepFM 实现。对于其他一些实现，请查看参考部分。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="61f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从<a class="ae kv" href="https://github.com/shenweichen/DeepCTR/blob/master/examples/criteo_sample.txt" rel="noopener ugc nofollow" target="_blank">这里</a>下载样本数据，然后使用以下代码读取数据并将特征分为密集或稀疏。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="95f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">稀疏特征需要嵌入，而密集特征需要归一化。我们使用 MinMax scaler 进行标准化，而 LabelEncoder 进行特征编码。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/32c55958f68f933ca45221b7a0416c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*scK8uqtStpczXAY32FezQA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">This is how sparse_feature_columns[0] look</figcaption></figure><pre class="kg kh ki kj gt pc pd pe pf aw pg bi"><span id="97be" class="oe mi iq pd b gy ph pi l pj pk"># creating a dense feat<br/>dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pl"><img src="../Images/7c5d8253918ccfa26fb9fec35c5a5fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M9yLcidrujpHA8kAuaQazg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">This is how dense_feature_columns[0] looks like</figcaption></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="a8ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下一个代码 blob 中，我们将使用 xDeepFM 进行初始化、编译、训练和预测</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz pa l"/></div></figure><h1 id="1896" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">6.参考</h1><ol class=""><li id="a5db" class="ls lt iq ky b kz mz lc na lf pm lj pn ln po lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/abs/1803.05170" rel="noopener ugc nofollow" target="_blank"> xDeepFM 论文</a></li><li id="75b6" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">一些现成的 xDeepFM 实现有:- <br/> 1。<a class="ae kv" href="https://github.com/Leavingseason/xDeepFM" rel="noopener ugc nofollow" target="_blank">代码来自论文作者 xDeepFM </a> <br/> 2。<a class="ae kv" href="https://github.com/microsoft/recommenders" rel="noopener ugc nofollow" target="_blank">来自微软推荐库</a> <br/> 3 的代码。<a class="ae kv" href="https://github.com/shenweichen/DeepCTR" rel="noopener ugc nofollow" target="_blank"> deepctr </a>用于以下实施</li></ol><h1 id="c040" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">7.摘要</h1><p id="bd4b" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">xDeepFM 是混合架构的一个例子。它将 MF 和 DNN 结合起来，以得到更好的性能。这是通过使用 CIN(压缩交互网络)实现的。CIN 有两种特殊的美德:</p><ol class=""><li id="b1b9" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">它可以有效地学习有界度特征交互。</li><li id="7d25" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">它在向量水平上学习特征交互。</li></ol><p id="5e1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像其他流行的混合方法一样，xDeepFM 将 CIN 和 DNN 结合在一起。<br/>由此可以显式和隐式地学习高阶特征交互。这减少了手动特征工程的需要。</p><p id="c74b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我会敦促你尝试一下微软的库或 xDeepFM 作者的 T2 代码，尝试不同的 xDeepFM 实现。</p><p id="3b7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你也可以看看<a class="ae kv" href="https://medium.com/@abhisheksharma_57055/neural-collaborative-filtering-96cef1009401" rel="noopener">这篇</a>帖子来简要了解 NCF，这是另一种流行的混合架构。</p><div class="nf ng gp gr nh ni"><a href="https://medium.com/@abhisheksharma_57055/neural-collaborative-filtering-96cef1009401" rel="noopener follow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">神经协同过滤</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">基于神经网络的增压协同过滤</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">medium.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw kp ni"/></div></div></a></div></div><div class="ab cl pp pq hu pr" role="separator"><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu"/></div><div class="ij ik il im in"><p id="7c72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请在评论区分享你的想法和主意。</p></div></div>    
</body>
</html>