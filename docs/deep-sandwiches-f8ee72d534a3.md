# 厚三明治

> 原文：<https://towardsdatascience.com/deep-sandwiches-f8ee72d534a3?source=collection_archive---------15----------------------->

![](img/5d637eecc99b239db678281d1d5e878e.png)

Picture a room with grid lines on the walls, and look at the corner. If you’ve ever tried to learn some A.I., you’ll have heard that deep neural networks are “stacks of layers built from linear transformations followed by rectified nonlinearities.” All that jargon hides a simple idea. Our room is just the “graph” of one layer. Each layer takes a grid of squares, and bends it to look like the room. As intimidating as the mathematics of modern A.I. can seem from the outside, behind the scenes we’re mostly just taking the space where your data lives and then doing the room thing over and over. (Photo by [Kevin Laminto](https://unsplash.com/@kxvn_lx) on [Unsplash](https://unsplash.com/photos/ui4sf7FzJLE)).

想象一下，通过食物基本定律的奇迹，有人发现我们可以制作任何我们想要的东西，只要用两种原料:面包和火腿制作三明治。如果我们只是单独堆放面包，这种现象就不会出现。一大堆面包只是，嗯，面包。但是假设我们发现用正确的烹饪程序，一个足够大的顺序面包+火腿+面包+火腿+等等。可以变成鸡汤、千层面或鱼子酱——或者人类厨师从未想过要创造的食物。

如果这个例子看起来不可信，很好！大多数人工智能研究人员也没有预见到它的到来。但从 2012 年(或 2009 年(或 20 世纪 80 年代(或 20 世纪 60 年代，取决于你开始计算的时间))开始，某个专注于“如何从数据中学习东西”的数学领域有了自己的“不可能的三明治”时刻。从那时起，那个领域的“杂货店”，有着各种不同的成分，用于不同的特定目的，在“通用火腿”的稳步前进下，开始变空并改变形式。在非常真实的意义上，这就是最近围绕深度学习的所有炒作的全部内容。

人工神经网络是我们的三明治。深度神经网络只是有很多层的三明治。什么算“深”？那并不重要。为了理解最近围绕深度学习的所有炒作的原因，我们真正需要理解的是(1)“成分”，和(2)“烹饪”过程。

“面包”层就是数学家所说的线性变换。想象空间是一张有网格线的有弹性的图纸。任何扰乱空间的方式，其中之前是线的东西保持在之后是线，都是面包的一个例子。所以，旋转是面包，因为当我们旋转空间时，任何一条线都是一条线。通过把所有东西都变大一倍(或一半，等等)来延伸空间。)是另一种面包。我们也可以抓住空间的两边，向下移动我们的左手，向上移动我们的右手，这样垂直线保持垂直，水平线倾斜，小格子变成平行四边形。那叫剪羊毛，也是一种面包。这差不多是所有的面包了。如果我们把一堆这样的东西一个接一个地堆叠起来，那么由于每片面包都保持线条作为线条，整个堆叠也会保持线条作为线条。

所以，一堆面包不是三明治。仅仅靠堆积面包本身，我们不会取得任何进展[1–3]。那么，什么是“火腿”？

事实证明火腿几乎可以是任何东西。重要的是，这是一些步骤，弯曲网格线，使他们不再是线。所以，一个随机选择的三明治会旋转/缩放/剪切空间，然后弯曲一些网格线，然后再次旋转/缩放/剪切，然后弯曲更多的网格线。如果我们把三明治做得足够“深”，我们可以想象原来的空间可能会弯曲并折叠很多。

目前尚不清楚的是，我们究竟应该如何利用这一切完成任何事情。

# **现实世界中的深度神经网络**

现实世界的使用问题都归结于烹饪过程。

实际上，我们并不知道如何*写出*好的神经网络。我们知道如何*训练*他们。训练神经网络意味着向它展示我们希望它解决的问题的例子，然后告诉它如何稍微改变自己，使其更像一台机器，让*给出正确的答案。我们一遍又一遍地这样做，直到网络“学会”解决问题。*

这给了我们一种与计算机互动的新方式——一种通常被称为“软件 2.0”的方法[6]。有了软件 2.0，就好像你不知道如何烹饪千层面，但你有一些旧的千层面，你有一个神奇的烤箱——一个让你只需把一个大(即“深”)火腿三明治放在一堆千层面旁边，然后等一会儿，瞧！…是千层面。你刚刚“煮”了千层面吗？有点——还不清楚。显而易见的是，你现在又多了一份千层面，而不需要知道怎么做。

这对整个计算来说意味着*我们现在可以解决我们不知道如何解决的问题*。我们可以编写我们不知道如何编写的程序。委婉地说，这改变了一切。

这种方法在现实世界中的用例(和限制)或多或少是您从烹饪类比中所期望的。如果我们没有这些千层面样品展示给我们万能火腿，我们就不可能做出新的千层面。这在实践中意味着，深度学习最直接适用的用例是那些我们有很多很多例子的用例。也就是说，大数据集，沿着我们想要解决的任何问题的边界被清理和分离。如果我们想训练一个深度神经网络来检测图片中的猫，我们不需要知道如何编写代码来实现一个检测猫的视觉系统，一行一行，一条一条地。我们确实需要大量的猫的照片。加上一个巨大的未煮过的网络，和一些时间。

# 深度神经网络的市场采用

深度学习已经被你能说出的每个行业的市场所采用，并且它的采用没有放缓的迹象。你的手机中有深层网络，可以将语音转录成文本，听你说“好的，谷歌”或“嘿，Siri”。有网络可以检测相机视野中的人脸，并根据他们的位置和地点设置自动对焦。他们已经学会了如何阅读。他们是 Google Translate 等系统的支持者，Google Translate 曾经包含大量短语表以及每一对语言的可能翻译。他们不仅打败了国际象棋和围棋的世界冠军，而且更令人印象深刻的是(也很少有人强调)，他们打败了 30 多年来最优秀的人工智能研究人员试图通过使用特定领域的知识来设计手工编码的“智能”算法来玩这些游戏 AlphaZero 的最新版本不是通过对单个游戏的内置知识来实现超人的性能，而是通过让算法一次又一次地与自己对弈，直到它超越最熟练的人类和我们人类所构想的最佳算法。

甚至在科学研究中，以前不可逾越的障碍也开始消失。生物化学中最基本的问题之一是蛋白质折叠。这个问题对于药物研发来说至关重要，因为蛋白质在体内的功能是由其三维形状决定的。2018 年 12 月，谷歌的 DeepMind 发布了一个名为 alpha fold[7–8]的模型。他们训练了一个神经网络，根据其残基之间的成对距离来预测蛋白质的结构，这样做成功地远远超过了现有的方法，导致一位科学家评论说，在第一次听到结果时，他觉得自己和他所在领域的其他学者已经过时了[9-10]。

虽然距离我们学会在所有活动中超越人类智力还有很长一段时间，但有一点是肯定的:深度学习将会持续下去。人类生活的任何领域都不可能不变。

# 进一步阅读

**面包**
【1】3 蓝色 1 棕色。[线性变换和矩阵。](https://www.youtube.com/watch?v=kYB8IZa5AuE)
【2】3 蓝色 1 棕色。[矩阵相乘作为合成。](https://www.youtube.com/watch?v=XkY2DOUCWMU)
【3】3 蓝色 1 棕色。[三维线性变换。](https://www.youtube.com/watch?v=rHLEWRxRGiM)

**火腿**
【4】deep mind。[深度学习第三讲。神经网络基础。](https://www.youtube.com/watch?v=5eAXoPSBgnE&t=37m47s)(相关章节从 37:47 开始)。
【5】蒙图法尔，帕斯卡努，乔，&本吉奥(2014)。[关于深度神经网络的线性区域数。](https://arxiv.org/abs/1402.1869)

**烹饪**
【6】卡帕西。[软件 2.0。](https://medium.com/@karpathy/software-2-0-a64152b37c35)

**结果**
【7】罗伯特·f·服务。[谷歌的 DeepMind aces 蛋白质折叠。](https://www.sciencemag.org/news/2018/12/google-s-deepmind-aces-protein-folding)
【8】deep mind。 [AlphaFold:利用人工智能进行科学发现。](https://deepmind.com/blog/alphafold/)
【9】西加尔缪尔。一位科学家如何应对艾在他一生的工作中击败他。
【10】穆罕默德·阿尔库莱希。刚刚发生了什么？