<html>
<head>
<title>Why better weight initialization is important in neural networks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么更好的权重初始化在神经网络中很重要？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-better-weight-initialization-is-important-in-neural-networks-ff9acf01026d?source=collection_archive---------19-----------------------#2019-05-07">https://towardsdatascience.com/why-better-weight-initialization-is-important-in-neural-networks-ff9acf01026d?source=collection_archive---------19-----------------------#2019-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="afbd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我深度学习之旅的开始，我总是低估权重初始化。我认为权重应该初始化为随机值，而不知道为什么要随机初始化？为什么没有其他方法呢？权重初始化应该有多大的威力或意义？等等。</p><p id="316c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一系列的问题激发了我写博客的灵感，我将在博客中讨论不同的权重初始化技术，每种技术的优缺点，对个别技术的需求等。用更简单的方式。</p><blockquote class="ko kp kq"><p id="d82d" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu">补充说明</strong></p><p id="c4c4" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated">本文假设读者对神经网络的概念、前向和后向传播、激活函数、优化算法等有基本的了解。如果你不熟悉，那么我会推荐你关注<strong class="js iu">我的其他</strong> <strong class="js iu">关于这些话题的文章</strong>。</p><p id="a8b9" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/forward-propagation-in-neural-networks-simplified-math-and-code-version-bbcfef6f9250">神经网络中的前向传播——简化的数学和代码版本</a></p><p id="ecf3" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/analyzing-different-types-of-activation-functions-in-neural-networks-which-one-to-prefer-e11649256209">分析神经网络中不同类型的激活函数——选择哪一种？</a></p><p id="6e30" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/why-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096">为什么梯度下降还不够:神经网络优化算法综合介绍</a></p></blockquote><p id="ccff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在继续之前，首先让我们看看整篇文章中使用的参数符号。</p><h2 id="b64b" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated"><strong class="ak">使用的术语</strong></h2><p id="17bd" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated">考虑一个<em class="kr"> L </em> <em class="kr">层</em>网络，具有<em class="kr"> L-2 </em>(不包括输入输出层)<em class="kr">隐藏层</em>。任意层<em class="kr"> l </em>的<em class="kr">参数</em>表示为</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/ae50effe5b4d7e8fa3f857623155a136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*8gaXoS1YksVvRU8Ii8isaw.png"/></div></figure><p id="9ef6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了得到具有更好和最优结果的神经网络，权重初始化是首先要考虑的步骤。权重初始化不当的网络会使整个学习过程变得繁琐而耗时。因此，要实现更好的优化、更快的收敛，可行的学习过程权重初始化是非常关键的。现在让我们从一些重量初始化问题开始。</p><h1 id="a2ac" class="mc kx it bd ky md me mf lb mg mh mi le mj mk ml lh mm mn mo lk mp mq mr ln ms bi translated">为什么不简单地将所有权重初始化为零？</h1><p id="ac1d" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated">考虑一个场景，其中所有的<em class="kr">权重</em>被初始化为<strong class="js iu"> 0 </strong>。参考下面为<em class="kr">多类分类</em>问题设计的网络。使用来自<code class="fe mt mu mv mw b">sklearn.datasets</code>的<code class="fe mt mu mv mw b">make_blobs()</code>函数生成数据集。数据集有四个不同的类，包含两种不同的要素。网络有一个具有两个神经元的<em class="kr">隐藏</em>和<em class="kr">输入</em>层，一个具有四个神经元的<em class="kr">输出</em>层。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/88704ddd6e9294b9bab2b995f7022c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*kOPmEdmhjhhkn9AlfE3Jig.png"/></div></figure><p id="1ebe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中 W1 是 2×2 矩阵，W2 是 4×2 矩阵。B1 和 B2 分别是大小为 2 和 4 的列向量。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi my"><img src="../Images/3c1ea8f61c6722b01cfe43bcac7d20d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*5GMeqCRraYxKbI3lBCTP-w.png"/></div></figure><p id="ac63" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在隐藏层<em class="kr">预激活</em>期间</p><p id="c5b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">a11 = w1x1 + w2x2 + b1</p><p id="f15c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">a12 = w3x1 + w4x2 + b2</p><p id="05c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">a11 = a12(假设 b1 = b2)那么 h11 = h12</p><p id="0150" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">类似地，a21 = a22 = a23 =a24，那么 h21 = h22 = h23 =h24</p><p id="3d7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 w1 和 w3 的<em class="kr">反向传播</em>期间</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/2deb685e1d85a7267fc35ef8201de341.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*8ilmX0cYkaFG8IUS_DniyQ.png"/></div></figure><p id="12af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注- <em class="kr">此处所示 w1 和 w3 的梯度仅通过单路径计算，事实上，这些梯度是通过考虑多条可能路径上的所有导数计算的。</em></p><p id="aa27" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果梯度相等，那么权重将被更新相同的量。连接到同一神经元的权重在整个训练期间继续保持不变。它使隐藏的单元对称，这个问题被称为<strong class="js iu"> <em class="kr">对称问题</em> </strong>。</p><p id="f212" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，为了打破这种<strong class="js iu">对称性</strong>，连接到同一神经元的权重不应被初始化为相同的值。</p><p id="2e1f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">要点</strong></p><ul class=""><li id="4343" class="na nb it js b jt ju jx jy kb nc kf nd kj ne kn nf ng nh ni bi translated">切勿将所有重量初始化为零。</li><li id="36c9" class="na nb it js b jt nj jx nk kb nl kf nm kj nn kn nf ng nh ni bi translated">切勿将所有重量初始化为<strong class="js iu">相同的</strong>值。</li></ul><h1 id="68aa" class="mc kx it bd ky md me mf lb mg mh mi le mj mk ml lh mm mn mo lk mp mq mr ln ms bi translated">我们可以将权重初始化为大值吗？</h1><p id="960e" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated">如果权重被初始化为大值，并且如果使用了 sigmoid 激活函数(logistic，tanh ),则<em class="kr">饱和问题</em>可能发生，这导致<em class="kr">消失梯度问题。</em>因此，梯度变化缓慢，学习变得乏味。类似地<em class="kr">爆炸梯度问题</em>也可能发生。</p><p id="0525" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">要点</strong></p><ul class=""><li id="b6d9" class="na nb it js b jt ju jx jy kb nc kf nd kj ne kn nf ng nh ni bi translated">切勿将权重初始化为<strong class="js iu"> <em class="kr">大的</em> </strong>值。</li><li id="e2ee" class="na nb it js b jt nj jx nk kb nl kf nm kj nn kn nf ng nh ni bi translated">最好将<strong class="js iu"> <em class="kr">标准化</em> </strong> <em class="kr"> / </em> <strong class="js iu"> <em class="kr">标准化</em> </strong>输入，使它们位于一个小的公共范围内。</li></ul><h1 id="d671" class="mc kx it bd ky md me mf lb mg mh mi le mj mk ml lh mm mn mo lk mp mq mr ln ms bi translated">什么在实践中有效？</h1><p id="37d5" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated">知道了零初始化和高值初始化的问题后，还有什么可行的初始化方法呢？</p><p id="e535" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">嗯，我们可以按照特定的分布(均匀分布、正态分布、截尾正态分布等)随机初始化权重。</p><p id="1319" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，我可以在 Python 中使用<code class="fe mt mu mv mw b">np.random.randn(size_l, size_l-1)</code>来随机初始化权重，遵循均值为 0、标准差为 1 的标准正态分布。</p><p id="af5c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">但是对于更深更广的网络，隐含层神经元数量多，隐含层数量大，随机初始化可能会产生问题。</strong></p><p id="c3bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，如果在一个隐藏层中有<strong class="js iu"> <em class="kr"> n </em> </strong>个神经元，并且如果<strong class="js iu"> <em class="kr"> n </em> </strong>是一个非常大的数字，那么在预激活期间</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi no"><img src="../Images/a2d5731f7ec7151d5a939937195b4945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*EDdz_ODzFQF5Yr5HCXIafA.png"/></div></figure><p id="5855" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 a21 中，由于大量的神经元，所有这些加权和都会爆炸。因为 a21 值很高，所以会再次出现饱和问题，这导致梯度消失问题。类似地<em class="kr">爆炸梯度问题</em>也可能发生。</p><p id="77bd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">如何克服这一点？</strong></p><p id="5810" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了处理这些<em class="kr">梯度</em>问题，我们可以以合适的分布方式初始化随机权重。不使用标准的正态分布，而是使用具有 k/n 方差的正态分布，其中 k 是基于激活函数选择的。</p><p id="557b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kr"> Xavier Glorot </em>和<em class="kr"> He 等人</em>是更好的随机权重初始化这一概念的第一贡献者。</p><h1 id="310d" class="mc kx it bd ky md me mf lb mg mh mi le mj mk ml lh mm mn mo lk mp mq mr ln ms bi translated">tanh 和逻辑激活函数的 Xavier 初始化</h1><p id="15d4" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated"><em class="kr"> Xavier </em>提出了一种更好的随机权重初始化方法，该方法在初始化权重时还包括网络的大小(输入和输出神经元的数量)。</p><p id="ef84" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据这种方法，权重应该与前一层中神经元数量的平方根成反比。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi np"><img src="../Images/fafe51ad08f8da38f0406a5a30bacf63.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*ZLJiB88aR0zjgbt7kc5RVQ.png"/></div></figure><p id="7e24" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中权重可以在 Python 中初始化为<code class="fe mt mu mv mw b">np.random.randn(sizes[i-1],sizes[i])*np.sqrt(1/sizes[i-1])</code></p><p id="be56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">按照这种方法，隐藏层输入的加权和将不会取很大的值，并且减少了<em class="kr">消失</em> / <em class="kr">爆发梯度问题的机会。</em></p><h1 id="b090" class="mc kx it bd ky md me mf lb mg mh mi le mj mk ml lh mm mn mo lk mp mq mr ln ms bi translated">ReLu 和泄漏 ReLu 激活功能的初始化</h1><p id="be78" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated"><strong class="js iu"> <em class="kr"> He </em> </strong>初始化类似于<strong class="js iu"> <em class="kr"> Xavier </em> </strong>初始化，其中前一层中神经元的数量被赋予重要性。但是因子是乘以两个而不是一个<strong class="js iu">和<em class="kr">。</em></strong></p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/aceec84ef2d445b9483d5ffdc78d1c74.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*r1xULrvfItQ_Pt7RAH226A.png"/></div></figure><p id="3aec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中权重可以在 Python 中初始化为<code class="fe mt mu mv mw b">np.random.randn(sizes[i-1],sizes[i])*np.sqrt(2/sizes[i-1])</code></p><p id="41cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kr">濒死神经元</em> </strong>是<em class="kr"> ReLu 激活</em>功能的常见问题。使用 ReLu 激活功能，<strong class="js iu"> <em class="kr">多达 50% </em> </strong>的神经元可能在训练过程中<strong class="js iu"><em class="kr"/></strong>死亡。为了处理这种情况，在初始化中，该因子乘以<strong class="js iu">两个</strong>。</p><h2 id="a023" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">结束注释</h2><p id="404b" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated">在这个旅程中，到目前为止，我们已经讨论了各种<strong class="js iu"> <em class="kr">权重初始化</em> </strong>方法以及与每种方法相关的问题。建议使用<strong class="js iu"> <em class="kr"> Xavier 初始化</em><em class="kr">用于<strong class="js iu"/><em class="kr">sigmoid 基础</em>激活函数<strong class="js iu"> <em class="kr"> He 初始化</em> </strong>用于<strong class="js iu"> <em class="kr"> ReLu 和 Leaky ReLu </em> </strong> <em class="kr">激活函数</em>。</em></strong></p></div></div>    
</body>
</html>