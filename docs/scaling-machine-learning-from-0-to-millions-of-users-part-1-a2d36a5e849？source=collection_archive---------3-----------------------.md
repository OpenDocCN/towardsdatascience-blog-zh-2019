# 从 0 到数百万用户扩展机器学习—第 1 部分

> 原文：<https://towardsdatascience.com/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849?source=collection_archive---------3----------------------->

## 突破笔记本电脑

我认为大多数机器学习(ML)模型都是在白板或餐巾纸上构思出来的，诞生于笔记本电脑上。当这些羽翼未丰的生物开始咿咿呀呀地说出它们的第一个预言时，我们充满了自豪，并对它们未来的能力寄予厚望。唉，我们内心深处知道，并非所有人都会成功，远非如此。

当我们建造它们时，一小部分很快就让我们失望了。其他的看起来很有希望，并显示出一定程度的预测能力。然后，我们面临着在生产环境中部署它们的严峻挑战，在生产环境中，它们要么证明自己传奇的勇气，要么不光彩地死去。

![](img/2df04d74b8943c90661030c40417f710.png)

One day, your models will rule the world… if you read all these posts and pay attention ;)

在这一系列观点鲜明的帖子中，我们将讨论**如何训练 ML 模型并将其部署到生产中，从卑微的开始到统治世界**。在这一过程中，我们将努力采取公正合理的措施，与过度设计、炒作驱动的开发以及“为什么不直接用 XYZ 呢？”的邪恶势力作斗争。

> 尽可能享受您的数据科学沙盒的安全舒适，并为寒冷、严酷的生产世界做好准备。

# **第 0 天**

所以你想建立一个 ML 模型。嗯。让我们停下来想一想:

*   你的业务问题能否由**的高级 AWS 服务**解决，比如[亚马逊认证](http://aws.amazon.com/rekognition)、[亚马逊 Polly](http://aws.amazon.com/rekognition) 等。？
*   还是通过嵌入在其他 AWS 服务中的[不断增长的应用 ML 特性列表](https://medium.com/@julsimon/applying-machine-learning-to-aws-services-9768f926f11f)？

不要对此置之不理:**没有机器学习比没有机器学习更容易管理**。找到一种使用高级服务的方法可以为你节省几周甚至几个月的时间。

## 如果答案是“是”

请扪心自问:

*   为什么您要费尽周折构建一个冗余的定制解决方案呢？
*   你真的是“*缺失功能*”吗？什么是**真正的**业务影响？
*   你真的需要“*更高的精确度*”你怎么知道**你**能达到呢？

如果你不确定，为什么不用你自己的数据进行一次快速概念验证呢？这些服务是完全托管的(不需要…更多…服务器),并且非常容易集成到任何应用程序中。弄清楚它们不需要花太多时间，然后你就会有可靠的数据来做出有根据的决定，决定你是否真的需要训练你自己的模型。

> 如果这些服务对你来说足够好，那么恭喜你，你基本上完成了！如果你决定建造，我希望听到你的反馈。请保持联系。

## **如果这个答案是“否”**

请再问自己一次这个问题！我们大多数人都有扭曲现实和欺骗自己的惊人能力:)如果诚实的答案真的是“不”，那么我仍然建议考虑一下您可以使用高级服务的**子流程**，例如:

*   使用 [Amazon 翻译](http://aws.amazon.com/translate)支持的语言对，并使用您自己的解决方案来翻译其余的语言对。
*   在将人脸输入到你的模型之前，使用亚马逊的识别功能来检测人脸，
*   使用 [Amazon Textract](http://aws.amazon.com/textract) 来提取文本，然后将其提供给 NLP 模型。

这不是推销 AWS 服务(我看起来像销售人员吗？).我只是**想让你免于重新发明轮子**(或轮子的部件):你真的应该**专注于手头的业务问题**，而不是建造你在博客帖子中读到的或在会议上看到的卡片房子。是的，它在你的简历上可能看起来很棒，这个轮子最初是一个有趣的旋转木马…然后，它变成了**痛苦之轮，你被拴在它上面，别人拿着鞭子**。

![](img/78d62a263d7211429e72c16d29cdef64.png)

Why did I blindly trust that meetup talk? Crom! Help me escape and bash that guy’s skull with his laptop.

反正消极够了:)你确实需要一个模型，我们继续吧。

# 第一天:一个用户(你)

我们将从您在本地机器(或本地开发服务器)上训练模型的阶段开始我们的旅程，使用流行的开源库，如 [scikit-learn](https://scikit-learn.org) 、 [TensorFlow](https://www.tensorflow.org) 或 [Apache MXNet](https://mxnet.incubator.apache.org) 。也许你甚至已经实现了自己的算法(数据科学家，你们这些魔鬼)。

您已经使用测试集测量了模型的准确性，情况看起来不错。现在您想要将模型部署到生产中，以便检查它的实际行为，运行 A/B 测试，等等。从哪里开始？

## 批量预测还是实时预测？

首先，您应该弄清楚您的应用程序是否需要**批量预测**(即收集大量数据点，定期处理它们并将结果存储在某个地方)，或者**实时预测**(即向 web 服务发送一个数据点并接收一个即时预测)。我之所以提前提出这一点，是因为它对部署复杂性有很大的影响。

乍一看，实时预测听起来更有吸引力(因为…实时，耶！)，但它也带来了 web 服务固有的更强的需求:高可用性、处理流量突发的能力等。批处理更加轻松，因为它只需要时不时地运行:只要不丢失数据，没有人会看到它是否在中间被破坏了；)

缩放现在不是一个问题:您关心的只是部署您的模型、测试轮胎、运行一些性能测试等等。从我的经验来看，**您可能选择了最短的路线，将所有东西部署到一个 Amazon EC2 实例**。每个人都知道一点 Linux CLI，你在某处读到过使用“IaaS 将保护你免受邪恶的供应商锁定”。哈！那就 EC2 吧！

> 我听到 AWS 时空连续体中充满恐惧和怀疑的尖叫声，也许还有一些类似“哦，这太愚蠢了，没有人真的这么做！”。嗯，我敢打赌，到目前为止，大多数人都是这样开始的。如果你没有，恭喜你，但是请让我告诉这些好人，在他们真正伤害自己之前，哪条路是出路；)

所以，盯着我的魔镜，我看到…

## 批量预测

您已经将模型、批处理脚本和应用程序复制到 EC2 实例中。您的批处理脚本作为 cron 作业定期运行，并将预测数据保存到本地存储。您的应用程序在启动时加载模型和初始预测数据，并使用它来做任何它必须做的事情。它还会定期检查更新的预测，并在它们可用时加载它们。

## 实时预测

您已经将模型嵌入到应用程序中，在启动时加载它，并使用各种数据(用户输入、文件、API 等)提供预测。).

不管怎样，你现在正在云中运行预测，生活是美好的。你用一品脱黑啤酒来庆祝…或者可能是无麸质、公平交易的有机豆奶拿铁，因为毕竟是 2019 年了。

# 第一周:一个抱歉的用户(你)

该模型预测得很好，并且您希望投入更多时间来收集更多数据和添加功能。不幸的是，没过多久事情就变糟了，你现在**陷入了各种各样的问题**(下面是不完整的列表):

*   在您的笔记本电脑上进行培训并手动部署到云是一件痛苦且容易出错的事情。
*   您意外地终止了 EC2 实例，不得不从头开始重新安装。
*   你' *pip 安装了*-Python 库，现在你的 EC2 实例全乱了。
*   您必须为您的同事手动安装另外两个实例，现在您真的不能确定你们都在使用相同的环境。
*   您的第一个负载测试失败了，但是您不确定应该归咎于什么:应用程序？模特？阴间的古人巫师？
*   您希望在 TensorFlow 中实现相同的算法，也许在 Apache MXNet 中也是如此:更多的环境，更多的部署。没时间了。
*   当然还有大家的最爱:销售听说“你的产品现在有 AI 能力了”。你害怕他们会把它卖给一个客户，然后让你下周去大规模现场。

这样的例子不胜枚举。如果不是真的会很搞笑(欢迎在评论里补充自己的例子)。突然之间，这个 ML 的冒险听起来不那么令人兴奋了，不是吗？**你将大部分时间花在救火上，而不是建立最好的模型上**。不能这样下去了！

![](img/257de239613454bf842737a8b3d2fef8.png)

I’ve revoked your IAM credentials on ‘*TerminateInstances*’. Yes, even in the dev account. Any questions?

# 第 2 周:反击

团队中有人观看了这个非常酷的 AWS 视频，其中展示了一个名为[亚马逊 SageMaker](http://aws.amazon.com/sagemaker) 的新 ML 服务。您记住了这一点，但是现在，没有时间重新构建一切:销售人员正紧盯着您，几天后您有一个客户演示，您需要强化现有的解决方案。

很有可能，你还没有堆积如山的数据:训练可以等等。你需要专注于让预测变得可靠。这里有一些可靠的技术措施，实施起来不会超过几天。

## 使用深度学习 AMI

由 AWS 维护的这个[亚马逊机器映像](https://aws.amazon.com/machine-learning/amis/)带有**预装的**许多你可能需要的工具和库:开源、NVIDIA 驱动程序等。不必管理它们将为您节省大量时间，并且还将保证您的多个实例以相同的设置运行。

AMI 还附带了 [Conda](https://conda.io/en/latest/) **依赖和环境管理器**，它可以让您快速轻松地创建许多隔离的环境:这是一个用不同的 Python 版本或不同的库测试您的代码的好方法，而不会意外地破坏一切。

最后但同样重要的是，这个 AMI 是**免费的**，就像任何其他 AMI 一样，如果你*真的*有必要，你可以定制。

## 打破巨石

您的应用程序代码和您的预测代码有**不同的需求**。除非你有令人信服的理由这样做(超低延迟可能是一个)，否则它们不应该生活在同一个屋檐下。让我们来看一些原因:

*   **部署**:每次更新模型都要重启或者更新 app 吗？或者 ping 你的应用程序重新加载它或其他什么？不不不，保持简单:说到解耦，没有什么比构建独立的服务更好的了。
*   **性能**:如果您的应用程序代码在内存密集型实例上运行得最好，而您的 ML 模型需要 GPU，该怎么办？你将如何处理这种权衡？你为什么会偏爱其中一个？将它们分开让你**为每个用例**挑选最佳的实例类型。
*   **可伸缩性**:如果您的应用程序代码和您的模型具有不同的可伸缩性配置文件，那该怎么办？在 GPU 实例上横向扩展是一种耻辱，因为您的一小部分应用程序代码正在热运行…同样，最好将事情分开，这将有助于采取最**适当的扩展决策**以及降低成本。

那么，预处理/后处理代码呢，也就是说，你需要在预测之前和之后对数据采取的行动。它该何去何从？很难给出一个明确的答案:我会说**独立于模型的动作**(格式化、日志记录等)。)应该留在应用程序中，而**依赖于模型的动作**(特征工程)应该靠近模型以避免部署不一致。

## 构建预测服务

将预测代码从应用程序代码中分离出来并不一定很痛苦，您可以重用**可靠的、可扩展的工具**来构建预测服务。让我们来看看一些选项:

*   **Scikit-learn** :说到用 Python 构建 web 服务，我是 [Flask](http://flask.pocoo.org) 的忠实粉丝。它整洁、简单并且可伸缩性好。不用再找了。你的代码应该是这样的。

```
# My awesome API
from flask import Flask
import pickleapp = Flask(__name__)
model = pickle.load(open("my_awesome_model.sav", 'rb'))
...
@app.route('/predict', methods=['POST'])
def predict():
    # Grab data from the HTTP request
    ...
    model.predict(...)
    ...
```

*   **张量流**:不需要编码！您可以使用[**tensor flow Serving**](https://www.tensorflow.org/serving/)来提供大规模预测。一旦训练好模型并将其保存为正确的格式，为预测服务所需要做的就是:

```
docker run -p 8500:8500 \
--mount type=bind,source=/tmp/myModel,target=/models/myModel \
-e MODEL_NAME=myModel -t tensorflow/serving &
```

*   **Apache MXNet** :同样的，Apache MXNet 提供了一个 [**模型服务器**，](https://github.com/awslabs/mxnet-model-server)能够服务 MXNet 和 [**ONNX**](https://onnx.ai/) 模型(后者是 PyTorch、Caffe2 等支持的常用格式)。它既可以作为独立的应用程序运行，也可以在 Docker 容器中运行[。](https://github.com/awslabs/mxnet-model-server/blob/master/docker/README.md)

两个模型服务器都预装在**深度学习 AMI:** 这是使用它的另一个原因。为了简单起见，您可以将您的前/后处理留在应用程序中，并调用由模型服务器部署的模型。但是，有一个警告:这些模型服务器既不实现认证也不实现节流，所以请确保不要将它们直接暴露给互联网流量。

*   **还有什么**:如果你正在使用另一个环境(比如定制代码)或者非 web 架构(比如消息传递)，同样的模式应该适用:构建一个可以独立**部署和扩展的独立服务**。

## (可选)容器化您的应用程序

既然你已经决定拆分你的代码，我强烈建议你利用这个机会将不同的部分打包到 Docker 容器中:一个用于**训练**，一个用于**预测**，一个(或多个)用于**应用**。这个阶段严格来说没有必要，但是如果你能抽出时间，我相信过早的投资是值得的。

> 如果你一直生活在岩石下，或者从未真正关注过容器，现在可能是赶上的时候了:)我强烈推荐运行 [Docker 教程](https://docs.docker.com/get-started/)，它将教你为了我们的目的需要知道的一切。

容器使得跨不同环境(开发、测试、生产等)移动代码变得容易。)和实例。它们解决了各种依赖问题，即使您只管理少量实例，这些问题也会突然出现。随后，容器也将成为 Docker clusters 或 Amazon SageMaker 等大型解决方案的先决条件。

# 第二周结束

经过一个艰难的开始，事情正在好转！

*   深度学习 AMI 提供了一个稳定的、维护良好的基础。
*   容器帮助您移动和部署您的应用程序，比以前少了很多基础设施。
*   预测现在存在于您的应用程序之外，使得测试、部署和扩展更加简单。
*   如果您可以使用它们，模型服务器将为您省去编写预测服务的大部分麻烦。

不过，不要太激动。是的，我们回到了正轨，并准备好做更大的事情，但仍有大量的工作要做。那么**对多个实例的伸缩预测、** **高可用性**、**管理成本**等呢？当堆积如山的训练数据开始堆积时，我们该怎么办？面对现实吧，我们仅仅触及了表面。

“*老傻瓜！负载平衡器！自动缩放！自动化！*“我听到你的哭声。哦，你是说你又急着管理基础设施了？我以为你们想要机器学习。)

这个重磅炸弹，是时候收工了。[在下一篇](https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-2-80b0d1d7fc61)中，我们将开始比较和挑战更大规模 ML 培训的选项:**EC2**vs**ECS/EKS**vs**SageMaker**。一场史诗般的战斗，毫无疑问。

感谢阅读。同意吗？不同意？太好了！乐于在此讨论或在 [Twitter](https://twitter.com/julsimon) 上讨论。

有史以来最佳电影配乐。是的，魔戒团契只排第二:)