<html>
<head>
<title>AutoEncoder on Dimension Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/autoencoder-on-dimension-reduction-100f2c98608c?source=collection_archive---------6-----------------------#2019-10-11">https://towardsdatascience.com/autoencoder-on-dimension-reduction-100f2c98608c?source=collection_archive---------6-----------------------#2019-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="416d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对表格数据应用自动编码器的一个例子</h2></div><p id="705c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在特征工程过程中，特别是在一些比赛中，一种常见的情况是，一个人穷尽地尝试所有类型的特征组合，最终得到太多难以选择的特征。为了避免过度拟合，可以选择具有最高重要性的特征子集，或者应用一些降维技术。</p><p id="6686" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我依稀记得有一次 Kaggle 比赛，一等奖的解决方案是在降维中使用 autoencoder。因此，在这篇文章中，让我们来谈谈 autoencoder 以及如何将它应用于一般的表格数据。结构如下:</p><ol class=""><li id="47a0" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">通过一个简单的例子来理解自动编码器的概念</li><li id="42da" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">对竞赛数据应用自动编码器</li></ol><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/e13496345863e577d3de66c526c97182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2qaJIFF3ADlyKVAbD0Z3Q.jpeg"/></div></div></figure><h1 id="2a85" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">自动编码器的一个好例子</h1><p id="7af9" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">这里有一个关于自动编码器<a class="ae nb" href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener ugc nofollow" target="_blank">的很好的解释</a>。让我们从最基本的例子开始，说明 autoencoder 如何工作，然后将其应用于竞争数据中的一般用例。</p><p id="9297" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最基本的自动编码器遵循以下结构:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nc"><img src="../Images/644fd779bb185874bf775f24cdbf0f21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3CQ-377CR7I4ozCRGaOk7w.png"/></div></div></figure><p id="5469" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，输入和输出具有相同的维数(事实上，输入被用作输出的“标签”)，而隐藏层具有较少的维数，因此它包含输入层的压缩信息，这就是为什么它充当原始输入的降维。从隐藏层，神经网络能够解码信息到它的原始维度。从<code class="fe nd ne nf ng b">input_layer -&gt; hidden_layer</code>开始叫编码，<code class="fe nd ne nf ng b">hidden_layer -&gt; output_layer</code>叫解码。</p><p id="7df2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从某种意义上说，Autoencoder 是一种无监督学习，因为它不需要外部标签。编码和解码过程都发生在数据集内。</p><blockquote class="nh ni nj"><p id="8855" class="ki kj nk kk b kl km ju kn ko kp jx kq nl ks kt ku nm kw kx ky nn la lb lc ld im bi translated">自动编码器是从数据示例中自动学习的，这是一个有用的特性:这意味着很容易训练算法的专门实例，这些实例将在特定类型的输入上表现良好。它不需要任何新的工程，只需要适当的训练数据。</p></blockquote><p id="8f43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们通过一个例子来理解 autoencoder 的机制。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ededcc6856a70afe01efdf81fdefe714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P7aFcjaMGLwzTvjW3sD-5Q.jpeg"/></div></figure><p id="aaeb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用著名的<a class="ae nb" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据来看看图像是如何被压缩和恢复的。</p><h2 id="6262" class="np mf it bd mg nq nr dn mk ns nt dp mo kr nu nv mq kv nw nx ms kz ny nz mu oa bi translated">建立网络</h2><p id="f08e" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">这个例子来自 Keras 博客。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="a94a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的网络非常简单:大小为 784 的输入图像将通过密集层，并被编码为大小为 32 的图像，解码层将从该层恢复到大小为 784 的原始尺寸。结构简单如:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi od"><img src="../Images/ac13bde05ec576397a38c1d35db9115d.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/format:webp/1*cX5CtqXw3DcbwYyhkiznCA.png"/></div></figure><p id="b2c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了得到自动编码器的中间结果，我们还记录了<code class="fe nd ne nf ng b">encoded</code>。</p><h2 id="1390" class="np mf it bd mg nq nr dn mk ns nt dp mo kr nu nv mq kv nw nx ms kz ny nz mu oa bi translated">准备数据集和培训</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="64a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据被标准化为 0 和 1，并传递到我们的自动编码器。<strong class="kk iu">注意，输入和输出都是</strong> <code class="fe nd ne nf ng b"><strong class="kk iu">x_train</strong></code> <strong class="kk iu">，我们的想法是希望我们的编码层足够丰富，以恢复尽可能多的信息。</strong></p><h2 id="84f3" class="np mf it bd mg nq nr dn mk ns nt dp mo kr nu nv mq kv nw nx ms kz ny nz mu oa bi translated">想象结果</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="e231" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将原始图像与从我们的编码层恢复的图像进行比较。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi oe"><img src="../Images/6a688bb7f962da47af29487cc6198aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_muHPiDPrU5TLz-PvoyT3A.png"/></div></div></figure><p id="252f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我们的 32 维隐藏层能够恢复 784 维的图像，并能够很好地捕捉信息。</p><p id="ed1e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们将这种降维技术应用于一个竞争数据集。</p><h1 id="e9a3" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">表格数据的自动编码器</h1><p id="af49" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">在帖子<a class="ae nb" href="https://medium.com/@zhangyue9306/kaggle-competition-ncaa-match-prediction-65b46b8f3892" rel="noopener">这里</a>中，我们应用了一些通用特征工程技术，在数据集上生成了 170 多个特征。让我们试着降低它的维度。</p><p id="9ae0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在训练和验证中拆分数据，训练数据如下所示，有 171 列:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi of"><img src="../Images/99cafab19ca5216e2a5b97ece3107cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d8zPQw6bbgf_xBrhxWKS-g.png"/></div></div></figure><p id="d31a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们使用同样的技术，把维数减少到 40，</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="750f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nd ne nf ng b">encoder</code>将在以后用于降维。</p><p id="80a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们对降维后的维度进行预测，</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="925c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了进行比较，我们仍然应用 lightgbm 进行预测，结果是 0.595，只有 40 个特征，而之前是 0.57，有 171 个特征。虽然降维模型并没有优于上一个，但我相信我们看到了 autoencoder 的优势。(<a class="ae nb" href="https://github.com/MJeremy2017/Machine-Learning-Models/tree/master/AutoEncoder" rel="noopener ugc nofollow" target="_blank">代码</a>)</p><h1 id="857a" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">结论</h1><p id="1b14" class="pw-post-body-paragraph ki kj it kk b kl mw ju kn ko mx jx kq kr my kt ku kv mz kx ky kz na lb lc ld im bi translated">这里介绍的自动编码器是最基本的，在此基础上可以扩展到深度自动编码器和去噪自动编码器等。</p><p id="92e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">autoencoder 在竞争中的另一个优势是，人们可以基于训练和测试数据来构建 autoencoder，这意味着编码层也将包含来自测试数据的信息！我在 Kaggle 竞赛中读到的一等奖解决方案的帖子使用了去噪 autoencoder，方法是在原始特征中添加一些噪声，以使他的网络更加健壮，同时，他还将测试数据用于他的模型中，我相信这也是他获胜的原因。</p><p id="88b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考</strong>:</p><p id="6182" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]<a class="ae nb" href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener ugc nofollow" target="_blank">https://blog.keras.io/building-autoencoders-in-keras.html</a></p></div></div>    
</body>
</html>