<html>
<head>
<title>Beyond the Derivative — Subderivatives</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超越导数——次导数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beyond-the-derivative-subderivatives-1c4e5bf20679?source=collection_archive---------23-----------------------#2019-08-29">https://towardsdatascience.com/beyond-the-derivative-subderivatives-1c4e5bf20679?source=collection_archive---------23-----------------------#2019-08-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="90bb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">机器学习技术通常依赖于可微函数，但实际情况很少如此。我们能做些什么呢？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0a358ec6082408824735da61036f22a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bV8ZYTDzJYDGu2S_AJwv_A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Not every function is so nice!</figcaption></figure><h1 id="345f" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="45de" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如果你已经克服了理解机器学习的最初几个障碍，那么你很可能听说过一种被称为<strong class="lp ir">梯度下降</strong>的技术。</p><p id="6a25" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">当谈到最小化一个函数<em class="mo"> f </em>时，我们可以用下面的规则更新我们对最小值<em class="mo"> x </em>的猜测:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/5610571bdff319614bc25a7d53cd22d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_s8YPWr0x2ghZeHvd-XKA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Old faithful (gradient descent).</figcaption></figure><p id="dd31" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这一切都很好，但往往有一个重要的细节被掩盖了。我们的函数<em class="mo"> f </em>需要<strong class="lp ir">可微</strong>，这意味着我们可以计算它在所有可能的点<em class="mo"> x </em>的导数。</p><p id="68ae" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">停下来问自己:</strong>这是我们真的可以掩盖的细节吗？我是说，大多数函数都是可微的吗？退一步说，所有可能函数的空间是巨大的，你可能在街上遇到的典型函数可能是相当讨厌的。事实上，几乎每个函数在一个点上都是不可微的！这仍然是<a class="ae mq" href="http://homepages.math.uic.edu/~marker/math414/fs.pdf" rel="noopener ugc nofollow" target="_blank">即使你限制为连续函数的情况。</a></p><h1 id="75d9" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">损失函数的可微性</h1><p id="06a2" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如果你有足够的勇气点击那个链接，那么你会很快意识到我们正在进入技术领域。但这不是重点——希望我们能在这里阐明这一点。</p><p id="b26c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">记住，在机器学习中，正在优化的函数通常是<strong class="lp ir">损失函数。</strong>我们来看一个简单的损失函数:绝对值函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/0d905773fddb55bf2769fb22c502ad2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZqDVP_N2EBVRpYbqoUCW7Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">f(x) = |x|, absolutely.</figcaption></figure><p id="fdbf" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">如果你已经有一段时间没学微积分了，那么记住导数的一种解释是该点切线的<em class="mo">斜率。</em></p><p id="3abe" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">绝对值函数是<strong class="lp ir">不</strong>可微的函数的典型例子，特别是在点<em class="mo"> x = </em> 0 处。</p><p id="6018" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">如果你看看其他损失函数，你可能会惊讶地发现它们也是不可微的。那么是什么原因呢？</p><p id="52c6" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">技术性的是，对于机器学习问题，我们真的只需要我们的损失函数在几乎每一点都是可微的。这实际上是一个技术术语，它被称为几乎处处可微。</p><p id="5331" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">但这仍然只是一个小小的修正。如果我们想把像梯度下降这样的东西推广到<strong class="lp ir">不可微的函数上呢？</strong></p><h1 id="abdc" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">定义次导数</h1><p id="90f2" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如果你回头看看绝对值函数，你可以给出一个关于为什么它在<em class="mo"> x = </em> 0 处不可微的论点，因为在那一点没有唯一的切线斜率。</p><p id="be7a" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">把在<em class="mo"> x </em> = 0 处的函数想象成一种尖边，然后你可以想象一大串不同斜率的线接触原点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/4e7898d6194f45b71b73c58caa80a09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cZz-OPCIJymC1A3IrVKVOw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Play with this graph <a class="ae mq" href="https://www.desmos.com/calculator/shg8v05ven" rel="noopener ugc nofollow" target="_blank">here</a>!</figcaption></figure><p id="4bcc" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">让我们从切线斜率的概念出发，推广导数的概念。</p><p id="f733" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">次导数</strong>是导数的延伸。与导数不同，次导数不仅仅是一个数字，而是一个点的集合。形式上，一个点上的次导数是位于图中该点下的所有切线的斜率集合。</p><p id="70f9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">看一看。在下图中，点 x_0 处的函数(蓝色)至少有两条切线(红色)位于图形下方。这些切线的斜率构成了次导数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/7fcbc7203683fccf0dbffe88e523af57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9RE0bkJR1T_o5UE9vssgcA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The subderivative at a point is <strong class="bd mu">all slopes</strong> that lie beneath the graph.</figcaption></figure><p id="ed1e" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">它所要求的只是函数是<em class="mo">凸的</em>，这意味着在图上的任意两点之间画一条线保持在函数的同一侧。</p><p id="ddf0" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">为了把我们刚才所说的放到一个正式的定义中，我们可以把在一个点上的次导数(或次梯度)定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/ba4bca93c54cd21a0472b80450e2d3d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYw9R-sJEnCnR_WseP2H3Q.png"/></div></div></figure><p id="b0bd" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">这里使用了用于偏导数的相同符号，但是在这个上下文中，它用于表示点的集合，而不仅仅是单个值。</p><h1 id="bd91" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">含义</h1><p id="f6a0" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">那么现在我们已经定义了什么是次梯度，我们能继续修改梯度下降来得到…次梯度下降吗？事实上，我们可以！</p><p id="15ca" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">有许多<a class="ae mq" href="https://en.wikipedia.org/wiki/Subgradient_method" rel="noopener ugc nofollow" target="_blank">方法</a>使用次导数作为导数的替代，我不会阻止你自己去寻找这些方法。</p><p id="d56f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">随着机器学习技术变得越来越复杂，它们将更有可能依赖于不可微的损失函数和优化。因为我们仍然想解决这些问题，所以我们有可行的技术来处理这些情况变得更加重要。</p><p id="6d96" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">如果你想了解更多关于次梯度的知识，这里有很多关于它的好的幻灯片。这将是相当技术性的，但这不应该阻止你浏览或至少看漂亮的图片和方程！</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="f6a6" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">一如既往，欢迎在下方评论联系！过去我真的很高兴收到人们的来信，这有助于我聚焦未来的文章！</p></div></div>    
</body>
</html>