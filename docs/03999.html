<html>
<head>
<title>Deep Dive into Catboost Functionalities for Model Interpretation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入研究用于模型解释的 Catboost 功能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-into-catboost-functionalities-for-model-interpretation-7cdef669aeed?source=collection_archive---------2-----------------------#2019-06-24">https://towardsdatascience.com/deep-dive-into-catboost-functionalities-for-model-interpretation-7cdef669aeed?source=collection_archive---------2-----------------------#2019-06-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2bdd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们真的了解我们构建的 ML 模型内部发生了什么吗？我们来探索一下。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/508e4497b7acaaff425f4faa84a4c663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PVsk4fHxyT6vocqN"/></div></div></figure><p id="3da6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我之前的博客中，我们看到了 XGBoost 和 LightGBM 的对比研究。通过分析，我们可以得出结论，catboost 在速度和准确性方面都优于其他两个。在这一部分中，我们将深入研究 catboost，探索 catboost 为高效建模和理解超参数提供的新特性。</p><p id="29c1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于新读者，<a class="ae ln" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank"> catboost </a>是<a class="ae ln" href="https://en.wikipedia.org/wiki/Yandex" rel="noopener ugc nofollow" target="_blank"> Yandex </a>团队在 2017 年开发的开源梯度提升算法。这是一种机器学习算法，允许用户快速处理大型数据集的分类特征，这与 XGBoost &amp; LightGBM 不同。Catboost 可以用来解决<strong class="kt ir">回归、分类、排序问题</strong>。</p><p id="bcf3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作为数据科学家，我们可以轻松地训练模型并做出预测，但是，我们经常无法理解那些花哨的算法内部发生了什么。这也是为什么我们看到离线评测和最终生产的模型性能存在巨大差异的原因之一。我们早就应该停止将 ML 作为一个<strong class="kt ir">【黑箱】</strong>来对待，并在提高模型准确性的同时重视模型解释。这也将帮助我们识别数据偏差。在本部分中，我们将了解 catboost 如何通过以下功能帮助我们分析模型并提高可见性:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lo"><img src="../Images/b4c81672fdc40741a15b8567bdaf4065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XrQOPzH1aaaUQbubhnDURA.png"/></div></div></figure><h1 id="8872" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">特征重要性</h1><p id="7498" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated"><strong class="kt ir">为什么要知道？<br/></strong></p><p id="9dbb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">除了选择要素重要性的类型之外，我们还应该知道要使用哪些数据来确定要素的重要性——训练、测试或完整数据集。选择一个比另一个有利也有弊，但是最终，您需要决定您是否想要知道模型在多大程度上依赖每个特征来进行预测(<strong class="kt ir">使用训练数据</strong>)或者该特征在多大程度上有助于模型在看不见的数据上的性能(<strong class="kt ir">使用测试数据</strong>)。我们将在本博客的后面看到，只有一些方法可以用来发现不用于训练模型的数据的特征重要性。</p><p id="980b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你关心第二个，并假设你有所有的时间和资源，找到特性重要性的最粗略和最可靠的方法是训练多个模型，一次留下一个特性，并在测试集上比较性能。如果性能相对于基线(当我们使用所有特性时的性能)变化很大，这意味着该特性很重要<strong class="kt ir">。</strong>但是由于我们生活在一个需要优化准确性和计算时间的现实世界中，这种方法是不必要的。以下是 catboost 让您找到模型最佳功能的几种智能方法:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mm mn l"/></div></figure><h2 id="be96" class="mo lq iq bd lr mp mq dn lv mr ms dp lz la mt mu mb le mv mw md li mx my mf mz bi translated"><strong class="ak">预测值变化</strong></h2><p id="d3d3" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">对于每个要素，PredictionValuesChange 显示了当要素值发生变化时，预测的平均变化量。重要性的值越大，如果该特征被改变，则预测值的平均改变越大。</p><blockquote class="na nb nc"><p id="ca19" class="kr ks nd kt b ku kv jr kw kx ky ju kz ne lb lc ld nf lf lg lh ng lj lk ll lm ij bi translated">优点:计算成本很低，因为你不必进行多次训练或测试，也不用存储任何额外的信息。您将得到作为输出的<strong class="kt ir">标准化值</strong>(所有的重要性加起来为 100)。<br/> <strong class="kt ir">缺点:</strong>它可能会给排名目标带来误导性的结果，它可能会将 groupwise 功能放在顶部，即使它们对最终的损失值有一点影响。</p></blockquote><h2 id="4f00" class="mo lq iq bd lr mp mq dn lv mr ms dp lz la mt mu mb le mv mw md li mx my mf mz bi translated"><strong class="ak"> LossFunctionChange </strong></h2><p id="dd40" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">为了获得该特征的重要性，catboost 简单地采用在正常情况下(当我们包括该特征时)使用模型获得的度量(损失函数)和没有该特征的模型(该模型近似地使用原始模型构建，该特征从集合中的所有树中移除)之间的差异。差异越大，该特征越重要。catboost 文档中没有明确提到我们如何找到没有特征的模型。</p><blockquote class="na nb nc"><p id="3af8" class="kr ks nd kt b ku kv jr kw kx ky ju kz ne lb lc ld nf lf lg lh ng lj lk ll lm ij bi translated"><strong class="kt ir">优点&amp;缺点:</strong>这适用于大多数类型的问题，不像<code class="fe nh ni nj nk b"><em class="iq">predictionvalueschange</em></code>那样，在排序问题时会得到误导性的结果，同时，它的计算量很大。</p></blockquote><h2 id="478a" class="mo lq iq bd lr mp mq dn lv mr ms dp lz la mt mu mb le mv mw md li mx my mf mz bi translated">形状值</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/2bbba28d682de2580e0dd73f8fb409be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XOBU3sjOrNXitFbY.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk"><a class="ae ln" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">https://github.com/slundberg/shap</a></figcaption></figure><p id="df25" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae ln" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir">SHAP</strong></a><strong class="kt ir"/>值将预测值分解成每个特征的贡献。与基线预测(训练数据集的目标值的平均值)相比，它测量特征对单个预测值的影响。</p><p id="7285" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">shap 值的两个主要用例:</p><ol class=""><li id="7a6c" class="nq nr iq kt b ku kv kx ky la ns le nt li nu lm nv nw nx ny bi translated"><strong class="kt ir">特征的对象级贡献</strong></li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mm mn l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/01858e0433d8877cb707633d9f0569a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OV5X-ngPehQJd6Kt.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk"><a class="ae ln" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">https://github.com/slundberg/shap</a></figcaption></figure><p id="b155" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 2。整个数据集的摘要(整体特征重要性)</strong></p><pre class="kg kh ki kj gt oa nk ob oc aw od bi"><span id="5270" class="mo lq iq nk b gy oe of l og oh">shap.summary_plot(shap_values, X_test)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a32f3705da857560ebd0c5f6eb6f493e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*PUtxqFwPp51gGt56zxaqwA.png"/></div></figure><p id="154f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然我们可以通过 shap 获得准确的特征重要性，但它们在计算上比 catboost 内置的特征重要性更昂贵。关于 SHAP 值的更多细节，请阅读这个<a class="ae ln" href="https://www.kaggle.com/dansbecker/shap-values" rel="noopener ugc nofollow" target="_blank">内核</a>。</p><p id="abba" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">加成</strong> <br/>另一个基于相同概念但不同实现的特征重要性是——<a class="ae ln" href="https://christophm.github.io/interpretable-ml-book/feature-importance.html" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir">基于排列的特征重要性</strong> </a> <strong class="kt ir">。</strong>尽管 catboost 不使用这个，但这纯粹是<strong class="kt ir">与模型无关的</strong>并且易于计算。</p><h2 id="45b2" class="mo lq iq bd lr mp mq dn lv mr ms dp lz la mt mu mb le mv mw md li mx my mf mz bi translated">我们如何选择一个呢？</h2><p id="7ad0" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">虽然<code class="fe nh ni nj nk b">PredictionValuesChange</code> &amp; <code class="fe nh ni nj nk b">LossFunctionChange </code>都可用于所有类型的指标，但建议使用<code class="fe nh ni nj nk b">LossFunctionChange</code>对指标进行排名。除了<code class="fe nh ni nj nk b">PredictionValuesChange</code>之外，其他所有方法都可以使用测试数据，使用在训练数据上训练的模型来发现特征重要性。</p><p id="f1b6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了更好地理解差异，下面是我们讨论的所有方法的结果:</p><div class="kg kh ki kj gt ab cb"><figure class="oj kk ok ol om on oo paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/40a77f56a83a2560ba4c09f0a104a4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*SXXyixVgWIWQf8yZwaRCSQ.png"/></div></figure><figure class="oj kk op ol om on oo paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/0823d4bc0ffd9caa985e2c8abb956701.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*fzUVeMyUglFm4NxilHd09Q.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk oq di or os">Results of catboost feature imp. to predict if people will report over $50k of income from the classic <a class="ae ln" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">“adult” census dataset </a>(using log-loss)</figcaption></figure></div><div class="ab cb"><figure class="oj kk ot ol om on oo paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ee0edb35e16ec700684b0fbf82bd0cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*i9nvsKsNmthObS1_FIwWEw.png"/></div></figure><figure class="oj kk ou ol om on oo paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/0f00cad35faf50288d42220434778965.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*PUtxqFwPp51gGt56zxaqwA.png"/></div></figure></div><p id="2629" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从上面的图中，我们可以看到大多数方法在顶部特性上是一致的。看起来<code class="fe nh ni nj nk b">LossFunctionChange</code>最接近 shap(更可靠)。然而，直接比较这些方法是不公平的，因为<code class="fe nh ni nj nk b">predictionvalueschange</code>是基于列车数据，而所有其他方法都是基于测试数据。</p><p id="7c8e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们还应该看到运行所有这些应用程序所需的时间:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/38d7baccf3f268b28258f51a12477dcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oNORwlKkC3EjeSaThRWUNg.png"/></div></div></figure><h2 id="9017" class="mo lq iq bd lr mp mq dn lv mr ms dp lz la mt mu mb le mv mw md li mx my mf mz bi translated">互动</h2><p id="334f" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">使用此参数，<strong class="kt ir"> </strong>可以找到一对特征的强度(两个特征加在一起的重要性)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/86d657aaadd76e8f9fc5784f40f86ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*0j9_B8MNqqwGpO3MzPsbig.png"/></div></figure><p id="edd0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在输出中，您将获得每对要素的列表。该列表将有 3 个值，第一个值是该对中第一个要素的索引，第二个值是该对中第二个要素的索引，第三个值是该对的要素重要性分数。有关实现细节，请查看嵌入式笔记本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/7d59c6e13248deb7b8adec4f00fdf054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-r1vx7VJe9yZisi-_EI-A.png"/></div></div></figure><p id="0c61" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有趣的是，单个特征重要性中的前两个特征不一定是最强的一对。</p><h2 id="52e4" class="mo lq iq bd lr mp mq dn lv mr ms dp lz la mt mu mb le mv mw md li mx my mf mz bi translated">笔记本</h2><p id="f7c1" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated"><a class="ae ln" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">笔记本中使用的数据集</a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mm mn l"/></div></figure><h1 id="e40f" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">对象重要性</h1><h2 id="4d8a" class="mo lq iq bd lr mp mq dn lv mr ms dp lz la mt mu mb le mv mw md li mx my mf mz bi translated">你为什么要知道？</h2><ul class=""><li id="bbd5" class="nq nr iq kt b ku mh kx mi la oy le oz li pa lm pb nw nx ny bi translated">从训练数据中移除最无用的训练对象</li><li id="82c4" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated">根据哪些对象被认为是最“有帮助”的，对一批新对象进行优先排序，类似于主动学习</li></ul><p id="180d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">利用这个功能，您可以计算每个对象对测试数据的优化指标的影响。正值反映优化指标增加，负值反映优化指标减少。这个方法是在<a class="ae ln" href="http://This mode is an implementation of the approach described in the Finding Influential Training Samples for Gradient Boosted Decision Trees paper" rel="noopener ugc nofollow" target="_blank">这篇文章</a>中描述的方法的一个实现。这些算法的细节超出了本博客的范围。</p><h2 id="fa77" class="mo lq iq bd lr mp mq dn lv mr ms dp lz la mt mu mb le mv mw md li mx my mf mz bi translated">对象重要性的 Catboost 教程</h2><div class="ph pi gp gr pj pk"><a href="https://github.com/catboost/tutorials/blob/master/model_analysis/object_importance_tutorial.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd ir gy z fp pp fr fs pq fu fw ip bi translated">catboost/教程</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">CatBoost 教程存储库。在 GitHub 上创建一个帐户，为 catboost/教程开发做贡献。</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">github.com</p></div></div><div class="pt l"><div class="pu l pv pw px pt py kp pk"/></div></div></a></div><p id="a73e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><code class="fe nh ni nj nk b">cb.get_object_importance</code>中有三种类型的<code class="fe nh ni nj nk b">update_method</code>:</p><ul class=""><li id="a865" class="nq nr iq kt b ku kv kx ky la ns le nt li nu lm pb nw nx ny bi translated">单点:最快和最不精确的方法</li><li id="068c" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated">TopKLeaves:指定叶子的数量。该值越高，计算越精确，速度越慢</li><li id="9fd7" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated">AllPoints:最慢也是最准确的方法</li></ul><p id="f30e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">例如，以下值将方法设置为 TopKLeaves，并将叶子数限制为 3:</p><pre class="kg kh ki kj gt oa nk ob oc aw od bi"><span id="358e" class="mo lq iq nk b gy oe of l og oh">TopKLeaves:top=3</span></pre><h1 id="b9cf" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">模型分析图</h1><p id="d9ef" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">Catboost 最近在其最新更新<strong class="kt ir">中推出了这一功能。</strong>有了这个功能，我们将能够直观地看到算法如何分割每个特征的数据，并查看特定于特征的统计数据。更具体地说，我们将能够看到:</p><ul class=""><li id="aa23" class="nq nr iq kt b ku kv kx ky la ns le nt li nu lm pb nw nx ny bi translated">每个条柱(条柱用于连续要素)或类别(当前仅支持 OHE 要素)的平均目标值</li><li id="7ba3" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated">每个箱/类别的平均预测值</li><li id="501e" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated">每个箱中的对象数量</li><li id="01af" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated">不同特征值的预测:对于每个对象，特征值是变化的，以使其落入某个箱中。然后，该模型根据该特征的新值来预测目标，并在一个箱(由红点给出)中取预测的平均值。</li></ul><blockquote class="na nb nc"><p id="9832" class="kr ks nd kt b ku kv jr kw kx ky ju kz ne lb lc ld nf lf lg lh ng lj lk ll lm ij bi translated">该图将为我们提供信息，如我们的分割有多均匀(我们不希望所有对象都在一个容器中)，我们的预测是否接近目标(蓝色和橙色线)，红线将告诉我们我们的预测对某个特征有多敏感。</p></blockquote><h2 id="cf95" class="mo lq iq bd lr mp mq dn lv mr ms dp lz la mt mu mb le mv mw md li mx my mf mz bi translated">数字特征分析</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pz"><img src="../Images/5f6f8bffe7c89a1b159a376fe3088522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tKpZfdAeuK79qbfISr8FCQ.png"/></div></div></figure><h2 id="4f64" class="mo lq iq bd lr mp mq dn lv mr ms dp lz la mt mu mb le mv mw md li mx my mf mz bi translated">一键编码特征分析</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qa"><img src="../Images/19a888c5e336c0ac96e0c6beac912666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uvW-qsNSVjtiOSuykA3w-A.png"/></div></div></figure><p id="d00f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">感谢您阅读本文。希望下次您能够利用这些工具来更好地理解您的模型。</p><p id="0c35" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在机器学习社区对我之前关于<a class="ae ln" rel="noopener" target="_blank" href="/catboost-vs-light-gbm-vs-xgboost-5f93620723db">CatBoost vs . Light GBM vs . XGBoost</a>的博客做出积极回应后，CatBoost 团队联系我，看我是否有兴趣报道关于该库的更深入的主题。感谢 CatBoost 团队帮助回答我的问题！</p><p id="ecdc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">关于我:</strong>我目前是优步地图团队的一名数据科学家。如果你有兴趣在优步解决具有挑战性的问题，请通过<a class="ae ln" href="http://www.linkedin.com/in/alvira-swalin" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。你可以在这里阅读我的其他博客<a class="ae ln" href="https://medium.com/@aswalin" rel="noopener"/>。</p><h1 id="8736" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">参考</h1><ul class=""><li id="f6c6" class="nq nr iq kt b ku mh kx mi la oy le oz li pa lm pb nw nx ny bi translated"><a class="ae ln" href="https://tech.yandex.com/catboost/doc/dg/concepts/about-docpage/" rel="noopener ugc nofollow" target="_blank"> Catboost 文档</a></li><li id="5ca4" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated"><a class="ae ln" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> SHAP 价值观</a></li><li id="6970" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated"><a class="ae ln" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank"> Catboost 官网</a></li><li id="a031" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated"><a class="ae ln" href="https://arxiv.org/abs/1706.09516" rel="noopener ugc nofollow" target="_blank"> CatBoost 纸</a></li><li id="27c5" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated"><a class="ae ln" href="https://gist.github.com/aswalin/595ac73f91c6268f9ca449a4ee05dee1" rel="noopener ugc nofollow" target="_blank">笔记本</a></li><li id="6a46" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated"><a class="ae ln" href="https://arxiv.org/pdf/1802.06640.pdf" rel="noopener ugc nofollow" target="_blank">关于寻找有影响力的训练样本的论文</a></li><li id="5d3b" class="nq nr iq kt b ku pc kx pd la pe le pf li pg lm pb nw nx ny bi translated"><a class="ae ln" href="https://github.com/catboost/tutorials/tree/master/model_analysis" rel="noopener ugc nofollow" target="_blank"> Catboost 教程</a></li></ul></div></div>    
</body>
</html>