<html>
<head>
<title>KL Divergence Python Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">KL Divergence Python 示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810?source=collection_archive---------1-----------------------#2019-08-20">https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810?source=collection_archive---------1-----------------------#2019-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/5a4550f5254a97021fdf881839fd30a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DTkcUzAtx9gJVlzd"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@macroman?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Immo Wegmann</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="ea96" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着您作为数据科学家的职业生涯的进展，您将不可避免地遇到 kull back-lei bler(KL)分歧。我们可以把 KL 散度看作距离度量(尽管它是不对称的),它量化了两个概率分布之间的差异。这很有用的一个常见场景是当我们处理一个复杂的发行版时。与直接使用分布相比，我们可以使用另一种具有众所周知特性的分布(即正态分布)来更好地描述数据，从而使我们的生活更加轻松。换句话说，我们可以使用 KL 散度来判断是泊松分布还是正态分布更适合逼近数据。KL 散度也是<a class="ae jg" rel="noopener" target="_blank" href="/gaussian-mixture-models-d13a5e915c8e">高斯混合模型</a>和<a class="ae jg" rel="noopener" target="_blank" href="/t-sne-python-example-1ded9953f26"> t-SNE </a>的关键组成部分。</p><p id="e01c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<strong class="ki jk">连续随机变量</strong>的分布 P 和 Q，Kullback-Leibler 散度被计算为积分。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi le"><img src="../Images/10961f99ec9a1c34c14b5a78354d7d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*sl99uqWnmAcZw71aUgcAfQ.png"/></div></figure><p id="2a7b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，如果 P 和 Q 表示离散随机变量的概率分布，则 Kullback-Leibler 散度被计算为总和。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/3e5136cbb9e1265173983df8b0161f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*pqnS3f2aWcKSFqfXxx75JQ.png"/></div></figure><h1 id="fe17" class="lk ll jj bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">Python 代码</h1><p id="dac7" class="pw-post-body-paragraph kg kh jj ki b kj mi kl km kn mj kp kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">首先，我们导入以下库。</p><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="59e2" class="ms ll jj mo b gy mt mu l mv mw">import numpy as np<br/>from scipy.stats import norm<br/>from matplotlib import pyplot as plt<br/>import tensorflow as tf<br/>import seaborn as sns<br/>sns.set()</span></pre><p id="e0d5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们定义一个函数来计算两个概率分布的 KL 散度。我们需要确保不包含任何等于 0 的概率，因为 0 的对数是负无穷大。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/474df44badd5863619cd183a6f4f6eed.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*1VmW7HCrDYu-50w0.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://en.wikipedia.org/wiki/Binary_logarithm" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Binary_logarithm</a></figcaption></figure><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="e8d2" class="ms ll jj mo b gy mt mu l mv mw">def kl_divergence(p, q):<br/>    return np.sum(np.where(p != 0, p * np.log(p / q), 0))</span></pre><p id="ebae" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">均值为 0 且标准差为 2 的正态分布与均值为 2 且标准差为 2 的另一分布之间的 KL 散度等于 500。</p><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="fe8a" class="ms ll jj mo b gy mt mu l mv mw">x = np.arange(-10, 10, 0.001)<br/>p = norm.pdf(x, 0, 2)<br/>q = norm.pdf(x, 2, 2)</span><span id="54d9" class="ms ll jj mo b gy my mu l mv mw">plt.title('KL(P||Q) = %1.3f' % kl_divergence(p, q))<br/>plt.plot(x, p)<br/>plt.plot(x, q, c='red')</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/62a29991c5e22c7dc109d57115d00d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*HnVIctudlTZ2sheA9DaZ-A.png"/></div></figure><p id="3c6e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们测量初始概率分布和另一个平均值为 5、标准差为 4 的分布之间的 KL 散度，我们预计 KL 散度将高于上一个示例。</p><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="8153" class="ms ll jj mo b gy mt mu l mv mw">q = norm.pdf(x, 5, 4)</span><span id="67cb" class="ms ll jj mo b gy my mu l mv mw">plt.title('KL(P||Q) = %1.3f' % kl_divergence(p, q))<br/>plt.plot(x, p)<br/>plt.plot(x, q, c='red')</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/607451bcf3a76670dcab8be1026b4350.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*MoIAqCwC-26Hd42gyDF1SQ.png"/></div></figure><p id="e09c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">值得注意的是，KL 散度是不对称的。换句话说，如果我们把 P 换成 Q，反之亦然，我们会得到不同的结果。</p><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="bfe9" class="ms ll jj mo b gy mt mu l mv mw">q = norm.pdf(x, 5, 4)</span><span id="061b" class="ms ll jj mo b gy my mu l mv mw">plt.title('KL(P||Q) = %1.3f' % kl_divergence(q, p))<br/>plt.plot(x, p)<br/>plt.plot(x, q, c='red')</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5deffaf066dc1b734eecd1b8793b7fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*dQYmnPh7zMWulfLmvBf5Yg.png"/></div></figure><p id="510b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">KL 散度越低，两个分布就越接近。因此，在 t-SNE 和高斯混合模型的情况下，我们可以通过最小化一个分布相对于另一个分布的 KL 散度来估计该分布的高斯参数。</p><h2 id="b395" class="ms ll jj bd lm nc nd dn lq ne nf dp lu kr ng nh ly kv ni nj mc kz nk nl mg nm bi translated">最小化 KL 散度</h2><p id="c618" class="pw-post-body-paragraph kg kh jj ki b kj mi kl km kn mj kp kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">让我们看看如何利用梯度下降来最小化两个概率分布之间的 KL 散度。首先，我们创建一个具有已知均值(0)和方差(2)的概率分布。然后，我们用随机参数创建另一个分布。</p><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="02de" class="ms ll jj mo b gy mt mu l mv mw">x = np.arange(-10, 10, 0.001)<br/>p_pdf = norm.pdf(x, 0, 2).reshape(1, -1)<br/>np.random.seed(0)<br/>random_mean = np.random.randint(10, size=1)<br/>random_sigma = np.random.randint(10, size=1)<br/>random_pdf = norm.pdf(x, random_mean, random_sigma).reshape(1, -1)</span></pre><p id="fab6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们使用梯度下降，我们需要为超参数(即步长、迭代次数)选择值。</p><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="f1de" class="ms ll jj mo b gy mt mu l mv mw">learning_rate = 0.001<br/>epochs = 100</span></pre><p id="942a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像<code class="fe nn no np mo b">numpy</code>一样，在<code class="fe nn no np mo b">tensorflow</code>中我们需要为变量分配内存。对于变量<code class="fe nn no np mo b">q</code>，我们使用给定 mu 和 sigma 的正态分布方程，只是我们排除了指数之前的部分，因为我们正在归一化结果。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/18878375c4f0eeca840d45162025dbbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aPI1YDiN9lFv0RLF.jpg"/></div></div></figure><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="e4df" class="ms ll jj mo b gy mt mu l mv mw">p = tf.placeholder(tf.float64, shape=pdf.shape)<br/>mu = tf.Variable(np.zeros(1))<br/>sigma = tf.Variable(np.eye(1))<br/>normal = tf.exp(-tf.square(x - mu) / (2 * sigma))<br/>q = normal / tf.reduce_sum(normal)</span></pre><p id="489b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像之前一样，我们定义一个函数来计算 KL 散度，它排除了概率等于零的情况。</p><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="e656" class="ms ll jj mo b gy mt mu l mv mw">kl_divergence = tf.reduce_sum(<br/>    tf.where(p == 0, tf.zeros(pdf.shape, tf.float64), p * tf.log(p / q))<br/>)</span></pre><p id="9fdd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们初始化<code class="fe nn no np mo b">GradientDescentOptimizer</code>类的一个实例，并使用 KL divergence 函数作为参数调用<code class="fe nn no np mo b">minimize</code>方法。</p><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="179e" class="ms ll jj mo b gy mt mu l mv mw">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(kl_divergence)</span></pre><p id="de47" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">只有在运行<code class="fe nn no np mo b">tf.global_variables_initializer()</code>之后，变量才会保持我们声明它们时设置的值(即<code class="fe nn no np mo b">tf.zeros</code>)。</p><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="db22" class="ms ll jj mo b gy mt mu l mv mw">init = tf.global_variables_initializer()</span></pre><p id="0e08" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">tensorflow 中的所有操作都必须在一个会话中完成。在前面的代码块中，我们使用梯度下降来最小化 KL 散度。</p><pre class="lf lg lh li gt mn mo mp mq aw mr bi"><span id="5285" class="ms ll jj mo b gy mt mu l mv mw">with tf.Session() as sess:<br/>    sess.run(init)<br/>    <br/>    history = []<br/>    means = []<br/>    variances = []<br/>    <br/>    for i in range(epochs):<br/>        sess.run(optimizer, { p: pdf })<br/>        <br/>        if i % 10 == 0:<br/>            history.append(sess.run(kl_divergence, { p: pdf }))<br/>            means.append(sess.run(mu)[0])<br/>            variances.append(sess.run(sigma)[0][0])<br/>    <br/>    for mean, variance in zip(means, variances):<br/>        q_pdf = norm.pdf(x, mean, np.sqrt(variance))<br/>        plt.plot(x, q_pdf.reshape(-1, 1), c='red')</span><span id="0ec5" class="ms ll jj mo b gy my mu l mv mw">plt.title('KL(P||Q) = %1.3f' % history[-1])<br/>    plt.plot(x, p_pdf.reshape(-1, 1), linewidth=3)<br/>    plt.show()<br/>    <br/>    plt.plot(history)<br/>    plt.show()<br/>    <br/>    sess.close()</span></pre><p id="1846" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们绘制不同时间点的概率分布和 KL 散度。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/64b32350a4e1ed374b015d77d9cf1885.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*NFeB6pIKN9wATRV1my15CQ.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8b16ed1dafd2535e284417055a961371.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*_p6ZK_K_MRv4Zqqv_GpfUg.png"/></div></figure></div></div>    
</body>
</html>