<html>
<head>
<title>Modeling Price with Regularized Linear Model &amp; Xgboost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用正则化线性模型&amp; Xgboost 对价格建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/modeling-price-with-regularized-linear-model-xgboost-55e59eae4482?source=collection_archive---------5-----------------------#2019-02-20">https://towardsdatascience.com/modeling-price-with-regularized-linear-model-xgboost-55e59eae4482?source=collection_archive---------5-----------------------#2019-02-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/af2f57df958220473eee81742108f23a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VXBrs86x46sHEu8m-cBhpQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo Credit: Pixabay</figcaption></figure><div class=""/><div class=""><h2 id="ed32" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">开发用于预测个人房价的统计模型</h2></div><p id="2757" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们想模拟房子的价格，我们知道价格取决于房子的位置、房子的面积、建造年份、翻新年份、卧室数量、车库数量等。因此，这些因素促成了这种模式——优质的位置通常会导致更高的价格。然而，在相同的面积和相同的平方英尺内，所有的房子并没有完全相同的价格。价格的变化就是噪音。我们在价格建模中的目标是对模式建模，忽略噪声。同样的概念也适用于建模酒店房间价格。</p><p id="cce8" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">因此，首先，我们将为房价数据的线性回归实现正则化技术。</p><h1 id="95c0" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">数据</h1><p id="9f7b" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">有一个优秀的房价数据集可以在这里找到<a class="ae mn" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="b7de" class="mx lr jf mt b gy my mz l na nb">import warnings<br/>def ignore_warn(*args, **kwargs):<br/>    pass<br/>warnings.warn = ignore_warn<br/>import numpy as np <br/>import pandas as pd <br/>%matplotlib inline<br/>import matplotlib.pyplot as plt <br/>import seaborn as sns<br/>from scipy import stats<br/>from scipy.stats import norm, skew<br/>from sklearn import preprocessing<br/>from sklearn.metrics import r2_score<br/>from sklearn.metrics import mean_squared_error<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import ElasticNetCV, ElasticNet<br/>from xgboost import XGBRegressor, plot_importance <br/>from sklearn.model_selection import RandomizedSearchCV<br/>from sklearn.model_selection import StratifiedKFold<br/>pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))<br/>df = pd.read_csv('house_train.csv')<br/>df.shape</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/a9330bb54e167d9873ae98fb6acefcd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*HcjF4xQuIEoorRFeK7IONQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 1</figcaption></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="aa5e" class="mx lr jf mt b gy my mz l na nb">(df.isnull().sum() / len(df)).sort_values(ascending=False)[:20]</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/37cee200c999f5236e02f49dc36581e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*cVBbGkHOfieu02QO0FCGrw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 2</figcaption></figure><p id="6ad5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">好消息是我们有很多特性可以玩(81)，坏消息是 19 个特性有缺失值，其中 4 个缺失值超过 80%。对于任何特性，如果它丢失了 80%的值，那么它就不会那么重要，因此，我决定删除这 4 个特性。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="42e6" class="mx lr jf mt b gy my mz l na nb">df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'Id'], axis=1, inplace=True)</span></pre><h1 id="e80f" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">探索功能</h1><h2 id="983f" class="mx lr jf bd ls ne nf dn lw ng nh dp ma ld ni nj mc lh nk nl me ll nm nn mg no bi translated">目标特征分布</h2><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="ec78" class="mx lr jf mt b gy my mz l na nb">sns.distplot(df['SalePrice'] , fit=norm);</span><span id="3f2c" class="mx lr jf mt b gy np mz l na nb"># Get the fitted parameters used by the function<br/>(mu, sigma) = norm.fit(df['SalePrice'])<br/>print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))</span><span id="0af5" class="mx lr jf mt b gy np mz l na nb">#Now plot the distribution<br/>plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],<br/>            loc='best')<br/>plt.ylabel('Frequency')<br/>plt.title('Sale Price distribution')</span><span id="c284" class="mx lr jf mt b gy np mz l na nb">#Get also the QQ-plot<br/>fig = plt.figure()<br/>res = stats.probplot(df['SalePrice'], plot=plt)<br/>plt.show();</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4065283657413860d832e103c599e8dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*lip6Zlnb6QOHpa3jkvoW0Q.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 3</figcaption></figure><p id="da6d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">目标特征 SalePrice 是右偏的。由于线性模型喜欢正态分布的数据，我们将对 SalePrice 进行转换，使其更符合正态分布。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="3ab3" class="mx lr jf mt b gy my mz l na nb">sns.distplot(np.log1p(df['SalePrice']) , fit=norm);</span><span id="0fa4" class="mx lr jf mt b gy np mz l na nb"># Get the fitted parameters used by the function<br/>(mu, sigma) = norm.fit(np.log1p(df['SalePrice']))<br/>print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))</span><span id="b740" class="mx lr jf mt b gy np mz l na nb">#Now plot the distribution<br/>plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],<br/>            loc='best')<br/>plt.ylabel('Frequency')<br/>plt.title('log(Sale Price+1) distribution')</span><span id="c4b8" class="mx lr jf mt b gy np mz l na nb">#Get also the QQ-plot<br/>fig = plt.figure()<br/>res = stats.probplot(np.log1p(df['SalePrice']), plot=plt)<br/>plt.show();</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/dcb0953fca1ae863e115e25484663408.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*BOrEgJ20iRZKHH48o9Sdeg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 4</figcaption></figure><h2 id="3987" class="mx lr jf bd ls ne nf dn lw ng nh dp ma ld ni nj mc lh nk nl me ll nm nn mg no bi translated"><strong class="ak">数字特征之间的相关性</strong></h2><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="0865" class="mx lr jf mt b gy my mz l na nb">pd.set_option('precision',2)<br/>plt.figure(figsize=(10, 8))<br/>sns.heatmap(df.drop(['SalePrice'],axis=1).corr(), square=True)<br/>plt.suptitle("Pearson Correlation Heatmap")<br/>plt.show();</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/fb3e9bb5916d04603ee110a9e0241d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JoZucBpMMDM0YemG1Vj7YA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 5</figcaption></figure><p id="e7bd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一些特征之间存在很强的相关性。比如 GarageYrBlt 和 YearBuilt，TotRmsAbvGrd 和 GrLivArea，GarageArea 和 GarageCars 都是强相关的。它们实际上或多或少表达了相同的东西。我以后会让<a class="ae mn" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html" rel="noopener ugc nofollow" target="_blank"><strong class="kw jg"><em class="nt">elastic netcv</em></strong></a>帮助减少冗余。</p><h2 id="a5c5" class="mx lr jf bd ls ne nf dn lw ng nh dp ma ld ni nj mc lh nk nl me ll nm nn mg no bi translated"><strong class="ak">销售价格和其他数字特征之间的相关性</strong></h2><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="2e59" class="mx lr jf mt b gy my mz l na nb">corr_with_sale_price = df.corr()["SalePrice"].sort_values(ascending=False)<br/>plt.figure(figsize=(14,6))<br/>corr_with_sale_price.drop("SalePrice").plot.bar()<br/>plt.show();</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/85e1d6172ea7a98cffed4c1978d90326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kPcP9PYqxo8MWki5QLikww.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 6</figcaption></figure><p id="4ff2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">销售价格与总体质量的相关性最大(约 0.8)。此外，GrLivArea 的相关性超过 0.7，GarageCars 的相关性超过 0.6。让我们更详细地看一下这 4 个特性。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="6175" class="mx lr jf mt b gy my mz l na nb">sns.pairplot(df[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars']])<br/>plt.show();</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/434318efcfd053e37c462262fc8f8870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYCrCKtVW3Rtzj1T8n6KTw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 7</figcaption></figure><h1 id="839a" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">特征工程</h1><ul class=""><li id="2ffa" class="nw nx jf kw b kx mi la mj ld ny lh nz ll oa lp ob oc od oe bi translated">具有高度偏斜分布(偏斜&gt; 0.75)的对数变换要素</li><li id="ae28" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">虚拟编码分类特征</li><li id="046f" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">用该列的平均值填写 NaN。</li><li id="5952" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">训练集和测试集分离。</li></ul><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ok ol l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">feature_engineering_price.py</figcaption></figure><h1 id="8f33" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated"><strong class="ak">弹力网</strong></h1><ul class=""><li id="f9c6" class="nw nx jf kw b kx mi la mj ld ny lh nz ll oa lp ob oc od oe bi translated"><strong class="kw jg">岭</strong>和<strong class="kw jg">套索</strong>回归是正则化的线性回归模型。</li><li id="e01c" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated"><strong class="kw jg"> ElasticNe </strong> t 本质上是一个<strong class="kw jg">套索/脊</strong>混合体，它需要一个目标函数的最小化，这个目标函数包括<strong class="kw jg"> L1 </strong>(套索)和<strong class="kw jg"> L2 </strong>(脊)规范。</li><li id="6016" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated"><strong class="kw jg">当存在多个相互关联的特征时，ElasticNet </strong>非常有用。</li><li id="b49d" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">类<strong class="kw jg"> ElasticNetCV </strong>可用于通过交叉验证设置参数<code class="fe om on oo mt b">alpha</code> (α)和<code class="fe om on oo mt b">l1_ratio</code> (ρ)。</li><li id="9b45" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated"><strong class="kw jg">elastic net cv</strong>:<strong class="kw jg">elastic net</strong>模型，通过交叉验证选择最佳模型。</li></ul><p id="3f60" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们看看<strong class="kw jg">elastic tcv</strong>将为我们选择什么。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ok ol l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">ElasticNetCV.py</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/dcbc58fb78c13c49be1f340b2125fad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*RFY1niN_2OjL_xhSWd-YSQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 8</figcaption></figure><p id="3ee2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">0 &lt; The optimal l1_ratio &lt;1 , indicating the penalty is a combination of L1 and L2, that is, the combination of <strong class="kw jg">拉索</strong>和<strong class="kw jg">脊</strong>。</p><h2 id="5c72" class="mx lr jf bd ls ne nf dn lw ng nh dp ma ld ni nj mc lh nk nl me ll nm nn mg no bi translated">模型评估</h2><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ok ol l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">ElasticNetCV_evaluation.py</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/d1c594995e2164177dd80140c3e0dffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*4L7zs4hgPDiQF8GThI_IBQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 9</figcaption></figure><p id="234b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这里的<strong class="kw jg"> RMSE </strong>其实就是<strong class="kw jg"> RMSLE </strong>(均方根对数误差)。因为我们取了实际值的对数。这是一篇很好的文章，解释了 RMSE 和 RMSLE 的不同之处。</p><h2 id="c874" class="mx lr jf bd ls ne nf dn lw ng nh dp ma ld ni nj mc lh nk nl me ll nm nn mg no bi translated">特征重要性</h2><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="b4bd" class="mx lr jf mt b gy my mz l na nb">feature_importance = pd.Series(index = X_train.columns, data = np.abs(cv_model.coef_))</span><span id="67dc" class="mx lr jf mt b gy np mz l na nb">n_selected_features = (feature_importance&gt;0).sum()<br/>print('{0:d} features, reduction of {1:2.2f}%'.format(<br/>    n_selected_features,(1-n_selected_features/len(feature_importance))*100))</span><span id="c0f3" class="mx lr jf mt b gy np mz l na nb">feature_importance.sort_values().tail(30).plot(kind = 'bar', figsize = (12,5));</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/604d4606d3e93ba0ed2ea4148fdf5a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DsdErYlsmb_j3aegq6zlw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 10</figcaption></figure><p id="e5cd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">减少 58.91%的功能看起来很有效。ElasticNetCV 选出的前 4 个最重要的特性分别是<strong class="kw jg"> Condition2_PosN </strong>、<strong class="kw jg"> MSZoning_C(all) </strong>、<strong class="kw jg">exterior 1st _ brk comm</strong>&amp;<strong class="kw jg">GrLivArea</strong>。我们将看看这些特性与 Xgboost 选择的特性相比如何。</p><h1 id="48ac" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">Xgboost</h1><p id="0c0b" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">第一个 Xgboost 模型，我们从默认参数开始。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ok ol l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">xgb_model1.py</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi os"><img src="../Images/69fa0766c7d3f5ca6f3023cf733f5c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*xJkbdB4XUOoDHvImg1eRpw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 11</figcaption></figure><p id="878a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">它已经比 ElasticNetCV 选择的型号好得多了！</p><p id="e87b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">第二个 Xgboost 模型，我们逐渐添加一些参数，以增加模型的准确性。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ok ol l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">xgb_model2.py</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/a9e27f15e949faf2b778a4efce827a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*VIBKUuid-10UXGelajkx4g.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 12</figcaption></figure><p id="b044" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">又有进步了！</p><p id="cd61" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">第三个 Xgboost 模型，我们添加了一个学习率，希望它会产生一个更准确的模型。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ok ol l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">xgb_model3.py</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/6b99b480c34dc606948601015dac7b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*QGGIaVfNDH5NaGJfZ5Ih3g.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 13</figcaption></figure><p id="5c5e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">可惜没有什么起色。我的结论是 xgb_model2 是最好的型号。</p><h2 id="2728" class="mx lr jf bd ls ne nf dn lw ng nh dp ma ld ni nj mc lh nk nl me ll nm nn mg no bi translated">特征重要性</h2><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="cb71" class="mx lr jf mt b gy my mz l na nb">from collections import OrderedDict<br/>OrderedDict(sorted(xgb_model2.get_booster().get_fscore().items(), key=lambda t: t[1], reverse=True))</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/e569f36bc70f2c81fc732007d4c6c7ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*1ksHCW5_8OOZisz9wJdVCw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 14</figcaption></figure><p id="b9fe" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Xgboost 选择的前 4 个最重要的特性是<strong class="kw jg"> LotArea </strong>，<strong class="kw jg"> GrLivArea </strong>，<strong class="kw jg">overall qual</strong>&amp;<strong class="kw jg">TotalBsmtSF</strong>。</p><p id="e7f3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">只有一个特性<strong class="kw jg"> GrLivArea </strong>被 ElasticNetCV 和 Xgboost 同时选中。</p><p id="966a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">因此，现在我们将选择一些相关的功能，并再次适应 Xgboost。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ok ol l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">xgb_model5.py</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/7cb32dbeee5d3de98e7538c33ce6ffdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*sOtVdf8mrwm0_3iUbojHnw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 15</figcaption></figure><p id="507e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">又一个小改进！</p><p id="ce4d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Jupyter 笔记本可以在<a class="ae mn" href="https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Modeling%20House%20Price%20with%20Regularized%20Linear%20Model%20%26%20Xgboost.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。享受这周剩下的时光吧！</p></div></div>    
</body>
</html>