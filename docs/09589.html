<html>
<head>
<title>A limitation of Random Forest Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林回归的一个局限性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-limitation-of-random-forest-regression-db8ed7419e9f?source=collection_archive---------5-----------------------#2019-12-17">https://towardsdatascience.com/a-limitation-of-random-forest-regression-db8ed7419e9f?source=collection_archive---------5-----------------------#2019-12-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cd19" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">邻居的规则如何影响你的预测。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9076ca12d3dd2950fcb089f2116eba88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hvae6fXpnu5BzgMUfLRAXw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">One Tree from a Random Forest of Trees</figcaption></figure><p id="49ce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">随机森林</a>是一种流行的机器学习模型，通常用于分类任务，这在许多学术论文、Kaggle 竞赛和博客帖子中可以看到。除了分类，随机森林还可以用于回归任务。随机森林的非线性特性可以帮助它超越线性算法，成为一个很好的选择。但是，了解您的数据并记住随机森林无法进行外推是很重要的。它只能做出预测，该预测是先前观察到的标签的平均值。从这个意义上说，它非常类似于<a class="ae lr" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener ugc nofollow" target="_blank"> KNN </a>。换句话说，在回归问题中，随机森林可以做出的预测范围受到训练数据中最高和最低标签的限制。在训练和预测输入的范围和/或分布不同的情况下，这种行为会变得有问题。这被称为<a class="ae lr" href="https://www.analyticsvidhya.com/blog/2017/07/covariate-shift-the-hidden-problem-of-real-world-data-science/" rel="noopener ugc nofollow" target="_blank">协变量移位</a>，对于大多数模型来说很难处理，尤其是对于随机森林，因为它不能外推。</p><p id="bd30" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，假设您正在处理具有潜在趋势的数据，如股价、房价或销售额。如果您的训练数据缺少任何时间段，您的随机森林模型将会根据趋势低估或高估训练数据中时间段之外的示例。如果您将模型的预测值与真实值进行对比，这一点会非常明显。让我们通过创建一些数据来看看这一点。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="590a" class="lx ly iq lt b gy lz ma l mb mc">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.ensemble import RandomForestRegressor<br/>%matplotlib inline</span><span id="67a7" class="lx ly iq lt b gy md ma l mb mc">#make fake data with a time trend<br/>X = np.random.rand(1000,10)</span><span id="d9f7" class="lx ly iq lt b gy md ma l mb mc">#add time feature simulating years 2000-2010<br/>time = np.random.randint(2000,2011,size=1000)</span><span id="9a71" class="lx ly iq lt b gy md ma l mb mc">#add time to X<br/>X = np.hstack((X,time.reshape(-1,1)))</span><span id="33e2" class="lx ly iq lt b gy md ma l mb mc">#create target via a linear relationship to X<br/>weights = np.random.rand(11)<br/>y = X.dot(weights)</span><span id="9e77" class="lx ly iq lt b gy md ma l mb mc">#create test data that includes years<br/>#not in training data 2000 - 2019<br/>X_test = np.random.rand(1000,10)<br/>time_test = np.random.randint(2000,2020,size=1000)<br/>X_test = np.hstack((X_test,time_test.reshape(-1,1)))<br/>y_test = X_test.dot(weights)</span></pre><p id="1d8f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看一个随机森林能多好地预测测试数据。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="8964" class="lx ly iq lt b gy lz ma l mb mc">#fit and score the data using RF<br/>RF = RandomForestRegressor(n_estimators=100)<br/>RF.fit(X,y)<br/>RF.score(X_test,y_test)<br/>&gt;&gt;0.5872576516824577</span></pre><p id="d85e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那不太好。让我们对照它们的已知值来绘制我们的预测，看看发生了什么。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="956f" class="lx ly iq lt b gy lz ma l mb mc">#plot RF as trees increase<br/>#set starting point for subplots<br/>index = 1</span><span id="e637" class="lx ly iq lt b gy md ma l mb mc">#set the size of the subplot to something large<br/>plt.figure(figsize=(20,20))</span><span id="74f4" class="lx ly iq lt b gy md ma l mb mc">#iterate through number of trees in model<br/>#and plot predictions v actual<br/>for i in [1,5,10,100]:<br/>    plt.subplot(2, 2, index)<br/>    RF_plot = RandomForestRegressor(n_estimators=i)<br/>    RF_plot.fit(X,y)<br/>    #split data btw vals RF can interploate vs. data<br/>    #it needs to exptrapolate<br/>    interpolate_index = X_test[:,10]&lt;=2010<br/>    extrapolate_index = X_test[:,10]&gt;2010<br/>    X_interpolate = X_test[interpolate_index]<br/>    X_extrapolate = X_test[extrapolate_index]<br/>    y_interpolate = y_test[interpolate_index]<br/>    y_extrapolate = y_test[extrapolate_index]<br/>    #plot predictions vs. actual<br/>    plt.scatter(RFplot.predict(X_interpolate),<br/>                y_interpolate,<br/>                color="g",label="interpolate")<br/>    plt.scatter(RFplot.predict(X_extrapolate),<br/>                y_extrapolate,<br/>                color="b",label="extrapolate")<br/>    plt.xlabel('Predicted')<br/>    plt.ylabel('Actual')<br/>    plt.title('Random Forest with {} trees'.format(i))<br/>    plt.subplots_adjust(wspace=.4, hspace=.4)<br/>    plt.legend(loc="best")<br/>    index += 1</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi me"><img src="../Images/75285af24dcd7155d6bf6c8f34022a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2xeDb8sQ_K7aHYlfyssmjA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Predicted vs. Actual Random Forest</figcaption></figure><p id="adc9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该图清楚地表明，模型可以预测的最高值约为 961，而数据中的潜在趋势将更近的值推高至 966。不幸的是，随机森林无法外推线性趋势，也无法准确预测时间值高于训练数据(2000–2010)中所见时间值的新示例。即使调整树木的数量也不能解决问题。在这种情况下，由于我们对数据施加了完美的线性关系，因此像线性回归这样的模型将是更好的选择，并且在检测数据趋势和对训练数据中时间范围之外的数据做出准确预测方面不会有问题。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="27b8" class="lx ly iq lt b gy lz ma l mb mc">#fit the data using Linear Regression<br/>LR = LinearRegression()<br/>LR.fit(X,y)<br/>LR.score(X_test,y_test)<br/>&gt;&gt;1.0</span><span id="5e2c" class="lx ly iq lt b gy md ma l mb mc">#plot predictions of Linear Regression against actual<br/>plt.figure(figsize=(7,7))<br/>plt.xlabel('Predicted')<br/>plt.ylabel('Actual')<br/>plt.title('Linear Regression - Test Data')<br/>_ = plt.scatter(LR.predict(X_interpolate),y_interpolate,<br/>               color="g",label="interpolate")<br/>_ = plt.scatter(LR.predict(X_extrapolate),y_extrapolate,<br/>               color="b",label="extrapolate")<br/>plt.legend(loc="best")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mf"><img src="../Images/41f4dea3f48120b83a06123a85470588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SA4SISZAWDUjetIuWZmSGw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Predicted vs. Actual Linear Regression</figcaption></figure><p id="17c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然随机森林通常是一个很好的模型选择，但了解它是如何工作的，以及在给定数据的情况下它是否有任何限制仍然很重要。在这种情况下，因为它是一个基于邻域的模型，所以它阻止我们对训练数据之外的时间范围进行准确预测。如果你发现自己处于这种情况，最好是测试其他模型，如线性回归或立体模型，和/或考虑在模型集合中使用随机森林。预测快乐！</p></div></div>    
</body>
</html>