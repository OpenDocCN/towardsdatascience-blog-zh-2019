<html>
<head>
<title>Naive Bayes Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯解释道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/naive-bayes-explained-108c095241eb?source=collection_archive---------11-----------------------#2019-08-07">https://towardsdatascience.com/naive-bayes-explained-108c095241eb?source=collection_archive---------11-----------------------#2019-08-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/3faabf8174bde385d0493158fb403119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tODiUUswpqyFjss2hb2g2w.png"/></div></div></figure><div class=""/><div class=""><h2 id="8074" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">从直觉到实施</h2></div><p id="ca7e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">用于分类的机器学习方法本质上可以是<em class="lp">判别型</em>和<em class="lp">生成型</em>。基本上，一个和另一个之间的区别在于根据我们的数据获得预测的过程。让我们假设我们的数据集由几个实例<strong class="kv jf"> x </strong>组成，其中每个实例包含一系列由 1 到<em class="lp"> D </em>索引的特征，其中<em class="lp"> D </em>是我们数据集的维数。鉴于这种直觉，我们可以将<strong class="kv jf"> x </strong>表示为:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/fe960900dbd14290ebc301c6e1e79772.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*4y4N5sQSTLjLU6ebCphMnQ.png"/></div></figure><p id="7380" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这里的特征<em class="lp"> x </em> 1、<em class="lp"> x </em> 2、<em class="lp"> x </em> 3 等。是唯一标识我们的实例的属性。例如，如果我们正在开发一个模型，根据某些标准来预测个人是否应该获得信贷，那么<em class="lp"> x </em> 1，<em class="lp"> x </em> 2，<em class="lp"> x </em> 3… <em class="lp"> xD </em>。将是个人的相关属性，例如他/她的个人信息、工资、债务金额等。鉴于这些数据，我们希望训练一个模型，能够预测这个人是否应该获得信贷。因此，对于这个特殊的问题我们将有两种可能性:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/4f0aa5414bac55810b98c593b837d841.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*9D3or3uzLbfdi6Gfn2Daow.png"/></div></figure><p id="4384" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">其中<em class="lp"> k </em> = {1，…，<em class="lp"> K </em> }是类别索引，由于在这个特定的问题中我们有两个类别，那么<em class="lp"> K </em> =2。现在，一个判别模型将如何预测<strong class="kv jf"> <em class="lp"> y </em> </strong> <em class="lp"> k </em>？它会告诉我们，给定数据<strong class="kv jf"> x </strong>，一个人获得信贷的概率。我们可以用这个数学表达式来表达这个推理:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/e068cf9e054cd8636a9e492913a609c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Kd3pS76VzA_Zlin7mvJ0ZQ.png"/></div></figure><p id="bc47" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">通过调整一组参数，<em class="lp">判别</em>模型的目标将是给我们这个概率作为结果，这将反过来成为我们的预测。另一方面，一个<em class="lp">创成式</em>模型会做什么？它的目标是最终给我们同样的概率，但是它遵循的过程会有一点不同。假设我们有<em class="lp">p</em>(<strong class="kv jf">x</strong><em class="lp">|</em><strong class="kv jf"><em class="lp">y</em></strong><em class="lp">k</em>)和<em class="lp">p</em>(<strong class="kv jf"><em class="lp">y</em></strong><em class="lp">k</em>)。然后，贝叶斯法则将允许我们通过使用以下等式找到<em class="lp">p</em>(<strong class="kv jf"><em class="lp">y</em></strong><em class="lp">k |</em><strong class="kv jf">x</strong>):</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/8e7114dc0bfa86430907790ffe4d7302.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*e-ZF67XMPT-rSxdvd2yfyA.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Equation 1. Bayes Theorem for x and y</figcaption></figure><p id="c1f4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这里，<em class="lp">p</em>(<strong class="kv jf">x</strong>|<strong class="kv jf">T5】y</strong>T8】k)又称为<em class="lp">可能性</em>，<em class="lp">p</em>(<strong class="kv jf"><em class="lp">y</em></strong><em class="lp">k</em>)是<em class="lp">事前，p </em> ( <strong class="kv jf"> x </strong>)是<em class="lp">证据</em>，而<em class="lp">分母中的证据<em class="lp"> p </em> ( <strong class="kv jf"> x </strong>)可以被视为常数，为了简单起见，我们将忽略它。这个贝叶斯框架的好处在于，我们可以通过在概率分布后对其建模来轻松确定<em class="lp">可能性</em>，并且可以通过计算我们的数据集中有多少个<strong class="kv jf"><em class="lp">【y】</em></strong><em class="lp">k</em>出现，然后除以数据点的总数来找到<em class="lp">先验</em>。事实上，通过知道似然值和先验，我们能够根据我们的数据进行预测。</em></p><p id="4bb5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">总之，<em class="lp">生成模型</em>将使用贝叶斯定理通过使用可能性和先验来进行预测。在贝叶斯学习领域中，有各种各样的用于不同目的的生成模型。其中之一被称为<em class="lp">朴素贝叶斯</em>，我们将在接下来的章节中深入讨论它。稍后，我们将通过使用手写数字的 MNIST 数据集在 Python 中实现这个模型。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="cf61" class="mj mk je bd ml mm mn dn mo mp mq dp mr lc ms mt mu lg mv mw mx lk my mz na nb bi translated">推导朴素贝叶斯</h2><p id="d91c" class="pw-post-body-paragraph kt ku je kv b kw nc kf ky kz nd ki lb lc ne le lf lg nf li lj lk ng lm ln lo im bi translated">让我们回到我们早期预测信用的例子。我们说过<strong class="kv jf"> x </strong>将包含一系列属于特定个人的特征。这些特征包含了我们的模型进行信用或非信用预测所需的信息。我们的假设是特征<em class="lp"> 1，…，D </em>在统计上相互独立。这意味着，例如，个人的年龄不会决定他/她的债务数额。同样，工资也不是由他/她有多少债务等因素决定的。显然，我们可以看到，这种独立性假设并不总是成立的，但正如我们将在后面看到的，它将使我们的模型更加简单，而不会失去有效性。</p><p id="ead5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然后，通过使用图形表示，我们可以将 p(<strong class="kv jf">x</strong>|<strong class="kv jf">|<em class="lp">y</em></strong><em class="lp">k</em>)描述如下:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/ec4f5eb0a9a491cfc44913bc468b6d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*8Ebe5vPB-Ch7cYoTjObUww.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Figure 1. Illustration for independence between features</figcaption></figure><p id="c830" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以看到，特征<strong class="kv jf"> <em class="lp"> x </em> </strong> <em class="lp"> i </em>它们之间并不相连，而是<strong class="kv jf"> y </strong> <em class="lp"> k </em> <strong class="kv jf"> </strong>连接到每个特征上。由此我们可以清楚地看到，这些特征互为<em class="lp">条件独立</em>，但它们只依赖于<strong class="kv jf"> <em class="lp"> y </em> </strong> <em class="lp"> k </em>。超越信用问题，我们可以将每个<strong class="kv jf"> <em class="lp"> x </em> </strong> <em class="lp"> i </em>想象为数字图像中的一个像素，如果我们正在进行计算机视觉，或者是句子中的一个单词，如果我们正在进行自然语言处理。事实上，稍后我们将实现朴素贝叶斯来分类手写数字图像。</p><p id="7b77" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在让我们将迪普深入到直觉中，并假设我们有两个相互独立的变量<em class="lp"> A </em>和<em class="lp"> B </em>。通过应用乘积概率，我们知道:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/44ac4e69315067db675c9e1542a10648.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*g_Uy6ScwwUfvJPEDUwyP0w.png"/></div></figure><p id="f690" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这种情况下，因为独立条件，<em class="lp">p</em>(<em class="lp">A</em>|<em class="lp">B</em>)干脆就变成了<em class="lp"> p </em> ( <em class="lp"> A </em>)。如果我们将同样的推理以及一些归纳应用于<strong class="kv jf"> x </strong>和<strong class="kv jf">T43】y</strong><em class="lp">k</em>，我们将能够容易地推导出朴素贝叶斯模型的主要关系。</p><p id="0683" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们要做一些归纳！先假设我们只有两个特征<em class="lp"> x </em> 1 和<em class="lp"> x </em> 2。考虑到我们之前提到的独立性问题，我们应用乘积规则的方式是:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2772ed863b7da607482a20f9ded00c27.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*RX607EKcbq0JTToS3tCrTg.png"/></div></figure><p id="1f06" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">太好了！现在让我们用三个特性来试试<em class="lp"> x </em> 1、<em class="lp"> x </em> 2 和<em class="lp"> x </em> 3:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b9230abe7a9147159988c1fd8a60973d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*g9DMc4aPzDScOtnVdkN6Uw.png"/></div></figure><p id="bf20" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">注意到模式了吗？是啊，我们可以把这个概率仅仅表示为<em class="lp"> x </em> 1、<em class="lp"> x </em> 2、<em class="lp"> x </em> 3、…、<em class="lp"> xD </em>和<em class="lp">p</em>(<strong class="kv jf"><em class="lp">y</em></strong><em class="lp">k</em>)的概率的乘积。因此，我们有:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/489452145e148d9c933c824d5e27f72b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bvH84z-Jl_DYPABP4B7-Sw.png"/></div></div></figure><p id="2a87" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">厉害！现在你可能已经注意到，这个表达式实际上是我们在等式 1 中定义的。因此，将它代入等式 1，我们得到:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0d9b0f09fd75385af6be7111f9b9ee75.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*DruDzmhLUcH0G2Kbzfxn4g.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Equation 2. Probability of y given x</figcaption></figure><p id="1990" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">那么，我们如何将它转化为实际的预测呢？当评估这个表达式时，我们将有一组概率，每个类一个。我们需要做的是找到突出的概率指数，我们将有预测！因此，等式 2 变为:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/c4f3d96774fb832d18a4909bd10f0fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*kHW0XvFIJmq_V3zJDeiK_g.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Equation 3. Naive Bayes prediction</figcaption></figure><p id="387c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">很好。这里一个有趣的事情是，我们可以通过采用合适的概率分布来模拟<em class="lp">p</em>(<strong class="kv jf"><em class="lp">x</em></strong><em class="lp">I</em>|<strong class="kv jf"><em class="lp">y</em></strong><em class="lp">k</em>)，其选择将取决于我们数据的性质。例如，如果我们的<strong class="kv jf"> x </strong>是二元变量，我们将使用伯努利分布，或者如果它们是实数，那么我们将使用高斯函数。我们将在下一节更详细地讨论后者。</p><p id="aab6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在继续之前，我们只需要对等式 3 做最后一件事。如果仔细观察，会发现<em class="lp"> argmax </em>里面的所有项都是概率，也就是零和一之间的值。当我们把所有这些项相乘时，我们会得到一个越来越接近零的结果。当我们有许多特征时，情况会变得更糟，例如，数字图像中的像素。我们的预测会崩溃！我们能做什么？简单，应用对数。最好是一个大的负数，而不是一个非常接近零的值，这可能会搞乱我们的模型。由于我们只对预测感兴趣，并不关心概率值，我们应用<strong class="kv jf">对数</strong>不会影响期望的结果。通过这种修改，等式 3 变为:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c63b9a815e326712bfcd4c1e59cc04f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*d6q4ohFP3TUSg5_KsSfjQw.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Equation 4. Naive Bayes prediction with logarithms</figcaption></figure><p id="ac2e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这将确保在处理许多特征时更好的数值稳定性。接下来，我们讨论高斯朴素贝叶斯处理数据集中的实值<strong class="kv jf"> x </strong>。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="735a" class="mj mk je bd ml mm mn dn mo mp mq dp mr lc ms mt mu lg mv mw mx lk my mz na nb bi translated">高斯朴素贝叶斯</h2><p id="7a45" class="pw-post-body-paragraph kt ku je kv b kw nc kf ky kz nd ki lb lc ne le lf lg nf li lj lk ng lm ln lo im bi translated">关于朴素贝叶斯，什么叫“幼稚”？是啊，你猜对了！特征的独立性<em class="lp"> x </em> 1、<em class="lp"> x </em> 2、<em class="lp"> x </em> 3 等等。在前几节中，我们一直在讨论这个导致我们得出一些有趣结论的特殊特征。尽管我们不能总是假设我们的特征是独立的，朴素贝叶斯允许我们大大减少进行预测所需的计算成本。</p><p id="68e6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们现在将注意力转向当<strong class="kv jf"> x </strong>为实值时的特殊情况，看看我们如何使用高斯密度函数来模拟<em class="lp">p</em>(<strong class="kv jf"><em class="lp">x</em></strong><em class="lp">I</em>|<strong class="kv jf">y</strong>)。</p><p id="467c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们如何定义多元高斯函数？我们应该记住，它由两个基本参数组成:均值<strong class="kv jf"> μ </strong>和协方差<strong class="kv jf">σ</strong>，由下式给出</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/c9c4231eff105d62b50c92368cd88b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MbWr-aarFe7hZyNB5QRAWQ.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Equation 5. Multivariate Gaussian</figcaption></figure><p id="5604" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">并且该表达式的对数由下式给出:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/5284fe5d320be0480af2b81dac8ab7f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KYFKpvBff7gGA3qYnXE-uA.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Equation 6. Log of Multivariate Gaussian</figcaption></figure><p id="c0f5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这里，均值<strong class="kv jf"> μ </strong>将是一个具有<em class="lp"> K </em>个元素的向量，协方差<strong class="kv jf">σ</strong>将是一个<em class="lp"> K </em> x <em class="lp"> K </em>矩阵。正如我们所见，等式 5 和 6 要求我们计算该矩阵的逆矩阵。我们可以清楚地看到，这个操作将变得非常昂贵，特别是如果我们有大量的功能。那么我们如何利用朴素贝叶斯的独立性假设来简化这些计算呢？我们需要在这里应用一些概念。</p><p id="9cd5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们阐明一些重要的定义。当两个变量<strong class="kv jf"> x </strong> 1 和<strong class="kv jf"> x </strong> 2 独立且服从高斯分布时，那么它们的协方差为零。我们可以很容易地证明这一点，首先说明协方差的一般定义，它由下式给出</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/25bdb36823879210a99020862babceec.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*vI7kGBEascVHEJ3vgV0ODA.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Equation 7. Covariance definition</figcaption></figure><p id="c553" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，我们暂时将注意力转向 E[<strong class="kv jf">x</strong>1<strong class="kv jf">x</strong>2]。给定<strong class="kv jf"> x </strong> 1 和<strong class="kv jf"> x </strong> 2 是独立的，那么我们将有:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/71bf5c24efad3cb4415ac33c68fe9ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*z5uunatWgYhA6uITrPsU2w.png"/></div></figure><p id="0fcd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果我们在等式 7 中替换这个表达式，我们会发现 Cov( <strong class="kv jf"> x </strong> 1，<strong class="kv jf"> x </strong> 2)等于零。现在，回到我们之前对高斯密度函数的定义，矩阵<strong class="kv jf">σ</strong>有哪些元素？让我们来看看:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/1cf25e5385ee5a0db9698e6bfd08564f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lLeqwzufsEpnBXygUhvLhA.png"/></div></div></figure><p id="2fc0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以看到，<strong class="kv jf">σ</strong>实际上是一个仅由特征级方差组成的对角矩阵。这是特征之间独立性假设的结果，因为正如我们刚刚展示的，当<strong class="kv jf">x</strong>T40】I 和<strong class="kv jf">x</strong>j 不同时，Cov(<strong class="kv jf">x</strong>I，<strong class="kv jf">x</strong>j<em class="lp">j</em>)为零。</p><p id="f15e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了推导高斯朴素贝叶斯模型的似然性，了解以下两个表达式将对我们非常有用:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/8db27c12efbafbf53dc42b17068a0850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*YbfpmxKiQaQ-TcKk84gkTw.png"/></div></figure><p id="eca1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">它们分别是对角协方差矩阵的行列式和逆矩阵。如果我们将这些代入等式 6，我们得到:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/8c38b06ec300bf167983389eca07b9ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*8-SBrcX8NkQJ6ZRP-ibsaA.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Equation 8. Gaussian log-likelihood expression</figcaption></figure><p id="bc1f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了方便起见，我们去掉了涉及<strong class="kv jf"> ln(2π) </strong>的第一项。由于这是一个常数，它只会放大我们的结果，所以删除它不会改变实际的预测值。对于给定的<strong class="kv jf"> x </strong>，等式 8 是我们模型的对数似然。这意味着，以下表达式实际上是等价的:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/3672e40c278b2d642a4b67ec0e7b280a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*beemlFOx4yFqyJthrPqRqA.png"/></div></div></figure><p id="ea9f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因此，将我们在上一节中推导的等式 8 代入等式 4，我们得到:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/811322895f6c76c7c8901a41ffb07f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*HLEhO33F81gVuwrJAlefww.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Equation 9. Prediction for Gaussian Naive Bayes</figcaption></figure><p id="9dd7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">精彩！我们想出了一个非常简单的方程来进行预测。请注意，我们已经去掉了所有涉及潜在的昂贵计算操作的表达式，如逆协方差、行列式等。我们可以很容易地在任何现代编程语言中实现这个模型，甚至对于包含大量条目的数据集。在下一节中，我们将集中精力使用手写数字的 MNIST 数据集在 Python 中实现高斯朴素贝叶斯。请继续阅读！</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="2fe3" class="mj mk je bd ml mm mn dn mo mp mq dp mr lc ms mt mu lg mv mw mx lk my mz na nb bi translated">用 Python 实现</h2><p id="c748" class="pw-post-body-paragraph kt ku je kv b kw nc kf ky kz nd ki lb lc ne le lf lg nf li lj lk ng lm ln lo im bi translated"><em class="lp">注意:完整的源代码可以在 https://bit.ly/2T94GF5</em><a class="ae ny" href="https://bit.ly/2T94GF5" rel="noopener ugc nofollow" target="_blank"/>的 Jupyter 笔记本上获得</p><p id="e23d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们现在要实现朴素贝叶斯来对手写数字的 MNIST 数据集进行预测[2]。该数据集由 60，000 幅用于训练的图像和 10，000 幅用于测试的图像组成。所有图像都有相应的标签。一个有趣的方面是，Keras 库已经附带了一个 API，允许我们根据需要轻松导入 MNIST。让我们首先导入数据集:</p><figure class="lr ls lt lu gt iv"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="ba9a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">作为预处理步骤的一部分，我们还将 HOG 描述符应用于训练和测试数据集中的每个图像。这已经表明提高了我们的模型的性能。使用这些描述符的另一个优点是，它将需要评估的特征数量减少到一半以下。如果你想用 OpenCV [3]找到更多关于猪的描述符，你可以看看 https://bit.ly/2OJKLOt 的<a class="ae ny" href="https://bit.ly/2OJKLOt" rel="noopener ugc nofollow" target="_blank"/>。总共为每个图像计算 108 个 HOG 特征，因此<em class="lp"> D </em> =108。</p><figure class="lr ls lt lu gt iv"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="7fd4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">接下来我们计算先验<em class="lp">p</em>(<strong class="kv jf"><em class="lp">y</em></strong><em class="lp">k</em>)。我们可以通过计算属于每个类的项目数，然后除以训练数据集中的条目总数来轻松实现这一点:</p><figure class="lr ls lt lu gt iv"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="c781" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然后，我们还有计算高斯对数似然的逻辑，由下式给出</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/db8de0f13ef28f94fff8aebc5bd146f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*7sZRZR46edtZRptyjv1svQ.png"/></div></figure><p id="20dd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这方面的 Python 代码如下:</p><figure class="lr ls lt lu gt iv"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="77cc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">最后，为了进行预测，我们简单地计算后验概率的 argmax。我们根据等式 9 和高斯对数似然性来实现这一点:</p><figure class="lr ls lt lu gt iv"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="b893" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们还通过使用 10，000 张图像进行测试来评估我们的模型。为此，我们计算整体精度:</p><figure class="lr ls lt lu gt iv"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="2b94" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们获得了 91.5%的准确率，考虑到我们已经假设了特征独立性，这是一个很好的设置。如果我们不做这样的假设，也许我们的精度会更高，但是反过来，我们在这里确实获得了重要的速度增益。您应该能够在几秒钟内使用整个数据集训练该模型！</p><p id="abcd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">以下是通过实施获得的一些预测示例:</p><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2bc48a3c133caa69d538d2e7fa30271f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*_pw4rqNu7tXrOd-uOhTikg.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Figure 2. Model predictions.<br/>Source: [2] The MNIST Database of Handwritten Digits and own Implementation</figcaption></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="c360" class="mj mk je bd ml mm mn dn mo mp mq dp mr lc ms mt mu lg mv mw mx lk my mz na nb bi translated">结束语</h2><p id="5b6d" class="pw-post-body-paragraph kt ku je kv b kw nc kf ky kz nd ki lb lc ne le lf lg nf li lj lk ng lm ln lo im bi translated">我们已经将朴素贝叶斯作为假设特征级独立性的分类生成模型。我们已经看到，尽管这种假设可能并不总是有效，但它有助于我们提出一个非常容易实现和训练的模型。为了对数据集进行预测，我们可以只计算简单的运算，而不是进行矩阵运算。我强烈建议您查看整个实现，并使用自己的数据集进行尝试。如果你有任何问题或意见，请随时联系我，我真的希望你喜欢这篇文章！</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="5645" class="mj mk je bd ml mm mn dn mo mp mq dp mr lc ms mt mu lg mv mw mx lk my mz na nb bi translated">参考</h2><p id="1d25" class="pw-post-body-paragraph kt ku je kv b kw nc kf ky kz nd ki lb lc ne le lf lg nf li lj lk ng lm ln lo im bi translated">[1]墨菲，凯文 P. <em class="lp">机器学习:概率观点</em> (2012)麻省理工学院出版社，剑桥，马萨诸塞州。</p><p id="50bd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">[2]勒村，扬。科尔特斯科琳娜。克里斯托夫·伯格斯。<em class="lp">MNIST 手写数字数据库</em>。在 http://yann.lecun.com/exdb/mnist/<a class="ae ny" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">有售</a></p><p id="024d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">[3] OpenCV。<em class="lp"> cv::HOGDescriptor 结构引用。</em>可从<a class="ae ny" href="https://docs.opencv.org/3.4.6/d5/d33/structcv_1_1HOGDescriptor.html" rel="noopener ugc nofollow" target="_blank">https://docs . opencv . org/3 . 4 . 6/D5/d33/struct cv _ 1 _ 1 hog descriptor . html</a>获得</p></div></div>    
</body>
</html>