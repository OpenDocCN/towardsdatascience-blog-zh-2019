<html>
<head>
<title>Breaking down the agglomerative clustering process</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">打破聚集的聚类过程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/breaking-down-the-agglomerative-clustering-process-1c367f74c7c2?source=collection_archive---------9-----------------------#2019-12-17">https://towardsdatascience.com/breaking-down-the-agglomerative-clustering-process-1c367f74c7c2?source=collection_archive---------9-----------------------#2019-12-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9851" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何将聚合聚类应用于数据的详细步骤</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/b4b33ed1293b176283356c82ffc4f4d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*fInDU-nPtJM__pNbqsMSfg.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">16S rRNA species tree with Neighbour Joining Cluster method and Kimura-2 Parameter model using 1000 bootstrap from my thesis “ CHEMICAL AND MOLECULAR TAXONOMY ACTINOMYCETES ISOLATE FROM JAMBI, TIMOR, AND LOMBOK (2015)<strong class="bd ku">”</strong></figcaption></figure><p id="7f5d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果你没有认出上面的图片，这是意料之中的，因为这张图片大多只能在生物杂志或教科书中找到。我上面的是一个物种进化树，这是一个物种共享的历史生物树，目的是看它们彼此之间有多接近。更一般地说，如果你熟悉<strong class="kx iu">层次聚类</strong>，它基本上就是这样。准确地说，我上面所说的是<strong class="kx iu">自下而上的</strong>或<strong class="kx iu">聚集聚类</strong>方法来创建一棵被称为邻居加入的系统发育树。</p><p id="1e01" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">准备好通过 365 Data Science 上的<strong class="kx iu">折扣价向所有专家学习数据科学吧！</strong></p><div class="lr ls gp gr lt lu"><a href="https://365datascience.pxf.io/c/3452806/1037878/11148" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd iu gy z fp lz fr fs ma fu fw is bi translated">为数据科学夏季做好准备- 65%折扣| 365 数据科学</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">选择一条职业道路，掌握你想要的特定工作角色的技能——数据科学家、数据分析师或商业…</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">365datascience.pxf.io</p></div></div><div class="md l"><div class="me l mf mg mh md mi ko lu"/></div></div></a></div></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="7b4e" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">凝聚聚类</h1><h2 id="196b" class="ni mr it bd ms nj nk dn mw nl nm dp na le nn no nc li np nq ne lm nr ns ng nt bi translated">介绍</h2><p id="9834" class="pw-post-body-paragraph kv kw it kx b ky nu ju la lb nv jx ld le nw lg lh li nx lk ll lm ny lo lp lq im bi translated">在机器学习中，无监督学习是一种在没有任何指导或标签的情况下推断数据模式的机器学习模型。无监督学习家族中包括许多模型，但我最喜欢的模型之一是凝聚聚类。</p><p id="7bfd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">凝聚聚类或自下而上聚类本质上是从一个<strong class="kx iu">个体聚类</strong>(每个数据点被视为一个个体聚类，也称为<strong class="kx iu">叶</strong>)开始，然后每个聚类计算它们彼此之间的<strong class="kx iu">距离</strong>。彼此距离最短的两个集群将<strong class="kx iu">合并</strong>，创建我们所谓的<strong class="kx iu">节点</strong>。新形成的集群再次计算其集群成员与该集群之外的另一个集群的距离。重复该过程，直到所有数据点都被分配到一个称为<strong class="kx iu">根</strong>的簇。结果是一个名为<strong class="kx iu">树状图</strong>的基于树的对象表示。</p><p id="5fa6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">提醒一下，虽然我们看到了数据应该如何聚类的结果；聚集聚类并没有给出我们的数据应该如何被聚类的确切数字。应该由我们来决定分界点在哪里。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/23b13209b9401f49fdded969d5ac7591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*R2sjYaCbSdf7kh7T66MQiQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Dendrogram as the output of the Agglomerative Clustering</figcaption></figure><p id="3ae3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们试着更详细地分解每一步。为了简单起见，我将只使用最常见的参数来解释凝聚星团是如何工作的。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="3da5" class="ni mr it bd ms nj nk dn mw nl nm dp na le nn no nc li np nq ne lm nr ns ng nt bi translated">距离测量</h2><p id="d2bb" class="pw-post-body-paragraph kv kw it kx b ky nu ju la lb nv jx ld le nw lg lh li nx lk ll lm ny lo lp lq im bi translated">我们通过测量数据点之间的距离来开始凝聚聚类过程。具体是怎么算出来的？我举一个哑数据的例子。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="d003" class="ni mr it ob b gy of og l oh oi">#creating dummy data<br/>import pandas as pd<br/>dummy = pd.DataFrame([[30,100,5],<br/>              [18, 200, 2],<br/>              [35, 150, 7],<br/>             [48, 300, 4],<br/>             [50, 200, 6]], index = ['Anne', 'Ben', 'Chad', 'Dave', 'Eric'], columns =['Age', 'Expense($)', 'Distance(KM)'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ok ol di om bf on"><div class="gh gi oj"><img src="../Images/bc13a3645eabcd996e8ed059ef343f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*_PSqgVCYI5yTWyVzSHNNgA.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Dummy data of 5 peoples with 3 dimensions</figcaption></figure><p id="d9f9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">假设我们有 5 个不同的人，他们有 3 个不同的连续特征，我们想看看如何对这些人进行聚类。首先，我们需要决定我们的聚类距离度量。</p><p id="565f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最常用的距离测量方法之一叫做<a class="ae oo" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">欧几里德距离</strong> </a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/c7fe22921034e3dda2e0984246e9d815.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*mL9YVkqNN3tCHq04kSOAHA.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Euclidean distance calculation</figcaption></figure><p id="d424" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">用一个简单的术语来说，欧几里得距离是从 x 点到 y 点的直线。我将通过我们的虚拟数据中 Anne 和 Ben 之间的距离来举例说明。</p><p id="bd71" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在虚拟数据中，我们有 3 个特征(或尺寸)代表 3 个不同的连续特征。在这种情况下，我们可以使用下面的公式计算安妮和本之间的欧几里德距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ok ol di om bf on"><div class="gh gi oq"><img src="../Images/b15f77dce59885d531244f17d3ad79a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tPvflhCO5mswMpcLkZ4U6A.png"/></div></div></figure><p id="a61b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果我们把所有的数字放在一起。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/ebb5f64a0076f336954a24d436d09131.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*0c0iiRopvm5kCro2pSilKw.png"/></div></figure><p id="6901" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">使用欧几里德距离测量，我们获得安妮和本之间的欧几里德距离为 100.76。类似地，将测量应用于所有数据点将产生以下距离矩阵。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="65c1" class="ni mr it ob b gy of og l oh oi">import numpy as np<br/>from scipy.spatial import distance_matrix</span><span id="b92d" class="ni mr it ob b gy os og l oh oi">#distance_matrix from scipy.spatial would calculate the distance between data point based on euclidean distance, and I round it to 2 decimal</span><span id="9ef2" class="ni mr it ob b gy os og l oh oi">pd.DataFrame(np.round(distance_matrix(dummy.values, dummy.values), 2), index = dummy.index, columns = dummy.index)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/8b367f81b74f4c97aa246d8b2ea29227.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*W1fEsWccONK6T2cxJ9vJ_g.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Euclidean distance matrix of the dummy data</figcaption></figure><p id="7410" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">之后，我们合并矩阵中最小的非零距离来创建我们的第一个节点。在这种情况下，是本和埃里克。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/f02b077a6b04b06942ca7eaf265549d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*SCW0PMf-8ThRHMyOJGLi4g.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Merging event between two data point to create a node</figcaption></figure><p id="dd52" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">对于新的节点或集群，我们需要更新我们的距离矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/74bc4fdad8cef43d5181d347c9b5dbb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*ekYyHyFg3RkswgFwF-lM6A.png"/></div></figure><p id="aae6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在我们有了 Ben 和 Eric 的新聚类，但是我们仍然不知道(Ben，Eric)聚类到其他数据点之间的距离。我们如何计算新的聚类距离呢？为此，我们需要首先设置链接标准。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="d6d1" class="ni mr it bd ms nj nk dn mw nl nm dp na le nn no nc li np nq ne lm nr ns ng nt bi translated">连锁标准</h2><p id="1fab" class="pw-post-body-paragraph kv kw it kx b ky nu ju la lb nv jx ld le nw lg lh li nx lk ll lm ny lo lp lq im bi translated">链接标准是精确测量距离的地方。这是我们建立的一个规则，用来定义集群之间的距离。</p><p id="b482" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">有许多联动标准，但这次我只使用最简单的联动，称为<a class="ae oo" href="https://en.wikipedia.org/wiki/Single-linkage_clustering" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">单联动</strong> </a>。它是如何工作的？我会在下面的图片中展示它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/c8dc487255599c63bfba6007458477bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*GbWxxLt_qFyqOibByHXxxQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Representation of Single linkage criterion</figcaption></figure><p id="ac9a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在单一连锁标准中，我们将我们的距离定义为聚类数据点之间的最小距离。如果我们把它放在一个数学公式里，它会是这样的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/b257b361db295f94d68a733470b18f89.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*LwWjs9ObAWYirg1vAfgzXg.png"/></div></figure><p id="0ddd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">其中聚类 X 到聚类 Y 之间的距离由分别是 X 和 Y 聚类成员的 X 和 Y 之间的最小距离定义。</p><p id="ed28" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们举一个例子。如果我们将单一连锁标准应用于我们的虚拟数据，比如 Anne 和 cluster (Ben，Eric)之间的数据，它将被描述为下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/427d515725cb3339309c4faf3292e13a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*gYuFvJZsskJlQXnZIq54nw.png"/></div></figure><p id="b08b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在单一连锁标准下，我们得到 Anne 到 cluster (Ben，Eric)之间的欧氏距离为 100.76。将单一连锁标准应用于我们的虚拟数据将产生以下距离矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/29ad992f3e0f8371efc619b829e8d3cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*yWU8ooQ_7ayvco3uDPBLeA.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Updated distance matrix</figcaption></figure><p id="b632" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，我们有了新聚类到另一个数据点之间的距离。虽然如果你注意到，安妮和乍得之间的距离现在是最小的。在这种情况下，下一个合并事件将发生在 Anne 和 Chad 之间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/7c5c3e8ee62d05696c603177f8f62fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*mFtURsqX5gV6FzuYB6aCyA.png"/></div></figure><p id="7ddc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们保持合并事件发生，直到所有的数据都聚集到一个集群中。最后，我们将获得一个树状图，其中所有数据都被合并到一个集群中。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="49a2" class="ni mr it ob b gy of og l oh oi">#importing linkage and denrogram from scipy</span><span id="2bb2" class="ni mr it ob b gy os og l oh oi">from scipy.cluster.hierarchy import linkage, dendrogram<br/>import matplotlib.pyplot as plt</span><span id="1183" class="ni mr it ob b gy os og l oh oi">#creating dendrogram based on the dummy data with single linkage criterion</span><span id="929b" class="ni mr it ob b gy os og l oh oi">den = dendrogram(linkage(dummy, method='single'), <br/>labels = dummy.index)<br/>plt.ylabel('Euclidean Distance', fontsize = 14)<br/>plt.title('Dendrogram of the Dummy Data')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/82aa53840c5498fb9d102392b762ebd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*pp17u6Li2D_LzIbiZumgHQ.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Final Dendrogram of the Agglomerative Clustering</figcaption></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="5c37" class="ni mr it bd ms nj nk dn mw nl nm dp na le nn no nc li np nq ne lm nr ns ng nt bi translated">确定聚类的数量</h2><p id="0013" class="pw-post-body-paragraph kv kw it kx b ky nu ju la lb nv jx ld le nw lg lh li nx lk ll lm ny lo lp lq im bi translated">我们已经得到了树状图，那么我们该怎么处理它呢？我们将使用它来为我们的数据选择一些集群。记住，树状图只向我们展示了数据的层次结构；它并没有准确地给出最佳的集群数量。</p><p id="54d4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">确定聚类数的最好方法是目测我们的树状图，并选择某个值作为我们的分界点(手动方式)。通常，我们选择切割最高垂直线的分界点。我会用下面的图片展示一个例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/7d114440f53d1e54a581278265e40a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*LRhprmyAsiW9sE_MdP9laA.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Dendrogram with a cut-off point at 60</figcaption></figure><p id="75bc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">水平线与垂直线相交的次数将产生该组的数目。</p><p id="65b2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">选择 60 的截止点将给出两个不同的聚类(Dave 和(Ben，Eric，Anne，Chad))。选择不同的分界点也会给我们不同的集群数量。例如，如果我们将截止点移到 52。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/ad4a78cc2fb23138b60486c36b319cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*Uvr4jNzF6giQI0SacXaZ1Q.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Dendrogram with a cut-off value at 52</figcaption></figure><p id="58c3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这一次，以 52 为截止点，我们将得到 3 个不同的集群(Dave，(Ben，Eric)和(Anne，Chad))。</p><p id="fd51" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，我们决定哪个聚类数对我们的数据有意义。在这种情况下，拥有数据的领域知识肯定会有所帮助。</p><p id="0db6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">当然，我们可以通过<a class="ae oo" href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set" rel="noopener ugc nofollow" target="_blank">特定的方法</a>自动找到集群的最佳数目；但是我相信确定聚类数的最好方法是通过观察聚类方法产生的结果。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="6dca" class="ni mr it bd ms nj nk dn mw nl nm dp na le nn no nc li np nq ne lm nr ns ng nt bi translated">凝聚聚类模型</h2><p id="0d36" class="pw-post-body-paragraph kv kw it kx b ky nu ju la lb nv jx ld le nw lg lh li nx lk ll lm ny lo lp lq im bi translated">假设我会选择值 52 作为我的分界点。这意味着我最终会有 3 个集群。有了这些知识，我们可以将它实现为一个机器学习模型。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="d2d0" class="ni mr it ob b gy of og l oh oi">from sklearn.cluster import AgglomerativeClustering</span><span id="0600" class="ni mr it ob b gy os og l oh oi">aglo = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='single')<br/>aglo.fit_predict(dummy)</span></pre><p id="a6fd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">凝聚聚类模型将产生[0，2，0，1，2]作为聚类结果。然后，我们可以将聚类结果返回给虚拟数据。在我的例子中，我将其命名为“Aglo-label”。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="b822" class="ni mr it ob b gy of og l oh oi">dummy['Aglo-label'] = aglo.fit_predict(dummy)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/5041b07e2aa7fa6b4c7c817637755a1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*SojWrhpYQQUln0BlfcPCDg.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Clustered data via Agglomerative Clustering</figcaption></figure><p id="9510" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，我的数据已经被聚类，并准备进一步分析。有必要分析结果，因为无监督学习仅推断数据模式，但它产生何种模式需要更深入的分析。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="6f80" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">结论</h1><p id="2699" class="pw-post-body-paragraph kv kw it kx b ky nu ju la lb nv jx ld le nw lg lh li nx lk ll lm ny lo lp lq im bi translated">凝聚聚类是层次聚类家族中的一员，它通过合并每个单独的聚类进行工作，这一过程一直重复，直到所有数据都成为一个聚类。</p><p id="a817" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">聚集聚类采取步骤是:</p><ol class=""><li id="f259" class="pf pg it kx b ky kz lb lc le ph li pi lm pj lq pk pl pm pn bi translated">每个数据点被指定为单个聚类</li><li id="8a3c" class="pf pg it kx b ky po lb pp le pq li pr lm ps lq pk pl pm pn bi translated">确定距离测量值并计算距离矩阵</li><li id="0284" class="pf pg it kx b ky po lb pp le pq li pr lm ps lq pk pl pm pn bi translated">确定合并集群的链接标准</li><li id="0b02" class="pf pg it kx b ky po lb pp le pq li pr lm ps lq pk pl pm pn bi translated">更新距离矩阵</li><li id="72b0" class="pf pg it kx b ky po lb pp le pq li pr lm ps lq pk pl pm pn bi translated">重复该过程，直到每个数据点成为一个聚类</li></ol><p id="438c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">用一个树状图，然后我们选择我们的截止值来获得该类的数目。</p><p id="238a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，凝聚聚类是一种无监督的学习方法，目的是从我们的数据中学习。如何解释聚类结果仍然取决于我们。</p><p id="0e3d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我在这里提供笔记本的 GitHub <a class="ae oo" href="https://github.com/cornelliusyudhawijaya/Agglomerative_Clustering/blob/master/Agglomerative%20Clustering.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>作为进一步参考。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="baed" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">访问我的<a class="ae oo" href="https://bio.link/cornelli" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">社交媒体！</strong> </a></p><blockquote class="pt"><p id="ab38" class="pu pv it bd pw px py pz qa qb qc lq dk translated">如果您没有订阅为中等会员，请考虑通过<a class="ae oo" href="https://cornelliusyudhawijaya.medium.com/membership" rel="noopener">我的推荐</a>订阅。</p></blockquote></div></div>    
</body>
</html>