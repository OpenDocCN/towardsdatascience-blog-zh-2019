<html>
<head>
<title>Model Parameters and Hyperparameters in Machine Learning — What is the difference?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的模型参数和超参数—有什么区别？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-parameters-and-hyperparameters-in-machine-learning-what-is-the-difference-702d30970f6?source=collection_archive---------6-----------------------#2019-10-30">https://towardsdatascience.com/model-parameters-and-hyperparameters-in-machine-learning-what-is-the-difference-702d30970f6?source=collection_archive---------6-----------------------#2019-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5d90" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">分析影响模型质量的参数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5a5cfaa7dafbdcf79ddc05399189e376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FIIGhzbuTo2vI62mFcbMTg.png"/></div></div></figure><p id="f404" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在机器学习模型中，有两种类型的参数:</p><ol class=""><li id="0338" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated"><strong class="kw iu">模型参数:</strong>这些是模型中必须使用训练数据集确定的参数。这些是拟合参数。</li><li id="8b91" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><strong class="kw iu">超参数:</strong>这些是可调整的参数，为了获得具有最佳性能的模型，必须对其进行调整。</li></ol><p id="0b46" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，假设您想要使用 m 维训练数据集构建一个简单的线性回归模型。那么你的模型可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/363f760718aa4a163128879b17d948c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*jHmx8cuRn6Kyd0y4q0PxGg.png"/></div></figure><p id="14db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<strong class="kw iu"> X </strong>是预测矩阵，而<strong class="kw iu"> w </strong>是权重。这里 w_0，w_1，w_2，…，w_m 是<strong class="kw iu">模型参数</strong>。<strong class="kw iu"> </strong>如果模型为了确定权重 w_0，w_1，w_2，…，w_m，使用梯度下降算法最小化目标函数，那么我们可以有一个优化器，比如 GradientDescent(eta，n_iter)。这里，eta(学习速率)和 n_iter(迭代次数)是<strong class="kw iu">超参数</strong>，为了获得模型参数 w_0，w_1，w_2，…，w_m 的最佳值，必须对其进行调整。有关这方面的更多信息，请参见以下示例:<a class="ae mf" href="https://medium.com/towards-artificial-intelligence/machine-leaning-python-linear-regression-estimator-using-gradient-descent-b0b2c496e463" rel="noopener"> <strong class="kw iu">机器学习:使用梯度下降的 Python 线性回归估计器。</strong> </a></p><h2 id="af16" class="mg mh it bd mi mj mk dn ml mm mn dp mo ld mp mq mr lh ms mt mu ll mv mw mx my bi translated">scikit-learn 软件包中使用的超参数示例</h2><ol class=""><li id="74ed" class="lq lr it kw b kx mz la na ld nb lh nc ll nd lp lv lw lx ly bi translated"><strong class="kw iu">感知器分类器</strong></li></ol><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="f4d9" class="mg mh it nf b gy nj nk l nl nm">Perceptron(n_iter=40, eta0=0.1, random_state=0)</span></pre><p id="0d28" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，n_iter 是迭代次数，eta0 是学习速率，random_state 是混洗数据时使用的伪随机数生成器的种子。</p><p id="0fea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 2。训练、测试分割估计器</strong></p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="97f1" class="mg mh it nf b gy nj nk l nl nm">train_test_split( X, y, test_size=0.4, random_state=0)</span></pre><p id="c53f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，test_size 表示要包含在测试分割中的数据集的比例，random_state 是随机数生成器使用的种子。</p><p id="94e7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 3。逻辑回归分类器</strong></p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="c8aa" class="mg mh it nf b gy nj nk l nl nm">LogisticRegression(C=1000.0, random_state=0)</span></pre><p id="de99" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，C 是正则化强度的倒数，random_state 是混洗数据时使用的伪随机数生成器的种子。</p><p id="77a5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 4。KNN (k 近邻)分类器</strong></p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="76f7" class="mg mh it nf b gy nj nk l nl nm">KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')</span></pre><p id="afec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，n_neighbors 是要使用的邻居数量，p 是闵可夫斯基度量的幂参数。当 p = 1 时，这相当于对 p = 2 使用 manhattan_distance 和 euclidean_distance。</p><p id="b197" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 5。支持向量机分类器</strong></p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="d8a9" class="mg mh it nf b gy nj nk l nl nm">SVC(kernel='linear', C=1.0, random_state=0)</span></pre><p id="ec83" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，kernel 指定了算法中使用的核类型，例如 kernel = 'linear '表示线性分类，kernel = 'rbf '表示非线性分类。c 是误差项的惩罚参数，random_state 是在混洗数据以进行概率估计时使用的伪随机数发生器的种子。</p><p id="603e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 6。决策树分类器</strong></p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="f586" class="mg mh it nf b gy nj nk l nl nm">DecisionTreeClassifier(criterion='entropy', <br/>                       max_depth=3, random_state=0)</span></pre><p id="fa22" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这里，criterion 是衡量分割质量的函数，max_depth 是树的最大深度，random_state 是随机数生成器使用的种子。</p><p id="3a73" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 7。套索回归</strong></p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="4f6d" class="mg mh it nf b gy nj nk l nl nm">Lasso(alpha = 0.1)</span></pre><p id="7676" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，α是正则化参数。</p><p id="cdb6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 8。主成分分析</strong></p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="16e0" class="mg mh it nf b gy nj nk l nl nm">PCA(n_components = 4)</span></pre><p id="720a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，n_components 是要保留的组件数。如果未设置 n_components，则保留所有组件。</p><p id="f930" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">重要的是，在模型建立期间，这些超参数被微调，以便获得具有最高质量的模型。一个模型的预测能力如何依赖于超参数的很好的例子可以从下图中找到(来源:<a class="ae mf" href="https://medium.com/towards-artificial-intelligence/bad-and-good-regression-analysis-700ca9b506ff" rel="noopener"> <strong class="kw iu">好坏回归分析</strong> </a>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/1ffa49f4c5454850e04e5627df2380c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KKjR5PliXjgLrX8X.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Regression analysis using different values of the learning rate parameter. Source: </strong><a class="ae mf" href="https://medium.com/towards-artificial-intelligence/bad-and-good-regression-analysis-700ca9b506ff" rel="noopener"><strong class="bd ns">Bad and Good Regression Analysis</strong></a><strong class="bd ns">, Published in Towards AI, February 2019, by Benjamin O. Tayo.</strong></figcaption></figure><p id="3f6e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从上图可以看出，我们模型的可靠性取决于超参数调整。如果我们只是为学习率选择一个随机值，比如 eta = 0.1，这将导致一个糟糕的模型。为 eta 选择一个太小的值，比如 eta = 0.00001，也会产生一个不好的模型。我们的分析表明，最佳选择是当 eta = 0.0001 时，从 R 平方值可以看出。</p><p id="5511" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好的和坏的机器学习模型之间的区别取决于一个人理解模型的所有细节的能力，包括关于不同超参数的知识以及如何调整这些参数以获得具有最佳性能的模型。在没有完全理解模型的错综复杂的情况下，将任何机器学习模型作为黑箱，都会导致模型被证伪。</p><h2 id="96d7" class="mg mh it bd mi mj mk dn ml mm mn dp mo ld mp mq mr lh ms mt mu ll mv mw mx my bi translated">参考</h2><ol class=""><li id="0c25" class="lq lr it kw b kx mz la na ld nb lh nc ll nd lp lv lw lx ly bi translated">《Python 机器学习》，第二版，塞巴斯蒂安·拉什卡。</li><li id="2e7d" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae mf" href="https://medium.com/towards-artificial-intelligence/machine-leaning-python-linear-regression-estimator-using-gradient-descent-b0b2c496e463" rel="noopener"> <strong class="kw iu">机器学习:使用梯度下降的 Python 线性回归估计器</strong> </a> <strong class="kw iu">。</strong></li><li id="f94c" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae mf" href="https://medium.com/towards-artificial-intelligence/bad-and-good-regression-analysis-700ca9b506ff" rel="noopener"> <strong class="kw iu">坏与好的回归分析</strong> </a>。</li></ol></div></div>    
</body>
</html>