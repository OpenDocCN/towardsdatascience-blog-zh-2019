<html>
<head>
<title>NLP Part 2| Pre-Processing Text Data Using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 第 2 部分|使用 Python 预处理文本数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/preprocessing-text-data-using-python-576206753c28?source=collection_archive---------5-----------------------#2019-04-19">https://towardsdatascience.com/preprocessing-text-data-using-python-576206753c28?source=collection_archive---------5-----------------------#2019-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4295" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让我们为分析准备/清理数据，不要忘记这是一个迭代过程。</h2></div><p id="e3e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">卡米尔·米西亚克</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/41ce951f855bed808ba3e24c07e08c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VAwL3J8M8Nxzdxw0"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Photo by <a class="ae lv" href="https://unsplash.com/@ratushny?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dmitry Ratushny</a> on <a class="ae lv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="84b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我以前的<a class="ae lv" href="https://medium.com/@kamilmysiak/scraping-the-web-using-beautifulsoup-and-python-5df8e63d9de3" rel="noopener"> <strong class="kk iu">文章</strong> </a> <strong class="kk iu"> </strong>探讨了使用名为 BeautifulSoup 的 python 库从网站上抓取文本信息的概念。我们很快就能从 Indeed.com 获取员工公司评级，并将数据导出到本地 CSV 文件中。抓取数据仅仅是从我们新获得的文本数据中收集有用见解的第一步。本文的目的是采取下一个步骤，应用一些标准的预处理步骤，以便为分析准备数据。</p><p id="2e70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您选择执行哪种预处理方法将取决于您的数据、预期结果和/或您选择的数据分析方式。也就是说，下面列出的预处理方法是一些最常用的方法。</p><ol class=""><li id="4ff8" class="md me it kk b kl km ko kp kr mf kv mg kz mh ld mi mj mk ml bi translated">将库与我们的数据一起导入</li><li id="31aa" class="md me it kk b kl mm ko mn kr mo kv mp kz mq ld mi mj mk ml bi translated">扩张收缩</li><li id="b944" class="md me it kk b kl mm ko mn kr mo kv mp kz mq ld mi mj mk ml bi translated">语言检测</li><li id="348e" class="md me it kk b kl mm ko mn kr mo kv mp kz mq ld mi mj mk ml bi translated">标记化</li><li id="05e5" class="md me it kk b kl mm ko mn kr mo kv mp kz mq ld mi mj mk ml bi translated">将所有字符转换为小写</li><li id="8046" class="md me it kk b kl mm ko mn kr mo kv mp kz mq ld mi mj mk ml bi translated">删除标点符号</li><li id="6dac" class="md me it kk b kl mm ko mn kr mo kv mp kz mq ld mi mj mk ml bi translated">删除停用词</li><li id="87be" class="md me it kk b kl mm ko mn kr mo kv mp kz mq ld mi mj mk ml bi translated">词性标注</li><li id="3fe6" class="md me it kk b kl mm ko mn kr mo kv mp kz mq ld mi mj mk ml bi translated">词汇化</li></ol></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="fe6b" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated"><strong class="ak">导入必要的库</strong></h1><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="7e38" class="no ms it nk b gy np nq l nr ns">import pandas as pd<br/>import numpy as np<br/>import nltk<br/>import string<br/>import fasttext<br/>import contractions<br/>from nltk.tokenize import word_tokenize<br/>from nltk.corpus import stopwords, wordnet<br/>from nltk.stem import WordNetLemmatizer</span><span id="895b" class="no ms it nk b gy nt nq l nr ns">plt.xticks(rotation=70)<br/>pd.options.mode.chained_assignment = None<br/>pd.set_option('display.max_colwidth', 100)<br/>%matplotlib inline</span></pre><h1 id="985b" class="mr ms it bd mt mu nu mw mx my nv na nb jz nw ka nd kc nx kd nf kf ny kg nh ni bi translated">导入我们的数据</h1><p id="650d" class="pw-post-body-paragraph ki kj it kk b kl nz ju kn ko oa jx kq kr ob kt ku kv oc kx ky kz od lb lc ld im bi translated">我们将导入在之前的<a class="ae lv" href="https://medium.com/@kamilmysiak/scraping-the-web-using-beautifulsoup-and-python-5df8e63d9de3" rel="noopener"> <strong class="kk iu">教程</strong> </a> <strong class="kk iu"> </strong>中获得的粗略员工评估评级，并快速检查数据。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="23e3" class="no ms it nk b gy np nq l nr ns">with open('indeed_scrape.csv') as f:<br/>    df = pd.read_csv(f)<br/>f.close()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oe"><img src="../Images/e98c597635f8e078a6805e544b8535d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbBulECIb8ySZLTjmSELaA.png"/></div></div></figure><p id="e5ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将重点关注“评级”和“评级 _ 描述”列，因为它们包含最有价值的定性信息。尽管我们不会对“rating”列应用任何预处理步骤。</p><p id="da49" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们删除“Unnamed: 0”列，因为它只是复制了索引。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="d5b6" class="no ms it nk b gy np nq l nr ns">df.drop('Unnamed: 0', axis=1, inplace=True)</span></pre><p id="23e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们检查是否有任何缺失的值。“评级”和“评级 _ 描述”似乎都不包含任何缺失值。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="17ef" class="no ms it nk b gy np nq l nr ns">for col in df.columns:<br/>    print(col, df[col].isnull().sum())</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/db229fff724d3171200752734b78d4cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*jQvjK75mwR57u68mUMG00A.png"/></div></figure><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="9f16" class="no ms it nk b gy np nq l nr ns">rws = df.loc[:, ['rating', 'rating_description']]</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/ad5c55ad529e534d96501b08a1c49785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*MhHtx05EAcF9KkmlNsDwdw.png"/></div></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="c4f7" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">文本预处理</h1><h2 id="c6bd" class="no ms it bd mt oh oi dn mx oj ok dp nb kr ol om nd kv on oo nf kz op oq nh or bi translated">扩张收缩</h2><p id="0265" class="pw-post-body-paragraph ki kj it kk b kl nz ju kn ko oa jx kq kr ob kt ku kv oc kx ky kz od lb lc ld im bi translated">缩写是我们采取的文学上的小捷径，我们更喜欢“应该”而不是“应该”,或者“不”很快变成了“不”。我们将向我们的数据框架添加一个名为“no_contract”的新列，并对“rating_description”字段应用一个 lambda 函数，这将扩展任何收缩。请注意，扩展的收缩将被有效地标记在一起。换句话说，“我有”=“我有”而不是“我”，“有”。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="327b" class="no ms it nk b gy np nq l nr ns">rws['no_contract'] = rws['rating_description'].apply(lambda x: [contractions.fix(word) for word in x.split()])<br/>rws.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi os"><img src="../Images/a6ee2b04f15339ac146f657622318726.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVhccXWkQcKzHN0F4dAnIQ.png"/></div></div></figure><p id="8d22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们最终希望将扩展的缩写分别标记为“I”、“have”，因此，让我们将“no_contract”列下的列表转换回字符串。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="1df5" class="no ms it nk b gy np nq l nr ns">rws['rating_description_str'] = [' '.join(map(str, l)) for l in rws['no_contract']]<br/>rws.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ot"><img src="../Images/d880dbf24a73d089d1304f332dc6f428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVlF4OhQcD0bnnBxcohy0g.png"/></div></div></figure><h2 id="8b71" class="no ms it bd mt oh oi dn mx oj ok dp nb kr ol om nd kv on oo nf kz op oq nh or bi translated">英语语言检测</h2><p id="6b2f" class="pw-post-body-paragraph ki kj it kk b kl nz ju kn ko oa jx kq kr ob kt ku kv oc kx ky kz od lb lc ld im bi translated">下一步是确定每篇评论使用的语言，然后删除任何非英语的评论。我们首先必须为我们的快速文本库下载预先训练好的语言模型(<em class="le">每个人都需要感谢 facebook 的这个</em>)。TextBlob 是一个用于检测字符串语言的常用库，但是当您解析大量文本时，它会很快抛出一个错误。一旦我们下载了模型，我们将使用 for 循环来遍历我们的评论。结果是预测语言和预测概率的元组。在我们的例子中，我们只需要第一个(即。语言预测)部分。最后，我们只选择最后两个字符。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="a9fe" class="no ms it nk b gy np nq l nr ns">pretrained_model = "lid.176.bin" <br/>model = fasttext.load_model(pretrained_model)</span><span id="116c" class="no ms it nk b gy nt nq l nr ns">langs = []<br/>for sent in rws['rating_description_str']:<br/>    lang = model.predict(sent)[0]<br/>    langs.append(str(lang)[11:13])</span><span id="07ab" class="no ms it nk b gy nt nq l nr ns">rws['langs'] = langs</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ot"><img src="../Images/c3f7d71fb8483f3d79c2f36170058a64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*03b2sGUOR_rmSl7XX8yOuA.png"/></div></div></figure><p id="675b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们要做的就是删除所有非英语评论。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ou"><img src="../Images/1e5a176ad031cce02a5922af8b3e76ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gAsMhb3bPFu6IgEjdfTWOw.png"/></div></div></figure><h2 id="5158" class="no ms it bd mt oh oi dn mx oj ok dp nb kr ol om nd kv on oo nf kz op oq nh or bi translated">标记化</h2><p id="7b1c" class="pw-post-body-paragraph ki kj it kk b kl nz ju kn ko oa jx kq kr ob kt ku kv oc kx ky kz od lb lc ld im bi translated">现在，我们已经删除了所有非英语评论，让我们应用我们的分词器，将每个单词拆分成一个单词。我们将对“rating_description_str”列应用 NLTK.word_tokenize()函数，并创建一个名为“tokenized”的新列。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="68ce" class="no ms it nk b gy np nq l nr ns">rws['tokenized'] = rws['rating_description_str'].apply(word_tokenize)<br/>rws.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi os"><img src="../Images/0ba786d25d0a7694b6b44bb946ce3157.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hn5F3cJShSjVN9HbBhZAfQ.png"/></div></div></figure><h2 id="1e08" class="no ms it bd mt oh oi dn mx oj ok dp nb kr ol om nd kv on oo nf kz op oq nh or bi translated">将所有字符转换为小写</h2><p id="2f0a" class="pw-post-body-paragraph ki kj it kk b kl nz ju kn ko oa jx kq kr ob kt ku kv oc kx ky kz od lb lc ld im bi translated">将所有单词转换成小写也是非常常见的预处理步骤。在这种情况下，我们将再次向 dataframe 追加一个名为“lower”的新列，这将把所有标记化的单词转换为小写。然而，因为我们必须迭代多个单词，所以我们将在 lambda 函数中使用一个简单的 for 循环来对每个单词应用“lower”函数。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="3031" class="no ms it nk b gy np nq l nr ns">rws['lower'] = rws['tokenized'].apply(lambda x: [word.lower() for word in x])<br/>rws.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ov"><img src="../Images/dbf85f16179178b4c34fca6669b9175f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_2uV_pDpxUddepfQRaQXA.png"/></div></div></figure><h2 id="c255" class="no ms it bd mt oh oi dn mx oj ok dp nb kr ol om nd kv on oo nf kz op oq nh or bi translated">删除标点符号</h2><p id="60f0" class="pw-post-body-paragraph ki kj it kk b kl nz ju kn ko oa jx kq kr ob kt ku kv oc kx ky kz od lb lc ld im bi translated">标点符号经常从我们的语料库中删除，因为一旦我们开始分析我们的数据，它们就没有什么价值了。继续前面的模式，我们将创建一个删除了标点符号的新列。我们将再次在 lambda 函数中使用 for 循环来迭代令牌，但这次使用 IF 条件来仅输出 alpha 字符。可能看起来有点困难，但“下方”列中的符号化“句点”已被删除。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="16e1" class="no ms it nk b gy np nq l nr ns">punc = string.punctuation<br/>rws['no_punc'] = rws['lower'].apply(lambda x: [word for word in x if word not in punc])<br/>rws.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ow"><img src="../Images/3702a3da8a41b97daa4c7d03d68ffc50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8aOR_oQ707Q34HHMV3vUpA.png"/></div></div></figure><h2 id="3155" class="no ms it bd mt oh oi dn mx oj ok dp nb kr ol om nd kv on oo nf kz op oq nh or bi translated">删除停用词</h2><p id="0abd" class="pw-post-body-paragraph ki kj it kk b kl nz ju kn ko oa jx kq kr ob kt ku kv oc kx ky kz od lb lc ld im bi translated">停用词通常是无用的词，对句子没有多大意义。英语中常见的停用词包括“你、他、她、在、一个、有、是”等。。首先，我们需要导入 NLTK 停用词库，并将停用词设置为“english”。我们将添加一个新列“no_stopwords ”,该列将从“no_punc”列中删除停用词，因为它已被标记化，已被转换为小写，并且标点符号已被删除。lambda 函数中的 for 循环将再次遍历“no_punc”中的标记，并且只返回“stop_words”变量中不存在的标记。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="f7ab" class="no ms it nk b gy np nq l nr ns">stop_words = set(stopwords.words('english'))<br/>rws['stopwords_removed'] = rws['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])<br/>rws.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ox"><img src="../Images/8a487a6235ad42619357772fab418f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*byc6Bcti8qPc3-DnIRN_WQ.png"/></div></div></figure><h1 id="433a" class="mr ms it bd mt mu nu mw mx my nv na nb jz nw ka nd kc nx kd nf kf ny kg nh ni bi translated">词干化与词汇化</h1><p id="701f" class="pw-post-body-paragraph ki kj it kk b kl nz ju kn ko oa jx kq kr ob kt ku kv oc kx ky kz od lb lc ld im bi translated">词干化的想法是将不同形式的单词用法减少到它的词根。例如，“驱动”、“被驱动”、“驾驶”、“被驱动”、“驱动者”是“驱动”一词的派生词，研究人员经常想从他们的语料库中去除这种可变性。与词干化相比，词干化当然是不太复杂的方法，但它通常不会产生特定于词典的词根。换句话说，对单词“pies”进行词干化通常会产生“pi”的词根，而词汇化会找到“pie”的词根。</p><p id="aa72" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们对我们的数据应用词汇化，而不是简单地使用词干化，但是与词干化相比，这需要一些额外的步骤。</p><p id="9d00" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们要应用词性标签，换句话说，确定词性(即。名词、动词、副词等。)每个单词。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="bd7f" class="no ms it nk b gy np nq l nr ns">rws['pos_tags'] = rws['stopwords_removed'].apply(nltk.tag.pos_tag)<br/>rws.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oy"><img src="../Images/a57910766d2b64073c9da0a2ef7f2ece.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qS47FzyVz7IZZ9QIcWCG-Q.png"/></div></div></figure><p id="2c65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用 NLTK 的 word lemmatizer，它需要将词性标签转换成 wordnet 的格式。我们将编写一个进行适当转换的函数，然后在 list comprehension 中使用该函数来应用转换。最后，我们应用 NLTK 的单词 lemmatizer。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="aaba" class="no ms it nk b gy np nq l nr ns">def get_wordnet_pos(tag):<br/>    if tag.startswith('J'):<br/>        return wordnet.ADJ<br/>    elif tag.startswith('V'):<br/>        return wordnet.VERB<br/>    elif tag.startswith('N'):<br/>        return wordnet.NOUN<br/>    elif tag.startswith('R'):<br/>        return wordnet.ADV<br/>    else:<br/>        return wordnet.NOUN</span><span id="fa67" class="no ms it nk b gy nt nq l nr ns">rws['wordnet_pos'] = rws['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])<br/>rws.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oz"><img src="../Images/2deb20789e60ffb3ef555b2489896543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0EP4v3LZsVqZOi_EwkuISw.png"/></div></div></figure><p id="1125" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们可以在可靠的列表理解中应用 NLTK 的单词 lemmatizer。注意，lemmatizer 函数需要两个参数单词及其标签(以 wordnet 的形式)。</p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="c6de" class="no ms it nk b gy np nq l nr ns">wnl = WordNetLemmatizer()<br/>rws['lemmatized'] = rws['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])<br/>rws.head()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pa"><img src="../Images/514993a31361453deca55c0552ed635d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QWW1y3-UAzDVDMyEuW2ZSQ.png"/></div></div></figure><p id="e626" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们将把这项工作保存到一个 csv 文件中，用于进一步的探索性数据分析，您可以在我的下一篇<a class="ae lv" href="https://medium.com/@kamilmysiak/nlp-part-3-exploratory-data-analysis-of-text-data-1caa8ab3f79d" rel="noopener"> <strong class="kk iu">博客</strong> </a> <strong class="kk iu">中读到所有相关内容。</strong></p><pre class="lg lh li lj gt nj nk nl nm aw nn bi"><span id="1f92" class="no ms it nk b gy np nq l nr ns">rws.to_csv('indeed_scrape_clean.csv')</span></pre><p id="723e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">文本预处理可能很快成为编辑和进一步技术的兔子洞，但我们必须在某处划清界限。随着分析过程的深入，我经常会回到数据预处理阶段，因为我发现了一些需要解决的数据问题。也就是说，要学会在某处划清界限。</p></div></div>    
</body>
</html>