<html>
<head>
<title>From Pandas to PySpark with Koalas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从熊猫到有考拉的派斯帕克</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-pandas-to-pyspark-with-koalas-e40f293be7c8?source=collection_archive---------7-----------------------#2019-10-28">https://towardsdatascience.com/from-pandas-to-pyspark-with-koalas-e40f293be7c8?source=collection_archive---------7-----------------------#2019-10-28</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><figure class="gm go js jt ju jv gi gj paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="gi gj jr"><img src="../Images/26455d3ec084a4747b02a79c6605e8a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KLDD9LqbqW_mKM2i"/></div></div><figcaption class="kc kd gk gi gj ke kf bd b be z dk">Photo by <a class="ae kg" href="https://unsplash.com/@ozgut?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ozgu Ozden</a> on <a class="ae kg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="3ca0" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">对于那些熟悉<a class="ae kg" href="https://pandas.pydata.org" rel="noopener ugc nofollow" target="_blank">熊猫</a>数据帧的人来说，切换到 PySpark 可能会相当混乱。API 是不一样的，当切换到分布式特性时，由于该特性所施加的限制，有些事情的处理方式会非常不同。</p><p id="2db1" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我最近在一个非常有趣的关于 Apache Spark 3.0、Delta Lake 和考拉的 Databricks 演示中偶然发现了<a class="ae kg" href="https://github.com/databricks/koalas" rel="noopener ugc nofollow" target="_blank">考拉</a>，我想探索一下会很不错。</p><blockquote class="lf lg lh"><p id="9ce8" class="kh ki li kj b kk kl km kn ko kp kq kr lj kt ku kv lk kx ky kz ll lb lc ld le in bi translated">考拉项目通过在 Apache Spark 上实现 pandas DataFrame API，使数据科学家在与大数据交互时更有效率。</p><p id="823a" class="kh ki li kj b kk kl km kn ko kp kq kr lj kt ku kv lk kx ky kz ll lb lc ld le in bi translated">pandas 是 Python 中事实上的标准(单节点)DataFrame 实现，而 Spark 是大数据处理的事实上的标准。使用此软件包，您可以:</p><p id="7ff8" class="kh ki li kj b kk kl km kn ko kp kq kr lj kt ku kv lk kx ky kz ll lb lc ld le in bi translated">-如果您已经熟悉熊猫，使用 Spark 可以立即提高工作效率，无需学习曲线。</p><p id="0a75" class="kh ki li kj b kk kl km kn ko kp kq kr lj kt ku kv lk kx ky kz ll lb lc ld le in bi translated">-拥有一个既适用于 pandas(测试，较小的数据集)又适用于 Spark(分布式数据集)的单一代码库。</p></blockquote><p id="956a" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">来源:<a class="ae kg" href="https://koalas.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">https://koalas.readthedocs.io/en/latest/index.html</a></p><figure class="lm ln lo lp gu jv"><div class="bz fq l di"><div class="lq lr l"/></div></figure><h1 id="7284" class="ls lt iu bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated"><strong class="ak">如何入门</strong></h1><p id="b04b" class="pw-post-body-paragraph kh ki iu kj b kk mq km kn ko mr kq kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">考拉支持≥ <strong class="kj iv"> Python 3.5 </strong>，从我从文档中看到的来看，<strong class="kj iv"> PySpark 2.4.x. </strong>依赖项包括 pandas ≥ 0.23.0，pyarrow ≥ 0.10 用于使用柱状内存格式以获得更好的矢量操作性能，matplotlib ≥ 3.0.0 用于绘图。</p><h2 id="5328" class="mv lt iu bd lu mw mx dn ly my mz dp mc ks na nb mg kw nc nd mk la ne nf mo ng bi translated">装置</h2><p id="3068" class="pw-post-body-paragraph kh ki iu kj b kk mq km kn ko mr kq kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">下面列出了安装考拉的不同方法:</p><div class="nh ni gq gs nj nk"><a href="https://koalas.readthedocs.io/en/latest/getting_started/install.html" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fp"><div class="nm ab nn cl cj no"><h2 class="bd iv gz z fq np fs ft nq fv fx it bi translated">安装-考拉 0.20.0 文档</h2><div class="nr l"><h3 class="bd b gz z fq np fs ft nq fv fx dk translated">正式 Python 3.5 及以上。首先你需要安装 Conda。此后，我们应该创造一个新的环境</h3></div><div class="ns l"><p class="bd b dl z fq np fs ft nq fv fx dk translated">考拉. readthedocs.io</p></div></div></div></a></div><p id="40f3" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">但是让我们从简单的开始:</p><p id="17de" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><code class="fe nt nu nv nw b">pip install koalas</code>和<code class="fe nt nu nv nw b">pip install pyspark</code></p><p id="c3c4" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">请记住上面提到的依赖性。</p><h2 id="c62e" class="mv lt iu bd lu mw mx dn ly my mz dp mc ks na nb mg kw nc nd mk la ne nf mo ng bi translated">使用</h2><p id="f04e" class="pw-post-body-paragraph kh ki iu kj b kk mq km kn ko mr kq kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">给定以下数据:</p><pre class="lm ln lo lp gu nx nw ny nz aw oa bi"><span id="7cf7" class="mv lt iu nw b gz ob oc l od oe"><strong class="nw iv">import </strong>pandas <strong class="nw iv">as </strong>pd<br/><strong class="nw iv">from </strong>databricks <strong class="nw iv">import </strong>koalas <strong class="nw iv">as </strong>ks<br/><strong class="nw iv">from </strong>pyspark.sql <strong class="nw iv">import </strong>SparkSession<br/><br/><br/>data = {<strong class="nw iv">'a'</strong>: [1, 2, 3, 4, 5, 6],<br/>        <strong class="nw iv">'b'</strong>: [100, 200, 300, 400, 500, 600],<br/>        <strong class="nw iv">'c'</strong>: [<strong class="nw iv">"one"</strong>, <strong class="nw iv">"two"</strong>, <strong class="nw iv">"three"</strong>, <strong class="nw iv">"four"</strong>, <strong class="nw iv">"five"</strong>, <strong class="nw iv">"six"</strong>]}<br/><br/>index = [10, 20, 30, 40, 50, 60]</span></pre><p id="ba86" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">你可以从熊猫的数据框架开始:</p><pre class="lm ln lo lp gu nx nw ny nz aw oa bi"><span id="d9fc" class="mv lt iu nw b gz ob oc l od oe">pdf = pd.DataFrame(data, index=index)</span><span id="5388" class="mv lt iu nw b gz of oc l od oe"><em class="li"># from a pandas dataframe<br/></em>kdf = ks.from_pandas(pdf)</span></pre><p id="4f36" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">来自考拉的数据框架:</p><pre class="lm ln lo lp gu nx nw ny nz aw oa bi"><span id="f373" class="mv lt iu nw b gz ob oc l od oe"><em class="li"># start from raw data<br/></em>kdf = ks.DataFrame(data, index=index)</span></pre><p id="a42a" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">或来自火花数据帧(单向):</p><pre class="lm ln lo lp gu nx nw ny nz aw oa bi"><span id="42b4" class="mv lt iu nw b gz ob oc l od oe"># creating a spark dataframe from a pandas dataframe<br/>sdf2 = spark_session.createDataFrame(pdf)</span><span id="3e41" class="mv lt iu nw b gz of oc l od oe"># and then converting the spark dataframe to a koalas dataframe<br/>kdf = sdf.to_koalas('index')</span></pre><p id="1435" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">一个完整的简单输出示例:</p><figure class="lm ln lo lp gu jv"><div class="bz fq l di"><div class="og lr l"/></div><figcaption class="kc kd gk gi gj ke kf bd b be z dk">A simple comparison of pandas, Koalas, pyspark Dataframe API</figcaption></figure><p id="3e70" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">熊猫和考拉的 API 差不多。官方文档中的更多示例:</p><div class="nh ni gq gs nj nk"><a href="https://koalas.readthedocs.io/en/latest/getting_started/10min.html" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fp"><div class="nm ab nn cl cj no"><h2 class="bd iv gz z fq np fs ft nq fv fx it bi translated">10 分钟到考拉-考拉 0.20.0 文档</h2><div class="nr l"><h3 class="bd b gz z fq np fs ft nq fv fx dk translated">这是对考拉的简短介绍，主要面向新用户。本笔记本向您展示了一些关键的不同之处…</h3></div><div class="ns l"><p class="bd b dl z fq np fs ft nq fv fx dk translated">考拉. readthedocs.io</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om ka nk"/></div></div></a></div></div><div class="ab cl on oo hy op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="in io ip iq ir"><h1 id="50b2" class="ls lt iu bd lu lv ou lx ly lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp bi translated">谨记在心</h1><p id="ca31" class="pw-post-body-paragraph kh ki iu kj b kk mq km kn ko mr kq kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">关于考拉项目的一些说明:</p><ul class=""><li id="f91e" class="oz pa iu kj b kk kl ko kp ks pb kw pc la pd le pe pf pg ph bi translated">如果你是从零开始，没有任何关于熊猫的知识，那么直接进入 PySpark 可能是一个更好的学习方法。</li><li id="5a66" class="oz pa iu kj b kk pi ko pj ks pk kw pl la pm le pe pf pg ph bi translated">一些功能可能<strong class="kj iv">丢失</strong> —丢失的功能在<a class="ae kg" href="https://github.com/databricks/koalas/tree/master/databricks/koalas/missing" rel="noopener ugc nofollow" target="_blank">这里</a>记录</li><li id="9391" class="oz pa iu kj b kk pi ko pj ks pk kw pl la pm le pe pf pg ph bi translated">一些行为可能<strong class="kj iv">不同</strong>(例如，Null 与 NaN，NaN 用于考拉，更适合熊猫，Null 用于 Spark)</li><li id="eb6a" class="oz pa iu kj b kk pi ko pj ks pk kw pl la pm le pe pf pg ph bi translated">请记住，由于它是在幕后使用 Spark，s <strong class="kj iv"> ome 操作是懒惰的</strong>，这意味着在有 Spark 动作之前，它们不会真正被评估和执行，比如打印出前 20 行。</li><li id="9e0c" class="oz pa iu kj b kk pi ko pj ks pk kw pl la pm le pe pf pg ph bi translated">我确实对一些操作的效率有些担心，例如<code class="fe nt nu nv nw b"><em class="li">.to_koalas()</em></code> <em class="li"> </em>给出了一个<code class="fe nt nu nv nw b"><em class="li">No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</em></code> <em class="li"> </em>警告，这似乎是因为索引操作和<strong class="kj iv">可能会有相当大的问题，这取决于您拥有的数据量。</strong>请注意，当您在<code class="fe nt nu nv nw b">.to_koalas('index')</code>中指定索引列名称时，警告并不存在，这是合理的，因为 spark/koalas 知道使用哪一列作为索引，并且不需要将所有数据放入一个分区来计算全局排名/索引。更多细节请看这里:<a class="ae kg" rel="noopener" target="_blank" href="/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6">https://towards data science . com/adding-sequential-ids-to-a-spark-data frame-fa 0 df 5566 ff 6</a></li></ul><h1 id="1654" class="ls lt iu bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated"><strong class="ak">结论</strong></h1><p id="a0dd" class="pw-post-body-paragraph kh ki iu kj b kk mq km kn ko mr kq kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated"><strong class="kj iv">免责声明:</strong>我真的没有怎么使用它，因为当我开始学习 spark 时，这还不可用，但我真的认为了解可用的工具是很好的，并且它可能对那些来自熊猫环境的人有所帮助——我仍然记得我从熊猫数据帧切换到 Spark 数据帧时的困惑。</p><p id="21e0" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我希望这是有帮助的，并且知道考拉将会节省你一些时间和麻烦。任何想法，问题，更正和建议都非常欢迎:)</p><p id="e545" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">如果您想了解更多关于 Spark 的工作原理，请访问:</p><div class="nh ni gq gs nj nk"><a rel="noopener follow" target="_blank" href="/explaining-technical-stuff-in-a-non-techincal-way-apache-spark-274d6c9f70e9"><div class="nl ab fp"><div class="nm ab nn cl cj no"><h2 class="bd iv gz z fq np fs ft nq fv fx it bi translated">用非技术性的方式解释技术性的东西——Apache Spark</h2><div class="nr l"><h3 class="bd b gz z fq np fs ft nq fv fx dk translated">什么是 Spark 和 PySpark，我可以用它做什么？</h3></div><div class="ns l"><p class="bd b dl z fq np fs ft nq fv fx dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="pn l oj ok ol oh om ka nk"/></div></div></a></div><p id="3c5e" class="pw-post-body-paragraph kh ki iu kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">关于在 Spark 中添加索引:</p><div class="nh ni gq gs nj nk"><a rel="noopener follow" target="_blank" href="/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6"><div class="nl ab fp"><div class="nm ab nn cl cj no"><h2 class="bd iv gz z fq np fs ft nq fv fx it bi translated">向 Spark 数据帧添加顺序 id</h2><div class="nr l"><h3 class="bd b gz z fq np fs ft nq fv fx dk translated">怎么做，这是个好主意吗？</h3></div><div class="ns l"><p class="bd b dl z fq np fs ft nq fv fx dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="po l oj ok ol oh om ka nk"/></div></div></a></div></div></div>    
</body>
</html>