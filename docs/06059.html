<html>
<head>
<title>Probability Learning IV : The Math Behind Bayes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">概率学习 IV:贝叶斯背后的数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/probability-learning-iv-the-math-behind-bayes-bfb94ea03dd8?source=collection_archive---------18-----------------------#2019-09-03">https://towardsdatascience.com/probability-learning-iv-the-math-behind-bayes-bfb94ea03dd8?source=collection_archive---------18-----------------------#2019-09-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c08c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">支持分类和回归的贝叶斯定理的数学充分解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ce7dc680182913ecca27dbf595fcc79a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYUC3cHTrwm-ajRETWCpJA.jpeg"/></div></div></figure><p id="5a2b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在之前的两篇关于<strong class="kw iu">贝叶斯定理</strong>的帖子之后，我收到了很多请求，要求对定理的回归和分类用途背后的<strong class="kw iu">数学进行更深入的解释。</strong></p><p id="3708" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正因为如此，在上一篇文章中，我们介绍了<strong class="kw iu">最大似然原理</strong>背后的数学原理，以构建一个坚实的基础，让我们能够轻松理解并享受贝叶斯背后的数学原理。</p><p id="3335" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可以在这里找到所有这些帖子:</p><ul class=""><li id="6752" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/probability-learning-i-bayes-theorem-708a4c02909a">概率学习 I:贝叶斯定理</a></li><li id="a656" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp lv lw lx ly bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/probability-learning-ii-how-bayes-theorem-is-applied-in-machine-learning-bd747a960962">概率学习 II:贝叶斯定理如何应用于机器学习</a></li><li id="6e56" class="lq lr it kw b kx ma la mb ld mc lh md ll me lp lv lw lx ly bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/probability-learning-iii-maximum-likelihood-e78d5ebea80c">概率学习三:最大似然</a></li></ul><p id="0a44" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这篇文章将致力于<strong class="kw iu">解释贝叶斯定理</strong>背后的数学，当它的应用有意义时，以及它与最大似然法的差异。</p><h1 id="5aac" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated"><strong class="ak">闪回贝叶斯</strong></h1><p id="f28b" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">正如在<a class="ae lz" rel="noopener" target="_blank" href="/probability-learning-iii-maximum-likelihood-e78d5ebea80c">上一篇文章</a>中我们解释了<strong class="kw iu">最大似然</strong>一样，我们将在这篇文章的第一部分记住贝叶斯定理背后的公式，特别是与我们在机器学习中相关的公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/ded305b557300a3b29f2e188de2a6393.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*gdgddVSaJQ_BXWJJNYtZ9g.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 1: Bayes formula particularised for a Machine Learning model and its relevant data</figcaption></figure><p id="2b90" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们将此公式放入<strong class="kw iu">与我们在<a class="ae lz" rel="noopener" target="_blank" href="/probability-learning-iii-maximum-likelihood-e78d5ebea80c">上一篇文章</a>中使用的关于最大似然的相同的数学术语</strong>，我们会得到以下公式，其中<strong class="kw iu"> θ是模型的参数，X 是我们的数据矩阵</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c9f778753e31e65f095fd3a843cf399e.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*tvaribyQUbPz5FBmGbM9FQ.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 2: Bayes formula expressed in terms of the model parameters “θ” and the data matrix “X”</figcaption></figure><p id="20e8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们在致力于<a class="ae lz" rel="noopener" target="_blank" href="/probability-learning-ii-how-bayes-theorem-is-applied-in-machine-learning-bd747a960962">贝叶斯定理和机器学习</a>的帖子中提到的，贝叶斯定理的<strong class="kw iu">优势是能够<strong class="kw iu">将一些关于模型的先前知识</strong>整合到我们的工具集中，使其在某些情况下更加健壮。</strong></p><h1 id="368c" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">贝叶斯背后的数学解释</h1><p id="fd1b" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">既然我们已经很快记住了贝叶斯定理是关于什么的，让我们充分开发它背后的数学知识。如果我们采用前面的公式，并且<strong class="kw iu">以对数方式表示它</strong>，我们得到下面的等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e765c5392edc5f3044c22ebc38dcc60a.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*8495SXWOV0IbxOa-xeatVg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 3: Bayes’ formula expressed in logarithms</figcaption></figure><p id="9170" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">还记得最大似然公式吗？等号右边的第一项你看着眼熟吗？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/98d350cd85408f4742fb2bb4a362fead.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*70YkafomXu_lXNY0j2Obpg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 4: Likelihood function</figcaption></figure><p id="9cba" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果有，说明你做足了功课，看了前面的帖子，理解了。等号右边的第一项正好是似然函数。<strong class="kw iu">这是什么意思？</strong>这意味着<strong class="kw iu">最大似然和贝叶斯定理以某种方式相关</strong>。</p><p id="d0de" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看看当我们<strong class="kw iu">对模型参数</strong>求导时会发生什么，类似于我们在最大似然法中所做的，以计算最适合我们数据的分布(我们在上一篇文章中对最大似然法所做的也可以用于计算特定机器学习模型的参数，最大化我们数据的概率，而不是某个分布)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/89544121655bfec47c03df6e86596227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-QCsoCYiNflJet0BpE_YNA.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 5: Taking derivatives with respect to the model parameters</figcaption></figure><p id="2478" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到这个方程有<strong class="kw iu">两个依赖于θ </strong>、<strong class="kw iu">的项，其中之一我们在</strong>之前已经看到过:<strong class="kw iu">似然函数</strong>关于θ的导数。然而，另一个术语对我们来说是陌生的。这个术语代表了我们可能拥有的关于模型的<strong class="kw iu">先验知识，稍后我们将看到它如何对我们大有用处。</strong></p><p id="51af" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们用一个例子来说明。</p><h1 id="da21" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">回归的最大似然和贝叶斯定理的比较</h1><p id="1e4a" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">让我们用一个我们以前探索过的例子来看看这个术语是如何使用的:<strong class="kw iu">线性回归</strong>。让我们恢复等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/e618de35ad7595712eb07fc3d4ac296b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*kiOmyKPFfzjH_0SEJVUJKw.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 6: Equation of linear regression of degree 1</figcaption></figure><p id="7788" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们将这个线性回归方程表示为依赖于一些数据和一些未知参数向量θ 的更一般的函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/b522e6d37febbd54328674c220915d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gxYFlkpODC_P6zxneH1wEw.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 7: Regression function</figcaption></figure><p id="4614" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，让我们假设当我们使用这个回归函数进行预测时，存在某个<strong class="kw iu">相关误差<em class="nn">ɛ</em></strong><em class="nn">。</em>然后，每当我们做一个预测<strong class="kw iu"> <em class="nn"> y(i) </em> </strong>(忘记上面用于 LR 的 y，那个现在已经被<strong class="kw iu"> <em class="nn"> f </em> </strong>代替了)，我们就有了一个表示回归函数得到的值的项，<strong class="kw iu">和某个相关的误差</strong>。</p><p id="ec0d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所有这些的组合看起来像是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/0cd4806b1f0c58a4248ac5ee670c2769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mqGqbmlnUypn99gggJvC8w.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 8: Final form of the regression equation</figcaption></figure><p id="997a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">获得模型<strong class="kw iu"> θ </strong>的一个众所周知的方法是使用<strong class="kw iu">最小二乘法(LSM) </strong>并寻找减少某种误差的参数集。具体来说，我们希望<strong class="kw iu">减少一个误差</strong>，该误差被公式化为每个数据点<strong class="kw iu"> <em class="nn"> y </em> </strong>的实际标签和模型<strong class="kw iu"> <em class="nn"> f </em> </strong>的预测输出之间的平方的平均差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/01499c486fa5603e9be884c0d0c2aff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w7hA0VxfqmSZRvYyLjbmYw.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 9: The error that we want to reduce in the Least squares method</figcaption></figure><p id="6a91" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将看到,<strong class="kw iu">试图减少这一误差相当于使用最大似然估计法最大化观察我们的数据</strong>与某些模型参数的概率。</p><p id="e827" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">首先，然而</strong>我们必须做出一个非常重要的假设，尽管<strong class="kw iu">自然假设</strong>:回归误差<strong class="kw iu"><em class="nn">【ɛ(i】，</em> </strong>对于每一个数据点来说，都是独立于<strong class="kw iu"><em class="nn">【x(I)</em></strong>(数据点)的值，并且正态分布着一个平均值 0 和一个标准差<strong class="kw iu"> <em class="nn"> σ。对于大多数错误类型，这种假设通常是正确的。</em></strong></p><p id="e00e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，特定参数集θ 的<strong class="kw iu"> ML 估计由以下等式给出，其中我们应用了<strong class="kw iu">条件概率的公式，假设 X 独立于模型参数的</strong>，并且<strong class="kw iu"> <em class="nn"> y(i) </em> </strong>的值彼此独立(为了能够使用乘法)</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/28c44b5d036942ebbc46dab953a9f324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L6cgyNmBm_g-JDxFo3yX2A.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 10: Likelihood function</figcaption></figure><p id="1b65" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个公式可以读作:X 和 Y 给定<strong class="kw iu"> θ </strong>的概率等于 X 的概率乘以 Y 给定 X 和θ <strong class="kw iu">的概率。</strong></p><p id="4562" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于那些不熟悉<strong class="kw iu">联合或组合和条件概率的人，你可以在这里找到一个简单易懂的解释</strong><a class="ae lz" href="http://apt.cs.manchester.ac.uk/ftp/pub/ai/jls/CS2411/prob97/node5.html" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"/></a><strong class="kw iu">。如果你仍然不能从最左边的术语找到最终结果，请随时联系我；我的信息在文末。</strong></p><p id="e480" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，如果我们像过去一样取对数，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/09ed64ab5dbe991318c9ba87258c7654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rPqwO-kbMGHlO5svb1j3OQ.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 11: Same equation as above expressed in logs</figcaption></figure><p id="5065" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果<strong class="kw iu"> X(数据点的特征)</strong>是<strong class="kw iu">静态的并且彼此独立</strong>(就像我们之前在条件概率中假设的那样)<strong class="kw iu">，那么 y(i)的分布与误差的分布</strong>(来自公式 8)相同，除了平均值现在已经被移动到 f(x(i)|θ而不是 0。这意味着<strong class="kw iu"> y(i)也有正态分布</strong>，我们可以将条件概率<strong class="kw iu"> <em class="nn"> p(y(i)|X，θ) </em> </strong>表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/c979513ebc90900537f82e1857081063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C1HELFGU5_gK-UmQf8oOeg.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 12: Conditional probability of y(i) given the data and the parameters</figcaption></figure><p id="4171" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们使常数等于 1 以简化，并在公式 11 内替换公式 12，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/d7c6b3385931de74c63d32606941d58b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbZ9yW-AJK30I0qyg7j-xw.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 13: Logarithm of the Likelihood function</figcaption></figure><p id="6c3c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们试图最大化它(对θ求导)，项<strong class="kw iu"> ln <em class="nn"> p </em> (X) </strong>将不复存在，我们只有负平方和的导数:<strong class="kw iu">这意味着最大化似然函数相当于最小化平方和！</strong></p><h2 id="99b0" class="nu mg it bd mh nv nw dn ml nx ny dp mp ld nz oa mr lh ob oc mt ll od oe mv of bi translated">Bayes 能做些什么来让这一切变得更好呢？</h2><p id="dbea" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">让我们恢复用对数表示的贝叶斯定理公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e765c5392edc5f3044c22ebc38dcc60a.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*8495SXWOV0IbxOa-xeatVg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 3: Bayes’ formula expressed in logarithms</figcaption></figure><p id="5dd9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">等号右边的第一项，正如我们之前看到的，是<strong class="kw iu">可能性项</strong>，这正是我们在公式 13 中的内容。如果我们将公式 13 的值代入公式 3，考虑到我们也有数据标签 Y，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/094893573eadb34ae7af373fdb42e483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Ig0FNtDSUlX8DcwbElqMA.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 14: Bayes formula expressed in terms on likelihood and logarithms</figcaption></figure><p id="f820" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，如果我们试图最大化这个函数，以找到最有可能使我们的数据被观察到的模型参数，我们就有了一个<strong class="kw iu">额外项:<em class="nn"> ln p(θ)。</em>还记得这个术语代表什么吗？就是这样，模型参数的先验知识。</strong></p><p id="2e22" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里<strong class="kw iu">我们可以开始看到一些有趣的东西</strong> : <strong class="kw iu"> σ与数据</strong>的噪声方差相关。由于项<strong class="kw iu"> <em class="nn"> ln p(θ) </em> </strong>在求和之外，如果我们有一个<strong class="kw iu">非常大的噪声方差，求和项变小，以前的知识占优势</strong>。但如果数据准确，误差小，这个先验知识术语就没那么有用了<strong class="kw iu"> <em class="nn">。</em>T19】</strong></p><p id="e67e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">还看不出用途？让我们用一个例子来总结这一切。</p><h2 id="ee32" class="nu mg it bd mh nv nw dn ml nx ny dp mp ld nz oa mr lh ob oc mt ll od oe mv of bi translated">ML 与 Bayes:线性回归示例</h2><p id="5d66" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">假设我们有一个一阶线性回归模型，就像我们在这篇文章和之前的文章中一直使用的模型。</p><p id="2824" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在下面的等式中，我已经将模型参数的<strong class="kw iu"> <em class="nn"> θs </em> </strong>替换为<strong class="kw iu"><em class="nn"/></strong>a<strong class="kw iu"><em class="nn">【b</em></strong>(它们代表相同的东西，但符号更简单)并添加了误差项。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d97e1a24c7b13edddf93c9f447dee109.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*2M_wwTiGvCG3BNAjHY_I3A.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 15: Our linear regression model</figcaption></figure><p id="1143" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们使用 Bayes 估计，假设我们有一些关于<strong class="kw iu"> <em class="nn"> a </em> </strong>和<strong class="kw iu"><em class="nn">b</em></strong>:<strong class="kw iu"><em class="nn"/></strong>a<strong class="kw iu"><em class="nn">b</em></strong>的均值为 0，标准差为 0.1，均值为 1，标准差为 0.5。这意味着 a 和 b 的密度函数分别为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/02ba4a4749fcbf00b59304c947d1cf9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*ihLKxdAhYVGZ_ltC-aOFRg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 16: Density functions for a and b</figcaption></figure><p id="6c88" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们移除常数，并将该信息代入公式 14，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/dea8ca4113aa2eb7cdd32e2cf272c306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvC1fPtp4KWVOtEARNVjLA.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Formula 17: Final form of Bayes Formula</figcaption></figure><p id="d87f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，如果我们<strong class="kw iu">用</strong>对 a 求导，假设所有其他参数为常数，我们得到以下值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4a5029337d1e3288633a7f7fd2af1c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*EByygoPtG0S1y_lwtO8rEg.png"/></div></figure><p id="86b7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样，对于 b，给我们一个带有两个变量的线性方程，从中我们将<strong class="kw iu">获得模型参数的值，该值报告观察我们的数据</strong>(或减少误差)的最高概率。</p><p id="f4db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">贝叶斯贡献在哪里？</strong>非常简单，没有它，我们将失去术语 100σ。这是什么意思？<strong class="kw iu">你看，σ与模型的误差方差有关，正如我们之前提到的。如果这个误差方差很小，说明数据是可靠的、准确的，所以计算出的参数可以取很大的值，这就可以了</strong>。</p><p id="064f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">然而，通过引入 100σ项，如果该噪声显著，它将迫使参数值更小，这通常使得回归模型的表现优于具有非常大的参数值的回归模型。</strong></p><p id="a37c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们还可以在这里看到分母中的值 n，它表示我们有多少数据。与σ值无关，如果我们增加 n，这一项就失去了重要性。这突出了这种方法的另一个特点:我们拥有的数据越多，贝叶斯的初始先验知识的影响就越小。</p><p id="95d0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就是这样:拥有以前的数据知识，有助于我们限制模型参数的值，因为加入贝叶斯总是会导致更小的值，这往往会使模型表现得更好。</p><h1 id="fe4c" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结论</h1><p id="3030" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们已经看到了贝叶斯定理、<strong class="kw iu">最大似然法及其比较</strong>背后的完整数学。我希望一切都尽可能清楚，并且回答了你的许多问题。</p><p id="6485" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我有一些好消息:<strong class="kw iu">繁重的数学岗位结束了</strong>；在下一篇文章中，我们将讨论<strong class="kw iu">朴素贝叶斯</strong>，一种贝叶斯定理的简化，以及它在<strong class="kw iu">自然语言处理中的应用。</strong></p><p id="5118" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">来看看吧<a class="ae lz" href="https://medium.com/@jaimezornoza" rel="noopener"> <strong class="kw iu">关注我的</strong> </a>，敬请关注！</p><p id="6ae9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就这些，我希望你喜欢这个帖子。请随时在 LinkedIn 上与我联系，或者在 Twitter 上关注我，地址是:jaimezorno。还有，你可以看看我其他关于数据科学和机器学习的帖子<strong class="kw iu"> </strong> <a class="ae lz" href="https://medium.com/@jaimezornoza" rel="noopener"> <strong class="kw iu">这里</strong> </a>。好好读！</p><p id="356c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一如既往，有任何问题联系我。祝你有美好的一天，继续学习。</p></div></div>    
</body>
</html>