<html>
<head>
<title>Geometric Deep Learning for Pose Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于姿态估计的几何深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/geometric-deep-learning-for-pose-estimation-6af45da05922?source=collection_archive---------3-----------------------#2019-05-18">https://towardsdatascience.com/geometric-deep-learning-for-pose-estimation-6af45da05922?source=collection_archive---------3-----------------------#2019-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="920d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从单目图像中寻找物体姿态的理论和 Pytorch 实现教程</h2></div><p id="08c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">计算机视觉和机器人学的中心问题之一是理解物体相对于机器人或环境是如何定位的。在这篇文章中，我将解释背后的理论，并给出 Pavlakos 等人的论文<a class="ae le" href="https://arxiv.org/pdf/1703.04670.pdf" rel="noopener ugc nofollow" target="_blank">“基于语义关键点的 6 自由度物体姿态”的 pytorch 实现教程。</a></p><p id="acf2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种方法中，有两个步骤。第一步是预测 2D 图像上的“语义关键点”。在第二步中，我们通过使用透视相机模型最大化预测的语义关键点集合和对象的 3D 模型之间的几何一致性来估计对象的姿态。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/a338973db3e5efa9bfc1f246b830a270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxvA2KGmjSCR9Q8IQkZl5g.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Overlay of the projected 3D model on the monocular image</figcaption></figure><h1 id="6a39" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">关键点定位</h1><p id="1247" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">首先，我们使用标准的现成对象检测算法，如 fast-RCNN，在感兴趣的对象上放置一个边界框。然后，我们只考虑关键点定位的对象区域。为此，本文使用了“堆叠沙漏”网络架构。该网络接收一幅 RGB 图像并输出一组热图——每个关键点一个热图。热图允许网络表达其对一个区域的信心，而不是回归关键点的单个 x，y 位置。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ms"><img src="../Images/b022791ec3f91c18f4f7ce2b15a7bc72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwM2A2dIiBHE4cLfEzxUfA.png"/></div></div></figure><p id="c73c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上图可以看出，网络有两个沙漏。此外，每个沙漏具有下采样部分和上采样部分。第二个沙漏的目的是优化第一个沙漏的输出。缩减像素采样部分由交替的卷积层和最大池层组成。当输出达到 4 X 4 的分辨率时，上采样开始。上采样部分由卷积和上采样(去卷积)层组成。使用第一和第二沙漏的输出的 L2 损耗来训练网络。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mt"><img src="../Images/82b66857c5a956eaf2af3c52081cd1b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TwKH8PVjOBeqDR7aoXODwg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Left — Raw image with bounding box detection, Right — Cropped Image with keypoint heatmaps overlayed</figcaption></figure><h1 id="93f6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">姿态优化</h1><p id="1e59" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">现在我们有了关键点，我们可以用它们来找到姿势。我们所需要的只是我们感兴趣的物体的模型。本文作者定义了一个可变形模型 S，它由一个平均形状 B_0 加上一些用主成分分析计算的变量 B_i 组成。形状被定义为 3xP 矩阵，其中 P 是关键点的数量。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/95dccd2791a5527e382756a30116e6b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*n8PNKFtEszTX39wSsqj-vQ.png"/></div></figure><p id="b49d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主要的优化问题是减少下面的残差。这里 W 是齐次坐标中的归一化 2D 关键点的集合。z 是表示关键点深度的对角矩阵。r 和 T 分别是旋转矩阵和平移向量。我们优化的未知数是 Z，R，T 和 c。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mv"><img src="../Images/3afea26bd0e9027d48f0d5365ecb8942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*KXwwu-DRdLlXgGkRpzV2iA.png"/></div></div></figure><p id="8d50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是我们希望最小化的实际损失。这里，D 是关键点的置信度——我们这样做是为了惩罚网络更确定的关键点上的错误。损失的第二项是正则化项，意在惩罚与感兴趣模型的平均形状的大偏差。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/36f1e9f38ffc1f92967e50a6bb9ccf3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*0cBz1oomD8VFZKCFCNCm5Q.png"/></div></figure><p id="b3fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就是这样！得到的 R 和 T 定义了对象的姿态。现在让我们看看 pytorch 的实现。</p><h1 id="a6a4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Pytorch 实现</h1><p id="4447" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">对于实现，我们将严格遵循宾夕法尼亚大学 CIS 580 中提供的代码。我通过合并一些文件和删除一些数据扩充步骤简化了代码。</p><p id="db13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们直接进入代码。首先，我们需要克隆一个存储库:</p><pre class="lg lh li lj gt mx my mz na aw nb bi"><span id="72e1" class="nc lw it my b gy nd ne l nf ng">git clone <a class="ae le" href="https://github.com/vaishak2future/posefromkeypoints.git" rel="noopener ugc nofollow" target="_blank">https://github.com/vaishak2future/posefromkeypoints.git</a></span></pre><p id="0019" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要解压缩 data.zip，以便顶层目录包含三个文件夹:data、output 和 utils。现在，让我们运行朱庇特笔记本。我们将检查的第一个代码块是 Trainer 类。这个类加载训练和测试数据集，并对其进行一些转换，使其成为我们想要的格式。数据被裁剪和填充，因此我们只查看感兴趣对象周围的边界框。然后，地面真实关键点的位置被表示为热图。然后，数据被转换成适当格式的张量并归一化。最后，培训师还会加载沙漏模型。一旦我们调用了 train 方法，我们就完成了关键点定位。</p><pre class="lg lh li lj gt mx my mz na aw nb bi"><span id="a275" class="nc lw it my b gy nd ne l nf ng"><strong class="my iu">class</strong> <strong class="my iu">Trainer</strong>(object):<br/><br/>    <strong class="my iu">def</strong> __init__(self):<br/>        self.device = torch.device('cuda' <strong class="my iu">if</strong> torch.cuda.is_available() <strong class="my iu">else</strong> 'cpu')<br/><br/>        train_transform_list = [CropAndPad(out_size=(256, 256)),LocsToHeatmaps(out_size=(64, 64)),ToTensor(),Normalize()]<br/>        test_transform_list = [CropAndPad(out_size=(256, 256)),LocsToHeatmaps(out_size=(64, 64)),ToTensor(),Normalize()]<br/>        self.train_ds = Dataset(is_train=<strong class="my iu">True</strong>, transform=transforms.Compose(train_transform_list))<br/>        self.test_ds = Dataset(is_train=<strong class="my iu">False</strong>, transform=transforms.Compose(test_transform_list))<br/><br/>        self.model = hg(num_stacks=1, num_blocks=1, num_classes=10).to(self.device)<br/>        <em class="nh"># define loss function and optimizer</em><br/>        self.heatmap_loss = torch.nn.MSELoss().to(self.device) <em class="nh"># for Global loss</em><br/>        self.optimizer = torch.optim.RMSprop(self.model.parameters(),<br/>                                             lr = 2.5e-4)<br/>        self.train_data_loader = DataLoader(self.train_ds, batch_size=8,<br/>                                            num_workers=8,<br/>                                            pin_memory=<strong class="my iu">True</strong>,<br/>                                            shuffle=<strong class="my iu">True</strong>)<br/>        self.test_data_loader = DataLoader(self.test_ds, batch_size=32,<br/>                                           num_workers=8,<br/>                                           pin_memory=<strong class="my iu">True</strong>,<br/>                                           shuffle=<strong class="my iu">True</strong>)<br/><br/>        self.summary_iters = []<br/>        self.losses = []<br/>        self.pcks = []<br/><br/>    <strong class="my iu">def</strong> train(self):<br/>        self.total_step_count = 0<br/>        start_time = time()<br/>        <strong class="my iu">for</strong> epoch <strong class="my iu">in</strong> range(1,400+1):<br/><br/>            print("Epoch <strong class="my iu">%d</strong>/<strong class="my iu">%d</strong>"%<br/>                    (epoch,400))<br/><br/>            <strong class="my iu">for</strong> step, batch <strong class="my iu">in</strong> enumerate(self.train_data_loader):<br/>                self.model.train()<br/>                batch = {k: v.to(self.device) <strong class="my iu">if</strong> isinstance(v, torch.Tensor) <strong class="my iu">else</strong> v <strong class="my iu">for</strong> k,v <strong class="my iu">in</strong> batch.items()}<br/>                self.optimizer.zero_grad()<br/>                pred_heatmap_list = self.model(batch['image'])<br/>                loss = self.heatmap_loss(pred_heatmap_list[-1], batch['keypoint_heatmaps'])<br/>                loss.backward()<br/>                self.optimizer.step()                                          <br/>                <br/>                self.total_step_count += 1<br/><br/><br/>        checkpoint = {'model': self.model.state_dict()}<br/>        torch.save(checkpoint, './output/model_checkpoint.pt')</span></pre><p id="faf9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们开始姿势优化之前，我们需要定义一个我们会经常用到的自定义函数。Rodrigues 函数将帮助我们将轴角表示向量转换为 3×3 旋转矩阵。我们这样定义它，以便我们可以使用 pytorch 的亲笔签名的功能。</p><pre class="lg lh li lj gt mx my mz na aw nb bi"><span id="d307" class="nc lw it my b gy nd ne l nf ng"><strong class="my iu">class</strong> <strong class="my iu">Rodrigues</strong>(torch.autograd.Function):<br/>    @staticmethod<br/>    <strong class="my iu">def</strong> forward(self, inp):<br/>        pose = inp.detach().cpu().numpy()<br/>        rotm, part_jacob = cv2.Rodrigues(pose)<br/>        self.jacob = torch.Tensor(np.transpose(part_jacob)).contiguous()<br/>        rotation_matrix = torch.Tensor(rotm.ravel())<br/>        <strong class="my iu">return</strong> rotation_matrix.view(3,3)<br/><br/>    @staticmethod<br/>    <strong class="my iu">def</strong> backward(self, grad_output):<br/>        grad_output = grad_output.view(1,-1)<br/>        grad_input = torch.mm(grad_output, self.jacob)<br/>        grad_input = grad_input.view(-1)<br/>        <strong class="my iu">return</strong> grad_input<br/><br/>rodrigues = Rodrigues.apply</span></pre><p id="49e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们编写姿态优化函数，其中我们使用相机模型将 2D 关键点转换成归一化的齐次坐标，然后将它们与旋转和平移后的地面真实 3D 关键点进行比较。当损失低于我们的阈值时，我们停止优化。</p><pre class="lg lh li lj gt mx my mz na aw nb bi"><span id="4fdd" class="nc lw it my b gy nd ne l nf ng"><strong class="my iu">def</strong> pose_optimization(img, vertices, faces, keypoints_2d, conf, keypoints_3d, K):<br/>    <em class="nh"># Send variables to GPU</em><br/>    device = keypoints_2d.device<br/>    keypoints_3d = keypoints_3d.to(device)<br/>    K = K.to(device)<br/>    r = torch.rand(3, requires_grad=<strong class="my iu">True</strong>, device=device) <em class="nh"># rotation in axis-angle representation</em><br/>    t = torch.rand(3 ,requires_grad=<strong class="my iu">True</strong>, device=device)<br/>    d = conf.sqrt()[:, <strong class="my iu">None</strong>]<br/>    <em class="nh"># 2D keypoints in normalized coordinates</em><br/>    norm_keypoints_2d = torch.matmul(K.inverse(), torch.cat((keypoints_2d, torch.ones(keypoints_2d.shape[0],1, device=device)), dim=-1).t()).t()[:,:-1]<br/>    <em class="nh"># set up optimizer</em><br/>    optimizer = torch.optim.Adam([r,t], lr=1e-2)<br/>    <em class="nh"># converge check</em><br/>    converged = <strong class="my iu">False</strong><br/>    rel_tol = 1e-7<br/>    loss_old = 100<br/>    <strong class="my iu">while</strong> <strong class="my iu">not</strong> converged:<br/>      optimizer.zero_grad()<br/>      <em class="nh"># convert axis-angle to rotation matrix</em><br/>      R = rodrigues(r)<br/>      <em class="nh"># 1) Compute projected keypoints based on current estimate of R and t</em><br/>      k3d = torch.matmul(R, keypoints_3d.transpose(1, 0)) + t[:, <strong class="my iu">None</strong>]<br/>      proj_keypoints = (k3d / k3d[2])[0:2,:].transpose(1,0) <br/>      <em class="nh"># 2) Compute error (based on distance between projected keypoints and detected keypoints)</em><br/>      err = torch.norm(((norm_keypoints_2d - proj_keypoints)*d)**2, 'fro')<br/>      <em class="nh"># 3) Update based on error</em><br/>      err.backward()<br/>      optimizer.step()<br/>      <em class="nh"># 4) Check for convergence</em><br/>      <strong class="my iu">if</strong> abs(err.detach() - loss_old)/loss_old &lt; rel_tol:<br/>        <strong class="my iu">break</strong><br/>      <strong class="my iu">else</strong>:<br/>        loss_old = err.detach()    <br/>        <br/><em class="nh">#       print(err.detach().cpu().numpy())</em><br/><br/>    R = rodrigues(r)<br/>    plt.figure()<br/>    plot_mesh(img, vertices, faces, R.detach().cpu().numpy(), t.detach().cpu().numpy()[:,<strong class="my iu">None</strong>], K.detach().cpu().numpy())<br/>    plt.show()<br/>    <strong class="my iu">return</strong> rodrigues(r)[0].detach(), t.detach()</span></pre><p id="5936" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当您运行上述函数时，您应该会得到如下令人惊叹的结果:</p><div class="lg lh li lj gt ab cb"><figure class="ni lk nj nk nl nm nn paragraph-image"><img src="../Images/87623c9cf7cf2f7f7160993dcecce8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*v6DG24NJMoL2DFfa52nEeg.png"/></figure><figure class="ni lk nj nk nl nm nn paragraph-image"><img src="../Images/ff67f0267411e75748abbb39b45ce464.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*joLagDhH57gcT8Osn5-zxw.png"/></figure><figure class="ni lk nj nk nl nm nn paragraph-image"><img src="../Images/7a340689c1f04a75ce1ca74f3a678a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*AxoQW3UeTvLNiEosTlngyA.png"/></figure></div><p id="7912" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">重要的实现细节到此为止！确保您运行它，并试验不同的超参数和转换，以了解它们如何影响结果。我希望这有所帮助。请给我任何意见或更正！</p><h1 id="4e5f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><p id="4628" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/pdf/1703.04670.pdf" rel="noopener ugc nofollow" target="_blank"> [1] G .帕夫拉科斯等 2017。来自语义关键点的 6 自由度物体姿态</a></p></div></div>    
</body>
</html>