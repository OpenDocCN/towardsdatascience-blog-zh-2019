<html>
<head>
<title>Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-reinforcement-learning-pong-from-pixels-keras-version-bf8a0860689f?source=collection_archive---------10-----------------------#2019-06-29">https://towardsdatascience.com/deep-reinforcement-learning-pong-from-pixels-keras-version-bf8a0860689f?source=collection_archive---------10-----------------------#2019-06-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e5d2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">像素乒乓:Keras 版本</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a020e28cd63e6199acffaa56440ba15b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vFT4k3JA3W9ySjaKPrpVYQ.png"/></div></div></figure><p id="b398" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是 Andrej Karpathy 关于强化学习的博客文章的后续。我希望这篇文章能简化 Karpathy 的文章，去掉数学运算(感谢 Keras)。这篇文章应该是独立的，即使你还没有看过其他的博客。</p><p id="7926" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里的博客是为了配合更深入的视频教程(YouTube 视频描述中的代码):</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lo lp l"/></div></figure><h2 id="11dc" class="lq lr iq bd ls lt lu dn lv lw lx dp ly la lz ma mb le mc md me li mf mg mh mi bi translated">什么是强化学习</h2><p id="15a7" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la ml lc ld le mm lg lh li mn lk ll lm ij bi translated">与机器学习/深度学习中的其他问题不同，强化学习的问题在于我们没有一个合适的“y”变量。然而，输入“X”也没有什么不同。例如，在这个特定的例子中，我们将使用 openAI 中的 pong 环境。输入将是游戏当前状态的图像。输出是要播放的移动。</p><h2 id="4f58" class="lq lr iq bd ls lt lu dn lv lw lx dp ly la lz ma mb le mc md me li mf mg mh mi bi translated">奖励函数</h2><p id="9a04" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la ml lc ld le mm lg lh li mn lk ll lm ij bi translated">RL 中的任务给定游戏/环境的当前状态(X ),以采取将最大化**未来* *预期折扣奖励的行动。一个替代方法是，我们将游戏玩到最后，用一个折扣变量 gamma(数字在 0 到 1 之间)将当前时间点的所有奖励相加，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/8762caac0903d669794644104d4377d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*yVWPY6_Yd_jv6IJh7le7HQ.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">R variable</figcaption></figure><p id="485c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为什么不简单的当前奖励 r_t？这是由于延迟奖励。当一个动作被采取时，它的影响不仅影响当前状态，还影响随后的状态，但是以衰减的速率。因此，当前行动负责当前奖励<strong class="kt ir">和</strong>未来奖励，但是越来越少的责任移动到未来。</p><h2 id="de19" class="lq lr iq bd ls lt lu dn lv lw lx dp ly la lz ma mb le mc md me li mf mg mh mi bi translated">模型</h2><p id="0846" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la ml lc ld le mm lg lh li mn lk ll lm ij bi translated">该模型用于生成动作。因此，在我们的例子中，我们使用图像作为输入，通过 sigmoid 输出来决定是向上还是向下。</p><p id="e92e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">关于上面提到的 R 变量，请注意我们的模型所产生的行动是如何导致奖励的。这很大程度上是一个盲人领盲人的例子。但是随着更多的迭代完成，我们收敛到更好的输出。</p><p id="a865" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将使用的模型不同于 AK 博客中使用的模型，因为我们使用了卷积神经网络(CNN ),如下所述。使用 CNN 的优势在于我们需要处理的参数数量明显减少。具有 100 个神经元的 1 个隐藏层的密集网络将产生大约 640000 个参数(因为我们有 6400 = 80×80 像素)。然而在下面显示的模型中我们只有 3100 个参数。</p><p id="3d17" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">另请注意，最后一层有一个 sigmoid 输出。这是为了让模型预测桨向上或向下移动的概率。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="439c" class="lq lr iq mu b gy my mz l na nb">model = Sequential()<br/>model.add(Conv2D(4, kernel_size=(3,3), padding='same', activation='relu', input_shape = (80,80,1)))<br/>model.add(MaxPool2D(pool_size=(2, 2)))<br/>model.add(Conv2D(8, kernel_size=(3,3), padding='same', activation='relu'))<br/>model.add(MaxPool2D(pool_size=(2, 2)))<br/>model.add(Conv2D(16, kernel_size=(3,3), padding='same', activation='relu'))<br/>model.add(MaxPool2D(pool_size=(2, 2)))<br/>model.add(Flatten())<br/>model.add(Dense(2, activation='softmax'))</span></pre><h2 id="bfa2" class="lq lr iq bd ls lt lu dn lv lw lx dp ly la lz ma mb le mc md me li mf mg mh mi bi translated">预处理</h2><p id="f6a0" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la ml lc ld le mm lg lh li mn lk ll lm ij bi translated">我们裁剪图像的顶部和底部，并在水平和垂直方向上每隔一个像素进行二次采样。我们将球拍和球的值设置为 1，而背景设置为 0。然而，输入到 DL 算法中的是两个连续帧的差。这导致输入图像的大小为 80x80。</p><h2 id="cd59" class="lq lr iq bd ls lt lu dn lv lw lx dp ly la lz ma mb le mc md me li mf mg mh mi bi translated">损失函数</h2><p id="b5fb" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la ml lc ld le mm lg lh li mn lk ll lm ij bi translated">拼图的最后一块是损失函数。我们有自己的输入，也就是上面提到的 X 变量，但是，目标 y 变量是在那个时间步采取的动作。即向上将是 1，向下将是 0。</p><p id="2489" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是等等，y 变量不是模型规定的吗？是的，你完全正确。因此，我们不能简单地使用通常的交叉熵损失，因为概率 p(X)和 y 是由同一模型生成的。相反，我们所做的是用在那个时间点的预期未来回报来衡量。</p><p id="6a77" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，在每集结束时，我们运行以下代码进行训练:</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="50df" class="lq lr iq mu b gy my mz l na nb">model.fit(x, y, sample_weight=R, epochs=1)</span></pre><p id="4b11" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，实际损失函数保持不变，</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="2c60" class="lq lr iq mu b gy my mz l na nb">model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy')</span></pre><p id="87f4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">关键是，如果这一步棋走得好，我们使用上面的<code class="fe nc nd ne mu b">sample_weight</code>功能来衡量他们。</p><h2 id="c981" class="lq lr iq bd ls lt lu dn lv lw lx dp ly la lz ma mb le mc md me li mf mg mh mi bi translated">摘要</h2><p id="1528" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la ml lc ld le mm lg lh li mn lk ll lm ij bi translated">总而言之，当你不关心实际的梯度计算时，政策梯度更容易理解。所有当前的深度学习框架都考虑到了你可能需要的任何衍生品。</p><p id="83e9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">策略梯度是更基本的强化学习问题之一。如果你想了解更多关于强化学习的知识，请订阅我的 YouTube 频道。<a class="ae ln" href="https://www.youtube.com/playlist?list=PLIx9QCwIhuRQnQVejhNrN62s46KDyFRnX" rel="noopener ugc nofollow" target="_blank">该播放列表</a>包含更高级 RL 算法的教程，如 Q-learning。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><p id="774f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">看这里是我关于机器学习和深度学习的<a class="ae ln" href="https://www.udemy.com/course/machine-learning-and-data-science-2021/?referralCode=E79228C7436D74315787" rel="noopener ugc nofollow" target="_blank">课程</a>(使用代码 DEEPSCHOOL-MARCH 到 85 折)。</p></div></div>    
</body>
</html>