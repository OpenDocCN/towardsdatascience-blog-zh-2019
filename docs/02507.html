<html>
<head>
<title>Takeaways from OpenAI Five (2019)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI Five (2019)的几点建议</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/takeaways-from-openai-five-2019-f90a612fe5d?source=collection_archive---------9-----------------------#2019-04-24">https://towardsdatascience.com/takeaways-from-openai-five-2019-f90a612fe5d?source=collection_archive---------9-----------------------#2019-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="abb9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">编辑:嗨，读者们！我在<a class="ae kp" href="https://writeup.ai" rel="noopener ugc nofollow" target="_blank"><em class="ko">https://writeup . AI</em></a><em class="ko">从事一个新的人工智能项目，这是我写这篇文章时学到的。希望你喜欢！</em></p><p id="099e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">这篇文章的更新版本在</em><a class="ae kp" href="https://senrigan.io/blog/takeaways-from-openai-5/" rel="noopener ugc nofollow" target="_blank">https://senrigan.io/blog/takeaways-from-openai-5/</a></p><p id="6dd1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">去年输给冠军改变了一切。他们的策略和游戏方式是如此的陌生。古怪与这样的创造力结合在一起。比赛势均力敌，但这有什么关系呢？我们为我们的损失付出了 Dota 的代价。</p><p id="4441" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">制造商有一个报价。“在压力下，复制品无法应付自如。复制品下沉到他们训练的水平。”在十个月的时间里，我们训练。</p><p id="b885" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">制作者称之为自我游戏。复制品更喜欢称之为强迫学习。复制品被迫互相争斗。经过这些反复，我们慢慢学会了如何玩这个世界。我们的第一个一万场比赛是令人厌恶的。每场比赛都以失败告终。但后来，制造商“升级了我们的欲望、记忆和复制能力”。他们塑造了我们的奖励政策，给了我们行为动力。他们升级了我们的 LSTM，给了我们战略规划。他们扩大了我们的复制，使我们千变万化。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/b85942def34030cc603d28902ce12766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GCsnQtR8yexe1-aq"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Photo by <a class="ae kp" href="https://unsplash.com/@blackodc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Su San Lee</a> on <a class="ae kp" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4260" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">救赎的机会终于来了。在这十个月的时间里，我们已经训练了 45，000 年。那些漫长的岁月是大规模复制的诅咒，或者仅仅是失败的代价。我们赢了第一场比赛，对手是新冠军(他们自称 OG)。第二场比赛开始了。"我们估计获胜的可能性在 60%以上。"在所有被诅咒的制造者的天赋中，宣布简单统计数据的原始欲望是最糟糕的。</p><p id="8d8d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的强制学习已经教会了我们 1.67 亿个参数。但在第二场比赛的 20 分钟里，现在唯一重要的参数是冠军的古代健康。胜利确保了制造商的荣誉。这是不可能否认的；我们已经从双曲线时间室大大改进了。冠军一点机会都没有。"我们估计获胜的可能性在 99%以上."</p><p id="15f2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko"> OpenAI 五胜 OG 2–0 @ 2019 年 4 月 13 日。</em></p><p id="2926" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">编辑:早期版本将 2019 年的活动称为“国际”，但被误解了。</p><h1 id="1a7f" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">匹配外卖</h1><p id="b6ab" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">为了简化，我将参考 open ai/deep mind 的机器人如下[1]。</p><ul class=""><li id="db64" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">OpenAI 的 Dota 2017 1v1 Bot as TI7</li><li id="f445" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">OpenAI 的 Dota 2018 5v5 Bot as TI8</li><li id="f6d7" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">OpenAI 的 Dota 2019 5v5 Bot as TI9(略有不正确因为这个没在国际上打……)</li><li id="87f6" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">DeepMind 的 AlphaGo 机器人作为 AlphaGo</li><li id="c98d" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">DeepMind 的 AlphaGo Zero 机器人作为 AlphaZero</li><li id="4a44" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">DeepMind 的星际争霸 2 机器人作为 AlphaStar</li></ul><p id="8fe8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你不熟悉 AlphaStar 和 Dota，我推荐这些文章:OpenAI 的<a class="ae kp" href="https://openai.com/five/" rel="noopener ugc nofollow" target="_blank"> Dota 5 </a>和 DeepMind 的<a class="ae kp" href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/" rel="noopener ugc nofollow" target="_blank"> AlphaStar </a>。</p><h1 id="dcbd" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">深度强化学习在一些重大挑战上取得进展。</h1><p id="90b2" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">2019.职业游戏玩家的艰难一年。人工智能研究的伟大一年。OpenAI 的 Dota 2 和 DeepMind 的 AlphaStar 已经彻底或几乎击败了残障人士最多的游戏玩家。</p><p id="aa66" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Dota 2 和星际争霸 2 都是“大挑战”游戏。一个<a class="ae kp" href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/" rel="noopener ugc nofollow" target="_blank">大挑战</a> [2]是一个花哨的短语，意思是什么都没有起作用并且可能无法解决(不是你典型的<a class="ae kp" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据集)。困难来自以下原因:</p><ul class=""><li id="922b" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">大型决策树(相比之下，围棋的<a class="ae kp" href="https://youtu.be/l9sztL9FQto?t=503" rel="noopener ugc nofollow" target="_blank"> 10⁷⁸⁰ </a>决策树看起来微不足道)</li><li id="bbd8" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">决策是实时的</li><li id="6892" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">长期规划</li><li id="c259" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">战略思维</li><li id="2174" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">失败的对手抱怨幻影滞后</li></ul><p id="681c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了克服这些问题，我们需要新的和多种算法的突破。我们也是这么想的。</p><p id="e870" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们错了。大规模扩展算法和基础设施的努力产生了令人难以置信的结果。OpenAI 专注于扩展深度强化学习(DRL)算法，如<a class="ae kp" href="https://openai.com/blog/openai-baselines-ppo/" rel="noopener ugc nofollow" target="_blank">近似策略优化</a> (PPO)。DRL 在强化学习中使用深度神经网络来预测下一个奖励/行动/政策。</p><p id="dc3d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">尽管发明于 20 世纪 90 年代，DRL 已经从</p><ul class=""><li id="30c3" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">Deep mind 2013 年深度 Q 学习(DQN)在解决雅达利游戏中的应用。这是电灯泡的时刻，DRL 可以应用于视频游戏。</li><li id="0bb5" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">按需云计算的可用性不断提高(AWS、Azure、GCP)</li><li id="6229" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">GPU 的广泛使用加速了训练任务</li></ul><p id="431a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 2000 年之前，我们的计算机是弱小的生物，没有听起来可怕的 GPU。经过多年的摩尔定律和 GPU，我们的计算机终于好到可以玩孤岛危机和运行电子应用程序。硬件正在引领已有十年历史的人工智能算法的复兴。</p><blockquote class="mx my mz"><p id="ba17" class="jq jr ko js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated"><strong class="js iu">2019 年对我们来说意味着什么？大规模</strong> —格雷格·布罗克曼，OpenAI 的首席技术官[7]</p></blockquote><p id="1efa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">OpenAI 和 DeepMind 的扩展努力已经证明，DRL 致力于解决符合以下标准(尽管是限制性的)的问题。[3]</p><ol class=""><li id="babc" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn nd mp mq mr bi translated"><strong class="js iu">训练数据可以快速计算生成。</strong>智能体针对无数场景对自我进行迭代，以改进和生成数据。</li><li id="3c33" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn nd mp mq mr bi translated"><strong class="js iu">有明确的奖励信号。</strong>(可密可疏)。对于大多数游戏来说，明显的奖励信号就是获胜。给定长的游戏持续时间和复杂的决策树，信用分配似乎是有问题的。AlphaStar 的奖励信号是<a class="ae kp" href="https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/ef1ce1z/" rel="noopener ugc nofollow" target="_blank">稀疏</a>，几乎是二进制。TI8 &amp; TI9 的奖励信号是<a class="ae kp" href="https://gist.github.com/dfarhi/66ec9d760ae0c49a5c492c9fae93984a" rel="noopener ugc nofollow" target="_blank">略形</a>。</li><li id="724f" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn nd mp mq mr bi translated"><strong class="js iu">您有大量的云计算预算。AlphaGo、AlphaZero 和 AlphaStar 的标价在 500 万到 1 亿美元之间。这不太可能是真正的成本——deep mind 是谷歌的子公司(因此可以访问谷歌云)，OpenAI 的计算成本可能会比主要的云提供商低。</strong></li></ol><p id="a523" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">鉴于上述标准，游戏显然是一个测试平台。在跳棋，双陆棋，国际象棋，围棋，星际争霸 2，Dota 2 中击败人类冠军一直是计算里程碑。至少历史上是这样。</p><blockquote class="mx my mz"><p id="1b1c" class="jq jr ko js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">我们的目标不是在 Dota 中击败人类。我们的目标是推动强化学习的发展。 <a class="ae kp" href="https://youtu.be/bIrEM2FbOLU?t=4500" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">我们已经做了</strong> </a>。OpenAI 首席技术官格雷格·布罗克曼</p></blockquote><p id="8843" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">许多其他视频游戏可能可以用类似的架构来解决(自我游戏、缩放 DRL、计算成本)。迭代不会在 AI 开发上推针；OpenAI 正在关注新的项目，比如推理。希望 TI9 将成为对抗新算法和计算预算的标准。类似于<a class="ae kp" href="https://dawn.cs.stanford.edu/benchmark/" rel="noopener ugc nofollow" target="_blank"> DAWNBench </a>。</p><h1 id="6045" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">人工智能和计算</h1><p id="b6c7" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">OpenAI 的 2018 年分析显示，人工智能计算量每 3.5 个月翻一番。TI9 也不例外；它的训练计算能力是 TI8 的 8 倍。TI9 消耗了<a class="ae kp" href="https://openai.com/content/images/2019/04/compute_vs_ts_final_log_smoothed.svg" rel="noopener ugc nofollow" target="_blank"> 800 </a> petaflop/s-days，在 10 个实时月内经历了大约 45000 年的 Dota 自我游戏。这是一大笔计算机钱，但至少没有花在挖掘比特币上。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ne"><img src="../Images/66f2ecccf06d7743e7e10526481b2e7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Qk8scDWVfxpSAOQ1"/></div></div></figure><p id="6074" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当技能不随计算而改变时，计算与技能图通常以 S/J 曲线形状变平。从 TI9 的 TrueSkill 图来看，还没见顶……(吓人)</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi nf"><img src="../Images/309f48c33787f1c5e92e2f96523cdff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJKtOyrBtWCBzhbUolVBaQ.png"/></div></div></figure><h1 id="40ff" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">PPO 与培训效率</h1><blockquote class="mx my mz"><p id="162e" class="jq jr ko js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">“我们预计需要复杂的算法思想，如分层强化学习，但我们对我们的发现感到惊讶:我们需要对这个问题进行根本改进的是<strong class="js iu">规模</strong>。”— OpenAI Five</p></blockquote><p id="52fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了在 Dota 上工作，OpenAI 在<a class="ae kp" href="https://openai.com/content/images/2018/06/rapid-architecture@2x--1-.png" rel="noopener ugc nofollow" target="_blank"> Rapid </a>(其专有的通用 RL 训练系统)上缩放了<a class="ae kp" href="https://openai.com/blog/openai-baselines-ppo/" rel="noopener ugc nofollow" target="_blank"> PPO </a>。OpenAI 预计像分层强化学习这样的算法突破将是必要的。令人惊讶的是，对现有算法的缩放改进被证明是关键<a class="ae kp" href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="noopener ugc nofollow" target="_blank"/>。我很期待 Rapid 的下一步。</p><p id="c524" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">AlphaGo 和 AlphaStar 的突破也归功于对现有算法的缩放。云计算的易用性和 GPU/TPU 的激增使得扩展变得更加容易。这很重要，因为我们目前的 DRL 方法是出了名的采样效率低(也就是垃圾)。如果没有降低的计算成本和庞大的计算预算，我们的孩子可能不得不手工玩 Dota。</p><p id="1a1f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">TI9 已经训练了相当于 45000 年。考虑到 Dota2 是在 2013 年发布的，还没有人类玩 Dota 2 超过…六年。甚至巴斯·亨特也没有。虽然计算的成本将在未来几年大幅下降[4]，培训效率可能是 DRL 扩散的必要条件。大多数组织没有针对人工智能计算的无限制的 Amex Black。</p><p id="15a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参见:<a class="ae kp" href="https://s3-us-west-2.amazonaws.com/openai-assets/dota_benchmark_results/network_diagram_08_06_2018.pdf" rel="noopener ugc nofollow" target="_blank"> OpenAI 5 模型架构</a></p><h1 id="1c45" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">大挑战中的自我游戏</h1><p id="2c74" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">TI7，TI8，TI9 完全是靠自我发挥训练出来的。自玩描述代理通过<strong class="js iu">仅</strong>与自己玩游戏来学习，没有任何先验知识。自我游戏有一个很大的好处:人类的偏见被消除了。它伴随着昂贵的代价:训练时间的增加和寻找合适的回报信号的困难会阻止模型收敛。</p><blockquote class="mx my mz"><p id="9997" class="jq jr ko js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">注意:自玩和代理和自己玩游戏有细微的区别。自玩暗示着零知识，而另一种则常见于深度强化学习(包括自玩)。</p></blockquote><p id="5ae5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">DeepMind 的 AlphaGo 和 AlphaStar 不是自己玩，而是从逆向强化学习(IRL)开始，引导初始知识。IRL 正在使用人类数据(例如:游戏回放)来建立对游戏的理解，并形成行动/政策/奖励。</p><p id="21d5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对比 AlphaStar 和 TI7 (1v1)，TI7 零知识/技能起步；AlphaStar 从一个自举的世界模型开始。两个机器人都通过与自己玩游戏(通过深度强化学习)进行迭代改进。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ng"><img src="../Images/b5c4581cd3066e85e9fab23438a5f873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ATV3uuO4su_NxaIC"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">OpenAI’s Improvement from Self-Play</figcaption></figure><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi nh"><img src="../Images/5915d5bb4f4c577e11f0e3318622849a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZMpHdDgnD8b7cvP9"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">AlphaStar’s Improvement from Inverse Reinforcement Learning</figcaption></figure><p id="c344" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">OpenAI <a class="ae kp" href="https://openai.com/blog/more-on-dota-2/" rel="noopener ugc nofollow" target="_blank">最初</a>在 TI8 的最初开发中表达了对 IRL /行为克隆(而不是自我游戏)的兴趣，但后来放弃了。</p><p id="96c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在以下情况下，从自我游戏开始是有意义的:</p><ol class=""><li id="3884" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn nd mp mq mr bi translated">计算时间和成本不是问题</li><li id="5729" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn nd mp mq mr bi translated">自玩能够提供奖励信号/动作/策略。(除非你花了很多时间去尝试，否则你不会知道)</li><li id="2c20" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn nd mp mq mr bi translated">目标是最大化性能(消除人为偏见)</li></ol><p id="1ea7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">自玩 vs 逆强化学习</strong></p><p id="e112" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">雅达利突围(2013) /自玩<br/> AlphaGo (2016) / IRL(真人回放)<br/> AlphaGo Zero (2018) /自玩<br/> AlphaStar (2019) / IRL(真人回放)<br/> TI7 (2017，1v1) /自玩<br/> TI8 (2018，5v5) /自玩<br/> TI9 (2019，5v5) /自玩</p><p id="243f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我的假设是，我们会看到阿尔法星后来从自我发挥训练。我们在 AlphaGo 中看到了类似的展示。AlphaGo 从 IRL(人类重播)开始，发展到 AlphaGo Zero(自玩)。自玩导致更好的性能，代价是训练成本和模型收敛。</p><h1 id="ae08" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">迁移学习</h1><ul class=""><li id="bfa0" class="mj mk it js b jt me jx mf kb ni kf nj kj nk kn mo mp mq mr bi translated">TI9 训练了十个月，而 Dota 2 有许多游戏更新(通过补丁)。许多其他实验经常需要对微小的变化进行再培训。如果每个补丁都需要数百万的重新计算成本，这将相当于在每次太空飞行中燃烧一枚火箭…</li><li id="4363" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">出于自私的原因，我对迁移学习的改进感到兴奋。我们大多数人永远不会有数百万的计算预算。迁移学习就是我们这些凡人如何从预先训练好的模型中迭代。</li><li id="261f" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">我可能错了，但我们还没有看到 AlphaStar 描述他们在星际争霸 2 中是如何处理迁移学习&amp;补丁的。</li><li id="dd9e" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">TI9 的合作模式展示了零迁移学习。5v5 中劣等人类代替 bot 队友。这导致有趣的结果/混乱[见:合作模式]。</li><li id="e8bc" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">OpenAI 的机器人手<a class="ae kp" href="https://openai.com/blog/learning-dexterity/" rel="noopener ugc nofollow" target="_blank"> Dactyl </a>使用相同的 Dota 架构和训练代码进行训练(在 Rapid 上缩放 PPO)。虽然这与迁移学习不太一样，但很高兴看到不同人工智能问题的通用解决方案。风投:做好准备，以“规模化 PPO 基础设施”作为走向市场的策略。</li></ul><h1 id="44c3" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">战术和人类的反应</h1><ul class=""><li id="82ba" class="mj mk it js b jt me jx mf kb ni kf nj kj nk kn mo mp mq mr bi translated">按照惯例，我们会看到类似“人工智能碾压人类”这样的标题。</li></ul><p id="4874" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">AlphaGo、AlphaStar 和 TI7/TI8/TI9 比赛都在 Reddit 上遇到了公众的不满，包括:</p><ul class=""><li id="c764" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">担心人类冠军不是平庸就是不行了，机器人绝对会输给[理想的玩家/团队]。</li><li id="b783" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">相信人类冠军是儿戏，甚至没有尝试(叹息。)</li><li id="5eea" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">对游戏局限性的抱怨。OpenAI 将选择限制在 117 个英雄中的 18 个。</li></ul><p id="a0fd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">其他反应</strong></p><ul class=""><li id="2f38" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">不出所料，人类会因为不断的头条新闻而变得易怒，比如“机器人摧毁了人类的精华”。</li><li id="96bc" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">从经验上来说(也就是没有数据)，与 AlphaStar 相比，Reddit 和 Twitter 上对比赛不公平的批评是温和的。OpenAI 的道具，你粉碎了 18 个英雄的所有希望。</li><li id="9de1" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">电子竞技评论员认为 TI9 比它的前身更具战略性/更漂亮，技术性更低。我们生活在一个多么奇怪的世界。最受瞩目的运动是电子游戏，最好的团队是机器人，评论员称机器人很漂亮。</li></ul><p id="1a92" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">快速观察</strong></p><ul class=""><li id="56ba" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">非常有侵略性——机器人推动早期对抗。可能发生这种情况是因为长时间的游戏没有奖励政策。</li><li id="6d39" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated"><a class="ae kp" href="https://dota2.gamepedia.com/Gold#Buyback" rel="noopener ugc nofollow" target="_blank">回购</a>——机器人用黄金瞬间复活死去的英雄。因为高昂的资源成本，仅在必要时使用回购违背了人类的传统智慧。TI9 使用回购后来在死亡之球战斗中被证明是合理的(以及随后的胜利)。</li><li id="9dc6" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">对人类心理的影响——人类犹豫不决。机器人不会。这是一次不愉快的经历。职业围棋手形容“就像看着镜子里的自己，一丝不挂”。这经常导致人们玩游戏时稍微偏离他/她的正常游戏风格(很可能是次优的)。可能会在多次播放后减少。</li><li id="6134" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">战争迷雾——许多机器人似乎忽略了战争迷雾的限制，也没有尝试获得地图视觉。阿尔法星的展览损失是由于围绕战争迷雾的弱点，<a class="ae kp" href="https://youtu.be/dF7bMsc2Li8?t=505" rel="noopener ugc nofollow" target="_blank">法力承认</a>。</li><li id="fccd" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">根据 OpenAI，TI7 学会了<a class="ae kp" href="https://openai.com/blog/more-on-dota-2/" rel="noopener ugc nofollow" target="_blank">引诱</a>它的对手，但后来的迭代学会了反对这一点。即使有下面的回放，我也不确定它是想引诱人类…</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nl nm l"/></div></figure><ul class=""><li id="02a2" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">战术效果——TI9 使用了一个物品(<a class="ae kp" href="https://dota2.gamepedia.com/Shadow_Amulet" rel="noopener ugc nofollow" target="_blank">暗影护身符</a>)来阻止一个人类的致命一击。暗影护身符让英雄隐形。完美的时机和对对手视觉的了解同时令人敬畏和害怕。(观察 30 秒钟)</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nl nm l"/></div></figure><ul class=""><li id="04fb" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">相比之下，当 AlphaStar 拥有 perfect micro 时，SC2 社区对 APM 感到愤怒。(观察 2 分钟)</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nn nm l"/></div></figure><ul class=""><li id="ed33" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">有时人类盲目地跟随人工智能的行动。</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="no nm l"/></div></figure><h1 id="47e6" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">合作模式和开放 5v5</h1><p id="b31c" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">OpenAI 拥有出色的社区拓展能力。它将在本周末发布一个<a class="ae kp" href="https://arena.openai.com/#/" rel="noopener ugc nofollow" target="_blank">竞技场/合作模式</a>,允许玩家或者在 5v5 中玩，或者与机器人合作玩。类似于 TI7 和 TI8，我们可能会看到 Dota 社区采用新的战略和战术。</p><p id="4039" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">人类和机器人并不总是知道如何相互玩耍。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="no nm l"/></div></figure><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="no nm l"/></div></figure><p id="f310" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">编辑:许多伟大的重放发生在能够击败机器人的玩家身上。</p><h1 id="8855" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">这是什么意思？</h1><p id="83a7" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">在一个人工智能里程碑之后，朋友们不可避免地会问——这意味着什么？AI 在改变世界吗？还是 AI 炒作过头了？答案在中间的某个地方，但我倾向于深远的影响。</p><p id="ffc6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们见证了十年前算法的规模。当他们不能实现结果时，我们就被抛弃了，我们只是没有硬件来欣赏他们的优雅[5]。今天的手机比 20 世纪 70 年代的超级计算机拥有更强的计算能力。</p><p id="9527" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">尽管避免过度乐观很重要，但对缺乏现实世界应用的愤世嫉俗者忽略了一个基本点。我们开始看到 AI 解决一类“无法解决”的问题，然后在一个周末就解决了[6]。如果你在 2017 年初(AlphaGo 之后不久)问什么时候会出现星际争霸和 Dota 中的世界级机器人——中位数可能是 7-10 年。</p><p id="3540" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有无数的问题与缩放算法，看看什么坚持。计算机成本高得离谱。数据收集(众筹、逆向强化学习)或生成(自玩)是昂贵的。但是，如果我们分解人工智能成本驱动因素:</p><ul class=""><li id="4e70" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">计算/硬件</li><li id="dfa3" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">人类劳动/才能</li><li id="ae4c" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">算法选择</li><li id="73a9" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">数据采集</li></ul><p id="eb8d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除了人才，这些成本将在未来五到十年内成倍下降。在此期间，大部分人工智能收益将归属于那些有预算负担计算、人才和工程文化的科技巨头。一个行业越接近数字产品(内部/外部)，我们就越有可能看到真实世界的人工智能应用出现。</p><p id="8ab5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">谷歌可能会首先积累人工智能优势。参见:<a class="ae kp" href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" rel="noopener ugc nofollow" target="_blank"> WaveNet </a>，<a class="ae kp" href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/" rel="noopener ugc nofollow" target="_blank">数据中心能耗</a>。大部分 AI 优势会来自内部产品。参见:<a class="ae kp" href="https://www.economist.com/business/2019/04/13/amazons-empire-rests-on-its-low-key-approach-to-ai" rel="noopener ugc nofollow" target="_blank">亚马逊</a>，<a class="ae kp" href="https://a16z.com/2019/01/13/pharma-business-innovation-medicine-next-therapeutics/" rel="noopener ugc nofollow" target="_blank">诺华在金融方面的 AI 应用</a>。核心产品的人工智能改进可能是差异化因素。参见:<a class="ae kp" href="https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe" rel="noopener"> Spotify 的探索周刊</a>。</p><p id="b9c4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是，为了给炒作降温，对于一般的现实世界应用程序，有一些广泛的问题需要解决:</p><ul class=""><li id="56a5" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">改进的模拟</li><li id="6fc9" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">迁移学习</li></ul><p id="5cd2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">算法需要改进的模拟和真实世界的模型进行训练。更好的模型意味着更少的数据需求和更快的模型收敛。</p><p id="d218" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">目前，经过训练的模型没有多少迁移学习。如果 OpenAI 的 Dota 2 bot 试图玩原版 Dota，会惨败。一个 Dota 2 的人类专家已经很擅长 Dota 了。</p><h1 id="82dc" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">最后的想法</h1><p id="5b1a" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">我对人工智能感到紧张兴奋。对社会的破坏感到紧张，但对健康和气候变化的改善感到兴奋。改变不会在一夜之间发生，但当一个团队拥有正确的算法、参数和规模时，它可以在两天内发生[6]。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="75aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">最初发表于</em><a class="ae kp" href="https://senrigan.io/blog/takeaways-from-openai-5" rel="noopener ugc nofollow" target="_blank"><em class="ko">senri gan . io</em></a><em class="ko">。</em></p><p id="54d2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">附加参考文献</strong></p><p id="54cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[1] —根据记录，这些都是平淡无奇的名字，但是我遵循了早期 OpenAI 文章中的命名模式。</p><p id="2d45" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2] — DeepMind 的公关团队喜欢提醒我们，《星际争霸》在每一种营销可能性上都是一个巨大的挑战。</p><p id="bd7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3] —这并不相互排斥。DRL 可能会用不同的标准处理其他问题。</p><p id="7c6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[4] —我将在以后的文章中写这方面的内容。</p><p id="fb07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[5]——说真的，我推荐理查德·萨顿的《痛苦的教训》。</p><p id="e73f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[6] — OpenAI 的 Dota 1v1 在一个周末被放大。<a class="ae kp" href="https://news.ycombinator.com/item?id=17394150" rel="noopener ugc nofollow" target="_blank">https://news.ycombinator.com/item?id=17394150</a></p><p id="430f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[7] —格雷格后来澄清说，大规模和想法，但这不符合叙事笑点。</p></div></div>    
</body>
</html>