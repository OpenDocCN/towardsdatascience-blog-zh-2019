<html>
<head>
<title>Introduction to Reinforcement Learning — Chapter 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习简介—第 1 章</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-reinforcement-learning-chapter-1-fc8a196a09e8?source=collection_archive---------16-----------------------#2019-01-14">https://towardsdatascience.com/introduction-to-reinforcement-learning-chapter-1-fc8a196a09e8?source=collection_archive---------16-----------------------#2019-01-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="25db" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">RLBook 的章节摘要。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/726080a0ab110ec82018f775d12eeb70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5DX8TO8h9tvTjMfzSqWsQg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Fig 1. Toons talking about Reinforcement Learning</strong></figcaption></figure><p id="f328" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">这是从一本最流行的强化学习书中摘出的一章，作者是</em> <strong class="ky ir"> <em class="ls">理查德·萨顿</em> </strong> <em class="ls">和</em> <strong class="ky ir"> <em class="ls">安德鲁·g·巴尔托</em> </strong> <em class="ls"> ( </em>第二版)<em class="ls">。书可以在这里找到:</em> <a class="ae lt" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank"> <em class="ls">链接</em> </a> <em class="ls">。</em></p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="9e53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">强化学习是学习做什么——如何将情况映射到行动——以便最大化数字奖励信号。学习代理可以采取影响环境状态的行动，并且具有与环境状态相关的目标。强化学习中出现的挑战之一，而不是其他类型的学习，是探索和利用之间的权衡。在所有形式的机器学习中，强化学习是最接近人类和其他动物所做的学习。</p><h2 id="b48c" class="mb mc iq bd md me mf dn mg mh mi dp mj lf mk ml mm lj mn mo mp ln mq mr ms mt bi translated">强化学习的要素</h2><p id="3e38" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">除了<strong class="ky ir"> <em class="ls">代理</em> </strong>和<strong class="ky ir"> <em class="ls">环境</em> </strong>之外，人们可以识别 RL 的四个主要子元素</p><ol class=""><li id="92ed" class="mz na iq ky b kz la lc ld lf nb lj nc ln nd lr ne nf ng nh bi translated"><strong class="ky ir"> <em class="ls">策略— </em> </strong> <em class="ls">是从感知的环境状态到处于这些状态时要采取的行动的映射。策略是强化学习代理的核心，因为它本身就足以决定行为。它可能是随机的，规定了每个动作的概率。</em></li><li id="adcf" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated"><strong class="ky ir"> <em class="ls">奖励— </em> </strong> <em class="ls">在每个时间步上，环境向强化学习代理发送一个称为奖励的单一数字。代理人的唯一目标是最大化其长期获得的总回报。因此，回报信号定义了对代理人来说什么是好信号，什么是坏信号。它可能是状态和行为的随机函数。</em></li><li id="9cbf" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated"><strong class="ky ir"> <em class="ls">价值函数— </em> </strong> <em class="ls">粗略地说，一个状态的价值就是一个主体从那个状态开始，在未来可以期望积累的报酬总额。奖励决定了环境状态的直接的、内在的可取性，而值在考虑了可能发生的状态和这些状态中可用的奖励之后，指示了状态的长期可取性。例如，一个州可能总是产生较低的即时奖励，但仍然有很高的价值，因为它经常被产生高奖励的其他州跟随，或者相反。</em></li><li id="bb10" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated"><strong class="ky ir"> <em class="ls">环境模型— </em> </strong> <em class="ls">这模仿了环境的行为，允许对环境将如何表现做出推断。例如，给定一个状态和一个动作，模型可以预测下一个状态和下一个奖励。使用模型解决强化学习问题的方法称为基于模型的方法，与更简单的无模型方法，即试错学习器相对。</em></li></ol><blockquote class="nn no np"><p id="d122" class="kw kx ls ky b kz la jr lb lc ld ju le nq lg lh li nr lk ll lm ns lo lp lq lr ij bi translated">从某种意义上说，回报是第一位的，而作为回报预测的价值是第二位的。没有回报就没有价值，评估价值的唯一目的是为了获得更多的回报。然而，在做决策和评估决策时，我们最关心的是价值观。</p></blockquote><h2 id="9fb8" class="mb mc iq bd md me mf dn mg mh mi dp mj lf mk ml mm lj mn mo mp ln mq mr ms mt bi translated">一个例子:井字游戏</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/89f458075a75492b1a952a3740704b6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4nU3v7u3YG8lKH_IWOQ2Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Fig 2. Sequence of Tic-Tac-Toe Moves</strong></figcaption></figure><p id="4842" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解决井字游戏的强化学习方法；</p><ol class=""><li id="baf3" class="mz na iq ky b kz la lc ld lf nb lj nc ln nd lr ne nf ng nh bi translated">建立一个数字表，每个数字代表一种可能的游戏状态。</li><li id="4679" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated">每一个数字都是我们对该州获胜概率的最新估计。</li><li id="6ee2" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated">这个估计值就是状态的<strong class="ky ir"> <em class="ls">值</em> </strong>，整个表就是学习值函数。</li><li id="bf02" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated">假设我们总是玩 x，那么对于一行(列和对角线)中有 3 个 x 的所有状态，获胜的概率是 1.0</li><li id="475d" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated">对于一行(列和对角线)中有 3 个 0 的所有州，获胜的概率是 0.0</li><li id="64e0" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated">我们将所有其他状态的初始值设置为 0.5 <em class="ls">(表示我们有 50%的胜算。)</em></li></ol><p id="1717" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们和对手打很多场比赛。要选择我们的行动:</p><ol class=""><li id="e70d" class="mz na iq ky b kz la lc ld lf nb lj nc ln nd lr ne nf ng nh bi translated">我们检查每个可能的移动所产生的状态，并在表中查找它们的当前值。</li><li id="ab16" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated">大多数时候，我们贪婪地行动，选择能带来最大价值的行动。<em class="ls">(最高中奖概率)</em></li><li id="fc58" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated">偶尔，我们会从其他动作中随机选择。<em class="ls">(探索)</em></li></ol><p id="2e4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">玩的时候，我们会改变自己所处状态的价值观:</p><ol class=""><li id="3366" class="mz na iq ky b kz la lc ld lf nb lj nc ln nd lr ne nf ng nh bi translated">在每次贪婪的移动之后，从 A 到 B，我们更新 A 的值以更接近 B 的值。</li><li id="b23a" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated">这是通过以下公式实现的</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/381fd36dcc13de1082d789163ae2467b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fA3JJ_meoGeofirCmKI_Og.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Fig 3. Temporal Difference Learning Update</strong></figcaption></figure><p id="dc2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，<br/><strong class="ky ir"><em class="ls">【S _ t】</em></strong>—旧状态的值，贪心移动前的状态<em class="ls">(A)<br/></em><strong class="ky ir"><em class="ls">V(S _ t+1)</em></strong>—新状态的值，贪心移动后的状态<em class="ls">(B)<br/></em><strong class="ky ir"><em class="ls">alpha</em></strong>—学习率</p><p id="c9fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个更新规则是<strong class="ky ir"> <em class="ls">时间差异学习</em> </strong>方法的一个例子，这样称呼是因为它的变化是基于两个连续时间的估计值之间的差异<code class="fe nv nw nx ny b">V(S_t+1) — V(S_t)</code>。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><blockquote class="nz"><p id="bd4c" class="oa ob iq bd oc od oe of og oh oi lr dk translated"><em class="oj">感谢阅读！如果我从书中发现一些需要提及的见解，我会更新。</em></p></blockquote></div></div>    
</body>
</html>