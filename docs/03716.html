<html>
<head>
<title>How to Train Convolutional Neural Networks in Python (TensorFlow Eager API)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 Python (TensorFlow Eager API)训练卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/convolutional-neural-networks-an-introduction-tensorflow-eager-api-7e99614a2879?source=collection_archive---------18-----------------------#2019-06-12">https://towardsdatascience.com/convolutional-neural-networks-an-introduction-tensorflow-eager-api-7e99614a2879?source=collection_archive---------18-----------------------#2019-06-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ed431c403c43febfbbf440aa8741ab67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tBjPkUtIyepvIbl1ZRgyOQ.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Keep an eye out for Deep Learning. Source: <a class="ae kc" href="https://pixabay.com/photos/eye-iris-look-focus-green-1132531/" rel="noopener ugc nofollow" target="_blank">Pixabay</a>.</figcaption></figure><p id="2852" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">卷积神经网络是过去十年深度学习经常成为头条新闻的一部分。今天我们将使用 TensorFlow 的 eager API 训练一个<strong class="kf ir">图像分类器</strong>来告诉我们一幅图像中包含的是一只狗还是一只猫。</p><p id="b0cb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于人工神经网络在许多领域具有前所未有的能力，它最近已经颠覆了几个行业。然而，不同的深度学习架构在每个方面都很出色:</p><ul class=""><li id="1c45" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">图像分类(卷积神经网络)。</li><li id="eea5" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">图像、音频和文本生成(GANs、RNNs)。</li><li id="7ce7" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">时间序列预测(RNNs，LSTM)。</li><li id="954c" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">推荐系统。</li><li id="bc19" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">巨大的等等(例如，回归)。</li></ul><p id="9c1c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">今天我们将关注列表中的第一项，尽管每一项都值得单独写一篇文章<em class="lp">。</em></p><h1 id="bde1" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">什么是卷积神经网络？</h1><p id="8039" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">在多层感知器(MLP)中，<em class="lp">香草</em>神经网络，每层的神经元连接到下一层的所有<strong class="kf ir">神经元。我们称这种类型的层为完全连接的层。</strong></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/36357612d2d014b2e9881f5afc192598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YpFy4XNcRdH0Dhow.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">A MLP. Source: <a class="ae kc" href="http://www.astroml.org/book_figures/appendix/fig_neural_network.html" rel="noopener ugc nofollow" target="_blank">astroml</a></figcaption></figure><p id="c408" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">卷积神经网络是不同的:它们有卷积层。</p><p id="873c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在完全连接的层上，每个神经元的输出将是前一层的线性变换，由非线性激活函数组成(例如，<em class="lp"> ReLu </em>或<em class="lp"> Sigmoid </em>)。</p><p id="f3b7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相反，<strong class="kf ir">卷积层</strong>中每个神经元的输出只是前一层神经元的一个(通常很小)<strong class="kf ir">子集</strong>的函数。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/87a0f2ca9379539ff0e7e85b487f27e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ulpGWgOsrF74H5KU"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Source: <a class="ae kc" href="https://brilliant.org/wiki/convolutional-neural-network/" rel="noopener ugc nofollow" target="_blank">Brilliant</a></figcaption></figure><p id="e7d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">卷积层上的输出将是对前一层神经元的子集应用<strong class="kf ir">卷积</strong>的结果，然后是激活函数。</p><h1 id="5855" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">什么是卷积？</h1><p id="d8e2" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">给定一个输入矩阵<em class="lp"> A </em>(通常是前一层的值)和一个被称为<strong class="kf ir">内核</strong>或<strong class="kf ir">滤波器</strong> <em class="lp"> K </em>的权重矩阵(通常小得多)，卷积运算将输出一个新的矩阵<em class="lp"> B </em>。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="ab gu cl mz"><img src="../Images/7533eba844ff7a09a5fcf69bf8ad2620.png" data-original-src="https://miro.medium.com/v2/format:webp/1*4yv0yIH0nVhSOv3AkLUIiw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">by @<a class="ae kc" href="https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148" rel="noopener">RaghavPrabhu</a></figcaption></figure><p id="ebb3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<em class="lp"> K </em>是一个<em class="lp"> CxC </em>矩阵，那么<em class="lp"> B </em>中的第一个元素将是以下结果:</p><ul class=""><li id="68cd" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">取<em class="lp"> A </em>的第一个<em class="lp"> CxC </em>子矩阵。</li><li id="5a36" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">将每个元素乘以其在<em class="lp"> K </em>中的相应权重。</li><li id="c753" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">添加所有产品。</li></ul><p id="6dd6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这最后两个步骤相当于展平<em class="lp"> A </em>的子矩阵和<em class="lp"> K </em>，并计算结果向量的点积。</p><p id="c175" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们向右滑动 K 以获得下一个元素，依此类推，对每一行重复这个过程。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="ab gu cl mz"><img src="../Images/f90d00f55bfbcbe5962056bdeb47f214.png" data-original-src="https://miro.medium.com/v2/1*MrGSULUtkXc0Ou07QouV8A.gif"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Convolution visualization by @<a class="ae kc" href="https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148" rel="noopener">RaghavPrabhu</a></figcaption></figure><p id="3578" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据我们的需要，我们只能从以第<em class="lp"> Cth </em>行和列为中心的内核开始，以避免“越界”，或者假设“A 之外”的所有元素都有某个默认值(通常为 0)——这将定义<em class="lp"> B </em>的大小是小于<em class="lp"> A </em>还是相同。</p><p id="cc64" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如你所见，如果<em class="lp"> A </em>是一个<em class="lp"> NxM </em>矩阵，那么现在<em class="lp"> B </em>中每个神经元的值将不取决于<em class="lp"> N*M </em>权重，而只取决于其中的<em class="lp"> C*C </em>(少得多)。这使得卷积层比全连接层轻得多，有助于卷积模型学习得更快。</p><p id="3379" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，我们最终会在每一层上使用许多内核(得到一堆矩阵作为每一层的输出)。然而，这仍然需要比我们的老 MLP 少得多的重量。</p><h1 id="1b32" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">为什么会这样？</h1><p id="49fe" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">为什么我们可以忽略每个神经元如何影响大多数其他神经元？嗯，这整个系统成立的前提是每个神经元都受到其“邻居”的强烈影响。然而，远处的神经元对它只有很小的影响。</p><p id="da9c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个假设在图像中<strong class="kf ir">直观上是真实的——如果我们想到输入层，每个神经元将是一个像素或一个像素的 RGB 值。这也是为什么这种方法对图像分类如此有效的部分原因。</strong></p><p id="2316" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，如果我拍摄一张照片中有蓝天的区域，很可能附近的区域也会使用相似的色调来显示天空。</p><p id="cdaa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像素的相邻像素通常具有相似的 RGB 值。如果他们没有，那么这可能意味着我们在一个图形或物体的边缘。</p><p id="0769" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你用笔和纸(或计算器)做一些卷积，你会意识到如果在某种边缘上，某些内核会增加输入的强度。在其他边缘，他们可以减少它。</p><p id="587d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为一个例子，让我们考虑下面的内核<em class="lp"> V </em>和<em class="lp"> H </em>:</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/c65db07d22ecec45a8f9c89e35969a34.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/0*pyY6Yqi5xyzpkmYC.png"/></div></figure><p id="93b5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lp"> V </em>过滤垂直边缘(上面的颜色与下面的颜色非常不同)，而<em class="lp"> H </em>过滤水平边缘。注意一个是另一个的转置版本。</p><h1 id="65c9" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">卷积示例</h1><p id="b35b" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">这是一窝小猫未经过滤的照片:</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/e7d5a21362986d2d51d5ea641b8abd92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*8dZsX54Gpku0XtJn.jpg"/></div></figure><p id="7713" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们分别应用水平和垂直边缘滤镜，会发生以下情况:</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/0349ad72990313fd58b0597acb8e39ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*2u3YVp5qkTaBwtPJ.png"/></div></figure><p id="0495" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到一些特征变得更加明显，而另一些逐渐消失。有趣的是，每个过滤器展示了不同的功能。</p><p id="141e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是卷积神经网络如何学习识别图像中的特征。让它适合自己的内核权重比任何手动方法都要容易得多。想象一下，试图用手弄清楚你应该如何表达像素之间的关系… <em class="lp">！</em></p><p id="af79" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了真正掌握每个卷积对一张图片的影响，我强烈建议你在这个网站上玩一玩。它对我的帮助比任何书籍或教程都大。去吧，把它收藏起来。很好玩。</p><p id="71a0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，你已经学了一些理论。现在让我们进入实际部分。</p><h1 id="7e08" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">如何在 TensorFlow 中训练一个卷积神经网络？</h1><p id="fe68" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">TensorFlow 是 Python 最流行的深度学习框架。我也听说过 PyTorch 的优点，虽然我从来没有机会尝试。</p><p id="1fa7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我已经写了一篇关于<a class="ae kc" href="http://www.datastuff.tech/machine-learning/autoencoder-deep-learning-tensorflow-eager-api-keras/" rel="noopener ugc nofollow" target="_blank">如何用 TensorFlow 的 Eager API </a>训练神经网络的教程，重点是自动编码器。</p><p id="3aaa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">今天会有所不同:我们将尝试三种不同的架构，看看哪一种做得更好。像往常一样，所有的代码都可以在<a class="ae kc" href="https://github.com/StrikingLoo/Cats-and-dogs-classifier-tensorflow-CNN" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得，所以你可以自己尝试或者跟着做。当然，我还会展示 Python 代码片段。</p><h1 id="7cb4" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">数据集</h1><p id="6b2e" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">我们将训练一个神经网络来预测一幅图像是包含一只狗还是一只猫。为此，我们将使用 Kaggle 的<a class="ae kc" href="https://www.kaggle.com/c/dogs-vs-cats" rel="noopener ugc nofollow" target="_blank">猫狗数据集</a>。它包含 12500 张猫和 12500 张狗的图片，分辨率各不相同。</p><h1 id="43c2" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">用 NumPy 加载和预处理我们的图像数据</h1><p id="328e" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">神经网络接收特征向量或矩阵作为输入，通常具有<strong class="kf ir">固定维度</strong>。我们如何从我们的照片中产生这种感觉？</p><p id="a504" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">幸运的是，Python 的图像库为我们提供了一种将图像作为 NumPy 数组加载的简单方法。RGB 值的高 x 宽矩阵。<br/>我们已经在<a class="ae kc" href="http://www.datastuff.tech/machine-learning/k-means-clustering-with-dask-editing-pictures-of-kittens/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>中这样做了，所以我将重用这段代码。</p><p id="0550" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，我们仍然必须解决固定维度部分:我们为输入层选择哪些维度？这很重要，因为我们必须将每张图片调整到所选的分辨率。我们不想扭曲太多的长宽比，以免给网络带来太多的噪音。</p><p id="7c0e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是我们如何看到数据集中最常见的形状。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="b183" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我对前 1000 张图片进行了采样，尽管当我查看 5000 张图片时，结果并没有改变。最常见的形状是 375×500，尽管我决定将它除以 4 作为我们网络的输入。</p><p id="5e05" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是我们的图像加载代码现在的样子。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="6d41" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，您可以用这个代码片段加载数据。我选择使用 4096 张图片作为训练集的样本，1024 张图片作为验证集的样本。然而，这只是因为我的电脑不能处理更多的内存大小。</p><p id="1602" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你在家尝试，可以随意增加这些数字到最大值(比如训练用的 10K 和验证用的 2500 )!</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h1 id="92eb" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">训练我们的神经网络</h1><p id="3211" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">首先，作为一种基线，让我们看看一个普通的 MLP 在这项任务中表现如何。如果卷积神经网络如此具有革命性，我预计这个实验的结果会是<strong class="kf ir">可怕的</strong>。</p><p id="878f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个单隐层全连接神经网络。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="b6f3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文的所有训练都是使用 AdamOptimizer 完成的，因为它是最快的。我只调整了每个模型的学习率(这里是 1e-5)。</p><p id="d95c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个模型我训练了 10 个纪元，基本收敛到<strong class="kf ir">随机猜测</strong>。我确保<strong class="kf ir">打乱了训练数据</strong>，因为我是按顺序加载的，这可能会使模型产生偏差。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="5d82" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我使用<strong class="kf ir"> MSE </strong>作为损失函数，因为它通常更直观地解释。如果你的 MSE 在二进制分类中是 0.5，你就等于<strong class="kf ir">总是预测 0 </strong>。然而，具有更多层或不同损失函数的 MLP<strong class="kf ir">并没有表现得更好</strong>。</p><h1 id="ccf6" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">训练卷积神经网络</h1><p id="f123" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">单个卷积层能有多大好处？让我们给我们的模型加一个看看。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="8a5c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这个网络，我决定添加一个单一的卷积层(24 个内核)，然后是 2 个完全连接的层。</p><p id="1303" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Max Pooling 所做的只是将每四个神经元减少到一个，四个之间的值最高。</p><p id="f75b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">仅仅过了 5 个时代，它已经比以前的网络表现得更好了。验证 MSE 为 0.36，这已经比随机猜测好得多了。但是请注意，我不得不使用一个小得多的学习率。此外，即使它在更少的纪元中学习，<strong class="kf ir">每个纪元</strong>花费<strong class="kf ir">更长的时间</strong>。该型号也重了很多(200 多 MB)。</p><p id="eb09" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我决定也开始测量预测和验证标签之间的皮尔逊相关性。这款车型得分 15.2%。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h1 id="a1c9" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">两个卷积层的神经网络</h1><p id="5f60" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">因为那个模型做得更好，我决定尝试一个更大的。我添加了<strong class="kf ir">另一个卷积层</strong>，并使两者都大了很多(每个 48 个内核)。这意味着模型可以从图像中学习更复杂的特征。然而，这也不出所料地意味着我的内存几乎爆炸。同样，训练花费了<strong class="kf ir">更长的时间</strong>(15 个周期半小时)。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="2f5d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果非常好。预测和标签之间的皮尔逊相关系数达到 0.21，验证 MSE 低至 0.33。</p><p id="64b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们来测量网络的准确性。因为 1 是猫，0 是狗，我可以说“如果模型预测的值高于某个阈值<em class="lp"> t </em>，那么预测<em class="lp">猫</em>。否则预测<em class="lp">狗</em>在尝试了 10 个简单的阈值后，这个网络的最大准确度达到了 61% 。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h1 id="74d7" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">更大的卷积神经网络</h1><p id="707e" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">显然，增加模型的大小可以让它学习得更好，所以我试着把两个卷积层都做得更大，每个都有 128 个滤波器。我没有改动模型的其他部分，也没有改变学习率。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="8237" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个模型最后达到了 30%的关联度！它最好的<strong class="kf ir">准确率是 67% </strong>，这意味着它有三分之二的时间是正确的。我认为一个更大的模型可能更符合数据。然而，这个已经花了 7 分钟了，我不想离开下一个训练一上午。</p><p id="28a1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常，在模型的<strong class="kf ir">尺寸</strong>和<strong class="kf ir">时间限制</strong>之间需要做出<strong class="kf ir">的权衡。大小限制了网络能够多好地适应数据(一个<strong class="kf ir">小模型</strong>会<strong class="kf ir">不适应</strong>)，然而我不会等 3 个小时让我的模型学习。</strong></p><p id="853d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你有一个商业截止日期，同样的担心也适用。</p><h1 id="74a4" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">结论</h1><p id="6dcb" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">我们已经看到，在图像分类任务中，卷积神经网络比传统架构明显更好(T21)。我们还尝试了不同的<strong class="kf ir">指标</strong>来衡量<strong class="kf ir">模型性能</strong>(相关性、准确性)。</p><p id="621a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们了解了<strong class="kf ir">模型的尺寸</strong>(防止欠拟合)和它的<strong class="kf ir">收敛速度</strong>之间的<strong class="kf ir">权衡</strong>。</p><p id="86eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们使用 TensorFlow 的 eager API 来轻松训练一个深度神经网络，并使用 numpy 进行(尽管很简单)图像预处理。</p><p id="163d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在以后的文章中，我相信我们可以用不同的池层、过滤器大小、步长和不同的预处理进行更多的实验。</p><p id="27b5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你觉得这篇文章有用吗？你会更喜欢学习其他的东西吗？还有什么不清楚的吗？请在评论中告诉我！</p><p id="6f5f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lp">在</em><a class="ae kc" href="http://www.twitter.com/strikingloo" rel="noopener ugc nofollow" target="_blank"><em class="lp">Twitter</em></a><em class="lp"/><a class="ae kc" href="http://www.medium.com/@strikingloo" rel="noopener"><em class="lp">Medium</em></a><em class="lp">或</em><a class="ae kc" href="http://www.dev.to/strikingloo" rel="noopener ugc nofollow" target="_blank"><em class="lp">dev . to</em></a><em class="lp">上找我如果有什么问题，或者想联系我什么的。如果你想从事机器学习事业，下面是我的</em> <a class="ae kc" href="http://www.datastuff.tech/data-science/3-machine-learning-books-that-helped-me-level-up-as-a-data-scientist/" rel="noopener ugc nofollow" target="_blank"> <em class="lp">推荐阅读清单</em> </a> <em class="lp">。</em></p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><p id="61ac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lp">原载于 2019 年 6 月 12 日</em><a class="ae kc" href="http://www.datastuff.tech/machine-learning/convolutional-neural-networks-an-introduction-tensorflow-eager/" rel="noopener ugc nofollow" target="_blank"><em class="lp">http://www . data stuff . tech</em></a><em class="lp">。</em></p></div></div>    
</body>
</html>