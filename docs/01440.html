<html>
<head>
<title>Sigmoid Neuron — Building Block of Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Sigmoid 神经元——深度神经网络的构建模块</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sigmoid-neuron-deep-neural-networks-a4cd35b629d7?source=collection_archive---------1-----------------------#2019-03-07">https://towardsdatascience.com/sigmoid-neuron-deep-neural-networks-a4cd35b629d7?source=collection_archive---------1-----------------------#2019-03-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4fbb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深层神经网络的构建模块被称为乙状结肠神经元。Sigmoid 神经元类似于<a class="ae kl" href="https://hackernoon.com/perceptron-deep-learning-basics-3a938c5f84b6" rel="noopener ugc nofollow" target="_blank">感知器</a>，但是它们稍有修改，使得 sigmoid 神经元的输出比感知器的阶跃函数输出平滑得多。在这篇文章中，我们将讨论创造乙状结肠神经元的动机和乙状结肠神经元模型的工作原理。</p><p id="0659" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="km">引用注:本文内容和结构基于四分之一实验室的深度学习讲座——</em><a class="ae kl" href="https://padhai.onefourthlabs.in" rel="noopener ugc nofollow" target="_blank"><em class="km">pad hai</em></a><em class="km">。</em></p><p id="f205" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是讨论 sigmoid 神经元的工作及其学习算法的两部分系列的第一部分:</p><p id="f4ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1 | Sigmoid 神经元——深度神经网络的构建模块</p><p id="b7c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2 | <a class="ae kl" rel="noopener" target="_blank" href="/sigmoid-neuron-learning-algorithm-explained-with-math-eb9280e53f07">用数学解释的 Sigmoid 神经元学习算法</a></p><h1 id="e751" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">为什么是乙状结肠神经元</h1><p id="675f" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">在我们进入 sigmoid 神经元的工作之前，让我们简单地讨论一下感知器模型及其局限性。</p><p id="620d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感知器模型接受几个实值输入，并给出一个二进制输出。在感知器模型中，每个输入<code class="fe lq lr ls lt b">xi</code>都有与之相关的权重<code class="fe lq lr ls lt b">wi</code>。权重表明决策过程中输入的重要性。模型输出由阈值<strong class="jp ir"> Wₒ </strong>决定，如果输入的加权和大于阈值<strong class="jp ir"> Wₒ </strong>输出将为 1，否则输出将为 0。换句话说，如果加权和大于阈值，模型将被触发。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/d2fb97077a5d18042270d893a64441a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*2UxzoPxrqUYqIep29MgpFw.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Perceptron (Left) &amp; Mathematical Representation (Right)</figcaption></figure><p id="1893" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据数学表示，我们可以说感知器使用的阈值逻辑非常苛刻。让我们用一个例子来看看苛刻的阈值逻辑。考虑一个人的决策过程，他/她是否愿意购买一辆汽车仅基于一个输入<code class="fe lq lr ls lt b">X1</code> —工资，并通过设置阈值<strong class="jp ir"> b </strong> ( <strong class="jp ir"> Wₒ </strong> ) = -10 和权重<strong class="jp ir"> W </strong> ₁ <strong class="jp ir"> </strong> = 0.2。感知器模型的输出如下图所示。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/0d695e2c5bacb6ac832bea55f90f318f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*Ax5MRwNFheaHjIil-cPhNQ.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Data (Left) &amp; Graphical Representation of Output(Right)</figcaption></figure><p id="0831" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">红点表示一个人不会买车，绿点表示这个人想买车。一个 50.1K 的人会买车而一个 49.9K 的人不会买车是不是有点奇怪？感知器输入的微小变化有时会导致输出完全翻转，比如从 0 到 1。这种行为并不是我们选择的具体问题或者我们选择的具体权重和阈值的特征。这是感知器神经元本身的特性，其行为类似于阶跃函数。我们可以通过引入一种叫做<em class="km">s 形</em>神经元的新型人工神经元来克服这个问题。</p><blockquote class="mh mi mj"><p id="8dde" class="jn jo km jp b jq jr js jt ju jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj kk ij bi translated">要了解更多关于感知器的工作，请参考我以前关于<a class="ae kl" href="https://hackernoon.com/perceptron-deep-learning-basics-3a938c5f84b6" rel="noopener ugc nofollow" target="_blank">感知器模型</a>的帖子</p></blockquote></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h1 id="a001" class="kn ko iq bd kp kq mu ks kt ku mv kw kx ky mw la lb lc mx le lf lg my li lj lk bi translated">乙状结肠神经元</h1><p id="32ed" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">我们能有一个更平滑(不那么苛刻)的函数吗？</p><p id="26b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">引入 sigmoid 神经元，其输出函数比阶跃函数平滑得多。在乙状结肠神经元中，与阶跃输出相反，输入的小变化仅引起输出的小变化。有许多具有“<strong class="jp ir">S</strong>”<strong class="jp ir"/>形曲线特征的函数称为 sigmoid 函数。最常用的函数是逻辑函数。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/1ac14f79ff8d6deb4f6aa3e71438c580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*N7dfPwbiXC-Kk4TCbfRerA.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Sigmoid Neuron Representation (logistic function)</figcaption></figure><p id="1c34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们不再看到临界点<strong class="jp ir"> b </strong>的急剧转变。乙状结肠神经元的输出不是 0 或 1。相反，它是一个介于 0-1 之间的真实值，可以解释为一个概率。</p><h1 id="3eee" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">数据和任务</h1><blockquote class="na"><p id="2314" class="nb nc iq bd nd ne nf ng nh ni nj kk dk translated">回归和分类</p></blockquote><p id="7572" class="pw-post-body-paragraph jn jo iq jp b jq nk js jt ju nl jw jx jy nm ka kb kc nn ke kf kg no ki kj kk ij bi translated">与<a class="ae kl" href="https://hackernoon.com/mcculloch-pitts-neuron-deep-learning-building-blocks-7928f4e0504d" rel="noopener ugc nofollow" target="_blank"> MP 神经元</a>中的布尔输入不同，sigmoid 神经元的输入可以是实数，输出也是 0-1 之间的实数。在乙状结肠神经元中，我们试图回归<strong class="jp ir"> X </strong>和<strong class="jp ir"> Y </strong>之间的概率关系。即使输出在 0-1 之间，我们仍然可以通过选择一些阈值来使用 sigmoid 函数进行二进制分类任务。</p><h1 id="611e" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">学习算法</h1><p id="291e" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">在本节中，我们将讨论通过使用梯度下降算法来学习 sigmoid 神经元模型的参数<strong class="jp ir"> w </strong>和<strong class="jp ir"> b </strong>的算法。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7274327ed6a30650b17a1ad8941abd88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*IW9fCn7_fw2QVgbHx2VkvQ.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Minimize the Squared Error Loss</figcaption></figure><p id="c603" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">学习算法的目标是确定参数的最佳可能值，使得模型的总损失(平方误差损失)尽可能最小。学习算法是这样的:</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/7971a34b5e9379d4f18d23a99dc69fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*fBxEzbzP1KkqR7PTexJZdw.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Sigmoid Learning Algorithm</figcaption></figure><p id="930a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们随机初始化<strong class="jp ir"> w </strong>和<strong class="jp ir"> b </strong>。然后，我们迭代数据中的所有观察值，对于每个观察值，使用 sigmoid 函数找到相应的预测结果，并计算平方误差损失。基于损失值，我们将更新权重，使得在新参数下模型的总损失将<strong class="jp ir">小于模型的当前损失</strong>。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/3f6ed39b5fd31b9a28836f789e8303c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*J820Ot0_aaR_9tE-ECbFsQ.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Loss Optimization</figcaption></figure><p id="1f0a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将继续进行更新操作，直到我们满意为止。直到满意可能意味着以下任何一种情况:</p><ul class=""><li id="d22e" class="ns nt iq jp b jq jr ju jv jy nu kc nv kg nw kk nx ny nz oa bi translated">模型的总损失变为零。</li><li id="6bc3" class="ns nt iq jp b jq ob ju oc jy od kc oe kg of kk nx ny nz oa bi translated">模型的总损失变成接近于零的非常小的值。</li><li id="5073" class="ns nt iq jp b jq ob ju oc jy od kc oe kg of kk nx ny nz oa bi translated">基于计算能力迭代固定次数。</li></ul><h1 id="b06d" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">它能处理非线性数据吗？</h1><p id="e017" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">感知器模型的局限性之一是，学习算法只有在数据是线性可分的情况下才有效。这意味着正的点在边界的一边，负的点在边界的另一边。sigmoid 神经元能处理非线性可分数据吗？。</p><p id="4a81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们举一个例子，一个人是否会购买一辆汽车基于两个输入，x₁——年薪 10 万卢比(LPA)和 x₂——家庭规模。我假设在<strong class="jp ir"> X </strong>和<strong class="jp ir"> Y </strong>之间有一个关系，它是使用 sigmoid 函数近似得到的。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi og"><img src="../Images/498bcf1e387f012d323b6c516dc7343c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*mYd1fmZIulL8u0BignG4GQ.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Input Data(Left) &amp; Scatter Plot of Data(Right)</figcaption></figure><p id="b57c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">红点表示输出为 0，绿点表示输出为 1。从图中我们可以看到，没有一条线或者一条线性的边界可以有效的将红绿点分开。如果我们在这个数据上训练一个感知器，学习算法将<strong class="jp ir">永远不会收敛</strong>，因为数据不是线性可分的。我将运行模型一定次数的迭代，以尽可能减少误差，而不是追求收敛。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/9137ceb2577f47aeb5cbee9e24d39f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*Wmu9ibPHR0Iz-s327ENsTw.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Perceptron Decision boundary for fixed iterations</figcaption></figure><p id="0561" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从感知器决策边界，我们可以看到，由于苛刻的阈值逻辑，感知器无法区分靠近边界的点和位于内部的点。但是在现实世界的场景中，我们会期望一个站在边界围栏上的人可以选择任何一条路，而不像在决策边界内的人。</p><p id="8359" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看 sigmoid neuron 将如何处理这些非线性可分离数据。一旦我使用 sigmoid 神经元拟合我们的二维数据，我将能够生成如下所示的 3D 等值线图，以表示所有观察的决策边界。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/803f566effa16b9d8867ddcdb2b18976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*5r3j_aGz6gbGBM8j9GWH_A.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Sigmoid Neuron Decision Boundary (Left) &amp; Top View of Decision Boundary (Right)</figcaption></figure><p id="60c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了进行比较，让我们进行同样的两次观察，看看这些观察的乙状结肠神经元的预测结果是什么。如您所见，在图的最左侧出现的观察的预测值为零(出现在暗红色区域),而另一个观察的预测值约为 0.35，即该人有 35%的机会购买汽车。与感知器的刚性输出不同，现在我们有一个介于 0-1 之间的平滑连续的输出，可以解释为一个概率。</p><blockquote class="na"><p id="8901" class="nb nc iq bd nd ne nf ng nh ni nj kk dk translated">仍然不能完全解决非线性数据的问题。</p></blockquote><p id="efa3" class="pw-post-body-paragraph jn jo iq jp b jq nk js jt ju nl jw jx jy nm ka kb kc nn ke kf kg no ki kj kk ij bi translated">尽管我们已经引入了非线性 sigmoid 神经元函数，但是它仍然不能够有效地将红点与绿点分开。重要的一点是，从感知器中的刚性决策边界开始，我们已经朝着创建适用于非线性可分离数据的决策边界的方向迈出了第一步。因此，乙状结肠神经元是深层神经网络的构建模块，最终我们不得不使用神经元网络来帮助我们创建一个“<em class="km">完美的</em>决策边界。</p><h1 id="0a63" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">继续学习</h1><p id="f88e" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">如果你有兴趣了解更多关于人工神经网络的知识，请查看来自<a class="ae kl" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank"> Starttechacademy </a>的 Abhishek 和 Pukhraj 的<a class="ae kl" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>。还有，课程是用最新版本的 Tensorflow 2.0 (Keras 后端)讲授的。他们也有一个非常好的关于 Python 和 R 语言的<a class="ae kl" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">机器学习(基础+高级)</a>的包。</p><h1 id="3015" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">结论</h1><p id="8e7a" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">在这篇文章中，我们看到了导致乙状结肠神经元产生的感知机的局限性。我们还看到了乙状结肠神经元的工作示例，以及它如何克服一些限制。我们已经看到感知器和 sigmoid 神经元模型是如何处理非线性可分离数据的。</p><p id="0488" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在下一篇<a class="ae kl" rel="noopener" target="_blank" href="/sigmoid-neuron-learning-algorithm-explained-with-math-eb9280e53f07">文章</a>中，我们将利用 math 详细讨论 sigmoid 神经元学习算法，并直观地了解为什么特定的更新规则有效。</p><p id="d2ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="km">推荐阅读:</em></p><div class="oj ok gp gr ol om"><a rel="noopener follow" target="_blank" href="/sigmoid-neuron-learning-algorithm-explained-with-math-eb9280e53f07"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ir gy z fp or fr fs os fu fw ip bi translated">用数学解释的 Sigmoid 神经元学习算法</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">在本帖中，我们将详细讨论 sigmoid 神经元学习算法背后的数学直觉。</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa ma om"/></div></div></a></div><div class="oj ok gp gr ol om"><a href="https://hackernoon.com/perceptron-deep-learning-basics-3a938c5f84b6" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ir gy z fp or fr fs os fu fw ip bi translated">感知器——深度学习基础</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">麦卡洛克-皮茨神经元的升级。</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">hackernoon.com</p></div></div><div class="ov l"><div class="pb l ox oy oz ov pa ma om"/></div></div></a></div><blockquote class="mh mi mj"><p id="7e97" class="jn jo km jp b jq jr js jt ju jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj kk ij bi translated"><em class="iq">联系我<br/>GitHub:</em><a class="ae kl" href="https://github.com/Niranjankumar-c" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://github.com/Niranjankumar-c</em></a><em class="iq"><br/>LinkedIn:</em><a class="ae kl" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://www.linkedin.com/in/niranjankumar-c/</em></a></p></blockquote><p id="608a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">免责声明</strong> —这篇文章中可能有一些相关资源的附属链接。你可以以尽可能低的价格购买捆绑包。如果你购买这门课程，我会收到一小笔佣金。</p></div></div>    
</body>
</html>