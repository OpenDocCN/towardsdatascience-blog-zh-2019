# 哈希幂概率数据结构

> 原文：<https://towardsdatascience.com/hashes-power-probabilistic-data-structures-d1398d1335c6?source=collection_archive---------24----------------------->

![](img/04253495a1e879a992688457114147a9.png)

Photo by [Ryan Thomas Ang](https://unsplash.com/photos/_SdoUGFBhnI?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/search/photos/coin?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

散列函数在计算机科学中被广泛使用，但我想提一下它们在概率数据结构和算法中的用途。我们都知道数据结构是大多数算法的构建模块。一个糟糕的选择可能会导致艰难而低效的解决方案，而不是优雅而高效的解决方案。

正如 Linus Torvalds 曾经在谈论 Git 时所说的:

> …事实上，我是围绕数据设计代码的强烈支持者，而不是 T2，我认为这是 git 相当成功的原因之一(*)。

此外:

> (*)事实上，我会说一个糟糕的程序员和一个好的程序员的区别在于他是否认为他的代码或者他的数据结构
> 更重要。糟糕的程序员担心代码。优秀的程序员担心数据结构和它们之间的关系。

如果你是一名计算机科学家、工程师或程序员，你会非常清楚不同细节层次的数据结构。和任何一个程序员聊十分钟都可能会听到:链表、树、堆、地图等等。

我与同事和朋友讨论过，根据你的工作内容，掌握这些数据结构有多重要。如果你是一名学者或者在一家公司工作，在那里数量是一个真正的问题，那么你应该掌握它们。

大多数情况下，如果你是用高级语言编程，那么重新发明轮子并自己实现已知的算法就是一种代码味道。大多数框架都内置了这些数据结构，或者有超高效的库，这些库很有可能会大有作为。

从另一个角度来看，大多数人从事软件维护工作，或者资源限制不是真正问题的地方。痴迷于将性能或资源消耗作为数据结构或算法决策的唯一衡量标准是不明智的，尤其是当它们不是那么相关的时候。同样，这取决于您正在解决的问题及其扩展需求。

但是有时资源限制确实是一个问题，在这种情况下，您应该考虑工具箱中的一个重要工具:*概率数据结构。它们以相当可观的资源需求来牺牲精度。如果你正在解决一个*小*问题，其中内存空间不受限制，计算不是瓶颈，精确的精度是*必须的*，那么在使用它们之前你可能会三思。如果不是这样，你应该知道他们的存在。另外，学习它们很有趣。*

有许多书籍、论文和文章都在谈论这个话题。在这里，我将谈论其中两个我认为非常漂亮并且在现实世界问题中大量使用的方法。我将使用一些数学，但不是非常详细，因为网上有很多优秀的资源。即使你不懂数学，你也会得到它们的基本概念(这是这里的目标)。

## 布隆过滤器

如果我的记忆正确的话， *Bloom filters* 是我几年前在阅读一些 *petascale* 数据库时听说的第一个概率数据结构。与许多其他概率数据结构一样，考虑到它们需要的空间和计算是如此之少，您会对它们的有效性感到非常惊讶。

假设你有一大组整数 *S* ，你想检查 *S* 是否包含一个元素 *i* 。你可以简单地使用一个带有空间/时间的链表来解决这个问题。还有，试试用*O(*n*)*/*O(log(n))*的平衡二叉树。或者很明显，一个有 *O(n)* / *O(1)* 的地图。

似乎大多数选项都需要与 *S* 的基数成线性关系的内存空间，考虑到如果我们不存储 *S，*的每个元素的值，我们怎么能检查一个元素是否存在于 *S* 中呢？。

*布隆过滤器*的思想是将整个集合 *S* 编码成一个固定长度的二进制串，我们称之为 *Q* 。这是通过将 *S* 中的每个元素编码到 *Q 中来实现的。*正如您可以想象的那样，将一个可变大小的域映射到一个固定大小的域最终会导致冲突。

让我们来看一个*布隆过滤器*的典型例子。假设我们将 *Q* 定义为固定长度 64 位的二进制字符串。此外，我们选择 *k* 不同的散列函数，如 MD5 或 SHA-1。接下来，我们执行以下操作:

*   取 *S* 中的第一个元素
*   使用 *k* 哈希函数对元素进行哈希运算，并将它们*取模* 64，以生成 *Q.* 中的 *k* 索引
*   在之前计算的索引处，将 *Q* 中的每一位设置为 1
*   对 *S* 中的每个剩余元素执行前两个步骤

当我们完成时，我们有一个 64 位的值 *Q* ，其中一些位被设置为 1，其他的被设置为 0。

现在我们要检查一个元素是否在 *S 中。*我们做与上面完全相同的程序来检查在 *Q* 中哪些位应该被置位。如果任何相应的 *k* 位**未**置位，我们可以确定该元素不在 *S* 中。如果它们都被设置，我们可以说元素是**可能是 *Q* 中的**，因为这些位也可以被许多元素部分地设置为 1。事实上，当您在 *S* 中不断添加元素时，越来越多的位在 *Q* 中被设置为 1，因此您不断增加这种可能性。

> 布隆过滤器会产生假阳性，但不会产生假阴性。

如果我们增加 *Q* 的大小，我们就避免了冲突，从而避免了误报的可能性。k 值也在碰撞概率中起作用。这是在*大小(Q)* 和 *k* 和*假阳性*率之间的权衡。如果你对量化误报率的数学感兴趣，你可以在这里阅读。同样，你可以在这里看到最佳*尺寸(Q)* 和 *k* 考虑 *#S* 或期望的假阳性率。

还有一个更重要的考虑:选择散列函数来生成 *Q* 中的索引。之前，我提到了 MD5 或 SHA-1，但这些都不是明智的选择。加密哈希函数试图生成不可逆转的输出。这不是我们关心的问题。我们对随机输出感兴趣，并尽可能快地进行计算，因此有更好的选择。

大多数实现使用单个散列函数来生成所需的输出。特别是 [MurmurHash](https://en.wikipedia.org/wiki/MurmurHash) 函数，其中计算出 *MurmurHash* 输出的某个恒定基集，然后通过组合这些基哈希生成 *k* 输出。你可以在这里看到一个流行的*布隆过滤器*在 Go 中的实现。

还有另一种概率数据结构称为 [*计数最小草图*](https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch) ，它估计集合中每个项目的频率。这个想法与*布鲁姆滤镜*的工作原理非常相似，所以你可能有兴趣看一看。

如果你对以太坊感兴趣，布隆过滤器被用来检查一个块是否包含与某些*主题*相关的 [*日志*](https://codeburst.io/deep-dive-into-ethereum-logs-a8d2047c7371) 。在以太坊中，主题与*事件*和*索引参数*相关。一个不存储任何关于*世界状态*、*交易*或*收据*的数据的轻型客户端，可以非常快速地检查一个块是否包含与任何感兴趣的*主题*相关的*日志*。如果 Bloom filter 检查匹配，考虑到*误报*的小概率，我们可以非常确定这个块包含一个用于查询的*主题*的*日志*条目。一个*假阳性*的成本超过了分析所有块的所有交易的所有收据的永久成本。

## 超对数

HyperLogLog 是对以前的想法的改进，如 *LogLog* 和*线性计数*，它们关注的是*计数不同*问题。

假设你想知道一个大集合 *S 的基数，即:*S 中有多少不同的元素。我们也想在 O(1)空间中这样做。引用提出该想法的[原始论文](http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf):

> 例如，新的算法使得有可能在仅使用 1.5 千字节的存储器的情况下，以 2 %的典型精度来估计远超过 10 ⁹的基数。

这种数据结构中的形式数学比*Bloom filters*的情况更复杂，但背后的主要思想相当简单。

假设您有 8000 个随机生成的二进制字符串。我们预计其中有多少至少有 3 个前导零？嗯，至少有 3 个前导零的概率是 1/8，因此，我们可以估计大约有 1000 个前导零。当然，由于这是一个随机过程，我们可以看到从 0 到 8000 的二进制字符串满足这个属性，但每个情况的概率很重要。更一般地说，如果我们正好有 n 个前导零，那么基数为 2^n.似乎是合理的。这与将一枚硬币抛 100 次，我们大致会看到 50 个正面和 50 个反面是一样的。

当你深入细节时，你很快就会意识到差异是一个问题。这是一个很大的问题，因为每一个误差单位都会对估计产生指数影响。换句话说，如果我们碰巧有 K+1 个前导零，而不是 K 个前导零，那么我们的估计将会加倍。用于改进这一方面的思想是将集合分成多个子集，并使用在每个子集中找到的最大前导零的平均值。

从双对数到超双对数的一个演变是改变估计的均值类型，以控制对*异常值*的敏感性。特别是，对数对数使用的是*算术平均值*，而超对数使用的是*调和平均值*。此外，应用偏差校正系数来校正甚至更多的剩余偏差。

通过将集合分成多个集合来重复这个实验是很棒的，但是它产生了另一个问题:如果集合的基数太小，那么我们将没有足够的数据来进行统计。这可以通过简单地识别案例并使用在这种情况下更有效的其他技术来解决。

像在 Bloom filters 中一样，每个子集中的每个元素都被散列以将其转换成固定长度的二进制字符串，从中我们可以遵循上面的逻辑。同样，杂音散列函数在实现中被广泛使用。

## 结果

我们可以理解，在所有这些情况下，散列函数都扮演着重要的角色。优雅地说，它们提供了许多对概率数据结构和算法非常有用的特征:

*   它们将非均匀分布的数据转换成均匀分布的数据，这为概率假设提供了一个起点。
*   数据的通用*身份*，如果我们将操作设计为*等幂*，这将导致自动重复数据删除，有助于解决*计数不同*等问题。

利用散列不可逆性不是一个要求的事实，非加密散列函数是一个可以更快帮助算法速度的选项。