<html>
<head>
<title>Explain NLP models with LIME &amp; SHAP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用莱姆和 SHAP 解释自然语言处理模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explain-nlp-models-with-lime-shap-5c5a9f84d59b?source=collection_archive---------5-----------------------#2019-07-03">https://towardsdatascience.com/explain-nlp-models-with-lime-shap-5c5a9f84d59b?source=collection_archive---------5-----------------------#2019-07-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/bd9d34f3ee980c284e499c386ea42043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Biki7WvDVfDkCTT8178DA.png"/></div></div></figure><div class=""/><div class=""><h2 id="8b03" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">文本分类解释</h2></div><p id="2043" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">上周，我在 QCon New York 做了一个关于 NLP   的“<a class="ae lm" href="https://qconnewyork.com/ny2019/presentation/techniques-and-best-practices-prepping-data-ml-projects" rel="noopener ugc nofollow" target="_blank"> <strong class="ks jc"> <em class="ln">动手特征工程”的演讲。作为演示的一小部分，我简单演示了一下<a class="ae lm" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"><strong class="ks jc"><em class="ln">LIME</em></strong></a>&amp;<a class="ae lm" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"><strong class="ks jc"><em class="ln">SHAP</em></strong></a>在文本分类解释方面是如何工作的。</em></strong></a></p><p id="7141" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我决定写一篇关于它们的博客，因为它们很有趣，易于使用，视觉上引人注目。</p><p id="ac52" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">所有在比人脑可以直接可视化的维度更高的维度上运行的机器学习模型都可以被称为黑盒模型，这归结于模型的可解释性。特别是在<a class="ae lm" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP </a>领域，总是出现特征维数非常巨大的情况，解释特征重要性变得更加复杂。</p><p id="cdae" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><a class="ae lm" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"> <strong class="ks jc"> <em class="ln">石灰</em></strong></a>&amp;<a class="ae lm" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"><strong class="ks jc"><em class="ln">SHAP</em></strong></a>帮助我们不仅向最终用户，也向我们自己提供一个关于 NLP 模型如何工作的解释。</p><p id="fa98" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">利用<a class="ae lm" href="https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv" rel="noopener ugc nofollow" target="_blank">栈溢出问题标签分类数据集</a>，我们将构建一个多类文本分类模型，然后分别应用<strong class="ks jc"><em class="ln"/></strong>&amp;<strong class="ks jc"><em class="ln">【SHAP】</em></strong>来解释该模型。因为我们之前已经做了很多次文本分类，我们将快速构建 NLP 模型，并关注模型的可解释性。</p><h1 id="c391" class="lo lp jb bd lq lr ls lt lu lv lw lx ly kh lz ki ma kk mb kl mc kn md ko me mf bi translated">数据预处理、特征工程和逻辑回归</h1><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">preprocessing_featureEngineering_logreg.py</figcaption></figure><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mq"><img src="../Images/862dad715558143d57b545f4cc3585f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*65fRkk5li7j2jni6QrX5Ug.png"/></div></div></figure><p id="c3d8" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们在这里的目标不是产生最高的结果。我想尽快潜入莱姆 SHAP，这就是接下来发生的事情。</p><h1 id="daa4" class="lo lp jb bd lq lr ls lt lu lv lw lx ly kh lz ki ma kk mb kl mc kn md ko me mf bi translated">用 LIME 解释文本预测</h1><p id="a125" class="pw-post-body-paragraph kq kr jb ks b kt mr kc kv kw ms kf ky kz mt lb lc ld mu lf lg lh mv lj lk ll ij bi translated">从现在开始，这是有趣的部分。下面的代码片段大部分是从<a class="ae lm" href="https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html" rel="noopener ugc nofollow" target="_blank"> LIME 教程</a>中借来的。</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">explainer.py</figcaption></figure><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/74de693c4c01a3bc7ab9fa17249791ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*HuewrnreU47c_YR4lk_ckw.png"/></div></figure><p id="db1c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们在测试集中随机选择一个文档，它恰好是一个标记为<strong class="ks jc"> <em class="ln"> sql </em> </strong>的文档，我们的模型预测它也是<strong class="ks jc"> <em class="ln"> sql </em> </strong>。使用该文档，我们为标签 4(即<strong class="ks jc"> <em class="ln"> sql </em> </strong>和标签 8(即<strong class="ks jc"> <em class="ln"> python </em> </strong>)生成解释。</p><pre class="mg mh mi mj gt mx my mz na aw nb bi"><span id="6d52" class="nc lp jb my b gy nd ne l nf ng">print ('Explanation for class %s' % class_names[4])<br/>print ('\n'.join(map(str, exp.as_list(label=4))))</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/6454655eff73b13255bfd7c37874cf90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*0tfSuMPYLYR2BKfv4XlAsw.png"/></div></figure><pre class="mg mh mi mj gt mx my mz na aw nb bi"><span id="a3af" class="nc lp jb my b gy nd ne l nf ng">print ('Explanation for class %s' % class_names[8])<br/>print ('\n'.join(map(str, exp.as_list(label=8))))</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/a843749de6a910cef096d96b8ad676f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*LFk-YBzWU6laFoWQsrzYrw.png"/></div></figure><p id="c176" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">很明显这个文档对标签<strong class="ks jc"> <em class="ln"> sql 的解释最高。</em> </strong> <em class="ln"> </em>我们还注意到，正负符号都是相对于某个特定标签而言的，比如单词“sql”正对类<strong class="ks jc"> <em class="ln"> sql </em> </strong>而负对类<strong class="ks jc"> <em class="ln"> python </em> </strong>，反之亦然。</p><p id="5a90" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们将为该文档的前两个类生成标签。</p><pre class="mg mh mi mj gt mx my mz na aw nb bi"><span id="bc23" class="nc lp jb my b gy nd ne l nf ng">exp = explainer.explain_instance(X_test[idx], c.predict_proba, num_features=6, top_labels=2)<br/>print(exp.available_labels())</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/3989f4393c2d147c0b7282dbf942b075.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*ZcEBFaafFhf4TneMMDOjyg.png"/></div></figure><p id="4b04" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">它给了我们<strong class="ks jc"> <em class="ln"> sql </em> </strong>和<strong class="ks jc"> <em class="ln"> python。</em> </strong></p><pre class="mg mh mi mj gt mx my mz na aw nb bi"><span id="4ac9" class="nc lp jb my b gy nd ne l nf ng">exp.show_in_notebook(text=False)</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/c3bc8faececb1521acc2c8b809766ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dmz1pbkJX7Qdb8eUMmpldg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Figure 1</figcaption></figure><p id="8a03" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我试着解释一下这种形象化:</p><ul class=""><li id="2aaf" class="nl nm jb ks b kt ku kw kx kz nn ld no lh np ll nq nr ns nt bi translated">对于该文档，单词“sql”对于类别<strong class="ks jc"> <em class="ln"> sql </em> </strong>具有最高的正面得分。</li><li id="6869" class="nl nm jb ks b kt nu kw nv kz nw ld nx lh ny ll nq nr ns nt bi translated">我们的模型预测这个文档应该以 100%的概率被标记为<strong class="ks jc"> <em class="ln"> sql </em> </strong>。</li><li id="c3b9" class="nl nm jb ks b kt nu kw nv kz nw ld nx lh ny ll nq nr ns nt bi translated">如果我们将单词“sql”从文档中删除，我们会期望模型以 100% — 65% = 35%的概率预测标签<strong class="ks jc"> <em class="ln"> sql </em> </strong>。</li><li id="970a" class="nl nm jb ks b kt nu kw nv kz nw ld nx lh ny ll nq nr ns nt bi translated">另一方面，word“SQL”对于 class <strong class="ks jc"> <em class="ln"> python </em> </strong>是负数，我们的模型已经了解到 word“range”对于 class <strong class="ks jc"> <em class="ln"> python </em> </strong>有一个很小的正数分数。</li></ul><p id="d792" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们可能想要放大并研究对类<strong class="ks jc"> <em class="ln"> sql </em> </strong>的解释，以及文档本身。</p><pre class="mg mh mi mj gt mx my mz na aw nb bi"><span id="73ee" class="nc lp jb my b gy nd ne l nf ng">exp.show_in_notebook(text=y_test[idx], labels=(4,))</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/96f37dd47fc4179ef0cc7f55d981e961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tyw4-RKIbgcvUaOztpaS3Q.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Figure 2</figcaption></figure><h1 id="5644" class="lo lp jb bd lq lr ls lt lu lv lw lx ly kh lz ki ma kk mb kl mc kn md ko me mf bi translated">用 SHAP 解释文本预测</h1><p id="de04" class="pw-post-body-paragraph kq kr jb ks b kt mr kc kv kw ms kf ky kz mt lb lc ld mu lf lg lh mv lj lk ll ij bi translated">以下过程都是从<a class="ae lm" href="https://stackoverflow.blog/2019/05/06/predicting-stack-overflow-tags-with-googles-cloud-ai/" rel="noopener ugc nofollow" target="_blank">本教程</a>中学到的。</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">tf_model.py</figcaption></figure><ul class=""><li id="160e" class="nl nm jb ks b kt ku kw kx kz nn ld no lh np ll nq nr ns nt bi translated">在模型被训练后，我们使用前 200 个训练文档作为我们的背景数据集进行集成，并创建一个 SHAP 解释器对象。</li><li id="0844" class="nl nm jb ks b kt nu kw nv kz nw ld nx lh ny ll nq nr ns nt bi translated">我们在测试集的子集上得到个体预测的属性值。</li><li id="4256" class="nl nm jb ks b kt nu kw nv kz nw ld nx lh ny ll nq nr ns nt bi translated">将索引转换为单词。</li><li id="4424" class="nl nm jb ks b kt nu kw nv kz nw ld nx lh ny ll nq nr ns nt bi translated">使用 SHAP 的<code class="fe oa ob oc my b">summary_plot</code>方法显示影响模型预测的主要特征。</li></ul><pre class="mg mh mi mj gt mx my mz na aw nb bi"><span id="b4a7" class="nc lp jb my b gy nd ne l nf ng">attrib_data = X_train[:200]<br/>explainer = shap.DeepExplainer(model, attrib_data)<br/>num_explanations = 20<br/>shap_vals = explainer.shap_values(X_test[:num_explanations])</span><span id="5717" class="nc lp jb my b gy od ne l nf ng">words = processor._tokenizer.word_index<br/>word_lookup = list()<br/>for i in words.keys():<br/>  word_lookup.append(i)</span><span id="1aeb" class="nc lp jb my b gy od ne l nf ng">word_lookup = [''] + word_lookup<br/>shap.summary_plot(shap_vals, feature_names=word_lookup, class_names=tag_encoder.classes_)</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/be79f1dc652c1f110e40b129db6d59f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uO0u943JxOJlteKfWYr6Yg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Figure 3</figcaption></figure><ul class=""><li id="0984" class="nl nm jb ks b kt ku kw kx kz nn ld no lh np ll nq nr ns nt bi translated">单词“want”是我们的模型使用的最大信号词，对类<strong class="ks jc"> <em class="ln"> jquery </em> </strong>预测贡献最大。</li><li id="74c1" class="nl nm jb ks b kt nu kw nv kz nw ld nx lh ny ll nq nr ns nt bi translated">单词“php”是我们的模型使用的第四大信号词，当然对类<strong class="ks jc"> <em class="ln"> php </em> </strong>贡献最大。</li><li id="19ef" class="nl nm jb ks b kt nu kw nv kz nw ld nx lh ny ll nq nr ns nt bi translated">另一方面，单词“php”可能对另一个类产生负面信号，因为它不太可能出现在<strong class="ks jc"> <em class="ln"> python </em> </strong>文档中。</li></ul><p id="0de5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">就机器学习的可解释性而言，莱姆&amp; SHAP 有很多东西需要学习。我只介绍了 NLP 的一小部分。<a class="ae lm" href="https://github.com/susanli2016/NLP-with-Python/blob/master/LIME_SHAP_StackOverflow.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>可以在<a class="ae lm" href="https://github.com/susanli2016/NLP-with-Python/blob/master/LIME_SHAP_StackOverflow.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。享受乐趣！</p></div></div>    
</body>
</html>