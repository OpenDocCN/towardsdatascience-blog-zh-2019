# æ•°æ®ç§‘å­¦å®¶çš„æ–‡æœ¬é¢„å¤„ç†

> åŸæ–‡ï¼š<https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d?source=collection_archive---------12----------------------->

## æ–‡æœ¬é¢„å¤„ç†çš„ç®€ä¾¿æŒ‡å—

![](img/0da906e23fb47d9e68372812c704fd6e.png)

Image by [Devanath](https://pixabay.com/users/Devanath-1785462/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1248088) from [Pixabay](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1248088)

## æ–‡æœ¬é¢„å¤„ç†

æ–‡æœ¬é¢„å¤„ç†æ˜¯æ–‡æœ¬åˆ†æå’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„é‡è¦ä»»åŠ¡å’Œå…³é”®æ­¥éª¤ã€‚å®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºå¯é¢„æµ‹å’Œå¯åˆ†æçš„å½¢å¼ï¼Œä»¥ä¾¿æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥æ›´å¥½åœ°æ‰§è¡Œã€‚è¿™æ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„æ–‡æœ¬é¢„å¤„ç†æŒ‡å—ï¼Œæ˜¯æˆ‘ä¹‹å‰å…³äºæ–‡æœ¬æŒ–æ˜çš„åšå®¢çš„å»¶ç»­ã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†æ¥è‡ª Kaggle çš„ twitter æ•°æ®é›†ã€‚

æœ‰ä¸åŒçš„æ–¹æ³•æ¥é¢„å¤„ç†æ–‡æœ¬ã€‚è¿™é‡Œæœ‰ä¸€äº›ä½ åº”è¯¥çŸ¥é“çš„å¸¸ç”¨æ–¹æ³•ï¼Œæˆ‘ä¼šè¯•ç€å¼ºè°ƒæ¯ç§æ–¹æ³•çš„é‡è¦æ€§ã€‚

![](img/603a5cb5b86800366a2b1b035ed130c4.png)

Image by the author

## å¯†ç 

```
**#Importing necessary libraries**import numpy as np
import pandas as pd
import re
import nltk
import spacy
import string**# Reading the dataset**df = pd.read_csv("sample.csv")
df.head()
```

## è¾“å‡º

![](img/e42a59e89a05e4bc113e873f6888483d.png)

# ä¸‹éƒ¨å¤–å£³

è¿™æ˜¯æœ€å¸¸è§å’Œæœ€ç®€å•çš„æ–‡æœ¬é¢„å¤„ç†æŠ€æœ¯ã€‚é€‚ç”¨äºå¤§å¤šæ•°æ–‡æœ¬æŒ–æ˜å’Œ NLP é—®é¢˜ã€‚ä¸»è¦ç›®æ ‡æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ï¼Œä»¥ä¾¿â€œappleâ€ã€â€œAPPLEâ€å’Œâ€œAppleâ€å¾—åˆ°ç›¸åŒçš„å¤„ç†ã€‚

## å¯†ç 

```
**# Lower Casing --> creating new column called text_lower**df['text_lower']  = df['text'].str.lower()
df['text_lower'].head()
```

## è¾“å‡º

```
0    @applesupport causing the reply to be disregar...
1    @105835 your business means a lot to us. pleas...
2    @76328 i really hope you all change but i'm su...
3    @105836 livechat is online at the moment - htt...
4    @virgintrains see attached error message. i've...
Name: text_lower, dtype: object
```

# åˆ é™¤æ ‡ç‚¹ç¬¦å·

## å¯†ç 

```
**#removing punctuation, creating a new column called 'text_punct]'**
df['text_punct'] = df['text'].str.replace('[^\w\s]','')
df['text_punct'].head()
```

## è¾“å‡º

```
0    applesupport causing the reply to be disregard...
1    105835 your business means a lot to us please ...
2    76328 I really hope you all change but im sure...
3    105836 LiveChat is online at the moment  https...
4    virginTrains see attached error message Ive tr...
Name: text_punct, dtype: object
```

# åœç”¨è¯åˆ é™¤

åœç”¨è¯æ˜¯ä¸€ç§è¯­è¨€ä¸­çš„ä¸€ç»„å¸¸ç”¨è¯ã€‚è‹±è¯­ä¸­åœç”¨è¯çš„ä¾‹å­æœ‰â€œaâ€ã€â€œweâ€ã€â€œtheâ€ã€â€œisâ€ã€â€œareâ€ç­‰ã€‚ä½¿ç”¨åœç”¨è¯èƒŒåçš„æƒ³æ³•æ˜¯ï¼Œé€šè¿‡ä»æ–‡æœ¬ä¸­åˆ é™¤ä½ä¿¡æ¯é‡çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºé‡è¦çš„è¯ã€‚æˆ‘ä»¬å¯ä»¥è‡ªå·±åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„åœç”¨è¯åˆ—è¡¨(åŸºäºç”¨ä¾‹)ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é¢„å®šä¹‰çš„åº“ã€‚

## å¯†ç 

```
**#Importing stopwords from nltk library**
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))**# Function to remove the stopwords**
def stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])**# Applying the stopwords to 'text_punct' and store into 'text_stop'**
df["text_stop"] = df["text_punct"].apply(stopwords)
df["text_stop"].head()
```

## **è¾“å‡º**

```
0    appleSupport causing reply disregarded tapped ...
1    105835 your business means lot us please DM na...
2    76328 I really hope change Im sure wont becaus...
3    105836 LiveChat online moment httpstcoSY94VtU8...
4    virgintrains see attached error message Ive tr...
Name: text_stop, dtype: object
```

# å¸¸ç”¨è¯å»é™¤

æˆ‘ä»¬è¿˜å¯ä»¥ä»æ–‡æœ¬æ•°æ®ä¸­åˆ é™¤å¸¸è§çš„å•è¯ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ–‡æœ¬æ•°æ®ä¸­æœ€å¸¸å‡ºç°çš„ 10 ä¸ªå•è¯ã€‚

## å¯†ç 

```
**# Checking the first 10 most frequent words**
from collections import Counter
cnt = Counter()
for text in df["text_stop"].values:
    for word in text.split():
        cnt[word] += 1

cnt.most_common(10)
```

## è¾“å‡º

```
[('I', 34),
 ('us', 25),
 ('DM', 19),
 ('help', 17),
 ('httpstcoGDrqU22YpT', 12),
 ('AppleSupport', 11),
 ('Thanks', 11),
 ('phone', 9),
 ('Ive', 8),
 ('Hi', 8)]
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ é™¤ç»™å®šè¯­æ–™åº“ä¸­çš„å¸¸ç”¨è¯ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ tf-idfï¼Œè¿™å¯ä»¥è‡ªåŠ¨å¤„ç†

## å¯†ç 

```
**# Removing the frequent words**
freq = set([w for (w, wc) in cnt.most_common(10)])**# function to remove the frequent words**
def freqwords(text):
    return " ".join([word for word in str(text).split() if word not 
in freq])**# Passing the function freqwords**
df["text_common"] = df["text_stop"].apply(freqwords)
df["text_common"].head()
```

## è¾“å‡º

```
0    causing reply disregarded tapped notification ...
1    105835 Your business means lot please name zip...
2    76328 really hope change Im sure wont because ...
3    105836 LiveChat online moment httpstcoSY94VtU8...
4    virgintrains see attached error message tried ...
Name: text_common, dtype: object
```

# å»é™¤ç”Ÿåƒ»å­—

è¿™æ˜¯éå¸¸ç›´è§‚çš„ï¼Œå› ä¸ºå¯¹äºä¸åŒçš„ NLP ä»»åŠ¡ï¼Œä¸€äº›æœ¬è´¨ä¸Šéå¸¸ç‹¬ç‰¹çš„è¯ï¼Œå¦‚åç§°ã€å“ç‰Œã€äº§å“åç§°ï¼Œä»¥åŠä¸€äº›å¹²æ‰°å­—ç¬¦ï¼Œå¦‚ html çœç•¥ï¼Œä¹Ÿéœ€è¦è¢«åˆ é™¤ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨å•è¯çš„é•¿åº¦ä½œä¸ºæ ‡å‡†æ¥åˆ é™¤éå¸¸çŸ­æˆ–éå¸¸é•¿çš„å•è¯

## å¯†ç 

```
**# Removal of 10 rare words and store into new column called** 'text_rare'
freq = pd.Series(' '.join(df['text_common']).split()).value_counts()[-10:] # 10 rare words
freq = list(freq.index)
df['text_rare'] = df['text_common'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
df['text_rare'].head()
```

## è¾“å‡º

```
0    causing reply disregarded tapped notification ...
1    105835 Your business means lot please name zip...
2    76328 really hope change Im sure wont because ...
3    105836 liveChat online moment httpstcoSY94VtU8...
4    virgintrains see attached error message tried ...
Name: text_rare, dtype: object
```

# æ‹¼å†™çº æ­£

ç¤¾äº¤åª’ä½“æ•°æ®æ€»æ˜¯æ‚ä¹±çš„æ•°æ®ï¼Œè€Œä¸”æœ‰æ‹¼å†™é”™è¯¯ã€‚å› æ­¤ï¼Œæ‹¼å†™çº æ­£æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå› ä¸ºè¿™å°†å¸®åŠ©æˆ‘ä»¬é¿å…å¤šä¸ªå•è¯ã€‚ä¾‹å¦‚ï¼Œâ€œtextâ€å’Œâ€œtxtâ€å°†è¢«è§†ä¸ºä¸åŒçš„å•è¯ï¼Œå³ä½¿å®ƒä»¬åœ¨ç›¸åŒçš„æ„ä¹‰ä¸Šä½¿ç”¨ã€‚è¿™å¯ä»¥é€šè¿‡ textblob åº“æ¥å®Œæˆ

## **ä»£å·**

```
**# Spell check using text blob for the first 5 records**
from textblob import TextBlob
df['text_rare'][:5].apply(lambda x: str(TextBlob(x).correct()))
```

## è¾“å‡º

![](img/a1d85bd21ffa329b780f90c466311c07.png)

# è¡¨æƒ…ç¬¦å·ç§»é™¤

è¡¨æƒ…ç¬¦å·æ˜¯æˆ‘ä»¬ç”Ÿæ´»çš„ä¸€éƒ¨åˆ†ã€‚ç¤¾äº¤åª’ä½“æ–‡å­—æœ‰å¾ˆå¤šè¡¨æƒ…ç¬¦å·ã€‚æˆ‘ä»¬éœ€è¦åœ¨æ–‡æœ¬åˆ†æä¸­åˆ é™¤ç›¸åŒçš„å†…å®¹

## å¯†ç 

ä»£ç å‚è€ƒ: [Github](https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b)

```
**# Function to remove emoji.**
def emoji(string):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)emoji("Hi, I am Emoji  ğŸ˜œ")
**#passing the emoji function to 'text_rare'**
df['text_rare'] = df['text_rare'].apply(remove_emoji)
```

## è¾“å‡º

```
'Hi, I am Emoji  '
```

# è¡¨æƒ…ç§»é™¤

åœ¨å‰é¢çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å·²ç»åˆ é™¤äº†è¡¨æƒ…ç¬¦å·ã€‚ç°åœ¨ï¼Œæˆ‘è¦ç§»é™¤è¡¨æƒ…ç¬¦å·ã€‚è¡¨æƒ…ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ:-)æ˜¯ä¸€ä¸ªè¡¨æƒ…ç¬¦å·ğŸ˜œâ†’è¡¨æƒ…ç¬¦å·ã€‚

ä½¿ç”¨ emot åº“ã€‚è¯·å‚è€ƒæ›´å¤šå…³äº[è¡¨æƒ…](https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py)

## å¯†ç 

```
from emot.emo_unicode import UNICODE_EMO, EMOTICONS**# Function for removing emoticons**
def remove_emoticons(text):
    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')
    return emoticon_pattern.sub(r'', text)remove_emoticons("Hello :-)")
**# applying remove_emoticons to 'text_rare'**
df['text_rare'] = df['text_rare'].apply(remove_emoticons)
```

## è¾“å‡º

```
'Hello '
```

# å°†è¡¨æƒ…ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·è½¬æ¢ä¸ºæ–‡å­—

åœ¨æƒ…æ„Ÿåˆ†æä¸­ï¼Œè¡¨æƒ…ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·è¡¨è¾¾äº†ä¸€ç§æƒ…æ„Ÿã€‚å› æ­¤ï¼Œåˆ é™¤å®ƒä»¬å¯èƒ½ä¸æ˜¯ä¸€ä¸ªå¥½çš„è§£å†³æ–¹æ¡ˆã€‚

## å¯†ç 

```
from emot.emo_unicode import UNICODE_EMO, EMOTICONS**# Converting emojis to words**
def convert_emojis(text):
    for emot in UNICODE_EMO:
        text = text.replace(emot, "_".join(UNICODE_EMO[emot].replace(",","").replace(":","").split()))
        return text**# Converting emoticons to words **   
def convert_emoticons(text):
    for emot in EMOTICONS:
        text = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), text)
        return text**# Example**
text = "Hello :-) :-)"
convert_emoticons(text)text1 = "Hilarious ğŸ˜‚"
convert_emojis(text1)**# Passing both functions to 'text_rare'**
df['text_rare'] = df['text_rare'].apply(convert_emoticons)
df['text_rare'] = df['text_rare'].apply(convert_emojis)
```

## **è¾“å‡º**

```
'Hello happy smiley face happy smiley face:-)'
'Hilarious face_with_tears_of_joy'
```

# ç§»é™¤ URL

åˆ é™¤æ–‡æœ¬ä¸­çš„ URLã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¼‚äº®çš„æ±¤åº“

## å¯†ç 

```
**# Function for url's**
def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)**# Examples**
text = "This is my website, [https://www.abc.com](https://www.abc.com)"
remove_urls(text)**#Passing the function to 'text_rare'**
df['text_rare'] = df['text_rare'].apply(remove_urls)
```

## è¾“å‡º

```
'This is my website, '
```

# ç§»é™¤ HTML æ ‡ç­¾

å¦ä¸€ç§å¸¸è§çš„é¢„å¤„ç†æŠ€æœ¯æ˜¯åˆ é™¤ HTML æ ‡ç­¾ã€‚é€šå¸¸å‡ºç°åœ¨æŠ“å–æ•°æ®ä¸­çš„ HTML æ ‡ç­¾ã€‚

## å¯†ç 

```
from bs4 import BeautifulSoup**#Function for removing html**
def html(text):
    return BeautifulSoup(text, "lxml").text
**# Examples**
text = """<div>
<h1> This</h1>
<p> is</p>
<a href="[https://www.abc.com/](https://www.abc.com/)"> ABCD</a>
</div>
"""
print(html(text))
**# Passing the function to 'text_rare'**
df['text_rare'] = df['text_rare'].apply(html)
```

## è¾“å‡º

```
This
 is
 ABCD
```

# æ ‡è®°åŒ–

æ ‡è®°åŒ–æ˜¯æŒ‡å°†æ–‡æœ¬åˆ†æˆä¸€ç³»åˆ—å•è¯æˆ–å¥å­ã€‚

## å¯†ç 

```
**#Creating function for tokenization**
def tokenization(text):
    text = re.split('\W+', text)
    return text
**# Passing the function to 'text_rare' and store into'text_token'**
df['text_token'] = df['text_rare'].apply(lambda x: tokenization(x.lower()))
df[['text_token']].head()
```

## è¾“å‡º

![](img/e5243e44cf7a3c124064bc31c72fc623.png)

# è¯å¹²åŒ–å’Œè¯æ±‡åŒ–

è¯æ±‡åŒ–æ˜¯å°†ä¸€ä¸ªè¯è½¬æ¢æˆå®ƒçš„åŸºæœ¬å½¢å¼çš„è¿‡ç¨‹ã€‚è¯å¹²åŒ–å’Œè¯å…ƒåŒ–çš„åŒºåˆ«åœ¨äºï¼Œè¯å…ƒåŒ–è€ƒè™‘ä¸Šä¸‹æ–‡å¹¶å°†å•è¯è½¬æ¢ä¸ºå…¶æœ‰æ„ä¹‰çš„åŸºæœ¬å½¢å¼ï¼Œè€Œè¯å¹²åŒ–åªæ˜¯åˆ é™¤æœ€åå‡ ä¸ªå­—ç¬¦ï¼Œé€šå¸¸ä¼šå¯¼è‡´ä¸æ­£ç¡®çš„æ„æ€å’Œæ‹¼å†™é”™è¯¯ã€‚è¿™é‡Œï¼Œä»…æ‰§è¡Œäº†æœ¯è¯­åŒ–ã€‚æˆ‘ä»¬éœ€è¦ä¸º NLTK ä¸­çš„ lemmatizer æä¾›å•è¯çš„ POS æ ‡ç­¾ã€‚æ ¹æ®ä½ç½®çš„ä¸åŒï¼Œlemmatizer å¯èƒ½ä¼šè¿”å›ä¸åŒçš„ç»“æœã€‚

## å¯†ç 

```
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer()
wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV} # Pos tag, used Noun, Verb, Adjective and Adverb**# Function for lemmatization using POS tag**
def lemmatize_words(text):
    pos_tagged_text = nltk.pos_tag(text.split())
    return " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])**# Passing the function to 'text_rare' and store in 'text_lemma'**
df["text_lemma"] = df["text_rare"].apply(lemmatize_words)
```

## è¾“å‡º

![](img/891a78c8aabf0c0552f7547f99a5bdaf.png)

ä»¥ä¸Šæ–¹æ³•æ˜¯å¸¸è§çš„æ–‡æœ¬é¢„å¤„ç†æ­¥éª¤ã€‚

æ„Ÿè°¢é˜…è¯»ã€‚è¯·ç»§ç»­å­¦ä¹ ï¼Œå¹¶å…³æ³¨æ›´å¤šå†…å®¹ï¼

# å‚è€ƒ:

1.  ã€https://www.nltk.org 
2.  [https://www.edureka.co](https://www.geeksforgeeks.org/nlp-chunk-tree-to-text-and-chaining-chunk-transformation/)
3.  [https://www . geeks forgeeks . org/part-speech-tagging-stop-words-using-nltk-python/](https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/)