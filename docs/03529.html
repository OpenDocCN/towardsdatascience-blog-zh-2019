<html>
<head>
<title>Hyper-Parameter Tuning and Model Selection, Like a Movie Star</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调整和模型选择，就像电影明星一样</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyper-parameter-tuning-and-model-selection-like-a-movie-star-a884b8ee8d68?source=collection_archive---------13-----------------------#2019-06-05">https://towardsdatascience.com/hyper-parameter-tuning-and-model-selection-like-a-movie-star-a884b8ee8d68?source=collection_archive---------13-----------------------#2019-06-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9cc598f0cb2f90de3e6886f4cb7f1c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*B0GL7035-Zbuh1P_"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="04ed" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">像你一样编码、分析、选择和调整<em class="ky">真的</em>知道你在做什么。</h2></div><p id="d11f" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“针对随机森林分类器优化的超参数调整”是那些在电影场景中听起来很轻松的短语之一，在电影场景中，黑客正在积极地输入以“获得对大型机的访问”，就像在关于数据科学的媒体文章中一样。然而，事实是，这样的短语是将数学和计算概念结合在一个领域的不幸结果，更糟糕的是，是一个名字。虽然本文中的概念将受益于对使用 scikit-learn 的基本 python 建模以及其中一些模型如何工作的扎实理解，但我将尝试自下而上地解释一切，以便所有级别的读者都可以享受和学习这些概念；你也可以听起来(和编码)像好莱坞黑客。</p><p id="7aa9" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将尝试解决以下问题:</p><ul class=""><li id="1f07" class="lv lw jj lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">什么是超参数，它与参数有何不同？</li><li id="b7cf" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">什么时候应该使用超参数？</li><li id="d34d" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">超参数实际上是做什么的？</li><li id="165a" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如何调整超参数？</li><li id="0dac" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">什么是网格搜索？</li><li id="03fd" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">什么是流水线？</li><li id="f92d" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如何定义单个超参数？</li></ul><p id="064a" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">跳到最后，查看所有这些主题的摘要。</p><h1 id="d161" class="mj mk jj bd ml mm mn mo mp mq mr ms mt kp mu kq mv ks mw kt mx kv my kw mz na bi translated">什么是超参数？</h1><p id="d92a" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">超参数这个术语是由于机器学习在编程和大数据中日益流行而产生的。许多作为数据科学家或程序员开始其旅程的人都知道参数这个词被定义为一个值，该值被传递到一个函数中，使得该函数对这些值执行操作和/或被这些值通知。然而，在机器学习和建模中，参数<strong class="lb jk">不是由程序员<strong class="lb jk">输入的</strong>，而是由机器学习模型</strong>开发的。这是由于机器学习和传统编程的根本区别；在传统编程中，规则和数据由程序员输入以便输出结果，而在机器学习中，输入数据和结果以便输出规则(在这种情况下通常称为参数)。这个<a class="ae jg" href="https://youtu.be/VwVg9jCtqaU?t=111" rel="noopener ugc nofollow" target="_blank"> Google I/O 2019 演讲</a>在最初几分钟非常简洁地解决了这个翻转。</p><p id="84dd" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果模型本身生成参数，那么将我们(程序员、数据科学家等)输入的内容也称为模型参数会非常混乱。这就是超参数这个术语的诞生。超参数被输入到生成其自身参数的任何机器学习模型中，以便影响所述生成的参数的值，希望使模型更加精确。在本文的稍后部分，我将展示具体的例子，以及定义什么是单个的超参数。</p><h1 id="7daa" class="mj mk jj bd ml mm mn mo mp mq mr ms mt kp mu kq mv ks mw kt mx kv my kw mz na bi translated">这些单独的超参数是如何定义的，它们有什么影响？</h1><p id="a3de" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">让我们快速浏览一下<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn 关于逻辑回归的文档</a>，以便更好地理解这个问题的真正含义。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="6439" class="np mk jj nl b gy nq nr l ns nt"><strong class="nl jk">LogisticRegression</strong>(<em class="nu">penalty=’l2’</em>, <em class="nu">dual=False</em>, <em class="nu">tol=0.0001</em>, <em class="nu">C=1.0</em>, <em class="nu">fit_intercept=True</em>, <em class="nu">intercept_scaling=1</em>, <em class="nu">class_weight=None</em>, <em class="nu">random_state=None</em>, <em class="nu">solver=’warn’</em>, <em class="nu">max_iter=100</em>, <em class="nu">multi_class=’warn’</em>, <em class="nu">verbose=0</em>, <em class="nu">warm_start=False</em>, <em class="nu">n_jobs=None</em>, <em class="nu">l1_ratio=None</em>)</span></pre><p id="f561" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在这里看到的，<code class="fe nv nw nx nl b">LogisticRegression()</code>接受了 15 个不同的值，我们现在知道这些值被称为超参数。然而，这 15 个值中的每一个都定义了一个默认值，这意味着在没有指定任何超参数的情况下创建一个<code class="fe nv nw nx nl b">LogisticRegression()</code>对象是非常可能的，甚至是常见的。这是 scikit-learn 中所有模型的情况。因此，我将只花时间来定义和解释四种常见建模方法的一些更相关和通常修改的超参数。</p><h1 id="ee24" class="mj mk jj bd ml mm mn mo mp mq mr ms mt kp mu kq mv ks mw kt mx kv my kw mz na bi translated">逻辑回归:</h1><ul class=""><li id="fb42" class="lv lw jj lb b lc nb lf nc li ny lm nz lq oa lu ma mb mc md bi translated"><strong class="lb jk">惩罚</strong>:用于指定对非贡献变量系数的惩罚方式。</li><li id="7da2" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">Lasso (L1)执行要素选择，因为它将不太重要的要素的系数缩小到零。</li><li id="a6ae" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">里奇(L2)所有的变量都包括在模型中，尽管有些被缩小了。计算强度低于 lasso。</li><li id="a24d" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">两个罚值都限制解算器的选择，如这里的<a class="ae jg" href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" rel="noopener ugc nofollow" target="_blank">所示</a>。</li><li id="f5aa" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb jk"> C </strong>:是正则项的逆(1/λ)。它告诉模型有多大的参数被惩罚，较小的值导致较大的惩罚；必须是正浮点数。</li><li id="18f3" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">常用值:[0.001，0.1 …10..100]</li><li id="2a53" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb jk"> class_weight </strong>:允许你更强调一个类。例如，如果类别 1 和类别 2 之间的分布严重不平衡，则模型可以适当地处理这两种分布。</li><li id="d5ba" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">默认值是所有权重= 1。类别权重可以在字典中指定。</li><li id="f5af" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">“平衡”将创建与类别频率成反比的类别权重，给予较小类别的个别事件更多权重。</li></ul><h1 id="6dc0" class="mj mk jj bd ml mm mn mo mp mq mr ms mt kp mu kq mv ks mw kt mx kv my kw mz na bi translated">线性回归:</h1><ul class=""><li id="1956" class="lv lw jj lb b lc nb lf nc li ny lm nz lq oa lu ma mb mc md bi translated"><strong class="lb jk">拟合截距</strong>:指定是否计算模型截距或设置为零。</li><li id="3b5b" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如果为假，回归线的截距将为 0。</li><li id="9665" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如果为真，模型将计算截距。</li><li id="8b20" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb jk">规格化:</strong>指定是否使用 L2 范数规格化模型的数据。</li></ul><h1 id="a082" class="mj mk jj bd ml mm mn mo mp mq mr ms mt kp mu kq mv ks mw kt mx kv my kw mz na bi translated">SVM</h1><ul class=""><li id="88d1" class="lv lw jj lb b lc nb lf nc li ny lm nz lq oa lu ma mb mc md bi translated"><strong class="lb jk"> C </strong>:是正则项的逆(1/λ)。它告诉模型有多大的参数被惩罚，较小的值导致较大的惩罚；必须是正浮点数。</li><li id="b4b9" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">较高的 C 将导致模型的错误分类较少，但更有可能导致过度拟合。</li><li id="56f8" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">良好的值范围:[0.001，0.01，10，100，1000…]</li><li id="2e6d" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb jk"> class_weight </strong>:将 class i 的参数设置为 class_weight[i] *C。</li><li id="65a3" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">这可以让你更加重视一门课。例如，如果类别 1 和类别 2 之间的分布严重不平衡，则模型可以适当地处理这两种分布。</li><li id="4651" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">默认值是所有权重= 1。类别权重可以在字典中指定。</li><li id="8c0b" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">“平衡”将创建与类别频率成反比的类别权重，给予较小类别的个别事件更多权重。</li></ul><h1 id="73bf" class="mj mk jj bd ml mm mn mo mp mq mr ms mt kp mu kq mv ks mw kt mx kv my kw mz na bi translated">k-最近邻</h1><ul class=""><li id="6819" class="lv lw jj lb b lc nb lf nc li ny lm nz lq oa lu ma mb mc md bi translated"><strong class="lb jk"> n_neighbors </strong>:确定计算最近邻算法时使用的邻居数量。</li><li id="244e" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">良好的值范围:[2，4，8，16]</li><li id="1ff6" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb jk"> p </strong>:计算闵可夫斯基度规时的功率度规，这是一个数学上相当复杂的话题。在评估模型时，简单地尝试这里的 1 和 2 通常就足够了。</li><li id="3a3f" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用值 1 计算曼哈顿距离</li><li id="9075" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用值 2 计算欧几里德距离(默认)</li></ul><h1 id="0e5b" class="mj mk jj bd ml mm mn mo mp mq mr ms mt kp mu kq mv ks mw kt mx kv my kw mz na bi translated">随机森林</h1><ul class=""><li id="c26a" class="lv lw jj lb b lc nb lf nc li ny lm nz lq oa lu ma mb mc md bi translated"><strong class="lb jk"> n_estimators </strong>:设置要在林中使用的决策树的数量。</li><li id="d489" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">默认值为 100</li><li id="f240" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">良好的值范围:[100，120，300，500，800，1200]</li><li id="80e3" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb jk"> max_depth </strong>:设置树的最大深度。</li><li id="a041" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如果未设置，则没有上限。这棵树会一直生长，直到所有的叶子都变得纯净。</li><li id="2df8" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">限制深度有利于修剪树，以防止对噪声数据的过度拟合。</li><li id="defa" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">良好的值范围:[5，8，15，25，30，无]</li><li id="e97d" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb jk"> min_samples_split </strong>:在内部节点进行分割(微分)之前所需的最小样本数</li><li id="1920" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">默认值为 2</li><li id="366d" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">良好的值范围:[1，2，5，10，15，100]</li><li id="1a10" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb jk"> min_samples_leaf </strong>:创建叶(决策)节点所需的最小样本数。</li><li id="a3e9" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">默认值为 1。这意味着，仅当每条路径至少有 1 个样本时，才允许在任何深度的分割点。</li><li id="ad9b" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">良好的值范围:[1，2，5，10]</li><li id="c654" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb jk"> max_features </strong>:设置考虑最佳节点分割的特征数量</li><li id="c5ef" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">默认为“自动”，这意味着特征数量的平方根用于树中的每次分割。</li><li id="19db" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">“无”表示所有特征都用于每次分割。</li><li id="c9d8" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">随机森林中的每个决策树通常使用随机的特征子集进行分割。</li><li id="af4a" class="lv lw jj lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">良好的值范围:[log2，sqrt，auto，None]</li></ul><h1 id="2eef" class="mj mk jj bd ml mm mn mo mp mq mr ms mt kp mu kq mv ks mw kt mx kv my kw mz na bi translated">如何调整超参数，它们实际上有什么作用？</h1><p id="41f2" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">为了弄清楚这两个问题，让我们用经典的加州大学欧文分校虹膜数据集来解决一个例子。</p><p id="0595" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将加载数据集并导入我们将使用的一些包:</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="ff4c" class="np mk jj nl b gy nq nr l ns nt"># import packages<br/>import numpy as np<br/>from sklearn import linear_model, datasets<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.linear_model import LogisticRegression <br/>from sklearn.ensemble import RandomForestClassifier <br/>from sklearn.model_selection import GridSearchCV <br/>from sklearn.pipeline import Pipeline</span><span id="9235" class="np mk jj nl b gy ob nr l ns nt"># Loading dataset<br/>iris = datasets.load_iris()<br/>features = iris.data<br/>target = iris.target</span></pre><p id="a0f6" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们创建一个快速模型，不使用额外的超参数，并获得分数供以后评估。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="ea86" class="np mk jj nl b gy nq nr l ns nt">logistic.fit(features, target)<br/>print(logistic.score(features, target))</span></pre><p id="9903" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出:</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="4abc" class="np mk jj nl b gy nq nr l ns nt">0.96</span></pre><p id="f482" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们尝试一些超参数调整的方法，看看我们是否可以提高我们的模型的准确性。</p><h2 id="2cbe" class="np mk jj bd ml oc od dn mp oe of dp mt li og oh mv lm oi oj mx lq ok ol mz om bi translated">什么是网格搜索？</h2><p id="f266" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">网格搜索是一种方法，通过这种方法，我们为每个超参数创建可能的超参数值集，然后在“网格”中相互测试它们例如，如果我想用值<code class="fe nv nw nx nl b">[L1, L2]</code>和值 C 作为<code class="fe nv nw nx nl b">[1,2]</code>来测试一个逻辑回归，<code class="fe nv nw nx nl b">GridSearchCV()</code>方法会用<code class="fe nv nw nx nl b">C=1</code>测试<code class="fe nv nw nx nl b">L1</code>，然后用<code class="fe nv nw nx nl b">C=2</code>测试<code class="fe nv nw nx nl b">L1</code>，然后用两个值<code class="fe nv nw nx nl b">C</code>测试<code class="fe nv nw nx nl b">L2</code>，创建一个 2x2 的网格和总共四个组合。让我们看一个没有当前数据集的例子。verbose 参数指示函数运行时是否打印信息，cv 参数指的是<a class="ae jg" rel="noopener" target="_blank" href="/cross-validation-a-beginners-guide-5b8ca04962cd">交叉验证</a>折叠。关于<code class="fe nv nw nx nl b">GridSearchCV()</code>的完整文档可以在这里找到<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="7da3" class="np mk jj nl b gy nq nr l ns nt"># Create range of candidate penalty hyperparameter values<br/>penalty = ['l1', 'l2']</span><span id="d9e8" class="np mk jj nl b gy ob nr l ns nt"># Create range of candidate regularization hyperparameter values C<br/># Choose 10 values, between 0 and 4<br/>C = np.logspace(0, 4, 10)</span><span id="d07e" class="np mk jj nl b gy ob nr l ns nt"># Create dictionary hyperparameter candidates<br/>hyperparameters = dict(C=C, penalty=penalty)</span><span id="373f" class="np mk jj nl b gy ob nr l ns nt"># Create grid search, and pass in all defined values<br/>gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=1) <br/># the verbose parameter above will give output updates as the calculations are complete. </span><span id="e67b" class="np mk jj nl b gy ob nr l ns nt"># select the best model and create a fit<br/>best_model = gridsearch.fit(features, target)</span></pre><p id="5d7e" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们的模型是基于更大的输入空间创建的，我们可以希望看到改进。让我们检查一下:</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="c981" class="np mk jj nl b gy nq nr l ns nt">print('Best Penalty:', best_model.best_estimator_.get_params(['penalty']) <br/>print('Best C:', best_model.best_estimator_.get_params()['C'])<br/>print("The mean accuracy of the model is:",best_model.score(features, target))</span></pre><p id="778a" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出:</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="5e0a" class="np mk jj nl b gy nq nr l ns nt">Best Penalty: l1 <br/>Best C: 7.742636826811269<br/>The mean accuracy of the model is: 0.98</span></pre><p id="172f" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用相同的模型并增加超参数的小变化，精度提高了 0.02。尝试用不同的超参数集进行试验，并将它们添加到超参数字典中，然后再次运行<code class="fe nv nw nx nl b">GridSearchCV()</code>。请注意添加许多超参数如何快速增加计算时间。</p><h2 id="6bee" class="np mk jj bd ml oc od dn mp oe of dp mt li og oh mv lm oi oj mx lq ok ol mz om bi translated">什么是流水线？</h2><p id="f32b" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">如果我们想要用多个超参数测试多个算法，以便找到可能的最佳模型，该怎么办？流水线允许我们以一种代码高效的方式做到这一点。让我们看一个 Iris 数据集的例子，看看我们是否可以改进我们的逻辑回归模型。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="b6ae" class="np mk jj nl b gy nq nr l ns nt"># Create a pipeline<br/>pipe = Pipeline([("classifier", RandomForestClassifier())])</span><span id="df5d" class="np mk jj nl b gy ob nr l ns nt"># Create dictionary with candidate learning algorithms and their hyperparameters<br/>search_space = [<br/>                {"classifier": [LogisticRegression()],<br/>                 "classifier__penalty": ['l2','l1'],<br/>                 "classifier__C": np.logspace(0, 4, 10)<br/>                 },<br/>                {"classifier": [LogisticRegression()],<br/>                 "classifier__penalty": ['l2'],<br/>                 "classifier__C": np.logspace(0, 4, 10),<br/>                 "classifier__solver":['newton-cg','saga','sag','liblinear'] ##This solvers don't allow L1 penalty<br/>                 },<br/>                {"classifier": [RandomForestClassifier()],<br/>                 "classifier__n_estimators": [10, 100, 1000],<br/>                 "classifier__max_depth":[5,8,15,25,30,None],<br/>                 "classifier__min_samples_leaf":[1,2,5,10,15,100],<br/>                 "classifier__max_leaf_nodes": [2, 5,10]}]</span><span id="eaa8" class="np mk jj nl b gy ob nr l ns nt"># create a gridsearch of the pipeline, the fit the best model<br/>gridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0,n_jobs=-1) # Fit grid search<br/>best_model = gridsearch.fit(features, target)</span></pre><p id="336e" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意这个函数运行需要多长时间。在另一篇文章中，我将讨论如何减少运行时间和挑选有效的超参数，以及如何将一个<code class="fe nv nw nx nl b">RandomizedSearchCV()</code>和一个<code class="fe nv nw nx nl b">GridSearchCV</code>结合起来。运行该方法后，让我们检查结果。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="04b9" class="np mk jj nl b gy nq nr l ns nt">print(best_model.best_estimator_)<br/>print("The mean accuracy of the model is:",best_model.score(features, target))</span></pre><p id="fc95" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出:</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="37f1" class="np mk jj nl b gy nq nr l ns nt">Pipeline(memory=None, steps=[('classifier', LogisticRegression(C=7.742636826811269, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, l1_ratio=None,                                     max_iter=100, multi_class='warn', n_jobs=None, penalty='l1',                                     random_state=None, solver='warn', tol=0.0001, verbose=0, warm_start=False))], verbose=False) </span><span id="3087" class="np mk jj nl b gy ob nr l ns nt">The mean accuracy of the model is: 0.98</span></pre><p id="cbe4" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据我们的管道搜索，具有指定超参数的<code class="fe nv nw nx nl b">LogisticRegression()</code>比具有任何给定超参数的<code class="fe nv nw nx nl b">RandomForestClassifier()</code>执行得更好。有意思！</p><p id="3346" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，我们已经使用了一个管道方法来实现这一切，但是它实际上做什么，为什么我们要传入一个<code class="fe nv nw nx nl b">RandomForestClassifier()</code>？</p><p id="98d8" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">pipeline 方法允许我们传入预处理方法以及我们想要用来创建数据模型的算法。在这个简单的例子中，我们跳过了预处理步骤，但是我们仍然输入了一个模型。我们输入的算法只是用于实例化管道对象的算法，但是将被我们创建的<code class="fe nv nw nx nl b">search_space</code>变量的内容所替代，该变量稍后将被传递到我们的<code class="fe nv nw nx nl b">GridSearchCV()</code>中。这里可以找到一个简化的帖子，只关注管道<a class="ae jg" rel="noopener" target="_blank" href="/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976">。</a></p><p id="dbaa" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的原始基线模型和用我们的超参数调整生成的模型之间的精度差异显示了超参数调整的效果。通过指导我们的机器学习模型的创建，我们可以提高它们的性能，并创建更好、更可靠的模型。</p><h1 id="ff4f" class="mj mk jj bd ml mm mn mo mp mq mr ms mt kp mu kq mv ks mw kt mx kv my kw mz na bi translated">摘要</h1><h2 id="1daa" class="np mk jj bd ml oc od dn mp oe of dp mt li og oh mv lm oi oj mx lq ok ol mz om bi translated">什么是超参数，它与参数有何不同？</h2><p id="4291" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在机器学习模型中使用超参数来更好地指导模型用来生成数据预测的参数的创建。超参数由程序员设置，而参数由模型生成。</p><h2 id="e642" class="np mk jj bd ml oc od dn mp oe of dp mt li og oh mv lm oi oj mx lq ok ol mz om bi translated">什么时候应该使用超参数？</h2><p id="b507" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">永远！模型通常有内置的默认超参数，可用于大多数目的。然而，在许多情况下，使用超参数调优会挤出模型的额外性能。了解不同超参数的限制和影响有助于限制过度拟合等负面影响，同时提高性能。</p><h2 id="93f5" class="np mk jj bd ml oc od dn mp oe of dp mt li og oh mv lm oi oj mx lq ok ol mz om bi translated">超参数实际上是做什么的？</h2><p id="698d" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">简单地说，它们改变了模型寻找模型参数的方式。个别定义可以在上面的文章中找到。</p><h2 id="2130" class="np mk jj bd ml oc od dn mp oe of dp mt li og oh mv lm oi oj mx lq ok ol mz om bi translated">如何调整超参数？</h2><p id="02cc" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">网格搜索、随机搜索和流水线是常用的方法。这篇文章没有提到随机搜索，但是你可以在这里阅读更多的<a class="ae jg" href="https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="bf61" class="np mk jj bd ml oc od dn mp oe of dp mt li og oh mv lm oi oj mx lq ok ol mz om bi translated">什么是网格搜索？</h2><p id="6a0a" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">网格搜索是对传递给<code class="fe nv nw nx nl b">GridSearchCV()</code>函数的所有超参数的元素测试。网格搜索在大搜索空间上的计算开销很大，它的测试也很详尽。</p><h2 id="9b35" class="np mk jj bd ml oc od dn mp oe of dp mt li og oh mv lm oi oj mx lq ok ol mz om bi translated">什么是流水线？</h2><p id="e6c3" class="pw-post-body-paragraph kz la jj lb b lc nb kk le lf nc kn lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">流水线允许搜索多个算法，每个算法都有许多超参数。这是一种非常高效的测试许多模型的方法，以便选择最好的一个。此外，它还可以处理再加工方法，允许进一步控制过程。</p><p id="f824" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，下面是一些函数，它们可以通过传入参数来执行一些不同类型的超参数调整。包含本文中使用的所有代码的 Google Colab 笔记本也可以在这里找到。使用这些函数，您可以在一行代码中高效地执行超参数调整！</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="3a30" class="np mk jj nl b gy nq nr l ns nt"># # # Hyperparameter tuning and model selection<br/>import numpy as np<br/>from sklearn import linear_model<br/>from sklearn import datasets<br/>from sklearn.linear_model import LogisticRegression <br/>from sklearn.ensemble import RandomForestClassifier <br/>from sklearn.model_selection import GridSearchCV <br/>from sklearn.pipeline import Pipeline<br/>from sklearn.model_selection import RandomizedSearchCV<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.ensemble import RandomForestRegressor</span><span id="e62d" class="np mk jj nl b gy ob nr l ns nt">def perform_gridsearch_log(features, labels,<br/>                       log_params = {'penalty': ['l1', 'l2'], 'C': np.logspace(0, 4, 10)},<br/>                       cv=5, verbose = 1):<br/>  import numpy as np<br/>  from sklearn import linear_model, datasets<br/>  from sklearn.model_selection import GridSearchCV<br/>  <br/>  global best_model<br/>  logistic = linear_model.LogisticRegression()<br/>  penalty = log_params['penalty']<br/>  C = log_params['C']<br/>  hyperparameters = dict(C=C, penalty=penalty)</span><span id="a596" class="np mk jj nl b gy ob nr l ns nt">gridsearch = GridSearchCV(logistic, hyperparameters, cv=cv, verbose=verbose) # Fit grid search<br/>  best_model = gridsearch.fit(features, target)<br/>  <br/>  print(best_model.best_estimator_)<br/>  print("The mean accuracy of the model is:",best_model.score(features, labels))</span><span id="508e" class="np mk jj nl b gy ob nr l ns nt">def rand_forest_rand_grid(features, labels, n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],<br/>                                           max_features = ['auto', 'sqrt'],<br/>                                           max_depth = [int(x) for x in np.linspace(10, 110, num = 11)],<br/>                                           min_samples_split = [2, 5, 10],<br/>                                           min_samples_leaf = [1, 2, 4], bootstrap = [True, False]):<br/>  <br/>  max_depth.append(None)<br/>  global best_model<br/> <br/>  random_grid = {'n_estimators': n_estimators,<br/>                 'max_features': max_features,<br/>                 'max_depth': max_depth,<br/>                 'min_samples_split': min_samples_split,<br/>                 'min_samples_leaf': min_samples_leaf,<br/>                 'bootstrap': bootstrap}<br/>  <br/>  rf = RandomForestRegressor()<br/>  <br/>  rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=42, n_jobs = -1)<br/>  <br/>  best_model = rf_random.fit(features, labels)<br/>  print(best_model.best_estimator_)<br/>  print("The mean accuracy of the model is:",best_model.score(features, labels))</span><span id="bfe8" class="np mk jj nl b gy ob nr l ns nt">def rand_forest_grid_search(features, labels, n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],<br/>                                           max_features = ['auto', 'sqrt'],<br/>                                           max_depth = [int(x) for x in np.linspace(10, 110, num = 11)],<br/>                                           min_samples_split = [2, 5, 10],<br/>                                           min_samples_leaf = [1, 2, 4], bootstrap = [True, False]):<br/>  param_grid = {'n_estimators': n_estimators,<br/>                 'max_features': max_features,<br/>                 'max_depth': max_depth,<br/>                 'min_samples_split': min_samples_split,<br/>                 'min_samples_leaf': min_samples_leaf,<br/>                 'bootstrap': bootstrap}<br/>  <br/>  global best_model<br/>  rf = RandomForestRegressor()<br/>  <br/>  grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, <br/>                          cv = 3, n_jobs = -1, verbose = 1)</span><span id="ad7c" class="np mk jj nl b gy ob nr l ns nt">best_model = grid_search.fit(train_features, train_labels)<br/>  print(best_model.best_estimator_)<br/>  print("The mean accuracy of the model is:",best_model.score(features, labels))</span><span id="4966" class="np mk jj nl b gy ob nr l ns nt">def execute_pipeline(features,labels, search_space=[<br/>                {"classifier": [LogisticRegression()],<br/>                 "classifier__penalty": ['l2','l1'],<br/>                 "classifier__C": np.logspace(0, 4, 10)<br/>                 },<br/>                {"classifier": [LogisticRegression()],<br/>                 "classifier__penalty": ['l2'],<br/>                 "classifier__C": np.logspace(0, 4, 10),<br/>                 "classifier__solver":['newton-cg','saga','sag','liblinear'] ##This solvers don't allow L1 penalty<br/>                 },<br/>                {"classifier": [RandomForestClassifier()],<br/>                 "classifier__n_estimators": [10, 100, 1000],<br/>                 "classifier__max_depth":[5,8,15,25,30,None],<br/>                 "classifier__min_samples_leaf":[1,2,5,10,15,100],<br/>                 "classifier__max_leaf_nodes": [2, 5,10]}], cv=5, verbose=0, n_jobs=-1):</span><span id="d93d" class="np mk jj nl b gy ob nr l ns nt">global best_model<br/>  <br/>  pipe = Pipeline([("classifier", RandomForestClassifier())])<br/>  <br/>  gridsearch = GridSearchCV(pipe, search_space, cv=cv, verbose=verbose,n_jobs=n_jobs) # Fit grid search<br/>  best_model = gridsearch.fit(features, labels)<br/>  print(best_model.best_estimator_)<br/>  print("The mean accuracy of the model is:",best_model.score(features, labels))</span></pre><p id="5592" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div></div>    
</body>
</html>