<html>
<head>
<title>Principal Components of PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析的主要成分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-components-of-pca-bea010cc1d33?source=collection_archive---------9-----------------------#2019-07-06">https://towardsdatascience.com/principal-components-of-pca-bea010cc1d33?source=collection_archive---------9-----------------------#2019-07-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/6be1d7eddc844fb09c625fd7d5334321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aAqR30YXYfW_Vdwv"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@kate5oh3?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Katie Smith</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="57da" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主成分分析(<a class="ae kf" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> PCA </a>)用于机器学习应用中，以降低数据的维度。在其他应用中，它对<a class="ae kf" href="http://people.ciirc.cvut.cz/~hlavac/TeachPresEn/11ImageProc/15PCA.pdf" rel="noopener ugc nofollow" target="_blank">图像压缩</a>特别有用。在本帖中，我们将浏览 Lindsay Smith 的<a class="ae kf" href="https://ourarchive.otago.ac.nz/bitstream/handle/10523/7534/OUCS-2002-12.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="le">关于主成分分析的教程</em> </a> <em class="le"> </em>以及 python 中的实现。</p><p id="d419" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的目标是通过布置数学公式来开发 PCA 的直觉，并超越<em class="le"> fit </em>和<em class="le"> fit_transform </em>方法。在继续之前，熟悉<a class="ae kf" href="https://www.abs.gov.au/websitedbs/a3121120.nsf/home/statistical+language+-+measures+of+spread" rel="noopener ugc nofollow" target="_blank">扩散度量</a>和<a class="ae kf" href="https://en.wikipedia.org/wiki/Linear_algebra" rel="noopener ugc nofollow" target="_blank">线性代数</a>是有帮助的。如果解释看起来微不足道，可以直接跳到实现。</p><h1 id="b898" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">传播的量度</h1><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/9cb75c9e57a4669f6e0fce1f1c8ce6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5TglMSJ_Lt0Xf4nD5yxX0Q.jpeg"/></div></div></figure><h2 id="9b85" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">差异</h2><p id="6cd5" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">方差是对分布的度量，它表示一组数字与它们的平均值相差多远。对于一维数组 X，方差 s2 为:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/5942deca22e9e730ca322aa5fe8a5d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IlXMYl9xjG8lKA3GxEMy2A.png"/></div></div></figure><ul class=""><li id="a38d" class="na nb it ki b kj kk kn ko kr nc kv nd kz ne ld nf ng nh ni bi translated">Xi =数组 X 的第 I 个条目的值</li><li id="6f43" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">X 条= X 的平均值</li><li id="120b" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">n =条目的数量</li></ul><h2 id="9336" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">二维之间的协方差</h2><p id="ac47" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">如果我们想测量两个随机变量的联合可变性呢？<a class="ae kf" href="https://en.wikipedia.org/wiki/Covariance" rel="noopener ugc nofollow" target="_blank">协方差</a>测量各条目相对于平均值的变化程度。它由下式给出:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/ab2d308f7b07025dd747d8f08f65a2a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9KODM2Lce5SKJKsWw1XuNw.png"/></div></div></figure><ul class=""><li id="5e67" class="na nb it ki b kj kk kn ko kr nc kv nd kz ne ld nf ng nh ni bi translated">Xi =数组 X 的第 I 个条目的值</li><li id="564f" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">Yi =数组 Y 的第 I 个条目的值</li><li id="7673" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">X bar =数组 X 的平均值</li><li id="bd9b" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">Y 条=数组 Y 的平均值</li><li id="9553" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">n =条目数，X 和 Y 相同</li></ul><p id="f53f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看协方差的输出时，查看符号很重要。如果是正的，那么就有正相关，意味着 X 和 Y 一起增加；如果是负的，那么两者都增加。如果我们关心值，那么最好使用<a class="ae kf" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">皮尔逊相关系数</a>。</p><h2 id="a340" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">n 维之间的协方差</h2><p id="7239" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">如果我们有一个 n 维数据，我们如何测量协方差？回想一下，协方差仅在 2 维之间，因此结果是一个<a class="ae kf" href="https://en.wikipedia.org/wiki/Covariance_matrix" rel="noopener ugc nofollow" target="_blank">协方差矩阵</a>。知道我们可以计算的协方差值的数量是有用的:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/47cb82489f883d33cb4c6162b5617f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Nv8clOyyHVERGzMw_rogxQ.png"/></div></figure><ul class=""><li id="e110" class="na nb it ki b kj kk kn ko kr nc kv nd kz ne ld nf ng nh ni bi translated">n =维数</li></ul><p id="6a5f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，如果我们有一个维度为 x、y 和 z 的数据集，那么我们的协方差矩阵 C 将为:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/475b7ccf8e9a4346255c2cf844d1db57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*knhQQIsY3fHeYf8ZY-RkXA.png"/></div></div></figure><h1 id="a88f" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">线性代数</h1><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/d6757d5cf1931f075ae44fb470075aeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HnxAPU-XQ1u8kbJv"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@alexblock?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alex Block</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="0f08" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">特征向量</h2><p id="d97e" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">当我们将两个兼容的矩阵相乘时，我们实际上是在做一个<a class="ae kf" href="https://en.wikipedia.org/wiki/Linear_map" rel="noopener ugc nofollow" target="_blank">线性变换</a>。考虑一个方阵<em class="le"> A </em>和一个向量<em class="le"> v </em>以及以下属性:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/431f8fbdba494fc17a0556c7d55eec33.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*XTYWztUqZ0FWj9Q3QgIZoQ.png"/></div></figure><ul class=""><li id="cd7f" class="na nb it ki b kj kk kn ko kr nc kv nd kz ne ld nf ng nh ni bi translated">A =方阵</li><li id="a08e" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">v =矢量</li><li id="c17e" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">λ =标量值</li></ul><p id="e06e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面告诉我们的是<em class="le"> v </em>实际上是一个特征向量。从几何学上讲，该表达式意味着对向量<em class="le"> v </em>应用线性变换会返回其缩放版本。</p><p id="fb1a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果 A 是一个可对角化的矩阵，那么它有 n 个特征向量。特征向量的一个重要性质是它们都是正交的。稍后，我们将会看到如何用这些向量来表达数据，而不是原始维度。</p><h2 id="5b8e" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">本征值</h2><p id="c2f8" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">在上一节中，我们在没有意识到的情况下计算了特征值:它不是别人，正是λ。在计算特征向量的时候，我们也会计算特征值。如本<a class="ae kf" href="https://www.scss.tcd.ie/~dahyotr/CS1BA1/SolutionEigen.pdf" rel="noopener ugc nofollow" target="_blank">教程</a>所示，可以手动计算这些值。然而，一旦 A 的维数超过 3×3，那么获得特征值和特征向量会非常繁琐和耗时。</p><h1 id="9bc7" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">PCA 实施</h1><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/7c06bb715d998d7223efcd74d911db29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BJWUuKAuAWWll_cf"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@laurenmancke?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Lauren Mancke</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="3cb6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在知道了在我们选择的例子中实现 PCA 的必要成分。在本节中，我们将详细介绍每个步骤，并了解前面章节中的知识是如何应用的。</p><h2 id="7983" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">第一步:获取数据</h2><p id="8597" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">在本练习中，我们将创建一个 3D 玩具数据。这是任意的数据，所以我们可以提前猜测我们的结果会是什么样子。为了使接下来的部分更简单，我们将把数据转换成玩具数据帧。</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="c0d7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我们的数据没有任何缺失值，因为这是 PCA 正常运行的关键。对于缺失的数据，我们可以用均值、方差、众数、零值或我们选择的任何值来代替空的观测值。每种技术都会将方差归入数据集，由我们根据具体情况做出适当的判断。</p><p id="db71" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看 3D 散点图中的数据:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/19d829812bc37dee57f9815309758b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*SjU2tRARFD3-X29_-8qGCA.png"/></div></figure><h2 id="11cf" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">第二步:减去平均值</h2><p id="fbbb" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">为了使 PCA 工作，我们需要具有平均值为零的数据集。我们可以用下面的代码轻松减去每个维度的平均值:</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="cb4f" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">步骤 3:计算协方差矩阵</h2><p id="f292" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">再次挑选熊猫让我们的生活变得更容易，我们使用了<a class="ae kf" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cov.html" rel="noopener ugc nofollow" target="_blank"> cov </a>方法。</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="d684" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下，非对角项是一个维度与另一个维度的协方差。例如，在下面的示例输出中，我们不太关心值，而是关心符号。这里 X 和 Y 是负相关的，而 Y 和 Z 是正相关的。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e87ae9371e87ba11a05321e5ecc3833d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*RWXgLeIPYnV5Dk-sVinrWQ.png"/></div></figure><h2 id="7790" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">第四步:计算协方差矩阵的特征向量和特征值</h2><p id="f349" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">我们用 numpy 的线性代数包<a class="ae kf" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html" rel="noopener ugc nofollow" target="_blank"> numpy.linalg.eig </a>计算特征向量<strong class="ki iu"> v </strong>和特征值<strong class="ki iu"> w </strong>。特征向量被归一化，使得列<strong class="ki iu"> v[:，i] </strong>是对应于特征值<strong class="ki iu"> w[i] </strong>的特征向量。计算出的特征值不一定是有序的，这将与下一步相关。</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="f59c" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">步骤 5:选择组件和新的特征向量</h2><p id="f7bb" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">现在我们有了特征向量和特征值，我们可以开始降维了。事实证明，具有最高特征值的特征向量是数据集的主分量。事实上，具有最大特征值的特征向量代表了数据维度之间最重要的关系。</p><p id="8563" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在的任务是将特征值从最高到最低排序，按照重要性的顺序给出分量。我们从作为向量矩阵的特征向量中提取想要保留的特征向量:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ny"><img src="../Images/dd7bd4bd252aecd7169f33332a0fb94b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iVEocYhiNMIDlOk6FXq05w.png"/></div></div></figure><p id="35a7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看如何在代码中做到这一点。这里我们决定忽略最小的特征值，因此我们的索引从 1 开始。</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="6edd" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">步骤 6:派生新数据集</h2><p id="0294" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">在最后一部分中，我们取向量的转置，并将其乘以原始数据集的左边，转置。</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="7fa7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的代码中，转置的特征向量是行特征向量，其中特征向量现在位于行中，因此最重要的特征向量位于顶部。行数据调整是转置的平均调整数据，其中每行保存一个单独的维度。我们的新数据如下所示:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/0e6ecf01287ff17f00f675ee179ef6e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*E1IZeHoL9qmBi7fqx9Loiw.png"/></div></figure><p id="a9e3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能会注意到上面轴的标签不是 X 和 y。通过我们的变换，我们已经改变了我们的数据，用我们的 2 个特征向量来表示。</p><p id="d4b7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们决定保留所有的特征向量，我们的数据将保持不变，只是旋转，使特征向量是轴。</p><h2 id="5e87" class="mi lg it bd lh mj mk dn ll ml mm dp lp kr mn mo lt kv mp mq lx kz mr ms mb mt bi translated">步骤 1:取回旧数据</h2><p id="41cf" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">假设我们减少了图像压缩的数据，现在我们想检索它。为此，让我们回顾一下我们是如何获得最终数据的。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/209fa24163db9b8bcf99e4831d285ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QNXpz8AO4Q4gMKokPBYdjQ.png"/></div></div></figure><p id="a336" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以把这个表达式反过来得到:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/463206dfc52356817a794c1f6efa427b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lWp_hpPPSwchh65buSHL4Q.png"/></div></div></figure><p id="2305" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为行特征向量的倒数等于我们的特征向量的转置。这是真的，因为矩阵的元素都是数据集的单位向量。我们的等式变成:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/50b3a6ce872a6e4d8fc30c1886495135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSCf-poyuihTPT7CxkMMvw.png"/></div></div></figure><p id="16c2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们加上从开始减去的平均值:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/618d18c657d76d0a58292fab162b5909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7e7dQrio6u2UFYVefqT5ig.png"/></div></div></figure><p id="d425" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 python 代码中，这看起来像:</p><figure class="me mf mg mh gt ju"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h1 id="bb96" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">结论</h1><p id="27e9" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">恭喜你！我们在练习的最后，从头开始执行 PCA 以减少数据的维数，然后返回我们的步骤以取回数据。</p><p id="6672" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管我们没有深入探究<em class="le">为什么</em>会起作用的细节，但我们可以自信地解读来自<a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"> sklearn 的 PCA </a>等软件包的输出。</p><p id="7bca" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">对于完整的 python 代码，可以随意探索本</em> <a class="ae kf" href="https://github.com/NadimKawwa/Statistics/blob/master/pca_tutorial.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="le">笔记本</em> </a> <em class="le">。</em></p></div></div>    
</body>
</html>