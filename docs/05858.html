<html>
<head>
<title>When Pytorch-transformers meets Fastai (w/ Google Colab)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当 Pytorch-transformers 遇上 Fastai (w/ Google Colab)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/best-of-two-worlds-pytorch-transformers-meets-fastai-5fd51ef34b0f?source=collection_archive---------19-----------------------#2019-08-26">https://towardsdatascience.com/best-of-two-worlds-pytorch-transformers-meets-fastai-5fd51ef34b0f?source=collection_archive---------19-----------------------#2019-08-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/3ab6b30525dcaa6b4ba73d7d7395152c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aE-FEdM3UXnFit-7gjI_iw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo from Kapa65 at <a class="kf kg ep" href="https://medium.com/u/a640208c527a?source=post_page-----5fd51ef34b0f--------------------------------" rel="noopener" target="_blank">Pixabay</a></figcaption></figure><h1 id="c2af" class="kh ki it bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">更新:</h1><p id="aabe" class="pw-post-body-paragraph lf lg it lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我发表这篇文章后，我有幸得到了 fastai 创始人之一杰瑞米·霍华德的回应，他建议我们可以使用 fastai 的回调系统来简化这里的工作，而无需调整 fastai 库中的基本训练循环。</p><p id="22a4" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">这似乎是一个简单的解决办法，然而，我已经尝试了很长时间(在<a class="ae mi" href="https://twitter.com/waydegilliam" rel="noopener ugc nofollow" target="_blank"> @waydegilli </a> am 的帮助下)，但并不能完全正确(训练指标根本没有改善……)。我已经在<a class="ae mi" href="https://forums.fast.ai/t/problem-with-the-metrics-unchanged-during-the-training-process/53622" rel="noopener ugc nofollow" target="_blank"> fastai 论坛</a>上解决了这个问题，并且幸运地让<a class="ae mi" href="https://sgugger.github.io/" rel="noopener ugc nofollow" target="_blank"> Sylvain </a>检查了代码。然而，度量仍然不动，训练循环无效。</p><p id="44e5" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">为了您的方便，我已经用所有的结果编辑了一个 Google Colab，如果您对问题所在有任何想法，我将不胜感激:</p><div class="mj mk gp gr ml mm"><a href="https://colab.research.google.com/drive/1KFlyttLs7aAX35lMLiDw9Bb0s_74ILMy#scrollTo=NPgXRybJk2XN" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">谷歌联合实验室</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">编辑描述</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">colab.research.google.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na jz mm"/></div></div></a></div><p id="7e49" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">鉴于我在使用回调系统时遇到的小缺陷，下面的解决方案，看似“愚蠢”的改变训练循环的方式，仍然是合并 fastai 和 Pytorch-transformers 的最佳方式。</p><h1 id="268a" class="kh ki it bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">介绍</h1><p id="19c6" class="pw-post-body-paragraph lf lg it lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你关注深度学习的趋势，尤其是 NLP，你不可能没有听说过 Fastai 或<a class="ae mi" href="https://github.com/huggingface/pytorch-transformers" rel="noopener ugc nofollow" target="_blank"> Pytorch-transformers </a>。这两个库为 NLP 实践者提供了友好的 API 和灵活的定制能力来进行原型设计和实验。Fastai 是一个通用的深度学习库，具有精心配置的 API，以适应各种应用程序:例如，文本，视觉，表格和协同过滤。Pytorch-transformers 是一个用于自然语言处理(NLP)的最新预训练模型库，包括 SOTA 模型，如 BERT 和 GPT2。同样值得注意的是，它们都是建立在 Pytorch 之上的，因此在它们之间建立连接不会太困难。考虑到这两个库的优点，把它们结合起来，这样我们就可以通过同时使用这两个包来简化建模过程，这不是很好吗？</p><p id="fc48" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">答案是肯定的，并且在这篇精彩的文章中有详细的记录。本文提供了将数据集转换为特殊形式的过程，这样它就可以适合 BERT 模型，而这又可以通过使用 fastai API 进行微调。这篇文章也激发了下面这篇文章的写作，这篇文章比较了伯特和乌尔菲特的表现。</p><div class="mj mk gp gr ml mm"><a href="https://medium.com/@abhikjha/fastai-integration-with-bert-a0a66b1cecbe" rel="noopener follow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">Fastai 与 BERT 的集成</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">序言</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">medium.com</p></div></div><div class="mv l"><div class="nb l mx my mz mv na jz mm"/></div></div></a></div><p id="1a17" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">上述文章在解释实现中的所有必要细节方面做了令人惊讶的工作，因此这篇文章不会进一步讨论，而是主要通过解决 Pytorch-transformers 库(来自 pytorch-pretrained-bert)的更新引起的一个关键问题来做出贡献。</p><h1 id="8e96" class="kh ki it bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">我们有一个问题</h1><p id="0fe9" class="pw-post-body-paragraph lf lg it lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在 Pytorch-transformers 更新之前，上面提供的解决方案和代码都运行良好。如果您浏览了更新之前撰写的上述文章中的代码。您可能会遇到这样一个异常:</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/17a3096c89f68515db3c681a67daebc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xK6il3hJDPCt5-FK2DBR1g.png"/></div></div></figure><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/2e07f8219491169ca4338c8829a2edf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PFT1aSvxPLKfF5mSvgfslg.png"/></div></div></figure><p id="d570" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">如果你像我一样，对 Pytorch-transformers 没有太多经验，你可能会感到非常困惑，想知道为什么它对其他人有效，但对我的机器无效！在没有深入探究异常回溯的情况下，我认为这是因为我的手电筒过时了。我将 Torch 更新到了最新版本，但是仍然收到了异常。</p><p id="48bb" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">在阅读 Pytorch-transformers 文档后，我意识到这个异常是由 Pytorch-transformers 包中 API 的变化引起的。简而言之，在 Pytorch-transformers 更新之前的日子里，<code class="fe ni nj nk nl b">model()</code>会在网络向前传递之后产生结果；更新之后，模型的 forward 方法生成一个元组，其第一个元素是原始模型输出。如果你想了解更多细节，请<a class="ae mi" href="https://github.com/huggingface/pytorch-transformers#models-always-output-tuples" rel="noopener ugc nofollow" target="_blank">阅读这里</a>。</p><pre class="nd ne nf ng gt nm nl nn no aw np bi"><span id="ac17" class="nq ki it nl b gy nr ns l nt nu"># If you used to have this line in pytorch-pretrained-bert:<br/>loss = model(input_ids, labels=labels)</span><span id="4e0e" class="nq ki it nl b gy nv ns l nt nu"># Now just use this line in pytorch-transformers to extract the loss from the output tuple:<br/>outputs = model(input_ids, labels=labels)<br/>loss = outputs<strong class="nl iu">[0]</strong></span></pre><p id="88b4" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">现在我们找到了代码中断的原因，让我们想出一个解决方案。</p><h1 id="e2c8" class="kh ki it bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">解决办法</h1><p id="208b" class="pw-post-body-paragraph lf lg it lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">解决方案是我们需要修改 Fastai 的代码，以适应 Pytorch-transformers 中<code class="fe ni nj nk nl b">model()</code>行为的变化。</p><p id="cd97" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">代码的关键部分是<code class="fe ni nj nk nl b">basic_train.py</code>中的<code class="fe ni nj nk nl b">loss_batch()</code>，我们可以看到<code class="fe ni nj nk nl b">model()</code>输出不是一个元组，而是直接发送给损失函数。</p><pre class="nd ne nf ng gt nm nl nn no aw np bi"><span id="25e9" class="nq ki it nl b gy nr ns l nt nu">def loss_batch(model:nn.Module, xb:Tensor, yb:Tensor, loss_func:OptLossFunc=None, opt:OptOptimizer=None,<br/>cb_handler:Optional[CallbackHandler]=None)-&gt;Tuple[Union[Tensor,int,float,str]]:<br/>    "Calculate loss and metrics for a batch, call out to callbacks as necessary."<br/>    cb_handler = ifnone(cb_handler, CallbackHandler())<br/>    if not is_listy(xb): xb = [xb]<br/>    if not is_listy(yb): yb = [yb]<br/>    out = model(*xb) <strong class="nl iu"># Here the output is NOT a tuple</strong><br/>    out = cb_handler.on_loss_begin(out)</span><span id="699d" class="nq ki it nl b gy nv ns l nt nu">if not loss_func: return to_detach(out), yb[0].detach()<br/>         loss = loss_func(out, *yb)</span><span id="7a82" class="nq ki it nl b gy nv ns l nt nu">if opt is not None:<br/>        loss,skip_bwd = cb_handler.on_backward_begin(loss)<br/>        if not skip_bwd:                     loss.backward()<br/>        if not cb_handler.on_backward_end(): opt.step()<br/>        if not cb_handler.on_step_end():     opt.zero_grad()</span><span id="76e3" class="nq ki it nl b gy nv ns l nt nu">return loss.detach().cpu()</span></pre><p id="6369" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">一旦我们在库中找到了确切的位置，更改就变得轻而易举了:简单地选择如下所示的<code class="fe ni nj nk nl b">model()</code>输出中的第一个元素，并为该函数提供一个新名称:</p><pre class="nd ne nf ng gt nm nl nn no aw np bi"><span id="af4e" class="nq ki it nl b gy nr ns l nt nu">def loss_batch_bert(model:nn.Module, xb:Tensor, yb:Tensor, loss_func:OptLossFunc=None, opt:OptOptimizer=None,<br/>               cb_handler:Optional[CallbackHandler]=None)-&gt;Tuple[Union[Tensor,int,float,str]]:<br/>    "Calculate loss and metrics for a batch, call out to callbacks as necessary."<br/>    cb_handler = ifnone(cb_handler, CallbackHandler())<br/>    if not is_listy(xb): xb = [xb]<br/>    if not is_listy(yb): yb = [yb]<br/>    out = model(*xb)[0] <strong class="nl iu"># we take the first element as the model output<br/></strong>    out = cb_handler.on_loss_begin(out)</span><span id="2dc2" class="nq ki it nl b gy nv ns l nt nu">if not loss_func: return to_detach(out), yb[0].detach()<br/>         loss = loss_func(out, *yb)</span><span id="086e" class="nq ki it nl b gy nv ns l nt nu">if opt is not None:<br/>        loss,skip_bwd = cb_handler.on_backward_begin(loss)<br/>        if not skip_bwd:                     loss.backward()<br/>        if not cb_handler.on_backward_end(): opt.step()<br/>        if not cb_handler.on_step_end():     opt.zero_grad()</span><span id="bcca" class="nq ki it nl b gy nv ns l nt nu">return loss.detach().cpu()</span></pre><p id="549f" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">现在我们有了一个新的<code class="fe ni nj nk nl b">loss_batch_bert()</code>函数，我们需要它来替换 Fastai 中加载到我们环境中的原始<code class="fe ni nj nk nl b">loss_batch()</code>。我们可以通过以下方式做到这一点:</p><pre class="nd ne nf ng gt nm nl nn no aw np bi"><span id="66c7" class="nq ki it nl b gy nr ns l nt nu"><strong class="nl iu"><em class="nw"># To change the loss_batch function in the loaded fastai module</em></strong></span><span id="127c" class="nq ki it nl b gy nv ns l nt nu">import sys<br/>module_basic_train = sys.modules['fastai.basic_train']<br/>module_basic_train.loss_batch = loss_batch_bert<br/>sys.modules['fastai.basic_train'] = module_basic_train</span></pre><p id="aa55" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">然后我们调整后的<code class="fe ni nj nk nl b">loss_batch_bert()</code>被嵌入到 Fastai 库中，它应该可以满足 Pytorch-transformers 的需求。剩下的都是 Fastai 魔法:组装一个<code class="fe ni nj nk nl b">Learner()</code>物体，进行试衣！</p><p id="6a19" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">只需两个周期的训练，我们就可以在私人排行榜上取得 98.427%的成绩！考虑到我们的短期训练和非常“普通”的训练循环，这还不错。</p><p id="3011" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">如果您没有更新 Fastai 库，请注意。如果您想在训练过程中查看准确性，您可以传递一个准确性回调函数。但是，accuracy 函数可能已经过时，它的 return 语句中包含一个小错误:</p><p id="231d" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">((y_pred&gt;thresh) == y_true.byte())。浮动()。<strong class="lh iu">意思是()#缺乏。byte() </strong></p><p id="b187" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">它可以被固定如下</p><pre class="nd ne nf ng gt nm nl nn no aw np bi"><span id="70e6" class="nq ki it nl b gy nr ns l nt nu"><em class="nw"># The more recent version fastai implemented exactly this version thus you could just directly call a partial funtion</em><br/><em class="nw"># accuracy_thresh</em><br/><strong class="nl iu">def</strong> accuracy_thresh2(y_pred:Tensor, y_true:Tensor, thresh:float=0.5, sigmoid:bool=<strong class="nl iu">True</strong>)-&gt;Rank0Tensor:<br/>    "Computes accuracy when `y_pred` and `y_true` are the same size."<br/>    <strong class="nl iu">if</strong> sigmoid: y_pred = y_pred.sigmoid()<br/>    <strong class="nl iu">return</strong> ((y_pred&gt;thresh)<strong class="nl iu">.byte()</strong>==y_true.byte()).float().mean()</span></pre><p id="84c6" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">我希望这篇文章是有帮助的，特别是对那些不熟悉 Fastai 或 Pytorch-transformers 的人。</p><p id="d599" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">代码可以在这里的<a class="ae mi" href="https://github.com/DavidykZhao/Pytorch_transformers_Fastai/blob/master/Pytorch_transformers_Fastai.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>中找到。</p><p id="e8c6" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">如果您在此过程中遇到其他问题，请随时告诉我。</p><p id="7480" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">编码快乐！</p></div></div>    
</body>
</html>