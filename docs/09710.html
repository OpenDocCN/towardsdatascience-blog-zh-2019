<html>
<head>
<title>Tricks for Manipulating Probability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">操纵概率的技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tricks-for-manipulating-probability-470b7eb7dfd?source=collection_archive---------16-----------------------#2019-12-20">https://towardsdatascience.com/tricks-for-manipulating-probability-470b7eb7dfd?source=collection_archive---------16-----------------------#2019-12-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="dee0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">概述</strong></p><ul class=""><li id="09a2" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated"><em class="kx">身份诡计</em></li><li id="86f1" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><em class="kx">跳跃戏法</em></li><li id="2a22" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><em class="kx">密度比例技巧</em></li><li id="7b47" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><em class="kx">对数导数技巧</em></li><li id="49b4" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><em class="kx">重新参数化技巧</em></li></ul></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/cd7fcf6f5b01ac84855b8a19dc3bdd3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8KH4_KIeSSOMk4Jei_8EoA.jpeg"/></div></div></figure><p id="2943" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">解决机器学习和人工智能的基本问题需要在概率方面的灵活性。这个博客旨在总结不同概率问题中应用的各种技术，使计算更容易，有时甚至是可能的！本博客假设对概率和期望有基本的理解。因此，对基础知识的预先修订可能有助于进一步理解博客。</p><p id="9213" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们快速概述一下机器学习中常见的推理问题及其概率方程。我们将不讨论这些问题的细节。这个列表应该是作为一个复习，而不是指南。</p><h2 id="fe23" class="lw lx it bd ly lz ma dn mb mc md dp me kb mf mg mh kf mi mj mk kj ml mm mn mo bi translated">证据估计:</h2><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/ac27b4a6ed8c057b0d0339daf47d590a.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*FFk5mUID4SCYYigu_gM_Rg.png"/></div></figure><h2 id="2f26" class="lw lx it bd ly lz ma dn mb mc md dp me kb mf mg mh kf mi mj mk kj ml mm mn mo bi translated">力矩计算:</h2><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/37c8d968ec5019c886bd4881a5243487.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*T99piL9wpJA7AjPBrXLkLA.png"/></div></figure><h2 id="87b3" class="lw lx it bd ly lz ma dn mb mc md dp me kb mf mg mh kf mi mj mk kj ml mm mn mo bi translated">参数估计:</h2><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/9a1649e1d4f0374a5226e753a5cf01b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*1XcmVOTslvZmsuIeobDZZQ.png"/></div></figure><h2 id="c867" class="lw lx it bd ly lz ma dn mb mc md dp me kb mf mg mh kf mi mj mk kj ml mm mn mo bi translated">预测:</h2><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/bd581887e6f31524f79b70fa7c11fe9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*48WXd5PNZ05KBBEaeR7jeg.png"/></div></figure><h2 id="fbdd" class="lw lx it bd ly lz ma dn mb mc md dp me kb mf mg mh kf mi mj mk kj ml mm mn mo bi translated">假设检验:</h2><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/4250ba31f529509fa09a677a504c90ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*vm-YwQZaYhsO0CumyZL4xw.png"/></div></figure></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="611a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你不知道其中的一些应用，不要担心。这不是即将到来的讨论的焦点，而是强调 ML 中概率建模的不同领域。因此，没有任何延迟，让我们深入到上述概率问题的操纵技术。</p><ol class=""><li id="7bd1" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn mu ku kv kw bi translated"><strong class="js iu">恒等式技巧:<br/> </strong>这将一个对<em class="kx"> f(x) </em> w.r.t. a 分布<em class="kx">p(x)</em>的期望转换成一个对<em class="kx">g(x；f) </em> w.r.t .分配<em class="kx"> q(x)。这是一个非常简单却非常有效的技巧。<em class="kx"> </em>在很多场景下，计算期望 w.r.t. <em class="kx"> p(x) </em>是比较困难的。因此，我们引入一个分布<em class="kx"> q(x)，</em>，它在数学上对我们来说很方便，并将期望值转换如下:</em></li></ol><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mv"><img src="../Images/3f83fb9f00a71053ed8d954a6d0db809.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xawr2Vrgn8a05IOBwv6xw.png"/></div></div></figure><p id="3778" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">应用:</strong></p><p id="0884" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们用这个技巧来看看 ML 域的一个真实问题。</p><p id="405f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">贝叶斯公式中的证据计算</strong>是多维变量中的一个棘手问题。在<em class="kx">生成模型中，</em>计算<em class="kx"> p(x) </em>是主要目标。这里，<em class="kx"> x </em>是样本，<em class="kx"> z </em>是潜在变量，<em class="kx"> p(x|z) </em>是观察到<em class="kx"> x </em>的可能性给定<em class="kx">z</em>下面给出的等式是联合概率<em class="kx"> p(x，z) </em> w.r.t. <em class="kx"> z. </em>的边缘化</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/546b039bab3096f67089936dc08e7a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*WAZKcEQ98WnMozCfaRWK7Q.png"/></div></figure><p id="988f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">p(z) 是实际的潜在变量分布，因此很难处理。我们引入一个不同的分布<em class="kx"> q(z) </em>，它将逼近<em class="kx"> p(z) </em>，并且具有良好的数学性质。从这种分布<em class="kx">q(z)</em>中采样很容易，而且在许多情况下，是一种<em class="kx">高斯分布。我们简单地乘以和除以 q(z ),并将期望值的分布改为 q(z)。</em></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/ebbb9a57cc626e13d15fdc927ae73824.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*1yCedi0COrZV3bkd2ac09A.png"/></div></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi my"><img src="../Images/0cb525ed01cfed4271b822a67987771d.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*lBQcxl1ME6dXeMP33sZGdg.png"/></div></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mz"><img src="../Images/43ab817c3e714c0ac065563f0419995f.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*MVzusf_7k0f18UZpNbzgaQ.png"/></div></div></figure><p id="88ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">条件:</p><ul class=""><li id="dcee" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated"><em class="kx"> q(z) </em> &gt; 0，当<em class="kx">p(x | z)p(z)</em>≦<em class="kx">0</em></li><li id="cd06" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><em class="kx"> q(z) </em>已知或者容易处理。</li></ul><p id="c236" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以进一步使用蒙特卡罗<br/>抽样方法计算期望值，因为<em class="kx"> q(z) </em>很容易从中抽样。这表现为:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi na"><img src="../Images/2d57f08229e5a2f1c767f19d41470248.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*7b3TXEKYYL1P1-Fu1Juy8w.png"/></div></figure><p id="592c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在哪里，</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/69fd767a73f9d66345a93c527225a04e.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*7t0rbY_Mv_rLJ92yCxUpMQ.png"/></div></figure><p id="f409" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注:<em class="kx">我们将在《边界戏法》中讨论解决这个问题的另一种方法。</em></p><p id="ff85" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其他地方的身份诡计:</p><ul class=""><li id="5ac3" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">操纵随机梯度。</li><li id="43a3" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">推导概率界限。</li><li id="3936" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">RL 用于政策修正。</li></ul></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="c7c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.<strong class="js iu">边界技巧:</strong> <br/>这个技巧来自于‘凸分析’，我们在其中对要计算的积分进行边界处理。其中之一是<strong class="js iu">詹森不等式，</strong></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/e22fdcf4a841d7eb2bed71bd3043bf44.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*pxrJWEoid23eryvTVeyxew.png"/></div></figure><p id="df3d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在大多数 ML 问题中，我们使用对数作为函数<em class="kx"> f，</em>，因为它们的可加性和严格的凹性。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3e056242cd8994466da37770ff2940ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*3YfnDavJzfKegt-bb2Ay5w.png"/></div></figure><p id="4151" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其他跳跃技巧:</p><ul class=""><li id="594e" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">芬切尔对偶。</li><li id="4ccb" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">霍尔德不等式。</li><li id="d1a0" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">蒙日-坎特罗维奇不等式。</li></ul><p id="b9e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个应用出现在<em class="kx">变分推理中的<em class="kx">证据下界(ELBO) </em>估计中。</em>我们将使用<em class="kx">詹森不等式</em>找到一个下限进行优化，而不是使用第一招中<em class="kx">证据计算</em>的蒙特卡洛估计。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/133c677208f4ac5a476aa471295a2a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*J_deFa6Pr6jqrxKMlISgOw.png"/></div></figure><p id="64ba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">取<em class="kx">对数</em>并使用詹森不等式，</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8d477424e59dd019cf45f871db8ac514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*_vmp-NZo7Z0eM-ubFivzJA.png"/></div></figure><p id="ca3e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">RHS 是变分下限，其中</p><ul class=""><li id="5618" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">第一项是重建损失。</li><li id="ebf9" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">第二项是在潜在变量上引入的近似分布族<em class="kx"> q(z) </em>和原始的<em class="kx"> p(z)之间的 KL 散度。</em></li></ul><p id="df55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一项采用 l2 损失或交叉熵损失的形式，取决于<em class="kx"> p(x|z) </em>的分布，并且可以使用<em class="kx">期望最大化(em)算法来优化该问题。</em></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/fd456050d77d97a639d2810264133ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*n7eLy1StDa71_es-zRGU-w.png"/></div></figure></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="72ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3。密度比技巧:</strong> <br/>通常，当我们需要一个概率密度的比值时，一个简单的计算方法是计算两者，然后取比值。这个技巧说<strong class="js iu"> <em class="kx">两个概率密度的比率可以用一个分类器来计算，这个分类器使用从两个分布中抽取的样本。</em> </strong>由下式给出其中 p*(x)和 q(x)是两个概率分布，p(y=1|x) &amp; p(y=-1|x)是分类器。</p><p id="448d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注:p*和 p 不相关。(抱歉符号滥用)。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c113e4f1779213260822dba16cc29b3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*VqwSPOTfa-eY9rVVXW-gtg.png"/></div></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ae0c9ebffac874f129c1d5259e7892bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*Gj0_2K8nGtdxWxmwaNljYg.png"/></div></figure><p id="305c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看它的起源，找出它工作的基本原理。</p><p id="745a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过组合来自两种分布的样本来创建数据集{显示为“组合数据”}，并将+/- 1 指定为标签来表示分布{显示为“指定标签”}。现在，在等价设置中，我们可以将原始分布 p*(x)和 q(x)定义为组合数据集上的条件概率 p(x | y){显示为‘等价’}。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nj"><img src="../Images/56f6c685725f49edb74c6e4bc73e8df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZbTLwf5HBiDSnW4CE8jufA.png"/></div></div></figure><p id="39e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">回想一下贝叶斯规则，该规则应用于条件分布 p(x|y=1)和 p(x|y=-1 ),如下所示:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nk"><img src="../Images/d06772f5bc17dc73712bab35e69d6424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6worFzzfmC56NBz1lmbc4A.png"/></div></div></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/fc364155408562dc6c26f13c972148e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*7_o4IOJ_LW2WdlV1mPtQdQ.png"/></div></figure><p id="cbac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">注意:</strong>创建一个平衡数据集，使得<em class="kx"> p(y=+1)等于 p(y=-1)。</em>因此，<em class="kx"> </em>它进一步简化为:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f477991c517fdcdc6fb3c2e8d93d6c23.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*NCRI7Q7VkWhGEGWwgfCjHg.png"/></div></figure><p id="88c4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其他地方的密度比技巧:</p><ul class=""><li id="22eb" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">生成对抗网络。</li><li id="d76e" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">噪声对比估计。</li><li id="7810" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">双样本测试。</li></ul></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="19e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们来看看操纵函数<em class="kx"> f. </em>的期望梯度的技巧</p><p id="9e00" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是统计科学中最常见的梯度问题。如果分布<em class="kx"> q(z) </em>是简单的，并且积分是一维的，则可以计算出期望值，因此也可以计算出它的梯度。但是，在一般的框架中，我们不能以封闭的形式计算期望。因此，计算其梯度成为一个不平凡的问题。</p><p id="23be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里<em class="kx">φ</em>和<em class="kx"> θ </em>分别是<em class="kx"> q(z) </em>和<em class="kx"> f(z) </em>的参数。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2fdb9e35af8b097302ac6091a6a4d2c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*_p1PGzDpXtt-OP-stojjrQ.png"/></div></figure><p id="17e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将期望值表示为积分形式，我们可以看到有两种方法来进行梯度估计:</p><ol class=""><li id="1ec4" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn mu ku kv kw bi translated">求密度的微分<em class="kx"> q(z):(得分函数估计器)</em></li><li id="dd16" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn mu ku kv kw bi translated">求函数<em class="kx"> f(z)的微分:(路径估计器)</em></li></ol><p id="42e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将详细了解这两种方法是如何工作的，并以 RL 为例给出一个算法来获得直觉。在示例中，我们将公式化上述梯度估计问题，并使用我们的技巧来解决它。</p><p id="a14b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在此之前，这些是出现上述问题的典型领域:</p><ul class=""><li id="8117" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">生成模型和推理。</li><li id="1365" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">强化学习和控制。</li><li id="fb24" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">运筹学</li><li id="63da" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">蒙特卡洛模拟。</li><li id="483a" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">金融和资产定价。</li><li id="56a7" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">灵敏度估计。</li></ul></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="9cfd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.<strong class="js iu">对数导数技巧:</strong></p><p id="b7ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一种方法是微分密度<em class="kx">q(z)</em>也称为<em class="kx">得分函数估计量。</em></p><p id="a0e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">统计学中的‘分数’是什么？<br/>得分(或线人)是对数似然函数 log 的梯度(<strong class="js iu"><em class="kx">【θ】</em></strong><em class="kx">)</em>w . r . t .参数向量<em class="kx"> θ </em>(见<a class="ae no" href="https://en.wikipedia.org/wiki/Score_(statistics)" rel="noopener ugc nofollow" target="_blank"> <em class="kx"> wiki </em> </a> <em class="kx">)。</em></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi np"><img src="../Images/24aecc90972eea042a230bccac492947.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*rWB844UMRXtYGucc_kyw3Q.png"/></div></figure><p id="e6c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，在我们的例子中，诀窍是:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3f9d358b453d77879566b442849c7b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*dAdm6X3WxZRHNSaVTJEOWQ.png"/></div></figure><p id="da89" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(这里，<em class="kx"> ϕ </em>为<em class="kx">参数，</em>t47】q 为<em class="kx">似然函数，梯度为 w.r.t. ϕ </em>)</p><p id="b3c0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">得分函数的性质:<br/> 1。得分函数的期望值是 0。(容易证明。提示:詹森不等式)</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1da6751bfea05bd512599dc166133387.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*Sh5uactkpUY902KG-20CUg.png"/></div></figure><p id="c3c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.得分函数的方差由<em class="kx"> Fisher 矩阵给出。(只是一个有用的属性)</em></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/cbef869895dc04f4ccaa30d8f38c8841.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*yU5FILTg9SbC6c_Lc3sjWw.png"/></div></figure><p id="be64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们来看看上面的技巧是如何用来解决随机优化问题的。</p><p id="ba26" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在下图中，<br/> 1。梯度为 w.r.t. <em class="kx"> ϕ </em>，积分和梯度的顺序可以互换。<br/> 2。取<em class="kx"> q(z) </em>的梯度，并通过乘以和除以<em class="kx"> q(z) </em>来应用恒等技巧。<br/> 3。对数导数技巧开始发挥作用。<br/> 4。所示期望的梯度被转换成所示函数的期望。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ff64c415043b0d71ce346d74e67f3138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*w93GGajaRFdM43YD9eJ1wg.png"/></div></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nu"><img src="../Images/fa8e2f4373a93bb69fba7c32119facd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-RffGrYQznoW_gCWahqcgg.png"/></div></div></figure><p id="9a43" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意，我们引入了一个<em class="kx">常数 c </em>，它不会改变使用属性“得分函数的期望值为 0”的等式。但是从 f(z) 中减去计算的<em class="kx">常数 c 减小了梯度的方差。</em></p><p id="d02f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们准备在强化学习中导出<strong class="js iu">香草</strong> <strong class="js iu">策略梯度</strong>。</p><p id="4e9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看推导中使用的一些定义:</p><ul class=""><li id="b78d" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">一条<strong class="js iu">轨迹</strong>是世界上一系列的状态和行为。</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/dafe62da7caee63a82ce1313f5471a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*HF9qP257Uhm-SrSyzaQTNw.png"/></div></figure><p id="28ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kx">这里，s 和 a 分别表示任意时刻 t 的状态和动作。</em></p><ul class=""><li id="ead2" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">代理人的目标是最大化某个轨迹上的累积回报，称为<strong class="js iu"> <em class="kx">回报。</em> </strong></li><li id="8af7" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">一种回报是<strong class="js iu">无限期贴现回报</strong>，这是代理人曾经获得的所有回报<em class="kx">的总和，但按照他们在未来多远获得的时间贴现。这个奖励公式包括一个折扣因子<em class="kx"> γ </em> ∊ <em class="kx"> (0，1)</em></em></li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f4202071377d81be03162b9794acbfd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*QNddl0eWyxGWsaezW1qALg.png"/></div></figure><ul class=""><li id="a93e" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">另一种是<strong class="js iu">有限期限未贴现回报</strong>，也就是在一个固定的台阶窗口内获得的回报之和。我们将在推导中使用这一点。</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/7b417efcb5976f8b84cb0df7eb3faa59.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*6Y-lQpv-kwp-NdwoclSVHg.png"/></div></figure><ul class=""><li id="7292" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">一个<strong class="js iu">策略<em class="kx"> π </em> </strong>是代理用来决定采取什么行动的规则。它可能是随机的。下面等式中的<strong class="js iu"> <em class="kx"> π </em> </strong>表示给定状态下动作的概率分布。我们通常用<em class="kx"> θ </em>来表示这种政策的参数，</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d6f541fcac8e3a089890ae22b4cef838.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*kXCzmc4pzD9l0pfQvrVmvA.png"/></div></figure><ul class=""><li id="312c" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">让我们假设环境转变<strong class="js iu"> <em class="kx"> P </em> </strong>和策略<strong class="js iu"> π </strong>都是随机的。在这种情况下，<strong class="js iu"><em class="kx">T</em></strong>-步进轨迹给定策略<strong class="js iu"> π </strong>的概率为:</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/32aa3208eac3dfd337a195e2efb271a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*dE2v0bicQzbscCj15-widQ.png"/></div></figure><ul class=""><li id="4158" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">对数导数技巧:</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/c4f68ddc0b5a9f23dc9fba17c6fc4bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*-hGSght-vV8VWloUJBGpNA.png"/></div></figure><ul class=""><li id="7bc7" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">轨迹的对数概率是:</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/80dba18ed6856c98727f10338e2128ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*lFcidzJxlTsOAH5mRx1g9g.png"/></div></figure><ul class=""><li id="bae0" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">因此，轨迹对数概率的梯度为:</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/bc8f883cc75ac4334b9dc85992e14224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*P7EGTNhW0z5zatij4LCx3A.png"/></div></figure><ul class=""><li id="7e7c" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">注意环境对<em class="kx"> θ </em>没有依赖性，所以<em class="kx">初始状态概率ρ </em>和<em class="kx">跃迁概率 P </em>的梯度为零。</li><li id="6d0a" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">预期回报由下式表示:</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d462c6b93d39135a27dade7b93ca4368.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*7Y9H6U3_l1zMBVxTvJEPqg.png"/></div></figure><p id="34fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">RL 的目标是选择一个策略，当代理人按照这个策略行动时，这个策略使<strong class="js iu">期望收益<em class="kx"> J(π) </em> </strong>最大化。RL 中的中心优化问题可以表示为:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/b644bbb11ee0e1d021d4d10d929c3cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*7CozXU4WbK5iY6252vhhig.png"/></div></figure><p id="9a3c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中，π*是最优策略。</p><p id="f7a3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们希望通过梯度上升来优化策略，例如</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi of"><img src="../Images/17cd3545473a668b1b51248cad272da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*NiRdA_l2CDDC_PsW-fgFeQ.png"/></div></figure><p id="a4ff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">政策绩效的梯度，<em class="kx">【∇j(π】</em>，称为<strong class="js iu">政策梯度。</strong>以这种方式优化策略的算法被称为<strong class="js iu">策略梯度算法。</strong>任何政策梯度背后的关键思想是提高导致更高<strong class="js iu">回报</strong>的行动概率，并降低导致更低回报的行动概率，直到你达到最优<strong class="js iu">政策</strong>。</p><blockquote class="og oh oi"><p id="734a" class="jq jr kx js b jt ju jv jw jx jy jz ka oj kc kd ke ok kg kh ki ol kk kl km kn im bi translated">下面给出了推导过程(取自旋转 RL 的图像片段)。我强烈推荐所有 RL 从业者阅读这个资源。<a class="ae no" href="https://spinningup.openai.com/en/latest/user/introduction.html" rel="noopener ugc nofollow" target="_blank">链接<em class="it">此处</em> </a> <em class="it">。</em></p></blockquote><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi om"><img src="../Images/7f2c31e56e9b33af07e8b3134087d914.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*mDFDiyZLe99gkCjCx_YoQw.png"/></div></figure><p id="d2ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个期望值，这意味着我们可以用一个样本均值来估计它。如果我们收集一组轨迹<em class="kx"> D = {τ}，</em>政策梯度可以用以下公式估算</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi on"><img src="../Images/10b4d774d43fc60f16cbefcd022b4182.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*JIYjv3-5IzeL9JBKr4-Jkg.png"/></div></figure><p id="5ccf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我们的最后一个魔术，</p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="1bb9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">5.<strong class="js iu">重新参数化技巧:<br/> </strong>你可能在处理变分自动编码器时遇到过这个技巧。这是它在一般框架中的执行方式:</p><p id="975c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">任何分布都可以表示为其他分布的变换。这方面的一个例子是<em class="kx">逆采样方法，</em>它说所有的均匀分布都可以转化为我们感兴趣的分布的逆 CDF(见<a class="ae no" href="https://en.wikipedia.org/wiki/Inverse_transform_sampling" rel="noopener ugc nofollow" target="_blank"> wiki </a>)。一般来说，一个分布<em class="kx"> p(z) </em>可以转换成另一个分布<em class="kx"> p( </em> ε <em class="kx"> ) </em>如下:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/fbafaf87b3530eb356862e0bca7faaa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*BGQDxqx0xIAqkGDYkZtp7g.png"/></div></figure><p id="d5a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">回到随机优化的第二种方法，我们有路径估计或重新参数化技巧。这次我们将操纵<em class="kx"> f(z)。</em></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi op"><img src="../Images/72906ae791fa76e8d24192c33fce00f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*FZe6ul70BeuQNogbIwmfNA.png"/></div></figure><p id="72c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意<em class="kx"> z </em>是从<em class="kx"> q(z)采样的。</em>我们引入了<em class="kx"> p( </em> ε <em class="kx"> ) </em>和上式定义的一个函数<em class="kx"> g </em>。现在，代替我们问题中的<em class="kx"> z </em>(由下式给出)，我们有</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/96c18d1d1656428b67eb113543059983.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*IoQ3nzz7-CiLVxDLSA2VrQ.png"/></div></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi or"><img src="../Images/d55a78792d75dd83a6f8d6f30716a52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*yY1U1u_Tzu3e7nU88GLXnA.png"/></div></div></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi os"><img src="../Images/311950f03eb3e9635b0ce10ab6dd3792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*lHdrT47qxTvRln85STLnvA.png"/></div></figure><p id="516d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上述两种方法都有助于将期望梯度转换成某种梯度的期望，这种期望可以进一步用变分法或蒙特卡罗法来近似或有效地计算。</p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="31f2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这都是为了操纵概率。希望下次遇到你的概率问题时，你会备上这些招数。</p><p id="0ccf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我的第一个博客。任何反馈都将受到高度赞赏。我真诚地建议仔细阅读参考资料，以便更好、更全面地理解。如果你能坚持到这里，非常感谢！</p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><h2 id="a98a" class="lw lx it bd ly lz ma dn mb mc md dp me kb mf mg mh kf mi mj mk kj ml mm mn mo bi translated">参考资料:</h2><p id="3177" class="pw-post-body-paragraph jq jr it js b jt ot jv jw jx ou jz ka kb ov kd ke kf ow kh ki kj ox kl km kn im bi translated">[1]Deep mind 的高级深度学习和强化学习。<br/> [2]通过打开 AI 旋转 RL。</p></div></div>    
</body>
</html>