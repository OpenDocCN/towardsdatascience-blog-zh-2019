<html>
<head>
<title>Custom TensorFlow Loss Functions for Advanced Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于高级机器学习的定制张量流损失函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/custom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a?source=collection_archive---------5-----------------------#2019-01-02">https://towardsdatascience.com/custom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a?source=collection_archive---------5-----------------------#2019-01-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5a40" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">和少量迁移学习示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/64983f900e4021d77659d37c983d8a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KO21R03vjfK34KGdr0pcA.png"/></div></div></figure><p id="1433" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">在这篇文章中，我们将看看:</strong></p><ul class=""><li id="0c1d" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated"><strong class="kw iu">在高级 ML 应用中使用自定义损失函数</strong></li><li id="fab0" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><strong class="kw iu">定义自定义损失函数并集成到基本张量流神经网络模型</strong></li><li id="b72d" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><strong class="kw iu">知识提炼学习的一个简单示例，使用高斯过程参考应用于少量学习问题</strong></li></ul><p id="f8ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">链接到我的其他文章:</p><ol class=""><li id="1256" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp me lw lx ly bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/deep-kernels-and-gaussian-processes-for-few-shot-learning-38a4ac0b64db">深度内核转移和高斯过程</a></li><li id="8b02" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp me lw lx ly bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/decision-trees-and-random-forests-for-classification-and-regression-pt-1-dbb65a458df">随机森林</a></li><li id="9add" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp me lw lx ly bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/the-softmax-function-neural-net-outputs-as-probabilities-and-ensemble-classifiers-9bd94d75932"> Softmax 分类</a></li><li id="f669" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp me lw lx ly bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/analyzing-climate-patterns-with-self-organizing-maps-soms-8d4ef322705b">气候分析</a></li></ol><p id="af7a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">【注:</strong><a class="ae mf" href="https://www.tensorflow.org/community/roadmap" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu">tensor flow 2.0</strong></a><strong class="kw iu">现已发布，与本文所基于的 the 1.x 版本有较大不同。我仍在研究 TF 2.0 w.r.t 自定义损失和张量操作，如本文所述，可能会写一篇新文章或更新这篇文章。请记住这一点。】</strong></p><p id="8ffc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">简介</strong></p><p id="9315" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">机器学习的老路包括旅行到熟悉的地标和风景点。一组熟悉的界标是预定义的损失函数，为您试图优化的问题提供合适的损失值。我们熟悉分类的交叉熵损失和回归问题的均方误差(MSE)或均方根误差(RMSE)。包括前端(如 Keras)和后端(如 Tensorflow)的流行 ML 包包括一组用于大多数分类和回归任务的基本损失函数。但是，在常规方法之外，还存在自定义的损失函数，您可能需要使用它们来解决某个问题，这些损失函数只受有效张量运算的约束。</p><p id="dd16" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在 Keras 中，技术上你可以创建自己的损失函数，但是损失函数的形式仅限于<em class="mg"> some_loss </em> ( <em class="mg"> y_true，y_pred </em>)并且仅此而已。如果您试图以<em class="mg"> some_loss_1 </em> ( <em class="mg"> y_true，y_pred，**kwargs </em>)的形式向损失中添加额外的参数，Keras 将会抛出一个运行时异常，并且您会丢失用于聚合数据集的计算时间。有一些方法可以解决这个问题，但一般来说，我们希望有一种<em class="mg"> </em>可伸缩的方式来编写一个损失函数，接受我们传递给它的任何有效参数，并以标准和预期的方式对张量进行操作。我们将看到如何直接使用 Tensorflow 从头开始编写一个神经网络，并构建一个自定义的损失函数来训练它。</p><p id="37ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">张量流</strong></p><p id="a023" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Tensorflow (TF)是一个符号和数字计算引擎，它允许我们将张量*串成计算图形，并对它们进行反向传播。Keras 是一个运行在 Tensorflow 之上的 API 或前端，它可以方便地打包使用 Tensorflow 构建的标准结构(如各种预定义的神经网络层)，并从程序员那里抽象出许多 TF 的低级机制(Keras 也可以运行在 Theano 之上，同样的概念也适用)。然而，在使这些构造‘现成’的过程中，粒度级控制和做非常具体的事情的能力丧失了。</p><p id="1bc9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">*为了简单起见，张量是具有类似(feature_dim，n_features)的形状元组的多维数组</p><p id="2365" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一个例子是定义接受任意数量参数的定制损失函数的能力，并且可以使用网络内部的任意张量<em class="mg">和网络外部的输入张量</em>来计算损失。<em class="mg"> </em>严格来说，<em class="mg"/>TF 中的一个损失函数甚至不需要是 python 函数，只需要对 TF 张量对象进行有效的运算组合即可。前一点很重要，因为自定义损失的能力来自于计算任意张量上的损失的能力，而不仅仅是严格意义上的监督目标张量和网络输出张量，其形式为(<em class="mg"> y_true，y_pred </em>)。</p><p id="78d8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们讨论客户损失之前，让我们简要回顾一个基本的 2 层密集网络(MLP ),看看它是如何在 TF 中定义和训练的。虽然有预定义的 TF 层，但让我们从权重和偏差张量开始定义这些层。我们想熟悉 TF 中的<strong class="kw iu">占位符</strong>和<strong class="kw iu">变量</strong>张量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="6ff3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那真是太酷了。通过用 soft max _ cross _ entropy _ with _ logits 替换损失，并用 tf.nn.softmax 替换最终 sigmoid 激活，可以修改上面的代码以用于多类分类。</p><p id="7514" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来，为了演示如何使用具有任意张量的自定义损失函数，让我们实现一个知识提取模型，该模型对二进制分类损失以及正在训练的模型和参考模型之间的损失进行优化。知识提炼是迁移学习的一种形式，我们用目标模型(我们想要训练的模型)学习，但也间接从参考模型迁移知识表示。我们将使用来自 sklearn 的<a class="ae mf" href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier" rel="noopener ugc nofollow" target="_blank">高斯过程分类器</a> (GPC)作为我们的参考模型。我们还将通过将我们的训练数据减少到 sklearn 乳腺癌数据集… <em class="mg">中 569 个样本的 1%来使问题变得更有趣，无论是参考还是目标，并从头开始训练它们。* </em>这就是所谓的少数镜头学习问题。</p><p id="7342" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">*在传统迁移学习中，参考模型通常是一个广泛和/或深入的网络，在许多示例/类上进行了预训练，而目标是一个较窄/较浅的网络，在少数可用的特定示例/类上进行了训练。</p><p id="9b57" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个知识提炼方案的损失看起来像</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/f00c2bcc41cf84bb7e0b1d3722e53a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/0*JrPlY_DF_SItDeaV"/></div></figure><p id="6334" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">二元交叉熵损失只是常规的二元分类损失，第二项涉及目标<em class="mg"> f(x) </em>和参考<em class="mg"> g(x)的输出之间的另一个损失<em class="mg"> D </em>。</em>我们设<em class="mg"> D </em>为<em class="mg"> f(x) </em>和<em class="mg">g(x)</em>之间的<a class="ae mf" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> Kullback-Leibler 散度</a> (DKL)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/1943d0911a7c5a138806c67052523acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/0*GJVWmDMR6-fGLlrW"/></div></figure><p id="7d87" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">DKL 简单地量化了一个分布<em class="mg"> f </em>与<em class="mg"> g、</em>在<em class="mg">信息</em>(大致信息与确定性成反比)<em class="mg">方面的差异；</em>可以认为是分布之间的交叉熵，是可以取负值*的不对称损失。通过最小化<em class="mg"> f </em>和<em class="mg"> g </em>之间的 DKL，我们基本上想要增加<em class="mg"> f </em>相对于<em class="mg"> g </em>的信息含量。当<em class="mg"> f </em>和<em class="mg"> g </em>的信息量相同时，上面的<em class="mg"> </em>对数项为 0，DKL 损失也为 0。当使用 GPC 作为参考模型时，使用 DKL 作为损失是有意义的，因为当从 GPC 进行预测时，我们从它的后验分布(softmax)进行采样，尽管我们的神经网络是这个后验分布的粗略近似，但它也是一个分布。</p><p id="0d1a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">*在下面的实现中，我们在组合损失中采用 abs( D_KL( f(x)，g(x))。理论上，由于对数和不等式，D_KL 将总是非负的，但是在实际计算机上用舍入误差计算 D_KL 会导致负值。</p><p id="b6e0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请注意，我们现在需要将外部输入<em class="mg"> g(x) </em>反馈到我们的损耗中。在 Keras 中，这个过程是人为的，不可扩展的。但在 TF 中，它就像创建一个新的占位张量、向组合损失添加必要的项，以及在运行训练或预测会话时输入一样简单。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="00de" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于只有 5 个训练样本，上述示例比 DKL 损失被设置为零(即没有迁移学习)的网络收敛得更快，并且给出更好的原始测试准确度。<strong class="kw iu">注意，测试集不平衡未被考虑！精明的读者应该在准确度读数上加上 F1 分数。</strong>谢天谢地，我们数据集的伪随机样本给出了正类与负类的 2:3 比例。</p><p id="f965" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样值得注意的是，迁移学习模型的 softmax 输出在来自维持者的 100 个示例上进行了测试:</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="ba83" class="mq mr it mm b gy ms mt l mu mv">[0.5134167 ]<br/> [0.5767678 ]<br/> [0.5299671 ]<br/> [0.529941  ]<br/> [0.51807505]<br/> [0.4615707 ]<br/> [0.61761355]<br/> [0.5744497 ]<br/> [0.6092696 ]<br/> [0.55092424]<br/> [0.5866923 ]<br/> [0.5522269 ]<br/> [0.5679551 ]</span></pre><p id="c9c6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">相对于非迁移学习模式</p><pre class="kj kk kl km gt ml mm mn mo aw mp bi"><span id="632b" class="mq mr it mm b gy ms mt l mu mv">[0.44836044]<br/> [0.99457294]<br/> [0.47165167]<br/> [0.573235  ]<br/> [0.94637066]<br/> [0.00909297]<br/> [0.99778605]<br/> [0.99487936]<br/> [0.99742365]<br/> [0.96588767]<br/> [0.99646676]<br/> [0.9843067 ]<br/> [0.99225134]</span></pre><p id="e7a2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">迁移学习模型的预测反映了<em class="mg">的不确定性</em>,给出了它被训练的有限信息。选择 GPC 作为参考模型有一个很好的理由，毕竟，当我们只从零开始训练 5 个样本的分类器时，我们怎么能如此确定一个新患者患有癌症？</p><p id="fdd5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">结论</strong></p><p id="86f4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们看到了如何在 TF 中从零开始实现神经网络，如何将张量运算结合到损失函数中，并触及了迁移学习的一个有趣应用。总的来说，在高级或特殊的监督学习应用中，TF 对于数据科学家来说要灵活得多。如果你喜欢阅读这篇文章和使用代码，请查看我的其他文章！</p></div></div>    
</body>
</html>