<html>
<head>
<title>Beginners Guide to the Three Types of Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">三种机器学习的初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beginners-guide-to-the-three-types-of-machine-learning-3141730ef45d?source=collection_archive---------5-----------------------#2019-10-25">https://towardsdatascience.com/beginners-guide-to-the-three-types-of-machine-learning-3141730ef45d?source=collection_archive---------5-----------------------#2019-10-25</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><figure class="it iu gq gs iv iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj is"><img src="../Images/58a203eea69932942352540f8f26816a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwJVtULuoyAlrHs0nyg1xA.png"/></div></div><figcaption class="jd je gk gi gj jf jg bd b be z dk">Visualising KMeans performance with <a class="ae jh" href="https://www.scikit-yb.org/en/latest/" rel="noopener ugc nofollow" target="_blank">Yellowbrick</a></figcaption></figure><div class=""/><div class=""><h2 id="fbfb" class="pw-subtitle-paragraph kh jj jk bd b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky dk translated">python 中的分类、回归和无监督学习</h2></div><p id="349a" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">机器学习问题一般可以分为三种。分类和回归，被称为监督学习，以及无监督学习，在机器学习应用的环境中通常指聚类。</p><p id="8f2c" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在接下来的文章中，我将简要介绍这三个问题中的每一个，并将包括流行的 python 库<a class="ae jh" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>中的一个演练。</p><p id="18f7" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在开始之前，我将简要解释一下监督学习和非监督学习这两个术语背后的含义。</p><p id="1ff6" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><strong class="lb jl">监督学习:</strong> <em class="lv">在监督学习中，你有一组已知的输入(特征)和一组已知的输出(标签)。传统上，它们被称为 X 和 y。该算法的目标是学习将输入映射到输出的映射函数。以便当给定 X 的新例子时，机器可以正确地预测相应的 y 标签。</em></p><p id="9bbe" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><strong class="lb jl">无监督学习:</strong> <em class="lv">在无监督学习中，你只有一组输入(X)，没有对应的标签(y)。该算法的目标是在数据中找到以前未知的模式。这些算法经常被用来寻找 X 的相似样本的有意义的聚类，因此实际上找到了数据固有的类别。</em></p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h2 id="53ce" class="md me jk bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">分类</h2><p id="f324" class="pw-post-body-paragraph kz la jk lb b lc mw kl le lf mx ko lh li my lk ll lm mz lo lp lq na ls lt lu in bi translated">在分类中，输出(y)是类别。这些可以是二进制的，例如，如果我们对垃圾邮件和非垃圾邮件进行分类。它们也可以是多个类别，如对<a class="ae jh" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank">花</a>的种类进行分类，这就是所谓的多类分类。</p><p id="fcfa" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">让我们使用 scikit-learn 完成一个简单的分类示例。如果您尚未安装，可以通过 pip 或 conda 安装，如这里的<a class="ae jh" href="https://scikit-learn.org/stable/install.html" rel="noopener ugc nofollow" target="_blank">所述</a>。</p><p id="3826" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">Scikit-learn 有许多可以通过图书馆直接访问的数据集。为了方便起见，在本文中，我将通篇使用这些示例数据集。为了说明分类，我将使用葡萄酒数据集，这是一个多类分类问题。在数据集中，输入(X)由与每种葡萄酒的各种属性相关的 13 个特征组成。已知的输出(y)是葡萄酒类型，在数据集中，这些类型被赋予数字 0、1 或 2。</p><p id="d990" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我在本文中使用的所有代码的导入如下所示。</p><pre class="nb nc nd ne gu nf ng nh ni aw nj bi"><span id="0649" class="md me jk ng b gz nk nl l nm nn">import pandas as pd<br/>import numpy as np</span><span id="83a3" class="md me jk ng b gz no nl l nm nn">from sklearn.datasets import load_wine<br/>from sklearn.datasets import load_boston</span><span id="767a" class="md me jk ng b gz no nl l nm nn">from sklearn.model_selection import train_test_split<br/>from sklearn import preprocessing</span><span id="d380" class="md me jk ng b gz no nl l nm nn">from sklearn.metrics import f1_score<br/>from sklearn.metrics import mean_squared_error<br/>from math import sqrt</span><span id="b686" class="md me jk ng b gz no nl l nm nn">from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC, LinearSVC, NuSVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier<br/>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/>from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis<br/>from sklearn import linear_model<br/>from sklearn.linear_model import ElasticNetCV<br/>from sklearn.svm import SVR</span><span id="7bb1" class="md me jk ng b gz no nl l nm nn">from sklearn.cluster import KMeans<br/>from yellowbrick.cluster import KElbowVisualizer<br/>from yellowbrick.cluster import SilhouetteVisualizer</span></pre><p id="077c" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在下面的代码中，我正在下载数据并转换成熊猫数据框。</p><pre class="nb nc nd ne gu nf ng nh ni aw nj bi"><span id="5c34" class="md me jk ng b gz nk nl l nm nn">wine = load_wine()<br/>wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)<br/>wine_df['TARGET'] = pd.Series(wine.target)</span></pre><p id="5034" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">监督学习问题的下一步是将数据分成测试集和训练集。算法可以使用训练集来学习输入和输出之间的映射，然后可以使用保留的测试集来评估模型学习该映射的程度。在下面的代码中，我使用 scikit-learn model_selection 函数<code class="fe np nq nr ng b">train_test_split</code>来做这件事。</p><pre class="nb nc nd ne gu nf ng nh ni aw nj bi"><span id="b947" class="md me jk ng b gz nk nl l nm nn">X_w = wine_df.drop(['TARGET'], axis=1)<br/>y_w = wine_df['TARGET']<br/>X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y_w, test_size=0.2)</span></pre><p id="fe67" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在下一步中，我们需要选择最适合学习所选数据集中的映射的算法。在 scikit-learn 中有许多不同的算法可供选择，它们都使用不同的函数和方法来学习映射，您可以在这里查看完整列表<a class="ae jh" href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="f284" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">为了确定最佳模型，我运行以下代码。我正在使用一系列算法来训练这个模型，并获得每一个算法的 F1 分数。F1 分数是分类器整体准确性的良好指标。我已经详细描述了可以用来评估分类器的各种指标<a class="ae jh" rel="noopener" target="_blank" href="/understanding-the-confusion-matrix-and-its-business-applications-c4e8aaf37f42">这里是</a>。</p><pre class="nb nc nd ne gu nf ng nh ni aw nj bi"><span id="63c4" class="md me jk ng b gz nk nl l nm nn">classifiers = [<br/>    KNeighborsClassifier(3),<br/>    SVC(kernel="rbf", C=0.025, probability=True),<br/>    NuSVC(probability=True),<br/>    DecisionTreeClassifier(),<br/>    RandomForestClassifier(),<br/>    AdaBoostClassifier(),<br/>    GradientBoostingClassifier()<br/>    ]<br/>for classifier in classifiers:<br/>    model = classifier<br/>    model.fit(X_train_w, y_train_w)  <br/>    y_pred_w = model.predict(X_test_w)<br/>    print(classifier)<br/>    print("model score: %.3f" % f1_score(y_test_w, y_pred_w, average='weighted'))</span></pre><figure class="nb nc nd ne gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj ns"><img src="../Images/7aac424ed266f47abfdf385e04ecba17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zOjll7N5gAWNNj8Rsj-aIQ.png"/></div></div></figure><p id="eb4a" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">完美的 F1 分数应该是 1.0，因此，数字越接近 1.0，模型性能越好。上述结果表明，随机森林分类器是该数据集的最佳模型。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h2 id="9aa0" class="md me jk bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">回归</h2><p id="e0e7" class="pw-post-body-paragraph kz la jk lb b lc mw kl le lf mx ko lh li my lk ll lm mz lo lp lq na ls lt lu in bi translated">在回归中，输出(y)是连续值而不是类别。回归的一个例子是预测一个商店下个月的销售额，或者你房子的未来价格。</p><p id="7ba1" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">为了再次说明回归，我将使用 scikit-learn 的数据集，即波士顿住房数据集。这由 13 个特征(X)组成，这些特征是房子的各种属性，例如房间数量、年龄和该位置的犯罪率。输出(y)是房子的价格。</p><p id="905f" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我使用下面的代码加载数据，并使用与葡萄酒数据集相同的方法将数据分成测试集和训练集。</p><pre class="nb nc nd ne gu nf ng nh ni aw nj bi"><span id="0a8a" class="md me jk ng b gz nk nl l nm nn">boston = load_boston()<br/>boston_df = pd.DataFrame(boston.data, columns=boston.feature_names)<br/>boston_df['TARGET'] = pd.Series(boston.target)</span><span id="020c" class="md me jk ng b gz no nl l nm nn">X_b = boston_df.drop(['TARGET'], axis=1)<br/>y_b = boston_df['TARGET']<br/>X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.2)</span></pre><p id="3023" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们可以使用这个<a class="ae jh" href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" rel="noopener ugc nofollow" target="_blank">备忘单</a>来查看 scikit-learn 中适用于回归问题的可用算法。我们将使用与分类问题类似的代码来循环选择并打印出每个选项的分数。</p><p id="4ee7" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">有许多不同的指标用于评估回归模型。这些本质上都是误差度量，并测量模型实现的实际值和预测值之间的差异。我使用了均方根误差(RMSE)。对于此指标，该值越接近零，模型的性能越好。这篇<a class="ae jh" href="https://www.dataquest.io/blog/understanding-regression-error-metrics/" rel="noopener ugc nofollow" target="_blank">文章</a>对回归问题的误差度量给出了很好的解释。</p><pre class="nb nc nd ne gu nf ng nh ni aw nj bi"><span id="0baa" class="md me jk ng b gz nk nl l nm nn">regressors = [<br/>    linear_model.Lasso(alpha=0.1),<br/>    linear_model.LinearRegression(),<br/>    ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,<br/>       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=None,<br/>       normalize=False, positive=False, precompute='auto', random_state=0,<br/>       selection='cyclic', tol=0.0001, verbose=0),<br/>    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,<br/>    gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,<br/>    tol=0.001, verbose=False),<br/>    linear_model.Ridge(alpha=.5)                <br/>    ]</span><span id="50cf" class="md me jk ng b gz no nl l nm nn">for regressor in regressors:<br/>    model = regressor<br/>    model.fit(X_train_b, y_train_b)  <br/>    y_pred_b = model.predict(X_test_b)<br/>    print(regressor)<br/>    print("mean squared error: %.3f" % sqrt(mean_squared_error(y_test_b, y_pred_b)))</span></pre><figure class="nb nc nd ne gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nt"><img src="../Images/ff45392cb5e4957730fcfa0e47cdcbd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UMmmKiZmII8yRmUEUYpJ9g.png"/></div></div></figure><p id="50b9" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">RMSE 分数表明，线性回归和岭回归算法对该数据集的性能最好。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h2 id="5f48" class="md me jk bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">无监督学习</h2><p id="9ae3" class="pw-post-body-paragraph kz la jk lb b lc mw kl le lf mx ko lh li my lk ll lm mz lo lp lq na ls lt lu in bi translated">有许多不同类型的无监督学习，但为了简单起见，这里我将集中讨论<a class="ae jh" href="https://en.wikipedia.org/wiki/Cluster_analysis" rel="noopener ugc nofollow" target="_blank">聚类方法</a>。有许多不同的聚类算法，所有这些算法都使用略有不同的技术来查找输入的聚类。</p><p id="0ace" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">可能最广泛使用的方法之一是 Kmeans。该算法执行迭代过程，由此启动指定数量的随机生成均值。距离度量<a class="ae jh" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">计算每个数据点到质心的欧几里德</a>距离，从而创建相似值的聚类。然后，每个聚类的质心成为新的平均值，并且重复该过程，直到获得最佳结果。</p><p id="8083" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">让我们使用在分类任务中使用的葡萄酒数据集，去掉 y 标签，看看 k-means 算法从输入中识别葡萄酒类型的能力如何。</p><p id="60e9" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">因为我们只使用这个模型的输入，所以我使用稍微不同的方法将数据分为测试和训练。</p><pre class="nb nc nd ne gu nf ng nh ni aw nj bi"><span id="8a2a" class="md me jk ng b gz nk nl l nm nn">np.random.seed(0)<br/>msk = np.random.rand(len(X_w)) &lt; 0.8<br/>train_w = X_w[msk]<br/>test_w = X_w[~msk]</span></pre><p id="95ad" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">由于 Kmeans 依赖于距离度量来确定聚类，因此通常有必要在训练模型之前执行特征缩放(确保所有特征具有相同的比例)。在下面的代码中，我使用 MinMaxScaler 来缩放特征，使所有的值都在 0 和 1 之间。</p><pre class="nb nc nd ne gu nf ng nh ni aw nj bi"><span id="e795" class="md me jk ng b gz nk nl l nm nn">x = train_w.values<br/>min_max_scaler = preprocessing.MinMaxScaler()<br/>x_scaled = min_max_scaler.fit_transform(x)<br/>X_scaled = pd.DataFrame(x_scaled,columns=train_w.columns)</span></pre><p id="b582" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">使用 K-means，您必须指定算法应该使用的聚类数。因此，第一步是确定最佳聚类数。这是通过迭代多个 k 值并将结果绘制在图表上来实现的。这就是所谓的肘方法，因为它通常会生成一个曲线，看起来有点像你的肘曲线。yellowbrick <a class="ae jh" href="https://www.scikit-yb.org/en/latest/quickstart.html" rel="noopener ugc nofollow" target="_blank">库</a>(这是一个可视化 scikit-learn 模型的伟大库，可以 pip 安装)对此有一个非常好的规划。下面的代码产生了这种可视化效果。</p><pre class="nb nc nd ne gu nf ng nh ni aw nj bi"><span id="35cd" class="md me jk ng b gz nk nl l nm nn">model = KMeans()<br/>visualizer = KElbowVisualizer(model, k=(1,8))<br/>visualizer.fit(X_scaled)       <br/>visualizer.show()</span></pre><figure class="nb nc nd ne gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nu"><img src="../Images/cf3ed10ebb198c5682db629dd4b66dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AbDwDKrPaY0tOIiiNJe_tA.png"/></div></div></figure><p id="9b67" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">通常，在使用聚类技术的情况下，我们不会知道数据集中有多少个类别。然而，在这种情况下，我们知道数据中有三种葡萄酒类型—曲线正确地选择了三种作为模型中使用的最佳分类数。</p><p id="2288" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">下一步是初始化 K-means 算法，使模型适合训练数据，并评估该算法对数据进行聚类的有效性。</p><p id="2dcf" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">用于此的一种方法被称为<a class="ae jh" href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" rel="noopener ugc nofollow" target="_blank">轮廓评分</a>。这度量了分类中值的一致性。或者换句话说，每个分类中的值彼此有多相似，以及分类之间有多大的间隔。将为每个值计算轮廓分数，范围从-1 到+1。然后将这些值绘制成轮廓图。同样，yellowbrick 提供了一种简单的方法来构建这种类型的地块。下面的代码创建了葡萄酒数据集的可视化。</p><pre class="nb nc nd ne gu nf ng nh ni aw nj bi"><span id="9900" class="md me jk ng b gz nk nl l nm nn">model = KMeans(3, random_state=42)<br/>visualizer = SilhouetteVisualizer(model, colors='yellowbrick')</span><span id="f462" class="md me jk ng b gz no nl l nm nn">visualizer.fit(X_scaled)      <br/>visualizer.show()</span></pre><figure class="nb nc nd ne gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj is"><img src="../Images/58a203eea69932942352540f8f26816a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwJVtULuoyAlrHs0nyg1xA.png"/></div></div></figure><p id="c4a4" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">侧影图可以用以下方式解释:</p><ul class=""><li id="c05d" class="nv nw jk lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">平均分数(上面的红色虚线)越接近+1，聚类中的数据点就越匹配。</li><li id="7d3e" class="nv nw jk lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">得分为 0 的数据点非常接近另一个聚类的判定边界(因此分离度很低)。</li><li id="45ab" class="nv nw jk lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">负值表示数据点可能被分配到了错误的聚类。</li><li id="6eb0" class="nv nw jk lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">每个聚类的宽度应该相当一致，如果不一致，则可能使用了不正确的 k 值。</li></ul><p id="c69f" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">上面的葡萄酒数据集的图显示，聚类 0 可能不如其他聚类一致，因为大多数数据点低于平均分数，少数数据点的分数低于 0。</p><p id="9a57" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">轮廓分数在将一种算法与另一种算法或不同的 k 值进行比较时特别有用。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><p id="45e6" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在这篇文章中，我想简单介绍一下这三种类型的机器学习。在所有这些过程中还涉及许多其他步骤，包括特征工程、数据处理和超参数优化，以确定最佳数据预处理技术和最佳使用模型。</p><p id="413f" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">感谢阅读！</p></div></div>    
</body>
</html>