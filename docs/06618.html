<html>
<head>
<title>Social Media Sentiment Analysis using Machine Learning : Part — II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于机器学习的社交媒体情感分析:第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/social-media-sentiment-analysis-part-ii-bcacca5aaa39?source=collection_archive---------1-----------------------#2019-09-22">https://towardsdatascience.com/social-media-sentiment-analysis-part-ii-bcacca5aaa39?source=collection_archive---------1-----------------------#2019-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/8a08b3f80da9f7b84c74e35fa3aa9334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l7Lr7nJ1LHWKOta8"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@tengyart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tengyart</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f3ed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大家好，让我们从第<strong class="kf ir">部分—第一部分</strong>停止的地方开始。</p><p id="f153" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本帖中，我们将讨论如何通过使用<strong class="kf ir">词汇袋</strong>和<strong class="kf ir"> TF-IDF </strong>从文本数据集中提取特征。然后，我们将看到我们如何应用<strong class="kf ir">机器学习</strong> <strong class="kf ir">模型</strong>，使用这些特征来预测一条推文是属于<strong class="kf ir">积极:【0】</strong>还是<strong class="kf ir">消极:【1】</strong>情绪。</p><p id="5c8e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">N</span><strong class="kf ir"><em class="lk">ote</em></strong>:<em class="lk">如果你还没有看过本系列的</em><strong class="kf ir"><em class="lk">Part—I</em></strong><em class="lk">一定要看一下，以便更好地理解</em> <strong class="kf ir"> <em class="lk"> Part — II。</em> </strong></p><div class="ll lm gp gr ln lo"><a href="https://medium.com/@deepakdas_13693/social-media-sentiment-analysis-49b395771197" rel="noopener follow" target="_blank"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd ir gy z fp lt fr fs lu fu fw ip bi translated">基于机器学习的社交媒体情感分析:第一部分</h2><div class="lv l"><h3 class="bd b gy z fp lt fr fs lu fu fw dk translated">社交媒体为世界各地的人们打开了一个全新的世界。人们只需点击一下鼠标就能变得巨大…</h3></div><div class="lw l"><p class="bd b dl z fp lt fr fs lu fu fw dk translated">medium.com</p></div></div><div class="lx l"><div class="ly l lz ma mb lx mc jw lo"/></div></div></a></div><p id="f051" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，我们开始吧？</p><h1 id="a9d5" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">从干净的推文中提取特征</h1><h2 id="cc1f" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">词袋特征</h2><p id="e434" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">单词包是一种从文本文档中提取特征的方法。这些特征可以用于训练机器学习算法。它创建了在训练集中的所有文档中出现的所有唯一单词的词汇表。</p><p id="7e61" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑一个名为 C of D documents {d1，d2…..dD}和从语料库 c 中提取出的 N 个唯一记号。这 N 个记号(单词)将形成一个列表，单词袋矩阵 M 的大小将由 D×N 给出。矩阵 M 中的每一行都包含文档 D(i)中记号的频率。</p><p id="0561" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，如果你有两份文件-</p><ul class=""><li id="aa0e" class="ns nt iq kf b kg kh kk kl ko nu ks nv kw nw la nx ny nz oa bi translated">D1:他是一个懒惰的男孩。她也很懒。 </li><li id="db7b" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated">D2:史密斯是个懒惰的人。T41】</li></ul><p id="41ed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，它使用所有文档中唯一的单词创建一个词汇表。</p><p id="432c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> ['他'，'她'，'懒惰'，'男孩'，'斯密'，'人'] </strong></p><p id="2f1d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们在上面的列表中看到的，我们不考虑这个集合中的<strong class="kf ir">“是”、“a”、“也”</strong>，因为它们不传达模型所需的必要信息。</p><ul class=""><li id="aced" class="ns nt iq kf b kg kh kk kl ko nu ks nv kw nw la nx ny nz oa bi translated">这里，<strong class="kf ir"> D=2，N=6 </strong></li><li id="110d" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated">大小为 2×6 的矩阵 M 将被表示为:</li></ul><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/ad0b609e1f76c8b7c7e1107486a56679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*p4ltgSCAp9tLQwHhHLqxhA.png"/></div></figure><p id="b818" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上表描述了包含每个文档中每个单词的词频的训练特征。这被称为单词袋方法，因为在这种方法中，单词的出现次数而不是顺序或次序是重要的。</p><p id="fa5c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，让我们将这个单词嵌入技术应用到我们可用的数据集。</p><p id="6dd1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们有一个名为<strong class="kf ir">计数矢量器</strong>的包来执行这项任务。</p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="c7cc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">输出:- </strong></p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi on"><img src="../Images/8ac110568df8fb936f830ee3e57b82cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYR8YK8Z8gvDQXVadbcl0g.png"/></div></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oo"><img src="../Images/826f2bde2b0258d0524933b49c24a826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GmEPyyWXoBu3GrdQCYHbrA.png"/></div></div></figure><h2 id="8267" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated"><strong class="ak"> TF-IDF 的特点</strong></h2><p id="a695" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated"><strong class="kf ir"> TF-IDF </strong>代表<strong class="kf ir">词频-逆文档频率，</strong>和<strong class="kf ir"> TF-IDF </strong>权重是信息检索和文本挖掘中经常使用的权重。该权重是一种统计度量，用于评估一个单词对集合或语料库中的文档有多重要。重要性与单词在文档中出现的次数成比例增加，但是被单词在语料库中的频率抵消。</p><p id="ccf1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常情况下，<strong class="kf ir"> TF-IDF </strong>重量由两项组成:</p><h2 id="90ea" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated"><strong class="ak">词频(TF) : </strong></h2><p id="cf1d" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">第一个计算归一化的<strong class="kf ir">项频率(TF) </strong>，aka。一个单词在文档中出现的次数，除以该文档中的总单词数。</p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ae7090d98ba5400b2db47df23d41a9d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*mTVJdMjpeZKpJaIEY9pLKA.png"/></div></figure><p id="083c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">示例:- </strong></p><p id="a775" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lk">考虑包含 100 个单词的文档，其中单词 cat 出现 3 次。</em></p><p id="7f27" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lk"/><strong class="kf ir"><em class="lk">【TF】</em></strong><em class="lk">为猫则</em><strong class="kf ir"><em class="lk">(3/100)= 0.03</em></strong></p><h2 id="3100" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated"><strong class="ak">逆文档频率(IDF) : </strong></h2><p id="2574" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">第二项是<strong class="kf ir">逆文档频率(IDF)，</strong>计算为语料库中文档数量的对数除以特定项出现的文档数量。</p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/f42f22ad905026d03a136df5c1adb91d.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*Vx94rslx0Msf0-UQfqQUOQ.png"/></div></figure><p id="17de" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">示例:- </strong></p><p id="e3e5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们有 1000 万个文档，单词 cat 出现在其中的 1000 个文档中。</p><p id="6487" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lk">然后，将</em> <strong class="kf ir"> <em class="lk">逆文档频率【IDF】</em></strong><em class="lk">计算为</em></p><p id="2cd9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk"> log(10，000，000/1000)= 4。</em>T47】</strong></p><h2 id="d2ab" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated"><strong class="ak"> TF-IDF 示例:</strong></h2><p id="3e17" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated"><strong class="kf ir">求<em class="lk"> </em> TF-IDF 重量的公式:- </strong></p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi or"><img src="../Images/74e4e003c0763498a464a98333c75914.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*pAWTy4Czff9EIxsIyA1uQQ.png"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi os"><img src="../Images/828fa655417aa333766d6e2f8b7ad2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*Uucq42G4ntPGJKzI84b3aA.png"/></div></div></figure><p id="c781" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从上面的例子来看，<strong class="kf ir">术语频率是 0.03 </strong>而<strong class="kf ir">逆文档频率是 4。</strong></p><p id="8744" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lk">这样，TF-IDF 重量就是这些量</em> <strong class="kf ir"> <em class="lk">的乘积:0.03 * 4 = 0.12。</em> </strong></p><p id="fdcc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">代码:- </strong></p><p id="4405" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们使用 Python 将这种技术应用于我们的数据集。</p><p id="c5aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在<strong class="kf ir"> Scikit-Learn </strong>中提供了一个包，称为<strong class="kf ir"> TfidfVectorizer。</strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="2281" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">输出:- </strong></p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ot"><img src="../Images/853d72b0752ada6bca499d80100b2ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PJhSU5S1NC7xQZYefHzKOg.png"/></div></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ou"><img src="../Images/9516c106173896c96391c811d5a7c999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3iZvy8ITPKe6Fz6T44s8Lg.png"/></div></div></figure><p id="f4ce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些是我们在数据集上用于特征提取的<strong class="kf ir">单词嵌入</strong>技术。</p><p id="037f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们进入下一步。</p><p id="167b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">将数据集分割成训练集和验证集。</strong></p><h1 id="24fe" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">将数据集分成训练集和验证集</h1><p id="3a1e" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">通过以上两种技术，即<strong class="kf ir">词袋和 TF-IDF </strong>，我们从数据集中的推文中提取了特征。</p><p id="7d79" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们有一个数据集包含来自<strong class="kf ir">单词袋</strong>模型的特征，另一个数据集包含来自<strong class="kf ir"> TF-IDF </strong>模型的特征。</p><p id="b4f1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一项任务是将数据集分为训练集和验证集，以便我们可以在应用模型预测未知和未标记的测试数据之前训练和测试我们的模型。</p><p id="e998" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">使用训练集的词袋特征</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="938d" class="nb me iq ow b gy pa pb l pc pd">train_bow = bow[:31962]<br/><br/>train_bow.todense()</span></pre><p id="1e63" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">输出:- </strong></p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/1329074675b615db6bcd43754ea7deb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*fh1syXM3lMR3U3xR8nWRcQ.png"/></div></figure><p id="c26d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">将 TF-IDF 中的功能用于训练集</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="db1f" class="nb me iq ow b gy pa pb l pc pd">train_tfidf_matrix = tfidf_matrix[:31962]<br/><br/>train_tfidf_matrix.todense()</span></pre><p id="8f11" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">输出:- </strong></p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/3a438799eb04baf2ff9394f8d347f340.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*xgnDDm9pXmTCieNDafrYvQ.png"/></div></figure><p id="22f1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">将数据分割成训练和验证集</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="b4d1" class="nb me iq ow b gy pa pb l pc pd">from sklearn.model_selection import train_test_split</span></pre><p id="4b74" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">词袋功能</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="38d6" class="nb me iq ow b gy pa pb l pc pd">x_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(train_bow,train['label'],test_size=0.3,random_state=2)</span></pre><p id="7382" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> TF-IDF 特点</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="5074" class="nb me iq ow b gy pa pb l pc pd">x_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(train_tfidf_matrix,train['label'],test_size=0.3,random_state=17)</span></pre><p id="f4ac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们已经将数据集分成了训练集和验证集。</p><p id="05c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想详细了解用于<a class="ae kc" href="https://en.wikipedia.org/wiki/Feature_extraction" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"/></a>特征提取的<strong class="kf ir">网袋</strong>和<strong class="kf ir"> TF-IDF </strong>型号。请务必阅读下面的博文，进行深入的讨论。</p><div class="ll lm gp gr ln lo"><a rel="noopener follow" target="_blank" href="/introduction-to-natural-language-processing-for-text-df845750fb63"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd ir gy z fp lt fr fs lu fu fw ip bi translated">文本自然语言处理导论</h2><div class="lv l"><h3 class="bd b gy z fp lt fr fs lu fu fw dk translated">读完这篇博文后，你会知道一些从一些文本中提取特征的基本技术，所以你可以使用…</h3></div><div class="lw l"><p class="bd b dl z fp lt fr fs lu fu fw dk translated">towardsdatascience.com</p></div></div><div class="lx l"><div class="pg l lz ma mb lx mc jw lo"/></div></div></a></div><p id="d76b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们来到了这个系列最期待的部分，即将<strong class="kf ir">机器学习模型</strong>应用于我们的数据集。</p><h1 id="d278" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">应用机器学习模型</h1><p id="edb9" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated"><em class="lk">我们要解决的底层问题属于</em> <strong class="kf ir"> <em class="lk">监督机器学习</em> </strong> <em class="lk">范畴。因此，在继续对我们的数据集应用不同的机器学习模型之前，让我们简单讨论一下这个主题。</em></p><h2 id="a080" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">监督机器学习:-</h2><p id="b35d" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">大多数实用的机器学习使用<strong class="kf ir">监督学习</strong>。</p><p id="920e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在监督学习中，您有输入变量(x)和输出变量(Y ),并使用算法来学习从输入到输出的映射函数。</p><h2 id="b6bb" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">Y = f(X)</h2><p id="5469" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">目标是很好地逼近映射函数，以便当您有新的<strong class="kf ir">输入</strong> <strong class="kf ir">数据(x) </strong>时，您可以预测该数据的<strong class="kf ir">输出变量(Y) </strong>。</p><p id="073a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它被称为监督学习，因为算法从训练数据集学习的过程可以被认为是教师监督学习过程。我们知道正确的答案，算法迭代地对训练数据进行预测，并由老师进行纠正。当算法达到可接受的性能水平时，学习停止。</p><p id="0e6a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">监督学习问题可以进一步分为回归和分类问题。</p><ul class=""><li id="5e89" class="ns nt iq kf b kg kh kk kl ko nu ks nv kw nw la nx ny nz oa bi translated"><strong class="kf ir">分类</strong>:分类问题是当输出变量是一个类别时，如<strong class="kf ir">【红色】</strong>或<strong class="kf ir">【蓝色】</strong>或<strong class="kf ir">【疾病】</strong><strong class="kf ir">【无疾病】</strong>或在我们的例子中<strong class="kf ir">【阳性】</strong>或<strong class="kf ir">【阴性】</strong></li><li id="7411" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated"><strong class="kf ir">回归</strong>:一个回归问题是当输出变量是一个实值时，比如<strong class="kf ir">“美元”</strong>或者<strong class="kf ir">“重量”。</strong></li></ul><p id="1002" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的问题属于分类类别，因为我们必须将我们的结果分为<strong class="kf ir">阳性</strong>或<strong class="kf ir">阴性</strong>类。</p><p id="3073" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有另一类机器学习算法叫做<strong class="kf ir">无监督机器学习</strong>你有一个输入数据，但没有相应的输出变量。无监督学习的目标是对数据中的底层结构或分布进行建模，以便了解更多关于数据的信息。但这与我们的问题陈述无关。</p><h2 id="2841" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">继续:-</h2><p id="6ae0" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">因此，从上面的数据集分割中，我们看到我们将使用来自<strong class="kf ir">单词袋</strong>和<strong class="kf ir"> TF-IDF </strong>的特征用于我们的<strong class="kf ir">机器学习模型</strong>。</p><p id="b8b2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们通常使用不同的模型来查看哪个模型最适合我们的数据集，然后我们使用该模型来预测测试数据的结果。</p><p id="73eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们将使用 3 种不同的模型</p><ul class=""><li id="3645" class="ns nt iq kf b kg kh kk kl ko nu ks nv kw nw la nx ny nz oa bi translated"><strong class="kf ir">逻辑回归</strong></li><li id="e25c" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated"><strong class="kf ir"> XGBoost </strong></li><li id="39bf" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated"><strong class="kf ir">决策树</strong></li></ul><p id="c289" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们将比较它们的性能，并选择具有最佳可能特征提取技术的最佳可能模型来预测我们的测试数据的结果。</p><h2 id="ea6a" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated"><strong class="ak"> <em class="ph">从 sklearn 导入 f1 _ score</em></strong></h2><p id="3cf3" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated"><em class="lk">我们将始终使用 F1 分数来评估我们模型的性能，而不是准确性。在这篇文章的结尾你会知道为什么。</em></p><p id="bc67" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">代码:- </strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="79f5" class="nb me iq ow b gy pa pb l pc pd">from sklearn.metrics import f1_score</span></pre><p id="0e48" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们继续对我们的数据集应用不同的模型，这些模型来自使用<strong class="kf ir">词汇袋</strong>和<strong class="kf ir"> TF-IDF 提取的特征。</strong></p><h1 id="dfac" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">逻辑回归</h1><p id="945d" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">我们要用的第一个模型是</em></strong><em class="lk"/><strong class="kf ir"><em class="lk">Logistic 回归。</em>T59】</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="8c5e" class="nb me iq ow b gy pa pb l pc pd">from sklearn.linear_model import LogisticRegression</span><span id="64d3" class="nb me iq ow b gy pi pb l pc pd">Log_Reg = LogisticRegression(random_state=0,solver='lbfgs')</span></pre><h2 id="146b" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated"><strong class="ak">文字袋功能</strong></h2><p id="514c" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">拟合 Logistic 回归模型。</em> </strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="abe5" class="nb me iq ow b gy pa pb l pc pd">Log_Reg.fit(x_train_bow,y_train_bow)</span></pre><p id="e27b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">预测概率。</em> </strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="9d9d" class="nb me iq ow b gy pa pb l pc pd">prediction_bow = Log_Reg.predict_proba(x_valid_bow)<br/><br/>prediction_bow</span></pre><p id="eeb8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">输出:- </strong></p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/6aab0850da13e469ece5a625b1a9960d.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*tz6Ah6-pTxjQrb3B_hJejw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Predicting the probabilities for a tweet falling into either Positive or Negative class.</figcaption></figure><p id="3c82" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你对上面的输出感到困惑，读一读这个堆栈溢出的回答，你会对此有一个清晰的认识。</p><div class="ll lm gp gr ln lo"><a href="https://stackoverflow.com/a/27780053/8138208" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd ir gy z fp lt fr fs lu fu fw ip bi translated">9.000000000 e-01 是一个什么样的数字？</h2><div class="lv l"><h3 class="bd b gy z fp lt fr fs lu fu fw dk translated">我声明了一个 x 变量并用 np.arange(-1，1，0.1)填充它。print(x)方法给了我类似于…</h3></div><div class="lw l"><p class="bd b dl z fp lt fr fs lu fu fw dk translated">stackoverflow.com</p></div></div><div class="lx l"><div class="pj l lz ma mb lx mc jw lo"/></div></div></a></div><p id="1f74" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出基本上为我们提供了推文落入负面或正面类别的概率。</p><p id="d0ba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">计算 F1 分数</em> </strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/304b4cf357f2e2f26a7f52e72543f88a.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*QUF5psd78A_mFgNkYRGVjQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">F1 Score</figcaption></figure><h2 id="dda0" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">TF-IDF 功能</h2><p id="0ea3" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">拟合 Logistic 回归模型。</em> </strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="7b28" class="nb me iq ow b gy pa pb l pc pd">Log_Reg.fit(x_train_tfidf,y_train_tfidf)</span></pre><p id="f849" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">预测概率。</em>T11】</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="697f" class="nb me iq ow b gy pa pb l pc pd">prediction_tfidf = Log_Reg.predict_proba(x_valid_tfidf)<br/><br/>prediction_tfidf</span></pre><p id="7a5d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">输出:- </strong></p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/3b62a16189b9525d76f8469e9e159b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*IctsJ9yAGzWILpxT58LT6g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Predicting the probabilities for a tweet falling into either Positive or Negative class</figcaption></figure><p id="cfdc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">计算 F1 分数</em> </strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/755315e76d030aac6e829aa1baf9f336.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*yuv91i-UvM9N0bRLEH1PmQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">F1 Score</figcaption></figure><p id="e6df" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">N</span><strong class="kf ir"><em class="lk">ote:</em></strong><a class="ae kc" href="https://stackoverflow.com/a/27527429/8138208" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lk">嵌套列表</em> </strong> </a> <em class="lk">中的元素[0][0]为</em> <strong class="kf ir"> <em class="lk">标签:0 或正推文</em> </strong> <em class="lk">，元素[0][1]为</em> <strong class="kf ir"> <em class="lk">标签:1 或负推文。</em> </strong></p><p id="edd4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要深入分析逻辑回归，请阅读以下文章。T47】</p><div class="ll lm gp gr ln lo"><a href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd ir gy z fp lt fr fs lu fu fw ip bi translated">机器学习的逻辑回归</h2><div class="lv l"><h3 class="bd b gy z fp lt fr fs lu fu fw dk translated">逻辑回归是机器学习从统计学领域借用的另一种技术。它是最受欢迎的…</h3></div><div class="lw l"><p class="bd b dl z fp lt fr fs lu fu fw dk translated">machinelearningmastery.com</p></div></div><div class="lx l"><div class="pn l lz ma mb lx mc jw lo"/></div></div></a></div><h1 id="fc58" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">XGBoost</h1><p id="4282" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">我们使用的下一个模型是 XGBoost。</em>T51】</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="465c" class="nb me iq ow b gy pa pb l pc pd">from xgboost import XGBClassifier</span></pre><h2 id="f858" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated"><strong class="ak">文字袋功能</strong></h2><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="9995" class="nb me iq ow b gy pa pb l pc pd">model_bow = XGBClassifier(random_state=22,learning_rate=0.9)</span></pre><p id="f08c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">拟合 XGBoost 模型</em> </strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="affb" class="nb me iq ow b gy pa pb l pc pd">model_bow.fit(x_train_bow, y_train_bow)</span></pre><p id="9c28" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">预测概率。</em> </strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="ead4" class="nb me iq ow b gy pa pb l pc pd">xgb = model_bow.predict_proba(x_valid_bow)<br/><br/>xgb</span></pre><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi po"><img src="../Images/b90d58e0e12c947140507055e2171e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*JxvFp2bwE7w50Z0R-gRYEw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Predicting the probability of a tweet falling into either Positive or Negative class.</figcaption></figure><p id="1c9d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">计算 F1 分数</em> </strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="de82" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">输出:- </strong></p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/8b8c56da65ed7b49cbee089c577b8a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*UxXI2NEZ_MQI3dHswY-XDQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">F1 Score</figcaption></figure><h2 id="7bd8" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">TF-IDF 功能</h2><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="6f1d" class="nb me iq ow b gy pa pb l pc pd">model_tfidf = XGBClassifier(random_state=29,learning_rate=0.7)</span></pre><p id="4cad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">拟合 XGBoost 模型</em> </strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="5896" class="nb me iq ow b gy pa pb l pc pd">model_tfidf.fit(x_train_tfidf, y_train_tfidf)</span></pre><p id="b14b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">预测概率。</em>T75】</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="498f" class="nb me iq ow b gy pa pb l pc pd">xgb_tfidf=model_tfidf.predict_proba(x_valid_tfidf)<br/><br/>xgb_tfidf</span></pre><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/6fb23d91e6d2245f85d753414a601511.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*pdCEX_ANDoyipU1eNYzkbQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Predicting the probability of a tweet falling into either Positive or Negative class.</figcaption></figure><p id="0d87" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">计算 F1 分数</em> </strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="b7c1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">输出:- </strong></p><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/f22563cc01672a7c083822b443fcd82e.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*iyfMHfikeQqPXhomgQ7w7w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">F1 Score</figcaption></figure><p id="be4c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要更深入地分析 XGBoost 模型，请阅读下面的文章。T85】</p><div class="ll lm gp gr ln lo"><a href="https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd ir gy z fp lt fr fs lu fu fw ip bi translated">用于应用机器学习的 XGBoost 简介</h2><div class="lv l"><h3 class="bd b gy z fp lt fr fs lu fu fw dk translated">XGBoost 是一种算法，最近一直主导着应用机器学习和 Kaggle 竞争…</h3></div><div class="lw l"><p class="bd b dl z fp lt fr fs lu fu fw dk translated">machinelearningmastery.com</p></div></div><div class="lx l"><div class="ps l lz ma mb lx mc jw lo"/></div></div></a></div><h1 id="0872" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">决策树</h1><p id="94bc" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">我们使用的最后一个模型是决策树。T89】</p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="2bd3" class="nb me iq ow b gy pa pb l pc pd">from sklearn.tree import DecisionTreeClassifier</span><span id="1054" class="nb me iq ow b gy pi pb l pc pd">dct = DecisionTreeClassifier(criterion='entropy', random_state=1)</span></pre><h2 id="dbba" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">词袋特征</h2><p id="0e46" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">拟合决策树模型。</em> </strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="d08d" class="nb me iq ow b gy pa pb l pc pd">dct.fit(x_train_bow,y_train_bow)</span></pre><p id="2c9e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">预测概率。</em> </strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="e8e5" class="nb me iq ow b gy pa pb l pc pd">dct_bow = dct.predict_proba(x_valid_bow)<br/><br/>dct_bow</span></pre><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/94be9fa7773ea0bb1b909615b695867c.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*-5sCLndeLM6PjJptf6SvXQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Predicting the probability of a tweet falling into either Positive or Negative class</figcaption></figure><p id="0c61" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">计算 F1 分数</em> </strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/21db82406ae1d4378cd4bc1a1a48baf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*X2PcfQS16kNvHLMkhAg3_Q.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">F1 Score</figcaption></figure><h2 id="dc2c" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">TF-IDF 功能</h2><p id="b9f9" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">拟合决策树模型</em> </strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="6201" class="nb me iq ow b gy pa pb l pc pd">dct.fit(x_train_tfidf,y_train_tfidf)</span></pre><p id="3998" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">预测概率。</em>T11】</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="9ca7" class="nb me iq ow b gy pa pb l pc pd">dct_tfidf = dct.predict_proba(x_valid_tfidf)<br/><br/>dct_tfidf</span></pre><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/01b0b6aa2f4b2468df0384d622dbc914.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*sJdbj2dXeEZ4kJNMwrcjFQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Predicting the probability of a tweet falling into either Positive or Negative class</figcaption></figure><p id="3806" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lk">计算 F1 分数</em> </strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/b5623b2ed9ff090581a18126c702ab2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*MjNAiFX4rt9q_tR84XkSyw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">F1 Score</figcaption></figure><p id="8371" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要更深入地分析决策树模型，请阅读下面的文章。T19】</p><div class="ll lm gp gr ln lo"><a rel="noopener follow" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-decision-trees-adb2165ccab7"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd ir gy z fp lt fr fs lu fu fw ip bi translated">理解决策树的直观指南</h2><div class="lv l"><h3 class="bd b gy z fp lt fr fs lu fu fw dk translated">理解决策树的底层机制和参数</h3></div><div class="lw l"><p class="bd b dl z fp lt fr fs lu fu fw dk translated">towardsdatascience.com</p></div></div><div class="lx l"><div class="pv l lz ma mb lx mc jw lo"/></div></div></a></div><h1 id="0a6b" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">模型比较</h1><p id="7916" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">现在，让我们用不同的单词嵌入技术来比较我们在数据集上应用的不同模型。</p><h2 id="d3fd" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated"><strong class="ak">话袋</strong></h2><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/bb06cd1a5e7e79a8f3a6015134e3122e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*08C7boT4BsG_kPb6uQQ5Eg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">F1 Score of different models using features from Bag-of-Words</figcaption></figure><p id="29a0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">对比图</strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi px"><img src="../Images/725f7481fe786f35b3106d0a7c48f94f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0M-Ogt6pC4hBP5LI0iTxyA.png"/></div></div></figure><h2 id="5492" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated"><strong class="ak"> TF-IDF </strong></h2><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi py"><img src="../Images/5683454bd6719f0004dc22f926d0dd55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*0IJ99nNjyUgxnNdF0OXYWg.png"/></div></figure><p id="ada2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">对比图</strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pz"><img src="../Images/6dc1d8f43a6f6ad68d2bc545df366c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JiUR24pwZs6KbWuEpcAE1Q.png"/></div></div></figure><p id="662c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们从<strong class="kf ir">词汇袋</strong>和<strong class="kf ir"> TF-IDF </strong>中看到的，最佳可能模型是<strong class="kf ir">逻辑回归。</strong></p><p id="cf24" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们比较一下<strong class="kf ir">逻辑回归</strong>模型与特征提取技术<strong class="kf ir">词汇袋</strong>和<strong class="kf ir"> TF-IDF 的得分。</strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/1a789a0a3c55bab04effb402c76ebec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*9isLVUvKgU9YTh5CKVGgrw.png"/></div></figure><p id="fca6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">对比图</strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pz"><img src="../Images/6eaf385809f8c1c477e840537273f10c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d67F7Rzz9N_GUwS95pO-5A.png"/></div></div></figure><h1 id="0902" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">预测我们测试数据的结果</h1><p id="8c4b" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">从上面的对比图中我们可以清楚地看到，最好的可能的<strong class="kf ir"> F1 得分</strong>是通过使用<strong class="kf ir"> TF-IDF </strong>特征的<strong class="kf ir">逻辑回归</strong>模型获得的。</p><p id="8dc5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">代码:- </strong></p><figure class="oh oi oj ok gt jr"><div class="bz fp l di"><div class="ol om l"/></div></figure><h1 id="8c3a" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">预测后的结果</h1><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="6016" class="nb me iq ow b gy pa pb l pc pd">res = pd.read_csv('result.csv')</span><span id="cfea" class="nb me iq ow b gy pi pb l pc pd">res</span></pre><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/2aea00e1bbcd2df1ea6e35e0560d1210.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*Plyw2tnjt4cgpCgBQY430Q.png"/></div></figure><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/f3a05e7b9e1d9312f0a3a3e5508653f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*eoFCSW3w8lNhdRAjreBt9w.png"/></div></figure><p id="8931" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从上面的输出我们可以看到，我们的具有<strong class="kf ir"> TF-IDF </strong>特征的<strong class="kf ir">逻辑回归</strong>模型预测一条推文是否属于<strong class="kf ir">正面-标签:0 </strong>或<strong class="kf ir">负面-标签:1 </strong>情绪的类别。</p><h1 id="36ac" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">摘要:-</h1><h2 id="a1bb" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">5.使用的特征提取技术</h2><ul class=""><li id="106f" class="ns nt iq kf b kg nn kk no ko qd ks qe kw qf la nx ny nz oa bi translated">词汇袋</li><li id="aaea" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated">TF-IDF</li></ul><h2 id="4c37" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">6.使用的机器学习模型</h2><ul class=""><li id="c7a7" class="ns nt iq kf b kg nn kk no ko qd ks qe kw qf la nx ny nz oa bi translated">逻辑回归</li><li id="5408" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated">XGBoost</li><li id="6011" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated">决策树</li></ul><h2 id="d93b" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">7.使用的评估指标</h2><ul class=""><li id="ad01" class="ns nt iq kf b kg nn kk no ko qd ks qe kw qf la nx ny nz oa bi translated">F1 分数</li></ul><p id="2af8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，我们终于到达了旅程的终点。我们完成了使用<strong class="kf ir">机器学习</strong>来预测特定推文情绪的任务。</p><h1 id="0880" class="md me iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">收拾一些残局。</h1><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/273eb576f3ef78007113f4fab30f3f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*Yzw_CrXZIfp0dc0Hkjeqsw.png"/></div></figure><p id="ad04" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出现的问题有<strong class="kf ir">“F1 成绩是多少？”</strong>和<strong class="kf ir">为什么 F1 的分数而不是准确度？”。</strong></p><p id="e204" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，在我们继续之前，你需要对术语有一个基本的概念，例如<strong class="kf ir">混淆矩阵</strong>及其内容。</p><p id="1b3c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，请参考本文，对与<strong class="kf ir">混淆矩阵</strong>相关的术语有一个基本的了解。</p><div class="ll lm gp gr ln lo"><a rel="noopener follow" target="_blank" href="/understanding-confusion-matrix-a9ad42dcfd62"><div class="lp ab fo"><div class="lq ab lr cl cj ls"><h2 class="bd ir gy z fp lt fr fs lu fu fw ip bi translated">理解混淆矩阵</h2><div class="lv l"><h3 class="bd b gy z fp lt fr fs lu fu fw dk translated">当我们得到数据，经过数据清洗，预处理和争论，我们做的第一步是把它提供给一个…</h3></div><div class="lw l"><p class="bd b dl z fp lt fr fs lu fu fw dk translated">towardsdatascience.com</p></div></div><div class="lx l"><div class="qh l lz ma mb lx mc jw lo"/></div></div></a></div><p id="7b48" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，让我们来回答上面的疑问。</p><h2 id="ceed" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">为什么 F1 是分数而不是准确度？</h2><p id="70b2" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">让我们为我们的训练数据集<strong class="kf ir">标签，即‘0’或‘1’，生成一个<strong class="kf ir">计数图</strong>。</strong></p><pre class="oh oi oj ok gt ov ow ox oy aw oz bi"><span id="7252" class="nb me iq ow b gy pa pb l pc pd">sns.countplot(train_original['label'])</span><span id="15bc" class="nb me iq ow b gy pi pb l pc pd">sns.despine()</span></pre><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/0ca71305a90f920d9a2d02e200b6ee35.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*0yqNgtrRK14SUDB-KASYEQ.png"/></div></figure><ul class=""><li id="d335" class="ns nt iq kf b kg kh kk kl ko nu ks nv kw nw la nx ny nz oa bi translated">从上面生成的<strong class="kf ir">计数图</strong>中，我们可以看到我们的数据集是多么不平衡。我们可以看到，与情绪为<strong class="kf ir">负——标签:1 </strong>的值相比，情绪为<strong class="kf ir">正——标签:0 </strong>的值在数量上相当高。</li><li id="2430" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated">因此，当我们将<strong class="kf ir">准确性</strong>作为我们的评估指标时，可能会遇到大量误报的情况。这就是为什么我们使用<strong class="kf ir"> F1 分数</strong>而不是<strong class="kf ir">准确度作为我们的评估标准。</strong></li></ul><h2 id="85af" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated">F1 成绩是多少？</h2><p id="1b5f" class="pw-post-body-paragraph kd ke iq kf b kg nn ki kj kk no km kn ko np kq kr ks nq ku kv kw nr ky kz la ij bi translated">要了解 F1 分数，我们首先要了解<strong class="kf ir">精度</strong>和<strong class="kf ir">召回。</strong></p><ul class=""><li id="34f5" class="ns nt iq kf b kg kh kk kl ko nu ks nv kw nw la nx ny nz oa bi translated"><strong class="kf ir">精度</strong>是指你的相关结果的百分比。</li><li id="eb1f" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated"><strong class="kf ir">召回</strong>是指被你的算法正确分类的相关结果总数的百分比。</li></ul><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi qj"><img src="../Images/3800f1a7ba45b0635037bab8bb4723fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXWF20kWLJSKtGhcWWVcaw.png"/></div></div></figure><ul class=""><li id="a9ba" class="ns nt iq kf b kg kh kk kl ko nu ks nv kw nw la nx ny nz oa bi translated">我们总是面临<strong class="kf ir">精度</strong>和<strong class="kf ir">召回</strong>之间的权衡情况，即高<strong class="kf ir">精度</strong>给出低<strong class="kf ir">召回</strong>，反之亦然。</li><li id="ca1d" class="ns nt iq kf b kg ob kk oc ko od ks oe kw of la nx ny nz oa bi translated">在大多数问题中，你可以优先考虑最大化精确度，或者回忆，这取决于你试图解决的问题。但总的来说，有一个更简单的衡量标准，它同时考虑了精确度和召回率，因此，你可以将这个数字最大化，以使你的模型更好。这个指标被称为<strong class="kf ir">F1-得分</strong>，它只是<strong class="kf ir">精度</strong>和<strong class="kf ir">召回</strong>的调和平均值。</li></ul><figure class="oh oi oj ok gt jr gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/240c71c7350d2120afbb55e505261c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*RrsHD7qix_BbGJ5fIaBZVg.png"/></div></figure><p id="f493" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以这个标准看起来更加容易和方便，因为你只需要最大化一个分数，而不是平衡两个独立的分数。</p></div><div class="ab cl ql qm hu qn" role="separator"><span class="qo bw bk qp qq qr"/><span class="qo bw bk qp qq qr"/><span class="qo bw bk qp qq"/></div><div class="ij ik il im in"><p id="ad92" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们已经到了这篇文章的两部分系列的结尾。我希望在这篇文章之后，你对如何从文本处理<strong class="kf ir"><em class="lk"/></strong><em class="lk">开始，并对文本数据应用</em> <strong class="kf ir"> <em class="lk">机器学习</em> </strong> <em class="lk">模型并从中提取信息有一个基本的了解。</em></p><p id="f6af" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lk">之后，我们可以尝试使用可用的 web 框架，如</em> <strong class="kf ir"> <em class="lk"> Flask、FastAPI </em> </strong> <em class="lk">等，在网站上部署我们的机器学习模型。到生产。但那是另一篇博文的内容了。</em></p><p id="8bb4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以通过以下方式联系到我:</p><p id="24f9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">LinkedIn</strong>:【https://www.linkedin.com/in/deepak-das-profile/ T2】</p><p id="fa98" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GitHub:<a class="ae kc" href="https://github.com/dD2405" rel="noopener ugc nofollow" target="_blank">https://github.com/dD2405</a></p><h2 id="91cd" class="nb me iq bd mf nc nd dn mj ne nf dp mn ko ng nh mr ks ni nj mv kw nk nl mz nm bi translated"><strong class="ak">快乐阅读！！！</strong></h2></div></div>    
</body>
</html>