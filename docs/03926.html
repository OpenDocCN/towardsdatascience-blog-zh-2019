<html>
<head>
<title>An Intuitive Explanation to Dropout</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对辍学的直观解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-explanation-to-dropout-749c7fb5395c?source=collection_archive---------13-----------------------#2019-06-20">https://towardsdatascience.com/an-intuitive-explanation-to-dropout-749c7fb5395c?source=collection_archive---------13-----------------------#2019-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b07a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><div class=""><h2 id="0e54" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">以及如何用它来对抗过度拟合</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3705ddcdfdbb64c11143c6d10ff126ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1OuAYSLQn-I5IRHT"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@rekamdanmainkan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">ray sangga kusuma</a> on <a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="e0c5" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">什么是过度拟合？</h1><p id="399e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">训练神经网络很棘手。人们应该注意，他的模型足够好，可以从现有数据中学习，也足够好，可以推广到看不见的数据。缺乏一个通用的模型主要是因为一个叫做<strong class="mc jd">过度拟合</strong>的问题。</p><p id="4e4b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">简而言之，过度拟合意味着模型在初始训练数据上达到非常高的精度，而在新出现的数据上达到非常低的精度。这就像老师总是在考试中给出同样的问题。他们的学生很容易得到高分，因为他们只是简单地记住了答案。因此，高分在这里不是一个好的衡量标准。另一个危险的后果是，学生们甚至懒得再学习这门学科了。</p><p id="d2af" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">针对过拟合有很多技巧，<strong class="mc jd"> <em class="nb">辍学</em> </strong>就是其中之一。在本文中，我们将发现辍学背后的直觉是什么，它是如何在神经网络中使用的，以及最终如何在 Keras 中实现它。</p><h1 id="dd0e" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">辍学背后的直觉</h1><p id="4bf6" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">考虑一个由参与者、一组观众和节目主持人组成的电视节目。该节目的工作原理如下:</p><ol class=""><li id="c823" class="nc nd it mc b md mw mg mx mj ne mn nf mr ng mv nh ni nj nk bi translated">在游戏开始时，主持人随机选择一部未看过的电影作为事件的主要目标。</li><li id="9e47" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">在每一个阶段，主持人展示所选电影的短片，</li><li id="22d9" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">然后问了一个关于电影到目前为止的事件的问题。</li><li id="18e5" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">每个观众都会给出一个答案，</li><li id="a2a6" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">参与者必须从观众中选择一个答案。</li><li id="ac1e" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">如果答案正确，参与者和从观众中选出的人将各获得 50 美元。</li><li id="9d70" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">如果答案是错的，他们都要付 100 美元。</li></ol><p id="f298" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">假设参与者注意到一个观众总是给出正确的答案。随着时间的推移，参与者将与此人建立信任，并会忽略其他人给出的答案。</p><p id="b591" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这种策略存在一些问题。受信任的人可能在游戏的早期阶段对问题分类很好，但在游戏的后期阶段对问题分类很差。如果只有一个人(或一小组人)总是被选中，其他观众会感到被冷落，不再关心播放的视频剪辑。这样——在以后的阶段——被信任的人将不再有帮助，而停止关注的其他观众将已经失去正确回答事件的顺序。所以，只靠一个人是不好的，肯定会损失很多钱。</p><p id="482a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">你认为我们如何解决这个问题？一个聪明的策略是总是给别人机会。通过这种方式，参与者将了解每位听众的优势，并根据问题类别知道该问哪一位。另外，这样每个人都会一直觉得有责任，有义务去关注。</p><h1 id="ab3c" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">神经网络中的辍学</h1><p id="af6a" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">你可能会问这跟神经网络有什么关系？好吧，让我们考虑以下网络…</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/2845e53da676af0e3136c982e4883159.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*c-sOl9zQqXgxOY9U2Z1Gnw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">A Simple Neural Network Example</figcaption></figure><p id="0e2f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们可以认为输入层(绿色)是主持人提出的问题，隐藏层(蓝色)中的每个神经元是观众中的一个人，输出层(红色)是一个选定观众的选择答案。如果输出层发现某个特定的神经元总是给出最佳答案，它可能会忽略其他的，而把所有的权重都给这个神经元。</p><p id="0988" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">基于我们之前的分析，我们选择禁止一些神经元回答，而给其他神经元机会。这样我们就会达到平衡，迫使所有神经元学习。这就是辍学的概念，从技术上讲，它的工作原理如下:</p><ol class=""><li id="5b72" class="nc nd it mc b md mw mg mx mj ne mn nf mr ng mv nh ni nj nk bi translated">我们指定一个退出率，它代表要退出的神经元的百分比(例如，20%的神经元)</li><li id="51ae" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">在每个阶段，我们根据预先定义的百分比移除随机神经元。</li><li id="1eca" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">我们根据剩余神经元的结果组合来计算最终输出。</li></ol><p id="bc67" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">使用这种技术，所有神经元都有机会投票，并且必须正确回答，以减少模型损失。</p><p id="88b1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这是一个神经网络在辍学前后的例子。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/57382a0ffd5bbbddc58d1f63531dfc1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OosJfL01iiPqXUHK.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Image source: <a class="ae lh" href="https://www.oreilly.com/library/view/deep-learning-for/9781788295628/" rel="noopener ugc nofollow" target="_blank">Deep Learning for Computer Vision</a></figcaption></figure><h1 id="0a2a" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">喀拉斯辍学</h1><p id="196a" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在 Keras 申请退学比你想象的要简单。你所要做的就是导入并创建一个新的 Dropout layer 对象，然后将其添加到网络结构中的相关位置。</p><pre class="ks kt ku kv gt ns nt nu nv aw nw bi"><span id="381b" class="nx lj it nt b gy ny nz l oa ob">from keras import models, layers</span><span id="3886" class="nx lj it nt b gy oc nz l oa ob">model = models.Sequential()<br/>model.add(layers.Dense(32))<br/>model.add(layers.Dropout(0.5))<br/>model.add(layers.Dense(1))</span></pre><p id="47a0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">通常，在全连接的密集层之后、输出之前添加漏失层，漏失率= 0.5 (50%)。一些最近的方法在卷积层或递归层的激活函数之后应用丢弃，丢弃率=0.2 (20%)。</p><h1 id="c445" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">最后的想法</h1><p id="c5b6" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在这篇文章中，我介绍了辍学，这是一种解决训练中过度适应问题的有趣方法。尽管如此，辍学背后的概念非常简单，当训练你的模型时，它会带来很大的改进。我试图让解释尽可能简单。所以，如果你有任何问题，请随时留下你的评论。</p><p id="f237" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果你喜欢这篇文章，请点击“鼓掌”按钮，我将不胜感激👏所以可能会传染给他人。也可以在 <a class="ae lh" href="https://twitter.com/alimasri1991" rel="noopener ugc nofollow" target="_blank"> <em class="nb">推特</em></a><em class="nb"/><a class="ae lh" href="https://www.facebook.com/alimasri91" rel="noopener ugc nofollow" target="_blank"><em class="nb">脸书</em></a><em class="nb"/><a class="ae lh" href="mailto:alimasri1991@gmail.com" rel="noopener ugc nofollow" target="_blank"><em class="nb">上关注我直接发邮件给我</em> </a> <em class="nb">或者在</em><a class="ae lh" href="https://www.linkedin.com/in/alimasri/" rel="noopener ugc nofollow" target="_blank"><em class="nb">LinkedIn</em></a><em class="nb">上找到我。</em></p></div></div>    
</body>
</html>