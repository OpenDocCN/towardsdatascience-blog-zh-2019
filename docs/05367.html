<html>
<head>
<title>My first contribution to Data Science -A Convolutional Neural Network that recognizes images of Nicolas Cage</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我对数据科学的第一个贡献——一个识别尼古拉斯·凯奇图像的卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-first-contribution-to-data-science-a-convolutional-neural-network-that-recognizes-images-of-fdf9b01c6925?source=collection_archive---------18-----------------------#2019-08-09">https://towardsdatascience.com/my-first-contribution-to-data-science-a-convolutional-neural-network-that-recognizes-images-of-fdf9b01c6925?source=collection_archive---------18-----------------------#2019-08-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5da3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解卷积神经网络如何应用于分类愚蠢的图像</h2></div><p id="b7f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在完成了吴恩达教授的流行的在线斯坦福机器学习课程后，我知道是时候进行我的第一个机器学习/深度学习项目了。我想创建一个愚蠢的神经网络，可以识别图像中的尼古拉斯凯奇。我们将使用卷积神经网络(CNN)来识别凯奇先生的图像。请记住，这是我的代码和我对机器学习/深度学习概念的理解的细分。如果您想只查看源代码或只查看。ipynb 文件将被链接到 GitHub 上的<a class="ae le" href="https://github.com/DrewScatterday/Nicolas_Cage?files=1" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="cb43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一步将是导入流行的深度学习库 Keras。Keras 将帮助我们训练 CNN 识别图像</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="341b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我不喜欢使用我不明白它在做什么的东西，我实际上发现机器学习的数学非常有趣。当我第一次学习数学时，看到数年的微积分课程实际上被用来解决一个问题是很酷的。在后面的代码中，我们将分析在使用每个导入时，每个导入在做什么。</p><ul class=""><li id="d4d8" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">顺序模型-顺序意味着可以逐层创建模型。顺序意味着只有一个输入和一个输出，就像流水线一样。层是深度学习网络中的一个单元。层包含权重、激活和偏差。</li></ul><p id="d7a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个图表有助于解释我们的 CNN 模型将会是什么样子。注意:这只是为了帮助形象化，我们在输出层中只有一个节点，而不是 7 个。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mc"><img src="../Images/7859c1b3ab5877e5a616b0e4599f8a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ijhrSf0gMNMBw24vEMwLOw.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">From the paper Real-time Eye Gaze Direction Classification Using Convolutional Neural Network by Anjith George and Aurobinda Routray (<a class="ae le" href="https://arxiv.org/abs/1605.05258" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1605.05258</a>)</figcaption></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="a40b" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">Conv2D:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="5608" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Conv2D 该层对初始输入图像执行卷积。下面的 gif 有助于说明卷积在做什么。卷积基本上是在初始图像矩阵上通过一个小的“过滤器”或“权重”框，并对图像像素值执行矩阵乘法。在训练期间，这允许我们的网络在图像中识别尼古拉斯凯奇的特征。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/0e7de8a67e0c197106c87d417e45b499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/1*ZCjPUFrB6eHPRi4eyP6aaA.gif"/></div></figure><h2 id="e6e9" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">Conv2D 参数:</h2><ul class=""><li id="ed98" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">在这里的代码中，我们将模型对象创建为顺序 Keras 模型</li><li id="d801" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">然后我们添加一个卷积层</li><li id="7dcf" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">32 意味着我们有 32 个大小为 3x3 的过滤器</li><li id="6ec3" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">输入形状意味着它是一个 200x200 的图像，3 意味着它是一个 rbg 彩色图像</li><li id="35ff" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">relu 激活函数是一个用于确保我们的值被缩放到特定范围的函数。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e4aa75c72cb13df52394eee087efed7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*2C3Z9XhWvvhUFU8TKzor-w.png"/></div></figure><ul class=""><li id="58fe" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">因此，如果 relu 的输入是&lt;= 0 it will make the value 0, otherwise, if the input is &gt; 0，那么它将只输出输入</li><li id="f514" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">激活功能被用来模拟我们大脑神经元的活动方式。当神经网络进行计算时，层中的节点可能输出不在特定范围(如 0 和 1)之间的值。激活功能有助于将我们的数字线强制或“挤压”成我们想要的特定输出范围。</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="41a4" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">最大池:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><ul class=""><li id="c092" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">最大池 2d-最大池有时也称为缩减采样。我们从卷积创建的卷积特征矩阵是(初始图像像素矩阵*滤波器矩阵)。池查看该矩阵，并在给定区域中取最大值。下图有助于解释最大池如何对图像进行缩减像素采样。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi od"><img src="../Images/a63f1ce245dd3b6ac4c427681cb64cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*2V9cFLEoVQz_hZpWpFzb5g.png"/></div></figure><ul class=""><li id="3cdb" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">在这里，我们初始化我们的池层，我们创建它的大小为 2x2，就像上面的图表一样</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="ad74" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">隐藏层:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><ul class=""><li id="6153" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">这段代码增加了两层卷积和最大池，以帮助我们的网络更好地找到我们的图像特征。我最初没有在我们的网络中添加这些额外的隐藏层，我有 89%的测试准确率。我添加了这两层，达到了 91%的测试精度。</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="4556" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">展平:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><ul class=""><li id="dda1" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">展平-展平操作采用矩阵或张量，并将其展平为一个很长的 1D 值数组。展平层通常紧跟在最后一个池层之后。展平操作是必要的，因为在我们应用卷积来查找尼古拉斯凯奇的要素后，我们需要将这些值展平到一个长 1D 数组中，并将其馈送到我们完全连接的密集网络层。如果你看看 CNN 图表的末尾，你就会明白我所说的馈入末尾密集层网络的值是什么意思。</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="f773" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">密集:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><ul class=""><li id="1ccb" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">密集-密集图层是一种线性运算，其中每个输入都通过权重连接到每个输出(因此有 n_inputs * n_outputs 个权重)。如果你看我们的第一个 CNN 概观图的末尾，那是密集层被使用的地方。</li><li id="291f" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我们在这里将单位设置为 64。单位是指密集层中的节点数。单位值始终介于输入节点数和输入节点数之间。选择单位数时，建议使用 2 的幂。</li><li id="7f3d" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我们再次使用 relu activation 函数来避免负值，并将我们的值保持在某个范围内。</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="1fa0" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">辍学:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><ul class=""><li id="f492" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">辍学是深度学习和神经网络中使用的一种正则化技术</li><li id="6b0b" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">正则化是一种技术，用于帮助网络不过度适应我们的数据</li><li id="e3c6" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">过度拟合是指我们的神经网络在训练数据上表现良好，但在测试数据上表现很差。这意味着网络没有很好地概括，这意味着它对以前没有见过的新图像的分类不正确/很差</li><li id="6632" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">在正式的<a class="ae le" href="http://jmlr.org/papers/v15/srivastava14a.html" rel="noopener ugc nofollow" target="_blank">论文</a>中解释说:“在神经网络中，每个参数接收到的导数告诉它应该如何改变，从而减少最终的损失函数，给定所有其他单元正在做的事情。因此，单位可能会以某种方式改变，以修正其他单位的错误。这可能导致复杂的共同适应。这反过来会导致过度拟合，因为这些共同适应不会推广到看不见的数据。”</li><li id="909f" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">因此，我们本质上关闭了一层中的一些神经元，以便它们在网络权重的更新(反向传播)期间不学习任何信息。这使得其他活跃的神经元可以更好地学习并减少错误。</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="3481" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">输出层:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><ul class=""><li id="7df2" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">这是输出我们预测的最后一层。我们只让它成为一个单位，因为我们希望它输出 0 或 1。1 是包含尼古拉斯·凯奇的图像，0 是不包含尼古拉斯·凯奇的图像</li><li id="cec5" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">这里我们使用一个不同的激活函数，称为 sigmoid。Sigmoid 用于“挤压”0 或 1 之间的最后一个值。下面是一张 sigmoid 函数的图片，上面有它的方程式。</li><li id="c516" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">它将我们的输出浓缩成 0 或 1。非常负的输入接近 0，非常正的输入接近 1。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/5e380b518a0c93a78974b9a70366de90.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*xKeY7nyqITsbJGjGQ0_hjA.png"/></div></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="c2a2" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">编译我们的模型:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><ul class=""><li id="66ac" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">这里我们编译我们的模型。我们将会遇到一点数学难题，所以抓紧你的帽子。</li></ul><h2 id="2b12" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">梯度下降:</h2><p id="6236" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">让我们首先分解什么是基本梯度下降。梯度下降是一个最小化损失函数的过程:</p><ol class=""><li id="6e89" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld oi lz ma mb bi translated">我们从损失函数曲线上的一点开始</li><li id="1972" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld oi lz ma mb bi translated">我们选择一个学习率，它将决定下降多少步</li><li id="898b" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld oi lz ma mb bi translated">然后我们在这一点上求导，也称为“下降”或梯度</li><li id="8fcf" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld oi lz ma mb bi translated">我们将下降乘以学习率值</li><li id="6acf" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld oi lz ma mb bi translated">我们将初始点移动到曲线上的这个新的(下降*学习率)值</li><li id="149c" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld oi lz ma mb bi translated">我们重复这一过程，直到达到指定的迭代次数或其他停止条件，如收敛于局部最小值</li><li id="bbc1" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld oi lz ma mb bi translated">下面是一个梯度下降的 gif，有助于可视化的过程:</li></ol><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/d76b9c06b5a0c44fa44f975a8789099a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*URgtPEeEylbOtvgUoTgU_A.gif"/></div></figure><h2 id="f2ce" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">随机梯度下降(SGD):</h2><ul class=""><li id="6fe4" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">这里，我们选择使用随机梯度下降优化算法来最小化我们的损失函数。</li><li id="bebf" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">SGD 是常规梯度下降的增强版本。让我们看看数学。</li></ul><p id="f234" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">常规梯度下降(有时称为批量梯度下降)对于大型数据集来说效率非常低，因为它会查看所有训练示例。下面是批量梯度下降的数学方程。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/69b76752f34bff0b696c9816a74fd0b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*HoZcRVr_8WfdQ3_a8PUMww.png"/></div></figure><ul class=""><li id="3d3c" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">Jtrain 是成本或损失函数。这个例子摘自吴恩达在斯坦福大学的机器学习课程。他使用平方误差作为损失函数，我们将使用对数损失函数，因此我们的示例略有不同。</li><li id="33c0" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">然后，我们对每个训练样本进行重复，并计算损失函数的偏导数。</li></ul><h2 id="46ff" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">SGD 的数学原理:</h2><ul class=""><li id="a52c" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">SGD 与批量梯度下降非常相似，但是它在循环过程中计算相对于<strong class="kk iu">一个</strong>训练样本的梯度，而不是相对于<strong class="kk iu">所有</strong>训练样本的梯度。</li><li id="8d4e" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">因此，如果我们有 20，000 个样本，批量梯度下降将扫描所有 20，000 个样本，然后在每次循环 20，000 个样本后改变我们神经网络的参数。</li><li id="e0d0" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">而 SGD 在我们遍历每个例子时会一点一点地调整梯度。所以 SGD 查看单个示例，然后在遍历每个示例时调整我们的参数。以下是新加坡元的数学公式:</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/c5c71ec483eb91ef1cbc2928d09506f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*3uvib-s6c8eoTlIZPSIuxA.png"/></div></figure><h2 id="6ddd" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">二元交叉熵:</h2><ul class=""><li id="86b1" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">通过随时间降低损失函数来训练神经网络。</li><li id="c6df" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">损失函数衡量我们的模型与实际发生的情况相比表现有多差。</li><li id="8a55" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">在机器学习/深度学习中，有两种类型的损失函数——一种用于分类，一种用于回归。因为我们正在对图像进行分类，所以我们想要使用分类损失函数。</li><li id="38ea" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我们选择二元交叉熵或有时称为对数损失作为我们的损失函数，因为我们希望输出在 0 或 1 之间。</li><li id="416b" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">交叉熵的方程式如下图所示</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/3d03a77bae900e120f1276a9a6853cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*GGdsJKF2aP-xmagJk68mOw.png"/></div></figure><p id="cde1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇<a class="ae le" rel="noopener" target="_blank" href="/common-loss-functions-in-machine-learning-46af0ffc4d23">文章</a>解释了我们为什么使用交叉熵:</p><ul class=""><li id="44a4" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">注意，当实际标签为 1 (y(i) = 1)时，函数的后半部分消失，而当实际标签为 0 (y(i) = 0)时，函数的前半部分消失。简而言之，我们只是将实际预测概率的对数乘以地面真实类。一个重要的方面是，交叉熵损失严重惩罚了那些有信心但却是错误的预测。”</li><li id="d95f" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">因此，换句话说，交叉熵损失随着预测概率偏离实际标签而增加。因此，当实际观察值为 1 时，预测 0.010 的概率将是糟糕的，并导致高损失值。完美的模型的对数损失为 0。</li><li id="f8de" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">下图是交叉熵损失函数的样子:</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi om"><img src="../Images/24cc8c7a7f3ac239d4f3d5ed45da6ade.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*nRm22de7rpGDteDUSNcJAw.png"/></div></figure><ul class=""><li id="bdb8" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">因此，为了用方程阐明我们的上述直觉，请注意，随着预测概率在图上接近 1，对数损失缓慢下降。随着预测概率接近 0，测井曲线损失迅速增加。日志丢失严重惩罚了自信和错误的预测。</li></ul><h2 id="33d4" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">指标:</h2><ul class=""><li id="4df6" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">我们将参数度量作为“准确性”传递，因为我们使用准确性作为度量来衡量模型的性能。</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="8f5d" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">创建我们的数据:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><h2 id="4eca" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">尼古拉斯·凯奇:</h2><ul class=""><li id="d2e7" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">我找不到尼古拉斯·凯奇的现有图像数据集，所以我创建了自己的数据集，使用包<a class="ae le" href="https://github.com/hardikvasa/google-images-download" rel="noopener ugc nofollow" target="_blank"> google-images-download </a>批量下载尼古拉斯·凯奇的图像。为了避免重复，我在 500 个图片搜索中指定了两个不同的日期。</li><li id="f469" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">上面的代码循环遍历我们的数据，并用一个数字和一个 Nicolas cage 的标签标记所有图像。</li><li id="649c" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">在手动删除了包含其他人的图片或不正确的图片后，我收集了 207 张尼古拉斯·凯奇的图片。</li></ul><h2 id="7d0d" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">不是尼古拉斯·凯奇:</h2><ul class=""><li id="19d2" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">从这个项目开始，我想我可以做一个单类分类神经网络。我只会有一类尼古拉斯凯奇的图像，并把它提供给网络。我想在训练期间，网络会学习尼古拉斯凯奇的特征，并能够在照片中认出他。</li><li id="0b5c" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我最终了解到，网络需要“负面”的例子来帮助它了解 Nic 框架映像和非 Nic 框架映像的样子。因此，我们需要收集随机图像的图像数据集，这些图像不是尼古拉斯·凯奇的。</li><li id="ee29" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我用这个 python 代码和 picsum 网站生成并保存了 207 张非尼古拉斯·凯奇的随机图像。</li><li id="373e" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">下一步是创建我们的训练集和测试集目录。</li></ul><h1 id="44fd" class="mn mo it bd mp mq on ms mt mu oo mw mx jz op ka mz kc oq kd nb kf or kg nd ne bi translated">训练和测试设备:</h1><ul class=""><li id="345d" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">因为我有少量的数据，所以我决定只手动创建和组织图像的测试和训练集。在一个有成千上万张图片的项目中，使用 python 来创建目录并使用 for 循环来组织数据会更现实。</li><li id="2226" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我决定对我们的数据使用 70/30 分割，因为我们没有大量的数据。所以 70%的数据作为训练数据，30%作为测试数据。</li><li id="e1c5" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">因此，我们的训练数据将有 145 张尼古拉斯·凯奇的图像，而我们的测试数据将有 62 张尼古拉斯·凯奇的图像。</li><li id="15bb" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我在我们的 train 和 test 目录中创建了两个子文件夹，分别名为“class0”和“class1”。类 1 包含我们的尼古拉斯凯奇的图像，类 0 包含我们的随机图像。</li><li id="ca4f" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">Keras flow_from_directory 按字母数字顺序读取我们的 train 和 test 目录中的文件夹。我花了一段时间才弄明白这一点，因为起初，我的网络将尼古拉斯·凯奇的照片标记为“0”类。这是因为我有两个文件夹:“尼古拉斯凯奇”和“非尼古拉斯”。当时是把“Nicolas_Cage”读成了 0 类。然后我解决了这个问题，看到了我的网络的巨大成果。</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="3809" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">图像数据生成器:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="4b44" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这段代码的打印语句和输出为我们提供了:</p><pre class="lm ln lo lp gt os ot ou ov aw ow bi"><span id="a846" class="ng mo it ot b gy ox oy l oz pa">Found 290 images belonging to 2 classes.<br/>Found 124 images belonging to 2 classes.<br/>{'class0': 0, 'class1': 1}</span></pre><p id="b057" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们希望预处理我们的图像，使它们准备好输入神经网络。我们在 Keras 内部使用 ImageDataGenerator 类。该类接收一批图像，并对图像进行缩放和剪切等增强操作。然后，它将增强的数据传递给神经网络。</p><h2 id="4865" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">什么是数据增强？：</h2><ul class=""><li id="e586" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">根据超级有用的网站<a class="ae le" href="https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/" rel="noopener ugc nofollow" target="_blank"> pyimagesearch </a>的说法，数据增强通过应用随机抖动和扰动(但同时确保数据的类别标签不变)，从原始样本中生成“新的”训练样本。</li><li id="189e" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">它本质上创建了与原始训练样本略有“不同”的新训练样本。</li><li id="9e6d" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">应用数据扩充的目标是提高模型的概化能力。我们使用增强，因为我们有这么少量的图像，这将有助于我们的模型概括尼古拉斯·凯奇的新图像。下面是一张有用的图片，解释了增强的作用:</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/bb5be6a8c15019181ea320abe74b012d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*pHXe5W7rZZFcvOujxKBzDA.png"/></div></figure><h2 id="2d6f" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">参数:</h2><ul class=""><li id="77e2" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">重新缩放-我们以 1 的比例缩放图像。/255</li><li id="3ff7" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">剪切范围——我们指定图像被剪切或向某个方向移动的范围</li><li id="4499" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">缩放范围——我们为图像指定一个随机放大的范围</li><li id="9bd2" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">horizontal _ flip 水平随机翻转输入</li><li id="9cfa" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">目标尺寸——是我们输入图像的尺寸，每个图像的尺寸都将调整为 200x200</li><li id="f4f4" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">batch _ size——我们分批循环我们的图像，因此我们使我们的批量大小为 32 的图像变大</li><li id="db61" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">class _ mode——我们将其设置为“二进制”,因为我们只有两个类来预测尼古拉斯·凯奇是否出现在图像中</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="b539" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">拟合模型:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><pre class="lm ln lo lp gt os ot ou ov aw ow bi"><span id="c299" class="ng mo it ot b gy ox oy l oz pa">Epoch 1/25<br/>1000/1000 [==============================] - 913s 913ms/step - loss: 0.3476 - acc: 0.8502 - val_loss: 2.2280 - val_acc: 0.5000<br/>Epoch 2/25<br/>1000/1000 [==============================] - 907s 907ms/step - loss: 0.1354 - acc: 0.9564 - val_loss: 0.5738 - val_acc: 0.8629<br/>Epoch 3/25<br/>1000/1000 [==============================] - 904s 904ms/step - loss: 0.0675 - acc: 0.9825 - val_loss: 0.6880 - val_acc: 0.8710<br/>Epoch 4/25<br/>1000/1000 [==============================] - 910s 910ms/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.7560 - val_acc: 0.8710<br/>Epoch 5/25<br/>1000/1000 [==============================] - 952s 952ms/step - loss: 0.0454 - acc: 0.9893 - val_loss: 0.7865 - val_acc: 0.8710<br/>Epoch 6/25<br/>1000/1000 [==============================] - 908s 908ms/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.7694 - val_acc: 0.8952<br/>Epoch 7/25<br/>1000/1000 [==============================] - 908s 908ms/step - loss: 0.0833 - acc: 0.9851 - val_loss: 0.7052 - val_acc: 0.8790<br/>Epoch 8/25<br/>1000/1000 [==============================] - 914s 914ms/step - loss: 0.0103 - acc: 0.9977 - val_loss: 0.7506 - val_acc: 0.8952<br/>Epoch 9/25<br/>1000/1000 [==============================] - 909s 909ms/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.7203 - val_acc: 0.9032<br/>Epoch 10/25<br/>1000/1000 [==============================] - 905s 905ms/step - loss: 0.0035 - acc: 0.9992 - val_loss: 0.7409 - val_acc: 0.8952<br/>Epoch 11/25<br/>1000/1000 [==============================] - 934s 934ms/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.8968 - val_acc: 0.8952<br/>Epoch 12/25<br/>1000/1000 [==============================] - 1193s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7880 - val_acc: 0.9032<br/>Epoch 13/25<br/>1000/1000 [==============================] - 1189s 1s/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.7822 - val_acc: 0.9113<br/>Epoch 14/25<br/>1000/1000 [==============================] - 1194s 1s/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.7832 - val_acc: 0.9032<br/>Epoch 15/25<br/>1000/1000 [==============================] - 1196s 1s/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.7775 - val_acc: 0.9032<br/>Epoch 16/25<br/>1000/1000 [==============================] - 1195s 1s/step - loss: 8.3008e-04 - acc: 0.9998 - val_loss: 0.8340 - val_acc: 0.9032<br/>Epoch 17/25<br/>1000/1000 [==============================] - 1198s 1s/step - loss: 0.0072 - acc: 0.9988 - val_loss: 0.7819 - val_acc: 0.8952<br/>Epoch 18/25<br/>1000/1000 [==============================] - 1201s 1s/step - loss: 0.0020 - acc: 0.9997 - val_loss: 0.7950 - val_acc: 0.9113<br/>Epoch 19/25<br/>1000/1000 [==============================] - 1202s 1s/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.7827 - val_acc: 0.9113<br/>Epoch 20/25<br/>1000/1000 [==============================] - 1170s 1s/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.8283 - val_acc: 0.9032<br/>Epoch 21/25<br/>1000/1000 [==============================] - 906s 906ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.8592 - val_acc: 0.8952<br/>Epoch 22/25<br/>1000/1000 [==============================] - 905s 905ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.8227 - val_acc: 0.9032<br/>Epoch 23/25<br/>1000/1000 [==============================] - 907s 907ms/step - loss: 8.1553e-04 - acc: 0.9997 - val_loss: 0.8221 - val_acc: 0.9113<br/>Epoch 24/25<br/>1000/1000 [==============================] - 934s 934ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.8540 - val_acc: 0.9032<br/>Epoch 25/25<br/>1000/1000 [==============================] - 1189s 1s/step - loss: 7.6795e-04 - acc: 0.9998 - val_loss: 0.8570 - val_acc: 0.9113</span></pre><ul class=""><li id="1ad3" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">这段代码使我们的模型能够对尼古拉斯·凯奇的图像进行分类。如你所见，在第 25 个纪元结束时，我们能够达到 99%的测试准确度和 91%的测试准确度。</li><li id="6e48" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">由于我们在训练期间对数据进行扩充，因此我们使用 classifier.fit_generator 函数。classifier.fit 在这里不起作用，因为我们正在对数据进行扩充。</li></ul><h2 id="3450" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">参数:</h2><ul class=""><li id="c375" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">training_set —我们传入训练集 ImageDataGenerator，以在训练过程中扩充我们的训练图像</li><li id="2565" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">steps_per_epoch —我们的 fit_generator 无限循环，所以我们指定我们希望它循环多少次</li><li id="d652" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">次数-定义学习算法在整个训练数据集中工作的次数</li><li id="5a34" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">test _ set——我们传递测试集 ImageDataGenerator 来增加训练期间的测试图像</li><li id="04f2" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">val _ steps——在每个时期结束时停止之前，测试数据生成器产生的总步骤数(样本批次)</li></ul><h1 id="0cce" class="mn mo it bd mp mq on ms mt mu oo mw mx jz op ka mz kc oq kd nb kf or kg nd ne bi translated">“学习”:</h1><p id="256c" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">神经网络通过称为反向传播的过程进行学习。喀拉山脉。fit 函数为我们自动执行这个过程，所以我们不必手动编写代码，但是到底什么是反向传播呢？对我理解这一点帮助最大的视频是<a class="ae le" href="https://youtu.be/UJwK6jAStmg?t=130" rel="noopener ugc nofollow" target="_blank">神经网络揭秘</a>和<a class="ae le" href="https://youtu.be/Ilg3gGewQ5U" rel="noopener ugc nofollow" target="_blank"> 3Blue1Brown 的背部道具视频</a></p><h2 id="51f6" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">正向传播:</h2><ul class=""><li id="3e46" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">神经网络接受输入并执行一个称为前向传播的过程。为了理解反向传播，我们首先需要理解正向传播的过程。</li><li id="49cb" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">神经网络中连接每个神经元的每一条细线都被称为突触，每一条都有一个“权重”值。下图有助于展示小型网络的可视化示例。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/8129a7c35d19c8a595c8e24aac463913.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*MsxjtLtFwKSPsq0jVRRXQw.png"/></div></figure><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="1e92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这显然是一个非常简单的神经网络，仅包含一个具有 3 个节点的隐藏层，但是前向传播在更大和更复杂的神经网络上同样有效。它本质上是每次向前馈送前一层输入值，并在我们的输入和权重矩阵加上我们的偏差之间应用矩阵乘法。然后，它应用一个激活函数将我们的值“挤压”到期望的范围内。开始向前传播将输出可怕的预测值。在我们的培训中，您可以看到一个这样的例子，因为我们在第一个时期的测试准确率为 50%。Back prop 将帮助调整我们网络中的权重值，以帮助我们的网络实际学习。</p><h2 id="a9cc" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">反向传播:</h2><ul class=""><li id="dcc4" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">神经网络通过最小化“成本”或“损失”函数来学习。当神经网络在前向传播后输出预测时，我们通过查看成本函数来查看该预测有多错误。我们的成本函数是对数损失函数。所以我们用梯度下降来最小化这个函数。</li><li id="842b" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">在梯度下降过程中，我们对曲线上的点求导。这个导数告诉我们切线的斜率。我们想把损失曲线上的点移到负斜率方向。基本上，我们希望将损失曲线上的点导向负斜率，因为我们希望最小化成本函数。但是我们如何得到这个导数呢？</li></ul><p id="c391" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在令人惊叹的 3Blue1Brown 反向传播视频中解释道，在高层次上，反向传播是确定对我们网络中的权重和偏差进行哪些改变或微小“推动”的过程，这些改变或微小“推动”将导致基于单个训练示例的成本函数最有效、最快速地降低。反向传播是递归的，这意味着输出图层斜率取决于前一图层斜率，而前一图层斜率又取决于整个网络中的前一图层斜率，依此类推。让我们用微积分来看看这意味着什么:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/4dc52b46c279adf575d83f976565c610.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*EL0X_LC4VctvFnmozQBZrQ.png"/></div></figure><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/c741757f52c3a46d03dbcb9275a0c9de.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*80YlWpT5WttWEbWEYNcGvw.png"/></div></figure><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><h2 id="7e34" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">将所有这些放在一起:</h2><ul class=""><li id="6d50" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">上述方程被简化为在简单网络的每一层中具有一个神经元。这是计算反向传播中导数的更高级的综合方程。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/877fd5d21431f601146e68512209687b.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*wXBDiofgJ_oXUn4LJw0e1w.png"/></div></figure><p id="b3fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这基本上与之前的等式相同，但在处理每层中有不止一个神经元的网络时稍微先进了一点，这就是<strong class="kk iu"> jk </strong>可迭代索引的来源。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/93d484b6b071c5f6ceb3f5f628382782.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*ZldEeGFw5D245Q44Pr1Ygg.png"/></div></figure><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/a35789460b5207e16189b50bf4d89d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*oBavhPNf8YLJRne52bPTTQ.png"/></div></figure><p id="ed3d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了让我们的网络运行良好，有必要理解看起来很粗糙的微积分吗？谢天谢地没有，因为 Keras 自动为我们做这个过程，但我认为这是有趣的数学兔子洞。反向传播非常令人困惑，老实说，当我看所有的微积分时，我有时还是会迷路。很多时候，我仍然不得不去回顾它，观看有用的 youtube 视频，试图完善我的理解。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="659a" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">准确度和损耗:</h1><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/9dd6f2ade3c96f1948c48626f4c56142.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*XCyY9hGu6VK_2FprkK_Zfw.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Accuracy</figcaption></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/8c08e20e84f876f7b353931fbe9d21b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*DENGqQ5IjOGtvYRehDF6qw.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Loss</figcaption></figure><ul class=""><li id="6e27" class="lt lu it kk b kl km ko kp kr lv kv lw kz lx ld ly lz ma mb bi translated">这段代码让我们可以可视化我们的模型准确性和模型损失。代码来自 Keras 文档中解释的<a class="ae le" href="https://keras.io/visualization/" rel="noopener ugc nofollow" target="_blank">这里的</a>。</li><li id="6e92" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">正如我们所看到的，我们的测试和训练精度非常接近第 25 个纪元，我们的模型损失也是如此！</li><li id="55c8" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我们以 91%的测试准确率和 99%的训练准确率结束了我们的训练，这已经很不错了。</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="7566" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">做出预测:</h1><ul class=""><li id="fcf3" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">正如你所看到的，我们的模特认出了尼古拉斯·凯奇的这些美丽而绝对鼓舞人心的照片。</li><li id="8f67" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我们的模型也能识别出不是尼古拉斯凯奇的图像，比如岩石的图片或者天空的图片。</li><li id="2db5" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我对我们网络的结果非常满意。我们的网络甚至能够识别尼古拉斯·凯奇的愚蠢的迷因图片，比如他的脸被编辑到比尔·克林顿的脸上。</li></ul><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="244d" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">结论:</h1><ul class=""><li id="0fc8" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">在做这个项目的过程中，我学到了很多关于深度学习的知识，并且在创建它的过程中获得了很多乐趣。我在斯坦福上的课是吴恩达教授的很受欢迎的机器学习课。这门课程给了我一个很好的机器学习概念的基础，并向我介绍了神经网络、反向传播、线性回归，甚至支持向量机等概念。但是我知道最重要的学习来自于钻研和创造一个项目。</li><li id="b5a4" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">我想我也许可以收集更多尼古拉斯·凯奇的数据，以获得更高的测试准确性，但不管怎样，我对结果很满意，因为模型能够很好地识别我们的模因。</li><li id="e9bd" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">如果我将二进制分类应用于不同的数据集，如分类星系和黑洞图像，甚至太空中的星座，这个项目可能会更有趣，但神经网络识别尼古拉斯·凯奇图像的想法似乎太有趣了，太好了，不能错过。</li></ul><h2 id="cdc7" class="ng mo it bd mp nh ni dn mt nj nk dp mx kr nl nm mz kv nn no nb kz np nq nd nr bi translated">联系人:</h2><ul class=""><li id="8dea" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated">如果你有问题、担忧、建设性的批评，或者想看到更多未来的项目，请查看我的<a class="ae le" href="https://github.com/DrewScatterday" rel="noopener ugc nofollow" target="_blank"> github </a>和我的<a class="ae le" href="https://www.linkedin.com/in/drew-scatterday-415146147/" rel="noopener ugc nofollow" target="_blank"> linkedin </a>。感谢阅读！</li></ul></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="41ab" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">资源:</h1><ul class=""><li id="d301" class="lt lu it kk b kl ns ko nt kr nu kv nv kz nw ld ly lz ma mb bi translated"><a class="ae le" href="https://ml-cheatsheet.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">https://ml-cheatsheet.readthedocs.io/en/latest/</a></li><li id="53b5" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated"><a class="ae le" href="https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8" rel="noopener ugc nofollow" target="_blank">https://becoming human . ai/building-an-image-classifier-using-deep-learning-in-python-total-from-a-初学者视角-be8dbaf22dd8 </a></li><li id="b579" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated"><a class="ae le" href="https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2018/12/31/keras-conv2d-and-卷积-层/ </a></li><li id="3d31" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated"><a class="ae le" rel="noopener" target="_blank" href="/common-loss-functions-in-machine-learning-46af0ffc4d23">https://towards data science . com/common-loss-functions-in-machine-learning-46 af 0 ffc 4d 23</a></li><li id="fe74" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated"><a class="ae le" href="https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.2-using-convnets-with-small-datasets.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/fchollet/deep-learning-with-python-notebooks/blob/master/5.2-using-conv nets-with-small-datasets . ipynb</a></li><li id="ad44" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated"><a class="ae le" href="https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2" rel="noopener">https://medium . com/Octavian-ai/which-optimizer-and-learning-rate-I-use-for-deep-learning-5a CB 418 F9 b 2</a></li><li id="99f7" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated"><a class="ae le" href="https://www.charlesbordet.com/en/gradient-descent/#" rel="noopener ugc nofollow" target="_blank">https://www.charlesbordet.com/en/gradient-descent/#</a></li><li id="9642" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated"><a class="ae le" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> https://keras.io </a></li><li id="70ec" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">sendex YouTube—<a class="ae le" href="https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ</a></li><li id="6b0c" class="lt lu it kk b kl nx ko ny kr nz kv oa kz ob ld ly lz ma mb bi translated">siraj Raval YouTube—<a class="ae le" href="https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A</a></li></ul></div></div>    
</body>
</html>