<html>
<head>
<title>Feature selection by random search in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中随机搜索的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-by-random-search-in-python-730ffd2912e9?source=collection_archive---------18-----------------------#2019-07-14">https://towardsdatascience.com/feature-selection-by-random-search-in-python-730ffd2912e9?source=collection_archive---------18-----------------------#2019-07-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="30b4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何在 Python 中使用随机搜索进行要素选择</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/14a9b91c9a4c90db4b662131bd9f027e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-lJ3huoCTMdRAL5L"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@ian_gonz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ian Gonzalez</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="21be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">特征选择</strong>一直是机器学习的一大任务。根据我的经验，我可以肯定地说，特征选择<strong class="lb iu">比型号选择本身更重要</strong>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="5694" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">特征选择和共线性</h1><p id="65c0" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我已经写了一篇关于功能选择的<a class="ae ky" href="https://medium.com/data-science-journal/how-to-measure-feature-importance-in-a-binary-classification-model-d284b8c9a301" rel="noopener">文章</a>。这是一种在<strong class="lb iu">二元分类</strong>模型中测量特征重要性的<strong class="lb iu">无监督</strong>方式，使用皮尔逊卡方检验和相关系数。</p><p id="f395" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般来说，对于简单的特征选择，无监督的方法通常是足够的。然而，每个模型都有自己的方式来“思考”这些特征，并处理它们与目标变量的相关性。而且，还有不太在意<strong class="lb iu">共线性</strong>(即特征之间的相关性)的模型，以及其他出现<strong class="lb iu">非常大问题</strong>的模型(例如线性模型)。</p><p id="84fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管可以通过模型引入的某种<strong class="lb iu">相关性</strong>度量标准(例如，对线性回归系数执行的 t-test 的 p 值)对<strong class="lb iu">特性进行排序，但仅采用最相关的变量是不够的。想想一个等于另一个的特征，只是乘以 2。这些特征之间的线性相关性如果 1 和这个简单的乘法不影响与目标变量的相关性，所以如果我们只取最相关的变量，我们就取原始特征和相乘的那个。这导致了<strong class="lb iu">共线性</strong>，这对我们的模型来说是相当危险的。</strong></p><p id="9384" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是为什么我们必须引入一些方法来更好地选择我们的功能。</p><h1 id="c8a4" class="mc md it bd me mf mz mh mi mj na ml mm jz nb ka mo kc nc kd mq kf nd kg ms mt bi translated">随机搜索</h1><p id="ac20" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">随机搜索是数据科学家工具箱中非常有用的工具。这是一个非常简单的技术，经常使用，例如，在交叉验证和<strong class="lb iu">超参数优化</strong>中。</p><p id="e9d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很简单。如果你有一个多维网格，并想在这个网格上寻找使某个<strong class="lb iu">目标函数</strong>最大化(或最小化)的点，随机搜索工作如下:</p><ol class=""><li id="f29c" class="ne nf it lb b lc ld lf lg li ng lm nh lq ni lu nj nk nl nm bi translated">在网格上随机取一点，测量目标函数值</li><li id="8514" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">如果该值比迄今为止达到的最佳值更好，则将该点保存在内存中。</li><li id="1dd0" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">重复预定的次数</li></ol><p id="be8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样。只是产生随机点，并寻找最好的一个。</p><p id="d30d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是寻找全局最小值(或最大值)的好方法吗？当然不是。我们寻找的点在一个非常大的空间中只有一个(如果我们幸运的话)，并且我们只有有限的迭代次数。在一个<em class="ns"> N- </em>点网格中得到那个单点的概率是<em class="ns"> 1/N </em>。</p><p id="1650" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，为什么随机搜索会被这么多使用呢？因为我们<strong class="lb iu">从来没有真正想要</strong>最大化我们的绩效评估；我们想要一个好的，<strong class="lb iu">合理的高值</strong>，它不是可能的最高值，以避免过度拟合。</p><p id="f228" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是为什么随机搜索是可行的，并且可以用于特征选择。</p><h1 id="df78" class="mc md it bd me mf mz mh mi mj na ml mm jz nb ka mo kc nc kd mq kf nd kg ms mt bi translated">如何使用随机搜索进行特征选择</h1><p id="4cf3" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">随机搜索可用于特征选择，结果相当<strong class="lb iu">好</strong>。类似于随机搜索的程序的一个例子是<strong class="lb iu">随机森林</strong>模型，它为每棵树执行特征的随机选择。</p><p id="1372" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个想法非常简单:随机选择特性<strong class="lb iu"/>，通过<strong class="lb iu"> k 倍交叉验证</strong>测量模型性能，并重复多次。提供最佳性能的功能组合正是我们所寻求的。</p><p id="66e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更准确地说，以下是要遵循的步骤:</p><ol class=""><li id="0405" class="ne nf it lb b lc ld lf lg li ng lm nh lq ni lu nj nk nl nm bi translated">生成一个介于 1 和特征数之间的随机整数<em class="ns"> N </em>。</li><li id="2813" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">生成一个 0 到<em class="ns"> N-1 </em>之间的<em class="ns"> N </em>整数随机序列，不重复。这个序列代表了我们的特征阵列。记住 Python 数组是从 0 开始的。</li><li id="921e" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">在这些特征上训练模型，并用 k-fold 交叉验证对其进行交叉验证，保存一些性能测量的平均值<strong class="lb iu"/>。</li><li id="f029" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">从第 1 点开始重复，重复次数不限。</li><li id="798a" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">最后，根据所选择的性能度量，获得给出最佳性能的特征阵列。</li></ol><h1 id="c83f" class="mc md it bd me mf mz mh mi mj na ml mm jz nb ka mo kc nc kd mq kf nd kg ms mt bi translated">Python 中的一个实际例子</h1><p id="2101" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">对于这个例子，我将使用包含在<strong class="lb iu"> sklearn </strong>模块中的<strong class="lb iu">乳腺癌数据集</strong>。我们的模型将是一个<strong class="lb iu">逻辑回归</strong>，我们将使用<strong class="lb iu">准确性</strong>作为性能测量来执行 5 重交叉验证。</p><p id="1944" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先要导入必要的模块。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="f305" class="ny md it nu b gy nz oa l ob oc">import sklearn.datasets<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_val_score<br/>import numpy as np</span></pre><p id="5cf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们可以导入乳腺癌数据，并将其分解为输入和目标。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="6080" class="ny md it nu b gy nz oa l ob oc">dataset= sklearn.datasets.load_breast_cancer()<br/>data = dataset.data<br/>target = dataset.target</span></pre><p id="72d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以创建一个逻辑回归对象。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="f237" class="ny md it nu b gy nz oa l ob oc">lr = LogisticRegression()</span></pre><p id="f7f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以测量所有特征在 k 倍 CV 中的平均准确度。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="ab33" class="ny md it nu b gy nz oa l ob oc"># Model accuracy using all the features<br/>np.mean(cross_val_score(lr,data,target,cv=5,scoring="accuracy"))<br/># 0.9509041939207385</span></pre><p id="5a40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是 95%。让我们记住这一点。</p><p id="87a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以实现一个随机搜索，例如，300 次迭代。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="d67f" class="ny md it nu b gy nz oa l ob oc">result = []</span><span id="0c86" class="ny md it nu b gy od oa l ob oc"># Number of iterations<br/>N_search = 300</span><span id="7b26" class="ny md it nu b gy od oa l ob oc"># Random seed initialization<br/>np.random.seed(1)</span><span id="9046" class="ny md it nu b gy od oa l ob oc">for i in range(N_search):<br/>    # Generate a random number of features<br/>    N_columns =  list(np.random.choice(range(data.shape[1]),1)+1)<br/>    <br/>    # Given the number of features, generate features without replacement<br/>    columns = list(np.random.choice(range(data.shape[1]), N_columns, replace=False))<br/>    <br/>    # Perform k-fold cross validation<br/>    scores = cross_val_score(lr,data[:,columns], target, cv=5, scoring="accuracy")<br/>    <br/>    # Store the result<br/>    result.append({'columns':columns,'performance':np.mean(scores)})</span><span id="68e6" class="ny md it nu b gy od oa l ob oc"># Sort the result array in descending order for performance measure<br/>result.sort(key=lambda x : -x['performance'])</span></pre><p id="4866" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在循环和排序函数结束时，<em class="ns">结果</em>列表的第一个元素就是我们要寻找的对象。</p><p id="7af5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用这个值来计算这个特性子集的新性能度量。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="d445" class="ny md it nu b gy nz oa l ob oc">np.mean(cross_val_score(lr, data[:,result[0][‘columns’]], target, cv=5, scoring=”accuracy”))<br/># 0.9526741054251634</span></pre><p id="c915" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，精确度提高了。</p><h1 id="1f8a" class="mc md it bd me mf mz mh mi mj na ml mm jz nb ka mo kc nc kd mq kf nd kg ms mt bi translated">结论</h1><p id="1eae" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">随机搜索可以是执行特征选择的强大工具。它并不意味着给出为什么一些特性比其他特性更有用的原因(相对于其他特性选择过程，如递归特性消除)，但它可以是一个有用的工具，可以在更短的时间内达到<strong class="lb iu">好的结果</strong>。</p></div></div>    
</body>
</html>