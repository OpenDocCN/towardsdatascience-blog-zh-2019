<html>
<head>
<title>Computer Vision — A journey from CNN to Mask R-CNN and YOLO -Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机视觉——从 CNN 到面具 R-CNN 和 YOLO 之旅——第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/computer-vision-a-journey-from-cnn-to-mask-r-cnn-and-yolo-part-2-b0b9e67762b1?source=collection_archive---------5-----------------------#2019-07-27">https://towardsdatascience.com/computer-vision-a-journey-from-cnn-to-mask-r-cnn-and-yolo-part-2-b0b9e67762b1?source=collection_archive---------5-----------------------#2019-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="af9c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是探索和理解 YOLO 建筑和工作方式系列的第二篇文章(你只看一次)。</p><p id="257a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">点击</em> <a class="ae kp" href="https://medium.com/datadriveninvestor/computer-vision-a-journey-from-cnn-to-mask-r-cnn-and-yolo-1d141eba6e04" rel="noopener"> <em class="ko"> CNN 上详细了解计算机视觉算法，基于区域的 CNN(R-CNN)，快速 R-CNN，更快 R-CNN </em> </a> <em class="ko">。这是第 1 部分。</em></p><p id="695e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">在下一篇文章中，我们将浏览 YOLO v3 的代码</em></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi kq"><img src="../Images/118f64adda100837f6615566444de212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*mUCtlKpXVVZtEniDwjVYqA.png"/></div></figure><p id="59f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">YOLO——你只需看一次</strong>就是更好、更快、更准确的计算机视觉算法的答案。</p><blockquote class="ky"><p id="6039" class="kz la it bd lb lc ld le lf lg lh kn dk translated">你只需看一次(YOLO)图像，就能预测出哪些物体存在以及它们在哪里。</p></blockquote><p id="381b" class="pw-post-body-paragraph jq jr it js b jt li jv jw jx lj jz ka kb lk kd ke kf ll kh ki kj lm kl km kn im bi translated"><strong class="js iu">单个卷积网络同时预测多个边界框和这些框的类别概率。</strong></p><p id="819a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">YOLO 在全图像上训练，直接优化检测性能。这使得 YOLO 非常快速和准确</p><h1 id="cef4" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">YOLO 的特色</h1><ul class=""><li id="6f23" class="ml mm it js b jt mn jx mo kb mp kf mq kj mr kn ms mt mu mv bi translated"><strong class="js iu"> YOLO，一种对象检测算法同时发现图像网格中的所有对象</strong></li><li id="91b1" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu">对完整图像使用单个卷积网络</strong></li><li id="7a87" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">与滑动窗口或基于区域的技术不同，YOLO 在训练和测试期间看到了整个图像，因此它隐式地编码了关于类及其外观的上下文信息。因此与快速的 R-CNN 相比，产生的背景错误不到一半。</li><li id="199b" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu"> YOLO 使用整个图像的特征来预测每个包围盒。</strong>它还同时预测一幅图像的所有类的所有边界框。<strong class="js iu">预测边界框和这些框的类别概率。</strong></li><li id="89ae" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu">将检测视为回归问题</strong></li><li id="aa99" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu">极其快速准确</strong></li></ul><h1 id="a68d" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">YOLO 的工作</h1><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/97b7a2509fde55647a730fea313d38ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*_RdBX-5E0k9gFoACJG_okg.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">You Only Look Once-YOLO</figcaption></figure><ul class=""><li id="8648" class="ml mm it js b jt ju jx jy kb ng kf nh kj ni kn ms mt mu mv bi translated">YOLO 拍摄了一幅图像，并将其分割成一个 SxS 网格。每个网格单元仅预测一个对象</li><li id="0621" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">在每个网格上应用图像分类和定位</li><li id="aa07" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">如果对象的中心落入网格单元，则该网格单元负责检测该对象</li><li id="5649" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">每个网格单元预测 B 个边界框，这些框具有置信度得分</li></ul><ol class=""><li id="8a7d" class="ml mm it js b jt ju jx jy kb ng kf nh kj ni kn nj mt mu mv bi translated"><strong class="js iu">置信度得分反映了模型对盒子包含对象的置信度，以及它认为盒子预测的准确性。如果对象不存在，则置信度得分将为零。</strong>置信度预测表示预测框和任何地面真实框之间的 IOU。<strong class="js iu">Pr(Object)∫IOU 真值 pred——预测框和实际真值之间的交集(IOU)</strong></li><li id="7c76" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn nj mt mu mv bi translated"><strong class="js iu">当网格单元中出现对象时的边界框</strong></li><li id="9843" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn nj mt mu mv bi translated"><strong class="js iu">C 类概率</strong></li></ol><h1 id="5fce" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">了解 YOLO 的输出</h1><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/aaee045b6a04f191fee120abcd9dd7e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*T6G1lNnZOSHiJR6Y-B5DXg.png"/></div></figure><ul class=""><li id="98f3" class="ml mm it js b jt ju jx jy kb ng kf nh kj ni kn ms mt mu mv bi translated"><strong class="js iu"> pc 定义网格中物体的存在，是概率</strong>。当一个物体存在时，它包含概率，当网格中没有物体存在时，它们的 pc 将为零</li><li id="10e1" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu"> bx，by，bh，bw 指定物体出现时的边界框</strong>。<strong class="js iu"> bx，by 是相对于网格单元边界的盒子中心。bw、bh 是相对于整个图像的宽度和高度</strong></li><li id="dfb7" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu"> c1、c2、c3 等。代表班级。</strong>维 c 等于类的数量。类别概率取决于包含对象 P(类别|对象)的网格单元。如果网格单元中存在对象，则存在的类的值为 1，其他类的值为 0</li></ul><p id="91ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">输出的维度会是什么？</em>T13】</strong></p><p id="52a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">将图像分成一个 S × S 的网格。</strong></p><p id="e229" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">每个网格单元预测 B 个边界框、这些框的置信度和 C 类概率。</strong></p><p id="ba3b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">每个边界框由 5 个预测组成:bx、by、bw、bh 和置信度</strong></p><p id="05c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输出维度将 S×S×(B∫(1+4)+C)张量</strong></p><p id="1943" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，如果我们将图像划分为 7×7 的网格，每个网格单元预测 2 个边界框，并且我们有 20 个标记类，则输出将是 7×7 ×( 2 * 5+20)= 7×7×30 张量</p><p id="f9cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">地面真值包围盒和预测包围盒之间的 IoU 是什么？</em>T25】</strong></p><h1 id="754c" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">并集上的交集— IoU</h1><p id="076f" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb nl kd ke kf nm kh ki kj nn kl km kn im bi translated">IoU 通过算法计算两个边界框的并集上的交集，即地面真实的边界框和预测框的边界框</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f703f040fdd60fd1554405b8384d4e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/0*WjXJQy6_3vtut_ug.png"/></div></figure><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi kq"><img src="../Images/007ff109781b069a391ff2291b1ac431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/0*Rjgs-fNqQVh0Uh7C.png"/></div></figure><p id="ac0a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">当 IoU 为 1 时，这将意味着预测的和真实边界框完全重叠。</strong></p><p id="30be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了在图像中检测一次对象，<strong class="js iu">非最大抑制会考虑 IoU &gt;为 0.5 </strong>的所有边界框</p><p id="387e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">如果我有多个 IoU 大于 0.5 的包围盒怎么办？</em> </strong></p><h1 id="d864" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">非最大抑制</h1><ul class=""><li id="9e4e" class="ml mm it js b jt mn jx mo kb mp kf mq kj mr kn ms mt mu mv bi translated"><strong class="js iu">非最大抑制将移除 IoU 小于或等于 0.5 的所有边界框</strong></li><li id="82bb" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu">选择 IoU 值最高的边界框，并抑制其他边界框，以识别同一物体</strong></li></ul><p id="4184" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，如果我们有三个分别为 0.6、0.7 和 0.9 的矩形。为了让 IoU 识别下图中的车辆，非最大抑制将保留 IoU 为 0.9 的边界框，并将抑制剩余的 IoU 为 0.6 和 0.7 的边界框。</p><p id="d540" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于下图中的汽车，非最大抑制将保持 IoU 为 0.8，抑制或移除 IoU 边界框为 0.7</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b23b503f411291863a2f09f8bddd93fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*B3VLNr77E6XiWiOJZD1fWw.png"/></div></figure><h1 id="2131" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">YOLO v1 网络设计</h1><p id="ce0b" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb nl kd ke kf nm kh ki kj nn kl km kn im bi translated"><strong class="js iu">YOLO 的网络架构受 GoogLeNet 图像分类模型的启发</strong></p><p id="3fbf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> YOLO 的检测网络有 24 个卷积层，后面是 2 个全连接层。</strong></p><p id="8514" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">YOLO 使用 1 × 1 缩减层，然后是 3 × 3 卷积层，而不是谷歌网络使用的初始网络</p><p id="4ce4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">交替的 1 × 1 卷积层减少了来自前面层的特征空间。</p><p id="6e27" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">在 ImageNet 分类任务中，卷积层以一半的分辨率(224 × 224 输入图像)进行预训练，然后以两倍的分辨率进行检测</strong></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nq"><img src="../Images/ced3b0b1e57ee6a9e65b600101807987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zzxTufTdDZGGRTsvo4uygg.png"/></div></div></figure><p id="645f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">YOLO 对最后一层使用线性激活函数，对所有其他层使用泄漏 ReLU。</p><p id="a4e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> YOLO 使用卷积特征提取器顶部的全连接层直接预测边界框的坐标。YOLO 只预测每张图片有 98 个盒子。</strong></p><h1 id="255a" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">YOLO 的优势</h1><ul class=""><li id="9e5d" class="ml mm it js b jt mn jx mo kb mp kf mq kj mr kn ms mt mu mv bi translated">与基于分类器的方法不同，YOLO 在测试时速度极快，因为它只需要一次网络评估</li><li id="af20" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">同时执行特征提取、边界框预测、非最大值抑制和上下文推理。</li><li id="06c1" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">YOLO 网络在线训练特征，并针对检测任务优化它们。统一架构带来更快、更准确的模型</li></ul><h1 id="0fff" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">YOLO 的局限性</h1><ul class=""><li id="f0f5" class="ml mm it js b jt mn jx mo kb mp kf mq kj mr kn ms mt mu mv bi translated">YOLO 对边界框预测施加了很强的空间约束，因为<strong class="js iu">每个网格单元只能预测两个框，并且只能有一个类，这限制了模型可以预测的附近对象的数量。</strong></li><li id="d8f7" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu"> YOLO 与成群出现的小物体搏斗，比如一群鸟</strong></li><li id="4854" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">努力推广到新的或不寻常的长宽比或配置的对象</li></ul><h1 id="2433" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">YOLO 的其他变体</h1><h2 id="4984" class="nv lo it bd lp nw nx dn lt ny nz dp lx kb oa ob mb kf oc od mf kj oe of mj og bi translated">快速 YOLO</h2><p id="ca2a" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb nl kd ke kf nm kh ki kj nn kl km kn im bi translated">快速 YOLO 是 YOLO 的快速变体。它<strong class="js iu">使用 9 个卷积层，而不是 YOLO </strong>使用的 24 个卷积层，并且还使用发烧过滤器。</p><p id="0870" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">YOLO 和快速 YOLO 之间的网络规模不同，但 YOLO 和快速 YOLO 之间的所有培训和测试参数都相同。</p><p id="ed02" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们网络的最终输出是预测的 7 × 7 × 30 张量。</p><h2 id="0a6b" class="nv lo it bd lp nw nx dn lt ny nz dp lx kb oa ob mb kf oc od mf kj oe of mj og bi translated">YOLOv2 或 YOLO9000</h2><ul class=""><li id="5e30" class="ml mm it js b jt mn jx mo kb mp kf mq kj mr kn ms mt mu mv bi translated"><strong class="js iu">yolov 2 中的输入尺寸从 224*224 增加到 448*448 </strong>。图像输入大小的增加改善了 mAP(平均精度)</li><li id="4d47" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu"> YOLOv2 将整个图像划分成 13×13 的网格</strong>。这有助于解决 YOLO v1 中较小物体探测的问题</li><li id="9046" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">YOLOv2 使用<strong class="js iu">批处理规范化</strong>，这导致收敛的显著改善，并消除了对其他形式的正则化的需要。我们也可以在不过度拟合的情况下从模型中删除掉的部分。</li><li id="dea4" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu"> YOLOv2 在边界框的维度上运行 k-means 聚类，以获得模型的良好先验或锚</strong>。YOLOv2 发现 k= 5 给出了召回率与模型复杂性之间的良好权衡。YOLOv2 使用 5 个锚盒</li><li id="df04" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated"><strong class="js iu"> YOLOv2 使用 Darknet 架构，具有 19 个卷积层、5 个最大池层和一个用于分类对象的 softmax 层</strong></li><li id="a21a" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">YOLOv2 使用定位框来预测边界框。</li></ul><p id="2187" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">什么是锚盒或先验，它有什么帮助？</em> </strong></p><h1 id="f634" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">锚盒或先验</h1><p id="ecad" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb nl kd ke kf nm kh ki kj nn kl km kn im bi translated">锚定框用于检测多个对象、不同比例的对象以及重叠的对象。这提高了目标检测的速度和效率。</p><p id="7e03" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">锚定框一次评估所有对象预测，消除了使用滑动窗口扫描图像的需要</p><p id="5023" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">锚点<em class="ko">框</em>是一组具有一定高度和宽度的预定义边界框。定义这些框是为了捕捉要检测的特定对象类的比例和纵横比。</p><p id="98ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">YOLOv2 将整个图像划分为 13 X 13 个网格单元。<strong class="js iu"> YOLOv2 在边界框的维度上运行 k-means 聚类，以获得模型的良好先验或锚</strong>。YOLOv2 发现 k= 5 给出更好的性能。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/87431bb0b8e54f96731dcb9e62afdf0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*NT5-e5Q5RqJsXJ4YN2YNbA.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Red is Ground Truth. Blue boxes are 5 Anchor boxes</figcaption></figure><p id="172d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了预测一张图片中的多个对象，YOLOv2 进行了数千次预测。最终的目标检测是通过去除属于背景类的锚框来完成的，剩余的锚框通过它们的置信度得分来过滤。我们发现 IoU 大于 0.5 的锚盒。使用前面解释的非最大抑制来选择具有最大置信度得分的锚框。</p><h2 id="6ce9" class="nv lo it bd lp nw nx dn lt ny nz dp lx kb oa ob mb kf oc od mf kj oe of mj og bi translated">YOLOv3</h2><ul class=""><li id="7d1d" class="ml mm it js b jt mn jx mo kb mp kf mq kj mr kn ms mt mu mv bi translated">使用 9 个锚</li><li id="1a79" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">使用逻辑回归来预测客观性分数，而不是 YOLO v2 中使用的 Softmax 函数</li><li id="1826" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">YOLO v3 使用 Darknet-53 网络作为具有 53 个卷积层的特征提取器</li></ul><h1 id="3fb8" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">YOLO v3 的代码将在下一篇文章中出现</h1><h2 id="b459" class="nv lo it bd lp nw nx dn lt ny nz dp lx kb oa ob mb kf oc od mf kj oe of mj og bi translated">参考资料:</h2><p id="f8d2" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb nl kd ke kf nm kh ki kj nn kl km kn im bi translated"><a class="ae kp" href="https://pjreddie.com/media/files/papers/yolo.pdf" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/media/files/papers/yolo.pdf</a></p><p id="cbd5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://arxiv.org/pdf/1612.08242.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242.pdf</a></p><p id="edbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></p><p id="6333" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="http://deeplearning.csail.mit.edu/instance_ross.pdf" rel="noopener ugc nofollow" target="_blank">http://deeplearning.csail.mit.edu/instance_ross.pdf</a></p></div></div>    
</body>
</html>