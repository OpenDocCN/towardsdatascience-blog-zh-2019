<html>
<head>
<title>Review: ION —Inside-Outside Net, 2nd Runner Up in 2015 COCO Detection (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">点评:离子—内外网，2015 年 COCO 探测(物体探测)亚军</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=collection_archive---------26-----------------------#2019-01-02">https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=collection_archive---------26-----------------------#2019-01-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="facf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">2015 年可可检测挑战赛最佳参赛学生和亚军</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e319433308c2e806644f1e2fd9c59f0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MXnHs6e52rYUEid0D5-lDA.png"/></div></div></figure><p id="61cf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi ln translated"><span class="l lo lp lq bm lr ls lt lu lv di">在</span>这个故事里，<strong class="kt ir">康乃尔大学</strong>和<strong class="kt ir">微软研究院</strong>的<strong class="kt ir"> ION(内外网)</strong>进行了回顾。通过使用 skip pooling 在多个尺度和抽象级别提取信息，并使用递归神经网络(RNN)捕捉上下文特征，ion 在<strong class="kt ir"> 2015 MS COCO 检测挑战赛</strong>中获得了<strong class="kt ir">最佳学生参赛作品</strong>和<strong class="kt ir">第三名</strong>。</p><ul class=""><li id="e3e6" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated"><strong class="kt ir">内部</strong>:跳过 L2 正常化连接。</li><li id="16a7" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm mb mc md me bi translated"><strong class="kt ir">外部</strong>:上下文堆叠 4 向 rnn。</li></ul><p id="ddb3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由于 ION 使用了一个简单的名为“IRNN”的 RNN，这也是了解 RNN 的一个良好开端。发表在<strong class="kt ir"> 2016 CVPR </strong>上，引用<strong class="kt ir"> 300 余次</strong>。(<a class="mk ml ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----da19993f4766--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h1 id="215e" class="mt mu iq bd mv mw mx my mz na nb nc nd jw ne jx nf jz ng ka nh kc ni kd nj nk bi translated">涵盖哪些内容</h1><ol class=""><li id="012e" class="lw lx iq kt b ku nl kx nm la nn le no li np lm nq mc md me bi translated"><strong class="kt ir">离子架构</strong></li><li id="bedd" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm nq mc md me bi translated"><strong class="kt ir">跳过池</strong></li><li id="a3ed" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm nq mc md me bi translated"><strong class="kt ir">从香草 RNN 到四方 IRNN </strong></li><li id="2a02" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm nq mc md me bi translated"><strong class="kt ir">部分设计评估</strong></li><li id="0db3" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm nq mc md me bi translated"><strong class="kt ir">结果</strong></li></ol></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h1 id="b86c" class="mt mu iq bd mv mw mx my mz na nb nc nd jw ne jx nf jz ng ka nh kc ni kd nj nk bi translated"><strong class="ak"> 1。离子架构</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/178dcc2c9e95707fc2b5ba4c1ceaa7f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fu1E6qQiza_vH8dG-7GLSA.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">ION Architecture</strong></figcaption></figure><h2 id="cfc2" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">1.1.快速 R-CNN</h2><p id="265e" class="pw-post-body-paragraph kr ks iq kt b ku nl jr kw kx nm ju kz la oj lc ld le ok lg lh li ol lk ll lm ij bi translated">在最初的<a class="ae om" href="http://Fast R-CNN" rel="noopener ugc nofollow" target="_blank">快速 R-CNN </a>中，ROI 汇集仅在 conv5 中执行。</p><ul class=""><li id="9494" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated">对于小对象，conv5 上的尺寸可能仅覆盖 1×1 单元，该单元可上采样至 7×7。</li><li id="98a0" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm mb mc md me bi translated">只有局部特征(ROI 内)用于分类。</li></ul><h2 id="dbea" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">1.2.离子</h2><p id="be62" class="pw-post-body-paragraph kr ks iq kt b ku nl jr kw kx nm ju kz la oj lc ld le ok lg lh li ol lk ll lm ij bi translated">为了分别解决上述问题，</p><ul class=""><li id="4024" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated"><strong class="kt ir">内部</strong>:con v3 至 conv5 的输出经过 L2 归一化、级联、重新缩放和 1×1 conv 降维。</li><li id="0f7b" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm mb mc md me bi translated"><strong class="kt ir"> Outside </strong>:在 conv5 的输出端，有 2 个堆叠的 IRNNs(一种 RNN，由 ReLUs 组成，用单位矩阵初始化。)以便利用 ROI 汇集区域之外的上下文特征。</li></ul><p id="f967" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">注意，离子架构是基于 <a class="ae om" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener"> <strong class="kt ir">快速 R-CNN </strong> </a> <strong class="kt ir">使用</strong> <a class="ae om" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> <strong class="kt ir"> VGG16 </strong> </a> <strong class="kt ir">主干</strong>开发的<strong class="kt ir">。因此，网络始端的 conv1 至 conv5 和末端的全连接(FC)层均来自使用<a class="ae om" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGG16 </a>主干的原始<a class="ae om" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>。</strong></p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h1 id="ce9a" class="mt mu iq bd mv mw mx my mz na nb nc nd jw ne jx nf jz ng ka nh kc ni kd nj nk bi translated">2.<strong class="ak">跳过池</strong></h1><ul class=""><li id="b4a6" class="lw lx iq kt b ku nl kx nm la nn le no li np lm mb mc md me bi translated"><strong class="kt ir"> ROI 映射在多个层上执行，从 conv3 到 conv5 </strong>，以及由 2 个 irnn 计算的<strong class="kt ir">上下文特征。</strong></li><li id="2ec4" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm mb mc md me bi translated">然而，为了匹配 FC 层的输入，需要 512×7×7 形状的尺寸。从而，在拼接之后，执行<strong class="kt ir"> 1×1 卷积</strong>以<strong class="kt ir">将维度</strong>减少到 512×7×7。</li><li id="7ba3" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm mb mc md me bi translated">但是<strong class="kt ir">较早的层通常比后面的层</strong>有更大的值，这一点在<a class="ae om" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener"> ParseNet </a>中有提到。因此，在拼接之前，每个汇集的 ROI 被<strong class="kt ir"> L2 归一化</strong> <strong class="kt ir">并通过经验确定的比例重新按比例放大</strong>。</li></ul></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h1 id="cb20" class="mt mu iq bd mv mw mx my mz na nb nc nd jw ne jx nf jz ng ka nh kc ni kd nj nk bi translated">3.<strong class="ak">从香草 RNN 到四方 IRNN </strong></h1><h2 id="c0dd" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">3.1.<strong class="ak">香草 RNN(RNN 平原谭)</strong></h2><p id="cb18" class="pw-post-body-paragraph kr ks iq kt b ku nl jr kw kx nm ju kz la oj lc ld le ok lg lh li ol lk ll lm ij bi translated">在<strong class="kt ir">香草 RNN </strong> ( <strong class="kt ir">普通 Tanh RNN </strong>)中，Tanh 用于激活:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/77bfa5e462670b69416bfee23f458cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZ8W-v7RRxGvxz2qEEQ2rg.png"/></div></div></figure><h2 id="c2a0" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">3.2.四向 IRNN</h2><p id="c58e" class="pw-post-body-paragraph kr ks iq kt b ku nl jr kw kx nm ju kz la oj lc ld le ok lg lh li ol lk ll lm ij bi translated">Hinton 教授的团队提出了 IRNN，它是一种 RNN，由 ReLUs 组成，并用单位矩阵初始化。(如果有兴趣，可以访问 arXiv 中的论文，论文名为“<a class="ae om" href="https://arxiv.org/abs/1504.00941" rel="noopener ugc nofollow" target="_blank">初始化整流线性单元递归网络的简单方法</a>”。)</p><p id="3d6a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因为它是在一个图像内完成的，所以它是一个横向 RNN。下面是一个例子。对每一行(右/左)或每一列(下/上)重复该步骤。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/035f53a8be19f0251d5fd8070a9c10cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3bdhRLRe_IZMtbc-1svaaQ.png"/></div></div></figure><p id="30ec" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">和<strong class="kt ir">使用基于 ReLU 的 IRNN 的 4 向 IRNN </strong>的简单版本如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ca"><img src="../Images/5ce676a08ee1182bbc8a025130d12247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pu0dCUOVxJtdoKMFjczDJg.png"/></div></div></figure><p id="33a3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上面的等式是针对右向 IRNN 的，对于左、上、下方向类似。为了提高效率，作者简化了上述四方向 IRNN。简而言之，首先，<strong class="kt ir">隐藏到输出被合并到单个 conv 中，即级联之后是 1×1 卷积</strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/f4e162d6800aa2aa26f39ca37e0f3742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Adr4il6RBwE8EsnXmViaVA.png"/></div></div></figure><p id="11d1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第二，输入到隐藏也通过<strong class="kt ir"> 1×1 卷积被简化，并且与 4 个循环转换共享 conv</strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/fd0f2b8f78b788d778684ca300404ff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H0fqYESZxgZGxPexFfzzeQ.png"/></div></div></figure><p id="696d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，<strong class="kt ir">将两个改进的 IRNNs 堆叠在一起</strong>以提取上下文特征:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/ec7eb227c1295ee7c0310223a3b6e067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KVJFiTDC4LbyquOjGmC5Pg.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Two Stacked IRNN</strong></figcaption></figure><p id="ca5b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">经过上述修改后，RNN 方程变成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/986bd278b80b3a3d1634b878609e200d.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*xUj4UQCCA9cQHZbFs4xvjQ.png"/></div></figure><p id="8fa2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如我们所见，<strong class="kt ir">没有输入<em class="ot"> x </em>，因为输入是由 1×1 卷积隐式完成的。</strong>发现<strong class="kt ir"> <em class="ot"> W </em>由于检测性能类似</strong>也可以去掉:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/811be033c55ab051d4845b6370059a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*P264Cerd41HHV9mnJXloKw.png"/></div></figure><p id="d494" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这就像一个累加器，但是每一步之后都有 ReLU。因此，IRNN 由重复的步骤组成:积累，再积累，积累，等等。请注意，这与积分/区域图像不同，因为每一步都有 ReLU。</p><h2 id="b555" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">3.3.分割损失</h2><p id="d97a" class="pw-post-body-paragraph kr ks iq kt b ku nl jr kw kx nm ju kz la oj lc ld le ok lg lh li ol lk ll lm ij bi translated">如上图所示，由于对象检测数据集也包含语义分割标签，因此在训练过程中，<strong class="kt ir">也使用上下文特征进行语义分割。</strong>就像<a class="ae om" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>一样，反卷积使用 32×32 的核进行 16×的上采样，并且增加了一个额外的 softmax 损失层，权重为 1。这种损失被用作<strong class="kt ir">正则化器</strong>。</p><p id="a311" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在测试过程中，该分段路径被移除。因此，推理时间与没有分割损失的情况下训练的网络相同。</p><h2 id="6b64" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">3.4.两阶段训练</h2><p id="96b3" class="pw-post-body-paragraph kr ks iq kt b ku nl jr kw kx nm ju kz la oj lc ld le ok lg lh li ol lk ll lm ij bi translated">如前所述，该网络基于<a class="ae om" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>，使用<a class="ae om" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGG16 </a>作为主干，因此，ImageNet 上预先训练的<a class="ae om" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGG16 </a>用于那些公共层。并且用 conv1 至 conv5 冻结来训练网络。然后，在 conv1 和 conv2 冻结的情况下训练网络。</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h1 id="316e" class="mt mu iq bd mv mw mx my mz na nb nc nd jw ne jx nf jz ng ka nh kc ni kd nj nk bi translated">4.<strong class="ak">部分设计评估</strong></h1><p id="178d" class="pw-post-body-paragraph kr ks iq kt b ku nl jr kw kx nm ju kz la oj lc ld le ok lg lh li ol lk ll lm ij bi translated">消融研究在 PASCAL VOC 2007 数据集上进行。所有的训练都使用 2007 trainval 和 2012 trainval 的联合，并在 2007 测试集上进行测试。</p><h2 id="ac1d" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">4.1.泳池来自哪几层？</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/c06d418ab9712fa065b7d475389f68d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*DtX04vS1xG0-5SjgfHeXUw.png"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Combining Features from Different Layers.</strong></figcaption></figure><ul class=""><li id="9698" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated">如上所示，将 conv3、conv4 和 conv5 与 L2 归一化相结合，可获得最高性能的 74.6% mAP。</li></ul><h2 id="efeb" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">4.2.应该如何归一化特征幅度？</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/a691c013bbb721a07afde94ab091a4e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*ZH0vyzLhIbyGKv028BmBUg.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Approaches to Normalizing Feature Amplitude.</strong></figcaption></figure><ul class=""><li id="6c20" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated">在<a class="ae om" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener"> ParseNet </a>中，它对通道求和，并对每个空间位置执行一次归一化。或者取而代之，对每个汇集的 ROI 中的所有条目求和，并将其归一化为单个斑点。</li><li id="df57" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm mb mc md me bi translated">在缩放过程中，每个通道是固定比例还是学习比例？</li><li id="0f51" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm mb mc md me bi translated">发现所有这些方法的性能大致相同。</li></ul><h2 id="027a" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">4.3.分割损失有多大帮助？</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/b0f279003b57f667019a9992abefa775.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*RLADTPL25Gp3xeF2bqOtvA.png"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Effect of Segmentation Loss.</strong></figcaption></figure><ul class=""><li id="0cf6" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated">如上所示，分段丢失的情况下，性能始终更好。</li></ul><h2 id="fc71" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">4.4.我们应该如何融入语境？</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/24e6ec4bae5289f6c32caa0d1834dfae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*jZGiNIHkopG-8VthYYMf9w.png"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Receptive Field of Different Layer Types.</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/82b419c89a98fba86c4183b4a2d87459.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*KVzmH-FWu4Di74rNPtEs9g.png"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Comparing Approaches to Adding Context.</strong></figcaption></figure><ul class=""><li id="595e" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated">在 conv5 之后，尝试不同类型的 conv。2 叠 3×3 和 2 叠 5×5 只是进一步的卷积。感受野是基于过滤器的大小。对于全球平均池，它有一个输入大小的感受野，但输出是相同的价值。对于 IRNN，它有一个输入大小的感受野，输出值在不同的位置是不同的。</li><li id="5a98" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm mb mc md me bi translated">当然，使用 IRNN 可以获得最高的地图。</li></ul><h2 id="5100" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">4.5.哪个 IRNN 建筑？</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/932925b922d994cb934731417de94cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*6l2M3yR_yLDs0LpUpS-61g.png"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Varying The Hidden Transition.</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/4af709d2991929cd674cfa8e6b843728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*yqGulJh76hRPBBxGpm6Ylw.png"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Other Variations.</strong></figcaption></figure><ul class=""><li id="ca42" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated">包括 Whh 和具有 256 个隐藏单元获得最佳结果。并且对于第一 IRNN 仅使用左右方向的 IRNN，并且仅使用上下方向的 IRNN，具有与两个堆叠的 4 方向 IRNN 相同的结果。然而，排除 Whh，使用 512 个隐藏单元，并且最终使用两个堆叠的 4 方向 IRNN。</li></ul></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h1 id="42b3" class="mt mu iq bd mv mw mx my mz na nb nc nd jw ne jx nf jz ng ka nh kc ni kd nj nk bi translated">5.结果</h1><h2 id="fb2d" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">5.1.帕斯卡 VOC 2007</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/9abfa335f2ee681622c06840e1afa2b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHEWa9x8NKMwstcMryhQgw.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">PASCAL VOC 2007 Test Set </strong>(<strong class="bd nw">07</strong>: 07 trainval,<strong class="bd nw"> 12</strong>: 12 trainval,<strong class="bd nw"> S</strong>: Segmentation labels,<strong class="bd nw"> R</strong>: 4-Dir IRNN, <strong class="bd nw">W</strong>: Two rounds of box regression and weighted voting<strong class="bd nw">, D</strong>: remove all dropout, <strong class="bd nw">SS</strong>: SelectiveSearch, <strong class="bd nw">EB</strong>: EdgeBoxes<strong class="bd nw">, RPN</strong>: region proposal network,<strong class="bd nw"> Time</strong>: per image, excluding proposal generation.)</figcaption></figure><ul class=""><li id="6371" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated">包括所有技术在内，ION 获得了 80.1%的 mAP，并且每幅图像花费 2.0 秒，这比 30 秒的最先进的 MR-CNN 快得多。</li></ul><h2 id="f920" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">5.2.帕斯卡 VOC 2012</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/258fa1108789c6c511f44846d432ad19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zNrL0GHEdXF_x_Iet60cYA.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">PASCAL VOC 2012 Test Set (07</strong>: 07 trainval,<strong class="bd nw"> 12</strong>: 12 trainval,<strong class="bd nw"> S</strong>: Segmentation labels,<strong class="bd nw"> R</strong>: 4-Dir IRNN, <strong class="bd nw">W</strong>: Two rounds of box regression and weighted voting<strong class="bd nw">, D</strong>: remove all dropout, <strong class="bd nw">SS</strong>: SelectiveSearch, <strong class="bd nw">EB</strong>: EdgeBoxes<strong class="bd nw">, RPN</strong>: region proposal network,<strong class="bd nw"> Time</strong>: per image, excluding proposal generation.)</figcaption></figure><ul class=""><li id="757e" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated">与 VOC 2007 相似，包括所有技术，ION 获得 77.9%的 mAP。</li></ul><h2 id="7a75" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">5.3.可可女士</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/a2766c782c6e0380b4a41bfbb8cbaeab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9cMzq-wIbDrh-smlbiJ8vQ.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">MS COCO 2015 Test-Dev </strong>(<strong class="bd nw">comp</strong>: competition submission result, <strong class="bd nw">post</strong>: post-competition result)</figcaption></figure><ul class=""><li id="0344" class="lw lx iq kt b ku kv kx ky la ly le lz li ma lm mb mc md me bi translated">仅使用<strong class="kt ir">单一模型</strong>(无集合)，区域提案使用 MCG+RPN，<strong class="kt ir">COCO 小姐竞赛提交结果获得 31.2% mAP </strong>。</li><li id="5e0a" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm mb mc md me bi translated">并且<strong class="kt ir">针对小对象的平均查准率(AP)和平均查全率(AR)分别比<a class="ae om" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener"> Fast R-CNN </a> (F-RCN)从 4.1%提高到 7.4%，从 7.4%提高到 11.7%。</strong></li><li id="9254" class="lw lx iq kt b ku mf kx mg la mh le mi li mj lm mb mc md me bi translated">通过左右翻转和调整训练参数，对于<strong class="kt ir">赛后成绩</strong>得到<strong class="kt ir"> 33.1% mAP </strong>。</li></ul></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><p id="0cc5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">希望大家喜欢看我的故事，快乐深度学习，祝大家新年快乐！</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h2 id="c0e7" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">参考</h2><p id="37cd" class="pw-post-body-paragraph kr ks iq kt b ku nl jr kw kx nm ju kz la oj lc ld le ok lg lh li ol lk ll lm ij bi translated">【2016 CVPR】【离子】<br/> <a class="ae om" href="https://arxiv.org/abs/1512.04143" rel="noopener ugc nofollow" target="_blank">内外网:用跳过池和递归神经网络检测上下文中的对象</a></p><h2 id="3efd" class="nx mu iq bd mv ny nz dn mz oa ob dp nd la oc od nf le oe of nh li og oh nj oi bi translated">我的相关评论</h2><p id="5972" class="pw-post-body-paragraph kr ks iq kt b ku nl jr kw kx nm ju kz la oj lc ld le ok lg lh li ol lk ll lm ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="8b77" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">物体检测<br/></strong><a class="ae om" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae om" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae om" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae om" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae om" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae om" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae om" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a><a class="ae om" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a><a class="ae om" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolov 1</a><a class="ae om" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65"/></p><p id="6582" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">语义切分<br/></strong>[<a class="ae om" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a>][<a class="ae om" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a>][<a class="ae om" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a>][<a class="ae om" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>][<a class="ae om" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a>][<a class="ae om" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a>]</p></div></div>    
</body>
</html>