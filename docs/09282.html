<html>
<head>
<title>Successor Uncertainties</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">后续不确定性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/successor-uncertainties-b498097827fb?source=collection_archive---------19-----------------------#2019-12-08">https://towardsdatascience.com/successor-uncertainties-b498097827fb?source=collection_archive---------19-----------------------#2019-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cf89" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">无模型强化学习的高效探索</h2></div><p id="effd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在这里描述一下我们最近的 NeurIPS 论文<a class="ae le" href="https://arxiv.org/abs/1810.06530" rel="noopener ugc nofollow" target="_blank">【1】</a><a class="ae le" href="https://github.com/DavidJanz/successor_uncertainties_tabular" rel="noopener ugc nofollow" target="_blank">【code】</a>，介绍了<strong class="kk iu">无模型强化学习中<strong class="kk iu">高效探索</strong>的最新方法<strong class="kk iu">后继不确定性</strong> (SU)。</strong></p><p id="0435" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主要作者是来自剑桥<a class="ae le" href="http://mlg.eng.cam.ac.uk" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">机器学习小组</strong> </a>的两名博士生<strong class="kk iu"> David Janz </strong>和<strong class="kk iu"> Jiri Hron </strong>，这项工作源于 David Janz 在<a class="ae le" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">微软剑桥</strong> </a>研究院实习期间。</p><p id="daf4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SU 背后的主要见解是使用概率模型描述 Q 函数，该模型直接考虑 Q 函数值之间的<strong class="kk iu">相关性，如<strong class="kk iu">贝尔曼方程</strong>所示。这些相关性在以前的工作中被忽略了。</strong></p><p id="ff6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果是一种方法，即<strong class="kk iu">在表格基准和 Atari 游戏上胜过竞争对手</strong>，同时仍然是<strong class="kk iu">快速</strong>和<strong class="kk iu">高度可扩展</strong>。</p><h1 id="2855" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">问题描述</h1><p id="b295" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">我们考虑一般的<strong class="kk iu">强化学习</strong> (RL)设置，其中<strong class="kk iu">代理</strong>通过采取<strong class="kk iu">动作</strong> a_t，与具有<strong class="kk iu">状态</strong> s_t 的<strong class="kk iu">环境</strong>进行交互，然后环境通过转换到状态 s_t+1 并给出奖励值 r_t+1 进行响应，如下图所示。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/ba611477fe9ef01f5397e48b75f83a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*Rh44g4puAbLKnQlJGg9bJA.png"/></div></figure><p id="ed25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">环境通常是由未知的<strong class="kk iu">跃迁分布</strong> P(s_t+1，r_t+1|s_t，a_t)完全确定的<strong class="kk iu">随机</strong>和<strong class="kk iu">马尔可夫</strong>，<strong class="kk iu"> </strong>。</p><p id="c3de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代理使用<strong class="kk iu">策略函数</strong> π(a|s)选择其下一个动作作为当前状态的函数，π(a|s)是给定当前状态下动作的概率分布，即π(a|s)=P(a_t=a|s_t=s)。</p><p id="fc65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RL 的目标是从与环境的相互作用中找到一个政策函数，该函数使<strong class="kk iu">贴现回报的期望总和</strong>最大化</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/58eb46b21bfc27b9ccff075e766bde83.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*o3C-oSPpRtQtHvzaCLTOpA.png"/></div></figure><p id="0060" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中 0≤γ&lt;1 保证和是有限的。</p><p id="d9c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RL <strong class="kk iu"> </strong>中的<strong class="kk iu">主要挑战</strong>之一是，行为对奖励的影响通常会<strong class="kk iu">延迟</strong>，直到很久以后才会被察觉。这使得很难确定在任何给定状态下采取的最佳行动。</p><h1 id="1b66" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">q-函数和广义策略迭代</h1><p id="8aa1" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">当根据政策π行动时，一种立即说明行动对未来回报的影响的方法是使用<strong class="kk iu">状态-行动值函数</strong>或<strong class="kk iu"> Q 函数。</strong></p><p id="c9c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Q 函数返回当在状态“s”采取行动“a”然后根据π行动时将获得的折扣奖励的预期总和。特别是，</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/d9683df752fea6b63630076a4a30c00b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*2GmmBsM9A7WR4uY7FPvJrQ.png"/></div></figure><p id="b985" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中期望是π选择的未来行动和环境的转移概率。</p><p id="417a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Q 函数可以被示为满足<strong class="kk iu">贝尔曼方程</strong>，因为在特定状态的动作的值取决于在<strong class="kk iu">后续状态</strong>的其他动作的值。特别是，</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/cf823b9f68d228cd60dc76b33e6062d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*CklFDrnMSuWeUNEAynSDUA.png"/></div></figure><p id="2c63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该等式清楚地表明 Q 函数不是任意函数，并且<strong class="kk iu">必须满足</strong> <strong class="kk iu">特定约束</strong>。这将是 SU 设计中的<strong class="kk iu">关键</strong>，这是在构建 Q 函数的概率模型时考虑这些约束的第一种方法。</p><h2 id="44c3" class="mn lg it bd lh mo mp dn ll mq mr dp lp kr ms mt lr kv mu mv lt kz mw mx lv my bi translated">政策改进</h2><p id="745f" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">Q-函数非常有用，因为我们可以通过<strong class="kk iu">贪婪地</strong>对π的 Q-函数采取行动来获得比π更好或等于π的新策略π_new。这个过程叫做<strong class="kk iu">政策完善</strong>。当行动空间离散时，我们得到以下贪婪策略</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a72f12b1b62c7fe486b0ef70b968595f.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*ipW7sm0ahu6O2qII3u85Ig.png"/></div></figure><h2 id="9927" class="mn lg it bd lh mo mp dn ll mq mr dp lp kr ms mt lr kv mu mv lt kz mw mx lv my bi translated">政策评价</h2><p id="a222" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">为了实现策略改进，我们需要对当前策略的 Q 函数进行估计。这一估计最初可能不可用。获得这种估计的一种方法是对 Q 函数的初始随机猜测重复应用<strong class="kk iu">贝尔曼算子</strong>:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi na"><img src="../Images/2f232eac1cfe25be2278c61d2598dce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*wryp6RXzUlRDoc2b68nKiw.png"/></div></figure><p id="7e78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它根据其他状态-动作对的 Q 值来更新一个状态-动作对的 Q 值。这个过程叫做<strong class="kk iu">政策评估</strong>。</p><h2 id="5d13" class="mn lg it bd lh mo mp dn ll mq mr dp lp kr ms mt lr kv mu mv lt kz mw mx lv my bi translated">广义策略迭代和有效探索</h2><p id="9c5a" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">大多数无模型 RL 方法通过从初始随机策略开始，然后以不同程度的粒度交替策略评估和策略改进步骤来找到最优策略。这被称为<strong class="kk iu">广义策略迭代</strong> (GPI)。</p><p id="df3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在许多 GPI 方法中，贝尔曼算子在政策评估步骤中所要求的期望值通常通过蒙特卡罗近似，通过对从与环境的交互中收集的数据求平均<strong class="kk iu">。</strong></p><p id="6e77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好的实证结果的一个关键因素是如何收集数据以便快速收敛到最优政策。<strong class="kk iu">高效探索</strong>指的是与环境的智能互动，旨在使这种快速融合在实践中发生。</p><p id="60bd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，大多数情况下，无模型 RL 方法不执行有效的探索，并且只是通过遵循具有概率 1-ɛ的贪婪策略，然后选择具有概率ɛ.的随机动作来收集数据</p><h1 id="e9f2" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">汤普森采样和概率 Q 函数</h1><p id="5f71" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">Thompson sampling (TS)是一种有效收集数据的<strong class="kk iu">策略</strong>，该策略在许多不同领域<strong class="kk iu">都非常成功</strong>。特别地，我们的方法 SU 使用 TS 的近似版本来进行有效的探索。</p><p id="79d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在一般设置中，TS 通过迭代执行以下两个步骤来工作:</p><ol class=""><li id="546b" class="nb nc it kk b kl km ko kp kr nd kv ne kz nf ld ng nh ni nj bi translated">在给定可用数据的情况下,<strong class="kk iu">后验分布</strong>中关于数据生成机制的样本假设“H”。</li><li id="b1de" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">根据采样的“H ”,通过<strong class="kk iu">最佳动作</strong>收集新数据。</li></ol><p id="057a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了当我们想要最大化一个未知的目标函数时，TS 迭代的一个例子。</p><p id="dc13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图 1 显示了通过在三种不同输入下评估目标获得的数据。图 2 显示了给定目前收集的数据的目标后验分布的样本。曲线 3 显示选定的样本，曲线 4 显示使采样函数最大化的输入。这个输入是我们下一步根据 TS 收集数据的最佳动作。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/6f680ad22e43e2f57700701d9a8f1f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TTx0WEBvy8nsFsHgZhmu8A.png"/></div></div></figure><p id="888d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 RL 设置中，TS 通常被称为<strong class="kk iu">强化学习的后验采样</strong>(PSRL)【5，6】。当 TS 应用于 RL 问题时，“H”是一个特定的转移分布，并且“H”上的后验概率是在给定观察元组(r_t+1，s_t+1，s_t，a_t)形式的数据的情况下计算的。相对于特定“H”的最佳动作包括<strong class="kk iu">为采样的 P(s_t+1，r_t+1|s_t，a_t)找到最佳策略</strong>。</p><p id="800f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着上面的步骤 2 在实践中是<strong class="kk iu">高成本的</strong>,因为一旦我们采样了特定的跃迁分布，最优行动需要解决由采样的 P(s_t+1，r_t+1|s_t，a_t)指定的新 RL 问题。</p><p id="7572" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果是<strong class="kk iu">汤普森采样在大多数 RL 问题中是不可行的</strong>。</p><h2 id="2826" class="mn lg it bd lh mo mp dn ll mq mr dp lp kr ms mt lr kv mu mv lt kz mw mx lv my bi translated">概率 Q 函数</h2><p id="3714" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">RL 中 TS 的巨大计算成本促使研究人员提出近似方法。</p><p id="e970" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一种常见的方法是在 Q 函数上使用<strong class="kk iu">后验分布，而不是在转移分布上使用后验分布。这种方法通常被称为<strong class="kk iu">随机化价值函数</strong>(RVF)【7】。在这种情况下，对从后验分布采样的假设采取最优行动<strong class="kk iu">与为采样的 Q 函数计算贪婪策略</strong>一样简单。</strong></p><p id="0d70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RVF 在实践中被证明是成功的，导致 RL 方法执行更有效的探索。然而，当 Q 函数由神经网络近似时，这种方法<strong class="kk iu">仍然具有局限性</strong>。</p><p id="7cdd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，获得 Q 函数的后验分布并不简单，因为当我们执行非策略 RL 时，我们无法直接访问 Q 函数的输入和相应输出值形式的<strong class="kk iu">数据。许多使用神经网络近似的 RVF 方法<strong class="kk iu">错误地假设情况就是这样</strong>。此外，这些方法还存在以下<strong class="kk iu">两个问题</strong>之一:</strong></p><ol class=""><li id="b843" class="nb nc it kk b kl km ko kp kr nd kv ne kz nf ld ng nh ni nj bi translated">Q 函数上的分布<strong class="kk iu">忽略了函数值</strong>之间的相关性，例如由<strong class="kk iu">贝尔曼方程</strong>给出的相关性。</li><li id="3914" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">Q 函数上的分布捕捉到了这些依赖性，但是计算成本<strong class="kk iu">很大</strong>。</li></ol><p id="2fa2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SU 没有这些问题，因为它捕获了由贝尔曼方程给出的依赖性，并且具有低计算成本。</p><h1 id="cec1" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">后续不确定性</h1><p id="0412" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">给定策略π的 Q 函数上的后验分布可以通过以下方式获得:首先，从观察到的元组(r_t+1，s_t+1，s_t，a_t)计算转移分布上的后验分布，然后将该后验分布映射到 Q 函数上的对应分布。</p><p id="42c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">令人惊讶的是，当转移分布满足以下假设时，前面的操作可以直接完成:</p><ol class=""><li id="8c8c" class="nb nc it kk b kl km ko kp kr nd kv ne kz nf ld ng nh ni nj bi translated">P(r_t+1|s_t，a_t)由<strong class="kk iu">线性高斯模型</strong>r _ t+1 =<strong class="kk iu">wϕ</strong>(s_t,a_t)+ϵ_t，其中<strong class="kk iu"> ϕ </strong> (s_t，a_t)是状态和动作的特征表示，ϵ_t 是高斯噪声。</li><li id="b544" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">忽略关于 P(s_t+1|s_t，a_t)的不确定性。</li></ol><p id="30e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">奖励的先前线性模型导致 Q 函数的对应<strong class="kk iu">线性模型。特别是，</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nu"><img src="../Images/a3441b5fdbd7b2fcb54812951a32f026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1_1MGmYfpBHEKk1TxJ1BsA.png"/></div></div></figure><p id="5f38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="kk iu"> ψ </strong> (s_t，a_t)为<strong class="kk iu">后继特征</strong>【10】，即π下每个<strong class="kk iu"> ϕ </strong> (s_t，a_t)的折现期望未来发生。</p><p id="4339" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在对于<strong class="kk iu"> w </strong>的高斯先验下，我们获得对于<strong class="kk iu"> w </strong>的高斯后验 N( <strong class="kk iu"> w|m，S </strong>),在前面的等式下，<strong class="kk iu"> </strong>导致对于 Q 函数的<strong class="kk iu">完全相关的</strong>高斯后验，均值和协方差函数由下式给出</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/daddcefa2395cba935942259f3fdc65d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*Po5THFnXkhRkdWxRtSHVfg.png"/></div></figure><p id="38f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于 Q 函数仅针对给定策略π定义，我们假设该策略是通过从后验分布中采样 Q 函数而获得的<strong class="kk iu">平均贪婪策略</strong>。</p><p id="d620" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个协方差函数保证采样的 Q 函数将满足<strong class="kk iu">贝尔曼方程</strong>。</p><h2 id="d8f4" class="mn lg it bd lh mo mp dn ll mq mr dp lp kr ms mt lr kv mu mv lt kz mw mx lv my bi translated">实际实施</h2><p id="c71c" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">初始特征<strong class="kk iu"> ϕ </strong> (s_t，a_t)和后续特征<strong class="kk iu"> ψ </strong> (s_t，a_t)使用<strong class="kk iu">多头神经网络</strong>计算，每个可能的动作值和特征类型(<strong class="kk iu"> ϕ </strong>或<strong class="kk iu"> ψ) </strong>一个头。每个头的输出乘以<strong class="kk iu"> m </strong>以获得对下一个奖励和 Q 函数值的预测。</p><p id="ae29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了 Atari 游戏设置的整个过程，头部的输入由一个卷积神经网络给出，后面是一个完全连接的层。在表格环境中，网络的这一部分可以由状态的一次热编码来代替。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/30f2b40bb456545863a214215c08c2c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*hcPk_RambH2VL7kjrWn33w.png"/></div></figure><p id="906a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">苏通过优化学习神经网络的参数和 m</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nx"><img src="../Images/c5e5e26f8f2cd7b79977eab46d35a854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tQc4kPXkWJHp2f7b2OU5aQ.png"/></div></div></figure><p id="55e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">期望超过通过与环境交互收集的元组(r_t+1，s_t+1，s_t，a_t)。</p><p id="8b20" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述目标包括三种不同的损失条款。第一个是<strong class="kk iu">后续特征</strong>的<strong class="kk iu">时间差误差</strong>。此项强制<strong class="kk iu"> ψ </strong>收敛于<strong class="kk iu"> ϕ </strong>的贴现预期未来发生。第二个损失项调整<strong class="kk iu"> m </strong>和<strong class="kk iu"> ϕ </strong>，使得回报预测中的<strong class="kk iu">误差较低</strong>。最后，第三项是<strong class="kk iu"> Q 函数</strong>和<strong class="kk iu"> </strong>的估计中的<strong class="kk iu">时间差异误差</strong>，它<strong class="kk iu"> </strong>强制<strong class="kk iu"> ψ </strong>也适用于预测 Q 函数值。</p><p id="e06c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，根据高斯线性模型中的贝叶斯规则在线更新<strong class="kk iu">协方差矩阵<strong class="kk iu"> S </strong>:</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/6a83f9055d3a3c3bbf048dd930f72306.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*LaxzGwUtd4-vbQlk3hMDSQ.png"/></div></figure><p id="8a96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中β是奖励的线性模型中的噪声方差，0≤ζ≤1 是允许模型遗忘的衰减因子，这有助于在<strong class="kk iu"> </strong>学习过程中对抗<strong class="kk iu"> ϕ </strong>的非平稳性。<strong class="kk iu"> </strong>在下面的实验中我们用了β = 0.001(二叉树)和β = 0.01(雅达利)和ζ = 1(二叉树)和ζ = 0.99999(雅达利)。</p><h1 id="36e8" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">二叉树基准的实验</h1><p id="a69a" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">我们评估了 SU 在一个需要<strong class="kk iu">高效探索</strong>的挑战性问题上的表现。在这个问题中，见下图，有<strong class="kk iu"> 2L+1 </strong>状态和两个可能的动作，这两个动作在环境生成时被随机映射到每个状态中的<strong class="kk iu">向上</strong>和<strong class="kk iu">向下</strong>的运动。奖励总是 0，除非在到达产生奖励 1 的 s_2L 州之后。奇数指数和 s_2L 的状态是终端。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/2ef509a557b1054abc0d93fc8660530c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*zkqHQGCYkKLRDtYOOFoKuw.png"/></div></figure><p id="46fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个问题<strong class="kk iu">对</strong>具有挑战性，因为代理将总是获得奖励 0，除非问题被解决，如果没有智能探索，这将以指数小概率发生，作为<strong class="kk iu"> L </strong>的函数。</p><p id="1220" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了学习最佳策略所需的发作次数的中位数。将 SU 与其他 RVF 方法进行比较，如贝叶斯 DQN ( <strong class="kk iu"> BDQN </strong> ) [3]、不确定性贝尔曼方程(<strong class="kk iu"> UBE </strong> ) [2]和<strong class="kk iu">自举 DQN </strong> [4]。后一种方法用一组 Q 函数估计量来逼近 Q 函数后验。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oa"><img src="../Images/df8ef7e9ed817552347cc3b94eb3b3c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8YEM1DFZo_HfExEmQIwcrQ.jpeg"/></div></div></figure><p id="9b73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">BDQN 和 UBE 执行与<strong class="kk iu">相同的策略，随机地对行为进行统一采样</strong><strong class="kk iu"/>，并且当<strong class="kk iu"> L </strong>很大时，他们都努力寻找最优策略。自举的也比苏差，当自举的使用比苏多 25 倍的计算时，差距变得更小。原因是我们的贝叶斯线性模型使用的分析更新允许不确定性得到真正快速的解释，而 bootstrap DQN 更依赖于基于梯度的更新。</p><h1 id="f768" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">雅达利游戏的实验</h1><p id="d1cb" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">为了表明它可以扩展到复杂的领域，我们在 49 个 Atari 游戏的标准集上评估了 SU[8]。实施、网络架构和培训程序的具体细节可以在[1]中找到。在[9]中描述的“无操作启动 30 分钟模拟器时间”测试协议下，在 200 米训练帧后，SU 获得了 2.09 的中值人类标准化得分(3 粒种子的平均值)。下表显示 SU 明显优于竞争方法。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ob"><img src="../Images/82b04663180b128079b5d0fb06f5711d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dVWaSYtJkVunADei1gQDHw.png"/></div></div></figure><p id="8bd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于更多细节，SU 和各个游戏的竞争算法之间的人类标准化分数的差异绘制在下图中。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oc"><img src="../Images/89b34bf0a465815ec73157a1743e1fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RJ5OLtgTt-i1zoFZnqdXOg.png"/></div></div><figcaption class="od oe gj gh gi of og bd b be z dk">Bars show the difference in human normalized score between SU and Bootstrap DQN (top),<br/>UBE (middle) and DQN (bottom) for each of the 49 Atari 2600 games. Blue indicates SU performed<br/>better, red worse. SU outperforms the baselines on 36/49, 43/49 and 42/49 games respectively.<br/>Y-axis values have been clipped to [−2.5, 2.5].</figcaption></figure><h1 id="6752" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">摘要</h1><p id="60dc" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">后继不确定性是<strong class="kk iu">无模型强化学习中<strong class="kk iu">高效探索</strong>的最新方法。</strong>尤其是苏</p><ol class=""><li id="ffbf" class="nb nc it kk b kl km ko kp kr nd kv ne kz nf ld ng nh ni nj bi translated">是 Q 函数的第一个概率方法，它结合了贝尔曼方程给出的<strong class="kk iu">依赖性。</strong></li><li id="e6d8" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">是高度可扩展的，和以前的方法一样快或者更快。</li><li id="0f3a" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated"><strong class="kk iu">在表格基准测试和雅达利游戏上胜过竞争对手</strong>。</li></ol><h1 id="94b2" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">参考</h1><p id="9781" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">[1] Janz* D .，Hron* J .，马祖尔 p .，霍夫曼 k .，埃尔南德斯-洛巴托 J. M .和 Tschiatschek S. <strong class="kk iu">后继者的不确定性:时间差异学习中的探索和不确定性</strong>，载于 NeurIPS，2019 年。*同等贡献者。</p><p id="fe17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]b .奥多诺格、I .奥斯本、r .穆诺斯和 v .姆尼赫<strong class="kk iu">《不确定性贝尔曼方程与探索</strong>。在 2018 年的 ICML。</p><p id="3c97" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3] Azizzadenesheli，k .、Brunskill，e .和 Anandkumar，A. <strong class="kk iu">通过贝叶斯深度 Q 网络进行有效探索</strong>。在 2018 年的 ICLR。<a class="ae le" href="https://openreview.net/forum?id=Bk6qQGWRb" rel="noopener ugc nofollow" target="_blank">https://openreview.net/forum?id=Bk6qQGWRb</a></p><p id="9262" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4] Osband，I .、Blundell，c .、Pritzel，a .、Van Roy，B. <strong class="kk iu">通过自举 DQN 进行深度探索</strong>。在 NeurIPS，2016。</p><p id="e470" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[5] Strens，M. <strong class="kk iu">强化学习的贝叶斯框架</strong>。2000 年在 ICML。</p><p id="1efc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[6] Osband，I .、Russo，d .和 Van Roy，B. <strong class="kk iu">(更)有效的通过后验抽样的强化学习</strong>。在 NeurIPS，2013 年。</p><p id="97db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[7] Osband，I .、Van Roy，b .、Wen，Z. <strong class="kk iu">通过随机值函数进行概括和探索</strong>。2016 年在 ICML。</p><p id="7d89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[8] Mnih，v .，Kavukcuoglu，k .，Silver，d .，鲁苏，A. A .，Veness，j .，Bellemare，M. G .，Graves，a .，Riedmiller，m .，Fidjeland，A. K .，Ostrovski，g .，等人<strong class="kk iu">通过深度强化学习实现人的水平控制</strong>。自然，518(7540):529，2015。</p><p id="5cf8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[9] Hessel，m .，Modayil，j .，van Hasselt，h .，Schaul，t .，Ostrovski，g .，Dabney，w .，Horgan，d .，Piot，b .，Azar，M. G .，和 Silver，D. <strong class="kk iu">彩虹:结合深度强化学习的改进</strong>。2018 年 AAAI 人工智能大会。</p><p id="a43a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[10] Dayan，P. <strong class="kk iu">改进时间差异学习的概括:后继表征</strong>。神经计算，5(4):613–624，1993。</p></div></div>    
</body>
</html>