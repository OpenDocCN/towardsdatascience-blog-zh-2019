<html>
<head>
<title>Don’t Sweat the Solver Stuff</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不要担心求解器的事情</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451?source=collection_archive---------1-----------------------#2019-09-27">https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451?source=collection_archive---------1-----------------------#2019-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9836" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Scikit-Learn 中更好的逻辑回归模型提示</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c634546106c4c781a3b5026a0d802f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mPPabdAkGBVQEAS-NXXTaw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Lighthouses Give Warnings</figcaption></figure><p id="ab48" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归是机器学习分类的基本算法。如果你是一名正在实践或有抱负的数据科学家，你会想知道如何使用它的来龙去脉。另外，Scikit-learn 的<code class="fe lu lv lw lx b">LogisticRegression</code>给出了关于改变默认求解器的警告，所以这是一个学习何时使用哪个求解器的好时机。😀</p><pre class="kj kk kl km gt ly lx lz ma aw mb bi"><span id="1bbb" class="mc md it lx b gy me mf l mg mh">FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.</span></pre><p id="e1f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，您将了解 Scikit-learn <code class="fe lu lv lw lx b">LogisticRegression</code>规划求解选项，并看到对它们的两个评估。此外，您将看到关键的 API 选项，并获得常见问题的答案。到本文结束时，您将对 Scikit 中的逻辑回归有更多的了解——学习而不是担心求解器的问题。😓</p><p id="1b9f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我在这个分析中使用的是 Scikit-learn 版本 0.21.3。</p><p id="0bdc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">2019 年 12 月 20 日更新</strong>:在 Scikit-learn 核心开发者和维护者 Andreas Mueller 提供了有益的反馈后，我对这篇文章做了几处编辑。</p><h1 id="06c6" class="mi md it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">何时使用逻辑回归</h1><p id="9933" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">分类问题是指你试图预测离散的结果，比如某人是否患有某种疾病。相比之下，回归问题是指试图预测一个连续变量的值，如房屋的销售价格。尽管逻辑回归的名字中有<em class="ne">回归</em>，但它是一种用于分类问题的算法。</p><p id="c61f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归可能是最重要的监督学习分类方法。这是广义线性模型的快速、通用扩展。</p><p id="45c9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归是一种优秀的基线算法。当特征和目标之间的关系不太复杂时，它工作得很好。</p><p id="937a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归生成通常可解释的要素权重，这在您需要能够解释决策原因时特别有用。这种可解释性经常会派上用场——例如，对于需要证明其贷款决策合理性的贷方。</p><p id="21e5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归问题没有封闭解。这很好——我们不使用线性回归问题的封闭解，因为它很慢。😉</p><p id="b34a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">求解逻辑回归是一个优化问题。谢天谢地，好人已经创造了几个求解算法，我们可以使用。😁</p><h1 id="58f0" class="mi md it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">规划求解选项</h1><p id="e442" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">Scikit-learn 提供了五种不同的解算器。每个求解程序都试图找到最小化成本函数的参数权重。这里有五个选项:</p><ul class=""><li id="56d5" class="nf ng it la b lb lc le lf lh nh ll ni lp nj lt nk nl nm nn bi translated"><code class="fe lu lv lw lx b">newton-cg</code> —一种牛顿法。牛顿法使用精确的海森矩阵。对于大型数据集来说，它很慢，因为它需要计算二阶导数。</li><li id="7d57" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated"><code class="fe lu lv lw lx b">lbfgs</code> —代表有限记忆 Broyden–Fletcher–gold farb–Shanno。它用梯度评估来近似二阶导数矩阵更新。它只存储最近的几次更新，因此节省了内存。对于大型数据集，它的速度不是很快。从 Scikit-learn 版本 0.22.0 起，它将成为默认求解器。</li><li id="6381" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated"><code class="fe lu lv lw lx b"><a class="ae nt" href="https://en.wikipedia.org/wiki/Coordinate_descent" rel="noopener ugc nofollow" target="_blank">liblinear</a></code> —大型线性分类库。使用坐标下降算法。坐标下降基于通过在循环中求解单变量优化问题来最小化多变量函数。换句话说，它一次朝一个方向向最小值移动。它是 0.22.0 之前的 Scikit-learn 版本的默认解算器。它在高维情况下表现很好。它确实有一些缺点。它会被卡住，无法并行运行，只能用一对多解决多类逻辑回归。</li><li id="9b8e" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated"><code class="fe lu lv lw lx b"><a class="ae nt" href="https://hal.inria.fr/hal-00860051/document" rel="noopener ugc nofollow" target="_blank">sag</a></code> —随机平均梯度下降。梯度下降和增量聚集梯度方法的变体，使用先前梯度值的随机样本。适用于大数据集。</li><li id="7e7b" class="nf ng it la b lb no le np lh nq ll nr lp ns lt nk nl nm nn bi translated"><code class="fe lu lv lw lx b">saga</code>—<em class="ne">下垂</em>的延伸，也允许 L1 调整。一般来说训练速度应该比<em class="ne">下垂</em>快。</li></ul><p id="2da0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于不同选项的精彩讨论可以在<a class="ae nt" href="https://stackoverflow.com/a/52388406/4590385" rel="noopener ugc nofollow" target="_blank">这个堆栈溢出答案</a>中找到。</p><p id="a9f3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">来自<a class="ae nt" href="https://scikit-learn.org/stable/modules/linear_model.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn 文档</a>的下图列出了求解器的特征，包括可用的正则化惩罚。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/3b24f78b19394ba9c86ede10feb7972b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*u35QBoGx3thqrSLXcpMJWQ.png"/></div></figure><h2 id="79ef" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">为什么默认求解器会改变？</h2><p id="a130" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated"><code class="fe lu lv lw lx b">liblinear</code>处理小数据集速度很快，但存在鞍点问题，无法在多个处理器内核上并行化。它只能用一 vs . rest 来解决多类问题。这也不利于截取，不利于解释。</p><p id="3789" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe lu lv lw lx b">lbfgs</code>避免了这些弊端，速度相对较快。对于没有真正大型数据集的大多数情况，这是最佳选择。关于为什么默认设置被改变的一些讨论在<a class="ae nt" href="https://github.com/scikit-learn/scikit-learn/issues/9997" rel="noopener ugc nofollow" target="_blank">这个 GitHub 问题</a>中。</p><p id="5d70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们用两个预测分类项目来评估逻辑回归求解器—一个二元预测分类项目和一个多类预测分类项目。</p><h1 id="dd0a" class="mi md it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">规划求解测试</h1><h2 id="cb4b" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">二元分类求解器示例</h2><p id="d312" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">首先，我们来看一个二元分类问题。我使用了内置的<a class="ae nt" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn 乳腺癌数据集</a>。目标是预测乳腺肿块是否是癌性的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/b9f0059fdc77a09a4b034fd3417b7fbc.png" data-original-src="https://miro.medium.com/v2/format:webp/1*NLY8IKHLJzGae5WQMW8UdA.jpeg"/></div></figure><p id="41a7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些特征由关于细胞核的数字数据组成。它们是由活检的数字化图像计算出来的。数据集包含 569 个观测值和 30 个数值特征。我将数据集分为训练集和测试集，并使用不同的求解器对训练集进行网格搜索。你可以访问我的 Jupyter 笔记本，它用于对<a class="ae nt" href="https://www.kaggle.com/discdiver/logistic-regression-don-t-sweat-the-solver-stuff" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的所有分析。</p><p id="7dcc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最相关的代码片段如下。</p><pre class="kj kk kl km gt ly lx lz ma aw mb bi"><span id="bfb1" class="mc md it lx b gy me mf l mg mh">solver_list = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']<br/>params = dict(solver=solver_list)<br/>log_reg = LogisticRegression(C=1, n_jobs=-1, random_state=34)<br/>clf = GridSearchCV(log_reg, params, cv=5)<br/>clf.fit(X_train, y_train)<br/>scores = clf.cv_results_['mean_test_score']<br/><br/>for score, solver in zip(scores, solver_list):<br/>    print(f"  {solver} {score:.3f}" )</span></pre><p id="ab23" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结果如下:</p><pre class="kj kk kl km gt ly lx lz ma aw mb bi"><span id="2973" class="mc md it lx b gy me mf l mg mh">  liblinear 0.939<br/>  newton-cg 0.939<br/>  lbfgs 0.934<br/>  sag 0.911<br/>  saga 0.904</span></pre><p id="6258" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ne"> sag </em>和<em class="ne"> saga </em>的精度值略低于同类产品。</p><p id="463d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缩放特征后，解算器都表现得更好，并且<em class="ne"> sag </em>和<em class="ne"> saga </em>与其他解算器一样准确。</p><pre class="kj kk kl km gt ly lx lz ma aw mb bi"><span id="094c" class="mc md it lx b gy me mf l mg mh">   liblinear 0.960<br/>    newton-cg 0.962<br/>    lbfgs 0.962<br/>    sag 0.962<br/>    saga 0.962</span></pre><p id="3e97" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们看一个有三个类的例子。</p><h2 id="4431" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">多类求解器示例</h2><p id="3bd9" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">我用 Scikit-learn 的<a class="ae nt" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine" rel="noopener ugc nofollow" target="_blank">葡萄酒数据集</a>评估了多类分类问题中的逻辑回归求解器。该数据集包含 178 个样本和 13 个数字特征。目标是从葡萄酒的化学特征来预测用来酿酒的葡萄的类型。</p><pre class="kj kk kl km gt ly lx lz ma aw mb bi"><span id="2f1d" class="mc md it lx b gy me mf l mg mh">solver_list = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']<br/>parameters = dict(solver=solver_list)<br/>lr = LogisticRegression(random_state=34, multi_class="auto", n_jobs=-1, C=1)<br/>clf = GridSearchCV(lr, parameters, cv=5)<br/>clf.fit(X_train, y_train)<br/>scores = clf.cv_results_['mean_test_score']<br/><br/>for score, solver, in zip(scores, solver_list):<br/>    print(f"{solver}: {score:.3f}")</span></pre><p id="c126" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Scikit-learn 给出警告，说明<em class="ne"> sag </em>和<em class="ne"> saga </em>模型没有收敛。换句话说，他们从来没有到达过最低点。不出所料，结果对那些解算者来说并不太好。</p><pre class="kj kk kl km gt ly lx lz ma aw mb bi"><span id="516c" class="mc md it lx b gy me mf l mg mh">liblinear: 0.962<br/>newton-cg: 0.947<br/>lbfgs: 0.955<br/>sag: 0.699<br/>saga: 0.662</span></pre><p id="8824" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们使用 Seaborn 库制作一个小小的条形图来显示分数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/bb8d06bd1ade6c9e243cb578e916caef.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Qqo_Y17-41gpMRCl3ywyjw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">sag and saga lagging show lower accuracy</figcaption></figure><p id="3c74" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 0 和 1 之间缩放特征后，然后<em class="ne"> sag </em>和<em class="ne"> saga </em>达到与其他模型相同的平均准确度分数。</p><pre class="kj kk kl km gt ly lx lz ma aw mb bi"><span id="e1e8" class="mc md it lx b gy me mf l mg mh">liblinear: 0.955<br/>newton-cg: 0.970<br/>lbfgs: 0.970<br/>sag: 0.970<br/>saga: 0.970</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/0b8e165fbcd4246cb99875ad783eeb21.png" data-original-src="https://miro.medium.com/v2/format:webp/1*zuqd-1kMjSxiJwdAuQ-GVA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Looking better for sag and saga</figcaption></figure><p id="8ed1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，这两个例子都是小数据集。此外，在这些示例中，我们没有考虑内存和速度需求。</p><p id="4eef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">底线:即将到来的默认<em class="ne"> lbfgs </em>解算器对于大多数情况来说是一个很好的首选。如果你正在处理一个大型数据集或者想要应用 L1 正则化，我建议你从<em class="ne"> saga </em>开始。记住,<em class="ne"> saga </em>需要相似比例的特征。</p><p id="6ec5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你有<em class="ne">牛顿-cg </em>或者<em class="ne">凹陷</em>的用例吗？如果有，请在评论中分享。💬</p><p id="da94" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我将在 Scikit-learn 中揭开 LogisticRegression 的关键参数选项的神秘面纱。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/97b2583b471669d64622d4041b7980f3.png" data-original-src="https://miro.medium.com/v2/format:webp/1*wVTEAYydp5smSuUsCkLg7Q.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Logistics</figcaption></figure><h2 id="4cef" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">因素</h2><p id="1384" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">Scikit-learn LogisticRegression 类可以接受以下参数。</p><p id="c95d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe lu lv lw lx b">penalty</code>、<code class="fe lu lv lw lx b">dual</code>、<code class="fe lu lv lw lx b">tol</code>、<code class="fe lu lv lw lx b">C</code>、<code class="fe lu lv lw lx b">fit_intercept</code>、<code class="fe lu lv lw lx b">intercept_scaling</code>、<code class="fe lu lv lw lx b">class_weight</code>、<code class="fe lu lv lw lx b">random_state</code>、<code class="fe lu lv lw lx b">solver</code>、<code class="fe lu lv lw lx b">max_iter</code>、<code class="fe lu lv lw lx b">verbose</code>、<code class="fe lu lv lw lx b">warm_start</code>、<code class="fe lu lv lw lx b">n_jobs</code>、<code class="fe lu lv lw lx b">l1_ratio</code></p><p id="e29e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我不会列出下面所有的参数，只是从那些对大多数人最有价值的参数中摘录一些。省略部分见<a class="ae nt" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">文件</a>。我已经用<em class="ne">斜体</em>添加了额外的信息。</p><p id="1922" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe lu lv lw lx b">C</code> —浮点型，可选，默认= 1。值越小，正则化程度越高。正则化强度的倒数。<em class="ne">必须是正值。通常用对数搜索:[.001，. 01，. 1，1，10，100，1000] </em></p><p id="da51" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe lu lv lw lx b">random_state</code> : int，RandomState instance 或 None，可选(默认=None) <em class="ne">注意，为了再现性，必须在此设置随机状态。</em></p><p id="2e88" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe lu lv lw lx b">solver</code> { '牛顿-cg '，' lbfgs '，' liblinear '，' sag '，' saga'}，可选(默认='liblinear ')。更多信息见上图。</p><p id="aa95" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">在 0.20 版本中更改:在 0.22 版本中，默认将从“liblinear”更改为“lbfgs”。</strong></p><p id="1df0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe lu lv lw lx b">multi_class</code> : str，{'ovr '，'多项式'，' auto'}，optional(默认='ovr ')如果选择的选项是' ovr '，那么二元问题适合每个标签。对于“多项式”,最小化的损失是整个概率分布的多项式损失拟合，即使数据是二进制的。当 solver =“liblinear”时，“多项式”不可用。如果数据是二进制的，或者如果 solver='liblinear ',则' auto '选择' ovr ',否则选择'多项式'。</p><p id="6231" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">在 0.20 版本中更改:在 0.22 中默认将从‘ovr’更改为‘auto’。</strong> <em class="ne"> ovr 代表一对休息。参见下面的进一步讨论。</em></p><p id="bc02" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe lu lv lw lx b">l1_ratio</code> : float 或 None，可选(默认=None)弹性网混合参数，0 &lt; = l1_ratio &lt; = 1。仅在 penalty='elasticnet '时使用。设置' l1_ratio=0 相当于使用 penalty='l2 '，而设置 l1_ratio=1 相当于使用 penalty='l1 '。对于 0 &lt; l1_ratio &lt; 1，罚的是 l1 和 L2 的组合。<em class="ne">只针对佐贺。</em></p><p id="7a66" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ne">注释:</em>如果你有一个多类问题，那么将<code class="fe lu lv lw lx b">multi-class</code>设置为<code class="fe lu lv lw lx b">auto</code>将会使用多项式选项。那是理论上最合理的选择。<code class="fe lu lv lw lx b">auto</code>很快就会默认。</p><p id="14bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果想用<em class="ne"> saga </em>解算器使用一些 l1 正则化，使用<em class="ne"> l1_ratio </em>。请注意，与 ElasticNet 线性回归选项一样，您可以混合使用 L1 和 L2 惩罚。</p><p id="232d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还要注意，默认情况下应用了<code class="fe lu lv lw lx b">C=1</code>的 L2 正则化。这种默认的正则化使模型对多重共线性更加稳健，但代价是可解释性降低(向 Andreas Mueller 致敬)。</p><p id="1c57" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">拟合模型后，属性为:<code class="fe lu lv lw lx b">classes_</code>、<code class="fe lu lv lw lx b">coef_</code>、<code class="fe lu lv lw lx b">intercept_</code>和<code class="fe lu lv lw lx b">n_iter</code>。<code class="fe lu lv lw lx b">coef_</code>包含一个特征权重数组。</p><h1 id="d096" class="mi md it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">逻辑回归常见问题</h1><p id="55a3" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">现在让我们在 Scikit-learn 中解决那些你可能对逻辑回归有疑问的问题。</p><h2 id="86a3" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">我可以使用 LogisticRegression 解决多标签问题吗——这意味着一个输出可以同时成为多个类的成员？</h2><p id="d4ee" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">没有。不好意思，如果你需要的话，在这里找另一个分类算法<a class="ae nt" href="https://scikit-learn.org/stable/modules/multiclass.html" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="8e19" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">我应该使用哪种正则化？</h2><p id="8a74" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">在偏差/方差权衡中，正则化使你的模型偏向事物的偏差一方。正则化使得逻辑回归模型更具普遍性，尤其是在数据点很少的情况下。您可能想要对正则化参数<em class="ne"> C </em>进行超参数搜索。</p><p id="d7ea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果想通过正则化做一些降维，就用 L1 正则化。L1 调整是曼哈顿或出租车调整。L2 正则化是欧几里德正则化，通常在广义线性回归问题中表现更好。</p><p id="5341" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想混合应用 L1 和 L2 正则化，你必须使用<em class="ne"> saga </em>解算器。<em class="ne"> liblinear </em>求解器要求您进行正则化。然而，你可以只让<em class="ne"> C </em>这样一个大的值，它有一个非常非常小的正则化代价。同样，<em class="ne"> C </em>当前默认设置为 1。</p><h2 id="007c" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">我应该缩放特征吗？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/09869eb3481c505b291fc706a70c9812.png" data-original-src="https://miro.medium.com/v2/format:webp/1*xK-BVFiS5dlP-DWy-T-CPg.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Scale</figcaption></figure><p id="c87a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果使用<em class="ne"> sag </em>和<em class="ne"> saga </em>解算器，确保特征的比例相似。我们在上面看到了这一点的重要性。Andreas Mueller 在私人信件中还提到，他在使用<em class="ne"> lbfgs </em>的未缩放数据上发现了收敛问题，尽管它比<em class="ne"> sag </em>和<em class="ne"> saga </em>更健壮。</p><p id="c966" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">底线:为了安全，扩展你的数据。</p><h2 id="f6f8" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">我应该删除异常值吗？</h2><p id="2fe1" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">大概吧。移除异常值通常会提高模型性能。标准化输入也将减少异常值的影响。</p><p id="908e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">RobustScaler 可以缩放特征，您可以避免丢弃异常值。点击这里查看我的文章<a class="ae nt" rel="noopener" target="_blank" href="/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02?source=friends_link&amp;sk=a82c5faefadd171fe07506db4d4f29db">讨论缩放和标准化。</a></p><h2 id="4659" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">其他哪些假设真的很重要？</h2><p id="18a8" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">观察值应该是相互独立的。</p><h2 id="b05b" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">我应该使用多项式和交互来变换我的特征吗？</h2><p id="fbd2" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">正如线性回归一样，您可以使用高阶多项式和交互作用。这种转换允许您的模型学习更复杂的决策边界。然后，你就不会受限于线性决策边界。然而，过度拟合成为一种风险，解释特征重要性变得更加棘手。求解器找到全局最小值也可能更加困难。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/6c32745ac02bdb1818418bcd8b477dce.png" data-original-src="https://miro.medium.com/v2/format:webp/1*nKHHxzCXV18eJWRabzbgwg.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Transformation</figcaption></figure><h2 id="8b80" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">如果有很多特征，我应该进行降维吗？</h2><p id="c359" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">也许吧。如果可解释性不重要，主成分分析是一个不错的选择。递归特征消除可以帮助您删除最不重要的特征。或者，如果使用<em class="ne">传奇</em>解算器，L1 正则化可以将不太重要的特征权重驱动为零。</p><h2 id="3280" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">我的要素中的多重共线性是一个问题吗？</h2><p id="838f" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">它用于解释特性的重要性。当变量之间存在高度相关性时，您不能依赖模型权重。影响结果变量的功劳可能只归于其中一个相关的特征。</p><p id="f891" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有许多方法可以测试多重共线性。参见<a class="ae nt" href="http://www.frontiersin.org/files/EBooks/194/assets/pdf/Sweating%20the%20Small%20Stuff%20-%20Does%20data%20cleaning%20and%20testing%20of%20assumptions%20really%20matter%20in%20the%2021st%20century.pdf" rel="noopener ugc nofollow" target="_blank">克拉哈等人(2012)此处</a>。</p><p id="290f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个流行的选择是检查方差膨胀因子(VIF)。大约 5 到 10 的 VIF 截止值是常见的，但是关于 VIF 截止值应该是多少有一个激烈的争论。</p><p id="27fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以通过对相关矩阵求逆，然后对每个要素取对角线上的值来计算 VIF。</p><p id="19ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">单独的相关系数不足以确定具有多个要素的有问题的多重共线性。</p><p id="3873" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果样本量较小，获取更多数据可能对消除多重共线性最有帮助。</p><h2 id="1a49" class="mc md it bd mj nv nw dn mn nx ny dp mr lh nz oa mt ll ob oc mv lp od oe mx of bi translated">什么时候应该使用 LogisticRegressionCV？</h2><p id="5df0" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated"><a class="ae nt" href="https://scikit-learn.org/stable/modules/linear_model.html" rel="noopener ugc nofollow" target="_blank"><em class="ne">LogisticRegressionCV</em></a>如果您拥有大量数据，并且希望在进行交叉验证以调整超参数的同时加快计算速度，那么这就是您想要的 Scikit-learn 算法。</p><h1 id="7248" class="mi md it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">包装</h1><p id="0717" class="pw-post-body-paragraph ky kz it la b lb mz ju ld le na jx lg lh nb lj lk ll nc ln lo lp nd lr ls lt im bi translated">现在你知道当你看到<code class="fe lu lv lw lx b">LogisticRegression</code>规划求解警告时该怎么做了——更好的是，知道如何在第一时间避免它。不再流汗！😅</p><p id="54d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我建议你在大多数情况下使用即将到来的默认<em class="ne"> lbfgs </em>解算器。如果你有很多数据或者需要 L1 正则化，试试<em class="ne"> saga </em>。如果你用的是<em class="ne"> saga </em>的话，一定要缩放你的特征。</p><p id="f915" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这个关于逻辑回归的讨论对你有所帮助。如果你有，请在你最喜欢的社交媒体上分享，这样其他人也可以找到它。👍</p><p id="5e01" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我写关于<a class="ae nt" href="https://memorablepython.com" rel="noopener ugc nofollow" target="_blank"> Python </a>、<a class="ae nt" href="https://memorabledocker.com" rel="noopener ugc nofollow" target="_blank"> Docker </a>、<a class="ae nt" href="https://memorablesql.com" rel="noopener ugc nofollow" target="_blank"> SQL </a>、数据科学和其他技术主题的文章。如果你对此感兴趣，在这里阅读更多<a class="ae nt" href="https://medium.com/@jeffhale" rel="noopener">并注册我的</a><a class="ae nt" href="https://dataawesome.com" rel="noopener ugc nofollow" target="_blank">时事通讯</a>。😄</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://dataawesome.com"><div class="ab gu cl nu"><img src="../Images/1c5a609331f9021c7594f8db324a27ca.png" data-original-src="https://miro.medium.com/v2/format:webp/1*hrCYUZpi5QwyvgbdcrCEiA.png"/></div></a></figure><p id="be95" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">物流快乐！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/a6be5aea137e2515ce5a2de49b9efbcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wmkqk5HGTOngI8QS37j_nQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Good night, lighthouse</figcaption></figure></div></div>    
</body>
</html>