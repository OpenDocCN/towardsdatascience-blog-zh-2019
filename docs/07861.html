<html>
<head>
<title>How vital are powerful graphics for Data-Science?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强大的图形对数据科学有多重要？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-vital-are-powerful-graphics-for-data-science-5ead7a54d50c?source=collection_archive---------21-----------------------#2019-10-30">https://towardsdatascience.com/how-vital-are-powerful-graphics-for-data-science-5ead7a54d50c?source=collection_archive---------21-----------------------#2019-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/6e28861138f1f302cd60fde5c5d1c446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dq21VqKRTBTvhJH_kh-Qaw.jpeg"/></div></div></figure><p id="2fd7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当谈到深度学习时，通常你可以将模型带到的规范与你机器内部的硬件绑定在一起。我们都见过强化的深度学习机器，它们配有巨大的鼓风机式冷却器、一体机，以及大量的内存插槽，只有最快的 DDR4 才能填满。考虑到这一点，我们的 GPU 对机器学习有多重要？当我们拟合模型和构建神经网络时，我们可以期望我们的 GPU 为我们执行什么样的角色？</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><blockquote class="lg"><p id="d1bc" class="lh li it bd lj lk ll lm ln lo lp ky dk translated">(确切地说)不重要。</p></blockquote><p id="e67a" class="pw-post-body-paragraph kb kc it kd b ke lq kg kh ki lr kk kl km ls ko kp kq lt ks kt ku lu kw kx ky im bi translated">硬件对标准深度学习的影响可以像这样映射出来。</p><ul class=""><li id="2e50" class="lv lw it kd b ke kf ki kj km lx kq ly ku lz ky ma mb mc md bi translated">处理器</li><li id="fe9f" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">随机存取存储器</li><li id="3b77" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">互换</li><li id="9cb7" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">读写速度(在某些情况下)</li><li id="34b2" class="lv lw it kd b ke me ki mf km mg kq mh ku mi ky ma mb mc md bi translated">图形卡</li></ul><p id="5b0a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">随着这些信息在你的大脑中跳跃，你可能会思考 GPU 在日常 ML 中可能扮演的角色。首先，虽然渲染可视化只依赖于处理器，但是 GPU 仍然需要估算所有的单独向量，向量 2，以及你放入屏幕的任何其他几何图形。因此，对于你绘制数百万对数百万的可视化来说，一个能够支持计算的显卡是必不可少的。</p><p id="b6de" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，有了股票包的经验，我们注定要 CPU 来做我们所有的计算，并拉和推一切来回到内存，并最终到我们的显卡。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="172a" class="mj mk it bd ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bi translated">人们添加它们的原因</h1><p id="69f9" class="pw-post-body-paragraph kb kc it kd b ke nh kg kh ki ni kk kl km nj ko kp kq nk ks kt ku nl kw kx ky im bi translated">GPU 的可取之处在于它以包的形式设计了 CUDA。大多数统计机器学习操作涉及以数字矩阵的形式移动大量数据，并实时计算值。幸运的是，这正是显卡的设计目的。</p><figure class="nn no np nq gt ju gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/cdf4f5a625b42e42626ab7b93296565b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*YxJwESFXOTtzkF_HCo3KFQ.png"/></div></figure><p id="5169" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">虽然在您的包中使用 CUDA 有一些缺点，但是好处远远超过这些问题。CUDA 允许你的 GPU 和 CPU 互换通信，其中你的显卡非常擅长矩阵乘法和矢量渲染，你的 CPU 非常擅长发送、接收和存储数据。当然，从优化的角度来看，这是一个粗略的话题。在我使用 CUDAnative.jl 在 Julia 中进行的测试中，结果肯定比使用原始 CPU 能力来提供所有结果要快。这是一个严重的缺点，我们称之为频率步进。在典型的情况下，中央处理器更小，更不专业。而图形处理单元更专用于较低的核心时钟。因此，在一起计算时，GPU 和 CPU 可以有效地交换意见。</p><p id="d07c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这不是问题，问题是跨硬件 IRQ 不平衡:当一个硬件必须等待另一个硬件时，时间就被浪费了，整个几百 mHz 都因为这个问题而丢失了。</p><p id="eff8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">考虑到这一点，CPU 绝对擅长处理这个问题，这个问题以任何明显的瓶颈的形式出现，所以一个平衡的系统可能是理想的。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><figure class="nn no np nq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/addc22628a926acbc466acc6b3d46348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cByY467LMa-LBYb6UOBRYA.jpeg"/></div></div></figure><p id="92c6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以回到一个系统上的四个 GPU，通常这对于机器学习来说是一个低效的设置，因为尽管图形能力非常棒，但单个 CPU 的能力根本不足以支持这样的事情。这就是为什么我们通常会在数据科学 ThinkStation 中看到匹配良好的双 CPU 和双 GPU。在这个特定的使用案例中，我们充分利用了两个显卡的全部潜力，并拥有支持每个显卡的处理速度。</p><p id="071b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">没有人知道未来几年处理器、显卡和中央处理器会怎么样。就我个人而言，当我们看到英特尔试图对抗新的锐龙处理器，同时试图推出一款产品来击败 amd 即将推出的 5 纳米台式机(而不是服务器)CPU 时，我非常兴奋地看到明年的“巨人的冲突”。我能确定的是，2020 年对计算机、人工智能和技术爱好者来说肯定会是激动人心的一年。</p></div></div>    
</body>
</html>