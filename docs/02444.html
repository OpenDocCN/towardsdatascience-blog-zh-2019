<html>
<head>
<title>New in Hadoop: You should know the Various File Format in Hadoop.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Hadoop 中的新特性:你应该知道 Hadoop 中的各种文件格式。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/new-in-hadoop-you-should-know-the-various-file-format-in-hadoop-4fcdfa25d42b?source=collection_archive---------5-----------------------#2019-04-22">https://towardsdatascience.com/new-in-hadoop-you-should-know-the-various-file-format-in-hadoop-4fcdfa25d42b?source=collection_archive---------5-----------------------#2019-04-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bcea" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">Hadoop 文件格式初学者指南</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/a347a26ce5c8097abc11cc6289761805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mSGG8kqyDeSNbMLRkaVvCA.png"/></div></div></figure><p id="1216" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi lr translated"><span class="l ls lt lu bm lv lw lx ly lz di">答</span>几周前，我写了一篇关于 Hadoop 的文章，并谈到了它的不同部分。以及它如何在数据工程中发挥重要作用。在这篇文章中，我将总结 Hadoop 中不同的文件格式。这个话题将会很简短。如果你想了解 Hadoop 是如何工作的，以及它在数据工程中的重要作用，请访问我关于 Hadoop 的文章，或者跳过它。</p><div class="ma mb gp gr mc md"><a rel="noopener follow" target="_blank" href="/a-brief-summary-of-apache-hadoop-a-solution-of-big-data-problem-and-hint-comes-from-google-95fd63b83623"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd iu gy z fp mi fr fs mj fu fw is bi translated">Apache Hadoop 小结:大数据问题的解决方案和来自 Google 的提示</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">欢迎学习大数据和 Hadoop 简介，我们将在这里讨论 Apache Hadoop 以及如此大的问题…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr kt md"/></div></div></a></div><p id="1ecb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="ms"> Hadoop </em>中的文件格式大致分为两类:<strong class="kx iu">面向行和面向列:</strong></p><p id="ae74" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> <em class="ms">面向行:</em> </strong> <br/>存储在一起的同一行数据即连续存储:SequenceFile、MapFile、Avro Datafile。这样，如果只需要访问该行的少量数据，则需要将整行数据读入内存。延迟序列化可以在一定程度上减轻问题，但是从磁盘读取整行数据的开销是无法收回的。面向行的存储适用于需要同时处理整行数据的情况。</p><p id="5697" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> <em class="ms">面向列:</em> </strong> <br/>将整个文件切割成几列数据，并将每列数据存储在一起:Parquet、RCFile、ORCFile。面向列的格式可以在读取数据时跳过不需要的列，适用于只有一小部分行在字段中的情况。但是这种读写格式需要更多的内存空间，因为缓存行需要在内存中(在多行中获取一列)。同时也不适合流式写入，因为一旦写入失败，当前文件无法恢复，而面向行的数据可以在写入失败时重新同步到上一个同步点，所以 Flume 使用面向行的存储格式。</p><div class="kk kl km kn gt ab cb"><figure class="mt ko mu mv mw mx my paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><img src="../Images/612eb10e7cbd32c3626db0056493d6cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*NEfrM4fgRqBmB_49X0gzrg.png"/></div></figure><figure class="mt ko mz mv mw mx my paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><img src="../Images/9fad0257cea180dd0c1f2e978ec3bd74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1632/format:webp/1*tqwYQA3kJt-Z79Tz8bWF2A.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk ne di nf ng">Picture 1.(Left Side )<strong class="bd nh">Show the Logical Table</strong> and Picture 2. ( Right Side) <strong class="bd nh"><em class="ki">Row-Oriented Layout(Sequence File)</em></strong></figcaption></figure></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ni"><img src="../Images/1f417943dfa49a0a72626915ba523fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p8ACZhv1nsjDAjNm75xicQ.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Picture 3. <strong class="bd nh"><em class="ki">Column-oriented Layout (RC File)</em></strong></figcaption></figure><p id="5f5e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果仍然不清楚什么是面向行和面向列，不要担心，你可以访问<a class="ae nj" href="https://www.geeksforgeeks.org/dbms-row-oriented-vs-column-oriented-data-stores/" rel="noopener ugc nofollow" target="_blank">这个链接</a>并知道它们之间的区别。</p><p id="4b62" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">以下是一些在 Hadoop 系统上广泛使用的相关文件格式:</p><h1 id="54d1" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated"><strong class="ak">序列文件</strong></h1><p id="2e46" class="pw-post-body-paragraph kv kw it kx b ky oc ju la lb od jx ld le oe lg lh li of lk ll lm og lo lp lq im bi translated">存储格式因是否压缩以及是使用记录压缩还是块压缩而异:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oh"><img src="../Images/0b1ef8f33c0b9fcc55af326cb4631bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dTzplKiDwtHOdCmgNRp0FQ.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">The Internal structure of a sequence file with no compression and with record compression.</figcaption></figure><p id="8c69" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">不压缩:</strong> <br/>按照记录长度、键值长度、值度、键值、值值顺序存储。该范围是字节数。使用指定的序列化执行序列化。</p><p id="1ee2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">记录压缩:</strong> <br/>只对值进行压缩，并将压缩后的编解码器存储在头中。</p><p id="6ff4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">分块压缩:</strong> <br/>多条记录一起压缩，利用记录间的相似性，节省空间。在块的前后添加同步标志。块的最小值是由属性设置的 io . seqfile . compress . block size。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oi"><img src="../Images/97102d931c1d2bc4a5db79beb31d1c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u7FMbRlHCqFe2G7PAHdvcw.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">The internal structure of a sequence file with block compression</figcaption></figure><h1 id="375c" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated"><strong class="ak">地图文件</strong></h1><p id="fa7c" class="pw-post-body-paragraph kv kw it kx b ky oc ju la lb od jx ld le oe lg lh li of lk ll lm og lo lp lq im bi translated">MapFile 是 SequenceFile 的变体。在向 SequenceFile 添加索引并对其排序后，它是一个映射文件。索引存储为一个单独的文件，通常每 128 条记录存储一个索引。可以将索引加载到内存中进行快速查找——这些文件按照键定义的顺序存储数据。<br/>映射文件记录必须按顺序写入。否则，将引发 IOException。</p><p id="81c2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">映射文件的派生类型:</p><ol class=""><li id="cb8e" class="oj ok it kx b ky kz lb lc le ol li om lm on lq oo op oq or bi translated"><strong class="kx iu"> SetFile : </strong>一个特殊的映射文件，用于存储一系列可写类型的键。钥匙是按顺序写的。</li><li id="f5d3" class="oj ok it kx b ky os lb ot le ou li ov lm ow lq oo op oq or bi translated"><strong class="kx iu"> ArrayFile: </strong> Key 是表示在数组中位置的整数，值可写。</li><li id="0cc0" class="oj ok it kx b ky os lb ot le ou li ov lm ow lq oo op oq or bi translated"><strong class="kx iu"> BloomMapFile: </strong>使用动态布隆过滤器针对 MapFile get()方法进行了优化。过滤器存储在内存中，只有当键值存在时，才调用常规的 get()方法来执行读取操作。</li></ol><p id="ffa3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">Hadoop 系统下面列出的文件包括 RCFile、ORCFile 和 Parquet。Avro 的面向列版本是 Trevni。</p><h1 id="8e39" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated"><strong class="ak"> RC 文件</strong></h1><p id="00d7" class="pw-post-body-paragraph kv kw it kx b ky oc ju la lb od jx ld le oe lg lh li of lk ll lm og lo lp lq im bi translated">Hive 的记录列式文件，这种类型的文件首先将数据逐行划分为行组，在行组内部，数据以列的形式存储。其结构如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/4b781538f7d8af9b3723516a6dc60a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*G2aI0TF0LXtfeoXVQC3rDg.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Data Layout of RC File in an HDFS block</figcaption></figure><p id="d4d2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">与纯粹的面向行和面向列相比:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/2e3ddf2a49805a6e3052f0496ed33f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*nvlYFk5g7jNnHfYIBf_lIQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Row-Store in an HDFS Block</figcaption></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/234f22269a32e27210bbf0d023dd680e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*lBBc2YXTlB1jXy-L0v5fVQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Column Group in HDFS Block</figcaption></figure><h1 id="51ff" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated"><strong class="ak">兽人档案</strong></h1><p id="58c3" class="pw-post-body-paragraph kv kw it kx b ky oc ju la lb od jx ld le oe lg lh li of lk ll lm og lo lp lq im bi translated">ORCFile(优化记录列文件)提供了比 RCFile 更有效的文件格式。它在内部将数据划分为默认大小为 250M 的条带。每个条带包括一个索引、数据和页脚。索引存储每列的最大值和最小值，以及每一行在列中的位置。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e504b3ab4d4824006c9b36239d5dafa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*Y8Urvk1yFoo0jmYf8XBvgQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">ORC File Layout</figcaption></figure><p id="e09d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在 Hive 中，以下命令用于使用 ORCFile:</p><pre class="kk kl km kn gt pb pc pd pe aw pf bi"><span id="d780" class="pg nl it pc b gy ph pi l pj pk">CREATE TABLE ...STORED AAS ORC <br/>ALTER TABLE ... SET FILEFORMAT ORC <br/>SET hive.default.fileformat=ORC</span></pre><h1 id="ad42" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">镶木地板</h1><p id="10fb" class="pw-post-body-paragraph kv kw it kx b ky oc ju la lb od jx ld le oe lg lh li of lk ll lm og lo lp lq im bi translated">基于 Google Dremel 的面向列的通用存储格式。尤其擅长处理深度嵌套的数据。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pl"><img src="../Images/e6f188089b771ba20530b5734fb1d4e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z6ZFxKQld7xdmekeQmedmA.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">The internal Structure of Parquet File</figcaption></figure><p id="fc95" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">对于嵌套结构，Parquet 将其转换为平面列存储，用重复级别和定义级别(R 和 D)表示，并在读取数据以重建整个文件时使用元数据来重建记录。结构。以下是研发的一个例子:</p><pre class="kk kl km kn gt pb pc pd pe aw pf bi"><span id="98e7" class="pg nl it pc b gy ph pi l pj pk">AddressBook {   <br/>     contacts: {     <br/>        phoneNumber: "555 987 6543"  <br/>}   <br/> contacts: {   <br/>} <br/>} <br/>AddressBook { <br/>}</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/8edc88553ddd0c3ad9d2d9f223fed316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*RlbAUs1fGMIXgbXsOaU9JA.png"/></div></figure><p id="c0ff" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，你知道 Hadoop 中不同的文件格式。如果您发现任何错误和建议，请随时与我联系。你可以通过我的 LinkedIn 联系我。</p><div class="ma mb gp gr mc md"><a rel="noopener follow" target="_blank" href="/forget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd iu gy z fp mi fr fs mj fu fw is bi translated">忘记 API 用漂亮的汤做 Python 抓取，从 web 导入数据文件:第 2 部分</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">API 并不都是为你准备的，但是美丽的汤会永远伴随着你。</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="pn l mo mp mq mm mr kt md"/></div></div></a></div></div></div>    
</body>
</html>