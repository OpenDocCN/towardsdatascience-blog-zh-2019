<html>
<head>
<title>Reinforcement Learning — Multi-Arm Bandit Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——多臂土匪实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-multi-arm-bandit-implementation-5399ef67b24b?source=collection_archive---------7-----------------------#2019-05-26">https://towardsdatascience.com/reinforcement-learning-multi-arm-bandit-implementation-5399ef67b24b?source=collection_archive---------7-----------------------#2019-05-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="40be" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">ϵ-greedy 与加州大学的比较</h2></div><p id="4358" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">多臂强盗是一个经典的强化学习问题，其中一个玩家面对<code class="fe lb lc ld le b">k</code>吃角子老虎机或强盗，每一个都有不同的奖励分配，并且玩家试图在试验的基础上最大化他的累积奖励。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/13b3f20228f83533901e45c99506d8dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5q0Mihf29fftuXpKWWX2uA.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk"><a class="ae mc" href="https://blog.lightningai.com/multi-armed-bandits-are-the-new-a-b-tests-27dd7b48765b" rel="noopener ugc nofollow" target="_blank">k-armed bandit</a></figcaption></figure><h1 id="6728" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">制定</h1><p id="b3c8" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">让我们直接切入这个问题。强化学习问题有 3 个关键组成部分——<strong class="kh ir">状态、动作</strong>和<strong class="kh ir">奖励</strong>。让我们回忆一下这个问题——k 台机器放在你面前，在每一集里，你选择一台机器并拉动手柄，通过这个动作，你会得到相应的奖励。因此，状态是所有机器的当前估计值，开始时为零，动作是您决定在每集选择的机器，奖励是您拉下手柄后的结果或支出。有了这三项，我们就可以编写<code class="fe lb lc ld le b">Bandit</code>的<code class="fe lb lc ld le b">init</code>功能了(这里查看<a class="ae mc" href="https://github.com/MJeremy2017/RL/blob/master/Multi-ArmBandit/bandit.py" rel="noopener ugc nofollow" target="_blank">代码</a>):</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="9e42" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">k</code>是我们希望初始化的盗匪数量，<code class="fe lb lc ld le b">actions</code>用数字表示，每个数字代表我们要搭载的机器，我们和往常一样有<code class="fe lb lc ld le b">exp_rate</code>(探索率)和<code class="fe lb lc ld le b">lr</code>(学习率)来控制探索和价值更新。<code class="fe lb lc ld le b">total_reward</code>和<code class="fe lb lc ld le b">avg_reward</code>用于衡量我们的结果。每台机器的赔付额是由正态分布生成的随机值，最终的初始估值被设置为 0。</p><h1 id="767d" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">行动</h1><h2 id="5ff6" class="nc me iq bd mf nd ne dn mj nf ng dp mn ko nh ni mp ks nj nk mr kw nl nm mt nn bi translated">选择行动</h2><p id="a79d" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">要找出报酬最高的机器，最直接的方法是尝试，尽可能多地尝试，直到对每台机器都有一定的信心，并从现在开始坚持使用最好的机器。问题是，我们可能可以用更明智的方式进行测试。由于我们的目标是沿途获得最大的回报，我们当然不应该浪费太多时间在一台总是给予低回报的机器上，另一方面，即使我们偶然发现了某台机器的诱人回报，我们仍然应该能够探索其他机器，希望一些没有被探索过的机器能够给我们更高的回报。</p><p id="2702" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，在选择行动时，我们将探索方法:</p><ol class=""><li id="f610" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated">ϵ-greedy</li><li id="5dca" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">UCB(置信上限)</li></ol><p id="35bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们已经多次谈到ϵ-greedy，在那里我们以ϵ概率采取随机行动，以 1 - ϵ概率采取最大行动。这在探索和开发之间取得了良好的平衡。让我们谈一谈基于以下公式的 UCB:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oc"><img src="../Images/32a722af81ec5a89d6a9fe51bb1d1a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*ZoBvrv8atg7_BosS37-m6w.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">UCB</figcaption></figure><p id="ec4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="od">其中</em> <code class="fe lb lc ld le b"><em class="od">lnt</em></code> <em class="od">表示</em> <code class="fe lb lc ld le b"><em class="od">t</em></code> <em class="od">的自然对数(为了等于</em> <code class="fe lb lc ld le b"><em class="od">t</em></code> <em class="od">而不得不将</em> <code class="fe lb lc ld le b"><em class="od">e = 2.71828</em></code> <em class="od">的数字)，而</em> <code class="fe lb lc ld le b"><em class="od">Nt(a)</em></code> <em class="od">表示动作 a 在时间</em> <code class="fe lb lc ld le b"><em class="od">t</em></code> <em class="od">之前被选择的次数(公式中的分母，数字</em> <code class="fe lb lc ld le b"><em class="od">c &gt; 0</em></code> <em class="od">控制探索的程度)。如果</em> <code class="fe lb lc ld le b"><em class="od">Nt(a)=0</em></code> <em class="od">，那么</em> <code class="fe lb lc ld le b"><em class="od">a</em></code> <em class="od">被认为是一个最大化动作。(摘自</em> <a class="ae mc" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">强化学习:入门</strong> </a> <em class="od"> ) </em></p><p id="d515" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以随着试验次数的增加，如果一个强盗还没有被探索，它的<code class="fe lb lc ld le b">Nt(a)</code>将会变小，因此公式的右边将会变大。这样，该公式有助于平衡当前估计值<code class="fe lb lc ld le b">Q</code>和勘探速度。</p><p id="1131" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了实现 UCB，我们需要在<code class="fe lb lc ld le b">init</code>函数中添加一些变量:</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="0fa3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">self.times</code>计算试验的总次数，而<code class="fe lb lc ld le b">self.action_times</code>计算每个土匪的行动次数。</p><p id="c61c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们可以将ϵ-greedy 和 UCB 放入我们的方法<code class="fe lb lc ld le b">chooseAction</code>函数中:</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="80c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当<code class="fe lb lc ld le b">self.ucb</code>打开时，我们将根据公式选择动作。如果值除以 0，则<code class="fe lb lc ld le b">self.action_times</code>增加 0.1。如果<code class="fe lb lc ld le b">exp_rate</code> if 设置为 0 并且<code class="fe lb lc ld le b">self.ucb</code>为真，动作将完全由 UCB 选择，但是如果<code class="fe lb lc ld le b">exp_rate &gt; 0</code>，那么我们将有一个混合ϵ-greedy 和 UCB 的混合方法。</p><h2 id="3bfa" class="nc me iq bd mf nd ne dn mj nf ng dp mn ko nh ni mp ks nj nk mr kw nl nm mt nn bi translated">采取行动和更新估计</h2><p id="b694" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated"><code class="fe lb lc ld le b">takeAction</code>函数接收一个动作，并在收到奖励后更新该动作的估计值。</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="2dbc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如问题所描述的，每个强盗的支付符合一定的分布，因此我们给每个真实的奖励增加了一些随机性。在收到奖励后，<strong class="kh ir">将通过添加当前观察和先前估计之间的差乘以学习速率</strong>来更新估计。</p><h2 id="5a98" class="nc me iq bd mf nd ne dn mj nf ng dp mn ko nh ni mp ks nj nk mr kw nl nm mt nn bi translated">玩</h2><p id="34d9" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">最后，我们需要一个函数来开始播放。</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="1cde" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">结果</h1><p id="a8fb" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">我们初始化了一个 5 臂 bandit，并比较了不同方法的平均增益。(点击查看<a class="ae mc" href="https://github.com/MJeremy2017/RL/blob/master/Multi-ArmBandit/bandit.py" rel="noopener ugc nofollow" target="_blank">代码)</a></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/a498f8649ce78f0a98ddcc18a286159a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*Gg8C5IzDujdR1x0ee_yTnA.png"/></div></figure><p id="3cb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是使用不同探索率的ϵ-greedy 方法超过 2000 次迭代的平均回报。当勘探率收敛时，0.1 的勘探率超过 0.3 的勘探率。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/98abdd47ffa8b51d2183e0a0448b7d29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*T0Ete3j0bbz3CAwKtPgXbA.png"/></div></figure><p id="e803" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们加入不同<code class="fe lb lc ld le b">c</code>值的 UCB 方法进行比较。两种 UCB 方法的勘探率都是 0.1。从结果中，我们看到，一般来说，UCB 优于ϵ-greedy，但除了 bandit 之外，在许多其他问题上也很难应用 UCB。</p><p id="ff2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请在这里查看完整的<a class="ae mc" href="https://github.com/MJeremy2017/RL/blob/master/Multi-ArmBandit/bandit.py" rel="noopener ugc nofollow" target="_blank">代码</a>，欢迎您贡献和提出问题！</p><p id="1f51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考:</strong></p><ul class=""><li id="a422" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la of nu nv nw bi translated"><a class="ae mc" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/the-book-2nd.html</a></li><li id="a655" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la of nu nv nw bi translated"><a class="ae mc" href="https://github.com/JaeDukSeo/reinforcement-learning-an-introduction" rel="noopener ugc nofollow" target="_blank">https://github . com/JaeDukSeo/reinforcement-learning-an-introduction</a></li></ul></div></div>    
</body>
</html>