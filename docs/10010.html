<html>
<head>
<title>SVD in Machine Learning: Underdetermined Least Squares</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的奇异值分解:欠定最小二乘</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/underdetermined-least-squares-feea1ac16a9?source=collection_archive---------11-----------------------#2019-12-31">https://towardsdatascience.com/underdetermined-least-squares-feea1ac16a9?source=collection_archive---------11-----------------------#2019-12-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0b02" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解 SVD 如何帮助导出超定和欠定线性系统的一致最小二乘权重向量</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b550b7f64fbcbdc8784e3f893ba2961e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emzEIrn1p5IMADngrPLzyw.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Source: Tim Hill on Pixabay</figcaption></figure><p id="6a47" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文讨论了超定和欠定线性系统中最小二乘权重向量的差异，以及如何应用<a class="ae lr" href="https://www.wikiwand.com/en/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">奇异值分解</a> (SVD)来导出一致的表达式。它在很大程度上基于<a class="ae lr" href="https://voices.uchicago.edu/willett/" rel="noopener ugc nofollow" target="_blank"> Rebecca Willet </a>教授的课程<a class="ae lr" href="https://voices.uchicago.edu/willett/teaching/fall-2019-mathematical-foundations-of-machine-learning/" rel="noopener ugc nofollow" target="_blank">机器学习的数学基础</a>，并且假设了线性代数的基础知识。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="29c4" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">“典型”最小二乘法</h1><p id="1b6c" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated"><strong class="kx ir">最小二乘</strong>可以描述为:给定形状<em class="mw"> n × p </em>的特征矩阵<strong class="kx ir"> <em class="mw"> X </em> </strong>和形状<em class="mw"> n × </em> 1 的目标向量<strong class="kx ir"> <em class="mw"> y </em> </strong>我们要找到一个形状<em class="mw"> n × </em> 1 <em class="mw"> </em>的系数向量<strong class="kx ir"><em class="mw">w’</em></strong><strong class="kx ir">直觉上</strong>，最小二乘法试图通过最小化每个单个方程结果的残差平方和来近似线性系统的解。</p><p id="647b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在大多数情况下，我们假设<em class="mw"> n ≥ p </em>并且 rank( <em class="mw"> X </em> ) = <em class="mw"> p. </em>换句话说，观测值的数量不小于特征的数量并且没有一个特征是其他特征的线性组合(没有“冗余”特征)。一个线性系统<em class="mw"> y </em> = <em class="mw"> Xw </em>如果<em class="mw"> n ≥ p </em>则<strong class="kx ir">超定</strong>。我们可以用<a class="ae lr" href="http://mlwiki.org/index.php/Normal_Equation#Normal_Equation" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">法方程</strong> </a>得到<em class="mw">w’</em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c569e728fe63bb22d08e260916cf3606.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*Wj5sRxGdlzKT-50ig24J-g.png"/></div></figure><p id="f7c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，如果<em class="mw"> n </em> &lt; <em class="mw"> p </em>或者当<em class="mw"> X </em>中的某些列是线性依赖时，矩阵<strong class="kx ir"> <em class="mw"> A </em> </strong>可能不可逆。当特征数大于观测数时，我们称线性系统<em class="mw"> y </em> = <em class="mw"> Xw </em> <strong class="kx ir">欠定</strong>。</p><h1 id="32f1" class="lz ma iq bd mb mc my me mf mg mz mi mj jw na jx ml jz nb ka mn kc nc kd mp mq bi translated">欠定最小二乘</h1><p id="57ea" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">当<em class="mw"> n </em> &lt; <em class="mw"> p </em>、秩(<em class="mw"> X </em> ) = <em class="mw"> n </em>时，系统<em class="mw"> y </em> = <em class="mw"> Xw </em>有无穷多个解。在这些解中，我们可以通过拉格朗日乘子法找到一个范数最小的解，并将其作为欠定线性系统的最小二乘权向量。</p><p id="78cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，<strong class="kx ir">为什么最小范数解是可取的</strong>？想象一个简单的例子，当你有下面的训练特征矩阵和目标向量时:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/8045ad8f33ec4cce7d879193d6763b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*BpFEaZYZfYn6l0gHcUZMHA.png"/></div></figure><p id="92cd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下两个权重向量为训练数据提供了完美的拟合:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3f628962bd0d156c5b5229576f3fb879.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*tYejT_bvCwONHRmHvcjq3g.png"/></div></figure><p id="0589" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">看这个的一种方式是:第三个特征是规模小。如果它容易受到测量误差的影响，并且我们对它赋予了很大的权重，那么我们对未知数据的预测可能会因为第三个特征而产生很大的偏差，从而远离真正的目标。所以第一种方案比较好。</p><p id="a7fc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面展示了我们如何用拉格朗日乘数法导出最小范数解:我们想要在约束条件为<em class="mw"> y </em> = <em class="mw"> Xw </em>的情况下最小化∨<em class="mw">w</em>∨。介绍拉格朗日乘数<strong class="kx ir"> <em class="mw"> L </em> </strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/a2030c9762ab18bdb8575be7225a97ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*zpiQwSBUVMJXZoPnG0wCiA.png"/></div></figure><p id="5f76" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在最佳条件下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/de35943568b5193b4f0e4b95f03c1e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*G4dvm4zINaOp53t2zkjBdw.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/e90047e5988d4b808334c8e79e21a099.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*dxy9X2EXDPw0xn8EFQIs_w.png"/></div></figure><p id="f4f8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">欠定最小二乘的权向量方程与超定最小二乘的权向量方程非常不同。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="6374" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">奇异值分解</h1><p id="8b30" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">本节提供了对 SVD 的基本介绍。考虑一个形状为<em class="mw">n</em>×p 的矩阵<em class="mw"> X </em>。总是存在矩阵<strong class="kx ir"><em class="mw"/></strong><strong class="kx ir"><em class="mw">【σ】</em></strong><strong class="kx ir"><em class="mw">V</em></strong>使得:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1b49a3b6cb6549dfc12f44d985ef3692.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*AVCPwLFJJEGmfVrRE0L5Zg.png"/></div></figure><p id="8411" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当<em class="mw"> U </em>和<em class="mw"> V </em>正交时:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/f0067f7c59fb34702741cfbf48c1a96d.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*u4xb5KODuiAs5V67g70eIA.png"/></div></figure><p id="6be8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mw">σ</em>是对角线:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/3bbe278e32f34c052b5a74be6e64ca97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aimhnCFYIO72gCzDi-oddA.png"/></div></div></figure><p id="67e6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mw"> U </em>的列是左奇异向量，它们构成了<em class="mw"> X </em>列的正交基。</p><p id="9b5f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mw">σ</em>的对角元素称为奇异值(<em class="mw">σ₁</em>≥<em class="mw">σ₂</em>≥…≥<em class="mw">σₚ</em>≥0)。非零奇异值的个数是矩阵<em class="mw"> X </em>的秩，<em class="mw">σ</em>的列是<em class="mw"> X </em>的行的基础。</p><p id="6268" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mw"> V </em>的行称为右奇异向量，它们是<em class="mw">uσ</em>列上的基系数，用来表示<em class="mw"> X </em>的每一列。</p><h1 id="648d" class="lz ma iq bd mb mc my me mf mg mz mi mj jw na jx ml jz nb ka mn kc nc kd mp mq bi translated">奇异值分解和最小二乘法</h1><p id="1683" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">使用 SVD，我们可以重写最小二乘权重向量。以欠定最小二乘法为例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/7b73c13802bf7c020f7bcc9056b32f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lC2wKfhlrje2Yrns_zgnog.png"/></div></div></figure><p id="f892" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面的表达式看起来有点吓人，但是如果我们仔细看看:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/abd4a23918f50200c15c3f78cfabe6a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*KUMqtPn16NDJ4BDxY7YQ5g.png"/></div></figure><p id="79a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里<strong class="kx ir"><em class="mw">σ⁺</em></strong>是<em class="mw">σ</em>的伪逆，形状为<em class="mw"> p </em> × <em class="mw"> n </em>。我们可以通过转置<em class="mw">σ</em>并取其对角元素的倒数来得到它。那么欠定线性系统的最小二乘向量可以重写为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a5893259162086cb686b34e881d99a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*FnFtWUrARK6aSsCB3ojJEQ.png"/></div></figure><p id="cad8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">超定最小二乘法也是如此(随意验证)。有了 SVD，我们就有了权重向量的一致表达式。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="ec8c" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">确认</h1><p id="f0b9" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">为了验证我们的发现，我们将使用<a class="ae lr" href="http://eigentaste.berkeley.edu/dataset/" rel="noopener ugc nofollow" target="_blank"> Jester 数据集</a>的子样本。样本包含 100 个观察值和 7200 个特征，可在<a class="ae lr" href="https://github.com/KunyuHe/SVD-in-Machine-Learning/blob/master/Underdetermined%20Least%20Squares/data/sample.mat" rel="noopener ugc nofollow" target="_blank">这里</a>获得。每个观察都是一个笑话，每个特征都是现有用户对该笑话的已知评级，等级为-10 到 10。</p><p id="c0c4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设我们为一家公司工作，该公司根据客户的已知评级向他们推荐笑话。对于评价了 25 个笑话的新客户 Joan，我们希望能够知道她喜欢剩下的 75 个笑话，并向她推荐预测评价最高的笑话。</p><p id="4a48" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要做到这一点，考虑一下我们已知的对 100 个笑话的评分的用户。他们代表了不同的品味。我们可以将 Joan 的评级看作这些客户评级的加权和。然后，我们将使用这些<em class="mw"> m </em>客户的 25 个笑话评级作为特征，并将 Joan 的已知评级作为目标来训练回归变量。它应该能够推广到琼还没有预测好的其他笑话，以便我们可以推荐预测得分最高的笑话。琼的评分在这里可以找到。</p><p id="4c9b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">加载数据，并使用以下块将其分为训练集和测试集。请注意，琼的未知评级表示为-99。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="df48" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当 m = 20 时，为了简单起见，我们将使用前 20 个用户作为我们的代表用户。我们将根据琼对 25 个笑话的评价来计算权重，并将琼的评价作为目标。线性系统是超定的。当<em class="mw"> m </em> = 7200 时，线性系统欠定。使用以下代码准备数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="5924" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用 SVD 的 scikit-learn 型最小二乘估计器实现如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="797b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">计算权重向量，并将其与从正规方程获得的权重向量进行比较:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="1d7b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于<em class="mw"> m </em> = 20 和<em class="mw"> m </em> = 7200，我都得到“真”。你可以自己验证。</p><p id="6e00" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了说明欠定最小二乘法如何提供对训练数据的完美拟合，我们可以在训练集和测试集上可视化预测值和真实目标。在这里有相应的代码<a class="ae lr" href="https://github.com/KunyuHe/SVD-in-Machine-Learning/blob/master/Underdetermined%20Least%20Squares/codes/viz.py" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/3ad38dc5e130b249cbf2a2b5e7dfec6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wW0Sj_nEnvnUGubc-8ilGg.png"/></div></div></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="3ab2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章是何坤宇写的。昆玉目前是芝加哥大学的硕士生。他发现理解统计建模和机器学习技术、将它们应用于真实世界的数据并帮助创建金融服务行业的端到端解决方案是一件有趣的事情。在 LinkedIn 上联系昆玉！🐷</p><div class="nr ns gp gr nt nu"><a href="https://www.linkedin.com/in/kunyuhe/" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd ir gy z fp nz fr fs oa fu fw ip bi translated">昆玉何-即将上任的全球量化策略非周期分析师-美银美林…</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">芝加哥大学正在接受理学硕士计算分析项目培训的数据科学家。对…充满热情</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">www.linkedin.com</p></div></div><div class="od l"><div class="oe l of og oh od oi kp nu"/></div></div></a></div></div></div>    
</body>
</html>