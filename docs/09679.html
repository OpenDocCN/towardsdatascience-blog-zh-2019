<html>
<head>
<title>How To Build A Spam Classifier Using Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用决策树构建垃圾邮件分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-spam-classifier-using-decision-tree-b75d0c7f25e?source=collection_archive---------13-----------------------#2019-12-19">https://towardsdatascience.com/how-to-build-a-spam-classifier-using-decision-tree-b75d0c7f25e?source=collection_archive---------13-----------------------#2019-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9a4a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习:监督学习</h2><div class=""/><div class=""><h2 id="da2c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">越简单的模型越好！</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/512f7d39b8960aa63c35c5dad12f1a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nX7sN0WCWLa-jo7uMpZ4yw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><a class="ae lh" href="https://commons.wikimedia.org/wiki/File:NASA_Mars_Rover.jpg" rel="noopener ugc nofollow" target="_blank">NASA/JPL/Cornell University, Maas Digital LLC</a></figcaption></figure><blockquote class="li"><p id="1be4" class="lj lk it bd ll lm ln lo lp lq lr ls dk translated">"像鸭子一样走路，像鸭子一样游泳，像鸭子一样嘎嘎叫，大概，它是一只鸭子！"</p></blockquote></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="bd9c" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">数据科学投诉</h1><p id="1afd" class="pw-post-body-paragraph ms mt it mu b mv mw kd mx my mz kg na nb nc nd ne nf ng nh ni nj nk nl nm ls im bi translated">在监督学习领域，有大量的分类器，包括逻辑回归(<a class="ae lh" rel="noopener" target="_blank" href="/machine-learning-101-predicting-drug-use-using-logistic-regression-in-r-769be90eb03d"> logit 101 </a>和<a class="ae lh" rel="noopener" target="_blank" href="/machine-learning-102-logistic-regression-with-polynomial-features-98a208688c17"> logit 102 </a>)、LDA、朴素贝叶斯、SVM、<a class="ae lh" rel="noopener" target="_blank" href="/beginners-guide-to-k-nearest-neighbors-in-r-from-zero-to-hero-d92cd4074bdb">、KNN </a>、随机森林、神经网络，每天都有更多的分类器出现！</p><p id="8d0a" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">所有数据科学家都应该问自己的真正问题是:</p><blockquote class="li"><p id="42fd" class="lj lk it bd ll lm ns nt nu nv nw ls dk translated">我们是否选择了正确的方法？</p><p id="fe8c" class="lj lk it bd ll lm ns nt nu nv nw ls dk translated">或者，</p><p id="12dd" class="lj lk it bd ll lm ns nt nu nv nw ls dk translated">我们选的是花式模特吗？</p></blockquote><p id="3995" class="pw-post-body-paragraph ms mt it mu b mv nx kd mx my ny kg na nb nz nd ne nf oa nh ni nj ob nl nm ls im bi translated"><em class="oc">关于 5 个最流行的 ML 分类器的快速比较和分步说明，请参考我的另一篇</em> <a class="ae lh" rel="noopener" target="_blank" href="/classifying-rare-events-using-five-machine-learning-techniques-fab464573233"> <em class="oc">帖子</em> </a> <em class="oc"> : </em></p><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/classifying-rare-events-using-five-machine-learning-techniques-fab464573233"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jd gy z fp ol fr fs om fu fw jc bi translated">脖子上的痛:使用 5 种机器学习方法预测罕见事件</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">哪一种最适合不平衡数据？有什么权衡吗？</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou lb og"/></div></div></a></div><p id="44ca" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">如下面这条推文所示，</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="0a67" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">一个简单的回归就够了，建立 10 层深度学习模型还有什么意义？</p><p id="d4ed" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">让我们面对现实吧。我们，数据科学家，有时可能是以自我为中心的毒枭，更关心炫耀技能，而不听取客户的需求。</p><p id="c4d3" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">无论是面向内部的 DS 角色(例如<strong class="mu jd">人员分析</strong>)还是面向外部的 DS 角色(例如<strong class="mu jd">统计顾问</strong>)，他们都需要提供非技术同事和客户能够立即理解和应用的快速解决方案。请不要将它们与 DS 术语、行话、系数解释或任何其他不必要的麻烦混淆。如果 ML 模型太难而没有用，那绝对没有附加值。</p><p id="0225" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">考虑到这一点，我们在这篇文章中学习如何使用可解释的 ML 分类器<strong class="mu jd">决策树</strong>来构建一个简单的垃圾邮件分类器(UCI 机器学习数据库托管数据集，可以在<a class="ae lh" href="https://archive.ics.uci.edu/ml/datasets/spambase" rel="noopener ugc nofollow" target="_blank">这里</a>访问)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/a35c1b9cdd57aec72772fb419829ef9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*8P66OkPpc-ZIykNN8fYXRQ.jpeg"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><a class="ae lh" href="https://pixabay.com/users/AndyPandy-43058/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=940526" rel="noopener ugc nofollow" target="_blank">AndyPandy</a> from <a class="ae lh" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=940526" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><h1 id="51fe" class="ma mb it bd mc md oy mf mg mh oz mj mk ki pa kj mm kl pb km mo ko pc kp mq mr bi translated">决策图表</h1><p id="ed65" class="pw-post-body-paragraph ms mt it mu b mv mw kd mx my mz kg na nb nc nd ne nf ng nh ni nj nk nl nm ls im bi translated">决策树是一种监督学习方法，它将结果空间分割成 J 个区域 R(1)，R(2)，…，R(J)，并预测每个区域 R 的响应</p><p id="7f19" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">使用<strong class="mu jd">递归二进制分裂</strong>，我们通过四个简单的步骤构建 DT 模型:</p><ol class=""><li id="89b1" class="pd pe it mu b mv nn my no nb pf nf pg nj ph ls pi pj pk pl bi translated">基于变量 X(i)分割区域 R(j)</li><li id="c69f" class="pd pe it mu b mv pm my pn nb po nf pp nj pq ls pi pj pk pl bi translated">设置截止点 s，并将 R(j)分成两个区域，如果</li></ol><ul class=""><li id="f8af" class="pd pe it mu b mv nn my no nb pf nf pg nj ph ls pr pj pk pl bi translated">{X|X(i) <s r=""/></li><li id="9224" class="pd pe it mu b mv pm my pn nb po nf pp nj pq ls pr pj pk pl bi translated">{X|X(i)&gt; s} = R(类别 2)</li></ul><p id="b8fe" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">3.对下一个区域重复前两步</p><p id="e70a" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">4.继续下去，直到我们用完了所有可用的单元，或者每个叶节点中只剩下少量的单元。</p><p id="c8e3" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">用日常俗人的话来说，DT 就是找到空间的最佳分割方式，让空间在每次分割后变得“更纯粹”。有三种方法可以测量空间的纯净度或不纯净度:</p><ol class=""><li id="43d1" class="pd pe it mu b mv nn my no nb pf nf pg nj ph ls pi pj pk pl bi translated"><strong class="mu jd">分类错误率</strong></li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ps"><img src="../Images/3539b328f6d97367f906c41d2574cf3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PiJ_SP-pZ8CIzRn9qRB8Vw.png"/></div></div></figure><p id="38de" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated"><strong class="mu jd"> 2。基尼指数</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/012fd3365b7257a1e4b88991c90d645b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBhBQqadq2m1a7GChWFUaA.png"/></div></div></figure><p id="ccd4" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated"><strong class="mu jd"> 3。熵</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/cb3821417d9939cb73fa68473e3ea2ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_JhBDyXuZr3H1E--0jLiw.png"/></div></div></figure><h2 id="f31b" class="pv mb it bd mc pw px dn mg py pz dp mk nb qa qb mm nf qc qd mo nj qe qf mq iz bi translated"><strong class="ak">如何选择最佳分割？</strong></h2><ol class=""><li id="9c10" class="pd pe it mu b mv mw my mz nb qg nf qh nj qi ls pi pj pk pl bi translated">对于区域 j，计算先前杂质<strong class="mu jd"> I(分割前)</strong>和变量<strong class="mu jd"> I(分割后)</strong>的分割后杂质。</li><li id="9589" class="pd pe it mu b mv pm my pn nb po nf pp nj pq ls pi pj pk pl bi translated">选择导致<strong class="mu jd"> I(分割前)</strong>和<strong class="mu jd"> I(分割后)之间最大减少的变量 v。</strong></li></ol><p id="8873" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">理想情况下，完美的 ML 分类器会一直分裂，直到每个单元都有自己的叶子，也就是 aka。然而，这导致过拟合，这使得它不太适合其他数据集。简单地说，过度拟合意味着我们将 ML 模型与我们正在使用的数据集拟合得太紧，如果我们想推广到其他情况，这就不太实际了。</p><p id="35ea" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">为了解决这个问题，我们需要<strong class="mu jd">修剪</strong>模型，并在算法每次想要进行另一次分割时设置惩罚。</p><p id="ecff" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">修剪减少了总的错误分类错误，同时保持较小的树。它可以表示如下:</p><p id="9d9e" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated"><strong class="mu jd">成本=总误分类误差+ αJ </strong></p><ul class=""><li id="e078" class="pd pe it mu b mv nn my no nb pf nf pg nj ph ls pr pj pk pl bi translated">α:调谐参数</li><li id="9445" class="pd pe it mu b mv pm my pn nb po nf pp nj pq ls pr pj pk pl bi translated">αJ:惩罚项</li></ul><p id="0606" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">顺便提一下，这是构建损失函数的一种非常常见的形式，你可能会在其他场景中看到。</p><h1 id="9900" class="ma mb it bd mc md oy mf mg mh oz mj mk ki pa kj mm kl pb km mo ko pc kp mq mr bi translated">r 实现</h1><h2 id="8731" class="pv mb it bd mc pw px dn mg py pz dp mk nb qa qb mm nf qc qd mo nj qe qf mq iz bi translated">1.r 包、库和加载数据</h2><pre class="ks kt ku kv gt qj qk ql qm aw qn bi"><span id="c276" class="pv mb it qk b gy qo qp l qq qr">library(tidyverse)<br/>library(dplyr)<br/>library(tree)<br/>library(maptree)</span><span id="a071" class="pv mb it qk b gy qs qp l qq qr">spam &lt;- read_table2("spambase.tab", guess_max=2000)<br/>spam &lt;- spam %&gt;%<br/>  mutate(y = factor(y, levels=c(0,1), labels=c("good","spam"))) %&gt;%<br/>  mutate_at(.vars=vars(-y), .funs=scale)</span><span id="551e" class="pv mb it qk b gy qs qp l qq qr">colnames(spam)</span></pre><h2 id="2951" class="pv mb it bd mc pw px dn mg py pz dp mk nb qa qb mm nf qc qd mo nj qe qf mq iz bi translated">2.数据分割:训练和测试</h2><pre class="ks kt ku kv gt qj qk ql qm aw qn bi"><span id="d65c" class="pv mb it qk b gy qo qp l qq qr">#set.seed() for version control<br/>set.seed(1)</span><span id="79b9" class="pv mb it qk b gy qs qp l qq qr">#sample the dataset<br/>test.indices = sample(1:nrow(spam), 1000) </span><span id="e80e" class="pv mb it qk b gy qs qp l qq qr">#create train and test sets<br/>spam.train=spam[-test.indices,]<br/>spam.test=spam[test.indices,]<br/>YTrain = spam.train$y<br/>XTrain = spam.train %&gt;% select(-y)<br/>YTest = spam.test$y<br/>XTest = spam.test %&gt;% select(-y)</span></pre><p id="a856" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">接下来，我们使用 10 重交叉验证。关于简历的 6 步总结，请参考我的另一篇帖子(<a class="ae lh" rel="noopener" target="_blank" href="/beginners-guide-to-k-nearest-neighbors-in-r-from-zero-to-hero-d92cd4074bdb"> <strong class="mu jd"> KNN </strong> </a>)。</p><pre class="ks kt ku kv gt qj qk ql qm aw qn bi"><span id="c5c9" class="pv mb it qk b gy qo qp l qq qr">nfold = 10<br/>set.seed(1)<br/>folds = seq.int(nrow(spam.train)) %&gt;% # sequential observations IDs<br/>  cut(breaks = nfold, labels=FALSE) %&gt;% # sequential fold IDs<br/>  sample</span></pre><p id="366f" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">让我们创建一个函数<strong class="mu jd"> calc_error_rate </strong>来计算分类误差。</p><pre class="ks kt ku kv gt qj qk ql qm aw qn bi"><span id="9fa7" class="pv mb it qk b gy qo qp l qq qr">calc_error_rate &lt;- function(predicted.value, true.value){<br/>  return(mean(true.value!=predicted.value)) <br/>}</span></pre><p id="4899" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">有趣的部分来了:构建一个简单的 DT 模型。</p><pre class="ks kt ku kv gt qj qk ql qm aw qn bi"><span id="6368" class="pv mb it qk b gy qo qp l qq qr"># the number = the row numbers of the spam.train <br/>nobs = nrow(spam.train)</span><span id="e3a2" class="pv mb it qk b gy qs qp l qq qr"># a DT model<br/># please check the official R documents for the parameters<br/>spamtree = tree(y~., data= spam.train,<br/>     na.action = na.pass, <br/>     control = tree.control(nobs, mincut =2, minsize = 5, mindev = 1e-5))<br/>summary(spamtree)</span><span id="6af5" class="pv mb it qk b gy qs qp l qq qr"># plot the original unpruned DT model<br/>draw.tree(prune, nodeinfo=TRUE)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qt"><img src="../Images/f55fddc61134761eb3cdaa446bbf477b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sd-LwnwDaRgWekOhdZc0lQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Figure 1</figcaption></figure><p id="b517" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">通常情况下，我们不绘制未修剪的 DT 模型，因为有太多的叶片难以解释。<strong class="mu jd">这是个坏例子！我这么做是出于教学原因。</strong></p><p id="c3db" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">3.<strong class="mu jd">修剪</strong></p><p id="e87b" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">为了让情节更漂亮，更简单，更容易理解，让我们把 DT 模型剪成只剩下 8 片叶子。</p><pre class="ks kt ku kv gt qj qk ql qm aw qn bi"><span id="7219" class="pv mb it qk b gy qo qp l qq qr">prune =prune.tree(spamtree, best=8)<br/>summary(prune) </span><span id="6b61" class="pv mb it qk b gy qs qp l qq qr"># plot the pruned model<br/>draw.tree(prune, nodeinfo=TRUE)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qu"><img src="../Images/0d6edd00f763a034a3511fc1a915c1a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dqJtzHJu0vt-ArUcXX_hGw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Figure 2</figcaption></figure><p id="800d" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">与之前的填充图相比，图 2 在各方面都要好得多。我们知道最重要的变量是什么。我们知道每个类别中有多少观察值。</p><p id="0a3d" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">此外，我们可以使用交叉验证来寻找最佳的修剪次数。幸运的是，<em class="oc">树</em>包包括一个默认的 CV 函数，<strong class="mu jd"> cv.tree </strong>，以最小化错误分类率。</p><pre class="ks kt ku kv gt qj qk ql qm aw qn bi"><span id="2157" class="pv mb it qk b gy qo qp l qq qr">set.seed(3)<br/>cv = cv.tree(spamtree,FUN=prune.misclass, K=10)<br/>cv</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qv"><img src="../Images/dc0e9b6a4f0213f60c28edaa54382ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TB9tZzIYlcHoms4lVNWA2Q.png"/></div></div></figure><pre class="ks kt ku kv gt qj qk ql qm aw qn bi"><span id="d8ff" class="pv mb it qk b gy qo qp l qq qr">plot(cv$size,cv$dev)<br/>abline(v=37,lty=1)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qw"><img src="../Images/65fcab1706508cafd03ff36532135207.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OghHoAiS3L_OPBze82t7Sw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Figure 3</figcaption></figure><p id="e4ce" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">最佳分裂数为 37。我们根据树的大小绘制错误分类图。设置叶子的数量= 37，我们建立一个新的树模型叫做<strong class="mu jd"> spamtree.pruned </strong>。</p><pre class="ks kt ku kv gt qj qk ql qm aw qn bi"><span id="7bb7" class="pv mb it qk b gy qo qp l qq qr">spamtree.pruned&lt;-prune.misclass(spamtree, best=37)<br/>summary(spamtree.pruned)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qx"><img src="../Images/644efd8696f98e953354803122047205.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4IwhAe5xiFWgEUmGdoge9Q.png"/></div></div></figure><pre class="ks kt ku kv gt qj qk ql qm aw qn bi"><span id="b9c0" class="pv mb it qk b gy qo qp l qq qr"># training and test errors of spamtree.pruned<br/># <strong class="qk jd">set type = "class"</strong> because we are predicting the category</span><span id="0e1b" class="pv mb it qk b gy qs qp l qq qr">pred.train = predict(spamtree.pruned, spam.train, <strong class="qk jd">type=”class”</strong>)<br/>pred.test = predict(spamtree.pruned, spam.test, <strong class="qk jd">type=”class”</strong>)</span><span id="c6f8" class="pv mb it qk b gy qs qp l qq qr"># training error<br/>DT_training_error &lt;- calc_error_rate(predicted.value=pred.train, true.value=YTrain)<br/>DT_training_error</span><span id="701b" class="pv mb it qk b gy qs qp l qq qr">[1] 0.05165232</span><span id="5975" class="pv mb it qk b gy qs qp l qq qr"># test error<br/>DT_test_error &lt;- calc_error_rate(predicted.value=pred.test, true.value=YTest)<br/>DT_test_error</span><span id="65a8" class="pv mb it qk b gy qs qp l qq qr">[1] 0.072</span></pre><p id="b9a5" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">仅此而已！伙计们，我们已经通过 3 个简单的步骤学会了一个超级简单但有用的 ML 分类器。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="001f" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">在这篇文章中，我们已经了解了什么是 DT，它的优点和 R 实现。以下是一些关键要点:</p><ul class=""><li id="67a6" class="pd pe it mu b mv nn my no nb pf nf pg nj ph ls pr pj pk pl bi translated">行业需要快速简单的解决方案。</li><li id="df5d" class="pd pe it mu b mv pm my pn nb po nf pp nj pq ls pr pj pk pl bi translated">决策树快速、简单且有用。</li><li id="4e7c" class="pd pe it mu b mv pm my pn nb po nf pp nj pq ls pr pj pk pl bi translated">DT 是可视化的，可解释的。</li></ul></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><blockquote class="qy qz ra"><p id="3e70" class="ms mt oc mu b mv nn kd mx my no kg na rb np nd ne rc nq nh ni rd nr nl nm ls im bi translated">喜欢读这本书吗？</p><p id="9e2e" class="ms mt oc mu b mv nn kd mx my no kg na rb np nd ne rc nq nh ni rd nr nl nm ls im bi translated">如果有，请查看我其他关于机器学习和编程的帖子。</p></blockquote><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/beginners-guide-to-k-nearest-neighbors-in-r-from-zero-to-hero-d92cd4074bdb"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jd gy z fp ol fr fs om fu fw jc bi translated">R 中 K-最近邻初学者指南:从零到英雄</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">使用各种度量标准在 R 中构建 KNN 模型的管道</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="re l or os ot op ou lb og"/></div></div></a></div><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/machine-learning-101-predicting-drug-use-using-logistic-regression-in-r-769be90eb03d"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jd gy z fp ol fr fs om fu fw jc bi translated">机器学习 101:使用逻辑回归预测 R</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">基础、链接功能和图</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="rf l or os ot op ou lb og"/></div></div></a></div><p id="4330" class="pw-post-body-paragraph ms mt it mu b mv nn kd mx my no kg na nb np nd ne nf nq nh ni nj nr nl nm ls im bi translated">在<a class="ae lh" href="https://www.linkedin.com/in/leihuaye/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://twitter.com/leihua_ye" rel="noopener ugc nofollow" target="_blank"> Twitter </a>找到我。</p></div></div>    
</body>
</html>