<html>
<head>
<title>An Evolution in Single Image Super Resolution using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度学习的单幅图像超分辨率进化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-evolution-in-single-image-super-resolution-using-deep-learning-66f0adfb2d6b?source=collection_archive---------9-----------------------#2019-12-03">https://towardsdatascience.com/an-evolution-in-single-image-super-resolution-using-deep-learning-66f0adfb2d6b?source=collection_archive---------9-----------------------#2019-12-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9215" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从经典插值到具有生成对抗网络的深度学习方法</h2></div><p id="c0ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在计算机视觉领域中，从对应的低分辨率图像重建高分辨率照片级真实感图像一直是一项长期的挑战性任务。当只有一张低分辨率图像作为输入来重建其高分辨率图像时，这项任务变得更加困难。</p><p id="9fe4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">什么是超分辨率？对来自<strong class="kk iu">的一幅<strong class="kk iu">高分辨率(HR) </strong>图像和一幅</strong>低分辨率(LR)图像的估计被称为<strong class="kk iu">超分辨率(SR) </strong>。换句话说，LR 是一个<strong class="kk iu">单幅图像</strong>输入，HR 是地面真实，SR 是预测的高分辨率图像。当应用 ML/DL 解决方案时，LR 图像通常是添加了一些模糊和噪声的下采样 HR 图像。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/2d0c056c270154f631853ecb7f73030b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4tVNlats95iVJUbpQvHeaQ.jpeg"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Photo by <a class="ae lu" href="https://unsplash.com/@robinmathlener?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Robin Mathlener</a> on <a class="ae lu" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="2ed2" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">进化的第 0 代:插值</h1><p id="ff69" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">首先，非常早期的解决方案是图像处理中的插值方法。这里，低分辨率图像使用一些插值方法，如最近邻法、双线性或双三次插值法，以 2 倍或 4 倍的因子调整大小。</p><blockquote class="mz na nb"><p id="5cb4" class="ki kj nc kk b kl km ju kn ko kp jx kq nd ks kt ku ne kw kx ky nf la lb lc ld im bi translated">“插值的工作原理是使用已知数据来估计未知点的值。图像插值在两个方向上起作用，并试图根据周围像素的值获得像素强度的最佳近似值。”— <a class="ae lu" href="https://sisu.ut.ee/imageprocessing/book/3" rel="noopener ugc nofollow" target="_blank">塔尔图大学的数字图像处理电子书</a>。</p></blockquote><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/22c8ce5bc564ed14cf2c596b779064ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*XSu3LkO1usO3Jwe-wfwl3w.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Figure: Effect of interpolation (<a class="ae lu" href="https://www.youtube.com/watch?v=lmUxbRY7H2I" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="512a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的插图可以清楚地看出，合成的图像是模糊的，不真实的。</p><h1 id="f871" class="mc md it bd me mf nh mh mi mj ni ml mm jz nj ka mo kc nk kd mq kf nl kg ms mt bi translated">进化的第一代:SRCNN</h1><p id="ab01" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">随着全卷积神经网络(FCNN)在解决语义分割方面的成功，它在计算机视觉的其他领域迅速普及。FCNN 是一个后面没有任何密集连接(全连接层)的 CNN。每个 CNN 有两个主要功能块，I)特征提取器和 ii)分类器。CNN 后面的密集连接是分类器，其任务是将提取的特征映射到类别概率。我认为 FCNN 是 DL 中从输入图像生成/预测输出图的基本设计实践。输出图可以是语义分割图、风格转移图甚至超分辨率图。换句话说，FCNN 是一个图像到图像的映射引擎。FCNN 在超分辨率中的一个这样的初步应用是<a class="ae lu" href="https://arxiv.org/abs/1501.00092" rel="noopener ugc nofollow" target="_blank"> SRCNN </a>。</p><p id="83c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 SRCNN 中，首先使用双三次插值对图像进行上采样，然后馈送到简单的 FCNN。需要注意的是，这里不涉及池操作。因此，产生与上采样输入图像相同空间大小的输出。最后，我们计算目标 HR 图像与输出之间的 MSE 损失。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/4409e1ff3c7983d1a8a1f3ae93a8eb3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/0*kMPkTap02DWj2_jy"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Figure: SRCNN Model (<a class="ae lu" href="https://www.youtube.com/watch?v=lmUxbRY7H2I" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h1 id="6cb9" class="mc md it bd me mf nh mh mi mj ni ml mm jz nj ka mo kc nk kd mq kf nl kg ms mt bi translated">进化的第二代:SRResNet 和用于上采样的子像素卷积</h1><p id="b944" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">由于使用 SRCNN 在单幅图像的超分辨率方面取得了一些成功，激发了其他人对该架构进行进一步的改进。众所周知，<a class="ae lu" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet </a>(带跳跃连接的 CNN)比传统 CNN 更好。<a class="ae lu" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank"> SRResNets </a>用残差块代替简单卷积块。结果，精确度显著提高。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/1cc3405e571fa488b78151d93ed08cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/0*7MQt50mjhQuqPpby"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Figure: SRResNet Model (<a class="ae lu" href="https://www.youtube.com/watch?v=lmUxbRY7H2I" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="8dd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">许多深度学习模型还结合了转置卷积进行上采样。双线性和双三次上采样是不可学习的，这意味着，它只能在深度学习架构之前或之后使用，而不能在两者之间使用。可学习的上采样的其他优点是它的速度和准确性。</p><p id="ec59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是正如人们可能已经观察到的那样，上面用步进卷积梯度实现的上采样操作增加了零值来放大图像，这必须在以后用有意义的值来填充。更糟糕的是，这些零值没有梯度信息可以反向传播。</p><blockquote class="mz na nb"><p id="ef01" class="ki kj nc kk b kl km ju kn ko kp jx kq nd ks kt ku ne kw kx ky nf la lb lc ld im bi translated"><a class="ae lu" href="https://arxiv.org/abs/1609.05158" rel="noopener ugc nofollow" target="_blank">“应付那个问题，石等人。al </a>提出了我们认为是最近最有用的 convnet 技巧之一(至少在我作为一个生成模型研究者看来是这样！)他们提出了一个用于升级的子像素卷积神经网络层。这一层基本上使用常规卷积层，然后是一种称为相移的特定类型的图像整形。换句话说，他们在较低分辨率下计算更多的卷积，并将结果图的大小调整为放大的图像，而不是在像素之间放置零并进行额外的计算。这样就不需要无意义的零了。”—[<a class="ae lu" href="https://github.com/atriumlts/subpixel" rel="noopener ugc nofollow" target="_blank">https://github.com/atriumlts/subpixel</a></p></blockquote><p id="cd95" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用相移进行图像整形也叫“<strong class="kk iu">像素混洗</strong>”，将 H × W × C r 张量的元素重新排列，形成 rH × rW × C 张量，如下图所示。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi no"><img src="../Images/9c9e79c3fbf391ca14e7e6fdd905127f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-82Ps97CPgtUVMua"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Figure: Sub-pixel convolution operation (<a class="ae lu" href="https://arxiv.org/abs/1609.05158" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h1 id="17c7" class="mc md it bd me mf nh mh mi mj ni ml mm jz nj ka mo kc nk kd mq kf nl kg ms mt bi translated">进化的第三代:知觉丧失</h1><p id="7f1e" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">在像超分辨率这样的应用中使用 MSE 或 MSE 类型的误差方法作为损失函数的主要缺点是它是按像素计算的。也就是说，它仅测量预测图像和目标图像中两个对应像素之间的变化。这鼓励寻找似是而非的解决方案的像素平均值，这些解决方案通常过于平滑，因此具有较差的感知质量。这个论点也适用于不仅仅使用 PSNR 作为质量指数，因为它也是按像素计算的。因此，我建议在比较这类任务中任何两种方法的性能时，不要只检查 PSNR。</p><p id="d0f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lu" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">感知损失</a>通过基于来自预训练 CNN 模型的高级表示比较两幅图像来计算。该函数用于比较图像之间的高级差异，如内容和风格差异。</p><p id="8fa9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，目标和预测输入都通过预先训练的网络，并计算两个结果特征图之间的欧几里德距离(在同一阶段)。感知损失函数的工作原理是将所有像素之间的所有平方误差相加并取平均值。这与每像素损失函数相反，每像素损失函数将像素之间的所有绝对误差相加。</p><h1 id="8e09" class="mc md it bd me mf nh mh mi mj ni ml mm jz nj ka mo kc nk kd mq kf nl kg ms mt bi translated">进化的第四代:SRGAN</h1><p id="8a55" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">生成对抗网络(GANs)提供了一个强大的框架来生成具有高感知质量的看似真实的自然图像。GAN 过程鼓励重建向搜索空间中包含照片级逼真图像的概率较高的区域移动，从而更接近自然图像流形。</p><p id="dd22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lu" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank"> SRGAN </a>是基于 GAN 的网络，其中生成器(G)学习从尽可能接近 HR 的 LR 图像生成 SR 图像。鉴别器(D)学习区分生成的 SR 图像和真实图像。G 利用 ResNet 和子像素卷积进行上采样。它还将感性损失与生成性或对抗性损失结合起来计算其损失。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi np"><img src="../Images/946072c892daf245ddd10829255e91e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fMB6MEKKRJayrK58"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Figure: Architecture of Generator and Discriminator Network in SRGAN. (<a class="ae lu" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="c5e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">损失</strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nq"><img src="../Images/fb680d35d5bf9d4ae60321fc48a27476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pj5P-iTobQcJMDg4"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Equation of modified perceptual loss in SRGAN. (<a class="ae lu" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h1 id="1934" class="mc md it bd me mf nh mh mi mj ni ml mm jz nj ka mo kc nk kd mq kf nl kg ms mt bi translated">结论</h1><p id="081f" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">研究使用深度学习估计单幅图像超分辨率的发展，显然，基于 ResNet 的 GAN 结合了感知损失和生成损失，并应用亚像素卷积进行上采样，可以生成更好的照片级逼真超分辨率图像。</p><h1 id="ff4e" class="mc md it bd me mf nh mh mi mj ni ml mm jz nj ka mo kc nk kd mq kf nl kg ms mt bi translated">显著的信誉</h1><p id="8646" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">我要感谢<a class="ae lu" href="https://pydata.org/warsaw2018/speaker/profile/115/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> Katarzyna Kańska </strong> </a>关于<em class="nc">“单图像超分辨率”</em> : Youtube 视频— <a class="ae lu" href="https://www.youtube.com/watch?v=lmUxbRY7H2I" rel="noopener ugc nofollow" target="_blank">你能增强一下吗？单幅图像超分辨率——Katarzyna kańska</a>。</p></div></div>    
</body>
</html>