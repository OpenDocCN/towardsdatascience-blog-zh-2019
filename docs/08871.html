<html>
<head>
<title>Building Your First Machine Learning Model: Linear Regression Estimator</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建您的第一个机器学习模型:线性回归估计器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-your-first-machine-learning-model-linear-regression-estimator-ba86450c4d24?source=collection_archive---------28-----------------------#2019-11-26">https://towardsdatascience.com/building-your-first-machine-learning-model-linear-regression-estimator-ba86450c4d24?source=collection_archive---------28-----------------------#2019-11-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/032e2f75765a8364af453850e9817515.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieQ8Nory3036kHv33nWuFw.png"/></div></div></figure><p id="a2bc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi kz translated">有这么多好的软件包可以用来构建预测模型。一些最常见的预测分析包包括</p><ul class=""><li id="9b11" class="li lj it kd b ke kf ki kj km lk kq ll ku lm ky ln lo lp lq bi translated">Sci-kit 学习包</li><li id="6380" class="li lj it kd b ke lr ki ls km lt kq lu ku lv ky ln lo lp lq bi translated">插入符号包</li><li id="c53f" class="li lj it kd b ke lr ki ls km lt kq lu ku lv ky ln lo lp lq bi translated">张量流</li></ul><p id="eb4a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">重要的是，在使用这些包之前，你要掌握预测建模的基础，这样你就不会把这些包简单地当作黑盒工具来使用。</p><p id="0fca" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">理解机器学习模型功能的一种方法是让你实际学习如何建立自己的模型。最简单的机器学习模型是线性回归模型。每个刚接触数据科学的人都应该掌握线性回归估计器的基础知识，因为大多数机器学习模型(如 SVM、KNN、逻辑回归等)都是线性回归估计器。)非常类似于线性回归估计量。</p><p id="0274" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在本文中，我们将描述如何构建一个简单的 python 估算器，以使用梯度下降法执行线性回归。假设我们有一个包含单个要素(X)和结果(y)的一维数据集，并假设数据集中有 N 个观测值:</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/c283a106611be4b849d9a0999304df36.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*jirks1Ium0qi7cHbAnO-Yw.png"/></div></figure><p id="a4b5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">拟合数据的线性模型如下所示:</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/3443b4dbe7c57519f77ffcc7642cc652.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/0*nw4f3ioltxI5EqMV.png"/></div></figure><p id="2983" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中 w0 和 w1 是算法在训练期间学习的权重。</p><h1 id="6f4f" class="mc md it bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">梯度下降算法</h1><p id="21fa" class="pw-post-body-paragraph kb kc it kd b ke na kg kh ki nb kk kl km nc ko kp kq nd ks kt ku ne kw kx ky im bi translated">如果我们假设模型中的误差是独立的且呈正态分布，则似然函数如下所示:</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/75b3a75f783b77884c2033a77b7f4662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/0*qx9oResBcReSBzWn.png"/></div></figure><p id="3857" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了最大化似然函数，我们最小化 w0 和 w1 的误差平方和(SSE ):</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/6d76d31cb6edca1eb4ca67e53f07ab09.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/0*FoJE2cfcp2OAgarJ.png"/></div></figure><p id="53bd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">目标函数或我们的 SSE 函数通常使用梯度下降(GD)算法来最小化。在 GD 方法中，权重根据以下程序更新:</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a75e479f8e9298f991ba7c02bead3c22.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/0*GEH5tvlMgrfSZdGl.png"/></div></figure><p id="028e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">即在与梯度相反的方向上。这里，eta 是一个小的正常数，称为学习率。该等式可以用分量形式写成:</p><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c239424ad005b07fce2222eaa4a54e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/0*SzsQropvdCj-y6Pv.png"/></div></figure><p id="231e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你想了解更多关于 GD 算法及其工作原理的信息，请参见以下文章:<a class="ae nj" href="https://medium.com/towards-artificial-intelligence/machine-learning-how-the-gradient-descent-algorithm-works-61682d8570b6" rel="noopener"> <strong class="kd iu">机器学习:梯度下降算法如何工作</strong> </a>。</p><h1 id="ceb2" class="mc md it bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">使用 Python Estimator 实现</h1><pre class="lx ly lz ma gt nk nl nm nn aw no bi"><span id="ec04" class="np md it nl b gy nq nr l ns nt">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="4ee3" class="np md it nl b gy nu nr l ns nt">class GradientDescent(object):<br/>    """Gradient descent optimizer.<br/>    Parameters<br/>    ------------<br/>    eta : float<br/>        Learning rate (between 0.0 and 1.0)<br/>    n_iter : int<br/>        Passes over the training dataset.<br/>        <br/>    Attributes<br/>    -----------<br/>    w_ : 1d-array<br/>        Weights after fitting.<br/>    errors_ : list<br/>        Error in every epoch.<br/>    """    def __init__(self, eta=0.01, n_iter=10):<br/>        self.eta = eta<br/>        self.n_iter = n_iter<br/>        <br/>    def fit(self, X, y):<br/>        """Fit the data.<br/>        <br/>        Parameters<br/>        ----------<br/>        X : {array-like}, shape = [n_points]<br/>        Independent variable or predictor.<br/>        y : array-like, shape = [n_points]<br/>        Outcome of prediction.<br/>        Returns<br/>        -------<br/>        self : object<br/>        """<br/>        self.w_ = np.zeros(2)<br/>        self.errors_ = []<br/>        <br/>        for i in range(self.n_iter):<br/>            errors = 0<br/>            for j in range(X.shape[0]):<br/>                self.w_[1:] += self.eta*X[j]*(y[j] - self.w_[0] -                     self.w_[1]*X[j])<br/>                self.w_[0] += self.eta*(y[j] - self.w_[0] - self.w_[1]*X[j])<br/>                errors += 0.5*(y[j] - self.w_[0] - self.w_[1]*X[j])**2<br/>            self.errors_.append(errors)<br/>        return self    def predict(self, X):<br/>        """Return predicted y values"""<br/>        return self.w_[0] + self.w_[1]*X</span></pre><h1 id="eb93" class="mc md it bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">Python 估计器的应用</h1><p id="4d57" class="pw-post-body-paragraph kb kc it kd b ke na kg kh ki nb kk kl km nc ko kp kq nd ks kt ku ne kw kx ky im bi translated"><strong class="kd iu"> a)创建数据集</strong></p><pre class="lx ly lz ma gt nk nl nm nn aw no bi"><span id="7014" class="np md it nl b gy nq nr l ns nt">np.random.seed(1)<br/>X=np.linspace(0,1,10)<br/>y = 2*X + 1<br/>y = y + np.random.normal(0,0.05,X.shape[0])</span></pre><p id="356a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> b)拟合和预测</strong></p><pre class="lx ly lz ma gt nk nl nm nn aw no bi"><span id="6323" class="np md it nl b gy nq nr l ns nt">gda = GradientDescent(eta=0.1, n_iter=100)<br/>gda.fit(X,y)<br/>y_hat=gda.predict(X)</span></pre><p id="5759" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> c)绘图输出</strong></p><pre class="lx ly lz ma gt nk nl nm nn aw no bi"><span id="8e27" class="np md it nl b gy nq nr l ns nt">plt.figure()<br/>plt.scatter(X,y, marker='x',c='r',alpha=0.5,label='data')<br/>plt.plot(X,y_hat, marker='s',c='b',alpha=0.5,label='fit')<br/>plt.xlabel('x')<br/>plt.ylabel('y')<br/>plt.legend()</span></pre><figure class="lx ly lz ma gt ju gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e0e82ed696f1f66c9de268a7bc4cc47e.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*Prff-fidzSB5o89X.png"/></div></figure><p id="496c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> d)计算 R 平方值</strong></p><pre class="lx ly lz ma gt nk nl nm nn aw no bi"><span id="5aff" class="np md it nl b gy nq nr l ns nt">R_sq = 1-((y_hat - y)**2).sum()/((y-np.mean(y))**2).sum()<br/>R_sq<br/>0.991281901588877</span></pre><p id="7159" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">总之，我们已经展示了如何使用 GD 算法在 Python 中构建和实现一个简单的线性回归估计器。如果你想看看 GD 算法在一个真实的机器学习分类算法中是如何使用的，请看下面的<a class="ae nj" href="https://github.com/bot13956/LogisticRegression_gradient_descent" rel="noopener ugc nofollow" target="_blank"> Github 知识库</a>。</p></div></div>    
</body>
</html>