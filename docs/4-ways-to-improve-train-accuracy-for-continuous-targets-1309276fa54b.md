# 提高连续目标训练精度的 4 种方法

> 原文：<https://towardsdatascience.com/4-ways-to-improve-train-accuracy-for-continuous-targets-1309276fa54b?source=collection_archive---------11----------------------->

## 快速、简单、有效。

![](img/75817e84e44d6f990986f25501bca60f.png)

> 连续目标…

我们都认识他们，我们都和他们一起工作。连续特征可以表示价格、GDP 以及任何量化的东西。连续目标是累加的目标，或者通常由于另一个特征而在值上增长和收缩的目标。一个很好的例子就是货币。尽管这可能很可笑，但大多数商业分析都围绕着钱，(很疯狂，对吗？)而且大多数(如果不是全部的话)这些货币目标都是连续的。

显然，在进行连续预测时，准确性非常重要，因为资金对企业很重要，国内生产总值对国家很重要，收入和支出需要加以利用，以确保稳定的运营。几乎每个企业都利用某种形式的分析，无论是数据分析师(DA)，数据科学家(DS)，甚至是数据工程师(DE)。

因此，如果你要解决连续的问题，有一些非常简单易行的方法来提高你的准确性，而且非常有效。

[(笔记本)](https://github.com/emmettgb/Emmetts-DS-NoteBooks/blob/master/Julia/Improve%20Continuous%20Accuracy.ipynb)

# 确认

为了训练一个准确的模型，我们需要知道如何验证它。通常，对于连续特征，我们使用三个家庭指标:

## 平均绝对误差

平均绝对误差是两个阵列之间的差的**总和，理想情况下是 y 和 y hat。这可以通过简单地用点减去两个数组，然后求差的平均值来实现。**

![](img/73465a778defc86be8155d72fab17ab9.png)

对于这个指标，重要的是要记住越低越好。分数越低，我们预测的 y 和我们得到的ŷ.之间的差异就越小为了在 Julia 中做到这一点(如图所示)，我们只需使用。-运算符，然后得到它的总和，对于这样的公式:

```
m = Σ(y - ŷ)
```

## 均方误差

均方误差是两个阵列之间的平方差的总和**，与我们之前的公式一样简单，在 sigma 之前对 y 和ŷ之间的结果差求平方。**

![](img/8a3e905934e42a8eee3d5b0b3b2e2b90.png)

和 MAE 一样，MSE 是连续的，越小越好。这个数字离零越远，我们的模型就越不精确。这里的一个区别是，这个数字(自然)会更大。

## 回归的相关系数

虽然另一个验证很简单，计算起来也很容易，但 r 却不是。r 分数以百分比表示，计算方法是先计算相关系数(r ),然后求平方。为了计算回归的相关系数，我们必须首先计算相关系数。

![](img/728020089fa2dad64a4dfbdc1850df5c.png)

我们已经了解了如何计算 MAE 和 MSE，但这是计算 r 的方法，虽然看起来有点令人畏惧，但一旦我们将其分解，就非常简单了。我们需要“插入”到这个方程中的主要部分是 n，σx，σy，σxy，σx，σy .为了计算这些，就像前面一样；我们需要使用点运算符。当然这是特定于 Julia 的，但对于其他统计语言来说没有太大的不同。在 Python 中，使用 Numpy，在 R 中你实际上可以只使用*操作符(它有一个多态属性，可以应用于偶数数组，以及单个整数和浮点数。)所以当然是在 sx2，sx，sy 等里面的 s 上面。是 sigma 或σ的缩写，我们当然可以在大多数高级语言中用 base sum()方法得到它。那么就像把这些值代入公式一样简单，

![](img/b53515a82463a7538c5adf65ee280b17.png)

应该注意的是，机器学习中使用的相关系数通常使用皮尔逊相关方法来计算 r。我还想说明 r 对于非回归问题来说不是一个好的指标。这包括带有加权系数和决策加法的公式(不是典型的基于公式的回归，如岭回归、线性回归和 LLSQ 回归)。)通常，对于加权最小二乘法和普通最小二乘法，您可能应该使用均方误差或平均绝对误差。

现在我们有了 r 的 r 部分，要得到 r，我们只需要求 r 的平方，我想我们会写一个函数来做这件事，尽管手工做起来并不困难。

![](img/dffa4806b186407fee20e3801a6e3a49.png)

轻松点。现在我们有了第三个也是最后一个指标，可以理解模型的准确性，并得到某种分数。

# 基线

基线精度是一个经常被低估的重要性，通常基线的优势在于它可以告诉您是否有必要使用模型来预测您的要素。这不是限制性的，因为这是对其他优势的补充，因为基线通常是一个很好的起点。为了获得基线，我们可以使用 Lathe.models.meanBaseline 构造函数，或者编写我们自己的函数。根据本文到目前为止的进展，您可以推测

> 我们要做的是后者。

但首先，让我们使用车床，这样你就可以在我们从头开始之前知道我们要做什么。

![](img/641e21af069b6ebe00832703db4a8fa0.png)

当然，如果需要，我们可以使用 Julia 的帮助功能(？())给我们模型的信息。

![](img/515603710d743f62e5dd3037ea4758b8.png)

至此，我们得到了一个详细的解释，并举例说明了如何准确地使用该模型，所以让我们快速地补充一些数据:

```
x = [7,6,7,5,6,7,8,8,4]
y = [3,5,6,4,6,8,9,0,6]
xtest = [6,4,5,8,6,5,5,7,8,3,5,7,8,]
```

现在，我们将它插入到构造函数中，并使用我们的 predict()函数。注意，我不会“使用”它，我将显式地导入它。这样我们就可以用多态来修改车床函数，以便利用我们自己的函数。

![](img/0119f0f170f990672298557aa65b94c4.png)

现在我们有了基线预测，我们可以使用之前制定的指标。我们将用平均绝对误差作为基线。

![](img/f44ab94f92c485d56088b1ee692007d4.png)

平均绝对误差约为 7，但重要的是要考虑到数据的平均值为 5，因此 7 实际上是一个非常可怕的精度。当然，这是由数据组成的，所以让我们从网上获取一个数据集，在这个例子中使用，这样我们实际上可以提高准确性。在快速浏览了 Kaggle 之后，我想出了这个[数据集](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data)。这个例子应该很适合我们。所以让我们读给朱莉娅听:

![](img/8e5d4f8b49282bc84b4508faf4937f8d.png)

我们必须做的是:

> 显示(df，allcols = true)

在对我们由此得到的(糟糕的)数据框视图进行了一番研究之后，我得出了这样一个数据框:

![](img/1e061fc4171055acdc1c389057bd702a.png)

现在让我们训练测试拆分数据，但是我们能用什么呢？车床中可能有一个功能，所以让我们检查一下…

![](img/431fd648881c416efd6a54200c8b9f41.png)

我们可以假设测试序列分割在预处理模块中，因为这一次其他的不一定符合要求。

![](img/9cb585b5b08cf854e620f9c95c641e27.png)

我们将完全按照它说的去做，并且使用？()来访问该模块的信息。这样做可以显示模块树，包括每个函数的单独部分。

当然，我们来这里是为了寻找 lathe . preprocess . train test split，所以我们现在可以使用？()功能在上面:

```
?(Lathe.preprocess.TrainTestSplit)
```

这将返回我们完成测试序列分割所需的所有细节，包括超参数和语法:

![](img/4f96ff7e127b860d8d198c8f35fbfbde.png)![](img/27bd37599b7e2bcf9d75d3d4c828db4b.png)

轻松 ML 是有原因的座右铭！现在，我们可以使用 Julia 符号来区分我们的单一特征和目标:

```
feature = :no_reviews
target = :Price
xtrain = train[feature]
ytrain = train[target]
xtest = test[feature]
ytest = test[target]
```

现在让我们像承诺的那样扩展预测函数，以便使用我们以前的可变结构为新的平均基线函数编写我们自己的 Lathe.models.predict()。首先，我们将把我们的三角形插入我们的新模型:

![](img/99eb2072429c88923e164a8ede04a6c4.png)

现在让我们写一个函数，本质上我们需要做的就是为数组中的每个元素添加一个替换数(平均值)。我们可以通过使用 Lathe.stats.mean()和一个简单的一行 for 循环快速而简洁地完成这项工作:

![](img/916bd604f205560b617a7628022a5a10.png)

现在我们把它作为可变结构的一个属性来扩展:

```
predict(m::Lathe.models.meanBaseline,x) = meanbaseline(m,x)
```

![](img/1cda456b0c0edd421e2d8babe157e348.png)

> 现在我们可以用前面的 MAE 函数来验证它:

![](img/7d87feb387a862ee79a60803f5d358fe.png)

这是一个相当糟糕的准确度分数！虽然我没有对特性选择进行任何测试，但我们肯定可以通过首先实现一个模型，然后投资一些预处理来进一步开发这个特性。对于我们的模型，我选择了 FourSquare 模型。FourSquare 模型基于四分位数使用不同的回归模型进行预测，类似于回归树——但它不使用相关性，而是使用数据段。这使得它非常适合时间序列数据和分段连续数据。但是现在我们的功能出现了问题:

![](img/85eb4aa31bc8a8dad280756b2b6ffbfd.png)

我们得到了比我们上一个模型更差的准确性，表明这个特征可能不是预测这个连续目标的最佳选择，因为评论的数量可能与目标没有很好的相关性。当然，这是我们的无效假设，即这些特征之间没有相关性。与普遍的看法相反，我们经常可以看到这一点，而从来没有使用统计测试来检查。因此，让我们通过执行统计测试并可视化我们的数据和模型来进一步验证零假设(耶！)我认为这也说明了基线精度为何如此重要，以及 segways 如何真正成为我们的第一个精度助推器:

# 1.改变特征

丢弃统计上不重要的特征，或者在我们的情况下(单一特征预测)一起改变我们的特征，是我们可以用来提高准确性的最有效的方法之一。在这种情况下，老话真的是正确的:

> 坏数据输入，坏结果输出。

![](img/c68a77a08d0d684e833a761519adbf78.png)

The original data scatterplot

仅从这个可视化，很容易看到，在大多数情况下，这些功能是扁平线在一起。这意味着这种情况下的特性对我们的目标并没有太大的影响。下面是我用来创建这个图形的代码:

```
scatter(xtest,ytest,
# title:
    title = "Original airBnB Data",
    # X label:
    xlabel = "Number of Reviews",
    # Y label
    ylabel = "Price",
    # Grid:
    grid = false,
    # Legend position
    legend=:bottomright,
    # Color of the marker
    color = :lightblue,
    # Marker stroke width and opacity
    markerstrokewidth = 4,
    markerstrokealpha = .75,
    # Marker stroke color
    markerstrokecolor = :lightblue,
    # Adjust out y ticks
    yticks = [500,1500,3500],
    xticks = [200,400,600],
    label = "AirBnB's",
    # Our font options
    fontfamily = :Courier,xtickfontsize=7,ytickfontsize=9,
    ytickfont=:Courier,xtickfont = :Courier,titlefontsize=13,)
```

> 现在让我们看看我们的模型做了什么…

![](img/caf803d13669e959daedb116e0dd36bc.png)

Our model — visualized

幸运的是，有意地用这种方式编写代码，很容易回过头来改变特性。我决定将我们的功能更改为今年的可用性，因为较低的数字似乎相当于较低的价格。当然，我们将使用测试和更多的可视化(耶！)用实际支持来巩固这个假设(数据科学的科学部分。)

让我们回到过去，用这个新特性来重建我们的数据帧:

```
df = DataFrame(:Price => df[:price],:Availability => df[:availability_365])
```

有了这个新功能，又出现了一个小问题，这是很常见的:Nans。因此，为了去掉 nan，我们将使用 DataFrames 中的 dropmissing()函数。

```
df = dropmissing!(df)
```

如果我们想替换这些，我们可以很容易地通过使用缺少的包:

```
using Missing
xbar = mean(df.feature)
df.feature = Missings.replace(df.feature, xbar)
```

然后，我们将替换定义功能和目标的符号:

![](img/6a48a1b5bd18fd8fd7a4d3888c948813.png)

既然已经结束了，为了进行统计测试，我们必须屏蔽我们的数据。布尔掩码允许我们为我们的数据设置一个要遵守的条件(否则—毫不留情地丢弃)。)但首先，让我们把这种形象化处理掉，因为在这种情况下，眼见为实。

![](img/24794c6627036f4f3b8404efecbd9bf1.png)

从表面上看，这个特性也不是最好的，我们可以通过快速 f 检验来巩固它。让我们来做我之前提到的布尔掩码:

```
high_df = df[(df[:Availability].>=300),:]
```

然后我们将把它插入到来自 Lathe.stats 的 f_test()函数中。

![](img/e8dbfea75615125c7adf8ac57b4cc015.png)

所以考虑我们的假设:P 值为 0.05 或更小通常意味着考虑拒绝我们的零假设具有统计学意义，对吗？因此，P 值为 0.032 表明统计学显著性相当不错，肯定高于我们之前的数据。

# 2.切换模型

让我们面对现实吧，有时你的模型可能不奏效。切换到梯度回归，甚至建立一个神经网络，有时可以是一个非常好的复杂问题的解决方案。虽然您的选择可能类似于 XGBoost，但是对于这个例子，我将切换回线性回归。这并不会提高准确性——在本例中，这只是一个必须考虑的问题。当然，如果车床不是在阿尔法，我们有更多的工具利用，我们可能会全力以赴与模型，使它非常准确，这和努力与回报；我对这个模型没什么意见，我用它作为一个不太精确的例子。

![](img/299650438607d214e9b1a2ae9d6a69bf.png)

转换我们的特征和我们的模型产生了平均绝对误差

> 16014.948562702863

鉴于平均值是 141，这是相当关闭…但一如既往，我们有另一个锦囊妙计…

# 3.特征缩放

众所周知，特征缩放可以提高准确性，让我们面对它:这就是它存在的意义。在这种情况下，我们将使用标准标量，也称为 Z 分数标量。这将根据标准偏差对数据进行标准化。如你所料，我们将编写一个函数来实现这一点。

Z 分数标量的公式为:

> xᵢ = (xᵢ-x̄ ) / σ

其中 sigma 是标准偏差，x 条是样本平均值，x 子条是当前迭代。回到编程的思维模式，我们最终会得到:

```
function StandardScalar(array)
    q = Lathe.stats.std(array)
    avg = Lathe.stats.mean(array)
    v = [i = (i-avg) / q for i in array]
    return(v)
end
```

现在，要使用我们的函数，我们只需插入它，并在定义 xtrain 和 xtest 时对它们进行预处理:

![](img/ddf1adede2df198d730a6b40d97de4e5.png)

在这个具体的例子中，这使我们的平均绝对误差下降到

> 15,249

这意味着我们正式低于我们的基线。

# 4.四分位数归一化

四分位数归一化是另一个有趣的方法，四分位数范围 q 和 q 之外的任何数据都被丢弃，将数据集中在中位数周围(记住，这是中位数，而不是平均值。)

![](img/8a533ac9484d0e3213cfc0e03f533c51.png)

拥有一个准确的模型是很棒的，这些数据科学原则肯定可以帮助你实现这一点。即使你的模型相当准确，其中一些肯定能把它提高到 98%。当像标准标量这样简单的东西对您的准确度分数产生如此大的影响时，这绝对是令人满意的。希望这个快速检查列表有助于阐明一些非常重要的想法和哲学。如果你要从这整个列表中选择一个，我建议你选择**好的特性。**底线是，无论如何，你的特征是你的算法唯一需要放弃的东西。就这样，机器学习快乐！