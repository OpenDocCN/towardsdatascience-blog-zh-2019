<html>
<head>
<title>Speech Emotion Recognition with Convolutional Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于卷积神经网络的语音情感识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speech-emotion-recognition-with-convolution-neural-network-1e6bb7130ce3?source=collection_archive---------4-----------------------#2019-06-01">https://towardsdatascience.com/speech-emotion-recognition-with-convolution-neural-network-1e6bb7130ce3?source=collection_archive---------4-----------------------#2019-06-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b1e4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从录音中识别人类情感</h2></div><p id="fe74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于数据科学家来说，识别人类情感一直是一项令人着迷的任务。最近，我正在进行一项实验性的语音情感识别(SER)项目，以探索它的潜力。我从<a class="ae le" href="https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> GitHub </strong> </a>中选择了最受欢迎的 SER 库作为我项目的主干。</p><p id="8265" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们完成这个项目之前，最好了解一下语音情感识别的主要瓶颈。</p><h2 id="06ac" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><strong class="ak">主要障碍:</strong></h2><ul class=""><li id="cd93" class="ly lz it kk b kl ma ko mb kr mc kv md kz me ld mf mg mh mi bi translated">情绪是主观的，人们会有不同的理解。很难定义情绪的概念。</li><li id="4fac" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">给录音添加注释是一项挑战。我们应该标记一个单词、句子还是整个对话？要定义多少种情绪才能识别？</li><li id="d5c7" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">收集数据很复杂。有许多音频数据可以从电影或新闻中获得。然而，他们都有偏见，因为新闻报道必须是中立的，演员的情绪是模仿的。很难找到不带任何偏见的中性录音。</li><li id="20f4" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">标记数据需要很高的人力和时间成本。与在图像上绘制边界框不同，它需要训练有素的人员聆听整个音频记录，对其进行分析并给出注释。由于其主观性，注释结果必须由多个个人来评估。</li></ul><h2 id="019c" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><strong class="ak">项目描述:</strong></h2><p id="4373" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">用卷积神经网络从录音中识别情感。并且存储库所有者不提供任何论文参考。</p><h2 id="c922" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><strong class="ak">数据描述:</strong></h2><p id="3f06" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">这是两个最初在 RAVDESS 和 SAVEE 库中使用的数据集，我只在我的模型中采用了 RAVDESS。在 RAVDESS 中，有两种类型的数据:语音和歌曲。</p><p id="937c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集:<a class="ae le" href="https://zenodo.org/record/1188976#.XN0fwnUzZhE" rel="noopener ugc nofollow" target="_blank">瑞尔森情感语音和歌曲视听数据库(RAVDESS) </a></p><ul class=""><li id="8203" class="ly lz it kk b kl km ko kp kr mr kv ms kz mt ld mf mg mh mi bi translated">12 位男演员和 12 位女演员分别录制了演讲和歌曲版本。</li><li id="e02b" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">18 号演员没有歌曲版本数据。</li><li id="5a10" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">歌曲版本数据中不包含情感<code class="fe mu mv mw mx b">Disgust</code>、<code class="fe mu mv mw mx b">Neutral</code>、<code class="fe mu mv mw mx b">Surprised</code>。</li></ul><h2 id="1fd3" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">总类别:</h2><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi my"><img src="../Images/7ab53e3d3a0a38ac509b56d00bbf118c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qImiLykubf8uVqU8BJTUqg.png"/></div></div></figure><p id="df1c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是情绪等级分布条形图。</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nk"><img src="../Images/5522736d76fde16263b8541d1fe266ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*24IqeFa-tHgt5XvpMKH1Qw.png"/></div></div></figure><h2 id="3d80" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><strong class="ak">特征提取:</strong></h2><p id="fe22" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">当我们执行语音识别任务时，MFCCs 是自 20 世纪 80 年代发明以来最先进的功能。</p><blockquote class="nl nm nn"><p id="9d81" class="ki kj no kk b kl km ju kn ko kp jx kq np ks kt ku nq kw kx ky nr la lb lc ld im bi translated">这个形状决定了发出什么声音。如果我们能够精确地确定形状，这将会给我们一个产生的<a class="ae le" href="http://en.wikipedia.org/wiki/Phoneme" rel="noopener ugc nofollow" target="_blank">音素</a>的精确表示。声道的形状在短时功率谱的包络中表现出来，而 MFCCs 的工作就是精确地表示这个包络。—摘自:<a class="ae le" href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" rel="noopener ugc nofollow" target="_blank"> MFCC 教程</a></p></blockquote><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ns"><img src="../Images/6d2264e43a17cae7f7174c452e15127f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t4v3_4s1Cp84OUWJw3bk2w.png"/></div></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk">Waveform</figcaption></figure><figure class="mz na nb nc gt nd gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8e75d6b3671eb6f18a7bf5ef169aa5db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*XJc259ThXmnYT1JrOYyImw.png"/></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk">Spectrogram</figcaption></figure><p id="95d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用 MFCCs 作为我们的输入特性。如果你想彻底了解<strong class="kk iu">MFCC</strong><em class="no">，这里有一个很棒的</em> <a class="ae le" href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" rel="noopener ugc nofollow" target="_blank"> <em class="no">教程</em> </a> <em class="no">给你。</em>加载音频数据并将其转换为 MFCCs 格式可以通过 Python 包<code class="fe mu mv mw mx b">librosa</code>轻松完成。</p><h2 id="ab6d" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">默认模型架构:</h2><p id="ed70" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">作者用 Keras 开发了 CNN 模型，它由 7 层构成——6 个 Conv1D 层和一个密集层。</p><pre class="mz na nb nc gt ny mx nz oa aw ob bi"><span id="d0fd" class="lf lg it mx b gy oc od l oe of">model = Sequential()<br/>model.add(Conv1D(256, 5,padding='same', input_shape=(216,1))) #1<br/>model.add(Activation('relu'))<br/>model.add(Conv1D(128, 5,padding='same')) #2<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.1))<br/>model.add(MaxPooling1D(pool_size=(8)))<br/>model.add(Conv1D(128, 5,padding='same')) #3<br/>model.add(Activation('relu'))<br/>#model.add(Conv1D(128, 5,padding='same')) #4<br/>#model.add(Activation('relu'))<br/>#model.add(Conv1D(128, 5,padding='same')) #5<br/>#model.add(Activation('relu'))<br/>#model.add(Dropout(0.2))<br/>model.add(Conv1D(128, 5,padding='same')) #6<br/>model.add(Activation('relu'))<br/>model.add(Flatten())<br/>model.add(Dense(10)) #7<br/>model.add(Activation('softmax'))<br/>opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)</span></pre><blockquote class="nl nm nn"><p id="e555" class="ki kj no kk b kl km ju kn ko kp jx kq np ks kt ku nq kw kx ky nr la lb lc ld im bi translated">作者在最新笔记本(2018 年 9 月 18 日更新)中对第 4 层和第 5 层进行了评论，模型重量文件不适合所提供的网络，因此，我无法加载重量提供并复制其结果<strong class="kk iu"> 72%的测试准确度。</strong></p></blockquote><p id="fb99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型只是简单地用<code class="fe mu mv mw mx b">batch_size=16</code>和 700 个历元进行训练，没有任何学习速率表等。</p><pre class="mz na nb nc gt ny mx nz oa aw ob bi"><span id="3195" class="lf lg it mx b gy oc od l oe of"># Compile Model<br/>model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])</span><span id="c1f9" class="lf lg it mx b gy og od l oe of"># Fit Model<br/>cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))</span></pre><p id="9494" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其损失函数为<code class="fe mu mv mw mx b">categorical_crossentropy</code>，评价指标为准确度。</p><h1 id="1a2d" class="oh lg it bd lh oi oj ok lk ol om on ln jz oo ka lq kc op kd lt kf oq kg lw or bi translated"><strong class="ak"> <em class="os">我的实验</em> </strong></h1><h2 id="67b6" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">探索性数据分析:</h2><p id="5eb1" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">在 RADVESS 数据集中，每个演员必须通过说和唱两句话来表现 8 种情绪，每种情绪两次。结果，除了中性、厌恶和惊讶之外，每个演员将为每种情绪归纳 4 个样本，因为没有这些情绪的歌唱数据。每个音频波大约 4 秒，第一秒和最后一秒很可能是无声的。</p><p id="7f5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">标准句子有:</strong></p><blockquote class="ot"><p id="73a7" class="ou ov it bd ow ox oy oz pa pb pc ld dk translated">1.孩子们在门边说话。<br/> 2。狗坐在门边。</p></blockquote><p id="bc9a" class="pw-post-body-paragraph ki kj it kk b kl pd ju kn ko pe jx kq kr pf kt ku kv pg kx ky kz ph lb lc ld im bi translated"><strong class="kk iu">观察:</strong></p><p id="c43d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我选择了一个男演员和一个女演员的数据集并听了他们所有人的歌之后。我发现男性和女性表达情感的方式不同。以下是一些发现:</p><ul class=""><li id="e4fc" class="ly lz it kk b kl km ko kp kr mr kv ms kz mt ld mf mg mh mi bi translated">男的<strong class="kk iu"> <em class="no">生气</em> </strong>就是单纯的加大音量。​</li><li id="b630" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">男性的<strong class="kk iu"> <em class="no">快乐的</em> </strong>和<strong class="kk iu"> <em class="no">悲伤的</em> </strong>显著特征是在音频中的静音期有笑有哭的语气。</li><li id="28e2" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">女性的<strong class="kk iu"> <em class="no">快乐的</em> </strong>，<strong class="kk iu"> <em class="no">愤怒的</em> </strong>和<strong class="kk iu"> <em class="no">悲伤的</em> </strong>都是音量增大。​</li><li id="ee98" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">女性的<strong class="kk iu"> <em class="no">厌恶</em> </strong>里面会加上呕吐的声音。</li></ul><p id="0146" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">复制结果:</strong></p><p id="99b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者排除了类别<code class="fe mu mv mw mx b">neutral</code>、<code class="fe mu mv mw mx b">disgust</code>和<code class="fe mu mv mw mx b">surprised</code>来对 RAVDESS 数据集进行 10 个类别的识别。</p><p id="ec37" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我试图用提供的模型复制他的结果，我可以达到的结果是</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pi"><img src="../Images/a0785aedad0178160b9fd7db0d4f9168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p9Q2o3v-QpveQ2IEI5D3VQ.png"/></div></div></figure><p id="d9c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，我发现在训练阶段使用的验证集与测试集相同的地方存在数据泄漏问题。因此，我重新做了数据分割部分，将两个男演员和两个女演员的数据隔离到测试集中，以确保它在训练阶段是不可见的。</p><ul class=""><li id="d609" class="ly lz it kk b kl km ko kp kr mr kv ms kz mt ld mf mg mh mi bi translated">1-20 号演员用于分割比为 8:2 的<code class="fe mu mv mw mx b">Train / Valid</code>场景。</li><li id="7a00" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">演员 21-24 被隔离以测试使用。</li><li id="0cd4" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">列车集合形状:(1248，216，1)</li><li id="e462" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">有效的集合形状:(312，216，1)</li><li id="0d98" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">测试集形状:(320，216，1)——(隔离)</li></ul><p id="0d1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我用新的数据分割设置重新训练了模型，结果如下:</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pj"><img src="../Images/f319ea789aa895b0620c2067922ba0cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O6QH63rFvvMQPhoMqjZ91Q.png"/></div></div></figure><h2 id="a0a4" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><strong class="ak">基准:</strong></h2><p id="6376" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">从列车有效损失图可以看出，该模型甚至不能很好地收敛于 10 个目标类别。因此，我决定通过只识别男性情感来降低模型的复杂性。我将这两个角色分离为<code class="fe mu mv mw mx b">test set</code>，其余的将是具有 8:2 <strong class="kk iu"> <em class="no">分层混洗分割</em> </strong>的<code class="fe mu mv mw mx b">train/valid set</code>，这确保了数据集中没有类别不平衡。之后，我分别训练了男性和女性数据，以探索基准。</p><p id="7050" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">男性数据集</strong></p><ul class=""><li id="cb4f" class="ly lz it kk b kl km ko kp kr mr kv ms kz mt ld mf mg mh mi bi translated">训练集=来自演员 1- 10 的 640 个样本。</li><li id="5602" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">有效集=来自演员 1- 10 的 160 个样本。</li><li id="ac25" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">测试集=来自演员 11- 12 的 160 个样本。</li></ul><p id="167b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">男性基线</strong></p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pk"><img src="../Images/19f47516078a58ce83123da8d6d24080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rf7IVRLl92KIsW2SbCxZ8Q.png"/></div></div></figure><p id="293d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">女性数据集</strong></p><ul class=""><li id="b7b2" class="ly lz it kk b kl km ko kp kr mr kv ms kz mt ld mf mg mh mi bi translated">训练集=来自女演员 1- 10 的 608 个样本。</li><li id="0c88" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">有效集=来自女演员 1- 10 的 152 个样本。</li><li id="b412" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">测试集=来自女演员 11- 12 的 160 个样本。</li></ul><p id="09df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">女性基线</strong></p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pk"><img src="../Images/6fa11643918acf41757c092f04ceed5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B1Y_RHckoH9t8tKzADUmKg.png"/></div></div></figure><p id="f9b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如你所见，男性和女性模型的混淆矩阵是不同的。</p><p id="8b7e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">- <strong class="kk iu"> <em class="no">男性</em> </strong> : <code class="fe mu mv mw mx b">Angry</code>和<code class="fe mu mv mw mx b">Happy</code>是男性模型中占主导地位的预测类，但它们不太可能混淆。​</p><p id="19d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">- <strong class="kk iu"> <em class="no">女</em> </strong> : <code class="fe mu mv mw mx b">Sad</code>和<code class="fe mu mv mw mx b">Happy</code>是女模型中占优势的预测类，<code class="fe mu mv mw mx b">Angry</code>和<code class="fe mu mv mw mx b">Happy</code>很有可能混淆。</p><p id="3d33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参照<strong class="kk iu"> EDA </strong>部分的观察，我怀疑女性<code class="fe mu mv mw mx b">Angry</code>和<code class="fe mu mv mw mx b">Happy</code>很可能混淆的原因是因为她们的表达方式只是简单地增加了说话的音量。</p><p id="9fb3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除此之外，我想知道如果我进一步简化模型，将目标类减少到<code class="fe mu mv mw mx b">Positive</code>、<code class="fe mu mv mw mx b">Neutral</code>和<code class="fe mu mv mw mx b">Negative</code>或者甚至仅仅是<code class="fe mu mv mw mx b">Positive</code>和<code class="fe mu mv mw mx b">Negative</code>会怎么样。所以，我把情绪分为 2 类和 3 类。</p><p id="4c74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 2 类:</strong></p><ul class=""><li id="a410" class="ly lz it kk b kl km ko kp kr mr kv ms kz mt ld mf mg mh mi bi translated">正:<code class="fe mu mv mw mx b">happy</code>，<code class="fe mu mv mw mx b">calm</code>。</li><li id="302b" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">负面:<code class="fe mu mv mw mx b">angry</code>、<code class="fe mu mv mw mx b">fearful</code>、<code class="fe mu mv mw mx b">sad</code>。</li></ul><p id="8ab1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3 类:</strong></p><ul class=""><li id="4617" class="ly lz it kk b kl km ko kp kr mr kv ms kz mt ld mf mg mh mi bi translated">阳性:<code class="fe mu mv mw mx b">happy</code>。</li><li id="b5a8" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">中性:<code class="fe mu mv mw mx b">calm</code>，<code class="fe mu mv mw mx b">neutral</code>。</li><li id="d333" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">负面:<code class="fe mu mv mw mx b">angry</code>、<code class="fe mu mv mw mx b">fearful</code>、<code class="fe mu mv mw mx b">sad</code>。</li></ul><p id="c7c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(在 3 类中添加了中性来探索结果。)</p><p id="6398" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我做训练实验之前，我通过做 5 类识别用男性数据调整模型架构。</p><pre class="mz na nb nc gt ny mx nz oa aw ob bi"><span id="a5ae" class="lf lg it mx b gy oc od l oe of"># Set the target class number<br/>target_class = 5</span><span id="94a8" class="lf lg it mx b gy og od l oe of"># Model <br/>model = Sequential()<br/>model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1))) #1<br/>model.add(Activation('relu'))<br/>model.add(Conv1D(256, 8, padding='same')) #2<br/>model.add(BatchNormalization())<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.25))<br/>model.add(MaxPooling1D(pool_size=(8)))<br/>model.add(Conv1D(128, 8, padding='same')) #3<br/>model.add(Activation('relu')) <br/>model.add(Conv1D(128, 8, padding='same')) #4<br/>model.add(Activation('relu'))<br/>model.add(Conv1D(128, 8, padding='same')) #5<br/>model.add(Activation('relu'))<br/>model.add(Conv1D(128, 8, padding='same')) #6<br/>model.add(BatchNormalization())<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.25))<br/>model.add(MaxPooling1D(pool_size=(8)))<br/>model.add(Conv1D(64, 8, padding='same')) #7<br/>model.add(Activation('relu'))<br/>model.add(Conv1D(64, 8, padding='same')) #8<br/>model.add(Activation('relu'))<br/>model.add(Flatten())<br/>model.add(Dense(target_class)) #9<br/>model.add(Activation('softmax'))<br/>opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)</span></pre><p id="15f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我添加了 2 个 Conv1D 层，1 个 MaxPooling1D 层和 2 个 BarchNormalization 层，此外，我还将 dropout 值更改为 0.25。最后，我将优化器改为 SGD，学习率为 0.0001。</p><pre class="mz na nb nc gt ny mx nz oa aw ob bi"><span id="e8c0" class="lf lg it mx b gy oc od l oe of">lr_reduce = ReduceLROnPlateau(monitor=’val_loss’, factor=0.9, patience=20, min_lr=0.000001)</span><span id="121c" class="lf lg it mx b gy og od l oe of">mcp_save = ModelCheckpoint(‘model/baseline_2class_np.h5’, save_best_only=True, monitor=’val_loss’, mode=’min’)</span><span id="f502" class="lf lg it mx b gy og od l oe of">cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test), callbacks=[mcp_save, lr_reduce])</span></pre><p id="84ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于模型训练，我采用<code class="fe mu mv mw mx b">Reduce Learning On Plateau</code>并只保存最小<code class="fe mu mv mw mx b">val_loss</code>的最佳模型。这里是不同目标类设置的模型性能。</p><h2 id="a82a" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">新型号性能</h2><p id="c68b" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated"><strong class="kk iu">男 5 班</strong></p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pj"><img src="../Images/7d62e1971485704b8f11cae33b60f500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9_8FW-RThilmRDSFrBeIUQ.png"/></div></div></figure><p id="df47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">女 5 班</strong></p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pk"><img src="../Images/ceaadc7d2e8cb8744e6f4e474eb8b74b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eOBDi3S2Ja-lmZnTikrPEA.png"/></div></div></figure><p id="ed94" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">男二班</strong></p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pl"><img src="../Images/52b4f8f320aa2c32a9b73a8465a45ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ruj6I8fODYZQk4Tiamo97g.png"/></div></div></figure><p id="9c25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">男 3 班</strong></p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pl"><img src="../Images/6739adfd76b02555359549129b37a2da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*McCmsojGDfMS_kSzfAYvfA.png"/></div></div></figure><h1 id="aea6" class="oh lg it bd lh oi oj ok lk ol om on ln jz oo ka lq kc op kd lt kf oq kg lw or bi translated">增大</h1><p id="9670" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">在我调整了模型架构、优化器和学习率计划之后，我发现模型在训练期间仍然无法收敛。我认为这是数据大小的问题，因为我们只有 800 个样本用于训练有效集。因此，我决定探索音频增强方法。让我们来看看一些带有代码的增强方法。我简单地增加了所有数据集一次，使训练/有效集大小加倍。</p><h2 id="d82a" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">男 5 级:</h2><p id="369f" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated"><strong class="kk iu">动态值变化</strong></p><pre class="mz na nb nc gt ny mx nz oa aw ob bi"><span id="ab39" class="lf lg it mx b gy oc od l oe of">def dyn_change(data):<br/>    """<br/>    Random Value Change.<br/>    """<br/>    dyn_change = np.random.uniform(low=1.5,high=3)<br/>    return (data * dyn_change)</span></pre><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pi"><img src="../Images/c614602ceafb8ae21b03ff422cf954a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dR1-3A2o4Zpwc0sw_ph6EA.png"/></div></div></figure><p id="552d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">音高调谐</strong></p><pre class="mz na nb nc gt ny mx nz oa aw ob bi"><span id="00cf" class="lf lg it mx b gy oc od l oe of">def pitch(data, sample_rate):<br/>    """<br/>    Pitch Tuning.<br/>    """<br/>    bins_per_octave = 12<br/>    pitch_pm = 2<br/>    pitch_change =  pitch_pm * 2*(np.random.uniform())   <br/>    data = librosa.effects.pitch_shift(data.astype('float64'), <br/>                                      sample_rate, n_steps=pitch_change, <br/>                                      bins_per_octave=bins_per_octave)</span></pre><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pj"><img src="../Images/0d3b480331cd585da9179680ead32a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yE5cHHQzVt-Ged6Fh2B7ag.png"/></div></div></figure><p id="2329" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">换挡</strong></p><pre class="mz na nb nc gt ny mx nz oa aw ob bi"><span id="7e70" class="lf lg it mx b gy oc od l oe of">def shift(data):<br/>    """<br/>    Random Shifting.<br/>    """<br/>    s_range = int(np.random.uniform(low=-5, high = 5)*500)<br/>    return np.roll(data, s_range)</span></pre><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pl"><img src="../Images/31e57612f501c4c806be4ec106433d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D5rR69BGVxdWwseFhMQlkg.png"/></div></div></figure><p id="3624" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">白噪声添加</strong></p><pre class="mz na nb nc gt ny mx nz oa aw ob bi"><span id="6bca" class="lf lg it mx b gy oc od l oe of">def noise(data):<br/>    """<br/>    Adding White Noise.<br/>    """<br/>    # you can take any distribution from <a class="ae le" href="https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html" rel="noopener ugc nofollow" target="_blank">https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html</a><br/>    noise_amp = 0.005*np.random.uniform()*np.amax(data)<br/>    data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0])<br/>    return data</span></pre><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pl"><img src="../Images/13d685baa21210cbf543a6b4e46bab42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wcgJinD4psnHim0YzEWzHQ.png"/></div></div></figure><blockquote class="nl nm nn"><p id="6790" class="ki kj no kk b kl km ju kn ko kp jx kq np ks kt ku nq kw kx ky nr la lb lc ld im bi translated">我们可以看到，这种增强可以大大提高<strong class="kk iu">验证的准确性</strong>，一般来说是 70+%。特别是加入白噪声可以达到 87.19%的<strong class="kk iu">验证准确率</strong>，然而<strong class="kk iu">测试准确率</strong>和<strong class="kk iu">测试 F1-score </strong>分别下降超过 5%。然后，我想知道如果我混合不同的增强方法会带来一个好的结果。</p></blockquote><h2 id="b32c" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">混合多种方法</h2><p id="8966" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated"><strong class="kk iu">加噪+移位</strong></p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pl"><img src="../Images/aa8a2d41f0768b7812e25459e2f6bece.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mz7oznM7YiWWSNa89XmV5w.png"/></div></div></figure><h1 id="4131" class="oh lg it bd lh oi oj ok lk ol om on ln jz oo ka lq kc op kd lt kf oq kg lw or bi translated">男性 2 类数据的扩充测试</h1><h2 id="80c1" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">男性 2 类:</h2><p id="e62d" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated"><strong class="kk iu">所有样本的加噪+移位<br/> </strong></p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pm"><img src="../Images/0199c3b51cbd12e9737aead3d50bc3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BqZgD8PvjaU3r8vUdmQohw.png"/></div></div></figure><p id="71aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">噪声添加+移位</strong> <br/>仅针对正样本，因为 2 类集合不平衡(向负方向倾斜)。</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pk"><img src="../Images/86ca1a9d248b76b6881f6d11fb30057d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nda3UKutRTfxx17FoyD1Vg.png"/></div></div></figure><p id="44fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">音高调谐+噪声添加<br/> </strong>针对所有样本</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pm"><img src="../Images/e835b0499feeac2387bcc367218b957c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9-CmRnGCCvGI9F-9sjKSIQ.png"/></div></div></figure><p id="f734" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">音调调谐+噪声添加<br/> </strong>仅用于阳性样本</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pn"><img src="../Images/6f8115a40cfb874b358632e9f17705a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8NB_NjB7nl0pKsbcsQeDow.png"/></div></div></figure><h1 id="7e20" class="oh lg it bd lh oi oj ok lk ol om on ln jz oo ka lq kc op kd lt kf oq kg lw or bi translated">结论</h1><p id="0acc" class="pw-post-body-paragraph ki kj it kk b kl ma ju kn ko mb jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">最后我只有时间用男性数据集做实验。我用分层混洗分割法重新分割数据，以确保没有数据不平衡或数据泄漏问题。我通过试验男性数据集来调整模型，因为我想在开始时简化模型。我还用不同的目标标签设置和增强方法测试了 by。我发现为不平衡的数据添加<strong class="kk iu">噪声</strong>和<strong class="kk iu">移位</strong>可以帮助达到更好的结果。</p><h1 id="259d" class="oh lg it bd lh oi oj ok lk ol om on ln jz oo ka lq kc op kd lt kf oq kg lw or bi translated">钥匙拿走</h1><ul class=""><li id="ce60" class="ly lz it kk b kl ma ko mb kr mc kv md kz me ld mf mg mh mi bi translated">情绪是主观的，很难用符号表示出来。</li><li id="218e" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">我们应该定义适合我们自己项目目标的情绪。</li><li id="6446" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">不要总是相信 GitHub 的内容，即使它有很多明星。</li><li id="4d94" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">注意数据分割。</li><li id="83d4" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">探索性数据分析总是给我们很好的洞察力，当你处理音频数据时，你必须有耐心！</li><li id="d630" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">决定模型的输入:一个句子，一段录音还是一段话语？</li><li id="8353" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">缺少数据是服务识别成功的关键因素，然而，建立一个好的语音情感数据集是复杂且昂贵的。</li><li id="3301" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">当你缺少数据时，简化你的模型。</li></ul><h1 id="5fee" class="oh lg it bd lh oi oj ok lk ol om on ln jz oo ka lq kc op kd lt kf oq kg lw or bi translated">进一步改进</h1><ul class=""><li id="dbf5" class="ly lz it kk b kl ma ko mb kr mc kv md kz me ld mf mg mh mi bi translated">我只选择了前 3 秒作为输入数据，因为这将减少维度，原来的笔记本只使用了 2.5 秒。我想用这段完整的音频来做实验。</li><li id="0b8f" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">对数据进行预处理，如裁剪无声语音、通过零填充归一化长度等。</li><li id="eb5f" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated">在这个主题上试验递归神经网络方法。</li></ul><h1 id="f400" class="oh lg it bd lh oi oj ok lk ol om on ln jz oo ka lq kc op kd lt kf oq kg lw or bi translated"><strong class="ak">关于我</strong></h1><ul class=""><li id="dbb5" class="ly lz it kk b kl ma ko mb kr mc kv md kz me ld mf mg mh mi bi translated"><strong class="kk iu">GitHub</strong>:<a class="ae le" href="https://github.com/rezachu/emotion_recognition_cnn" rel="noopener ugc nofollow" target="_blank">rezachu/emotion _ recognition _ CNN</a></li><li id="f28d" class="ly lz it kk b kl mj ko mk kr ml kv mm kz mn ld mf mg mh mi bi translated"><strong class="kk iu">Linkedin</strong>:<a class="ae le" href="https://www.linkedin.com/in/rezachukc/" rel="noopener ugc nofollow" target="_blank">Kai Cheong，Reza Chu </a></li></ul><p id="47e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="no">备注:GitHub 中很快会注意到一些与该主题相关的论文。</em></p></div></div>    
</body>
</html>