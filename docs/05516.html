<html>
<head>
<title>Building a Vocal Emotion Sensor with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用深度学习构建声音情感传感器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9?source=collection_archive---------14-----------------------#2019-08-14">https://towardsdatascience.com/building-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9?source=collection_archive---------14-----------------------#2019-08-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5b8d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">教机器更好地理解人类交流</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a49f8dffffaa82f0542c0e3e3e45d5aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02UQTzg8g_jnyWody0_Kkg.jpeg"/></div></div></figure><p id="9cb6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">人类的表达是多方面的，复杂的。例如，说话者不仅通过语言交流，还通过节奏、语调、面部表情和肢体语言交流。这就是为什么我们更喜欢面对面而不是电话会议来举行商务会议，也是为什么电话会议比电子邮件或短信更受欢迎。我们离得越近，通信带宽就越多。</p><p id="ab20" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">语音识别软件近年来有了很大的进步。这项技术现在在识别语音并将这些语音拼凑起来以再现口语单词和句子方面做得非常好。然而，简单地将语音翻译成文本并不能完全概括说话者的信息。除了面部表情和肢体语言，与音频相比，文本在捕捉情感意图方面的能力非常有限。</p><p id="e580" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最初，我选择构建一个声音情感传感器，因为这似乎是一个有趣的工作项目。尽管对这个问题想得更多，我意识到通过音频进行情感感应有一些真正有趣的应用。想象一下，如果你的智能家居设备可以播放符合你情绪的歌曲，例如当你悲伤时播放振奋人心的歌曲。客户服务部门可以使用情绪检测来培训员工，或者测量客户在服务电话过程中是否变得更开心。</p><h1 id="9f09" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">数据</h1><p id="f0fa" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">我用来构建我的情感分类器的数据集是<a class="ae mn" href="https://smartlaboratory.org/ravdess" rel="noopener ugc nofollow" target="_blank">拉夫德斯</a>、<a class="ae mn" href="https://tspace.library.utoronto.ca/handle/1807/24487" rel="noopener ugc nofollow" target="_blank">苔丝</a>和<a class="ae mn" href="http://kahlan.eps.surrey.ac.uk/savee/Download.html" rel="noopener ugc nofollow" target="_blank"> SAVEE </a>，它们都是免费向公众开放的(SAVEE 需要一个非常简单的注册)。这些数据集包含七个常见类别的音频文件:中性、快乐、悲伤、愤怒、恐惧、厌恶和惊讶。我总共获得了 30 名男女演员制作的 4500 个带标签的音频文件中超过 160 分钟的音频。这些文件通常由男演员或女演员说的带有特定情感意图的简短短语组成。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/9ae4c0283a5c327aea7073e30252d4c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PetO6UtN3M6jhIy4Y3bcVw.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">Actors for the SAVEE dataset</figcaption></figure><h1 id="8f8e" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">特征抽出</h1><p id="9855" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">接下来，我必须找到可以从音频中提取的有用特征。最初我想用短时傅立叶变换来提取频率信息。然而，对该主题的一些研究表明，当涉及到语音识别应用时，傅立叶变换是相当有缺陷的。原因是尽管傅立叶变换是声音极好的物理表示，但它并不表示人类如何感知声音。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/688da809c4a905c21df53a0d7063bf0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*wNqsNetuSDokJqvW09B16A.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">Raw audio waveform. In this form it is useless for classification.</figcaption></figure><p id="c103" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从音频中提取特征的更好方法是使用 Mel 频率倒谱系数，简称 MFCCs。这里提供了一个很好的解释，说明 MFCCs 是如何从音频中获得的。MFCCs 试图以更符合人类感知的方式来表示音频。</p><p id="c903" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了从音频中导出 MFCCs，需要决定使用多少频率仓以及分段的时间步长有多宽。这些决定决定了输出 MFCC 数据的粒度。语音识别应用的标准实践是应用 20Hz-20k Hz 之间的 26 个频率仓，并且仅使用前 13 个用于分类。大多数有用的信息都在较低的频率范围内，包含较高的频率范围通常会导致性能下降。对于时间步长，10 到 100 毫秒之间的值是常见的。我选择用 25 毫秒。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/167f809eb175450530fbac2af4a5c742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2t0ruJ62k_iP9Am0UPeh3Q.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">Mel filter banks used to bin audio frequency content.</figcaption></figure><p id="4134" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">导出的 MFCCs 可以绘制在热图上，并用于可视化音频。这样做并不能揭示情感类别之间的任何明显差异。这与其说是因为缺乏模式，不如说是因为人类没有受过训练，无法从视觉上识别这些微妙的情感差异。然而，从这些热图中很容易看出男性和女性说话者之间的差异。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/3d14929b9324eae7e10e09c5c1cd92fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xkQoLkq8MOTlO-Ksx8vuTw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/e3e40a6376818a484aedc7e5a619bb6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1tANPh59vuMIttfROoXk1w.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">Visualized MFCCs for Happy Male and Female Speakers. Women tend to have stronger high frequency components in their voices, as shown by the brighter colors towards the top of the heatmap.</figcaption></figure><h1 id="ab97" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">训练卷积神经网络</h1><p id="d886" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">通过推导 MFCCs，音频分类问题实质上被转化为图像识别问题。因此，在图像识别领域非常有效的工具、算法和技术在音频分类中也非常有效。为了解决情感分类问题，我选择使用卷积神经网络(CNN ),因为这些网络已经被证明在图像和音频识别方面都是有效的。</p><p id="aca8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在训练 CNN 之前，我将数据集中的文件随机分配给 80/20 分割的训练集或测试集。然后，我对训练文件执行了一些预处理步骤。每个文件的流程如下:</p><ol class=""><li id="85aa" class="mw mx it kw b kx ky la lb ld my lh mz ll na lp nb nc nd ne bi translated">切断所有的沉默。</li><li id="a36b" class="mw mx it kw b kx nf la ng ld nh lh ni ll nj lp nb nc nd ne bi translated">随机选择若干个 0.4s 窗口。</li><li id="4844" class="mw mx it kw b kx nf la ng ld nh lh ni ll nj lp nb nc nd ne bi translated">确定每个窗口的 MFCCs，产生 13×16 阵列。</li><li id="27ca" class="mw mx it kw b kx nf la ng ld nh lh ni ll nj lp nb nc nd ne bi translated">将 MFCCs 调整到 0 到 1 的范围。(这一步超级重要！它使模型无法适应录音的音量水平。)</li><li id="6485" class="mw mx it kw b kx nf la ng ld nh lh ni ll nj lp nb nc nd ne bi translated">将每个窗口与源文件的情感标签相关联。</li></ol><p id="69ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">完成预处理后，我生成了 75，000 个标记为 0.4s 的窗口用于训练，每个窗口由一个 13x16 的数组表示。然后我用这些数据训练了我的 CNN 25 个时代。</p><h1 id="2fde" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">模型检验</h1><p id="fbf2" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">为了对测试集上的模型进行基准测试，我应用了一个类似于用来创建训练数据的流程工作流。测试集中每个文件的流程是:</p><ol class=""><li id="7a1d" class="mw mx it kw b kx ky la lb ld my lh mz ll na lp nb nc nd ne bi translated">切断所有的沉默。</li><li id="6c0f" class="mw mx it kw b kx nf la ng ld nh lh ni ll nj lp nb nc nd ne bi translated">创建步长为 0.1s 的“滑动”0.4s 窗口。(例如，第一窗口的范围从 0.0s 到 0.4s，第二窗口的范围从 0.1s 到 0.5s，等等。)</li><li id="ef49" class="mw mx it kw b kx nf la ng ld nh lh ni ll nj lp nb nc nd ne bi translated">确定每个窗口的 MFCCs，范围从 0 到 1。</li><li id="c38f" class="mw mx it kw b kx nf la ng ld nh lh ni ll nj lp nb nc nd ne bi translated">对每个窗口进行分类并返回 softmax 输出。</li><li id="36e4" class="mw mx it kw b kx nf la ng ld nh lh ni ll nj lp nb nc nd ne bi translated">聚合每个窗口的预测。</li><li id="0a26" class="mw mx it kw b kx nf la ng ld nh lh ni ll nj lp nb nc nd ne bi translated">最终预测是聚合后的最大类。</li></ol><p id="fc00" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将这个过程应用于测试集中的所有 889 个文件，产生了 83%的总体准确率。我非常怀疑自己能否以接近 83%的准确率给这些文件贴上标签。下面的条形图显示了每种特定情绪的准确度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/70d83d511ca3e9aec462c6f25e4a6b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ytg_S5YRape3EEe1l3ysTw.png"/></div></div></figure><h1 id="6827" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">外卖食品</h1><p id="0047" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">这篇博文可能会让构建、训练和测试模型看起来简单明了。我可以向你保证，事实并非如此。在达到 83%的准确率之前，有许多版本的模型表现相当差。在一次迭代中，我没有正确地缩放我的输入，这导致了测试集中的几乎每个文件都被预测为“令人惊讶”。那么我从这次经历中学到了什么呢？</p><p id="0c50" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，这个项目很好地展示了简单地收集更多的数据如何能够极大地改善结果。我第一次成功的模型迭代只使用了 RAVDESS 数据集，大约 1400 个音频文件。仅用这个数据集我能达到的最高准确率是 67%。为了达到 83%的准确率，我所做的就是将数据集的大小增加到 4500 个文件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/1cc26369997e69d87136b05cb80728ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*d0j1ymGzcBAjiU4lcjba6A.jpeg"/></div></figure><p id="6965" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其次，我了解到对于音频分类来说，数据预处理是至关重要的。原始音频，甚至短时傅立叶变换，几乎完全没用。我从惨痛的教训中认识到，适当的缩放可以成就一个模型，也可以毁掉它。无法消除沉默是另一个简单的陷阱。一旦音频被适当地转化为信息特征，建立和训练深度学习模型就相对容易了。</p><p id="21ac" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">总而言之，为语音情感检测建立一个分类模型是一次富有挑战性但却值得的经历。在不久的将来，我可能会再次访问这个项目，以扩大它。我想做的一些事情包括:针对更广泛的输入测试模型，使模型适应更广泛的情绪，并将模型部署到云上进行实时情绪检测。</p><h1 id="82b5" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">附录</h1><p id="ef37" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated"><a class="ae mn" href="https://github.com/alexmuhr/Voice_Emotion" rel="noopener ugc nofollow" target="_blank"> Github </a>、<a class="ae mn" href="https://www.linkedin.com/in/alexander-muhr/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae mn" href="https://www.datascienceodyssey.com" rel="noopener ugc nofollow" target="_blank">个人博客</a></p></div></div>    
</body>
</html>