<html>
<head>
<title>Probability Learning III: Maximum Likelihood</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">概率学习 III:最大似然</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/probability-learning-iii-maximum-likelihood-e78d5ebea80c?source=collection_archive---------9-----------------------#2019-08-29">https://towardsdatascience.com/probability-learning-iii-maximum-likelihood-e78d5ebea80c?source=collection_archive---------9-----------------------#2019-08-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7736" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们成为概率大师的又一步…</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dbf063b52b14bff82c30e8a1b2474391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6EdnjiVkrsQw4GhkFTnLw.jpeg"/></div></div></figure><p id="3a97" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在之前的两篇关于贝叶斯定理的帖子之后，我收到了很多请求，要求对定理的回归和分类用途背后的数学进行更深入的解释。<strong class="kw iu">下一个系列的帖子是对这些要求的回答</strong>。</p><p id="8cb5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，我认为，如果我们首先涵盖概率机器学习的另一个基本方法<strong class="kw iu">最大似然法<strong class="kw iu">背后的理论和数学，贝叶斯背后的数学将会<strong class="kw iu">更好理解</strong>。</strong>本帖将致力于解释。</strong></p><p id="50b4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">前面的文章可以找到<a class="ae lq" rel="noopener" target="_blank" href="/probability-learning-i-bayes-theorem-708a4c02909a?source=friends_link&amp;sk=29a5c9c9301a1204f16460781eaba113"> <strong class="kw iu">这里</strong> </a> <strong class="kw iu"> </strong>和<strong class="kw iu"> </strong> <a class="ae lq" rel="noopener" target="_blank" href="/probability-learning-ii-how-bayes-theorem-is-applied-in-machine-learning-bd747a960962"> <strong class="kw iu">这里</strong> </a> <strong class="kw iu">。我建议在处理下一个问题之前先读一读，然后跟随我们一起创造的美丽故事线。</strong></p><p id="b6de" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lr">在我们开始之前，这里有一些额外的资源可以让你的机器学习生涯突飞猛进</em></p><pre class="kj kk kl km gt ls lt lu lv aw lw bi"><span id="4a1f" class="lx ly it lt b gy lz ma l mb mc"><em class="lr">Awesome Machine Learning Resources:</em></span><span id="3de3" class="lx ly it lt b gy md ma l mb mc"><em class="lr">- For </em><strong class="lt iu"><em class="lr">learning resources</em></strong><em class="lr"> go to </em><a class="ae lq" href="https://howtolearnmachinelearning.com/books/machine-learning-books/" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="lr">How to Learn Machine Learning</em></strong></a><em class="lr">! - For </em><strong class="lt iu"><em class="lr">professional</em></strong><em class="lr"> </em><strong class="lt iu"><em class="lr">resources</em></strong><em class="lr"> (jobs, events, skill tests) go to </em><a class="ae lq" href="https://aigents.co/" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="lr">AIgents.co</em></strong></a></span><span id="8de4" class="lx ly it lt b gy md ma l mb mc"><a class="ae lq" href="https://aigents.co/" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="lr">— A career community for Data Scientists &amp; Machine Learning Engineers</em></strong></a><strong class="lt iu"><em class="lr">.</em></strong></span></pre><h1 id="29e4" class="me ly it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">最大似然原则</h1><p id="2cab" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">最大似然法的目标是使一些数据符合最佳的统计分布。这使得数据更容易处理，更通用，允许我们查看新数据是否遵循与先前数据相同的分布，最后，它允许我们对未标记的数据点进行分类。</p><p id="9911" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就像在<a class="ae lq" rel="noopener" target="_blank" href="/probability-learning-ii-how-bayes-theorem-is-applied-in-machine-learning-bd747a960962">上一篇</a>中，想象一个<strong class="kw iu">男性和女性个体使用身高的二元分类问题</strong>。一旦我们计算了男女身高的概率分布，并且我们得到了一个<strong class="kw iu">新数据点</strong>(作为没有标签的身高)<strong class="kw iu">我们就可以将其分配到最有可能的类别</strong>，看看哪个分布报告了两者的最高概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/8fddbee12ed1220bdba25b62e4b029f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*twrMncyWo2RV21D_9QVZgA.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Graphical representation of this binary classification problem</figcaption></figure><p id="6b06" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在之前的图像中，该新数据点(<strong class="kw iu"> <em class="lr"> xnew，</em> </strong>对应于 172 cm 的高度)被分类为女性，对于该特定高度值，女性高度分布产生比男性高的概率。</p><p id="3aec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可能会说这很酷，但是我们如何计算这些概率分布呢？不要担心，我们现在就开始。首先我们将解释其背后的一般过程，然后我们将通过一个更具体的例子。</p><h1 id="c0bc" class="me ly it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">计算分布:估计参数密度函数</h1><p id="ba18" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">像往常一样在<strong class="kw iu">机器学习</strong>中，我们首先需要开始计算一个分布是要学习的东西:<strong class="kw iu">我们宝贵的数据</strong>。我们将把我们的<strong class="kw iu">大小 n </strong>的数据向量表示为<strong class="kw iu"> <em class="lr"> X </em> </strong>。在这个向量中，每一行都是具有 d 个特征的数据点<strong class="kw iu">，因此我们的数据向量<em class="lr"> X </em>实际上是向量的向量:大小为 n x d </strong>的矩阵<strong class="kw iu">；n 个数据点，每个具有 d 个特征。</strong></p><p id="53bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦我们收集了想要计算分布的数据，我们需要开始猜测。<strong class="kw iu">猜谜？</strong>是的，你没看错，我们需要猜测我们的数据符合哪种密度函数或分布:<strong class="kw iu">高斯、指数、泊松……</strong></p><p id="84d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">不要担心，这听起来可能不太科学，但大多数情况下，每种数据都有一个最可能符合的分布:温度或高度等特征的<strong class="kw iu">高斯分布</strong>，时间等特征的<strong class="kw iu">指数分布</strong>，电话通话时间或细菌种群寿命等特征的<strong class="kw iu">泊松分布</strong>。</p><p id="7229" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">完成后，我们<strong class="kw iu">计算最符合我们数据的特定分布参数</strong>。对于正态分布，这将是平均值和方差。由于高斯或正态分布可能是最容易解释和理解的分布，<strong class="kw iu">我们将继续这篇文章，假设我们已经选择了一个高斯密度函数来表示我们的数据。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/a80f0568a7a3fb2885052c0f22a51b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*e5kYWWxKa6KF_ubbW6Rd_w.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Data and parameters for our gaussian distribution</figcaption></figure><p id="3b94" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这种情况下，我们需要计算的参数个数是<strong class="kw iu"> <em class="lr"> d </em> </strong> means(每个特征一个)和<strong class="kw iu"> <em class="lr"> d(d+1)/2 </em> </strong>方差，因为协方差矩阵是对称的<em class="lr"> dxd </em>矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/5c1cae5b1acac46e981f53bad78be46f.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*s01YxSBUCGP6gNOM2G__2w.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Total parameters we need to calculate for a normal distribution depending on the number of features</figcaption></figure><p id="43c0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们称分布的整体参数组为<strong class="kw iu"> θ </strong>。在我们的例子中，这包括每个特征的平均值<strong class="kw iu">和方差</strong>。我们现在要做的是获得使数据向量的联合密度函数最大化的参数集<strong class="kw iu">θ</strong>；所谓<strong class="kw iu"> <em class="lr">似然函数</em> </strong> <strong class="kw iu"> <em class="lr"> L(θ)。</em> </strong>这个似然函数也可以表示为<strong class="kw iu"> <em class="lr"> P(X|θ) </em> </strong>，可以读作给定参数集θ时 X 的条件概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/1ba8ca90639d4a1f87b46311aa48defd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*cXVi6chnuGa5-ENWuKE21g.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Likelihood function</figcaption></figure><p id="6005" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这个符号<strong class="kw iu">中，X 是数据矩阵，X(1)到 X(n)是每个数据点，θ是分布的给定参数集。</strong>同样，由于最大似然法的目标是选择参数值，使观察到的数据尽可能接近，<strong class="kw iu">我们得出一个依赖于θ的优化问题。</strong></p><p id="0d8d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了获得该最佳参数集，我们<strong class="kw iu">对似然函数</strong>中的θ进行求导，并搜索最大值:该最大值代表尽可能观察到可用数据的参数值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f7fd9651b723e6fffceae3561f38cd68.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*ya2KadWsbiu4at9A6q_GVA.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Taking derivatives with respect to θ</figcaption></figure><p id="9b94" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，<strong class="kw iu">如果 X 的数据点彼此独立</strong>，则似然函数可以<strong class="kw iu">表示为给定参数集的每个数据点的个体概率</strong>的乘积:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e92e17ccb257a132e0fd166022be6f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*pnVjaUlvjra6qF-Jt1kBuw.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Likelihood function if the data points are independent of each other</figcaption></figure><p id="2566" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在保持其他参数不变的情况下，对该方程的每个参数(平均值、方差等)进行求导，得到数据点数值、数据点数和每个参数之间的<strong class="kw iu">关系。</strong></p><p id="1b09" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看一个使用正态分布和一个简单的男性身高数据集的例子。</p><h1 id="106c" class="me ly it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">使用正态分布深入了解最大似然法的数学原理</strong></h1><p id="f978" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">让我们来看一个例子，如何使用最大似然法来拟合一组数据点的正态分布，其中只有一个特征:以厘米为单位的身高。正如我们之前提到的，我们需要计算一些参数:<strong class="kw iu">平均值和方差。</strong></p><p id="0f5f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为此，我们必须知道正态分布的<strong class="kw iu">密度函数:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/9da366b3e92afd1da4f6031e7fc04829.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*LWmCtK8YmZXxEbcJthNd0g.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Density function for the normal distribution. Source <a class="ae lq" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><p id="b1b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦我们知道了这一点，我们就可以计算每个数据点的似然函数。对于第一个数据点，它将是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b84c07682a4e2c074f0ff13b03abf08e.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*kNShLYRR47yo0-h_l95DRg.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Likelihood equation for the first data point</figcaption></figure><p id="38c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于整个数据集，考虑到我们的数据点是独立的，因此我们可以将似然函数计算为各个点的似然性的乘积，它将是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/a4c44057830c5d706928d57d9222100a.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*9dcCRRNey95whKrFlzqzGg.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Likelihood equation for the whole dataset</figcaption></figure><p id="f325" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以用这个函数和<strong class="kw iu">以对数方式表达它</strong>，这有助于后验计算和<strong class="kw iu">产生完全相同的结果。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/73fc0fff286a2e58a1cc873a50310bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*JXCF2ZHRppfC4yqbI8eypg.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Same equation expressed in a logarithmic way</figcaption></figure><p id="b1f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们将似然函数相对于平均值的导数设置为零，得到一个表达式，其中我们<strong class="kw iu">获得第一个参数的值:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/d12a6ae5bbaa2723f64eb446187f60f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*vmrP4Uqw6gke_4pNYH5xVw.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Derivative of the likelihood function for the mean, and Maximum Likelihood value for this parameter</figcaption></figure><p id="adc9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">惊喜！</strong>正态分布均值的最大似然估计就是我们直观预期的<strong class="kw iu"/>:每个数据点的值之和除以数据点的数量。</p><p id="28d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们已经计算了平均值的估计值，是时候为其他相关参数做同样的事情了:<strong class="kw iu">方差</strong>。为此，就像以前一样，我们在似然函数中求导，目标是找到使观察数据的似然性最大化的方差值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/a9932dab77b9371498074ce3a121f329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*CIDOM88j0DlhEucHa5BvYQ.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Maximum likelihood estimate for the variance</figcaption></figure><p id="ccc8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这与上一个案例一样，<strong class="kw iu">给我们带来了同样的结果，我们熟悉每天的统计数据。</strong></p><p id="65c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就是这样！我们已经看到了计算正态分布的最大似然估计背后的一般数学和程序。<strong class="kw iu">最后，让我们看一个快速的数字示例！</strong></p><h1 id="2305" class="me ly it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">男性身高的最大似然估计:一个数值例子</h1><p id="f8e9" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">让我们举一个我们之前提到过的非常简单的例子:<strong class="kw iu">我们有一个某个地区男性身高的数据集，我们想使用最大似然法找到它的最优分布。</strong></p><p id="3245" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们没记错的话，第一步(在收集和理解数据之后)是<strong class="kw iu">选择我们想要估计的密度函数的形状。</strong>在我们的例子中，对于身高，我们将使用高斯分布，这也是我们在 ML 数学背后的一般推理中看到的。让我们重新看看定义这种分布的公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/9da366b3e92afd1da4f6031e7fc04829.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*LWmCtK8YmZXxEbcJthNd0g.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Density function for the normal distribution. Source <a class="ae lq" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><p id="abdb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，让我们只恢复数据集的一个点的似然函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b84c07682a4e2c074f0ff13b03abf08e.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*kNShLYRR47yo0-h_l95DRg.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Likelihood equation for the first data point</figcaption></figure><p id="5321" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">想象我们的<strong class="kw iu">数据向量 X </strong>，在我们的例子中如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/d52b5972c22b0ac4fd5d79c88da91ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*ZcZUDFReo3vp7CmN12Bc2w.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Data vector of male heights</figcaption></figure><p id="c4c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们有<strong class="kw iu"> 10 个数据点(n = 10) </strong>和<strong class="kw iu">每个数据点(d=1) </strong>一个特征。如果在上面所示的公式中，我们为每个数据点代入它们的实际值，我们会得到如下结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/180857e9da39858aad76992f63e661f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*R07TACEDR4LIUY2mWqFRrQ.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Likelihood of the first two data points</figcaption></figure><p id="808d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果在这些公式中，我们<strong class="kw iu">选择一个特定的平均值和方差值</strong>，我们将获得<strong class="kw iu">观察到具有这些特定平均值和方差值</strong>的每个高度值(在我们的例子中是 176 和 172cm)<strong class="kw iu">的可能性。例如，如果我们选择 180 厘米的平均值，方差为 4 厘米，我们将得到上面所示两点的如下可能性:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/4017fbacd97a3b2d670c45f7d63bf466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*W5dk3mAxsEiBdcfmi_CN1A.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Calculations of likelihood of observing points of 176 cm and 172 cm of height on a normal distribution with a mean of 180 cm and a variance of 4 cm</figcaption></figure><p id="37d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">简要说明后，如果我们继续该过程以获得最适合数据集的最大似然估计值，<strong class="kw iu">我们将不得不首先计算平均值</strong>。对于我们的例子来说，非常简单:我们只需将数据点的值相加，然后将这个和除以数据点的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ce08189d3ad6f5a95114623b6ab716f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*1UD0NUsCFF2hIeQC02fhCA.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Maximum likelihood estimate for the mean of our height data set</figcaption></figure><p id="8d32" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们<strong class="kw iu">对方差</strong>做同样的处理，计算每个数据点的值减去平均值的平方和，并除以我们得到的总点数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2fd29872c67b0e9fa840b78e5dd23710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*b25dCDbumQNg0tRHFbw_EQ.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Variance and Standard deviation estimates for our height data set</figcaption></figure><p id="8167" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">就是它！</strong>现在我们已经计算了平均值和方差，我们已经拥有了建模分布所需的所有参数。现在，当我们得到一个<strong class="kw iu">新数据点时，例如，一个高度为 177 cm </strong>的数据点，我们可以看到该点属于我们数据集的可能性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a70a82c617cc2df54e75dd7105ddecf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*B9PDeFyhc-yd5dYJKuGx0A.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Likelihood of the new data point belonging to our data set</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/ce8d21b99b8ca35d6fe3a1ca840e2781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YgEQTf-STy1MF9hXQQTkwg.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Representation of the obtained normal distribution and the likelihood of the new data point</figcaption></figure><p id="893d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，如果我们有另一组数据，例如女性身高，我们做同样的程序，我们会有两个身高分布:一个是男性，一个是女性。</p><p id="4869" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这样，我们可以<strong class="kw iu">使用两种分布来解决男性和女性身高的二元分类问题</strong>:当我们获得一个新的未标记身高数据点时，我们计算该新数据点<strong class="kw iu">属于两种分布</strong>的概率，并将其分配给该分布产生最高概率的类别(男性或女性)<strong class="kw iu">。</strong></p><h1 id="d6da" class="me ly it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">结论</h1><p id="4be3" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">我们已经看到了<strong class="kw iu">什么是最大似然，</strong>它背后的数学原理，以及如何应用它来解决现实世界的问题。这给了我们解决下一个问题的基础，你们都一直在问:<strong class="kw iu">贝叶斯定理背后的数学</strong>，它非常类似于最大似然法。</p><p id="2f2e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">来看看吧<a class="ae lq" href="https://medium.com/@jaimezornoza" rel="noopener"> <strong class="kw iu">关注我的</strong> </a>，敬请关注！</p><p id="b590" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就这些，我希望你喜欢这个帖子。请随时在 LinkedIn 上与我联系，或者在 Twitter 上关注我，地址是:jaimezorno。还有，你可以看看我其他关于数据科学和机器学习的帖子<strong class="kw iu"> </strong> <a class="ae lq" href="https://medium.com/@jaimezornoza" rel="noopener"> <strong class="kw iu">这里</strong> </a>。好好读！</p><h1 id="0b15" class="me ly it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">额外资源</h1><p id="d619" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">如果你想更深入地了解最大似然法和机器学习，请查看以下资源:</p><ul class=""><li id="e7f0" class="nv nw it kw b kx ky la lb ld nx lh ny ll nz lp oa ob oc od bi translated"><a class="ae lq" href="https://www.youtube.com/watch?v=XepXtl9YKwc" rel="noopener ugc nofollow" target="_blank">带有非常清晰的解释和最大可能性示例的视频</a></li><li id="955a" class="nv nw it kw b kx oe la of ld og lh oh ll oi lp oa ob oc od bi translated"><a class="ae lq" href="https://www.youtube.com/watch?v=Dn6b9fCIUpM" rel="noopener ugc nofollow" target="_blank">关于正态分布最大似然的非常详细的视频</a></li><li id="a903" class="nv nw it kw b kx oe la of ld og lh oh ll oi lp oa ob oc od bi translated"><a class="ae lq" href="https://www.univ-orleans.fr/deg/masters/ESA/CH/Chapter2_MLE.pdf" rel="noopener ugc nofollow" target="_blank">最大似然幻灯片</a></li></ul><p id="b61b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一如既往，有任何问题请联系我。祝你有美好的一天，继续学习。</p></div></div>    
</body>
</html>