# 你的汽车可能不知道什么时候停下来——对自动驾驶汽车的敌对攻击

> 原文：<https://towardsdatascience.com/your-car-may-not-know-when-to-stop-adversarial-attacks-against-autonomous-vehicles-a16df91511f4?source=collection_archive---------20----------------------->

![](img/67e792b615079ad277a02aed50dd4be0.png)

Photo by [Kevin Lee](https://unsplash.com/@kevin_lee?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

自动驾驶汽车代表了高速、清洁和高效交通的乌托邦梦想。自动驾驶汽车的时代可能即将到来，优步、马斯克的特斯拉(Tesla)和谷歌(Google)的力量等公司寻求成为第一个将自动驾驶汽车坚定地放在路上供所有人使用的公司。自动驾驶汽车最大的症结是安全——我们如何确保这些机器能够理解驾驶的复杂性，以及它们如何能够应对它们肯定会遇到的各种各样以前没有遇到过的场景？

# 作为汽车核心的电脑

自动驾驶汽车的即将到来部分是由于机器学习，特别是深度学习最近的成功。随着计算机的体积不断缩小，同时能力不断增强，计算机科学家在计算机视觉等领域取得了突飞猛进的发展。深度学习允许计算机从大量数据中学习模式，然后可以利用这些模式进行决策。

在自动驾驶汽车中，深度学习系统处理来自汽车传感器阵列的输入数据，并有助于车辆的高级控制。深度学习系统的一个元素是对汽车周围的物体进行分类，例如识别行人、骑自行车的人、交通标志等。，使其能够安全导航并遵循道路规则。

# 深度学习系统中的漏洞

随着深度学习系统被关键地嵌入到任何自动驾驶汽车的中枢神经系统中，人们会希望这种系统能够高度准确和强健，防止被故意误导。不幸的是，实验表明，深度学习图像分类系统非常容易受到精心制作的敌对攻击。通过向图像添加精心选择的噪声层，有可能导致深度学习系统对图像进行彻底的错误分类。

![](img/9905f8649f8232077362c26625f006ea.png)

Digital Adversarial Attacks confuse a Deep Learning system with imperceptible changes (from [OpenAI](https://openai.com/blog/adversarial-example-research/))

在数字领域，攻击者获取一张图像和一个目标深度学习系统，窥视深度学习模型的内部工作，并以某种方式转换图像，使计算机确信图像现在显示出不同的东西。显然，攻击者可以彻底改变图像，以改变深度学习分类的结果；相反，挑战是尽可能少地改变图像，或者以人类无法察觉的方式改变图像，但机器会出错。

![](img/c3d5e91a51b76701dc20a5d0c2334206.png)

Digital Adversarial Attacks are also effective at confusing the Deep Learning Systems in cars ([source](https://arxiv.org/abs/1904.07370))

# 身体对抗攻击

在实践中，数字对抗性攻击用于自动驾驶汽车是非常不可行的——攻击者必须能够拦截汽车传感器的传输，并在图像被移交给深度学习系统之前干扰图像。一个更可行的方法是改变物理环境，而不仅仅是车辆对它的感知。通过篡改路标，有可能混淆深度学习系统，对这些标志进行错误分类，即使它们仍然完全可以被人类理解。[研究人员已经成功误导了真实车辆中的交通标志分类系统](https://arxiv.org/abs/1707.08945)，通过使用旨在模仿标志上涂鸦的黑色贴纸，在 84%的时间里诱导了错误的标签。

![](img/c272f68f463c50fa378080a3b72d6d6c.png)

Example Physical Adversarial Attacks that are sufficient to drastically confuse a Deep Learning system ([source](https://arxiv.org/abs/1707.08945))

物理攻击带来了一些新的挑战。因为有许多不同的方式可以观察到一个物体，任何物理变化在不同的视角和距离下仍然有效。这些改变还必须足够显著，能够被摄像头捕捉到——再见了，数字敌对攻击中使用的细微噪声层。

然而，物理攻击的结果是，我们期望物体是不完美的——我们经常习惯于看到路标上的标记、贴纸和涂鸦，但我们仍然能够正确识别它们并安全驾驶。与数字攻击不同，数字攻击的目的是以最微妙的方式扰乱想象，物理攻击有更大的变化空间。

# 对自动驾驶未来的希望

那么，如果你想让你的车能够正确地阅读停车标志，该怎么做呢？消除对任何和所有符号的干扰是一个选择，但那是相当大的开销。一个更合理的方法是开发对这种攻击具有鲁棒性的机器学习系统，主要是通过训练它们正确识别对立的例子——一种暴露疗法。不幸的是，我们狡猾的攻击者可能只是创造了另一种系统没有见过的对抗方法，尽管这一次对他们来说可能会更难。

冗余是一个很好的解决方案；深度学习系统虽然非常重要，但仍然只是汽车控制系统的一部分。像 GPS 这样的其他系统可以用来验证深度学习系统的结果——就此打住有意义吗；限速提高与我掌握的道路交通数据相符吗？不幸的是，这并不包括环境中的动态元素，如行人，但它仍然是对篡改交通标志的例子的有力反驳。

制造对抗性攻击的一个缺点是，它们通常需要白盒模型:访问模型的内部工作方式，比如特定的参数和架构。幸运的是，这些功能受到自动驾驶汽车制造商的高度保护(既是出于安全原因，也是因为它们是自动驾驶汽车开发的最高宝石)，使得攻击者很难获得这些功能来策划他们的攻击。然而，对针对黑盒模型的对抗性攻击的研究正在进行中，所以我们还没有完全脱离险境。最终，任何有助于提高自动驾驶汽车中使用的机器学习系统的鲁棒性的方法都将是一种受欢迎的祝福。