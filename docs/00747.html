<html>
<head>
<title>Using Object Detection for Complex Image Classification Scenarios Part 4:</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将对象检测用于复杂的图像分类场景第 4 部分:</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-object-detection-for-complex-image-classification-scenarios-part-4-3e5da160d272?source=collection_archive---------14-----------------------#2019-02-04">https://towardsdatascience.com/using-object-detection-for-complex-image-classification-scenarios-part-4-3e5da160d272?source=collection_archive---------14-----------------------#2019-02-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a1bf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 Keras RetinaNet 进行策略检测</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/ae317347ccf3a1b22cc44765c3cbcbf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/0*_LmsQxkNDLnAjG47.jpg"/></div></figure><p id="ae3c" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">TLDR；本系列基于在下面的<a class="ae lj" href="https://www.microsoft.com/developerblog/2017/07/31/using-object-detection-complex-image-classification-scenarios/" rel="noopener ugc nofollow" target="_blank">现实生活代码故事</a>中检测复杂策略的工作。该系列的代码可以在<a class="ae lj" href="https://github.com/aribornstein/cvworkshop" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="3292" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">对象检测简介</h1><p id="233c" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated"><a class="ae lj" rel="noopener" target="_blank" href="/using-object-detection-for-complex-image-classification-scenarios-part-3-770d3fc5e3f7">到目前为止，我们一直在对原始图像进行分类</a>如果我们可以使用这些图像来生成有针对性的特征，以帮助我们应对高级分类挑战，那会怎么样。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/51d60df4f03f0195b6b6e256ab5b41a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KZFYJBZg1Mgpv7Lx"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">We will use object detection to extract features for more complex classification <a class="ae lj" href="https://mathematica.stackexchange.com/questions/141598/object-detection-and-localization-using-neural-network" rel="noopener ugc nofollow" target="_blank">Src</a></figcaption></figure><h1 id="32ff" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">端到端对象检测管道:</h1><p id="111a" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">对象检测需要大量带注释的数据。传统上，这是一项非常手动密集型的任务，理想的管道将标记与模型训练相结合，以实现主动学习。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mq"><img src="../Images/75199dbe488069f4080a4402dceea065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-ohmhZJJ0qinaatg"/></div></div></figure><h1 id="61e1" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">视觉对象标记工具(VoTT)</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mr"><img src="../Images/7a65fbb741a10b9392c87b49b370186d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MgHA-ntaWLRvp_ir"/></div></div></figure><p id="e95c" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">视觉对象标记工具 VoTT 为从视频和图像资产生成数据集和验证对象检测模型提供端到端支持。</p><p id="6ed3" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">VoTT 支持以下<strong class="kp ir">功能</strong>:</p><ul class=""><li id="4e40" class="ms mt iq kp b kq kr kt ku kw mu la mv le mw li mx my mz na bi translated">标记和注释图像目录或独立视频的能力。</li><li id="e795" class="ms mt iq kp b kq nb kt nc kw nd la ne le nf li mx my mz na bi translated">使用<a class="ae lj" href="http://opencv.jp/opencv-1.0.0_org/docs/papers/camshift.pdf" rel="noopener ugc nofollow" target="_blank"> Camshift 跟踪算法</a>对视频中的对象进行计算机辅助标记和跟踪。</li><li id="b1ac" class="ms mt iq kp b kq nb kt nc kw nd la ne le nf li mx my mz na bi translated">将标签和资源导出为 Tensorflow (PascalVOC)或 YOLO 格式，用于训练对象检测模型。</li><li id="9e64" class="ms mt iq kp b kq nb kt nc kw nd la ne le nf li mx my mz na bi translated">在新视频上运行和验证经过训练的对象检测模型，以利用 docker 端点生成更强的模型。</li></ul><h1 id="aabf" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">区域提议目标检测算法的基本进展</h1><p id="cae2" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">以下部分将简要强调将用于该任务的区域提议对象检测的进展。对于更深入的教程，我强烈推荐乔纳森·惠的《中级系列》</p><div class="ni nj gp gr nk nl"><a href="https://medium.com/@jonathan_hui/object-detection-series-24d03a12f904" rel="noopener follow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">物体探测系列</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">概观</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">medium.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz kl nl"/></div></div></a></div><h1 id="45b2" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">滑动窗接近和后退</h1><p id="92d7" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">本系列的最后两篇文章是关于图像分类的。将图像分类器转换成对象检测器的最简单方法是在给定图像上使用一系列不同维度的滑动窗口。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi oa"><img src="../Images/fb864872d569d1cfc8b411e248a0de91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uHKtcMbRDyWrHJbog9brQg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk"><a class="ae lj" href="https://medium.com/@jonathan_hui/object-detection-series-24d03a12f904" rel="noopener">Sliding Window Source</a></figcaption></figure><p id="2b26" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果我们认为在给定的窗口中有一个我们正在寻找的对象，那么我们可以返回被捕获对象的尺寸。</p><h1 id="7526" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">地区有线电视新闻网</h1><p id="fd23" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">不幸的是，滑动窗口方法虽然简单，但也有一些缺点。评估多个窗口大小很慢，而且不精确，因为我们事先不知道图像中每个对象的正确窗口大小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ob"><img src="../Images/b106a5e8122d907dec24f74638f99165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h8nxxx6sP27Fk-WE"/></div></div></figure><p id="8177" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果我们可以只对可能包含对象的感兴趣区域执行分类，而不是在整个图像上使用滑动窗口，会怎么样？这是区域提议对象检测器背后的主要直觉。传统上，我们使用一种叫做选择性搜索的算法来提出感兴趣的区域。</p><h1 id="e09d" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">快速 RCNN</h1><p id="f760" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">虽然传统的 RCNN 方法在准确性方面工作良好，但是它的计算成本非常高，因为必须对每个感兴趣的区域评估神经网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi oc"><img src="../Images/4c833a9a61a3d8db9ee9e47150e6b05e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*r1SLKGyI3WCEpmkF"/></div></div></figure><p id="676f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">快速 R-CNN 通过对每个图像只评估网络的大部分(具体来说:卷积层)一次来解决这个缺点。根据作者的说法，这导致测试期间的速度提高了 213 倍，训练期间的速度提高了 9 倍，而没有损失准确性。</p><h1 id="1c74" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">更快的 RCNN</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi od"><img src="../Images/1b440a2e95ee597b91d5417bc9069369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bJxvdlKYzCTtGk4t"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Faster RCNN architecture <a class="ae lj" href="https://jhui.github.io/assets/rcnn/st8.png" rel="noopener ugc nofollow" target="_blank">src</a></figcaption></figure><p id="4c4d" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">更快的 R-CNN 建立在以前的工作基础上，使用深度卷积网络有效地对对象提议进行分类。与以前的工作相比，更快的 R-CNN 采用了一个区域建议网络，它不需要对候选区域建议进行选择性搜索。</p><h1 id="1ad1" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">RetinaNet</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi oe"><img src="../Images/56824cddadf8a34cdefc444ecedb4b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RuoonaayBbvBIAGI"/></div></div></figure><p id="7f59" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">RetinaNet 是一个对象检测器，它建立在更快的 RCNN 的直觉基础上，提供了特征金字塔和优化的焦损失，实现了比 Faster RCNN 更快的评估时间，并提供了<a class="ae lj" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank">焦损失</a>，有助于防止过度拟合背景类。在撰写本文时，RetinaNet 是当前最先进的区域求婚网络。</p><p id="450c" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">更多信息请参见:</p><div class="ni nj gp gr nk nl"><a href="https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d" rel="noopener follow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">RetinaNet 背后的直觉</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">这篇博文的最终目的是让读者直观地了解 RetinaNet 的深层工作原理。</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">medium.com</p></div></div><div class="nu l"><div class="of l nw nx ny nu nz kl nl"/></div></div></a></div><h1 id="03bb" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">让我们用视网膜网络来解决我们的挑战</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi og"><img src="../Images/d540b046f8d6e14d195511aad34182ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*88sw0P1OZUWQtJRM"/></div></div></figure><p id="e72a" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">VoTT 可用于生成直接数据集，这些数据集可与 Azure 机器学习一起使用，以训练自定义对象检测模型。</p><div class="ni nj gp gr nk nl"><a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python?WT.mc_id=blog-medium-abornst" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">快速入门:Python - Azure 机器学习服务入门</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">Python 中的 Azure 机器学习服务入门。使用 Python SDK 创建一个工作空间，它是…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">docs.microsoft.com</p></div></div><div class="nu l"><div class="oh l nw nx ny nu nz kl nl"/></div></div></a></div><p id="2411" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">然而，由于培训 RetinaNet 需要访问 N 系列 GPU 机器，出于时间的考虑，并确保本教程仅针对 CPU，我冒昧地对模型进行了预培训。在下一篇文章中，我们将讨论如何用<a class="ae lj" href="https://docs.microsoft.com/azure/machine-learning/service/quickstart-create-workspace-with-python?WT.mc_id=cvworkshop-github-abornst" rel="noopener ugc nofollow" target="_blank"> Azure 机器学习服务</a>来训练这些模型。</p><h1 id="8473" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">让我们看看如何使用我们定制的预训练对象检测模型</h1><pre class="kg kh ki kj gt oi oj ok ol aw om bi"><span id="b7e4" class="on ll iq oj b gy oo op l oq or"><em class="os"># import keras_retinanet</em><br/><strong class="oj ir">import</strong> <strong class="oj ir">keras</strong><br/><strong class="oj ir">from</strong> <strong class="oj ir">keras_retinanet</strong> <strong class="oj ir">import</strong> models<br/><strong class="oj ir">from</strong> <strong class="oj ir">keras_retinanet.utils.image</strong> <strong class="oj ir">import</strong> read_image_bgr, preprocess_image, resize_image<br/><strong class="oj ir">from</strong> <strong class="oj ir">keras_retinanet.utils.visualization</strong> <strong class="oj ir">import</strong> draw_box, draw_caption<br/><strong class="oj ir">from</strong> <strong class="oj ir">keras_retinanet.utils.colors</strong> <strong class="oj ir">import</strong> label_color</span><span id="b02d" class="on ll iq oj b gy ot op l oq or"><em class="os"># load image</em><br/><strong class="oj ir">def</strong> evaluate_single_image(model, img_path):<br/>    image = read_image_bgr(img_path)</span><span id="e669" class="on ll iq oj b gy ot op l oq or">    <em class="os"># preprocess image for network</em><br/>    image = preprocess_image(image)<br/>    image, scale = resize_image(image)</span><span id="568d" class="on ll iq oj b gy ot op l oq or">    <em class="os"># process image</em><br/>    start = time.time()<br/>    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))<br/>    print("processing time: ", time.time() - start)</span><span id="7410" class="on ll iq oj b gy ot op l oq or">    <em class="os"># correct for image scale</em><br/>    boxes /= scale<br/>    <br/>    <strong class="oj ir">return</strong> (boxes[0], scores[0], labels[0])</span><span id="5c79" class="on ll iq oj b gy ot op l oq or"><strong class="oj ir">def</strong> visualize_detection(img_path, model_results):<br/>    image = read_image_bgr(img_path)<br/>    boxes, scores, labels = model_results</span><span id="3a26" class="on ll iq oj b gy ot op l oq or">    <em class="os"># visualize detections</em><br/>    draw = image.copy()<br/>    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)</span><span id="d912" class="on ll iq oj b gy ot op l oq or">    <strong class="oj ir">for</strong> box, score, label <strong class="oj ir">in</strong> zip(boxes, scores, labels):<br/>        <em class="os"># scores are sorted so we can break</em><br/>        <strong class="oj ir">if</strong> score &lt; 0.5:<br/>            <strong class="oj ir">break</strong></span><span id="5bbc" class="on ll iq oj b gy ot op l oq or">        color = label_color(label)<br/>        b = box.astype(int)<br/>        draw_box(draw, b, color=color)</span><span id="6d0f" class="on ll iq oj b gy ot op l oq or">        caption = "<strong class="oj ir">{}</strong> <strong class="oj ir">{:.3f}</strong>".format(labels_to_names[label], score)<br/>        draw_caption(draw, b, caption)</span><span id="0f5d" class="on ll iq oj b gy ot op l oq or">    plt.figure(figsize=(15, 15))<br/>    plt.axis('off')<br/>    plt.imshow(draw)<br/>    plt.show()</span><span id="1fee" class="on ll iq oj b gy ot op l oq or"><em class="os"># load retinanet model</em><br/>soda_model = models.load_model('models/retina_net_soda.h5', backbone_name='resnet50')<br/>labels_to_names = {0: 'shelf1', 1: 'shelf2', 2: 'shelf3'}</span><span id="c4ad" class="on ll iq oj b gy ot op l oq or">valid_example_path = 'dataset/Beverages/Test/Valid/IMG_4187.JPG'<br/>detection_results = evaluate_single_image(soda_model, valid_example_path)<br/>visualize_detection(valid_example_path, detection_results)</span></pre></div><div class="ab cl ou ov hu ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="ij ik il im in"><pre class="oi oj ok ol aw om bi"><span id="cfd5" class="on ll iq oj b gy pb pc pd pe pf op l oq or">processing time:  10.065604209899902</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi pg"><img src="../Images/c64e9014cfebd21de87067512e1803fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOxbo_zEaPuEWG4M_teHIQ.png"/></div></div></figure><p id="eba6" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">挑战:你能想出我们可以用这些盒子来表明政策无效的方法吗？</p><h1 id="bdf7" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">用瓶子启发式预测</h1><p id="2b54" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">现在，我们有了一个用于寻找瓶子的对象检测模型，让我们开发一个启发式方法来确定货架是否正确存储，并在两个样本图像上测试它。</p><p id="b275" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们的试探法将按如下方式工作:</p><ul class=""><li id="1cd6" class="ms mt iq kp b kq kr kt ku kw mu la mv le mw li mx my mz na bi translated">我们将找到每个架类的最小 y1 和最大 y2 的用法</li><li id="6bb3" class="ms mt iq kp b kq nb kt nc kw nd la ne le nf li mx my mz na bi translated">我们将确认汽水架在果汁架之上，果汁架在水架之上</li><li id="2df8" class="ms mt iq kp b kq nb kt nc kw nd la ne le nf li mx my mz na bi translated">对于每个最大值，我们将确保在它们之间没有其他类</li></ul><pre class="kg kh ki kj gt oi oj ok ol aw om bi"><span id="9b5f" class="on ll iq oj b gy oo op l oq or"><strong class="oj ir">def</strong> predict_bottles(model_results):<br/>    bounds = {}<br/>    beverages = {0: [], 1: [], 2: []}<br/>    boxes, scores, labels = model_results</span><span id="18bb" class="on ll iq oj b gy ot op l oq or">    <strong class="oj ir">for</strong> box, score, label <strong class="oj ir">in</strong> zip(boxes, scores, labels):<br/>        <em class="os"># scores are sorted so we can break</em><br/>        <strong class="oj ir">if</strong> score &lt; 0.5:<br/>            <strong class="oj ir">break</strong><br/>        beverages[label].append(box)<br/>        <br/>    <em class="os"># Find the use the min y1 and max y2 of each of the tag classes</em><br/>    <strong class="oj ir">for</strong> bev <strong class="oj ir">in</strong> beverages:<br/>        <strong class="oj ir">if</strong> len(beverages[bev]) == 0:<br/>            <strong class="oj ir">return</strong> <strong class="oj ir">False</strong><br/>        y1 = min(beverages[bev], key=<strong class="oj ir">lambda</strong> b: b[1])[1]<br/>        y2 = max(beverages[bev], key=<strong class="oj ir">lambda</strong> b: b[3])[3]<br/>        bounds[bev] = {"y1":y1, "y2":y2} <br/>    <em class="os"># Confirm that soda is above juice which is above water</em><br/>    <strong class="oj ir">if</strong> (bounds[0]["y1"] &lt; bounds[1]["y1"]) <strong class="oj ir">and</strong> (bounds[1]["y1"] &lt; bounds[2]["y1"]):<br/>        <em class="os"># For each of the max's we will ensure that there are no other clases that are in between them</em><br/>        <strong class="oj ir">for</strong> b <strong class="oj ir">in</strong> bounds.keys():<br/>            <strong class="oj ir">for</strong> bev_type <strong class="oj ir">in</strong> (set(bounds.keys()) - set([b])):<br/>                <strong class="oj ir">for</strong> bev <strong class="oj ir">in</strong> beverages[bev_type]:<br/>                    <strong class="oj ir">if</strong> bev[1] &gt; bounds[b]["y1"] <strong class="oj ir">and</strong> bev[3] &lt; bounds[b]["y2"]:<br/>                        <strong class="oj ir">return</strong> <strong class="oj ir">False</strong> <br/>        <strong class="oj ir">return</strong> <strong class="oj ir">True</strong><br/>    <strong class="oj ir">else</strong>:<br/>        <strong class="oj ir">return</strong> <strong class="oj ir">False</strong></span><span id="917c" class="on ll iq oj b gy ot op l oq or">visualize_detection(valid_example_path, detection_results)<br/>predict_bottles(detection_results)</span></pre></div><div class="ab cl ou ov hu ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="ij ik il im in"><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi pg"><img src="../Images/c64e9014cfebd21de87067512e1803fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOxbo_zEaPuEWG4M_teHIQ.png"/></div></div></figure><pre class="kg kh ki kj gt oi oj ok ol aw om bi"><span id="81f0" class="on ll iq oj b gy oo op l oq or">True</span><span id="e1fe" class="on ll iq oj b gy ot op l oq or">invalid_example_path = 'dataset/Beverages/Test/Invalid/IMG_4202.JPG'<br/>detection_results = evaluate_single_image(soda_model, invalid_example_path)<br/>visualize_detection(invalid_example_path, detection_results)<br/>predict_bottles(detection_results)</span></pre></div><div class="ab cl ou ov hu ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="ij ik il im in"><pre class="oi oj ok ol aw om bi"><span id="7f6c" class="on ll iq oj b gy pb pc pd pe pf op l oq or">processing time:  8.053469896316528</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ph"><img src="../Images/be24638d3b9dd97fd462d7848aa051ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*taxyphBapwrzJxnknAQJLQ.png"/></div></div></figure><pre class="kg kh ki kj gt oi oj ok ol aw om bi"><span id="31a1" class="on ll iq oj b gy oo op l oq or">False</span></pre><h1 id="b022" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">基准瓶启发式</h1><p id="9254" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">通常，为了将此应用于我们的整个数据集，我们将使用批处理，因为我们试图节省内存，我们将一次评估一个图像。</p><pre class="kg kh ki kj gt oi oj ok ol aw om bi"><span id="8c84" class="on ll iq oj b gy oo op l oq or"><strong class="oj ir">from</strong> <strong class="oj ir">tqdm</strong> <strong class="oj ir">import</strong> tqdm_notebook<br/><strong class="oj ir">from</strong> <strong class="oj ir">utils</strong> <strong class="oj ir">import</strong> classification_report<br/><strong class="oj ir">from</strong> <strong class="oj ir">keras.preprocessing.image</strong> <strong class="oj ir">import</strong> ImageDataGenerator</span><span id="bf37" class="on ll iq oj b gy ot op l oq or">y_pred = []<br/>y_true = []</span><span id="0e0b" class="on ll iq oj b gy ot op l oq or">print("Testing Invalid Cases")</span><span id="b6c8" class="on ll iq oj b gy ot op l oq or">img_dir = 'dataset/Beverages/Test/Invalid/'<br/><strong class="oj ir">for</strong> img_path <strong class="oj ir">in</strong> tqdm_notebook(os.listdir(img_dir)):<br/>    detection_results = evaluate_single_image(soda_model, os.path.join(img_dir, img_path))<br/>    y_pred.append(predict_bottles(detection_results))<br/>    y_true.append(<strong class="oj ir">False</strong>)</span><span id="fe41" class="on ll iq oj b gy ot op l oq or">print("Testing Valid Cases")<br/>img_dir = 'dataset/Beverages/Test/Valid/'<br/><strong class="oj ir">for</strong> img_path <strong class="oj ir">in</strong> tqdm_notebook(os.listdir(img_dir)):<br/>    detection_results = evaluate_single_image(soda_model, os.path.join(img_dir, img_path))<br/>    y_pred.append(predict_bottles(detection_results))<br/>    y_true.append(<strong class="oj ir">True</strong>)</span></pre><h2 id="d248" class="on ll iq bd lm pi pj dn lq pk pl dp lu kw pm pn lw la po pp ly le pq pr ma ps bi translated">分类报告</h2><pre class="kg kh ki kj gt oi oj ok ol aw om bi"><span id="233d" class="on ll iq oj b gy oo op l oq or">classification_report(y_true, y_pred)</span></pre></div><div class="ab cl ou ov hu ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="ij ik il im in"><pre class="oi oj ok ol aw om bi"><span id="7084" class="on ll iq oj b gy pb pc pd pe pf op l oq or">precision    recall  f1-score   support</span><span id="38c5" class="on ll iq oj b gy ot op l oq or">       False       1.00      1.00      1.00        30<br/>        True       1.00      1.00      1.00        30</span><span id="e274" class="on ll iq oj b gy ot op l oq or">   micro avg       1.00      1.00      1.00        60<br/>   macro avg       1.00      1.00      1.00        60<br/>weighted avg       1.00      1.00      1.00        60</span><span id="e55e" class="on ll iq oj b gy ot op l oq or">Confusion matrix, without normalization<br/>[[30  0]<br/> [ 0 30]]<br/>Normalized confusion matrix<br/>[[1. 0.]<br/> [0. 1.]]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/3b5e0cf629a027007aab26c9fc33803d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*tmRFkSJpnG088vJqKSLkyQ.png"/></div></figure><h1 id="4737" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">结论</h1><p id="799b" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">我们可以看到，对于某些任务，使用具有良好启发性的对象检测可以优于定制视觉服务。然而，重要的是要考虑注释我们的数据和构建对象检测模型所需的权衡和工作。</p><h1 id="fcde" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">资源</h1><div class="ni nj gp gr nk nl"><a href="https://medium.com/microsoftazure/the-pythic-coders-recommended-content-for-getting-started-with-machine-learning-on-azure-fcd1c5a8dbb4" rel="noopener follow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">Pythic Coder 推荐的 Azure 机器学习入门内容</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">Tldr 由于 DevOps 资源上的帖子很受欢迎，而且很难找到文档，所以我…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">medium.com</p></div></div><div class="nu l"><div class="pu l nw nx ny nu nz kl nl"/></div></div></a></div><div class="ni nj gp gr nk nl"><a href="https://github.com/aribornstein" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">aribornstein —概述</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">@ pythiccoder。aribornstein 有 68 个存储库。在 GitHub 上关注他们的代码。</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">github.com</p></div></div><div class="nu l"><div class="pv l nw nx ny nu nz kl nl"/></div></div></a></div><div class="ni nj gp gr nk nl"><a href="https://azure.microsoft.com/en-us/services/cognitive-services/?v=18.44a" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">认知服务|微软 Azure</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">微软 Azure Stack 是 Azure 的扩展——将云计算的灵活性和创新性带到您的…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">azure.microsoft.com</p></div></div><div class="nu l"><div class="pw l nw nx ny nu nz kl nl"/></div></div></a></div><div class="ni nj gp gr nk nl"><a href="https://medium.com/@jonathan_hui/object-detection-series-24d03a12f904" rel="noopener follow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">物体探测系列</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">概观</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">medium.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz kl nl"/></div></div></a></div><h1 id="6500" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">以前的帖子:</h1><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/using-object-detection-for-complex-image-classification-scenarios-part-1-779c87d1eecb"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">在复杂图像分类场景中使用对象检测第 1 部分:</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">人工智能计算机视觉革命</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="px l nw nx ny nu nz kl nl"/></div></div></a></div><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/using-object-detection-for-complex-image-classification-scenarios-part-2-54a3a7c60a63"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">将对象检测用于复杂的图像分类场景第 2 部分:</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">定制视觉服务</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="px l nw nx ny nu nz kl nl"/></div></div></a></div><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/using-object-detection-for-complex-image-classification-scenarios-part-3-770d3fc5e3f7"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">将对象检测用于复杂的图像分类场景第 3 部分:</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">利用 MobileNet 和迁移学习进行策略识别</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="px l nw nx ny nu nz kl nl"/></div></div></a></div><h1 id="3039" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">下一篇文章</h1><p id="1189" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">本系列的下一篇文章将回顾如何训练你自己的对象检测模型使用 Azure ML 服务的云，后续文章将讨论部署。</p><p id="5c0f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果您有任何问题、评论或希望我讨论的话题，请随时在<a class="ae lj" href="https://twitter.com/pythiccoder" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我。如果您认为我错过了某个里程碑，请告诉我。</p><h2 id="3378" class="on ll iq bd lm pi pj dn lq pk pl dp lu kw pm pn lw la po pp ly le pq pr ma ps bi translated">关于作者</h2><p id="4e4b" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">亚伦(阿里) 是一个狂热的人工智能爱好者，对历史充满热情，致力于新技术和计算医学。作为微软云开发倡导团队的开源工程师，他与以色列高科技社区合作，用改变游戏规则的技术解决现实世界的问题，然后将这些技术记录在案、开源并与世界其他地方共享。</p></div></div>    
</body>
</html>