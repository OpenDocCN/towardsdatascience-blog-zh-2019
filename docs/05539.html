<html>
<head>
<title>Spectral Graph Convolution Explained and Implemented Step By Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逐步解释和实现光谱图卷积</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801?source=collection_archive---------2-----------------------#2019-08-15">https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801?source=collection_archive---------2-----------------------#2019-08-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6ccb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">作为“计算机视觉图形神经网络教程”的一部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/720f32881d9a92dfa56d14848d9d6701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vx6uqv12rzb8HeUZl8d7-g.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The Fourier basis (DFT matrix) on the left, in which each column or row is a basis vector, reshaped to 28×28 (on the right), i.e. 20 basis vectors are shown on the right. The Fourier basis is used to compute spectral convolution is signal processing. In graphs, the Laplacian basis is used described in this post.</figcaption></figure><p id="5977" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，让我们回忆一下什么是图。图<em class="lu"> G </em>是由有向/无向<strong class="la iu">边</strong>连接的一组<strong class="la iu">节点</strong>(顶点)。在这篇文章中，我将假设一个无向图<em class="lu"> G </em>有<em class="lu"> N </em>个节点。该图中的每个<strong class="la iu">节点</strong>都有一个<em class="lu"> C </em>维特征向量，所有节点的特征都表示为一个<em class="lu"> N </em> × <em class="lu"> C </em>维矩阵<em class="lu"> X⁽ˡ⁾.图的</em> <strong class="la iu">边</strong>表示为一个<em class="lu"> N </em> × <em class="lu"> N </em>矩阵 a，其中条目 A <em class="lu"> ᵢⱼ </em>表示节点<em class="lu"> i </em>是否连接(T30 邻接)到节点<em class="lu"> j </em>。这个矩阵被称为<em class="lu">邻接矩阵</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/9c7d132e1e5e715f1364be7afbb96d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*68Gcr70UTpdaX7THbZSEcA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Two undirected graphs with N=5 and N=6 nodes. The order of nodes is arbitrary.</figcaption></figure><p id="5deb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图的谱分析(参见课堂讲稿<a class="ae lw" href="http://www.cs.yale.edu/homes/spielman/561/" rel="noopener ugc nofollow" target="_blank">这里</a>和早期的工作<a class="ae lw" href="https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering" rel="noopener ugc nofollow" target="_blank">这里</a>)已经对图聚类、社区发现和其他<em class="lu">主要是无监督的</em>学习任务有用。在这篇文章中，我主要描述了<a class="ae lw" href="https://arxiv.org/abs/1312.6203" rel="noopener ugc nofollow" target="_blank">布鲁纳等人，2014，ICLR 2014 </a>的工作，他们将谱分析与卷积神经网络(ConvNets)相结合，产生了谱<strong class="la iu">图卷积网络</strong>，它可以以<em class="lu">监督</em>的方式进行训练，例如用于图分类任务。</p><p id="b415" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管<em class="lu">光谱</em>图形卷积目前与<em class="lu">空间</em>图形卷积方法相比使用较少，但了解光谱卷积的工作原理仍然有助于理解和避免其他方法的潜在问题。此外，在结论中，我提到了一些最近令人兴奋的工作，使谱图卷积更具竞争力。</p><h1 id="408e" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">1.拉普拉斯图和一点物理知识</h1><p id="f4d4" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">虽然“频谱”听起来可能很复杂，但对于我们的目的来说，理解它仅仅意味着<em class="lu">将信号/音频/图像/图形分解为简单元素(小波、graphlets)的组合(通常是总和)就足够了。为了使这种<em class="lu">分解</em>具有一些好的特性，这些简单元素通常是<em class="lu">正交</em>，即相互线性独立，因此形成了<em class="lu">基</em>。</em></p><p id="4ada" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们谈论信号/图像处理中的“频谱”时，我们指的是<a class="ae lw" href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform" rel="noopener ugc nofollow" target="_blank">傅立叶变换</a>，它为我们提供了不同频率的基本正弦和余弦波的特定<em class="lu">基</em> ( <a class="ae lw" href="https://en.wikipedia.org/wiki/DFT_matrix" rel="noopener ugc nofollow" target="_blank"> DFT 矩阵</a>，例如 Python 中的<code class="fe mu mv mw mx b">scipy.linalg.dft</code>)，因此我们可以将信号/图像表示为这些波的总和。但是当我们谈论图和图神经网络(GNNs)时，“谱”意味着图拉普拉斯<a class="ae lw" href="https://en.wikipedia.org/wiki/Laplacian_matrix" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">的<em class="lu">本征分解</em></strong></a><strong class="la iu"/><em class="lu">L .</em>你可以把图拉普拉斯<em class="lu"> L </em>想象成一个以特殊方式归一化的邻接矩阵<em class="lu"> A </em>，而<em class="lu">本征分解</em>是一种寻找那些基本正交分量的方法</p><p id="713d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">直观地说，拉普拉斯图显示了如果我们在节点<em class="lu"> i </em>中放置一些“势”,<em class="lu">如何平滑地</em>“能量”将在图中扩散。拉普拉斯在数学和物理中的一个典型用例是解决信号(波)如何在动态系统中传播。当邻居之间的值没有突然变化时，扩散是平滑的，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/a4ce6366deb8298797206bdf72be89fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*gz2hyrcSSJG9MtDzmQLe3w.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Diffusion of some signal (for example, it can be heat) in a regular grid graph computed based on the graph Laplacian (<a class="ae lw" href="https://en.wikipedia.org/wiki/Laplacian_matrix" rel="noopener ugc nofollow" target="_blank">source</a>). Basically, the only things required to compute these dynamics are the Laplacian and initial values in nodes (pixels), i.e. red and yellow pixels corresponding to high intensity (of heat).</figcaption></figure><p id="2a92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章的其余部分，我将假设“<em class="lu">对称归一化拉普拉斯算子</em>”，它经常用于图形神经网络，因为它是归一化的，以便当你堆叠许多图形层时，节点特征以更平滑的方式传播，而不会出现特征值或梯度的爆炸或消失。它仅基于图的邻接矩阵<em class="lu">和</em>的<em class="lu">进行计算，这可以用几行 Python 代码完成，如下所示:</em></p><pre class="kj kk kl km gt mz mx na nb aw nc bi"><span id="6914" class="nd ly it mx b gy ne nf l ng nh"><strong class="mx iu"># Computing the graph Laplacian<br/># A is an adjacency matrix of some graph <em class="lu">G</em><br/></strong>import numpy as np</span><span id="7b32" class="nd ly it mx b gy ni nf l ng nh">N = A.shape[0] <strong class="mx iu"># number of nodes in a graph</strong><br/>D = np.sum(A, 0) <strong class="mx iu"># node degrees</strong><br/>D_hat = np.diag((D + 1e-5)**(-0.5)) <strong class="mx iu"># normalized node degrees</strong><br/>L = np.identity(N) — np.dot(D_hat, A).dot(D_hat) <strong class="mx iu"># Laplacian</strong></span></pre><p id="a24e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们假设<em class="lu"> A </em>是对称的，即<em class="lu"> A </em> = <em class="lu"> A </em> ᵀ，并且我们的图是无向图，否则节点度不是明确定义的，并且必须做出一些假设来计算拉普拉斯算子。邻接矩阵<em class="lu"> A </em>的一个有趣的性质是<em class="lu"> Aⁿ </em>(矩阵乘积取<em class="lu"> n </em>次)公开了节点之间的<em class="lu"> n </em>跳连接(更多细节见<a class="ae lw" href="https://en.wikipedia.org/wiki/Adjacency_matrix#Matrix_powers" rel="noopener ugc nofollow" target="_blank">此处</a>)。</p><p id="6bcd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们生成三个图，并可视化它们的邻接矩阵和拉普拉斯算子以及它们的能力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/660729a47c67add3ceec0192976f5d78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WSvWVAsQsGtQQpIcrCPOOQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Adjacency matrices, Laplacians and their powers for a random graph (left), “star graph” (middle) and “path graph” (right). I normalize A² such that the sum in each row equals 1 to have a probabilistic interpretation of 2-hop connections. Notice that Laplacians and their powers are symmetric matrices, which makes eigen-decomposition easier as well as facilitates feature propagation in a deep graph network.</figcaption></figure><p id="591a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，想象中间上方的星图是由金属制成的，这样它可以很好地传热。然后，如果我们开始加热节点 0(深蓝色)，这种热量将以拉普拉斯定义的方式传播到其他节点。在所有边都相等的星形图的特殊情况下，热量将均匀地传播到所有其他节点，这对于其他图来说是不正确的，因为它们的结构。</p><p id="bb26" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在计算机视觉和机器学习的背景下，图形拉普拉斯定义了如果我们堆叠几个图形神经层，节点特征将如何更新。与我的教程 的第一部分<a class="ae lw" href="https://medium.com/p/3d9fada3b80d" rel="noopener"> <em class="lu">类似，为了从计算机视觉的角度理解光谱图卷积，我将使用 MNIST 数据集，它在 28×28 的规则网格图上定义图像。</em></a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/71a4ac2c82438a4a4793329058b2f455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EUFjx4cvVq4TdmU1PfXRpA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">MNIST image defining features X (left), adjacency matrix A (middle) and the Laplacian (right) of a regular 28×28 grid. The reason that the graph Laplacian looks like an identity matrix is that the graph has a relatively large number of nodes (784), so that after normalization values outside the diagonal become much smaller than 1.</figcaption></figure><h1 id="d6ae" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">2.盘旋</h1><p id="c8db" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">在信号处理中，可以证明空间域中的卷积是频域中的乘法(又称为<a class="ae lw" href="https://en.wikipedia.org/wiki/Convolution_theorem" rel="noopener ugc nofollow" target="_blank">卷积定理</a>)。同样的定理也适用于图形。在信号处理中，为了将信号变换到频域，我们使用离散傅里叶变换，它基本上是信号与特殊矩阵(基，DFT 矩阵)的矩阵乘法。这个基础假设了一个<em class="lu">规则的</em>网格，所以我们不能把它用于<em class="lu">不规则的</em>图形，这是一个典型的情况。而是用一个更一般的基，就是图拉普拉斯<em class="lu"> L </em>的特征向量<em class="lu"> V </em>，可以通过特征分解找到:<em class="lu">l</em>=<em class="lu">vλvᵀ</em>，其中<em class="lu">λ</em>是<em class="lu"> L. </em>的特征值</p><p id="828b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">主成分分析 vs 拉普拉斯图的特征分解。</strong>在实际计算谱图卷积时，只需使用与<em class="lu">最小</em>特征值对应的几个特征向量就足够了。乍一看，与计算机视觉中经常使用的<a class="ae lw" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析(PCA) </a>相比，这似乎是一种相反的策略，其中我们对<em class="lu">最大</em>特征值对应的特征向量更感兴趣。然而，这种差异仅仅是由于上面用于计算拉普拉斯算子的<em class="lu">否定</em>，因此使用 PCA 计算的特征值与图拉普拉斯算子的特征值<em class="lu">成反比</em>(形式分析见<a class="ae lw" href="http://outobox.cs.umn.edu/PCA_on_a_Graph.pdf" rel="noopener ugc nofollow" target="_blank">本文</a>)。还要注意的是，PCA 应用于数据集的协方差矩阵，目的是提取最大的变化因素，即数据变化最大的维度，如<a class="ae lw" href="https://en.wikipedia.org/wiki/Eigenface" rel="noopener ugc nofollow" target="_blank">特征面</a>。这种变化通过特征值来测量，因此最小的特征值基本上对应于噪声或“伪”特征，这些特征在实践中被认为是无用的甚至是有害的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b69fd997c5082cd38117afc6769534c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*k8AfLWuLW9sgOsuCarR19Q.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Eigenvalues (in a descending order) and corresponding eigenvectors for the MNIST dataset.</figcaption></figure><p id="25a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">拉普拉斯图的特征分解应用于单个图，目的是提取节点的子图或集群(社区)，并且<a class="ae lw" href="http://blog.shriphani.com/2015/04/06/the-smallest-eigenvalues-of-a-graph-laplacian/" rel="noopener ugc nofollow" target="_blank">特征值告诉我们许多关于图连通性的信息</a>。我将在下面的例子中使用对应于 20 个最小特征值的特征向量，假设 20 远小于节点数<em class="lu"> N(在 MNIST <em class="lu"> ) </em>的情况下 N </em> =784)。为了找到下面左边的特征值和特征向量，我使用了一个 28×28 的规则图，而在右边，我遵循<a class="ae lw" href="https://arxiv.org/abs/1312.6203" rel="noopener ugc nofollow" target="_blank">布鲁纳等人</a>的实验，通过在 28×28 的规则网格上采样 400 个随机位置来构建一个不规则图(有关该实验的更多细节，请参见他们的论文)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/33dbee17f8d768c2c94d1aa62f8ac9c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*93nVzwz_V7IPC7TPlgAsjQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Eigenvalues <em class="nn">Λ (</em><strong class="bd no"><em class="nn">bottom</em></strong><em class="nn">) and e</em>igenvectors V (<strong class="bd no">top</strong>) of the graph Laplacian L for a regular 28<em class="nn">×</em>28 grid (<strong class="bd no">left</strong>) and non-uniformly subsampled grid with 400 points according to experiments in <a class="ae lw" href="https://arxiv.org/abs/1312.6203" rel="noopener ugc nofollow" target="_blank">Bruna et al., 2014, ICLR 2014</a> (<strong class="bd no">right</strong>). Eigenvectors corresponding to the 20 <strong class="bd no">smallest</strong> <strong class="bd no">eigenvalues</strong> are shown. Eigenvectors are 784 dimensional on the left and 400 dimensional on the right, so V is 784<em class="nn">×20 and 400×20 respectively. </em>Each of the 20 eigenvectors on the left was reshaped to 28<em class="nn">×</em>28, whereas on the right to reshape a 400 dimensional eigenvector to 28<em class="nn">×28, white pixels for missing nodes were added. So, e</em>ach pixel in each eigenvector corresponds to a node or a missing node (in white on the right). These eigenvectors can be viewed as a basis in which we decompose our graph.</figcaption></figure><p id="0b76" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，给定图的拉普拉斯<em class="lu"> L </em>，节点特征<em class="lu"> X </em>和滤波器<em class="lu"> W </em> _spectral，在 Python <strong class="la iu">图上进行谱卷积</strong>看起来非常简单:</p><pre class="kj kk kl km gt mz mx na nb aw nc bi"><span id="e6c3" class="nd ly it mx b gy ne nf l ng nh"><strong class="mx iu"># Spectral convolution on graphs<br/># X is an <em class="lu">N×1 matrix of 1-dimensional node features<br/></em></strong><strong class="mx iu"># L </strong><strong class="mx iu">is an </strong><strong class="mx iu"><em class="lu">N</em></strong><strong class="mx iu"><em class="lu">×N</em> graph Laplacian computed above<br/># W_spectral are </strong><strong class="mx iu"><em class="lu">N</em></strong><strong class="mx iu"><em class="lu">×</em></strong><strong class="mx iu"><em class="lu">F weights (filters) that we want to train<br/></em></strong>from <!-- -->scipy.sparse.linalg import eigsh <strong class="mx iu"># assumes <em class="lu">L</em> to be symmetric</strong></span><span id="a1e0" class="nd ly it mx b gy ni nf l ng nh"><em class="lu">Λ</em><em class="lu">,V</em> = eigsh(L,k=20,which=’SM’) <strong class="mx iu"># </strong><strong class="mx iu">eigen-decomposition (i.e. find <em class="lu">Λ</em></strong><strong class="mx iu"><em class="lu">,V)</em></strong><br/>X_hat = V.T.dot(X) <strong class="mx iu"># <em class="lu">20</em></strong><strong class="mx iu">×</strong><strong class="mx iu"><em class="lu">1</em> node features in the "spectral" domain</strong><br/>W_hat = V.T.dot(W_spectral)  <strong class="mx iu"># 20×<em class="lu">F</em> filters in the </strong><strong class="mx iu">"spectral" domain</strong><br/>Y = V.dot(X_hat * W_hat)  <strong class="mx iu"># <em class="lu">N</em></strong><strong class="mx iu"><em class="lu">×</em></strong><strong class="mx iu"><em class="lu">F</em> result of convolution</strong></span></pre><p id="3d9a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">形式上:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/d84facfea1cd1a9c42cd1a0dbf5a468b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wBIfFw54z8usWq_merON8A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Spectral graph convolution, where ⊙ means element-wise multiplication.</figcaption></figure><p id="bee5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中，我们假设我们的节点特征<em class="lu"> X⁽ˡ⁾ </em>是一维的，例如 m 像素，但是它可以扩展到<em class="lu"> C </em>维的情况:我们将只需要对每个<em class="lu">通道</em>重复这个卷积，然后像在信号/图像卷积中一样对<em class="lu"> C </em>求和。</p><p id="0544" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">公式(3)本质上与使用傅立叶变换的规则网格上的<a class="ae lw" href="https://en.wikipedia.org/wiki/Convolution_theorem" rel="noopener ugc nofollow" target="_blank">信号的频谱卷积</a>相同，因此为机器学习产生了一些问题:</p><ul class=""><li id="08c2" class="nq nr it la b lb lc le lf lh ns ll nt lp nu lt nv nw nx ny bi translated">可训练权重(滤波器)<em class="lu"> W_ </em>谱的维数取决于图中节点<em class="lu"> N </em>的数量；</li><li id="a36d" class="nq nr it la b lb nz le oa lh ob ll oc lp od lt nv nw nx ny bi translated"><em class="lu"> W_ </em>谱也取决于图结构中编码的特征向量<em class="lu"> V. </em></li></ul><p id="f14d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些问题阻碍了扩展到具有可变结构的大型图形的数据集。下文概述的进一步努力侧重于解决这些和其他问题。</p><h1 id="59fb" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated"><strong class="ak"> 3。谱域中的“平滑”</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/ef1eb6ee0fdbca4b76e966f746d7657e.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*PcKEUB4wTOG6gtoEIZl9iA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Strawberry and banana smoothie (source: <a class="ae lw" href="https://joyfoodsunshine.com/strawberry-banana-smoothie/" rel="noopener ugc nofollow" target="_blank">joyfoodsunshine.com</a>). Smoothing in the spectral domain is a little bit different 😃.</figcaption></figure><p id="25c7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lw" href="https://arxiv.org/abs/1312.6203" rel="noopener ugc nofollow" target="_blank">布鲁纳等人</a>是最早将谱图分析应用到<em class="lu">学习卷积滤波器</em>来解决图分类问题的人之一。使用上述公式(3)学习的滤波器作用于<em class="lu">整个图</em>，即它们具有<em class="lu">全局支持</em>。在计算机视觉环境中，这将与在 MNIST 上训练 28×28 像素大小的卷积滤波器相同，即滤波器具有与输入相同的大小(注意，我们仍将滑动滤波器，但在零填充图像上)。虽然对于 MNIST，我们实际上可以训练这样的过滤器，但常识建议避免这样做，因为这会使训练变得更加困难，因为参数数量可能会激增，并且难以训练可以捕捉不同图像之间共享的有用特征的大型过滤器。</p><p id="0a9f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上，我使用 PyTorch 和来自 GitHub 的代码成功地训练了这样一个模型。您应该使用<code class="fe mu mv mw mx b">mnist_fc.py --model conv</code>来运行它。经过 100 个时期的训练后，过滤器看起来像数字的混合物:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/35da0cbd0f3b3a69f5d8e26bc3189cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kNftNPG_J4i_pUN40DjAXQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Examples of filters with <strong class="bd no">global support</strong> typically used in spectral convolution. In this case, these are 28×28 filters learned using a ConvNet with a single convolutional layer followed by ReLU, 7×7 MaxPooling and a fully-connected classification layer. To make it clear, the output of the convolutional layer is still 28×28 due to zero-padding. Surprisingly, this net achieves 96.7% on MNIST. This can be explained by the simplicity of the dataset.</figcaption></figure><p id="bd62" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">重申一下，我们通常希望让过滤器更小，更局部(这和我下面要提到的不完全一样)。</p><p id="0b9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了更好地实现这一点，他们建议在光谱域中平滑滤光器，根据光谱理论，这使得滤光器在空间域中更接近 T2。其思想是，您可以将公式(3)中的滤波器<em class="lu"> W_ </em>频谱表示为𝐾预定义函数(如样条函数)的和，并且我们学习这个和的<em class="lu"> K </em>系数<em class="lu"> α </em>，而不是学习<em class="lu"> W </em>的<em class="lu"> N </em>值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/e4bc0d9cd74eadddfdd33be6cd0db3c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZoZfh6faYLBm7_Nq3xrQw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">We can approximate our N dimensional filter<strong class="bd no"> </strong><em class="nn">W_</em>spectral as a finite sum of<em class="nn"> K</em> functions f, such as splines shown below. So, instead of learning N values of <em class="nn">W_</em>spectral, we can learn K coefficients (alpha) of those functions; it becomes efficient when K &lt;&lt; N.</figcaption></figure><p id="22a3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然<em class="lu"> fk </em>的维数确实取决于节点<em class="lu"> N </em>的数量，但是这些函数是固定的，所以我们不学习它们。我们唯一知道的是系数<em class="lu"> α </em>，因此<em class="lu"> W_ </em>光谱不再依赖于<em class="lu"> N </em>。整洁，对不对？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/763b1810c52e0c662b90d0d4b8960b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJWQBxMX3hZz85pKhma34w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The spline basis used to smooth filters in the frequency domain, thereby making them more local. Splines and other polynomial functions are useful, because we can represent filters as their sums.</figcaption></figure><p id="8c19" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了使我们在公式(4)中的近似合理，我们希望<em class="lu"> K </em> &lt; &lt; <em class="lu"> N </em>将可训练参数的数量从<em class="lu"> N </em>减少到<em class="lu"> K </em>，更重要的是，使其独立于<em class="lu"> N </em>，这样我们的 GNN 可以消化任何大小的图。我们可以使用不同的碱基来进行这种“扩展”，这取决于我们需要哪些性质。例如，上面显示的三次样条函数被认为是非常平滑的函数(也就是说，你看不到节点，也就是分段样条多项式的各个部分相遇的地方)。我在<a class="ae lw" href="https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49" rel="noopener">的另一篇文章</a>中讨论的切比雪夫多项式具有逼近函数之间的最小𝑙∞距离。傅立叶基是在变换后保留大部分信号能量的基。大多数碱基是正交的，因为有可以相互表达的项是多余的。</p><p id="3adb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，滤波器<em class="lu"> W_ </em>光谱仍然与输入一样大，但是它们的<em class="lu">有效宽度</em>很小。在 MNIST 图像的情况下，我们将有 28×28 个滤波器，其中只有一小部分值的绝对量值大于 0，并且所有这些值应该彼此靠近，即滤波器将是局部的并且实际上很小，如下所示(左起第二个):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/6683bec24e75ddda45bbf050b62d9308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1NL_awIic9m5P-IF5_3J2A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">From left to right: (first) Input image. (second) Local filter with small effective width. Most values are very close to 0. (third) The result of spectral graph convolution of the MNIST image of digit 7 and the filter. (fourth) The result of spectral convolution using the Fourier transform. These results indicate that spectral graph convolution is quite limited if applied to images, perhaps, due to the weak spatial structure of the Laplacian basis compared to the Fourier basis.</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/9c0e01ba4835f57dd52db150fdc87197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WtfLzUxDwyU8gwAD8u5HgQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Reconstruction of the MNIST image using the Fourier and graph Laplacian bases using only M components of V: X’=V V<em class="nn">ᵀX</em>. We can see that the bases compress different patterns in images (orientated edges in the Fourier case and global patterns in the Laplacian case). This makes results of convolutions illustrated above very different.</figcaption></figure><p id="5e7f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总而言之，频谱域中的平滑允许<a class="ae lw" href="https://arxiv.org/abs/1312.6203" rel="noopener ugc nofollow" target="_blank">布鲁纳等人</a>学习更多的局部滤波器。具有这种过滤器的模型可以实现与没有平滑的模型(即，使用我们的公式(3))类似的结果，但是具有少得多的可训练参数，因为过滤器大小独立于输入图表大小，这对于将模型缩放到具有较大图表的数据集是重要的。然而，学习滤波器<em class="lu"> W </em> _spectral 仍然依赖于特征向量<em class="lu"> V </em>，这使得将该模型应用于具有可变图结构的数据集具有挑战性。</p><h1 id="87c3" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">结论</h1><p id="3c93" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">尽管最初的光谱图卷积方法存在缺点，但它已经得到了很多发展，并在一些应用中保持了相当有竞争力的方法，因为光谱滤波器可以更好地捕捉图中的全局复杂模式，而像 GCN ( <a class="ae lw" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank"> Kipf &amp; Welling，ICLR，2017 </a>)这样的局部方法除非堆叠在深度网络中，否则无法实现。例如，2019 年的两篇论文，分别是<a class="ae lw" href="https://arxiv.org/abs/1901.01484" rel="noopener ugc nofollow" target="_blank">廖等人</a>关于“LanczosNet”和<a class="ae lw" href="https://arxiv.org/abs/1904.07785" rel="noopener ugc nofollow" target="_blank">徐等人</a>关于“图小波神经网络”，解决了谱图卷积的一些缺点，并在预测分子性质和节点分类方面显示出很好的结果。<a class="ae lw" href="https://arxiv.org/abs/1705.07664" rel="noopener ugc nofollow" target="_blank"> Levie 等人的另一项有趣的工作，2018 </a>关于“CayleyNets”在节点分类、矩阵完成(推荐系统)和社区检测方面表现强劲。因此，根据您的应用和基础设施，谱图卷积可能是一个不错的选择。</p><p id="819b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我关于计算机视觉图形神经网络的<a class="ae lw" href="https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49" rel="noopener">教程的另一部分</a>中，我解释了由<a class="ae lw" href="https://arxiv.org/abs/1606.09375" rel="noopener ugc nofollow" target="_blank"> Defferrard 等人</a>在 2016 年引入的切比雪夫谱图卷积，它仍然是一个非常强大的基线，具有一些很好的属性，并且易于实现，正如我使用 PyTorch 演示的那样。</p><p id="e7de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">鸣谢:本教程的很大一部分是我在 SRI International 实习期间在</em> <a class="oj ok ep" href="https://medium.com/u/6cf41cb2c546?source=post_page-----2e495b57f801--------------------------------" rel="noopener" target="_blank"> <em class="lu">穆罕默德·阿梅尔</em> </a> <em class="lu"> ( </em> <a class="ae lw" href="https://mohamedramer.com/" rel="noopener ugc nofollow" target="_blank"> <em class="lu">主页</em> </a> <em class="lu">)和我的博士导师格拉汉姆·泰勒(</em> <a class="ae lw" href="https://www.gwtaylor.ca/" rel="noopener ugc nofollow" target="_blank"> <em class="lu">主页</em> </a> <em class="lu">)的指导下编写的。我也感谢</em> <a class="ae lw" href="https://www.linkedin.com/in/carolynaugusta/" rel="noopener ugc nofollow" target="_blank"> <em class="lu">卡洛琳·奥古斯塔</em> </a> <em class="lu">的有用反馈。</em></p><p id="349e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<a class="ae lw" href="https://github.com/bknyaz/" rel="noopener ugc nofollow" target="_blank"> Github </a>、<a class="ae lw" href="https://www.linkedin.com/in/boris-knyazev-39690948/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lw" href="https://twitter.com/BorisAKnyazev" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上找我。<a class="ae lw" href="https://bknyaz.github.io/" rel="noopener ugc nofollow" target="_blank">我的主页</a>。</p><p id="b554" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想在你的论文中引用这篇博文，请使用:<br/><a class="ae lw" href="http://twitter.com/misc" rel="noopener ugc nofollow" target="_blank"><em class="lu">@ misc</em></a><em class="lu">{ Knyazev 2019 Tutorial，<br/>title = {用于计算机视觉及超越的图形神经网络教程}，<br/> author={Knyazev，Boris and Taylor，Graham W and Amer，Mohamed R}，<br/> year={2019} <br/> } </em></p></div></div>    
</body>
</html>