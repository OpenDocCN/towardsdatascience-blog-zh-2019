<html>
<head>
<title>Reinforcement Learning : Markov-Decision Process (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习:马尔可夫决策过程(上)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da?source=collection_archive---------0-----------------------#2019-07-18">https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da?source=collection_archive---------0-----------------------#2019-07-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="275c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">#InsideRL</h2><div class=""/><blockquote class="jw jx jy"><p id="0953" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated">在一个典型的强化学习(RL)问题中，有一个学习者和一个决策者，称为<strong class="kc ja">代理</strong>，与其交互的环境称为<strong class="kc ja">环境</strong>。作为回报，环境基于代理的<strong class="kc ja">动作</strong>提供<strong class="kc ja">奖励</strong>和一个<strong class="kc ja">新状态</strong>。因此，在强化学习中，我们不教代理应该如何做某事，而是根据它的行为给它奖励，无论是积极的还是消极的。<strong class="kc ja">所以我们这个博客的根本问题是，我们如何用数学方法来表述 RL 中的任何问题</strong>。这就是马尔可夫决策过程(MDP)的用武之地。</p></blockquote><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi ky"><img src="../Images/2bc4988cb4d5d77f255e08c3741e8b76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywOrdJAHgSL5RP-AuxsfJQ.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Typical Reinforcement Learning cycle</figcaption></figure><p id="1ff9" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">在我们回答我们的根本问题之前，即我们如何用数学方法(使用 MDP)表述 RL 问题，我们需要发展我们对以下问题的直觉:</p><ul class=""><li id="6982" class="lr ls iq kc b kd ke kh ki lo lt lp lu lq lv kx lw lx ly lz bi translated">代理-环境关系</li><li id="ed35" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">马尔可夫性质</li><li id="699b" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">马尔可夫过程和马尔可夫链</li><li id="4443" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">马尔可夫奖励过程(MRP)</li><li id="9929" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">贝尔曼方程</li><li id="9790" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">马尔可夫奖励过程</li></ul><p id="b27e" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">拿起你的咖啡，直到你感到自豪才停下来！🧐</p><h2 id="f27c" class="mf mg iq bd mh mi mj dn mk ml mm dp mn lo mo mp mq lp mr ms mt lq mu mv mw iw bi translated">代理-环境关系</h2><p id="82ce" class="pw-post-body-paragraph jz ka iq kc b kd mx kf kg kh my kj kk lo mz kn ko lp na kr ks lq nb kv kw kx ij bi translated">首先让我们看看一些正式的定义:</p><blockquote class="jw jx jy"><p id="a8f6" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">代理</strong>:做出智能决策的软件程序，它们是 RL 中的学习者。这些代理通过行动与环境互动，并根据他们的行动获得奖励。</p><p id="3077" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">环境</strong>:是待解决问题的演示。现在，我们可以有一个真实世界的环境或一个模拟的环境，我们的代理将与它进行交互。</p></blockquote><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/52c0bdafd0c10385a11419b350ce50ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*mUyxMUpzQWX4GNTd7TT4nA.gif"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Demonstrating an environment with which agents are interacting.</figcaption></figure><blockquote class="jw jx jy"><p id="49a9" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">状态</strong>:这是代理在环境中特定时间步的位置。因此，每当代理执行一个动作时，环境给予代理奖励和代理通过执行该动作所达到的新状态。</p></blockquote><p id="1fd6" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">代理<strong class="kc ja">不能任意改变</strong>的任何东西都被认为是环境的一部分。简单来说，动作可以是<strong class="kc ja">我们希望代理学习的任何决定</strong>和<strong class="kc ja">状态可以是在选择动作</strong>时有用的任何东西。我们并不假设环境中的一切对代理人来说都是未知的，例如，奖励计算被认为是环境的一部分，即使代理人知道一点它的奖励是如何作为它的动作和状态的函数来计算的。这是因为<strong class="kc ja">奖励不能由代理人任意</strong>更改。有时，代理人可能完全知道其环境，但仍然发现很难最大化奖励，就像我们可能知道如何玩魔方，但仍然无法解决它。因此，我们可以有把握地说，代理-环境关系代表了代理控制的极限，而不是它的知识。</p><h2 id="4142" class="mf mg iq bd mh mi mj dn mk ml mm dp mn lo mo mp mq lp mr ms mt lq mu mv mw iw bi translated">马尔可夫性质</h2><blockquote class="jw jx jy"><p id="54dd" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">跃迁</strong>:从一种状态转移到另一种状态叫做跃迁。</p><p id="cf4c" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">转移概率</strong>:智能体从一种状态转移到另一种状态的概率称为转移概率。</p></blockquote><p id="3fb0" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated"><em class="kb">中的</em> <strong class="kc ja"> <em class="kb">马氏性</em> </strong> <em class="kb">表述为:</em></p><blockquote class="nd"><p id="bdcf" class="ne nf iq bd ng nh ni nj nk nl nm kx dk translated">“鉴于现在，未来独立于过去”</p></blockquote><p id="7e42" class="pw-post-body-paragraph jz ka iq kc b kd nn kf kg kh no kj kk lo np kn ko lp nq kr ks lq nr kv kw kx ij bi translated">从数学上讲，我们可以将这一陈述表达为:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b9fd4af64573d9abc97ab8b48362eba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*Tr7GE76SiHh8_jVSYl2mug.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Markov Property</figcaption></figure><p id="f6ee" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">S[t]表示代理的当前状态，s[t+1]表示下一个状态。这个等式的意思是，从状态 S[t]到 S[t+1]的转变完全独立于过去。因此，如果系统具有马尔可夫性质，方程的<strong class="kc ja"> RHS </strong>与<strong class="kc ja"> LHS </strong>的含义相同。直观上意味着我们的当前状态已经捕获了过去状态的信息。</p><p id="00f7" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated"><strong class="kc ja"> <em class="kb">状态转移概率:</em> </strong></p><p id="f2d5" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">现在我们知道了转移概率，我们可以将状态转移概率定义如下:</p><p id="8060" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">对于从 S[t]到 S[t+1]的马尔可夫状态，即任何其他后续状态，状态转移概率由下式给出</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/9bf20f03544c5ebb8af3ea2ead1dde7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*LM6oPMUZSiEV78z0I2C3Hg.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">State Transition Probability</figcaption></figure><p id="6521" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">我们可以通过以下方式将状态转移概率公式化为状态转移概率矩阵:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi nu"><img src="../Images/0fd9ea8c4c47e8de2b9ea2bc9e77548b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pvxuu_e3VNRm81eHWYoyew.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">State Transition Probability Matrix</figcaption></figure><p id="50f1" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">矩阵中的每一行代表从我们的原始或开始状态移动到任何后续状态的概率。每行的总和等于 1。</p><h2 id="532d" class="mf mg iq bd mh mi mj dn mk ml mm dp mn lo mo mp mq lp mr ms mt lq mu mv mw iw bi translated">马尔可夫过程或马尔可夫链</h2><p id="6400" class="pw-post-body-paragraph jz ka iq kc b kd mx kf kg kh my kj kk lo mz kn ko lp na kr ks lq nb kv kw kx ij bi translated"><em class="kb">马尔可夫过程是记忆较少的</em> <strong class="kc ja"> <em class="kb">随机过程</em> </strong> <em class="kb">即一个随机状态的序列 S[1]，S[2]，…。S[n]具有马尔可夫性质</em>。所以，它基本上是一个具有马尔可夫性质的状态序列。它可以用一组状态(S)和转移概率矩阵(P)来定义。使用状态(S)和转移概率矩阵(P)可以完全定义环境的动态。</p><p id="1e24" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">但是<strong class="kc ja"> <em class="kb">随机过程</em> </strong>是什么意思呢？</p><p id="fbc9" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">为了回答这个问题，我们来看一个例子:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/f8fa001fed5a81a07d6273dedc287065.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*MBcie302iU3qbQPbhU0psw.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Markov chain</figcaption></figure><p id="f5db" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">树的边表示<strong class="kc ja">转移概率</strong>。让我们从这个链条上取一些样品。现在，假设我们在睡觉，根据概率分布，有 0.6%的几率我们会跑，0.2%的几率我们会睡得更久，0.2%的几率我们会吃冰淇淋。类似地，我们可以从这条链中提取其他序列。</p><p id="e716" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">来自链条的一些样品:</p><ul class=""><li id="939e" class="lr ls iq kc b kd ke kh ki lo lt lp lu lq lv kx lw lx ly lz bi translated">睡觉——跑步——冰淇淋——睡觉</li><li id="7b94" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">睡眠——冰淇淋——冰淇淋——奔跑</li></ul><p id="b933" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">在上面的两个序列中，我们看到的是，每次运行链时，我们都会得到一组随机的状态(即睡眠、冰淇淋、睡眠)。霍普，现在清楚了为什么马尔可夫过程被称为随机序列集。</p><p id="ed26" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">在进入马尔可夫奖励流程之前，让我们先来看一些有助于理解 MRP 的重要概念。T3】</p><h2 id="8abe" class="mf mg iq bd mh mi mj dn mk ml mm dp mn lo mo mp mq lp mr ms mt lq mu mv mw iw bi translated">奖励和回报</h2><blockquote class="jw jx jy"><p id="04f2" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">奖励</strong>是代理在环境中的某个<strong class="kc ja">状态</strong>下执行某个动作时收到的<strong class="kc ja">数值</strong>。根据代理的动作，数值可以是正的，也可以是负的。</p><p id="05b0" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated">在<strong class="kc ja">强化学习</strong>中，我们关心的是<strong class="kc ja">最大化</strong>累积奖励(代理从环境中获得的所有奖励)而不是，代理从当前状态获得的奖励(也称为即时奖励)。这个<strong class="kc ja">代理人从环境中得到的报酬</strong>的总和称为<strong class="kc ja">回报</strong>。</p></blockquote><p id="aa99" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">我们可以将回报定义为:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/fb0fe2caff860c356f1283c6bff79e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*Q4ZahV2xyAoODr7EC7R5KA.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Returns (Total rewards from the environment)</figcaption></figure><p id="bec1" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">r[t+1]是代理在时间步长 t[0]执行从一个状态移动到另一个状态的动作(a)时收到的奖励。类似地，r[t+2]是代理在时间步长 t[1]通过执行移动到另一个状态的动作而收到的奖励。r[T]是代理在最后一个时间步通过执行移动到另一个状态的动作而收到的奖励。</p><p id="b0e1" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated"><strong class="kc ja">间断和连续任务</strong></p><blockquote class="jw jx jy"><p id="2406" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">偶发任务</strong>:这些任务有一个<strong class="kc ja">终止状态</strong>(结束状态)。我们可以说它们有有限的状态。比如赛车游戏，我们开始游戏(开始比赛)，一直玩到游戏结束(比赛结束！).这叫插曲。一旦我们重启游戏，它将从初始状态开始，因此，每一集<strong class="kc ja">都是独立的。</strong></p><p id="24d6" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">连续任务</strong>:没有结束的任务，即<strong class="kc ja">没有任何终止状态</strong>。这些类型的任务将<strong class="kc ja">永不结束</strong>。比如学习如何编码！</p></blockquote><p id="9ed5" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">现在，很容易计算出阶段性任务的回报，因为它们最终会结束，但连续任务呢，因为它会一直持续下去。从求和到无穷大的收益！那么，我们如何定义连续任务的回报呢？</p><p id="a0d5" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">这就是我们需要<strong class="kc ja">贴现 factor(ɤ) </strong>的地方。</p><blockquote class="jw jx jy"><p id="3b6a" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">贴现因子(ɤ) </strong>:决定<strong class="kc ja">当前奖励和</strong>未来奖励<strong class="kc ja">的重要性</strong>的大小。这基本上帮助我们避免<strong class="kc ja">无穷大</strong>作为连续任务中的奖励。它的值介于 0 和 1 之间。值<strong class="kc ja"> 0 </strong>意味着给予<strong class="kc ja">即时奖励</strong>更大的重要性，值<strong class="kc ja"> 1 </strong>意味着给予<strong class="kc ja">未来奖励</strong>更大的重要性。<strong class="kc ja">在实践中</strong>，折扣系数 0 永远不会学习，因为它只考虑即时奖励，而折扣系数 1 将继续用于未来奖励，这可能导致无穷大。因此，<strong class="kc ja">折扣系数的最佳值在 0.2 到 0.8 之间</strong>。</p></blockquote><p id="9fb0" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">因此，我们可以使用贴现因子定义回报如下:<strong class="kc ja">(假设这是等式 1，因为我们稍后将使用该等式来推导贝尔曼等式)</strong></p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e72b2ef7d48b85aeac4d4e4b4c39002e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*NKX4zhTk9YKx4J5b0PKbBA.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Returns using discount factor</figcaption></figure><p id="acae" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">让我们用一个例子来理解它，假设你住在一个面临水资源短缺的地方，如果有人来找你，说他会给你 100 升水！(请假设！)作为某个参数的函数(<strong class="kc ja"> ɤ) </strong>。让我们看看两种可能性:(假设这是等式 1，因为我们将在后面使用这个等式来推导贝尔曼等式)</p><p id="ab0f" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">一个带有折扣系数(<strong class="kc ja"> ɤ) 0.8 : </strong></p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/def987c65934711ffebc018df3ea7f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*UG4d2lAaXt7y5j9eSuJ_Ag.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Discount Factor (0.8)</figcaption></figure><p id="1b6f" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">这意味着我们应该等到第 15 个小时，因为下降不是非常显著，所以直到最后仍然是值得的。这意味着我们也对未来的回报感兴趣。所以，如果贴现因子接近 1，那么我们将努力达到终点，因为奖励非常重要。</p><p id="84f6" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">二、用贴现因子(<strong class="kc ja"> ɤ) 0.2 : </strong></p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/f46d3a380e63caeca5efd978758cf641.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*f816moXKnK7UH6Uc7qBWCg.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Discount Factor (0.2)</figcaption></figure><p id="0b33" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">这意味着我们对早期的奖励更感兴趣，因为奖励在第一小时变得非常低。所以，我们可能不想等到最后(直到第 15 个小时)，因为这将是毫无价值的。因此，如果贴现因子接近于零，那么眼前的回报比未来更重要。</p><blockquote class="jw jx jy"><p id="8571" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">那么使用哪个折扣系数值呢？</strong></p></blockquote><p id="25ae" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">这取决于我们想要训练代理的任务。假设，在一盘棋中，目标是击败对手的王。如果我们重视直接的奖励，如棋子击败任何对手的奖励，那么代理人将学习执行这些子目标，不管他的玩家是否也被击败。所以，在这个任务中，未来的回报更重要。在某些情况下，我们可能更喜欢使用即时奖励，就像我们之前看到的水的例子一样。</p><h2 id="8913" class="mf mg iq bd mh mi mj dn mk ml mm dp mn lo mo mp mq lp mr ms mt lq mu mv mw iw bi translated">马尔可夫奖励过程</h2><p id="522e" class="pw-post-body-paragraph jz ka iq kc b kd mx kf kg kh my kj kk lo mz kn ko lp na kr ks lq nb kv kw kx ij bi translated">到目前为止，我们已经看到马尔可夫链是如何使用状态集(S)和转移概率矩阵(P)来定义环境的动态性的。但是，我们知道强化学习的目的是最大化回报。所以，让我们把<strong class="kc ja"> <em class="kb">加奖励</em> </strong>到我们的马尔可夫链中。这给了我们<strong class="kc ja">马尔科夫的奖励过程。</strong></p><blockquote class="jw jx jy"><p id="2b1b" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">马尔可夫奖励过程</strong>:顾名思义，MDP 就是有价值判断的马尔可夫链。基本上，我们从代理所处的每个状态中获取一个值。</p></blockquote><p id="a420" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">在数学上，我们将马尔可夫奖励过程定义为:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/318e8ec8b6bc313201f155b6e4cc2e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*VlQTW70kSB4Bs79MkH2enA.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Markov Reward Process</figcaption></figure><p id="9a66" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">这个等式的意思是我们从一个特定的状态 S[t]中得到多少奖励(Rs)。这告诉我们，从我们的代理所处的特定状态中可以得到直接的回报。正如我们将在下一个故事中看到的，我们如何从我们的代理所处的每个状态中最大化这些奖励。简单来说，最大化我们从每个州获得的累积奖励。</p><p id="593f" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">我们将 MRP 定义为(标准普尔，R,ɤ)，其中:</p><ul class=""><li id="0592" class="lr ls iq kc b kd ke kh ki lo lt lp lu lq lv kx lw lx ly lz bi translated">s 是一组状态，</li><li id="6c8f" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">p 是转移概率矩阵，</li><li id="e20d" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">r 是奖励函数，我们之前看到过，</li><li id="8fc2" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">ɤ是贴现因子</li></ul><h2 id="b1d1" class="mf mg iq bd mh mi mj dn mk ml mm dp mn lo mo mp mq lp mr ms mt lq mu mv mw iw bi translated">马尔可夫决策过程</h2><p id="d981" class="pw-post-body-paragraph jz ka iq kc b kd mx kf kg kh my kj kk lo mz kn ko lp na kr ks lq nb kv kw kx ij bi translated">现在，让我们发展对贝尔曼方程和马尔可夫决策过程的直觉。</p><h2 id="a34e" class="mf mg iq bd mh mi mj dn mk ml mm dp mn lo mo mp mq lp mr ms mt lq mu mv mw iw bi translated">政策功能和价值功能</h2><p id="a4fc" class="pw-post-body-paragraph jz ka iq kc b kd mx kf kg kh my kj kk lo mz kn ko lp na kr ks lq nb kv kw kx ij bi translated"><strong class="kc ja">价值函数决定了代理处于某个特定状态有多好</strong>。当然，要确定一个特定的状态有多好，必须依赖于它将要采取的一些行动。这就是政策发挥作用的地方。策略定义了在特定状态下要执行的操作。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8bb74b14f00010c74b5d28d042c281d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*-rS3-7Wcl1sSs_gXgkvdTw.png"/></div></figure><p id="e1b8" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">策略是一个简单的函数，它为每个状态(s ∈ S)定义了动作(a∈ A)的概率分布<strong class="kc ja">。如果代理在时间 t 遵循策略π，那么<strong class="kc ja"> π(a|s) </strong>是代理在特定时间步长(t)采取行动(a)的概率。在强化学习中，代理人的经验决定了策略的变化。数学上，策略定义如下:</strong></p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/8621300616c4f65d80b469e0a2a5feb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*y86rTj5H-3XIo8uPWJCAMA.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Policy Function</figcaption></figure><p id="a23e" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">现在，<strong class="kc ja">我们如何找到一个状态的值</strong>。当代理人遵循 vπ(s)表示的策略π时，状态 s 的值是从 s 开始并遵循下一个状态的策略π直到到达终端状态的期望收益。我们可以将其公式化为:(<strong class="kc ja">该函数也称为状态值函数</strong>)</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi od"><img src="../Images/3193413ef6140c5fdbc43036fd79f13c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Kjo-ibNy_jDX2hvnLnROA.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Value Function</figcaption></figure><p id="ff94" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">这个等式给出了从<strong class="kc ja">状态</strong>开始到后续状态的<strong class="kc ja">预期收益</strong>，策略为π。需要注意的一点是，我们得到的回报是随机的，而一个状态的价值是<strong class="kc ja">而不是</strong>随机的。它是从起始状态 s 到任何其它状态的期望收益。还要注意，终端状态的值(如果有的话)是零。让我们看一个例子:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi oe"><img src="../Images/5ad1241d910a21bde716144f59719777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p5KQnP1rwTcXFooMF0n-TA.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Example</figcaption></figure><p id="d863" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">假设我们的开始状态是类 2，我们移动到类 3，然后通过，然后睡眠。简而言之，二班&gt;三班&gt;及格&gt;睡觉。</p><p id="7102" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">我们的预期回报是 0.5 的贴现因子:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi of"><img src="../Images/debd7076dfffc4557a30c7151059d300.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*uAXgaYD2VYwvntCaA7LwWw.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Calculating the Value of Class 2</figcaption></figure><p id="c2ee" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated"><strong class="kc ja">注:</strong> <em class="kb">是</em><strong class="kc ja"><em class="kb">-2+(-2 * 0.5)+10 * 0.25+0</em></strong><em class="kb">而不是</em><strong class="kc ja"><em class="kb">-2 *-2 * 0.5+10 * 0.25+0</em></strong>。<strong class="kc ja">那么 Class 2 的值就是-0.5 </strong>。</p><h2 id="8868" class="mf mg iq bd mh mi mj dn mk ml mm dp mn lo mo mp mq lp mr ms mt lq mu mv mw iw bi translated">价值函数的贝尔曼方程</h2><p id="334c" class="pw-post-body-paragraph jz ka iq kc b kd mx kf kg kh my kj kk lo mz kn ko lp na kr ks lq nb kv kw kx ij bi translated"><strong class="kc ja">贝尔曼方程</strong>帮助我们找到<strong class="kc ja">最优策略</strong>和<strong class="kc ja">价值函数</strong>。我们知道我们的政策会随着经验而改变，所以根据不同的政策，我们会有不同的价值函数。<strong class="kc ja"> <em class="kb">最优价值函数是与所有其他价值函数</em> </strong>相比给出最大值的函数。</p><p id="67fb" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">贝尔曼方程表明价值函数可以被分解成两部分:</p><ul class=""><li id="7981" class="lr ls iq kc b kd ke kh ki lo lt lp lu lq lv kx lw lx ly lz bi translated">即时奖励，R[t+1]</li><li id="08f8" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">继承国的贴现值，</li></ul><p id="53bd" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">数学上，我们可以将贝尔曼方程定义为:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi og"><img src="../Images/32ca8bf32f0dfba911d65345e315b7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*RIMbwQAUJymRMgo5gMJZVg.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Bellman Equation for Value Function</figcaption></figure><p id="0c9b" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">让我们借助一个例子来理解这个等式的含义:</p><p id="217b" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">假设，有一个机器人处于某个状态，然后他从这个状态移动到另一个状态。现在，问题是机器人处于这种状态有多好。利用贝尔曼方程，我们可以得出，它是离开状态时得到的<strong class="kc ja">奖励</strong>的期望值加上它所移动到的状态(s’)的<strong class="kc ja">值。</strong></p><p id="f3a6" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">让我们看另一个例子:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/fd32ca837b52983941c5aa673f9da91e.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*W-l4WG-VXHZukj883xn-0g.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Backup Diagram</figcaption></figure><p id="37f2" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">我们想知道状态 s 的<strong class="kc ja">值</strong>。状态的值是我们离开该状态时得到的回报，加上我们到达的状态的贴现值乘以我们将进入该状态的转移概率。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/766baae07071c61fa0fcf89a43f07b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*Dq2iZ2HPlUoU8MOtIn4UmQ.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Value Calculation</figcaption></figure><p id="85f7" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">上述等式可以用矩阵形式表示如下:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/713a5be97a60ae8789d24e3e91f7ff99.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*rRn2YD8Zh5I-7Ycv14L1Ug.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Bellman Linear Equation</figcaption></figure><p id="30ba" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">其中 v 是我们所处状态的价值，它等于<strong class="kc ja">即时回报加上下一个状态的贴现值乘以进入该状态的概率</strong>。</p><p id="8169" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">本次计算的运行时间复杂度为<strong class="kc ja"> O(n ) </strong>。因此，这对于<strong class="kc ja">解决更大的 MRP</strong>(同样对于<strong class="kc ja">MDP</strong>)显然不是一个实用的解决方案。在后面的博客中，我们将关注更有效的方法，如<strong class="kc ja">动态规划</strong>(价值迭代和策略迭代)<strong class="kc ja">蒙特-克拉罗方法</strong>和<strong class="kc ja"> TD-Learning </strong>。</p><p id="7616" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">我们将在下一个故事中更详细地讨论贝尔曼方程。</p><blockquote class="jw jx jy"><p id="ef4d" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">什么是马尔科夫决策过程？</strong></p><p id="2b57" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">马尔可夫决策过程</strong>:是一个决策的马尔可夫回报过程。一切都像 MRP 一样，但现在我们有了真正的机构来做决定或采取行动。</p></blockquote><p id="a4bb" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">它是一个(<strong class="kc ja"> <em class="kb"> S </em> </strong>，<strong class="kc ja"> <em class="kb"> A </em> </strong>，<strong class="kc ja"> <em class="kb"> P </em> </strong>，<strong class="kc ja"> <em class="kb"> R </em> </strong>，<strong class="kc ja"> 𝛾 </strong>)的元组，其中:</p><ul class=""><li id="0134" class="lr ls iq kc b kd ke kh ki lo lt lp lu lq lv kx lw lx ly lz bi translated">s 是一组状态，</li><li id="958c" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">a 是代理可以选择采取的一组操作，</li><li id="a2ab" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">p 是转移概率矩阵，</li><li id="12ea" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated">r 是代理人的行动所累积的报酬，</li><li id="06ae" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated"><strong class="kc ja"> 𝛾 </strong>是贴现因子。</li></ul><p id="58b8" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">p 和 R 将有轻微的变化，如下所示:</p><blockquote class="jw jx jy"><p id="5dc4" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">转移概率矩阵</strong></p></blockquote><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/25e7e2c865ed45a18246f3a189996e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*WjOyqRCE4zhjsrW-a_KXLA.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Transition Probability Matrix w.r.t action</figcaption></figure><blockquote class="jw jx jy"><p id="f699" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">奖励功能</strong></p></blockquote><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/b12138abdbeefbec012544af6bb09474.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*QVwSJKc3i6n3ZneI_ypa9Q.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Reward Function w.r.t action</figcaption></figure><p id="f4b6" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">现在，我们的奖励函数依赖于行动。</p><p id="6a86" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">到目前为止，我们已经讨论了当我们的代理遵循一个策略π通过一组状态时获得奖励(r)。实际上，在马尔可夫决策过程(MDP)中，策略是做出决策的机制。所以现在我们有了一个选择采取行动的机制。</p><p id="7719" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">MDP 的政策取决于当前的状态。他们不依赖于历史。这就是马尔可夫性质。因此，我们目前所处的状态是历史的特征。</p><p id="181b" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">我们已经看到代理处于特定状态(状态-值函数)有多好。现在，让我们看看遵循状态 s 的政策π(行动-价值函数)采取特定行动有多好。</p><h2 id="e88d" class="mf mg iq bd mh mi mj dn mk ml mm dp mn lo mo mp mq lp mr ms mt lq mu mv mw iw bi translated">状态-动作值函数或 Q 函数</h2><p id="f3a2" class="pw-post-body-paragraph jz ka iq kc b kd mx kf kg kh my kj kk lo mz kn ko lp na kr ks lq nb kv kw kx ij bi translated">该函数指定<strong class="kc ja">代理在具有<strong class="kc ja">策略π </strong>的状态下采取行动(a)的效果有多好。</strong></p><p id="f299" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">数学上，我们可以将<strong class="kc ja">状态-动作</strong>值函数定义为:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi om"><img src="../Images/6cbe07b078d438a27dc241ae4f7ab168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQvu_boVoGefGGwqFEFE0g.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">State-action value function</figcaption></figure><p id="f817" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">基本上，它告诉我们在具有策略π的状态下执行某个动作(a)的值。</p><p id="e3ea" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">让我们看一个马尔可夫决策过程的例子:</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c5903688e5ad4b4912b5bf749ca69006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*mdsa1XzgH4FsWF3gdFXEDA.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Example of MDP</figcaption></figure><p id="c07d" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">现在，我们可以看到没有更多的概率。事实上，现在我们的代理可以做出选择，比如醒来后，我们可以选择看《网飞》或者编码和调试。当然，代理人的行为是根据某个策略π来定义的，并且会得到相应的报酬。</p></div><div class="ab cl oo op hu oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="ij ik il im in"><p id="72a7" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">太棒了。</p><p id="c737" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">恭喜你坚持到最后！👍</p><p id="62ae" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">到目前为止，我们已经讨论了 MDP 的构建模块，在接下来的故事中，我们将讨论和<strong class="kc ja">贝尔曼期望方程</strong>，<strong class="kc ja">更多关于最优策略和最优价值函数，</strong>和<strong class="kc ja">高效的价值发现方法，即动态规划</strong>(价值迭代和策略迭代算法)并用<strong class="kc ja"> Python </strong>编程。</p><p id="8129" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">希望这个故事能增加你对 MDP 的了解。很乐意在<a class="ae ov" href="https://www.instagram.com/ayush07x/" rel="noopener ugc nofollow" target="_blank"> Instagram </a>上与您联系。</p><p id="291c" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">谢谢你与我分享你的时间！</p></div><div class="ab cl oo op hu oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="ij ik il im in"><h2 id="068b" class="mf mg iq bd mh mi mj dn mk ml mm dp mn lo mo mp mq lp mr ms mt lq mu mv mw iw bi translated">关于马尔可夫决策过程的第 1 部分、第 2 部分和第 3 部分:</h2><ul class=""><li id="803b" class="lr ls iq kc b kd mx kh my lo ow lp ox lq oy kx lw lx ly lz bi translated">强化学习:马尔可夫决策过程(上)</li><li id="2a48" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated"><a class="ae ov" rel="noopener" target="_blank" href="/reinforcement-learning-markov-decision-process-part-2-96837c936ec3">强化学习:贝尔曼方程与最优性(第二部分)</a></li><li id="517a" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated"><a class="ae ov" rel="noopener" target="_blank" href="/reinforcement-learning-solving-mdps-using-dynamic-programming-part-3-b53d32341540">强化学习:使用动态规划解决马尔可夫决策过程</a></li><li id="86c1" class="lr ls iq kc b kd ma kh mb lo mc lp md lq me kx lw lx ly lz bi translated"><a class="ae ov" href="https://pub.towardsai.net/reinforcement-learning-monte-carlo-learning-dc9b49aa16bd" rel="noopener ugc nofollow" target="_blank">强化学习:蒙特卡罗学习</a></li></ul><p id="b500" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated">参考资料:</p><ul class=""><li id="6800" class="lr ls iq kc b kd ke kh ki lo lt lp lu lq lv kx lw lx ly lz bi translated"><a class="ae ov" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">https://web . Stanford . edu/class/psych 209/Readings/suttonbartoiprlbook 2 nded . pdf</a></li></ul><p id="5e96" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk lo km kn ko lp kq kr ks lq ku kv kw kx ij bi translated"><strong class="kc ja"> <em class="kb">待深</em> </strong></p></div></div>    
</body>
</html>