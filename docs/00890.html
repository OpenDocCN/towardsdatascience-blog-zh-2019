<html>
<head>
<title>Feature Selection with sklearn and Pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 sklearn 和 Pandas 进行功能选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b?source=collection_archive---------0-----------------------#2019-02-11">https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b?source=collection_archive---------0-----------------------#2019-02-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3b66" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">要素选择方法及其在 Python 中的实现简介</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b14c53ab8d01c16619f4bf0b681b627c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wnTXS4cPw-Tw_oC79VweTQ.jpeg"/></div></div></figure><p id="04d2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在执行任何机器学习任务时，特征选择都是首要且重要的步骤之一。在数据集的情况下，一个特征仅仅意味着一列。当我们获取任何数据集时，不一定每个列(要素)都会对输出变量产生影响。如果我们在模型中添加这些不相关的特性，只会让模型变得更糟(垃圾进垃圾出)。这就产生了进行特征选择的需要。</p><p id="30fe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当在熊猫中实现特征选择时，数字特征和分类特征要区别对待。这里我们将首先讨论关于数字特征的选择。因此，在实现以下方法之前，我们需要确保数据帧只包含数字特征。此外，下面的方法是讨论回归问题，这意味着输入和输出变量都是连续的性质。</p><p id="750d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">特征选择可以通过多种方式完成，但大致有 3 类:1。过滤方法<br/> 2。包装方法<br/> 3。嵌入式方法</p><h2 id="78b7" class="ln lo iq bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me mf bi translated">关于数据集:</h2><p id="5561" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">我们将使用内置的波士顿数据集，它可以通过 sklearn 加载。我们将使用上面列出的方法为预测“MEDV”列的回归问题选择特征。在下面的代码片段中，我们将导入所有需要的库并加载数据集。</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="7c20" class="ln lo iq mm b gy mq mr l ms mt">#importing libraries<br/>from sklearn.datasets import load_boston<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import statsmodels.api as sm<br/>%matplotlib inline<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.feature_selection import RFE<br/>from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso</span><span id="8246" class="ln lo iq mm b gy mu mr l ms mt">#Loading the dataset<br/>x = load_boston()<br/>df = pd.DataFrame(x.data, columns = x.feature_names)<br/>df["MEDV"] = x.target<br/>X = df.drop("MEDV",1)   #Feature Matrix<br/>y = df["MEDV"]          #Target Variable<br/>df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/17234f21efdc731b646139a136c08678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*5ifpEDUd_O-oIr3lS87BEQ.png"/></div></figure><h1 id="d296" class="mw lo iq bd lp mx my mz ls na nb nc lv jw nd jx ly jz ne ka mb kc nf kd me ng bi translated">1.过滤方法:</h1><p id="51ba" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">顾名思义，在这种方法中，您只需要过滤并提取相关特性的子集。模型是在选择特征之后构建的。这里的过滤是使用相关矩阵完成的，最常用的是使用<a class="ae nh" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">皮尔逊相关</a>。</p><p id="4edb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这里，我们将首先绘制皮尔逊相关性热图，并查看自变量与输出变量 MEDV 的相关性。我们将只选择与输出变量的相关性高于 0.5(取绝对值)的特征。</p><p id="91fb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">相关系数的值在-1 到 1 之间<br/> —越接近 0 的值意味着相关性越弱(恰好 0 意味着不相关)<br/> —越接近 1 的值意味着正相关性越强<br/> —越接近-1 的值意味着负相关性越强</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="dd7e" class="ln lo iq mm b gy mq mr l ms mt">#Using Pearson Correlation<br/>plt.figure(figsize=(12,10))<br/>cor = df.corr()<br/>sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/2b0701b28039ffdcbd37d17c45934db7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*WCyPUnYwFajY-loYht2D8Q.png"/></div></figure><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="1b4f" class="ln lo iq mm b gy mq mr l ms mt">#Correlation with output variable<br/>cor_target = abs(cor["MEDV"])</span><span id="d2c0" class="ln lo iq mm b gy mu mr l ms mt">#Selecting highly correlated features<br/>relevant_features = cor_target[cor_target&gt;0.5]<br/>relevant_features</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/0cd10e2869e41b190f2b08fe7fbafde8.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*SU0sYwcWLuqYUoRFQ5jmtQ.png"/></div></figure><p id="14d9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如我们所见，只有特征 RM、PTRATIO 和 LSTAT 与输出变量 MEDV 高度相关。因此，我们将放弃除此之外的所有其他功能。然而，这并不是过程的结束。线性回归的假设之一是自变量需要彼此不相关。如果这些变量是相互关联的，那么我们只需要保留其中一个，去掉其余的。因此，让我们检查所选特征彼此之间的相关性。这可以通过从上面的相关矩阵或下面的代码片段中直观地检查来完成。</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="23b4" class="ln lo iq mm b gy mq mr l ms mt">print(df[["LSTAT","PTRATIO"]].corr())<br/>print(df[["RM","LSTAT"]].corr())</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/cecd1c5f718f23a90d60aac3f26a8dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*m1I3akbwGBZ0OJ7rfAwidQ.png"/></div></figure><p id="a391" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从上面的代码中可以看出，变量 RM 和 LSTAT 是高度相关的(-0.613808)。因此，我们将只保留一个变量，丢弃另一个。我们将保留 LSTAT，因为它与 MEDV 的相关性高于 RM。</p><p id="53f1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">删除 RM 后，我们只剩下两个特性，LSTAT 和 PTRATIO。这些是皮尔逊相关给出的最终特征。</p><h1 id="5125" class="mw lo iq bd lp mx my mz ls na nb nc lv jw nd jx ly jz ne ka mb kc nf kd me ng bi translated">2.包装方法:</h1><p id="6ec3" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">包装器方法需要一个机器学习算法，并使用其性能作为评估标准。这意味着，您将特征输入到选定的机器学习算法中，并根据模型性能添加/移除特征。这是一个迭代且计算量大的过程，但比滤波方法更精确。</p><p id="ff98" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有不同的包装方法，如向后消除、向前选择、双向消除和 RFE。我们将在这里讨论逆向淘汰和 RFE。</p><h2 id="b99c" class="ln lo iq bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me mf bi translated">一、逆向淘汰</h2><p id="c8ed" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">顾名思义，我们首先将所有可能的特征输入到模型中。我们检查模型的性能，然后迭代地逐一移除性能最差的特征，直到模型的整体性能在可接受的范围内。</p><p id="c6ad" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里用来评估特性性能的性能指标是<a class="ae nh" href="https://www.statsdirect.com/help/basics/p_values.htm" rel="noopener ugc nofollow" target="_blank"> pvalue </a>。如果 pvalue 大于 0.05，那么我们移除该特征，否则我们保留它。</p><p id="f7ff" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将首先在这里运行一次迭代，只是为了得到一个概念的想法，然后我们将在一个循环中运行相同的代码，这将给出最终的特性集。这里我们使用的是代表“普通最小二乘法”的 OLS 模型。该模型用于执行线性回归。</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="9fd3" class="ln lo iq mm b gy mq mr l ms mt">#Adding constant column of ones, mandatory for sm.OLS model<br/>X_1 = sm.add_constant(X)</span><span id="17bf" class="ln lo iq mm b gy mu mr l ms mt">#Fitting sm.OLS model<br/>model = sm.OLS(y,X_1).fit()<br/>model.pvalues</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/0487e3c6fba7b983203ea3c223eda2b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*sAIHPeFkK0v5RUs9AwSVYQ.png"/></div></figure><p id="010e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如我们所看到的，变量“年龄”的 p 值最高，为 0.9582293，大于 0.05。因此，我们将删除此功能，并再次构建模型。这是一个迭代过程，可以在 loop 的帮助下立即执行。这种方法在下面实施，它将给出最终的一组变量，即 CRIM、ZN、CHAS、NOX、RM、DIS、RAD、TAX、PTRATIO、B 和 LSTAT</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="fa29" class="ln lo iq mm b gy mq mr l ms mt">#Backward Elimination<br/>cols = list(X.columns)<br/>pmax = 1<br/>while (len(cols)&gt;0):<br/>    p= []<br/>    X_1 = X[cols]<br/>    X_1 = sm.add_constant(X_1)<br/>    model = sm.OLS(y,X_1).fit()<br/>    p = pd.Series(model.pvalues.values[1:],index = cols)      <br/>    pmax = max(p)<br/>    feature_with_p_max = p.idxmax()<br/>    if(pmax&gt;0.05):<br/>        cols.remove(feature_with_p_max)<br/>    else:<br/>        break</span><span id="6e94" class="ln lo iq mm b gy mu mr l ms mt">selected_features_BE = cols<br/>print(selected_features_BE)</span><span id="8845" class="ln lo iq mm b gy mu mr l ms mt"><strong class="mm ir">Output:</strong></span><span id="ce22" class="ln lo iq mm b gy mu mr l ms mt"><em class="nl">['CRIM', 'ZN', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']</em></span></pre><h2 id="8990" class="ln lo iq bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me mf bi translated">二。RFE(递归特征消除)</h2><p id="367a" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated"><a class="ae nh" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">递归特征消除</a> (RFE)方法的工作原理是递归移除属性，并在那些保留的属性上建立模型。它使用准确性度量来根据特征的重要性对其进行排序。RFE 方法将使用的模型和所需特征的数量作为输入。然后给出所有变量的排名，1 是最重要的。它也给出了支持，真的是相关特征，假的是不相关特征。</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="53c6" class="ln lo iq mm b gy mq mr l ms mt">model = LinearRegression()</span><span id="aa31" class="ln lo iq mm b gy mu mr l ms mt">#Initializing RFE model<br/>rfe = RFE(model, 7)</span><span id="3b7a" class="ln lo iq mm b gy mu mr l ms mt">#Transforming data using RFE<br/>X_rfe = rfe.fit_transform(X,y)  </span><span id="7cd0" class="ln lo iq mm b gy mu mr l ms mt">#Fitting the data to model<br/>model.fit(X_rfe,y)<br/>print(rfe.support_)<br/>print(rfe.ranking_)</span><span id="3594" class="ln lo iq mm b gy mu mr l ms mt"><strong class="mm ir">Output:</strong></span><span id="d944" class="ln lo iq mm b gy mu mr l ms mt"><em class="nl">[False False False  True  True  True False  True  True False  True False<br/>  True]<br/>[2 4 3 1 1 1 7 1 1 5 1 6 1]</em></span></pre><p id="45f9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里我们采用了 7 个特征的线性回归模型，RFE 给出了如上的特征排名，但是数字“7”的选择是随机的。现在，我们需要找到最佳数量的特征，其精度是最高的。我们通过使用从 1 个特征开始到 13 个特征的循环来实现。然后我们取精确度最高的那个。</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="8f41" class="ln lo iq mm b gy mq mr l ms mt">#no of features<br/>nof_list=np.arange(1,13)            <br/>high_score=0<br/>#Variable to store the optimum features<br/>nof=0           <br/>score_list =[]<br/>for n in range(len(nof_list)):<br/>    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)<br/>    model = LinearRegression()<br/>    rfe = RFE(model,nof_list[n])<br/>    X_train_rfe = rfe.fit_transform(X_train,y_train)<br/>    X_test_rfe = rfe.transform(X_test)<br/>    model.fit(X_train_rfe,y_train)<br/>    score = model.score(X_test_rfe,y_test)<br/>    score_list.append(score)<br/>    if(score&gt;high_score):<br/>        high_score = score<br/>        nof = nof_list[n]</span><span id="4fe9" class="ln lo iq mm b gy mu mr l ms mt">print("Optimum number of features: %d" %nof)<br/>print("Score with %d features: %f" % (nof, high_score))</span><span id="9703" class="ln lo iq mm b gy mu mr l ms mt"><strong class="mm ir">Output:</strong></span><span id="df37" class="ln lo iq mm b gy mu mr l ms mt"><em class="nl">Optimum number of features: 10<br/>Score with 10 features: 0.663581</em></span></pre><p id="7ef6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从上面的代码可以看出，特性的最佳数量是 10。我们现在向 RFE 输入 10 个特征，并得到由 RFE 方法给出的最终特征集，如下所示:</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="af96" class="ln lo iq mm b gy mq mr l ms mt">cols = list(X.columns)<br/>model = LinearRegression()</span><span id="c78e" class="ln lo iq mm b gy mu mr l ms mt">#Initializing RFE model<br/>rfe = RFE(model, 10)             </span><span id="b87f" class="ln lo iq mm b gy mu mr l ms mt">#Transforming data using RFE<br/>X_rfe = rfe.fit_transform(X,y)  </span><span id="0398" class="ln lo iq mm b gy mu mr l ms mt">#Fitting the data to model<br/>model.fit(X_rfe,y)              <br/>temp = pd.Series(rfe.support_,index = cols)<br/>selected_features_rfe = temp[temp==True].index<br/>print(selected_features_rfe)</span><span id="bc07" class="ln lo iq mm b gy mu mr l ms mt"><strong class="mm ir">Output:</strong></span><span id="9363" class="ln lo iq mm b gy mu mr l ms mt"><em class="nl">Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'PTRATIO',<br/>       'LSTAT'],<br/>      dtype='object')</em></span></pre><h1 id="2fa7" class="mw lo iq bd lp mx my mz ls na nb nc lv jw nd jx ly jz ne ka mb kc nf kd me ng bi translated">3.嵌入式方法</h1><p id="a52d" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">嵌入式方法在某种意义上是迭代的，它负责模型训练过程的每次迭代，并仔细提取那些对特定迭代的训练贡献最大的特征。正则化方法是最常用的嵌入式方法，其在给定系数阈值的情况下惩罚特征。</p><p id="843d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里我们将使用套索正则化进行特征选择。如果特征是不相关的，套索惩罚它的系数，使其为 0。因此，系数= 0 的特征被移除，其余的被采用。</p><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="aa73" class="ln lo iq mm b gy mq mr l ms mt">reg = LassoCV()<br/>reg.fit(X, y)<br/>print("Best alpha using built-in LassoCV: %f" % reg.alpha_)<br/>print("Best score using built-in LassoCV: %f" %reg.score(X,y))<br/>coef = pd.Series(reg.coef_, index = X.columns)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1e139d09d22be16d718b7be7bd64d978.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*H8RZZE92U0LFIp9CWwz6TQ.png"/></div></figure><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="3de3" class="ln lo iq mm b gy mq mr l ms mt">print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " +  str(sum(coef == 0)) + " variables")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/98f0a27cf412ae776d3ad006fbc5f102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*l3-0MGNHTVCqx6fWJ8LxAA.png"/></div></figure><pre class="kg kh ki kj gt ml mm mn mo aw mp bi"><span id="64c6" class="ln lo iq mm b gy mq mr l ms mt">imp_coef = coef.sort_values()<br/>import matplotlib<br/>matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)<br/>imp_coef.plot(kind = "barh")<br/>plt.title("Feature importance using Lasso Model")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/1696c6dde7c1997bc7d15b3075b11fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*3dreouxOqD6igkwAp5_ysg.png"/></div></figure><p id="5bc1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这里，套索模型已经采取了除氮氧化物，查斯和印度河的所有功能。</p><h1 id="7004" class="mw lo iq bd lp mx my mz ls na nb nc lv jw nd jx ly jz ne ka mb kc nf kd me ng bi translated">结论:</h1><p id="2c4a" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">我们看到了如何使用多种方法为数字数据选择特征，并比较了它们的结果。现在出现了在什么情况下选择哪种方法的困惑。以下几点将帮助你做出这个决定。</p><ol class=""><li id="a3f1" class="np nq iq kt b ku kv kx ky la nr le ns li nt lm nu nv nw nx bi translated">过滤方法不太准确。在做 EDA 的时候很棒，也可以用来检查数据中的多重共线性。</li><li id="27da" class="np nq iq kt b ku ny kx nz la oa le ob li oc lm nu nv nw nx bi translated">包装器和嵌入式方法给出了更精确的结果，但是由于它们的计算量很大，所以这些方法适用于特性较少的情况(~20)。</li></ol><p id="bcda" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在下一篇博客中，我们将会看到更多的用于选择数字和分类特征的特征选择方法。</p></div></div>    
</body>
</html>