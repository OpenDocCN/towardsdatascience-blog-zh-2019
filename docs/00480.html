<html>
<head>
<title>Introduction to Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148?source=collection_archive---------0-----------------------#2019-01-22">https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148?source=collection_archive---------0-----------------------#2019-01-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="fbf3" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">介绍</h1><p id="51dd" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在这个博客中，我们将讨论逻辑回归的基本概念以及它能帮助我们解决什么样的问题。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/3602ad0b655e2c70a41eb65112996d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*PQ8tdohapfm-YHlrRIRuOA.gif"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">GIF: University of Toronto</figcaption></figure><p id="50a6" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">逻辑回归是一种分类算法，用于将观察值分配给一组离散的类。分类问题的一些例子是电子邮件垃圾邮件或非垃圾邮件、在线交易欺诈或非欺诈、恶性肿瘤或良性肿瘤。逻辑回归使用逻辑 sigmoid 函数转换其输出，以返回概率值。</p><h2 id="c111" class="me jo iq bd jp mf mg dn jt mh mi dp jx kw mj mk kb la ml mm kf le mn mo kj mp bi translated"><strong class="ak">逻辑回归有哪些类型</strong></h2><ol class=""><li id="a54f" class="mq mr iq kn b ko kp ks kt kw ms la mt le mu li mv mw mx my bi translated">二元(如恶性肿瘤或良性肿瘤)</li><li id="2ba0" class="mq mr iq kn b ko mz ks na kw nb la nc le nd li mv mw mx my bi translated">多线性函数失败类(如猫、狗或羊的)</li></ol><h1 id="be77" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">逻辑回归</h1><p id="4951" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">逻辑回归是一种用于分类问题的机器学习算法，它是一种基于概率概念的预测分析算法。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi ne"><img src="../Images/03d65b6e8a2667093984831559c3bbf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Linear Regression VS Logistic Regression Graph| Image: Data Camp</figcaption></figure><p id="b451" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">我们可以将逻辑回归称为线性回归模型，但是逻辑回归使用更复杂的成本函数，该成本函数可以定义为“<strong class="kn ir"> Sigmoid 函数</strong>，或者也称为“逻辑函数”，而不是线性函数。</p><p id="daf1" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">逻辑回归的假设倾向于将成本函数限制在 0 和 1 之间。因此，线性函数不能表示它，因为它可以具有大于 1 或小于 0 的值，根据逻辑回归的假设，这是不可能的。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/aff70a58129349cdcae7c96ddd17b4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*GnceHPIeThNShGSmYzE4eA.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Logistic regression hypothesis expectation</figcaption></figure><h2 id="0145" class="me jo iq bd jp mf mg dn jt mh mi dp jx kw mj mk kb la ml mm kf le mn mo kj mp bi translated">什么是乙状结肠函数？</h2><p id="4e06" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了将预测值映射到概率，我们使用 Sigmoid 函数。该函数将任何实数值映射到 0 和 1 之间的另一个值。在机器学习中，我们使用 sigmoid 将预测映射到概率。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7cc57662cffc664f3c8fab70b24af42e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*OUOB_YF41M-O4GgZH_F2rw.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Sigmoid Function Graph</figcaption></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/29c50cb65f948e63552090e816307b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*Gp5E23P5d2PY5D5kOo8ePw.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Formula of a sigmoid function | Image: Analytics India Magazine</figcaption></figure><h1 id="03e1" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">假设表象</strong></h1><p id="2288" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">使用<em class="ni">线性回归</em>时，我们使用假设的公式，即</p><blockquote class="nj nk nl"><p id="5da8" class="kl km ni kn b ko lz kq kr ks ma ku kv nm mb ky kz nn mc lc ld no md lg lh li ij bi translated">hθ(x)=β₀+β₁x</p></blockquote><p id="d5be" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">对于逻辑回归，我们将对其稍加修改，即</p><blockquote class="nj nk nl"><p id="bcec" class="kl km ni kn b ko lz kq kr ks ma ku kv nm mb ky kz nn mc lc ld no md lg lh li ij bi translated">σ(Z) = σ(β₀ + β₁X)</p></blockquote><p id="a88e" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">我们预计我们的假设将给出介于 0 和 1 之间的值。</p><blockquote class="np"><p id="1bf7" class="nq nr iq bd ns nt nu nv nw nx ny li dk translated">Z = β₀ + β₁X</p><p id="d15f" class="nq nr iq bd ns nt nu nv nw nx ny li dk translated">hθ(x)= sigmoid(Z)</p><p id="f902" class="nq nr iq bd ns nt nu nv nw nx ny li dk translated">即 hθ(x)= 1/(1+e^-(β₀+β₁x)</p></blockquote><figure class="oa ob oc od oe lo gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a4ca3c59445cab44c7602ddc7c4fc1c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*l59BUnPwWHMf1H-GNxgZHQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">The Hypothesis of logistic regression</figcaption></figure><h2 id="b118" class="me jo iq bd jp mf mg dn jt mh mi dp jx kw mj mk kb la ml mm kf le mn mo kj mp bi translated">判别边界</h2><p id="6186" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">当我们通过一个预测函数传递输入并返回一个介于 0 和 1 之间的概率分数时，我们期望我们的分类器根据概率给出一组输出或类。</p><p id="04d8" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">例如，我们有 2 个类，让我们像猫和狗一样对待它们(1 —狗，0 —猫)。我们基本上确定一个阈值，高于该阈值的值我们将其分类为第 1 类，低于该阈值的值我们将其分类为第 2 类。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi of"><img src="../Images/3c81dd8c73bbac0bd7e52a15ef692922.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*2Vsum532aNQX9TgR7_rAzQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Example</figcaption></figure><p id="e1f9" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">如上图所示，我们选择阈值为 0.5，如果预测函数返回值为 0.7，则我们会将此观察结果分类为 1 类(狗)。如果我们的预测返回值为 0.2，那么我们会将观察结果分类为第 2 类(CAT)。</p><h1 id="43d1" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">价值函数</h1><p id="964f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们在<a class="ae og" rel="noopener" target="_blank" href="/introduction-to-linear-regression-and-polynomial-regression-f8adc96f31cb"><em class="ni"/></a>线性回归中学习了成本函数<em class="ni"> J </em> ( <em class="ni"> θ </em>，成本函数代表优化目标，即我们创建一个成本函数并将其最小化，以便我们可以开发一个误差最小的精确模型。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/4c989f0f2054546a20ff186ec5a4277e.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*N6THdTd451D4C2RAhhqRCQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">The Cost function of Linear regression</figcaption></figure><p id="4e04" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">如果我们试图在“逻辑回归”中使用线性回归的成本函数，那么它将是无用的，因为它最终将是一个具有许多局部最小值的<strong class="kn ir">非凸</strong>函数，其中很难<strong class="kn ir">使</strong>最小化成本值并找到全局最小值。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi oi"><img src="../Images/42a764e7f25956f9a2050f277320ea3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dPXwswig8RTCAjstnUZNGQ.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Non-convex function</figcaption></figure><p id="255b" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">对于逻辑回归，成本函数定义为:</p><blockquote class="nj nk nl"><p id="3705" class="kl km ni kn b ko lz kq kr ks ma ku kv nm mb ky kz nn mc lc ld no md lg lh li ij bi translated">如果 y = 1，log( <em class="iq"> hθ </em> ( <em class="iq"> x </em>))</p><p id="e057" class="kl km ni kn b ko lz kq kr ks ma ku kv nm mb ky kz nn mc lc ld no md lg lh li ij bi translated">如果 y = 0，log(1<em class="iq">hθ</em>(<em class="iq">x</em>))</p></blockquote><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi oj"><img src="../Images/f95e4160343f46cde07ea5a9f38c5954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2g14OVjyJqio2zXwJxgj2w.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Cost function of Logistic Regression</figcaption></figure><div class="lk ll lm ln gt ab cb"><figure class="ok lo ol om on oo op paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><img src="../Images/ddd4056a2379df55ca959d3b91431d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*gAsyT-YdsQZUMF81NTZQdQ.png"/></div></figure><figure class="ok lo oq om on oo op paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><img src="../Images/c18976be7ea7f9e72763c01d70b787ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*2QLAi8r4BWFZ4AC6aQLzbA.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk or di os ot">Graph of logistic regression</figcaption></figure></div><p id="c9d9" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">上述两个函数可以压缩成一个函数，即</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi ou"><img src="../Images/08f69b117bed7ebc09b6d58d5e3c01d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_52kKSp8zWgVTNtnE2eYrg.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Above functions compressed into one cost function</figcaption></figure><h1 id="0792" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">梯度下降</h1><p id="6db8" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在问题来了，我们如何降低成本价值。嗯，这个可以用<strong class="kn ir">梯度下降来实现。</strong>梯度下降的主要目标是<strong class="kn ir">最小化成本值。</strong>即 min J( <strong class="kn ir"> <em class="ni"> θ </em> </strong>)。</p><p id="7437" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">现在，为了最小化成本函数，我们需要对每个参数运行梯度下降函数，即</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/cb1de3a6b683e450130efb59ff6445b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*1--MUhjPjOL7oYdVo7R6gQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Objective: To minimize the cost function we have to run the gradient descent function on each parameter</figcaption></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi ow"><img src="../Images/409d209c9c42fd5ba6f090a3279893a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ecea3jVIRxK4Mkrh_Nie4w.jpeg"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Gradient Descent Simplified | Image: Andrew Ng Course</figcaption></figure><p id="fa75" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">梯度下降有一个类比，我们必须想象自己在一个山谷的山顶，被蒙住眼睛，束手无策，我们的目标是到达山脚。感觉你周围地形的坡度是每个人都会做的。这个动作类似于计算梯度下降，而走一步类似于更新参数的一次迭代。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi ox"><img src="../Images/c1e47785041e58c7cb8ee40eb29b19db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SzVGKaga11mEwpJ1EpQJOw.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Gradient Descent analogy</figcaption></figure><h1 id="7b46" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论</h1><p id="af74" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在这篇博客中，我向你介绍了逻辑回归的基本概念。我希望这篇博客对你有所帮助，并能激发你对这个话题的兴趣。</p></div></div>    
</body>
</html>