<html>
<head>
<title>Decision Trees and Random Forests:</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树和随机森林:</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-and-random-forests-74b89a374db?source=collection_archive---------17-----------------------#2019-09-08">https://towardsdatascience.com/decision-trees-and-random-forests-74b89a374db?source=collection_archive---------17-----------------------#2019-09-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="573f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">min _ 杂质 _ 减少是做什么的，应该怎么用？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5196e4c847e491adb9940bfaf0959e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_r3urDCO82xmHDjI9OgmQ.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@filipz?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Filip Zrnzević</a> on <a class="ae kv" href="https://unsplash.com/search/photos/trees?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1922" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我学习决策树和随机森林的过程中，我注意到许多超参数被广泛讨论和使用。最大深度、最小样本叶等。，包括仅适用于随机森林的超级参数。一个似乎很少被关注的超参数是最小杂质减少。</p><h2 id="662b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">最小杂质减少是什么意思，有什么作用？</strong></h2><p id="fae5" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">要理解这一点，你首先要理解超参数的基本标准是什么。所用的标准是测量每次分裂的杂质，从而计算出通过分裂获得的信息。然后，它可以决定如何进行最佳分割。当杂质为 0 时，则该叶子中的所有项目都被正确分类。</p><p id="ede2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于基尼指数和熵的更长的解释可以在这篇文章中看到:</p><div class="mq mr gp gr ms mt"><a rel="noopener follow" target="_blank" href="/gini-index-vs-information-entropy-7a7e4fed3fcb"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd ir gy z fp my fr fs mz fu fw ip bi translated">基尼指数与信息熵</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">关于杂质测量和信息增益，您需要了解的一切</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh kp mt"/></div></div></a></div><p id="b6c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">min _ infinity _ decrease 的作用是在分解的减少额小于输入额时停止分解。当我们继续将它形象化时，这将更容易理解。</p><h2 id="b09f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">可视化决策树以查看正在进行哪些拆分</h2><p id="904e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">让我们看看我用虹膜数据集和所有默认的超参数做的决策树。请注意，由于这个数据集的情况，从这个树上得到的分数对于训练和测试来说几乎是完美的。因此，我不会在本文中显示每个模型运行的分数，我只是试图展示如何使用 min _ infinity _ decrease 超参数。</p><p id="5482" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是训练数据的决策树的可视化，每个节点和叶都有基尼指数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/2bb768a29a8b109157423adb17e35720.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YmkdjuBhFkQ0jNzAF5Jv3A.png"/></div></div></figure><p id="5436" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练集中有 120 个样本，我们试图对 3 种类型的虹膜进行分类。第一次拆分右侧的值= [0，41，39]意味着第一组中没有任何项目被错误分类，第二组和第三组中的项目数量几乎相等。因此，该节点的杂质被舍入到 0.5，这意味着它们中的大约一半被正确分类，而大约一半被错误分类。</p><p id="973b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在建模中试图最小化的事情之一是过度拟合。其原因是，如果我们过度适应我们的训练数据，那么我们可能会对我们的测试数据或真实数据进行错误分类。因此，我们希望尽量减少分裂成很小数量的叶子。</p><p id="c380" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这也是最小杂质减少可以帮助我们的地方。如果分裂的不纯度上升，那么这将意味着我们正在进行分裂，这对我们没有帮助，而是过度适应训练数据。</p><p id="e67b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看上面决策树中的一个例子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/727389a695d814198a1055395f2acfa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u5uiPw0r_6_3aRohYMU6bg.png"/></div></div></figure><p id="3b66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是正在进行的第三级分割。<br/>从右侧可以看出，我们对 5 个分类错误的项目进行了拆分。在这条分界线的左边，有 4 个正确的和 4 个不正确的，基尼系数为 0.5。在右边，有 34 个正确的和 1 个不正确的。<br/>我们似乎在这方面做得太多了，而且这样做并没有获得太多。<br/>在左边，我们有 36 个正确的和一个不正确的，然后我们做一个分割来捕捉最后一个。这也过度拟合了我们的训练数据。</p><p id="6aad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们如何处理这两种分裂？</p><h2 id="4cde" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">最小杂质减少量的计算</h2><p id="2d76" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">这个超参数是怎么计算出来的？如果我们检查它的文档，可以在这里找到<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">，那么我们会看到这个:</a></p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="1b3b" class="ls lt iq nl b gy np nq l nr ns">min_impurity_decrease : float, optional (default=0.)</span><span id="3882" class="ls lt iq nl b gy nt nq l nr ns">A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</span><span id="2967" class="ls lt iq nl b gy nt nq l nr ns">The weighted impurity decrease equation is the following::</span><span id="23fb" class="ls lt iq nl b gy nt nq l nr ns">N_t / N * (impurity — N_t_R / N_t * right_impurity<br/> — N_t_L / N_t * left_impurity)</span><span id="bdac" class="ls lt iq nl b gy nt nq l nr ns">where ``N`` is the total number of samples, ``N_t`` is the number of<br/>samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child.</span><span id="6f50" class="ls lt iq nl b gy nt nq l nr ns">``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.</span></pre><p id="0784" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们分解这个计算，以便更好地理解它:<br/><strong class="ky ir">N _ t _ R/N _ t * right _ infinity:</strong>取分裂右侧的杂质，并根据来自节点的右侧分裂中的样本百分比对其进行加权。</p><p id="f6f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">N _ t _ L/N _ t * left _ infinity:</strong>取左侧分裂的杂质，用左侧分裂中样本占节点的百分比进行加权。</p><p id="40eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们从节点杂质中减去这两者。<br/>然后，我们根据节点中样本占样本总数的百分比对该数量进行加权。</p><p id="f5b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们把它写在分割的右边:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="46dd" class="ls lt iq nl b gy np nq l nr ns">43/120 * (.206 - 35/43 * .056 - 8/43*.5)</span></pre><p id="19b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们得到的总数是 0.024149999999999999998，或者四舍五入到 0.024。</p><p id="d97f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">43 是正在分裂的节点中的样本总数<br/> 120 是样本总数<br/>206 是节点的杂质<br/> 35 是分裂右侧的样本总数<br/>056 是分裂右侧的杂质<br/> 8 是分裂左侧的样本总数<br/>5 是分裂左侧的杂质</p><p id="8d96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">手动计算这一切并不容易，我制作了一个函数，允许您输入上述数字，它将为您计算分裂的杂质:</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="93af" class="ls lt iq nl b gy np nq l nr ns">def get_impurity_score(n=None, n_t=None, n_t_l=None, n_t_r=None, node_impurity=None, left_impurity=None, right_impurity=None):<br/>    '''<br/>    n = Total number of samples overall<br/>    n_t = Samples in the node you are checking out<br/>    n_t_l = Samples in the left split from the node<br/>    n_t_r = Samples in the right split from the node<br/>    node_impurity = Impurity score of the node<br/>    left_impurity = Impurity score of the left split from the node<br/>    right_impurity = Impurity score of the right split from the node<br/>    <br/>    Returns impurity decrease total of the 2 splits from the node impurity weighted by the total number of samples<br/>    '''<br/>    if n == None or n_t == None or n_t_l == None or n_t_r == None or node_impurity == None or left_impurity == None or right_impurity == None:<br/>        print('-----error------ missing values. Check inputs')</span><span id="860c" class="ls lt iq nl b gy nt nq l nr ns">    return(n_t / n * (node_impurity - (n_t_r / n_t * right_impurity) - (n_t_l / n_t * left_impurity)))</span></pre><p id="a195" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">左侧分离的杂质分数为 0.016341666666666666666，或四舍五入至 0.016。</p><h2 id="1d85" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">用这个来修复我们的决策树</strong></h2><p id="fbab" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">如果我们现在将最小杂质减少量设置为 0.025，则不会发生分裂，并且我们将减少模型中的过拟合。</p><p id="77c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们更新的树:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d5ae6801c35eae0eed4e2826136f1591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*D_BJ6HpXqAN0nUvMgT7u6A.png"/></div></figure><p id="a18e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在确实阻止了这两种分裂的发生，以帮助防止我们的决策树过度拟合。</p><h2 id="7439" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">在随机森林中使用最小杂质减少</h2><p id="87f6" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">要在随机森林中使用它，我建议使用 GridsearchCV 来帮助找到对您试图构建的模型有益的超参数。</p><p id="8ebf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一般来说，我会建议使用这个和其他超参数来防止过度拟合。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="b98b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这有助于你理解最小杂质减少和如何使用它。如果您有任何问题或意见，请在下面的评论区提出。</p></div></div>    
</body>
</html>