<html>
<head>
<title>Simple Transformers — Named Entity Recognition with Transformer Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单变压器——变压器模型的命名实体识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-transformers-named-entity-recognition-with-transformer-models-c04b9242a2a0?source=collection_archive---------7-----------------------#2019-10-29">https://towardsdatascience.com/simple-transformers-named-entity-recognition-with-transformer-models-c04b9242a2a0?source=collection_archive---------7-----------------------#2019-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e1f5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">简单的变形金刚是“它只是工作”的变形金刚库。使用 Transformer 模型进行命名实体识别，只需 3 行代码。是的，真的。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/278a18c2189278da16d229d14c5b0e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EBWanyh_UMq96bYshlcsPw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@brandi1?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Brandi Redd</a> on <a class="ae ky" href="https://unsplash.com/s/photos/book-glasses?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="1f5d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">前言</h1><p id="ce9d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">简单变形金刚库是为了使变形金刚模型易于使用而设计的。变形金刚是非常强大的(更不用说巨大的)深度学习模型，在处理各种各样的自然语言处理任务方面取得了巨大的成功。Simple Transformers 使 Transformer 模型的应用程序能够用三行代码完成序列分类任务(最初是二进制分类，但不久后增加了多类分类)。</p><p id="72e3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我很高兴地宣布，Simple Transformers 现在支持命名实体识别，这是另一个常见的 NLP 任务，以及序列分类。</p><p id="a95b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">其他功能链接:</em></p><ul class=""><li id="ac46" class="mt mu it lt b lu mn lx mo ma mv me mw mi mx mm my mz na nb bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3"> <em class="ms">二元序列分类用的简单变形金刚</em> </a></li><li id="5b0b" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><a class="ae ky" href="https://medium.com/swlh/simple-transformers-multi-class-text-classification-with-bert-roberta-xlnet-xlm-and-8b585000ce3a" rel="noopener"> <em class="ms">用简单变压器进行多类序列分类</em> </a></li></ul><p id="b9ad" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank">简单变形金刚</a>库是在优秀的<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库的基础上，通过抱脸的方式构建的。拥抱脸变形金刚图书馆是为研究人员和其他需要广泛控制事情如何完成的人准备的图书馆。当你需要偏离常规，做不同的事情，或者完全做新的事情时，这也是最好的选择。简单的变形金刚简单多了。</p><h1 id="5f6f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="d62c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你想尝试这个绝妙的想法，你想卷起袖子开始工作，但是成千上万行看起来神秘(但很酷)的代码甚至会让一个资深的 NLP 研究员感到害怕。简单变形金刚背后的核心思想是，使用变形金刚并不需要很困难(或令人沮丧)。</p><p id="6b74" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">简单的转换器抽象出所有复杂的设置代码，同时尽可能保留灵活性和配置空间。Transformer 模型只需要三行代码，一行用于初始化，一行用于训练，一行用于评估。</p><p id="4b93" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这篇文章演示了如何使用简单的变压器执行 NER。</p><p id="7650" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">所有源代码可在</em><a class="ae ky" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank"><em class="ms">Github Repo</em></a><em class="ms">上获得。如果您有任何问题或疑问，这是解决它们的地方。请务必检查一下！</em></p><h1 id="197e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">装置</h1><ol class=""><li id="bcdc" class="mt mu it lt b lu lv lx ly ma nh me ni mi nj mm nk mz na nb bi translated">从<a class="ae ky" href="https://www.anaconda.com/distribution/" rel="noopener ugc nofollow" target="_blank">这里</a>安装 Anaconda 或 Miniconda 包管理器</li><li id="221d" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm nk mz na nb bi translated">创建新的虚拟环境并安装所需的软件包。<br/> <code class="fe nl nm nn no b">conda create -n transformers python pandas tqdm</code> <br/> <code class="fe nl nm nn no b">conda activate transformers</code> <br/>如果使用 cuda: <br/> <code class="fe nl nm nn no b">conda install pytorch cudatoolkit=10.0 -c pytorch</code> <br/>其他:<br/><code class="fe nl nm nn no b">conda install pytorch cpuonly -c pytorch</code><br/><code class="fe nl nm nn no b">conda install -c anaconda scipy</code><br/><code class="fe nl nm nn no b">conda install -c anaconda scikit-learn</code><br/><code class="fe nl nm nn no b">pip install transformers</code><br/><code class="fe nl nm nn no b">pip install tensorboardx<br/></code><code class="fe nl nm nn no b">pip install seqeval</code></li><li id="d183" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm nk mz na nb bi translated"><strong class="lt iu">安装<em class="ms">简单变压器</em></strong> <br/> <code class="fe nl nm nn no b">pip install simpletransformers</code></li></ol><h1 id="1e84" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">使用</h1><p id="bc7b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了演示命名实体识别，我们将使用<a class="ae ky" href="https://www.clips.uantwerpen.be/conll2003/ner/" rel="noopener ugc nofollow" target="_blank"> CoNLL </a>数据集。获得这个数据集可能有点棘手，但我在 Kaggle 上找到了一个版本，它可以满足我们的目的。</p><h2 id="f27c" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">数据准备</h2><ol class=""><li id="899e" class="mt mu it lt b lu lv lx ly ma nh me ni mi nj mm nk mz na nb bi translated">从<a class="ae ky" href="https://www.kaggle.com/alaakhaled/conll003-englishversion/download" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载数据集。</li><li id="2162" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm nk mz na nb bi translated">将文本文件解压到<code class="fe nl nm nn no b">data/</code>目录。(它应该包含 3 个文本文件<code class="fe nl nm nn no b">train.txt, valid.txt, test.txt</code>。我们将使用<code class="fe nl nm nn no b">train</code>和<code class="fe nl nm nn no b">test</code>文件。您可以使用<code class="fe nl nm nn no b">valid</code>文件来执行超参数调整，以提高模型性能。</li></ol><p id="065c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">简单变形金刚的 NER 模型既可以使用</em> <code class="fe nl nm nn no b"><em class="ms">.txt</em></code> <em class="ms">文件，也可以使用熊猫</em> <code class="fe nl nm nn no b"><em class="ms">DataFrames</em></code> <em class="ms">。关于</em> <code class="fe nl nm nn no b"><em class="ms">DataFrames</em></code> <em class="ms">的使用示例，请参考</em> <a class="ae ky" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank"> <em class="ms">回购</em> </a> <em class="ms">单据中的 NER 最小启动示例。</em></p><p id="77e8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用自己的数据集时，输入文本文件应该遵循 CoNLL 格式。文件中的每一行都应该包含一个单词及其相关的标记，每个标记之间用一个空格隔开。简单的转换器假定一行中的第一个“单词”是实际的单词，一行中的最后一个“单词”是它的指定标签。为了表示一个新的句子，在前一个句子的最后一个单词和下一个句子的第一个单词之间添加一个空行。然而，在使用定制数据集时，使用<code class="fe nl nm nn no b">DataFrame</code>方法可能更容易。</p><h2 id="9e28" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">神经模型</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="eade" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们创建了一个<code class="fe nl nm nn no b">NERModel</code>,可用于 NER 任务中的训练、评估和预测。一个<code class="fe nl nm nn no b">NERModel</code>对象的完整参数列表如下。</p><ul class=""><li id="8f73" class="mt mu it lt b lu mn lx mo ma mv me mw mi mx mm my mz na nb bi translated"><code class="fe nl nm nn no b">model_type</code>:型号类型(伯特、罗伯塔)</li><li id="3a32" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><code class="fe nl nm nn no b">model_name</code>:默认的变压器模型名称或包含变压器模型文件的目录路径(pytorch_nodel.bin)。</li><li id="3880" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><code class="fe nl nm nn no b">labels</code>(可选):所有命名实体标签的列表。如果没有给出，将使用["O "，" B-MISC "，" I-MISC "，" B-PER "，" I-PER "，" B-ORG "，" I-ORG "，" B-LOC "，" I-LOC"]。</li><li id="b2a6" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><code class="fe nl nm nn no b">args</code>(可选):如果未提供此参数，将使用默认参数。如果提供的话，它应该是一个包含应该在默认参数中更改的参数的字典。</li><li id="715a" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><code class="fe nl nm nn no b">use_cuda</code>(可选):如果可用，使用 GPU。设置为 False 将强制模型仅使用 CPU。</li></ul><p id="77c5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要加载一个先前保存的模型而不是默认模型，您可以将<code class="fe nl nm nn no b">model_name</code> <em class="ms"> </em>更改为包含已保存模型的目录路径。</p><pre class="kj kk kl km gt od no oe of aw og bi"><span id="8d24" class="np la it no b gy oh oi l oj ok">model = NERModel(‘bert’, ‘path_to_model/’)</span></pre><p id="6bdf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一个<code class="fe nl nm nn no b">NERModel</code>包含一个 python 字典<code class="fe nl nm nn no b">args</code>,它有许多属性提供对超参数的控制。在<a class="ae ky" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank">回购文件</a>中提供了每一个的详细描述。默认值如下所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="d354" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当创建一个<code class="fe nl nm nn no b">NERModel</code>或调用它的<code class="fe nl nm nn no b">train_model</code>方法时，只要传入一个包含要更新的键值对的<code class="fe nl nm nn no b">dict</code>就可以修改这些属性。下面给出一个例子。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h2 id="b733" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">训练模型</h2><p id="280c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">正如承诺的那样，训练可以在一行代码中完成。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="b291" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe nl nm nn no b">train_model</code>方法将在每第 n 步创建一个模型的检查点(保存)，其中 n 是<code class="fe nl nm nn no b">self.args['save_steps']</code>。训练完成后，最终模型将保存到<code class="fe nl nm nn no b">self.args['output_dir']</code>。</p><p id="8067" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">加载保存的模型如下所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h2 id="3f86" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">评估模型</h2><p id="2ab1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">同样，评估只是一行代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="f8f1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里，三个返回值是:</p><ul class=""><li id="cfb2" class="mt mu it lt b lu mn lx mo ma mv me mw mi mx mm my mz na nb bi translated"><code class="fe nl nm nn no b">result</code>:包含评估结果的字典。(eval_loss，precision，recall，f1_score)</li><li id="5d5b" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><code class="fe nl nm nn no b">model_outputs</code>:原始模型输出列表</li><li id="8cb8" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><code class="fe nl nm nn no b">preds_list</code>:预测标签列表</li></ul><p id="1df4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里给出我得到的评测结果，供参考。</p><pre class="kj kk kl km gt od no oe of aw og bi"><span id="8843" class="np la it no b gy oh oi l oj ok">{'eval_loss': 0.10684790916955669, 'precision': 0.9023580786026201, 'recall': 0.9153082919914954, 'f1_score': 0.9087870525112148}</span></pre><p id="9b0b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于使用默认超参数值的单次运行来说，还不算太差！</p><h2 id="3b3f" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">把所有的放在一起</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h2 id="faf3" class="np la it bd lb nq nr dn lf ns nt dp lj ma nu nv ll me nw nx ln mi ny nz lp oa bi translated">预测和测试</h2><p id="1113" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在实际应用中，我们通常不知道真正的标签应该是什么。要对任意示例执行预测，可以使用<code class="fe nl nm nn no b">predict</code>方法。这个方法与<code class="fe nl nm nn no b">eval_model</code>方法非常相似，除了它接受一个文本列表并返回一个预测列表和一个模型输出列表。</p><pre class="kj kk kl km gt od no oe of aw og bi"><span id="5c01" class="np la it no b gy oh oi l oj ok">predictions, raw_outputs = model.predict(["Some arbitary sentence"])</span></pre><h1 id="ddcc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">包扎</h1><p id="f82b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Simple Transformers 提供了一种快速简单的方法来执行命名实体识别(以及其他令牌级分类任务)。借用伯特背后的人的一句台词，简单变形金刚“概念简单，经验丰富”。</p></div></div>    
</body>
</html>