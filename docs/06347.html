<html>
<head>
<title>Importance of Loss Function in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">损失函数在机器学习中的重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/importance-of-loss-function-in-machine-learning-eddaaec69519?source=collection_archive---------9-----------------------#2019-09-12">https://towardsdatascience.com/importance-of-loss-function-in-machine-learning-eddaaec69519?source=collection_archive---------9-----------------------#2019-09-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="b09f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最初发表于<a class="ae kl" href="https://iq.opengenus.org/importance-of-loss-function/" rel="noopener ugc nofollow" target="_blank"> OpenGenus IQ </a>。</p><p id="ff5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi km translated"><span class="l kn ko kp bm kq kr ks kt ku di">答</span>假设给你一个任务，用 10 公斤沙子装满一个袋子。你把它装满，直到测量机器给你一个 10 公斤的精确读数，或者如果读数超过 10 公斤，你就把沙子拿出来。</p><p id="358c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就像那个称重机，如果你的预测是错的，你的损失函数会输出一个更高的数字。如果他们很好，它会输出一个较低的数字。当你试验你的算法来尝试和改进你的模型时，你的损失函数会告诉你是否有所进展。</p><blockquote class="kv kw kx"><p id="d8c5" class="jn jo ky jp b jq jr js jt ju jv jw jx kz jz ka kb la kd ke kf lb kh ki kj kk ij bi translated"><em class="iq">“我们要最小化或最大化的函数叫做目标函数或准则。当我们将其最小化时，我们也可以称之为成本函数、损失函数或误差函数”——</em><a class="ae kl" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank"><em class="iq">来源</em> </a></p></blockquote><blockquote class="lc"><p id="527e" class="ld le iq bd lf lg lh li lj lk ll kk dk translated">本质上，损失函数是对预测模型在预测预期结果(或价值)方面表现的度量。我们将学习问题转化为优化问题，定义一个损失函数，然后优化算法以最小化损失函数。</p></blockquote><figure class="ln lo lp lq lr ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi lm"><img src="../Images/33e565061fc63d4721d3a7c1cbd910ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*D88aae6e_CG35R1p.png"/></div></div></figure><h1 id="dd69" class="lz ma iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">损失函数有哪些类型？</h1><p id="cf5c" class="pw-post-body-paragraph jn jo iq jp b jq mx js jt ju my jw jx jy mz ka kb kc na ke kf kg nb ki kj kk ij bi translated">最常用的损失函数有:</p><ul class=""><li id="2bf9" class="nc nd iq jp b jq jr ju jv jy ne kc nf kg ng kk nh ni nj nk bi translated">均方误差</li><li id="1fd1" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">绝对平均误差</li><li id="e976" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">对数似然损失</li><li id="0821" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">铰链损耗</li><li id="b4d7" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">胡伯损失</li></ul><ol class=""><li id="0bce" class="nc nd iq jp b jq jr ju jv jy ne kc nf kg ng kk nq ni nj nk bi translated"><strong class="jp ir">均方误差</strong></li></ol><p id="1c06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">均方差(MSE)是基本损失函数的工作空间，因为它易于理解和实现，并且通常工作得很好。要计算 MSE，您需要获取模型预测值和地面真实值之间的差异，将其平方，然后在整个数据集内取平均值。<br/>无论预测值和实际值的符号如何，结果总是正的，理想值为 0.0。</p><pre class="nr ns nt nu gt nv nw nx ny aw nz bi"><span id="4a3c" class="oa ma iq nw b gy ob oc l od oe"># function to calculate MSE<br/>def MSE(y_predicted, y_actual):</span><span id="24d1" class="oa ma iq nw b gy of oc l od oe">    squared_error = (y_predicted - y_actual) ** 2<br/>    sum_squared_error = np.sum(squared_error)<br/>    mse = sum_squared_error / y_actual.size</span><span id="87f0" class="oa ma iq nw b gy of oc l od oe">    return mse</span></pre><p id="4bd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2。平均绝对误差</strong></p><p id="4a1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">平均绝对误差(MAE)在定义上与 MSE 略有不同，但有趣的是，它提供了几乎完全相反的性质。要计算 MAE，您需要获取模型预测和地面实况之间的差异，将绝对值应用于该差异，然后在整个数据集内进行平均。</p><pre class="nr ns nt nu gt nv nw nx ny aw nz bi"><span id="abff" class="oa ma iq nw b gy ob oc l od oe"># function to calculate MAE<br/>def MAE(y_predicted, y_actual):<br/>    <br/>    abs_error = np.abs(y_predicted - y_actual)<br/>    sum_abs_error = np.sum(abs_error)<br/>    mae = sum_abs_error / y_actual.size<br/>    <br/>    return mae</span></pre><figure class="nr ns nt nu gt ls gh gi paragraph-image"><div class="gh gi og"><img src="../Images/589cfce04abf5c114df0aa6b13866532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*UbvgVxqK8_CY1CbP.png"/></div><figcaption class="oh oi gj gh gi oj ok bd b be z dk">MSE (blue) and MAE (red) loss functions</figcaption></figure><p id="c1c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3。对数似然损失</strong></p><p id="d895" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个损失函数也比较简单，常用于分类问题。在此，<br/>使用交叉熵来测量两个概率分布之间的误差。</p><pre class="nr ns nt nu gt nv nw nx ny aw nz bi"><span id="672d" class="oa ma iq nw b gy ob oc l od oe">-(y_actual * log(y_predicted) + (1 - y_actual) * log(1 - y_predicted))</span></pre><p id="f727" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里可以看到，当实际类为 1 时，函数的后半部分消失，当实际类为 0 时，前半部分下降。那样的话，我们最终会乘以实际预测概率的对数。</p><figure class="nr ns nt nu gt ls gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/d72d5a2e0479d883ac3cca83f1d1e484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/0*F8b_yCvAt4bdB8Cq.png"/></div><figcaption class="oh oi gj gh gi oj ok bd b be z dk">Source: <a class="ae kl" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank">fast.ai</a></figcaption></figure><p id="2132" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">二元或两类预测问题的交叉熵实际上是作为所有示例的平均交叉熵来计算的。</p><pre class="nr ns nt nu gt nv nw nx ny aw nz bi"><span id="25ca" class="oa ma iq nw b gy ob oc l od oe">from math import log<br/> <br/># function to calculate binary cross entropy<br/>def binary_cross_entropy(actual, predicted):<br/>	sum_score = 0.0<br/>	for i in range(len(actual)):<br/>		sum_score += actual[i] * log(1e-15 + predicted[i])<br/>	mean_sum_score = 1.0 / len(actual) * sum_score<br/>	return -mean_sum_score</span></pre><p id="ccc4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个函数是 Kaggle 竞赛中最受欢迎的度量之一。这只是对数似然函数的简单修改。</p><p id="40b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 4。铰链损耗</strong></p><p id="236c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">铰链损失函数在支持向量机中很流行。这些用于训练分类器。假设“t”是目标输出，使得 t = -1 或 1，并且分类器得分是“y”，则预测的铰链损失被给出为:<code class="fe om on oo nw b">L(y) = max(0, 1-t.y)</code></p><p id="832e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5。胡贝尔损失</strong></p><p id="8dde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们知道 MSE 非常适合学习异常值，而 MAE 非常适合忽略它们。但是中间的东西呢？<br/>考虑一个例子，我们有一个 100 个值的数据集，我们希望我们的模型被训练来预测。在所有这些数据中，25%的预期值是 5，而另外 75%是 10。</p><p id="4f0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于我们没有“异常值”，一个 MSE 损失不会完全解决问题；25%绝不是一个小分数。另一方面，我们不一定要用 MAE 来衡量太低的 25%。这些值 5 并不接近中值(10——因为 75%的点的值为 10 ),但它们也不是异常值。</p><h1 id="c2b5" class="lz ma iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">我们的解决方案？</h1><p id="e20c" class="pw-post-body-paragraph jn jo iq jp b jq mx js jt ju my jw jx jy mz ka kb kc na ke kf kg nb ki kj kk ij bi translated">胡伯损失函数。</p><p id="ddcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Huber Loss 通过同时平衡 MSE 和 MAE 提供了两个世界的最佳选择。我们可以使用以下分段函数来定义它:</p><figure class="nr ns nt nu gt ls gh gi paragraph-image"><div class="gh gi op"><img src="../Images/9c9fe48e6ee9fac3e3403788ac89cb99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/0*EYpuZzfYx3CVOF75.png"/></div></figure><p id="81c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个等式的实际意思是，对于小于δ的损耗值，使用 MSE 对于大于 delta 的损失值，使用 MAE。这有效地结合了两个损失函数的优点。</p><pre class="nr ns nt nu gt nv nw nx ny aw nz bi"><span id="34e0" class="oa ma iq nw b gy ob oc l od oe"># function to calculate Huber loss<br/>def huber_loss(y_predicted, y_actual, delta=1.0):</span><span id="8cd8" class="oa ma iq nw b gy of oc l od oe">    huber_mse = 0.5*(y_actual-y_predicted)**2<br/>    huber_mae = delta * (np.abs(y_actual - y_predicted) - 0.5 * delta)<br/>    <br/>    return np.where(np.abs(y_actual - y_predicted) &lt;= delta, <br/>                             huber_mse,  huber_mae)</span></pre><figure class="nr ns nt nu gt ls gh gi paragraph-image"><div class="gh gi og"><img src="../Images/126836db3704474950bc7709fd7129f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*34p3tGaQQUQWOm8E.png"/></div><figcaption class="oh oi gj gh gi oj ok bd b be z dk">MSE (blue), MAE (red) and Huber (green) loss functions</figcaption></figure><h1 id="826d" class="lz ma iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">结论</h1><p id="d630" class="pw-post-body-paragraph jn jo iq jp b jq mx js jt ju my jw jx jy mz ka kb kc na ke kf kg nb ki kj kk ij bi translated">损失函数提供的不仅仅是模型执行情况的静态表示，而是算法如何首先拟合数据。大多数机器学习算法在优化或为您的数据寻找最佳参数(权重)的过程中使用某种损失函数。</p><p id="19ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">重要的是，损失函数的选择与神经网络输出层中使用的激活函数直接相关。这两个设计元素是联系在一起的。</p><p id="50b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将输出图层的配置视为对预测问题框架的选择，将损失函数的选择视为对问题给定框架的误差计算方式。</p><p id="7e69" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">进一步阅读</strong> <br/> <a class="ae kl" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">深度学习</a>书作者<em class="ky">伊恩·古德菲勒、约舒阿·本吉奥和亚伦·库维尔</em>。</p></div></div>    
</body>
</html>