<html>
<head>
<title>Bayes’ Theorem and Movie Review Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯定理与电影评论分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayes-theorem-and-movie-review-analysis-fffae437a56?source=collection_archive---------27-----------------------#2019-05-14">https://towardsdatascience.com/bayes-theorem-and-movie-review-analysis-fffae437a56?source=collection_archive---------27-----------------------#2019-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="f205" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">贝叶斯定理是概率论中的一个基本公式。如果你想知道某个事件发生的概率，我们可以使用一些先验知识和贝叶斯定理来进行计算。</p><h2 id="7bd1" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">贝叶斯定理</h2><p id="fdd7" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">让我们来看看贝叶斯定理:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="51e6" class="ko kp it lr b gy lv lw l lx ly">P(A|B) = (P(B|A) * P(A)) / (P(B))</span></pre><blockquote class="lz ma mb"><p id="5f8e" class="jq jr mc js b jt ju jv jw jx jy jz ka md kc kd ke me kg kh ki mf kk kl km kn im bi translated"><em class="it"/>后验<em class="it">，</em> P(A|B) <em class="it">:假设事件 B 发生，事件 A 发生的概率</em></p><p id="e41e" class="jq jr mc js b jt ju jv jw jx jy jz ka md kc kd ke me kg kh ki mf kk kl km kn im bi translated"><em class="it"/>可能性<em class="it">，</em> P(B|A) <em class="it">:给定事件 A 发生，事件 B 发生的概率。</em></p><p id="81ea" class="jq jr mc js b jt ju jv jw jx jy jz ka md kc kd ke me kg kh ki mf kk kl km kn im bi translated"><em class="it"/>先验<em class="it">，</em> P(A) <em class="it">:是我们发生的概率。</em></p><p id="ed2d" class="jq jr mc js b jt ju jv jw jx jy jz ka md kc kd ke me kg kh ki mf kk kl km kn im bi translated">P(B) <em class="it">:事件 b 的概率，也改写为</em> P(B|A) + P(B| Aᶜ) <em class="it">。</em></p></blockquote><p id="5086" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">关于可能性与概率的一个注记:</p><p id="c851" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可能性类似于概率，但不是概率。可能性被认为是给定固定数据的假设。然而，我们常常不知道我们所知道的数据是什么样的样本。因此，可能性 P(B|A)度量了一个<em class="mc">样本</em>支持这个特定事件的程度。</p><h2 id="f591" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">简单贝叶斯例子</h2><p id="1feb" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">假设，1%的人口患有疾病，并且有针对该疾病的医学测试。如果一个人患有这种疾病，该测试预测他们患有这种疾病的准确率为 87%。如果一个人没有患病，该测试正确地测量出 72%的人没有患病。假设一个人接受了测试，并且测试结果呈阳性，那么他患这种疾病的概率是多少？</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="28fc" class="ko kp it lr b gy lv lw l lx ly"># posterior = P(has disease | positive test) <br/># likelihood = P(positive test | has disease)<br/># prior = P(has disease)<br/># P(positive test) = P(positive test | has disease) * P(has disease) + P(positive test | does not have disease) * P(does not have disease)</span><span id="9423" class="ko kp it lr b gy mg lw l lx ly">posterior = (0.87 * 0.01) / (0.87 * 0.01 + (1 - 0.72) * (1 - 0.01))</span><span id="1a1a" class="ko kp it lr b gy mg lw l lx ly">posterior = 0.03</span></pre><h2 id="10ae" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">院长</h2><p id="a4d9" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">让我们再多谈谈先验知识。先验是基于我们已经掌握的信息。如果我们对事件的认识改变了呢？假设一项新的研究发现，这种疾病实际上在 3%的人口中流行，而不是 1%。这大大改变了我们的答案，使这种疾病呈阳性结果的概率增加到 8.8%。因此，当我们考虑样本如何反映总体时，当我们有数据集样本时，考虑先验概率是很重要的。</p><h2 id="1beb" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">打破朴素贝叶斯</h2><p id="ec24" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">前面的例子只考虑了一个特征和目标，现在我们来想想，如果这个疾病有四个不相关的疾病检测结果呢？如果一个人每次测试都得到阳性结果，那么这说明了患这种疾病的可能性有多大。这就是朴素贝叶斯派上用场的地方。</p><p id="cb1c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">之所以称之为朴素贝叶斯，是因为它做出了一个在现实中几乎不可能满足的主要假设，这就是所有特征都是独立的且互不影响的假设。假设独立性的一个例子是球的大小不影响球的颜色。然而，这不是一个伟大的假设，因为大多数篮球(相当大)是橙色的，而大多数高尔夫球(相当小)是白色的。同样，朴素贝叶斯作出这种假设是为了使数学更简单，并使计算机程序在初始分类测试中运行得更快。它可能不会是您使用的最终模型，但它是一个很好的起点。</p><p id="bc0b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们通过一个例子来展示朴素贝叶斯的工作原理。</p><p id="bc6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设我们想计算在给定评论文本的情况下，在腐烂的西红柿上获得“新鲜”分数的概率(在评论文本中，我们计算每个单词出现的次数)。</p><ol class=""><li id="ca78" class="mh mi it js b jt ju jx jy kb mj kf mk kj ml kn mm mn mo mp bi translated">开始配方。给定评论包含 X 个单词，一部电影是<em class="mc"> C=fresh </em>的概率是多少？这等于新得分包含这些单词的可能性乘以新得分的概率除以单词的概率。</li></ol><figure class="lm ln lo lp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mq"><img src="../Images/d7a77bf9d9558194ff7800eb18bac7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FDT9bzyIE7JAKybb.png"/></div></div></figure><p id="751e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.使用链式法则，我们可以将似然性<em class="mc"> P(X|Ci) </em>扩展到每个单词在给定条件下依次减少的似然性:</p><figure class="lm ln lo lp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mq"><img src="../Images/9b5c46d9898932839174534669cd0032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FSdF7JJz2ll7D_Iu.png"/></div></div></figure><p id="3558" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.由于我们天真地认为单词是相互独立的，因此我们可以简化数学。贝叶斯的独立性法则表明，如果 A 和 B 是独立的，那么<em class="mc"> P(A|B) = P(A) </em>。我们只需要看|和类前的单词。然后我们将这些可能性相乘。这将等式简化为:</p><figure class="lm ln lo lp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mq"><img src="../Images/264bb6c496e0d228c667f1f19ec2dff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*G22QTQ3pMQmJRTyG.png"/></div></div></figure><p id="f594" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.最后，由于我们的分母是不变的，我们来看看比例性:</p><figure class="lm ln lo lp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mq"><img src="../Images/692ffc9e122e0734bbeac20b2ce72739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DIZyKoDnwUHEek9f.png"/></div></div></figure><p id="78a2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="mc">“给定该评论文本的电影是新的概率与给定新评论的每个词的可能性乘以新评论的概率的乘积成比例。”</em></p><h2 id="b070" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">在 Python 中应用朴素贝叶斯</h2><p id="eadb" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">这种数学可能有点复杂和冗长，尤其是对于文本分类。幸运的是<em class="mc"> Scikit-learn </em>有一个内置的库为我们做数学运算。有几种不同类型的朴素贝叶斯分类器，但我们希望使用多项式贝叶斯分类器，因为它测量离散计数(即电影评论中的字数)。我使用了来自<a class="ae my" href="https://www.kaggle.com/rpnuser8182/rotten-tomatoes" rel="noopener ugc nofollow" target="_blank">这个</a> Kaggle 数据集的评论。</p><ol class=""><li id="4109" class="mh mi it js b jt ju jx jy kb mj kf mk kj ml kn mm mn mo mp bi translated">导入必要的库并将数据存储在数据帧中</li></ol><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="a2dc" class="ko kp it lr b gy lv lw l lx ly">import csv<br/>import pandas as pd<br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer</span><span id="5183" class="ko kp it lr b gy mg lw l lx ly">data = []<br/>with open('reviews.tsv', encoding="utf8", errors='ignore') as tsvfile:<br/>    reader = csv.reader(tsvfile, delimiter='\t')<br/>    for row in reader:<br/>        data.append(row)</span><span id="5a3d" class="ko kp it lr b gy mg lw l lx ly">reviews = pd.DataFrame(data[1:], columns=data[0])</span></pre><p id="d74a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.清理数据帧。要做的一些改变是将新鲜和腐烂转换为二进制，并将评论和评分存储在自己的系列中。</p><figure class="lm ln lo lp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mz"><img src="../Images/0843f44f492871784a08d86e19ae9263.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WQM1AXZNf6UdsxwzUexZXw.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">An initial look at the data</figcaption></figure><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="b5e7" class="ko kp it lr b gy lv lw l lx ly">score_dict = {'fresh': 1, 'rotten': 0}<br/>reviews['fresh'] = reviews['fresh'].map(score_dict)</span><span id="6f1d" class="ko kp it lr b gy mg lw l lx ly">X = reviews['review']<br/>y = reviews['fresh']</span></pre><p id="97b3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.在 X 轴和 y 轴上执行训练、测试和分割</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="9a6b" class="ko kp it lr b gy lv lw l lx ly">X_train, X_test, y_train, y_test = train_test_split(X, y)</span></pre><p id="fb64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.创建一个 CountVectorizer 对象，并将其安装在<strong class="js iu"> X_train </strong>上。<strong class="js iu"> </strong>不要使用 X，因为我们希望将 X_train 视为我们拥有的唯一数据。使用 stop_words='english '删除不太重要的单词，如“the”、“and”和“that”然后转换 X_train 和 X_test。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="818c" class="ko kp it lr b gy lv lw l lx ly">count_vectorizer = CountVectorizer(stop_words='english')<br/>count_vectorizer.fit(X_train)</span><span id="53ad" class="ko kp it lr b gy mg lw l lx ly">X_train_counts = count_vectorizer.transform(X_train)<br/>X_test_counts = count_vectorizer.transform(X_test)</span></pre><p id="6c62" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">5.创建并调整 TfidfTransformer 对象。这将根据每个单词在文档中出现的频率以及在多少个文档中出现来对其进行加权。如果一个单词在文档中出现很多，那么这个值就会增加，因为它可能是一个重要的单词。但是，如果它出现在很多文档中，则价值会降低，因为这个词可能对整体分类影响不大。变换 X_train 和 X_test。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="a2bb" class="ko kp it lr b gy lv lw l lx ly">tfidf_transformer = TfidfTransformer()<br/>tfidf_transformer.fit(X_train_counts)</span><span id="b9c1" class="ko kp it lr b gy mg lw l lx ly">x_train_tfidf = tfidf_transformer.transform(X_train_counts)<br/>x_test_tfidf = tfidf_transformer.transform(X_test_counts)</span></pre><p id="1f50" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">6.在 tfidf 训练数据上创建和训练多项式朴素贝叶斯分类器。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="7ab4" class="ko kp it lr b gy lv lw l lx ly">classifier = MultinomialNB()<br/>classifier.fit(x_train_tfidf, y_train)</span></pre><p id="7c24" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">7.执行朴素贝叶斯。这一步为你做了所有的计算！我们可以做两件事:第一是严格计算后验概率，第二是得到最可能的结果。我们可以在我们的测试数据上做到这一点，但为了简单起见，让我们看看我写的一篇评论，应该是<em class="mc"> fresh </em>。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="9419" class="ko kp it lr b gy lv lw l lx ly">classifier.predict_proba(count_vectorizer.transform(['This is an awesome movie']))</span><span id="44e2" class="ko kp it lr b gy mg lw l lx ly"># Output: [0.36188785, 0.63811215]</span><span id="98f0" class="ko kp it lr b gy mg lw l lx ly">classifier.predict(count_vectorizer.transform(['This is an awesome movie']))</span><span id="40f6" class="ko kp it lr b gy mg lw l lx ly"># Output: [1]</span></pre><p id="c604" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个输出意味着这部电影有 0.362 的几率有<em class="mc">烂</em>分，有 0.638 的几率有<em class="mc">新</em>。它还预测评论是<em class="mc">新鲜的</em>。</p><p id="98e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">还有一种使用流水线的替代方法，它压缩了计数矢量化、tfidf 转换和分类器步骤。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="8550" class="ko kp it lr b gy lv lw l lx ly">from sklearn.pipeline import Pipeline</span><span id="50a0" class="ko kp it lr b gy mg lw l lx ly">text_classifier = Pipeline([('count_vectorizer', <br/>                             CountVectorizer(stop_words='english')),<br/>                            ('tfidf_vectorizer',   <br/>                             TfidfVectorizer()),<br/>                            ('clf', MultinomialNB())])</span><span id="e0e9" class="ko kp it lr b gy mg lw l lx ly">text_classifier.fit(X_train, y_train)</span><span id="f20e" class="ko kp it lr b gy mg lw l lx ly">text_classifier.predict_proba(['This is an awesome movie'])<br/>text_classifier.predict(['This is an awesome movie'])</span></pre><h2 id="566f" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">摘要</h2><p id="6bb8" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">贝叶斯定理利用先验知识和事件发生的可能性来预测在其他事情发生的情况下特定结果发生的概率。朴素贝叶斯是一种快速而强大的工具，我们可以用它来对信息进行分类，但它主要是一个起点，因为它做出了一些假设，所以要谨慎使用。它主要用于文本分类，因为数据集很大，非常符合独立性原则。关于代码的更多信息，请查看我在 GitHub 上的<a class="ae my" href="https://github.com/kayschulz/rotten_tomatoes_nb" rel="noopener ugc nofollow" target="_blank">库。</a></p></div></div>    
</body>
</html>