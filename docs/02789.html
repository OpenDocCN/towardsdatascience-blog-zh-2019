<html>
<head>
<title>Naive Bayes Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-part-16-naive-bayes-classifier-in-python-c9d3fa496fa4?source=collection_archive---------17-----------------------#2019-05-06">https://towardsdatascience.com/machine-learning-part-16-naive-bayes-classifier-in-python-c9d3fa496fa4?source=collection_archive---------17-----------------------#2019-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/3f7d697379860b1fff02e6842afefd0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XWqtPWy7LKD2Av-2"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@craftedbygc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Green Chameleon</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="8671" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与其他通过多次迭代来收敛到某个解决方案的机器学习算法相比，朴素贝叶斯仅基于条件概率对数据进行分类。朴素贝叶斯有以下优点:</p><ul class=""><li id="2e31" class="le lf jj ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">训练和预测速度极快</li><li id="4aee" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">可解释的</li><li id="f039" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">不需要超参数调整</li></ul><p id="eda2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在决定是否使用朴素贝叶斯时，您应该考虑朴素假设是否实际成立(在实践中非常罕见)，以及您正在处理的数据是否具有很高的维数。</p><h1 id="7922" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">贝叶斯定理</h1><p id="f838" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">很多时候，我们想知道某个事件发生的概率，假设另一个事件已经发生。这可以象征性地表示为 P(E|F)。如果两个事件不是独立的，那么这两个事件发生的概率由下面的公式表示。</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mv"><img src="../Images/740ddf27733defba2c2cf9093f842376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWYGZsVLVmvcu1DlYRY47w.png"/></div></div></figure><p id="b0be" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，假设您从一副标准的 52 张牌中抽取两张牌。这副牌中一半是红色的，一半是黑色的。这些事件不是独立的，因为第二次抽签的概率取决于第一次抽签的结果。</p><p id="c2c4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">P(E) = P(第一次抽黑牌)= 25/52 = 0.5</p><p id="b4a3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">P(F|E) = P(第二次抽黑牌|第一次抽黑牌)= 25/51 = 0.49</p><p id="f62f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用这些信息，我们可以计算连续抽两张黑牌的概率，如下所示:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi na"><img src="../Images/e92bb4bb1d1fa4ae41b4b640e1ce89a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mk5eTI6qsrhfesAMiNU6RQ.png"/></div></div></figure><p id="2e99" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Baye 定理是条件概率最常见的应用之一。例如，baye 定理被用来计算一个在特定疾病筛查测试中呈阳性的人实际患有该疾病的概率。贝叶斯定理可以表示为:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/d7fec71690845ac141c50dff0496cbab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*znBjCfqF05jvCJAJ9QQTNQ.png"/></div></div></figure><p id="fb08" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你已经知道 P(A)，P(B)和 P(B|A ),但想知道 P(A|B ),你可以使用这个公式。例如，假设我们正在测试一种罕见的疾病，这种疾病会感染 1%的人口。医学专家已经开发了一种高度敏感和特异的测试，但还不十分完善。</p><ul class=""><li id="8ec3" class="le lf jj ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">99%的病人测试呈阳性</li><li id="5a64" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">99%的健康患者测试呈阴性</li></ul><p id="6939" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">贝叶斯定理告诉我们:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/e4b9afc06769483e99813dc3857ae66f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zi8RRyzHyMI85Yc6mVqiTQ.png"/></div></div></figure><p id="1562" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们有 10，000 人，100 人生病，9，900 人健康。此外，在对所有人进行测试后，我们会让 99 名病人进行测试，但也让 99 名健康人进行测试。因此，我们会以下面的概率结束。</p><h2 id="91a0" class="nd lt jj bd lu ne nf dn ly ng nh dp mc kr ni nj mg kv nk nl mk kz nm nn mo no bi translated">p(生病)= 0.01</h2><h2 id="09f6" class="nd lt jj bd lu ne nf dn ly ng nh dp mc kr ni nj mg kv nk nl mk kz nm nn mo no bi translated">p(没病)= 1 - 0.01 = 0.99</h2><h2 id="168b" class="nd lt jj bd lu ne nf dn ly ng nh dp mc kr ni nj mg kv nk nl mk kz nm nn mo no bi translated">p(+|有病)= 0.99</h2><h2 id="d015" class="nd lt jj bd lu ne nf dn ly ng nh dp mc kr ni nj mg kv nk nl mk kz nm nn mo no bi translated">p(+|未患病)= 0.01</h2><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/d22b49360e91b65b35e94cd8357b2935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h1s_atx5r4W5LDo9DR6BeA.png"/></div></div></figure><h1 id="9069" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">针对单个单词的垃圾邮件过滤器</h1><p id="3a89" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">朴素贝叶斯通常用于构建垃圾邮件过滤器，将电子邮件分类为垃圾邮件或 ham(非垃圾邮件)。例如，我们可能会开发一个垃圾邮件过滤器，根据某个单词的出现将一封电子邮件归类为垃圾邮件。例如，如果一封电子邮件包含单词<strong class="ki jk"> <em class="nq">【伟哥】</em> </strong>，我们将其归类为垃圾邮件。另一方面，如果一封电子邮件包含单词 money，那么它有 80%的可能是垃圾邮件。</p><p id="10bc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据贝叶斯定理，给定包含“<em class="nq">单词</em>”的邮件是垃圾邮件的概率。</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/4715a9a6a6a043cc108374622404d7bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uDpsXviunQ-YpmmkmHyqrw.png"/></div></div></figure><p id="20e0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然我们不知道分母的值，但可以用下面的公式计算出来。简单地说，找到该单词的概率是在垃圾邮件中找到该单词的概率乘以电子邮件是垃圾邮件的概率，加上在垃圾邮件中找到该单词的概率乘以电子邮件是垃圾邮件的概率。</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/0cb10168a8d29be5d2da9452dbe08312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_HCJJzUL4E4VQ6WUHjbn0Q.png"/></div></div></figure><p id="3f56" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，公式变为:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/e1543a691c7040e7140218118fcad326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwyVMDGfV5ynWWQPXk9ruA.png"/></div></div></figure><p id="f8cc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练集包含足够的信息量来计算等式的右边。</p><h2 id="4c1e" class="nd lt jj bd lu ne nf dn ly ng nh dp mc kr ni nj mg kv nk nl mk kz nm nn mo no bi translated">p(垃圾邮件)</h2><p id="def3" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">被归类为垃圾邮件的电子邮件数量</p><h2 id="422a" class="nd lt jj bd lu ne nf dn ly ng nh dp mc kr ni nj mg kv nk nl mk kz nm nn mo no bi translated">火腿</h2><p id="531f" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">被归类为垃圾邮件(非垃圾邮件)的电子邮件数量</p><h2 id="096d" class="nd lt jj bd lu ne nf dn ly ng nh dp mc kr ni nj mg kv nk nl mk kz nm nn mo no bi translated">p(word|spam)</h2><p id="6c26" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">包含“<em class="nq">单词</em>的垃圾邮件数量</p><h2 id="0756" class="nd lt jj bd lu ne nf dn ly ng nh dp mc kr ni nj mg kv nk nl mk kz nm nn mo no bi translated">p(单词|火腿)</h2><p id="c219" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">包含“<em class="nq">单词”</em>的垃圾邮件数量</p><h1 id="1c1b" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">结合单词的垃圾邮件过滤器</h1><p id="267e" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">实际上，仅仅根据一个单词的出现来进行分类是不够的。有些词本身是无害的。当它们与特定的其他单词结合使用时，我们更应该将其归类为垃圾邮件。例如，根据包含单词“<strong class="ki jk"> <em class="nq">女孩</em> </strong>”的事实来过滤电子邮件，最终会将与您女儿的足球队相关的电子邮件放入您的垃圾邮件文件夹中。理想情况下，我们能够针对特定的句子，如“<strong class="ki jk"> <em class="nq">你所在地区的漂亮女孩</em> </strong>”。</p><h2 id="0950" class="nd lt jj bd lu ne nf dn ly ng nh dp mc kr ni nj mg kv nk nl mk kz nm nn mo no bi translated">天真的假设</h2><p id="305d" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">通常，在我们的训练集中，我们不会有带有适当标签的准确句子。换句话说，如果我们试图确定一封包含“<strong class="ki jk"> <em class="nq">你所在地区的漂亮女孩</em> </strong>”的电子邮件是否应该被归类为垃圾邮件，我们会查看我们的训练数据，看看是否有任何包含该句子的电子邮件已经被归类为垃圾邮件。如果没有，我们有两个选择，要么获取更多的数据，要么通过查看句子中的每个单词来计算给定的概率。</p><p id="eeaa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使数学更简单，我们做了简单的<strong class="ki jk"><em class="nq"/></strong>假设，文本中的单词是相互独立的<strong class="ki jk"/>。换句话说，文本中包含某些单词的事实对在文本中找到给定单词的概率没有影响。然而在现实中，某些词往往会被组合在一起使用。例如，如果你在一篇关于机器学习的文章中找到单词<strong class="ki jk"><em class="nq"/></strong>，很可能你也会找到单词<strong class="ki jk"><em class="nq"/></strong>和<strong class="ki jk"> <em class="nq">训练</em> </strong>。</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/c03556a27d5a3f9f094637fe05936f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8zMco9uKbLgRGdXxNFtctg.png"/></div></div></figure><p id="0f42" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假定一封电子邮件同时包含单词“有吸引力的”和“女孩”,则该电子邮件包含垃圾邮件的概率可以写为:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/e63330af41b938e87a133c0a6d2c9392.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7PO3-c8FstTdfuplzgxl1A.png"/></div></div></figure><h1 id="d7a8" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">密码</h1><p id="c765" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">我们将从导入所需的库开始。</p><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="6fe8" class="nd lt jj nx b gy ob oc l od oe">from sklearn.datasets import fetch_20newsgroups<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn import metrics</span></pre><p id="a754" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在前面的例子中，我们将使用新闻文章的语料库。幸运的是，sklearn 提供了一个简单的 API 来导入数据。</p><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="29dd" class="nd lt jj nx b gy ob oc l od oe">newsgroups = fetch_20newsgroups()</span></pre><p id="7424" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顾名思义，新闻文章分为 20 类。</p><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="25d6" class="nd lt jj nx b gy ob oc l od oe">print(newsgroups.target_names)</span></pre><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/458af227f28bf666fca2729f92c8e2ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORmMFHHcIXyR9Uw91yuL2w.png"/></div></div></figure><p id="1b36" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">语料库包含超过 11，000 个文档。</p><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="9599" class="nd lt jj nx b gy ob oc l od oe">print(newsgroups.filenames.shape)</span></pre><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/e499868566a9ff2b0e2cb728a3883279.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*uVxXWxW-RFmhivpicDRtpg.png"/></div></figure><p id="854b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了了解我们在做什么，让我们打印第一个文档。</p><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="5bfd" class="nd lt jj nx b gy ob oc l od oe">print(newsgroups.data[0])</span></pre><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/b7c6a104994f23bce8ed9e0707275ced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YrKtR-kF9M6A4mN8sSkkIQ.png"/></div></div></figure><p id="bdd6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了简单起见，我们将使用类别的子集。</p><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="d456" class="nd lt jj nx b gy ob oc l od oe">categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']</span><span id="19b8" class="nd lt jj nx b gy oi oc l od oe">train_newsgroups = fetch_20newsgroups(subset='train', categories=categories)<br/>train_X = train_newsgroups.data<br/>train_y = train_newsgroups.target</span><span id="30a8" class="nd lt jj nx b gy oi oc l od oe">test_newsgroups = fetch_20newsgroups(subset='test', categories=categories)<br/>test_X = test_newsgroups.data<br/>test_y = test_newsgroups.target</span></pre><p id="0ed8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们使用 TfidfVectorizer 类来解析每个文档中包含的单词，并根据相关性对它们进行排序。如果你想了解更多关于 TF-IDF 是如何工作的，我会在后续文章中详细介绍。</p><div class="is it gp gr iu oj"><a href="https://medium.com/@corymaklin/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76" rel="noopener follow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jk gy z fp oo fr fs op fu fw ji bi translated">自然语言处理:使用 TF-IDF 的特征工程</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">自然语言处理(NLP)是人工智能的一个子领域，处理理解和处理…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">medium.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ja oj"/></div></div></a></div><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="9d3d" class="nd lt jj nx b gy ob oc l od oe">vectorizer = TfidfVectorizer()<br/>train_vectors = vectorizer.fit_transform(train_X)<br/>test_vectors =  vectorizer.transform(test_X)</span></pre><p id="6b35" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用多维朴素贝叶斯，因为我们试图将数据分为多个类别。与其他机器学习算法相比，朴素贝叶斯的训练过程几乎是瞬时的。</p><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="b2fc" class="nd lt jj nx b gy ob oc l od oe">classifier = MultinomialNB()<br/>classifier.fit(train_vectors, train_y)</span></pre><p id="fa9e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用 f1 分数来衡量我们模型的性能。</p><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="c68b" class="nd lt jj nx b gy ob oc l od oe">predictions = classifier.predict(test_vectors)</span><span id="ce73" class="nd lt jj nx b gy oi oc l od oe">score = metrics.f1_score(test_y, predictions, average='macro')</span></pre><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c5db5fe9f4ff60ba07e606f201cdac73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*SJ7B1Pdk4tcerv8e4kjJgg.png"/></div></figure><p id="9f46" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另外，我们可以写一个函数来预测一个句子属于哪一类。</p><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="c44d" class="nd lt jj nx b gy ob oc l od oe">def predict_category(sentence):<br/>    sentence_vector = vectorizer.transform([sentence])<br/>    prediction = classifier.predict(sentence_vector)<br/>    return train_newsgroups.target_names[prediction[0]]</span></pre><p id="b6f9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你所看到的，这个模型正确地将这个句子归类到了计算机图形学中。</p><pre class="mw mx my mz gt nw nx ny nz aw oa bi"><span id="77b7" class="nd lt jj nx b gy ob oc l od oe">predict_category('determining the screen resolution')</span></pre><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/698fadc85030b97607d70e7503d79bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*PJYY4DnECTKt1ajVpd1o4A.png"/></div></figure><h1 id="9261" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">最后的想法</h1><p id="e833" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">朴素贝叶斯特别适合对具有大量特征的数据进行分类。与其他机器学习模型不同，朴素贝叶斯几乎不需要训练。当试图做出一个包含多个特征的预测时，我们通过做出<em class="nq">天真的</em>假设这些特征是独立的来简化数学。</p></div></div>    
</body>
</html>