<html>
<head>
<title>Neural Network Back-Propagation Revisited with Ordinary Differential Equations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用常微分方程重温神经网络反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-network-back-propagation-revisited-892f42320d31?source=collection_archive---------30-----------------------#2019-10-15">https://towardsdatascience.com/neural-network-back-propagation-revisited-892f42320d31?source=collection_archive---------30-----------------------#2019-10-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8303" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过使用微分方程的数值解来优化神经网络参数被评论为在反向传播期间收敛到成本函数的全局最小值的替代方法。</h2></div><div class="kf kg kh ki gt ab cb"><figure class="kj kk kl km kn ko kp paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><img src="../Images/c6ec20850fae28750da79819213219e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*td3zHcGxlSs5azwCqYJGqA.gif"/></div></figure><figure class="kj kk kl km kn ko kp paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><img src="../Images/38e0221f3b359d29ba90fdf3b6028c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*fi4g7E4HzT5nxDlgp6d7Aw.gif"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk la di lb lc">Different optimizers at work to find the global minimum of two different functions. The “ODE Solver” optimizer will be the subject of this post. If you focus on the white dot, you can see that the ODE solver reaches the minimum with fewer iterations in the two cases shown here. The animation on the left is the <a class="ae ld" href="http://www.sfu.ca/~ssurjano/camel6.html" rel="noopener ugc nofollow" target="_blank">six-hump camel function</a> with a global minimum at (0,0). The animation on the right is the <a class="ae ld" href="http://www.sfu.ca/~ssurjano/goldpr.html" rel="noopener ugc nofollow" target="_blank">Goldstein-Price function</a> where the global minimum at (0,-1) is only slightly deeper than the many other local minima. The animations have been created by adapting the code by Piotr Skalski. The code to reproduce the animation above can be found <a class="ae ld" href="https://gist.github.com/alessiot/aca064b64ff416f75a4d30e08b405c37" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure></div></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="cf44" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">这篇文章的重点是介绍一种不同的方法来优化神经网络的参数(<em class="mh">又名</em>权重)，同时在训练过程中反向传播损失梯度。核心思想是使用常微分方程(ODE)数值解算器来寻找最小化梯度的权重。这个想法是由<a class="ae ld" href="https://www.semanticscholar.org/paper/Efficient-training-of-the-backpropagation-network-a-Owens-Filkin/3ed4de93b828a2350489aaa40de382e3fec45e68?citingPapersSort=is-influential#citing-papers" rel="noopener ugc nofollow" target="_blank">亚伦·敖文思等人</a>在 1989 年首次提出的。虽然使用数值解算器增加了大量的计算时间，但是这种方法对于相对较小的数据集和/或收敛到稳定解需要额外努力来微调更“标准”优化器的超参数的情况是有意义的。事实上，数值求解器不需要任何超参数来调整。</p><p id="bd71" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">最近，在神经网络建模中使用 ODE 求解器已经被用来重新设计深度学习可以对健康变化等连续过程进行建模的方式(见<a class="ae ld" href="https://arxiv.org/abs/1806.07366" rel="noopener ugc nofollow" target="_blank">此处</a>)。</p><p id="2665" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">在介绍了神经网络之后，我将描述如何对反向传播步骤进行更改，以便使用 ODE 解算器和标准优化器在 TensorFlow 中训练神经网络。</p><h1 id="c06c" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">神经网络基础。</h1><p id="8e62" class="pw-post-body-paragraph ll lm iq ln b lo na jr lq lr nb ju lt lu nc lw lx ly nd ma mb mc ne me mf mg ij bi translated">神经网络是生物启发的数学模型，可以在计算机上编程，以发现和学习观察数据中的复杂关系。关于神经网络背后的数学的详细回顾，读者可以参考迈克尔·尼尔森的在线书籍。节点的输入层和输出层分别表示特征和结果。一个或多个中间层(称为隐藏层)的序列通常用于捕获初始输入特征的越来越复杂的表示。</p><figure class="kf kg kh ki gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8cb1ecbb564b7b568a67b5522f1cee7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*6w-2mJz4UvAtSXtM_WtlfQ.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">An example representation of neural network architecture with two input nodes and a single output node created with the <a class="ae ld" href="https://github.com/imranq/visualizeNN" rel="noopener ugc nofollow" target="_blank">VisualizeNN</a> module. Other online tools exist to create publication-ready images (see for example <a class="ae ld" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">NN-SVG</a>).</figcaption></figure><p id="0dab" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">利用这种架构进行学习的基本概念是可调权重，其被表示为网络节点之间的连接线。神经网络也被称为多层感知器网络(MLP)，因为中间层充当<em class="mh">感知器</em>，即传递来自前一层的传入值或阻止它的本地分类器。</p><p id="b38d" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">神经网络的训练包括两个阶段，前向和反向传播，这两个阶段重复几次，称为时期。在前向传播过程中，输入数据呈现给网络，并通过其各层进行转换。在任何给定的层<em class="mh"> l，</em>从前一层𝑙−1 扇出的输入值𝐴使用将层𝑙−1 连接到𝑙的权重(<em class="mh"> W </em>)和层𝑙的偏置(<em class="mh"> b </em>)进行线性变换，如下</p><figure class="kf kg kh ki gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/09a41549684c266aed01a8eafa5de4a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*U6yQFvUabia7kLbMJAK2kg.png"/></div></figure><p id="40a5" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">𝑍是𝑙.图层节点处的值的向量使用如下激活函数将非线性变换应用于𝑍</p><figure class="kf kg kh ki gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/29e816d928d1891f84f330629b49f24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*LebgfnpmEfnFdE5N5qglBA.png"/></div></figure><p id="066d" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">在反向传播期间，为每一层𝑙.计算成本𝐶相对于网络权重的梯度对于𝑙层来说，</p><figure class="kf kg kh ki gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/6b1a819c232db44e8b3e75659438b451.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*p10Ym9vc97ZMS314oPobIg.png"/></div></figure><p id="731f" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">在哪里</p><figure class="kf kg kh ki gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi nj"><img src="../Images/2e482d1a7866a6eb7e1812629b841d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOaZX6mJxR3tyxpicDn7_Q.png"/></div></div></figure><p id="b78a" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">知道这些等式就足以使用<em class="mh"> numpy 从头开始编写神经网络程序。</em>要查看，可以参考 Piotr Skalski 写的<a class="ae ld" href="https://github.com/SkalskiP/ILearnDeepLearning.py" rel="noopener ugc nofollow" target="_blank">代码</a>，并在他关于介质的<a class="ae ld" rel="noopener" target="_blank" href="/lets-code-a-neural-network-in-plain-numpy-ae7e74410795">文章</a>中查看。</p><h1 id="9c5b" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">用 sklearn 训练 MLP 网络。</h1><p id="cc7b" class="pw-post-body-paragraph ll lm iq ln b lo na jr lq lr nb ju lt lu nc lw lx ly nd ma mb mc ne me mf mg ij bi translated">在<em class="mh"> sklearn </em>中，可以使用任何<a class="ae ld" href="https://scikit-learn.org/stable/supervised_learning.html" rel="noopener ugc nofollow" target="_blank">监督机器学习技术</a>中使用的典型步骤来训练神经网络模型。为了简单起见，我在下面的代码片段中使用了异或(XOR)真值表。</p><figure class="kf kg kh ki gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="67bf" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">带 ODE 解算器的张量流。</h1><p id="3480" class="pw-post-body-paragraph ll lm iq ln b lo na jr lq lr nb ju lt lu nc lw lx ly nd ma mb mc ne me mf mg ij bi translated">常微分方程(ODE)可以用数值方法求解。事实上，前一节介绍的反向传播方程可以写成</p><figure class="kf kg kh ki gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/097d07c500a85dc55874c7fb91c7751b.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*TCdAhcy1emLNlXJ2KZD17Q.png"/></div></figure><p id="7eb0" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">并且是可以用数字求解<a class="ae ld" href="https://www.semanticscholar.org/paper/Efficient-training-of-the-backpropagation-network-a-Owens-Filkin/3ed4de93b828a2350489aaa40de382e3fec45e68?citingPapersSort=is-influential#citing-papers" rel="noopener ugc nofollow" target="_blank">的颂歌。在大多数情况下，这种 ODE 在数值上是严格的，也就是说，在积分过程中，通过不稳定性收敛到最优解。</a></p><p id="f2d0" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">刚性常微分方程的解要求积分器的步长非常小，并且可以随时间变化。在 Python 中，<a class="ae ld" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.ode.html#scipy.integrate.ode" rel="noopener ugc nofollow" target="_blank"> ODE 解算器</a>在<em class="mh"> scipy </em>库中实现。为了使用 ODE 积分器找到最佳权重，我们将变量<em class="mh"> t </em>引入反向传播方程，这对应于改变积分器的步长以达到稳定的数值解。因此，我们将使用下面的等式</p><figure class="kf kg kh ki gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/a0ad84832c1b60ab1fae4da93212f812.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ul6xu-5jR3caSeajkJuc3A.png"/></div></figure><p id="e319" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">本节末尾的笔记本更详细地解释了为了用 ODE 求解器优化张量流神经网络而进行的修改。为此，我们需要在“渴望”模式下运行 TensorFlow，如这里的<a class="ae ld" href="https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough" rel="noopener ugc nofollow" target="_blank">所示</a>。对于这个练习，我从 MNIST 数据集中提取了两个随机样本，其中一个用于训练，另一个用于验证，即调整网络参数。两个样本都包含 0 到 4 之间的数字的数据，每个数字用 100 个样本来表示。当在 50 次迭代之后没有观察到大于 5%的改进时，停止训练，因为在验证集上有最佳性能。该模型的最终性能是根据数字 0-4 的全部 MNIST 数据(35，000 个样本)计算的，如下所示。</p><div class="kf kg kh ki gt ab cb"><figure class="kj kk kl km kn ko kp paragraph-image"><img src="../Images/1c55bcfbd3ed23a11c7bdf80a345cb4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*zlwF32tygz5GCArl97U74g.png"/></figure><figure class="kj kk kl km kn ko kp paragraph-image"><img src="../Images/09aa58860dea5009f3fe65b8affdfc85.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*s8sDp8SYOP2LPPmol4kMQw.png"/><figcaption class="kw kx gj gh gi ky kz bd b be z dk la di lb lc">Accuracy of the TensorFlow neural network model on the whole datasets of digits between 0 and 4. (Left) Fewer iterations or presentations of the data to the network are needed to reach optimal performance. (Right) The use of a ODE solver requires significantly greater computation time.</figcaption></figure></div><p id="a7d5" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">用 ODE 求解器优化神经网络比用标准优化器优化要多花 8 倍的时间。然而，ODE 求解器的精度并不比其他优化器的精度差。</p><figure class="kf kg kh ki gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="dc49" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">结论。</h1><p id="bc0c" class="pw-post-body-paragraph ll lm iq ln b lo na jr lq lr nb ju lt lu nc lw lx ly nd ma mb mc ne me mf mg ij bi translated">使用常微分方程(ODE)数值解算器已被视为优化神经网络参数的替代方法。虽然这显著增加了训练神经网络模型的计算时间，但是它不需要任何超参数调整，并且可以成为收敛到成本函数全局最小值而不花费时间微调超参数的替代方式。</p></div></div>    
</body>
</html>