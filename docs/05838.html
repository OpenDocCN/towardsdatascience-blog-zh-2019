<html>
<head>
<title>Reimagining Plutarch with NLP: Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 NLP 重构 Plutarch:第 2 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reimagining-plutarch-with-nlp-part-2-dc4e360baa68?source=collection_archive---------17-----------------------#2019-08-25">https://towardsdatascience.com/reimagining-plutarch-with-nlp-part-2-dc4e360baa68?source=collection_archive---------17-----------------------#2019-08-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/e1be64ff5e6c1e1c83b8b3cbf537d946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AyLHoVDd1XIOuwmrsc0u5A.png"/></div></div></figure><div class=""/><div class=""><h2 id="9179" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">普鲁塔克通过自然语言处理研究希腊和罗马贵族的生活:这部分包括 word2vec、降维(PCA 和 t-SNE)和 Tensorflow 投影仪</h2></div><h1 id="6cae" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">序文</h1><p id="f3dd" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated"><a class="ae me" href="https://en.wikipedia.org/wiki/Parallel_Lives" rel="noopener ugc nofollow" target="_blank">普鲁塔克的《高贵的希腊人和罗马人的生活<em class="mf"/></a>，也被称为<em class="mf">平行生活</em>或者仅仅是<em class="mf">普鲁塔克的生活</em>，是一系列著名的古希腊人和罗马人的传记，从<a class="ae me" href="https://en.wikipedia.org/wiki/Theseus" rel="noopener ugc nofollow" target="_blank">忒修斯</a>和<a class="ae me" href="https://en.wikipedia.org/wiki/Lycurgus_of_Sparta" rel="noopener ugc nofollow" target="_blank">吕库古</a>到<a class="ae me" href="https://en.wikipedia.org/wiki/Mark_Antony" rel="noopener ugc nofollow" target="_blank">阿非利加努斯·戈狄亚努斯二世</a>。在这篇文章/教程中——继最近出版的第 1 部分之后——我将继续使用一些自然语言处理技术来探索这本书。</p><p id="3432" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">为了便于复制，我将代码改编为 Google Colab，并强调了该平台的独特之处——否则整个代码都可以在 Python 3.6+上本地运行。代码在整篇文章中依次出现，Github 文件的链接嵌入在文章的最后，因为我可能会跳过一些次要的细节或补充代码。</p><p id="59dc" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">古登堡计划已经提供了本分析中使用的文本<a class="ae me" href="https://www.gutenberg.org/ebooks/674" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="6107" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">设置事物</h1><p id="f366" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">在 Colab 上，我们先把运行时类型改成 GPU，然后导入 OS 和<a class="ae me" href="https://en.wikipedia.org/wiki/Regular_expression" rel="noopener ugc nofollow" target="_blank">正则表达式</a>库，保存+打印文件路径备查:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="2d08" class="mu kr jb mq b gy mv mw l mx my">import os<br/>import re<br/>fpath = os.getcwd(); fpath</span></pre><p id="1581" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">让我们将文本(Plutarch.txt)导入到 Google Colab 驱动器中——我们需要记住，我们在那里的文件是短暂的，我们需要在每次使用该平台较长时间后上传它们:</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="8ee3" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">上面的代码也可以在 Colab 的 Code Snippets 选项卡下找到——还有许多其他非常有用的代码。当执行这段代码时，我们将看到 Colab 上传文件，然后我们可以单击左边的 Colab Files 选项卡，以确保该文件与 Google 的默认示例数据目录在一起。</p><p id="5b93" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">接下来，我们需要对文本进行标记—但是，与上一篇文章不同，我们希望保持类似句子的结构，并且不希望删除停用词:</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="c0ef" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">通常在 NLP 任务中，停用词不经思考就消失了，而且在许多情况下是有意义的。然而，由于我们将处理单词嵌入，其中单词的训练和它的上下文是相互依赖的，我们希望确保这样的上下文(停用词是其中的一部分)得到保留。</p><h1 id="3210" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">单词嵌入和相似性</h1><p id="259d" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">让我们继续训练文本，以便我们可以例如计算彼此之间的单词相似度(被算法理解为几何邻近度)。如果你更喜欢从更理论化的方法开始，我认为这个斯坦福大学 word2vec 讲座非常棒——可能最好是喝一杯你最喜欢的饮料。我获得的信息金块之一是每个单词都有两个向量:作为中心单词和上下文单词(视频中的 41:37)，因此当我在一个散点图的两个地方看到同一个单词时，我最初的困惑得到了解决。</p><p id="9e10" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">接下来，让我们使用<a class="ae me" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec 模型</a>的四个变体:连续单词包(CBOW，基于上下文单词预测当前单词)和 skip-gram(CBOW 的逆-基于当前单词预测上下文单词)，每个都具有<a class="ae me" rel="noopener" target="_blank" href="/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08">负采样和分层 softmax </a>选项:</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="cdea" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">在上面的代码中，<em class="mf"> min_count </em>允许根据单词的频率忽略它们，所以在这种情况下，我们希望每个单词都被计算在内；<em class="mf">尺寸</em>表示尺寸的数量；<em class="mf">窗口</em>是当前字和预测字之间的最大距离；<em class="mf"> iter </em>是文本上的历元/迭代次数；<em class="mf"> sg=1 </em>表示我们正在使用 skip-gram，否则为 CBOW<em class="mf"> hs=1 </em>表示我们在使用分层 softmax，否则就是负采样(分层 softmax 对于不常用的词更好<a class="ae me" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">)。如果你想更深入了解，还有更多选择</a>。</p><p id="62cd" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">四个选项的结果差异很大，但为了时空的利益，让我们更深入地研究第一个选项，负采样的 CBOW:</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="f573" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">我们会注意到“凯撒”和“国王”之间的相似度非常高，考虑到这两个词是统治者的同义词，这是有道理的——尽管当时<a class="ae me" href="https://www.thoughtco.com/early-rome-and-issue-of-kings-118344" rel="noopener ugc nofollow" target="_blank">罗马人厌恶“国王”</a>的称号。下面是上面代码中的类似单词:</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nb"><img src="../Images/790a7ae9e09b12f6d62fa16278c5b830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wyna5NGtwLvqTuAcXpDWYg.png"/></div></div></figure><p id="172c" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">正如我们将看到的，皇帝的名字与其他皇帝和政治家的名字非常相似。“百”是接近其他数字和货币名称。“国王”与皇帝的名字和“死亡”联系在一起——可能部分是因为那时政治中的暗箭伤人更加字面化。真理接近上帝，反之亦然。</p><p id="2319" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">您的结果可能与我的不同，因为哈希随机化和模型不局限于单个工作线程(这里的<a class="ae me" href="https://stackoverflow.com/questions/34831551/ensure-the-gensim-generate-the-same-word2vec-model-for-different-runs-on-the-sam" rel="noopener ugc nofollow" target="_blank">解决方案</a>)。</p><h1 id="500f" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">降维</h1><p id="cc7c" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">除非你是《星际迷航》中的<a class="ae me" href="https://en.wikipedia.org/wiki/Q_(Star_Trek)" rel="noopener ugc nofollow" target="_blank"> Q，为了可视化你的单词嵌入，你需要大幅减少维数，理想情况下最多两到三个。现在，我们有 100 个。有两种流行的方法，我们将在本文中提到:主成分分析和 t-SNE。</a></p><p id="4f2b" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">PCA ( <a class="ae me" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a>)是一种线性和无监督的方法，允许提取数据的低维表示，从说明数据中大部分可变性的主成分开始:</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="c80b" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">该片段将产生一个数据帧，每个单词有两个坐标。</p><p id="7c9f" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">t-SNE(<a class="ae me" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank">t-分布式随机邻居嵌入</a>)是一种非线性且无监督的降低数据维数的方法。它主要集中在可视化上，所以——不像 PCA——我们不能在超过三个维度上运行它。有一篇关于如何有效使用 t-SNE 的优秀文章<a class="ae me" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="009f" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">运行下面的代码需要一段时间，所以请确保您有时间:</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="383e" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">如果你想试验各种超参数的变化，代码很容易操作；这对于 SNE 霸王龙来说尤其重要，因为它可以更好地了解自己的行为。t-SNE 的另一个棘手之处是它的非确定性(概率性)本质，这意味着每次运行时，我们可能会对相同的数据得到不同的结果——这是因为用于最小化目标函数的梯度下降优化是随机启动的。然而，尽管它比 PCA 年轻 75 岁(SNE 霸王龙是在 2008 年发明的，而 PCA 是在 1933 年发明的)，SNE 霸王龙还是很管用。</p><h1 id="cbac" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">形象化</h1><p id="4349" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">现在我们已经控制了尺寸，我们可以开始画一些图了。然而，如果我们试图绘制整个唯一单词集，我们将得到这个(PCA 版本):</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/671e7332f107744f7ae5cf976cf882b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GirXgvZyjsY0J36Sa85WYQ.png"/></div></div></figure><p id="0720" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">即使在扩大图片后，也很难提取东西——我甚至没有贴标签，因为这样散点图看起来简直糟透了。我们稍后将使用 Tensorflow 投影仪解决这个问题，但现在让我们只关注相似词子集，从 PCA 二维图开始:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="82e7" class="mu kr jb mq b gy mv mw l mx my">from matplotlib.pyplot import figure<br/>import matplotlib<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>from sklearn.decomposition import PCA</span><span id="05b5" class="mu kr jb mq b gy nd mw l mx my">fig = matplotlib.pyplot.gcf()<br/>fig.set_size_inches(18, 14)</span><span id="3fc6" class="mu kr jb mq b gy nd mw l mx my">simwords = sum([[k] + v for k, v in similar_words.items()], [])<br/>wvs = model_cbow.wv[simwords]</span><span id="f486" class="mu kr jb mq b gy nd mw l mx my">pca_wvs = PCA(n_components=2, random_state=0)<br/>np.set_printoptions(suppress=True)<br/>Tpca = pca_wvs.fit_transform(wvs)<br/>labels = simwords</span><span id="d10a" class="mu kr jb mq b gy nd mw l mx my">plt.figure(figsize=(16, 12))<br/>plt.scatter(Tpca[:, 0], Tpca[:, 1], c='purple', edgecolors='purple')<br/>for label, x, y in zip(labels, Tpca[:, 0], Tpca[:, 1]):<br/>    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')</span></pre><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ne"><img src="../Images/d8542faa8a937da187327a234ef727e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3x9f_pj0cGUQ6rRj1Zy7Q.png"/></div></div></figure><p id="5e38" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">我们看到数词聚集在图的右侧。在左上角，我们看到许多皇帝，随着我们沿着 Y 轴往下走，皇帝的名字越来越与地理名称混在一起。t-SNE 当量看起来不同，但相似的距离逻辑是显而易见的:</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/997803dbc95e6182c5363db42dd65c3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C0wd69OcghDhyTPYDK6gMQ.png"/></div></div></figure><h1 id="b043" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">使用张量流投影仪进行可视化</h1><p id="3096" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">如果不提到谷歌的一个令人惊叹的可视化工具——<a class="ae me" href="http://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">tensor flow Projector</a>，我们会觉得很不妥；它允许动态地与单词嵌入进行交互，并通过点击按钮进行更深入的挖掘。</p><p id="754a" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">在左上角，我们选择 Word2Vec All，在右边的搜索栏中，我们键入并单击“caesar”。原始空间中最近的点显示在下面，我们可以在平台上进行切换，在 PCA 和 t-SNE 之间切换，增加邻居的数量，选择球形化数据，选择余弦或欧几里德距离等。让我们根据我们的普鲁塔克分析将这些结果与“ceasar”进行比较。</p><p id="0cd4" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">但首先我们应该为投影仪准备数据，即两个 csv / tsv 文件，其中一个包含向量，另一个包含这些向量表示的单词(投影仪称之为元数据):</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="1d9b" class="mu kr jb mq b gy mv mw l mx my">import pandas as pd<br/>project_wvs = [(term, voc.index, voc.count) for term, voc in model_cbow.wv.vocab.items()]<br/>project_wvs = sorted(project_wvs, key=lambda k: k[2])<br/>ordered_terms, term_indices, term_counts = zip(*project_wvs)<br/>df_cbow100 = pd.DataFrame(model_cbow.wv.vectors[term_indices, :], index=ordered_terms)</span><span id="8c50" class="mu kr jb mq b gy nd mw l mx my">df_cbow100['word'] = df_cbow100.index<br/>df_cbow100['word'].to_csv('df_cbow100word.tsv', sep='\t', encoding='utf-8', index=False, header=False)</span><span id="dfb3" class="mu kr jb mq b gy nd mw l mx my">df_cbow100vector = df_cbow100.iloc[:,0:100].copy()<br/>df_cbow100vector.to_csv('df_cbow100vector.tsv', sep='\t', encoding='utf-8', index=False, header=False)</span></pre><p id="8197" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">上面，我们从之前生成的 word2vec 输出中创建了一个 dataframe (df_cbow100)。然后，我们将数据帧分成所需的仅标签(df_cbow100word.tsv)和仅矢量(df_cbow100vector.tsv)文件。要将文件下载到我们的本地机器上，我们只需双击 Colab 中相应的文件名。</p><p id="dbf4" class="pw-post-body-paragraph li lj jb lk b ll mg kc ln lo mh kf lq lr mi lt lu lv mj lx ly lz mk mb mc md ij bi translated">然后，让我们回到<a class="ae me" href="http://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">投影仪</a>，点击左上角的“加载”按钮，上传两个文件。瞧——现在我们自己的单词嵌入被反映出来了，我们可以使用投影仪工具来操纵我们认为合适的可视化。正如所料，与“caesar”最近的点相对于 Word2Vec All 有很大不同，word 2 vec All 是在更大的数据集上训练的。</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/97499a9208459e8bec79b586857f9daf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qQ1mkAw_6NniHownTB53AQ.gif"/></div></div></figure><h1 id="90e7" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">结论</h1><p id="3115" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">Github 上的<a class="ae me" href="https://github.com/mlai-demo/TextExplore/blob/master/RePlutarch_EmbedPub.ipynb" rel="noopener ugc nofollow" target="_blank">提供了完整的代码和更多内容，而《重新想象普鲁塔克》的</a><a class="ae me" rel="noopener" target="_blank" href="/reimagining-plutarch-with-nlp-part-1-24e82fc6556">第 1 部分可以在这里阅读</a>。在 Reimagining Plutarch 的两个部分中，我们谈到了使用 NLTK、Wordcloud、Word2Vec 嵌入和可视化来分析文本的方法，并为那些有兴趣深入研究的人提供了额外的资源。NLP 相关知识和编程诀窍的可访问性，以及像 Google 的 Colab 这样的工具，使得实现高级概念和跟踪该领域的进展比以往任何时候都更容易。</p></div></div>    
</body>
</html>