<html>
<head>
<title>Perceptron Algorithms for Linear Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性分类的感知器算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/perceptron-algorithms-for-linear-classification-e1bb3dcc7602?source=collection_archive---------7-----------------------#2019-08-20">https://towardsdatascience.com/perceptron-algorithms-for-linear-classification-e1bb3dcc7602?source=collection_archive---------7-----------------------#2019-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7bc3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解感知器算法是如何工作的，以及它们背后的直觉。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c68b54ec273580694422ec111a2cfc6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8zQW3SanV2FQr5iYPY-2Gg.png"/></div></div></figure><p id="beab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated">基本感知器算法是由<a class="ae lz" href="#6b44" rel="noopener ugc nofollow">参考文献 1 </a>在 20 世纪 50 年代末首次提出的。这是一个用于监督学习的二元线性分类器。二元线性分类器背后的思想可以描述如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/0ebc5f24d73221a8bdad2a29abfddaf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*LMHHvqD0SIjycr335cvFfQ.png"/></div></figure><p id="f1f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<em class="mb"> x </em>为特征向量，<em class="mb"> θ </em> <strong class="kw iu"> </strong>为权重向量，<em class="mb"> θ </em>为偏差。符号函数用于区分<strong class="kw iu"> </strong> <em class="mb"> x </em>是正(+1)还是负(-1)标签。存在用不同标签分离数据的决策边界，这发生在</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/000eeefbf3967e558b5144957f7db419.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*gFWVNAig4uitRsUXW7S_Jg.png"/></div></figure><p id="ad5e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">决策边界将超平面分成两个区域。在<strong class="kw iu"/><em class="mb">θ⋅x</em>+<em class="mb">θ</em>₀&gt;，<strong class="kw iu"> <em class="mb"> </em> </strong>的区域，数据将被标记为正，在<em class="mb"> θ⋅ x </em> + <em class="mb"> θ </em> ₀ &lt;的区域，数据将被标记为负。如果一个给定数据中的所有实例都是<strong class="kw iu">线性可分的</strong>，则对于每第<em class="mb"> i </em>个数据点，存在一个<em class="mb"> θ </em>和一个<em class="mb"> θ </em> ₀，使得 y⁽<em class="mb">ⁱ</em>⁾(<em class="mb">θ⋅x</em>⁽<em class="mb">ⁱ</em>⁾+<em class="mb">θ</em><em class="mb">)【t100，其中 y⊙<em class="mb"/></em></p><p id="b97e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图 1 用二维情况说明了上述概念，其中<em class="mb">x</em>=[<em class="mb">x</em>₁<em class="mb">x</em>₂]ᵀ、<em class="mb">θ</em>=[<em class="mb">θ</em>₁<em class="mb">θ</em>₂】和<em class="mb"> θ </em> ₀是偏移标量。请注意，边距边界与正则化相关，以防止数据过度拟合，这超出了这里讨论的范围。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi md"><img src="../Images/33f2f4ec80c5572ec2d150b146fca480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sfqR-rZSYaITJ5_PbXU_9w.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Figure 1. The concepts of binary linear classifier with the 2-D case.</figcaption></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="eecf" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">感知器</h1><p id="6164" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">找到决策边界的一种方法是使用感知器算法。只有当决策边界错误分类数据点时，感知器算法才会更新<em class="mb"> θ </em>和<em class="mb"> θ </em> ₀。算法的伪代码描述如下。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="27b2" class="nr mq it nn b gy ns nt l nu nv"><em class="mb"># Perceptron Algorithm</em></span><span id="6f44" class="nr mq it nn b gy nw nt l nu nv"><em class="mb"># initialize θ and θ₀ with 0</em><br/>θ = 0  (vector)<br/>θ₀ = 0 (scalar)</span><span id="c74c" class="nr mq it nn b gy nw nt l nu nv"><em class="mb"># totally T epoches to iterate</em><br/>for t = 1 .. T do                     <br/>   <em class="mb"> # totally m data points</em>    <br/>    for i = 1 .. m do<br/>        <em class="mb"># misclassify data points</em>                 <br/>        if y⁽ⁱ⁾(θ ⋅ x⁽ⁱ⁾ + θ₀) ≦ 0     <br/>        then<br/>            θ  = θ + y⁽ⁱ⁾ ⋅ x⁽ⁱ⁾<br/>            θ₀ = θ₀ + y⁽ⁱ⁾</span><span id="419d" class="nr mq it nn b gy nw nt l nu nv">return θ, θ₀</span></pre><p id="a617" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感知器算法遍历所有带标签的数据点，并相应地更新<em class="mb"> θ </em>和<em class="mb"> θ </em> ₀。更新规则背后的直觉是将 y⁽<em class="mb">ⁱ</em>⁾(<em class="mb">θ⋅x</em>⁽<em class="mb">ⁱ</em>⁾+<em class="mb">θ</em>₀)推得更接近一个正值，如果 y⁳<em class="mb">θ≡x</em>θ≦<em class="mb"/></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e450c4f0ea2929a5bd028e5cb3c7f3fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*O5zcE07P-gG6tpDezROULQ.png"/></div></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="83ca" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">感知器收敛</h1><p id="dbe9" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">构成感知器算法所犯错误数量界限的因素是数据点的最大范数和正负数据点之间的最大差值。感知器收敛定理已在<a class="ae lz" href="#cbb7" rel="noopener ugc nofollow">参考文献 2 中得到证明。</a>给定一组通过原点可线性分离的数据点，<em class="mb"> θ </em>的初始化不会影响感知器算法最终收敛的能力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c45b86ed18ef14ebb4edd85c30d84677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*6hO_dGP8iQY6DWDgr7duQg.png"/></div></figure><p id="3235" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">迭代次数<em class="mb"> k </em>具有有限值意味着一旦数据点通过原点是线性可分的，无论<em class="mb"> θ </em>的初始值是什么，感知器算法最终都会收敛。这些概念也代表了<em class="mb"> θ </em> ₀.的存在</p><p id="5de9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，一旦数据点是线性不可分的，这种感知器算法可能会遇到收敛问题。有两种感知器算法变体被引入来处理这些问题。一个是平均感知器算法，一个是 pegasos 算法。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="4771" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">平均感知器</h1><p id="ceaf" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">与感知器算法类似，平均感知器算法使用相同的规则来更新参数。然而，<em class="mb"> θ </em>和<em class="mb"> θ </em> ₀的最终返回值取每次迭代中<em class="mb"> θ </em>和<em class="mb"> θ </em> ₀的所有值的平均值。算法的伪代码描述如下。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="0eed" class="nr mq it nn b gy ns nt l nu nv"><em class="mb"># Average Perceptron Algorithm</em></span><span id="bc16" class="nr mq it nn b gy nw nt l nu nv"><em class="mb"># initialize θ, θ₀, sum_θ, sum_θ₀, and counter with 0</em><br/>θ = 0  (vector)<br/>θ₀ = 0 (scalar)<br/>sum_θ = 0  (vector)<br/>sum_θ₀ = 0 (scalar)<br/>counter = 0</span><span id="b437" class="nr mq it nn b gy nw nt l nu nv"><em class="mb"># totally T epoches to iterate</em><br/>for t = 1 .. T do                     <br/>    <em class="mb"># totally m data points </em>   <br/>    for i = 1 .. m do  <br/>        <em class="mb"># misclassify data points   </em>            <br/>        if y⁽ⁱ⁾(θ ⋅ x⁽ⁱ⁾ + θ₀) ≦ 0     <br/>        then<br/>            θ  = θ + y⁽ⁱ⁾ ⋅ x⁽ⁱ⁾<br/>            θ₀ = θ₀ + y⁽ⁱ⁾</span><span id="f2da" class="nr mq it nn b gy nw nt l nu nv">        sum_θ = sum_θ + θ<br/>        sum_θ₀ = sum_θ₀ + θ₀<br/>        counter = counter + 1</span><span id="6509" class="nr mq it nn b gy nw nt l nu nv">return (sum_θ/counter), (sum_θ₀/counter)</span></pre></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="c466" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">佩加索斯</h1><p id="8b43" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">pegasos 算法具有超参数<em class="mb"> λ </em>，为要调整的模型提供了更大的灵活性。无论数据点是否分类错误，都会更新<em class="mb"> θ </em>。详情见<a class="ae lz" href="#c12e" rel="noopener ugc nofollow">参考文件 3。</a>算法的伪代码描述如下。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="8b60" class="nr mq it nn b gy ns nt l nu nv"><em class="mb"># Pegasos Algorithm</em></span><span id="0f9e" class="nr mq it nn b gy nw nt l nu nv"><em class="mb"># initialize θ, θ₀, and counter with 0</em><br/>θ = 0  (vector)<br/>θ₀ = 0 (scalar)<br/>counter = 0</span><span id="685e" class="nr mq it nn b gy nw nt l nu nv"><em class="mb"># totally T epoches to iterate</em><br/>for t = 1 .. T do <br/>   <em class="mb"> # totally m data points </em>                   <br/>    for i = 1 .. m do                 <br/>        counter = counter + 1<br/>        η = 1/√counter <br/>         <br/>        if y⁽ⁱ⁾(θ⋅x⁽ⁱ⁾ + θ₀) ≦ 1    <br/>        then<br/>            θ  = (1 - ηλ)θ + ηy⁽ⁱ⁾⋅x⁽ⁱ⁾<br/>            θ₀ = θ₀ + ηy⁽ⁱ⁾<br/>        else<br/>        then<br/>            θ  = (1 - ηλ)θ<br/>            θ₀ = θ₀</span><span id="1d59" class="nr mq it nn b gy nw nt l nu nv">return θ,  θ₀</span></pre></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="ace6" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">可视化感知器算法</h1><p id="bf4f" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">图二。通过不同的感知器算法可视化决策边界的更新。请注意，给定的数据是线性不可分的，因此感知器算法绘制的决策边界会发散。平均感知器算法和 pegasos 算法都很快达到收敛。pegasos 算法的<em class="mb"> λ </em>在这里使用 0.2。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/20e4b434602296846ea7cf33731b33f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Pwp3CYKLspEqjDKjDYX5YA.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Figure 2. Updating the decision boundaries by the different perceptron algorithms. May take time to load.</figcaption></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="4a60" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">示例代码</h1><p id="b04a" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi translated">在 Jupyter 笔记本上写的感知器算法的样本代码可以在<a class="ae lz" href="https://github.com/AnHungTai/Medium-SampleCode/blob/master/Perceptron%20Algorithms%20for%20Linear%20Classification/Visualizing%20Perceptron%20Algorithm.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。你可以自己处理数据和超参数，看看不同的感知器算法表现如何。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="baf5" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">参考</h1><p id="6b44" class="pw-post-body-paragraph ku kv it kw b kx nh ju kz la ni jx lc ld nj lf lg lh nk lj lk ll nl ln lo lp im bi">[1]</p><p id="61c9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">F.罗森布拉特，“感知机:大脑中信息存储和组织的概率模型”，<em class="mb">《心理评论》，1958 年</em>。土井:<a class="ae lz" href="http://doi.org/10.1037/h0042519" rel="noopener ugc nofollow" target="_blank"> 10.1037/h0042519 </a></p><p id="cbb7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi">[2]</p><p id="603a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">米（meter 的缩写））Mohri 和 A. Rostamizadeh，“感知机错误界限”，<em class="mb"> arxiv </em>，2013 年。<a class="ae lz" href="https://arxiv.org/pdf/1305.0208.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1305.0208.pdf</a></p><p id="c12e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi">[3]</p><p id="0953" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">南 S.-Shwartz、Y. Singer、N. Srebro 和 A. Cotter，“Pegasos:SVM 初始估计次梯度求解器”，<em class="mb">数学规划</em>，2010 年。doi:<a class="ae lz" href="https://doi.org/10.1007/s10107-010-0420-4" rel="noopener ugc nofollow" target="_blank">10.1007/s 10107–010–0420–4</a></p></div></div>    
</body>
</html>