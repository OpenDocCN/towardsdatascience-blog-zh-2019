<html>
<head>
<title>GRU’s and LSTM’s</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GRU 和 LSTM 的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/grus-and-lstm-s-741709a9b9b1?source=collection_archive---------13-----------------------#2019-02-17">https://towardsdatascience.com/grus-and-lstm-s-741709a9b9b1?source=collection_archive---------13-----------------------#2019-02-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e042" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">递归神经网络是保存信息的网络。它们对序列相关的任务很有用，如语音识别、音乐生成等。然而，RNN 患有短期记忆。如果一个序列足够长，它们将很难把信息从前面的时间步带到后面的时间步。这就是所谓的消失梯度问题。在这篇文章中，我们将研究门控循环单位(GRU)和长短期记忆(LSTM)网络，它们解决了这个问题。如果你没有读过 RNN 氏症，这里有一个链接<a class="ae ko" href="https://medium.com/datadriveninvestor/understanding-recurrent-neural-networks-aea0078defc6" rel="noopener">指向我解释 RNN 是什么以及它如何工作的帖子。</a></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/d2b056fa0d035deb079ebe216d3f67b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*68EDFvjbY2EJG9IkKmWkZw.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Basic Architecture of RNN Cell</figcaption></figure><p id="5811" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个标准 RNN 的架构表明，中继模块具有非常简单的结构，只有一个<em class="lb"> tanh </em>层。GRU 和 LSTM 都有类似 RNN 的重复模块，但是重复模块有不同的结构。</p><p id="4cc7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GRU 和 LSTM 的关键思想是细胞状态或记忆细胞。它允许两个网络保留任何信息而不会丢失太多。这些网络也有闸门，帮助调节流向细胞状态的信息流。这些门可以知道序列中哪些数据是重要的，哪些是不重要的。通过这样做，它们以长序列传递信息。现在，在我们进入 LSTM 之前，让我们先试着理解 GRU 或门控循环单元。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi lc"><img src="../Images/46fc3f0eae6837a230633295a02bd42e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LNZTwVNuRQYZmUrtWDntGQ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Basic Architecture of a GRU Cell</figcaption></figure><p id="caaf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以清楚地看到，GRU 单元的架构比简单的 RNN 单元复杂得多。我觉得方程式比图表更直观，所以我会用方程式来解释一切。</p><p id="101b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 GRU 单元中，我们需要注意的第一件事是单元状态<em class="lb">h&lt;t&gt;T5】等于时间<em class="lb"> t </em>的输出。现在，让我们一个一个地看所有的方程。</em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/10d49970050d92c032495a6f751c543d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*pczMk9_83bwuZR7rtAmiPQ.png"/></div></figure><p id="942e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在每个时间步长，我们有两个选项:</p><ol class=""><li id="a753" class="li lj it js b jt ju jx jy kb lk kf ll kj lm kn ln lo lp lq bi translated">保留以前的单元状态。</li><li id="f254" class="li lj it js b jt lr jx ls kb lt kf lu kj lv kn ln lo lp lq bi translated">更新其值。</li></ol><p id="ca31" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的等式示出了在时间<em class="lb"> t </em>可以替换单元状态的更新值或候选值。它依赖于前一时间步<em class="lb">h&lt;t-1&gt;T11】的单元状态和一个称为<em class="lb">r&lt;t&gt;T13】的相关性门，该相关性门在计算当前单元状态时计算前一单元状态的相关性。</em></em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/0b78dc62e1fff178251f1bd9c1dca9af.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*dVvN1hXNYt2_IAAXUNyBnA.png"/></div></figure><p id="caab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如我们所见，关联门<em class="lb"> r &lt; t &gt; </em>具有 sigmoid 激活，其值在 0 和 1 之间，决定了先前信息的相关程度，然后用于更新值的候选中。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/78a801d4420402980baccee89887d128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*aIJYtaAo3FzfdSIO8pRqZw.png"/></div></figure><p id="12eb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当前单元状态<em class="lb">h&lt;t&gt;T17】是先前单元状态<em class="lb"> h &lt; t-1 &gt;和</em>更新候选状态<em class="lb"> h(tilde) &lt; t &gt; </em>的过滤组合。这里，更新门<em class="lb">z&lt;t&gt;T23】决定计算当前单元状态所需的更新候选的部分，这又决定保留的先前单元状态的部分。</em></em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/2bfe1fb4f5ab73265c99afd37dc68de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*UYmuP9eHMislJFUyJXMR0w.png"/></div></figure><p id="b979" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">像关联门一样，更新门也是一个 sigmoid 函数，它帮助 GRU 在需要时保持单元状态。现在，让我们看看在《RNN 邮报》上看到的例子，以便更好地理解 GRU</p><p id="768d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">史密斯太太养的狗意识到房子里有人，正在吠叫。</p><p id="4706" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里的“dogs”一词是必要的，因为 dogs 是复数，所以要知道单词“were”在末尾。让我们有一个单元格状态<em class="lb">c&lt;t&gt;T3】= 1 为复数。因此，当 GRU 到达单词“dogs”时，它理解我们在这里讨论的是句子的主语，并在单元状态中存储值<em class="lb"> c &lt; t &gt; </em> = 1。这个值一直保留到单词“were ”,在这里它理解主语是复数，单词应该是“were”而不是“was”。这里的更新门知道什么时候保留值，什么时候忘记值。所以一旦单词“were”完成，它就知道细胞状态不再有用，并忘记了它。这就是 GRU 如何保持记忆，从而解决渐变消失的问题。</em></p><p id="6826" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然 LSTM 的核心思想是相同的，但它是一个更复杂的网络。让我们试着用类似的方式来理解它。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi lz"><img src="../Images/70c6775b3e79a1745843017247711b1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kBdBYzR7lpimgb3AIRkOw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Basic Unit of a LSTM Cell</figcaption></figure><p id="17b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">乍一看，LSTM 单元看起来确实很可怕，但让我们试着像对 GRU 一样将其分解为简单的方程。GRU 有两个门，称为更新门和关联门，而 LSTM 有三个门，即遗忘门<em class="lb">f&lt;t&gt;T7】，更新门<em class="lb">I&lt;t&gt;T9】和输出门<em class="lb"> o &lt; t &gt; </em>。</em></em></p><p id="ed78" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 GRU，细胞状态等于激活状态/输出，但在 LSTM，它们并不完全相同。时间“t”处的输出由<em class="lb">h&lt;t&gt;T13】表示，而单元状态由<em class="lb">c&lt;t&gt;T15】表示。</em></em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/8cf2262ba1e7e8e41cdf94f190284301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*oR6yvD72NXviy5RsA1gwyg.png"/></div></figure><p id="06f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如同在 GRU，在时间‘t’的单元状态具有候选值<em class="lb"> c(波形符)&lt; t &gt; </em>，其依赖于先前的输出<em class="lb"> h &lt; t-1 &gt; </em>和输入 x &lt; t &gt;。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/d10fed018a191fb12551aa1557aed556.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*0MdR2xq4eE1RD4iC805DnQ.png"/></div></figure><p id="ea4c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">像在 GRU，LSTM 的当前单元格状态 c <em class="lb"> &lt; t &gt; </em>是前一单元格状态和候选值的过滤版本。然而，这里的过滤器由两个门决定，即更新门和忽略门。遗忘门与 GRU 中(1-updateGate &lt; t &gt;的值非常相似，遗忘门和更新门都是 sigmoid 函数。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/2b3d83e4e957ea57dfacf3f2602358b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*MMYZNVYSQ_NmnBx8_aAYsA.png"/></div></figure><p id="fc1e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">遗忘门计算当前单元状态需要多少来自前一单元状态的信息。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi md"><img src="../Images/538de289fd2288ca777d5eff6017acc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*q2G-FFs6jRppZOOw917SgQ.png"/></div></figure><p id="60e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更新门计算在当前单元状态中需要多少候选值<em class="lb"> c(波形符)&lt; t &gt; </em>。更新门和遗忘门的值都在 0 和 1 之间。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi me"><img src="../Images/0b4c226c8bea3e5dfc768d2c31870516.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*K-3M1xFmdal1L3o_2nzzSg.png"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/0fb771b4f550dbbad1029138616d44b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*uD_T7n63YwihRokXfOa_Ag.png"/></div></figure><p id="1ab6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们需要决定我们要输出什么。这个输出将是我们的细胞状态的过滤版本。因此，我们通过一个<em class="lb"> tanh </em>层传递单元状态，将值推到-1 和 1 之间，然后乘以一个输出门，该输出门有一个 sigmoid 激活，因此我们只输出我们决定输出的内容。</p><p id="e8ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">LSTM 氏症和 GRU 氏症在深度学习中基于序列的问题上都非常流行。虽然 GRU 的工作对某些问题有效，但 LSTM 的工作对其他问题也有效。GRU 的简单得多，需要较少的计算能力，因此可以用来形成真正的深度网络，然而 LSTM 的更强大，因为它们有更多的门，但需要大量的计算能力。至此，我希望你对 LSTM 和 GRU 有了基本的了解，并准备好深入序列模型的世界。</p><p id="b99d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参考资料:</p><ol class=""><li id="dd7f" class="li lj it js b jt ju jx jy kb lk kf ll kj lm kn ln lo lp lq bi translated">http://colah.github.io/posts/2015-08-Understanding-LSTMs/<a class="ae ko" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"/></li><li id="e877" class="li lj it js b jt lr jx ls kb lt kf lu kj lv kn ln lo lp lq bi translated">【https://www.coursera.org/learn/nlp-sequence-models T4】</li></ol></div></div>    
</body>
</html>