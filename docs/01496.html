<html>
<head>
<title>Word Distance between Word Embeddings with Weight</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带权重的单词嵌入之间的单词距离</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-distance-between-word-embeddings-with-weight-bf02869c50e1?source=collection_archive---------16-----------------------#2019-03-09">https://towardsdatascience.com/word-distance-between-word-embeddings-with-weight-bf02869c50e1?source=collection_archive---------16-----------------------#2019-03-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2189" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">大规模杀伤性武器和半大规模杀伤性武器的区别</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/034d8509772280a28ac6b15402b5cbff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ghoa4A0PwDH80cJt"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edward Ma</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="8a32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在前面的故事中，我介绍了<a class="ae ky" rel="noopener" target="_blank" href="/word-distance-between-word-embeddings-cc3e9cf1d632">单词移动器的距离(WMD) </a>，它测量单词嵌入之间的距离。你可能会注意到单词之间没有加权机制。加权对 NLP 任务有什么帮助？为此，黄等人提出了一种改进方法，命名为监督字移动器距离(S-WMD)。</p><h2 id="9773" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">引入监督字移动器的距离(S-WMD)</h2><p id="02f9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在介绍<a class="ae ky" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">单词嵌入</a>之前，<a class="ae ky" rel="noopener" target="_blank" href="/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016">词袋(BoW) </a>，<a class="ae ky" rel="noopener" target="_blank" href="/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547">潜在语义索引(LSI)和潜在语义分析(LSA) </a>是衡量 NLP 任务最有前途的技能。</p><p id="779b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/word-distance-between-word-embeddings-cc3e9cf1d632">字动子的距离(WMD)</a>2015 年推出。它利用了 word emveddings (word2vec 于 2013 年推出)。它使用另一种方法，即推土机距离来测量向量之间的差异。一年后，黄等人提出了一种改进方法，称为监督字移动距离(S-WMD)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/5a06b85bcf858a99416a63e5dcda3b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rxffoc4c0lBYV0Ns0gHa_w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The difference t-SNE plots of WMD and S-WMD (Huang et al., 2016)</figcaption></figure><p id="086e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，WMD 算法测量将两个文档中的一个单词向量传输到另一个向量的最小值。如果两个文档共享大量的单词，那么在两个文档之间传输只需要很小的移动。换句话说，这两个文档可以归类为相似文档。</p><h1 id="2640" class="mu lw it bd lx mv mw mx ma my mz na md jz nb ka mg kc nc kd mj kf nd kg mm ne bi translated">加权机制</h1><p id="be6d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">权重机制对 NLP 任务有什么帮助？通过引入权重，有助于解决文档分类问题。</p><p id="21de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直观上，预训练的单词向量应该非常好，因为它是在大量数据上训练的。然而，有一个已知的问题是，预先训练的向量可能不会很好地应用于某些问题。例如，预先训练的向量可以将所有可吃的食物放在一起，并将蔬菜和肉类混合在一起。如果分类问题是要分类它是不是织女星呢？</p><p id="217d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，两个文档共享大量的单词并不意味着它们都描述相同的主题。例如，“我去学校教学生”和“我去学校学英语”。可以是谈论学校的生活，也可以是在不同的团体中谈论学校的任务。换句话说，它确实依赖于 NLP 任务。它可能相关也可能不相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/ca476a0a8f64061ff093b053c054cd08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EEyQusXeRfD4RFh2rgYcA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">kNN test error (Huang et al., 2016)</figcaption></figure><h1 id="f3b9" class="mu lw it bd lx mv mw mx ma my mz na md jz nb ka mg kc nc kd mj kf nd kg mm ne bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae ky" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="a986" class="mu lw it bd lx mv mw mx ma my mz na md jz nb ka mg kc nc kd mj kf nd kg mm ne bi translated">参考</h1><p id="8261" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">黄高、郭川、库斯纳·马特 j、孙瑜、温伯格·基连 q、沙飞。2016.<a class="ae ky" href="https://papers.nips.cc/paper/6139-supervised-word-movers-distance.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/6139-supervised-word-movers-distance . pdf</a></p><p id="fb73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/gaohuang/S-WMD" rel="noopener ugc nofollow" target="_blank">Matlab 中的 S-WMD</a>(原文)</p><p id="4572" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/mkusner/wmd" rel="noopener ugc nofollow" target="_blank">python 中的 S-WMD</a></p></div></div>    
</body>
</html>