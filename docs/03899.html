<html>
<head>
<title>[CVPR 2019] Pose2Seg: Detection Free Human Instance Segmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[CVPR 2019] Pose2Seg:无检测人体实例分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cvpr-2019-pose2seg-detection-free-human-instance-segmentation-61e4948ba6db?source=collection_archive---------18-----------------------#2019-06-19">https://towardsdatascience.com/cvpr-2019-pose2seg-detection-free-human-instance-segmentation-61e4948ba6db?source=collection_archive---------18-----------------------#2019-06-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8925" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本帖中，我们将回顾来自<strong class="jp ir"> CVPR 2019 </strong>的论文<strong class="jp ir">“pose 2 seg:检测自由人体实例分割”</strong>。提出了一种新的人体实例分割方法，该方法基于人体姿态而不是提议区域检测来分离实例。</p><p id="66b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文的一些亮点:</p><ul class=""><li id="4021" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">基于姿态的人体实例分割框架</strong>比基于 SOTA 检测的方法能够达到<strong class="jp ir">更好的准确性</strong>并且能够<strong class="jp ir">更好地处理遮挡</strong>。</li><li id="e915" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">一个新的基准<strong class="jp ir">“被遮挡的人(OCHuman)”</strong>，它关注被遮挡的人，带有包括边界框、人姿势和实例遮罩的注释。</li></ul><h1 id="dcf0" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">概述</h1><ul class=""><li id="46b9" class="kl km iq jp b jq lx ju ly jy lz kc ma kg mb kk kq kr ks kt bi translated">介绍</li><li id="5a03" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">被遮挡人类基准(OCHuman)</li><li id="b73c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">体系结构</li><li id="bcff" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">实验</li><li id="2cfc" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">履行</li><li id="afd5" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">参考</li></ul><h1 id="c3cc" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">介绍</h1><p id="74d3" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">人体姿态估计和分割是更好地理解人体活动的重要信息。有很多研究集中在这个主题上。最流行的深度学习方法之一是 Mask R-CNN，它是一个简单而通用的对象实例分割框架。</p><p id="f577" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">即使通过像 Mask-RCNN 这样的方法，也可以检测对象并为图像中的每个实例生成分割掩模。这些方法存在一些问题:</p><ul class=""><li id="1ee3" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">这些方法首先执行对象检测，然后使用<em class="mf">非最大值抑制(NMS) </em>移除冗余区域，并从检测包围盒中分割出对象。<strong class="jp ir">当同一类别的两个对象有较大重叠时，NSM 会将其中一个作为冗余的提议区域，并将其消除</strong>。</li><li id="00f2" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">“人”是一个特殊的类别，可以基于姿态骨架来定义。目前的方法如 Mask-RCNN 没有利用姿态信息进行分割。</li></ul><p id="77e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文提出的方法<strong class="jp ir"> Pose2Seg </strong>，是专门为人体实例分割而设计的，具有以下优点:</p><ul class=""><li id="a9a1" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">使用自下而上的方法，这种方法基于姿态信息而不是边界框检测。</li><li id="d081" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">旨在解决每个人类实例被一个或几个其他实例严重遮挡的问题。</li></ul><h1 id="2147" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">被遮挡人类基准(OCHuman)</h1><p id="89db" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated"><strong class="jp ir">遮挡人体基准(OCHuman) </strong>本文引入数据集，强调遮挡是研究人员研究的一个挑战性问题，并鼓励算法变得更适用于现实生活情况。</p><p id="1905" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">遮挡人体基准(OCHuman) </strong>数据集包含<strong class="jp ir"> 8110 </strong>带有<strong class="jp ir"> 4731 </strong>图像的详细注释人体实例。平均而言，一个人的超过 67%的包围盒区域被一个或几个其他人遮挡。</p><p id="f62a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是一些来自 OCHuman 数据集的图片:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mg"><img src="../Images/c294e0b107193c3faef53179e75b9e70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ccpuFj1-GR5Ih3nX.jpg"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Source: OCHuman Dataset</figcaption></figure><h2 id="1877" class="mw la iq bd lb mx my dn lf mz na dp lj jy nb nc ln kc nd ne lr kg nf ng lv nh bi translated">OCHuman 数据集</h2><p id="9061" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">这是一个比较 COCO 数据集和 OCHuman 数据集的表格。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ni"><img src="../Images/dc02f1683326cf8900f83bb6cb31608f.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/0*5ScT4IxvseY-FiXt.png"/></div></div></figure><p id="8008" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如你所见，COCO 包含很少的被遮挡的人类案例，当面对被遮挡的人脸时，它不能帮助评估方法的能力。</p><p id="89e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">OCHuman 是为与人类相关的所有三个最重要的任务而设计的:检测、姿势估计和实例分割。</p><p id="e5b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">奥乔曼最重要的方面是平均马西欧是<strong class="jp ir"> 0.67 </strong>。这意味着超过<strong class="jp ir"> 67% </strong>的人的包围盒区域被一个或几个其他人遮挡。这在视频监控或店内人体行为分析等实际应用中是一个非常常见的问题。</p><h1 id="8602" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">体系结构</h1><p id="a842" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">Pose2Seg 的结构如下图所示:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mg"><img src="../Images/33e35311f61c70a26efeca4987239854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wTTWEuihnbSdXTbp.jpg"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Source: Pose2Seg project page <a class="ae nj" href="http://www.liruilong.cn/projects/pose2seg/index.html" rel="noopener ugc nofollow" target="_blank">http://www.liruilong.cn/projects/pose2seg/index.html</a></figcaption></figure><p id="9206" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该方法的步骤可以描述如下:</p><ul class=""><li id="3de7" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">首先，模型将<strong class="jp ir">图像</strong>和<strong class="jp ir">人体姿态</strong>作为输入。人体姿态可以是其他方法的输出，如 OpenPose 或数据集的地面实况。</li><li id="f7a4" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">整个图像通过<strong class="jp ir">基网络</strong>来提取图像的特征。</li><li id="1ebf" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">对准模块<strong class="jp ir">仿射对准</strong>用于将<strong class="jp ir">感兴趣区域</strong>对准到统一尺寸。可以想象这个模块会从大图中提取多个固定大小的区域。每个固定大小的区域对应于图像中的每个人。然后，<strong class="jp ir">仿射对齐</strong>区域将执行仿射变换，以将每个姿势与<strong class="jp ir">姿势模板</strong>之一对齐。</li><li id="4666" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">仿射对齐</strong>的对齐输出将与<strong class="jp ir">骨架特征</strong>连接，并馈入<strong class="jp ir">分割模块</strong>以生成分割掩模。</li><li id="b6d9" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">骨架特征</strong>:简单来说就是<em class="mf">零件亲和场(PAF)</em>，它是每个骨架的双通道矢量场图。这是 OpenPose 的输出。</li><li id="2dcc" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir"> SegModule </strong>:是一个 CNN 网络，以<strong class="jp ir"> 7 x 7 stride-2 conv 层</strong>开始，后面是几个标准剩余单元。然后，双线性上采样层用于恢复分辨率，并且<strong class="jp ir">1×1 conv 层</strong>用于预测遮罩结果。</li><li id="784c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">最后，使用来自<strong class="jp ir">仿射对齐</strong>的仿射变换的逆变换，将每个人的分割掩模组合成一个最终分割掩模。</li></ul><h1 id="e07f" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">实验</h1><h1 id="8c4a" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">咬合性能</h1><ul class=""><li id="69e1" class="kl km iq jp b jq lx ju ly jy lz kc ma kg mb kk kq kr ks kt bi translated">Pose2Seg 在 OCHuman 数据集上可以达到比 Mask R-CNN 高近 50%的性能。作者还使用地面实况关键点作为输入进行了测试，准确度提高了一倍多<strong class="jp ir"> (GT Kpt) </strong>。</li></ul><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nk"><img src="../Images/849d0921426fb9f062345b8d41080faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bvvpRSxvJGhKyORw.png"/></div></div></figure><ul class=""><li id="4d8b" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">关于闭塞情况的一些结果:</li></ul><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nl"><img src="../Images/de41c1fce2bf216c65a7be686513f741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*htnaM9oKECnmnd53.png"/></div></div></figure><h1 id="0df1" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">一般情况下的性能</h1><ul class=""><li id="6825" class="kl km iq jp b jq lx ju ly jy lz kc ma kg mb kk kq kr ks kt bi translated">Pose2Seg 在 COCOPersons 数据集上也能达到比其他方法更高的准确率。</li></ul><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/7767fa30aea4599cb7da6081073c936d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*Z7pkPrrv17nCA_t6.png"/></div></figure><h1 id="5a23" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">履行</h1><p id="7362" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">这篇论文的官方 PyTorch 代码可以在 https://github.com/liruilong940607/Pose2Seg 找到</p><h1 id="9756" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">参考</h1><ul class=""><li id="cbde" class="kl km iq jp b jq lx ju ly jy lz kc ma kg mb kk kq kr ks kt bi translated">【1】张，宋海和李，瑞龙和董，辛和松香，保罗 L 和蔡，子希和，韩和杨，丁成和黄，郝智和胡，石世民，Pose2Seg:检测自由人体实例分割(2019)，2019 </li><li id="2f53" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><a class="ae nj" href="https://arxiv.org/abs/1611.08050" rel="noopener ugc nofollow" target="_blank">【2】曹哲，托马斯·西蒙，施-韦恩，亚塞尔·谢赫，OpenPose:利用局部亲和场的实时多人 2D 姿态估计(2017)，CVPR 2017 口述</a></li></ul><h1 id="da0d" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">我的评论</h1><p id="8236" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">图像分类:<a class="ae nj" href="https://medium.com/p/alexnet-review-and-implementation-e37a8e4dab54" rel="noopener">【NIPS 2012】AlexNet</a><br/>图像分割:<a class="ae nj" rel="noopener" target="_blank" href="/cvpr-2019-pose2seg-detection-free-human-instance-segmentation-61e4948ba6db">【CVPR 2019】Pose 2 seg</a><br/>姿态估计:<a class="ae nj" rel="noopener" target="_blank" href="/cvpr-2017-openpose-realtime-multi-person-2d-pose-estimation-using-part-affinity-fields-f2ce18d720e8">【CVPR 2017】open Pose</a><br/>姿态跟踪:<a class="ae nj" rel="noopener" target="_blank" href="/cvpr-2019-efficient-online-multi-person-2d-pose-tracking-with-recurrent-spatio-temporal-affinity-25c4914e5f6">【CVPR 2019】STAF</a></p></div></div>    
</body>
</html>