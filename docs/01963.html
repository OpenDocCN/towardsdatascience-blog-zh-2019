<html>
<head>
<title>Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">忘记 API 用漂亮的汤做 Python 抓取，从 web 导入数据文件:第 2 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/forget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246?source=collection_archive---------5-----------------------#2019-04-01">https://towardsdatascience.com/forget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246?source=collection_archive---------5-----------------------#2019-04-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="8bd1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">不是每个网站都有 API，但 Beautiful Soup 将永远与您在一起，从任何网站收集任何类型的数据。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/4f1528627399263ea9f2616e5f600c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4BnBQE9Bu-EQ-gGz25x8pg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Souce: gurutechnolabs.com</figcaption></figure><p id="d275" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi lf translated"><span class="l lg lh li bm lj lk ll lm ln di">如今，数据在每个行业都扮演着至关重要的角色。这些数据大部分来自互联网。大多数公司在一项技术上投资数百万美元来获得用户，却没有从投资产品回报中获利。互联网是如此之大，它包含了比你的书呆子教授更多的信息。<br/>从网络中提取信息的重要性变得越来越明显。大多数时候，当我们在你的 facebook、twitter、LinkedIn 上添加任何信息并在 Yelp 上提供反馈时，这些信息都被视为数据。</span></p><p id="e4f3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种来自互联网的数据有很多不同的来源，例如评论、Yelp 上的餐厅反馈、Twitter 讨论、Reddit 用户讨论和股票价格等。你可以收集所有这些数据，组织并分析它们。这就是我们在本教程中要讨论的内容。<br/>从互联网上提取或导入数据有多种方式。您可以使用 API 从任何主要网站检索信息。这就是如今每个人从互联网上导入数据的方式——所有主要网站，如 Twitter、Twitch、Instagram、脸书，都提供 API 来访问他们的网站数据集。所有这些数据都以结构化的形式存在。但是大部分网站都没有提供 API。我认为他们不希望我们使用他们的用户数据，或者他们因为缺乏知识而不提供这些数据。因此，在这个主题中，我们将从 web 导入数据，而不使用任何 API。但是在我们处理之前，请看看我们这个系列的<a class="ae lo" rel="noopener" target="_blank" href="/something-you-dont-know-about-data-file-if-you-just-a-starter-in-data-science-import-data-file-e2e007a154c4">部分 1 </a>，因为一切都像点一样连接在一起。</p><div class="lp lq gp gr lr ls"><a rel="noopener follow" target="_blank" href="/something-you-dont-know-about-data-file-if-you-just-a-starter-in-data-science-import-data-file-e2e007a154c4"><div class="lt ab fo"><div class="lu ab lv cl cj lw"><h2 class="bd iu gy z fp lx fr fs ly fu fw is bi translated">一些你不知道的数据文件如果你只是一个数据科学的初学者，导入数据文件…</h2><div class="lz l"><h3 class="bd b gy z fp lx fr fs ly fu fw dk translated">如果你是数据科学领域的新手，那么你必须努力学习数据科学概念。现在…</h3></div><div class="ma l"><p class="bd b dl z fp lx fr fs ly fu fw dk translated">towardsdatascience.com</p></div></div><div class="mb l"><div class="mc l md me mf mb mg kz ls"/></div></div></a></div><h1 id="2694" class="mh mi it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">什么是美丽的汤</h1><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nf"><img src="../Images/440334bfcaba902a63d0a3b421784035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L-uFn5fHWJ-qVtWdYXSiIg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Don’t write that awful page ( Source: crummy.com)</figcaption></figure><p id="7a29" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Beautiful Soup 是从特定网站或互联网上删除数据的最佳图书馆。这也是最舒适的工作。它从<strong class="js iu"> HTML </strong>中解析并提取结构化数据。Beautiful Soup 自动将传入文本转换为 Unicode，将传出版本转换为 UTF-8。除了文档没有定义编码和 Beautiful Soup 抓不到编码之外，你不需要记住编码。那就不得不提原始编码了。</p><blockquote class="ng nh ni"><p id="dccb" class="jq jr ko js b jt ju jv jw jx jy jz ka nj kc kd ke nk kg kh ki nl kk kl km kn im bi translated"><strong class="js iu">规则:</strong>要运行您的程序，请使用 Jupyter python 环境运行您的程序。而不是一次运行整个程序。我们只是采取预防措施，所以你的程序不会破坏网站。请检查网站的条款和条件，然后再开始从那里提取数据。请务必阅读关于合法使用数据的声明。</p></blockquote><h1 id="ace8" class="mh mi it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">基础-熟悉 HTML</h1><p id="7551" class="pw-post-body-paragraph jq jr it js b jt nm jv jw jx nn jz ka kb no kd ke kf np kh ki kj nq kl km kn im bi translated">HTML 代码在从网站提取数据的过程中起着重要的作用。所以，在我们处理之前，让我们跳到 HTML 标签的基础。如果你对 HTML 标签有一点点的了解，你就可以前进到下一个层次。</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="0210" class="nw mi it ns b gy nx ny l nz oa">&lt;!DOCTYPE html&gt;  <br/>&lt;html&gt;  <br/>    &lt;head&gt;<br/>    &lt;/head&gt;<br/>    &lt;body&gt;<br/>        &lt;h1&gt; Learning about Data&lt;/h1&gt;<br/>        &lt;p&gt; Beautiful Soup&lt;/p&gt;<br/>    &lt;body&gt;<br/>&lt;/html&gt;</span></pre><p id="22d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是 HTML 网页的基本语法。每个<strong class="js iu"> &lt;标签&gt; </strong>都服务于网页内部的一个区块:<br/> 1。<strong class="js iu">T46】！DOCTYPE html &gt; </strong> : HTML 文档必须以类型声明开头。<br/> 2。HTML 文档包含在<strong class="js iu"> &lt; html &gt; </strong>和<strong class="js iu"> &lt; /html &gt; </strong>之间。<br/> 3。HTML 文档的元和脚本声明在<strong class="js iu"> &lt;头&gt; </strong>和<strong class="js iu">&lt;/头&gt; </strong>之间。<br/> 4。HTML 文档的可见部分在<strong class="js iu"> &lt;正文&gt; </strong>和<strong class="js iu">&lt;/正文&gt; </strong>标签之间。<br/> 5。标题用<strong class="js iu"> &lt; h1 &gt; </strong>到<strong class="js iu"> &lt; h6 &gt; </strong>标签定义。6。用<strong class="js iu"> &lt; p &gt; </strong>标签定义段落。</p><p id="52ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其他有用的标签包括用于超链接的<strong class="js iu"> &lt; a &gt; </strong>，用于表格的<strong class="js iu"> &lt; table &gt; </strong>，用于表格行的<strong class="js iu"> &lt; tr &gt; </strong>，以及用于表格列的&lt; td &gt; 。</p><p id="33fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">让我们检查一下你的 HTML 页面</strong></p><div class="lp lq gp gr lr ls"><a href="https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab fo"><div class="lu ab lv cl cj lw"><h2 class="bd iu gy z fp lx fr fs ly fu fw is bi translated">亚洲国家地区列表-维基百科</h2><div class="lz l"><h3 class="bd b gy z fp lx fr fs ly fu fw dk translated">需要额外的引用来验证。通过增加对可靠来源的引用来改进这篇文章。无来源…</h3></div><div class="ma l"><p class="bd b dl z fp lx fr fs ly fu fw dk translated">en.wikipedia.org</p></div></div><div class="mb l"><div class="ob l md me mf mb mg kz ls"/></div></div></a></div><p id="6669" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们用一个<a class="ae lo" href="https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area" rel="noopener ugc nofollow" target="_blank">维基百科页面</a>来做报废。如果你有谷歌 chrome 浏览器，那么进入页面，首先右击它，打开浏览器检查器检查网页。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oc"><img src="../Images/915997e5545c6dbcea27bd535aae858f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bIM3hvebHgk-lN845qpA9g.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Inspect Wikipedia Page</figcaption></figure><p id="b3c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从结果中你可以看到这个表在 wiki 表中是可排序的，如果你仔细查看，你可以在那里找到你所有的表信息，太棒了！！！。看看你能用漂亮的汤做什么会更令人惊奇。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi od"><img src="../Images/b5c2936d9848729e5c3320b8ca260dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xFenHei3jvdtMnuOJ419ag.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Wikitable Sortanble</figcaption></figure><h1 id="fea0" class="mh mi it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">让我们开始你的 DIY 项目吧</h1><p id="519b" class="pw-post-body-paragraph jq jr it js b jt nm jv jw jx nn jz ka kb no kd ke kf np kh ki kj nq kl km kn im bi translated">现在我们知道了我们的数据及其位置。因此，我们将开始废弃我们的数据。</p><p id="0ee1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们处理之前，您需要安装或导入一些库。</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="72fe" class="nw mi it ns b gy nx ny l nz oa">#Import Libraries<br/><em class="ko">from </em><strong class="ns iu"><em class="ko">bs4</em></strong><em class="ko"> import </em><strong class="ns iu"><em class="ko">BeautifulSoup</em></strong><em class="ko"><br/>import </em><strong class="ns iu"><em class="ko">requests</em></strong><em class="ko"><br/></em></span></pre><p id="cbe7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你在安装中遇到任何麻烦，你可以在每一行前面使用<strong class="js iu"> sudo </strong>。</p><p id="b753" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">请求</strong> <br/>它本来是人类用来交流的语言。这意味着您不必手动将查询字符串连接到 URL，或者对您的帖子数据进行格式编码。请求将使您能够利用 Python 发送 HTTP/1.1 请求。在其中，您可以通过简单的 Python 库组合内容，如标题、表单数据、多部分文件和参数。它还使您能够以同样的方式获得 Python 的响应数据。</p><p id="c8a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">BS4—Beautiful Soup</strong><br/>Beautiful Soup 是一个 Python 库，用于从 HTML 和 XML 文件中提取数据。它与您最喜欢的解析器一起工作，产生操作、检查和转换解析树的自然方式。它通常会节省程序员几个小时或几天的工作。</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="d118" class="nw mi it ns b gy nx ny l nz oa"><em class="ko"># Specify with which URL/web page we are going to be scraping</em><br/>url = requests.get(‘<a class="ae lo" href="https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area</a><a class="ae lo" href="https://en.wikipedia.org/wiki/Premier_League).text" rel="noopener ugc nofollow" target="_blank">’).text</a></span></pre><p id="8993" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们首先研究给定网页的源代码，并使用 BeautifulSoup 函数构建一个 BeautifulSoup (soup)对象。现在，我们需要使用漂亮的 Soap 函数，它将帮助我们解析并应用从维基百科页面获取的 HTML:</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="eef6" class="nw mi it ns b gy nx ny l nz oa"><em class="ko"># import the BeautifulSoup library so we can parse HTML and XML documents</em></span><span id="c46b" class="nw mi it ns b gy oe ny l nz oa"><strong class="ns iu">from</strong> <!-- -->bs4 <strong class="ns iu">import</strong> <!-- -->BeautifulSoup</span></pre><p id="e07c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们将使用 Beautiful Soup 来解析我们在“URL”变量中收集的 HTML 数据，并分配一个不同的变量来以名为“Soup”的 Beautiful Soup 格式存储数据</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="f326" class="nw mi it ns b gy nx ny l nz oa"><em class="ko">#Parse the HTML from our URL into the BeautifulSoup parse tree format</em></span><span id="fbf6" class="nw mi it ns b gy oe ny l nz oa">soup <strong class="ns iu">=</strong> <!-- -->BeautifulSoup(url, "lxml")</span></pre><p id="2d62" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要了解我们的 web 页面中底层 HTML 的结构，请使用 Beautiful Soup 的 prettify 函数并检查它。</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="7e03" class="nw mi it ns b gy nx ny l nz oa"><em class="ko">#To look at the HTML underlying to the web </em><br/>print(soup.prettify())</span></pre><p id="1be7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我们从<strong class="js iu">pretify()</strong>函数中得到的结果:</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="4333" class="nw mi it ns b gy nx ny l nz oa">&lt;!DOCTYPE html&gt;<br/>&lt;html class="client-nojs" dir="ltr" lang="en"&gt;<br/> &lt;head&gt;<br/>  &lt;meta charset="utf-8"/&gt;<br/>  &lt;title&gt;<br/>   List of Asian countries by area - Wikipedia<br/>  &lt;/title&gt;<br/>  &lt;script&gt;</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi of"><img src="../Images/6dfc5cdad7d595fbba1871c14449bb34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ijzx2OD7zEcB2YChDMXcqg.png"/></div></div></figure><p id="57d4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您访问此<strong class="js iu"> </strong> <a class="ae lo" href="https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">链接</strong> </a>并查看我们的亚洲国家维基百科页面，我们可以看到关于国家地区的更多信息。维基百科表格已经设置好了，这使得我们的工作更加容易。</p><p id="ef74" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们在我们美化的 HTML 中寻找它:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi og"><img src="../Images/0c0ed62e47a4abd6216c593ad43a191c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TOo61og6q19bBWKAY4zIxQ.png"/></div></div></figure><p id="d695" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就这样，以一个 HTML <strong class="js iu"> &lt; table &gt; </strong>标签开始，标签的类标识符为“wikitable sortable”我们将记住这个类以备将来使用。如果你在你的程序中往下走，你会看到表格是如何组成的，你会看到开始和结束的行都有<strong class="js iu">&lt;&gt;</strong>和<strong class="js iu"> &lt; /tr &gt; </strong>标签。</p><p id="d905" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一行标题具有第<strong class="js iu"> &lt;个&gt; </strong>标签，而下面每个俱乐部的数据行具有<strong class="js iu"> &lt; td &gt; </strong>标签。使用<strong class="js iu"> &lt; td &gt; </strong>标签，我们将告诉 Python 保护我们的数据。</p><p id="0ad8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们继续之前，让我们设计一些漂亮的 Soup 函数来演示它如何从<strong class="js iu"> HTML </strong>网站获取数据并传递给我们。如果我们做标题函数，Beautiful Soup 将返回标题的 HTML 标签和其中的内容。</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="ba0f" class="nw mi it ns b gy nx ny l nz oa"><em class="ko">#To get the title of the page</em><br/>soup.title()</span></pre><p id="f543" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以利用这些信息开始准备对 HTML 的攻击。<br/>我们知道数据保存在一个<strong class="js iu"> HTML </strong>表中，所以首先，我们让 Beautiful Soup off 检索页面中所有出现的<strong class="js iu"> &lt; table &gt; </strong>标签，并将它们添加到一个名为 all_tables 的数组中。</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="fb10" class="nw mi it ns b gy nx ny l nz oa"><em class="ko"># use the 'find_all' function to bring back all instances of the 'table' tag in the HTML and store in 'all_tables' variable</em></span><span id="7479" class="nw mi it ns b gy oe ny l nz oa">all_tables<strong class="ns iu">=</strong>soup.find_all("table")</span><span id="c3cf" class="nw mi it ns b gy oe ny l nz oa">all_tables</span></pre><p id="d2fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 table class '<strong class="js iu">wiki table sortable</strong>下，我们有以国家名称为标题的链接。</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="cd02" class="nw mi it ns b gy nx ny l nz oa">#<em class="ko"> use the 'find_all' function to bring back all instances of the 'table' tag in the HTML and store in 'all_tables' variable</em></span><span id="11d1" class="nw mi it ns b gy oe ny l nz oa">My_table = soup.find('table',{'class':'wikitable sortable'})<br/>My_table</span></pre><p id="2d81" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 table class '<strong class="js iu">wiki table sortable</strong>下，我们有以国家名称作为标题的连接。现在，我们要提取<strong class="js iu"> &lt; a &gt; </strong>内的所有链接，我们使用<strong class="js iu"> find_all() </strong>。</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="7026" class="nw mi it ns b gy nx ny l nz oa">links = My_table.findAll('a')<br/>links</span></pre><p id="e3c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从 URL 中，我们必须提取标题，即国家的名称。为此，我们必须创建一个国家列表，以便我们可以从链接中提取国家的名称，并将其添加到国家列表中。</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="3904" class="nw mi it ns b gy nx ny l nz oa">Countries = []<br/><strong class="ns iu">for</strong> link <strong class="ns iu">in</strong> links:<br/>    Countries.append(link.get('title'))<br/>    <br/>print(Countries)</span></pre><p id="964a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们必须将列表国家转换成熊猫数据框架，以便在 python 中工作。</p><pre class="kq kr ks kt gt nr ns nt nu aw nv bi"><span id="04b5" class="nw mi it ns b gy nx ny l nz oa"><strong class="ns iu">import</strong> <strong class="ns iu">pandas</strong> <strong class="ns iu">as</strong> <strong class="ns iu">pd</strong><br/>df = pd.DataFrame()<br/>df['Country'] = Countries<br/><br/>df</span></pre><p id="9255" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你对大量废弃数据感兴趣，你应该考虑使用<strong class="js iu"> Scrapy </strong>，一个强大的 python 抓取框架，并尝试将你的代码与一些公共 API 集成。<br/>数据检索的性能明显高于抓取网页。例如，看看<strong class="js iu">脸书图形 API </strong>，它可以帮助你获得脸书网页上没有显示的隐藏数据。当你的信息变得太大时，考虑使用数据库后端，比如 MySQL T21。</p><p id="74d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是我们美丽的汤教程的结尾。自信地说，它让你开始为下一个项目检查一些刮掉的东西。我们已经引入了获取<strong class="js iu"> URL </strong>和<strong class="js iu"> HTML </strong>数据的请求，解析<strong class="js iu"> HTML </strong>和<strong class="js iu"> Pandas </strong>的 Beautiful Soup 将数据转换成数据帧以便正确显示。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d4f8255ade65c51b2b0dcad5320d4c0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*oBMxiCgk9P23OCoWljf3Hw.gif"/></div></figure><p id="71db" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在这里找到这本<a class="ae lo" href="https://github.com/sahildhankhad/Web-Scrapping" rel="noopener ugc nofollow" target="_blank">教程笔记本</a>。如果你有问题，请随时提问。在下一个教程中，我们将讨论 API。欢迎随时在<a class="ae lo" href="http://www.linkedin.com/in/sahil-dhankhad-303350135" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p><p id="880b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参考文献:1。<a class="ae lo" href="http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/" rel="noopener ugc nofollow" target="_blank">http://www . Greg reda . com/2013/03/03/we B- scraping-101-with-python/</a>。</p><p id="c9bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae lo" href="http://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/" rel="noopener ugc nofollow" target="_blank"> 2。http://www . analyticsvidhya . com/blog/2015/10/初学者-指南-web-刮-美-汤-python/ </a>。</p><p id="dbcc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.<a class="ae lo" href="https://github.com/stewync/Web-Scraping-Wiki-tables-using-BeautifulSoup-and-Python/blob/master/Scraping%2BWiki%2Btable%2Busing%2BPython%2Band%2BBeautifulSoup.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/stewync/we B- Scraping-Wiki-tables-using-beautiful soup-and-Python/blob/master/Scraping % 2b Wiki % 2b table % 2b busing % 2b Python % 2b band % 2b eautiful soup . ipynb</a></p><p id="2092" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.<a class="ae lo" href="https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/List _ of _ Asian _ countries _ by _ area</a></p><p id="da77" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">5.【https://www.crummy.com/software/BeautifulSoup/ T4】</p></div></div>    
</body>
</html>