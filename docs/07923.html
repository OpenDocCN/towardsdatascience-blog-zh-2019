<html>
<head>
<title>Let’s Underfit and Overfit a Machine Learning Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们对机器学习模型进行欠适应和过适应</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lets-underfit-and-overfit-a-machine-learning-model-26e1aebca233?source=collection_archive---------23-----------------------#2019-11-01">https://towardsdatascience.com/lets-underfit-and-overfit-a-machine-learning-model-26e1aebca233?source=collection_archive---------23-----------------------#2019-11-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a424" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">构建过度和不足的模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6a062f3f922d5a3cf77ffadc86509d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRireZz1XjK6tWsr22gCFQ.png"/></div></div></figure><p id="959a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一位同事最近开始使用术语“欠拟合”来指代命名实体识别(NER)模型，该模型遗漏了它应该标记的实体。</p><p id="79a6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我必须澄清事实。这实际上并不是不合身，但我可以理解有人会有这种印象。</p><p id="b178" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么什么是适配不足，或者适配过度呢？</p><p id="6db6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们训练一些低估和高估数据的模型！</p><p id="c42a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们先用 sklearn 的“make_classification”函数生成一个数据集。每个数据点将有 2 个特征(所以很容易绘制)和一个标签。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="4022" class="lv lw it lr b gy lx ly l lz ma">from sklearn.datasets import make_classification</span><span id="026a" class="lv lw it lr b gy mb ly l lz ma"># We didn't need to display all params but I like to see defaults<br/># I've edited some of these<br/>X,y = make_classification(<br/>    n_samples=30, <br/>    n_features=2, <br/>    n_informative=2,<br/>    n_redundant=0,<br/>    n_repeated=0, <br/>    n_classes=2, <br/>    n_clusters_per_class=2, <br/>    weights=None, <br/>    flip_y=0.01, <br/>    class_sep=1.0, <br/>    hypercube=True, <br/>    shift=0.0, <br/>    scale=1.0, <br/>    shuffle=True, <br/>    random_state=None<br/>)</span><span id="4b3d" class="lv lw it lr b gy mb ly l lz ma"># Split examples by class (positive/negative) to give diff colors<br/>pos_feat0 = []<br/>pos_feat1 = []<br/>neg_feat0 = []<br/>neg_feat1 = []</span><span id="a735" class="lv lw it lr b gy mb ly l lz ma">for idx,klass in enumerate(y):<br/>    if klass == 1:<br/>        pos_feat0.append(X[idx][0])<br/>        pos_feat1.append(X[idx][1])<br/>    else:<br/>        neg_feat0.append(X[idx][0])<br/>        neg_feat1.append(X[idx][1])</span><span id="4d12" class="lv lw it lr b gy mb ly l lz ma"># And plot them<br/>import matplotlib.pyplot as plt<br/>plt.scatter(pos_feat0,pos_feat1, c='blue')<br/>plt.scatter(neg_feat0,neg_feat1, c='red')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/a0e8d4f0533229be74ece2fb72a3ee80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1e4kln5Vk8PytpO0sZ6IVQ.png"/></div></div></figure><p id="af00" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">嘣。我们有数据。</p><p id="a2a5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将浏览欠拟合和过拟合的定义，然后有意识地选择将欠拟合和过拟合数据的算法。</p><h1 id="9c81" class="md lw it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">欠拟合</h1><p id="4631" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">根据<a class="ae mz" href="http://Underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data." rel="noopener ugc nofollow" target="_blank">维基百科</a>。</p><blockquote class="na nb nc"><p id="5a04" class="ku kv nd kw b kx ky ju kz la lb jx lc ne le lf lg nf li lj lk ng lm ln lo lp im bi translated"><strong class="kw iu">当统计模型无法充分捕捉数据的底层结构时，就会出现</strong>欠拟合。</p></blockquote><p id="c767" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">翻译:</strong>模型在数据中找不到可靠的模式。这并不意味着没有模式。只是模特找不到。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="34b8" class="lv lw it lr b gy lx ly l lz ma">from sklearn.linear_model import SGDClassifier</span><span id="157c" class="lv lw it lr b gy mb ly l lz ma">model = SGDClassifier()<br/>model.fit(X, y)</span><span id="00bd" class="lv lw it lr b gy mb ly l lz ma"># set min and max values for the x and y axes<br/>x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1<br/>y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1<br/>a = np.arange(x_min, x_max, 0.1)<br/>b = np.arange(y_min, y_max, 0.1)</span><span id="9091" class="lv lw it lr b gy mb ly l lz ma"># build a grid of each unique combination of x and y<br/>xx, yy = np.meshgrid(a, b)</span><span id="ce40" class="lv lw it lr b gy mb ly l lz ma"># make predictions for every combination of x and y on that grid<br/>Z = model.predict(np.c_[xx.ravel(), yy.ravel()])<br/>Z = Z.reshape(xx.shape)</span><span id="da06" class="lv lw it lr b gy mb ly l lz ma"># draw the classification boundary<br/>plt.contourf(xx, yy, Z, alpha=0.4)</span><span id="088e" class="lv lw it lr b gy mb ly l lz ma"># adds the points from our training data<br/>plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')</span><span id="84a6" class="lv lw it lr b gy mb ly l lz ma">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/f557a69e2ab64d80a9c1a307444f7fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dkNiodFjcZ3kX6zhyqykQw.png"/></div></div></figure><p id="d373" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">完美。该模型在绘制决策边界方面做得很糟糕。它不能使用特征来确定一个例子的类别。吃不饱！</p><h1 id="3e00" class="md lw it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">过度拟合</h1><p id="90d4" class="pw-post-body-paragraph ku kv it kw b kx mu ju kz la mv jx lc ld mw lf lg lh mx lj lk ll my ln lo lp im bi translated">据<a class="ae mz" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</p><blockquote class="na nb nc"><p id="21e6" class="ku kv nd kw b kx ky ju kz la lb jx lc ne le lf lg nf li lj lk ng lm ln lo lp im bi translated">产生的分析与一组特定的数据过于接近或精确，因此可能无法拟合额外的数据或可靠地预测未来的观察结果</p></blockquote><p id="6be8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">翻译:模型学习输入的例子，但它不能推广到其他例子。</p><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="0bee" class="lv lw it lr b gy lx ly l lz ma">from sklearn.tree import DecisionTreeClassifier<br/>model = DecisionTreeClassifier(max_depth=4)</span><span id="6de6" class="lv lw it lr b gy mb ly l lz ma">model.fit(X, y)</span><span id="234a" class="lv lw it lr b gy mb ly l lz ma"># set min and max values for the x and y axes<br/>x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1<br/>y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1<br/>a = np.arange(x_min, x_max, 0.1)<br/>b = np.arange(y_min, y_max, 0.1)</span><span id="7dc3" class="lv lw it lr b gy mb ly l lz ma"># build a grid of each unique combination of x and y<br/>xx, yy = np.meshgrid(a, b)</span><span id="61ab" class="lv lw it lr b gy mb ly l lz ma"># make predictions for every combination of x and y on that grid<br/>Z = model.predict(np.c_[xx.ravel(), yy.ravel()])<br/>Z = Z.reshape(xx.shape)</span><span id="c30a" class="lv lw it lr b gy mb ly l lz ma"># draw the classification boundary<br/>plt.contourf(xx, yy, Z, alpha=0.4)</span><span id="a8a3" class="lv lw it lr b gy mb ly l lz ma"># adds the points in our training data<br/>plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')</span><span id="5997" class="lv lw it lr b gy mb ly l lz ma">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6a062f3f922d5a3cf77ffadc86509d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRireZz1XjK6tWsr22gCFQ.png"/></div></div></figure><p id="70cb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">太美了。又一个可怕的模型。它在应该出现的例子周围画出了界限，但是它发现的模式毫无意义，并且可能无法推断出新的例子。</p><h1 id="28f3" class="md lw it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">让我们现在拟合数据，只是为了好玩</h1><pre class="kj kk kl km gt lq lr ls lt aw lu bi"><span id="b0df" class="lv lw it lr b gy lx ly l lz ma">from sklearn.linear_model import LinearRegression,LogisticRegression<br/>model = LogisticRegression()</span><span id="3af8" class="lv lw it lr b gy mb ly l lz ma">model.fit(X, y)</span><span id="cf3a" class="lv lw it lr b gy mb ly l lz ma"># set min and max values for the x and y axes<br/>x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1<br/>y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1<br/>a = np.arange(x_min, x_max, 0.1)<br/>b = np.arange(y_min, y_max, 0.1)</span><span id="32ff" class="lv lw it lr b gy mb ly l lz ma"># build a grid of each unique combination of x and y<br/>xx, yy = np.meshgrid(a, b)</span><span id="2f6f" class="lv lw it lr b gy mb ly l lz ma"># make predictions for every combination of x and y on that grid<br/>Z = model.predict(np.c_[xx.ravel(), yy.ravel()])<br/>Z = Z.reshape(xx.shape)</span><span id="d829" class="lv lw it lr b gy mb ly l lz ma"># draw the classification boundary<br/>plt.contourf(xx, yy, Z, alpha=0.4)</span><span id="8134" class="lv lw it lr b gy mb ly l lz ma"># adds the points in our training data<br/>plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')</span><span id="a11c" class="lv lw it lr b gy mb ly l lz ma">plt.title('Underfitting')</span><span id="6e41" class="lv lw it lr b gy mb ly l lz ma">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/c1307e541aa070cff7b30178920889fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iTRAK3bspJMyTlJQW9oW8g.png"/></div></div></figure><p id="813b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">太好了。不完美。但是比之前的 2 好多了。</p><p id="6061" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是了。欠拟合、过拟合和计划拟合。</p><p id="5bdd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们有意选择了一个简单的 2 要素数据集，以便您可以在图表上看到决策边界。</p><p id="7010" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">具有数千个特征的真实例子需要一种更数值的方法来测量欠拟合和过拟合。但是我们会留到下一天。</p></div></div>    
</body>
</html>