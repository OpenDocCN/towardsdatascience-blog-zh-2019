<html>
<head>
<title>A Minimalist End-to-End Scrapy Tutorial (Part I)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个极简的端到端的零碎教程(第一部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-minimalist-end-to-end-scrapy-tutorial-part-i-11e350bcdec0?source=collection_archive---------2-----------------------#2019-09-07">https://towardsdatascience.com/a-minimalist-end-to-end-scrapy-tutorial-part-i-11e350bcdec0?source=collection_archive---------2-----------------------#2019-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e82e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">面向初学者的系统化网页抓取</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/93272f161a09104d13e23d6f0e239104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2OeoRtfHganECeFf"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@pawel_czerwinski?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Paweł Czerwiński</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5aa9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-i-11e350bcdec0?source=friends_link&amp;sk=c9f8e32f28a88c61987ec60f93b93e6d" rel="noopener">第一部分</a>、<a class="ae ky" rel="noopener" target="_blank" href="/a-minimalist-end-to-end-scrapy-tutorial-part-ii-b917509b73f7?source=friends_link&amp;sk=ebd3a9cee8b2097b3857194fee3821a6">第二部分</a>、<a class="ae ky" rel="noopener" target="_blank" href="/a-minimalist-end-to-end-scrapy-tutorial-part-iii-bcd94a2e8bf3?source=friends_link&amp;sk=a1fdde9c9dd5383d8de2e08395ee3f98">第三部分</a>、<a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-iv-3290d76a2aef?sk=6f0902f9a15092575814ab533a56f8ef" rel="noopener">第四部分</a>、<a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-v-e7743ee9a8ef?source=friends_link&amp;sk=c1c5110f63c7ccbe4eb8c6209ee2f57c" rel="noopener">第五部分</a></p><p id="29fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网络抓取是数据科学家的一项重要技能。在过去的几年里，我使用 Python、BeautifulSoup 和 Scrapy 开发了许多专门的 web 抓取项目，并阅读了一些书籍和大量在线教程。然而，我还没有找到一个简单的初学者水平的教程，它是端到端的，涵盖了一个典型的零碎的 web 抓取项目中的所有基本步骤和概念(因此标题中的<em class="lv">极简主义者</em>)——这就是为什么我写这篇文章，并希望代码回购可以作为一个模板，帮助启动您的 web 抓取项目。</p><p id="be64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很多人问:我应该用 BeautifulSoup 还是 Scrapy？它们是不同的东西:BeautifulSoup 是一个用于解析 HTML 和 XML 的库，Scrapy 是一个 web 抓取框架。如果你愿意，你可以使用 BeautifulSoup 而不是 Scrapy 内置选择器，但是将 BeautifulSoup 与 Scrapy 进行比较就像将 Mac 键盘与 iMac 进行比较，或者更好的比喻，如官方文档中所述“就像将 jinja2 与 Django 进行比较”，如果你知道它们是什么的话:)—简而言之，如果你想进行严肃而系统的 web 抓取，你应该学习 Scrapy。</p><p id="db94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TL；博士，给我看看代码:</p><div class="lw lx gp gr ly lz"><a href="https://github.com/harrywang/scrapy-tutorial" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">哈利旺/剪贴簿-教程</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">这个报告包含了我的教程的代码:一个极简的端到端的零碎教程(…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">github.com</p></div></div></div></a></div><p id="be41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本系列教程中，我将讲述以下步骤:</p><ol class=""><li id="191a" class="mi mj it lb b lc ld lf lg li mk lm ml lq mm lu mn mo mp mq bi translated">(本教程)从头开始一个 Scrapy 项目，开发一个简单的蜘蛛。一件重要的事情是使用 Scrapy Shell 来分析页面和调试，这是你应该使用 Scrapy 而不是 BeautifulSoup 的主要原因之一。</li><li id="8f32" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">(<a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-ii-b917509b73f7?sk=ebd3a9cee8b2097b3857194fee3821a6" rel="noopener">第二部分</a>)介绍 Item 和 ItemLoader，并解释为什么要使用它们(尽管它们让你的代码一开始看起来更复杂)。</li><li id="d959" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">(<a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-iii-bcd94a2e8bf3?sk=a1fdde9c9dd5383d8de2e08395ee3f98" rel="noopener">第三部分</a>)通过管道使用<a class="ae ky" href="https://blog.bitsrc.io/what-is-an-orm-and-why-you-should-use-it-b2b6f75f5e2a" rel="noopener ugc nofollow" target="_blank"> ORM </a> (SQLAlchemy)将数据存储到数据库，并展示如何建立最常见的一对多和多对多关系。</li><li id="7f60" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">(<a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-iv-3290d76a2aef?sk=6f0902f9a15092575814ab533a56f8ef" rel="noopener">第四部分</a>)将项目部署到 Scrapinghub(你必须为预定的抓取作业等服务付费)或者通过使用伟大的开源项目<a class="ae ky" href="https://github.com/my8100/scrapydweb" rel="noopener ugc nofollow" target="_blank"> ScrapydWeb </a>和 Heroku 完全免费地建立自己的服务器。</li><li id="ae9e" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">(<a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-v-e7743ee9a8ef?source=friends_link&amp;sk=c1c5110f63c7ccbe4eb8c6209ee2f57c" rel="noopener"> Part V </a>)我创建了一个单独的 repo ( <a class="ae ky" href="https://github.com/harrywang/scrapy-selenium-demo" rel="noopener ugc nofollow" target="_blank"> Scrapy + Selenium </a>)来展示如何抓取动态网页(比如通过滚动加载附加内容的页面)以及如何使用代理网络(ProxyMesh)来避免被禁止。</li></ol><p id="ffbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些先决条件:</p><ul class=""><li id="ddd4" class="mi mj it lb b lc ld lf lg li mk lm ml lq mm lu mw mo mp mq bi translated">Python(本教程 Python 3)、虚拟环境、自制等基础知识。，参见我的另一篇文章，了解如何设置环境:<a class="ae ky" href="https://link.medium.com/tw6Ylq9wjZ" rel="noopener">如何为 Python 开发设置 Mac</a></li><li id="7059" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mw mo mp mq bi translated">Git 和 Github 的基础知识。我推荐<a class="ae ky" href="https://git-scm.com/book/en/v2" rel="noopener ugc nofollow" target="_blank"> Pro Git 的书</a>。</li><li id="9814" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mw mo mp mq bi translated">数据库和 ORM 的基础知识，如<a class="ae ky" href="https://www.coursera.org/learn/intro-sql" rel="noopener ugc nofollow" target="_blank">结构化查询语言(SQL)介绍</a>。</li></ul><p id="8015" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧！</p><p id="dbaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先创建一个新文件夹，在文件夹里面设置 Python 3 虚拟环境，安装 Scrapy。为了使这一步变得容易，我创建了一个<a class="ae ky" href="https://github.com/harrywang/scrapy-tutorial-starter" rel="noopener ugc nofollow" target="_blank"> starter repo </a>，你可以派生和克隆它(如果需要，请参见<a class="ae ky" href="https://docs.python.org/3/tutorial/venv.html" rel="noopener ugc nofollow" target="_blank"> Python3 虚拟环境文档</a>):</p><pre class="kj kk kl km gt mx my mz na aw nb bi"><span id="e917" class="nc nd it my b gy ne nf l ng nh">$ git clone <a class="ae ky" href="https://github.com/yourusername/scrapy-tutorial-starter.git" rel="noopener ugc nofollow" target="_blank">https://github.com/yourusername/scrapy-tutorial-starter.git</a><br/>$ cd scrapy-tutorial-starter<br/>$ python3.6 -m venv venv<br/>$ source venv/bin/activate<br/>$ pip install -r requirements.txt</span></pre><p id="6488" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您的文件夹应该如下所示，我假设我们总是在虚拟环境中工作。注意到目前为止我们在 requirements.txt 中只有一个包。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f4c127a60b5aa28de09c4ee25d272c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dN15tIZ0LeISCUYbMTAXzA.png"/></div></figure><p id="c283" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行<code class="fe nj nk nl my b">scrapy startproject tutorial</code>创建一个空的 scrapy 项目，你的文件夹看起来像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/5f1402b9ec65e1ccc10d0a998ee3e42d.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*4sdDP1t3okK6p5p_94VtyA.png"/></div></figure><p id="e01c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建了两个相同的“教程”文件夹。我们不需要第一级“教程”文件夹—删除它，并将第二级“教程”文件夹及其内容向上移动一级—我知道这很混乱，但这就是你对文件夹结构所做的一切。现在，您的文件夹应该看起来像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/52ed65175be8de97561d294164f75ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*K61KJLWgTBSfH0zWDA9dPQ.png"/></div></figure><p id="5f17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，不要担心自动生成的文件，我们稍后将回到这些文件。本教程基于官方<a class="ae ky" href="https://docs.scrapy.org/en/latest/intro/tutorial.html" rel="noopener ugc nofollow" target="_blank"> Scrapy 教程</a>。所以我们要爬的网站是<a class="ae ky" href="http://quotes.toscrape.com" rel="noopener ugc nofollow" target="_blank">http://quotes.toscrape.com</a>，很简单:有几页引用作者和标签:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/62d3d29cde47b350311be23ebffcc33b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXkxnsl61RuOIP5_qGE0eA.png"/></div></div></figure><p id="ace0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您单击作者时，它会转到作者详细信息页面，包括姓名、生日和简历。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/d4d311690ff1affca1ab9f80418a3f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WIBE22Qu-wJaxkPabYUfoA.png"/></div></div></figure><p id="d52d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，在“spider”文件夹中创建一个名为“quotes-spider.py”的新文件，其内容如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/3c6c3b9fb01d8d323f6a5a0c0147c1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LYk0X4YcY3kaRHp37Kr7OA.png"/></div></div></figure><p id="1868" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您刚刚创建了一个名为“quotes”的蜘蛛，它向<a class="ae ky" href="http://quotes.toscrape.com" rel="noopener ugc nofollow" target="_blank">http://quotes.toscrape.com</a>发送请求，并从服务器获得响应。然而，到目前为止，蜘蛛在解析响应时不做任何事情，只是向控制台输出一个字符串。让我们运行这个蜘蛛:<code class="fe nj nk nl my b">scrapy crawl quotes</code>，您应该会看到如下输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/e161a19a9d2e9be50e55e7696fb8e8c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZVnS5mYVm89DXvaHQDPJQ.png"/></div></div></figure><p id="d349" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们使用 Scrapy Shell 通过运行以下命令来分析响应，即位于<a class="ae ky" href="http://quotes.toscrape.com" rel="noopener ugc nofollow" target="_blank">http://quotes.toscrape.com</a>的 HTML 页面:</p><pre class="kj kk kl km gt mx my mz na aw nb bi"><span id="574e" class="nc nd it my b gy ne nf l ng nh">$ scrapy shell http://quotes.toscrape.com/</span><span id="3b7c" class="nc nd it my b gy ns nf l ng nh">...</span><span id="173f" class="nc nd it my b gy ns nf l ng nh">2019-08-21 20:10:40 [scrapy.core.engine] INFO: Spider opened</span><span id="e8e5" class="nc nd it my b gy ns nf l ng nh">2019-08-21 20:10:41 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http://quotes.toscrape.com/robots.txt&gt; (referer: None)</span><span id="d68e" class="nc nd it my b gy ns nf l ng nh">2019-08-21 20:10:41 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/&gt; (referer: None)</span><span id="8152" class="nc nd it my b gy ns nf l ng nh">[s] Available Scrapy objects:</span><span id="66e6" class="nc nd it my b gy ns nf l ng nh">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</span><span id="da9d" class="nc nd it my b gy ns nf l ng nh">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x105d01dd8&gt;</span><span id="84ca" class="nc nd it my b gy ns nf l ng nh">[s]   item       {}</span><span id="fa71" class="nc nd it my b gy ns nf l ng nh">[s]   request    &lt;GET http://quotes.toscrape.com/&gt;</span><span id="5126" class="nc nd it my b gy ns nf l ng nh">[s]   response   &lt;200 http://quotes.toscrape.com/&gt;</span><span id="ff1d" class="nc nd it my b gy ns nf l ng nh">[s]   settings   &lt;scrapy.settings.Settings object at 0x106ae34e0&gt;</span><span id="b5f6" class="nc nd it my b gy ns nf l ng nh">[s]   spider     &lt;DefaultSpider 'default' at 0x106f13780&gt;</span><span id="2016" class="nc nd it my b gy ns nf l ng nh">[s] Useful shortcuts:</span><span id="df59" class="nc nd it my b gy ns nf l ng nh">[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)</span><span id="8c88" class="nc nd it my b gy ns nf l ng nh">[s]   fetch(req)                  Fetch a scrapy.Request and update local objects</span><span id="935c" class="nc nd it my b gy ns nf l ng nh">[s]   shelp()           Shell help (print this help)</span><span id="4bd5" class="nc nd it my b gy ns nf l ng nh">[s]   view(response)    View response in a browser</span><span id="e937" class="nc nd it my b gy ns nf l ng nh">&gt;&gt;&gt;</span></pre><p id="05b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用<a class="ae ky" href="https://www.w3schools.com/xml/xpath_syntax.asp" rel="noopener ugc nofollow" target="_blank"> Xpath 选择器</a>或<a class="ae ky" href="https://www.w3schools.com/cssref/css_selectors.asp" rel="noopener ugc nofollow" target="_blank"> CSS 选择器</a>和<a class="ae ky" href="https://developers.google.com/web/tools/chrome-devtools/css/" rel="noopener ugc nofollow" target="_blank"> Chrome DevTools </a>来选择元素，它们通常用于分析页面(我们不会涉及选择器细节，请阅读文档以了解如何使用它们):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/83a1d5ad3f5e4cd13a45e875692fcfe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idTIgtpUhMtbeLErYRAvFg.png"/></div></div></figure><p id="aadb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，您可以测试选择器并在 Scrapy Shell 中查看结果——假设我们想要获得上面显示的报价块:</p><p id="744a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以使用 Xpath <code class="fe nj nk nl my b">response.xpath(“//div[@class=’quote’]”).get()</code> ( <code class="fe nj nk nl my b">.get()</code>显示第一个选中的元素，使用<code class="fe nj nk nl my b">.getall()</code>显示全部)或者 CSS <code class="fe nj nk nl my b">response.css(“div .quote”).get()</code>。我用粗体显示了我们希望从这个报价块中获得的报价文本、作者和标签:</p><pre class="kj kk kl km gt mx my mz na aw nb bi"><span id="e864" class="nc nd it my b gy ne nf l ng nh">&gt;&gt;&gt; response.xpath("//div[@class='quote']").get()</span><span id="762c" class="nc nd it my b gy ns nf l ng nh">'&lt;div class="quote" itemscope itemtype="http://schema.org/CreativeWork"&gt;\n        &lt;span class="text" itemprop="text"&gt;<strong class="my iu">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</strong>&lt;/span&gt;\n        &lt;span&gt;by &lt;small class="author" itemprop="author"&gt;<strong class="my iu">Albert Einstein</strong>&lt;/small&gt;\n        &lt;a href="/author/Albert-Einstein"&gt;(about)&lt;/a&gt;\n        &lt;/span&gt;\n        &lt;div class="tags"&gt;\n            Tags:\n            &lt;meta class="keywords" itemprop="keywords" content="change,deep-thoughts,thinking,world"&gt; \n            \n            &lt;a class="tag" href="/tag/change/page/1/"&gt;<strong class="my iu">change</strong>&lt;/a&gt;\n            \n            &lt;a class="tag" href="/tag/deep-thoughts/page/1/"&gt;<strong class="my iu">deep-thoughts</strong>&lt;/a&gt;\n            \n            &lt;a class="tag" href="/tag/thinking/page/1/"&gt;<strong class="my iu">thinking</strong>&lt;/a&gt;\n            \n            &lt;a class="tag" href="/tag/world/page/1/"&gt;<strong class="my iu">world</strong>&lt;/a&gt;\n            \n        &lt;/div&gt;\n    &lt;/div&gt;'</span></pre><p id="11a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以继续在 shell 中获取如下数据:</p><ul class=""><li id="e0fb" class="mi mj it lb b lc ld lf lg li mk lm ml lq mm lu mw mo mp mq bi translated">将所有报价块转换成“报价”</li><li id="3035" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mw mo mp mq bi translated">使用“quotes”中的第一个引号:quotes[0]</li><li id="b895" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mw mo mp mq bi translated">尝试 css 选择器</li></ul><pre class="kj kk kl km gt mx my mz na aw nb bi"><span id="2099" class="nc nd it my b gy ne nf l ng nh">&gt;&gt;&gt; quotes = response.xpath("//div[@class='quote']")<br/>&gt;&gt;&gt; quotes[0].css(".text::text").getall()</span><span id="6758" class="nc nd it my b gy ns nf l ng nh">['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”']</span><span id="c7e6" class="nc nd it my b gy ns nf l ng nh">&gt;&gt;&gt; quotes[0].css(".author::text").getall()</span><span id="2816" class="nc nd it my b gy ns nf l ng nh">['Albert Einstein']</span><span id="50a3" class="nc nd it my b gy ns nf l ng nh">&gt;&gt;&gt; quotes[0].css(".tag::text").getall()</span><span id="59bf" class="nc nd it my b gy ns nf l ng nh">['change', 'deep-thoughts', 'thinking', 'world']</span></pre><p id="3a7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来上面显示的选择器得到了我们需要的东西。注意，出于演示目的，我在这里混合了 Xpath 和 CSS 选择器——在本教程中不需要两者都用。</p><p id="0add" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们修改蜘蛛文件并使用关键字<code class="fe nj nk nl my b">yield</code>将选择的数据输出到控制台(注意，每个页面都有许多引用，我们使用一个循环来遍历所有引用):</p><pre class="kj kk kl km gt mx my mz na aw nb bi"><span id="6052" class="nc nd it my b gy ne nf l ng nh">import scrapy</span><span id="e253" class="nc nd it my b gy ns nf l ng nh">class QuotesSpider(scrapy.Spider):<br/>    name = "quotes"</span><span id="8c9f" class="nc nd it my b gy ns nf l ng nh">start_urls = ['<a class="ae ky" href="http://quotes.toscrape.com'" rel="noopener ugc nofollow" target="_blank">http://quotes.toscrape.com'</a>]</span><span id="62a5" class="nc nd it my b gy ns nf l ng nh">def parse(self, response):<br/>        self.logger.info('hello this is my first spider')<br/>        quotes = response.css('div.quote')<br/>        for quote in quotes:<br/>            yield {<br/>                'text': quote.css('.text::text').get(),<br/>                'author': quote.css('.author::text').get(),<br/>                'tags': quote.css('.tag::text').getall(),<br/>            }</span></pre><p id="e517" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">再次运行蜘蛛:<code class="fe nj nk nl my b">scrapy crawl quotes</code>可以在日志中看到提取的数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/bd78e42348b2718740698e59e7b6c8c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lpqu_VWTgiNFYpXip_CfWA.png"/></div></div></figure><p id="7ab6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以通过运行<code class="fe nj nk nl my b">scrapy crawl quotes -o quotes.json</code>将数据保存在一个 JSON 文件中</p><p id="fa77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们从第一页获得所有报价信息，我们的下一个任务是抓取所有页面。您应该注意到首页底部有一个“下一页”按钮用于页面导航，逻辑是:单击“下一页”按钮进入下一页，获取报价，再次单击“下一页”直到没有“下一页”按钮的最后一页。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/8388ebce55a038ccfa19b95cce28c369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oimG6llPwkQjgELJcSV6qg.png"/></div></div></figure><p id="3367" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过 Chrome DevTools，我们可以获得下一页的 URL:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/a6a657c6e3dbc9bf5bc440d5c40cb444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IuinM1PhEOtuDrO1unb7cA.png"/></div></div></figure><p id="2474" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们通过再次运行<code class="fe nj nk nl my b">scrapy shell <a class="ae ky" href="http://quotes.toscrape.com/" rel="noopener ugc nofollow" target="_blank">http://quotes.toscrape.com/</a></code>在 Scrapy Shell 中进行测试:</p><pre class="kj kk kl km gt mx my mz na aw nb bi"><span id="17c9" class="nc nd it my b gy ne nf l ng nh">$ scrapy shell <a class="ae ky" href="http://quotes.toscrape.com/" rel="noopener ugc nofollow" target="_blank">http://quotes.toscrape.com/</a><br/>...</span><span id="6763" class="nc nd it my b gy ns nf l ng nh">&gt;&gt;&gt; response.css('li.next a::attr(href)').get()</span><span id="f123" class="nc nd it my b gy ns nf l ng nh">'/page/2/'</span></pre><p id="47ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以编写以下代码，让蜘蛛浏览所有页面以获取所有报价:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="9e2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nj nk nl my b">next_page = response.urljoin(next_page)</code>获取完整的 URL，<code class="fe nj nk nl my b">yield scrapy.Request(next_page, callback=self.parse)</code>发送一个新的请求来获取下一个页面，并使用回调函数调用相同的解析函数来获取新页面的报价。</p><p id="a95b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以使用快捷键进一步简化上面的代码:<a class="ae ky" href="https://docs.scrapy.org/en/latest/intro/tutorial.html#a-shortcut-for-creating-requests" rel="noopener ugc nofollow" target="_blank">参见本节</a>。本质上，<code class="fe nj nk nl my b">response.follow</code>支持相对 URL(不需要调用<code class="fe nj nk nl my b">urljoin</code>)，并自动为<code class="fe nj nk nl my b">&lt;a&gt;</code>使用<code class="fe nj nk nl my b">href</code>属性。因此，代码可以进一步缩短:</p><pre class="kj kk kl km gt mx my mz na aw nb bi"><span id="c6a6" class="nc nd it my b gy ne nf l ng nh">for a in response.css('li.next a'):<br/>            yield response.follow(a, callback=self.parse)</span></pre><p id="c239" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，再次运行蜘蛛程序<code class="fe nj nk nl my b">scrapy crawl quotes</code>,你应该会看到所有 10 页的引文都被提取出来了。坚持住——我们就要完成第一部分了。下一个任务是抓取单个作者的页面。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/a63e843d269f0fd7af518e6215069cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Oh1JM4hPUdVaZXRFGId3Q.png"/></div></div></figure><p id="c6c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所示，当我们处理每个报价时，我们可以通过跟随突出显示的链接进入单个作者的页面—让我们使用 Scrapy Shell 来获得链接:</p><pre class="kj kk kl km gt mx my mz na aw nb bi"><span id="fc22" class="nc nd it my b gy ne nf l ng nh">$ scrapy shell <a class="ae ky" href="http://quotes.toscrape.com/" rel="noopener ugc nofollow" target="_blank">http://quotes.toscrape.com/</a><br/>...<br/>&gt;&gt;&gt; response.css('.author + a::attr(href)').get()</span><span id="8f3e" class="nc nd it my b gy ns nf l ng nh">'/author/Albert-Einstein'</span></pre><p id="ab1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在提取每个引文的循环过程中，我们发出另一个请求，以转到相应作者的页面，并创建另一个<code class="fe nj nk nl my b">parse_author</code>函数来提取作者的姓名、生日、出生地点和简历，并输出到控制台。更新后的蜘蛛如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="663e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">再次运行 spider<code class="fe nj nk nl my b">scrapy crawl quotes</code>并再次检查您需要提取的所有内容是否正确输出到控制台。注意 Scrapy 基于<a class="ae ky" href="https://twistedmatrix.com/trac/" rel="noopener ugc nofollow" target="_blank"> Twisted </a>，这是一个流行的 Python 事件驱动网络框架，因此是异步的。这意味着单独的作者页面可能不会与相应的报价同步处理，例如，作者页面结果的顺序可能与页面上的报价顺序不匹配。我们将在后面的部分讨论如何将报价与其对应的作者页面链接起来。</p><p id="4e02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">恭喜你，你已经完成了本教程的第一部分。</p><p id="6ec4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-ii-b917509b73f7?sk=ebd3a9cee8b2097b3857194fee3821a6" rel="noopener">第二部分</a>中了解更多关于物品和物品装载器的信息。</p><p id="f030" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-i-11e350bcdec0?source=friends_link&amp;sk=c9f8e32f28a88c61987ec60f93b93e6d" rel="noopener">第一部分</a>，<a class="ae ky" rel="noopener" target="_blank" href="/a-minimalist-end-to-end-scrapy-tutorial-part-ii-b917509b73f7?source=friends_link&amp;sk=ebd3a9cee8b2097b3857194fee3821a6">第二部分</a>，<a class="ae ky" rel="noopener" target="_blank" href="/a-minimalist-end-to-end-scrapy-tutorial-part-iii-bcd94a2e8bf3?source=friends_link&amp;sk=a1fdde9c9dd5383d8de2e08395ee3f98">第三部分</a>，<a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-iv-3290d76a2aef?sk=6f0902f9a15092575814ab533a56f8ef" rel="noopener">第四部分</a>，<a class="ae ky" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-v-e7743ee9a8ef?source=friends_link&amp;sk=c1c5110f63c7ccbe4eb8c6209ee2f57c" rel="noopener">第五部分</a></p></div></div>    
</body>
</html>