<html>
<head>
<title>Airbnb Price Prediction Using Linear Regression (Scikit-Learn and StatsModels)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用线性回归(Scikit-Learn 和 StatsModels)进行 Airbnb 价格预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/airbnb-price-prediction-using-linear-regression-scikit-learn-and-statsmodels-6e1fc2bd51a6?source=collection_archive---------6-----------------------#2019-10-16">https://towardsdatascience.com/airbnb-price-prediction-using-linear-regression-scikit-learn-and-statsmodels-6e1fc2bd51a6?source=collection_archive---------6-----------------------#2019-10-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f7f6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Airbnb 的租赁价格是由什么决定的？床位数？允许的客人数量？复习分数？取消政策？</h2></div><p id="9c7d" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这个问题的答案提供了有趣的见解，可以使寻求利润最大化的主机受益。为了更深入地探究影响 Airbnb 租赁价格的可能因素，我用 Python <em class="lc">中的 Scikit-Learn 和 StatsModels 使用了各种线性回归模型。</em>在这篇文章中，我将重点介绍我用来回答这个问题的方法，以及我如何利用两个流行的线性回归模型。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="4045" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">主要问题:</strong>什么预测 Airbnb 租房价格？</p><p id="ccba" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">数据:</strong>约 74k Airbnb 租房的 Kaggle 数据集。该数据集包括许多功能，如:床位数、允许的客人数、描述、评论数等等。有关特性和数据的更详细预览，请参见数据集<a class="ae lk" href="https://www.kaggle.com/stevezhenghp/airbnb-price-prediction" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="85ca" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">方法/模型:</strong>这个问题属于回归和预测的范畴，所以使用了线性回归模型。我使用 StatsModels 生成一个起点普通最小二乘模型，使用 Scikit-Learn 生成一个 LassoCV 模型。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="e6d8" class="ll lm iq bd ln lo lp lq lr ls lt lu lv jw lw jx lx jz ly ka lz kc ma kd mb mc bi translated">步骤 0:思考问题和数据集</h1><p id="1ce6" class="pw-post-body-paragraph kg kh iq ki b kj md jr kl km me ju ko kp mf kr ks kt mg kv kw kx mh kz la lb ij bi translated">在一头扎进数据和产生大的相关矩阵之前，我总是试图思考这个问题并对特征有一个感觉。我为什么要做这个分析？目标是什么？特征和目标变量之间的关系有什么意义？</p><h1 id="3067" class="ll lm iq bd ln lo mi lq lr ls mj lu lv jw mk jx lx jz ml ka lz kc mm kd mb mc bi translated"><strong class="ak">步骤 0:导入包</strong></h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="5426" class="mw lm iq ms b gy mx my l mz na"># Import packages<br/>import pandas as pd<br/>import patsy<br/>import statsmodels.api as sm<br/>import statsmodels.formula.api as smf<br/>import statsmodels.api as sm<br/>from statsmodels.stats.outliers_influence import variance_inflation_factor<br/>from sklearn.preprocessing import StandardScaler, PolynomialFeatures<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import lars_path<br/>from sklearn.linear_model import LinearRegression, Lasso, LassoCV<br/>from sklearn.metrics import r2_score<br/>import scipy.stats as stats</span><span id="9abc" class="mw lm iq ms b gy nb my l mz na"># Visualization<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><h1 id="741a" class="ll lm iq bd ln lo mi lq lr ls mj lu lv jw mk jx lx jz ml ka lz kc mm kd mb mc bi translated">步骤 1:探索性数据分析和熊猫模型准备</h1><p id="aef6" class="pw-post-body-paragraph kg kh iq ki b kj md jr kl km me ju ko kp mf kr ks kt mg kv kw kx mh kz la lb ij bi translated">下面是一些我经常采取的一般数据清理和 EDA 步骤。我没有包括一些更细微的数据集特定步骤。</p><ul class=""><li id="bfb9" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">我从 pd.describe()和 pd.info()开始，了解丢失了哪些数据以及每个特性的值</li><li id="87a0" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">清理了包含空格的功能名称</li><li id="f726" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">将一些功能与许多类别相结合。例如，“property_type”有 25 个不同的类别(这是使用 pd.value_counts()检查的)。我把最低的 23 个组合成了“其他”类别。这也是在其他一些特性上完成的。这很有帮助，因为我没有从虚拟代码中创建 20+个新列。</li><li id="deba" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">删除分布非常不均匀或不会被使用的功能。比如‘host _ has _ profile _ pic’有 32k 个“是”，只有 97 个“否”。这在模型中没有用。</li><li id="a1db" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">缺失数据的进一步检查。对于我认为重要的特征(例如“点评分数评级”)，我用中间值填充。注意-这里的插值可能比用中值填充更微妙。例如，我可以用他们自己的中间值填充租赁的每个子类(公寓、房子、其他)。</li><li id="064f" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">检查与 pandas (pd.corr())和 Seaborn heatmap 的视觉相关性:特征和结果之间，以及特征本身之间。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="1dbc" class="mw lm iq ms b gy mx my l mz na">model_df.corr() # Whole correlation matrix<br/>model_df.corr()['log_price'] # Check correlations with outcome only</span><span id="e4c4" class="mw lm iq ms b gy nb my l mz na">sns.heatmap(model_df.corr(), cmap="seismic", annot=True, vmin=-1, vmax=1);</span></pre><ul class=""><li id="538c" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">各种分类变量的虚拟代码(如“取消 _ 政策”、“财产 _ 类型”)。确保设置 drop_first=True，以便不包括引用列。注意-如果特征是分类的，但只有两个选项(例如“男性”、“女性”)，则不需要手动生成虚拟代码，只需确保它是一个整数(0，1)。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="a7b1" class="mw lm iq ms b gy mx my l mz na"># Example dummy coding for 'cancellation_policy'<br/>model_df = pd.get_dummies(model_df, columns=['cancellation_policy'], drop_first=True)</span></pre><ul class=""><li id="dfc6" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">确保目标变量(' log_price ')是:正态分布，峰度和偏度是正态的。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="f15a" class="mw lm iq ms b gy mx my l mz na">sns.distplot(model_df['log_price'], kde=True,);<br/>fig = plt.figure()<br/>res = stats.probplot(model_df['log_price'], plot=plt)<br/>print("Skewness: %f" % model_df['log_price'].skew())<br/>print("Kurtosis: %f" % model_df['log_price'].kurt())</span></pre><figure class="mn mo mp mq gt nr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d4b30dd6e43f2720e2399b5a6080e58b.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*jGv5NkSnsUaLbbel4vmOTA.png"/></div></figure><h1 id="44f3" class="ll lm iq bd ln lo mi lq lr ls mj lu lv jw mk jx lx jz ml ka lz kc mm kd mb mc bi translated">步骤 2:在 StatsModels 中运行 OLS，并检查线性回归假设</h1><p id="b97c" class="pw-post-body-paragraph kg kh iq ki b kj md jr kl km me ju ko kp mf kr ks kt mg kv kw kx mh kz la lb ij bi translated">StatsModels 中的 OLS 模型将为我们提供最简单的(非正则化)线性回归模型，作为我们未来模型的基础。从简单开始，然后增加复杂性总是好的。此外，它还提供了一个很好的汇总表，很容易理解。这是检查线性回归假设的好地方。</p><ul class=""><li id="32a3" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">用<a class="ae lk" href="https://patsy.readthedocs.io/en/latest/quickstart.html" rel="noopener ugc nofollow" target="_blank"> Patsy </a>创建特征矩阵。Patsy 很好，因为它用简单的<em class="lc"> R </em>语法创建了我们的模型。默认情况下，它还会在您的模型中添加一个截距(如果您不使用 Patsy，请确保手动添加一个截距)。</li><li id="bd6e" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">训练/测试分割数据。80%将用于培训，20%将用于测试(稍后将详细说明为什么没有验证拆分)。</li><li id="b5d7" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">运行 OLS 模型，检查汇总表。</li></ul><figure class="mn mo mp mq gt nr"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="d106" class="mw lm iq bd ln nw nx dn lr ny nz dp lv kp oa ob lx kt oc od lz kx oe of mb og bi translated">要从 OLS 摘要表中检查的关键指标:</h2><ul class=""><li id="d173" class="nc nd iq ki b kj md km me kp oh kt oi kx oj lb nh ni nj nk bi translated"><em class="lc"> R Adjusted </em>:告诉你模型中的预测者解释了你的结果(租赁价格)的多少差异。</li><li id="9e35" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">对数似然:这个模型在新数据上表现相同的可能性有多大。您希望这个数字接近于零(很可能是负数)。</li><li id="5970" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated"><em class="lc"> P 值</em>:检查每个变量。低于 0.05 是标准阈值，让您了解哪些对模型有显著影响。</li><li id="3e6a" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated"><em class="lc">德宾-沃森</em>:有自相关吗？这应该是两点左右。</li><li id="211e" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">也可以查看<em class="lc">综合</em>和<em class="lc">条件号</em>以获得模型质量的整体感觉。</li></ul><h2 id="5807" class="mw lm iq bd ln nw nx dn lr ny nz dp lv kp oa ob lx kt oc od lz kx oe of mb og bi translated"><strong class="ak">获取方差膨胀因子(VIFs) </strong></h2><ul class=""><li id="78d3" class="nc nd iq ki b kj md km me kp oh kt oi kx oj lb nh ni nj nk bi translated"><em class="lc">Vif</em>不是从上面的 OLS 表中产生的，因此您应该手动提取它们。</li><li id="29ea" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">它们是检查模型中多重共线性的好方法。多重共线性是指要素之间高度相关。线性回归假设您的数据没有多重共线性，因此请确保检查这一点。你希望你的波动率指数低于 4。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="c074" class="mw lm iq ms b gy mx my l mz na"># Check for VIFs of each feature, then save to its own DF<br/>vif_census = pd.DataFrame()<br/>vif_census[“VIF Factor”] = [variance_inflation_factor(X_census.values, i) for i in range(X.shape[1])]<br/>vif_census[“features”] = X_census.columns</span></pre><h2 id="b9e8" class="mw lm iq bd ln nw nx dn lr ny nz dp lv kp oa ob lx kt oc od lz kx oe of mb og bi translated"><strong class="ak">检查模型的残差</strong></h2><ul class=""><li id="1584" class="nc nd iq ki b kj md km me kp oh kt oi kx oj lb nh ni nj nk bi translated">线性回归的另一个假设是残差是正态分布的。你可以很容易地用图来验证这一点。在下图中，你可以看到有一些传播(特别是左上)，但残差大多落在中线附近。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="1003" class="mw lm iq ms b gy mx my l mz na"># Use statsmodels to plot the residuals vs the fitted values<br/>plt.figure(figsize=(12,8))<br/>plt.scatter(fit_census.predict(), fit_census.resid); # print resids vs predictions<br/>plt.title("Residuals plot from OLS Model")<br/>plt.xlabel("Predicted Values")<br/>plt.ylabel("Residuals")<br/>plt.savefig('LR_Residual_Plot')</span></pre><figure class="mn mo mp mq gt nr gh gi paragraph-image"><div role="button" tabindex="0" class="ol om di on bf oo"><div class="gh gi ok"><img src="../Images/b8ed36dd68b578362ab53bfcd29e56fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIRdyQUy6-ouCUgGH0ZE7g.png"/></div></div></figure><h1 id="f430" class="ll lm iq bd ln lo mi lq lr ls mj lu lv jw mk jx lx jz ml ka lz kc mm kd mb mc bi translated">第三步:<strong class="ak">拉索夫</strong></h1><p id="d5dd" class="pw-post-body-paragraph kg kh iq ki b kj md jr kl km me ju ko kp mf kr ks kt mg kv kw kx mh kz la lb ij bi translated">接下来，我使用正则化线性回归模型。这些类型的模型对变量施加惩罚，并有助于降低模型的复杂性。正则化有助于偏差-方差权衡，并有助于模型对新的测试数据更具普遍性。参见<a class="ae lk" rel="noopener" target="_blank" href="/regularization-in-machine-learning-76441ddcf99a#targetText=Regularization,linear%20regression%20looks%20like%20this.">这篇文章</a>对正规化的一个很好的概述。</p><ul class=""><li id="eeba" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">由于正则化模型惩罚变量，我们需要将它们标准化，这样它们都以相同的比率被惩罚。这是通过对数据应用标准定标器来实现的。把标准化想象成把所有的特性放在同一个平面上，并给它们一个<em class="lc"> z </em>的分数。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="32bc" class="mw lm iq ms b gy mx my l mz na"># Standard scale the data</span><span id="3af4" class="mw lm iq ms b gy nb my l mz na">std = StandardScaler()<br/>std.fit(X_train_census.values) # only std.fit on train set<br/>X_tr_census = std.transform(X_train_census.values)<br/>X_te_census = std.transform(X_test_census.values)</span></pre><p id="597a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">型号类型快速说明</strong></p><ul class=""><li id="e1d6" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">现在我们准备运行我们的正则化模型。我们是要用套索，脊，弹力网，还是三者都用？我只在这里使用套索，不会去详细说明，但套索和山脊惩罚变量/系数不同，所以每个可以用来获得不同的见解。简而言之，Lasso 惩罚了一些系数，以至于它们变为零并被排除在模型之外。这基本上是一个自动的特征选择。Ridge 通过“缩小”系数和“抹黑”它们的影响来惩罚系数。当您希望平稳地处理多重共线性时，岭是很好的选择。ElasticNet 本质上是两者的结合。Scikit-learn 的网站上有关于所有这些的很好的文档。这三者的代码本质上是相同的。</li></ul><h1 id="342d" class="ll lm iq bd ln lo mi lq lr ls mj lu lv jw mk jx lx jz ml ka lz kc mm kd mb mc bi translated">步骤 3a:拉索夫</h1><ul class=""><li id="5dc1" class="nc nd iq ki b kj md km me kp oh kt oi kx oj lb nh ni nj nk bi translated">我使用 CV 版本的 Lasso，因为它们有内置的交叉验证。这意味着您不必在新的数据集上手动验证，而是由 CV 模型为您进行训练和交叉验证。注意前面(训练-测试分割代码)我只做了 80-20 分割——没有留出验证数据，而是将整个 80 用于训练和验证。</li><li id="6c7e" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">我喜欢先运行 LassoCV，因为它固有的特性选择。从具有 0.0 beta 系数的模型中得出的特征可能不是强特征，并且可以在以后被移除。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="4858" class="mw lm iq ms b gy mx my l mz na"># Run the cross validation, find the best alpha, refit the model on all the data with that alpha</span><span id="10bc" class="mw lm iq ms b gy nb my l mz na">alphavec = 10**np.linspace(-2,2,200)<br/>lasso_model_census = LassoCV(alphas = alphavec, cv=5)<br/>lasso_model_census.fit(X_tr_census, y_train_census)</span></pre><ul class=""><li id="5349" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">在上面的代码中，我做了三件事:1)创建一个将放入 CV 模型的 alpha 数组 2)使用设置为步骤 1 的 alpha 数组的 alpha 参数初始化模型，并设置五重交叉验证 3)拟合训练数据。</li><li id="5dc0" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">在这一步之后，我们可以检查每个特性的 beta 系数，以及访问关于模型的一些重要信息。</li></ul><p id="00e3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">关于模型比较指标的简要说明</strong></p><ul class=""><li id="e720" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">有几种方法可以比较回归模型，但这里我用的是 R 和平均绝对误差(MAE)。r 是我们用模型计算的结果变量的方差的度量。MAE 是我们模型中平均误差量的度量。这很容易解释，也是一个很好的分享指标。例如，在我预测 Airbnb 租赁价格的模型中，如果我的 MAE 是($)20，那么在进行预测时，我可以说我的模型误差了大约 20 美元。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="8a28" class="mw lm iq ms b gy mx my l mz na"># Print feature name zipped with its beta<br/>lasso_betas = list(zip(X_train_census.columns,<br/>lasso_model_census.coef_))</span><span id="fed1" class="mw lm iq ms b gy nb my l mz na"># R2 of Training set<br/>lasso_model_census.score(X_tr_census,y_train_census)</span></pre><p id="17a7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如果满意，运行测试数据的时间。如果没有，就回去做一些特色工程。如果你满意，以下步骤。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="0832" class="mw lm iq ms b gy mx my l mz na"># Predict model on test data<br/>y_census_pred = lasso_model_census.predict(X_te_census)</span><span id="eb21" class="mw lm iq ms b gy nb my l mz na"># R2 of test set using this model<br/>r2_score(y_test_census, y_census_pred)</span><span id="5334" class="mw lm iq ms b gy nb my l mz na">#Mean Absolute Error (MAE)<br/>def mae(y_true, y_pred):<br/>    return np.mean(np.abs(y_pred - y_true))</span><span id="c945" class="mw lm iq ms b gy nb my l mz na">mae(y_test_census, y_census_pred)</span><span id="42df" class="mw lm iq ms b gy nb my l mz na"># Plot<br/>plt.scatter(y_test_census, y_census_pred)<br/>plt.plot([0,10],[0,10],color='red')<br/>plt.grid(True)<br/>plt.title('Predicted vs. Actual Rental Price (log) with LASSO CV')<br/>plt.ylabel('Rental Price (log) Predicted')<br/>plt.xlabel('Rental Price (log) Actual');</span></pre><figure class="mn mo mp mq gt nr gh gi paragraph-image"><div role="button" tabindex="0" class="ol om di on bf oo"><div class="gh gi op"><img src="../Images/07044df72cd3b1e16baba5913f7442ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7GwDAsNcmSs7gmutIJDdiA.png"/></div></div></figure><ul class=""><li id="dcbe" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">这里我们可以看到，我们的模型是体面的，有一些低于性能时，租金价格高。这可能是因为在这个范围内没有太多的。</li></ul><p id="66b0" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">使用 LARS 路径确定哪些功能最重要</strong></p><ul class=""><li id="fd92" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">LARS path 是一个很好的工具，可以用来查看 LassoCV 模型中哪些特性最重要/最不重要。简单来说，在下图中，首先从零(正或负)变化的特征是最重要的，最后变成非零的特征是最不重要的。也就是说，出现在左边的特征是最重要的。这也反映在这些特征的β系数中。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="ad37" class="mw lm iq ms b gy mx my l mz na">print("Computing regularization path using the LARS ...")<br/>alphas, _, coefs = lars_path(X_tr_census, y_train_census, method='lasso')</span><span id="1f51" class="mw lm iq ms b gy nb my l mz na"># # plotting the LARS path<br/>xx = np.sum(np.abs(coefs.T), axis=1)<br/>xx /= xx[-1]<br/>plt.figure(figsize=(10,10))<br/>plt.plot(xx, coefs.T)<br/>ymin, ymax = plt.ylim()<br/>plt.vlines(xx, ymin, ymax, linestyle='dashed')<br/>plt.xlabel('|coef| / max|coef|')<br/>plt.ylabel('Coefficients')<br/>plt.title('LASSO Path')<br/>plt.axis('tight')<br/>plt.legend(X_train_census.columns)<br/>plt.show()</span></pre><figure class="mn mo mp mq gt nr gh gi paragraph-image"><div role="button" tabindex="0" class="ol om di on bf oo"><div class="gh gi oq"><img src="../Images/27294a3485121ba666b9d96679480dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8uKdE1Lv5iCVIqqA49fZw.png"/></div></div></figure></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="e68a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">概括地说，这里我介绍了我喜欢在数据探索和清理中采取的一些基本步骤(还有更多的事情可以做，参见<a class="ae lk" rel="noopener" target="_blank" href="/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184">这篇文章</a>以获得更详细的 EDA 技巧)。此外，我运行了一个简单的 OLS 回归和一个正则化套索模型。在这些步骤之后，人们可以运行 RidgeCV 和/或 ElasticNetCV 并比较模型。</p><p id="a483" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">感谢您的阅读，如果有我需要补充的地方，请告诉我。</p></div></div>    
</body>
</html>