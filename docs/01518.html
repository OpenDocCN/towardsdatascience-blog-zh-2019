<html>
<head>
<title>Which machine learning model to use?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用哪个机器学习模型？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/which-machine-learning-model-to-use-db5fdf37f3dd?source=collection_archive---------4-----------------------#2019-03-11">https://towardsdatascience.com/which-machine-learning-model-to-use-db5fdf37f3dd?source=collection_archive---------4-----------------------#2019-03-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="89bf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">陈述你的问题，并跟随这篇文章来知道使用哪个模型。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/19922ec25dc39d676a7a4b71b8a9c404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m0ELEPbDTh9fYp6vNzQn2A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">image by <a class="ae ky" href="https://pixabay.com/users/stevepb-282134" rel="noopener ugc nofollow" target="_blank">stevepb</a></figcaption></figure><p id="3e0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">—如果你不知道什么是 ML 型号，看看这篇<a class="ae ky" rel="noopener" target="_blank" href="/introduction-to-machine-learning-top-down-approach-8f40d3afa6d7">文章</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/5712334330468ccf7015e2c7614d17c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xMwqdbPMm3l_1cOVHAUYcQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">summary of ML models, <a class="ae ky" href="http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="6aee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lw translated"><span class="l lx ly lz bm ma mb mc md me di"> T </span>参加机器学习课程和阅读相关文章并不一定会告诉你使用哪种机器学习模型。它们只是给你一个关于这些模型如何工作的直觉，这可能会让你为选择适合你的问题的模型而烦恼。</p><p id="f07d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我开始 ML 之旅的时候，为了解决一个问题，我会尝试许多 ML 模型，并使用最有效的模型，我现在仍然这样做，但我遵循一些最佳实践——关于如何选择机器学习模型——这些最佳实践是我从经验、直觉和同事那里学到的，这些最佳实践使事情变得更容易，以下是我收集的。</p><p id="134f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我会根据你的问题性质告诉你用哪种机器学习模型，我会试着解释一些概念。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mf"><img src="../Images/5351184f2d8619c0f2ecd75c24fb6d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*itpAdE6O7ldkLqBqvW2ZTA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="http://pexels.com" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="4f8a" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">分类</h1><p id="3bb9" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">首先，如果你有一个分类问题“预测给定输入的类别”。</p><p id="b793" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住您要将输入分类到多少个类，因为有些分类器不支持多类预测，它们只支持 2 类预测。</p><h2 id="869a" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">-缓慢但准确</h2><ul class=""><li id="6905" class="nw nx it lb b lc nf lf ng li ny lm nz lq oa lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank">非线性 SVM </a>查看分类部分末尾的<strong class="lb iu">注释</strong>了解更多关于 SVM 使用的信息。</li><li id="8388" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">随机森林</a></li><li id="7077" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a>(需要大量数据点)</li><li id="1a4b" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">梯度增强树</a>(类似于随机森林，但更容易过度拟合)</li></ul><h2 id="0129" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">-快</h2><ul class=""><li id="4451" class="nw nx it lb b lc nf lf ng li ny lm nz lq oa lu ob oc od oe bi translated">可讲解的模型:<a class="ae ky" href="https://en.wikipedia.org/wiki/Decision_tree" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">决策树</strong> </a> <em class="ok">和</em> <strong class="lb iu"> </strong> <a class="ae ky" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">逻辑回归</strong> </a></li><li id="c8cc" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated">不可解释模型:<a class="ae ky" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">线性 SVM </strong> </a> <strong class="lb iu"> </strong> <em class="ok">和</em> <a class="ae ky" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">朴素贝叶斯</strong> </a></li></ul><h2 id="a828" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">注:SVM 内核使用(摘自吴伟雄的课程)</h2><ul class=""><li id="298e" class="nw nx it lb b lc nf lf ng li ny lm nz lq oa lu ob oc od oe bi translated">当特征数量大于观察数量时，使用线性核。</li><li id="867d" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated">当观测值的数量大于要素的数量时，使用高斯核。</li><li id="4d4a" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated">如果观测值的数量大于 50k，那么在使用高斯核时，速度可能是一个问题；因此，人们可能想使用线性核。</li></ul></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="af83" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">回归</h1><p id="9b45" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">如果你有一个回归问题，“这是预测一个连续的值，就像预测房子的价格，给定房子的特征，如大小，房间数量等”。</p><h2 id="49ed" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">-准确但缓慢</h2><ul class=""><li id="d88e" class="nw nx it lb b lc nf lf ng li ny lm nz lq oa lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">随机森林</a></li><li id="7e81" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a>(需要大量数据点)</li><li id="7cf1" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">渐变提升树</a>(类似随机森林，但更容易过度拟合)</li></ul><h2 id="3f21" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">-快</h2><ul class=""><li id="d47b" class="nw nx it lb b lc nf lf ng li ny lm nz lq oa lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Decision_tree" rel="noopener ugc nofollow" target="_blank">决策树</a></li><li id="660e" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">线性回归</a></li></ul></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="36e4" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">使聚集</h1><p id="9b2c" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">如果你有一个聚类问题“根据特征将数据分成 k 组，使得同一组中的对象具有某种程度的相似性”。</p><p id="b5e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Hierarchical_clustering" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">层次聚类</strong> </a>(也称为<strong class="lb iu">层次聚类分析</strong>或<strong class="lb iu"> HCA </strong>)是一种聚类分析方法，旨在构建聚类的层次结构。分层聚类的策略通常分为两种类型:</p><ul class=""><li id="d3cb" class="nw nx it lb b lc ld lf lg li ol lm om lq on lu ob oc od oe bi translated"><strong class="lb iu">凝聚</strong>:这是一种“自下而上”的方法:每个观察从它自己的集群开始，随着一个集群在层次结构中向上移动，集群对被合并。</li><li id="9b38" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><strong class="lb iu">分裂</strong>:这是一种“自上而下”的方法:所有的观察从一个集群开始，随着层级的下移，分裂被递归地执行。</li></ul><p id="55c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">非层次聚类:</strong></p><ul class=""><li id="03bf" class="nw nx it lb b lc ld lf lg li ol lm om lq on lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener ugc nofollow" target="_blank"> DBSCAN </a>(不需要指定 k 的值，它是集群的数量)</li><li id="1db6" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank"> k-means </a></li><li id="f406" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model" rel="noopener ugc nofollow" target="_blank">高斯混合模型</a></li></ul><p id="fd1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你正在聚类一个<strong class="lb iu">分类数据</strong>使用</p><ul class=""><li id="7b09" class="nw nx it lb b lc ld lf lg li ol lm om lq on lu ob oc od oe bi translated"><a class="ae ky" href="https://amva4newphysics.wordpress.com/2016/10/26/into-the-world-of-clustering-algorithms-k-means-k-modes-and-k-prototypes/" rel="noopener ugc nofollow" target="_blank"> k 模式</a></li></ul></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="6a61" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">降维</h1><p id="dac0" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">主成分分析</strong> </a></p><p id="f028" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> PCA </strong>可以被认为是将一个<em class="ok"> n </em>维椭球体拟合到数据上，其中椭球体的每个轴代表一个主分量。如果椭球的某个轴很小，则沿该轴的方差也很小，并且通过从数据集的表示中省略该轴及其相应的主分量，我们仅损失了相应少量的信息。</p><p id="05ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想做<a class="ae ky" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">主题建模</strong> </a> <strong class="lb iu"> </strong>(下面解释)你用<strong class="lb iu">奇异值分解</strong> ( <strong class="lb iu"> SVD </strong> ) <strong class="lb iu"> </strong>或者<strong class="lb iu">潜在狄利克雷分析</strong> ( <strong class="lb iu"> LDA </strong>)，概率主题建模用<strong class="lb iu"> LDA </strong>。</p><ul class=""><li id="937c" class="nw nx it lb b lc ld lf lg li ol lm om lq on lu ob oc od oe bi translated"><strong class="lb iu">主题建模</strong>是一种统计模型，用于发现文档集合中出现的抽象“主题”。主题建模是一种常用的文本挖掘工具，用于发现文本中隐藏的语义结构。</li></ul></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="c4a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望现在事情对你来说更容易了，我会用我从你的反馈和实验中得到的信息来更新这篇文章。</p><p id="ea47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将留给你这两个精彩的总结。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/e925921994dfee26a918d81c237a8ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FrmBl7WZBbedTku_2SQV3A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="http://peekaboo-vision.blogspot.de/2013/01/machine-learning-cheat-sheet-for-scikit.html" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/7736cee50fab4b277eca74f7e1aadbe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gccuMDV8fXjcvz1RSk4kgQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="http://www.asimovinstitute.org/neural-network-zoo/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure></div></div>    
</body>
</html>