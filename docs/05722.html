<html>
<head>
<title>Demystifying Generative Models by Generating Passwords — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过生成密码来揭开生成模型的神秘面纱—第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-passwords-with-generative-models-from-probabilistic-to-deep-learning-approaches-54d41d8810e3?source=collection_archive---------23-----------------------#2019-08-21">https://towardsdatascience.com/generating-passwords-with-generative-models-from-probabilistic-to-deep-learning-approaches-54d41d8810e3?source=collection_archive---------23-----------------------#2019-08-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="e633" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">从概率到深度学习方法</h2><div class=""/><div class=""><h2 id="614f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">理解朴素贝叶斯模型和变分自动编码器(VAE)在生成任务中的区别。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/8fb3769acec1ccf45ea5ff3e05b182d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DyfylWbsZle_2nUfPTdfjA.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@dmey503?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Dan Meyers</a> on <a class="ae le" href="https://unsplash.com/search/photos/generator?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="0afb" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">介绍</h1><p id="99ab" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">机器学习和深度学习模型已经在广泛的行业中展示了它们的能力，网络安全当然也不例外。一个相对较新的研究例子是 PassGAN，这是一个深度学习模型，可以生成逼真的密码，从而提高暴力攻击的有效性[1]。</p><p id="1caa" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在这篇文章中，我想借用这个想法，并用它作为一个例子来解释生成模型和判别模型之间的差异，以及强调(并证明)深度学习在高维数据方面优于传统概率建模的优势。</p><h1 id="f1f6" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">判别模型与生成模型</h1><p id="a343" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在 ML/DL 模型中有一个非常著名的分类法，它由生成模型和判别模型组成，每种模型都有自己独特的特征。然而，给初学数据的科学家造成困惑是极其常见的。事实上，这是完全可以理解的，因为这个名字可能会误导他们的用例。直觉上，你会认为一个判别模型会被用来判别多个类和生成新的合成数据，当然，这是绝对正确的。然而，这并不意味着创成式模型不能用作分类器，正如其名称最初所暗示的那样。</p><p id="3ff9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">不同之处在于模型如何学习。简而言之，生成模型学习数据是如何生成的，并因此使用它来对看不见的数据进行分类，而判别模型只学习每个类之间的差异(边界)。</p><p id="68b8" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">为了给你一个更具体的例子，想象一下:我们有一个人和动物的涂鸦集合，我们想创建一个模型，可以将涂鸦作为输入，并反馈它看起来更像人还是动物。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi my"><img src="../Images/dc09df876ca157a7487464a1e2e91e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRvcXEcJM8m2CDsBWn-oAg.png"/></div></div></figure><p id="1e33" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">有趣的是，生成模型将首先学习如何绘制人类和动物的涂鸦，当给定一个看不见的涂鸦时，它将绘制一个动物和一个人，并将其与给定的进行比较。</p><p id="6d85" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">或者，辨别模型将学习每个类别的细微差别，即动物有尾巴、人类的姿势、形状等，并在没有学习任何绘画知识的情况下使用这些来辨别两个类别。简单吧？</p><p id="88f4" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">尽管判别模型因其简单性而在传统上更受青睐[2]，但生成模型也有其自身的优势，正变得越来越受欢迎。当有缺失数据时，甚至在检测异常值时，它们表现得特别好。甚至，毕竟，我们可能想要生成新的数据，这是判别模型所不能做到的。事实上，直到最近，生成模型才达到生成内容的真实水平(见下图)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mz"><img src="../Images/7daac6b4771f4ceec1b8718a432b714c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FOvNKsF0x3iIsU9DCcRUyQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Face generation with generative modelling. Source: David Foster. “Generative Deep Learning.” [3]</figcaption></figure><h2 id="72ed" class="na lg iq bd lh nb nc dn ll nd ne dp lp mg nf ng lr mk nh ni lt mo nj nk lv iw bi translated">可能性</h2><p id="7e0f" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">我尽可能地避免使用数学术语，但是为了更清楚地建立判别和生成模型，引入一些数学概念是必要的。</p><p id="9541" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">假设我们有一些观察数据<strong class="lz ja"> X </strong>用标签<strong class="lz ja"> Y. </strong>标记，从概率的角度来看，一个判别分类器将试图对<strong class="lz ja"> P(Y|X) </strong>的条件概率建模。也就是说，给定观察值 X，我标记 Y 的概率是多少？(不要把条件概率和联合概率<strong class="lz ja"> P(A，B) </strong>混淆，联合概率是指 A 和 B 同时发生的概率)。另一方面，生成建模试图直接对概率<strong class="lz ja">P(X)</strong>建模，简单来说，首先获得观察值 X 的概率是多少？注意，标签对于这个模型不是必需的，但是，如果我们想要执行分类，标签可以用于定义<strong class="lz ja"> P(X|Y)。</strong></p><h1 id="fce0" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">简化的生成模型</h1><p id="aaeb" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">为了实际理解什么是生成模型，让我们想象一个极其简化的场景，其中只有二维的生成模型。这个例子的灵感来自大卫·福斯特的《生成性深度学习》一书，我强烈推荐这本书！</p><p id="728a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">考虑在下面的二维空间上由称为<strong class="lz ja">P</strong>T14】数据的规则生成的一些点:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/bc9851b157cb5e8f9faed3131e181b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*enOIIfcHzG-eFoXEKdSfDQ.png"/></div></div></figure><p id="22b5" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">生成模型的目的是生成一个新点<strong class="lz ja"> X </strong> = <strong class="lz ja"> </strong> (x，y)，该点看起来像是由<strong class="lz ja">P</strong>T22】数据生成的。由此，我们来构造一个对<strong class="lz ja">P</strong>T26】数据的估计，称为<strong class="lz ja">P</strong>T30】模型。一种可能的估计是在橙色边界框内的任何地方生成一个点的均匀概率分布，以及在边界框外生成一个点的概率为零，如下图所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/c5b1bd023024d13a2f28e0a9c63f4322.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZkMZtB_J9upde0i9fzJaA.png"/></div></div></figure><p id="a789" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">回到前面章节的创成式模型描述，我们刚刚创建了一个模型，它首先定义了如何创建数据点！虽然过于简单，但您刚刚开发了一个生成模型！</p><p id="7a87" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">现在让我们更深入一点，观察我们的模型相对于真实的<strong class="lz ja"> P </strong> <em class="nl">数据</em>分布的表现。注意，原始的<strong class="lz ja"> P </strong> <em class="nl">数据</em>代表该土地上生长的树木的概率分布(见下图)。一棵树在有土壤的地方生长的几率大致相等，但在水中生长的几率为零。</p><blockquote class="nn no np"><p id="5970" class="lx ly nl lz b ma mt ka mc md mu kd mf nq mv mi mj nr mw mm mn ns mx mq mr ms ij bi translated">这强调了数据科学家的另一个非常可取的特征。对先验数据分布的高级认知能力与数据科学家的领域专业知识直接相关。对于这个例子，根据自然经验，我们知道树木不能在水上生长！类似地，在真实世界的场景中，大多数情况下，数据科学家后退一步分析问题域是有帮助的，这样在未来就有可能对数据模型的结果进行推理。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/775a0b796d0b2e3ad0f251571b798b0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*loKyteolhMYMFkdWnjq3rw.png"/></div></div></figure><p id="4159" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">值得注意的是，尽管<strong class="lz ja"> P </strong> <em class="nl">模型</em>是<strong class="lz ja"> P </strong> <em class="nl">数据</em>的过度简化，但它设法掌握了原始分布的主要机制。从<strong class="lz ja">P</strong><em class="nl"/>模型中采样 3 点 A、B、C，很明显我们的模型并不完美。点<strong class="lz ja"> C </strong>无法从<strong class="lz ja">P</strong>T24】数据中生成，但人工点<strong class="lz ja"> A </strong>和<strong class="lz ja"> B </strong>与真实点无法区分。这是生成模型的主要目的。生成不同于现有数据的新数据，但看起来像是由相同的规则创建的。</p><h1 id="3bbb" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">概率与深度学习方法</h1><p id="5b36" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在这一部分，我们将通过实际例子来探索为什么深度学习对生成模型的快速发展做出了重大贡献。但是首先，有必要提出概率方法，以便有一个可以比较的基准。</p><h2 id="54c3" class="na lg iq bd lh nb nc dn ll nd ne dp lp mg nf ng lr mk nh ni lt mo nj nk lv iw bi translated">最大似然估计</h2><p id="0d79" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">最大似然估计是一种统计技术，可用于计算参数，使我们的模型生成给定数据的可能性最大化。这一开始听起来可能有点混乱，所以让我们把它放到上面例子的上下文中。</p><p id="d9d0" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">假设上面的橙色框是一个<strong class="lz ja">概率密度函数，</strong>简单来说是一个函数，给定我们的<strong class="lz ja">样本空间中的一个点(地图上的 x，y)</strong>返回一个从 0 到 1 的值，一个概率。直观上，概率密度函数(积分)上所有点的总和应该总是等于 1。回到我们的例子，因为盒子代表均匀分布，边界盒子外面的概率是 0，里面的概率是常数。在数学符号中，它可以表示为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/407b47f49470bdac9f6afc4383691918.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/1*J7MGzGbHsLCv4BFy0mqYSw.gif"/></div></figure><p id="6f64" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">继续，设θ是一组 4 个参数{θ1，θ2，θ3，θ4}，一个似然函数<em class="nl">L(θ|</em><strong class="lz ja"><em class="nl">x</em></strong><em class="nl">)</em>试图回答以下问题:</p><blockquote class="nn no np"><p id="a0c2" class="lx ly nl lz b ma mt ka mc md mu kd mf nq mv mi mj nr mw mm mn ns mx mq mr ms ij bi translated">给定点<strong class="lz ja"> x </strong>参数的一些具体值有多大可能？</p></blockquote><p id="42f9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">但是你可能想知道这些参数是干什么的？这些来自一个众所周知的统计学领域，叫做<strong class="lz ja">参数建模。</strong>换句话说，这是一种用有限的一组参数来表示概率分布的方法。在这种背景下，我们的概率分布(框！)可以用四个参数建模，左上角点(θ1，θ2)和右下角点(θ3，θ4)。更现实的例子是高斯(正态)分布，它有两个参数，均值μ和标准差σ。</p><p id="148f" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">更准确地说，似然函数定义如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/97370b73cb44ac505b324f95c3178409.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/1*l2Qvs3_Y3g5VykQvMQnLLg.gif"/></div></figure><p id="a069" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这是由θ参数化的概率密度函数返回的在<strong class="lz ja"> x </strong>处的概率。</p><p id="4da9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">现在，如果不是只有一个数据点，而是有<strong class="lz ja"> n </strong>个数据点<strong class="lz ja"> Xn </strong> = { <strong class="lz ja"> x1 </strong>，<strong class="lz ja"> x2 </strong>，…，<strong class="lz ja"> xn </strong> }那么可能性可以被评估为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/33e11beb563c830472a5acb7a1548bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/1*qJ8p0ZY550gg_AUptLSzhw.gif"/></div></figure><p id="eed0" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">然而，由于该乘积可能很难处理，因此也可以用对数形式表示，此时乘积变成一个和，从而更容易区分:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a5cabef48478ca02933cf89661ee169d.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/1*QBO3C46HdSdBVAsEo7FVIQ.gif"/></div></figure><blockquote class="nn no np"><p id="cf28" class="lx ly nl lz b ma mt ka mc md mu kd mf nq mv mi mj nr mw mm mn ns mx mq mr ms ij bi translated">顺便说一下，转换成对数形式是可能的，因为自然对数是一个单调递增的函数。简单地说，当 x 增加时，y 也增加，所以最大值不变。</p></blockquote><p id="ead0" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">既然有了这些，就有可能回到最初的概念，<strong class="lz ja">最大似然估计</strong>。或者简单地说，找到最好地解释数据集<strong class="lz ja"> X. </strong>的一些最佳参数θ，相当于说这些参数最有可能被用于对生成<strong class="lz ja"> X. </strong>的概率分布进行建模。正式地，这被定义为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/db9d517fdd804328445ec4d0acaab822.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/1*qnkxtz2XCWNjVSk4NBJjQQ.gif"/></div></figure><p id="6294" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">所有这些应该给你一个更清晰的画面，如何最大似然估计可以用来创建一个生成模型，但仍然，这都是理论，上面使用的例子只是为了有一个更容易理解的可视化参考。在下一节中，我们将应用这里所描述的来开发一个有些用处的生成模型。</p><h2 id="c0c1" class="na lg iq bd lh nb nc dn ll nd ne dp lp mg nf ng lr mk nh ni lt mo nj nk lv iw bi translated">用于密码生成的朴素贝叶斯</h2><p id="3df6" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">让我们再一次构建一个故事，假设你很难想出密码，所以你想出了一个好主意，创建一个为你创建密码的生成模型！</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ny nz l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Source: Giphy.com</figcaption></figure><p id="b124" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">请记住，这是一个假设的应用程序，用于在实践中应用这里描述的模型，并获得实践经验，而不仅仅是解释背后的理论。更重要的是，这里展示的方法和技术也有实际应用，例如特定于域的密码暴力破解，将随机密码生成过程替换为更容易记住但具有同等安全性的过程等等。这里不做描述，但提供给读者作为进一步探索的启示。</p><p id="0d99" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">您已经创建了您的密码想要包含的元素列表:</p><blockquote class="nn no np"><p id="ce57" class="lx ly nl lz b ma mt ka mc md mu kd mf nq mv mi mj nr mw mm mn ns mx mq mr ms ij bi translated"><strong class="lz ja">文字:</strong>过关，咖啡馆，宾果，hyper，李特，哈克曼，忍者，贝比<br/>T3】数字: 1234，111，777，000，0101，2019，2018，1，2，10，123 <br/>，特殊:！、@、$、%、*、_ <br/> <strong class="lz ja">大写:</strong> A、E、T、P</p></blockquote><p id="dca7" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">并定义了您的密码应该遵循的标准格式:</p><blockquote class="oa"><p id="2c9a" class="ob oc iq bd od oe of og oh oi oj ms dk translated">大写+单词+数字+特殊</p></blockquote><p id="0202" class="pw-post-body-paragraph lx ly iq lz b ma ok ka mc md ol kd mf mg om mi mj mk on mm mn mo oo mq mr ms ij bi translated">基本上，你的每个密码都可以用这 4 个特征来定义，它们总是以相同的顺序排列:一个大写字母，一个单词，一个数值和一个特殊字符。所有这些都来自上面提供的列表。</p><p id="7ed0" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">总的来说，这等于 8 * 11 * 6 * 4 = 2112 种组合！当然，您可以简单地为您的密码中的每个特征选择一个随机元素，然后使用它<strong class="lz ja">但是</strong>您希望每次都避免随机密码，因为它会变得更难记住。相反，根据对您以前的密码的观察，最好比其他密码更频繁地使用一些密码。用更正式的术语来说，应该有一个有利于某些元素的分布过程。</p><p id="156a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">最初，您按照相同的格式收集了过去使用过的 30 个密码:</p><p id="d896" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">以下是数据集的前 10 个密码:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/b74d28ddd5ddc31f20e8da1ec6e99d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*i9AklC_2yaxlkbXH26aMuA.png"/></div></figure><p id="3c49" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">让我们问自己一个问题。拥有密码<strong class="lz ja"> x </strong>的概率是如何定义的？</p><blockquote class="nn no np"><p id="9bbe" class="lx ly nl lz b ma mt ka mc md mu kd mf nq mv mi mj nr mw mm mn ns mx mq mr ms ij bi translated">回想一下，密码<strong class="lz ja"> x </strong>由 4 个特征定义。这相当于我们在上面的简单例子中描述的地图点，但是它现在有 4 个维度——x1，x2，x3，x4，而不是<strong class="lz ja"> x，y</strong><strong class="lz ja">T22。</strong></p></blockquote><p id="faec" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">直观上，我们可以说答案是同时有<strong class="lz ja"> x1，x2，x3，x4 </strong>一起发生的概率。所以:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/96a06a489625cd06828fc79a5ddadbb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/1*SgXJ5B0f47ua52ogXCbqfQ.gif"/></div></figure><p id="b324" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这可以使用<a class="ae le" href="https://www.eecs.qmul.ac.uk/~norman/BBNs/Chain_rule.htm" rel="noopener ugc nofollow" target="_blank"> <strong class="lz ja">链式规则</strong> </a>进一步扩展到条件概率。因此，上面的表达式变成:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/c90e865caa90544454c92b3318a8e85e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/1*4KoSbnV0SwmoQjn8nQaWJw.gif"/></div></figure><p id="1650" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">为了继续使用朴素贝叶斯模型，我们必须做一个强有力的假设。每个特征都是相互独立的。也就是说，大写字母与单词或数值的选择无关。这是一个相当<strong class="lz ja">幼稚的</strong>假设，因此得名。更明确地说:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d5226f4b51d748692b31d0bca6a19229.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/1*coBbJdWkteOVHlW5CugV8Q.gif"/></div></figure><p id="480a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">考虑到这一点，前面从链规则导出的表达式被简化为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/bfc7ad24e656bf571674b914947e6da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/1*BTixzGOGvQwuEfhxND9FVQ.gif"/></div></figure><p id="af58" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">等等，当我们描述最大似然估计时，概率密度函数是由分布参数集θ参数化的，它有 4 个参数，因为我们知道分布可以描述为一个盒子。但是，在这种情况下，概率是如何参数化的呢？</p><p id="6284" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">因为我们的数据集由离散值组成，或者更抽象地说，由一组有限值的元素组成，所以我们可以使用<strong class="lz ja">多项式分布</strong>，结果是<strong class="lz ja">多项式朴素贝叶斯</strong>。在这种情况下，我们可以简单地为模型的每个特征的每个值分配一个参数，这样就产生了总共<code class="fe ou ov ow ox b">8+11+6+4-4=25</code>个参数。</p><blockquote class="nn no np"><p id="b498" class="lx ly nl lz b ma mt ka mc md mu kd mf nq mv mi mj nr mw mm mn ns mx mq mr ms ij bi translated">-4 是为了补偿每个要素的最后一个值，因为它不必进行计算，因为它被强制使总和等于 1。</p></blockquote><p id="8e16" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">此外，要找到多项式分布中的最大似然估计，我们只需将每个特征值的出现次数除以总观察次数，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/6a0173fce4c13f4c35329be732eb4b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/1*dJQ0-OaZygU9U78f-5tkwQ.gif"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oz"><img src="../Images/bbafd974d9794426a0496c16b87cb036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dQIkLrBzzPY3ROzIYQDeUA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Maximum Likelihood Estimation of feature parameters.</figcaption></figure><p id="f5f9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">既然我们已经计算了每个特征的最大似然估计值，就可以从每个特征中抽取一个值，并将它们连接在一起以获得一个新密码！以下是我列出的由该模型生成的 10 个新密码:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/2210a4da351995a8135969c9ac7ce52b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NkHq9Yk8VGf3eRK83uKZmA.png"/></div></div></figure><p id="3607" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">注意，我们的模型已经生成了最初在数据集中没有的密码(Ahyper777！，Apass10$ …。)!</p><p id="6b22" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">您可以在下面的 Google Colab 链接中找到生成式朴素贝叶斯模型的相关代码:</p><div class="pb pc gp gr pd pe"><a href="https://colab.research.google.com/drive/1oUEPz7Kl7TqgWHpfSHPU4NY4CCs3aZlU#scrollTo=aWdPKcXXlVl7" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd ja gy z fp pj fr fs pk fu fw iz bi translated">谷歌联合实验室</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">多项式朴素贝叶斯——生成模型</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">colab.research.google.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps ky pe"/></div></div></a></div><h2 id="1045" class="na lg iq bd lh nb nc dn ll nd ne dp lp mg nf ng lr mk nh ni lt mo nj nk lv iw bi translated">旁注—可选阅读</h2><p id="dc93" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">当然，为了使文章尽可能简洁，有些细节被省略了，但我会在这里提到它们，以供参考，如果你想自己查找的话。</p><p id="9dba" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">例如:如果一个元素没有出现在初始数据集中，会发生什么？假设“hyper”这个词在之前的密码中没有使用过？那么 MLE 将返回 0 概率，因此没有机会由模型生成。为了缓解这个问题，有平滑技术，如<strong class="lz ja">拉普拉斯或李德斯通平滑。</strong></p><p id="71f9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">此外，我们只讨论了生成数据，而没有讨论分类，因为这些帖子的概念是生成模型。朴素贝叶斯也可以通过使用<strong class="lz ja">贝叶斯定理、</strong>作为分类器，但为此，要求数据点有标签。</p><h1 id="1403" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">转到第 2 部分</h1><p id="cd08" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">恭喜你！您刚刚从头开始创建了一个真正的概率生成模型！当然，这是一个非常简单的方法，但是希望你明白了！</p><p id="2ac8" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">回想一下，这仅仅是因为我们依赖于朴素贝叶斯独立性假设，这很好！在某些情况下，它工作得非常好，但在<a class="ae le" href="https://medium.com/@apogiatzis/demystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46" rel="noopener">下一部分</a>中，我们将探索当这个假设崩溃时会发生什么，以及深度学习如何能够拯救我们！</p><h1 id="0b10" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">参考</h1><p id="3265" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">[1] B. Hitaj，P. Gasti，G. Ateniese，F. Perez-Cruz，<a class="ae le" href="https://arxiv.org/abs/1709.00440" rel="noopener ugc nofollow" target="_blank"> PassGAN:密码猜测的深度学习方法，</a><em class="nl">计算机科学中的应用密码学和网络安全讲义</em>，217–237 页，2019。</p><p id="69d6" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">[2] Ng、Andrew Y 和 Michael I. Jordan。判别分类器与生成分类器:逻辑回归和朴素贝叶斯的比较。<em class="nl">神经信息处理系统的进展</em>。2002.</p><p id="0ea9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">[3]福斯特 D. <em class="nl">生成性深度学习:教机器画画、写字、作曲、演奏</em>。第一版。奥莱利；2019.</p></div></div>    
</body>
</html>