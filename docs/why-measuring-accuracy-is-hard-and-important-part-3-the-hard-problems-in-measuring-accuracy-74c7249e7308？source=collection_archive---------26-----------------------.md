# 为什么测量精度很难(也很重要！)第三部分。

> 原文：<https://towardsdatascience.com/why-measuring-accuracy-is-hard-and-important-part-3-the-hard-problems-in-measuring-accuracy-74c7249e7308?source=collection_archive---------26----------------------->

## 测量精度中的难题。

这是讨论测量精度主题的系列文章的第三篇。

在本系列的第 1 部分中，我们讨论了正确测量精度非常重要的原因。点击这里查看:[https://www . electric brain . io/blog/why-measuring-accuracy-is-hard-and-very-important-part-1](https://www.electricbrain.io/blog/why-measuring-accuracy-is-hard-and-very-important-part-1)快速 TLDR:测量准确度很重要，有两个主要原因。首先，它让我们可以让我们的模型变得更好——并且知道它们目前有多好。第二，它允许我们与利益相关者交流他们对模型的期望。

在本系列的第 2 部分中，我们讨论了影响测量精度的最常见和最典型的问题。如果你还没有读过，请点击这里查看:[https://www . electric brain . io/blog/-why-measuring-accuracy-is-hard-and-important-part-2](https://www.electricbrain.io/blog/why-measuring-accuracy-is-hard-and-important-part-2)快速 TLDR:有一大堆事情阻碍了测量准确度，从我们希望我们的模型如何表现的复杂预期，到它预期操作的数据的复杂性质，再到我们必须将多种算法拼凑在一起以形成一个完整系统的复杂方式。

在这篇文章中，即本系列的第 3 部分，我将讨论在尝试度量模型的准确性时出现的更不寻常、有趣和困难的场景。

# 您的模型可能没有任何实际数据

一般来说，这可能是机器学习中最难处理的场景。你可能会问，没有任何地面真实数据，你怎么可能训练一个机器学习算法？难道用于训练算法的数据不能用于地面实况吗？

如果你仔细想想，有很多情况下，我们在没有实际数据的情况下应用机器学习:

*   当执行多种类型的无监督学习时，例如聚类或推荐引擎。对于什么集群或什么建议是正确的，没有正确的答案，只有各种更好或更差的衡量标准
*   当我们从头开始构建新产品时，使用合成数据来引导系统，但在发布后基于真实世界的数据构建数据集。对于算法所应用的数据来说，合成数据不是真实的地面事实。
*   在许多数据科学流程中，例如排列两个不同的噪声数据集，例如基于公司和品牌名称匹配两个不同的数据库。对于两个数据集之间的正确比对，我们可能没有任何基本事实。建立一些基本事实也许是可能的，但是过于昂贵或耗时，以至于组织觉得不值得。在这种情况下，模型的要点是避免手动排列条目
*   我们可能正在从系统的中间步骤训练我们的机器学习算法，但输出然后经过一系列后处理算法，将它转化为最终结果。我们不知道最终结果。
*   当进行强化学习时，比如在模拟环境中训练机器人人工智能，然后试图将这些知识转移到真正的机器人身上。
*   我们可能会大量使用迁移学习来构建我们的产品
*   我们正在建立一个不涉及机器学习的管道，例如，使用传统的计算机视觉技术来建立一个人跟踪算法。在你的环境中，标记任何大量的数据可能太昂贵，或者当你的 R&D 预算更大时你会做的事情。现在，你只需要一个看起来可以工作的原型。

你在机器学习上工作得越多，你就越会在没有你需要解决的问题的真实基础的环境中工作，或者至少它是如此之少以至于几乎没有用处。

# 可能没有现成的度量标准来有效地衡量您的模型的准确性

当你在人工智能和数据科学领域做真正开创性的工作时，你可能会遇到不太清楚如何衡量模型性能的情况。这并不是说性能不可测量，而是可能有多种看似同样有效的方法来测量性能，但对结果却有不同的说法。可能有隐藏在研究和文献中的模糊的度量和测量，或者甚至一些你不熟悉的更常见的度量。

我发现这种情况出现在一些不同的环境中:

*   你正在将一种最近建立的机器学习方法带入一个全新的领域:例如，生成-对抗-网络，图像生成 GAN 通常由一种称为初始分数的东西来衡量，它使用标准的分类类型图像网络来检查它们的行为，并结合熵的度量来得出分数。现在，如果你将甘的应用于建筑、3d 模型或音乐的蓝图，可能没有现成的分类模型可以用来形成类似盗梦空间的乐谱
*   你的产品可能有一种独特的输出/行为形式，这种形式是新颖的，在研究社区中并没有真正建立起来。例如将夜间图像变成白天图像的神经网络。你是如何衡量的？
*   你可能不是你工作的特定领域的专家。可能确实存在一些度量标准，但是如果没有这个狭窄研究领域的深厚专业知识，你永远不会知道它们
*   您的机器学习类型的常用指标可能有与您的算法相关的特定弱点，这意味着它将对与您相关的重要失败案例给予较低的权重

我会告诉你这是我最近想到的，我觉得有必要真正设计一个指标。我在开发一个分析简历的系统。该系统的基本前提如下:

1.  用户上传 1000 份简历
2.  用户浏览 20-30 份简历，给每个候选人一个“喜欢，也许，不喜欢”
3.  该算法根据你教给它的喜欢/不喜欢的偏好，对其他 970 份简历进行排序和排名

构建和测量这个系统面临几个挑战。首先是构建适当的测试用例，我们可以用它来精确地测量系统的结果。第二是挑选什么样的度量标准和形式有意义。第三是提高成绩。

我们构建测试用例的方式是创建一组算法应该找到的“目标”简历。然后，我们会为每份目标简历添加 25-50 份“负面”简历。底片是从整个数据集中随机抽取的。然后，我们会像任何筛选一样检查测试案例，审查 20-30 份简历，其中只有几个目标是喜欢的，并查看系统如何对剩余的简历进行排名。

由于不熟悉信息检索研究，我真的不知道这里应该使用什么度量标准。一个衡量标准可能存在，但我不想深究并找出答案。另外，这个系统是基于喜欢/不喜欢，而不是关键字匹配，更像是一个推荐系统。在我的脑海中，有一些最初的想法:

*   在较大的简历排序中，目标简历的平均“排名顺序”。较低的排名意味着更接近顶端，这正是我们想要的。它有一个很大的问题——它根据目标与否定的比率以及目标的总数而变化。
*   人们可以做到以上所述，但是将等级顺序标准化在 0 和 1 之间。这个度量是一个改进，但是如果目标与负面的比率发生变化，它仍然会受到阻碍，这在真实环境中是肯定会发生的。它也没有明确的最佳值——多个目标的平均排名永远不会是 0，因为它们也是相互排名的
*   相反，我们可以衡量为了让所有目标排名靠前，你需要浏览的简历的最少数量。这将直接从用户的角度构建衡量标准——与手动浏览整叠简历相比，我从这种机器学习算法中获得了多少价值。
*   我们也可以用准确度、精确度和回忆来表示分数。例如，我们会衡量前 10 名、前 20 名或前 30 名(甚至前 10%)的目标简历数量。这个指标的优点是对数据集大小和比率的一些变化有一定的不变性，但是我不喜欢引入额外的问题，为 Top-K 指标选择什么 K。
*   我的另一个想法是，最终我们可能会根据算法的准确性来衡量它，根据以前的简历来预测用户是否喜欢/不喜欢一份简历。但是我们没有足够的数据来训练这种类型的机器学习算法——更不用说测量它了。此外，它并不能真正代表我们试图解决的问题，也不能代表用户界面中显示结果的方式。用户界面中的结果是分级的，而不是分组的。

所以最终我不得不自己想出一个度量标准应该是什么样子。它必须捕捉算法的能力，仅基于少量的学习将目标简历放在堆的顶部，它必须对测试案例的大小和目标与否定的比率不变，如果可能的话，它需要抗噪声。

因此，我将我想要的度量的数学属性公式化如下:

*   如果所有目标简历的排名都高于所有负面，则该值为 1.0
*   如果所有负面简历的排名都高于所有目标，则该值为 0.0
*   该值介于其他所有值之间
*   该值尽可能不随目标与负数的比率以及测试用例的总大小而变化。

因此，我得出的公式如下(请原谅我的 pythonic 数学):

*   归一化等级[Nr] =等级/总简历数
*   bestyposablemeanrank[Bpmr]= average(Nr(Rank)表示 Rank in range(NumberOfTargets))
*   worstpossible mean Rank[Wpmr]= average(Nr(total resumes-Rank)表示 Rank in range(NumberOfTargets))
*   mean rank[Mr]= average(Nr(target rank)for target rank in Targets)
*   得分= 1.0 — ( (Mr — Bpmr) / (Wpmr — Bpmr))

该等式基本上使用目标简历在简历总体排名中的平均排名。这些等级首先被标准化为 0 到 1 之间，然后所有目标的平均等级被标准化为介于最佳可能平均等级和最差可能平均等级之间。最好的平均排名实际上不是 0——因为目标简历也是相对于彼此排序的。最好的平均排名是所有目标简历都排在列表顶部时的排名。因此，如果 10 份简历中有 3 份目标简历，那么最佳可能平均排名是 0.0、0.1 和 0.2 的平均值，整体最佳可能平均排名是 0.1。对最差可能排名也是如此，测试用例的最终得分是它在最好和最差可能平均排名之间的归一化位置。

有了这个自定义评分公式，我就能够创建一个与我喜欢的其他类型的指标具有相对一致行为的指标，例如，1.0 是可能的最佳结果，0.0 是可能的最差结果。对于目标值与负值之间的比率以及数据集的整体大小的变化，它也具有良好的行为。这允许我们比较不同测试用例之间的分数，然后进一步允许我们计算所有测试用例的平均分数。

像这样设计你自己的度量方程是一种先进的技术，但是当你在一个模型上开发测量时，它应该是你的工具库中包含的东西。

# 可能没有明确的方法来定义模型的准确性

这种挑战通常出现在你进行无监督学习的时候，例如聚类和生成模型，或者一些形式的强化学习。

如果您正在处理一个集群问题，您最终会遇到这样的问题:什么是好的集群？这个问题不明显。您肯定可以使用一些指标来以统计的方式定义什么是更好的或更差的聚类，例如:

*   轮廓系数-测量每个聚类相对于其他聚类的平均中心的平均中心
*   Davies-Bouldwin——测量每个数据点到聚类中心的平均距离与聚类之间的距离

但是使用这些指标可能无法从算法中捕捉到您真正想要的东西。例如，如果在营销环境中部署算法来细分客户，那么您真正想要的是根据特定营销策略对他们的影响程度将他们分组，因为这最终是这些集群的用途。但是，您可能会对非常嘈杂的数据进行聚类，而这些数据对营销策略可能只有很小的预测价值。你在那里开发任何有价值的东西，然后继续前进。像上面这样的措施可能会完全误导你开发这种算法来实现这一目的——创建有效的数据聚类，而不是在任何程度上与有效营销策略中的相似性相一致的数据聚类。

另一个有趣的例子是生成模型。我们关心的是模型是否能够产生健康多样的现实输出。但是现实的定义是什么？很难根据它被训练的数据集来衡量算法——如果算法只产生与其训练数据集高度相似的输出，那么它将是一个非常糟糕的生成模型。目前用于此的技术，如 Inception-score(使用神经网络和熵的统计测量来对模型进行评分)，是非直观的，并且有缺陷。在某些情况下，如在自然语言合成中，评估改进的最可靠技术仍然是由人类进行的主观评分，例如从 1 到 10 的质量评分。你还会如何评价一个通用聊天机器人能和你聊得多好？

# 你真正关心的结果是不容易衡量的

在这种情况下，我们可能有一些方法来衡量我们的模型的准确性，并且我们有合理的期望我们的度量应该与模型的真实世界性能相关，在某种意义上，更高的度量可能意味着更好的真实世界性能。但是模型在现实世界中的表现可能不是你实际上可以容易地测量和报告的。可能很难确定您的指标与现实世界的表现有多相关，以及您损失了多少准确性。

我能想到很多可能发生这种情况的场景:

*   您在边缘部署模型，例如在物联网设备中，这些设备不可能上传它们正在咀嚼的所有数据
*   您部署在隐私受限的环境中，如安全摄像头，在那里您无法真正将数据传回总部以测量准确性
*   您最关心的是模型在边缘情况下的性能，例如在危险驾驶条件下运行的自动驾驶车辆，或者经济衰退时股票交易模型的性能。你关心的边缘情况并不经常发生
*   你可能正在为迁移学习建立一个模型，例如，旨在作为预先训练好的网络运行的模型，然后将这些模型粘合到最终用户系统中并应用于他们的问题。您关心模型如何适应新的和不可预见的问题，根据定义，这些问题是您无法度量的
*   您正在开发一个通用算法，该算法在您无法访问的客户端环境中的客户端数据上进行训练和运行。同样，你关心算法在不可预见的情况下工作得如何，包括训练和执行
*   你的算法必须将总部的培训元素与客户的私人数据结合起来，才能产生一个结果

像这样的一些最困难的情况是，你的算法可能需要几年的时间来解决好的或坏的选择。例如，如果一个算法预测你应该买一栋特定的房子或一支特定的股票，因为从长期来看它似乎被低估了。根据这个问题的定义，可能需要数年时间来确定算法的猜测是否会变成现实。

# 有效地测量精确度在计算上太昂贵了

在某些情况下，通常有非常合理的方法来衡量算法的准确性。但是，要有效地做到这一点，计算量可能非常大，需要对精确度进行部分测量。

我能想到很多可能出现这种情况的情况:

*   在训练神经网络时测量其验证准确性。为了获得精度如何随时间变化的精确、无噪声的测量，通常必须在分配给验证测量的 CPU 能力和分配给训练测量的 CPU 能力之间进行折衷
*   您的数据集如此之大，以至于您不可能对所有数据集进行训练甚至测量准确性，而是必须对数据样本进行训练(例如大数据中常见的样本)
*   算法的生产环境与它的训练方式不同。例如用于一次性学习和图像搜索的连体/三元网络。以可预测的缩放特性分批训练神经网络。但是在生产中，神经网络使用 K-最近邻来产生与非常大的数据库中的其他向量相匹配的向量。这是您关心并想要度量的完整数据库的最终结果，但是大量进行这些查找可能会非常昂贵
*   你的算法噪音很大，计算量也很大。如果在一个已经很贵的模型上不增加 5 倍的计算时间，5 重交叉验证将是你减少测量中噪声的常用技术。

在这些情况下，我们必须非常小心地从较大的生产数据集中抽取数据，以便测量我们的算法。我们必须选择一个合适的样本大小，该样本大小在计算上是易处理的，但也是整体的代表，我们必须以一种考虑到较大数据集中各种可能的类别不平衡的方式进行随机选择。

# 你的算法可能与人类协同工作

在测量精确度方面，可能最良性的难题是当一个算法要与人类结合使用时。总的来说，我们真正关心的是人机结合系统的有效性——这才是最重要的。衡量机器方面可能很容易——衡量人的方面通常要昂贵得多。

想想下面的情况:

*   人工智能供电的安全摄像头，旨在向安全人员触发警报。这些系统通常倾向于发出假警报，以确保安全人员捕捉到每一个可能的入侵。但是如果你给了太多的假警报呢？警卫可能会习惯他们，并开始忽略他们。在太多的假警报和太少的假警报之间有一个最佳的平衡，这使得整个系统在捕捉入侵者方面的效率最大化。
*   许多人正在研究人工智能医生，帮助人们诊断。许多团队不只是取代医生，而是采取这样的方法，即人工智能应该与真正的医生协同工作，以便诊断病人。现在你关心的是医生-人工智能团队的有效性，而不是仅仅一个医生或仅仅一个人工智能。人工智能的建议只是迷惑医生，还是真正帮助他们诊断情况。
*   许多人工智能团队经常使用他们来自人工智能系统的输出来指导人类的数据标签，这些人类正在建立训练人工智能系统的数据集。例如，当构建一个文档解析系统时，我们有一个人工智能来解析文档，然后人类会检查人工智能做了什么并纠正任何错误。虽然这对人类来说使事情变得更快，但它也导致了自满，因为系统在识别文档方面变得越来越好。人类将无法捕捉错误，批准错误填充的文档进入数据集，然后对其进行训练，导致人工智能在未来犯更多的错误。

如果方法正确，人类在解决方案中的存在通常会比单独进行机器学习带来更好的性能。通常，当开发新的人工智能产品时，让人类参与进来以增加数据集是非常必要的。但是在这种情况下，人类的存在确实使准确性的测量变得复杂。

人类很难衡量。例如，我们如何衡量数据集中有多少噪音和错误？当数据集由人构建时，唯一的选择是将一个人的意见与另一个人的意见进行比较。例如，管理人员可能会检查其下属的标记数据子集，并将其与他们自己对正确分类的意见进行比较。

在极端情况下，这种技术变成了我所说的双重验证，同样的数据被送到两个不同的人那里进行分析和分类。如果两个人意见不一致，它可以被发送给第三个人，第四个人，第五个人，等等，直到就正确答案达成共识。这不仅可以生成高质量的标注数据，还可以为您提供数据集中错误数量的清晰统计数据。但是这个过程是昂贵的——大大增加了已经昂贵的数据标签过程的成本。

# 您的数据集或问题空间可能会随着时间的推移而变化和发展

生活中有许多事情是一成不变的，或者至少演变得如此缓慢，以至于需要几代人的时间才能改变。从一年到下一年，有时甚至从一个月到下一个月，还有其他事物在特征和形式上迅速变化。数据集和机器学习问题也是如此。

以下是一些我们可能会发现我们正在解决的问题正在我们脚下演变的情况:

*   你是一家新的创业公司，你的数据集是在你成长的同时建立的。您可能从英语语言数据开始，但是由于数据的增长，数据集开始包括许多其他语言
*   你是一个聊天机器人，你不断地添加新的特性和功能，同时对你的核心 NLP 进行研究。数据集在种类和形式上都在扩展，需要更多的功能，因此，在具有 10 个标签的小数据集上工作的算法在具有 100 个标签的大数据集上可能不再获得好的结果。
*   你在 21 世纪初做电子商务，互联网上的消费者行为和我们跟踪消费者行为的复杂性随着时间的推移在迅速发展和变化，2005 年相关的趋势到 2010 年完全不相关。
*   你正在为股票市场的高速交易做一个算法，你在与一组进化中的其他公司的算法竞争，这些算法也在做同样的事情，每个算法的最佳行为由其他算法的行为决定

在这些情况下，有时有必要不断地将 R&D 应用到您的核心算法中，只是为了停留在同一位置，更不用说改进它了。随着你使用机器学习的雄心的增长，你的数据集也随之增长，维护这样一个雄心勃勃的系统的挑战也在增加。测量准确性的主要用途是了解您的系统在现实世界中的工作情况，这受到了“现实世界”不断变化的定义的挑战。在事情发生变化和准确度下降之前，你的准确度测量可能只有几个月是好的。

# 有多个维度需要你的算法去归纳

这个概念是我最近在考虑对来自复杂数据库的数据进行机器学习时产生的，这些复杂数据库中有多个与预测相关的表，数据集相当小，因此泛化很重要。在思考了这个问题之后，我开始意识到它实际上已经在我过去的一些不同的项目中出现过。

考虑下面的例子。您正在构建一个算法来解析文档中的数据。您正在解析的文档有许多不同的总体格式，在这些总体格式中，文档中可能有不同的数据。在您的训练数据集中，您可能积累了 50 种不同格式的文档，每种格式的文档都有大约 500 个示例。你有理由相信，最多有 100-200 种格式“存在”，其中大多数至少与现有格式相似。

现在，如果您只是对这些数据进行传统的 80/20 分割，您可能会获得非常好的结果，假设您获得了完美的训练准确性和 99%以上的验证准确性。在这种情况下这是非常可能的(已经发生在我身上)，特别是如果每种格式呈现的数据非常一致，并且你有很好的数据扩充。您可以根据验证的准确性将您的模型投入生产。但是当你收到一个 51 格式的文件时会发生什么呢？

事实上，您的算法需要沿着两个维度进行归纳，而第二维度并没有被所有样本的基本 80/20 分割所捕获。您的算法首先需要能够从相同格式的新文档示例中学习。那是维度 1。不过，理想情况下，您的算法也可以推广到新格式的文档。那就是次元 2。如果您有文档格式的标签，您也可以按照不同的格式进行 80/20 分割(在本例中为 40/10)。这两种划分都很重要，并且告诉你不同的东西，关于你的模型归纳到新例子的能力。

如果您对格式进行 80/20 分割，您的验证准确性可能与 99%大相径庭，这并不奇怪。如果一种格式中的文档彼此高度相似，但格式彼此非常不同，那么当机器学习算法只接触 40 种现有格式的文档时，无论数据集有多大或有多少文档，它都无法推广到新格式就不足为奇了。

我想得越多，就越意识到这个问题发生的频率有多高。例如，如果您正在为一个电子商务平台构建一个推荐引擎，您的算法需要在一系列维度上进行归纳:

*   跨系统的不同用户，例如，概括为在训练数据中没有确切看到的行为的新用户
*   目录中的不同项目，例如，当它们被添加到系统中时，需要归纳为新的项目和描述
*   涵盖全新类别的商品，使用现有商品目录中未使用的词汇和短语
*   沿着时间维度——例如，它需要随着用户行为和产品趋势的变化和发展而概括。它还需要具有前瞻性，例如，在给定用户先前行为的情况下，它可以预测用户的未来行为。你上个月推荐的不一定是这个月最好的推荐。

如果您正在训练一个来自复杂数据库的数据的算法，包括连接一堆不同的表，这些表实际上代表不同类型的数据，那么非常合理的做法是对这些表中的每一个进行 80/20 分割，每一个产生不同的度量，或者它们的某种组合。例如，设想一个房地产估价系统，该系统必须将地理数据(如特定地区的市场状况)与特定房产的数据以及该地区最近相关销售交易的数据相结合。您需要该算法对所有三个不同的数据源进行归纳，因为所有三个数据源都是在生产环境中混合和匹配进行预测的，并且所有三个数据源都随着业务的增长而不断增长和变化。模型需要能够适应。

这是一个需要处理的不寻常且微妙的场景，它使准确性的测量变得复杂。无论你用哪种方法做 80/20 分割，没有一种方法能完全捕捉到算法的准确性和行为。您的算法可能很好地概括了您测量的维度，例如预测房地产估价时数据集中的不同建筑物，但不擅长概括不同的维度，例如市场条件不同的不同地理区域。在这些情况下，为了充分理解算法的行为，您总是需要多个度量。

# 结论

在本文中，我们讨论了许多更具挑战性和微妙的问题，这些问题阻碍了有效地测量准确性。这些挑战中有许多是从机器学习的力量中固有地衍生出来的——它可以灵活地应用于不完美的数据和新的环境和情况，并仍然可以获得合理的结果。机器学习是强大的。

如果您一直读到这一点，特别是如果您一直在阅读我们在本系列中以前的文章，那么您可能会像我一样得出结论，测量准确性非常困难。有这么多的方法可以让你做得不正确，有这么多的问题可以阻止你做好它，很容易接受错误是生活中不可改变的事实。然而，作为模型的构建者，测量准确性是我们工作的基础。因此，除了翻身接受失败，我们还能做些什么呢？

在本系列文章的第 4 部分，也是本系列文章的最后一部分，我将尝试介绍一个更好地测量准确性的框架。这个框架将试图给你一个更好的方法来思考如何为你的模型选择和设计度量标准。我们的提议很简单。我们不是从您的模型和它的作用开始，而是走向可以衡量它的指标，然后分析该指标，而是向您展示如何反向进行。从你到底希望你的度量标准告诉你什么关于你的模型，以及你想如何分析它的问题开始，然后向后移动到什么样的度量标准可能支持告诉你这些信息。指标是为一个目的服务的，因此不同的指标必须为不同的目的而设计——没有放之四海而皆准的解决方案。

*原载于*[*www . electric brain . io*](https://www.electricbrain.io/blog/why-measuring-accuracy-is-hard-and-important-part-3)*。*