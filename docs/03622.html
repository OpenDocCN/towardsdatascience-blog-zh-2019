<html>
<head>
<title>Kernel Secrets in Machine Learning Pt. 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的核心秘密。一</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/kernel-secrets-in-machine-learning-2aab4c8a295f?source=collection_archive---------5-----------------------#2019-06-09">https://towardsdatascience.com/kernel-secrets-in-machine-learning-2aab4c8a295f?source=collection_archive---------5-----------------------#2019-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f6816a6d021155e81ac5fcfea74568b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q70-tqozKAXwBa5ztRM_rg.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="ef78" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">这个帖子不是关于深度学习的。但也可能是无妨的。这就是内核的力量。它们普遍适用于任何机器学习算法。为什么你会问？我将在这篇文章中尝试回答这个问题。</h2></div><p id="39c1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">一般在机器学习中，我们要把相似的东西放在相似的地方。这条规则适用于所有的机器学习，无论是监督的、非监督的、分类的还是回归的。问题是，我们如何确切地确定什么是相似的？为了阐明这个问题，我们将从学习内核的基本基础开始，点积。</p><p id="61a0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">两个向量之间的点积是一件神奇的事情。我们可以肯定地说，它在某种意义上衡量了相似性。通常，在机器学习文献中，点积表示如下:</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/02d3c6218b842244cfd16ab05dcc0100.png" data-original-src="https://miro.medium.com/v2/resize:fit:140/0*fpfFK_SyUrk63qf7"/></div></figure><p id="c6ca" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">表示向量 x 和 x’之间的点积。注意，为了简洁起见，我省略了向量符号的箭头。这个符号是矢量分量乘积之和的简写:</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/89c82d4f0141dbdb3f2e1ba8254d4a57.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/0*3eBJ_e_dfLwQ1NdN"/></div></figure><p id="2fe2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">巧合的是，向量的范数是与其自身的点积的平方根，表示为:</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/d82fe80c73816d889994e723de3cd643.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/0*n83QvuFVag_sctK3"/></div></figure><p id="6b18" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这当然不是全部。我们还知道余弦法则，即点积等于向量之间夹角的余弦乘以它们的范数(这很容易通过简单的三角学来证明):</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/8335f7f5e204d31277436e9f276cac03.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/0*A3S5vRcg4-K3dUCR"/></div></figure><p id="9411" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">谈论角度和范数的好处是，我们现在可以想象这个点积是什么意思。让我们画出这两个向量，它们之间的夹角为α:</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lx"><img src="../Images/12598d03b20d55ce7c41ddfc01803755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mhphA8mnAE4XE12hOeDpnA.png"/></div></div></figure><p id="d99a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">所以如果我们用点积来衡量相似性。什么时候会达到最大值？意味着向量是最相似的。显然，当余弦等于 1 时，会发生这种情况，当角度为 0 度或弧度时会发生这种情况。如果向量各自的范数相同，那么显然我们说的是同一个向量！还不错。让我们把到目前为止学到的东西刻在石头上:</p><blockquote class="ly"><p id="a1c8" class="lz ma je bd mb mc md me mf mg mh lo dk translated">点积是向量之间相似性的度量。</p></blockquote><p id="917f" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">现在你有希望理解为什么讨论点积是有用的了。</p><p id="7a6c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当然，点积作为相似性的度量，在问题中可能有用，也可能完全无用，这取决于您要解决的问题。因此，我们需要对输入空间进行某种变换，使点积作为相似性的度量变得实际有用。我们用ϕ.来表示这种转变现在，我们可以定义一个<strong class="kv jf">核的含义，</strong>映射空间中的点积:</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/b4a16530b9870657226d53f2a321d366.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/0*zV4aXaC5Y5Hwo8_g"/></div></figure><p id="397e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因此，核的定义非常直接，它是映射空间中相似性的一种度量。事实是，数学家喜欢具体化。对于它们所处理的底层函数和空间，不应该有隐含的假设，因此函数分析的内核背后有相当多的理论，这需要另一篇或几篇文章来讨论。简而言之，我们需要明确说明我们希望ϕ:发挥什么样的作用</p><blockquote class="ly"><p id="9a30" class="lz ma je bd mb mc md me mf mg mh lo dk translated">我们想要一个函数，它从定义域 X 映射到一个点积定义明确的空间，这意味着它是一个很好的相似性度量。</p></blockquote><p id="a9cb" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">内核可以作为任何可以用点积(或范数)来定义的算法的一般化。使用核作为其主干的算法的最著名的例子是支持向量机和高斯过程，但是也有核用于神经网络的例子。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mn"><img src="../Images/b7dfaeef98e3f05bd5e37eedeff6f9b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2CRc9ZAWTXxNpIcK"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Photo by <a class="ae ms" href="https://unsplash.com/@anniespratt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Annie Spratt</a> on <a class="ae ms" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="609a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们需要内核和映射函数ϕ的另一个原因是输入空间可能没有明确定义的点积。让我们简短地研究一下<strong class="kv jf">文档分析</strong>的例子，我们只是想根据主题得出两个文档之间的相似性，然后对它们进行聚类。在这种情况下，这两个文档之间的点积到底是多少？一种选择是获取文档字符的 ASCII 码，并将它们连接成一个巨大的向量——当然，这不是你在实践中会做的事情，但这是相当值得思考的。很高兴我们现在把文档定义为向量。但是问题仍然在于长度，即不同的文件有不同的长度。但是没什么大不了的，我们可以通过用 EOS 字符将较短的文档填充到一定的长度来解决这个问题。然后我们可以在这个高维空间里计算一个点积。但是，还有一个问题，这个点积的相关性，或者说，这个点积实际上意味着什么。显然，字符的微小变化都会改变点积。即使我们把一个词和它的同义词交换，它也会改变点积。这是您在按主题比较两个文档时想要避免的事情。</p><p id="8dbb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">那么内核是如何发挥作用的呢？理想情况下，您会希望找到一个映射函数ϕ，该函数将您的输入空间映射到点积具有您想要的含义的特征空间。在文档比较的情况下，对于语义相似的文档，点积较高。换句话说，这种映射应该使分类器的工作更容易，因为数据变得更容易分离。</p><p id="8f0c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们现在可以看看典型的 XOR 例子来理解这个概念。XOR 函数是一个二元函数，看起来像这样:</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mt"><img src="../Images/ad701bc948a3fdc75dd80d9bfc595270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSbFVGUTSqUANDWjvFhzLw.png"/></div></div></figure><p id="d36c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">蓝色点被分类为 0，红色点被分类为 1。我们可以假设这是一个噪声异或，因为集群有一个大的传播。我们马上注意到一件事，数据不是线性可分的。也就是说，我们不能在红点和蓝点之间划一条线，将它们分开。</p><p id="6920" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这种情况下我们能做什么？我们可以应用一个特定的映射函数，使我们的工作容易得多。具体来说，让我们构造一个映射函数，该函数将对穿过红点簇的线周围的输入空间进行单侧反射。我们将围绕这条线来反映这条线下面的所有点。我们的映射函数会有以下结果:</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mu"><img src="../Images/c368aa585b533876f54c60af1bb1ee5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WHD7oq7v9pgFu4QY20MAJA.png"/></div></div></figure><p id="e49f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">映射后，我们的数据变得很好地线性分离，所以如果我们有一个模型试图适应一个分离的超平面(例如感知器)，这是一个理想的情况。显然，线性可分性是一件非常好的事情。但是为了建立有效的模型，我们不一定需要线性可分性，这意味着为了建立有效的模型，不是所有的映射函数都需要导致线性可分的数据。</p><p id="47f3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">人们经常混淆应用内核和应用映射函数的概念。核函数的输出是一个标量，是两点的相似性或不相似性的度量。映射函数的输出是一个向量，我们基于它来计算相似性。关于内核有趣的事情是，我们有时可以计算原始空间中映射的点积，而不需要显式地映射输入。这允许我们处理无限维映射！这是一件很难理解的事情，所以我将在以后的文章中讨论。</p><p id="05f9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">作为结束语，我想推荐 Smola 和 Schoelkopf 的书:<a class="ae ms" href="http://agbs.kyb.tuebingen.mpg.de/lwk/" rel="noopener ugc nofollow" target="_blank">用内核学习</a>。这本书对核心机器及其理论背景进行了全面的论述。除此之外，请继续关注内核！</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mv"><img src="../Images/9e2465f67af0b5ea4d8a416f5c79c7dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5g1Bnv1V9P5Kebn2"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Photo by <a class="ae ms" href="https://unsplash.com/@rexcuando?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Eric Nopanen</a> on <a class="ae ms" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="08d5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">直接相关的文章/推荐阅读:</p><ol class=""><li id="da06" class="mw mx je kv b kw kx kz la lc my lg mz lk na lo nb nc nd ne bi translated"><a class="ae ms" rel="noopener" target="_blank" href="/on-the-curse-of-dimensionality-b91a3a51268">关于维度的诅咒</a></li><li id="4ce3" class="mw mx je kv b kw nf kz ng lc nh lg ni lk nj lo nb nc nd ne bi translated"><a class="ae ms" rel="noopener" target="_blank" href="/kernel-secrets-in-machine-learning-pt-2-16266c3ac37c">机器学习中的内核秘密 Pt。2 </a></li></ol></div></div>    
</body>
</html>