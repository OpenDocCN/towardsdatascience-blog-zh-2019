<html>
<head>
<title>Reward Engineering for Classic Control Problems on OpenAI Gym |DQN |RL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI 健身房经典控制问题的奖励工程|DQN |RL</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/open-ai-gym-classic-control-problems-rl-dqn-reward-functions-16a1bc2b007?source=collection_archive---------15-----------------------#2019-11-17">https://towardsdatascience.com/open-ai-gym-classic-control-problems-rl-dqn-reward-functions-16a1bc2b007?source=collection_archive---------15-----------------------#2019-11-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9355" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">自定义奖励功能，加快学习速度！</h2></div><p id="e376" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我通过尝试在<a class="ae le" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>健身房解决问题开始学习强化学习。我特别选择了<a class="ae le" href="https://gym.openai.com/envs/#classic_control" rel="noopener ugc nofollow" target="_blank">经典控制</a>问题，因为它们是力学和强化学习的结合。在这篇文章中，我将使用深度 Q 网络(DQN)展示选择一个合适的奖励函数如何导致更快的学习。</p><h1 id="e63c" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated"><strong class="ak"> 1。扁担</strong></h1><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/51d52486773254661bee309eb1a5a88e.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/1*bthNFouB07_-ZXzw1N0SSw.gif"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Episode 40 (unbalanced)</figcaption></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/42aff9896069229d63bf2a164b889fe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/1*zhT96vNZQ5sLafMYoUrNnw.gif"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Episode 60 (balanced)</figcaption></figure><p id="169f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是 OpenAI 健身房上最简单的经典控制题。极点保持平衡的每个时间步长的默认奖励值是 1。我将这个默认奖励改为与磁极角度绝对值的减少成比例的值，这样，它会因为使磁极更接近平衡位置的动作而得到奖励。具有 2 个隐藏层的神经网络，每个隐藏层具有 24 个节点，在 50 集内解决了这个问题。</p><pre class="ly lz ma mb gt mk ml mm mn aw mo bi"><span id="9734" class="mp lg it ml b gy mq mr l ms mt"><strong class="ml iu">#Code Snippet, the reward function is the decrease in pole angle</strong><br/>def train_dqn(episode):<br/>    global env<br/>    loss = []<br/>    agent = DQN(2, env.observation_space.shape[0])<br/>    for e in range(episode):<br/>        temp=[]<br/>        state = env.reset()<br/>        state = np.reshape(state, (1, 4))<br/>        score = 0<br/>        maxp = -1.2<br/>        max_steps = 1000<br/>        for i in range(max_steps):<br/>            env.render()<br/>            action = agent.act(state)<br/>            next_state, reward, done, _ = env.step(action)<br/>            next_state = np.reshape(next_state, (1, 4)) <br/>            <strong class="ml iu"><em class="mu">#Customised reward function</em></strong>                        <br/>            reward = -100*(abs(next_state[0,2]) - abs(state[0,2])) <br/>            agent.remember(state, action, reward, next_state, done)<br/>            state = next_state<br/>            score=score+1<br/>            agent.replay()</span></pre><p id="bf83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里找到完整的代码<a class="ae le" href="https://github.com/msachin93/RL/blob/master/CartPole/cartpole.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/35e91701ecc6533f1cb654e4a701335f.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*3Km-ZKTVvutGrRrLTJrrMg.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Time steps for which the pole stayed balanced in different episodes</figcaption></figure><h1 id="041e" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated"><strong class="ak"> 2。山地车</strong></h1><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/41564fcaf13a5e4e41580a1806b49a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/1*Ysl6O2aFOLmZLoyGg2zkMQ.gif"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Episode 2</figcaption></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/53e3e664908df541c0e1548b04eefaf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/1*cC5nAn_WU0GYjT9Pvd2g4A.gif"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Episode 200</figcaption></figure><p id="bbb2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的目标是到达正确的山顶，汽车将不得不来回到达正确的山顶。为了让网络自己想出这个策略，需要为它提供一个适当的奖励函数。在某些情节中，一旦汽车通过试错法到达目的地，简单地给予积极的奖励，而对所有其他时间步骤给予消极的奖励是行不通的，并且在网络学习最佳策略之前需要相当长的时间。</p><p id="714d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了从谷底到达顶峰，汽车需要获得机械能，因此最佳策略是汽车在每个时间步获得机械能(势能+动能)。所以一个好的回报函数应该是，在每一个时间步，机械能的增加。</p><pre class="ly lz ma mb gt mk ml mm mn aw mo bi"><span id="8917" class="mp lg it ml b gy mq mr l ms mt"><strong class="ml iu">#Code Snippet, the reward function is the increase in mechanical energy<br/>def</strong> train_dqn(episode):<br/>    <strong class="ml iu">global</strong> env<br/>    loss = []<br/>    agent = DQN(3, env.observation_space.shape[0])<br/>    <strong class="ml iu">for</strong> e <strong class="ml iu">in</strong> range(episode):<br/>        state = env.reset()<br/>        state = np.reshape(state, (1, 2))<br/>        score = 0<br/>        max_steps = 1000<br/>        <strong class="ml iu">for</strong> i <strong class="ml iu">in</strong> range(max_steps):<br/>            env.render()<br/>            action = agent.act(state)<br/>            next_state, reward, done, _ = env.step(action)<br/>            next_state = np.reshape(next_state, (1, 2))<br/>            <strong class="ml iu"><em class="mu">#Customised reward function<br/>            </em></strong>reward = 100*((math.sin(3*next_state[0,0]) * 0.0025 + 0.5 * next_state[0,1] * next_state[0,1]) - (math.sin(3*state[0,0]) * 0.0025 + 0.5 * state[0,1] * state[0,1])) <br/>            agent.remember(state, action, reward, next_state, done)<br/>            state = next_state<br/>            agent.replay()  </span></pre><p id="61b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里找到完整的代码<a class="ae le" href="https://github.com/msachin93/RL/blob/master/MountainCar" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="94a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了这个奖励函数，网络可以很快地学习到最佳策略。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/663a717495629651d9a82d4c11d11e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*51HwQxNMW-hmVNJwRFcHrA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">TIme steps taken by the car to reach the flag at different episodes.</figcaption></figure><h1 id="d9c8" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">3.钟摆</h1><div class="ly lz ma mb gt ab cb"><figure class="mx mc my mz na nb nc paragraph-image"><img src="../Images/672c876ee6840672453ee80a2d9e6188.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/1*8dYXW9mrESnp6L9EziHevg.gif"/></figure><figure class="mx mc nd mz na nb nc paragraph-image"><img src="../Images/164e19d2190c80e93b9831a77cfbdaca.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*034TvTFZKeyP0qZvNb1kSA.gif"/></figure><figure class="mx mc nd mz na nb nc paragraph-image"><img src="../Images/54f36ef587712c05c492a395360552fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*SVj8fFSo4KQWwSdr-PZpUg.gif"/><figcaption class="mf mg gj gh gi mh mi bd b be z dk ne di nf ng">Performance of the RL Agent at different episodes (From left: episode 6, episode 40, episode 200)</figcaption></figure></div><p id="e535" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个问题中，我需要摆动一个钟摆，并使用顺时针或逆时针方向的扭矩使它保持平衡。这个问题类似于山地车问题，但是当杆到达倒置位置时，保持杆平衡有额外的困难。<br/>最初，杆的机械能需要增加，但是一旦它获得足够的能量到达倒置位置，任何更多的能量增加将迫使摆锤保持旋转。</p><p id="7d63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最佳策略是以这样一种方式提供扭矩，即不断增加摆的机械能至极限 mg1/2，该极限等于倒立摆的势能。这将确保钟摆到达倒置位置。除此之外，为了保持极点的平衡，应该对倒立位置附近的钟摆位置给予正奖励。</p><pre class="ly lz ma mb gt mk ml mm mn aw mo bi"><span id="060f" class="mp lg it ml b gy mq mr l ms mt"><strong class="ml iu">#Snippet of the code, the reward function is the increase in mechanical energy of the pendulum upto a limit of 5 with an added positive reward for positions close to the inverted pendulum.<br/>for</strong> e <strong class="ml iu">in</strong> range(episode):<br/>        temp=[]<br/>        state = env.reset()<br/>        state = np.reshape(state, (1, 3))<br/>        score = 0<br/>        maxp = -1.2<br/>        max_steps = 1000<br/>        <strong class="ml iu">for</strong> i <strong class="ml iu">in</strong> range(max_steps):<br/>            env.render()<br/>            action = agent.act(state)<br/>            torque = [-2+action]<br/>            next_state, reward, done, _ = env.step(torque)<br/>            next_state = np.reshape(next_state, (1, 3))<br/>            <strong class="ml iu">if</strong> (next_state[0,0]&gt;0.95):<br/>                score=score+1<br/>            <strong class="ml iu"><em class="mu">#Customised reward function</em></strong><br/>            reward= 25*np.exp(-1*(next_state[0,0]-1)*   (next_state[0,0]-1)/0.001)-100*np.abs(10*0.5 - (10*0.5*next_state[0,0] + 0.5*0.3333*next_state[0,2] * next_state[0,2])) + 100*np.abs(10*0.5 - (10*0.5*state[0,0] + 0.5*0.3333*state[0,2] * state[0,2]))<br/>            maxp = max(maxp, next_state[0,0])<br/>            temp.append(next_state[0,0])<br/>            agent.remember(state, action, reward, next_state, done)<br/>            state = next_state<br/>            agent.replay()</span></pre><p id="22f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此找到完整的代码<a class="ae le" href="https://github.com/msachin93/RL/blob/master/Pendulum/pendulum.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="1779" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了在不同事件的 200 个时间步中，钟摆在倒置位置保持平衡的时间。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/ade84f58ef80421e02a78c97fed53186.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*eA0DVMIwSEF74YahaWG6XQ.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Time spent in the inverted position for different episodes</figcaption></figure><h1 id="6066" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">结论</h1><p id="d4b5" class="pw-post-body-paragraph ki kj it kk b kl nh ju kn ko ni jx kq kr nj kt ku kv nk kx ky kz nl lb lc ld im bi translated">强化学习是机器学习以最优方式执行任务或实现目标的过程。关于手头问题的人类直觉可以以设计的奖励函数的形式添加到神经网络算法中。我们考虑的问题中的默认奖励函数是粗糙的，只在完成任务时奖励代理，这使得 RL 代理很难学习。设计的奖励功能使训练过程更快、更直观，便于我们理解。<br/>阅读更多博客<a class="ae le" href="https://smodi93.wixsite.com/msachin/blogs" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div></div>    
</body>
</html>