<html>
<head>
<title>Machine Learning 102: Logistic Regression With Polynomial Features</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习 102:具有多项式特征的逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-102-logistic-regression-with-polynomial-features-98a208688c17?source=collection_archive---------15-----------------------#2019-11-09">https://towardsdatascience.com/machine-learning-102-logistic-regression-with-polynomial-features-98a208688c17?source=collection_archive---------15-----------------------#2019-11-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="296e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习:监督学习</h2><div class=""/><div class=""><h2 id="14a4" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">有非线性成分时如何分类</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/dbfc73a162a586bc13cf9a715c495612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XlKhcreFN5R1gHscELtmHQ.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@harleydavidson?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Harley-Davidson</a> on <a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="li"><p id="da10" class="lj lk it bd ll lm ln lo lp lq lr ls dk translated">数据科学家是摇滚明星！</p><p id="a455" class="lj lk it bd ll lm lt lu lv lw lx ls dk translated">摇滚！</p></blockquote></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="84be" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">在我之前的<a class="ae lh" rel="noopener" target="_blank" href="/machine-learning-101-predicting-drug-use-using-logistic-regression-in-r-769be90eb03d"> ML 101 文章</a>中，我解释了我们如何应用逻辑回归来分类线性问题。在这篇文章中，我想通过包含非线性特征来使事情变得复杂一点。就像现实世界一样，事物盘根错节，杂乱无章。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="3155" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">让我们深入研究一下 r。</p><pre class="ks kt ku kv gt na nb nc nd aw ne bi"><span id="a28f" class="nf ng it nb b gy nh ni l nj nk"># Load the dataset <br/>library(tidyverse)<br/>data5 = read_csv("nonlinear.csv")<br/>require(ggplot2)<br/>qplot(X1,X2,colour = Y,data=data5)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nl"><img src="../Images/a1ae17eeec7439ee0427f34b73595911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OktTNSpU0cKQSEe6yUBAtQ.png"/></div></div></figure><p id="4b91" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">我们可以看到，黑点被蓝点包围着。我们的工作是找到一种 ML 技术来巧妙地分离这两种类型的点。</p><pre class="ks kt ku kv gt na nb nc nd aw ne bi"><span id="4098" class="nf ng it nb b gy nh ni l nj nk"># build a regular logistic regression<br/>glm_5b = glm(Y~X1+X2,data=data5)<br/>summary(glm_5b)</span><span id="c32c" class="nf ng it nb b gy nm ni l nj nk">Call:<br/>glm(formula = Y ~ X1 + X2, data = data5)</span><span id="a641" class="nf ng it nb b gy nm ni l nj nk">Deviance Residuals: <br/>    Min       1Q   Median       3Q      Max  <br/>-0.6944  -0.5504   0.1937   0.3584   0.6213  </span><span id="abc4" class="nf ng it nb b gy nm ni l nj nk">Coefficients:<br/>            Estimate Std. Error t value Pr(&gt;|t|)    <br/>(Intercept)  0.71038    0.05672  12.524   &lt;2e-16 ***<br/>X1          -0.05446    0.02496  -2.182   0.0325 *  <br/>X2           0.04278    0.02708   1.580   0.1187    <br/>---<br/>Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><span id="ddbc" class="nf ng it nb b gy nm ni l nj nk">(Dispersion parameter for gaussian family taken to be 0.2109993)</span><span id="dd67" class="nf ng it nb b gy nm ni l nj nk">    Null deviance: 16.000  on 71  degrees of freedom<br/>Residual deviance: 14.559  on 69  degrees of freedom<br/>AIC: 97.238</span><span id="ea7b" class="nf ng it nb b gy nm ni l nj nk">Number of Fisher Scoring iterations: 2</span></pre><p id="f29b" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">可以看出，常规逻辑回归没有考虑非线性特征，并且表现不佳。</p><p id="644f" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">接下来，让我们将类标签投影到精细采样的网格点上，并在网格上用类标签着色的每个点上绘制预测。</p><pre class="ks kt ku kv gt na nb nc nd aw ne bi"><span id="5e1c" class="nf ng it nb b gy nh ni l nj nk"># grid of points over sample space<br/>gr &lt;- expand.grid(X1=seq(-5, 5, by=0.1), # sample points in X1<br/> X2=seq(-5, 5, by=0.1)) # sample points in X2</span><span id="484e" class="nf ng it nb b gy nm ni l nj nk">#predict class label<br/>probability_pred = predict(glm_5b,gr,type=”response”)</span><span id="7663" class="nf ng it nb b gy nm ni l nj nk"># set the cutoff point at 0.5<br/>class_pred = as.factor(ifelse(probability_pred&lt;=0.5, “0”, “1”))<br/>color_array &lt;- c(“red”, “blue”)[as.numeric(class_pred)] <br/>plot(gr,col=color_array,pch=20,cex=0.25)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/9e996b051ad03e10cb3d2e271475e3f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MgAJ2oB0MDe1iqi-Sf2q7Q.png"/></div></div></figure><p id="1e54" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">从上面的图来看，一个常规的逻辑回归模型并没有那么好。</p><p id="1eb4" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">让我们包括 x1 和 x2 的二次多项式项。</p><pre class="ks kt ku kv gt na nb nc nd aw ne bi"><span id="25f4" class="nf ng it nb b gy nh ni l nj nk">glm_5c =glm(Y~poly(X1,deg=2)*poly(X2,deg=2),data=data5)<br/>summary(glm_5c)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/137c95f0619ec076cdf6540f085a4601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kKku3CvEAfEEGlZsUnttlA.png"/></div></div></figure><pre class="ks kt ku kv gt na nb nc nd aw ne bi"><span id="ddcf" class="nf ng it nb b gy nh ni l nj nk">probability_pred_5c = predict(glm_5c,gr,type=”response”)<br/>class_pred_5c = as.factor(ifelse(probability_pred_5c&lt;=0.5, “0”, “1”))<br/>color_array_5c &lt;- c(“red”, “blue”)[as.numeric(class_pred_5c)] <br/>plot(gr,col=color_array_5c,pch=20,cex=0.25)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/db699a0133fe1506d217678181f5d1d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2fIfZEt2gp4esEc-M9V39A.png"/></div></div></figure><p id="efd4" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">从 ANOVA 输出可以看出，以下变量具有统计显著性:X1、X1、X2、X2、X1*(X2)和(X1) *(X2)。由于二阶项意义重大，我们无法进行简单的线性分类。</p><p id="bd2e" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">此外，从图中可以看出，具有更高项的逻辑斯蒂模型比简单模型表现得更好。</p><p id="1135" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">为了安全起见，让我们尝试一个没有任何交互项的 5 次多项式逻辑模型。</p><pre class="ks kt ku kv gt na nb nc nd aw ne bi"><span id="e9bb" class="nf ng it nb b gy nh ni l nj nk">glm_5d =glm(Y~poly(data5$X1,deg=5)+poly(data5$X2,deg=5),data=data5)<br/>summary(glm_5d)</span><span id="67e8" class="nf ng it nb b gy nm ni l nj nk">Call:<br/>glm(formula = Y ~ poly(data5$X1, deg = 5) + poly(data5$X2, deg = 5), <br/>    data = data5)</span><span id="2bcc" class="nf ng it nb b gy nm ni l nj nk">Deviance Residuals: <br/>     Min        1Q    Median        3Q       Max  <br/>-0.51652  -0.15930  -0.06256   0.17439   0.73943  </span><span id="44c3" class="nf ng it nb b gy nm ni l nj nk">Coefficients:<br/>                         Estimate Std. Error t value Pr(&gt;|t|)    <br/>(Intercept)               0.66667    0.03309  20.145  &lt; 2e-16 ***<br/>poly(data5$X1, deg = 5)1 -0.70547    0.30163  -2.339  0.02264 *  <br/>poly(data5$X1, deg = 5)2  0.94681    0.28791   3.289  0.00167 ** <br/>poly(data5$X1, deg = 5)3  0.82225    0.28386   2.897  0.00523 ** <br/>poly(data5$X1, deg = 5)4 -0.24777    0.29833  -0.831  0.40948    <br/>poly(data5$X1, deg = 5)5 -0.00171    0.29624  -0.006  0.99541    <br/>poly(data5$X2, deg = 5)1  0.62673    0.28989   2.162  0.03456 *  <br/>poly(data5$X2, deg = 5)2  1.70311    0.30479   5.588 5.69e-07 ***<br/>poly(data5$X2, deg = 5)3 -1.60001    0.29074  -5.503 7.84e-07 ***<br/>poly(data5$X2, deg = 5)4 -0.83690    0.28945  -2.891  0.00531 ** <br/>poly(data5$X2, deg = 5)5  0.88545    0.29321   3.020  0.00369 ** <br/>---<br/>Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><span id="e5b1" class="nf ng it nb b gy nm ni l nj nk">(Dispersion parameter for gaussian family taken to be 0.07885262)</span><span id="f790" class="nf ng it nb b gy nm ni l nj nk">    Null deviance: 16.00  on 71  degrees of freedom<br/>Residual deviance:  4.81  on 61  degrees of freedom<br/>AIC: 33.498</span><span id="a142" class="nf ng it nb b gy nm ni l nj nk">Number of Fisher Scoring iterations: 2</span><span id="7ab7" class="nf ng it nb b gy nm ni l nj nk">probability_pred_5d = predict(glm_5d,gr,type=”response”)<br/>class_pred_5d = as.factor(ifelse(probability_pred_5d&lt;=0.5, “0”, “1”))<br/>color_array_5d &lt;- c(“red”, “blue”)[as.numeric(class_pred_5d)]<br/>plot(gr,col=color_array_5d,pch=20,cex=0.25)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/2cf4c9a95d432351df87bd9a1979ff33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGy9H5ZMa4uT2XbyiAek5A.png"/></div></div></figure><p id="f419" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">五次多项式不会提高性能。</p><p id="0350" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">综上所述，让我们从偏倚和方差权衡的角度来比较所比较的模型。</p><p id="0f38" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">没有交互作用和高阶项的一般 logistic 模型具有最低的方差，但具有最高的偏差。</p><p id="7a6a" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">具有 5 阶多项式项的模型具有最高的方差和最低的偏差。</p><p id="29f1" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">具有二阶多项式和交互项的模型在偏差-方差权衡方面表现最佳。</p><pre class="ks kt ku kv gt na nb nc nd aw ne bi"><span id="44be" class="nf ng it nb b gy nh ni l nj nk"># let's create three additional bootstrap replicates of the original dataset and fit regression models to the replicates.<br/>Boot_sample_5f &lt;- lapply(1:3,function(i)data5[sample(1:nrow(data5),replace = TRUE),])<br/>for (i in 1:3) {<br/> glm_5b = glm(Y~X1+X2,data=Boot_sample_5f[[i]])<br/> probability_pred_5f = predict(glm_5b,gr,type=”response”)<br/> class_pred_5f = as.factor(ifelse(probability_pred_5f&lt;=0.5, “0”, “1”))</span><span id="2794" class="nf ng it nb b gy nm ni l nj nk">#plot class predictions on the grid of values for each of both linear and 5th order models</span><span id="81d3" class="nf ng it nb b gy nm ni l nj nk">color_array_5f &lt;- c(“red”, “blue”)[as.numeric(class_pred_5f)] <br/> plot(gr,col=color_array_5f,pch=20,cex=0.25)<br/>}</span><span id="70d5" class="nf ng it nb b gy nm ni l nj nk"># the 5th order polynomial term.<br/>Boot_sample_5f_2 &lt;- lapply(1:3,function(i)data5[sample(1:nrow(data5),replace = TRUE),])</span><span id="2b58" class="nf ng it nb b gy nm ni l nj nk">for (i in 1:3) {<br/> glm_5order = glm(Y~poly(data5$X1,deg=5)+poly(data5$X2,deg=5),data=Boot_sample_5f_2[[i]])</span><span id="e364" class="nf ng it nb b gy nm ni l nj nk"> probability_pred_5order = predict(glm_5order,gr,type=”response”)<br/> class_pred_5order = as.factor(ifelse(probability_pred_5order&lt;=0.5, “0”, “1”))<br/> color_array_5order &lt;- c(“red”, “blue”)[as.numeric(class_pred_5order)] <br/> plot(gr,col=color_array_5order,pch=20,cex=0.25)<br/>}</span></pre><div class="ks kt ku kv gt ab cb"><figure class="nr kw ns nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/ee3bde2d816dc57137e471e799f96fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*942PxYWXgJOnO1RResLYTw.png"/></div></figure><figure class="nr kw nx nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/b4c15910a94f214ee4b1622e0e399d69.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*yzvraaDrK0HRZiL7dibQag.png"/></div></figure><figure class="nr kw ny nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/be002b7e874519527b46199bbd7975dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*m4peCMZDE4BsFVBpFB0yMg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk nz di oa ob">Plots 1–3</figcaption></figure></div><div class="ab cb"><figure class="nr kw oc nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/d9a857cc38a7a395bd79944b09a3250f.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*msR6xpNjJPz8wzgLRLV4hw.png"/></div></figure><figure class="nr kw od nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/cd65b67b82904311603c5e03ad87eae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*sv536jgBUuMLHAeDBD7Wug.png"/></div></figure><figure class="nr kw oe nt nu nv nw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/421bf5439125fd8d95dd3dcda5c8ed89.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*HtmRUD8lZqCsTUOpkx5Z8w.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk of di og ob">Plots 4–6</figcaption></figure></div><p id="5172" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">从图 1-3 中，网格被一条直线分成两部分，指的是大偏差和小方差。</p><p id="3d32" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated">从图 4-6 中，我们观察到一个相反的模式:它有一个小的偏差，因为有更多的变量来最小化预测和真实值之间的距离。它有很大的差异，因为点分布在整个图表。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="1839" class="pw-post-body-paragraph mf mg it mh b mi mj kd mk ml mm kg mn mo mp mq mr ms mt mu mv mw mx my mz ls im bi translated"><em class="oh"> Medium 最近进化出了自己的</em> <a class="ae lh" href="https://blog.medium.com/evolving-the-partner-program-2613708f9f3c" rel="noopener"> <em class="oh">作家伙伴计划</em> </a> <em class="oh">，支持像我这样的普通作家。如果你还不是订户，通过下面的链接注册，我会收到一部分会员费。</em></p><div class="oi oj gp gr ok ol"><a href="https://leihua-ye.medium.com/membership" rel="noopener follow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd jd gy z fp oq fr fs or fu fw jc bi translated">阅读叶雷华博士研究员(以及其他成千上万的媒体作家)的每一个故事</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">leihua-ye.medium.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz lb ol"/></div></div></a></div></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="5e40" class="pa ng it bd pb pc pd pe pf pg ph pi pj ki pk kj pl kl pm km pn ko po kp pp pq bi translated">喜欢读这本书吗？</h1><blockquote class="pr ps pt"><p id="99b8" class="mf mg oh mh b mi mj kd mk ml mm kg mn pu mp mq mr pv mt mu mv pw mx my mz ls im bi translated">请在<a class="ae lh" href="https://www.linkedin.com/in/leihuaye/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://www.youtube.com/channel/UCBBu2nqs6iZPyNSgMjXUGPg" rel="noopener ugc nofollow" target="_blank"> Youtube </a>上找到我。</p><p id="0cb1" class="mf mg oh mh b mi mj kd mk ml mm kg mn pu mp mq mr pv mt mu mv pw mx my mz ls im bi translated">还有，看看我其他关于人工智能和机器学习的帖子。</p></blockquote></div></div>    
</body>
</html>