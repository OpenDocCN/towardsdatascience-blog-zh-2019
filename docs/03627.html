<html>
<head>
<title>Hyperparameter Tuning with callbacks in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Keras 中使用回调进行超参数调优</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-with-callbacks-in-keras-5230f51f29b3?source=collection_archive---------10-----------------------#2019-06-09">https://towardsdatascience.com/hyperparameter-tuning-with-callbacks-in-keras-5230f51f29b3?source=collection_archive---------10-----------------------#2019-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ccf0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过可视化梯度下降调整超参数的简单方法</h2></div><h1 id="5f5c" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">为什么这很重要？</h1><p id="c1f4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">本文展示了一种简单的方法，通过在 Keras 中使用回调访问模型权重来调整超参数。</p><p id="0588" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">应用机器学习是一个经验过程，您需要尝试不同的超参数设置，并推断哪些设置最适合您的应用。</p><p id="bb27" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这种技术通常被称为超参数调谐。</p><p id="0853" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这些超参数可以是学习率(alpha)、迭代次数、最小批量等。</p><h1 id="aca9" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">目标</strong></h1><p id="9b1b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">通常通过<em class="mb">在连续迭代中观察成本函数</em>的趋势来执行调整。一个好的机器学习模型具有不断降低的成本函数，直到某个最小值。</p><p id="0e8f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">本文展示了一种简单的方法，借助<strong class="lc iu">等高线图、</strong>为<strong class="lc iu"> Keras 模型</strong>可视化成本函数的最小化。</p><p id="e49a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在我们的例子中，我们将考虑一个单变量线性回归问题，该问题根据花在广告上的钱数来预测特定产品的销售额。</p><blockquote class="mc md me"><p id="0e0b" class="la lb mb lc b ld lw ju lf lg lx jx li mf ly ll lm mg lz lp lq mh ma lt lu lv im bi translated"><strong class="lc iu">注意</strong>:虽然选择的问题相当简单，但这项技术也适用于深度神经网络。</p></blockquote><h1 id="b713" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">背景</strong></h1><h2 id="78cc" class="mi kj it bd kk mj mk dn ko ml mm dp ks lj mn mo ku ln mp mq kw lr mr ms ky mt bi translated">成本函数和梯度下降</h2><p id="3d4b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">成本函数</strong>是根据模型估计输入和相应输出之间关系的能力来衡量模型错误程度的方法。</p><p id="d271" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">简单来说..</p><blockquote class="mu"><p id="6e43" class="mv mw it bd mx my mz na nb nc nd lv dk translated"><em class="ne">“你的模型表现有多差”</em></p></blockquote><p id="fe4e" class="pw-post-body-paragraph la lb it lc b ld nf ju lf lg ng jx li lj nh ll lm ln ni lp lq lr nj lt lu lv im bi translated"><strong class="lc iu">另一方面，梯度下降</strong>是一种通过重复更新网络的参数值来最小化成本函数的技术。</p><p id="f1dc" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">梯度下降的目标可以被认为是…</p><blockquote class="mu"><p id="687b" class="mv mw it bd mx my mz na nb nc nd lv dk translated"><em class="ne">“反复调整参数，直到达到局部最小值”</em></p></blockquote><p id="58eb" class="pw-post-body-paragraph la lb it lc b ld nf ju lf lg ng jx li lj nh ll lm ln ni lp lq lr nj lt lu lv im bi translated">线性回归的成本函数通常是<a class="ae nk" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差</a>，在这里<a class="ae nk" href="https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/squared-error-of-regression-line" rel="noopener ugc nofollow" target="_blank">有很好的解释</a>。</p><h1 id="4a53" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">问题描述</strong></h1><p id="78fe" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><a class="ae nk" href="https://github.com/abhishekr7/cost-minimization-visual/blob/master/Advertising.csv" rel="noopener ugc nofollow" target="_blank"> Advertising.csv </a>文件包含分配给各种来源(<em class="mb">电视</em>、<em class="mb">广播、</em>、<em class="mb">报纸</em>)的广告预算及其对特定产品销售<em class="mb">的影响。</em></p><p id="e1ea" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">由于我们的重点是单变量回归，我们将只考虑分配给<em class="mb"> TV </em>的预算作为我们的<a class="ae nk" href="https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics_synonyms" rel="noopener ugc nofollow" target="_blank">独立</a>变量<em class="mb">。</em></p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><p id="9f08" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">本文的代码和数据可以在<a class="ae nk" href="https://github.com/abhishekr7/cost-minimization-visual" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><p id="0e02" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在将 csv 文件加载到一个<a class="ae nk" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> pandas </a> dataframe 中并删除不必要的列之后…</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="cfc5" class="mi kj it nx b gy ob oc l od oe">df = pd.read_csv(‘path/to/file/Advertising.csv’)<br/>df.drop([‘Unnamed: 0’,’radio’,’newspaper’],axis = 1 , inplace=True)<br/>X = df[‘TV’]<br/>Y = df[‘sales’]<br/>df.head()</span></pre><p id="5762" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">…最终的数据帧将是这样的</p><figure class="ns nt nu nv gt og gh gi paragraph-image"><div class="gh gi of"><img src="../Images/87735a6aafdd02b42ee136d24e178096.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*8E6aDnvgL8jzbT1LMMFurQ.png"/></div><figcaption class="oj ok gj gh gi ol om bd b be z dk">Advertising Data Set</figcaption></figure><p id="b078" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">后来，我们将数据分成训练集和测试集</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="cdc4" class="mi kj it nx b gy ob oc l od oe">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, <br/>                                                    test_size = 0.2)</span></pre><p id="3df5" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在，请注意，Keras 并没有明确提供像<a class="ae nk" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>这样的线性回归模型。但是我们可以用单个神经元的密集层来模拟线性回归。</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="0e2d" class="mi kj it nx b gy ob oc l od oe">model = Sequential()<br/>model.add(Dense(1, activation = ‘linear’, use_bias = True,<br/>                                                     input_dim = 1))<br/>model.compile(optimizer = optimizers.RMSprop(lr = 0.01),<br/>                     loss = ‘mean_squared_error’, metrics = [‘mae’])</span></pre><p id="2042" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">设计的模型看起来会像…</p><figure class="ns nt nu nv gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oo op di oq bf or"><div class="gh gi on"><img src="../Images/71c27e821b65fea450c8f77d9f0d1154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*US1-ZL8Uha8ygt7VcLk_cQ.png"/></div></div><figcaption class="oj ok gj gh gi ol om bd b be z dk">Univariate Linear Regression “Network”</figcaption></figure><p id="45e6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">训练模型，我们得到一个相当令人满意的预测图…</p><figure class="ns nt nu nv gt og gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ba114b93977153d9f5d79e4bf07a8d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*LS40fgwzmpttdxU2myqTdw.png"/></div></figure><h1 id="11a6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">绘制成本函数</strong></h1><p id="a1bb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">线性回归的成本函数由下式给出</p><figure class="ns nt nu nv gt og gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/01414b959994e49c7ff49e84e20a2a0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*F62_3mUb2XTxuEggtoKxGA.png"/></div></figure><p id="99ce" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">从等式中可以清楚地看出，我们对可视化成本最小化的要求是每次迭代后更新的图层的权重(和偏差)。</p><p id="1076" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果我们能够以某种方式访问图层的权重，我们将能够轻松地可视化成本最小化/梯度下降。</p><p id="a75b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">Keras 提供了一个<strong class="lc iu"> get_weights() </strong>函数供用户访问网络层的权重。</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="7f55" class="mi kj it nx b gy ob oc l od oe">model.get_weights()</span></pre><p id="e1b1" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">但是该函数在训练后返回模型的最终权重(和偏差)。</p><p id="d3f2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们需要一种方法来访问每次迭代(或每批)结束时的权重。</p><p id="66e5" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">为此，我们将利用一个<a class="ae nk" href="http://keras.io/callbacks/" rel="noopener ugc nofollow" target="_blank">回调</a>。</p><h1 id="aa64" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">在 Keras 中定义回调</strong></h1><p id="0d8d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Keras 回调帮助您更快地修复 bug 并构建更好的模型。</p><blockquote class="mc md me"><p id="6efb" class="la lb mb lc b ld lw ju lf lg lx jx li mf ly ll lm mg lz lp lq mh ma lt lu lv im bi translated">“回调是在训练过程的给定阶段应用的一组函数。您可以在训练期间使用回调来查看模型的内部状态和统计数据。</p></blockquote><p id="b8a6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这正是我们所需要的，因为现在我们可以在每次小批量之后(即每次迭代之后)获得 _weights()。重量存储在一个<em class="mb">重量历史</em>列表中，以备后用。还为偏差项维护了一个单独的列表。</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="2c77" class="mi kj it nx b gy ob oc l od oe">weight_history = []<br/>bias_history = []<br/>class MyCallback(keras.callbacks.Callback):<br/>    def on_batch_end(self, batch, logs):<br/>        weight, bias = model.get_weights()</span><span id="1e45" class="mi kj it nx b gy ou oc l od oe">        B = bias[0]<br/>        W = weight[0][0]<br/>        weight_history.append(W)<br/>        bias_history.append(B)</span><span id="fd34" class="mi kj it nx b gy ou oc l od oe">callback = MyCallback()</span></pre><p id="3667" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">创建的回调与用于训练模型的输入和输出一起传递。</p><pre class="ns nt nu nv gt nw nx ny nz aw oa bi"><span id="f32c" class="mi kj it nx b gy ob oc l od oe">model.fit(X_train, Y_train, epochs = 10, batch_size = 10,<br/>                               verbose = True, <strong class="nx iu">callbacks=[callback]</strong>)</span></pre><p id="3072" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在，存储的权重可用于绘制关于权重(W)和偏差(B)的成本函数(J)。</p><p id="f8fd" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">等高线图仅根据<em class="mb">重量历史</em>和<em class="mb">偏差历史绘制。</em>这里不需要计算成本函数。</p><figure class="ns nt nu nv gt og gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/4c1497ae53f5661673e1aa53e30e53ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*5LyaaaCKcL20kqFdyKfswQ.gif"/></div></figure><h1 id="020f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">解读等高线图</strong></h1><p id="e6ff" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">等高线图的基本直觉是，连续的线代表恒定的量值(称为等高线),并且量值随着我们从图的中间部分到向外部分而增加。</p><p id="7b0b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">等高线的大小已经给出，在这里，它表示成本函数(J)的可能值。您可以大致观察到，成本(红线)从接近 5000 开始，并继续下降，直到在特定点停止。</p><p id="315f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这与损失函数值相对应，损失函数值也被认为是均方误差。</p><figure class="ns nt nu nv gt og gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/64444642d16f340e4e337a71e546d0cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*2s4cX5ORl51LAq7H7WnAmg.png"/></div></figure><blockquote class="mc md me"><p id="a9d6" class="la lb mb lc b ld lw ju lf lg lx jx li mf ly ll lm mg lz lp lq mh ma lt lu lv im bi translated"><strong class="lc iu">注意</strong>:两个图之间的误差是由于均方误差(如上)是根据验证分割计算的，而等高线图是使用整个训练数据绘制的。</p></blockquote><h2 id="2786" class="mi kj it bd kk mj mk dn ko ml mm dp ks lj mn mo ku ln mp mq kw lr mr ms ky mt bi translated">什么也有用？</h2><p id="3f93" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如上所述，绘制迭代损失函数也可用于超参数调整。事实上，这是数据科学家最常用的技术。</p><h2 id="9f4e" class="mi kj it bd kk mj mk dn ko ml mm dp ks lj mn mo ku ln mp mq kw lr mr ms ky mt bi translated">为什么要使用等高线图？</h2><p id="77f4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">等高线图提供的优势在于，它们给出了关于梯度下降算法在迭代中更新模型/网络参数所遵循的轨迹的更好的直觉。</p><h1 id="6e2a" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">最后…</h1><p id="e8db" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因为我们已经获得了模型参数，所以观察它们随时间变化的趋势是值得的。</p><figure class="ns nt nu nv gt og gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/7f737bbc0427d0a437376aa8459d9c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*ggWvDlKobzTsMooQl_J21Q.png"/></div><figcaption class="oj ok gj gh gi ol om bd b be z dk">Weight vs. Time</figcaption></figure><figure class="ns nt nu nv gt og gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/61a098be1f0948cfa3fe5436417f3068.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*d8kRAyrPN42yL64zUJXjPg.png"/></div><figcaption class="oj ok gj gh gi ol om bd b be z dk">Bias vs. Time</figcaption></figure><p id="c41b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">因此，可以观察到我们的模型的权重和偏差达到成本函数的局部最小值所遵循的趋势。</p><p id="6c6a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在，您已经可以访问所有的图，您可以有效地检查您的模型是否学习缓慢或超调(学习率)，是否小批量产生可观察到的好处，理想的迭代次数(或甚至是时期)等。</p></div></div>    
</body>
</html>