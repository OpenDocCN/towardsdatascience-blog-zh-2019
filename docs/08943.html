<html>
<head>
<title>Bayesian deep learning with Fastai : how not to be uncertain about your uncertainty !</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Fastai 的贝叶斯深度学习:如何不确定自己的不确定性！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-deep-learning-with-fastai-how-not-to-be-uncertain-about-your-uncertainty-6a99d1aa686e?source=collection_archive---------1-----------------------#2019-11-29">https://towardsdatascience.com/bayesian-deep-learning-with-fastai-how-not-to-be-uncertain-about-your-uncertainty-6a99d1aa686e?source=collection_archive---------1-----------------------#2019-11-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/d1f06ea9434978a739dea3575f8474f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*3ZXwCt_1V4r-myzrCc_dbQ.jpeg"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Bobby Axelrod speaks the words!</figcaption></figure><p id="8e0a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">今天，神经网络已经成为许多领域的头条新闻，如癌症组织的图像分类，文本生成，甚至信用评分。然而，这些模型很少解决的一个大问题是预测的不确定性。</p><p id="3276" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们人类学习时，我们最大的优势之一是知道我们的弱点，并且在有太多不确定性时不采取行动。然而，对于大多数机器学习模型来说，情况并非如此，在这些模型中，决策是在没有考虑不确定性的情况下做出的。</p><p id="005f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如，如果你训练一个关于猫和狗的分类器，它将只能输出猫或狗。但如果你给它一张卡车的图片，模型仍然会做出预测，有时概率很大，即猫的概率接近 1，狗的概率接近 0，而你的模型应该简单地说“好吧，这是我无法识别的东西”。当我们着眼于医疗应用时，这一点尤其重要，因为医疗应用必须处理不确定性。</p><p id="cdc8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，理想情况下，我们希望有一种方法在我们的模型中添加不确定性评估，以便知道模型何时不应做出决定，并将其传递给能够做出正确决定的人，以帮助模型学习。这被称为<strong class="ka ir">主动学习</strong>，我们将在另一篇文章中讨论这个话题。</p><p id="5197" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，获得不确定性的度量是非常可取的，因为这是我们人类所拥有的，而大多数模型都缺乏。在本文中，我们将看到如何在我们的模型中用几行代码实现这一点，并看到流行任务的结果，如情感分析、图像分类或信用评分。</p><p id="ea8c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">一.快速浏览贝叶斯神经网络</strong></p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kw"><img src="../Images/defde87506d63d38f255245a6349a24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNHFbAe8wpupn9u7Hk-zZg.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Bayesian Neural Networks seen as an ensemble of learners</figcaption></figure><p id="7d18" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">贝叶斯神经网络(BNNs)是一种在我们的模型中增加不确定性处理的方法。这个想法很简单，我们不是学习确定性权重，而是学习随机变量的参数，我们将使用这些参数在前向传播过程中对权重进行采样。然后，为了学习参数，我们将使用反向传播，有时用一点小技巧使我们的参数可微。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi lf"><img src="../Images/c0d4ee6c9b4d49abb783dc361992caf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xftnpT4ZdgZquyrvCHghEg.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Comparison of a standard NN with a Bayesian NN</figcaption></figure><p id="316c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这张图可以帮助你理解其中的区别。虽然常规 NN 通过将预测损失反向传播到权重来学习权重，但这里我们使用分布的参数(这里是高斯分布)对 W 进行采样，并照常进行，但这次我们一直反向传播到高斯分布的参数(这里是均值和方差)。</p><p id="2b49" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">尽管乍一看，使用随机变量而不是确定性权重来对权重进行采样似乎很奇怪，但如果您认为这不是一个单一的网络，而是一个潜在的无限网络集合，这实际上是有意义的。</p><p id="f7a6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">事实上，每次你对你的网络的权重进行采样，就像从一个群体中采样一个批评家，并且每次向前传递可以有不同的输出，因为对于你采样的每个批评家。即使你可能有大量(甚至无限)的批评者，每个批评者都将帮助整个群体变得更好，因为我们将对这个给定批评者的表现使用反向传播，以适应整个群体的参数。在上面的例子中，我们不是只学习<strong class="ka ir"> W </strong>，而是学习<strong class="ka ir"> W </strong>的均值和方差。</p><p id="7809" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以最后，使用贝叶斯神经网络时的训练过程如下:</p><ul class=""><li id="bad2" class="lg lh iq ka b kb kc kf kg kj li kn lj kr lk kv ll lm ln lo bi translated">从 BNN 分布的随机参数开始，即使用一些参数初始化您的总体，例如，如果您想要高斯权重，从 0 均值和单位方差开始。</li><li id="e3f5" class="lg lh iq ka b kb lp kf lq kj lr kn ls kr lt kv ll lm ln lo bi translated">对于训练循环中的每一批:<br/> -根据与每一层相关的分布对你的体重进行抽样。<br/> -使用这些权重进行正向传递，就像使用常规网络一样<br/> -将损失反向传递给生成权重的分布参数</li></ul><p id="4a9c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">数学细节将在另一篇文章中解释，这里我们将只关注 bnn 如何工作的全局思想，所以如果你记得 bnn 的行为就像一种神经网络的集合，这已经是很好的一步了！</p><p id="40cc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">那么最后，为什么我们要学习权重的概率分布，而不是直接学习权重呢？有两个主要原因:</p><ul class=""><li id="68b0" class="lg lh iq ka b kb kc kf kg kj li kn lj kr lk kv ll lm ln lo bi translated">首先，这防止了过度拟合</li><li id="7b80" class="lg lh iq ka b kb lp kf lq kj lr kn ls kr lt kv ll lm ln lo bi translated">第二，这允许我们得到不确定性估计</li></ul><p id="e330" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将更详细地讨论第二点。事实上，因为我们的模型本质上是随机的，所以对于相同的输入，我们的模型可以有不同的输出，因为对于每个前向传递，我们将有不同的权重被采样，因此可能有不同的输出。</p><p id="70fe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，您可以选择采样<strong class="ka ir"> T </strong>次，这样您将拥有<strong class="ka ir"> T </strong>不同的权重，以及<strong class="ka ir"> T </strong>不同的输出。因此，<strong class="ka ir">您现在可以通过对这些结果进行平均来进行投票，但是您也可以计算方差来查看您的模型之间的一致程度！</strong></p><p id="db41" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过相同输入的预测方差，贝叶斯神经网络能够输出关于其预测的不确定性。让我们来看看下面这个简单的回归问题:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi lu"><img src="../Images/f78ad7ced0d191007c09dca39f2c8590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8OvPt6P1mPogX5z3aNlsWA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Source : <a class="ae lv" href="http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html" rel="noopener ugc nofollow" target="_blank">http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html</a></figcaption></figure><p id="6e8f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里，我们使用蓝色虚线之前的训练数据点，并要求模型预测虚线之前和之后的点。我们训练一个贝叶斯神经网络，输出几个我们平均的估计值，并基于方差用置信区间来绘制它。</p><p id="e332" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有趣的是，我们注意到模型在左侧输出的点没有任何差异，正如在训练期间所看到的那样，模型对其预测很有信心，但之后，随着我们看到的点离我们的训练集越来越远，差异也在增加。</p><p id="b725" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是意料之中的，因为我们不知道虚线之外是什么，所以我们越往前走，不确定性越大的模型是合理的。</p><p id="165e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们已经了解了贝叶斯神经网络的原理，以及如何使用它们来输出模型的不确定性，我们将看到一种简单的方法，用几行代码就可以使您的模型贝叶斯化！</p><p id="4908" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">二。辍学作为一种方式，使你的 NNs 贝叶斯</strong></p><p id="1b8d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Dropout 是一种流行的正则化技术，其目标是通过在给定层中随机将激活设置为零来帮助减少过拟合。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi lw"><img src="../Images/271f29855451434f956c71d7870a3a4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIGb-kfxCAK0xiXipo6utA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Illustration of Dropout from the paper “Dropout: A Simple Way to Prevent Neural Networks fromOverfitting”</figcaption></figure><p id="d7d9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个想法很简单，我们不希望让一些神经元变得过于专业，只依赖这些神经元进行预测，因此我们在我们的层中随机置零神经元，每层都有不同的丢失概率<strong class="ka ir"> p </strong>。这样，没有神经元变得过度专业化，这是一种确保所有神经元都对预测做出贡献的方法，而不会过度拟合。</p><p id="5e0c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Dropout 通常以下列方式使用:</p><ul class=""><li id="2616" class="lg lh iq ka b kb kc kf kg kj li kn lj kr lk kv ll lm ln lo bi translated">在训练过程中，我们激活 Dropout，这意味着我们每一层都有不同的概率丢弃神经元。一旦神经元被丢弃，我们通常会除以保留神经元的概率，以保持相同的数量级。</li><li id="3329" class="lg lh iq ka b kb lp kf lq kj lr kn ls kr lt kv ll lm ln lo bi translated">在推理过程中，我们停用丢弃，因此我们使用所有的神经元，并且我们的输出是完全确定的，因为随机丢弃神经元的随机部分被关闭。</li></ul><p id="9ce5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">那么辍学和贝叶斯神经网络有什么关系呢？嗯，使用辍学可以被视为使用伯努利随机变量来采样你的权重，因此<strong class="ka ir">简单地添加辍学，你使你的神经网络贝叶斯！</strong></p><p id="12a3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">数学在论文<a class="ae lv" href="https://arxiv.org/pdf/1506.02142.pdf" rel="noopener ugc nofollow" target="_blank">“作为贝叶斯近似的辍学:表示深度学习中的模型不确定性”</a>中进行了探索。我强烈推荐你阅读，尽管乍一看并不容易，但它解释了作为贝叶斯神经网络的辍学背后的许多理论，尤其是它们涵盖了什么是先验、后验和相关的 KL 项，但我们将在另一篇文章中讨论这一点。</p><p id="5f11" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，通过简单地添加 Dropout，我们现在了解了一个贝叶斯模型，其中权重是从行(或列，取决于矩阵乘法是如何完成的)伯努利分布中采样的。需要注意的一点是，训练将与之前完全相同，唯一的区别是在推断过程中，我们不关闭随机性，而是采样几个权重，将其传递给前向传递，并获得输出的分布。我们在推断过程中使用多个权重样本的事实被称为<strong class="ka ir">蒙特卡洛遗漏(或 MC 遗漏)。</strong></p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi lx"><img src="../Images/7a383bb8438c5ff33a0c57b7afecb819.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5nGhRz2oFLxodv33cZbb-w.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">How Dropout creates an ensemble of NNs which can output a distribution of results</figcaption></figure><p id="3fca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，我们可以看到压差如何为我们提供不同的网络，每个网络提供不同的输出，这允许我们对网络的输出进行分布。这样，我们就可以知道有不确定性度量，比如方差用于回归，或者如我们将看到的，熵用于分类。</p><p id="5b8c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，在我们用具体的例子探究这些结果之前，让我总结一下我们目前所看到的一切:</p><ul class=""><li id="b8e8" class="lg lh iq ka b kb kc kf kg kj li kn lj kr lk kv ll lm ln lo bi translated">贝叶斯神经网络使用权重的概率分布，而不是具有确定性的权重。每次向前传递将采样不同的权重，因此具有不同的输出。人们可以将贝叶斯神经网络视为不具有单一模型，而是模型的集合，这为每个样本提供了不同的预测器。</li><li id="9a00" class="lg lh iq ka b kb lp kf lq kj lr kn ls kr lt kv ll lm ln lo bi translated">假设相同的输入在向前传递后可以有不同的输出，我们可以通过查看不同预测的方差来了解我们的模型有多确定，因为如果模型不确定，我们预计方差会很高。</li><li id="3656" class="lg lh iq ka b kb lp kf lq kj lr kn ls kr lt kv ll lm ln lo bi translated">辍学是一种让你的神经网络贝叶斯几乎免费的方式，在推理过程中使用它，你只需要保持辍学，并对几个模型进行采样，这被称为<strong class="ka ir"> MC 辍学。</strong></li></ul><p id="2080" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">三世。探索 MC 辍学的 Fastai 示例</strong></p><p id="f370" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们已经看到了 bnn 是如何工作的，以及如何用 Dropout 轻松地实现它们，我们现在将看到一些使用 Fastai 的例子。</p><p id="b1ab" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">代码可以在这个 Github 的资源库中找到:<a class="ae lv" href="https://github.com/DanyWind/fastai_bayesian" rel="noopener ugc nofollow" target="_blank">https://github.com/DanyWind/fastai_bayesian</a>，如果你想直接在 Colab 上运行，这里有链接:<a class="ae lv" href="https://colab.research.google.com/drive/1bIFAWl_o__ZKFSVvE9994WJ2cfD9976i" rel="noopener ugc nofollow" target="_blank">https://Colab . research . Google . com/drive/1 bifawl _ o _ _ zkfsvve 9994 wj2 CFD 9976 I</a></p><p id="1567" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">无论任务是什么，流程都是一样的:</p><ul class=""><li id="47d5" class="lg lh iq ka b kb kc kf kg kj li kn lj kr lk kv ll lm ln lo bi translated">首先用我们都知道的 Fastai API 训练你的模型:lr_find，fit_one_cycle 等…</li><li id="78cf" class="lg lh iq ka b kb lp kf lq kj lr kn ls kr lt kv ll lm ln lo bi translated">最后，要在 MC Dropout 推断期间保持 Dropout，只需添加以下代码行:</li></ul><figure class="kx ky kz la gt jr"><div class="bz fp l di"><div class="ly lz l"/></div></figure><p id="c346" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">定义函数和类的代码行就在这里，它们并不复杂。我选择用一个模块函数来替换另一个模块，因为我们将在以后的文章中使用它来计算自动退学率。</p><figure class="kx ky kz la gt jr"><div class="bz fp l di"><div class="ly lz l"/></div></figure><p id="66f7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Colab 中也定义了度量标准，但我们不会涉及它们，因为它非常标准。我们将特别使用一个只对分类有效的不确定性度量:<strong class="ka ir">熵。</strong></p><p id="2d8e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果有<strong class="ka ir"> C </strong>类要预测，并且我们采样<strong class="ka ir"> T </strong>倍我们的模型进行推断，那么熵被定义为:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/6d76c80bdddd49e3ddb7db414f1eee43.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*A68fzGZJT1Q773rBWIrY9A.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Definition of Entropy</figcaption></figure><p id="33c7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个度量可以看作是对无序度的度量，无序度越高，我们模型的不确定性就越高。这比简单地查看分类概率的方差更有意义，因为 softmax 有压缩一切的趋势。</p><p id="2069" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> A .猫狗分类器</strong></p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mb"><img src="../Images/d3345218e684bbcb6352da19e4cc5100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Esb8nw6KdGaNrZEcBwuqQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Cats and Dogs dataset</figcaption></figure><p id="8e67" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，我们将重点放在猫和狗的数据集。从这张图片上可以看出，我们将尝试对图片上的狗或猫的品种进行分类。</p><p id="3ea5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我不会详细介绍如何训练该模型，这是使用 Resnet34 的经典迁移学习，您可以直接查看代码以了解更多细节。</p><p id="b461" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">更有趣的是，用我们用辍学训练的模型做推断，看看猫和狗的常规图像，也看看分布图像，看看模型有多确定。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mc"><img src="../Images/07a40fa7c79edf96e589b82e57c0a95c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QjCdlnG4ITfgRJTTjShw0w.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Entropy of regular and out of distribution images</figcaption></figure><p id="f266" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如您在这里看到的，已知模型的图像具有低熵，而完全不在分布范围内的图像具有高熵。有趣的是，我们注意到 softmax 的概率并没有给出不确定性的信息，因为在 Colab 笔记本中，我们可以看到分配给夜王类的最高概率是 90%，甚至超过了猫的 78%！</p><p id="4d6d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可以看到，在某些应用中，测量不确定性是多么重要，尤其是在医疗领域，<strong class="ka ir">例如，如果我们想对一个人是否应该接受化疗进行分类，我们希望知道我们的模型有多确定，如果不确定，就让一个人参与进来。</strong></p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi md"><img src="../Images/cf5433c90f3ccc76f0689eb27b228b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*gqPG3JfVne4oLsogawuKwA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Distribution of entropy based on classification results on Cats and Dogs</figcaption></figure><p id="f386" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">每个分类结果的熵分布直方图也可以帮助我们诊断我们的模型。正如我们在上面可以看到的，当我们的模型正确执行时，即当存在真阳性或真阴性时，我们观察到熵往往比它错误时(即假阳性和假阴性)低得多。</p><p id="6acd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有趣的是，无论我们的模型正确与否，熵的表现都是不同的，高熵似乎与错误分类高度相关！如果我们回头看看夜王的例子，我们会发现 2.2 的熵确实很高，并且通常出现在错误分类的样本中，而普通猫的熵通常只有 0.7。</p><p id="d402" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> B .使用 IMDB 评论进行情感分析</strong></p><p id="0be1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们将快速回顾另一个著名的数据集:IMDB 评论。这个数据集只是收集正面和负面的评论。这里我们将跳过绘制直方图，直接显示数据集最确定和最不确定的样本:</p><p id="a5c4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面的评论拥有最高的熵，这是可以理解的，因为评论很难定义是好是坏。即使我们人类可以感觉到整体情绪是好的，但这也不是微不足道的，我们可以预计机器会更加挣扎。</p><pre class="kx ky kz la gt me mf mg mh aw mi bi"><span id="50c6" class="mj mk iq mf b gy ml mm l mn mo">(Text xxbos xxmaj just got my copy of this xxup dvd two disc set and while not perfect , i found the overall experience to be a fun way to waste some time . i have to say right up front that i am a huge fan of xxmaj zombie movies , and i truly think that the fine people who made these films must be too . i also have a soft spot for people who are trying , sometimes against all odds , to live a dream . xxmaj and again , these people are doing it . xxmaj is this some award - winning collection of amazing film ? xxmaj no . xxmaj not even close . xxmaj but for what they do on their xxunk xxunk , these films should be recommended . xxmaj for me , the bottom line is always , was i entertained ? xxmaj did i have a good time with this movie ? xxmaj and here the answer to both was " xxmaj yes ... Category positive)</span></pre><p id="f434" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> C .成年人收入分类表</strong></p><p id="773d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我们将在这里探索成人收入数据集，我们试图根据社会经济变量来预测某人的年收入是否超过 5 万英镑。我们也将看看最确定和不确定的预测。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mp"><img src="../Images/9a1c0aa57b5c76170b297dd93013ef91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgJvUs83e4EN0nTwL3_HCg.png"/></div></div></figure><p id="4ab6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我认为这个例子很有趣，因为它可能会对没有考虑不确定性的模型提出一些伦理问题。</p><p id="a56d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">事实上，该模型对预测某人是否有高工资非常有信心，这些人通常是白人、男性和受过高等教育的人。因此，该模型很容易区分这些情况。</p><p id="19c8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但另一方面，当处理第二种更困难的情况时，这个模型就更不确定了，比如一个白人男性，受过一些大学教育，工作时间比平均水平长得多。这里的模型是相当不确定的，因为这个人可能在 50K 以上或以下。相反，如果目标是决定是否给予信用，一个好的模型应该让人类在存在巨大不确定性时决定是否做出决定，我们可能会重现模型中可能存在的偏差。</p><p id="df7f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">结论</strong></p><p id="baf7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我们已经看到了贝叶斯神经网络是什么，我们如何在已经训练好的架构上快速使用它们的属性，通过使用简单的 MC Dropout 在推断过程中简单地利用 Dropout，最后我们如何通过这些模型获得不确定性估计。</p><p id="fc94" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些不确定性估计是宝贵的，因为它们在人工智能的大多数应用中是至关重要的，不仅可以为可信任的人工智能提供保障，而且正如我们将在未来的文章中看到的那样，还可以帮助我们通过主动学习将人类置于循环中。</p><p id="93a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以，我希望你喜欢读这篇文章，如果能得到你的反馈，那就太好了，我会在未来试着发布更多的文章。</p><p id="a7e2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您有任何问题，请不要犹豫，通过<a class="ae lv" href="https://www.linkedin.com/in/dhuynh95/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我，您也可以通过<a class="ae lv" href="https://twitter.com/dhuynh95" rel="noopener ugc nofollow" target="_blank"> Twitter 找到我！</a></p></div></div>    
</body>
</html>