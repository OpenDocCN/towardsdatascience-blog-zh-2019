<html>
<head>
<title>Other than GANs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">除了甘</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gans-or-d0fb38ff8ddb?source=collection_archive---------35-----------------------#2019-11-22">https://towardsdatascience.com/gans-or-d0fb38ff8ddb?source=collection_archive---------35-----------------------#2019-11-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b9a0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在这篇博文中，我将讨论更精确地使用 GANs 的替代方法。我建议读者阅读 GANs 来理解和比较下面解释的策略。让我们开始吧。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/21fd5250244c20ba5d93790a95c0a11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T1jfC_Yb8w557T17K0-rBw.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by <a class="ae kv" href="https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1991865" rel="noopener ugc nofollow" target="_blank">Gerd Altmann</a> from <a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1991865" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="3944" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望到现在为止，你们都非常熟悉 GANs 的概念。但是，我们仍然不能用 GANs 达到很高的精确度。如果你看过我在 GANs 上的博客，下面是最后阶段的输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/6db5df7ec42b7fd2fa2a8e69b4578fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UaT4ct5C1W-jLPQNFvJgiA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/1a0f2afd06a26a9f97cd1bc8527d9a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OiyyXAs4OemtNpWTjCpQyw.png"/></div></div></figure><p id="c192" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">中间的图像是 GAN 预测的。虽然它有点类似于目标图像，但仍然不是可接受的图像，因为预测图像中猫的眼睛不亮，猫的爪子不清晰。如果我们开始根据特征来区分猫，这些小事情就很重要了。</p><p id="9762" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可能会在更大程度上改进结果，但 GANs 仍然无法提供足够的特征间隙，因为我们没有以这种方式训练模型。甘不在乎猫的眼睛，也不在乎猫的爪子和其他东西。</p><h2 id="d9a8" class="lu lv iq bd lw lx ly dn lz ma mb dp mc lf md me mf lj mg mh mi ln mj mk ml mm bi translated">我们能摆脱甘一家吗？</h2><p id="c8fc" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf mp lh li lj mq ll lm ln mr lp lq lr ij bi translated">Fastai 想出了一些更好的东西，恢复图像的力量。现在，当我们谈论一个更好的模型时，首先想到的是改进 loff 函数。如果我们可以增强损失函数，那么我们的模型将训练得更好，从而得到更准确的结果。所以，我们的主要目的是创建一个更好的损失函数。现在，我们还可以提出更复杂的架构设计，但这无疑是最后一个要实施的选项。</p><p id="9186" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我在这里关心的事情在<a class="ae kv" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">实时风格转换和超分辨率的感知损失</a>中有所解释。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/f4f19eb8e6cf7a101c7607549a8c39be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oj2CiLsg9pBifRub3qsuCw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image source — research paper mentioned above</figcaption></figure><ul class=""><li id="331e" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated">图像转换网络——它基本上是 GANs 的 UNet 或生成器部分。它产生输出，即预测的图像。</li><li id="a002" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">上述类型的模型被称为生成模型。在这些类型的模型中，我们有一个下采样部分，也称为编码器，还有一个上采样部分，称为解码器。</li><li id="d14a" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><code class="fe nh ni nj nk b">Ŷ</code>是预测图像。<code class="fe nh ni nj nk b">Yc</code>是我们想要得出的目标图像。</li><li id="4bab" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">现在，我们将预测图像和目标图像通过预先训练的图像分类模型，如 ResNet34、VGG-16 等。这些模型在许多类别的图像上被预先训练，并且它们对输入图像进行分类。</li><li id="f742" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">通常，它的输出会告诉你，“嘿，这个生成的东西是狗，猫，飞机，还是别的什么。”但是在最终分类的过程中，它会经过许多不同的层次。在这种情况下，他们使用相同的格网大小对所有图层进行了颜色编码，并使用相同的颜色对要素地图进行了颜色编码。所以每次我们改变颜色，我们就改变了网格的大小。所以有一个跨步二卷积，或者在 VGG 的情况下，他们使用最大池层。</li><li id="f12a" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">现在，我们可以做的是，我们可以比较中间层的激活，然后找出预测图像层和目标图像层之间的像素损失，而不是比较最后几层。如果我们可以这样做，那么我们就可以引导模型的中间层变得更加具体和准确。</li><li id="261b" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">让我们用一个例子来理解这个比较中间层的概念。</li></ul><blockquote class="nl"><p id="611f" class="nm nn iq bd no np nq nr ns nt nu lr dk translated">max-pooling 层之前的输出中的层的结构是 216*28*28。这意味着我们有 216 个通道，图像大小为 28*28。每个通道告诉我们图像中不同种类的特征。可能第 100 个频道比较了猫形象的眼球。如果我们可以比较预测图像和目标图像的第 100 层，那么我们的模型将为期望的结果准备得更好。</p></blockquote><ul class=""><li id="c1d7" class="mt mu iq ky b kz nv lc nw lf nx lj ny ln nz lr my mz na nb bi translated">在我们的例子中，特征图会说，“这里有眼球(在目标图像中)，但在生成的版本中没有，所以请多训练，做得更好。制造更好的眼球。”这就是我们的想法。这就是 fastai 所说的特征损失或 Johnson 等人所说的感知损失。</li></ul><p id="e528" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是我们如何在不使用 GANs 的情况下改进图像恢复的方法。</p><p id="309a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们了解损失在实际中是如何起作用的。我正在使用 fastai 库的代码。</p><pre class="kg kh ki kj gt oa nk ob oc aw od bi"><span id="25bf" class="lu lv iq nk b gy oe of l og oh">class class <strong class="nk ir">FeatureLoss</strong>(nn.Module):<br/>    def __init__(self, m_feat, layer_ids, layer_wgts):<br/>        super().__init__()<br/>        <strong class="nk ir">self.m_feat</strong> = m_feat<br/>        <strong class="nk ir">self.loss_features</strong> = [self.m_feat[i] for i in layer_ids]<br/>        <strong class="nk ir">self.hooks</strong> = hook_outputs(self.loss_features, detach=False)<br/>        <strong class="nk ir">self.wgts</strong> = layer_wgts</span><span id="2af9" class="lu lv iq nk b gy oi of l og oh">def <strong class="nk ir">make_features</strong>(self, x, clone=False):<br/>        <strong class="nk ir">self.m_feat</strong>(x)<br/>        <strong class="nk ir">return</strong> [(o.clone() if clone else o) for o in self.hooks.stored]<br/>    <br/>    def <strong class="nk ir">forward</strong>(self, input, target):<br/>        <strong class="nk ir">out_feat</strong> = self.make_features(target, clone=True)<br/>        <strong class="nk ir">in_feat</strong> = self.make_features(input)<br/>        <strong class="nk ir">self.feat_losses</strong> = [base_loss(input,target)]<br/>        <strong class="nk ir">self.feat_losses</strong> += [base_loss(f_in, f_out)*w<br/>                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]<br/>        <strong class="nk ir">self.feat_losses</strong> += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3<br/>                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]<br/>       <strong class="nk ir"> self.metrics</strong> = dict(zip(self.metric_names, self.feat_losses))<br/>        <strong class="nk ir">return sum(self.feat_losses)</strong><br/>    <br/>    def __del__(self): self.hooks.remove()(nn.Module):</span></pre><ul class=""><li id="2d97" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated"><code class="fe nh ni nj nk b">m_feat</code> —是预训练的模型；在我们的例子中，我们使用的是 VGG。</li><li id="c320" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><code class="fe nh ni nj nk b">layer_ids</code> —这是变化前的图层 id 列表。这是我们在最大池层数之前的层数列表。毫不奇怪，所有这些都是 ReLU 的。这些是我们想要抓住一些特征的地方。只是一串身份证。</li><li id="750e" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><code class="fe nh ni nj nk b">out_feat</code> —它包括目标图像中 layer_ids 中提到的相关层的列表。</li><li id="bc98" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><code class="fe nh ni nj nk b">in_feat</code> —由预测图像的 layer_ids 中指定的相关图层列表组成。</li><li id="b2e1" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><code class="fe nh ni nj nk b">self.feat_losses</code> —它是发生在 out_feat 和 in_feat 层的损失以及预测图像和目标图像之间的基本 MSE 损失的总和，也在 GANs 中定义。</li></ul><p id="1710" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是我们如何改进损失函数。在对模型进行了相当长时间的训练后，我们得到了下面的输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/65fb19424c7c6d6097dbed1218ece86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sa0CJe1_-cC1yfkc4r2Qtw.png"/></div></div></figure><p id="cc82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们预测图像，它与目标图像非常相似。目前，与目标图像相比，我们预测的猫具有更合理的特征。</p><p id="dc00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">值得注意的是，这就是我们如何使用不同于 GANs 的东西。如上所述，我建议用户更多地探索 fastai 库，以了解该方法背后的更多信息。</p></div></div>    
</body>
</html>