<html>
<head>
<title>Review: NIN — Network In Network (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:NIN —网络中的网络(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-nin-network-in-network-image-classification-69e271e499ee?source=collection_archive---------9-----------------------#2019-04-25">https://towardsdatascience.com/review-nin-network-in-network-image-classification-69e271e499ee?source=collection_archive---------9-----------------------#2019-04-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8ba9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用具有 1×1 卷积核的卷积层</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/f062cb3c978630a5880571e889e43df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/0*wnfNkvx87GOX3CrY.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">A few example images from the CIFAR10 dataset.</strong></figcaption></figure><p id="a39e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi lo translated"><span class="l lp lq lr bm ls lt lu lv lw di">在</span>这个故事中，对<strong class="ku ir">综合科学与工程研究生院</strong>和<strong class="ku ir">新加坡国立大学</strong>的<strong class="ku ir">网络中的网络(NIN) </strong>进行了简要回顾。具有复杂结构的微型神经网络，用于提取感受野内的数据。这是一篇<strong class="ku ir"> 2014 ICLR </strong>论文，引用超过<strong class="ku ir"> 2300 次</strong>。(<a class="lx ly ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----69e271e499ee--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="798c" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">概述</h1><ol class=""><li id="59b4" class="my mz iq ku b kv na ky nb lb nc lf nd lj ne ln nf ng nh ni bi translated"><strong class="ku ir">线性卷积层 VS mlpconv 层</strong></li><li id="221b" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln nf ng nh ni bi translated"><strong class="ku ir">全连接层 VS 全球平均池层</strong></li><li id="69c0" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln nf ng nh ni bi translated"><strong class="ku ir">网络中网络的整体结构(NIN) </strong></li><li id="438d" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln nf ng nh ni bi translated"><strong class="ku ir">结果</strong></li></ol></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="fabf" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated"><strong class="ak"> 1。线性卷积层 VS mlpconv 层</strong></h1><h2 id="916e" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated">1.1.线性卷积层</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/249a7d462f80728282efbfd03578db6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*GtkmhdamzMUKqQZ6mHOT8g.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Linear Convolutional Layer</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/46a6953420e7e861a2bb7fbe1182cb85.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*v_Fe-LFI20Z8BE5kR_-74g.png"/></div></figure><ul class=""><li id="b9a6" class="my mz iq ku b kv kw ky kz lb oc lf od lj oe ln of ng nh ni bi translated">这里(<em class="og"> i </em>，<em class="og"> j </em>)是特征图中的像素索引，<em class="og"> xij </em>代表以位置(<em class="og"> i </em>，<em class="og"> j </em>)为中心的输入面片，<em class="og"> k </em>用于索引特征图的通道。</li><li id="7db3" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">然而，实现良好抽象的表示通常是输入数据的高度非线性函数。</li><li id="88db" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">作者认为，在将它们组合成更高层次的概念之前，对每个局部补丁进行更好的抽象是有益的。</li></ul><h2 id="f61a" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated">1.2.mlpconv 层</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/877770516a386d1b912e0d2cfde57e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*Oa-HQ4r0TJ7eMb0SLj8YvQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">mlpconv Layer</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/076bb4ff511a56dfd1ee250a4268f21d.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*9clb69rSFx2SHhAyURDyrA.png"/></div></figure><ul class=""><li id="844f" class="my mz iq ku b kv kw ky kz lb oc lf od lj oe ln of ng nh ni bi translated"><em class="og"> n </em>是多层感知器的层数。在多层感知器中，校正线性单元被用作激活函数。</li><li id="940c" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">上述结构<strong class="ku ir">允许跨渠道信息的复杂和可学习的交互。</strong></li><li id="2e08" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">它<strong class="ku ir">相当于一个 1×1 卷积核</strong>的卷积层。</li></ul></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="f1d4" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated"><strong class="ak"> 2。</strong>全连接层 VS <strong class="ak">全局平均池层</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl oj"><img src="../Images/e93e9f585f8e169741fc5470c26d327e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*0-wMHcASLDFzx9YBRCZXHg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">An Example of Fully Connected Layer VS Global Average Pooling Layer</strong></figcaption></figure><h2 id="6c6c" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated"><strong class="ak"> 2.1。全连接层</strong></h2><ul class=""><li id="ef01" class="my mz iq ku b kv na ky nb lb nc lf nd lj ne ln of ng nh ni bi translated">通常，<strong class="ku ir">全连接层</strong>用于网络末端。</li><li id="c5bf" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">然而，它们<strong class="ku ir">容易过度配合</strong>。</li></ul><h2 id="162b" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated">2.2.全球平均池层</h2><ul class=""><li id="4f47" class="my mz iq ku b kv na ky nb lb nc lf nd lj ne ln of ng nh ni bi translated">这里引入了全球平均池。</li><li id="aed2" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">其思想是在最后的 mlpconv 层中为分类任务的每个相应类别生成一个特征图。我们<strong class="ku ir">没有在特征地图上添加完全连接的层，而是取每个特征地图的平均值，得到的矢量直接输入到 softmax 层。</strong></li><li id="73de" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">一个优点是，通过<strong class="ku ir">加强特征图和类别之间的对应，它对卷积结构来说更加自然。</strong></li><li id="c94e" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">另一个优点是在全局平均池中没有<strong class="ku ir">参数</strong>要优化，因此在这一层避免了<strong class="ku ir">过拟合。</strong></li><li id="9fc5" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">此外，全局平均池汇总了空间信息，因此对于输入的空间平移来说<strong class="ku ir">更加健壮。</strong></li></ul><h1 id="f801" class="mg mh iq bd mi mj ok ml mm mn ol mp mq jw om jx ms jz on ka mu kc oo kd mw mx bi translated">3.网络中网络的总体结构</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="oq or di os bf ot"><div class="gh gi op"><img src="../Images/cd70f0dd2707929e3007cd9be060f164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PoNVgeyx6_KWR8vY083VdA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Overall Structure of Network In Network (NIN)</strong></figcaption></figure><ul class=""><li id="f935" class="my mz iq ku b kv kw ky kz lb oc lf od lj oe ln of ng nh ni bi translated">这样，以上就是 NIN 的整体结构。</li><li id="5a13" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">最后是全球平均池。</li></ul></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="06b8" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">4.结果</h1><h2 id="1a89" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated">4.1.CIFAR-10</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/8c48631c9d189a89730cb77d7764b621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*59X6YpbKP_5KWG4gKKvW-w.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Error Rates on CIFAR-10 Test Set</strong></figcaption></figure><ul class=""><li id="1646" class="my mz iq ku b kv kw ky kz lb oc lf od lj oe ln of ng nh ni bi translated">NIN + Dropout 仅获得 10.41%的错误率，优于 Maxout + Dropout。</li><li id="aba6" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">通过数据扩充(翻译和水平翻转)，NIN 甚至获得了 8.81%的错误率。</li><li id="e102" class="my mz iq ku b kv nj ky nk lb nl lf nm lj nn ln of ng nh ni bi translated">(有兴趣的话，<a class="ae ov" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener"> NoC </a>里有一个非常简短的关于 Maxout 的介绍。)</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/d5767b92852a62bcc62df39bcc59e4e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*Al5SFW2siSFGXImW0cFn5A.png"/></div></figure><ul class=""><li id="ae29" class="my mz iq ku b kv kw ky kz lb oc lf od lj oe ln of ng nh ni bi translated">如上所示，在 mlpconv 层之间引入脱落层将测试误差降低了 20%以上。</li></ul><h2 id="1f14" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated">4.1.西发尔-100</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/7e19a59aaa1dd883bca47cd24a93dddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*XZ8AffpU1fO39G4cM6Hkfw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Error Rates on CIFAR-100 Test Set</strong></figcaption></figure><ul class=""><li id="23f1" class="my mz iq ku b kv kw ky kz lb oc lf od lj oe ln of ng nh ni bi translated">类似地，NIN + Dropout 仅获得 35.68%的错误率，这优于 Maxout + Dropout。</li></ul><h2 id="c1b5" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated">4.3.街景门牌号(SVHN)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/b4dfba54ed80e36f55caf25879aa9a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*rt3ONvKJ8WFNw4uDekpsYA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Error Rates on SVHN Test Set</strong></figcaption></figure><ul class=""><li id="6b27" class="my mz iq ku b kv kw ky kz lb oc lf od lj oe ln of ng nh ni bi translated">但是 NIN + Dropout 得到了 2.35%的错误率，比 DropConnect 还要差。</li></ul><h2 id="0be8" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated">4.4.MNIST</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/b692591072304c61e542be87f2efd168.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*NLEi6neDPXzpToDWHeEDUw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Error Rates on MNIST Test Set</strong></figcaption></figure><ul class=""><li id="c781" class="my mz iq ku b kv kw ky kz lb oc lf od lj oe ln of ng nh ni bi translated">在 MNIST，NIN + Dropout 得到 0.47%的错误率，比 Maxout + Dropout 差一点。</li></ul><h2 id="728b" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated">4.5.作为调整者的全球平均池</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/31d66a37fe0c778cf261616096bb5db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*Ja0BGTbzf3kYAaEyX6q8aQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Error Rates on CIFAR-10 Test Set</strong></figcaption></figure><ul class=""><li id="5d0c" class="my mz iq ku b kv kw ky kz lb oc lf od lj oe ln of ng nh ni bi translated">使用全球平均池，NIN 获得了 10.41%的错误率，这比完全连接+10.88%的退出要好。</li></ul></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="32eb" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在 NIN 中，对于 1×1 卷积，引入了更多的非线性，这使得错误率更低。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h2 id="e963" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated">参考</h2><p id="e5b7" class="pw-post-body-paragraph ks kt iq ku b kv na jr kx ky nb ju la lb pb ld le lf pc lh li lj pd ll lm ln ij bi translated">【2014 ICLR】【NIN】<br/><a class="ae ov" href="https://arxiv.org/abs/1312.4400" rel="noopener ugc nofollow" target="_blank">网络中的网络</a></p><h2 id="62de" class="no mh iq bd mi np nq dn mm nr ns dp mq lb nt nu ms lf nv nw mu lj nx ny mw nz bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph ks kt iq ku b kv na jr kx ky nb ju la lb pb ld le lf pc lh li lj pd ll lm ln ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">物体检测<br/></strong><a class="ae ov" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae ov" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae ov" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae ov" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae ov" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae ov" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae ov" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae ov" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae ov" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae ov" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4">G-RMI</a>][<a class="ae ov" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae ov" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae ov" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae ov" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae ov" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae ov" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3</a>[<a class="ae ov" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>[<a class="ae ov" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a>[<a class="ae ov" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a></p><p id="6582" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">语义切分<br/></strong><a class="ae ov" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae ov" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae ov" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae ov" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae ov" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae ov" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae ov" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae ov" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae ov" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae ov" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">生物医学图像分割<br/></strong>[<a class="ae ov" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae ov" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae ov" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae ov" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae ov" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae ov" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae ov" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae ov" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>][<a class="ae ov" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1">M FCN<br/></a></p><p id="3134" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">实例分割<br/> </strong> [ <a class="ae ov" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a> ] [ <a class="ae ov" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae ov" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae ov" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae ov" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae ov" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae ov" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae ov" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></p><p id="58de" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir"/><br/><a class="ae ov" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">【DeepPose】</a><a class="ae ov" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">【汤普森 NIPS'14】</a><a class="ae ov" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">【汤普森 CVPR'15】</a></p></div></div>    
</body>
</html>