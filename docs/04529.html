<html>
<head>
<title>Reinforcement Learning — Model Based Planning Methods Extension</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——基于模型的规划方法扩展</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb?source=collection_archive---------6-----------------------#2019-07-12">https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb?source=collection_archive---------6-----------------------#2019-07-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="184a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Dyna-Q+和优先级扫描的实现</h2></div><p id="3f1e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上一篇<a class="ae lb" rel="noopener" target="_blank" href="/reinforcement-learning-model-based-planning-methods-5e99cae0abb8">文章</a>中，我们介绍了如何在强化学习环境中建模，以及如何利用该模型加速学习过程。在本文中，我想进一步阐述这个主题，并介绍另外两个算法，<strong class="kh ir"> Dyna-Q+ </strong>和<strong class="kh ir">优先级扫描</strong>，这两个算法都基于我们在上一篇文章中学习的 Dyna-Q 方法。(如果你觉得有些游戏设定混乱，请查看我上一篇<a class="ae lb" rel="noopener" target="_blank" href="/reinforcement-learning-model-based-planning-methods-5e99cae0abb8">文章</a>)</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/50a066a4a2d121697b8a99a1240086f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wQ_jbKkQNTPJ3TUtubXh0A.jpeg"/></div></div></figure><p id="5bd8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下面的段落中，我们将利用这两种算法来解决两个问题:</p><ol class=""><li id="54f9" class="lo lp iq kh b ki kj kl km ko lq ks lr kw ls la lt lu lv lw bi translated">模型错了怎么办？</li><li id="cc86" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">如何更高效的更新 Q 函数？</li></ol><h1 id="c1fa" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">模型错了怎么办？</h1><p id="7bc8" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">在上一篇文章中，我介绍了一个 Dyna-Maze 的例子，其中的动作是确定性的，代理通过记录它所经历的步骤来学习模型，这是从<code class="fe mz na nb nc b">(currentState, action) -&gt; (nextState, reward)</code>开始的映射，然后在规划阶段，正在学习的模型被应用<code class="fe mz na nb nc b">n</code>次以加强学习过程。简而言之，这个过程可以概括为</p><ol class=""><li id="1e2e" class="lo lp iq kh b ki kj kl km ko lq ks lr kw ls la lt lu lv lw bi translated">通过经验学习模型</li><li id="61fe" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">充分信任该模型，并应用它来强化价值函数</li></ol><p id="9be3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是事情并不总是这样，因为环境可能是复杂和动态的。</p><blockquote class="nd ne nf"><p id="fcd5" class="kf kg ng kh b ki kj jr kk kl km ju kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">模型可能是不正确的，因为环境是随机的，并且只有有限数量的样本被观察到，或者因为模型是使用函数近似法学习的，而函数近似法并不完全概括，或者仅仅因为环境已经改变并且其新的行为还没有被观察到。当模型不正确时，规划过程可能会计算出一个次优的策略。</p></blockquote><h2 id="45c0" class="nk md iq bd me nl nm dn mi nn no dp mm ko np nq mo ks nr ns mq kw nt nu ms nv bi translated">捷径迷宫</h2><p id="a3fe" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">考虑一个叫做捷径迷宫的案例，环境是动态变化的。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nw"><img src="../Images/5949441d39b6aa0c195740c327f78c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ofSCF7jFAenddm7bjy8-A.png"/></div></div></figure><p id="6136" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个代理从<code class="fe mz na nb nc b">S</code>出发，目的是尽快到达<code class="fe mz na nb nc b">G</code>，黑灰色的区域是代理不能通过的区域。左边的图片代表原始设置，我们的代理能够使用 Dyna-Q 方法找到最短的路径，通过棋盘的左侧一直到<code class="fe mz na nb nc b">G</code>。但是，环境会在某个时间戳发生变化，棋盘最右边的一个快捷方式会打开。在这种设置下，Dyna-Q 代理还能找到最优解吗？</p><p id="323d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">答案是否定的。即使使用ϵ-greedy 方法，代理人总是以一定的概率探索，也不太可能找到从最左边到最右边的最优路径，因为已经学习的 q 函数将总是引导代理人选择左边的路径，并且没有足够强的动机或奖励推动代理人探索其他路径。</p><h2 id="06b0" class="nk md iq bd me nl nm dn mi nn no dp mm ko np nq mo ks nr ns mq kw nt nu ms nv bi translated">Dyna-Q+和实现</h2><p id="5891" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">如何解决这个问题？其本质是保持代理人能够探索新的状态以适应不断变化的环境，而驱动代理人探索的诀窍是给予奖励。因此，这里我们介绍一下 Dyna-Q+的理论:</p><blockquote class="nd ne nf"><p id="d5d9" class="kf kg ng kh b ki kj jr kk kl km ju kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated"><strong class="kh ir">代理跟踪每个状态-动作对，记录自从该对在与环境的真实交互中最后一次尝试</strong>以来已经过去了多少时间步。经过的时间越长，这一对的动态变化和模型不正确的可能性就越大(我们可以假设)。为了鼓励测试长期未尝试行为的行为，对涉及这些行为的模拟体验给予特殊的“额外奖励”。特别是，如果转换的模型奖励是<code class="fe mz na nb nc b">r</code>，并且在<code class="fe mz na nb nc b">τ</code>时间步中没有尝试转换，那么计划更新被完成，就好像对于一些小的<code class="fe mz na nb nc b">κ</code>，转换产生了<code class="fe mz na nb nc b">r + κ*sqrt(τ)</code>的奖励。这鼓励代理继续测试所有可访问的状态转换，甚至找到长的动作序列来执行这样的测试。</p></blockquote><p id="cf14" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总而言之，该算法与 Dyna-Q 完全相同，除了<strong class="kh ir">它跟踪一个状态被访问的次数，并奖励长期未被访问的状态</strong>(因为这些状态可能会随着时间的推移而改变)。</p><p id="9163" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们对 Dyna-Q 进行一些修改，并实现 Dyna-Q+(由于基本设置基本相同，下面的代码我将主要关注不同之处)。你也可以在这里查看完整的实现<a class="ae lb" href="https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaQ%2B.py" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="5351" class="nk md iq bd me nl nm dn mi nn no dp mm ko np nq mo ks nr ns mq kw nt nu ms nv bi translated">初始化</h2><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="ad70" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 init 函数中，添加了两个组件来奖励未访问的状态。<code class="fe mz na nb nc b">self.time</code>记录每集内的总时间步数<strong class="kh ir">(游戏结束后会重置)，而<code class="fe mz na nb nc b">self.timeWeight</code>本质上是奖励函数中的<code class="fe mz na nb nc b">κ</code>，表示我们希望代理探索的程度。除了这些通用设置之外，环境模型再次被初始化，但这次是<code class="fe mz na nb nc b">(currentState, action) -&gt; (reward, nxtState, timestep)</code>的映射。</strong></p><h2 id="a443" class="nk md iq bd me nl nm dn mi nn no dp mm ko np nq mo ks nr ns mq kw nt nu ms nv bi translated">模型更新</h2><p id="9278" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">为了使代码更有条理，定义了一个更新模型函数:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="542b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该函数在每个时间步更新模型。需要注意的是<strong class="kh ir">我们希望在计划阶段考虑从未尝试过的行动，我们将这些行动定义为将代理带回到原始状态，奖励为 0 </strong>，因此您可以看到，对于从未发生的行动，模型为:</p><pre class="ld le lf lg gt nz nc oa ob aw oc bi"><span id="5374" class="nk md iq nc b gy od oe l of og">self.model[state][a] = (0, state, 1)</span></pre><p id="da48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">奖励设置为 0，状态设置为相同，时间步长设置为 1(因此未访问该状态的次数可能很高)。此外，对于在该状态下发生动作，将用当前时间步长进行标记。</p><h2 id="e444" class="nk md iq bd me nl nm dn mi nn no dp mm ko np nq mo ks nr ns mq kw nt nu ms nv bi translated">播放功能</h2><p id="96e7" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">播放函数遵循与 Dyna-Q 方法相同的结构:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="a50d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每一步之后，我们通过调用<code class="fe mz na nb nc b">self.updateModel()</code>函数来更新模型，存储在模型中的时间步长在下面的循环中使用，以添加到奖励中:</p><pre class="ld le lf lg gt nz nc oa ob aw oc bi"><span id="020f" class="nk md iq nc b gy od oe l of og">_reward += self.timeWeight * np.sqrt(self.time - _time)</span></pre><p id="1f32" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了在一个州采取特定行动的原始奖励之外，还分配了额外的奖励。<code class="fe mz na nb nc b">self.time - _time</code>本质上是未被访问的次数。</p><h2 id="7bd8" class="nk md iq bd me nl nm dn mi nn no dp mm ko np nq mo ks nr ns mq kw nt nu ms nv bi translated">Dyna-Q 和 Dyna-Q+比较</h2><p id="ef7e" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">通过对非探索状态给予额外奖励，Dyna-Q+更容易察觉到环境的变化，而 Dyna-Q 几乎做不到。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oh"><img src="../Images/9af5f7decfbf75e685fdda1864dad91f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7xemwuKDtSXgVS-Cl1gfQ.png"/></div></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk">From Reinforcement Learning an Introduction</figcaption></figure><p id="95f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参考 Sutton 书中的结果，当环境在时间步长 3000 发生变化时，Dyna-Q+方法能够逐渐感知这些变化，并最终找到最优解，而 Dyna-Q 总是遵循它之前发现的相同路径。事实上，由于规划步骤加强了 Dyna-Q 中的经验，规划步骤越多，Dyna-Q 代理找到最佳路径的可能性就越小，相反，Dyna-Q+中的规划步骤增加了未充分探索的状态和动作的值，从而使代理更有可能探索和找到最佳路径。</p><h1 id="ce96" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">优先级扫描(更新更高效)</h1><p id="7481" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我们现在已经了解了动态环境下形成强化学习的基础知识。你可能注意到了<strong class="kh ir">在计划阶段，实际上有很多无效更新</strong>，尤其是在所有状态和动作的 Q 函数都为 0，沿途奖励也为 0 的开始阶段。在这些场景中，暂时的差异，</p><pre class="ld le lf lg gt nz nc oa ob aw oc bi"><span id="5ade" class="nk md iq nc b gy od oe l of og">R + Q(S', A') - Q(S, A)</span></pre><p id="ee68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">等于 0，因而 Q 函数更新了许多状态，动作仍为 0。那么问题来了:我们能够更高效地更新吗？</p><p id="95a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我将介绍一种叫做优先级扫描的方法，重点是在规划阶段更新非零值。直觉是，由于许多更新是 0，我们是否能够只更新高于某个阈值的值，从而使学习更快？</p><blockquote class="nd ne nf"><p id="ad64" class="kf kg ng kh b ki kj jr kk kl km ju kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">根据更新的紧急程度确定更新的优先级，并按照优先级顺序执行更新是很自然的。这就是优先清扫背后的想法。维护每个状态-动作对的队列，这些状态-动作对的估计值如果被更新将会发生非平凡的变化，并根据变化的大小区分优先级。</p></blockquote><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi om"><img src="../Images/e6aa60748a37254bd8e452986ae3592c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10UTaQiLvvxEtxQo-pmkQg.png"/></div></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk">Priority Sweeping</figcaption></figure><p id="4055" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有几点你需要注意:</p><ol class=""><li id="2eb6" class="lo lp iq kh b ki kj kl km ko lq ks lr kw ls la lt lu lv lw bi translated">模型是<code class="fe mz na nb nc b">(currentState, action) -&gt; (reward, nxtState)</code>的映射</li><li id="0f6e" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">只有大于<code class="fe mz na nb nc b">θ</code>的时间差值将被包括在队列中</li><li id="9f62" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">在计划阶段，所有导致所选状态的状态也将被更新</li></ol><p id="5ef4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">在非平凡更新状态之后，所有在先状态需要被更新的原因是因为当前状态更新值是非平凡的，向后更新将绝对导致非零更新。</strong> <em class="ng">(考虑我们在值迭代中谈到的例子，当目标达到时，我们进行向后更新，在此过程中更新的所有状态值都是非零和有用的)</em></p><h2 id="d45a" class="nk md iq bd me nl nm dn mi nn no dp mm ko np nq mo ks nr ns mq kw nt nu ms nv bi translated">算法实现</h2><p id="f489" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">是时候着手实现了，您可以在这里查看完整的实现<a class="ae lb" href="https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/PrioritySweeping.py" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="ba43" class="nk md iq bd me nl nm dn mi nn no dp mm ko np nq mo ks nr ns mq kw nt nu ms nv bi translated">初始化</h2><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="550d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<code class="fe mz na nb nc b">init</code>函数中，我们初始化了一个阈值<code class="fe mz na nb nc b">θ</code>，一个根据优先级存储状态和动作对的优先级队列，以及一个前趋字典，以便更新所有导致当前更新状态的状态。</p><p id="5e4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">播放功能</strong></p><p id="4bfa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">主要区别在于播放功能:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="c1af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每一步，不是直接更新当前状态动作对的 Q 值，而是记录一个<code class="fe mz na nb nc b">tmp_diff</code>，如果该值足够大，则将其插入优先级队列。然后，模型和前任都被更新，注意前任字典是一个<code class="fe mz na nb nc b">nxtState -&gt; List((currentState, action), ...)</code>的映射，因为许多状态和动作可能导致相同的状态。</p><p id="5006" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在计划阶段(在<code class="fe mz na nb nc b">for</code>循环中)，最高优先级状态，动作对被检索(<code class="fe mz na nb nc b">self.queue.get(1)</code>，并且导致该状态的所有状态(在<code class="fe mz na nb nc b">pre_state_action_list</code>内)被更新。</p><h2 id="15b8" class="nk md iq bd me nl nm dn mi nn no dp mm ko np nq mo ks nr ns mq kw nt nu ms nv bi translated">Dyna-Q 和优先级扫描比较</h2><p id="eec5" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">如上所述，优先级扫描会在整个过程中更新重要的值，因此效率更高，速度更快。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi on"><img src="../Images/6bb48092404f2c38c55ddbfc9bd36bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NDF7Wtc7k6JIsfzxEQX11g.png"/></div></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk">From Reinforcement Learning an Introduction</figcaption></figure><p id="1329" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参考 Sutton 书中的情节，priority 比 Dyna-Q 更快地找到最优解。</p><p id="730d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面介绍的算法是针对确定性环境，针对非确定性环境，正如萨顿所说:</p><blockquote class="nd ne nf"><p id="e3f8" class="kf kg ng kh b ki kj jr kk kl km ju kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">将优先扫描扩展到随机环境是简单的。通过记录每个状态-动作对已经经历的次数以及下一个状态是什么来维护该模型。然后，很自然地，不是像我们到目前为止一直使用的那样用样本更新来更新每一对，而是用预期更新来更新，考虑所有可能的下一状态及其发生的概率。</p></blockquote><h1 id="3831" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">结论</h1><p id="826d" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">在这篇文章中，我们学习了两个算法，要点是:</p><ol class=""><li id="bb1e" class="lo lp iq kh b ki kj kl km ko lq ks lr kw ls la lt lu lv lw bi translated">Dyna-Q+是为不断变化的环境而设计的，它对未充分利用的状态、动作对给予奖励，以驱动代理进行探索</li><li id="fb0e" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">优先级扫描能够通过用非平凡更新来更新和传播值来加速学习过程</li></ol><p id="a602" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，请查看我的 Github。欢迎您投稿，如果您有任何问题或建议，请在下面发表评论！</p><p id="d732" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考</strong>:</p><ul class=""><li id="a422" class="lo lp iq kh b ki kj kl km ko lq ks lr kw ls la oo lu lv lw bi translated"><a class="ae lb" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/the-book-2nd.html</a></li><li id="a655" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la oo lu lv lw bi translated"><a class="ae lb" href="https://github.com/JaeDukSeo/reinforcement-learning-an-introduction" rel="noopener ugc nofollow" target="_blank">https://github . com/JaeDukSeo/reinforcement-learning-an-introduction</a></li></ul></div></div>    
</body>
</html>