<html>
<head>
<title>On-Device Machine Learning: Text Generation on Android 📝</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">设备上的机器学习:Android 上的文本生成📝</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/on-device-machine-learning-text-generation-on-android-6ad940c00911?source=collection_archive---------10-----------------------#2019-12-19">https://towardsdatascience.com/on-device-machine-learning-text-generation-on-android-6ad940c00911?source=collection_archive---------10-----------------------#2019-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/0c164a90cccd70031f00f219923d0019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xdrcJT-FkrAMqkPX"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@sebastian123?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Pereanu Sebastian</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="10b3" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">结合 GPT-2、TensorFlow 和 Kotlin 的力量，在移动设备上实现最先进的自然语言处理</h2></div><p id="fd2a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在拥抱脸，我们的目标是解决和民主化自然语言处理(NLP)。目前，生产中的大多数模型都是在服务器上远程运行的，例如谷歌的搜索服务器。尽管如此，<strong class="la jk">移动设备上硬件的改进和对隐私的日益关注使得它们越来越适合离线运行</strong>。</p><p id="41bf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文的目标是给出一个完全在设备上运行的用于文本生成的 Android 应用程序<strong class="la jk">的高级视图。代码在这里:<a class="ae jg" href="https://github.com/huggingface/tflite-android-transformers/tree/master/gpt2" rel="noopener ugc nofollow" target="_blank">https://github . com/hugging face/TF lite-Android-transformers/tree/master/gp T2</a></strong></p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/6cea666a19655cb32c71bb064cd2e0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/1*8htmR6xWiznbf_ozdNjxnQ.gif"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">What we’re going to build 🤓</figcaption></figure></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><h1 id="1240" class="mg mh jj bd mi mj mk ml mm mn mo mp mq kp mr kq ms ks mt kt mu kv mv kw mw mx bi translated">第一部分:将 GPT-2 转换为 TensorFlow Lite 格式</h1><p id="537b" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated"><a class="ae jg" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>是 2019 年发布的一款车型，其自然语言生成能力(NLG，NLP 的一个子集)令人印象深刻，以至于最大版本的发布被推迟了几个月。<strong class="la jk">你可以用</strong> <a class="ae jg" href="https://transformer.huggingface.co/doc/gpt2-large" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">这个搞笑(吓人？)工具</strong> </a> <strong class="la jk">我们发布</strong>。在这个应用程序中，我们将使用最小版本的模型。它的发电能力不如最大的那台令人印象深刻，但它的大小(500MB <em class="nd">对</em> 6GB)使它更适合移动使用！</p><p id="b972" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在能够在设备上运行之前，<strong class="la jk">我们需要将它转换成合适的格式</strong>、<a class="ae jg" href="https://www.tensorflow.org/lite" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite </a> (TFLite)。为此，我们可以运行以下 Python 脚本:</p><figure class="lv lw lx ly gt iv"><div class="bz fp l di"><div class="ne nf l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">“<a class="ae jg" href="https://pypi.org/project/tf-nightly/" rel="noopener ugc nofollow" target="_blank">tf-nightly</a>” and “<a class="ae jg" href="https://pypi.org/project/transformers/" rel="noopener ugc nofollow" target="_blank">transformers</a>” libraries need to be installed in your environment. You can also try it directly in your browser using <a class="ae jg" href="https://colab.research.google.com/drive/18JPzizAH995pd0iFWx4Xdf-sqjmZwHUD" rel="noopener ugc nofollow" target="_blank">this colab notebook</a>.</figcaption></figure><p id="4528" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个脚本使用了我们的<a class="ae jg" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">🤗Transformers </a>库导入“原始”模型，然后将其转换为 TFLite 格式。<strong class="la jk">注意脚本</strong>的第 15/16 行:在运行转换之前，我们使用 TFLite 指定我们想要<a class="ae jg" href="https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-float16-quantization-halves-model-size-cc113c75a2fa" rel="noopener">将模型</a>的权重(参数)量化为<em class="nd">半精度浮点格式</em>。这导致我们转换后的模型的最终大小为 237MB，即<strong class="la jk">原始“输入”模型大小的一半🎉。</strong>不利方面？精确度损失极小，但考虑到节省的存储空间，在移动设备上绝对值得！</p><blockquote class="ng nh ni"><p id="db11" class="ky kz nd la b lb lc kk ld le lf kn lg nj li lj lk nk lm ln lo nl lq lr ls lt im bi translated">我们可以通过将权重转换为 8 位整数表示形式来进一步压缩我们的模型，结果只有 128MB。但是我们对这个版本的测试显示在设备上要慢得多。因此，我们更喜欢在这里使用半精度浮点版本。你仍然可以通过<a class="ae jg" href="https://github.com/huggingface/tflite-android-transformers/tree/master/gpt2#change-the-model" rel="noopener ugc nofollow" target="_blank">改变默认模式</a>来试验我们的应用程序的 8 位版本。</p></blockquote></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><h1 id="07ca" class="mg mh jj bd mi mj mk ml mm mn mo mp mq kp mr kq ms ks mt kt mu kv mv kw mw mx bi translated">第二部分:将转换后的 GPT-2 模型集成到 Android 应用程序中</h1><p id="a57e" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">既然我们已经转换了我们的模型，我们可以专注于实际构建我们的应用程序。<a class="ae jg" href="https://github.com/huggingface/tflite-android-transformers/tree/master/gpt2" rel="noopener ugc nofollow" target="_blank">GitHub</a>上有完整的源代码，所以这里我只关注最有趣的部分。</p><p id="8f55" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 Python 脚本中，我们指定(第 6/7 行)我们的模型将接受一个形状为<em class="nd"> [1，64] </em>的<strong class="la jk">二维整数</strong>数组作为输入，即类似这样的内容，其中内部数组包含 64 个元素:</p><pre class="lv lw lx ly gt nm nn no np aw nq bi"><span id="ac89" class="nr mh jj nn b gy ns nt l nu nv">[[142, 34, 100, 535, 30234, 45, 2934, ...]]</span></pre><p id="e305" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是我们在现实生活中将要拥有的是一个字符串，对应于当前文本。因此，我们需要将该字符串转换成整数，<em class="nd">又称为</em> <strong class="la jk"> <em class="nd">记号</em> </strong>。粗略地说，我们可以说,<strong class="la jk">一个令牌是我们的字符串</strong>的一部分的数字表示。</p><p id="261b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nd">令牌</em>也是模型作为输出返回的内容。模型的每一次运行都允许我们确定文本的下一个标记，然后我们将它与前一个文本一起传递给我们的模型进行下一次运行，<em class="nd">等等</em> …</p><p id="4b53" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要一些东西来把我们的字符串转换成记号，再把记号转换回字符串。这就是<em class="nd">记号赋予器</em>的作用。记号赋予器的两个主要功能通常是<em class="nd">编码</em>和<em class="nd">解码。</em></p><figure class="lv lw lx ly gt iv"><div class="bz fp l di"><div class="ne nf l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Full implementation of the Tokenizer available <a class="ae jg" href="https://github.com/huggingface/tflite-android-transformers/blob/master/gpt2/src/main/java/co/huggingface/android_transformers/gpt2/tokenization/GPT2Tokenizer.kt" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="d092" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nd"> encode </em>函数将我们的起始/前一个文本作为参数，使用正则表达式对其进行解析，然后将每个字符转换为特定的表示。它最后应用一个<a class="ae jg" href="https://en.wikipedia.org/wiki/Byte_pair_encoding" rel="noopener ugc nofollow" target="_blank">字节对编码</a> (BPE)算法，由于有了<a class="ae jg" href="https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json" rel="noopener ugc nofollow" target="_blank">模型词汇表</a>，该算法的输出被映射成整数。🤯</p><p id="3915" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nd"> decode </em>函数执行相反的操作，将标记映射到它们的词汇表示，然后将这个表示解码为最终的字符串。</p><p id="ea45" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们知道如何编码和解码我们的文本，我们可以调用我们的模型！这是下面的<em class="nd">生成</em>功能的作用:</p><figure class="lv lw lx ly gt iv"><div class="bz fp l di"><div class="ne nf l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://github.com/huggingface/tflite-android-transformers/blob/master/gpt2/src/main/java/co/huggingface/android_transformers/gpt2/ml/GPT2Client.kt" rel="noopener ugc nofollow" target="_blank">Click here</a> to see the full implementation</figcaption></figure><p id="280e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该函数的输入是初始文本和我们想要生成的令牌数(<em class="nd">，即我们的模型被调用的次数</em>)。第一步是对我们的文本进行标记。</p><p id="8fe1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还记得我们说过模型的输入是一个形状数组<em class="nd">【1，64】</em>吗？我们需要去掉之前的文本标记，只保留最后的最大值。那是我们的<em class="nd">输入。</em>表示<strong class="la jk">下一个令牌的生成只依赖于这 64 个之前的令牌，忽略任何之前的令牌</strong>。</p><blockquote class="ng nh ni"><p id="6031" class="ky kz nd la b lb lc kk ld le lf kn lg nj li lj lk nk lm ln lo nl lq lr ls lt im bi translated">当我们转换我们的模型时，我们可以指定一个更长的序列长度，但是这将意味着更多的推理计算，减慢我们的应用程序。这是我们这一代人在速度和“质量”之间的权衡。<em class="jj">🤔</em></p></blockquote><p id="40b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还需要创建数据结构，我们的模型将使用它的输出。我们的模型有许多输出，但是我们只对第一个“预测”感兴趣。</p><pre class="lv lw lx ly gt nm nn no np aw nq bi"><span id="e58a" class="nr mh jj nn b gy ns nt l nu nv">val predictions = Array(1) <strong class="nn jk">{ </strong>Array(SEQUENCE_LENGTH) <strong class="nn jk">{ </strong>FloatArray(VOCAB_SIZE) <strong class="nn jk">} }</strong></span></pre><p id="87dd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当谈到多维数组时，我们在 Kotlin 的表达能力方面达到了一个极限；下面是它在 Java 中的样子:</p><pre class="lv lw lx ly gt nm nn no np aw nq bi"><span id="bd10" class="nr mh jj nn b gy ns nt l nu nv">float[][][] predictions = new float[1][SEQUENCE_LENGTH][VOCAB_SIZE]</span></pre><p id="2e36" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我远不是一个 Java 迷，但是右边的表达对我来说似乎更容易读懂！</p><p id="f7f3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们终于可以——了！—通过调用 TFLite 解释器来运行我们的模型:</p><pre class="lv lw lx ly gt nm nn no np aw nq bi"><span id="3a5e" class="nr mh jj nn b gy ns nt l nu nv">tflite.runForMultipleInputsOutputs(arrayOf(inputIds), outputs)</span></pre><p id="2705" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦解释器填充了我们的“预测”数组，<strong class="la jk">我们需要确定将是我们的“下一个”令牌</strong>。这样做有许多不同的方法；这里我们首先使用<em class="nd"> Top-K </em>过滤，选择<em class="nd"> k </em>更高的预测。然后，我们应用一个<a class="ae jg" href="https://www.wikiwand.com/en/Softmax_function" rel="noopener ugc nofollow" target="_blank"> <em class="nd"> Softmax </em>函数</a>来获得这些值的概率分布，然后通过多项式采样最终选择“那一个”。</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><h1 id="12fa" class="mg mh jj bd mi mj mk ml mm mn mo mp mq kp mr kq ms ks mt kt mu kv mv kw mw mx bi translated">第三部分:借助 Kotlin 协程，以用户界面友好的方式与活动交互</h1><p id="eb17" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">现在是时候将我们的模型链接到应用程序的接口了！<strong class="la jk">在设备上运行 GPT-2 等模型，即使是量化版本，也需要计算资源</strong>。如果我们做错了，我们可能会在模型运行时以界面冻结而告终，这<strong class="la jk">对用户不太友好</strong>！😱</p><p id="cbec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了避免这种糟糕的结果，<strong class="la jk">我们将使用</strong> <a class="ae jg" href="https://kotlinlang.org/docs/reference/coroutines-overview.html" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">协程</strong> </a> <strong class="la jk">，这是在 Kotlin </strong>中进行非阻塞编程的一种非常好的方式。这是我们(几乎)完整的<em class="nd"> GPT2Client </em>类，它是从我们的主活动加载的<a class="ae jg" href="https://developer.android.com/topic/libraries/architecture/viewmodel" rel="noopener ugc nofollow" target="_blank"> <em class="nd"> ViewModel </em> </a>:</p><figure class="lv lw lx ly gt iv"><div class="bz fp l di"><div class="ne nf l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">For full implementation, <a class="ae jg" href="https://github.com/huggingface/tflite-android-transformers/blob/master/gpt2/src/main/java/co/huggingface/android_transformers/gpt2/ml/GPT2Client.kt" rel="noopener ugc nofollow" target="_blank">check here</a></figcaption></figure><p id="3939" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该类首先需要加载我们模型的所有资产，并初始化 TFLite 解释器。为了在不阻塞 UI 的情况下做到这一点，在<em class="nd"> init </em>块<strong class="la jk">中，我们启动了一个新的协程</strong>，这要归功于<a class="ae jg" href="https://developer.android.com/topic/libraries/architecture/coroutines" rel="noopener ugc nofollow" target="_blank"><em class="nd">viewmodelscope . launch</em></a>。在这个协程中，我们通过调用 3 个"<em class="nd"> load" </em>方法来加载我们的模型资产。以下是<em class="nd">负载模型</em>的签名:</p><pre class="lv lw lx ly gt nm nn no np aw nq bi"><span id="222c" class="nr mh jj nn b gy ns nt l nu nv">private suspend fun loadModel() = <strong class="nn jk">withContext(Dispatchers.IO)</strong> <strong class="nn jk">{<br/>    </strong>// Load the model file and initialize the interpreter with it...<strong class="nn jk"><br/>}</strong></span></pre><p id="e2aa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里重要的是带有上下文的<em class="nd">(调度程序。</em>IO)部分。我们说<strong class="la jk">我们想在一个不同于主线程</strong>的线程上执行这个方法，这里使用一个为 I/O 操作设计的线程(<a class="ae jg" href="https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/-coroutine-dispatcher/index.html" rel="noopener ugc nofollow" target="_blank">更多细节见这里</a>)。</p><blockquote class="ng nh ni"><p id="5c29" class="ky kz nd la b lb lc kk ld le lf kn lg nj li lj lk nk lm ln lo nl lq lr ls lt im bi translated">通过<em class="jj"> viewModelScope.launch </em>创建协程的“妙处”在于它将自己的生命周期与<em class="jj"> ViewModel </em>的生命周期联系在一起。它确保当<em class="jj">视图模型</em>被清除时，协程被取消！<em class="jj">🙌</em></p></blockquote><p id="1a4b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，当用户点击应用程序中的“触发自动完成”按钮时，执行<em class="nd"> launchAutocomplete </em>方法，<strong class="la jk">创建另一个协程</strong>，我们将从其中调用我们的模型。</p><pre class="lv lw lx ly gt nm nn no np aw nq bi"><span id="6301" class="nr mh jj nn b gy ns nt l nu nv">fun launchAutocomplete() {<br/>    autocompleteJob = <strong class="nn jk">viewModelScope.launch</strong> <strong class="nn jk">{<br/>        </strong>initJob.<strong class="nn jk">join</strong>()<br/>        autocompleteJob?.<strong class="nn jk">cancelAndJoin</strong>()<br/>        _completion.value = ""<br/>        generate(_prompt.value!!)<br/>    <strong class="nn jk">}<br/></strong>}</span></pre><p id="0702" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个协程中，我们首先确保资产的初始化(<em class="nd"> initJob </em>)已经完成，然后对潜在的先前模型运行(<em class="nd"> autocompleteJob </em>)，<strong class="la jk">做同样的事情，如果还在运行</strong>，我们就取消它。然后我们可以调用我们的<em class="nd">生成</em>方法:</p><pre class="lv lw lx ly gt nm nn no np aw nq bi"><span id="8ef1" class="nr mh jj nn b gy ns nt l nu nv">private suspend fun generate(text: String, nbTokens: Int = 100) = <strong class="nn jk">withContext(Dispatchers.Default)</strong> <strong class="nn jk">{<br/>    </strong>val tokens = tokenizer.encode(text)<br/>    repeat (nbTokens) <strong class="nn jk">{<br/>        </strong>// Run the model...<br/>        // ...</span><span id="049b" class="nr mh jj nn b gy nw nt l nu nv">        tokens.add(nextToken)<br/>        val decodedToken = tokenizer.decode(listOf(nextToken))<br/>        _completion.postValue(_completion.value + decodedToken)<br/><br/>        <strong class="nn jk">yield()</strong><br/>    <strong class="nn jk">}<br/>}</strong></span></pre><p id="973a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用于此方法的调度员不是<em class="nd">调度员。IO </em>因为我们在这里不做任何 I/O 操作，而是一个更通用的<strong class="la jk"> <em class="nd">调度程序。默认</em> </strong>使用共享后台线程的公共池<em class="nd">。</em></p><p id="ef5f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个方法另一个有趣的部分是<a class="ae jg" href="https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/yield.html" rel="noopener ugc nofollow" target="_blank"><strong class="la jk"><em class="nd">yield()</em></strong></a>方法调用在<em class="nd">结束时重复</em>块。<strong class="la jk">这就是允许该方法检查最终取消的原因。没有它，就不可能取消，我们必须等到整个一代结束后才能释放资源！☠️ </strong>这里我们在每次令牌生成后检查取消。</p><blockquote class="ng nh ni"><p id="7783" class="ky kz nd la b lb lc kk ld le lf kn lg nj li lj lk nk lm ln lo nl lq lr ls lt im bi translated">检查取消的另一种方式是检查<a class="ae jg" href="https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/is-active.html" rel="noopener ugc nofollow" target="_blank"> <em class="jj"> isActive </em>属性</a>的值</p></blockquote><p id="b1ad" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于使用了<a class="ae jg" href="https://developer.android.com/topic/libraries/architecture/livedata" rel="noopener ugc nofollow" target="_blank"> LiveData 结构</a>(我们的<em class="nd">完成</em>属性)，完成的文本然后“自动地”显示在应用程序中。🧙‍♀️</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="0d39" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就是这样！在拥抱脸，我们相信我们只是处于人工智能在设备上运行的时代的开始。随着一方面专用硬件和相关驱动程序和框架的不断发展，另一方面量化和提取等技术的不断发展，<strong class="la jk">我们智能手机的功能有望拥有光明的未来</strong>，允许以更高效和更高性能的方式运行更复杂的模型。</p><p id="ea07" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想要更多的 Android 例子，你可以检查整个库<a class="ae jg" href="https://github.com/huggingface/tflite-android-transformers" rel="noopener ugc nofollow" target="_blank">。我们还发布了<strong class="la jk"> </strong> </a><a class="ae jg" href="https://github.com/huggingface/swift-coreml-transformers" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">一个回购协议，其中包含 iOS </strong> </a>的模型和应用，利用了苹果特有的<a class="ae jg" href="https://developer.apple.com/documentation/coreml" rel="noopener ugc nofollow" target="_blank"> CoreML </a>框架。如果您对更深入的最新 NLP 感兴趣，我们的<a class="ae jg" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">🤗变形金刚</strong> </a>库来了！</p></div></div>    
</body>
</html>