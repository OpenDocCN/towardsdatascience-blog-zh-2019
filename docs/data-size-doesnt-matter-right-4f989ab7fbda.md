# 数据:大小不重要…对吧？

> 原文：<https://towardsdatascience.com/data-size-doesnt-matter-right-4f989ab7fbda?source=collection_archive---------21----------------------->

斯堪的纳维亚北部的萨米部落有 180 到 300 种不同的词语来描述雪、雪的种类、雪的痕迹以及雪的用途。侍酒师用几十个不同的词来描述葡萄酒，包括华丽的、松弛的、烘烤的、木炭的和激光似的。同样，数据科学家有许多概念来讨论数据、数据类型和数据的用途。

![](img/d07082623fba9c9f6ace34bc0b93b747.png)

Photo by [Franki Chamaki](https://unsplash.com/@franki?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

我怀疑大多数人认为数据讨论是深奥的。我每天都和痴迷于数据的人在一起，我每天都听到他们抱怨数据。有些事情，所有优秀的数据科学家都知道，普通人类甚至不会怀疑。只是为了好玩，这里有几个金块。

首先，数据是有用途的。对工作有用的数据是信号。不相关的数据就是噪音。有时当任务改变时，噪音变成了信号。反之亦然。然而，并不总是清楚哪些数据是有用的，哪些是无用的。目的可能不清楚。客户或项目经理可能没有解释我们试图实现的目标。也许他们不了解自己，只是在寻找一些模糊定义的“洞察力”

gif from giphy.com

即使目的很明确，一些数据的有用性仍然是有争议的。假设我们试图预测销售点的美元净销售额。销售点的总销售额数据有用吗？前制造商单位销售呢？从 ERP 中提取数据时，通常会有许多不同概念的数据。这是有充分理由的，会计部门需要能够跟踪所有维度，这对他们来说是有意义的。然而，在数据科学的背景下，只有当总销售额与净销售额之比被认为是问题的一个要素时，同时拥有净销售额和总销售额才是有趣的。在许多其他情况下，我们将只使用一个销售术语，例如净销售额，并放弃其他数据维度。

第二，是数据的数量问题。数据科学家喜欢有许多观察结果，这意味着已经观察到相同变量的许多数据点。拥有大量的观察数据让数据科学家的生活更加舒适。算法的选择再也不用想太多了。如果有足够的观察值，任何现成的算法都可以很好地拟合。如果数量巨大，我们快乐的数据科学家就有机会玩 Spark 集群，使用深度学习算法。她的朋友们会嫉妒死的。最好的是，当数据量很大时，算法需要花费大量的时间来训练。这意味着有更多的时间喝咖啡休息和在线阅读博客。的确，拥有大量的数据观测是一件幸事。

第三，这是数据科学家的噩梦:拥有大量数据，但数据的形式多种多样。拥有大量变量会增加虚假相关性和过度拟合的风险。正如纳西姆·塔勒布所说，给他足够多的股票代码，他就能找到一个回报与你的血压精确相关的股票。这就是所谓的维数灾难。有解决这个问题的方法，但它们都有问题。数据科学家可以花时间进行探索性的数据分析，以减少要使用的变量数量。那是艰苦的工作。她可以找到一个更好的模型来适应这种情况，或者调整当前模型的超参数。那也是更多的工作。或者，她可以使用流形方法来减少数据的维数。然而，有许多方法可用。她应该默认 PCA 吗？如何证明这种选择是正确的？更糟糕的是，现在她已经使用了 PCA，她失去了一些模型的可解释性。这个模型变得更加难以想象和解释。事实上，拥有大量的数据变量是一种诅咒。

第四，变量的类型。如果数字变量是数据科学的超级模特，那么二进制变量就是邻家女孩，分类变量就是丑小鸭。数字变量是数字，如 3.14 或 2.7474。关于数值变量的一切都很可爱。我们可能仍然需要扩展和标准化，但这很容易。二进制变量也可以。然而，分类变量是丑陋的。他们需要一些严肃的整形手术才能有用。我们勇敢的数据科学家可以尝试一键编码。这是一种将一个分类变量转换成许多二进制变量的方法，但它增加了变量的总数，这是一个如上所述的诅咒。嵌入是完美的极端改造。然而，说起来容易做起来难。并不总是清楚哪种嵌入方法将提供最好的性能提升，这是额外的工作。

gif from giphy.com

第五，存在数据缺失的问题。假设一个调查回答者跳过了一个问题。我们应该如何处理丢失的数据点？令人愤怒的是，大多数算法根本不能容忍丢失数据。这意味着遗漏的点需要以某种方式猜测和输入。大多数初学者学会在有疑问时使用其他观察值的平均值。在现实世界中，这往往会导致灾难。正态分布变量在教科书中是标准的，但在商界却是奢侈品。如果变量不是正态分布，那么使用平均值可能是错误的。然后呢？中位数，众数？我们是否需要使用另一个模型来学习联合分布，以便输入更好的值？如此多的头痛。为什么客户不能给我们高质量的数据？

你怎么想呢?还有什么是数据科学圈里的常识，却不被别人怀疑的？