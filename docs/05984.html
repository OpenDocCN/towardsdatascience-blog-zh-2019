<html>
<head>
<title>It Is Necessary to Combine Batch Normalization and Skip Connections</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有必要将批处理规范化和跳过连接结合起来</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/its-necessary-to-combine-batch-norm-and-skip-connections-e92210ca04da?source=collection_archive---------11-----------------------#2019-08-31">https://towardsdatascience.com/its-necessary-to-combine-batch-norm-and-skip-connections-e92210ca04da?source=collection_archive---------11-----------------------#2019-08-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="85c6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这些技术必须齐头并进</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/def20882cfe66c2731fa16211944f0a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uCBnIvJrePuVBie4p4Qe5Q.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by geralt from pixabay</figcaption></figure><p id="947d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章基于我在 ICML 2019 年发表的<a class="ae lu" href="https://arxiv.org/abs/1811.03087" rel="noopener ugc nofollow" target="_blank">论文</a>。它试图提供新颖的见解来回答以下问题:</p><ul class=""><li id="5e99" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">为什么深度神经网络(DNNs)的特定架构有效，而其他架构无效？</li><li id="43b6" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">批量范数[2]和跳过连接[3]起到了什么作用？</li></ul><p id="4ec4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回答这些问题是一个热门的研究课题，事关重大。事实上，这样的答案将推进对 DNNs 的理论理解，并有可能进一步改进它们的设计。</p><h1 id="8a1f" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">如何评估一个架构的质量？</h1><p id="d716" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">首先，第一个关键的观察是，任何从输入到输出的 DNN 映射都需要指定两个元素:(1)架构；(2)架构内部的模型参数值——权重和偏差。通过固定架构和改变架构内的模型参数获得的所有 DNN 映射的集合被称为<strong class="la iu">假设类</strong>。</p><p id="7379" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设类的目的是对训练施加约束。实际上，训练在于找到同时满足以下条件的 DNN 映射:(1)属于假设类；(2)与训练数据一致。DNN 映射必须属于假设类的约束是表达真实映射本身属于假设类的先验知识的一种方式。本质上，这种先验知识使得仅使用训练数据的测试数据的<strong class="la iu">归纳</strong>成为可能。出于这个原因，先前的知识通常被称为<strong class="la iu">感应偏置</strong>(要了解更多细节，我建议读者参考 Shalev-Shwartz 和 Ben-David 的这本伟大的<a class="ae lu" href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/" rel="noopener ugc nofollow" target="_blank">书</a>)。</p><p id="6666" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回到我们最初的目标，我们可以通过评估归纳偏差的质量来评估架构的质量。这种评估可以使用以下过程来执行:修复体系结构，并在体系结构内随机抽取模型参数。如果用该过程采样的大多数 DNN 映射具有不好的属性，这意味着对不好的属性存在归纳偏差，即坏的属性在训练期间将被偏爱。反过来，这将导致<strong class="la iu">无法适应</strong>——即欠拟合——当坏属性与低训练损失不相容时，或者导致<strong class="la iu">泛化能力差</strong>——即过拟合——当坏属性与低训练损失相容但不太可能泛化时。</p><p id="f8bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在能够应用这个过程之前，我们仍然需要使 DNN 映射的“坏性质”的概念更加精确。</p><h1 id="816f" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">DNN 映射有什么不好的性质？</h1><p id="d309" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">让我们考虑一个固定的 DNN 映射，由<strong class="la iu">固定架构</strong>和<strong class="la iu">固定架构内的模型参数</strong>指定。<em class="ng"> </em>这个固定的 DNN 映射接收一个<strong class="la iu">随机输入</strong>并通过它的层传播这个输入。我们通过定义以下内容来跟踪传播:</p><ul class=""><li id="f2b4" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated"><strong class="la iu">随机信号</strong><em class="ng">:</em><strong class="la iu">y</strong>^<em class="ng">l =</em>φ^<em class="ng">l</em>(<strong class="la iu">x</strong>)，通过将固定 DNN 映射φ^<em class="ng">l</em>到层<em class="ng"> l </em>应用到随机输入<strong class="la iu"> x </strong>得到</li><li id="af48" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">随机噪声<strong class="la iu">随机噪声</strong>:d<strong class="la iu">y</strong>^<em class="ng">l =</em>φ^<em class="ng">l</em>(<strong class="la iu">x</strong>+d<strong class="la iu">x</strong>)-φ^<em class="ng">l</em>(<strong class="la iu">x</strong>)<strong class="la iu">、</strong>作为随机腐败 d <strong class="la iu"> x </strong>产生的层腐败</li></ul><p id="f926" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，DNN 映射的“坏特性”可以定义为信号<em class="ng"> </em>和噪声的不必要行为:或者信号<strong class="la iu"> y </strong> ^ <em class="ng"> l </em>变得无意义，或者噪声 d <strong class="la iu"> y </strong> ^ <em class="ng"> l </em>失去控制。更准确地说，两种“病理”可以被定义为将这种“不良属性”推向极端(我们关注这两种“病理”，因为它们是在我们的上下文中观察到的，但是其他“病理”可以在其他上下文中被定义和观察到):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/3432f202fb6828a8ed00684e2d957712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z71mlOLJhBcKVc_-UIDYxw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Pathological Signal</figcaption></figure><ul class=""><li id="c51a" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated"><strong class="la iu">病理信号:</strong>信号<strong class="la iu"> y </strong> <em class="ng"> ^l </em>失去其变化方向，并在深层沿一条线集中。这种病理例如与多类分类的一次性目标(具有通常等于类的数量减 1 的多个变化方向)不兼容。对这种病理的诱导性偏见可能导致无法治愈。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/cb74506aa412eda99d2616d600f0988e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NBh1JRqaxal2VulI-ShpnA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Pathological SNR</figcaption></figure><ul class=""><li id="de5d" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated"><strong class="la iu">病理 SNR: </strong>噪声 d<strong class="la iu">y</strong>T4】^l 相对于信号<em class="ng"> </em> <strong class="la iu"> y </strong> <em class="ng"> ^l </em>，信噪比 SNR <em class="ng"> ^l </em>随<em class="ng"> l 呈指数衰减</em>这种病理可能兼容低训练损失， 但是测试集上的任何输入讹误 d <strong class="la iu"> x </strong>都会导致讹误信号<strong class="la iu">y</strong>t20】^l+d<strong class="la iu">y</strong>t24】^l =φ^<em class="ng">l</em>(<strong class="la iu">x</strong>+d<strong class="la iu">x</strong>)变成纯噪声——即无意义。 对这种病理学的归纳性偏见可能导致不良的概括。</li></ul><h1 id="87bc" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">将我们的过程应用于各种架构</h1><p id="31b4" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">现在让我们将我们的过程应用于<strong class="la iu">卷积</strong><strong class="la iu">DNNs</strong>—<strong class="la iu">—</strong>的各种架构，包括作为空间大小等于 1 的特例的<strong class="la iu">全连接 DNNs</strong>—具有<strong class="la iu"> ReLU </strong>激活函数:</p><ul class=""><li id="5b92" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">没有批次定额的 DNNs <strong class="la iu">和没有</strong> <em class="ng"> </em>跳过连接的<strong class="la iu">病理信号</strong>——即<strong class="la iu"> y </strong> <em class="ng"> ^l </em>的#个方差方向在深层接近 1</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/a455e33ff2773e19a9721a0894fa0d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aP27OheI25rZEeoLY_zsAA.png"/></div></div></figure><ul class=""><li id="6b08" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">没有批次标准<em class="ng"> </em>的 DNNs <strong class="la iu">和有</strong>跳过连接的<strong class="la iu">同样有<strong class="la iu">病理信号</strong>——即<strong class="la iu"> y </strong> <em class="ng"> ^l </em>的#个方差方向在深层接近 1</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/9f7085878467830348c7e190fd99dd1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7nKB-qlW_hFWq53J00l8Q.png"/></div></div></figure><ul class=""><li id="3ec9" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">具有批处理规范的 DNNs <strong class="la iu">和没有</strong>跳过连接的<strong class="la iu">遭受<strong class="la iu">病理 SNR </strong> — <strong class="la iu"> </strong>即 SNR<em class="ng">^l</em>/SNR<em class="ng">^</em>0<em class="ng"/>随<em class="ng"> l </em>呈指数衰减</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/d70b48b31e10b4c06a14594befa41a34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_u2vGAR-632kqw1qIFjg1w.png"/></div></div></figure><ul class=""><li id="b43e" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">带有批次标准的 DNNs <strong class="la iu">和带有</strong>跳过连接的<strong class="la iu">不会出现任何问题——即在所有深度都保持<strong class="la iu">良好运行</strong></strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/f4dc965eb8ea421604337eecc311dc6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJO_YEmhHbJj1K6iRrFBpw.png"/></div></div></figure><h1 id="c9be" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">这是怎么回事？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/6e8a8991f66d573f226e4928c8cbb69a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HbXDEfEWwiE254lEPBg8Eg.png"/></div></div></figure><p id="1c5d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">吸引病理学的主要力量是前馈层组成<strong class="la iu"> </strong>的<strong class="la iu">乘法性</strong>(conv 层和 ReLU 层可以分别看作是与随机矩阵相乘和与 Bernouilli 随机向量相乘):</p><ul class=""><li id="1946" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">没有跳跃连接的 DNNs <strong class="la iu">在深层是病态的，因为它们受到简单前馈乘法的影响</strong></li><li id="6670" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">没有批范数的 DNNs <strong class="la iu">和具有</strong>跳过连接的<strong class="la iu">在深层是病态的，因为残差和跳过连接分支<strong class="la iu">中大致相等的方差不</strong>有效地对抗前馈乘法</strong></li><li id="00d4" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">具有批处理规范的 DNNs <strong class="la iu">和具有</strong>跳过连接的<strong class="la iu">在所有深度都保持良好性能，因为剩余连接分支和跳过连接分支<strong class="la iu">之间的信号方差的衰减比率∝1/(<em class="ng">l</em>+1)</strong>有效地对抗前馈乘法</strong></li></ul><h1 id="5b64" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结论</h1><p id="c332" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">让我们总结一下我们的结果(为了更深入地挖掘，我建议感兴趣的读者参考<a class="ae lu" href="https://arxiv.org/abs/1811.03087" rel="noopener ugc nofollow" target="_blank">论文</a>和<a class="ae lu" href="https://github.com/alabatie/moments-dnns" rel="noopener ugc nofollow" target="_blank">代码</a>):</p><ul class=""><li id="3f34" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">批范数和跳过连接的组合在深网中编码了一个<strong class="la iu">性能良好的</strong>感应偏置<strong class="la iu">T21</strong></li><li id="39c5" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">然而，这些技术的好处很难理清。只有当它们结合时——通过将剩余分支稀释到跳过连接分支——它们才能抵消前馈乘法</li></ul><p id="14a1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这些结果将打开理解深网的新视角。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="4337" class="mj mk it bd ml mm ns mo mp mq nt ms mt jz nu ka mv kc nv kd mx kf nw kg mz na bi translated">放弃</h1><p id="6bf5" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">为了便于阐述(并且不改变分析)，与本文相比，本文中的一些符号被简化了:</p><ul class=""><li id="c044" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">采用了预激活视角，每层<em class="ng"> l </em>在卷积后开始，在卷积后再次结束</li><li id="428a" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">#方差方向<strong class="la iu">y</strong>t28】^l 在文中对应于<strong class="la iu">y</strong>t32】^l 的有效秩</li><li id="2306" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">SNR <em class="ng"> ^l / </em> SNR^0 在本文中对应于归一化灵敏度平方的倒数</li></ul><h1 id="74e9" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><p id="9167" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">[1] A. Labatie，<a class="ae lu" href="https://arxiv.org/abs/1811.03087" rel="noopener ugc nofollow" target="_blank">表征行为良好与病理性深度神经网络</a> (2019)，ICML 2019</p><p id="af0e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2] S. Ioffe 和 C. Szegedy，<a class="ae lu" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批量归一化:通过减少内部协变量移位加速深度网络训练</a> (2015)，ICML 2015</p><p id="3101" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3]何国光，张，任，孙，<a class="ae lu" href="https://arxiv.org/abs/1603.05027" rel="noopener ugc nofollow" target="_blank">深度剩余网络中的身份映射</a> (2016)，2016</p></div></div>    
</body>
</html>