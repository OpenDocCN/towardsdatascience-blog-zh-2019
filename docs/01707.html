<html>
<head>
<title>Adaptive - and Cyclical Learning Rates using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PyTorch 的自适应和循环学习率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee?source=collection_archive---------7-----------------------#2019-03-20">https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee?source=collection_archive---------7-----------------------#2019-03-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/fd378827193a1346c176dffff448f18e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4TuK55s1jaXONQWS"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@forevercarrieon?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sirma Krusteva</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ec55" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">学习率(LR)是调整神经网络的关键参数之一。具有自适应学习率的 SGD 优化器已经流行了一段时间:Adam、Adamax 和它的老兄弟们通常是事实上的标准。它们消除了手动搜索和安排你的学习速度(如衰减率)的痛苦。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/98a0975b88fcf01d6a7840ec4a8be046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*An4tZEyQAYgPAZl396JzWg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Source: Jeremy Jordan’s blogpost</figcaption></figure><p id="d143" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章将给出一个简短的例子，概述和比较最流行的自适应学习率优化。我们将使用 PyTorch，时髦的神经网络库的选择！</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/cda1e97f5b9df9a14b6b2ea2bc2cb859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*kcm1Po5R4Anyh8VXMduXnQ.png"/></div></figure><p id="268c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除此之外，fast.ai 还宣扬了循环学习率(CLR)的概念，提到了莱斯利·史密斯的伟大论文(<a class="ae kf" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank">链接</a>)。我们将看看 SGD 如何与其他优化程序相抗衡。</p><p id="896d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了演示，我们将承担两项分类任务:</p><ul class=""><li id="dae3" class="lk ll it ki b kj kk kn ko kr lm kv ln kz lo ld lp lq lr ls bi translated">基于普通 CNN 架构的图像分类</li><li id="0fb7" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated">使用预训练(在 ImageNet 上)的 ResNet34 网络进行图像分类</li></ul><p id="69f7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该职位的所有代码和培训日志可以在<a class="ae kf" href="https://github.com/TDehaene/blogposts/tree/master/learning_rates" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="2594" class="mf mg it bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">数据</h1><p id="f076" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">在这篇文章中，我们将使用 Kaggle 的“鲜花识别”数据集(<a class="ae kf" href="https://www.kaggle.com/alxmamaev/flowers-recognition" rel="noopener ugc nofollow" target="_blank">链接</a>)。这是一个很好的测试图像分类网络的基本真实数据集。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/0f63e34bf61a1f1a64e80baedb20a0ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*_gJfI05dsrpzHhOD_eVJMA.jpeg"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">example image of the class ‘dandelion’</figcaption></figure><p id="b83f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 20%进行验证的数据被平均分配到 5 个类别中进行预测:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="nj nk l"/></div></figure></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="f0a6" class="mf mg it bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">自适应优化器</h1><p id="d6b6" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">在不深入研究每一个优化器的数学原理的情况下，这里有一个简短(有点过于简单)的概述，介绍一下我们将要与之对抗的优化器:</p><ul class=""><li id="03a6" class="lk ll it ki b kj kk kn ko kr lm kv ln kz lo ld lp lq lr ls bi translated"><strong class="ki iu"> Adagrad </strong>:这将根据梯度的过去历史，缩放每个参数的学习率。本质上:大梯度= &gt;小α，反之亦然。然而不利的一面是学习率会很快下降。</li><li id="b529" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated"><strong class="ki iu"> Adadelta </strong>:在 Adagrad 上继续，但是有了新的花样:只存储过去的 w 渐变而不是整个历史，(现在有限的)历史存储为衰减的平均值。</li><li id="9ae1" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated">RMSProp :有点类似(请不要向我开枪，辛顿先生，先生)，但是 RMSProp 将 LR 除以梯度平方的指数衰减平均值。</li><li id="1be4" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated"><strong class="ki iu"> Adam </strong>:除了存储历史梯度平方和，它还计算过去梯度的指数衰减平均值(类似于动量)。</li><li id="3e2c" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated"><strong class="ki iu"> Adamax </strong>:这里，另一个技巧应用于平方梯度 v(t)的移动平均值，作者应用无穷范数ℓ∞来获得新的范数约束向量 v(t)，将其插入 Adam，从而获得令人惊讶的稳定算法。</li></ul><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/198ed6b5d85b582d286e388413f3357a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*yJmnLoI6jT5W846df8miaA.png"/></div></figure><p id="1bdb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉提示:如果你正在寻找优化者的更深入的数学比较，看看 Sebastian Ruder 的<a class="ae kf" href="http://ruder.io/optimizing-gradient-descent/index.html#momentum" rel="noopener ugc nofollow" target="_blank">这篇精彩的博客文章</a>，它对我写这篇文章有很大帮助。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="539e" class="mf mg it bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">循环学习率</h1><p id="f01c" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">CLR 的论文提出了两个非常有趣的观点:</p><ol class=""><li id="6d31" class="lk ll it ki b kj kk kn ko kr lm kv ln kz lo ld nm lq lr ls bi translated">它为我们提供了一种在训练过程中有效安排学习速率的方法，即以三角形的方式在上限和下限之间变化。</li><li id="9ea9" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld nm lq lr ls bi translated">这给了我们一个非常合理的估计，哪个学习率范围最适合你的特定网络。</li></ol><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/8f17c863b43f7327283840f2f7055c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*fI0l_gIezdHLFlZpJ2s8iA.png"/></div></figure><p id="9ba7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有许多参数可供选择:</p><ul class=""><li id="2256" class="lk ll it ki b kj kk kn ko kr lm kv ln kz lo ld lp lq lr ls bi translated"><strong class="ki iu">步长</strong>:LR 从下限上升到上限需要多少个历元。</li><li id="99cc" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated"><strong class="ki iu"> max_lr </strong>:调度中最高的 lr。</li><li id="d036" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated"><strong class="ki iu"> base_lr </strong>:进度表中最低的 lr，实际上:本文作者建议将此取比<strong class="ki iu"> max_lr </strong>小一个因子 R。我们的利用系数是 6。</li></ul><p id="3791" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，很难分析出这种方法行得通的确切原因。LR 的发展可能会导致网络在短期内出现更高的损耗，但这种短期的缺点在长期内证明是有利的。如果当前的网络不太稳定，它可以让网络跳到另一个局部最小值。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/1c6e344306475c62bce59012adcbb349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TX18Q_hD457zuKFB2d2P2w.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Source: Snapsshot Ensembles (<a class="ae kf" href="https://arxiv.org/abs/1704.00109" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1704.00109</a>)</figcaption></figure><p id="168d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CLR 优于上述自适应方法的另一个优点是计算量较小。</p><p id="1a8d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在论文中，还提到你可以随着时间线性或指数递减上界，但这在这篇博文中没有实现。</p><p id="5d3f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么这在代码中是如何工作的呢？…</p><h2 id="fd5b" class="np mg it bd mh nq nr dn ml ns nt dp mp kr nu nv mt kv nw nx mx kz ny nz nb oa bi translated">第一步:找到上面的 LR</h2><p id="ea19" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">以一个普通的 CNN 为例:第一步是为你的模型计算学习率的上限。做到这一点的方法是:</p><ul class=""><li id="3502" class="lk ll it ki b kj kk kn ko kr lm kv ln kz lo ld lp lq lr ls bi translated">定义一个初始学习率，即您想要测试的范围的下限(假设为 1e-7)</li><li id="f45c" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated">定义范围的上限(假设为 0.1)</li><li id="7849" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated">定义一个指数方案来逐步完成这个过程:</li></ul><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8355eb4f46ef7e12ea13c9fc11b610b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*7-AUPOcaCvO2yrtkiAGnaw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Used formula for the LR finder scheduling (N = number of images, BS = Batch Size, lr = learning rate)</figcaption></figure><p id="ac35" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">幸运的是，PyTorch 有一个 LambdaLR 对象，它允许我们在 lambda 函数中定义上述内容:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="oc nk l"/></div></figure><ul class=""><li id="9752" class="lk ll it ki b kj kk kn ko kr lm kv ln kz lo ld lp lq lr ls bi translated">接下来，在你的网络中运行(我用了两个纪元)。在每个步骤(每个批量):获取 LR、获取损失并优化梯度:</li></ul><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="oc nk l"/></div></figure><p id="604f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉注意:我们不是在每一步取“原始”损失，而是平滑损失，即:损失= α。损耗+ (1- α)。以前的损失</p><p id="52cc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此之后，我们可以清楚地看到 LR 遵循一个很好的指数规律:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="2ca3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基本网络的损耗-lr 曲线(见下文)如下所示:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="bd27" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以清楚地看到，过高的 LR 会导致网络损耗发散，过低的 LR 根本不会导致网络学习太多…</p><p id="312a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在他的 fast.ai 课程中，杰瑞米·霍华德提到一个好的上限并不在最低点，而是向左 10 倍。</p><p id="0269" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑到这一点，我们可以说学习率的一个好的上限是:3e-3。</p><p id="e437" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据这篇论文和其他资料，一个好的下限是上限除以因子 6。</p><h2 id="08d1" class="np mg it bd mh nq nr dn ml ns nt dp mp kr nu nv mt kv nw nx mx kz ny nz nb oa bi translated">步骤 2: CLR 调度程序</h2><p id="8940" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">第二步是创建一个循环学习时间表，它在下限和上限之间改变学习速率。</p><p id="002c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可以通过多种方式实现:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/8059cb660309d64be15e85980e2129ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*2URpGDr5a9aJ1YuA7OaTKg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Various possibilites for the CLR shape (source: <a class="ae kf" href="https://www.jeremyjordan.me/nn-learning-rate/" rel="noopener ugc nofollow" target="_blank">jeremy jordan’s blog</a>)</figcaption></figure><p id="1491" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们要用简单的三角 CLR 时间表。</p><p id="f1e2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以编程方式，我们只需要创建一个自定义函数:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="oc nk l"/></div></figure><h2 id="156d" class="np mg it bd mh nq nr dn ml ns nt dp mp kr nu nv mt kv nw nx mx kz ny nz nb oa bi translated">第三步:包装</h2><p id="0bea" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">在步骤 3 中，这可以被包装在 PyTorch 中的 LambdaLR 对象中:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="oc nk l"/></div></figure><h2 id="354a" class="np mg it bd mh nq nr dn ml ns nt dp mp kr nu nv mt kv nw nx mx kz ny nz nb oa bi translated">训练</h2><p id="6b22" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">在一个时期内，我们需要使用'更新 LR。scheduler 对象的“step()”方法:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="oc nk l"/></div></figure></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="8730" class="mf mg it bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">对比 1:香草 CNN</h1><p id="7804" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">首先是使用普通的(非预训练的)CNN 进行分类。我使用了以下网络架构:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="oc nk l"/></div></figure><p id="6366" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了防止模型过度适应(相对较小的)数据集，我们使用以下技术:</p><ul class=""><li id="b1b8" class="lk ll it ki b kj kk kn ko kr lm kv ln kz lo ld lp lq lr ls bi translated">线性图层中的丢失</li><li id="60e6" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated">CNN 块中的 Batchnorm 层</li><li id="6107" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated">数据扩充:</li></ul><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="oc nk l"/></div></figure><p id="9f87" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉提示:你需要提前计算通道标准化的均值和标准差，查看完整的笔记本，看看如何解决这个问题。</p><p id="b1e2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们为 6 个优化器中的每一个训练了 150 个时期的网络。为了消除一些可变性，我们为每个优化器运行了 3 次。</p><p id="0f0b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练和验证准确性看起来像这样:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="nj nk l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Training accuracy</figcaption></figure><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="nj nk l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Validation accuracy</figcaption></figure><p id="c541" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，孩子们，我们在这里能看到什么:</p><p id="8d23" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉阿达格拉德:平庸的表现，正如所料</p><p id="42e1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉阿达德尔塔:在 acc 训练中不是真正的冠军，但在验证中表现非常出色</p><p id="4d38" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉RMSProp:除非我在这里做错了什么，否则我对糟糕的表现感到有点惊讶</p><p id="a223" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉亚当:一直很好</p><p id="8d4a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉Adamax:有希望的训练精度发展，但没有完美地反映在验证精度上</p><p id="98af" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉带 CLR 的 SGD:训练精度收敛快得多，验证精度收敛快，不算太差…</p><p id="9e56" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最终，SGD+CLR、Adam 和 Adadelta 似乎都以大约 83%的最终验证准确率结束。</p><h1 id="1877" class="mf mg it bd mh mi oe mk ml mm of mo mp mq og ms mt mu oh mw mx my oi na nb nc bi translated">对比二:Resnet34 迁移学习</h1><p id="5362" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">如果你说:“小数据集上的图像分类”，你需要考虑迁移学习。</p><p id="d175" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以我们就这么做了，使用 Resnet34，在 ImageNet 上进行预训练。我相信数据集相当接近 Imagenet 图片，所以我只解冻了 5 个卷积块的最后一个块，并用新的线性层替换了最后一个线性层:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="oc nk l"/></div></figure><p id="00d9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于 6 个优化器中的每一个，网络被训练 100 个时期(由于更快的收敛):</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="nj nk l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Training accuracy</figcaption></figure><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="nj nk l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Validation accuracy</figcaption></figure><p id="c017" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此处有重要提示:</p><p id="9228" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉总的来说:优化器之间的差异要小得多，尤其是在观察验证准确性时</p><p id="2b7f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉RMSProp:仍然有点表现不佳</p><p id="2faf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">👉SGD+CLR 在训练准确性方面再次表现良好，但这并没有立即反映在验证准确性上。</p><p id="7f73" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于迁移学习来说，调整学习速度和仔细选择优化器的绝对回报似乎不太大。</p><p id="83b4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可能是由于两个主要影响:</p><ul class=""><li id="2d88" class="lk ll it ki b kj kk kn ko kr lm kv ln kz lo ld lp lq lr ls bi translated">网络权重已经大大优化</li><li id="5052" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated">优化器通常只能优化整个网络权重的一小部分，因为大部分权重保持冻结</li></ul></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="f14f" class="mf mg it bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">结论</h1><p id="f75d" class="pw-post-body-paragraph kg kh it ki b kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">这篇博文的主要观点是:</p><blockquote class="oj ok ol"><p id="5f93" class="kg kh om ki b kj kk kl km kn ko kp kq on ks kt ku oo kw kx ky op la lb lc ld im bi translated">不要只是采用任何旧的现成的优化程序。学习率是最重要的超参数之一，因此仔细研究它是值得的。如果你想比较，看看 SGD 的 CLR 时间表。</p></blockquote><p id="e3b4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再次声明:所有代码都可以在这里找到<a class="ae kf" href="https://github.com/TDehaene/blogposts/tree/master/learning_rates" rel="noopener ugc nofollow" target="_blank">，可以随意查看！</a></p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="oq nk l"/></div></figure></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="b7d5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">资料来源和进一步阅读</p><ul class=""><li id="77d4" class="lk ll it ki b kj kk kn ko kr lm kv ln kz lo ld lp lq lr ls bi translated">https://arxiv.org/abs/1506.01186<a class="ae kf" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank"/></li><li id="12f8" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated"><a class="ae kf" href="https://www.datacamp.com/community/tutorials/cyclical-learning-neural-nets" rel="noopener ugc nofollow" target="_blank">https://www . data camp . com/community/tutorials/cyclic-learning-neural-nets</a></li><li id="12e9" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated"><a class="ae kf" href="https://medium.com/@lipeng2/cyclical-learning-rates-for-training-neural-networks-4de755927d46" rel="noopener">https://medium . com/@ Li Peng 2/cyclic-learning-rates-for-training-neural-networks-4de 755927 d46</a></li><li id="bd57" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated"><a class="ae kf" href="http://ruder.io/optimizing-gradient-descent/index.html#momentum" rel="noopener ugc nofollow" target="_blank">http://ruder . io/optimizing-gradient-descent/index . html # momentum</a></li><li id="dfbc" class="lk ll it ki b kj lt kn lu kr lv kv lw kz lx ld lp lq lr ls bi translated"><a class="ae kf" href="http://teleported.in/posts/cyclic-learning-rate/" rel="noopener ugc nofollow" target="_blank">http://teleported.in/posts/cyclic-learning-rate/</a></li></ul></div></div>    
</body>
</html>