# 走向跨包容的人工智能

> 原文：<https://towardsdatascience.com/towards-trans-inclusive-ai-a4abe9ad4e62?source=collection_archive---------19----------------------->

## 性别二进制算法的问题是

人工智能像设计它们的人一样“思考”——对性别有一种二元的、非规范性的概念。他们排斥变性人，强化性别陈规定型观念。更糟糕的是，世界各国政府花费数十亿美元将顺式性别歧视人工智能扩展到政府机构等新行业和图像识别等新应用，而很少考虑它们的性别影响。计算机科学界、技术界和政府机构应该对他们算法的性别影响承担更多责任。他们需要学会使用酷儿和跨性别理论来分析算法的性别影响，然后将这种学习应用于社会中人工智能算法的设计、部署和监控。

# **背景**

人工智能算法通常是两种类型之一:机器学习算法或专家系统。**机器学习**是人工智能的一个子领域，算法从训练数据中学习以做出决策。该领域的三位先驱，Yann LeCun、Geoffrey Hinton 和 Yoshua Bengio 最近因他们的工作获得了图灵奖。这证明了机器学习算法在图像识别、无人驾驶汽车和语言翻译等众多应用中的强大功能。**专家系统**是更简单的算法，由领域专家创建的 *if-then* 规则组成。例如，由医生创建的算法可以执行以下规则"*如果*患者显示 X 和 Y 症状，*则*进行 Z 干预"。

我对以性别作为输入或输出的机器学习算法感兴趣。这里有两个例子:

**自动性别识别(AGR)** —输入是某人的图片、视频或社交媒体帖子。输出是他们预测的性别，问题是只限于男性或女性。

**脸书的广告推荐算法** —输入是用户的个人资料，包括他们的性别身份。输出是一个脸书新闻提要广告。

# **性别歧视 AI——性别输出**

自动性别识别是顺性别歧视和跨性别歧视。顺性别歧视是一种假设，即某人出生时分配的性别与其性别认同相匹配，并且性别认同不会随着时间的推移而改变。ARG 的顺性别歧视排斥跨性别者和性别酷儿，强化了“真正的”男人和女人的性别刻板印象。 ***自动性别识别不应该做*** 。跨性别者和性别酷儿的认同向我们表明，性别既不是由性别决定的，也不是二元的，更不是不可改变的。假设其他暴力抹去这些群体的经验。

对看起来不像“真正的”男人或女人的人的压迫——无论他们是否认为自己是跨性别者——是他们与人工智能定期互动的一部分。例如，在同工同酬日，柏林运输公司(BVG) [向“真正的”女性提供机票折扣](https://www.theguardian.com/cities/2019/mar/14/mind-the-gender-pay-gap-berlin-women-to-get-public-transport-discount)。BVG 指出，“在柏林乘坐地铁、公共汽车或电车的女性将比男性少支付 21%的费用……这是为了突出德国日益扩大的性别薪酬差距。”BVG 试图让人们意识到德国严重的薪酬差距，却忽视了跨性别和性别不合群的人。BVG 的工作人员被授权对乘客的性别身份做出决定，这是他们本不该拥有的权力。如果一名跨性别女性因为 BVG 的工作人员不相信她的性别身份而被拒绝打折机票，会怎么样？现在想象一下，不是 BVG 的员工来做这些关于性别身份的决定，而是一台相机和一个人工智能算法。整个柏林的火车站都安装了摄像头来进行面部识别。一个人工智能算法现在有权决定任何在柏林乘坐公共交通工具的人的性别身份，这是一个非常可怕的想法。一个可能的跨性别替代方案是向自我认定为非独联体男性的人发放折扣票，这将使人们意识到独联体男性与其他性别之间的薪酬差距。当然，一些独联体国家的人会滥用这一制度，但跨性别的薪酬差距信息将会清晰响亮。

一般来说，人工智能算法不应该对性别身份做出决定。使用性别作为人工智能算法的输出是不科学的，因为性别身份是在个人的权威之下。这不是看身体或外在表情就能决定的。如果一个组织需要知道某人的性别身份，他们应该问，而不是推断。

# 性别歧视人工智能——性别输入

脸书广告算法有提供基于自我认同的性别的歧视性广告的历史。其中一个例子是脸书算法[拒绝女性观看在线招聘广告](https://www.thesun.co.uk/tech/7293789/facebook-sexism-job-ads-women-discrimination-targeting/)。然而，脸书算法并不是唯一一个使用性别输入做出性别歧视决定的算法。有一种[广告牌广告算法](https://theoutline.com/post/1528/this-pizza-billboard-used-facial-recognition-tech-to-show-women-ads-for-salad)，它使用隐藏的摄像头来猜测路过的人的性别，然后向他们提供广告。被认为是男性的人会收到比萨饼的广告，而被认为是女性的人会收到沙拉的广告。将性别化的输入纳入人工智能算法，更有可能做出顺性别歧视和性别歧视的决定。

由于上面的例子，还不清楚是否有一种负责任地使用性别输入的方法。也许我们可以找到一种方法，负责任地使用性别身份来提高一些算法的能力，而不会对女性、跨性别者和性别酷儿造成伤害，但也许不会。当然，唯一有效的性别输入是打破 M 或 F 性别二元结构的自我识别的性别。我们需要回答的问题是“性别输入是提高了算法性能还是仅仅延续了性别刻板印象？”

# **用人工智能缩放性别歧视**

我们正在通过扩大人工智能在社会中的作用来克服顺性别歧视，同时忽略它的偏见。由于在公共和私营行业的广泛使用，AI 算法在社会上越来越普遍。美国、中国和其他国家的政府每年花费数十亿美元资助人工智能的研究和开发。杰出的人工智能研究者吴恩达称之为下一个电力时代，科技行业正等待人工智能将世界变得更好。然而，人们很少意识到人工智能系统如何对跨性别者、同性恋者和异性恋者产生异规范效应。

现在很多行业都在使用人工智能。临床决策支持系统被医生用于医疗保健中的一系列任务，自动决策系统被市政府用于预测刑事再犯和进行预测性警务。这一趋势只会继续，因为机器学习研究人员正在扩展算法的能力，像亚马逊网络服务(AWS)和谷歌云平台(GCP)这样的云计算平台正在让非专家更容易使用人工智能系统。部署人工智能算法所必需的人力资本正在增长。

唐纳德·特朗普(Donald Trump)最近签署了一项行政命令，建立了“美国人工智能倡议——这将要求联邦机构向人工智能(AI)研究、培训和推广投入更多资源”。对行政命令最大的批评？它没有足够的资金来跟上中国、韩国和加拿大的人工智能项目，这些项目涉及数亿美元的多年战略。

我们必须承认人工智能不断扩大的作用。人工智能算法不再是政治中立的。他们做出的决定会对社会产生巨大而复杂的影响，可能会产生性别偏见，特别是对跨性别和性别不一致的人。研究、设计和使用人工智能算法的人必须理解这一点，这样他们才能负责任地推动人工智能在社会中的应用。

# **怎么修**

**人工智能系统的透明使用。人工智能算法的设计者声称的“商业秘密”和普遍缺乏公开披露阻止了公众、记者和学者探索人工智能算法的性别影响。最迫切需要这种透明度的是公共机构使用的自动决策系统。NYU AI Now 部门的一个团队设计了一个[算法影响评估](https://medium.com/@AINowInstitute/algorithmic-impact-assessments-toward-accountable-automation-in-public-agencies-bd9856e6fdde)，为公共机构负责任地使用人工智能算法提供了一个框架。**

重视跨知识。我们需要解决创建、使用和部署人工智能算法的社区中预先存在的性和性别观念。例如，设计 AGR 算法的人机交互研究人员[分享了他们算法的性别歧视观点](https://ironholds.org/resources/papers/agr_paper.pdf)。从广义上讲，有影响力的人工智能社区包括机器学习研究人员、数据科学家和技术高管，所有这些人都主要由顺式性别男性组成，他们通常对性别有二元概念。为了使人工智能系统的专家审计有效，审查算法的人必须了解机器学习/人工智能的技术方法和跨性别观点。两个领域都有专长的人很少。这里有一个关于跨性别观点的简短阅读列表，可能有助于分析人工智能算法的性别影响:

1.让我们从出生证明上删除性别身份

2.[反肯定的语言不仅仅是一种礼貌](https://medium.com/@michelle.sabina.avenant/trans-affirming-language-is-more-than-a-courtesy-4c4a508d6947)

3.[一名变性人工智能研究人员对面部识别软件的噩梦场景](https://venturebeat.com/2019/04/24/a-transgender-ai-researchers-nightmare-scenarios-for-facial-recognition-software/)

4.[边飞行边运输](https://www.nytimes.com/2019/04/17/opinion/tsa-transgender.html)

# **致谢**

我要感谢 Davy Knittle 在撰写这篇博文中发挥的核心作用。他在跨性别研究方面的专业知识以及对人工智能和性别话题的兴奋使这篇博文成为现实。