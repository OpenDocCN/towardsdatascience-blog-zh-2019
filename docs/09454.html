<html>
<head>
<title>Text Preprocessing for Data Scientists</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">æ•°æ®ç§‘å­¦å®¶çš„æ–‡æœ¬é¢„å¤„ç†</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d?source=collection_archive---------12-----------------------#2019-12-13">https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d?source=collection_archive---------12-----------------------#2019-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9052" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">æ–‡æœ¬é¢„å¤„ç†çš„ç®€ä¾¿æŒ‡å—</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0da906e23fb47d9e68372812c704fd6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*np2bP68pEJVkFoc9HwMJMA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by <a class="ae ky" href="https://pixabay.com/users/Devanath-1785462/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1248088" rel="noopener ugc nofollow" target="_blank">Devanath</a> from <a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1248088" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><h2 id="b6f0" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">æ–‡æœ¬é¢„å¤„ç†</h2><p id="a1a6" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">æ–‡æœ¬é¢„å¤„ç†æ˜¯æ–‡æœ¬åˆ†æå’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„é‡è¦ä»»åŠ¡å’Œå…³é”®æ­¥éª¤ã€‚å®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºå¯é¢„æµ‹å’Œå¯åˆ†æçš„å½¢å¼ï¼Œä»¥ä¾¿æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥æ›´å¥½åœ°æ‰§è¡Œã€‚è¿™æ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„æ–‡æœ¬é¢„å¤„ç†æŒ‡å—ï¼Œæ˜¯æˆ‘ä¹‹å‰å…³äºæ–‡æœ¬æŒ–æ˜çš„åšå®¢çš„å»¶ç»­ã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†æ¥è‡ª Kaggle çš„ twitter æ•°æ®é›†ã€‚</p><p id="889d" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">æœ‰ä¸åŒçš„æ–¹æ³•æ¥é¢„å¤„ç†æ–‡æœ¬ã€‚è¿™é‡Œæœ‰ä¸€äº›ä½ åº”è¯¥çŸ¥é“çš„å¸¸ç”¨æ–¹æ³•ï¼Œæˆ‘ä¼šè¯•ç€å¼ºè°ƒæ¯ç§æ–¹æ³•çš„é‡è¦æ€§ã€‚</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/603a5cb5b86800366a2b1b035ed130c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FaHctxmUp5O1IB6n0CL1GA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by the author</figcaption></figure><h2 id="e045" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="7369" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu">#Importing necessary libraries</strong></span><span id="3343" class="kz la it nc b gy nk nh l ni nj">import numpy as np<br/>import pandas as pd<br/>import re<br/>import nltk<br/>import spacy<br/>import string</span><span id="d6c7" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Reading the dataset</strong></span><span id="0f92" class="kz la it nc b gy nk nh l ni nj">df = pd.read_csv("sample.csv")<br/>df.head()</span></pre><h2 id="10c3" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/e42a59e89a05e4bc113e873f6888483d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HkiskB-PL-xSET6vVluPQ.png"/></div></div></figure><h1 id="8ba7" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">ä¸‹éƒ¨å¤–å£³</h1><p id="9413" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">è¿™æ˜¯æœ€å¸¸è§å’Œæœ€ç®€å•çš„æ–‡æœ¬é¢„å¤„ç†æŠ€æœ¯ã€‚é€‚ç”¨äºå¤§å¤šæ•°æ–‡æœ¬æŒ–æ˜å’Œ NLP é—®é¢˜ã€‚ä¸»è¦ç›®æ ‡æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ï¼Œä»¥ä¾¿â€œappleâ€ã€â€œAPPLEâ€å’Œâ€œAppleâ€å¾—åˆ°ç›¸åŒçš„å¤„ç†ã€‚</p><h2 id="686d" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="f7b7" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Lower Casing --&gt; creating new column called text_lower</strong></span><span id="d761" class="kz la it nc b gy nk nh l ni nj">df['text_lower']  = df['text'].str.lower()<br/>df['text_lower'].head()</span></pre><h2 id="c6d9" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="68cd" class="kz la it nc b gy ng nh l ni nj">0    @applesupport causing the reply to be disregar...<br/>1    @105835 your business means a lot to us. pleas...<br/>2    @76328 i really hope you all change but i'm su...<br/>3    @105836 livechat is online at the moment - htt...<br/>4    @virgintrains see attached error message. i've...<br/>Name: text_lower, dtype: object</span></pre><h1 id="07f2" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">åˆ é™¤æ ‡ç‚¹ç¬¦å·</h1><h2 id="456b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="5d33" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu">#removing punctuation, creating a new column called 'text_punct]'</strong><br/>df['text_punct'] = df['text'].str.replace('[^\w\s]','')<br/>df['text_punct'].head()</span></pre><h2 id="5479" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="427b" class="kz la it nc b gy ng nh l ni nj">0    applesupport causing the reply to be disregard...<br/>1    105835 your business means a lot to us please ...<br/>2    76328 I really hope you all change but im sure...<br/>3    105836 LiveChat is online at the moment  https...<br/>4    virginTrains see attached error message Ive tr...<br/>Name: text_punct, dtype: object</span></pre><h1 id="81c3" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">åœç”¨è¯åˆ é™¤</h1><p id="3e19" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">åœç”¨è¯æ˜¯ä¸€ç§è¯­è¨€ä¸­çš„ä¸€ç»„å¸¸ç”¨è¯ã€‚è‹±è¯­ä¸­åœç”¨è¯çš„ä¾‹å­æœ‰â€œaâ€ã€â€œweâ€ã€â€œtheâ€ã€â€œisâ€ã€â€œareâ€ç­‰ã€‚ä½¿ç”¨åœç”¨è¯èƒŒåçš„æƒ³æ³•æ˜¯ï¼Œé€šè¿‡ä»æ–‡æœ¬ä¸­åˆ é™¤ä½ä¿¡æ¯é‡çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºé‡è¦çš„è¯ã€‚æˆ‘ä»¬å¯ä»¥è‡ªå·±åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„åœç”¨è¯åˆ—è¡¨(åŸºäºç”¨ä¾‹)ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é¢„å®šä¹‰çš„åº“ã€‚</p><h2 id="03ae" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="315a" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu">#Importing stopwords from nltk library</strong><br/>from nltk.corpus import stopwords<br/>STOPWORDS = set(stopwords.words('english'))</span><span id="3ae8" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Function to remove the stopwords</strong><br/>def stopwords(text):<br/>    return " ".join([word for word in str(text).split() if word not in STOPWORDS])</span><span id="fb64" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Applying the stopwords to 'text_punct' and store into 'text_stop'</strong><br/>df["text_stop"] = df["text_punct"].apply(stopwords)<br/>df["text_stop"].head()</span></pre><h2 id="dd24" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">è¾“å‡º</strong></h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="fb35" class="kz la it nc b gy ng nh l ni nj">0    appleSupport causing reply disregarded tapped ...<br/>1    105835 your business means lot us please DM na...<br/>2    76328 I really hope change Im sure wont becaus...<br/>3    105836 LiveChat online moment httpstcoSY94VtU8...<br/>4    virgintrains see attached error message Ive tr...<br/>Name: text_stop, dtype: object</span></pre><h1 id="089d" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">å¸¸ç”¨è¯å»é™¤</h1><p id="c5ca" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">æˆ‘ä»¬è¿˜å¯ä»¥ä»æ–‡æœ¬æ•°æ®ä¸­åˆ é™¤å¸¸è§çš„å•è¯ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ–‡æœ¬æ•°æ®ä¸­æœ€å¸¸å‡ºç°çš„ 10 ä¸ªå•è¯ã€‚</p><h2 id="13fb" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="c93c" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Checking the first 10 most frequent words</strong><br/>from collections import Counter<br/>cnt = Counter()<br/>for text in df["text_stop"].values:<br/>    for word in text.split():<br/>        cnt[word] += 1<br/>        <br/>cnt.most_common(10)</span></pre><h2 id="432e" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="6f67" class="kz la it nc b gy ng nh l ni nj">[('I', 34),<br/> ('us', 25),<br/> ('DM', 19),<br/> ('help', 17),<br/> ('httpstcoGDrqU22YpT', 12),<br/> ('AppleSupport', 11),<br/> ('Thanks', 11),<br/> ('phone', 9),<br/> ('Ive', 8),<br/> ('Hi', 8)]</span></pre><p id="e6a0" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ é™¤ç»™å®šè¯­æ–™åº“ä¸­çš„å¸¸ç”¨è¯ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ tf-idfï¼Œè¿™å¯ä»¥è‡ªåŠ¨å¤„ç†</p><h2 id="7f92" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="cfbe" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Removing the frequent words</strong><br/>freq = set([w for (w, wc) in cnt.most_common(10)])</span><span id="b6b6" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># function to remove the frequent words</strong><br/>def freqwords(text):<br/>    return " ".join([word for word in str(text).split() if word not <br/>in freq])</span><span id="7b60" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Passing the function freqwords</strong><br/>df["text_common"] = df["text_stop"].apply(freqwords)<br/>df["text_common"].head()</span></pre><h2 id="22db" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="ed47" class="kz la it nc b gy ng nh l ni nj">0    causing reply disregarded tapped notification ...<br/>1    105835 Your business means lot please name zip...<br/>2    76328 really hope change Im sure wont because ...<br/>3    105836 LiveChat online moment httpstcoSY94VtU8...<br/>4    virgintrains see attached error message tried ...<br/>Name: text_common, dtype: object</span></pre><h1 id="beb2" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">å»é™¤ç”Ÿåƒ»å­—</h1><p id="9049" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">è¿™æ˜¯éå¸¸ç›´è§‚çš„ï¼Œå› ä¸ºå¯¹äºä¸åŒçš„ NLP ä»»åŠ¡ï¼Œä¸€äº›æœ¬è´¨ä¸Šéå¸¸ç‹¬ç‰¹çš„è¯ï¼Œå¦‚åç§°ã€å“ç‰Œã€äº§å“åç§°ï¼Œä»¥åŠä¸€äº›å¹²æ‰°å­—ç¬¦ï¼Œå¦‚ html çœç•¥ï¼Œä¹Ÿéœ€è¦è¢«åˆ é™¤ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨å•è¯çš„é•¿åº¦ä½œä¸ºæ ‡å‡†æ¥åˆ é™¤éå¸¸çŸ­æˆ–éå¸¸é•¿çš„å•è¯</p><h2 id="15fd" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="084c" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Removal of 10 rare words and store into new column called </strong>'text_rare'<br/>freq = pd.Series(' '.join(df['text_common']).split()).value_counts()[-10:] # 10 rare words<br/>freq = list(freq.index)<br/>df['text_rare'] = df['text_common'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))<br/>df['text_rare'].head()</span></pre><h2 id="9127" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="814b" class="kz la it nc b gy ng nh l ni nj">0    causing reply disregarded tapped notification ...<br/>1    105835 Your business means lot please name zip...<br/>2    76328 really hope change Im sure wont because ...<br/>3    105836 liveChat online moment httpstcoSY94VtU8...<br/>4    virgintrains see attached error message tried ...<br/>Name: text_rare, dtype: object</span></pre><h1 id="1d14" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">æ‹¼å†™çº æ­£</h1><p id="54ec" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">ç¤¾äº¤åª’ä½“æ•°æ®æ€»æ˜¯æ‚ä¹±çš„æ•°æ®ï¼Œè€Œä¸”æœ‰æ‹¼å†™é”™è¯¯ã€‚å› æ­¤ï¼Œæ‹¼å†™çº æ­£æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå› ä¸ºè¿™å°†å¸®åŠ©æˆ‘ä»¬é¿å…å¤šä¸ªå•è¯ã€‚ä¾‹å¦‚ï¼Œâ€œtextâ€å’Œâ€œtxtâ€å°†è¢«è§†ä¸ºä¸åŒçš„å•è¯ï¼Œå³ä½¿å®ƒä»¬åœ¨ç›¸åŒçš„æ„ä¹‰ä¸Šä½¿ç”¨ã€‚è¿™å¯ä»¥é€šè¿‡ textblob åº“æ¥å®Œæˆ</p><h2 id="de1b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">ä»£å·</strong></h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="75fd" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Spell check using text blob for the first 5 records</strong><br/>from textblob import TextBlob<br/>df['text_rare'][:5].apply(lambda x: str(TextBlob(x).correct()))</span></pre><h2 id="980a" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a1d85bd21ffa329b780f90c466311c07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*clZN5DVIIuxWGnq5RGH4qQ.png"/></div></figure><h1 id="72f8" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">è¡¨æƒ…ç¬¦å·ç§»é™¤</h1><p id="6637" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">è¡¨æƒ…ç¬¦å·æ˜¯æˆ‘ä»¬ç”Ÿæ´»çš„ä¸€éƒ¨åˆ†ã€‚ç¤¾äº¤åª’ä½“æ–‡å­—æœ‰å¾ˆå¤šè¡¨æƒ…ç¬¦å·ã€‚æˆ‘ä»¬éœ€è¦åœ¨æ–‡æœ¬åˆ†æä¸­åˆ é™¤ç›¸åŒçš„å†…å®¹</p><h2 id="d156" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><p id="bcdf" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">ä»£ç å‚è€ƒ:<a class="ae ky" href="https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b" rel="noopener ugc nofollow" target="_blank"> Github </a></p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="1b29" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Function to remove emoji.</strong><br/>def emoji(string):<br/>    emoji_pattern = re.compile("["<br/>                           u"\U0001F600-\U0001F64F"  # emoticons<br/>                           u"\U0001F300-\U0001F5FF"  # symbols &amp; pictographs<br/>                           u"\U0001F680-\U0001F6FF"  # transport &amp; map symbols<br/>                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)<br/>                           u"\U00002702-\U000027B0"<br/>                           u"\U000024C2-\U0001F251"<br/>                           "]+", flags=re.UNICODE)<br/>    return emoji_pattern.sub(r'', string)</span><span id="f99d" class="kz la it nc b gy nk nh l ni nj">emoji("Hi, I am Emoji  ğŸ˜œ")<br/><strong class="nc iu">#passing the emoji function to 'text_rare'</strong><br/>df['text_rare'] = df['text_rare'].apply(remove_emoji)</span></pre><h2 id="1889" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="21d5" class="kz la it nc b gy ng nh l ni nj">'Hi, I am Emoji  '</span></pre><h1 id="1dab" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">è¡¨æƒ…ç§»é™¤</h1><p id="ef13" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">åœ¨å‰é¢çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å·²ç»åˆ é™¤äº†è¡¨æƒ…ç¬¦å·ã€‚ç°åœ¨ï¼Œæˆ‘è¦ç§»é™¤è¡¨æƒ…ç¬¦å·ã€‚è¡¨æƒ…ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ:-)æ˜¯ä¸€ä¸ªè¡¨æƒ…ç¬¦å·ğŸ˜œâ†’è¡¨æƒ…ç¬¦å·ã€‚</p><p id="1f27" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">ä½¿ç”¨ emot åº“ã€‚è¯·å‚è€ƒæ›´å¤šå…³äº<a class="ae ky" href="https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py" rel="noopener ugc nofollow" target="_blank">è¡¨æƒ…</a></p><h2 id="873e" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="f2b8" class="kz la it nc b gy ng nh l ni nj">from emot.emo_unicode import UNICODE_EMO, EMOTICONS</span><span id="ce82" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Function for removing emoticons</strong><br/>def remove_emoticons(text):<br/>    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')<br/>    return emoticon_pattern.sub(r'', text)</span><span id="a229" class="kz la it nc b gy nk nh l ni nj">remove_emoticons("Hello :-)")<br/><strong class="nc iu"># applying remove_emoticons to 'text_rare'</strong><br/>df['text_rare'] = df['text_rare'].apply(remove_emoticons)</span></pre><h2 id="c02c" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="1a95" class="kz la it nc b gy ng nh l ni nj">'Hello '</span></pre><h1 id="b5d4" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">å°†è¡¨æƒ…ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·è½¬æ¢ä¸ºæ–‡å­—</h1><p id="2f60" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">åœ¨æƒ…æ„Ÿåˆ†æä¸­ï¼Œè¡¨æƒ…ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·è¡¨è¾¾äº†ä¸€ç§æƒ…æ„Ÿã€‚å› æ­¤ï¼Œåˆ é™¤å®ƒä»¬å¯èƒ½ä¸æ˜¯ä¸€ä¸ªå¥½çš„è§£å†³æ–¹æ¡ˆã€‚</p><h2 id="a14b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="5e17" class="kz la it nc b gy ng nh l ni nj">from emot.emo_unicode import UNICODE_EMO, EMOTICONS</span><span id="4df8" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Converting emojis to words</strong><br/>def convert_emojis(text):<br/>    for emot in UNICODE_EMO:<br/>        text = text.replace(emot, "_".join(UNICODE_EMO[emot].replace(",","").replace(":","").split()))<br/>        return text</span><span id="917e" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Converting emoticons to words </strong>   <br/>def convert_emoticons(text):<br/>    for emot in EMOTICONS:<br/>        text = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), text)<br/>        return text</span><span id="6954" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Example</strong><br/>text = "Hello :-) :-)"<br/>convert_emoticons(text)</span><span id="f887" class="kz la it nc b gy nk nh l ni nj">text1 = "Hilarious ğŸ˜‚"<br/>convert_emojis(text1)</span><span id="d434" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Passing both functions to 'text_rare'</strong><br/>df['text_rare'] = df['text_rare'].apply(convert_emoticons)<br/>df['text_rare'] = df['text_rare'].apply(convert_emojis)</span></pre><h2 id="a9f5" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">è¾“å‡º</strong></h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="9c76" class="kz la it nc b gy ng nh l ni nj">'Hello happy smiley face happy smiley face:-)'<br/>'Hilarious face_with_tears_of_joy'</span></pre><h1 id="cf71" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">ç§»é™¤ URL</h1><p id="c062" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">åˆ é™¤æ–‡æœ¬ä¸­çš„ URLã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¼‚äº®çš„æ±¤åº“</p><h2 id="92d8" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="1eac" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Function for url's</strong><br/>def remove_urls(text):<br/>    url_pattern = re.compile(r'https?://\S+|www\.\S+')<br/>    return url_pattern.sub(r'', text)</span><span id="245e" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Examples</strong><br/>text = "This is my website, <a class="ae ky" href="https://www.abc.com" rel="noopener ugc nofollow" target="_blank">https://www.abc.com</a>"<br/>remove_urls(text)</span><span id="fe2e" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu">#Passing the function to 'text_rare'</strong><br/>df['text_rare'] = df['text_rare'].apply(remove_urls)</span></pre><h2 id="ff18" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="f824" class="kz la it nc b gy ng nh l ni nj">'This is my website, '</span></pre><h1 id="aa16" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">ç§»é™¤ HTML æ ‡ç­¾</h1><p id="f307" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">å¦ä¸€ç§å¸¸è§çš„é¢„å¤„ç†æŠ€æœ¯æ˜¯åˆ é™¤ HTML æ ‡ç­¾ã€‚é€šå¸¸å‡ºç°åœ¨æŠ“å–æ•°æ®ä¸­çš„ HTML æ ‡ç­¾ã€‚</p><h2 id="624e" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="d389" class="kz la it nc b gy ng nh l ni nj">from bs4 import BeautifulSoup</span><span id="08dc" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu">#Function for removing html</strong><br/>def html(text):<br/>    return BeautifulSoup(text, "lxml").text<br/><strong class="nc iu"># Examples</strong><br/>text = """&lt;div&gt;<br/>&lt;h1&gt; This&lt;/h1&gt;<br/>&lt;p&gt; is&lt;/p&gt;<br/>&lt;a href="<a class="ae ky" href="https://www.abc.com/" rel="noopener ugc nofollow" target="_blank">https://www.abc.com/</a>"&gt; ABCD&lt;/a&gt;<br/>&lt;/div&gt;<br/>"""<br/>print(html(text))<br/><strong class="nc iu"># Passing the function to 'text_rare'</strong><br/>df['text_rare'] = df['text_rare'].apply(html)</span></pre><h2 id="17d6" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="ee1d" class="kz la it nc b gy ng nh l ni nj">This<br/> is<br/> ABCD</span></pre><h1 id="b7ca" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">æ ‡è®°åŒ–</h1><p id="4a77" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">æ ‡è®°åŒ–æ˜¯æŒ‡å°†æ–‡æœ¬åˆ†æˆä¸€ç³»åˆ—å•è¯æˆ–å¥å­ã€‚</p><h2 id="a40c" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="8ac0" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu">#Creating function for tokenization</strong><br/>def tokenization(text):<br/>    text = re.split('\W+', text)<br/>    return text<br/><strong class="nc iu"># Passing the function to 'text_rare' and store into'text_token'</strong><br/>df['text_token'] = df['text_rare'].apply(lambda x: tokenization(x.lower()))<br/>df[['text_token']].head()</span></pre><h2 id="3b6c" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e5243e44cf7a3c124064bc31c72fc623.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*ADOhD2l2ScUF6ik3mJFzow.png"/></div></figure><h1 id="02ea" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">è¯å¹²åŒ–å’Œè¯æ±‡åŒ–</h1><p id="2e0e" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">è¯æ±‡åŒ–æ˜¯å°†ä¸€ä¸ªè¯è½¬æ¢æˆå®ƒçš„åŸºæœ¬å½¢å¼çš„è¿‡ç¨‹ã€‚è¯å¹²åŒ–å’Œè¯å…ƒåŒ–çš„åŒºåˆ«åœ¨äºï¼Œè¯å…ƒåŒ–è€ƒè™‘ä¸Šä¸‹æ–‡å¹¶å°†å•è¯è½¬æ¢ä¸ºå…¶æœ‰æ„ä¹‰çš„åŸºæœ¬å½¢å¼ï¼Œè€Œè¯å¹²åŒ–åªæ˜¯åˆ é™¤æœ€åå‡ ä¸ªå­—ç¬¦ï¼Œé€šå¸¸ä¼šå¯¼è‡´ä¸æ­£ç¡®çš„æ„æ€å’Œæ‹¼å†™é”™è¯¯ã€‚è¿™é‡Œï¼Œä»…æ‰§è¡Œäº†æœ¯è¯­åŒ–ã€‚æˆ‘ä»¬éœ€è¦ä¸º NLTK ä¸­çš„ lemmatizer æä¾›å•è¯çš„ POS æ ‡ç­¾ã€‚æ ¹æ®ä½ç½®çš„ä¸åŒï¼Œlemmatizer å¯èƒ½ä¼šè¿”å›ä¸åŒçš„ç»“æœã€‚</p><h2 id="7a58" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">å¯†ç </h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="76ea" class="kz la it nc b gy ng nh l ni nj">from nltk.corpus import wordnet<br/>from nltk.stem import WordNetLemmatizer</span><span id="c661" class="kz la it nc b gy nk nh l ni nj">lemmatizer = WordNetLemmatizer()<br/>wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV} # Pos tag, used Noun, Verb, Adjective and Adverb</span><span id="6a30" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Function for lemmatization using POS tag</strong><br/>def lemmatize_words(text):<br/>    pos_tagged_text = nltk.pos_tag(text.split())<br/>    return " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])</span><span id="4ef6" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Passing the function to 'text_rare' and store in 'text_lemma'</strong><br/>df["text_lemma"] = df["text_rare"].apply(lemmatize_words)</span></pre><h2 id="c6fe" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">è¾“å‡º</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/891a78c8aabf0c0552f7547f99a5bdaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*pRlfp5riGw9Rk5lPrKyMIg.png"/></div></figure><p id="e699" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">ä»¥ä¸Šæ–¹æ³•æ˜¯å¸¸è§çš„æ–‡æœ¬é¢„å¤„ç†æ­¥éª¤ã€‚</p><p id="3540" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">æ„Ÿè°¢é˜…è¯»ã€‚è¯·ç»§ç»­å­¦ä¹ ï¼Œå¹¶å…³æ³¨æ›´å¤šå†…å®¹ï¼</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="3821" class="nm la it bd lb nn oa np le nq ob ns lh jz oc ka ll kc od kd lp kf oe kg lt nw bi translated">å‚è€ƒ:</h1><ol class=""><li id="0ab0" class="of og it lx b ly lz mb mc li oh lm oi lq oj mn ok ol om on bi translated">ã€https://www.nltk.org T4ã€‘</li><li id="0967" class="of og it lx b ly oo mb op li oq lm or lq os mn ok ol om on bi translated"><a class="ae ky" href="https://www.geeksforgeeks.org/nlp-chunk-tree-to-text-and-chaining-chunk-transformation/" rel="noopener ugc nofollow" target="_blank">https://www.edureka.co</a></li><li id="b697" class="of og it lx b ly oo mb op li oq lm or lq os mn ok ol om on bi translated"><a class="ae ky" href="https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/part-speech-tagging-stop-words-using-nltk-python/</a></li></ol></div></div>    
</body>
</html>