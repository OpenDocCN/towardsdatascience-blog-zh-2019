<html>
<head>
<title>Text Preprocessing for Data Scientists</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学家的文本预处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d?source=collection_archive---------12-----------------------#2019-12-13">https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d?source=collection_archive---------12-----------------------#2019-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9052" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">文本预处理的简便指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0da906e23fb47d9e68372812c704fd6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*np2bP68pEJVkFoc9HwMJMA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by <a class="ae ky" href="https://pixabay.com/users/Devanath-1785462/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1248088" rel="noopener ugc nofollow" target="_blank">Devanath</a> from <a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1248088" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><h2 id="b6f0" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">文本预处理</h2><p id="a1a6" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">文本预处理是文本分析和自然语言处理的重要任务和关键步骤。它将文本转换为可预测和可分析的形式，以便机器学习算法可以更好地执行。这是一个方便的文本预处理指南，是我之前关于文本挖掘的博客的延续。在这篇博客中，我使用了来自 Kaggle 的 twitter 数据集。</p><p id="889d" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">有不同的方法来预处理文本。这里有一些你应该知道的常用方法，我会试着强调每种方法的重要性。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/603a5cb5b86800366a2b1b035ed130c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FaHctxmUp5O1IB6n0CL1GA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by the author</figcaption></figure><h2 id="e045" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="7369" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu">#Importing necessary libraries</strong></span><span id="3343" class="kz la it nc b gy nk nh l ni nj">import numpy as np<br/>import pandas as pd<br/>import re<br/>import nltk<br/>import spacy<br/>import string</span><span id="d6c7" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Reading the dataset</strong></span><span id="0f92" class="kz la it nc b gy nk nh l ni nj">df = pd.read_csv("sample.csv")<br/>df.head()</span></pre><h2 id="10c3" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/e42a59e89a05e4bc113e873f6888483d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HkiskB-PL-xSET6vVluPQ.png"/></div></div></figure><h1 id="8ba7" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">下部外壳</h1><p id="9413" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">这是最常见和最简单的文本预处理技术。适用于大多数文本挖掘和 NLP 问题。主要目标是将文本转换为小写，以便“apple”、“APPLE”和“Apple”得到相同的处理。</p><h2 id="686d" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="f7b7" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Lower Casing --&gt; creating new column called text_lower</strong></span><span id="d761" class="kz la it nc b gy nk nh l ni nj">df['text_lower']  = df['text'].str.lower()<br/>df['text_lower'].head()</span></pre><h2 id="c6d9" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="68cd" class="kz la it nc b gy ng nh l ni nj">0    @applesupport causing the reply to be disregar...<br/>1    @105835 your business means a lot to us. pleas...<br/>2    @76328 i really hope you all change but i'm su...<br/>3    @105836 livechat is online at the moment - htt...<br/>4    @virgintrains see attached error message. i've...<br/>Name: text_lower, dtype: object</span></pre><h1 id="07f2" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">删除标点符号</h1><h2 id="456b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="5d33" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu">#removing punctuation, creating a new column called 'text_punct]'</strong><br/>df['text_punct'] = df['text'].str.replace('[^\w\s]','')<br/>df['text_punct'].head()</span></pre><h2 id="5479" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="427b" class="kz la it nc b gy ng nh l ni nj">0    applesupport causing the reply to be disregard...<br/>1    105835 your business means a lot to us please ...<br/>2    76328 I really hope you all change but im sure...<br/>3    105836 LiveChat is online at the moment  https...<br/>4    virginTrains see attached error message Ive tr...<br/>Name: text_punct, dtype: object</span></pre><h1 id="81c3" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">停用词删除</h1><p id="3e19" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">停用词是一种语言中的一组常用词。英语中停用词的例子有“a”、“we”、“the”、“is”、“are”等。使用停用词背后的想法是，通过从文本中删除低信息量的词，我们可以专注于重要的词。我们可以自己创建一个自定义的停用词列表(基于用例)，也可以使用预定义的库。</p><h2 id="03ae" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="315a" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu">#Importing stopwords from nltk library</strong><br/>from nltk.corpus import stopwords<br/>STOPWORDS = set(stopwords.words('english'))</span><span id="3ae8" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Function to remove the stopwords</strong><br/>def stopwords(text):<br/>    return " ".join([word for word in str(text).split() if word not in STOPWORDS])</span><span id="fb64" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Applying the stopwords to 'text_punct' and store into 'text_stop'</strong><br/>df["text_stop"] = df["text_punct"].apply(stopwords)<br/>df["text_stop"].head()</span></pre><h2 id="dd24" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">输出</strong></h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="fb35" class="kz la it nc b gy ng nh l ni nj">0    appleSupport causing reply disregarded tapped ...<br/>1    105835 your business means lot us please DM na...<br/>2    76328 I really hope change Im sure wont becaus...<br/>3    105836 LiveChat online moment httpstcoSY94VtU8...<br/>4    virgintrains see attached error message Ive tr...<br/>Name: text_stop, dtype: object</span></pre><h1 id="089d" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">常用词去除</h1><p id="c5ca" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">我们还可以从文本数据中删除常见的单词。首先，让我们检查一下文本数据中最常出现的 10 个单词。</p><h2 id="13fb" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="c93c" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Checking the first 10 most frequent words</strong><br/>from collections import Counter<br/>cnt = Counter()<br/>for text in df["text_stop"].values:<br/>    for word in text.split():<br/>        cnt[word] += 1<br/>        <br/>cnt.most_common(10)</span></pre><h2 id="432e" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="6f67" class="kz la it nc b gy ng nh l ni nj">[('I', 34),<br/> ('us', 25),<br/> ('DM', 19),<br/> ('help', 17),<br/> ('httpstcoGDrqU22YpT', 12),<br/> ('AppleSupport', 11),<br/> ('Thanks', 11),<br/> ('phone', 9),<br/> ('Ive', 8),<br/> ('Hi', 8)]</span></pre><p id="e6a0" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">现在，我们可以删除给定语料库中的常用词。如果我们使用 tf-idf，这可以自动处理</p><h2 id="7f92" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="cfbe" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Removing the frequent words</strong><br/>freq = set([w for (w, wc) in cnt.most_common(10)])</span><span id="b6b6" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># function to remove the frequent words</strong><br/>def freqwords(text):<br/>    return " ".join([word for word in str(text).split() if word not <br/>in freq])</span><span id="7b60" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Passing the function freqwords</strong><br/>df["text_common"] = df["text_stop"].apply(freqwords)<br/>df["text_common"].head()</span></pre><h2 id="22db" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="ed47" class="kz la it nc b gy ng nh l ni nj">0    causing reply disregarded tapped notification ...<br/>1    105835 Your business means lot please name zip...<br/>2    76328 really hope change Im sure wont because ...<br/>3    105836 LiveChat online moment httpstcoSY94VtU8...<br/>4    virgintrains see attached error message tried ...<br/>Name: text_common, dtype: object</span></pre><h1 id="beb2" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">去除生僻字</h1><p id="9049" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">这是非常直观的，因为对于不同的 NLP 任务，一些本质上非常独特的词，如名称、品牌、产品名称，以及一些干扰字符，如 html 省略，也需要被删除。我们还使用单词的长度作为标准来删除非常短或非常长的单词</p><h2 id="15fd" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="084c" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Removal of 10 rare words and store into new column called </strong>'text_rare'<br/>freq = pd.Series(' '.join(df['text_common']).split()).value_counts()[-10:] # 10 rare words<br/>freq = list(freq.index)<br/>df['text_rare'] = df['text_common'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))<br/>df['text_rare'].head()</span></pre><h2 id="9127" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="814b" class="kz la it nc b gy ng nh l ni nj">0    causing reply disregarded tapped notification ...<br/>1    105835 Your business means lot please name zip...<br/>2    76328 really hope change Im sure wont because ...<br/>3    105836 liveChat online moment httpstcoSY94VtU8...<br/>4    virgintrains see attached error message tried ...<br/>Name: text_rare, dtype: object</span></pre><h1 id="1d14" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">拼写纠正</h1><p id="54ec" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">社交媒体数据总是杂乱的数据，而且有拼写错误。因此，拼写纠正是一个有用的预处理步骤，因为这将帮助我们避免多个单词。例如，“text”和“txt”将被视为不同的单词，即使它们在相同的意义上使用。这可以通过 textblob 库来完成</p><h2 id="de1b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">代号</strong></h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="75fd" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Spell check using text blob for the first 5 records</strong><br/>from textblob import TextBlob<br/>df['text_rare'][:5].apply(lambda x: str(TextBlob(x).correct()))</span></pre><h2 id="980a" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a1d85bd21ffa329b780f90c466311c07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*clZN5DVIIuxWGnq5RGH4qQ.png"/></div></figure><h1 id="72f8" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">表情符号移除</h1><p id="6637" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">表情符号是我们生活的一部分。社交媒体文字有很多表情符号。我们需要在文本分析中删除相同的内容</p><h2 id="d156" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><p id="bcdf" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">代码参考:<a class="ae ky" href="https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b" rel="noopener ugc nofollow" target="_blank"> Github </a></p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="1b29" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Function to remove emoji.</strong><br/>def emoji(string):<br/>    emoji_pattern = re.compile("["<br/>                           u"\U0001F600-\U0001F64F"  # emoticons<br/>                           u"\U0001F300-\U0001F5FF"  # symbols &amp; pictographs<br/>                           u"\U0001F680-\U0001F6FF"  # transport &amp; map symbols<br/>                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)<br/>                           u"\U00002702-\U000027B0"<br/>                           u"\U000024C2-\U0001F251"<br/>                           "]+", flags=re.UNICODE)<br/>    return emoji_pattern.sub(r'', string)</span><span id="f99d" class="kz la it nc b gy nk nh l ni nj">emoji("Hi, I am Emoji  😜")<br/><strong class="nc iu">#passing the emoji function to 'text_rare'</strong><br/>df['text_rare'] = df['text_rare'].apply(remove_emoji)</span></pre><h2 id="1889" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="21d5" class="kz la it nc b gy ng nh l ni nj">'Hi, I am Emoji  '</span></pre><h1 id="1dab" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">表情移除</h1><p id="ef13" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">在前面的步骤中，我们已经删除了表情符号。现在，我要移除表情符号。表情符号和表情符号有什么区别？:-)是一个表情符号😜→表情符号。</p><p id="1f27" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">使用 emot 库。请参考更多关于<a class="ae ky" href="https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py" rel="noopener ugc nofollow" target="_blank">表情</a></p><h2 id="873e" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="f2b8" class="kz la it nc b gy ng nh l ni nj">from emot.emo_unicode import UNICODE_EMO, EMOTICONS</span><span id="ce82" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Function for removing emoticons</strong><br/>def remove_emoticons(text):<br/>    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')<br/>    return emoticon_pattern.sub(r'', text)</span><span id="a229" class="kz la it nc b gy nk nh l ni nj">remove_emoticons("Hello :-)")<br/><strong class="nc iu"># applying remove_emoticons to 'text_rare'</strong><br/>df['text_rare'] = df['text_rare'].apply(remove_emoticons)</span></pre><h2 id="c02c" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="1a95" class="kz la it nc b gy ng nh l ni nj">'Hello '</span></pre><h1 id="b5d4" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">将表情符号和表情符号转换为文字</h1><p id="2f60" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">在情感分析中，表情符号和表情符号表达了一种情感。因此，删除它们可能不是一个好的解决方案。</p><h2 id="a14b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="5e17" class="kz la it nc b gy ng nh l ni nj">from emot.emo_unicode import UNICODE_EMO, EMOTICONS</span><span id="4df8" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Converting emojis to words</strong><br/>def convert_emojis(text):<br/>    for emot in UNICODE_EMO:<br/>        text = text.replace(emot, "_".join(UNICODE_EMO[emot].replace(",","").replace(":","").split()))<br/>        return text</span><span id="917e" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Converting emoticons to words </strong>   <br/>def convert_emoticons(text):<br/>    for emot in EMOTICONS:<br/>        text = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), text)<br/>        return text</span><span id="6954" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Example</strong><br/>text = "Hello :-) :-)"<br/>convert_emoticons(text)</span><span id="f887" class="kz la it nc b gy nk nh l ni nj">text1 = "Hilarious 😂"<br/>convert_emojis(text1)</span><span id="d434" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Passing both functions to 'text_rare'</strong><br/>df['text_rare'] = df['text_rare'].apply(convert_emoticons)<br/>df['text_rare'] = df['text_rare'].apply(convert_emojis)</span></pre><h2 id="a9f5" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">输出</strong></h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="9c76" class="kz la it nc b gy ng nh l ni nj">'Hello happy smiley face happy smiley face:-)'<br/>'Hilarious face_with_tears_of_joy'</span></pre><h1 id="cf71" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">移除 URL</h1><p id="c062" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">删除文本中的 URL。我们可以使用漂亮的汤库</p><h2 id="92d8" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="1eac" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu"># Function for url's</strong><br/>def remove_urls(text):<br/>    url_pattern = re.compile(r'https?://\S+|www\.\S+')<br/>    return url_pattern.sub(r'', text)</span><span id="245e" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Examples</strong><br/>text = "This is my website, <a class="ae ky" href="https://www.abc.com" rel="noopener ugc nofollow" target="_blank">https://www.abc.com</a>"<br/>remove_urls(text)</span><span id="fe2e" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu">#Passing the function to 'text_rare'</strong><br/>df['text_rare'] = df['text_rare'].apply(remove_urls)</span></pre><h2 id="ff18" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="f824" class="kz la it nc b gy ng nh l ni nj">'This is my website, '</span></pre><h1 id="aa16" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">移除 HTML 标签</h1><p id="f307" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">另一种常见的预处理技术是删除 HTML 标签。通常出现在抓取数据中的 HTML 标签。</p><h2 id="624e" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="d389" class="kz la it nc b gy ng nh l ni nj">from bs4 import BeautifulSoup</span><span id="08dc" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu">#Function for removing html</strong><br/>def html(text):<br/>    return BeautifulSoup(text, "lxml").text<br/><strong class="nc iu"># Examples</strong><br/>text = """&lt;div&gt;<br/>&lt;h1&gt; This&lt;/h1&gt;<br/>&lt;p&gt; is&lt;/p&gt;<br/>&lt;a href="<a class="ae ky" href="https://www.abc.com/" rel="noopener ugc nofollow" target="_blank">https://www.abc.com/</a>"&gt; ABCD&lt;/a&gt;<br/>&lt;/div&gt;<br/>"""<br/>print(html(text))<br/><strong class="nc iu"># Passing the function to 'text_rare'</strong><br/>df['text_rare'] = df['text_rare'].apply(html)</span></pre><h2 id="17d6" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="ee1d" class="kz la it nc b gy ng nh l ni nj">This<br/> is<br/> ABCD</span></pre><h1 id="b7ca" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">标记化</h1><p id="4a77" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">标记化是指将文本分成一系列单词或句子。</p><h2 id="a40c" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="8ac0" class="kz la it nc b gy ng nh l ni nj"><strong class="nc iu">#Creating function for tokenization</strong><br/>def tokenization(text):<br/>    text = re.split('\W+', text)<br/>    return text<br/><strong class="nc iu"># Passing the function to 'text_rare' and store into'text_token'</strong><br/>df['text_token'] = df['text_rare'].apply(lambda x: tokenization(x.lower()))<br/>df[['text_token']].head()</span></pre><h2 id="3b6c" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e5243e44cf7a3c124064bc31c72fc623.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*ADOhD2l2ScUF6ik3mJFzow.png"/></div></figure><h1 id="02ea" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">词干化和词汇化</h1><p id="2e0e" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">词汇化是将一个词转换成它的基本形式的过程。词干化和词元化的区别在于，词元化考虑上下文并将单词转换为其有意义的基本形式，而词干化只是删除最后几个字符，通常会导致不正确的意思和拼写错误。这里，仅执行了术语化。我们需要为 NLTK 中的 lemmatizer 提供单词的 POS 标签。根据位置的不同，lemmatizer 可能会返回不同的结果。</p><h2 id="7a58" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="76ea" class="kz la it nc b gy ng nh l ni nj">from nltk.corpus import wordnet<br/>from nltk.stem import WordNetLemmatizer</span><span id="c661" class="kz la it nc b gy nk nh l ni nj">lemmatizer = WordNetLemmatizer()<br/>wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV} # Pos tag, used Noun, Verb, Adjective and Adverb</span><span id="6a30" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Function for lemmatization using POS tag</strong><br/>def lemmatize_words(text):<br/>    pos_tagged_text = nltk.pos_tag(text.split())<br/>    return " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])</span><span id="4ef6" class="kz la it nc b gy nk nh l ni nj"><strong class="nc iu"># Passing the function to 'text_rare' and store in 'text_lemma'</strong><br/>df["text_lemma"] = df["text_rare"].apply(lemmatize_words)</span></pre><h2 id="c6fe" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">输出</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/891a78c8aabf0c0552f7547f99a5bdaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*pRlfp5riGw9Rk5lPrKyMIg.png"/></div></figure><p id="e699" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">以上方法是常见的文本预处理步骤。</p><p id="3540" class="pw-post-body-paragraph lv lw it lx b ly mo ju ma mb mp jx md li mq mf mg lm mr mi mj lq ms ml mm mn im bi translated">感谢阅读。请继续学习，并关注更多内容！</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="3821" class="nm la it bd lb nn oa np le nq ob ns lh jz oc ka ll kc od kd lp kf oe kg lt nw bi translated">参考:</h1><ol class=""><li id="0ab0" class="of og it lx b ly lz mb mc li oh lm oi lq oj mn ok ol om on bi translated">【https://www.nltk.org T4】</li><li id="0967" class="of og it lx b ly oo mb op li oq lm or lq os mn ok ol om on bi translated"><a class="ae ky" href="https://www.geeksforgeeks.org/nlp-chunk-tree-to-text-and-chaining-chunk-transformation/" rel="noopener ugc nofollow" target="_blank">https://www.edureka.co</a></li><li id="b697" class="of og it lx b ly oo mb op li oq lm or lq os mn ok ol om on bi translated"><a class="ae ky" href="https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/part-speech-tagging-stop-words-using-nltk-python/</a></li></ol></div></div>    
</body>
</html>