<html>
<head>
<title>Simple BERT using TensorFlow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TensorFlow 2.0 的简单 BERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22?source=collection_archive---------4-----------------------#2019-10-30">https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22?source=collection_archive---------4-----------------------#2019-10-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9cdd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在 15 行代码中使用 BERT 和 TensorFlow Hub。最后更新时间:2020 年 11 月 15 日。</h2></div><p id="d745" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个故事展示了一个使用 TensorFlow 2.0 嵌入 BERT [1]的简单例子。最近发布了 TensorFlow 2.0，该模块旨在使用基于高级 Keras API 的简单易用的模型。伯特之前的用法在<a class="ae lb" href="https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=LL5W8gEGRTAf" rel="noopener ugc nofollow" target="_blank">的长篇笔记本</a>中有描述，它实现了一个电影评论预测。在这个故事中，我们将看到一个使用 Keras 和最新的<a class="ae lb" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>和<a class="ae lb" href="https://www.tensorflow.org/hub" rel="noopener ugc nofollow" target="_blank"> TensorFlow Hub </a>模块的简单 BERT 嵌入生成器。</p><p id="d49d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Google Colab 上新的<a class="ae lb" href="https://colab.research.google.com/drive/1PJOmDL7oN_NmLRRbdhQPcABculw8Gbzk?usp=sharing" rel="noopener ugc nofollow" target="_blank">更新版本在这里</a>(2020–11–15)。老版本在这里<a class="ae lb" href="https://colab.research.google.com/drive/1hMLd5-r82FrnFnBub-B-fVW78Px4KPX1" rel="noopener ugc nofollow" target="_blank">有。</a></p><p id="99f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我之前的文章使用了<code class="fe lc ld le lf b"><a class="ae lb" href="https://pypi.org/project/bert-embedding/" rel="noopener ugc nofollow" target="_blank">bert-embedding</a> </code>模块，使用预先训练的无案例 BERT 基本模型生成句子级和标记级嵌入。这里，我们将只通过几个步骤来实现这个模块的用法。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/8edf3510a9fa88809aaa000c285ed788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d9fr2hmMzk-3MjOw2cOt0w.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk"><a class="ae lb" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">TensorFlow 2.0</a></figcaption></figure><h1 id="ebd0" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">更新 2020 年 11 月 15 日:HUB 上的新模型版本 v3</h1><p id="fbc6" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">对于 TensorFlow Hub 上的新模型版本 v3，它们包括一个<a class="ae lb" href="https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1" rel="noopener ugc nofollow" target="_blank">预处理器模型</a>来实现本故事中描述的步骤。Hub 版本还更改为使用字典输入和输出变量，因此如果您想以原始故事中描述的方式实现，请在使用较新的模型版本时考虑到这一点。</p><p id="32d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了新版本，我们有 3 个步骤要遵循:1)从 TF、TF-Hub 和 TF-text 导入正确的模块和模型；2)将输入加载到预处理器模型中；3)将预处理的输入加载到 BERT 编码器中。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="mt mu l"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">BERT with TensorFlow HUB — 15 lines of code (from the<a class="ae lb" href="https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3" rel="noopener ugc nofollow" target="_blank"> official HUB model example</a>)</figcaption></figure><p id="46e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我没有更新 Colab，而是用上面的例子创建了一个新的笔记本。我把原始版本留在这里，因为我相信它有助于理解预处理器模型的步骤。使用不同版本时，请注意轮毂模型导入结束时的版本(<code class="fe lc ld le lf b">/3</code>)。</p><h1 id="d3f1" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">模块导入</h1><p id="46f9" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">我们将使用最新的 TensorFlow (2.0+)和 TensorFlow Hub (0.7+)，因此，它可能需要在系统中进行升级。对于模型创建，我们使用高级 Keras API 模型类(新集成到 tf.keras 中)。</p><p id="582e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">BERT 记号赋予器仍然来自 BERT python 模块(bert-for-tf2)。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="mt mu l"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Imports of the project</figcaption></figure><h1 id="19bc" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">模型</h1><p id="0bac" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">我们将基于 TensorFlow Hub 上的示例实现一个模型。在这里，我们可以看到<code class="fe lc ld le lf b"> bert_layer </code>可以像其他 Keras 层一样用于更复杂的模型。</p><p id="6ca7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型的目标是使用预训练的 BERT 来生成嵌入向量。因此，我们只需要 BERT 层所需的输入，并且模型只将 BERT 层作为隐藏层。当然，在 BERT 层内部，还有一个更复杂的架构。</p><p id="95da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lc ld le lf b">hub.KerasLayer</code>函数将预训练的模型作为 Keras 层导入。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="mt mu l"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">BERT embedding model in Keras</figcaption></figure><h1 id="ff32" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">预处理</h1><p id="686f" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">BERT 层需要 3 个输入序列:</p><ul class=""><li id="57c8" class="mv mw iq kh b ki kj kl km ko mx ks my kw mz la na nb nc nd bi translated">标记 id:针对句子中的每个标记。我们从伯特词汇词典中恢复它</li><li id="7dea" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">mask ids:for each token，用于屏蔽仅用于序列填充的标记(因此每个序列都具有相同的长度)。</li><li id="f621" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">Segment ids:对于单句序列为 0，如果序列中有两个句子，并且是第二个，则为 1(更多信息请参见原始论文或 GitHub 上 BERT 的相应部分:<code class="fe lc ld le lf b"><a class="ae lb" href="https://github.com/google-research/bert/blob/master/run_classifier.py" rel="noopener ugc nofollow" target="_blank">run_classifier.py</a></code>中的<code class="fe lc ld le lf b">convert_single_example</code>)。</li></ul><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="mt mu l"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Functions to generate the input based on the tokens and the max sequence length</figcaption></figure><h1 id="d92b" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">预言；预测；预告</h1><p id="95bc" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">通过这些步骤，我们可以为我们的句子生成 BERT 上下文化嵌入向量！不要忘记添加<code class="fe lc ld le lf b">[CLS]</code>和<code class="fe lc ld le lf b">[SEP]</code>分隔符标记以保持原始格式！</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="mt mu l"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Bert Embedding Generator in use</figcaption></figure><h1 id="a407" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">作为句子级嵌入的混合嵌入</h1><p id="8cc0" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">原始论文建议使用<code class="fe lc ld le lf b">[CLS]</code>分隔符作为整个句子的表示，因为每个句子都有一个<code class="fe lc ld le lf b">[CLS]</code>标记，并且因为它是一个上下文化的嵌入，所以它可以表示整个句子。在我之前的作品中，也是用这个 token 的嵌入作为句子级的表示。来自 TensorFlow Hub 的<code class="fe lc ld le lf b">bert_layer</code>返回一个不同的合并输出，用于表示整个输入序列。</p><p id="bbc3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了比较这两种嵌入，我们使用余弦相似度。汇集嵌入和第一个标记嵌入在例句中的区别<em class="nj">“这是一个好句子。”</em>为 0.0276。</p><h1 id="a256" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">BERT 中的偏差</h1><p id="2098" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">当有人使用预先训练好的模型时，调查它的缺点和优点是很重要的。模型就像数据集一样有偏差，因此，如果使用有偏差的预训练模型，新模型很可能会继承缺陷。如果你使用 BERT，我建议你阅读我关于 BERT 中<a class="ae lb" rel="noopener" target="_blank" href="/racial-bias-in-bert-c1c77da6b25a">偏差的帖子。</a></p><h1 id="c44e" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">摘要</h1><p id="d054" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">这个故事介绍了一个简单的、基于 Keras 的 TensorFlow 2.0 对 BERT 嵌入模型的使用。像阿尔伯特这样的其他模型也可以在 TensorFlow Hub 上<a class="ae lb" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank">找到。</a></p><p id="1569" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://colab.research.google.com/drive/1hMLd5-r82FrnFnBub-B-fVW78Px4KPX1" rel="noopener ugc nofollow" target="_blank">这个故事的所有代码都可以在 Google Colab 上获得。</a></p><h1 id="a5cf" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">参考</h1><p id="77f5" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">[1] Devlin，j .，Chang，M. W .，Lee，k .，&amp; Toutanova，K. (2018 年)。<a class="ae lb" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Bert:用于语言理解的深度双向转换器的预训练。</a> <em class="nj"> arXiv 预印本 arXiv:1810.04805 </em>。</p><h1 id="b931" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">用伯特的故事学习 NMT</h1><ol class=""><li id="a443" class="mv mw iq kh b ki mo kl mp ko nk ks nl kw nm la nn nb nc nd bi translated"><a class="ae lb" href="https://medium.com/@neged.ng/bleu-bert-y-comparing-sentence-scores-307e0975994d" rel="noopener"> BLEU-BERT-y:比较句子得分</a></li><li id="33b8" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la nn nb nc nd bi translated"><a class="ae lb" href="https://medium.com/@neged.ng/visualisation-of-embedding-relations-word2vec-bert-64d695b7f36" rel="noopener">嵌入关系的可视化(word2vec，BERT) </a></li><li id="bd1c" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la nn nb nc nd bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/machine-translation-a-short-overview-91343ff39c9f">机器翻译:简要概述</a></li><li id="7580" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la nn nb nc nd bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/identifying-the-right-meaning-of-the-words-using-bert-817eef2ac1f0">使用 BERT 识别单词的正确含义</a></li><li id="3da0" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la nn nb nc nd bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/machine-translation-compare-to-sota-6f71cb2cd784">机器翻译:对比 SOTA </a></li><li id="90f4" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la nn nb nc nd bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/simple-bert-using-tensorflow-2-0-132cb19e9b22">使用 TensorFlow 2.0 的简单 BERT</a></li></ol></div></div>    
</body>
</html>