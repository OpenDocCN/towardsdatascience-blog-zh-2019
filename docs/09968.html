<html>
<head>
<title>Working with Hive using AWS S3 and Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 AWS S3 和 Python 处理 Hive</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/working-with-hive-using-aws-s3-and-python-4c7471533f98?source=collection_archive---------12-----------------------#2019-12-30">https://towardsdatascience.com/working-with-hive-using-aws-s3-and-python-4c7471533f98?source=collection_archive---------12-----------------------#2019-12-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ee08" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用外部数据存储维护配置单元模式和使用 Python 执行配置单元查询的初学者指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6fe6838b4d3936364e9009b8c36bea4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-vdX3pGVaZAgLZEHB5q18A.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@aleksow?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Aleksander Vlad</a> on <a class="ae ky" href="https://unsplash.com/s/photos/computer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0ff0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将分享我维护 Hive 模式的经验。这将对那些愿意涉足大数据技术的大一新生很有用。这将主要描述如何使用 python 连接到 Hive，以及如何使用 AWS S3 作为数据存储。如果你不熟悉 Hive 概念，请浏览维基百科上的文章。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="cb39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文的主要目的是提供一个通过 python 连接 Hive 和执行查询的指南。为此，我使用了“<em class="mc">py hive”</em>库。我将我的连接类创建为“<em class="mc">Hive connection”</em>，Hive 查询将被传递到函数中。AWS S3 将用作配置单元表的文件存储。</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="ea11" class="mi mj it me b gy mk ml l mm mn"><strong class="me iu">import </strong>pandas <strong class="me iu">as </strong>pd<strong class="me iu"><br/>from </strong>pyhive <strong class="me iu">import </strong>hive</span><span id="e72f" class="mi mj it me b gy mo ml l mm mn"><strong class="me iu">class </strong>HiveConnection:<br/>    @staticmethod<br/>    <strong class="me iu">def </strong>select_query(query_str: str, database:str =HIVE_SCHEMA) -&gt; pd.DataFrame:<br/>        <em class="mc">"""<br/>        Execute a select query which returns a result set<br/>        </em><strong class="me iu">:param</strong><em class="mc"> query_str: select query to be executed<br/>        </em><strong class="me iu">:param</strong><em class="mc"> database: Hive Schema<br/>        </em><strong class="me iu">:return</strong><em class="mc">:<br/>        """</em><br/>        conn = hive.Connection(host=HIVE_URL, port=HIVE_PORT, database=database, username=HIVE_USER)<br/><br/>        <strong class="me iu">try</strong>:<br/>            result = pd.read_sql(query_str, conn)<br/>            <strong class="me iu">return </strong>result<br/>        <strong class="me iu">finally</strong>:<br/>            conn.close()<br/><br/>    @staticmethod<br/>    <strong class="me iu">def </strong>execute_query(query_str: str, database: str=HIVE_SCHEMA):<br/>        <em class="mc">"""<br/>        execute an query which does not return a result set.<br/>        ex: INSERT, CREATE, DROP, ALTER TABLE<br/>        </em><strong class="me iu">:param</strong><em class="mc"> query_str: Hive query to be executed<br/>        </em><strong class="me iu">:param</strong><em class="mc"> database: Hive Schema<br/>        </em><strong class="me iu">:return</strong><em class="mc">:<br/>        """</em><br/>        conn = hive.Connection(host=HIVE_URL, port=HIVE_PORT, database=database, username=HIVE_USER)<br/>        cur = conn.cursor()<br/>        <em class="mc"># Make sure to set the staging default to HDFS to avoid some potential S3 related errors<br/>        </em>cur.execute(<strong class="me iu">"SET hive.exec.stagingdir=/tmp/hive/"</strong>)<br/>        cur.execute(<strong class="me iu">"SET hive.exec.scratchdir=/tmp/hive/"</strong>)<br/>        <strong class="me iu">try</strong>:<br/>            cur.execute(query_str)<br/>            <strong class="me iu">return "SUCCESS"<br/>        finally</strong>:<br/>            conn.close()</span></pre><p id="e22f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将查询保存为单独的字符串。这样，您可以在必要时用外部参数格式化查询。配置单元配置(配置单元 URL、配置单元端口、配置单元用户、配置单元模式)为常量。函数"<em class="mc"> select_query" </em>将用于检索数据，函数"<em class="mc"> execute_query </em>将用于其他查询。</p><p id="e314" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Hive 提供了一个 shell 交互工具来初始化数据库、表和操作表中的数据。我们可以通过输入命令“<em class="mc">Hive”</em>进入 Hive 命令行。您也可以在 shell 中执行本文中给出的所有查询。</p><h1 id="93a5" class="mp mj it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">创建新模式</h1><p id="e93f" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">模式是类似于数据库的表的集合。Hive 中允许使用关键字 SCHEMA 和 DATABASE。我们可以选择任何一个。这里我们用模式代替数据库。可以使用“创建模式”来创建模式。要进入模式内部，可以使用关键字“USE”。</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="79d9" class="mi mj it me b gy mk ml l mm mn"><strong class="me iu">CREATE SCHEMA </strong>userdb;<br/><strong class="me iu">USE</strong> userdb;</span></pre><h1 id="1bad" class="mp mj it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">创建表格</h1><p id="e854" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">有三种类型的配置单元表。它们是内部的、外部的和暂时的。内部表存储数据库中表的元数据以及表数据。但是外部表将元数据存储在数据库中，而表数据存储在远程位置，如 AWS S3 和 hdfs。删除内部表时，所有表数据都将随元数据一起被删除。删除外部表时，只会删除元数据。而不是表数据。这样，实际数据将得到保护。如果您将新表指向相同的位置，数据将通过新表可见。</p><p id="03b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Hive 是一个数据仓库，使用 MapReduce 框架。因此，数据检索的速度对于小型查询来说可能不够公平。为了提高性能，可以对配置单元表进行分区。分区技术可以应用于外部表和内部表。像 bucketing 这样的概念也是有的。您可以选择这些技术中的任何一种来提高性能。</p><p id="e027" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将数据从一个地方复制到另一个地方时，临时表非常有用。它充当数据库会话中保存数据的临时位置。会话超时后，所有临时表都将被清除。创建临时表对"<em class="mc"> Pyhive" </em>库没有用，因为单个会话中不支持多个查询。即使我们创建了一个表，也不能再使用同一个会话来访问表。但是这在 Hive 命令行中是可能的。您可以创建一个临时表，然后在单个会话中从该表中选择数据。</p><h2 id="002b" class="mi mj it bd mq nl nm dn mu nn no dp my li np nq na lm nr ns nc lq nt nu ne nv bi translated">内部表格</h2><p id="d5cd" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">以下查询将创建一个具有远程数据存储 AWS S3 的内部表。文件格式为 CSV，字段以逗号结尾。“<em class="mc"> s3_location </em>指向数据文件所在的 s3 目录。这是用户为查询字符串定义的外部参数。应该在查询格式化的时候传递。</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="f832" class="mi mj it me b gy mk ml l mm mn">CREATE TABLE `user_info` (<br/>`business_unit` INT,<br/>`employee_id` INT,<br/>`email` VARCHAR(250),<br/>`first_name` VARCHAR(250),<br/>`last_name` VARCHAR(250),<br/>`gender` VARCHAR(250),<br/>`birthday` DATE,<br/>`created_date` DATE,<br/>`updated_date` DATE<strong class="me iu"><br/></strong>)<br/>ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ESCAPED BY '\\'<br/>LOCATION '{s3_location}'<br/>TBLPROPERTIES (<br/>"s3select.format" = "csv",<br/>"skip.header.line.count" = "1"<br/>);</span></pre><p id="fbcd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果数据字符串包含逗号，将会破坏表格结构。所以我定义了一个转义符，在创建表之前，所有不必要的逗号都需要放在这个转义符的前面。</p><p id="be18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是一个示例记录。请注意，电子邮件包含一个逗号。</p><p id="bfa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1，1，<strong class="lb iu">安，史密斯@加米尔</strong>。com，Ann，Smith，女，' 1992–07–01 '，' 2019–09–01 '，' 2019–12–31<strong class="lb iu">'</strong></p><p id="e1e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的记录需要这样格式化<strong class="lb iu"> : <br/> </strong> 1，1，<strong class="lb iu">ann\\,smith@gamil.com</strong>，安，史密斯，女，“1992–07–01”，“2019–09–01”，“2019–12–31”</p><h2 id="13ab" class="mi mj it bd mq nl nm dn mu nn no dp my li np nq na lm nr ns nc lq nt nu ne nv bi translated">外部表格</h2><p id="b940" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">这里，我用“<em class="mc">业务单位”</em>和“<em class="mc">创建日期”</em>对“<em class="mc">用户信息”</em>表进行了分区</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="7f85" class="mi mj it me b gy mk ml l mm mn">CREATE EXTERNAL TABLE `user_info` (<br/> `employee_id` INT,<br/> `email` VARCHAR(250), <br/> `first_name` VARCHAR(250),<br/> `last_name` VARCHAR(250),<br/> `gender` VARCHAR(250),<br/> `birthday` DATE,<br/> `updated_date` DATE<br/>)  partitioned by(<br/>`business_unit` INT, <br/>`created_date` DATE,<br/>)<br/>ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ESCAPED BY '\\'<br/>STORED AS<br/>INPUTFORMAT<br/>'com.amazonaws.emr.s3select.hive.S3SelectableTextInputFormat'<br/>OUTPUTFORMAT<br/>'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'<br/>LOCATION '{s3_location}' <br/>TBLPROPERTIES (<br/>"s3select.format" = "csv",<br/>"s3select.headerInfo" = "ignore"<br/>);</span></pre><h2 id="246c" class="mi mj it bd mq nl nm dn mu nn no dp my li np nq na lm nr ns nc lq nt nu ne nv bi translated">工作单元表</h2><p id="0722" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">创建临时表的查询。</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="44bb" class="mi mj it me b gy mk ml l mm mn">CREATE TEMPORARY TABLE `user_info` (<br/> `business_unit` INT,<br/> `employee_id` VARCHAR(250),<br/> `email` VARCHAR(250),<br/> `first_name` VARCHAR(250),<br/> `last_name` VARCHAR(250),<br/> `gender` VARCHAR(250),<br/> `birthday` DATE,<br/> `created_date` DATE,<br/> `updated_date` DATE<br/>) ;</span></pre><h2 id="7ff1" class="mi mj it bd mq nl nm dn mu nn no dp my li np nq na lm nr ns nc lq nt nu ne nv bi translated">翻桌</h2><p id="6dc9" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">删除表的查询。如果删除外部表，远程文件存储中的数据不会被删除。</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="1c9f" class="mi mj it me b gy mk ml l mm mn">DROP TABLE IF EXISTS `user_info`;</span></pre><h2 id="73b9" class="mi mj it bd mq nl nm dn mu nn no dp my li np nq na lm nr ns nc lq nt nu ne nv bi translated">插入数据</h2><p id="a47f" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">一旦使用外部文件存储创建了表，远程位置的数据将通过没有分区的表可见。但是当涉及到带有分区的表时，情况就不一样了。这意味着数据不能直接复制到分区表中。我们需要创建一个没有分区的临时表，并通过提供分区值将数据插入到分区表中。以下查询描述了如何向这样的表中插入记录。</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="9191" class="mi mj it me b gy mk ml l mm mn">INSERT INTO TABLE user_static_info PARTITION (business_unit={business_unit}, `created_date`='{execution_date}')<br/>SELECT<br/>   Employee_id,<br/>   email,<br/>   secondary_email,<br/>   first_name,<br/>   last_name,<br/>   orig_gender,<br/>   gender,<br/>   signup_channel ,<br/>   signup_from_fb ,<br/>   birthday,<br/>   signup_date,<br/>   updated_date,<br/>   last_activity_date,<br/>   subscription_status<br/>FROM<br/>   tmp_user_static_info<br/>WHERE business_id={business_unit}</span></pre><p id="de05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为单个会话中的多个查询不支持“<em class="mc">py hive</em>”；我必须创建内部表“<em class="mc">tmp _ user _ static _ info”</em>，它指向没有分区的 S3 数据目录。然后，在将数据插入外部分区表后，删除了该表。</p><h2 id="4aab" class="mi mj it bd mq nl nm dn mu nn no dp my li np nq na lm nr ns nc lq nt nu ne nv bi translated">检索数据</h2><p id="8907" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">选择查询用于检索配置单元中的数据。这些非常类似于 SQL 选择查询。它具有以下形式。您可以根据自己的需求构建查询。</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="8cd1" class="mi mj it me b gy mk ml l mm mn">SELECT [ALL | DISTINCT] select_expr, select_expr, …<br/>FROM table_reference<br/>[WHERE where_condition]<br/>[GROUP BY col_list]<br/>[HAVING having_condition]<br/>[CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list]]<br/>[LIMIT number];</span></pre><h2 id="6e3c" class="mi mj it bd mq nl nm dn mu nn no dp my li np nq na lm nr ns nc lq nt nu ne nv bi translated">更新和删除数据</h2><p id="582f" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">配置单元不支持直接更新和删除数据。如果您想更改表格中的任何内容；使用 SELECT 查询将必要的数据复制到新表中。然后，通过删除旧表并重命名新表，可以用新表替换旧表。</p><h2 id="92d8" class="mi mj it bd mq nl nm dn mu nn no dp my li np nq na lm nr ns nc lq nt nu ne nv bi translated">更改表格</h2><p id="4e24" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">在 Hive 中可以更改表格。但这需要在不影响现有数据的情况下非常小心地完成。因为我们不能改变数据。例如，在中间添加一个新字段不会移动数据。如果我们添加一个新字段作为第二个字段，属于第三列的数据仍将出现在第二列，第四个字段的数据出现在第三个字段，依此类推。最后一个字段将不包含任何数据。这是因为更新配置单元表数据的限制。如果我们添加一个新字段作为最后一个字段，将会有一个空字段，我们可以将数据插入到该字段中。</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="82a8" class="mi mj it me b gy mk ml l mm mn">ALTER TABLE user_static_info ADD COLUMNS (last_sign_in DATE);</span></pre><p id="34e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想删除外部数据，我们可以使用以下步骤。</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="c8ba" class="mi mj it me b gy mk ml l mm mn">ALTER TABLE user_static_info SET TBLPROPERTIES('EXTERNAL'='False');<br/>DROP TABLE user_static_info;</span></pre><h1 id="6648" class="mp mj it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">例子</h1><p id="b165" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">最后，以下代码显示了如何使用“<em class="mc"> HiveConnection </em>”类中的“<em class="mc"> execute_query </em>”函数执行查询。</p><pre class="kj kk kl km gt md me mf mg aw mh bi"><span id="e746" class="mi mj it me b gy mk ml l mm mn">from src.database.hive_con import HiveConnection<br/><br/>create_temp_table_query_str = """CREATE TABLE `user_info` (<br/>`business_unit` INT,<br/>`employee_id` INT,<br/>`email` VARCHAR(250),<br/>`first_name` VARCHAR(250),<br/>`last_name` VARCHAR(250),<br/>`gender` VARCHAR(250),<br/>`birthday` DATE,<br/>`created_date` DATE,<br/>`updated_date` DATE<br/>)<br/>ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ESCAPED BY '\\'<br/>LOCATION '{s3_location}'<br/>TBLPROPERTIES (<br/>"s3select.format" = "csv",<br/>"skip.header.line.count" = "1"<br/>);""".format(<br/>    s3_location="s3://hive-test/data/user_info/"<br/>)<br/><br/>HiveConnection.execute_query(query_str=create_temp_table_query_str, database=userdb)</span></pre></div></div>    
</body>
</html>