<html>
<head>
<title>ROC Curve, a Complete Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ROC 曲线，完整介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/roc-curve-a-complete-introduction-2f2da2e0434c?source=collection_archive---------3-----------------------#2019-10-22">https://towardsdatascience.com/roc-curve-a-complete-introduction-2f2da2e0434c?source=collection_archive---------3-----------------------#2019-10-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="69db" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作者:雷扎·巴盖里</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e6f6501e696d3a637b9aa5100b1d7fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ry76ghyh4z4YxMnCR9R7pQ.jpeg"/></div></div></figure><p id="3c66" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ROC(接收器操作特性)曲线是一种有用的图形工具，用于评估二元分类器的性能，因为其区分阈值是变化的。为了理解 ROC 曲线，我们应该首先熟悉二元分类器和混淆矩阵。在<a class="ae la" href="https://en.wikipedia.org/wiki/Binary_classification" rel="noopener ugc nofollow" target="_blank">二进制分类</a>中，给出了一个对象集合，任务是根据对象的特征将其分为两组。例如，在医学测试中，我们希望分类器根据某些特征(如医学测试结果)来确定患者是否患有某种疾病。</p><p id="84a6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">评估分类器意味着测量预测类或标签与评估集中真实标签的匹配程度。在二元分类法中，我们通常把两类中较小的和较有趣的称为正类，把较大的/另一类称为负类。在医学诊断问题中，患有疾病的患者通常是阳性的，而其他人是阴性的。图 1 显示了一个简化的数据集，其中只有一个特征和两个标签。横轴表示特征 x，纵轴表示标签，可以是正的也可以是负的。在这个图中，蓝圈是正数，红圈是负数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lb"><img src="../Images/5058b020e258c3f79e294ab4ca8bff68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fq0uIRJSjo7T8Rl4VVaDSA.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 1</figcaption></figure><p id="1bb8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我们的训练数据集，每个点称为一个例子。现在，我们希望我们的分类器学习训练数据集，并仅基于特征值预测示例的标签。分类器预测可能不完美，并且在预测实际标签时可能出错。因此，二元分类器的预测有四种可能的结果:</p><p id="159c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">True Positive ( <em class="lg"> TP </em>):这里分类器预测或标记一个肯定的项目为肯定的，这是一个正确的预测。</p><p id="211b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">真否定(<em class="lg"> TN </em>):这里，分类器正确地确定否定类别的成员应该得到否定标签，这又是正确的预测。</p><p id="a366" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假阳性(<em class="lg"> FP </em>):分类器误将一个阴性项目预测为阳性，称为 I 型分类错误。</p><p id="a9c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假阴性(<em class="lg"> FN </em>):分类器错误地将一个阳性项目标记为阴性，称为 II 型分类错误。</p><p id="0c19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图 2 显示了这些结果的图形表示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lh"><img src="../Images/d0785a91013402c8c43fd00eff6e3b18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RY4tjWuIAMe1bRELdEfZdQ.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 2</figcaption></figure><p id="bce1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这四个结果定义了一个 2×2 a 的列联表或混淆矩阵，如图 3 所示。为了记住这些术语，您可以分别用正确和不正确替换 True 和 False，也可以分别用选择和拒绝替换肯定和否定。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi li"><img src="../Images/726c1e3104e268e676c22f4430196833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gr8txCIQQ37xnFYzR0YRag.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 3</figcaption></figure><p id="d638" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以，比如假阳性(<em class="lg"> FP </em>)就是选错了。现在，我们可以根据这些新概念为分类器定义一些评估统计数据或指标:</p><p id="b3c4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">准确率</em>:正确预测数与总预测数之比。所以:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lj"><img src="../Images/32309a001a800fa0b33a3a0749fa067d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*llIM4NSEGyTXKL6TKCTRWg@2x.png"/></div></div></figure><p id="bc5b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">灵敏度</em>也称为<em class="lg">召回</em>或真阳性率(<em class="lg"> TPR </em>):它测量被分类器正确预测(或召回)为阳性的初始阳性的比例。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lk"><img src="../Images/c264b3e61b293d8e4d48e7ddf7fe814f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-sw-vX-ANfU2nHCufegRQ@2x.png"/></div></div></figure><p id="d37a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">精度</em>:分类器的肯定预测中真正肯定的部分。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/fc258bf6d0899eea50d106349ad41a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*yGybrnyuExSjMUqJgYHgmQ@2x.png"/></div></figure><p id="2226" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">特异性</em>或真阴性率(<em class="lg"> TNR </em>):被分类器正确预测(作为阴性被拒绝)的阴性比例。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/b7d4bdd67c3f80f16558a6d24247d360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*okDIvONuWMW3UlcxShag0Q@2x.png"/></div></figure><p id="7b65" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假阳性率(<em class="lg"> FPR </em>):被错误选择为阳性的阴性比例。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ln"><img src="../Images/c22dad4ff681735b478ac47b492d6479.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ovYlojgsazTk1hlrwe47jQ@2x.png"/></div></div></figure><p id="fcb6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图 4 显示了这些指标的图形表示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lh"><img src="../Images/2d1124739bbb9a010daf6ca8103b04a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T4dIJOinDaLKwTV3ECufQQ.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 4</figcaption></figure><p id="d795" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">随机分类器</strong></p><p id="b19e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们考虑一些特殊情况，并尝试为它们计算这些性能指标。对于第一种情况，我们假设我们有一个预测任何数据点为阳性的差分类器。如图 5 所示，这个分类器从不拒绝任何数据点为负，所以<em class="lg"> TN = 0。</em>它也从不漏掉一个正数，所以<em class="lg"> FN=0 </em>。现在我们有:</p><p id="ddbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">回忆或 TPR = TP/(TP+FN) = TP/TP = 1 </em></p><p id="c813" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> FPR = FP/(FP+TN) = FP/FP =1 </em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lo"><img src="../Images/fe2625510d9f91d9713975302c881694.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oADVgCKslgfbW6tF_cxBnw.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 5</figcaption></figure><p id="17b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于第二种情况，我们假设我们有另一个糟糕的分类器，它拒绝所有的数据点，并且从不选择任何数据点作为阳性(图 6)。由于没有选择(无论是正确的还是不正确的)<em class="lg"> TP=FP=0，</em>所以:</p><p id="9665" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">回忆或 TPR = TP/(TP+FN) = 0/(0+FN) = 0 </em></p><p id="d331" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">FPR = FP/(FP+TN)= 0/(0+TN)= 0</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lb"><img src="../Images/37def204224409691da2bb5f0b406409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JdgAT8R0_6dpuo7HCER0-w.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 6</figcaption></figure><p id="527e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止，我们有两个总是选择或拒绝所有数据点的分类器。现在我们假设有第三个分类器位于两者之间。它被称为随机分类器。对于每个数据点，它以相等的概率随机地将它们标记为正或负。因此，这就像挑选一个数据点，然后抛硬币来决定应该给它分配哪个标签。现在想象我们总共有<em class="lg"> N </em>个数据点，并且<em class="lg"> N </em>是一个非常大的数字。我们还假设实际正的数量是<em class="lg"> Np </em>，实际负的数量是<em class="lg"> Nn </em>。现在，对于分类器选取的每个点，它实际上是正数的概率是<em class="lg"> Np/N </em>，它实际上是负数的概率是<em class="lg"> Nn/N </em>。对于阳性点，其被分类器标记为阳性或阴性的概率为 0.5。现在我们可以计算这个分类器得到每个结果的概率。</p><p id="c92e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于一个<em class="lg"> TP </em>为正的概率是<em class="lg"> Np/N </em>，被预测为正的概率是 0.5 所以总概率是<em class="lg"> (Np/N)×0.5 </em>。用类似的方法，我们可以计算出其他结果的概率。</p><p id="fac2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> FN= (Np/N)×0.5 </em></p><p id="22e9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> FP= (Nn/N)×0.5 </em></p><p id="d5e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> TN=(Nn/N)×0.5 </em></p><p id="6eda" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以我们看到得到<em class="lg"> TP </em>的概率等于<em class="lg"> FN </em>也就是说<em class="lg"> TP=FN </em>。同理，<em class="lg"> FP=TN </em>。这意味着:</p><p id="82c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">TPR = TP/(TP+FN)= TP/(TP+TP)= 0.5</em></p><p id="88b8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">FPR = FP/(FP+TN)= FP/(FP+FP)= 0.5</em></p><p id="947f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果分类器选择了一个概率较低的正呢？例如，想象一个分类器，它以 0.3 的概率预测一个肯定的结果，并以 0.7 的概率将其拒绝为否定的结果。然后:</p><p id="c31a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> TP= (Np/N)×0.3 </em></p><p id="11d2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> FN= (Np/N)×0.7 </em></p><p id="7bce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> FP= (Nn/N)×0.3 </em></p><p id="3f05" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> TN= (Nn/N)×0.7 </em></p><p id="579e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">TPR = TP/(TP+FN)= 0.3/(0.3+0.7)= 0.3</em></p><p id="b567" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">FPR = FP/(FP+TN)= 0.3/(0.3+0.7)= 0.3</em></p><p id="7a67" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以，通过改变选择概率，<em class="lg"> TPR </em>和<em class="lg"> FPR </em>都会改变，但始终保持等于选择概率。事实上，我们之前研究的前两个分类器也可以描述为随机分类器的一个特例。对于预测一切为正的分类器，选择概率为 1，所以<em class="lg"> TPR=FPR=1 </em>，对于拒绝一切的分类器，选择概率为零，所以<em class="lg"> TPR=FPR=0 </em>。图 7 显示了<em class="lg"> TPR </em>与<em class="lg"> FPR </em>的关系图，以及这些分类器各自的分数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lp"><img src="../Images/f14643d789ecd038cf42301ca9bc37e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6nkJyoHs6mOhi0xxVv-DQ.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 7</figcaption></figure><p id="effd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在该图中，点 B 代表一般随机分类器，其以概率<em class="lg"> p </em>预测正点。A 点是预测一切为负的分类器，可以认为是随机分类器<em class="lg"> p=0 </em>。C 点是预测一切为正的分类器，是随机分类器<em class="lg"> p=1 </em>。<em class="lg"> TPR </em>和<em class="lg"> FPR </em>的范围都是从 0 到 1，并且所有这些点都位于对角线上。通过改变选择概率，可以改变随机分类器沿对角线的位置。</p><p id="6efa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在可以用一种更数学的方式来表达选择概率。对于数据集中的每个点<em class="lg"> i </em>，我们将使用<em class="lg"> x(i) </em>来表示输入特征，使用<em class="lg"> y(i) </em>来表示我们试图预测的实际标签或目标变量。在二进制分类的情况下，<em class="lg"> y(i) </em>取二进制值。这里我们假设对于正的点<em class="lg"> y(i)=1 </em>，对于负的点<em class="lg"> y(i)=0 </em>。因此，一对<em class="lg"> (x(i)，y(i)) </em>定义了训练数据集中的每个点或示例。我们的任务是学习一个函数<em class="lg"> h: X → Y </em>使得<em class="lg"> h(x) </em>是<em class="lg"> y </em>对应值的良好预测器，函数<em class="lg"> h </em>称为假设。<em class="lg"> h(x) </em>应该给出每个点的正标签的概率<em class="lg"> x(i) </em>，其数学上可以写成:</p><p id="bbef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> P(y=1|x) = h(x) </em></p><p id="3991" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="lg"> P </em>是给定<em class="lg"> x </em>对于每个点<em class="lg"> i </em>的条件概率<em class="lg"> y=1 </em>。因为我们只有两个可能的类别，所以负标签的概率简单地是:</p><p id="4c59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> P(y=0|x) = 1-h(x) </em></p><p id="3332" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">分类器应该使用<em class="lg"> x </em>值为每个点计算<em class="lg"> h(x) </em>以预测该点的标签。我们如何定义随机分类器的<em class="lg"> h(x) </em>？这里我们假设<em class="lg"> h(x) </em>在 0 和 1 之间具有均匀分布。这种分布具有恒定的概率，并且这与我们的随机分类器一致，该随机分类器总是以恒定的概率选择阳性。所以:</p><p id="d019" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> h(x) ~均匀(0，1) </em></p><p id="fb8d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是<em class="lg"> h(x) </em>仅仅给出了点<em class="lg"> x(i) </em>具有正标签的概率。我们怎么能说它是积极的还是消极的呢？我们定义了一个选择阳性的阈值。如果<em class="lg"> h(x) </em>大于或等于阈值，我们将<em class="lg"> x </em>标记为正，如果小于阈值，我们将它标记为负:</p><p id="2266" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> h(x) ~均匀(0，1) </em></p><p id="8405" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">如果 h(x)≥阈值则 y(I)= 1</em></p><p id="a032" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> y(i)=0 如果 h(x) &lt;阈值</em></p><p id="c4c4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我可以说明，如果你想让随机分类器的选择概率为<em class="lg"> p </em>，阈值应该等于<em class="lg"> 1-p </em>。</p><p id="bee4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们知道:</p><p id="7ec4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">如果 h(x)≥阈值，y(I)= 1</em></p><p id="ecfb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">设<em class="lg"> X </em>是一个随机变量，可以取<em class="lg"> x </em>的值。如果我们假设<em class="lg"> h(X) </em>在 0 和 1 之间具有均匀分布。那么<em class="lg"> h(X) </em>的<a class="ae la" href="https://en.wikipedia.org/wiki/Probability_density_function" rel="noopener ugc nofollow" target="_blank">概率分布函数</a> ( <em class="lg"> pdf </em>)简单来说就是 1。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/157b06885d3c14838c6036385efa4963.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*zuJJuvtqUx8JRv-zlJoSWA@2x.png"/></div></figure><p id="fbf4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以计算它大于阈值的累积概率:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lr"><img src="../Images/8ef988b587e34dcb927c00ad1cefe75a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQCUFSIyRB0GISP3N8tkrw@2x.png"/></div></div></figure><p id="2575" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以<em class="lg">h(X)≥阈值</em>的概率是<em class="lg"> p </em>也就是说<em class="lg"> y(i)=1 </em>的概率也是<em class="lg"> p </em>，分类器以概率<em class="lg"> p </em>预测一个正。</p><p id="a7ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当选择概率为零时，阈值将为 1 ( <em class="lg"> TPR=FPR=0 </em>)，当选择概率为 1 时，阈值将为零(<em class="lg"> TPR=FPR=1 </em>)。因此，在图中，通过将阈值从 1 变为 0，我们沿着图 8 中从点<em class="lg"> A </em>到点<em class="lg"> C </em>的对角线移动。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lp"><img src="../Images/2c37c71b9c026d3d6cf4d60cafcf3707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*18rVtnC9Yz_uB-A59rdN1g.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 8</figcaption></figure><p id="88ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们将学习如何用 Python 实现这个随机分类器。首先，我们需要定义一个非常简单的数据集(清单 1)。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="8f4d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您运行这个代码，您将得到这个数据集的图，如图 9 所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/1e2eb3255917746939f57e166e6ee6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8UhOIvHX3vCr9CnNUpRTUA.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 9</figcaption></figure><p id="08a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里我们有 50 个只有一个特征的例子(<em class="lg"> x </em>)。一半点标为负(<em class="lg"> y=0 </em>)，另一半点标为正(<em class="lg"> y=1 </em>)。正负点之间没有重叠(在 x 值上)。现在我们需要定义假设函数<em class="lg"> h(x) </em>(清单 2)。函数<code class="fe lv lw lx ly b">predict_proba()</code>负责为每个<em class="lg"> x </em>返回<em class="lg"> h(x) </em>。它接受一个数据实例数组(<em class="lg"> x </em>)，并使用<code class="fe lv lw lx ly b">Numpy</code>的均匀分布返回一个概率数组。因此，现在每个点都被赋予一个概率，该概率来自 0 和 1 之间的均匀分布。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="ca12" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们定义准备 ROC 曲线中的点的函数(清单 3)。该函数获取由<code class="fe lv lw lx ly b">predict_proba()</code>生成的所有点的概率数组以及实际标签数组(<em class="lg"> y </em>)。然后，它定义介于 0 和 1.1 之间的阈值。你可能会问为什么门槛的上限是 1.1？而不是 1？稍后我会解释。对于每个阈值<em class="lg"> t </em>，如果该点的<em class="lg"> h(x) </em>(具有正标签的概率)大于或等于<em class="lg"> t </em>，则该函数预测为 1，否则为零。然后基于这些预测值和 y 中的实际值，建立混淆矩阵，计算出<em class="lg"> TPR </em>和<em class="lg"> FPR </em>值。最后，它为每个阈值返回具有相应值<em class="lg"> TPR </em>和<em class="lg"> FPR </em>的阈值数组。<em class="lg"> TPR </em>和<em class="lg"> FPR </em>阵列将用于绘制 ROC 曲线。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="b695" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图 10 显示了使用 Python 代码绘制的 ROC 曲线。你可能会注意到 ROC 曲线没有那么接近对角线，这也意味着每个阈值的<em class="lg"> TPR </em>和<em class="lg"> FPR </em>值不够接近。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/b7000ccc0da2f85208c539f38da818f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UShrTsqeEhCoRvzL4HTBWA.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 10</figcaption></figure><p id="0b51" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是因为我们没有那么多的数据点。想象你有一枚公平的硬币，你把它抛 10 次。正面和反面的总数不一定相等，但是，如果你增加投掷次数，正面和反面的总数会更接近。因此，如果我们显著增加例子的数量，我们将得到一个更真实的 ROC 曲线。清单 4 通过将示例数量增加到 10000(同样，一半的点标记为 1，另一半标记为 0，没有重叠)做到了这一点。结果如图 11 所示。现在 ROC 曲线非常接近对角线，并且清楚地表示随机分类器。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/3790fd00350128aad81fa0d0c678ab0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z4De2EIc8pYy5O5yqEHVLA.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 11</figcaption></figure><p id="800d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些结果表明，<em class="lg"> TPR </em> vs <em class="lg"> FPR </em>的图可以用于评估二元分类器的性能，然而，我们迄今为止所研究的分类器并不真正有用，因为它们都是盲目工作并随机选择正数据点。我们如何在这个图中包含一个真正的分类器？</p><p id="ffdd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">逻辑回归分类器</strong></p><p id="e98f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae la" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>是一种统计模型，可用于二元分类。这里我们使用逻辑回归来研究二元分类器的行为。我假设您已经对此很熟悉，并且只给出了简要的描述。该模型假设假设具有以下形式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/defb1423301d788fadccb1bbb97c62b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*l0unTcLvlqZIJgmxpBDRvg@2x.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/eaf4ac7e0a24b08a5388320109110f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*NfD4u0EcM2wJ4LtdLRK-Gg@2x.png"/></div></figure><p id="e863" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="lg"> xi </em>为特征，<em class="lg"> g </em>称为逻辑函数或 sigmoid 函数。图 12 显示了 sigmoid 函数的曲线图。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/11f1e8e958727351eadfb9e69854ab9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bE0WNBVa1a8FZpcgEBGuFg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 12</figcaption></figure><p id="3849" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> ai </em>的最佳值将在学习过程中确定。因为我们的数据集中只有一个特征:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/8852a03955999e4f2da93fd2daadb6c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*x9Jly6k8cRInL5WbDudSNQ@2x.png"/></div></figure><p id="3e25" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在和以前一样，我们有:</p><p id="2914" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> P(y=1|x) = h(x) </em></p><p id="efb8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> P(y=0|x) = 1-h(x) </em></p><p id="3f8b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以，成为正点的概率，现在用一个 sigmoid 函数来描述。我们现在准备将逻辑回归应用于 Scikit-learn 中的一个简单数据集。我们首先定义我们的数据集，它有 20 个例子:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="1ed7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后我们使用 Scikit-learn 中的逻辑回归模型来拟合数据。该模型将 sigmoid 函数拟合到数据集，以预测每个特征输入的阳性标签的概率。Scikit-learn 的<code class="fe lv lw lx ly b">LogisticRegression</code>模型有一个叫做<code class="fe lv lw lx ly b">predict_proba()</code>的方法可以返回这个概率。清单 6 中使用了这种方法来预测一些测试数据点的概率。需要注意的是，<code class="fe lv lw lx ly b">predict_proba()</code>返回的是负标签和正标签的概率，正标签的概率存储在它的第二列中(第一列存储负标签的概率，所以如果我们将第二列的值称为<em class="lg"> p </em>，第一列的值就是<em class="lg"> 1-p </em>)。因此，我们应该首先通过分割返回的数组来分离概率，并将其存储在<code class="fe lv lw lx ly b">skl_prob</code>变量中。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="16b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图 13 显示了原始训练数据集加上<code class="fe lv lw lx ly b">predict_proba()</code>预测的概率图(红色曲线)。很明显，它是 s 形的。换一种方式来剧情也是有启发的。我们可以先定义一个 sigmoid 函数。然后让<code class="fe lv lw lx ly b">LogisticRegression</code>返回它学习到的 sigmoid 函数的系数(<em class="lg"> a0 </em>和<em class="lg"> a1 </em> in <em class="lg"> h(x) </em>)。这些系数存储在<code class="fe lv lw lx ly b">clf.coef_</code> ( <em class="lg"> =a1 </em>)和<code class="fe lv lw lx ly b">clf.intercept_</code> ( <em class="lg"> =a0 </em>)中。我定义了一个名为<code class="fe lv lw lx ly b">logistic_predict_proba()</code>的函数，使用这些系数来计算<em class="lg"> h(x) </em>。如果我们用这些系数绘制<em class="lg"> h(x) </em>，我们会得到与 Scikit-learn 的<code class="fe lv lw lx ly b">predict_proba()</code>方法完全相同的结果(图 13 中的黄色曲线)。在该图中，我用<em class="lg"> p </em>标记了<em class="lg"> y </em>轴，以强调它是一个概率，但是它也可以标记为<em class="lg"> h(x ),因为 h(x) </em>是一个概率，并且我在这里绘制了它的值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/fb34ed15a3e236373ea8591bc68058be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g_-IXxMznfKFGwt9RT5dKQ.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 13</figcaption></figure><p id="0ed6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是这些概率如何变成每个数据点的 0 或 1 标签呢？再次使用阈值。对于该模型，预测概率与阈值 0.5 进行比较。如果概率大于 0.5，它将被指定为 1，否则为 0。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/a352239d564ba1b978b2362b16a7bf68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*zd_Xk_ywmC9ypTc1tSyguw@2x.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi md"><img src="../Images/183b898bbb7d3f39c8db151e37f9ae40.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*SvxU7BxgU_26Z_C9DKnQuA@2x.png"/></div></figure><p id="d6c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在清单 7 中，我定义了一个名为<code class="fe lv lw lx ly b">logistic_predict()</code>的函数，使用我们之前生成的概率来预测标签。它将概率作为一个参数，并将其与作为阈值的 0.5 进行比较，以预测标签。我们实际上不需要定义这样一个函数。Scikit-learn 的<code class="fe lv lw lx ly b">LogisticRegression</code>模型有一个叫做<code class="fe lv lw lx ly b">predict()</code>的方法来做同样的事情。它获取输入要素并使用之前由模型计算的概率(与我们使用<code class="fe lv lw lx ly b">predict_proba()</code>检索的概率相同)，并以类似的方式预测标注。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="7180" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">清单 7 的输出显示两个函数给出了相同的结果。现在我们有了所有必要的工具来绘制这个分类器的 ROC 曲线。我们使用与随机分类器相同的过程。我们产生一些介于 0 和 1.1 之间的阈值，对于每个阈值，使用来自<code class="fe lv lw lx ly b">LogisitcRegression</code>模型的概率来预测标签。然后，我们使用这些预测标签和每个数据点的实际标签来计算<em class="lg"> TPR </em>和<em class="lg"> FPR </em>值。为此，我们可以很容易地使用之前定义的<code class="fe lv lw lx ly b">roc_curve()</code>函数。最后，我们可以使用生成的<em class="lg"> TPR </em>和<em class="lg"> FPR </em>值绘制 ROC 曲线(图 14)。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/f500b479685f38f3a4ed9f6d113a89ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PK_4ZunNFbmWmiX21y2jrw.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 14</figcaption></figure><p id="7df1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，我们真的不需要定义自己的函数。Scikit-learn 有一个名为<code class="fe lv lw lx ly b">metrics.roc_curve()</code>的函数，它在清单 9 中做了同样的事情。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="45bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它非常类似于我的函数，具有相同的参数和返回值，结果图将类似于图 14。检查其返回的数组是有益的。在清单 10 中，我取了一个阈值(实际上是 threshold 数组的第二个值)，并使用该值预测标签。然后，我计算混淆矩阵，<em class="lg"> TPR </em>和<em class="lg"> FPR </em>的值。这些值等于<code class="fe lv lw lx ly b">metrics.roc_curve()</code>在<em class="lg"> TPR </em>和<em class="lg"> FPR </em>数组中返回的对应阈值的值(<em class="lg"> TPR </em>和<em class="lg"> FPR </em>数组的第二个值)。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="a411" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您检查阈值的值，您会注意到它从一个大于 1 的数字开始。这里我解释一下原因。如您所知，阈值 1 意味着我们想要一个不预测任何阳性标签的分类器。我们知道:</p><p id="94ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">如果 h(x)≥阈值，y(I)= 1</em></p><p id="9e96" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于随机分类器，我们可以取<em class="lg"> pdf </em>的积分来计算<em class="lg"> P(h(x)≥threshold) </em>。问题是我们通常不知道<em class="lg"> pdf </em>函数取这个积分。相反，我们计算由该阈值产生的阳性和阴性标签的数量，并直接计算概率。这个概率总是小于或等于 1，但是阈值仍然需要从一个大于 1 的数开始。这是因为当概率大于或等于阈值时，<code class="fe lv lw lx ly b">roc_curve()</code>函数预测一个正标签。现在有可能某些<em class="lg"> h(x) </em>值等于 1。如果阈值等于 1，该函数仍然预测它们的正标签。所以分类器仍然预测一些阳性标签，这不是我们想要的分类器。我们知道<em class="lg"> h(x) </em>是一个概率，永远不可能大于 1。现在，通过将阈值设置为 1.1，我们确保阈值总是大于所有概率(<em class="lg"> h(x) </em>值)，并且不会预测到阳性标签。结果是一个不预测任何肯定结果的分类器。</p><p id="a24c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，现在应该清楚 Scikit-learn 中的<code class="fe lv lw lx ly b">roc_curve()</code>函数是如何工作的了。现在让我把重点放在 ROC 情节本身。在图 15 中，突出显示了 ROC 曲线中的一些点。这个数字有点夸张，因为当通过数据点时，s 形曲线的斜率应该慢得多(如图 13 所示)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi me"><img src="../Images/9e65839ad39c1eec4e80e85a0ba02ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zEpYhfNdlRConReRPfB-rA.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 15</figcaption></figure><p id="6eb4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">点<em class="lg"> A </em>对应于大于 1 的阈值，对于该阈值分类器不选择任何东西。以前研究过这种分类器。由于没有选择<em class="lg"> TP=FP =0 </em>，并且<em class="lg"> TPR=FPR=0 </em>如前所示。对于 B 点，只有一些正点选择正确，所以<em class="lg"> 0 &lt; TPR &lt; 1 </em>，但是<em class="lg"> FPR </em>仍然为零。对于点 D，所有的正点都被正确选择，但是一些负点也被选择为正点，所以<em class="lg"> TPR=1 </em>和<em class="lg"> 0 &lt; FPR &lt; 1 </em>。对于点 E，我们有一个阈值 0，分类器为其选择所有内容。所以，<em class="lg"> TN =FN = 0 </em>。我们之前也展示过，对于这种情况，<em class="lg"> TPR=FPR=1 </em>。</p><p id="25d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">重要的是要注意，无论您对分类器使用什么模型，ROC 曲线的这两点总是相同的。大于 1 的阈值总是给出<em class="lg"> TPR=FPR=0 </em>，而 0 的阈值总是给出<em class="lg"> TPR=FPR=1 </em>。结果，所有的 ROC 曲线都经过这两点。我们还有一个点 C，它对应于阈值 0.5。如图 15 所示，对于这个特定的数据集，这个阈值正确地预测了一切。因此，不存在不正确的选择或拒绝以及<em class="lg"> FP=FT=0 </em>。因此:</p><p id="fa12" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> TPR = TP/(TP+FN) = TP/TP = 1 </em></p><p id="5d69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> FPR = FP/(FP+TN) = 0 </em></p><p id="1a3a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，我们有一个理想的分类器，可以在阈值为 0.5 的情况下正确预测训练数据集的所有标签。理想的分类器总是经过这个点(<em class="lg"> TPR=1，FPR=0 </em>)，这个 ROC 曲线就是这种分类器的特征曲线。</p><p id="e62b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如前所述，逻辑回归模型总是使用阈值 0.5 来预测标签。那么用其他阈值来绘制 ROC 曲线有什么意义呢？答案是，我们使用一系列所有可能的阈值以某种方式扫描我们的数据集，并查看数据点在特征空间中的结构。如果我们只有理想的分类器，ROC 曲线就没有多大用处，然而，在许多情况下，分类器不能正确预测所有事情，这就是 ROC 曲线对我们有用的地方。让我在清单 11 中定义一个新的数据集。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="aee3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图 16 显示了该数据集的曲线图。它有 50 个数据点。一半的点标记为 1，另一半标记为 0。但是，与之前的数据集相比，有很大的不同。这里正负点部分重叠。这意味着对于一些负点来说，<em class="lg"> x </em>的值高于正点的最小 x 值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/a787278c034f67dbc1e2dcf35e236f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wJ89n67rdaUVGkm_ezQSBA.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 16</figcaption></figure><p id="5e57" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们使用前面的步骤来拟合逻辑回归模型并绘制 ROC 曲线。这次只使用了 Scikit-learn 功能。首先，我们计算概率(<em class="lg"> h(x) </em>)。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="b691" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图 17 显示了带有数据集点的结果概率曲线。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/80d4d502f973c0a935ac512919db122a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHcP-pYFnjFurzUkm57L7w.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 17</figcaption></figure><p id="6aac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们使用清单 13 中的这些概率绘制 ROC 曲线。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="c21b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">得到的 ROC 曲线绘制在图 18 中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/bbb12c500419b3f883baadb327a83cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PmMhlTgYuArzkgyolp0y4w.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 18</figcaption></figure><p id="1302" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个 ROC 曲线现在和之前的相比有点不同。图 19 突出显示了这条曲线上的一些点，以便更好地理解它。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi me"><img src="../Images/e0982c85463b941b1703a594d6ca592b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yLvDaZTjNDkn3O9nsByADw.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 19</figcaption></figure><p id="014e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如前所述，阈值 0 和大于 1 的阈值(点 A 和 E)保持在相同的位置。例如，对于阈值为 0.9 的点 B，我们仍然在重叠区域之外。这里<em class="lg"> TP </em>和<em class="lg"> FN </em>都大于零，但是<em class="lg"> FP </em>还是零。所以<em class="lg"> FPR=0 </em>和<em class="lg"> 0 &lt; TPR &lt; 1 </em>和 B 点位于垂直线上。对于 C 点，我们在重叠区域内，所以<em class="lg"> FP </em>不再为零，该点偏离垂直线。此外，如图所示，C 点位于对角线上方，这意味着<em class="lg"> TPR &gt; FPR </em>。我们可以简化这个不等式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mf"><img src="../Images/3044e680b0fb0d74ec474163ce31241f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SUyX4-MK9luzhrhqcP2YQQ@2x.png"/></div></div></figure><p id="973b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从图中可以清楚地看出，这个不等式对于这个数据集是成立的。这是因为当我们开始降低阈值时，我们首先会遇到正的点。换句话说，实际的正点平均起来具有更高的<em class="lg"> h(x) </em>或者被预测为正点的概率更高。</p><p id="8b19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于 D 点，我们又在重叠区域之外，但是这次<em class="lg"> FN=0 </em>和<em class="lg"> TN </em>和<em class="lg"> FP </em>都大于零，所以<em class="lg"> TPR=1 </em>和<em class="lg"> 0 &lt; FPR &lt; 1 </em>和 D 点位于水平线。事实是重叠点的标签不能通过仅使用一个特征来预测。您需要一个以上的特征来分离这些点。</p><p id="c71d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们增加数据集中重叠点的比例会发生什么？清单 14 定义了这样一个数据集，然后绘制了 ROC 曲线。数据集和 ROC 曲线分别显示在图 20 和 21 中。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/a35111340241bc2ba2806b71e886cce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gmJJ6FQ8-4asDTzK_K-IIw.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 20</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/92901d4e080f8a3d499b8dba80d9e332.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RF0kZCndlCw_5IjGQwzKPg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 21</figcaption></figure><p id="8201" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如你所看到的，现在我们有了一个更大的重叠区域，所以从垂直线的偏离开始得更快了。有一种方法来表征 ROC 曲线与理想分类器(只有一条垂直线和一条水平线)的偏离。为此，我们计算 ROC 曲线下的面积，如图 22 所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mg"><img src="../Images/70786847843cef37f3106d351d63a42d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t1NXadJZSrPWn74vFhQIow.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 22</figcaption></figure><p id="4fc8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们称这个量为 AUC(曲线下面积)。对于理想的分类器，AUC 是长度为 1 的矩形的面积，因此它正好是 1。对于随机分类器，它大约是下三角形的面积 0.5。对于其他分类器，AUC 介于 0.5 和 1 之间。AUC 越高，分类器越好，因为它更接近理想的分类器。要在 Scikit-learn 中计算 AUC，您可以使用<code class="fe lv lw lx ly b">metrics.auc()</code>函数，该函数接收由<code class="fe lv lw lx ly b">metrics.roc_rurve()</code>函数生成的<em class="lg"> TPR </em>和<em class="lg"> FPR </em>数组，并返回 AUC 值。清单 13 和清单 14 中都使用了这个函数。</p><p id="7bb2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是我们可以将重叠区域的大小增加多少呢？最坏的情况是正负点完全重叠。清单 15 定义了这样一个数据集，并符合逻辑回归模型。结果如图 23 所示。如你所见，假设函数看起来像一条直线，而不是一个 sigmoid 函数。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/2dfbc132645850a14c8fa6c13c374f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PXoSEnznoh7Rsr0QMd-fvg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 23</figcaption></figure><p id="4020" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们绘制清单 16 中的 ROC 曲线。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="1853" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ROC 曲线如图 24 所示。如您所见，它类似于随机分类器的 ROC 曲线，AUC 也接近 0.5。事实上，如果增加数据集中的点数，ROC 曲线会看起来更平滑，AUC 会更接近 0.5。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/2886c919871a014fe84bbfb8379ef1e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bHrMpBTzgovmcCpvHTtS9g.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 24</figcaption></figure><p id="b179" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是为什么它看起来像一个随机分类器呢？图 25 解释了原因。正如您看到的</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mh"><img src="../Images/f0116d3d4fe14ccc8196773325f15131.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WP4DUGcEtUm_IQooIX3TUQ@2x.png"/></div></div></figure><p id="4c8a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实上，当数据点完全重叠时，分类器无法仅使用一个特征来区分它们，并且它将正标签分配给几乎一半的数据点，而将负标签分配给其他数据点。因此，这个分类器并不比随机分类器好。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mi"><img src="../Images/60f5e4717de238dcb3fd123e88c12e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F1osADoJhHUBrd_0DXAdlA.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 25</figcaption></figure><p id="2f16" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，表现得像一个随机分类器似乎是最糟糕的情况。但是有可能在对角线下面有一条 ROC 曲线吗？我指的是每一点都有 AUC &lt; 0.5. To have such a case, we need a classifier which is even worse than a random classifier! Such a classifier should favor a wrong label for each data point which means something is systematically wrong with it. Let’s build a classifier like this.</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="e0c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">As you see, this classifier is similar to a logistic regression classifier. In the logistic regression model, the function  【T0】  returns the probability of being a positive (<em class="lg"> p </em>的曲线。这里我定义了一个名为<code class="fe lv lw lx ly b">logistic_mispredict_proba()</code>的新函数，它返回<em class="lg"> 1-p </em>。事实上，对于明显错误的正点，它返回负点的概率(由逻辑回归分类器计算)。这样，它会支持每个数据点的错误标签。现在，我们使用清单 18 中的这些错误概率来绘制图 16 中相同重叠数据集的 ROC 曲线。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="57bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图 26 显示了由<code class="fe lv lw lx ly b">logistic_mispredict_proba()</code>返回的数据点和概率函数(<em class="lg"> h(x) </em>)。正如你看到的，它类似于图 17，然而，概率函数现在是沿着 x 轴镜像的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/444d7da74c719653883b6e8b0cbd4821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5lQA-2NMe6PbbzHYPAMo2g.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 26</figcaption></figure><p id="366e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图 27 显示了 ROC 曲线，其现在位于对角线下方，AUC 为 0.02。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lu"><img src="../Images/b9ce55e38598045501789b53fc88ef09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*STU1eapg9Z_SbpPobGVO9w.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 27</figcaption></figure><p id="bbaf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，图 28 将 ROC 曲线上的点连接到概率函数上的点。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mj"><img src="../Images/2bfe87d76c8999226c9857d596b4fcfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GGQ-8ZiUOz-6MsAVkzyBBA.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 28</figcaption></figure><p id="82f2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于 B 这样的点，现在<em class="lg"> TPR </em>小于<em class="lg"> FPR </em>。当你得到这样的 ROC 曲线时，说明学习算法没有正常工作，是在错误标注数据点。</p><p id="2053" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">数学解释</strong></p><p id="5698" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是本文的最后一部分，我将讨论一些更高级的话题。如果你对 ROC 曲线的数学理论不感兴趣，可以跳过这一节。</p><p id="fd07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了描述 ROC 曲线的数学解释，我们需要定义一种新的符号。正如你所记得的，最初我们有一些积极和消极的数据点。我们用<em class="lg"> D+ </em>来表示一个数据点的实际标签为正的事件(我们简称它们为正)，用 D-来表示一个数据点的实际标签为负的事件(我们简称它们为负)。我们还使用<em class="lg"> T+ </em>来表示数据点的预测标签(通过分类器)为正的事件，以及<em class="lg"> T- </em>来表示数据点的预测标签为负的事件。</p><p id="49c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以定义一个类似于<em class="lg"> P(T+|D+) </em>的条件概率。它是在给定数据点的实际标签为正的情况下，预测该数据点为正的概率。其实就是指得到一个<em class="lg"> TP 的概率。</em>现在假设具有实际正标签的点的总数是<em class="lg"> Np </em>。分类后，它们中的一些将被正确地预测为阳性，而另一些将被错误地预测为阴性。前者是<em class="lg"> TP </em>，后者是<em class="lg"> FN </em>。所以，<em class="lg"> Np </em> = TP+FN。同样，如果真否定的总数是<em class="lg"> Nn </em>那么<em class="lg"> Nn </em> = TN+FP。根据定义<em class="lg"> P(A|B) = P(A，B)/P(B) </em>，所以</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mk"><img src="../Images/f281f2d0ed4cbf005f10c2bf6a84c7a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ee3FxmKSSGcBlo-exmsXOA@2x.png"/></div></div></figure><p id="68e9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你还记得，这就是<em class="lg">灵敏度</em>或者<em class="lg"> TPR </em>的定义。所以，<em class="lg">灵敏度</em>实际上是从正数据点中得到一个<em class="lg"> TP </em>的概率。</p><p id="cd13" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同理，<em class="lg"> P(T-|D-) = TN/(TN + FP) </em>等于<em class="lg">特异性</em>。<em class="lg"> P(T-|D-) </em>表示假设一个数据点的实际标签为负，则该数据点被预测为负的概率。所以，<em class="lg">特异性</em>是从负点中得到一个 TN 的概率。最后，<em class="lg"> FPR </em>是从负点中得到一个<em class="lg"> FP </em>的概率。<em class="lg">FPR = P(T+| D-)= FP/(FP+TN)</em>。如你所见，我们可以描述所有我们之前定义为条件概率的量。</p><p id="68e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如前所述，分类器通常不是理想的，并且有可能错误标记测试点。想象一下，一个分类器预测一个测试数据点的阳性标签，这个点真的是阳性的可能性有多大？数学上，我们感兴趣的是计算<em class="lg"> P(D+|T+) </em>。这是一个重要的量。例如，一个分类器应该根据一些特征，比如他的医学测试结果，来预测一个病人是否患有某种疾病。如果分类器预测患者患有该疾病，那么这个患者真的患有该疾病的可能性有多大？要回答这个问题，我们需要贝叶斯定理。贝叶斯定理描述了一个事件发生的概率，基于可能与该事件相关的条件的先验知识。让事件<em class="lg"> A1，。。。，Ak </em>形成空间<em class="lg"> S </em>的分隔，使得</p><p id="fa25" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> Pr(Aj) &gt; 0 </em>对于<em class="lg"> j = 1，.。。，k </em>，设<em class="lg"> B </em>为事件，使得<em class="lg"> Pr(B) &gt; 0 </em>。然后，对于<em class="lg"> i = 1，.。。，k </em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/bae76eeb00ec5dd599c544100ecb0771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*K_BY9g0HrtqK4XXXpwuAxQ@2x.png"/></div></figure><p id="b30d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="lg"> P(Ai|B) </em>称为事件<em class="lg"> Ai </em>的条件概率，假设事件<em class="lg"> B </em>已经发生。在这个等式中<em class="lg"> P(Ai) </em>通常被称为先验概率，因为<em class="lg"> P(Ai) </em>是在我们知道关于事件<em class="lg"> B </em>的任何事情之前该事件的概率，而<em class="lg"> P(Ai|B) </em>被称为后验概率，因为它是在事件<em class="lg"> B </em>发生之后事件<em class="lg"> Ai </em>的概率。此外，<em class="lg"> P(B|Ai) </em>称为给定<em class="lg"> Ai </em>的<em class="lg"> B </em>的可能性，表示事件<em class="lg"> B </em>给定事件<em class="lg"> A </em>的可能性有多大。</p><p id="2127" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实上，贝叶斯定理通过纳入观察事件<em class="lg"> B </em>提供的证据，将先验概率<em class="lg"> P(Ai) </em>转化为后验概率<em class="lg"> P(Ai|B) </em>。</p><p id="161d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在二元分类的情况下，数据点最初应该是正的或负的，因此:</p><p id="f545" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> P(D+)+P(D-)=1 </em></p><p id="e5f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此贝叶斯定理给出:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mm"><img src="../Images/25d6071a4bcdba364cd443f4ee1471ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kG_zib_b7QkQU4jtem4cKQ@2x.png"/></div></div></figure><p id="a75a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将<em class="lg"> P(D-) </em>替换为<em class="lg"> 1-P(D+) </em>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mn"><img src="../Images/8440f9de3af6e4e15ffc50e9f6e551af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-OZDgRzw-XhuO7wXCcFHwQ@2x.png"/></div></div></figure><p id="4b15" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了简化这个方程，我们可以把分子和分母都乘以<em class="lg">(1-P(D+)/P(T+| D-)</em>。然后我们有:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mo"><img src="../Images/7952fa606c1ebf7c408f19f024e59159.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LoEbHM0HV0gJYFTJ6RjfMQ@2x.png"/></div></div></figure><p id="db40" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了使这个等式更简单，我们需要定义一个新概念。在概率论中，一个事件<em class="lg"> e </em>的<a class="ae la" href="https://en.wikipedia.org/wiki/Odds" rel="noopener ugc nofollow" target="_blank"> <em class="lg">几率</em> </a>定义为该事件发生的概率，<em class="lg"> P(e)，</em>除以该事件不发生的概率，<em class="lg"> 1 — P(e) </em>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mp"><img src="../Images/e46f7ff6f785e57e3ebb482d7786680b.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*URCNFFpOOzYPK0jnbifi-Q@2x.png"/></div></div></figure><p id="f167" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，如果我们知道一个事件的概率或几率，我们可以很容易地计算出另一个。举个例子，如果一个特定事件的概率是 0.9，那么这个事件的<em class="lg">几率</em>就是事件 0.9 除以 0.1，也就是 9:1。根据这个等式，当概率为零时，<em class="lg">赔率</em>也为零，但当概率为 1 时，<em class="lg">赔率</em>无穷大。此外，在<em class="lg">赔率</em>和概率之间存在相关性:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/4286d71ce1254f4540625858ac496a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*waMz47nIHtuO30MfEpELNA@2x.png"/></div></figure><p id="8d4e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以用这个概念来简化为<em class="lg"> P(D+|T+)导出的最后一个方程。</em>我们可以将<em class="lg"> P(D+)/(1-P(D+)) </em>替换为<em class="lg"> odds(D+) </em>得到:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mr"><img src="../Images/5646fe0435547c2f8d5c9b9e0a692659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aCJHV7n1By8tJ5sRuCyoWA@2x.png"/></div></div></figure><p id="40f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们重新排列这个等式，我们得到:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ms"><img src="../Images/32f8fd7eb1e61a7ce038b45f49aa0774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7IrryQFg1GuzcvUQNwsGQ@2x.png"/></div></div></figure><p id="3441" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是左手边简直等于<em class="lg">赔率(D+|T+) </em>。所以最后我们有了:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mt"><img src="../Images/52264639650eb3c15b04278d901cb99b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e3HnyhyS6SyqX_9QSTwp4Q@2x.png"/></div></div></figure><p id="7a55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">术语<em class="lg"> P(T+|D+)/P(T+|D-) </em>是<em class="lg"> T+ </em>的两种可能性之比，我们称之为似然比(<em class="lg"> LR </em>)。一般来说，可能性比率是特定事件<em class="lg"> A </em>发生的概率除以事件<em class="lg">A</em>B<em class="lg">B</em>未发生的概率。事件<em class="lg"> T+ </em>实际上是阈值的函数，因为当<em class="lg"> h(x) </em>(由分类器计算的点的阳性标记的概率)大于或等于阈值时，分类器预测数据点的阳性标记:</p><p id="76af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">P(T+| D+)= P(h(x)≥阈值| D+) </em></p><p id="7d78" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">P(T+| D-)= P(h(x)≥阈值| D-) </em></p><p id="2485" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，对于具有特征 x 的数据点，似然比是假定该数据点的真实标签为正的情况下<em class="lg">h(x)≥阈值</em>的概率除以假定该数据点的真实标签为负的情况下<em class="lg">h(x)≥阈值</em>的概率。现在还记得我展示了<em class="lg"> P(T+|D+) </em>等于<em class="lg"> TPR </em>和<em class="lg"> P(T+|D-) </em>等于<em class="lg"> FPR </em>，所以我们有:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/a34eff842620253af33b6ab7046e70a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*R40mOK-D3M2mXSuoh4hI3w@2x.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mv"><img src="../Images/03579ed5a2bf81b636dee9c0f4ec2733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOfaPSxxXmj1CxdcW4sQMg@2x.png"/></div></div></figure><p id="6612" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于对应于 ROC 曲线上一点的每个阈值，<em class="lg"> TPR/FPR </em>等于似然比(<em class="lg"> LR </em>)。这也等于连接原点和该点的直线的斜率。例如，在图 29 中，点<em class="lg"> t </em>的<em class="lg"> LR </em>等于将其连接到原点的蓝线的斜率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mg"><img src="../Images/34dccd73bdbeb6948f436b2712a77e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zIC2iFaFmfunSSLqP2RPww.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 29</figcaption></figure><p id="310a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个等式能告诉我们什么？假设我们有一个数据集。我们通常知道<em class="lg"> D+ </em>的先验<em class="lg">赔率</em>。例如，考虑图 16 的数据集，其中 25 个点是正的，25 个点是负的。这里:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mw"><img src="../Images/be9bf48e231470945bb72bec58fe3595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nuyNRbtJrTsXwWObnbgz7A@2x.png"/></div></div></figure><p id="0fc6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我们对<em class="lg"> D+ </em>的先验概率，或者说是我们在观察分类器预测之前的初始知识。请不要混淆<em class="lg"> P(D+) </em>和<em class="lg"> h(x) </em>。<em class="lg"> h(x) </em>是在给定特征<em class="lg"> x </em>的情况下预测正点的条件概率，由分类器<em class="lg">，</em>计算，但<em class="lg"> P(D+) </em>是<em class="lg"> </em>在开始分类<em class="lg">之前，点的实际标记为正而不考虑其特征<em class="lg"> x </em>的概率。</em>现在给我们一个测试点。我们不知道它的<em class="lg"> x </em>值，因此我们也不知道<em class="lg"> h(x) </em>的确切值，但是我们有一个阈值，我们被告知<em class="lg"> h(x) &gt; = threshold。这意味着分类器预测该测试点的阳性标签。现在，这个点真的是正的概率是多少？如果我们没有这个预测，我们对这个测试点的最佳猜测仍然是先前的<em class="lg">赔率</em>或<em class="lg">赔率(D+) </em>。</em></p><p id="3338" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是有了这个预测，我们就可以计算 ROC 曲线上给定阈值的似然比，然后计算后验<em class="lg"> odds </em>也就是<em class="lg"> odds(D+|T) </em>。后验<em class="lg">概率</em>可以转换回概率，这是测试点确实为正的概率。现在，再次查看图 19。对于垂直线上的点，如点 B，我们有:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mx"><img src="../Images/75f7fe26298797e57941bb1acdc560d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QO-o54ofh3h_QAd8eXDAxg@2x.png"/></div></div></figure><p id="108d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，通过观察事件<em class="lg"> T+ </em>(这意味着分类器预测具有点 B 的阈值的正标签)，我们可以 100%确定我们有一个<em class="lg"> D+ </em>事件，这意味着该点的实际标签是正的。这是有意义的，因为高于该阈值的所有点实际上是阳性的(图 19)。</p><p id="c743" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于像点 D 这样的水平线上的点:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi my"><img src="../Images/8d847e1f4f85d35054fa9674aa313ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TSVcBjZd31DUm_C6o713Iw@2x.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mv"><img src="../Images/52367a2fcaa02d78a7a2f17dc1de4cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gHJ7w4pin5wFHvW4C5AwJg@2x.png"/></div></div></figure><p id="8890" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以通过观察一个<em class="lg"> T+ </em>事件或一个<em class="lg"> TP </em>预测与 D 点的阈值，后验<em class="lg">赔率</em>或概率变得大于前验<em class="lg">赔率</em>或概率。换句话说，这样的观察减少了我们对测试数据点实际标签的不确定性。但是<em class="lg"> P(D+|T+) </em>不像上一种情况是 1。那是因为在这个阈值以上有一些负的点被预测为正的(<em class="lg"> FP </em>)，有可能我们的测试点就是其中之一。</p><p id="ab24" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实上，对于非理想分类器对角线上方的任何点(如图 19 中的点<em class="lg"> C </em>),我们有类似的情况。对于这样一个点<em class="lg">TPR&gt;FPR</em>so1&lt;LR&lt;无穷大，我们得到同样的结果。</p><p id="45c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于随机分类器，所有点都在对角线上，因此:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mz"><img src="../Images/bdda43ebd06cf2402cfd03b334bf121b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tpu-LweV6FNX3TQp2cPtoQ@2x.png"/></div></div></figure><p id="0268" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随机分类器不会改变先前的<em class="lg">概率</em>，并且由于其盲目的性质，它不能改变我们对测试数据点的实际标签的不确定性。对于对角线下方的点，如图 28 中的点 B:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mz"><img src="../Images/61e219cbc383eb7e5665e57718733699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSmTF266CMenMx4EwCbXEg@2x.png"/></div></div></figure><p id="de6b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们知道这个分类器给出了一个错误的答案，所以当它预测一个肯定的答案时，更有可能答案是错误的，并且测试点的实际标签是否定的，所以给出这个预测的实际肯定的<em class="lg">几率</em>(或概率)变得低于先前的<em class="lg">几率</em>。同样，这个观察结果(尽管它是一个错误的预测)减少了我们对测试数据点的实际标签的不确定性。</p><p id="3754" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止，为<em class="lg"> T+ </em>事件计算了似然比，这意味着<em class="lg">h(x)&gt;=阈值</em>。但是，我们可以有其他条件。通常，对于事件<em class="lg"> e: </em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi na"><img src="../Images/24995dfcd8255eee3b6cc9864d1f4e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CBKq9ms1nVa95Y86qXUi4w@2x.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/eb033bad21d28b2e6eeedec0c777216d.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*nCteUEv9U58WviJJSCDWsg@2x.png"/></div></figure><p id="7120" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">推导和前面一样。将<em class="lg"> T+ </em>替换为<em class="lg"> e </em>即可。例如，我们可以有两个阈值<em class="lg"> t1 </em>和<em class="lg"> t2 </em>，并计算事件<em class="lg"> t1≤h(x) &lt; t2 </em>的可能性。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/75fb6c0215560cb832d99c08ba72d042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*nl9uhWFI2xk4CU-C7_Ha9w@2x.png"/></div></figure><p id="7e90" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以稍微简化一下这个分数。我们知道:</p><p id="de2d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">P(t1≤h(x)&lt;T2 | D+)= P(h(x)≥t1 | D+)—P(h(x)≥T2 | D+)</em></p><p id="19a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">前面说过，<em class="lg">P(h(x)&gt;= threshold | D+)= P(T+| D+)</em>等于<em class="lg"> TPR </em>。因此</p><p id="9861" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">P(t1≤h(x)&lt;T2 | D+)</em>=<em class="lg">TPR(t1)</em><em class="lg">—TPR(T2)</em></p><p id="584e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg"> TPR(t1) </em>表示使用阈值<em class="lg"> t1 </em>计算的<em class="lg"> TPR </em>。此外:</p><p id="59dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">P(t1≤h(x)&lt;T2 | D-)= P(h(x)≥t1 | D-)—P(h(x)≥T2 | D-)</em></p><p id="cd8e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">=[1—P(h(x)&lt;t1 | D-)]—[1—P(h(x)&lt;T2 | D-)]</em></p><p id="6166" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是:</p><p id="dc26" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lg">P(h(x)&lt;t1 | D-)= P(T-| D-)=特异性</em></p><p id="beb3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据定义<em class="lg">FPR = 1-特异性，所以</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nd"><img src="../Images/8542e72308159a2a747a794595280645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFM95N5eY8vUhKK4xML3_w@2x.png"/></div></div></figure><p id="e26b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们有</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ne"><img src="../Images/815a4c8e59efbd863bfe99a0919b18d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B78DDATmXBi3KtKL4xIg5A@2x.png"/></div></div></figure><p id="70ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，请看图 30。如果我们在 ROC 曲线上有两个阈值为<em class="lg"> t1 </em>和<em class="lg"> t2 </em>的点，LR 实际上是连接它们的直线的斜率。我们现在可以用这个<em class="lg"> LR </em>来计算<em class="lg"> D+ </em>的<em class="lg">赔率</em>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/36a7e00eccef77f854829f33d1ab5bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-xYza8p-rLY24SWhnZM5Tg@2x.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mg"><img src="../Images/aa6aa81c82b4d0f6bd18b39bd6e67fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c120fe0HB546A98zOkWIfQ.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 30</figcaption></figure><p id="20f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们又有了一个测试点。我们不知道它的<em class="lg"> x </em>值，所以我们不知道<em class="lg"> h(x) </em>的确切值；我们只知道这个测试点的<em class="lg"> t1≤h(x) &lt; t2 </em>。<em class="lg">odds(D+| t1≤h(x)&lt;T2)</em>给出<em class="lg"> odds </em>这个测试数据点在这个条件下确实是正的。</p><p id="2232" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这可以带给我们一个关于 ROC 曲线的结论。LR 实际上是两个条件概率的比值。根据定义，概率是正数，所以比率也是正数。这样一来，任意两点之间的斜率(with 等于<em class="lg"> LR </em>)永远不可能是负数，ROC 曲线永远是增函数。</p><p id="e1c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，请看图 31。我们在 ROC 曲线上有五个点<em class="lg"> t1 </em>到<em class="lg"> t5 </em>，每个点对应一个阈值，这个阈值用相同的名字来称呼。如果你还记得，当我们从<em class="lg"> TPR=FPR=0 </em>到<em class="lg"> TPR=FPR=1 </em>时，阈值是递减的。所以我们有五个阈值<em class="lg">t1&lt;T2&lt;T3&lt;T4&lt;t5</em>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mg"><img src="../Images/82e70fcab99eac9c07e40e0239adc7b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iUNjyfiBLrK99jkjTjXpzw.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 31</figcaption></figure><p id="d397" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">点<em class="lg"> t4 </em>和<em class="lg"> t5 </em>用垂直线连接。这条线的斜率是无穷大，这意味着 LR 也是无穷大。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ng"><img src="../Images/102944dc4a3b7213f30b17c98df63e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pSEAnPpxxhYFK5lDAABSyA@2x.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/276d89dace5b91878f9134d24adb21c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*hfzxVjH8KLgcOKnhlp5O8A@2x.png"/></div></figure><p id="16c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以如果一个数据点的<em class="lg"> h(x) </em>位于这个阈值范围内，那么它的实际标号肯定是正的。以图 19 中的点 B 为例，显示了对于这样的间隔，我们只有实际标签为正的点，这是一致的结果。</p><p id="7bdf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">点<em class="lg"> t1 </em>和<em class="lg"> t2 </em>用一条水平线连接。这条线的斜率为零，这意味着<em class="lg"> LR </em>也为零。因此:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ni"><img src="../Images/4dccaa64934b59a43b198672ed93c0cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O77uQtiqpmWJ6WG-me8RpQ@2x.png"/></div></div></figure><p id="6bdd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以当<em class="lg"> h(x) </em>在这个阈值范围内时，我们就可以确定该点的实际标签为负。以图 19 中的点<em class="lg"> D </em>为例，可以看出在这样的区间中，我们只有实际标签为负的点，这解释了为什么<em class="lg">几率</em>为零。</p><p id="408c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">连接<em class="lg"> t3 </em>和<em class="lg"> t4 </em>的直线斜率远大于连接<em class="lg"> t2 </em>和<em class="lg"> t3 </em>的直线斜率。这意味着:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/254348c73d393d5c7ab8ded7522d8589.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*1svZDQsvRKdM3l602coAiQ@2x.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nk"><img src="../Images/7b0077c049bb51575fbb799c9b691d15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QaxKD911_Wh-LV1K8y0geQ@2x.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ng"><img src="../Images/1ff23ca95ed762004d18bf2e570efa23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g6ycydYGyRzy3gciApM5eA@2x.png"/></div></div></figure><p id="5d54" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，当<em class="lg"> t3≤h(x) &lt; t4，</em>数据点的实际标签为正的机会比<em class="lg"> t2≤h(x) &lt; t3 高得多。</em></p><p id="becc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，连接点<em class="lg"> t1 </em>和<em class="lg"> t5 </em>的线永远是对角线。这条线的斜率是 1，所以<em class="lg"> LR </em>也是 1。结果，LR 不改变后验 odd，并且前验 odd 等于后验 odd。这也是有道理的。记住对于像<em class="lg"> t1 </em>、<em class="lg"> TPR=FPR=1 </em>和<em class="lg">阈值&gt; 1 这样的点。这是拒绝一切的分类器。此外，对于像<em class="lg"> t5 </em>、<em class="lg"> TPR=FPR=0 </em>和<em class="lg">阈值=0 </em>这样的点，因为这是预测一切为正的分类器。所以当我们说<em class="lg"> t1≤h(x) &lt; t5 </em>时，意思是<em class="lg"> h(x) </em>可以取 0 到 1 之间任何可能的值。这是一个不添加任何关于<em class="lg"> h(x) </em>的更多信息的条件，因为我们已经知道<em class="lg"> h(x) </em>是一个概率，并且在 0 和 1 之间有界。结果，通过观察这个条件，我们关于测试点的不确定性没有改变。该区间的 LR 为 1，后验概率保持与先验概率相同。</em></p><p id="bc0d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，ROC 曲线也可以被认为是一种显示似然比的图形工具，而似然比是计算后验概率所必需的。一个好的分类器是给出高后验概率的分类器。这意味着当分类器预测一个正点时，分类器是正确的概率很高，并且该点确实是正的。现在，请看图 32。这里显示了三个分类器 ROC 曲线。一种是理想分类器(分类器 A)。其他的都不理想，但是有一个比较接近理想的分类器(分类器 B)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nl"><img src="../Images/e3bc79f9a7a8024b5e23f3339ba492f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdNiYB0B5g2NurmRqHlKkg.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 32</figcaption></figure><p id="4c57" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们知道逻辑回归使用 0.5 的阈值。所以对应于每个分类器的这个阈值的点已经被标绘。连接这些点和原点的线也已经显示出来。如前所述，这条线的斜率给出了该阈值的<em class="lg"> LR </em>。分类器<em class="lg"> A </em>是理想的分类器，斜率是可能的最高值(无穷大)。所以<em class="lg"> LR </em>和后验<em class="lg">几率</em>都是无穷大，后验概率等于 1。这意味着当分类器预测到阳性标签时，我们可以确保它是正确的标签。</p><p id="c289" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们比较一下量词<em class="lg"> B </em>和<em class="lg"> C </em>。对于分类器<em class="lg"> B </em>，连接原点到<em class="lg"> t=0.5 </em>点的直线的斜率明显较高。这意味着<em class="lg"> LR </em>、后验<em class="lg">几率</em>以及后验概率更高(我们假设对于所有这些分类器，我们具有相同的先验<em class="lg">几率</em>)。因此，当分类器 B 做出预测时，预测的标签更有可能是正确的。事实上，分类器<em class="lg"> B </em>的预测比<em class="lg"> C </em>更可靠，这使其成为更好的分类器(它也具有更高的<em class="lg"> AUC) </em>。随着<em class="lg"> t=0.5 </em>点越来越接近(<em class="lg"> TPR=1，FPR=0 </em>)，这个斜率越来越大，后验概率<em class="lg">也越来越大。</em></p><p id="b5a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是<em class="lg"> AUC </em>的统计解释是什么？<em class="lg"> AUC </em>是<em class="lg"> TPR </em> vs <em class="lg"> FPR </em>的曲线下面积。所以数学上:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/f9abb871a824d82a0b64ad78eb96933c.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*W2neo3eKe-P2CWK1kOqIlQ@2x.png"/></div></figure><p id="9492" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随机变量 X 的<a class="ae la" href="https://en.wikipedia.org/wiki/Cumulative_distribution_function" rel="noopener ugc nofollow" target="_blank">累积分布函数(<em class="lg"> cdf </em> ) </a>定义为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/095870cd7095facb467bf219f09b8da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*_AEEtR3liiCnqj9xjyQcFQ@2x.png"/></div></figure><p id="2aae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">并且<em class="lg"> cdf </em>与该随机变量的概率分布函数(<em class="lg"> pdf </em>)之间存在关系:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e74def1c68ca0c124189162dc7c26337.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*E4rn5_K2ZnsTSWxmV8hZJw@2x.png"/></div></figure><p id="1f81" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">和</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/1e7c07bd527aa85cad20723f0a5925b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*CNjQATjkP043jOca7wxkbg@2x.png"/></div></figure><p id="c533" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">类似的等式适用于<a class="ae la" href="https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/140-conditional-beta.pdf" rel="noopener ugc nofollow" target="_blank">条件<em class="lg"> cdf </em>和<em class="lg"> pdf </em>和</a>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi np"><img src="../Images/1c336da9ebe7ec86f186e04e75db4568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bKMZ-F-BGTC-d_UIgOg2vQ@2x.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e1aa49c070b04050e032449c6f5f3c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*ghIg4FsyzVhS97HNxTuZvQ@2x.png"/></div></figure><p id="a616" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以根据<em class="lg"> cdf: </em>来写<em class="lg"> TPR </em>和<em class="lg"> FPR </em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mf"><img src="../Images/c7308bd4907e5c8ee08c9d12945b5d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bEtc3rK-5s3Q2dFdXKd49A@2x.png"/></div></div></figure><p id="7aa1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可能注意到了，我把<em class="lg"> h(x) &lt; t </em>换成了<em class="lg"> h(x)≤t </em>。如果我们假设<em class="lg"> h(x) </em>具有连续分布，那么<em class="lg"> P(h(x)=a)=0 </em>对于任意数<em class="lg"> a、</em>和<em class="lg"> </em>多加一个点不改变概率。同样，我们有:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/8dd0004eaab4b48086b27cb7479a21da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45_eZRQvJcqq0cBWBBk1xw@2x.png"/></div></div></figure><p id="efa6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以写:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/f21b5039ec5983107d43ab2b053e4d7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7_A8X9eH1lV7Gl8GKCBjEQ@2x.png"/></div></div></figure><p id="7de7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是我们知道:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/eab3acae8da1a5ac0578c92fe84f3a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*cbw9fbhdpoSErTlYkO5jdw@2x.png"/></div></figure><p id="f783" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以在积分中替换它，但是这会把我们的积分变量从<em class="lg"> FPR </em>变成<em class="lg"> t </em>，所以积分的极限也应该改变。我们已经有了 FPR 的 cdf。使用<em class="lg"> cdf </em>的定义，我们有:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/01e4332d38fbb80bb322cadf7d08d639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*REnQ3c-TdYGU-xvQ4zjV3Q@2x.png"/></div></div></figure><p id="4c3c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，如果<em class="lg"> t=-infinity </em>，积分将为 1，因为<em class="lg"> pdf </em>被归一化，如果<em class="lg"> t=infinity </em>，积分为零。所以:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e62dc903b45bbb1e8334b38eb0abdc1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*CehlHK90U-CFVxYUGUxEOg@2x.png"/></div></figure><p id="6828" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">需要注意的是，当我们用一个<em class="lg"> pdf </em>到<em class="lg">T3】计算<em class="lg"> FPR </em>和<em class="lg"> TPR 时，</em>的阈值范围是从<em class="lg">-无穷大到无穷大。</em>这是因为<em class="lg"> pdf </em>不是一个概率，不在零和 1 之间。然而，在代码中，我们使用了 0 到 1.1 的阈值范围。这是因为我们没有使用<em class="lg"> pdf </em>直接计算介于 0 和 1 之间的概率。</em></p><p id="b645" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们有:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/6b8b1eccee085accee5eb27caf5452fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X8wpS4Li9DD6G3e_QJ8Pcg@2x.png"/></div></div></figure><p id="3c6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们应该用<em class="lg"> pdf </em>替换<em class="lg"> G </em>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/edb95e284ed1f98d4322e492aa32fb90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ddrs5GOPC8eNtaXEjfbYw@2x.png"/></div></div></figure><p id="1c9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将其代入<em class="lg"> AUC </em>方程，我们得到:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/d7f0d27f211574b11cccf788f64c798e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QupSojGvfJ-Zgs84iE9hWQ@2x.png"/></div></div></figure><p id="0e7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们用一个小技巧来简化它。我们可以将 g 乘以一个系数，该系数可以是 0 或 1:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nz"><img src="../Images/c0b0d41c7bc24c5bcd1c4a32f76f2af7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*06fMUE5pxJi4BNIvxqRd5g@2x.png"/></div></div></figure><p id="a6f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了进一步简化，我们可以使用<a class="ae la" href="https://en.wikipedia.org/wiki/Indicator_function" rel="noopener ugc nofollow" target="_blank">指示器功能</a>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/76097dbf87888db20f0367e62cb7be48.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*P4TC1Rjg3HOg13L38dk3vQ@2x.png"/></div></figure><p id="7b62" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事件的指示器函数在事件发生时取值 1，在事件未发生时取值 0。事件<em class="lg"> E </em>的指示函数的期望值等于<em class="lg"> P(E) </em>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ob"><img src="../Images/1000f0381870bec55c6d83fffe877b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyPZDnTgSjsT6NZZEZqurQ@2x.png"/></div></div></figure><p id="6d47" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们回到<em class="lg"> AUC </em>等式，我们看到当<em class="lg"> s </em>在-无穷大和<em class="lg"> t </em>之间或者简单地小于等于<em class="lg"> t </em>时，系数为 0，当<em class="lg"> s </em>在 t 和无穷大之间或者简单地大于<em class="lg"/>t 时，系数为 1(事实上它大于或等于<em class="lg"> t </em>，但是所以我们可以用事件<em class="lg"> s &gt; t </em>的指示函数来代替。但是<em class="lg"> s </em>是随机变量<em class="lg"> h(X)|D+ </em>的值<em class="lg"> t </em>是随机变量<em class="lg"> h(X)|D-的值。</em>所以指标作用是针对事件<em class="lg">h(X)| D+</em>&gt;<em class="lg">h(X)| D-。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/645660ab3ebe66321e77d790dfe237a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lud3XN-qHTc40y0dok1xUw@2x.png"/></div></div></figure><p id="c7a0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">值得注意的是，该指示函数有两个变量，为了计算其期望值，我们应该对 s 和 t 进行积分。最后，通过用指示函数替换虚拟系数，我们得到:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/8cec22f2cf30ffeed9fc8275b18a72ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1al3EsES_-mgbA2lLDFJDg@2x.png"/></div></div></figure><p id="43b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但这意味着什么呢？假设我们从带有实际正标签(<em class="lg"> D+ </em>)的点中随机抽取一个点<em class="lg"> x1 </em>，从带有实际负标签(<em class="lg"> D- </em>)的点中随机抽取一个点<em class="lg"> x2 </em>。<em class="lg"> AUC </em>简单来说就是<em class="lg"> h(x1) &gt; h(x2) </em>的概率。你应该注意到这不是<em class="lg"> x1 </em>预测为正而<em class="lg"> x2 </em>预测为负的概率。预测是将<em class="lg"> h(x) </em>与阈值进行比较的结果。这里我们只是针对这些点比较<em class="lg"> h(x) </em>。然而，这个概率给了我们一些关于数据集中正负点的可分性的信息。</p><p id="95e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请看图 13。对于一个理想的分类器<em class="lg"> AUC=P(x1 &gt; x2)=1 </em>。这意味着，如果我们随机选择一个正点和一个负点，正点的<em class="lg"> h(x) </em>将总是更高。这是因为所有的正点都比所有的负点具有更高的<em class="lg"> h(x) </em>。现在，请看图 23。对于随机分类器<em class="lg"> AUC=P(x1 &gt; x2)=0.5 </em>。意思是，如果我们随机选择一个正的和一个负的，那么<em class="lg"> h(x1) &gt; h(x2) </em>和<em class="lg"> h(x1)≤h(x2) </em>两者的概率是相等的。这是数据点重叠的结果。对于对角线上方的非理想分类器<em class="lg"> AUC=P(x1 &gt; x2) &gt; 0.5。</em><em class="lg">AUC 越高，</em>数据集中出现的重叠越少，并且在将最佳 sigmoid 函数拟合到该数据集后，预测正确的标签将更容易。</p><p id="9727" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如你所见，ROC 曲线的数学理论与我们在前面章节中所学的一致。这里我们假设数据集只有一个特征<em class="lg"> x，</em>并且<em class="lg"> x </em>和<em class="lg"> h(x) </em>都是标量。如果我们有一个以上的特征，那么<em class="lg"> x </em>将是一个矢量，但是<em class="lg"> h(x) </em>仍然是一个标量，这里推导的所有方程仍然有效。</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="20c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本文中，我们只使用了 Scikit-learn 的逻辑回归模型。Scikit-learn 有更多的机器学习算法，但绘制 ROC 曲线可以用类似的方式完成。首先，您使用该模型的<code class="fe lv lw lx ly b">predict_proba()<strong class="js iu"> </strong></code>方法来计算概率。然后你需要将这些概率和真实标签放入<code class="fe lv lw lx ly b">metrics.roc_curve()</code>中，以计算<em class="lg"> TPR </em>和<em class="lg"> FPR </em>。您也可以使用计算出的<em class="lg"> TPR </em>和<em class="lg"> FPR </em>通过<code class="fe lv lw lx ly b">metrics.auc()</code>计算 AUC。您最终可以使用类似<code class="fe lv lw lx ly b">matplotlib</code>的绘图库来绘制结果。</p><p id="0ccf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我希望你喜欢阅读这篇文章。如果您有任何问题或建议，请告诉我。本文中的所有代码清单都可以作为 Jupyter 笔记本从 GitHub 下载，网址:<a class="ae la" href="https://github.com/reza-bagheri/ROC_curve_article" rel="noopener ugc nofollow" target="_blank">https://github.com/reza-bagheri/ROC_curve_article</a></p></div></div>    
</body>
</html>