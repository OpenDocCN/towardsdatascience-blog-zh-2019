<html>
<head>
<title>Data Leakage in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的数据泄漏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-leakage-in-machine-learning-10bdd3eec742?source=collection_archive---------10-----------------------#2019-07-16">https://towardsdatascience.com/data-leakage-in-machine-learning-10bdd3eec742?source=collection_archive---------10-----------------------#2019-07-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bc63" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何防止降低模型质量和/或导致不一致结果的问题</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/a72adbb4510c57dd58dd2320934ad2f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*kkt0hZGOzdZvnUliU3Mh5g.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk"><a class="ae ku" href="https://media.giphy.com/media/EHcpe9guGONCU/giphy.gif" rel="noopener ugc nofollow" target="_blank">https://media.giphy.com/media/EHcpe9guGONCU/giphy.gif</a></figcaption></figure><h1 id="18d1" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">介绍</h1><p id="518d" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">当训练机器学习模型时，我们通常会瞄准在某些指标上得分最高的模型，例如准确性。自然地，当我们训练一个在我们的验证或测试数据集上表现很好的模型时，我们选择它作为一个表现良好的模型，并生产/最终确定它。</p><p id="4fba" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">但是，您是否遇到过这样的情况:一个模型在测试期间表现良好，但在实际使用中却无法达到相同的性能水平？例如，您的模型在测试期间是否达到了 99%的准确性，但是一旦它被生产出来并作用于真实数据，它就无法达到那个性能水平？</p><p id="30e8" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">测试性能和真实性能之间的这种差异通常可以用一种叫做<strong class="lp iu">数据泄漏</strong>的现象来解释。</p><h1 id="57a7" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">数据泄露</h1><p id="57f3" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">数据泄漏是指机器学习模型的创建者犯下的错误，其中他们意外地在测试和训练数据集之间共享信息。通常，当将数据集划分为测试集和训练集时，目标是确保两者之间没有数据共享。这是因为测试集的目的是模拟真实世界中看不见的数据。然而，当评估一个模型时，我们确实可以完全访问我们的训练集和测试集，所以由我们来确保训练集中没有数据出现在测试集中。</p><p id="f8aa" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">数据泄漏通常会导致测试集的性能达到不切实际的高水平，因为模型是基于它在训练集中已经看到的数据(在某种程度上)运行的。该模型有效地记忆训练集数据，并且能够容易地正确输出那些测试数据集示例的标签/值。显然，这并不理想，因为它误导了评估模型的人。当这种模型用于真正看不见的数据时，性能会比预期的低得多。</p><h1 id="8c79" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">数据泄露的原因</h1><p id="5a81" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">现在我将提到一些数据泄漏的常见原因。在训练你自己的模型时，避免这些情况是很重要的。一般来说，您应该避免对您的训练集做任何涉及测试集知识的事情。</p><h2 id="7ed8" class="mo kw it bd kx mp mq dn lb mr ms dp lf lw mt mu lh ma mv mw lj me mx my ll mz bi translated">预处理</h2><p id="cd5b" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">人们犯的一个很常见的错误是在机器学习的数据预处理步骤中泄露信息。重要的是，这些转换只知道训练集，即使它们也应用于测试集。例如，如果您决定将运行 PCA 作为预处理步骤，那么您应该只让 PCA 模型适合训练集。然后，要将它应用到您的测试集，您只需在测试集上调用它的<code class="fe na nb nc nd b">transform</code>方法(在 scikit-learn 模型的情况下)。相反，如果您在整个数据集上安装您的预处理器，您将从测试集中泄漏信息，因为预处理模型的参数将与测试集的知识相适应。</p><h2 id="0698" class="mo kw it bd kx mp mq dn lb mr ms dp lf lw mt mu lh ma mv mw lj me mx my ll mz bi translated">复制</h2><p id="88bd" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">另一个错误是数据重复，当您的数据集来自嘈杂的真实数据时，这种错误尤其常见。当数据集包含几个具有相同或接近相同数据的点时，会出现这种情况。例如，如果您的数据集包含消息平台上的用户消息，重复的消息可能对应于向许多用户发送相同消息的垃圾邮件发送者。在这种情况下，您可能会遇到数据泄漏，这仅仅是因为您的训练集和测试集可能包含相同的数据点，即使它们可能对应于不同的观察值。这可以通过在分割成训练集和测试集之前消除数据集的重复来解决。您可以通过删除完全重复的内容，或者使用模糊匹配方法(例如通过编辑文本数据的距离)来删除近似匹配的内容。</p><h2 id="aaec" class="mo kw it bd kx mp mq dn lb mr ms dp lf lw mt mu lh ma mv mw lj me mx my ll mz bi translated">时态数据(隐式泄漏)</h2><p id="1901" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">即使您没有明确地泄漏信息，如果您的测试和训练集之间存在依赖关系，您仍然可能会遇到数据泄漏。一个常见的例子是时间数据，即时间是一个相关因素的数据，如时序数据。考虑下面的玩具例子:你的训练集由两个数据点<strong class="lp iu"> A </strong>和<strong class="lp iu"> C </strong>组成，你的训练集由一个数据点<strong class="lp iu"> B </strong>组成。现在，假设这些数据点的时间顺序是<strong class="lp iu"> A </strong> → <strong class="lp iu"> B </strong> → <strong class="lp iu"> C </strong>。在这里，我们很可能通过创建训练集和测试集的方式造成了数据泄漏。通过在点<strong class="lp iu"> C </strong>上的训练和在点<strong class="lp iu"> B </strong>上的测试，我们创建了一个不切实际的情况，在这种情况下，我们根据相对于测试集时间点的未来知识来训练我们的模型。因此，我们已经泄露了信息，因为在现实世界的场景中，我们的模型显然不知道未来。为了解决这个问题，您应该确保您的测试序列分割也是跨时间分割的。因此，训练集中的所有内容都应该出现在测试集中的所有内容之前。这将创建一个更加真实的训练环境，并允许您正确地评估您的模型，就像它正在处理输入的真实世界数据一样。</p></div></div>    
</body>
</html>