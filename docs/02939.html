<html>
<head>
<title>Shallow Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">浅层神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/shallow-neural-networks-23594aa97a5?source=collection_archive---------2-----------------------#2019-05-13">https://towardsdatascience.com/shallow-neural-networks-23594aa97a5?source=collection_archive---------2-----------------------#2019-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/8901556147ea424b95645210ae513c2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_kDIpLJyhj2y2TUEXKKWNQ.png"/></div></div></figure><div class=""/><p id="8986" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们听到神经网络这个名字时，我们觉得它由许多许多隐藏层组成，但有一种神经网络只有少数几个隐藏层。浅层神经网络仅由 1 或 2 个隐藏层组成。理解一个浅层神经网络能让我们深入了解深层神经网络内部到底发生了什么。在这篇文章中，让我们看看什么是浅层神经网络及其在数学环境中的工作。下图显示了一个具有 1 个隐藏层、1 个输入层和 1 个输出层的浅层神经网络。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi kw"><img src="../Images/15c149e022ad3686b42e730daee10820.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*CfdaqnNb6RHLzPJTt1UXjQ.png"/></div></figure><h1 id="4162" class="lb lc jb bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">神经元</h1><p id="dee7" class="pw-post-body-paragraph jy jz jb ka b kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv ij bi translated">神经元是神经网络的原子单位。给定一个输入，它提供输出并将该输出作为输入传递给后续层。一个神经元可以被认为是两部分的组合:</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi me"><img src="../Images/865af4a56ad35a9ca78af34e4f9e10f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*eQymgb-r347UOVBvBGvi0Q.png"/></div></figure><ol class=""><li id="be15" class="mf mg jb ka b kb kc kf kg kj mh kn mi kr mj kv mk ml mm mn bi translated">第一部分使用输入和权重计算输出<strong class="ka jc"> <em class="mo">、Z </em> </strong>。</li><li id="82d9" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated">第二部分对<strong class="ka jc"> Z </strong>进行激活，给出神经元的最终输出<strong class="ka jc"> A </strong>。</li></ol><h1 id="ee34" class="lb lc jb bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">隐藏层</h1><p id="5f58" class="pw-post-body-paragraph jy jz jb ka b kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv ij bi translated">隐藏层由各种神经元组成，每个神经元执行上述两种计算。存在于我们的浅层神经网络的隐藏层中的 4 个神经元计算如下:</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/c3535d447f60657b56b8201056e664e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*n6VFcapcbmSwR3kDOOCjNA.png"/></div></figure><p id="75f5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上面的等式中，</p><ol class=""><li id="75cd" class="mf mg jb ka b kb kc kf kg kj mh kn mi kr mj kv mk ml mm mn bi translated">上标数字<strong class="ka jc"><em class="mo">【I】</em></strong>表示层数，下标数字<strong class="ka jc"> <em class="mo"> j </em> </strong>表示特定层中的神经元数。</li><li id="d70d" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated"><strong class="ka jc"> <em class="mo"> X </em> </strong>是由 3 个特征组成的输入向量。</li><li id="35f1" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated"><strong class="ka jc"> <em class="mo"> W[i]j </em> </strong>是与层<strong class="ka jc"> <em class="mo"> i </em> </strong>中存在的神经元<strong class="ka jc"> <em class="mo"> j </em> </strong>相关联的权重。</li><li id="cc37" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated"><strong class="ka jc"><em class="mo">b【I】j</em></strong>是与层<strong class="ka jc"> <em class="mo"> i </em> </strong>中存在的神经元<strong class="ka jc"> <em class="mo"> j </em> </strong>相关联的偏置。</li><li id="a214" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated"><strong class="ka jc"><em class="mo">Z【I】j</em></strong>是与层<strong class="ka jc"> <em class="mo"> i </em> </strong>中存在的神经元<strong class="ka jc"> <em class="mo"> j </em> </strong>相关联的中间输出。</li><li id="3654" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated"><strong class="ka jc"><em class="mo">A【I】j</em></strong>是与层<strong class="ka jc"> <em class="mo"> i </em> </strong>中存在的神经元<strong class="ka jc"> <em class="mo"> j </em> </strong>相关联的最终输出。</li><li id="44cb" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated"><strong class="ka jc"> <em class="mo">适马</em> </strong>是乙状结肠的激活功能。数学上它被定义为:</li></ol><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/38341ebd7205789adda2246598a3f0f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*s5aP_gj-4Yp4M43rdJOldw.png"/></div></figure><p id="a745" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以看到，上面的 4 个方程似乎是多余的。因此，我们将它们矢量化为:</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/c0d2ffa0c5afc167185521174a64c9d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*Jray1MUI1EJU-W-rGUPPZA.png"/></div></figure><ol class=""><li id="318d" class="mf mg jb ka b kb kc kf kg kj mh kn mi kr mj kv mk ml mm mn bi translated">第一个等式在单个矩阵乘法中计算所有中间输出<strong class="ka jc"> <em class="mo"> Z </em> </strong>。</li><li id="9653" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated">第二个等式计算单个矩阵乘法中的所有激活<strong class="ka jc"> A </strong>。</li></ol><h1 id="f83e" class="lb lc jb bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">浅层神经网络</h1><p id="9f3a" class="pw-post-body-paragraph jy jz jb ka b kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv ij bi translated">使用各种隐藏层来构建神经网络。现在我们知道了在特定层中发生的计算，让我们理解对于给定的输入<strong class="ka jc"> <em class="mo"> X </em> </strong>，整个神经网络如何计算输出。这些也可以称为<strong class="ka jc"> <em class="mo">正向传播</em> </strong>方程。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/81a93db148aa7d736f47aa1c4c6ab535.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*YAnJpLREsrnodz6IkKsGFg.png"/></div></figure><ol class=""><li id="4549" class="mf mg jb ka b kb kc kf kg kj mh kn mi kr mj kv mk ml mm mn bi translated">第一个等式计算第一个隐藏层的中间输出<strong class="ka jc"><em class="mo">Z【1】</em></strong>。</li><li id="2f38" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated">第二个等式计算第一个隐藏层的最终输出<strong class="ka jc"><em class="mo">A【1】</em></strong>。</li><li id="37ce" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated">第三个等式计算输出层的中间输出<strong class="ka jc"><em class="mo">Z【2】</em></strong>。</li><li id="57a4" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated">第四个等式计算输出层的最终输出<strong class="ka jc"><em class="mo">A【2】</em></strong>，这也是整个神经网络的最终输出。</li></ol><h1 id="f05b" class="lb lc jb bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">激活功能</h1><p id="2379" class="pw-post-body-paragraph jy jz jb ka b kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv ij bi translated">我们知道神经网络基本上是一组数学方程和权重。为了使网络健壮，以便它在不同的场景中表现良好，我们利用激活功能。这些激活函数在神经网络中引入了非线性特性。让我们借助我们的浅层神经网络来尝试理解为什么激活函数对于任何神经网络都是至关重要的。</p><p id="e1a2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果没有激活函数，我们的浅层神经网络可以表示为:</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6472da2de504e0ac7a5595f569bbb633.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*dBGztsbx02ziWJ2jjUovFQ.png"/></div></figure><p id="53b7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们将等式 1 中 Z[1]的值代入等式 2，则我们得到以下等式:</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/1e263d6d884f19bebf01ee12f54e47fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*7sMJuMgRHhBam9aB_aQDwA.png"/></div></figure><p id="67fc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如您所见，输出将变成新的权重矩阵<strong class="ka jc"> <em class="mo"> W </em> </strong>、输入<strong class="ka jc"> <em class="mo"> X </em> </strong>和新的偏差<strong class="ka jc"> <em class="mo"> b </em> </strong>的线性组合，这意味着隐藏层中存在的神经元和隐藏层中存在的权重没有任何意义。因此，为了在网络中引入非线性，我们使用激活函数。</p><p id="7b5a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有许多激活功能可以使用。其中包括<strong class="ka jc"><em class="mo"/></strong><strong class="ka jc"><em class="mo">Tanh</em></strong><strong class="ka jc"><em class="mo">ReLU</em></strong><strong class="ka jc"><em class="mo">漏液 ReLU </em> </strong>等等很多。对所有层使用特定的激活功能不是强制性的。您可以为特定层选择激活功能，并为另一层选择不同的激活，依此类推。你可以在<a class="ae na" rel="noopener" target="_blank" href="/activation-functions-neural-networks-1cbd9f8d91d6">这篇文章</a>中读到更多关于这些激活功能的内容。</p><h1 id="2e68" class="lb lc jb bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">重量初始化</h1><p id="a163" class="pw-post-body-paragraph jy jz jb ka b kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv ij bi translated">我们知道一个神经网络的权重矩阵<strong class="ka jc"> <em class="mo"> W </em> </strong>是随机初始化的。有人可能想知道，为什么不能用 0 或任何特定的值初始化<strong class="ka jc"> <em class="mo"> W </em> </strong>。让我们借助我们浅薄的神经网络来理解这一点。</p><p id="cd1e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">设第一层的权重矩阵<strong class="ka jc"> <em class="mo"> W1 </em> </strong>和第二层的权重矩阵<strong class="ka jc"> <em class="mo"> W2 </em> </strong>初始化为 0 或其他值。现在，如果权重矩阵是相同的，隐藏层中神经元的激活将是相同的。此外，激活的导数将是相同的。因此，该隐藏层中的神经元将以类似的方式修改权重，即在特定隐藏层中具有多于 1 个神经元没有意义。但是，我们不想这样。相反，我们希望隐藏层中的每个神经元都是唯一的，具有不同的权重，并作为唯一的功能工作。因此，我们随机初始化权重。</p><p id="f9b9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最好的初始化方法是<strong class="ka jc"> <em class="mo"> Xavier 的初始化</em> </strong>。数学上它被定义为:</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/33306a27232b137915986cc5bb7ef756.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*NTL6KFMmRa_aW2k6WQGNkw.png"/></div></figure><p id="27c6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它陈述了特定层<strong class="ka jc"><em class="mo">【l】</em></strong>的权重矩阵<strong class="ka jc"><em class="mo">W</em></strong>从<em class="mo">正态分布</em>中随机选取，其中<strong class="ka jc"> <em class="mo">均值μ= 0</em></strong><strong class="ka jc"><em class="mo">方差σ=层 L1</em></strong>中神经元数量的乘积倒数。所有层的偏置<strong class="ka jc"> <em class="mo"> b </em> </strong>初始化为 0。</p><h1 id="f411" class="lb lc jb bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">梯度下降</h1><p id="e874" class="pw-post-body-paragraph jy jz jb ka b kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv ij bi translated">我们知道神经网络的权重是随机初始化的。为了使用神经网络进行正确的预测，我们需要更新这些权重。我们更新这些权重的方法被称为梯度下降。让我们用一个计算图来理解这一点。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/7e15c4629618b01f4fbda2c75523ee41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jF9ZQerGSAGZMnzEQIroJg.jpeg"/></div></div></figure><p id="2ad0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上图中，正向传播(用黑线表示)用于计算给定输入<strong class="ka jc"> <em class="mo"> X </em> </strong>的输出。反向传播(用红线表示)用于更新权重矩阵<strong class="ka jc"><em class="mo">【W[1】】、W[2] </em> </strong>和偏差<strong class="ka jc"><em class="mo">【b[1】、b[2】</em></strong>。这个<strong class="ka jc"> <em class="mo"> </em> </strong>是通过计算图中每一步输入的导数来完成的。我们知道损耗<strong class="ka jc"> <em class="mo"> L </em> </strong>在数学上定义为<em class="mo"> : </em></p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/5b9e959154cd8c74fd963d85a22a17dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*yXVVHG47pTXfNyJ60dEd6Q.png"/></div></figure><p id="3e9f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用上述损耗等式<strong class="ka jc"> <em class="mo"> L </em> </strong>并使用 sigmoid 函数作为隐藏和输出层的激活函数，借助导数的链式法则，我们计算如下:</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ne"><img src="../Images/9448e070ae3ff5e94a113414059024d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EPTVQQkVZGx3ABc8D0y-pQ.png"/></div></div></figure><p id="3049" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上面的等式可能会令人困惑，但是它们非常适合梯度下降。在<strong class="ka jc"> <em class="mo">的等式中，dZ【1】</em></strong>，<strong class="ka jc"> * </strong>表示乘积明智乘法，<strong class="ka jc"><em class="mo">【σ’</em></strong><em class="mo"/>表示σ的导数。</p><blockquote class="nf ng nh"><p id="10e7" class="jy jz mo ka b kb kc kd ke kf kg kh ki ni kk kl km nj ko kp kq nk ks kt ku kv ij bi translated">我强烈建议读者，如果他们懂微积分的话，自己算出上面的方程，以便更好地理解梯度下降是如何工作的。</p></blockquote><p id="8210" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，在这个故事中，我们研究了浅层神经网络如何在数学环境中工作。尽管如此，我已经尽可能详细地解释了一切，如果你觉得缺少了什么，请查看我以前的帖子，或者在下面的评论区提出你的疑问。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="58b6" class="lb lc jb bd ld le ns lg lh li nt lk ll lm nu lo lp lq nv ls lt lu nw lw lx ly bi translated">参考</h1><ol class=""><li id="bbf8" class="mf mg jb ka b kb lz kf ma kj nx kn ny kr nz kv mk ml mm mn bi translated"><a class="ae na" href="https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning" rel="noopener ugc nofollow" target="_blank"> Coursera —深度学习课程 1 </a></li><li id="61e9" class="mf mg jb ka b kb mp kf mq kj mr kn ms kr mt kv mk ml mm mn bi translated"><a class="ae na" href="http://www.deeplearning.ai/ai-notes/initialization/" rel="noopener ugc nofollow" target="_blank">深度学习笔记—初始化</a></li></ol></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><p id="10b6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感谢读者阅读了这个故事。如果你有任何问题或疑问，请在下面的评论区提问。我将非常乐意回答这些问题并帮助你。如果你喜欢这个故事，请关注我，以便在我发布新故事时获得定期更新。我欢迎任何能改进我的故事的建议。</p></div></div>    
</body>
</html>