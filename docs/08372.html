<html>
<head>
<title>Nothing but NumPy: Understanding &amp; Creating Binary Classification Neural Networks with Computational Graphs from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">只有数字:从零开始理解和创建带有计算图的二元分类神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c?source=collection_archive---------1-----------------------#2019-11-14">https://towardsdatascience.com/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c?source=collection_archive---------1-----------------------#2019-11-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="9d86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl"> Nothing but Numpy 是我的神经网络系列的延续。要查看本系列的前一篇博客或重温神经网络，你可以点击</em> <a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener"> <em class="kl">这里</em> </a> <em class="kl">。</em></p><p id="f1b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章延续了<a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener">从零开始理解和创建带有计算图的神经网络</a>。</p><p id="b10f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当你打开 20 个浏览器标签试图理解一个复杂的概念，而你遇到的大多数文章都重复着同样肤浅的解释时，你很容易感到困惑。在<em class="kl"> Nothing but NumPy 的第二部分中，</em>我将再次努力让读者更深入地了解神经网络，因为我们将更深入地研究一种称为“二进制分类神经网络”的特定神经网络。<em class="kl">如果你读过我的</em> <a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener"> <em class="kl">上一篇文章</em> </a> <em class="kl">那么这看起来会很熟悉。</em></p><p id="50dc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">理解“二元分类”将有助于我们放下主要概念，这些概念有助于我们理解我们在多分类中做出的许多选择，这就是为什么这篇文章也将作为“从头开始理解和创建具有计算图的 Softmax 层”的前奏。</p><p id="292c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇博文分为两部分，第一部分将理解二进制分类神经网络的基础知识，第二部分将包含实现从第一部分学到的所有内容的代码。</p><h1 id="6e65" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">第一部分:了解二元分类</h1><h1 id="f54a" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">让我们开始吃吧🍽️</h1><p id="b6b5" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">二元分类是一种常见的机器学习任务。它包括预测一个给定的例子是属于一个类还是另一个类。这两个类可以被任意分配为一个<strong class="jp ir"><em class="kl"/></strong>或一个<strong class="jp ir"><em class="kl">【1】</em></strong>用于数学表示，但更常见的是感兴趣的对象/类被分配为一个<strong class="jp ir"><em class="kl">1</em></strong><strong class="jp ir"><em class="kl">【正标签】</em> </strong>，其余的为一个<em class="kl">0</em><strong class="jp ir"><em class="kl">(负标签)例如:</em></strong></p><ul class=""><li id="b9a2" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">给出的图片是猫(<strong class="jp ir"> <em class="kl"> 1 </em> </strong>)还是非猫(<strong class="jp ir"> <em class="kl"> 0 </em> </strong>)？</li><li id="de9d" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">给定一个病人的检查结果，肿瘤是良性的(<strong class="jp ir"><em class="kl">0</em></strong>)；无害)或恶性(<strong class="jp ir"><em class="kl">1</em></strong>)；有害)？</li><li id="50aa" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">给定一个人的信息(如年龄、教育程度、婚姻状况等)作为特征，预测他们的年收入是低于 5 万美元(<strong class="jp ir"> <em class="kl"> 0 </em> </strong>)还是高于 5 万美元(<strong class="jp ir"> <em class="kl"> 1 </em> </strong>)。</li><li id="9488" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">给定的电子邮件是垃圾邮件(1)还是非垃圾邮件(0)？</li></ul><p id="c26e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上述所有示例中，感兴趣的对象/类别被分配了正标签(<strong class="jp ir"> <em class="kl"> 1 </em> </strong>)。</p><p id="5417" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大多数时候，一个给定的机器学习问题是否需要<em class="kl">二元分类</em>是相当明显的。一般的经验法则是<em class="kl">二元分类帮助我们回答是(1)/否(0)问题。</em></p><p id="87d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们构建一个简单的 1 层神经网络(仅输入和输出层)，并手动求解它以获得更好的图像。<em class="kl">(我们将制作一个与我的</em> <a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener"> <em class="kl">上一篇文章</em> </a> <em class="kl">中阐述的神经网络相同的神经网络，但有一个关键的区别，</em> <strong class="jp ir"> <em class="kl">神经网络的输出被解释为概率</em> </strong> <em class="kl">而不是原始值)。</em></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi me"><img src="../Images/5dd2d6a4a747aa11b5f15ad276cda86b.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*UFKYlEHiJ68ARwpvWyWadg.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 1. Simple input-output only neural network</figcaption></figure><p id="093e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们扩展这个神经网络来揭示它的复杂性。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/394d6069d5a5a9c5bbd28852337313fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q4h7QH67_KheHeecbGAipw.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 2. Expanded neural network</figcaption></figure><p id="9101" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于那些不熟悉神经网络所有不同部分的人，我将简要介绍一下它们。(在我的<a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener">上一篇文章</a>中提供了更详细的解释)</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nc"><img src="../Images/fefa33bdd7fa157c654752af2e49f72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPabWCRU3ZN9Bss7rH5Leg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 3. Annotated Expanded Neural Network</figcaption></figure><ul class=""><li id="e4ec" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated"><strong class="jp ir">输入:<em class="kl"> x₁ </em> </strong>和<strong class="jp ir"> <em class="kl"> x₂ </em> </strong>是两个特征的输入节点，这两个特征表示我们希望我们的神经网络从中学习的示例。由于输入节点形成网络的第一层，因此它们被统称为“<strong class="jp ir"> <em class="kl">输入层</em> </strong>”。</li><li id="034d" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><strong class="jp ir">权重:<em class="kl">w₁</em></strong>&amp;<strong class="jp ir"><em class="kl">w₂</em></strong>分别代表我们与输入<strong class="jp ir"><em class="kl"/></strong>&amp;<strong class="jp ir"><em class="kl">【x₂】</em></strong><strong class="jp ir"><em class="kl">关联的权重值。</em> </strong>权重控制每个输入对下一个节点计算的影响。神经网络“学习”这些权重以做出准确的预测。最初，权重是随机分配的。</li><li id="020a" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><strong class="jp ir">线性节点(z):</strong>“<strong class="jp ir"><em class="kl">z</em></strong>”节点从所有进入它的输入中创建一个线性函数<em class="kl">，即</em><strong class="jp ir"><em class="kl">z =w₁x₁+w₂x₂+b</em></strong></li><li id="b2b7" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><strong class="jp ir">偏置:</strong><strong class="jp ir"><em class="kl">b</em></strong><strong class="jp ir"><em class="kl"/></strong>代表偏置节点。偏差节点将一个加性量插入线性函数<strong class="jp ir"> <em class="kl"> </em> </strong>节点(<strong class="jp ir"> <em class="kl"> z </em> </strong>)。顾名思义<em class="kl">，偏置会摆动输出，以便更好地与我们期望的输出</em>保持一致。偏差的值被初始化为<strong class="jp ir"> <em class="kl"> b=0 </em> </strong>并且<strong class="jp ir"> <em class="kl"> </em> </strong>也在训练阶段被学习。</li><li id="3f5f" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><strong class="jp ir"> Sigmoid 节点:</strong>这个<strong class="jp ir"> <em class="kl"> σ </em> </strong>节点，称为 Sigmoid 节点，从前面的线性节点(<strong class="jp ir"> <em class="kl"> z </em> </strong>)获取输入，并将其传递给下面的激活函数，称为<strong class="jp ir"> Sigmoid 函数</strong>(因为它是 S 形曲线)，也称为<strong class="jp ir">逻辑函数</strong>:</li></ul><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/70d612a3e805bc5c3446226a071277a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*9Opw1gREJLI9a0-VrJa2Yw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 4. Output sigmoid/logistic node</figcaption></figure><p id="2811" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Sigmoid 是神经网络中使用的许多“激活函数”之一。激活函数是非线性函数(<em class="kl">不是简单的直线</em>)。他们<em class="kl">通过扩展其维度</em>为神经网络添加非线性，反过来帮助它学习复杂的东西<em class="kl">(更多细节请参考我之前的</em> <a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener"> <em class="kl">帖子</em> </a> <em class="kl"> ) </em>。由于它是我们神经网络中的最后一个节点，它是神经网络的输出，因此被称为“输出层”。</p><p id="fc5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">一个线性节点(z)结合一个偏置节点(b)和一个激活节点</em>，如乙状结肠节点(σ)，在人工神经网络</strong> <em class="kl">中形成一个“<em class="kl">神经元”</em>。</em></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/7fbd152386da7a4d13a1c9617d3b15b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*iybo8eDQnZIB0Ep8Qqo70A.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 5. A neuron in a neural network</figcaption></figure><p id="e6fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">在神经网络文献中，假设人工神经网络中的每一个神经元都有一个线性节点及其对应的偏置，因此线性节点和偏置节点在神经网络图中没有显示</em>，如<strong class="jp ir"> <em class="kl">图 1 </em> </strong> <em class="kl">。</em>为了更深入地理解神经网络中的计算，我将在这篇博文中继续展示神经网络的扩展版本，如图<strong class="jp ir"> <em class="kl">图 2 </em> </strong>。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="2f30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在输出层使用<em class="kl">单个</em> Sigmoid/Logistic 神经元是<em class="kl">二元分类神经网络的支柱。</em> </strong>这是因为一个 Sigmoid/Logistic 函数的<strong class="jp ir"> <em class="kl">输出可以方便地解释为估计的 probability(p̂，</em> </strong> <em class="kl">读作 p-hat </em> <strong class="jp ir"> <em class="kl">)，即给定的输入属于“正”类。</em> </strong>如何？让我们再深入一点。</p><p id="6a63" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Sigmoid 函数将任何输入压缩到输出范围<strong class="jp ir"><em class="kl">0&lt;σ&lt;1</em></strong>内。因此，例如，如果我们正在创建一个基于神经网络的“cat(1) vs. not-cat(0)”检测器，给定图像作为输入示例，我们的输出层仍将是一个<em class="kl">单个</em> Sigmoid 神经元，将先前层的所有计算转换为<strong class="jp ir"> <em class="kl">、</em> </strong>简单的 0–1 输出范围。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nf"><img src="../Images/75651f5995f6b95e54a7c3f1836f407e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DOFaiENKAT2UGuTOf9EhnQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 6. Structure of the output layer of any binary classification neural network</figcaption></figure><p id="de84" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后我们可以简单地将<strong class="jp ir"><em class="kl">【p̂</em></strong>解释为<em class="kl">给定的输入图像是猫的概率是多少？</em>”，其中“猫”为阳性标签。如果<strong class="jp ir"> <em class="kl"> p̂≈0 </em> </strong>，那么<strong class="jp ir">极不可能</strong>输入图像是猫的，另一方面，<strong class="jp ir"> <em class="kl"> p̂≈1 </em> </strong>那么<strong class="jp ir">很可能</strong>输入图像是猫的。<em class="kl">简单来说，</em> <strong class="jp ir"> <em class="kl"> p̂ </em> </strong> <em class="kl">就是我们的神经网络模型在预测输入是一只猫也就是正类的时候有多自信(</em> <strong class="jp ir"> <em class="kl"> 1 </em> </strong> <em class="kl">)。</em></p><p id="d7c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这在数学上可以简单地概括为条件概率:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/036e7fe0c14c4ffa01710ff1979a9e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*aZP3cBbcwxVePaaNWzQBKw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 7. Piecewise probability equation</figcaption></figure><p id="eace" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于每个二元分类神经网络架构在输出层都有一个<em class="kl">单个 Sigmoid 神经元，如上面<strong class="jp ir"> <em class="kl">图 6 </em> </strong>所示，Sigmoid 的输出(估计概率)取决于与神经元相关联的线性节点(<strong class="jp ir"><em class="kl">z</em></strong>)<strong class="jp ir"><em class="kl"/></strong>的输出。如果线性节点的值(<strong class="jp ir"><em class="kl">)z</em></strong>)为:</em></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nh"><img src="../Images/f646bbb02d14f8e8c419cc0215c5a36e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kyw5XHJqwoDubjOblaLGDw.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 8. Sigmoid Curve interpreted as a probability distribution</figcaption></figure><ol class=""><li id="10e7" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk ni lw lx ly bi translated"><strong class="jp ir"> <em class="kl">大于零(z &gt; 0) </em> </strong>那么 Sigmoid 节点的输出是<strong class="jp ir"> <em class="kl">大于 0.5</em></strong><em class="kl">(σ(z)&gt;0.5)</em>，可以解释为“输入图像是猫的概率是<strong class="jp ir"> <em class="kl">大于</em> </strong>大于 50%”。</li><li id="64fb" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk ni lw lx ly bi translated"><strong class="jp ir"> <em class="kl">小于零(z &lt; 0) </em> </strong>那么乙状结肠节点的输出是<strong class="jp ir"> <em class="kl">小于 0.5(σ(z) &lt; 0.5) </em> </strong>，可以解释为“输入图像是猫的概率是<strong class="jp ir"> <em class="kl">小于</em> </strong>小于 50%”。</li><li id="ddba" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk ni lw lx ly bi translated"><strong class="jp ir"> <em class="kl">等于零(z=0) </em> </strong>那么 Sigmoid 节点<strong class="jp ir"> <em class="kl">的输出等于 0.5(σ(z)=0.5) </em> </strong>，也就是说“输入图像是猫的概率是<strong class="jp ir"> <em class="kl">正好是</em> </strong> 50%”。</li></ol><p id="f456" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们知道了我们的神经网络中的每样东西代表什么，让我们看看我们的二元分类神经网络在给定以下数据集的情况下执行什么计算:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/f6e3dd6d5e11f2f4af1d16ef44a90045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*AZoMOk30bm5IpLrykhDAiQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 9. AND logic gate data and plot</figcaption></figure><p id="4af0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的数据代表 AND 逻辑门，其中只有当两个输入都是<strong class="jp ir"> <em class="kl"> x₁=1 </em> </strong>和<strong class="jp ir"> <em class="kl"> x₂=1 </em> </strong>时，输出才被赋予一个正标签(<strong class="jp ir"> <em class="kl"> 1 </em> </strong>)，所有其他情况都被赋予一个负标签(<strong class="jp ir"> <em class="kl"> 0 </em> </strong>)。数据的每一行都代表了我们希望我们的神经网络从中学习然后进行分类的一个例子。我还在二维平面上绘制了这些点，以便于可视化<em class="kl">(红点表示类(</em> <strong class="jp ir"> <em class="kl"> y </em> </strong> <em class="kl">)为 0 的点，绿色十字表示类为</em><strong class="jp ir"><em class="kl">1</em></strong><em class="kl">)</em>的点)。这个数据集也恰好是<strong class="jp ir"> <em class="kl">线性可分的</em> </strong>，即我们可以画一条直线来将阳性标记的样本与阴性标记的样本分开。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/cfa5d1c26fe2d85b9f58c1d6061d60bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*t1mE0L9LVyxRpT-JLY0_yQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 10. Showing linear separation of AND gate data points</figcaption></figure><p id="9564" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面显示的蓝线，称为<strong class="jp ir"><em class="kl"/></strong><em class="kl">，</em>把我们两个阶级分开。线上是我们的阳性标记示例(绿色十字)，线下是我们的阴性标记示例(红色十字)。在幕后，这条蓝线是由<strong class="jp ir"> <em class="kl"> z </em> </strong> <em class="kl">(线性函数)节点</em>形成的。我们稍后将看到神经网络如何学习这个<em class="kl">决策边界</em>。</p><p id="ca07" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">像我之前的博文一样，首先，我们将执行<strong class="jp ir"> <em class="kl">随机梯度下降</em> </strong>，这是使用来自我们训练数据的一个例子来训练神经网络。然后，我们将把我们从随机过程中学到的知识推广到<strong class="jp ir"> <em class="kl">批量梯度下降</em> </strong> ( <em class="kl">首选方法</em>)，在这里，我们使用训练数据中的所有示例来训练神经网络。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="b021" class="kn ko iq bd kp kq nl ks kt ku nm kw kx ky nn la lb lc no le lf lg np li lj lk bi translated">随机梯度下降</h1><p id="6b40" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">神经网络中的计算从左向右移动，这被称为<strong class="jp ir"> <em class="kl">正向传播。</em> </strong>让我们来看一下当提供了<em class="kl">仅第一个训练示例</em> <strong class="jp ir"> <em class="kl"> x₁ = 0 </em> </strong>和<strong class="jp ir"> <em class="kl"> x₂ = 0 </em> </strong>时，我们的神经网络将执行的所有正向计算。此外，我们将随机初始化权重为<strong class="jp ir"> <em class="kl"> w₁=0.1 </em> </strong>和<strong class="jp ir"> <em class="kl"> w₂=0.6 </em> </strong>，偏差为<strong class="jp ir"> <em class="kl"> b=0。</em> </strong></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nq"><img src="../Images/41f30e71610fb037daec2e977b91d1d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0NOAS-pP-nrEa5U8-0AX3w.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 11. Forward propagation on the first example from AND gate table</figcaption></figure><p id="9634" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，神经网络的预测是<strong class="jp ir">p̂=0.5.的<em class="kl"/> </strong> <em class="kl">回想一下，这是一个二进制分类神经网络，</em><strong class="jp ir"><em class="kl">【p̂】</em></strong>这里代表输入例子的估计概率，具有特征的<strong class="jp ir"><em class="kl"/></strong>&amp;<strong class="jp ir"><em class="kl">【x₂=0</em></strong>，属于正类(<strong class="jp ir"> <em class="kl"> 1 </em> </strong>)。我们的神经网络目前认为有一个<strong class="jp ir"> <em class="kl"> 0.5 </em> </strong>(或<strong class="jp ir"> <em class="kl"> 50% </em> </strong>)的几率第一个训练样本属于正类(<em class="kl">从概率方程回忆这等同于</em> <strong class="jp ir"> <em class="kl"> P(1∣ x₁，x₂；w,b)=p̂=0.5 </em> </strong>)。</p><p id="9f4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">呀！这有点可怜😕，尤其是因为<em class="kl">负</em>标签与第一个示例相关联，即<strong class="jp ir"> <em class="kl"> y=0。</em> </strong>估计概率应该在<strong class="jp ir"> <em class="kl"> p̂≈0 左右；</em> </strong> <em class="kl">第一个例子属于积极阶层的可能性应该很小，这样一来，</em> <strong class="jp ir"> <em class="kl"> </em> </strong> <em class="kl">属于消极阶层的几率就高</em>(即<strong class="jp ir"> <em class="kl"> P(0∣ x₁、x₂；</em> </strong> ) <em class="kl">。</em></p><p id="2b78" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你读过我以前的文章，那么你知道在这一点上，我们需要一个损失函数来帮助我们。那么，我们应该用什么损失函数来告诉一个<strong class="jp ir"> <em class="kl">二元分类</em>神经网络</strong>来修正它的估计概率呢？进来的是<strong class="jp ir"> <em class="kl">二进制</em> </strong> <strong class="jp ir"> <em class="kl">交叉熵损失函数</em> </strong>来解救我们。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h2 id="867c" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">二元交叉熵损失函数</h2><p id="aa5e" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated"><em class="kl">注:在大多数编程语言中“</em><strong class="jp ir"><em class="kl">log</em></strong><em class="kl">”是自然对数(log 以-</em><strong class="jp ir"><em class="kl">e</em></strong><em class="kl">)，在数学上表示为“</em> <strong class="jp ir"> <em class="kl"> ln </em> </strong> <em class="kl">”。为了代码和等式之间的一致性，请将“</em><strong class="jp ir"><em class="kl">log</em></strong><em class="kl">”视为自然对数，而不是“</em><strong class="jp ir"><em class="kl">log₁₀</em></strong><em class="kl">”(以 10 为底的 log)。</em></p><p id="4bd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">二元交叉熵(BCE)损失函数定义如下:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi od"><img src="../Images/06a42657147a0ed63b1e8632782c5d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DP8zAOMvMQeVQOcqrfgdqg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 12. The Binary Cross-Entropy Loss function</figcaption></figure><p id="e4ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">所有损失函数本质上都告诉我们，我们的预测输出离我们的期望输出有多远，仅举一个例子</em><strong class="jp ir"><em class="kl"/></strong><em class="kl">。简单地说，损失函数计算预测值和实际值之间的误差</em>。考虑到这一点，当训练示例的关联标签为<strong class="jp ir"> <em class="kl"> y=1 </em> </strong> <em class="kl">【正】</em>时，二元交叉熵(BCE)损失函数计算不同的损失，并且当标签为<strong class="jp ir"> <em class="kl"> y=0 </em> </strong> <em class="kl">【负】</em>时，计算不同的损失。让我们看看:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi oe"><img src="../Images/a3d8cccac30f0df8ba93624dfb30683b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wGKsNaxjRA07yaSxaTwdcg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 13. Binary Cross-Entropy Loss function broken down into piecewise equation</figcaption></figure><p id="fe57" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在很明显<strong class="jp ir"> <em class="kl">图 12 </em> </strong>中的 BCE 损失函数只是分段方程<strong class="jp ir">的优雅压缩版本。</strong></p><p id="a47a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们绘制上面的分段函数来可视化下面发生的事情。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi of"><img src="../Images/79a8a6a3ef466035ef8a61bb9ce4fd3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QohPFy6wBfbjK5VUWxlyoA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 14. Visualizing the BCE Loss function for each class</figcaption></figure><p id="3ec7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，BCE 损失函数捕捉了当估计的概率相对于训练示例的标签完全错误时，神经网络应该支付高惩罚(<strong class="jp ir"> <em class="kl">损失→∞ </em> </strong>)的直觉。另一方面，当估计的概率相对于训练示例的标签是正确的时，损失应该等于零(<strong class="jp ir"> <em class="kl">损失=0 </em> </strong>)。简而言之，BCE 损失应该仅在两种情况下等于零:</p><ol class=""><li id="9937" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk ni lw lx ly bi translated">如果该示例被正标记(y=1)，则神经网络模型应该<em class="kl">完全确定该示例</em> <strong class="jp ir"> <em class="kl">属于正类</em> </strong> <em class="kl">，即</em> <strong class="jp ir"> <em class="kl"> p̂=1.</em>T89】</strong></li><li id="4161" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk ni lw lx ly bi translated">如果该示例被负标记(y=0)，则神经网络模型应该是 c <em class="kl">完全确定该示例</em> <strong class="jp ir"> <em class="kl">不属于正的</em> </strong> <em class="kl">类，即</em> <strong class="jp ir"> <em class="kl"> p̂=0.</em> </strong></li></ol><p id="18bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">在神经网络中，损失函数的梯度/导数决定了是增加还是减少神经网络的权重和偏差。</em>让我们看看二元交叉熵(BCE)损失函数的导数是什么样子的:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi og"><img src="../Images/014a5d2a4c132bb3ff3ccf25498f0d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rqwspWXDw0_h1Mug-2Wacg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 15. The derivative of the Binary Cross-Entropy Loss function</figcaption></figure><p id="14d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们还可以将导数分解成分段函数，并可视化其效果:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi oh"><img src="../Images/ee40df292f13be6775cd3f5f60613053.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9NgZWECrZFzeTsNGD4T48g.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 16. Breaking down the derivative of the loss function and visualizing the gradient</figcaption></figure><p id="e9e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">正的导数意味着减少权重，负的导数意味着增加权重。</em> </strong> <em class="kl"> </em> <strong class="jp ir"> <em class="kl">坡度越陡，预测越不正确。</em> </strong>让我们先拿<strong class="jp ir"><em class="kl"/></strong><strong class="jp ir"><em class="kl"/></strong>一会儿来确定我们理解了这种说法:</p><ul class=""><li id="4709" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated"><em class="kl">如果梯度为</em> <strong class="jp ir"> <em class="kl">负</em> </strong>，这将意味着我们正在查看第一条损失曲线，其中示例的实际标签为<em class="kl">正(y=1) </em>。将损耗降低到零的唯一方法是沿着斜率(梯度)的相反方向移动，<em class="kl">从负到正。</em>因此，我们需要<em class="kl">增加权重和偏差</em>以便<strong class="jp ir"><em class="kl">z = w₁x₁+w₂x₂+b&gt;0</em></strong><em class="kl"><em class="kl">图 8</em><em class="kl">)</em><strong class="jp ir"><em class="kl"/></strong>并依次<strong class="jp ir"> <em class="kl"> </em> </strong> <em class="kl">估计归属概率</em>T75】</em></li><li id="99fa" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">同样，当梯度为<strong class="jp ir"> <em class="kl">正</em> </strong>时，我们看到的是第二条损耗曲线，该示例的实际标签为<em class="kl">负(y=0)。</em>将损耗降至零的唯一方法是再次向斜率(梯度)的相反方向移动，这一次<em class="kl">从正到负。</em>在这种情况下，<em class="kl">我们将需要减少权重和偏差，以便</em><strong class="jp ir"><em class="kl">z = w₁x₁+w₂x₂+b&lt;0</em></strong>并因此<em class="kl">估计属于阳性类别</em> <strong class="jp ir"> <em class="kl"> p̂≈σ(z)≈0.的概率</em>T95】</strong></li></ul><p id="a2ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止，对 BCE 损失的解释足以满足所有意图和目的，但你们中的好奇者可能想知道这个损失函数是从哪里来的，为什么不像在<a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener">上一篇文章</a>中那样使用均方误差损失函数呢？稍后将详细介绍。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="e614" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们知道了损失函数的目的和二元交叉熵损失函数如何工作，让我们在我们当前的例子(<strong class="jp ir"> <em class="kl"> x₁ = 0 </em> </strong>和<strong class="jp ir"> <em class="kl"> x₂ = 0 </em> </strong>)上计算 BCE 损失，对于这个例子，我们的神经网络估计属于正类的概率是<strong class="jp ir"><em class="kl">【p̂=0.5</em></strong>，而它的标签(<strong class="jp ir"> <em class="kl"> y </em> </strong>)是<strong class="jp ir"> <em class="kl"> y=0</em></strong></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/406e50f7553823dce3ba6158cc7ea6a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*UyHYNarnHLplBt11EJStrQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 17. Loss on the first example</figcaption></figure><p id="7551" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">损失约为<strong class="jp ir"> <em class="kl"> 0.693 </em> </strong>(四舍五入到小数点后 3 位)。<em class="kl">我们现在可以使用 BCE 损失函数的导数来检查我们是否需要增加或减少权重和偏差，使用称为</em> <strong class="jp ir"> <em class="kl">反向传播的过程；</em> </strong> <em class="kl">它与正向传播相反，我们从输出到输入反向跟踪</em>。反向传播允许我们计算出神经网络的每个部分造成了多少损失，然后我们可以相应地调整神经网络的这些部分。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="74f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如我之前的<a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener">帖子</a>所示，我们将使用以下图形技术将梯度从输出层传播回神经网络的输入层:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/9ff759382894655e2438b62645887f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*jmfVit0CiT7GdiJo1eeW6w.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 18. Gradient Flow</figcaption></figure><p id="d640" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在每个节点，我们只计算局部梯度(该节点的偏导数)。然后，在反向传播期间，当我们从上游接收梯度的数值时，我们将上游梯度与局部梯度相乘，并将它们传递到它们各自连接的节点。 <em class="kl">这是对</em> <strong class="jp ir"> <em class="kl">链式法则</em> </strong> <em class="kl">的概括，来自微积分。</em></p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="2502" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们一步一步地回顾反向传播:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ok"><img src="../Images/c7fb9e87374665cca35e07506363023e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oi37HQci1fhDKEGTe142jg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 19.a. Backpropagation on the 1ˢᵗ example</figcaption></figure></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="1461" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一次计算，我们需要 Sigmoid 函数的导数，它在红色节点形成局部梯度。Sigmoid 函数的导数(在我的<a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener">上一篇文章</a>中有详细推导):</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ol"><img src="../Images/412f43029a045066f36e90a5604fb7c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3-rpK6HHIU3fiIgyYw_MA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 20. The derivative of the sigmoid function</figcaption></figure></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="8748" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们使用 Sigmoid 节点的导数，并进一步反向传播梯度:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi om"><img src="../Images/57a439b64cc1c04333667d220ef67b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k3isF0vbsadAiSVWGKpq5Q.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 19.b. Backpropagation on the 1ˢᵗ example</figcaption></figure><p id="118e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度不应传播回输入节点(即红色箭头不应朝向绿色节点),因为我们不想改变我们的输入数据，我们只想改变与它们相关的权重。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi on"><img src="../Images/971dd3b54064b871467d30e710250699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ZMCuXpMiol_eEjjvS4SVA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 19.c. Backpropagation on the 1ˢᵗ example</figcaption></figure><p id="eda2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们可以通过执行<strong class="jp ir"> <em class="kl">梯度下降来更新神经网络的参数(权重和偏差)。</em>T3】</strong></p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h2 id="a870" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">梯度下降</h2><p id="d5fa" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">梯度下降</em> </strong>是通过向梯度<em class="kl">的负方向移动，即从倾斜区域向更平坦的区域</em>移动来调整神经网络的参数。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi oo"><img src="../Images/3696e639564a8a71eb9d6bb444240f4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1xqczfI7juygoLkJSnWQNQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 21. Visualizing Gradient Descent on Binary Cross-Entropy Loss function</figcaption></figure><p id="d65a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降的一般方程为:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi op"><img src="../Images/734a0ade087475b81d15fb77519a0c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*XeOLDBlfQoQqN3seB8AVXA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 22. The general equation for <strong class="bd oq">gradient descent</strong></figcaption></figure><p id="f9a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">学习率，</em> <strong class="jp ir"> <em class="kl"> α </em> </strong> <em class="kl">(读作 alpha) </em> <strong class="jp ir"> <em class="kl">，</em> </strong>用于控制损耗曲线下降的步长(<strong class="jp ir"> <em class="kl">图 21 </em> </strong>)。学习率是神经网络的<strong class="jp ir"> <em class="kl">超参数</em> </strong>，这意味着它不能通过梯度的反向传播来学习，必须由神经网络的创建者来设置，理想情况下是在一些实验之后。更多关于学习率影响的信息，你可以参考我之前的<a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener">帖子</a>。</p><p id="9b5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，梯度下降的步长(蓝色箭头)越来越小，这是因为当我们从倾斜区域向更平坦的区域移动时，在最小点附近，梯度的幅度也会减小，从而导致步长逐渐变小。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="35a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将<em class="kl">学习率</em> ( <strong class="jp ir"> <em class="kl"> α </em> </strong>)设为<strong class="jp ir"> <em class="kl"> α=1。</em>T41】</strong></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi or"><img src="../Images/240782dbc4d556a084a3d771069a1100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*uIvPk8fSSD_Xh97a7FrhmA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 23. Calculating new weights and bias</figcaption></figure><p id="94af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经更新了权重和偏差(实际上我们只能在这次训练迭代中更新我们的偏差),让我们对同一个例子<strong class="jp ir"> </strong>和<strong class="jp ir">进行<strong class="jp ir">正向传播</strong>计算新的损失</strong>来检查我们是否做了正确的事情。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nq"><img src="../Images/eb9395558986b821ca454834a8bb76bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IpaR0sfWGfWa_LuN4hdEFQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 24. Forward propagation with the updated bias on 1ˢᵗ example</figcaption></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi os"><img src="../Images/8aafc724022847c7bf07ab32d5c61ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*Trimk1j8recWUlUQYYk52A.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 25. New loss after training on the 1ˢᵗ example</figcaption></figure><p id="4d03" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，属于正类(<strong class="jp ir"> <em class="kl"> p̂ </em> </strong>)的 1ˢᵗ示例的估计概率从 0.5 下降到大约<strong class="jp ir"> <em class="kl"> 0.378 </em> </strong>(四舍五入到 3 d.p)，因此，BCE 损耗也减少了一点，从 0.693 下降到大约<em class="kl"/>(到 3 d.p)。</p><p id="de31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止，我们已经执行了<strong class="jp ir"> <em class="kl">随机梯度下降。</em> </strong>我们仅使用了一个示例(<strong class="jp ir"> <em class="kl"> x₁=0 </em> </strong>和<strong class="jp ir"> <em class="kl"> x₂=0 </em> </strong>)，来自我们的四个示例的与门数据集，以执行单个训练迭代(<em class="kl">每个训练迭代是前向传播，计算损耗，随后是反向传播，并通过梯度下降更新权重</em>)。</p><p id="1d6c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以继续通过一次从一个例子中学习来更新权重，但理想情况下，我们希望一次从多个例子中学习，并减少所有例子中的损失。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="ca78" class="kn ko iq bd kp kq nl ks kt ku nm kw kx ky nn la lb lc no le lf lg np li lj lk bi translated">批量梯度下降</h1><p id="8b50" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">在批量梯度下降</em> </strong> <em class="kl"> ( </em>也称为<em class="kl">完全批量梯度下降)中，我们在每次训练迭代中使用数据集中的所有训练样本。(如果由于某种原因，批量梯度下降是不可能的，例如，所有训练数据的大小太大而不适合 RAM 或 GPU，我们可以在每次训练迭代中使用数据集的子集，这被称为</em> <strong class="jp ir"> <em class="kl">小批量梯度下降</em> </strong> <em class="kl">)。)</em></p><p id="b8fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">一批只是一个充满训练样本的向量/矩阵。</strong></p><p id="8ac5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们继续处理多个例子之前，我们需要定义一个<strong class="jp ir"> <em class="kl">成本函数。</em> </strong></p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h2 id="07c2" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">二元交叉熵代价函数</h2><p id="bce2" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">对于批量梯度下降，我们需要调整<em class="kl">二元交叉熵(BCE)损失函数</em>以适应批量中的所有样本，而不仅仅是一个样本。这种调整后的<em class="kl">损失</em>函数称为<strong class="jp ir"> <em class="kl">代价</em> </strong>函数<em class="kl">(在神经网络文献中也用字母</em> <strong class="jp ir"> <em class="kl"> J </em> </strong> <em class="kl">表示，有时也称为</em> <strong class="jp ir"> <em class="kl">目标函数</em> </strong> <em class="kl"> ) </em>。</p><p id="e025" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">不是计算一个示例的<em class="kl">损失</em>,<em class="kl">成本</em>函数计算批次中所有示例的<em class="kl">平均</em> <em class="kl">损失</em>。</strong></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/39daa197b124b28e3a9db1ecd84659ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*xPkZx-zIew40f-7KKXrrQg.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 26. Binary Cross-Entropy <strong class="bd oq">Cost</strong> function</figcaption></figure><p id="a517" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当执行批量梯度下降(或小批量梯度下降)时，我们对成本函数而不是损失函数求导。接下来，我们将看到如何对二元交叉熵成本函数求导，使用一个简单的例子，然后从那里推广。</p><h2 id="72c8" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">二元交叉熵代价函数的导数</h2><p id="3684" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">在矢量化形式中，我们的 BCE 成本函数如下所示:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ou"><img src="../Images/d5c6452668ca541c9160128ac7ad22ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dkdPrkC9IXhZxSSif4u0mg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 27. Cost function taking vectorized inputs</figcaption></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ov"><img src="../Images/cc46d9ade2d16b2a363c16dd378bd744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dJvkTbwWJzu1cCkKB5d_LA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 28. Calculation of Cost on a simple vectorized example</figcaption></figure><p id="be20" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如所料，成本只是两个示例的平均损失，但我们所有的计算都是矢量化的，允许我们一次性计算一个批次的二元交叉熵成本。我们更喜欢在神经网络中使用矢量化计算，因为计算机硬件(CPU 和 GPU)更适合矢量化形式的批量计算。(<em class="kl">注意:如果我们在批次中只有一个示例，BCE 成本将只是计算 BCE 损失，就像我们之前经历的随机梯度下降示例一样</em></p><p id="ba80" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，让我们推导这个矢量化成本函数的偏导数。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ow"><img src="../Images/809acdd5d88f65c8077b07f502303bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LA6mi6WqXD5-f3EdhNQhsQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 29. Calculation of Jacobian on a simple example</figcaption></figure><p id="578d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由此，我们可以推广<em class="kl">二元交叉熵代价</em>函数的偏导数。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/11dc973fb804f4f20895e5b96a10294a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*obbhqCPQYY98C19fjksWIg.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 30. The generalized derivative of the Cost Function</figcaption></figure><p id="53fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">成本</em>函数的一个非常重要的结果是<em class="kl">因为它计算一批样本的平均损耗，它</em>也<em class="kl">计算一批样本的梯度平均值，这有助于计算出一个噪声更小的总方向，其中所有样本的损耗减少</em>。相反，<em class="kl">随机梯度下降(只有一个例子的批次)给出了一个非常嘈杂的梯度估计</em>，因为它在每个训练迭代中只使用一个例子来引导<em class="kl">梯度下降</em>。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="1e15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于矢量化(批处理)计算，我们需要调整神经网络的线性节点(<strong class="jp ir"> <em class="kl"> z </em> </strong>)，以便它接受矢量化输入并使用成本函数而不是损失函数，也是出于同样的原因。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi oy"><img src="../Images/369c859c6696bee25ace24b63c7a42dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MP-k6HSCW7f-wajZYnnlAQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 31. Vectorized implementation of Z node</figcaption></figure><p id="084e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl"> Z </em> </strong>节点现在计算适当大小的权重矩阵(<strong class="jp ir"> <em class="kl"> W </em> </strong>)和训练数据(<strong class="jp ir"> <em class="kl"> X </em> </strong>)之间的<strong class="jp ir"> <em class="kl">点积</em> </strong>。<strong class="jp ir"> <em class="kl"> Z </em> </strong>节点的输出现在也是一个矢量/矩阵。</p><p id="ada0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们可以设置我们的数据(<strong class="jp ir"> <em class="kl"> X，W，b &amp; Y </em> </strong>)进行矢量化计算。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi oz"><img src="../Images/69034e7d80a11233111d648c05bdd172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P1RuHQQijorrnSEvOPyoew.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 32. Data setup for batch(vectorized) processing</figcaption></figure><p id="a8fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在终于准备好使用<strong class="jp ir"> Xₜᵣₐᵢₙ </strong>、<strong class="jp ir"> Yₜᵣₐᵢₙ </strong>、<strong class="jp ir"> W、</strong>和<strong class="jp ir"> b </strong>执行正向和反向传播。</p><p id="0fa8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">(注:以下所有结果均四舍五入至小数点后 3 位，仅为简洁起见)</em></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pa"><img src="../Images/b4bc517b567e4c2cbdb0f24bf3de1b86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yEabPaa6eeGbr6yeDFJRPQ.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pb"><img src="../Images/8e95f09a0831d63676a460808a067e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BTY4FF6GW3Wezburjw7iNg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 33. Vectorized batch forward propagation on AND gate dataset</figcaption></figure><p id="d908" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过向量化计算，我们进行了前向传播；一次性计算批次中每个示例的所有估计概率。</p><p id="fa1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们可以计算这些输出估计概率的 BCE 成本(P <strong class="jp ir"> <em class="kl">、̂ </em> </strong>)。<em class="kl">(下面，为了便于阅读，我用蓝色突出显示了成本函数中计算正例损失的部分，用红色突出显示了负例损失的部分)</em></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pc"><img src="../Images/56b0e8b79f812f5b9dfcaced9640cce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZyRhT3UeFG77Nbwe9fKAPA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 34. Calculation of Cost on the OR gate data(highlighted Loss in red for negative examples and blue for positive examples)</figcaption></figure><p id="fcf3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，用我们当前的权重和偏差计算的<em class="kl">成本</em>大约是<em class="kl"/><strong class="jp ir">0.720</strong>。我们现在的目标是使用反向传播和梯度下降来降低这个<em class="kl">成本</em>。让我们一步一步地进行反向传播。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pd"><img src="../Images/eb03445206fd625f373ff17240d48f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*en5wSAFUhfJmYz1_ZdLBPw.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pe"><img src="../Images/592fbf8edb3d3c5b888b85e30ce5c028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DB9YlWpfMEme4ZdXTW6G1Q.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 35.a. Vectorized Backpropagation on AND gate dataset</figcaption></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pf"><img src="../Images/447bf26e5635489c7c5460082b664ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H-nWRMRo3ceEb3cvXo7fIg.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pg"><img src="../Images/1369e7f23e4e86ea4333d150d3a2f9d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aIgLAwknksAWRySCymhiwg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 35.b. Vectorized Backpropagation on AND gate dataset</figcaption></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pf"><img src="../Images/f262f9dfc472fa70d502bac706ca096c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-o-3XBVQZmtalAue1zpRg.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ph"><img src="../Images/33ebcf64c24e82c509da2c2636552d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gycyllQH0D647LEgHDI2jg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 35.c. Vectorized Backpropagation on AND gate dataset</figcaption></figure><p id="cde7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就像这样，我们使用矢量化计算，一次性计算了整批训练样本的成本函数的所有梯度。我们现在可以执行<em class="kl">梯度下降</em>到<em class="kl">T11】更新权重和偏差。</em></p><p id="a510" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">(对于那些对上一个反向传播步骤中∂Cost/∂W 和∂Cost/∂b 是如何计算的感到困惑的人，请参考我之前的博客</em><a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener"><em class="kl"/></a><em class="kl">，在那里我分解了这个计算，更具体地说，为什么点积的导数会导致转置矩阵)</em></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/2a8dc8125b8f839d90ebcfb740c2e356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*7AbLn8mesmZmb-YS4zvtww.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 36. Updating weights and bias using <strong class="bd oq">batch gradient descent</strong></figcaption></figure><p id="911b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了检查我们是否做了正确的事情，我们可以使用新的权重和偏差来执行另一个<strong class="jp ir"> <em class="kl">前向传播</em> </strong>和<strong class="jp ir">计算新的成本。</strong></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pi"><img src="../Images/d79b2a56463dfd6b924b048f4c65f453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IsM6GFTMscMMGk3Ln2_qvg.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pj"><img src="../Images/f95025843be11632c5dcd69580fbe892.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bveJ07uTNcMQn8d9hRsq_Q.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 37. Forward propagation with updated weights and bias</figcaption></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi pk"><img src="../Images/4252bccb69f3a8584000b0fd5a147ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*evChVnsmaaE7y_FHd8VPUg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 38. New Cost(highlighted Loss in red for negative examples and blue for positive examples)</figcaption></figure><p id="78e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过一次训练迭代，我们已经将<em class="kl">二进制交叉熵成本</em>从 0.720 降低到大约<strong class="jp ir"> 0.618。</strong>我们将需要执行多次训练迭代，然后才能收敛到良好的权重和偏差值，从而实现整体较低的 BCE <em class="kl">成本</em>。</p><p id="d22e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">在这一点上，如果您想试一试并自己执行下一个反向传播步骤，作为一个练习，下面是成本 w.r.t 权重(</em> <strong class="jp ir"> <em class="kl"> W </em> </strong> <em class="kl">)和偏差(</em> <strong class="jp ir"> <em class="kl"> b </em> </strong> <em class="kl">)的近似梯度，您应该得到(四舍五入到 3 d.p): </em></p><ul class=""><li id="4732" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated"><strong class="jp ir"><em class="kl">【∂cost/∂w =[-0.002，0.027】</em></strong></li><li id="84cb" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><strong class="jp ir"><em class="kl">∂cost/∂b =【0.239】</em></strong></li></ul><p id="cbeb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在大约 5000 个历元之后(当神经网络在一次训练迭代中遍历所有训练示例时，一个历元完成)，成本稳定地降低到大约<strong class="jp ir"> <em class="kl"> 0.003 </em> </strong>，我们的权重稳定到大约<strong class="jp ir"><em class="kl">W =【10.678，10.678】，</em> </strong>偏差稳定到大约<strong class="jp ir"><em class="kl">b =[-16.1818]</em> </strong>我们通过下面的<em class="kl">成本曲线</em>看到，网络已经收敛到一组良好的参数(<em class="kl">即</em><strong class="jp ir"><em class="kl">W</em></strong>&amp;<strong class="jp ir"><em class="kl">b</em></strong>):</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/f3238b404a0647dcac298cbafaf30295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*nBMtRIkBxu6RoVCV_QeCRQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 39. Plotting the Cost Curve(left) and the Decision Boundary(right)</figcaption></figure><p id="a565" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">成本曲线</strong>(或学习曲线)是神经网络模型随时间的表现。它是每隔几个训练迭代(或时期)后绘制的成本。请注意<em class="kl">成本</em>最初下降的速度有多快，然后逐渐接近，回想一下<strong class="jp ir"> <em class="kl">图 21 </em> </strong>这是因为最初梯度的幅度很高，但是随着我们下降到接近最小<em class="kl">成本</em>的较平坦区域，梯度的幅度下降，并且进一步的训练仅略微改善了神经网络参数。</p><p id="33e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在神经网络已经训练了 5000 个时期之后，Xₜᵣₐᵢₙ上的预测输出概率(<strong class="jp ir"><em class="kl">【p̂】</em></strong>)是:</p><pre class="mf mg mh mi gt pm pn po pp aw pq bi"><span id="269a" class="nr ko iq pn b gy pr ps l pt pu">[[9.46258077e-08,  4.05463814e-03,  4.05463814e-03, 9.94323194e-01]]</span></pre><p id="067a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来分解一下:</p><ol class=""><li id="f11a" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk ni lw lx ly bi translated">对于<em class="kl"> x₁=0，x₂=0，</em>预测产量为<strong class="jp ir"><em class="kl">p̂≈9.46×10⁻⁸≈0.0000000946</em></strong></li><li id="0335" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk ni lw lx ly bi translated">对于<em class="kl"> x₁=0，x₂=1 </em>预测产量为<strong class="jp ir">t23】p̂≈4.05×10⁻≈0.00405T25】</strong></li><li id="a2f9" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk ni lw lx ly bi translated">对于<em class="kl"> x₁=1，x₂=0 </em>预测产量为<strong class="jp ir"><em class="kl">p̂≈4.05×10⁻≈0.00405</em></strong></li><li id="5760" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk ni lw lx ly bi translated">对于<em class="kl"> x₁=1，x₂=1 </em>预测产量为<strong class="jp ir"><em class="kl">p̂≈9.94×10⁻≈0.994</em></strong></li></ol><p id="3a55" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想一下，标签是<strong class="jp ir"><em class="kl">y =【0，0，0，1】。因此，</em>仅对于最后一个示例，神经网络有 99.4%的置信度认为它属于正类，对于其余的，它的置信度小于 1%</strong>。还有，还记得<strong class="jp ir"> <em class="kl">图 7 </em> </strong>的概率方程吗？<strong class="jp ir"><em class="kl"/></strong><em class="kl"/><strong class="jp ir"><em class="kl">p(0)=1-p̂</em></strong>，所以<strong class="jp ir"> <em class="kl"> </em> </strong>的预测概率(<strong class="jp ir"><em class="kl"/></strong>)证实我们的神经网络知道它在做什么👌。</p><p id="a419" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">既然我们知道神经网络预测的<strong class="jp ir"> <em class="kl">概率</em> </strong>是正确的，我们需要定义<strong class="jp ir"><em class="kl"/></strong>何时应该是<strong class="jp ir"> <em class="kl"> 1 </em> </strong>以及何时应该是<strong class="jp ir"> <em class="kl"> 0 </em> </strong> <em class="kl">，即根据这些概率</em>对示例进行分类。为此，我们需要定义一个<strong class="jp ir"> <em class="kl">分类阈值(</em> </strong>也叫<strong class="jp ir"> <em class="kl">决策阈值)。</em> </strong>那是什么？让我们开始吧</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h2 id="cbc9" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">分类阈值</h2><p id="c58d" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">在二元分类任务中，<strong class="jp ir"><em class="kl"/></strong><strong class="jp ir"><em class="kl">常见的是，如果估计的 probability(p̂大于某个阈值，则将神经网络的所有预测分类到正类(1)，并且类似地，如果估计的概率低于该阈值，则分类到负类(0)。</em> </strong></p><p id="ef7d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这可以用数学方法写成如下:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/6ea79276beb816890063b1c96ba375fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*V9zoxPREImIXMZkfRfDiZA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 40. Class prediction model</figcaption></figure><p id="082b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">阈值的值定义了我们的模型在将输入分配给阳性类别</em>时的严格程度。假设如果阈值是<strong class="jp ir"> <em class="kl"> thresh=0，</em> </strong>那么所有的输入实例将被分配到正类，即预测类<strong class="jp ir"><em class="kl">【ŷ】</em></strong>将总是<strong class="jp ir"><em class="kl"/></strong>。类似地，如果<strong class="jp ir"> <em class="kl"> thresh=1 </em> </strong>，那么所有的输入实例将被分配给负类，即预测类<strong class="jp ir"><em class="kl">【ŷ】</em></strong>将始终是<strong class="jp ir"> <em class="kl"> ŷ=0.</em> </strong> <em class="kl">(回想一下，sigmoid 激活函数在两端渐近线，因此它可能非常接近 0 或 1，但绝不会完全输出 0 或 1) </em></p><p id="571a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Sigmoid/Logistic 函数为我们提供了一个自然的阈值。回想一下前面的<strong class="jp ir"> <em class="kl">图 8 </em> </strong>。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/6c8e37dcb77688aaa2709cd425167545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*2vClvEzYB2N2jA1RttS1Bw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 41. The natural threshold value from the sigmoid function</figcaption></figure><p id="ebe2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，利用自然阈值<strong class="jp ir"> <em class="kl"> 0.5 </em> </strong>，可以如下预测类别:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi px"><img src="../Images/be96638c67998443aa6e7839a16d9c19.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*DcDZW-gJV8pgDipC_PSsdg.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 42. Class prediction model with <strong class="bd oq">thresh=0.5</strong></figcaption></figure><p id="9a48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们如何解读这一点？嗯如果神经网络至少有 50%<strong class="jp ir"><em class="kl"/></strong>(<strong class="jp ir"><em class="kl">0.5</em></strong>)的置信度比输入属于正类(<strong class="jp ir"> <em class="kl"> 1 </em> </strong>)那么我们就把它分配给正类(<strong class="jp ir"><em class="kl">【ŷ=1</em></strong>)，否则我们就把它分配给负类(<strong class="jp ir"><em class="kl">ŷ=0<em class="kl"/></em></strong></p><p id="084f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想一下我们如何在<strong class="jp ir"> <em class="kl">图 10 </em> </strong>中预测，神经网络可以通过绘制一条线来分隔阳性类别(绿色十字)和阴性类别(红色十字)来分隔与门数据集中的两个类别。<strong class="jp ir"> </strong>嗯，<strong class="jp ir">那条线的<em class="kl">位置是由我们的阈值定义的。</em></strong>我们来看看:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ou"><img src="../Images/7d4fee50a27f7def333cd7d9494fc622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YG0b-XAtrsh04ROVpFsqDg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 43. Deconstructing the inequality equation for class prediction</figcaption></figure><p id="b51b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想训练后我们的权重和偏差分别收敛到大约<strong class="jp ir"><em class="kl">W =【10.678】</em></strong>和<strong class="jp ir"> <em class="kl"> b = [-16.186] </em> </strong>。我们把这些代入上图<strong class="jp ir"> <em class="kl">图 43 </em> </strong>中导出的不等式。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi py"><img src="../Images/3b23d20b46a643f307b0d8b6d3912998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l1_1zCF_6M65fO8iDMp1Tg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 44. Plugging in values into the inequality</figcaption></figure><p id="04fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，认识到这个不等式给我们一个等式，一条线把我们两个阶级分开:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/879fc7936b2ccbc7bb1df70f12eee5e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*yD3jwAt3ewjbrdLHFb6cHw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 45. Deriving equation of the line that forms the Decision Boundary</figcaption></figure><p id="7853" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图 45  中<strong class="jp ir"> <em class="kl">标记的一条线的这个方程构成了<strong class="jp ir">判定边界</strong>。<strong class="jp ir">决策边界</strong>是神经网络将其预测从正类变为负类的线，反之亦然。<em class="kl">落在线上的所有点(</em><strong class="jp ir"><em class="kl">【x₁,x₂】</em></strong><em class="kl">)的估计概率恰好为 50%，即</em><strong class="jp ir"><em class="kl">【p̂=0.5】</em></strong>，<em class="kl">其上的所有点的估计概率大于 50%，即</em> <strong class="jp ir"> <em class="kl"> p̂ &gt; 0.5，</em> </strong> <em class="kl">线下的所有点的估计概率为</em></em></strong></p><p id="beda" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以通过给区域<strong class="jp ir"><em class="kl"/></strong>加阴影来可视化<em class="kl">决策边界</em>，其中神经网络预测<em class="kl">正类(1) </em>和<strong class="jp ir"> <em class="kl">红</em> </strong>神经网络预测<em class="kl">负类(0) </em>。</p><div class="mf mg mh mi gt ab cb"><figure class="pz mj qa qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/c403c6c3edbdfacff0eed6d9b6df28ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*OrWN4fG4lVl9KtSpO0Szuw.png"/></div></figure><figure class="pz mj qa qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/04367bdf112b8c042205d81e57ffe556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*X-FvMphNe7rraV1l96zU5w.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk qf di qg qh">Fig 46. (left) Decision Boundary with thresh=0.5. || (right) Decision Boundary <strong class="bd oq">shaded </strong>with thresh=0.5</figcaption></figure></div><p id="e5bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">在大多数情况下，我们可以在二元分类问题中设置 0.5 的阈值。</em>那么，在深入了解阈值之后，有什么收获呢？我们是不是应该把它设置为 0.5，然后忘掉它？<em class="kl">没有！在某些情况下，您会希望阈值很高，例如，如果您正在创建一个癌症检测神经网络模型，您会希望您的神经网络非常有信心，可能至少有 95%(0.95)甚至 99%(0.99)的患者患有癌症，因为如果他们没有，他们可能不得不白白经历毒性化疗。另一方面，猫检测器神经网络模型可以设置为低阈值，大约 0.5 左右，因为即使神经网络将猫错误分类，这也只是一个有趣的事故，没有伤害也没有犯规。</em></p><p id="8833" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，为了阐明分类阈值的概念，让我们将分类阈值对决策边界的位置以及神经网络模型的结果准确性的影响可视化:</p><div class="mf mg mh mi gt ab cb"><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/118540bb966a1b7c8d8fcac7a8f3147c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*pmaQO9_lZA9LHN5rjZwoqg.png"/></div></figure><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/9ce75f73e45a33fc5ee52b64691b46c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*I3aXHcOsB6MrZ_Gg5D_aSQ.png"/></div></figure><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/f5861be6158924c655c6f5d0d3edb57e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*3eakwIchbpVxIFyrCjm1zA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk qj di qk qh">Fig 47.a. <strong class="bd oq">thresh=0.000000001, </strong>all the values assigned to the positive class (accuracy = 25%)</figcaption></figure></div><div class="ab cb"><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/2e9c90ee287ebd3317dff033c6e20c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*TGCYtAXR8ZaNkwQqFluRaQ.png"/></div></figure><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/96fb389a5b6cfb4a7bead23e1cf9714c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*fhg0SDcub4Xgy4QT3tkkEw.png"/></div></figure><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/43aacb7cb6e631b4528529f82b8553c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*RUO9xYwKXbMojqQeKiUVdw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk qj di qk qh">Fig 47.b. <strong class="bd oq">thresh=0.00001, </strong>only one point(red cross) correctly assigned to negative class, rest to the positive class (accuracy = 50%)</figcaption></figure></div><div class="ab cb"><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/9b0d2c6a8ef640e57390a216f2e008a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*sXvY79mus2MCf4jSpzrRNA.png"/></div></figure><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/be072e2af96cb03bf34e01626f2e39ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*eyFYiIzXeeaFU2H9bVKHqw.png"/></div></figure><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/12c4c8e3a426423a32db798253459f28.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*xaWz8tPqgNA00a15AkSbFA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk qj di qk qh">Fig 47.c. <strong class="bd oq">thresh=0.5,</strong> all points correctly assigned their respective class(accuracy = 100%)</figcaption></figure></div><div class="ab cb"><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/ea6f5855bf02af4fb00394ff9840c709.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ulxifX0ShVXIlcqDO2ZQKA.png"/></div></figure><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/7a7afec5e8f3a8b5a5eb495ced82e529.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*po8Zc_LN3HrkSRNwTkg63Q.png"/></div></figure><figure class="pz mj qi qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/6c87b9a78c3d5b89813444d124190e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*amGkusXTmEi12zAjuDKO9A.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk qj di qk qh">Fig 47.d. <strong class="bd oq">thresh=0.9999,</strong> all points assigned negative class (accuracy = 75%)</figcaption></figure></div><p id="7549" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上述四幅图中训练神经网络之后，我绘制了决策边界(左)、阴影决策边界(中)和每个点离决策边界的最短距离(右)，分类阈值的范围从<em class="kl"> 0.000000001 到 0.9999。</em></p><p id="963e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">分类阈值也是神经网络</em> <strong class="jp ir"> <em class="kl">模型</em> </strong>的超参数，需要根据手头的问题<strong class="jp ir"> <em class="kl">进行调整。</em> </strong>分类阈值<strong class="jp ir"> <em class="kl"> </em> </strong>不直接影响神经网络(它不改变权重和偏差)，它仅用于将输出概率转换回我们类别的二进制表示<em class="kl">，即转换回 1 和 0</em>。</p><p id="984e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">最后说明一下，</em> <strong class="jp ir"> <em class="kl">决定边界的不是数据集</em> </strong> <em class="kl">的属性，它的形状(直的、弯曲的等等。)是神经网络的权重和偏差的结果，并且它的位置是分类阈值的结果。</em></p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="7b64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止我们已经学到了很多，对吗？😅在很大程度上，我们几乎知道关于二元分类问题的一切，以及如何通过神经网络来解决它们。不幸的是，我有一些坏消息，我们的二元交叉熵损失函数有一个严重的计算缺陷，它在目前的形式下非常不稳定😱。</p><p id="91e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">放心吧！通过一些简单的数学运算，我们就能解决这个问题</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="8334" class="kn ko iq bd kp kq nl ks kt ku nm kw kx ky nn la lb lc no le lf lg np li lj lk bi translated">二元交叉熵函数的实现</h1><p id="41c9" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">让我们再来看看<em class="kl">二元交叉熵(BCE)损失</em>函数:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi od"><img src="../Images/a40a778647eb4b25709ba4d5b7e9d26f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IV8dxRonbu6WusO6pghmZw.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 48. The Binary Cross-Entropy Function</figcaption></figure><p id="9b50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从分段方程中注意到，<em class="kl">二元交叉熵损失</em>函数的所有特征都依赖于"<strong class="jp ir"> log" </strong>函数(回想一下，这里的"<strong class="jp ir">log "</strong><strong class="jp ir"/>是自然对数)。</p><p id="2ce0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们画出<strong class="jp ir"> <em class="kl"> log </em> </strong>函数，并形象化其特征:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/70f4ac4f9a338c92a365ddb9865e1627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*rhdACt-TqzC0yeCV3RtNlQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 49. The Natural Logarithm function plotted</figcaption></figure><p id="4ce0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">二值交叉熵损失</em>中的<strong class="jp ir"> log </strong>函数定义了神经网络何时付出高惩罚(<strong class="jp ir"> <em class="kl">损失→∞ </em> </strong>)以及神经网络何时正确(<strong class="jp ir"> <em class="kl">损失→0 </em> </strong>)。<em class="kl">的域</em><strong class="jp ir"><em class="kl">log</em></strong><em class="kl">函数是</em><strong class="jp ir"><em class="kl">0&lt;x&lt;∩</em></strong><em class="kl">其范围是无界的</em><strong class="jp ir"><em class="kl">-∩&lt;log(x)&lt;∩</em></strong><em class="kl">，</em>更重要的是，作为<strong class="jp ir">因此，接近零的值的微小变化会对<em class="kl">二进制交叉熵损失</em>函数的结果产生极大影响，此外，我们的计算机只能以一定的浮点精度存储数字，当有函数趋向于无穷大时，它们会导致<a class="ae km" href="https://stats.stackexchange.com/questions/1389/what-is-numerical-overflow" rel="noopener ugc nofollow" target="_blank"> <em class="kl">数字溢出</em> </a> <em class="kl">(溢出是指数字太大而无法存储在计算机内存中，下溢是指数字太小而无法存储在计算机中)</em>。原来<em class="kl">二元交叉熵</em>函数的优点，即<strong class="jp ir">对数</strong>函数，也是它的弱点，使得它在小值附近不稳定。</strong></p><p id="2265" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">这对梯度的计算也有可怕的影响</em>。随着数值越来越接近零，梯度趋向于接近<strong class="jp ir"> <em class="kl">无穷大</em> </strong> <em class="kl">使得梯度计算也不稳定</em>。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/55d90c1d357122e179fe43a80ebb440e.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*mx8TGljZkGvQ_FWb1VytvA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 50. Near zero the gradient of the <strong class="bd oq">log </strong>function also becomes unstable</figcaption></figure><p id="a88f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑下面的例子:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi qn"><img src="../Images/c99c44bdb47f4e2d1ce6b9f4c21ba7d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ShoFHcFY48PpilyJO-dxaw.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 51. Instability of the natural Log function making it unable to calculate the Cost</figcaption></figure><p id="876e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似地，在计算上述示例的梯度时:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi qo"><img src="../Images/0e659aae08447f0a43294c64b760399c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A_DbYOX6i-W7cUocwpx53w.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 52. Unable to calculate gradient because we used derivate of natural Log, which is also unstable</figcaption></figure><p id="446f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们看看如何解决这个问题:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi qp"><img src="../Images/d038d009a3b48d2c52781b59456d0c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZfwOAyLkPM_7S-dX6aHa1w.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 53. Making the Binary Cross-Entropy Loss Function stable</figcaption></figure><p id="8d9b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经成功地将 then 自然对数(<strong class="jp ir"> <em class="kl"> log </em> </strong>)函数带出危险地带！<strong class="jp ir"><em class="kl"/>【1+e⁻ᶻ】<em class="kl">的范围大于 1 </em> ( </strong> <em class="kl">即</em> <strong class="jp ir"> 1+e⁻ ᶻ &gt; 1) <em class="kl">结果 BCE 损耗中</em>【log】<em class="kl">函数的范围变得大于 0 </em> ( </strong> <em class="kl">即</em> <strong class="jp ir"> log( <em class="kl"> 1+e⁻ </em>总体二元交叉熵函数不再是临界不稳定的。</strong></p><p id="f9ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以就此打住，但让我们更进一步，进一步简化损失函数:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nh"><img src="../Images/2c79f61bc6e84ce1b833f5a1f543c730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DPRxnMmUSca6r_r_XaLY6Q.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 54. Further simplified Binary Cross-Entropy Loss Function</figcaption></figure><p id="57fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经极大地简化了<em class="kl">二进制交叉熵(BCE) </em>表达式，但是它有一个问题。看着从<strong class="jp ir"> <em class="kl">到<strong class="jp ir"> 1+e⁻ ᶻ </strong>的曲线，你能猜出来吗？图 53 </em> </strong>？</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/643b75927bfe718d320759477c7825bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*yTyH5RklJCSudOCU2loOsg.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 55. <strong class="bd oq">1+e⁻ ᶻ </strong>unstable for negative values</figcaption></figure><p id="6405" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">表达式为“<strong class="jp ir">1+e⁻ᶻ</strong>”<strong class="jp ir">趋于无穷大为负值(</strong> <em class="kl">即</em> <strong class="jp ir"> 1+e⁻ ᶻ →∞，</strong> <em class="kl">当 z &lt; 0 </em> <strong class="jp ir">)！</strong> <em class="kl">所以，不幸的是，这个简化的表达式在遇到负值时会溢出。</em>让我们试着解决这个问题。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/20965781e52b8e6af741a70d605f45ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*_cz2DCEIQQ3RPq3Y5e-qGw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 56. Second simplification of Binary Cross-Entropy Loss function</figcaption></figure><p id="1179" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，通过这个“<strong class="jp ir"> eᶻ+1 </strong>”表达式，我们已经解决了对数函数在负值时不稳定的问题。不幸的是，现在我们面临相反的问题，新的二元交叉熵损失函数对于大的正值是不稳定的😕因为“<strong class="jp ir">eᶻ+1</strong>”<strong class="jp ir">对于正值趋向于无穷大(</strong> <em class="kl">即</em> <strong class="jp ir"> eᶻ+1 →∞，</strong> <em class="kl">当 z &gt; 0 </em> <strong class="jp ir">)！</strong></p><p id="c81e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们想象一下这两个指数表达式:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/39cef1a2b508f92021f0d9a3b89831e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*dpIO_AzlJrrBJbayZX3K7w.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 57.<strong class="bd oq"> eᶻ+1</strong>(blue)<strong class="bd oq"> </strong>and<strong class="bd oq"> 1+e⁻ ᶻ</strong>(green)<strong class="bd oq"> visualized</strong></figcaption></figure><p id="1957" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们需要以某种方式将这两个简化的函数(在<strong class="jp ir"> <em class="kl">图 54</em></strong>&amp;<strong class="jp ir"><em class="kl">56</em></strong>中)组合成一个二元交叉熵(BCE)函数，使得整体损失函数在所有值<em class="kl">正和负之间都是稳定的。</em></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi qt"><img src="../Images/dd7a3272b19e2e9afc322e402335c9c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TezGCUIYeDi6OYrdl2gPEg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 58. Final stable and simplified Binary Cross -Entropy Function</figcaption></figure><p id="649a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们确认它对负值和正值进行了正确的计算:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi qu"><img src="../Images/45cde7fa25f64a7f48d392a4d3b5b7f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fMLgL1sdPXp9R9dXq0bmxw.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 59. Example calculations with new <strong class="bd oq">stable </strong>Binary Cross-Entropy Function</figcaption></figure><p id="ebce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">花点时间理解这个，试着把它和来自<strong class="jp ir"> <em class="kl">图 58 </em> </strong>的分段<em class="kl">稳定二元交叉熵损失</em>函数拼凑起来。</p><p id="d8a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，通过一些简单的高中数学，我们解决了基本的二元交叉熵函数中的数值缺陷，并创建了一个<strong class="jp ir"> <em class="kl">稳定的二元交叉熵损失和成本函数。</em>T55】</strong></p><p id="6fc8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，先前的<strong class="jp ir"> <em class="kl">【不稳定】</em> </strong> <strong class="jp ir"> <em class="kl">二元交叉熵损失函数</em> </strong> <em class="kl">以</em> <strong class="jp ir"> <em class="kl"> </em> </strong>标签(<strong class="jp ir"> <em class="kl"> y </em> </strong>)和来自最后一个 sigmoid 节点的概率(<strong class="jp ir"><em class="kl">【p̂</em></strong>)作为输入但是新的稳定成本函数也是如此。</p><p id="af5a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们有了一个稳定的 BCE <em class="kl">损失</em>函数及其对应的 BCE <em class="kl">成本</em>函数，我们如何找到<em class="kl">二元交叉熵</em>函数的稳定梯度呢？</p><p id="7666" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">答案一直就在眼前！</p><p id="ac7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想一下<em class="kl">二元交叉熵损失</em>函数的导数(<strong class="jp ir"> <em class="kl">图 15 </em> </strong>):</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi qv"><img src="../Images/4f1080bf8180c1f060ae21a50f88f002.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*_PyFKop9Lv-1a6F067mR2g.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 60. The derivative of the Binary Cross Entropy Loss Function</figcaption></figure><p id="af8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还记得在反向传播期间，该导数流入 sigmoid 节点并与 Sigmoid 节点处的局部梯度相乘，这正是 Sigmoid 函数的导数(<strong class="jp ir"> <em class="kl">图 19.b. </em> </strong>):</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi qw"><img src="../Images/7eecbe9dbe23204fba40f794f0054ece.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*83eIdX0eerh5HSIzsKNXHQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 61. The derivative of the Sigmoid function</figcaption></figure><p id="d853" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们将两个导数相乘时，会发生一些美妙的数学现象:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/76d0e47bc9eac4b9939b6b1e1b24f9ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*1VcePqTPSQLjrrylbnUvyg.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 62. Reduction of derivative into simple expression after multiplying the two derivatives</figcaption></figure><p id="e60e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以为了计算导数<strong class="jp ir"> <em class="kl"> ∂Loss/∂z </em> </strong>我们甚至不需要计算损失函数的导数或者乙状结肠节点的导数，相反我们可以绕过乙状结肠节点并传递“<strong class="jp ir"><em class="kl"/></strong>”作为到最后一个线性节点(<strong class="jp ir"> <em class="kl"> z </em> </strong>)的上游梯度！</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi oz"><img src="../Images/4c0b0fe25d6eb44de8fcfdee79dea011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UbzYJyVf5Yq5x4sfh1xE5A.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 63. Visualizing the forward and backward training step after optimization</figcaption></figure><p id="46e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种优化有两大好处:</p><ol class=""><li id="c1f3" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk ni lw lx ly bi translated">我们不再需要使用二元交叉熵函数的不稳定导数。</li><li id="fde5" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk ni lw lx ly bi translated">我们也避免与 Sigmoid 函数的<strong class="jp ir"> <em class="kl">饱和梯度</em> </strong>相乘。</li></ol><p id="0f11" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">什么是饱和梯度？回想一下 Sigmoid 函数曲线</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi qx"><img src="../Images/7d0fd4d1b0c62da9c15104120b37c67d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*NHShglhpTea_4iljPmOYzQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 62 Sigmoid curve with flat gradients at either end</figcaption></figure><p id="4b55" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 Sigmoid 曲线的两端变得平坦。当权重大量增加或减少使得相关线性节点(<strong class="jp ir"> <em class="kl"> z </em> </strong>)的输出变得非常大或非常小时，这在神经网络中成为一个巨大的问题。在这些情况下，梯度(即 sigmoid 节点处的局部梯度)变为零或非常接近零。因此，当进入的上游梯度在 Sigmoid 节点处与非常小的或零的局部梯度相乘时，上游梯度值没有多少或没有能够通过。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi qy"><img src="../Images/3c12ca8a7b3091cb588078cc741c2cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OjZtxvIg_OM8fCizZQiUbQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 63. A consequence of saturating gradients is that very little of the upstream gradient is able to pass through</figcaption></figure><p id="3aa2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本节的最后一点上，我们可以找到<em class="kl">稳定二元交叉熵</em>函数的导数，并得出相同的结论，但我更喜欢上面的解释，因为它帮助我们理解为什么我们可以在二元分类神经网络中反向传播梯度时绕过最后一个 sigmoid 节点。为了完整起见，我还得出以下结论:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi qz"><img src="../Images/2736fb1498be39e448366be15a50a89f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LmrgbTKHhnCSZVq9BYEF3Q.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 64. The derivative using the piecewise stable Binary Cross-Entropy Loss function</figcaption></figure></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="c87b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们将我们所学的一切应用到稍微复杂的 XOR 门数据上，我们需要一个多层神经网络<em class="kl">(一个深度神经网络)</em>，因为单层神经网络的简单直线不会切割(查看我的<a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener">以前的帖子</a>以了解关于这一现象的更多信息):</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/2c25eeaf44e08c2a983621faca9caad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*F1lPFm31-h6NJO25SoqOmA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 65 XOR data and plot</figcaption></figure><p id="1966" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了对 XOR 数据集的数据点进行分类，我们将使用以下神经网络架构:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ra"><img src="../Images/9eca5d8b92c49021ca34dd45a960a92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIGHFTSjIvU_wMvOyv-PWQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 66. A 2-layer neural net architecture</figcaption></figure><p id="9bd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络中的一层是具有可调权重的同一深度的任意节点集。神经网络分为两层，中间(隐藏)层和最后一层输出层。</p><p id="3217" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们继续向前和向后传播之前，让我们扩展这个 2 层神经网络:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rb"><img src="../Images/8cffcf0b42a78fa29c59ae7aa5ea2506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1jIkCeYI-HU_n5jQabLtkg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 67. Expanded 2-layer neural net</figcaption></figure><p id="08d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们准备执行<strong class="jp ir"> <em class="kl">批量梯度下降</em> </strong>，从转发传播开始:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rc"><img src="../Images/0299508d51ee996d49e6d37017cc8dac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldCYIrCNdUnQi-I6xLknsA.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rd"><img src="../Images/ddee7c3fb5f2909e5a287d5c9397802c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vESszdCFAkVRdp-0k95uqw.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 68.a. Forward propagation on 2-layer neural net</figcaption></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi re"><img src="../Images/365b5b99da266635219c26ead304bf34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R5lGu8DS7-Z-Q6CxqAskIA.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rd"><img src="../Images/3cf3df2c68e7aed25f8928e0e74068a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8jcpUUaO3aumhnGbGKEhBQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 68.b. Forward propagation on 2-layer neural net</figcaption></figure><p id="6c27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在可以计算出<strong class="jp ir"><em class="kl"/></strong><em class="kl">的稳定成本</em>:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rf"><img src="../Images/aff8dd172a82b2503e4e1ea20fed767d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XpTMXytttVEH6B6LXiY5bg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 69. Calculation of Stable Binary Cross-Entropy Cost</figcaption></figure><p id="1e54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在计算了<em class="kl">成本</em>之后，我们现在可以继续进行反向传播并改进权重和偏差。<em class="kl">回想一下，我们可以通过优化技术绕过最后一个乙状结肠节点。</em></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rg"><img src="../Images/df23f239bb80d8036e4c21bc07fc1d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZcTjavEqAcVUDO8qSlbSQ.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rd"><img src="../Images/68c4b24f5becc437008bd91f071102d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BI2l1J5euKiCk_kefzn9jg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 70.a. Backpropagation on the 2-layer neural net</figcaption></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rh"><img src="../Images/518eafb2c50c1d3aad6a1b8336dad648.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9hcQVVpQgg8yh5_SxPqt6g.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rd"><img src="../Images/c9ca9197a8769d57ceb85d28b7e805a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KL-cP3t0cQj6tJjJn2h7BA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 70.b. Backpropagation on the 2-layer neural net</figcaption></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ri"><img src="../Images/3086a80fbc39c95e228b08807cf40562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BGJHIy9v1Adxms44AsCmbQ.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rd"><img src="../Images/807d03768d642ef2953dd494a5104911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5YbKcV55XCHRNaF5LQCHcg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 70.c. Backpropagation on the 2-layer neural net</figcaption></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ri"><img src="../Images/5290f8e05bc3c9a25f206bb1c38548d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mvU5BRJf9HeXL0Wba7hmVg.png"/></div></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rd"><img src="../Images/d1491fef38ad8c1559b173412c929ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QeZICY-Twb7nX2lPivDDHA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 70.e. Backpropagation on the 2-layer neural net</figcaption></figure><p id="89e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">伙计，太多了！😅但是现在我们已经深入了解了关于二分类神经网络的一切。最后，让我们继续进行<strong class="jp ir"> <em class="kl">梯度下降</em> </strong>并更新我们的权重。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/45bce750dad1ed50ef0313b639033556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*rXzpRy03k3uT6V3d4kBwaA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 71. Batch gradient descent update</figcaption></figure><p id="0da4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这一点上，如果你想自己执行下一个训练迭代并进一步理解，下面是你应该得到的近似梯度(四舍五入到 3 d.p):</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi rj"><img src="../Images/f817562b6af3ef9b6d27aa85d50fe9e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*94QxCRPj-5cSQoAMvisXGA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 72. Derivatives computed during 2ⁿᵈ training iteration</figcaption></figure><p id="f210" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在 5000 个历元之后，<em class="kl">成本</em>稳定地降低到大约<strong class="jp ir"><em class="kl"/></strong>0.0017，并且当 <strong class="jp ir"> <em class="kl">分类阈值设置为 0.5 </em> </strong> <em class="kl">(在编码部分，您可以摆弄阈值，看看它如何影响决策边界)</em>:</p><div class="mf mg mh mi gt ab cb"><figure class="pz mj rk qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/32cc49dd9bcb857c574dfd67e0a91bb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*0eWvsl_x7dkaUkYf9h_r5w.png"/></div></figure><figure class="pz mj rl qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/7bcee0b18317e37ad0cc25d0ae1c9ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*j8zLBS1I4huw4EOfp5gLEQ.png"/></div></figure><figure class="pz mj rl qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/f1038dbaa5f86fabc5192d33744ed777.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*lduIXhKmtX8ym9Lc4BZowQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk rm di rn qh">Fig 73. (right) The learning curve, (middle) Decision Boundary, (left) Shaded Decision Boundary <strong class="bd oq"><em class="ro">green </em></strong>positive label, <strong class="bd oq">red </strong>negative label</figcaption></figure></div><p id="d4e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在结束这一部分之前，我想回答一些可能困扰你的问题:</p><h2 id="e583" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">1-这不就是逻辑回归吗？</h2><p id="d89c" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated"><strong class="jp ir">是的</strong>，<strong class="jp ir">一个只有<em class="kl">一个 sigmoid 神经元，没有隐含层</em>的神经网络，如<em class="kl">图 1 </em>所示，就是逻辑回归</strong>。<em class="kl">单 sigmoid-neuron 神经网络/逻辑回归</em>可以对可以用直线分离的更简单的数据集进行分类(如与门数据)。对于一个复杂的数据集(如 XOR) <strong class="jp ir"> <em class="kl">特征工程</em> </strong>需要手工执行，以使<em class="kl">单 sigmoid-neuron 神经网络/逻辑回归充分工作</em>(在前一篇文章中解释过<a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener">)。</a></p><p id="256a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">具有多个隐层和多个神经元的多层神经网络称为深度神经网络。深度神经网络可以捕获关于数据集的更多信息，而不是单个神经元，并且可以在很少或没有人工干预的情况下对复杂数据集进行分类，唯一的警告是，它需要比简单分类模型(如<em class="kl">单 sigmoid-neuron 神经网络/逻辑回归)多得多的训练数据。</em></p><p id="6f39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，<em class="kl">单 sigmoid 神经元神经网络/逻辑回归的二元交叉熵成本函数是凸的(u 形),具有保证的全局最小点。</em>另一方面，对于深度神经网络，二元交叉熵代价函数<em class="kl">不保证有全局最小值</em>；实际上，这对训练深度神经网络没有严重影响，研究表明，这可以通过更多的训练数据来缓解。</p><h2 id="79be" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">2-我们可以按原样使用原始输出概率吗？</h2><p id="95ec" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">是的</em> </strong>，也可以使用来自神经网络的原始概率，这取决于你试图解决的问题的类型。例如，你训练一个二元分类模型来预测每天在一个路口发生车祸的概率，<strong class="jp ir"> <em class="kl"> P(事故∣日)</em> </strong>。假设概率为<strong class="jp ir"> <em class="kl"> P(事故∣日)=0.08。</em> </strong>所以在一年的那个关口，我们可以预期:</p><p id="3792" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl"> P(事故∣日)× 365 = 0.08 × 365 = 29.2 起事故</em></p><h2 id="a710" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">3-如何找到最优分类阈值？</h2><p id="d147" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">准确度是计算分类阈值的一个度量。我们需要一个分类阈值来最大化我们模型的准确性。</p><p id="1cd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不幸的是，在许多现实世界的案例中，准确性本身就是一个很差的指标。这在数据集中<strong class="jp ir"> <em class="kl">类倾斜</em> </strong>的情况下尤其明显(简单来说，一个类的例子比另一个多)。我们前面看到的与门也有这个问题；o <em class="kl">只有一个积极类的例子，其余的消极类</em>。如果您回过头来查看<strong class="jp ir"> <em class="kl">图 47d .</em></strong>，其中我们将分类阈值设置得如此之高(<strong class="jp ir"> <em class="kl"> 0.9999 </em> </strong>)，以至于模型预测了我们所有示例的负类，您会看到模型的准确度仍然是<strong class="jp ir"> <em class="kl"> 75%！</em> </strong> <em class="kl">这听起来挺能接受的，但看看数据就不是了。</em></p><p id="8c73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑另一种情况，您正在训练一个癌症检测模型，但是您的 1000 个患者数据集只有<strong class="jp ir"><em class="kl"/></strong><em class="kl">一个患有癌症</em>的患者的例子。现在，如果模型总是输出一个负类(即<strong class="jp ir"> not-cancer，<em class="kl"> 0 </em> </strong>)，不管输入是什么，您都会得到一个在数据集上具有 99.9%<em class="kl"/>准确率的分类器！</p><p id="05f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，为了处理现实世界中的问题，许多数据科学家使用使用<strong class="jp ir"><em class="kl"/></strong>和<strong class="jp ir"><em class="kl"/></strong>的指标。</p><p id="4183" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">精度</em> </strong>:分类器得到了多少个正确的肯定预测？(真阳性/预测阳性总数)</p><p id="b09c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">回忆</em> </strong>:分类器能够识别的正面例子比例是多少？(真阳性/实际阳性总数)</p><p id="520c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这两个指标都可以通过一个称为“<strong class="jp ir"> <em class="kl">混淆矩阵</em> </strong>”的 2×2 矩阵来可视化:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rp"><img src="../Images/a7fd1c523b4c7fe0e13555f3613d8f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mlvv0FuG87Dd63wsomrrEQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 74. The Confusion Matrix and metrics derived through it</figcaption></figure><p id="1f20" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">调优分类阈值是<em class="kl">精度</em>和<em class="kl">召回</em>之间的拉锯战。如果<em class="kl">精度</em>高(即高分类阈值)<em class="kl">召回</em>将低，反之亦然。理解<em class="kl">精度与召回之间的权衡</em>是一个<em class="kl">主题</em>超出了本文的范围，也将是未来<em class="kl"> Nothing but Numpy </em>博客的主题。</p><p id="f424" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大多数数据科学家用于调整分类阈值的一个常见指标是<a class="ae km" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="kl"> F1 得分</em> </strong> </a>，该指标结合了<em class="kl">精度</em>和<em class="kl">召回</em>。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="fcaa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">为了简洁起见，以下问题有自己的简短帖子，作为对我们讨论的补充(点击/轻触问题，转到相应的帖子)</em></p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h2 id="90f6" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">4- <a class="ae km" href="https://medium.com/@rafayak/where-did-the-binary-cross-entropy-loss-function-come-from-ac3de349a715" rel="noopener">这个二元交叉熵损失函数从何而来？</a></h2><h2 id="cd2e" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">5- <a class="ae km" href="https://medium.com/@rafayak/why-using-mean-squared-error-mse-cost-function-for-binary-classification-is-a-bad-idea-933089e90df7" rel="noopener">为什么不像你上一篇博客那样直接用均方差(MSE)函数呢？毕竟，您能够使用 MSE 解决相同的示例。</a></h2><h2 id="e18b" class="nr ko iq bd kp ns nt dn kt nu nv dp kx jy nw nx lb kc ny nz lf kg oa ob lj oc bi translated">6-<a class="ae km" href="https://medium.com/@rafayak/how-do-tensorflow-and-keras-implement-binary-classification-and-the-binary-cross-entropy-function-e9413826da7" rel="noopener">tensor flow 和 Keras 是如何实现二进制分类和二进制交叉熵函数(Bonus)的？</a></h2><p id="bb2d" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">第一部分到此结束<strong class="jp ir">。</strong></p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><a href="https://www.buymeacoffee.com/rafaykhan"><div class="gh gi rq"><img src="../Images/bdb1aed53d63b0bff2c1599be3aa9dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*q8O4kAyuVD-9slT_aAKc1Q.png"/></div></a><figcaption class="mm mn gj gh gi mo mp bd b be z dk">If you’re enjoying it</figcaption></figure></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="98ff" class="kn ko iq bd kp kq nl ks kt ku nm kw kx ky nn la lb lc no le lf lg np li lj lk bi translated">第二部分:模块化二元分类神经网络编码</h1><p id="03ff" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated"><em class="kl">这个实现建立在上一篇文章的代码基础上(要了解更多细节，您可以查看上一篇文章</em><a class="ae km" href="https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0" rel="noopener"><em class="kl"/></a><em class="kl">的编码部分，或者阅读代码中的文档)。</em></p><p id="e02a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">线状图层的代码</em> </strong>类保持不变。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="rr rs l"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 92. Linear Layer Class</figcaption></figure><p id="0ec3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Sigmoid 图层类的代码也保持不变:</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="rr rs l"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 93. Sigmoid Activation Layer class</figcaption></figure><p id="26ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">二进制交叉熵(BCE)代价函数(及其变体)是上次代码表单的主要新增内容。</p><p id="5939" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，让我们看看“不稳定的”二元交叉熵成本函数<code class="fe rt ru rv pn b">compute_bce_cost(Y, P_hat)</code>，它将真实标签(<code class="fe rt ru rv pn b">Y</code>)和来自最后一个 Sigmoid 层的概率(<code class="fe rt ru rv pn b">P_hat</code>)作为自变量。成本函数的这个简单版本返回二元交叉熵成本(<code class="fe rt ru rv pn b">cost</code>)及其关于概率的导数(<code class="fe rt ru rv pn b">dP_hat</code>)的<em class="kl">不稳定</em>版本:</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="rr rs l"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 94. Unstable Binary Cross-Entropy Function</figcaption></figure><p id="99dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们看看二进制交叉熵成本函数<code class="fe rt ru rv pn b">compute_stable_bce_cost(Y, Z)</code>的稳定版本，它将真实标签(<code class="fe rt ru rv pn b">Y</code>)和来自最后一个线性层的输出(<code class="fe rt ru rv pn b">Z</code>)作为自变量。该成本函数返回由 TensorFlow 计算的二进制交叉熵成本(<code class="fe rt ru rv pn b">cost</code>)的<em class="kl">稳定</em>版本，以及关于最后一个线性层的导数(<code class="fe rt ru rv pn b">dZ_last</code>):</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="rr rs l"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 95. Stable Binary Cross-Entropy Function</figcaption></figure><p id="2196" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们也来看看 Keras 实现二元交叉熵代价函数的方式。<code class="fe rt ru rv pn b">compute_keras_like_bce_cost(Y, P_hat, from_logits=Flase</code>将真实标签(<code class="fe rt ru rv pn b">Y</code>)、最后一个线性层(<code class="fe rt ru rv pn b">Z</code>)或最后一个 s 形层(<code class="fe rt ru rv pn b">P_hat</code>)的输出作为自变量，这取决于可选自变量<code class="fe rt ru rv pn b">from_logits</code>。If from <code class="fe rt ru rv pn b">from_logtis=Flase</code>(默认)然后 all 假设<code class="fe rt ru rv pn b">P_hat</code>包含需要转换为 logits 的概率，用于计算稳定成本函数。如果来自<code class="fe rt ru rv pn b">from_logtis=True</code>，那么所有假设<code class="fe rt ru rv pn b">P_hat</code>包含来自线性节点(<code class="fe rt ru rv pn b">Z</code>)的输出，并且可以直接计算稳定的成本函数。该函数返回成本(<code class="fe rt ru rv pn b">cost</code>)和最后一个线性层的导数(<code class="fe rt ru rv pn b">dZ_last</code>)。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="rr rs l"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 96. Keras-like Stable Binary Cross-Entropy Function</figcaption></figure><p id="c23e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">此时，你应该从这个<a class="ae km" href="https://github.com/RafayAK/NothingButNumPy/tree/master/Understanding_and_Creating_Binary_Classification_NNs" rel="noopener ugc nofollow" target="_blank"> <em class="kl">资源库</em> </a> <em class="kl">中打开</em><a class="ae km" href="https://github.com/RafayAK/NothingButNumPy/blob/master/Understanding_and_Creating_Binary_Classification_NNs/1_layer_toy_network_on_Iris_petals.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="kl">1 _ layer _ toy _ network _ on _ Iris _ petals</em></a>笔记本，在一个单独的窗口中并排浏览这个博客和笔记本。</em></p><p id="caa4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用 Iris flower 数据集，它恰好是为统计分析创建的第一批数据集之一。Iris 数据集包含属于 3 个物种的 150 个鸢尾花样本——鸢尾-刚毛鸢尾、鸢尾-杂色鸢尾和鸢尾-海滨鸢尾。每个例子都有 4 个特征——花瓣长度、花瓣宽度、萼片长度和萼片宽度。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi rw"><img src="../Images/8dba7f8af1b8ec81a9d87ef8a3050a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFC_U5j_Y8IXF4Ga87KNVg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 97. Three Species of Iris Flowers (courtesy <a class="ae km" href="https://thegoodpython.com/iris-dataset/" rel="noopener ugc nofollow" target="_blank">thegoodpython</a>)</figcaption></figure><p id="ad6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于我们的第一个二元分类神经网络，我们将创建一个 1 层神经网络，如<strong class="jp ir"> <em class="kl">图 1 </em> </strong>所示，仅使用花瓣长度和花瓣宽度作为输入特征来区分鸢尾-海滨鸢尾与其他。让我们来构建我们的神经网络层:</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="rr rs l"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 98. Defining the layers and training parameters</figcaption></figure><p id="2b93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们可以继续训练我们的神经网络:</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="rr rs l"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 99. The training loop</figcaption></figure><p id="6261" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">注意，我们是通过导数，</em> <code class="fe rt ru rv pn b"><em class="kl">dZ1</em></code> <em class="kl">，直接进入线性层</em> <code class="fe rt ru rv pn b"><em class="kl">Z1.backward(dZ1)</em></code> <em class="kl">绕过乙状结肠层，</em> <code class="fe rt ru rv pn b"><em class="kl">A1</em></code> <em class="kl">，因为优化的原因，我们想到了更早。</em></p><p id="173e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在运行 5000 个纪元的循环后，在笔记本中，我们看到成本稳步下降到大约<em class="kl"> 0.080。</em></p><pre class="mf mg mh mi gt pm pn po pp aw pq bi"><span id="12e4" class="nr ko iq pn b gy pr ps l pt pu">Cost at epoch#4700: 0.08127062969243247<br/>Cost at epoch#4800: 0.08099585868475366<br/>Cost at epoch#4900: 0.08073032792428664<br/>Cost at epoch#4999: 0.08047611054333165</span></pre><p id="4112" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">导致以下学习曲线和决策边界:</p><div class="mf mg mh mi gt ab cb"><figure class="pz mj rx qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/3b9bf50a5ca9939319cf84e8db97b1ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*b9o-m5XCbptsQuQipz5lOQ.png"/></div></figure><figure class="pz mj ry qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/0e4cb026b44107308433ecfd239f69fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*Q3eztYd9o8h5LGJqIRKlPg.png"/></div></figure><figure class="pz mj ry qb qc qd qe paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><img src="../Images/44ba1cc02ae450996347e7d5568484cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*haDr3q4oRfM4568oqljnNA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk rz di sa qh">Fig 100. The Learning Curve, Decision Boundary, and Shaded Decision Boundary.</figcaption></figure></div><p id="f8f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的模型对训练数据的准确度是:</p><pre class="mf mg mh mi gt pm pn po pp aw pq bi"><span id="86a2" class="nr ko iq pn b gy pr ps l pt pu">The predicted outputs of first 5 examples: <br/>[[ 0.  0.  1.  0.  1.]]<br/>The predicted prbabilities of first 5 examples:<br/> [[ 0.012  0.022  0.542  0.     0.719]]</span><span id="4417" class="nr ko iq pn b gy sb ps l pt pu">The accuracy of the model is: 96.0%</span></pre><p id="0b0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">查看<a class="ae km" href="https://github.com/RafayAK/NothingButNumPy/tree/master/Understanding_and_Creating_Binary_Classification_NNs" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="kl">资源库</em> </strong> </a>中的其他笔记本。我们将在以后的博客中学习更多的东西，因此，你应该从记忆中创建层类(如果你以前没有)和二进制交叉熵代价函数作为练习，并尝试重新创建来自<strong class="jp ir"> <em class="kl">部分</em></strong><em class="kl"/><strong class="jp ir"><em class="kl">ⅰ</em></strong><em class="kl">的 and 门示例。</em></p><p id="867f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">博客到此结束🙌🎉。感谢你花时间阅读这篇文章，希望你喜欢。</p><p id="b6f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如有任何问题，请随时通过推特<a class="ae km" href="https://twitter.com/RafayAK" rel="noopener ugc nofollow" target="_blank">推特</a>推特<a class="ae km" href="https://twitter.com/RafayAK" rel="noopener ugc nofollow" target="_blank">@拉法亚克</a>联系我</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><a href="https://www.buymeacoffee.com/rafaykhan"><div class="gh gi rq"><img src="../Images/bdb1aed53d63b0bff2c1599be3aa9dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*q8O4kAyuVD-9slT_aAKc1Q.png"/></div></a><figcaption class="mm mn gj gh gi mo mp bd b be z dk">If you enjoyed it!</figcaption></figure></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="ac5a" class="kn ko iq bd kp kq nl ks kt ku nm kw kx ky nn la lb lc no le lf lg np li lj lk bi translated">如果没有以下资源和人员，这个博客是不可能的:</h1><ul class=""><li id="c1ed" class="lq lr iq jp b jq ll ju lm jy sc kc sd kg se kk lv lw lx ly bi translated">TensorFlow <a class="ae km" href="https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits" rel="noopener ugc nofollow" target="_blank">文档</a>和<a class="ae km" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python" rel="noopener ugc nofollow" target="_blank"> GitHub </a>(特别是<a class="ae km" href="https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/nn_impl.py#L112" rel="noopener ugc nofollow" target="_blank">这个</a>)</li><li id="6d7b" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">Keras <a class="ae km" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy" rel="noopener ugc nofollow" target="_blank">文档</a>和 GitHub(特别是<a class="ae km" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/losses.py#L348-L406" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae km" href="https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/metrics.py#L2493" rel="noopener ugc nofollow" target="_blank">这个</a>)</li><li id="9f78" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae km" href="http://www.linkedin.com/in/saitcelebi" rel="noopener ugc nofollow" target="_blank">赛特雪拉比</a>的<a class="ae km" href="http://saitcelebi.com/" rel="noopener ugc nofollow" target="_blank">博客</a></li><li id="27f8" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">谷歌的<a class="ae km" href="https://developers.google.com/machine-learning/crash-course" rel="noopener ugc nofollow" target="_blank"> ML 速成班</a></li><li id="3d64" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">詹姆斯·d·麦卡弗里的<a class="ae km" href="https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/" rel="noopener ugc nofollow" target="_blank">博客</a></li><li id="aeb0" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">威尔·沃尔夫(<a class="ae km" href="https://twitter.com/willwolf_?s=20" rel="noopener ugc nofollow" target="_blank">@威尔·沃尔夫 _ </a>)关于通过 MLE 推导函数的惊人<a class="ae km" href="http://willwolf.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/" rel="noopener ugc nofollow" target="_blank">帖子</a></li><li id="59be" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">安德烈·卡帕西(<a class="ae km" href="https://twitter.com/karpathy" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> @ </strong>卡帕西</a>)斯坦福<a class="ae km" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank">课程</a></li><li id="7a18" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">克里斯托弗·奥拉赫(<a class="ae km" href="https://twitter.com/ch402" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> @ </strong> ch402 </a> ) <a class="ae km" href="https://colah.github.io/" rel="noopener ugc nofollow" target="_blank">博客</a> s</li><li id="a763" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">吴恩达(<a class="ae km" href="http://twitter.com/AndrewYNg" rel="noopener ugc nofollow" target="_blank">@安德温</a>)和他的 Coursera 关于<a class="ae km" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>和<a class="ae km" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>的课程</li><li id="1576" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">伊恩·古德费勒(<a class="ae km" href="https://twitter.com/goodfellow_ian" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> @ </strong>古德费勒 _ 伊恩</a>)和他那本令人惊叹的<a class="ae km" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">书</a></li><li id="8c74" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><a class="ae km" href="https://www.reddit.com/r/MachineLearning/comments/8im9eb/d_crossentropy_vs_meansquared_error_loss/" rel="noopener ugc nofollow" target="_blank"> Reddit </a>和<a class="ae km" href="https://stats.stackexchange.com/questions/217798/using-mse-instead-of-log-loss-in-logistic-regression" rel="noopener ugc nofollow" target="_blank"> StackExchange </a></li><li id="bcba" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">伯克利 CS294 <a class="ae km" href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/classes/cs294_f99/notes/lec3/lec3.pdf" rel="noopener ugc nofollow" target="_blank">讲义</a></li><li id="eb03" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">斯坦福 CS229 <a class="ae km" href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" rel="noopener ugc nofollow" target="_blank">课堂讲稿</a></li><li id="d1b3" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">斯坦福<a class="ae km" href="https://www.coursera.org/lecture/probabilistic-graphical-models-3-learning/maximum-likelihood-estimation-KzlS4" rel="noopener ugc nofollow" target="_blank">概率图形模型讲座</a></li><li id="941b" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><strong class="jp ir">最后，哈桑-乌兹-扎曼(</strong> <a class="ae km" href="https://twitter.com/OKidAmnesiac" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">，@OKidAmnesiac </strong> </a> <strong class="jp ir">)和哈桑·陶凯尔(</strong><a class="ae km" href="https://twitter.com/_hassaantauqeer?s=20" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">)@ _ 哈桑·陶凯尔</strong> </a> <strong class="jp ir">)进行宝贵的反馈。</strong></li></ul></div></div>    
</body>
</html>