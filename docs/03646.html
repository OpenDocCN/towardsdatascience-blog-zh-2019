<html>
<head>
<title>Word Embeddings Python Example — Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入 Python 示例—情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-sentiment-analysis-and-word-embeddings-python-keras-example-4dfb93c5a6cf?source=collection_archive---------12-----------------------#2019-06-10">https://towardsdatascience.com/machine-learning-sentiment-analysis-and-word-embeddings-python-keras-example-4dfb93c5a6cf?source=collection_archive---------12-----------------------#2019-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/eb6de9afa363cc8c1978ab128fb938c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gyNz1J-KJjmU2W7Y"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://www.pexels.com/photo/person-using-appliance-132700/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/person-using-appliance-132700/</a></figcaption></figure><div class=""/><p id="7884" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习的主要应用之一是情感分析。情感分析就是判断一个文档的基调。情感分析的输出通常是 0 到 1 之间的分数，其中 1 表示语气非常积极，0 表示非常消极。情绪分析经常用于交易。例如，情绪分析被应用于交易者的推文，以估计整体市场情绪。</p><p id="e0b3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如人们所料，情感分析是一个自然语言处理(NLP)问题。NLP 是人工智能的一个领域，涉及理解和处理语言。本文的目标是构建一个模型，从语料库的文档中获取单词的语义。在高层次上，可以想象我们将带有单词<strong class="ki jk"> <em class="le">好的</em> </strong>的文档分类为肯定的，而将单词<strong class="ki jk"> <em class="le">坏的</em> </strong>分类为否定的。不幸的是，问题并没有那么简单，因为单词前面可以有<strong class="ki jk"> <em class="le">而不是</em> </strong>如<strong class="ki jk"> <em class="le">不好。</em>T19】</strong></p><h1 id="64a1" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">密码</h1><p id="078c" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">说得够多了，让我们深入一些 Python 代码。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="745c" class="mr lg jj mn b gy ms mt l mu mv">import numpy as np<br/>from matplotlib import pyplot as plt<br/>plt.style.use('dark_background')<br/>from keras.datasets import imdb<br/>from keras.models import Sequential<br/>from keras.preprocessing.sequence import pad_sequences<br/>from keras.layers import Embedding, GlobalAveragePooling1D, Dense</span></pre><p id="3987" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参数<code class="fe mw mx my mn b">num_words=10000</code>确保我们只保留训练集中出现频率最高的前 10，000 个单词。为了保持数据的大小易于管理，我们会丢弃一些不常用的单词。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="93f2" class="mr lg jj mn b gy ms mt l mu mv">num_words = 10000</span></pre><p id="3dd0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用包含来自互联网电影数据库的 50，000 条电影评论的 IMDB 数据集。后者被分成 25，000 条用于训练的评论和 25，000 条用于测试的评论。训练集和测试集是<em class="le">平衡的</em>，这意味着它们包含相同数量的正面和负面评论。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="9615" class="mr lg jj mn b gy ms mt l mu mv">old = np.load<br/>np.load = lambda *a,**k: old(*a,**k, allow_pickle = True)<br/>(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)<br/>np.load = old<br/>del(old)</span><span id="f201" class="mr lg jj mn b gy mz mt l mu mv">print("Training entries: {}, labels: {}".format(len(X_train), len(y_train)))</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/659dd0eade1266be7216fd24df8368aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*0WAAV3p20xzDU_NbVbP5Mw.png"/></div></figure><p id="9df7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们使用<code class="fe mw mx my mn b">keras.datasets.imdb</code>将数据集导入我们的程序时，它已经经过了预处理。换句话说，每个例子都是一个整数列表，其中每个整数代表字典中的一个特定单词，每个标签都是 0 或 1 的整数值，其中 0 是负面评论，1 是正面评论。我们先在第一次复习的时候来个巅峰。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="2f44" class="mr lg jj mn b gy ms mt l mu mv">print(X_train[0])</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/13f9a59a50b65c13aba9a47853878f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6iu03fGEDrx_bFQmNZxD-g.png"/></div></div></figure><p id="863d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了更好地理解我们正在处理的内容，我们将创建一个助手函数，将每个训练示例中的整数映射到索引中的单词。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="0cfc" class="mr lg jj mn b gy ms mt l mu mv">word_index = imdb.get_word_index()</span><span id="0b98" class="mr lg jj mn b gy mz mt l mu mv"># The first indices are reserved<br/>word_index = {k:(v+3) for k,v in word_index.items()} <br/>word_index["&lt;PAD&gt;"] = 0<br/>word_index["&lt;START&gt;"] = 1<br/>word_index["&lt;UNK&gt;"] = 2  # unknown<br/>word_index["&lt;UNUSED&gt;"] = 3</span><span id="1e9b" class="mr lg jj mn b gy mz mt l mu mv">reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])</span><span id="bdf4" class="mr lg jj mn b gy mz mt l mu mv">def decode_review(text):<br/>    return ' '.join([reverse_word_index.get(i, '?') for i in text])</span></pre><p id="21b3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们可以使用<code class="fe mw mx my mn b">decode_review</code>功能来显示第一篇评论的文本。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="697f" class="mr lg jj mn b gy ms mt l mu mv">decode_review(X_train[0])</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/b2cc148df5def42ca9325f0ae3288f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s46RPwJR8oPAVvAopLUGPg.png"/></div></div></figure><p id="4345" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">鉴于文档中的每个单词都将被解释为一个特征，我们必须确保电影评论的长度相同，然后才能尝试将它们输入神经网络。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="935e" class="mr lg jj mn b gy ms mt l mu mv">len(X_train[0]), len(X_train[1])</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nd"><img src="../Images/707efc07bf03f19c1de77d64f55826aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*TsKu9NupSVbpltf9FuHEfw.png"/></div></div></figure><p id="a436" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用<code class="fe mw mx my mn b">pad_sequence</code>功能来标准化长度。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="c65b" class="mr lg jj mn b gy ms mt l mu mv">X_train = pad_sequences(<br/>    X_train,<br/>    value=word_index["&lt;PAD&gt;"],<br/>    padding='post',<br/>    maxlen=256<br/>)</span><span id="af1a" class="mr lg jj mn b gy mz mt l mu mv">X_test = pad_sequences(<br/>    X_test,<br/>    value=word_index["&lt;PAD&gt;"],<br/>    padding='post',<br/>    maxlen=256<br/>)</span></pre><p id="6316" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看前几个样本的长度。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="5a87" class="mr lg jj mn b gy ms mt l mu mv">len(X_train[0]), len(X_train[1])</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/275c95dd0a03c7f8744c11101fabb339.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*hSJxE_qhAVE6oEeVEuF32g.png"/></div></figure><p id="63aa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如敏锐的读者可能已经猜到的，单词是分类特征。因此，我们不能直接将它们输入神经网络。尽管它们已经被编码为整数，但是如果我们让它们保持原样，模型会将具有较高值的整数解释为比具有较低值的整数具有更高的优先级。通常，您可以通过将数组转换为指示单词出现的向量<em class="le">0</em>和<em class="le">1</em>来解决这个问题，这类似于一种热编码，但是对于单词来说，这是内存密集型的。给定 10，000 个单词的词汇表，我们需要在 RAM 中存储<code class="fe mw mx my mn b">num_words * num_reviews</code>大小的矩阵。</p><h1 id="0e44" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">嵌入</h1><p id="bd87" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">这就是嵌入发挥作用的地方。嵌入通过将我们的高维数据映射到低维空间(类似于 PCA)来解决稀疏输入数据(非常大的向量，具有相对较少的非零值)的核心问题。</p><p id="7097" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，假设我们有一个由以下两个句子组成的语料库。</p><ul class=""><li id="7cef" class="nf ng jj ki b kj kk kn ko kr nh kv ni kz nj ld nk nl nm nn bi translated"><em class="le">希望很快见到你</em></li><li id="4655" class="nf ng jj ki b kj no kn np kr nq kv nr kz ns ld nk nl nm nn bi translated">很高兴再次见到你</li></ul><p id="aefd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像 IMDB 数据集一样，我们可以为每个单词分配一个唯一的整数。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="2928" class="mr lg jj mn b gy ms mt l mu mv">[0, 1, 2, 3, 4]<br/><br/>[5, 1, 2, 3, 6]</span></pre><p id="8e27" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们可以定义一个嵌入层。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="3c4e" class="mr lg jj mn b gy ms mt l mu mv">Embedding(<!-- -->input_dim=<!-- -->7, <!-- -->output_dim=<!-- -->2, input_length=5)</span></pre><ul class=""><li id="ee2f" class="nf ng jj ki b kj kk kn ko kr nh kv ni kz nj ld nk nl nm nn bi translated"><strong class="ki jk"> input_dim </strong>:训练集中词汇的大小(即不同单词的数量)</li><li id="5783" class="nf ng jj ki b kj no kn np kr nq kv nr kz ns ld nk nl nm nn bi translated"><strong class="ki jk"> output_dim </strong>:嵌入向量的大小</li><li id="1d2f" class="nf ng jj ki b kj no kn np kr nq kv nr kz ns ld nk nl nm nn bi translated"><strong class="ki jk"> input_length </strong>:样本中特征的个数(即每个文档的字数)。例如，如果我们所有的文档都由 1000 个单词组成，那么输入长度将是 1000。</li></ul><p id="e500" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嵌入的工作方式类似于查找表。每个记号(即单词)充当存储向量的索引。当一个令牌被提供给嵌入层时，它返回与该令牌相关联的向量，并通过神经网络传递该向量。随着网络的训练，嵌入也被优化。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="1f91" class="mr lg jj mn b gy ms mt l mu mv">+------------+------------+<br/>|   index    |  Embedding |<br/>+------------+------------+<br/>|     0      | [1.2, 3.1] |<br/>|     1      | [0.1, 4.2] |<br/>|     2      | [1.0, 3.1] |<br/>|     3      | [0.3, 2.1] |<br/>|     4      | [2.2, 1.4] |<br/>|     5      | [0.7, 1.7] |<br/>|     6      | [4.1, 2.0] |<br/>+------------+------------+</span></pre><p id="dc2b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">比方说，我们有下面的单词<strong class="ki jk">教师</strong>的二维嵌入向量。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ae8c4a9363e09b00afa2980517f491f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*CAF7ViEeLwR9nbvOVtAMlg.png"/></div></figure><p id="86b4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以想象一个二维空间，其中相似的单词(即学校、导师)聚集在一起。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/3e03ef0b31f14cc41df6fde263c292be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*O9F7ZIw5LBv_Qnfs4V3euw.png"/></div></figure><p id="9bf4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的例子中，我们使用 16 维的嵌入向量。因此，我们可能会发现<strong class="ki jk">享受、喜欢和棒极了</strong>这几个词彼此非常接近。然后，我们的模型可以学习将其单词映射到 16 维空间中彼此接近的嵌入向量的评论分类为正面的。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="4d4d" class="mr lg jj mn b gy ms mt l mu mv">model = Sequential()<br/>model.add(Embedding(input_dim==num_words, output_dim=16, <!-- -->input_length=256<!-- -->))<br/>model.add(GlobalAveragePooling1D())<br/>model.add(Dense(16, activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))</span><span id="5ab8" class="mr lg jj mn b gy mz mt l mu mv">model.summary()</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/f9443d915e24b6afe7524b2efcfb393b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*nWpAZMWk3Rxr-hqmoln0UA.png"/></div></figure><p id="0e40" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用<strong class="ki jk">亚当</strong>作为我们的优化器，使用<strong class="ki jk">二元</strong> <strong class="ki jk">交叉熵</strong>作为我们的损失函数，因为我们试图在两个类之间进行选择。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="f74b" class="mr lg jj mn b gy ms mt l mu mv">model.compile(<br/>    optimizer='adam',<br/>    loss='binary_crossentropy',<br/>    metrics=['accuracy']<br/>)</span></pre><p id="a993" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们留出 10%的数据进行验证。每个时期，在更新权重之前，512 个评论通过神经网络。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="43b1" class="mr lg jj mn b gy ms mt l mu mv">history = model.fit(<br/>    X_train,<br/>    y_train,<br/>    epochs=20,<br/>    batch_size=512,<br/>    validation_split=0.1,<br/>    shuffle=True<br/>)</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/312afdcbab3f869a82920b491ce4f38d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sboMwu4-Q5k9TW2Yr-02JA.png"/></div></div></figure><p id="9ca8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过使用由<em class="le"> fit </em>函数返回的历史变量来绘制每个时期的训练和验证准确度和损失。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="e826" class="mr lg jj mn b gy ms mt l mu mv">loss = history.history['loss']<br/>val_loss = history.history['val_loss']<br/>epochs = range(1, len(loss) + 1)<br/>plt.plot(epochs, loss, 'y', label='Training loss')<br/>plt.plot(epochs, val_loss, 'r', label='Validation loss')<br/>plt.title('Training and validation loss')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/be4512e71d6ae583afe982ac8efbed99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*Es_CZKgxOdN5VD5zbu6hFg.png"/></div></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="f06e" class="mr lg jj mn b gy ms mt l mu mv">acc = history.history['acc']<br/>val_acc = history.history['val_acc']<br/>plt.plot(epochs, acc, 'y', label='Training acc')<br/>plt.plot(epochs, val_acc, 'r', label='Validation acc')<br/>plt.title('Training and validation accuracy')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Accuracy')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/3e68e09cc40f40ed18da9ac4f09f34cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*7nAbPf7URyJtBzulgBXyWQ.png"/></div></figure><p id="3880" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，让我们看看我们的模型在测试集上的表现如何。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="209d" class="mr lg jj mn b gy ms mt l mu mv">test_loss, test_acc = model.evaluate(X_test, y_test)<br/>print(test_acc)</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/5a3e2e83ac4ece6c8d916df77a72d3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*DF9_529xTXMN4bDP8m9mUA.png"/></div></figure><h1 id="6a1f" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">最后的想法</h1><p id="858d" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">当我们处理具有许多类别(即单词)的分类特征时，我们希望避免使用一种热编码，因为它需要我们在内存中存储一个大矩阵并训练许多参数。相反，我们可以将每个类别映射到一个<strong class="ki jk"> <em class="le"> n </em> </strong>维嵌入向量，并使用嵌入向量作为输入来训练我们的机器学习模型。</p></div></div>    
</body>
</html>