<html>
<head>
<title>restoration gains with GANs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">gan 的恢复收益</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/restoration-gains-with-gans-faad910c8b66?source=collection_archive---------25-----------------------#2019-11-21">https://towardsdatascience.com/restoration-gains-with-gans-faad910c8b66?source=collection_archive---------25-----------------------#2019-11-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2c59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇博文中，我将讨论 GANs 以及如何使用它来恢复图像。我将使用 fastai 库进行实践。我建议读者也去探索一下 fastai 。我们开始吧。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/a798a9030fc9923277de430b6b6912e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PStRrCrQvbxCbmHH63fk0Q.jpeg"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Image by <a class="ae kl" href="https://pixabay.com/users/Didgeman-153208/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4459235" rel="noopener ugc nofollow" target="_blank">Thomas B.</a> from <a class="ae kl" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4459235" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><h2 id="f263" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">什么是图像恢复？</h2><p id="0a91" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">图像恢复是从失真图像中恢复原始图像的过程。图像恢复有多种类型，例如:</p><ul class=""><li id="da54" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">拍摄低分辨率图像并将其转换为高分辨率图像</li><li id="d999" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">将黑白图像转换为彩色图像</li><li id="bc3d" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">恢复图像的破损部分</li></ul><p id="d465" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还有很多。</p><p id="be4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来处理第一种，即将图像上带有一些不想要的文本的低分辨率图像转换成高分辨率的清晰图像。我们需要一个图像数据集来恢复，fastai 为我们提供了图像数据集。让我们使用它。</p><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="d612" class="lc ld iq mp b gy mt mu l mv mw">import fastai<br/>from fastai.vision import *<br/>from fastai.callbacks import *<br/>from fastai.vision.gan import *</span><span id="2205" class="lc ld iq mp b gy mx mu l mv mw"><strong class="mp ir">path</strong> = untar_data(URLs.PETS)<br/><strong class="mp ir">path_hr</strong> = path/'images'<br/><strong class="mp ir">path_lr</strong> = path/'crappy'</span></pre><p id="8df4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">path _ HR</strong>—POSIX path('/root/)。fastai/data/Oxford-iiit-pet/images ')<br/><strong class="jp ir">path _ lr</strong>—POSIX path('/root/)。fastai/data/Oxford-iiit-pet/crappy’)</p><p id="b205" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们需要扭曲图像，我们可以创建任何扭曲图像的函数。下面，我将使用一个函数，在这个函数中，我将图像扭曲到低分辨率，上面有文字。让我们来理解一下。</p><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="48c8" class="lc ld iq mp b gy mt mu l mv mw">from fastai.vision import *<br/>from PIL import Image, ImageDraw, ImageFont</span><span id="5a94" class="lc ld iq mp b gy mx mu l mv mw">class crappifier(object):<br/>    def __init__(self, path_lr, path_hr):<br/>        <strong class="mp ir">self.path_lr</strong> = path_lr<br/>        <strong class="mp ir">self.path_hr</strong> = path_hr</span><span id="e267" class="lc ld iq mp b gy mx mu l mv mw">def __call__(self, fn, i):<br/>        <strong class="mp ir">dest</strong> = self.path_lr/fn.relative_to(self.path_hr)<br/>        dest.parent.mkdir(parents=True, exist_ok=True)<br/>        <strong class="mp ir">img_open</strong> = PIL.Image.open(fn)<br/>        <strong class="mp ir">targ_sz</strong> = resize_to(img_open, 96, use_min=True)<br/>        <strong class="mp ir">img</strong> = img.resize(targ_sz,     resample=PIL.Image.BILINEAR).convert('RGB')<br/>        <strong class="mp ir">w,h</strong> = img.size<br/>        <strong class="mp ir">q</strong> = random.randint(10,70)<br/>        ImageDraw.<br/>          <strong class="mp ir">Draw</strong>(img).<br/>          <strong class="mp ir">text</strong>((random.randint(0,w//2),random.randint(0,h//2)), str(q), fill=(255,255,255))</span><span id="60eb" class="lc ld iq mp b gy mx mu l mv mw"><strong class="mp ir"><em class="my">img.save(dest, quality=q)</em></strong></span></pre><ul class=""><li id="74e4" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated"><strong class="jp ir"> dest </strong> —我们正在初始化存储垃圾图像的目标路径。</li><li id="963f" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir"> img_open </strong> —使用 PIL 库打开图像</li><li id="d2f8" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir"> targ_sz </strong> —初始化蹩脚图像的大小。</li><li id="523c" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir"> img </strong> —调整大小后的图像</li><li id="d40f" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir"> w，h </strong> —图像的宽度和高度</li><li id="938b" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">问——选择任意一个随机数显示在图像上</li><li id="efdb" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir"> Draw() — </strong>在图像上绘制文本。</li><li id="2d13" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir"> text() — </strong>查找要在图像上绘制的文本。第一个参数声明了放置文本的尺寸。第二个参数是显示在图像上的数字。</li><li id="e476" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">最后，我们将图像保存到目标文件夹，质量为 q。</li></ul><p id="cd86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是我们如何复制图像。你可以以任何方式扭曲图像。</p><blockquote class="mz"><p id="9645" class="na nb iq bd nc nd ne nf ng nh ni kk dk translated">记住一件事，任何你没有包括在<code class="fe nj nk nl mp b">crappifier()</code>中的事，模型不会学习去修正它。</p></blockquote><pre class="nm nn no np nq mo mp mq mr aw ms bi"><span id="0484" class="lc ld iq mp b gy mt mu l mv mw">il = ImageList.from_folder(path_hr)<br/>parallel(crappify, il.items)</span></pre><p id="3e04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">复制图像的过程可能需要一段时间，但 fast.ai 有一个名为<code class="fe nj nk nl mp b">parallel</code>的功能。如果你给<code class="fe nj nk nl mp b">parallel</code>一个函数名和一个运行该函数的列表，它会并行地运行这些函数。所以，会节省很多时间。</p><h2 id="8834" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">现在让我们对发电机进行预训练。这是我们处理完数据后通常要做的事情。</h2><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="af1a" class="lc ld iq mp b gy mt mu l mv mw"><strong class="mp ir">bs,size </strong>= 32, 128<br/><strong class="mp ir">arch</strong> = models.resnet34<br/><strong class="mp ir">src</strong> = ImageImageList.from_folder(path_lr).split_by_rand_pct(0.1, seed=42)</span><span id="8573" class="lc ld iq mp b gy mx mu l mv mw">def get_data(bs,size):<br/>    <strong class="mp ir">data</strong> = (src.<strong class="mp ir">label_from_func</strong>(lambda x: path_hr/x.name)<br/>               .<strong class="mp ir">transform</strong>(get_transforms(max_zoom=2.), size=size, tfm_y=True)<br/>               .<strong class="mp ir">databunch</strong>(bs=bs).normalize(imagenet_stats, do_y=True))<br/>    <br/><em class="my">    </em><strong class="mp ir">return data</strong></span></pre><p id="4fc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们将 get_data()用于预处理的图像数据集。</p><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="4c53" class="lc ld iq mp b gy mt mu l mv mw">data_gen = <strong class="mp ir">get_data</strong>(bs,size)<br/>data_gen.<strong class="mp ir">show_batch</strong>(2)</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/6fa3f326ca7a932cb68e44650bbf6810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*GHbdzAAM3VWPSR394tHJ3g.png"/></div></figure><p id="fbb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们已经创建了数据群，我们必须使用这个数据群来创建学习者。</p><p id="091c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我希望读者了解 UNets 以及我们为什么使用它们。如果你对此知之甚少或一无所知，请参考这里的<a class="ae kl" href="https://medium.com/swlh/resnets-densenets-unets-6bbdbcfdf010" rel="noopener"/>。</p><p id="1b31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们所做的是通过学习原始图像来恢复图像，而恢复就是 UNet 所执行的。我们需要通过 UNet，我们的数据。让我们利用 UNets，建立对 GANs 的需求。</p><p id="8385" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">获得给我们的只是来自不同文件夹的图像列表，按照要求进行了标准化和转换。现在，我们在标准化方法<code class="fe nj nk nl mp b">normalize(imagenet_stats, do_y=True)</code>中使用上面的 ImageNet 统计数据，因为我们将使用预训练模型。现在，我们为什么要使用预训练模型呢？我们想恢复扭曲的图像。使用至少了解动物(不是全部，而是很多)的模型来训练模型总是比使用对动物一无所知的东西来训练模型更好。此外，我们希望恢复图像，即从图像中移除我们的模型通常应该移除的不需要的文本。</p><blockquote class="mz"><p id="6146" class="na nb iq bd nc nd ne nf ng nh ni kk dk translated">一个建议是，迁移学习对几乎所有类型的计算机视觉问题都有帮助。</p></blockquote><p id="d351" class="pw-post-body-paragraph jn jo iq jp b jq ns js jt ju nt jw jx jy nu ka kb kc nv ke kf kg nw ki kj kk ij bi translated">让我们声明参数。</p><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="15b1" class="lc ld iq mp b gy mt mu l mv mw">wd = 1e-3<br/>y_range = (-3.,3.)<br/>loss_gen = MSELossFlat()</span><span id="d2ed" class="lc ld iq mp b gy mx mu l mv mw">def <strong class="mp ir">create_gen_learner</strong>():<br/>    return <strong class="mp ir">unet_learner</strong>(<strong class="mp ir">data_gen</strong>,<br/>                        <strong class="mp ir">arch</strong>, <strong class="mp ir">wd</strong>=wd,<br/>                        <strong class="mp ir">blur</strong>=True,<br/>                        <strong class="mp ir">norm_type</strong>=NormType.Weight,<br/>                        <strong class="mp ir">self_attention</strong>=True,<br/>                        <strong class="mp ir">y_range</strong>=y_range,<br/>                        <strong class="mp ir">loss_func</strong>=loss_gen)</span><span id="2569" class="lc ld iq mp b gy mx mu l mv mw"><strong class="mp ir">learn_gen</strong> = create_gen_learner()</span></pre><ul class=""><li id="543c" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated"><code class="fe nj nk nl mp b">wd</code> —描述正则化的模型的权重衰减。</li><li id="4090" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><code class="fe nj nk nl mp b">y_range</code> —这是应用于上一步获得的激活的 sigmoid 函数。这些值是为这类问题定义的。</li><li id="2a26" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><code class="fe nj nk nl mp b">loss_gen</code> —定义了损失函数。由于我们是从原始图像恢复，因此我们需要将输出与原始图像进行比较，MSE loss 非常适合它。MSE 损耗基本上是在两个输入向量之间寻找损耗。在我们的例子中，输入是图像。因此，我们需要在将图像放入损失函数之前对其进行展平。<code class="fe nj nk nl mp b">MSELossFlat()</code>基本上做同样的事情。</li><li id="c6b6" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">如果你想了解更多，在这里学习<a class="ae kl" href="https://docs.fast.ai/vision.models.unet.html#DynamicUnet" rel="noopener ugc nofollow" target="_blank">。</a></li></ul><p id="1579" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们使用预定义和预训练的模型 ResNet34 创建了 UNet 学习器。这整个过程被称为<strong class="jp ir"> <em class="my">生成学习，</em> </strong>我们使用 unet 学习器生成图像。这不是确切的定义，而是更广泛的定义。现在让我们来拟合这个模型。</p><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="8dcf" class="lc ld iq mp b gy mt mu l mv mw">learn_gen.fit_one_cycle(2, pct_start=0.8)</span></pre><ul class=""><li id="26a9" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated"><code class="fe nj nk nl mp b">pct_start</code> —代表单个历元内的迭代次数，学习率将上升，学习率将下降的迭代次数。让我们只用上面的例子来理解它。</li></ul><p id="b3a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设每个时期的迭代次数= 100 <br/>每个时期学习率增加的迭代次数= (100 * 1) * 0.8 = 80 <br/>学习率减少的迭代次数= 100–80 = 20</p><p id="7241" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型的上述拟合发生在 UNet 部分，即 UNet 的 ResNet 部分中的编码器冻结的情况下。但是，由于我们使用的是迁移学习，所以我们可以解冻 UNet 的预训练部分(<strong class="jp ir">)U-Net 的预训练部分是下采样部分。ResNet 就在那里。</strong></p><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="05fa" class="lc ld iq mp b gy mt mu l mv mw">learn_gen.unfreeze()</span><span id="2ff5" class="lc ld iq mp b gy mx mu l mv mw">learn_gen.fit_one_cycle(3, slice(1e-6,1e-3))</span><span id="b10b" class="lc ld iq mp b gy mx mu l mv mw"><strong class="mp ir">learn_gen.show_results(rows=2)</strong></span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nx"><img src="../Images/6b865f0d4b4068d2b33260940b3a2368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NguYNqZjIJ3AqG1jK_NQzA.png"/></div></div></figure><ul class=""><li id="b7dc" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">我们已经获得了一个较好的预测模型。我们的图像越快越不清晰是由于损失函数。我们使用的是 MSE 损失，即获得的图像和实际图像之间的像素差异非常小。如果删除文本是唯一的任务，那么我们已经实现了我们的目标。但是，我们也在努力提高图像分辨率。</li><li id="9aea" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">基本上，该模型在上采样中表现出色，但在下采样中表现不佳。</li><li id="66a0" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">我们要改进的只是损失函数。一个更好的损失函数会给我们更好的结果。这就是神经网络的定义。这就确立了对 GANs 的需求。</li></ul><h1 id="064d" class="ny ld iq bd le nz oa ob lh oc od oe lk of og oh ln oi oj ok lq ol om on lt oo bi translated">生成对抗网络</h1><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi op"><img src="../Images/76821d87a929c4d3329b53b74810a79b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*teeyztRl5b5qTZXpn39QbA.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">image source — fastai</figcaption></figure><p id="da3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们理解 gan 背后的语义。</p><ul class=""><li id="2998" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">到目前为止，我们有一个模型，它适度地预测了与原始图像没有太大差别的图像。按照上面的图片，我们已经创建了蹩脚的图片，并且生成器也没有生成如此糟糕的图片。然后，我们使用像素 MSE 来比较预测图像和高分辨率图像。</li><li id="f02a" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">想象一下，如果我们能得到一些东西，而不是比较图像之间的像素，实际上将预测图像分类在高分辨率图像和低分辨率图像之间。如果我们可以欺骗二进制分类器，让它开始将生成的图像分类到高分辨率图像，那就更有趣了。</li><li id="6312" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">一旦我们开始愚弄分类器，那么我们将训练分类器更多地预测图像的实际类别，即如果图像是生成的，那么它应该正确地预测它，如果图像是高分辨率图像，那么它应该预测它是高分辨率图像。</li><li id="c330" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">一旦分类器训练正确地预测预测图像的类别，这意味着我们不能再欺骗分类器。在这种情况下，我们将进一步训练生成器，使其能够生成更接近高分辨率图像的图像。一旦我们训练了足够多的生成器，我们就可以再次欺骗分类器。</li><li id="6af1" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">一次，我们再次开始愚弄分类器，这次我们将开始更多地训练分类器。这个训练生成器和分类器的过程实际上越来越多地归结为 GANs。</li><li id="f685" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">因此，基本上，GANs 中的损失函数调用了我们的另一个模型，而这个模型本身已经达到了最先进的结果。所有的博弈都是为了得到越来越好的损失函数。</li></ul><h1 id="a5dc" class="ny ld iq bd le nz oa ob lh oc od oe lk of og oh ln oi oj ok lq ol om on lt oo bi translated">相信我！这都是干的。</h1><p id="4d8f" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">我们已经创建了生成器。现在让我们创建分类器。但是在我们创建分类器之前，我们需要将我们的预测存储在某个地方，因为我们需要在预测图像和高分辨率图像之间进行分类。所以，让我们把预测的图像存储在某个地方。</p><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="785c" class="lc ld iq mp b gy mt mu l mv mw">name_gen = 'image_gen'<br/>path_gen = path/name_gen</span><span id="2493" class="lc ld iq mp b gy mx mu l mv mw">path_gen.mkdir(exist_ok=True)</span></pre><ul class=""><li id="2d29" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">我们已经创建了一个路径<code class="fe nj nk nl mp b">PosixPath(‘/root/.fastai/data/oxford-iiit-pet/image_gen’)]</code>,用于存储生成的图像。</li><li id="008c" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">我们已经有了存储高分辨率图像的路径<code class="fe nj nk nl mp b">PosixPath(‘/root/.fastai/data/oxford-iiit-pet/images’)</code>。</li></ul><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="3339" class="lc ld iq mp b gy mt mu l mv mw">def save_preds(dl):<br/>    i=0<br/>    names = dl.dataset.items<br/>    <br/>    for b in dl:<br/>        preds = learn_gen.pred_batch(batch=b, reconstruct=True)<br/>        for o in preds:<br/>            o.save(path_gen/names[i].name)<br/>            i += 1</span><span id="d36a" class="lc ld iq mp b gy mx mu l mv mw">save_preds(data_gen.fix_dl)</span></pre><ul class=""><li id="a0d3" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">我们已将预测保存在文件夹中。</li><li id="06a7" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><code class="fe nj nk nl mp b">data_gen.fix_dl</code>将生成固定大小的数据加载器。</li><li id="a8dd" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">然后，我们迭代数据加载器，提取特定大小的数据，并将其传递给<code class="fe nj nk nl mp b">pred_batch</code>进行预测。</li><li id="abb1" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">然后，我们将预测的图像存储到图像名称下的文件夹中。</li></ul><p id="a85e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们需要编码分类器。如果您需要了解更多关于分类器的信息，请阅读此处的<a class="ae kl" href="https://medium.com/analytics-vidhya/image-classification-using-fastai-5ff5b374d414" rel="noopener"/>。</p><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="bd4a" class="lc ld iq mp b gy mt mu l mv mw">def <strong class="mp ir">get_crit_data</strong>(classes, bs, size):<br/>    <strong class="mp ir">src</strong> = ImageList.<strong class="mp ir">from_folder</strong>(path, include=classes)<br/>                   .<strong class="mp ir">split_by_rand_pct</strong>(0.1, seed=42)<br/>    <strong class="mp ir">ll</strong> = src.<strong class="mp ir">label_from_folder</strong>(classes=classes)<br/>    <strong class="mp ir">data</strong> = (ll.<strong class="mp ir">transform</strong>(get_transforms(max_zoom=2.), size=size)<br/>              .<strong class="mp ir">databunch</strong>(bs=bs).normalize(imagenet_stats))<br/>    return <strong class="mp ir">data</strong></span><span id="f38c" class="lc ld iq mp b gy mx mu l mv mw">data_crit = <strong class="mp ir">get_crit_data</strong>([name_gen, 'images'], bs=bs, size=size)</span></pre><ul class=""><li id="ecbb" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated"><code class="fe nj nk nl mp b">from_folder</code> —从位于<code class="fe nj nk nl mp b">path</code>的文件夹中提取图像。我们希望只包含那些名称在<code class="fe nj nk nl mp b">include=classes</code>中提到的文件夹的数据。</li><li id="b203" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><code class="fe nj nk nl mp b">label_from_folder</code> —使用基本上是文件夹名称本身的类来标记图像。</li><li id="6356" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">然后，我们转换数据，创建数据串，最后将数据标准化。</li></ul><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="8597" class="lc ld iq mp b gy mt mu l mv mw">data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oq"><img src="../Images/a52b715735f23c59a6c7f8642802cd70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u1DQ8Zyh1L-2eeAaEWNB0A.png"/></div></div></figure><p id="ae68" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们为分类器定义学习者。</p><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="3b3a" class="lc ld iq mp b gy mt mu l mv mw">loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())</span><span id="56ad" class="lc ld iq mp b gy mx mu l mv mw">def <strong class="mp ir">create_critic_learner</strong>(data, metrics):<br/>    return <strong class="mp ir">Learner</strong>(data, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd)</span><span id="c772" class="lc ld iq mp b gy mx mu l mv mw">learn_critic = <strong class="mp ir">create_critic_learner</strong>(data_crit, accuracy_thresh_expand)</span></pre><ul class=""><li id="7b39" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">如果你说<code class="fe nj nk nl mp b">gan_critic</code>，fast.ai 会给你一个适合 GANs 的二元分类器。</li><li id="bb77" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">因为我们有稍微不同的架构和稍微不同的损失函数，所以我们做了稍微不同的度量。<code class="fe nj nk nl mp b">accuracy_thresh_expand</code>是相当于甘版的准确性对于评论家来说。</li><li id="d4ec" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">最后，我们调用<code class="fe nj nk nl mp b">create_critic_learner</code>来为 GANs 创建学习器。Fastai 数据块 API 有利于创建学习者。</li></ul><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="e1f7" class="lc ld iq mp b gy mt mu l mv mw">learn_critic.fit_one_cycle(6, 1e-3)</span></pre><ul class=""><li id="a624" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">现在，我们有一个学习者，他非常擅长区分预测图像和高分辨率图像。</li><li id="a038" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">这也是预测的行为，因为我们已经有图像很好的区分。</li></ul><p id="6a22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们有了一个生成器、分类器/评论家。让我们转到这一节的最后一部分。</p><h2 id="0af1" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">完成 GAN</h2><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="2f4c" class="lc ld iq mp b gy mt mu l mv mw">learn_crit=None<br/>learn_gen=None</span><span id="5150" class="lc ld iq mp b gy mx mu l mv mw"><strong class="mp ir">data_crit</strong> = <strong class="mp ir">get_crit_data</strong>(['crappy', 'images'], bs=bs, size=size)</span><span id="b3fc" class="lc ld iq mp b gy mx mu l mv mw"><strong class="mp ir">learn_crit</strong> = <strong class="mp ir">create_critic_learner</strong>(data_crit, metrics=None).load('critic-pre2')</span><span id="60e3" class="lc ld iq mp b gy mx mu l mv mw"><strong class="mp ir">learn_gen</strong> = <strong class="mp ir">create_gen_learner</strong>().load('gen-pre2')</span></pre><ul class=""><li id="f8b7" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">既然我们已经预先训练了发生器和批评家，我们现在需要让它在两者之间进行乒乓式的训练。</li><li id="db10" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">你在每一件事情上花费的时间和你使用的学习率仍然有点模糊，所以 fastai 提供了一个<code class="fe nj nk nl mp b">GANLearner</code>，你只需传入你的生成器和你的批评家(我们刚刚从我们刚刚训练的生成器和批评家中加载的)它就会继续前进</li><li id="96a1" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">当你进入<code class="fe nj nk nl mp b">learn.fit</code>时，它会为你做这件事——它会计算出训练生成器需要多少时间，然后什么时候切换到训练鉴别器/批评家，它会前后移动。</li></ul><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="0bec" class="lc ld iq mp b gy mt mu l mv mw"><strong class="mp ir">switcher</strong> = <strong class="mp ir">partial</strong>(AdaptiveGANSwitcher, critic_thresh=0.65)<br/>learn = <strong class="mp ir">GANLearner</strong>.from_learners(<strong class="mp ir">learn_gen</strong>, <br/>                                 <strong class="mp ir">learn_crit</strong>, <br/>                                 <strong class="mp ir">weights_gen</strong>=(1.,50.),        <br/>                                 <strong class="mp ir">show_img</strong>=False, <br/>                                 <strong class="mp ir">switcher</strong>=switcher,    <br/>                                 <strong class="mp ir">opt_func</strong>=partial(optim.Adam, betas=(0.,0.99)), wd=wd)</span><span id="4514" class="lc ld iq mp b gy mx mu l mv mw"><strong class="mp ir">learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))</strong></span></pre><ul class=""><li id="9979" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">甘人讨厌你训练他们时的冲劲。用动力训练他们是没有意义的，因为你一直在发电机和批评家之间切换，所以这有点困难。所以这个数字(<code class="fe nj nk nl mp b">betas=(0.,...)</code>)当你创建一个亚当优化器的时候，就是动量去的地方，所以你应该把它设置为零。</li><li id="9dc7" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">基本上，上面定义的超参数通常适用于 GAns。每个 GAN 问题都可能用到这些参数。</li></ul><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="7baf" class="lc ld iq mp b gy mt mu l mv mw">lr = 1e-4<br/>learn.fit(40,lr)</span></pre><ul class=""><li id="3dc8" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">随着生成器变得更好，鉴别器(即批评家)变得更难，然后随着批评家变得更好，生成器变得更难。训练甘的困难之一是很难知道他们做得怎么样。了解他们做得如何的唯一方法是不时地看一看结果。</li><li id="8bba" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">如果你把<code class="fe nj nk nl mp b">show_img=True</code>放入<strong class="jp ir"> GANLearner，</strong>然后<strong class="jp ir"> </strong>它实际上会在每个历元后打印出一个样本。</li></ul><p id="fd0a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">做完这些，你可以检查下面的结果。</p><pre class="kn ko kp kq gt mo mp mq mr aw ms bi"><span id="253a" class="lc ld iq mp b gy mt mu l mv mw">learn.show_results(rows=1)</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi or"><img src="../Images/e5ef3d8c9f911432ad2fe76013ee8021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4n62XS9HmWvdbiqf3WE0Ug.png"/></div></div></figure><p id="fa32" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，您可以将以前的结果与现在的结果进行比较。这就是甘斯发挥作用的地方。</p><h2 id="3477" class="lc ld iq bd le lf lg dn lh li lj dp lk jy ll lm ln kc lo lp lq kg lr ls lt lu bi translated">GANs 的缺点</h2><p id="ef68" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">我们使用了一个 critic，它没有使用任何预先训练好的模型，比如 ResNet34，而是使用了 gan_critic()。因此，我们需要一个预训练的分类器模型，它不仅在 ResNet34 上训练，而且与 GANs 兼容。</p><p id="16c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi os translated">这是我对甘斯的看法。感谢阅读，并继续探索 fastai。</p></div></div>    
</body>
</html>