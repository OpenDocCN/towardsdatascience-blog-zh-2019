<html>
<head>
<title>Trees in data science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学中的树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/trees-in-data-science-dcd9830cbfcf?source=collection_archive---------28-----------------------#2019-07-25">https://towardsdatascience.com/trees-in-data-science-dcd9830cbfcf?source=collection_archive---------28-----------------------#2019-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b5d8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在简单的决策树中导航，引导/打包，最终形成随机森林模型</h2></div><p id="82b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习中最容易解释的模型之一是 CART(分类和回归树),俗称决策树。在这篇文章中，我希望给出一个决策树的概述，一些围绕决策树的基本概念，最后是随机森林。内容如下</p><ul class=""><li id="0602" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">理解决策树</li><li id="436b" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">纯洁</li><li id="ebfb" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">自举和打包</li><li id="2d4d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">随机森林</li></ul><p id="c1bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们走吧！</p><h1 id="44f9" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">决策树</strong></h1><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mk"><img src="../Images/c1336ffa2abdb61f911f7919e4c81fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bQ_W-a2Gtb81vtAN4O9_MQ.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Basic structure of a Decision Tree (Source: <a class="ae na" href="http://cway-quantlab.blogspot.com/2017/06/optimize-trading-system-with-gradient_21.html" rel="noopener ugc nofollow" target="_blank">cway-quan</a>)</figcaption></figure><p id="cf69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在机器学习宇宙中，树实际上是真实树的颠倒版本。假设我们有一个由特征‘X’和目标‘Y’组成的数据集。决策树所做的是在 X 中寻找模式，并基于这些模式将数据集分割成更小的子集。</p><p id="d8aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面略加简化的图像中想象这些分裂。这是一份工作是否被接受的问题。“X”包含“通勤时间”、“薪水”、“免费咖啡”等特征。</p><p id="a26b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于“X”中的模式，该树被分成分支，直到它到达到达“Y”的纯答案的点。在我们的场景中，被接受的工作机会必须提供超过 50k 的薪水，通勤时间&lt; 1hr and free coffee. In this manner the tree reaches the last leaf which is a pure decision about ‘Y’.</p><h2 id="2697" class="nb lt it bd lu nc nd dn ly ne nf dp mc kr ng nh me kv ni nj mg kz nk nl mi nm bi translated"><strong class="ak"> <em class="nn">决策树中的纯度</em> </strong></h2><p id="29cd" class="pw-post-body-paragraph ki kj it kk b kl no ju kn ko np jx kq kr nq kt ku kv nr kx ky kz ns lb lc ld im bi translated">决策树根据节点的纯度进行分裂。这种纯度是基于“Y”的分布来测量的。如果我们的“Y”是连续的，我们的问题是一个回归问题，节点是基于 MSE(均方误差)分裂的。如果“Y”是离散的，我们的模型正在处理一个分类问题，需要不同的纯度测量。</p><p id="bca2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在分类案例中，广泛使用的衡量标准是基尼系数。基尼系数的公式如下:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/166728de3859189c3f20b006b059f891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*sN5E9u7Z5-dVqGr6-NBhJw.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Source: General Assembly DSI curriculum (Authors:David Yerrington, Matt Brems)</figcaption></figure><p id="3896" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当决定在给定的节点上进行哪个分裂时，它挑选从父节点到子节点的基尼不纯度下降最大的分裂。</p><h1 id="7bba" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">自举和打包</h1><p id="fc64" class="pw-post-body-paragraph ki kj it kk b kl no ju kn ko np jx kq kr nq kt ku kv nr kx ky kz ns lb lc ld im bi translated">要理解 bootstrapping 和 bagging，第一步是理解为什么首先需要它们。它基本上是试图模仿“群体的智慧”原则，即多个模型的综合结果优于单个模型的结果。下面这张由 Lorna Yen 拍摄的图片给出了一个关于系鞋带的好主意。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/3246370dde13eff55be7d945f4be7880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6UVoQM6OiwbIKISzKA706w.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">(Author: Lorna yen, <a class="ae na" rel="noopener" target="_blank" href="/an-introduction-to-the-bootstrap-method-58bcb51b4d60">Source</a>)</figcaption></figure><p id="d51d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上所示的自举只是对数据进行随机采样，并进行替换。Bagging 只是在这些样本中的每一个上构建决策树并获得总体预测的过程。概括来说，装袋包括以下步骤:</p><ol class=""><li id="29ee" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld nv lk ll lm bi translated">从大小为 n 的原始数据，用替换引导 k 个大小为 n 的样本</li><li id="e9d9" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nv lk ll lm bi translated">在每个引导样本上构建一个决策树。</li><li id="f93c" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld nv lk ll lm bi translated">通过所有采油树传递测试数据，并开发一个综合预测</li></ol><p id="eb26" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Bagging 因此也被称为<strong class="kk iu">自举聚合</strong>。</p><h1 id="2ef8" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">随机森林模型</h1><p id="8357" class="pw-post-body-paragraph ki kj it kk b kl no ju kn ko np jx kq kr nq kt ku kv nr kx ky kz ns lb lc ld im bi translated">仔细观察下面的图片，你会对随机森林有一个基本的直觉。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nw"><img src="../Images/4a52d6af6e3449ff5085868bb7c8e898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*58f1CZ8M4il0OZYg2oRN4w.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Source: globalsoftwaresupport.com, (<a class="ae na" href="https://www.globalsoftwaresupport.com/random-forest-classifier-bagging-machine-learning/" rel="noopener ugc nofollow" target="_blank">link</a>)</figcaption></figure><p id="5648" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">bagging 的一个基本障碍是各个决策树高度相关，因为所有的树都使用相同的特征。所以我们模型的预测受到方差问题的困扰。要了解更多关于方差或偏差的信息，您可以阅读此<a class="ae na" rel="noopener" target="_blank" href="/bias-variance-and-regularization-f3a0eefe99af">链接</a>。去相关我们的模型是一个解决方案，这正是随机森林所做的。</p><p id="ecde" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机森林在 bagging 中遵循类似的步骤，除了它们在学习过程中的每个分裂处使用特征的<strong class="kk iu">随机子集。这减轻了装袋中的差异问题，并且通常产生更好的结果。这种有效而简单的方法使随机森林成为广泛实施的机器学习模型。</strong></p><h1 id="53a4" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated"><strong class="ak">奖金</strong></h1><p id="5c1e" class="pw-post-body-paragraph ki kj it kk b kl no ju kn ko np jx kq kr nq kt ku kv nr kx ky kz ns lb lc ld im bi translated">用于在 sklearn 中导入解释的三个分类模型的代码。</p><pre class="ml mm mn mo gt nx ny nz oa aw ob bi"><span id="dc30" class="nb lt it ny b gy oc od l oe of">#Import for decision trees<br/><strong class="ny iu">from</strong> <strong class="ny iu">sklearn.tree</strong> <strong class="ny iu">import</strong> DecisionTreeClassifier</span><span id="e00a" class="nb lt it ny b gy og od l oe of">#Import for bagging<br/><strong class="ny iu">from</strong> <strong class="ny iu">sklearn.ensemble</strong> <strong class="ny iu">import</strong> BaggingClassifier</span><span id="0211" class="nb lt it ny b gy og od l oe of">#Import for random Forests<br/><strong class="ny iu">from</strong> <strong class="ny iu">sklearn.ensemble</strong> <strong class="ny iu">import</strong> RandomForestClassifier<br/></span></pre></div></div>    
</body>
</html>