<html>
<head>
<title>Review: MSDNet — Multi-Scale Dense Networks (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:MSDNet——多尺度密集网络(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-msdnet-multi-scale-dense-networks-image-classification-4d949955f6d5?source=collection_archive---------9-----------------------#2019-03-10">https://towardsdatascience.com/review-msdnet-multi-scale-dense-networks-image-classification-4d949955f6d5?source=collection_archive---------9-----------------------#2019-03-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a42d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于有限计算能力设备的使用多尺度 DenseNet 的资源有效的图像分类</h2></div><p id="e647" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事里，<strong class="kh ir"> MSDNet(多尺度密集网络)</strong>，由<strong class="kh ir">康乃尔大学</strong>、<strong class="kh ir">复旦大学</strong>、<strong class="kh ir">清华大学</strong>、<strong class="kh ir">脸书 AI Research (FAIR) </strong>进行回顾。作者在 2017 年 CVPR 发明了<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>(超过 2900 次引用的最佳论文奖)，他们在<strong class="kh ir"> 2018 ICLR </strong>提出了<strong class="kh ir">数十次引用</strong>的 MSDNet，这是一个<strong class="kh ir">多尺度</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="kh ir">dense net</strong></a>。(<a class="ll lm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----4d949955f6d5--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="cf88" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用 MSDNet，训练具有不同资源需求的多个分类器，使得测试时间可以自适应地变化。因此，<strong class="kh ir">借助高计算设备，图像可以通过网络进行图像分类</strong>。<strong class="kh ir">利用资源有限的设备，图像可以提前退出网络，进行相对较粗的图像分类</strong>。让我们看看它是如何工作的。</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="e87b" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">概述</h1><ol class=""><li id="23a3" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la mt mu mv mw bi translated"><strong class="kh ir">具有计算约束的图像分类概念</strong></li><li id="cb16" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir"> MSDNet 架构</strong></li><li id="e936" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">随时分类和预算批量分类中的评估</strong></li><li id="846c" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">网络约简与懒评</strong></li><li id="c41b" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">结果</strong></li></ol></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="b888" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 1。具有计算约束的图像分类概念</strong></h1><p id="309a" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">具有计算约束的影像分类有两种设置</p><h2 id="691a" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated"><strong class="ak"> 1.1。随时分类</strong></h2><ul class=""><li id="35a5" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated">网络可以被强制在任何给定的时间点输出预测。</li><li id="d560" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">比如:<strong class="kh ir">安卓设备上的移动应用</strong>。</li></ul><h2 id="7bd8" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated"><strong class="ak"> 1.2。预算批次分类</strong></h2><ul class=""><li id="eb27" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated">一个固定的计算预算在大量的例子中共享，在“简单”和“困难”的例子中花费不均衡。这对<strong class="kh ir">大规模机器学习应用</strong>很有用。</li><li id="fbe2" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">比如:<strong class="kh ir">搜索引擎</strong>，<strong class="kh ir">社交媒体公司</strong>，<strong class="kh ir">网络广告代理</strong>，都必须在有限的硬件资源上处理大量的数据。</li><li id="31ad" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">截至 2010 年，谷歌图片搜索已经索引了超过 100 亿张图片，此后可能会增长到超过 1 万亿张。即使处理这些图像的新模型只比每幅图像慢 1/10 秒，这一额外成本也会增加 3170 年的 CPU 时间。</li><li id="3bd4" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在预算批次分类设置中，公司可以<strong class="kh ir">通过减少花费在“简单”案例上的计算量来提高平均准确性，从而为“困难”案例节省计算量。</strong></li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5d7121c28c99065d92823b828e5e265e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*sIShPbt9QKqjnFs_z1p4TA.png"/></div><figcaption class="oa ob gj gh gi oc od bd b be z dk"><strong class="bd oe">An Illustrative Example of MSDNet</strong></figcaption></figure><ul class=""><li id="09b2" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated">如上图，MSDNet 是多尺度<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>。上面的路径用于未缩小的图像，下面的路径用于较小比例的图像。</li><li id="2d09" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">比如说，有一个我们想要分类的猫的图像，通过网络，也许<strong class="kh ir">猫的分类有一个大于阈值的 0.6 的分类置信度，我们可以提前退出。</strong>下面的网络可以跳过<strong class="kh ir">节省“简易”图像的计算时间。</strong></li><li id="9438" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">另一方面，对于“硬”图像，我们可以通过更深的网络，直到分类置信度高于阈值。</li><li id="451e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">因此，通过利用花费在“容易”和“困难”图像上的时间，可以节省计算时间。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="ead6" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 2。MSDNet 架构</strong></h1><h2 id="81d5" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">2.1.体系结构</h2><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi oi"><img src="../Images/2d60c38783c8876b31f0b68d8168d913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WNj1D3cywkTpWfItsgroiw.png"/></div></div><figcaption class="oa ob gj gh gi oc od bd b be z dk"><strong class="bd oe">MSDNet Architecture</strong></figcaption></figure><ul class=""><li id="2320" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated">其纵向布局如同一个微型的<strong class="kh ir"><em class="on">S</em>-layers”卷积网络(<em class="on"> S </em> =3) </strong>。</li><li id="360a" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">对于第一层(<em class="on"> l </em> = 1)，通过下采样获得较粗尺度的特征图。</li><li id="7da4" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">对于<em class="on"> l </em> =1 且比例<em class="on"> s </em>的后续图层，来自比例<em class="on"> s </em>和<em class="on"> s </em> -1 的所有先前特征地图的特征地图被连接。使用 conv(1×1)-BN-ReLU-conv(3×3)-BN-ReLU。</li><li id="568c" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">更准确地说，下图和下表显示了在某些<em class="on"> s </em>和<em class="on"> l </em>处使用的特征图。</li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi oo"><img src="../Images/2307685cf75341209ad212de6f4cc5a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8a5jDfNYKoVAw9Sppzg3Wg.png"/></div></div><figcaption class="oa ob gj gh gi oc od bd b be z dk"><strong class="bd oe">The feature maps used at certain <em class="op">s</em> and <em class="op">l</em></strong></figcaption></figure><ul class=""><li id="7f8b" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated"><strong class="kh ir">在某些位置，有一些中间分类器被插入到网络的中间。</strong></li><li id="7455" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">每个分类器有两个带 128 维 3×3 滤波器的下采样卷积层，后面是一个 2×2 平均池层和一个线性层。</li><li id="e14e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在训练过程中，<strong class="kh ir">逻辑损失函数<em class="on"> L </em> ( <em class="on"> fk </em>)用于每个分类器</strong>，使加权累积损失最小化:</li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/d832369475ca14d23583db406b651fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*5-xZ7Gd5pnx9CGuph3P8Lg.png"/></div></figure><ul class=""><li id="d265" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated">其中 d 表示训练集，<em class="on"> wk </em> ⩾ 0 表示分类器的权重<em class="on"> k </em>。</li><li id="ec28" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><em class="on"> wk </em> =1 凭经验。</li></ul><h2 id="5050" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">2.2.使用不同网络的中间分类器的评估<strong class="ak"/></h2><ul class=""><li id="0b7a" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated">你可能会问，为什么不直接在<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>或者<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>中插入中间分类器呢？为什么我们必须需要 MSDNet？作者也对此进行了评估。主要有两个原因。</li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi or"><img src="../Images/9c8af2e2807269f40afe2ef2050d39b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fo2SjlhcSjsXKjwPmfewyQ.png"/></div></div><figcaption class="oa ob gj gh gi oc od bd b be z dk"><strong class="bd oe">Evaluation of Intermediate Classifiers Using Different Networks on CIFAR-100 Dataset</strong></figcaption></figure><h2 id="529f" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">2.2.1.第一个原因</h2><ul class=""><li id="acc5" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated"><strong class="kh ir">问题:缺乏粗层次的特征。</strong>传统的神经网络学习早期层的细尺度和后期层的粗尺度的特征。早期层缺乏粗级别特征，并且附属于这些层的早期退出分类器将可能产生不令人满意的高错误率。</li><li id="3689" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">上图左侧显示了中间分类器的结果，它们也被插入到<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>中。<strong class="kh ir">分类器的准确性与其在网络中的位置高度相关。</strong>特别是在<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>(蓝线)的情况下，可以观察到一个可见的“楼梯”图案，在第二和第四分类器之后有很大的改进——位于合并层之后。</li><li id="e657" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">解决方案:多尺度特征图</strong>。MSDNet 在整个网络中保持多个尺度的特征表示，并且所有分类器仅使用粗级别的特征。</li><li id="2bc7" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">横向连接保存并推进高分辨率信息</strong>，有利于后续图层高质量粗特征的构建。<strong class="kh ir">垂直连接产生易于分类的粗糙特征</strong>。</li></ul><h2 id="d4a0" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">2.2.2.第二个原因</h2><ul class=""><li id="02f4" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated"><strong class="kh ir">问题:前期量词干扰后期量词。</strong>上图右侧显示了作为单个中间分类器位置的函数的最终分类器的精度，相对于没有中间分类器的网络的精度。</li><li id="0f33" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">一个中间分类器的引入损害了最终的</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kh ir"> ResNet </strong> </a> <strong class="kh ir">分类器(蓝线)，降低其准确率高达 7%。在<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>中，这种精度下降可能是由影响早期特征的中间分类器引起的，这些早期特征是为短期而不是为最终层优化的。</strong></li><li id="90c3" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">解决方案:密集连接。</strong>相比之下，<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>(红线)受此影响要小得多。这是因为在<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>中，特征图是使用连接而不是在<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>中使用加法来组合的。早期图层的要素地图可以通过密集连接绕过后期图层。最终分类器的性能变得(或多或少)独立于中间分类器的位置。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="f4bc" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">3.<strong class="ak">随时分类和预算批量分类中的评估</strong></h1><h2 id="364c" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">3.1.随时分类</h2><ul class=""><li id="e309" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated">在任意时间分类中，每个测试实例都有<strong class="kh ir"> a 有限的计算预算<em class="on">B</em>T52】0 可用。</strong></li><li id="a4f8" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在任意时间设置的测试期间，<strong class="kh ir">输入通过网络传播，直到预算<em class="on"> B </em>用尽，并输出最近的预测。</strong></li></ul><h2 id="f2e3" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">3.2.预算批分类</h2><ul class=""><li id="0d93" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated">在预算批量分类中，模型需要<strong class="kh ir">在预先已知的有限计算预算<em class="on">B</em>T53】0</strong>内对一组实例 Dtest = { <em class="on"> x </em> 1，…，<em class="on"> xM </em> }进行分类。</li><li id="71fd" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">它可以通过<strong class="kh ir">花费少于<em class="on"> B </em> / <em class="on"> M </em>的计算对一个“简单”的例子</strong>进行分类，而<strong class="kh ir">使用多于<em class="on"> B </em> / <em class="on"> M </em>的计算对一个“困难”的例子</strong>进行分类。</li><li id="b130" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">所以这里考虑的预算<em class="on"> B </em>是我们有大批量测试样本时的软约束。</li><li id="628e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">然后，使用<strong class="kh ir">动态评估</strong>来解决这个问题:</li><li id="5008" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在测试时间，<strong class="kh ir">如果一个示例的预测置信度</strong>(作为置信度度量的 softmax 概率的最大值)<strong class="kh ir">超过预定阈值<em class="on"> θk </em>，则该示例遍历网络并在分类器<em class="on"> fk </em>之后退出。</strong></li><li id="ce26" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">在训练之前，我们计算处理网络直到第<em class="on"> k </em>个分类器所需的计算成本<em class="on"> Ck </em>。</strong></li><li id="47c8" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">我们用<strong class="kh ir"> 0 &lt; q ≤ 1 表示一个固定的退出概率，即到达分类器的样本将获得一个有足够置信度退出</strong>的分类。</li><li id="30df" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">样本在分类器<em class="on">k</em>T19】处存在的概率:</strong></li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div class="gh gi os"><img src="../Images/77d17f3eda0c7cae212a5ab28c6458c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*K3dO_fQI-eIB4nyPwux_EA.png"/></div></figure><ul class=""><li id="195e" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated">其中 z 是归一化常数，使得:</li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/36c9566c737ce4d103aa5de15e80fb41.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/format:webp/1*NxpkAXzf2uMB3xxaCDIz3A.png"/></div></figure><ul class=""><li id="38dd" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated">我们需要确保在 Dtest 中对所有样本进行分类的总成本不超过我们的预算 B，这就产生了约束条件:</li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/0e8b899e5b5158348f3e39bdd5018296.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*zAOem28_oR4SvTZB3N4HsA.png"/></div></figure><ul class=""><li id="ed44" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated">然后，我们可以<strong class="kh ir">为<em class="on"> q </em>解决上述问题，并在保留/确认集合上分配阈值<em class="on"> θk </em>，使得大约一小部分<em class="on"> qk </em>确认样本在第<em class="on"> k </em>分类器处退出。</strong></li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="55dc" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 4。网络还原和懒评</strong></h1><ul class=""><li id="f993" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated">有两种简单的方法可以进一步降低 MSDNets 的计算要求。</li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi ov"><img src="../Images/ff8f7087a4c83c34f07edbe1517996c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QAmO6PHSNKbQFePK__SsSQ.png"/></div></div></figure><ul class=""><li id="a146" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated">首先，在网络的最后一层之前维护所有更精细的尺度是低效的。减小网络大小的一个简单策略是通过沿着深度维度将其分成<em class="on"> S </em>个块，并且<strong class="kh ir">仅保留第<em class="on"> i </em>个块中的最粗略的(<em class="on"> S </em> - <em class="on"> i </em> +1)尺度，如上所示。</strong>这降低了训练和测试的计算成本。</li><li id="3ce7" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">第二，由于层<em class="on"> l </em>的分类器仅使用来自最粗尺度的特征，层<em class="on"> l </em>中的更精细特征图(以及先前的<em class="on"> S </em> -2 层中的一些更精细特征图)不影响该分类器的预测。因此，<strong class="kh ir">“对角块”中的计算被分组为</strong>，使得我们<strong class="kh ir">仅沿着下一个分类器的评估所需的路径传播该示例。当我们因为计算预算耗尽而需要停止时，这可以最大限度地减少不必要的计算。这个策略叫做<strong class="kh ir">懒评估。</strong></strong></li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="3de7" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 5。结果</strong></h1><h2 id="60b7" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">5.1.数据集</h2><ul class=""><li id="6bce" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated"><strong class="kh ir"> CIFAR-10 &amp; CIFAR-100 </strong>:两个 CIFAR 数据集包含 50000 个训练和 10000 个 32×32 像素的测试图像。<strong class="kh ir">5000 幅训练图像作为验证集。</strong>数据集分别包括 10 个和 100 个类。标准数据扩充、随机裁剪和水平翻转被应用于训练集。小批量是 64 个。</li><li id="762e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">ImageNet</strong>:ImageNet 数据集包含 1000 个类，共有 120 万张训练图像和 5 万张验证图像。<strong class="kh ir">从训练集中挑选出 50，000 幅图像来估计 MSDNet 中分类器的置信度阈值。</strong>应用标准数据扩充。在测试时，224×224 中心裁剪的图像被调整为 256×256 像素用于分类。小批量是 256 个。</li><li id="d9e0" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在 ImageNet 上，使用了 4 种比例尺，即<em class="on"> S </em> =4，每层分别生成 16、32、64 和 64 幅特征图。原始图像在进入第一层 MSDNets 之前，首先通过 7×7 卷积和 3×3 最大池(都具有步长 2)进行变换。</li></ul><h2 id="36a9" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">5.2.消融研究</h2><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/e3e14ffb6e48642596f87b580da1ae05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*qhL5FtlkrU1qE8T-tOKT5Q.png"/></div><figcaption class="oa ob gj gh gi oc od bd b be z dk"><strong class="bd oe">Ablation Study on CIFAR-100</strong></figcaption></figure><ul class=""><li id="6977" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated">使用具有六个中间分类器的 MSDNet，并且三个主要组件，<strong class="kh ir">多尺度特征图</strong>、<strong class="kh ir">密集连通性</strong>和<strong class="kh ir">中间分类器</strong>被一次移除一个。</li><li id="623b" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">如果去掉 MSDNet 中的所有三个部分，就得到一个规则的类似 VGG 的卷积网络。</li><li id="91a1" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">为了使我们的比较公平，我们通过调整网络宽度，即每层输出通道的数量，使整个网络的计算成本保持相似，约为 3.0×10⁸浮点。</li><li id="8124" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">最初的 MSDNet(黑色)当然具有最高的准确性。</li><li id="8af3" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">移除密集连接(橙色)后，整体精度会受到严重影响。</li><li id="3b8b" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">加上移除的多尺度卷积(浅蓝色)，精度仅在较低预算区域受到损害。这与作者的动机一致，即多尺度设计在早期引入了区别性特征。</li><li id="6e46" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">作者还提到，去掉所有 3 个组件后，在特定预算下，它(Star)的性能与 MSDNet 相似。(但是我在图上找不到星星……)</li></ul><h2 id="f88f" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">5.3.随时分类</h2><ul class=""><li id="869f" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated">MSDNet 网络有 24 层。</li><li id="4fc1" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">分类器对第 2×( <em class="on"> i </em> +1)层的输出进行操作，其中<em class="on"> i </em> =1，…，11。</li><li id="a7de" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在 ImageNet 上，第<em class="on"> i </em>个分类器对第(<em class="on"> k </em> × <em class="on"> i </em> +3)层进行操作，i=1，…，5，其中 k=4，6，7。</li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi ox"><img src="../Images/9cdd4e8158fd2b5c2ad0933da7867e5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FTI1JPQ1e_JuyfZqF2A_yA.png"/></div></div><figcaption class="oa ob gj gh gi oc od bd b be z dk"><strong class="bd oe">Top-1 Accuracy of Anytime Classification on ImageNet (Left),CIFAR-100 (Middle) &amp; CIFAR-10 (Right)</strong></figcaption></figure><ul class=""><li id="19e4" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated"><strong class="kh ir"> ResNetMC </strong> : <a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>具有 MC(多分类器)，62 层，每个空间分辨率具有 10 个残差块(对于三个分辨率):早期退出分类器在每个分辨率的第 4 和第 8 个残差块的输出上，产生总共 6 个中间分类器(加上最终分类层)。</li><li id="f233" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">dense netm</strong>:<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803">dense netm</a>带 MC，52 层，三个密集区块，每个区块 16 层。六个中间分类器被附加到每个块中的第 6 和第 12 层，也与该块中的所有先前层紧密连接。</li><li id="864e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">完全评估时，ResNetMC 和 DenseNetMC 都需要大约 1.3×10⁸浮点运算。</li><li id="0116" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">具有不同深度</strong>的<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">resnet</strong></a><strong class="kh ir">/</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="kh ir">dense net</strong></a><strong class="kh ir">的系综也被评估。在测试时，按顺序(按网络大小的升序)评估网络，以获得测试数据的预测。所有预测在评估的分类器上平均。在 ImageNet 上，<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">和</a><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803">的集合分别生成深度从 10 层到 50 层和 36 层到 121 层不等的</a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803">的集合。</a></strong></li><li id="8146" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在 CIFAR-100 上，MSDNet 在任何范围内都大大优于 ResNetMC 和 DenseNetMC。这是由于在仅仅几层之后，MSDNets 已经产生了低分辨率特征图，这些低分辨率特征图比<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNets </a>或<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNets </a>的早期层中的高分辨率特征图更适合分类。</li><li id="61c9" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在极低预算制度下，集合具有优势，因为它们的预测是由第一(小)网络执行的，该网络专门针对低预算进行优化。然而，当预算增加时，集合的精度不会增加得那么快。</li><li id="75b3" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">与 MSDNets 不同，<strong class="kh ir">集成重复相似的低级特征的计算。</strong></li><li id="688a" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">当所有的网络都很浅时，集合精度很快饱和。</strong></li></ul><h2 id="b940" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">5.4.预算批分类</h2><ul class=""><li id="4b6c" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated">在 CIFAR-10 和 CIFAR-100 上，MSDNet 网络从 10 层到 36 层不等。第<em class="on"> k </em>个分类器被附加到{1+...+ <em class="on"> k </em> }层。</li><li id="d2cf" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在 ImageNet 上，使用与任何时候分类中的网络相同的网络。</li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi oy"><img src="../Images/318e4f918fa7a5d54dc68fe95fb60d30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gLAFwG-szrRRHdvOdtfvxA.png"/></div></div><figcaption class="oa ob gj gh gi oc od bd b be z dk"><strong class="bd oe">Top-1 Accuracy of Budgeted Batch Classification on ImageNet (Left),CIFAR-100 (Middle) &amp; CIFAR-10 (Right)</strong></figcaption></figure><ul class=""><li id="8658" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated">在预算批分类中，预测模型接收一批<em class="on"> M </em>实例和用于分类所有<em class="on"> M </em>实例的计算预算<em class="on"> B </em>。使用动态评估。</li><li id="eadf" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在 ImageNet 上，<em class="on"> M </em> =128，比较了五个<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803">densenet</a>，五个<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">resnet</a>，一个 AlexNet，一个 GoogLeNet。</li><li id="0353" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">五个</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet</strong></a><strong class="kh ir">的集合:“易”图像仅通过最小的</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet-10</strong></a>传播，而<strong class="kh ir">“难”图像由所有五个</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet</strong></a><strong class="kh ir">模型分类。</strong>(预测是集合中所有评估网络的平均值)。</li><li id="0936" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在 CIFAR-100 上，<em class="on"> M </em> =256，<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNets </a>，<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNets </a>，<a class="ae lk" rel="noopener" target="_blank" href="/review-stochastic-depth-image-classification-a4e225807f4a">随机深度网</a>，<a class="ae lk" rel="noopener" target="_blank" href="/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004">宽 ResNets </a>，<a class="ae lk" href="https://medium.com/datadriveninvestor/review-fractalnet-image-classification-c5bdd855a090" rel="noopener"> FractalNets </a>，ResNetMC 和 DenseNetMC 进行了比较。</li><li id="9e9a" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">如上图所示，<strong class="kh ir">使用了三个不同深度的 MSD net</strong>，这样它们就可以组合在一起覆盖大范围的计算预算。</li><li id="7d6d" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">例如，在 ImageNet 上，<strong class="kh ir">以 1.7×10⁹ FLOPs 的平均预算，MSDNet 实现了大约 75%的顶级精度，比具有相同 FLOPs 数量的</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kh ir"> ResNet </strong> </a> <strong class="kh ir">实现的精度高大约 6%。</strong></li><li id="ef7f" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir">与计算效率高的</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="kh ir">dense nets</strong></a><strong class="kh ir">相比，MSDNet 使用约 2 至 3 倍的 FLOPs </strong>来实现相同的分类精度。</li><li id="6f21" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在 CIFAR-100 上，MSDNets 在所有预算中的表现始终优于所有基准。</li><li id="0d21" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir"> MSDNet 的性能与 110 层的</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet</strong></a><strong class="kh ir">不相上下，只使用 1/10 的计算预算。</strong></li><li id="7118" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><strong class="kh ir"> MSDNet 比</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="kh ir">dense nets</strong></a><strong class="kh ir"/><a class="ae lk" rel="noopener" target="_blank" href="/review-stochastic-depth-image-classification-a4e225807f4a"><strong class="kh ir">随机深度网络</strong></a><strong class="kh ir"/><a class="ae lk" rel="noopener" target="_blank" href="/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004"><strong class="kh ir">宽 ResNets</strong></a><strong class="kh ir"/><a class="ae lk" href="https://medium.com/datadriveninvestor/review-fractalnet-image-classification-c5bdd855a090" rel="noopener"><strong class="kh ir">fractal nets</strong></a><strong class="kh ir">效率高达 5 倍。</strong></li><li id="1585" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">与任意时间预测设置中的结果相似，<strong class="kh ir"> MSDNet 通过多个中间分类器</strong>显著优于 ResNetsMC 和 DenseNetsMC，这进一步证明了 MSDNet 中的粗糙特征对于早期层中的高性能非常重要。</li></ul><h2 id="cdad" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">5.5.形象化</h2><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/e0862dcefe191f2466016991c92b9978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*AviHFmftEU_mLpMxg52klw.png"/></div><figcaption class="oa ob gj gh gi oc od bd b be z dk"><strong class="bd oe">Visualization on Easy and Hard Images on ImageNet</strong></figcaption></figure><ul class=""><li id="c0fe" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated">简单图像(顶行):在第一个分类器处退出并被正确分类。</li><li id="d6b4" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">硬图像(底行):在第一个分类器处退出，并且被错误分类，而被最后一个分类器正确分类，其中它们是非典型图像。</li></ul><h2 id="aa52" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">5.6.计算效率更高<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNets </a></h2><ul class=""><li id="f41c" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nr mu mv mw bi translated">发现并研究了一种更有效的<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>。作者还认为这是探索 MSDNet 的一个有趣的发现。</li></ul><figure class="nt nu nv nw gt nx gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi pa"><img src="../Images/b3e6efb1b020de17e21393ab5308e094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FITEnWRAA4hXaJEAYYuA2Q.png"/></div></div><figcaption class="oa ob gj gh gi oc od bd b be z dk"><strong class="bd oe">Anytime Classification (Left) and Budgeted Batch Classification (Right)</strong></figcaption></figure><ul class=""><li id="de35" class="mm mn iq kh b ki kj kl km ko of ks og kw oh la nr mu mv mw bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="kh ir">dense net</strong></a><strong class="kh ir">*</strong>:对原来的<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803">dense net</a>进行修改，在每一个过渡层之后增加一倍的增长率，从而对低分辨率的特征地图应用更多的滤镜。</li><li id="9b6b" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a> *(绿色)在计算效率上明显优于原来的<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>(红色)。</li><li id="8654" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在 anytime 分类中，不同深度的<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNets </a> *的集合仅比 MSDNets 稍差。</li><li id="fbd8" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nr mu mv mw bi translated">在预算的批量分类中，MSDNets 仍然大大优于不同深度的<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNets </a> *的集合。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><p id="100e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于未来的工作，作者计划在分类(例如:图像分割)之外进行研究，将 MSDNets 与模型压缩、空间自适应计算和更有效的卷积运算相结合。对我来说，这篇论文有许多重要的事实和概念，这使我写了这么长的故事。</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h2 id="53a8" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">参考</h2><p id="96bc" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">【2018 ICLR】【MSDNet】<br/><a class="ae lk" href="https://arxiv.org/abs/1703.09844" rel="noopener ugc nofollow" target="_blank">用于资源高效图像分类的多尺度密集网络</a></p><h2 id="7e16" class="nf lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(是)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(情)(况)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">ION</a><a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">multipath Net</a>【T21 [ <a class="ae lk" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3 </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44"> DCN </a> ]</p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a>]</p><p id="fc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong>[<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a></p><p id="3134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 实例分段 <br/> </strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413"> MultiPathNet </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> InstanceFCN </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a>】</p><p id="58de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(T36) 人类姿势估计 (T37) (T38) (T39) 汤普森 NIPS'14 (T40)</p></div></div>    
</body>
</html>