<html>
<head>
<title>Universal Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通用变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/universal-transformers-782af05095ee?source=collection_archive---------23-----------------------#2019-09-07">https://towardsdatascience.com/universal-transformers-782af05095ee?source=collection_archive---------23-----------------------#2019-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/d6e798d9d47a6529ec5a3de8754fdb53.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*Adf3iqDERsZ8cPkgDpNsVQ.png"/></div></figure><p id="5890" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">本文将讨论 Universal Transformer，它结合了原始的 Transformer 模型和一种叫做自适应计算时间的技术。通用变形器的主要创新是对每个符号应用不同次数的变形器组件。</p><h2 id="b1d6" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated"><strong class="ak">纸张参考</strong></h2><p id="eafa" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated"><a class="ae lt" href="https://arxiv.org/abs/1807.03819" rel="noopener ugc nofollow" target="_blank">德加尼·M、古乌斯·S、维尼亚尔斯·O、乌兹科雷特·J、凯泽·日。万能变形金刚。ICLR 2019。</a></p><h1 id="158d" class="lu kw it bd kx lv lw lx la ly lz ma ld mb mc md lg me mf mg lj mh mi mj lm mk bi translated"><strong class="ak">背景及变压器回顾</strong></h1><p id="5db5" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">如果你还不熟悉变形金刚模型，你应该通读一下<a class="ae lt" href="https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/" rel="noopener ugc nofollow" target="_blank">“变形金刚:注意力是你所需要的。”</a>万能变压器是对变压器的简单改造，所以先了解变压器型号很重要。</p><p id="4ee1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果您已经熟悉了 Transformer 模型，并且想要快速回顾一下，那么可以这样做:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/372a9e47cb5a471c364d68c3332e3da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/0*fhNfWpbFeb-kR-nS"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">Figure modified from Transformer paper</figcaption></figure><p id="bc37" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">基本变换器由编码器和解码器组成:</p><p id="488f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">编码器:</p><ul class=""><li id="e36b" class="mu mv it jz b ka kb ke kf ki mw km mx kq my ku mz na nb nc bi translated">6 个编码器层</li><li id="7338" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">每个编码器有 2 个子层:(1)多头自关注；(2)前馈</li></ul><p id="ab8f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">解码器:</p><ul class=""><li id="9db4" class="mu mv it jz b ka kb ke kf ki mw km mx kq my ku mz na nb nc bi translated">6 个解码器层</li><li id="9589" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">每个解码层有 3 个子层:(1)掩蔽多头自注意；(2)编解码多头注意；(3)前馈</li></ul><p id="ca57" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">以下是对多头注意力的一个数字回顾，这是 Transformer 模型的关键创新之一:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/fe8d861b792de60e2c89102155d77817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*-8pVblCSutEwOasL"/></div></figure><p id="d93c" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">多头关注用于编码器自关注(其将先前的编码器层输出作为输入)、解码器自关注(其将先前的解码器层输出作为输入)以及编码器-解码器关注(其将密钥和值的最终编码器输出以及先前的解码器输出用作查询)。)在上图中，模型中使用多头注意力的部分在左侧用红色方框标出。在右侧，示出了多头注意力计算的每个部分的张量的维度。</p><p id="8893" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">最后，快速回顾一下编码器子层和解码器子层中使用的位置式全连接前馈网络:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/33dc3b7ade278901100a2ac413efcc53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*IjCHhvlDux3KcOSI"/></div></figure><h1 id="7dde" class="lu kw it bd kx lv lw lx la ly lz ma ld mb mc md lg me mf mg lj mh mi mj lm mk bi translated"><strong class="ak">通用变压器的动机</strong></h1><p id="366e" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">最初的 Transformer 是一种自然语言处理模型，它并行处理输入序列中的所有单词，同时利用注意机制来合并上下文。它的训练速度比 RNN 快，后者必须一个接一个地处理输入令牌。它在语言翻译方面取得了良好的性能。然而，它在诸如字符串复制的算法任务上性能较差(例如，给定“abc”作为输入，输出“abcabc”。)</p><p id="13b0" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">神经 GPU 和神经图灵机(不同种类的模型)在语言翻译上性能较差，但在算法任务上性能良好。</p><p id="de57" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">Universal Transformer 的目标是只用一个模型就能在语言翻译和算法任务上获得良好的性能。通用变压器的作者还指出，这是一个图灵完全模型。(“图灵完备”是指它可以模拟任何图灵机，这是计算机的正式定义。)</p><h1 id="4789" class="lu kw it bd kx lv lw lx la ly lz ma ld mb mc md lg me mf mg lj mh mi mj lm mk bi translated"><strong class="ak">概述</strong></h1><p id="a9d2" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">在 Universal Transformers 的论文中，作者提供了一个新的图表来描述他们的模型:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/3d944dd62425ee8b7a15df7325ccd45b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*8ABvHGHulyRwzVsz"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">Figure 4 from <a class="ae lt" href="https://arxiv.org/pdf/1807.03819.pdf" rel="noopener ugc nofollow" target="_blank">Universal Transformer paper</a></figcaption></figure><p id="0668" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然而，我认为相对于原始的 Transformer 文件使用不同的图形样式掩盖了模型之间的关键差异。因此，我修改了原始变形金刚论文中的图，以更清楚地强调变形金刚和通用变形金刚模型的相似性和差异性。关键差异用红色强调:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d4155a1b23650f20a5e414c1c7c162b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*H1gDLXaGsVQCvC1w"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">Modified from original Transformers paper figure</figcaption></figure><p id="b58b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">变压器和通用变压器的主要区别如下:</p><ol class=""><li id="3a22" class="mu mv it jz b ka kb ke kf ki mw km mx kq my ku nj na nb nc bi translated">通用转换器为每个输入令牌应用可变数量的步骤(T 个步骤)的编码器，而基本转换器恰好应用 6 个编码器层。</li><li id="964c" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku nj na nb nc bi translated">通用变换器将解码器应用于每个输出令牌的可变数量的步骤(T 个步骤)，而基本变换器恰好应用 6 个解码器层。</li><li id="df86" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku nj na nb nc bi translated">通用转换器使用稍微不同的输入表示:除了“位置编码”之外，它还包括“时间步长嵌入”</li></ol><p id="01f2" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">差异(1)和(2)，步骤的可变数量，通过使用“自适应计算时间”来实现，这将在后面更详细地描述。简而言之，自适应计算时间是一种动态的每位置暂停机制，允许对每个符号进行不同量的计算。</p><p id="9749" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">通用转换器是一种“时间并行自关注递归序列模型”，它可以在输入序列上并行化。像基本变压器一样，它有一个“全局感受野”(意味着它一次看很多单词。)主要的新思想是，在每个循环步骤中，通用变换器使用自关注迭代地改进其对序列中所有符号的表示，随后是跨所有位置和时间步骤共享的“转换函数”。</p><p id="1667" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这里有一个来自<a class="ae lt" href="https://twitter.com/OriolVinyalsML/status/1017523208059260929" rel="noopener ugc nofollow" target="_blank">Oriol Vinyals(@ OriolVinyalsML 在 Twitter 上)</a>的很酷的动画，演示了通用变压器:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/95f2bd8502376a96cd3e30cafa13240f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*FkFeMTAuNrMeihYX"/></div></figure><p id="5c8f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">通用变压器的参数，包括自我关注和过渡权重，与所有位置和时间步长相关联。如果通用变换器运行固定数量的步骤(而不是可变数量的步骤 T)，那么通用变换器相当于在所有层上具有绑定参数的多层变换器。</p><p id="bb95" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这是通用变形金刚的另一个很酷的动画，来自谷歌人工智能博客:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/87e9ed95a8c198a40681cdf58f5f8a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*CdIJlX7cWv-VR6bR"/></div></figure><h1 id="044a" class="lu kw it bd kx lv lw lx la ly lz ma ld mb mc md lg me mf mg lj mh mi mj lm mk bi translated"><strong class="ak">通用变压器的更多细节</strong></h1><h2 id="4696" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated"><strong class="ak">通用变压器输入</strong></h2><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c4c95a30fce01a353237d80e0a4ee46e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*7Z1AJgNM6toh6R43"/></div></figure><p id="31e6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如上图所示，通用转换器的输入是一个长度为<em class="nl"> m </em>的序列，表示为<em class="nl"> d </em>维嵌入。在每个时间步，“坐标嵌入”被添加。这些“坐标嵌入”包括位置嵌入(与原始变换器的位置嵌入相同)和时间步长嵌入(与位置嵌入类似的概念，除了它基于时间 t 而不是位置 I)</p><h2 id="1dca" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated"><strong class="ak">通用变压器编码器</strong></h2><p id="60fd" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">通用变形金刚编码器的第一部分是多头自关注，和原变形金刚编码器的第一部分一模一样。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/3804eee6faf63fa37e029de9cd930c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*VA-R3HG9wtp9mWny"/></div></figure><p id="938a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">通用转换器编码器的第二部分是一个转换函数。转换函数可以是全位置全连接神经网络，在这种情况下，它与原始变压器编码器的第二部分完全相同。或者，转换函数可以是可分离的卷积。作者没有讨论他们何时使用位置全连接网络而不是可分离卷积，但这种选择可能会影响通用转换器在不同任务上的性能。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f82a9046bf806af1debfe34cea57d275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*4OYtbSuLxzAxeEvc"/></div></figure><p id="a859" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">什么是可分卷积？可分离卷积将卷积核分割成两个独立的核，这两个核进行两次卷积:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ee1877825c1aa3ca34a649f0359621a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*dfUkJPG7R9dAVdmb"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">Figure modified from <a class="ae lt" rel="noopener" target="_blank" href="/a-basic-introduction-to-separable-convolutions-b99ec3102728">“Separable Convolutions” by Chi-Feng Wang</a></figcaption></figure><p id="b9c1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果你对可分卷积的更多细节感兴趣，可以看看这篇论文:<a class="ae lt" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank"> Francois Chollet。例外:具有深度可分卷积的深度学习。arXiv 2016 </a></p><p id="77a5" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这就是通用变压器编码器！如果选择位置前馈网络作为转换函数，通用变压器编码器与原始变压器编码器相同。</p><h2 id="4dd0" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated"><strong class="ak">通用变压器解码器</strong></h2><p id="9a7d" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">同样，如果选择位置式前馈网络作为转换函数，通用变压器解码器与原始变压器解码器相同。有三个解码器子层:</p><ul class=""><li id="5ff3" class="mu mv it jz b ka kb ke kf ki mw km mx kq my ku mz na nb nc bi translated">子层 1:多头自我关注(在先前的解码器输出上)</li><li id="8a5d" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">子层 2:多头编解码注意。通过投影先前的解码器输出来获得查询。通过投影最终编码器输出来获得键和值。</li><li id="73b9" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">子层 3:过渡函数。</li></ul><h2 id="e16f" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated"><strong class="ak">通用变压器解码器培训</strong></h2><p id="56eb" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">通用变压器论文的一个优点是，它提供了更多关于如何训练解码器的背景知识。这也适用于最初的 Transformer，但在最初的 Transformer 论文中没有详细讨论。</p><p id="a36a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">Transformer 解码器(原始和通用)是“自回归”的，这意味着它一次生成一个输出符号，解码器消耗其先前生成的输出。</p><p id="a091" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">它使用“教师强制”来训练，这意味着在训练期间，嵌入目标符号的地面真相被馈入(而不是解码器自己可能不正确的预测。)目标符号被右移(因此模型看不到它应该预测的当前单词)并被屏蔽(因此模型看不到未来的单词。)</p><p id="e806" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在通用变换器中，每个符号的目标分布如下获得:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/90d952bf187d447a035861f99c9c151f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*D42zJjgpLpCNotzD"/></div></figure><h2 id="c474" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated"><strong class="ak">自适应计算时间(ACT) </strong></h2><p id="3943" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">这是 Universal Transformers 论文的主要贡献:他们将最初在 RNNs 中开发的自适应计算时间应用于 Transformer 模型:</p><p id="235e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><a class="ae lt" href="https://arxiv.org/abs/1603.08983" rel="noopener ugc nofollow" target="_blank"> Graves A .递归神经网络的自适应计算时间。arXiv 预印本 arXiv:1603.08983。2016 年 3 月 29 日。</a></p><p id="74a9" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这是一种允许编码器应用可变次数和解码器应用可变次数的机制。</p><p id="8787" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">ACT 根据模型在每一步预测的标量“暂停概率”，动态调整处理每个输入符号所需的计算步骤数(“思考时间”)。通用变压器分别对每个位置(例如每个字)应用动态动作停止机制。一旦一个特定的循环块停止，它的状态就被复制到下一步，直到所有的块都停止，或者直到达到最大的步数。编码器的最终输出是以这种方式产生的最终表示层。</p><p id="b1c1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">以下是 ACT 工作原理的快速总结:</p><p id="147b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在每一步，我们都会得到:</p><ul class=""><li id="df16" class="mu mv it jz b ka kb ke kf ki mw km mx kq my ku mz na nb nc bi translated">停止概率和先前状态(初始化为零)</li><li id="8ec4" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">介于 0 和 1 之间的标量暂停阈值(超参数，即我们自己选择暂停阈值)</li></ul><p id="3a69" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">首先，我们使用通用转换器计算每个位置的新状态。</p><p id="ff95" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然后，我们使用一个全连接层来计算“思考”值，该层将状态降低到 1 维，并应用 sigmoid 激活来使输出成为介于 0 和 1 之间的类似概率的值。这就是思考的价值。“思考”值是模型对每个输入符号需要多少额外计算的估计。</p><p id="1a14" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们决定停止任何超过停止阈值的仓位:</p><ul class=""><li id="03e2" class="mu mv it jz b ka kb ke kf ki mw km mx kq my ku mz na nb nc bi translated">刚停止到这一步:(停止概率+思考) &gt;停止阈值</li><li id="cb29" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">仍在运行:(暂停概率+思考)≤暂停阈值</li></ul><p id="cd02" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">对于仍在运行的仓位，更新暂停概率:暂停概率+=思考。</p><p id="923c" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们更新其他位置的状态，直到模型停止所有位置或达到预定义的最大步数。</p><p id="42d1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">关于自适应计算时间的实现，参见<a class="ae lt" href="https://github.com/cfiken/universal_transformer/blob/master/model/ut.py" rel="noopener ugc nofollow" target="_blank">这个 Github 库</a>。</p><h2 id="eaab" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated"><strong class="ak">图灵完备性</strong></h2><p id="f2cf" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">Universal Transformer 论文的作者解释说，Universal Transformer 是图灵完全的，就像神经 GPU 是图灵完全的一样。如果你不熟悉图灵完备性或者模型之间相互“还原”的证明，可以跳过这一节。</p><p id="47c5" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">简而言之，作者通过将神经 GPU 简化为通用转换器来证明通用转换器是图灵完全的:</p><ul class=""><li id="6156" class="mu mv it jz b ka kb ke kf ki mw km mx kq my ku mz na nb nc bi translated">忽略解码器</li><li id="d78a" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">使自我关注模块成为身份功能</li><li id="7cc3" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">假设转移函数是卷积</li><li id="c025" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">将循环步骤的总数 T 设置为等于输入长度</li><li id="f99e" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">我们已经从 UT 获得了一个神经 GPU</li></ul><h1 id="7e15" class="lu kw it bd kx lv lw lx la ly lz ma ld mb mc md lg me mf mg lj mh mi mj lm mk bi translated"><strong class="ak">结果</strong></h1><p id="8108" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">我们现在已经讨论了通用变形金刚中的所有关键概念。万能变形金刚擅长什么？</p><p id="7b53" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">通用变压器白皮书中有五项任务，总结如下:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ee6534684979da16ffebe913e6c60ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*Gwh24KLXaT-5n4IQ"/></div></figure><p id="da58" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在 bAbi 问答上，通用转换器比原始转换器获得了更好的性能。此外，对于更困难的任务变体(需要更多支持事实来回答问题的变体),在测试数据中所有样本的所有位置上的平均思考时间(通用转换器对一个符号计算多少次)更长。)这意味着当任务更难时，通用变压器“想得更多”。</p><p id="058b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在 Universal Transformers 的论文中，有几个在 bAbi 任务的不同时间步骤中注意力权重的可视化。视觉化是基于一个芭比故事和一个问题中所有事实的不同观点。四种不同的注意力头对应四种不同的颜色:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ab576a91cff0d9ff9396553724e3896a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*T0S15u5hVRK-3pEJ"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">Figure 5 from the Universal Transformers paper.</figcaption></figure><p id="c099" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">更多这样的数字，可以看论文附录。</p><p id="c08c" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">Universal Transformer 在主谓一致和 LAMBADA 上也取得了不错的成绩。在 LAMBADA 上，作者注意到通用转换器平均走 8-9 步；然而，他们比较的基础变压器只有 6 层。因此，他们运行了一个 8-9 层的基本变压器，但发现通用变压器仍然优于这种更深层次的变压器变体。这表明更多的计算并不总是更好，在输入和输出序列中的某些符号上减少计算是有价值的。作者推测，自适应计算时间可能具有正则化效果，例如，通过帮助模型忽略(“较少计算”)对解决任务不重要的信息。</p><p id="dcc5" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">最后，作者表明，通用转换器在几个算法任务上取得了良好的性能，包括复制、反转和加法。通用转换器在英德机器翻译方面也优于基本转换器。</p><h1 id="b384" class="lu kw it bd kx lv lw lx la ly lz ma ld mb mc md lg me mf mg lj mh mi mj lm mk bi translated"><strong class="ak">总结</strong></h1><ul class=""><li id="a8bf" class="mu mv it jz b ka lo ke lp ki nm km nn kq no ku mz na nb nc bi translated">通用转换器=原始转换器+自适应计算时间</li><li id="4c62" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">通用转换器允许每个符号隐藏状态的同时演化，这些隐藏状态是通过关注前一步骤中的隐藏状态序列而生成的。</li><li id="ac0c" class="mu mv it jz b ka nd ke ne ki nf km ng kq nh ku mz na nb nc bi translated">通用变压器在各种任务上实现了改进的性能。</li></ul><h2 id="c3ee" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated"><strong class="ak">演讲</strong></h2><p id="ffaa" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">对于这篇通用变形金刚博客文章的 PowerPoint 演示版本，请点击<a class="ae lt" href="http://www.ece.duke.edu/~lcarin/Rachel8.16.2019.pptx" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><h2 id="32c5" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated"><strong class="ak">特色图片</strong></h2><p id="98bd" class="pw-post-body-paragraph jx jy it jz b ka lo kc kd ke lp kg kh ki lq kk kl km lr ko kp kq ls ks kt ku im bi translated">特色图像是约翰·威廉姆·沃特豪斯的画作<a class="ae lt" href="https://en.wikipedia.org/wiki/Crystal_ball#/media/File:John_William_Waterhouse_-_The_Crystal_Ball.JPG" rel="noopener ugc nofollow" target="_blank">“水晶球”的一部分，结合</a><a class="ae lt" href="https://en.wikipedia.org/wiki/Universe#/media/File:NASA-HS201427a-HubbleUltraDeepField2014-20140603.jpg" rel="noopener ugc nofollow" target="_blank">维基百科</a>的“宇宙”(各种星系)图像和<a class="ae lt" href="https://www.stickpng.com/assets/images/580b585b2edbce24c47b2710.png" rel="noopener ugc nofollow" target="_blank">卡通太阳。</a></p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="6ff1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="nl">原载于 2019 年 9 月 7 日</em><a class="ae lt" href="https://glassboxmedicine.com/2019/09/07/universal-transformers/" rel="noopener ugc nofollow" target="_blank"><em class="nl">http://glassboxmedicine.com</em></a><em class="nl">。</em></p></div></div>    
</body>
</html>