<html>
<head>
<title>Multiple Linear Regression — with math and code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多元线性回归—使用数学和代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiple-linear-regression-with-math-and-code-c1052f3c7446?source=collection_archive---------8-----------------------#2019-10-14">https://towardsdatascience.com/multiple-linear-regression-with-math-and-code-c1052f3c7446?source=collection_archive---------8-----------------------#2019-10-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f9b05f54d24562efe4e23ffa188df61c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fZufHATjob6ErLITgXYyAQ.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@danist07?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">贝莉儿 DANIST</a> on <a class="ae jg" href="https://unsplash.com/s/photos/multiple-planes?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="f154" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性回归是预测模型的一种形式，广泛用于许多现实世界的应用中。许多关于线性回归的文章都是基于单个解释变量，并详细解释了最小化均方误差(MSE)以优化最佳拟合参数。在本文中，多个解释变量(自变量)被用来推导 MSE 函数，最后梯度下降技术被用来估计最佳拟合回归参数。具有三个自变量和一个因变量的示例数据集用于构建多元回归模型，在本文的后面部分，提供了 R-code 来对示例数据集进行建模。</p><h1 id="62ce" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">多元回归模型</h1><p id="c7c5" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">线性回归模型的方程是众所周知的，它表示为:</p><p id="d3e5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"><em class="mh">y = MX+c</em>T3】</strong></p><p id="dd4d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="ki jk"> <em class="mh"> y </em> </strong>是模型的输出，称为响应变量，<strong class="ki jk"> <em class="mh"> x </em> </strong>是自变量，也称为解释变量。<strong class="ki jk"> <em class="mh"> m </em> </strong>是回归线的斜率，<strong class="ki jk"> <em class="mh"> c </em> </strong>表示截距。通常我们得到<strong class="ki jk"> <em class="mh"> x </em> </strong>和<strong class="ki jk"> <em class="mh"> y </em> </strong>的测量值，并尝试通过估计<strong class="ki jk"> <em class="mh"> m </em> </strong>和<strong class="ki jk"> <em class="mh"> c </em> </strong>的最优值来建立模型，以便我们可以通过给定<strong class="ki jk"> <em class="mh"> x </em> </strong>作为输入，使用该模型对<strong class="ki jk"><em class="mh"/></strong>y 进行未来预测。</p><p id="baf4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实际上，我们处理的独立变量不止一个，在这种情况下，使用多个输入变量构建线性模型对于准确建模系统以实现更好的预测非常重要。因此，本文对多元回归分析进行了详细介绍。线性回归模型需要矩阵表示来表示多元回归模型，使其更加紧凑，同时便于计算模型参数。我相信读者对矩阵运算和线性代数有基本的了解。然而，在最后一节中，提供了回归分析中使用的矩阵规则，以更新读者的知识。</p><h2 id="b27e" class="mi lf jj bd lg mj mk dn lk ml mm dp lo kr mn mo ls kv mp mq lw kz mr ms ma mt bi translated"><strong class="ak">线性回归的代数形式</strong></h2><p id="ec50" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">假设我们有以下数据显示一个班不同学生的分数。一年中有四次考试的分数，最后一栏是期末考试的分数。从数据中可以看出，期末考试的分数与前三次考试的成绩有某种关系。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/623c4c656096ca75e29e5ea3d2597132.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*xFMqjo0d4LKKTQD8K7EsoA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Source: <a class="ae jg" href="https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html" rel="noopener ugc nofollow" target="_blank">https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html</a></figcaption></figure><p id="bd2c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里考虑到前三次考试的分数与期末考试的分数线性相关，我们的第一次观察(表中第一行)的线性回归模型应该如下所示。</p><p id="db3d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"><em class="mh">152 = a</em></strong>×<strong class="ki jk"><em class="mh">73+b</em></strong>×<strong class="ki jk"><em class="mh">80+c</em></strong>×<strong class="ki jk"><em class="mh">75+d</em></strong></p><p id="5382" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="ki jk"> <em class="mh"> a </em> </strong>，<strong class="ki jk"> <em class="mh"> b </em> </strong>，<strong class="ki jk"> <em class="mh"> c </em> </strong>，<strong class="ki jk"> <em class="mh"> d </em> </strong>为模型参数。</p><p id="811a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">等式的右边是回归模型，该模型在使用适当的参数时应该产生等于 152 的输出。但是实际上没有一个模型可以完美地模仿 100%的现实。模型输出和真实观察之间总是存在误差。因此，正确的回归方程可以定义如下:</p><p id="9ccc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">152 = a</strong>×73+b×80+c×75+d×1+E1</p><p id="54f5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="ki jk"> <em class="mh"> e1 </em> </strong>为首次观测的预测误差。类似地，对于数据表中的其他行，等式可以写成</p><p id="5e74" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">185 = a</strong>×93+b×88+c×93+d×1<strong class="ki jk">E2</strong></p><p id="2580" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">180 = a</strong>×89+b×91+c×90+d×1+E3</p><p id="5ef7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">196 = a</strong>×<strong class="ki jk">96+b</strong>×<strong class="ki jk">98+c</strong>×<strong class="ki jk">100+d</strong>×<strong class="ki jk">1+E4</strong></p><p id="6abe" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">………………………………………………..</p><p id="dcc7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">………………………………………………..</p><p id="8c1c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">192 = a</strong>×<strong class="ki jk">96+b</strong>×<strong class="ki jk">93+c</strong>×95<strong class="ki jk">+d</strong>×<strong class="ki jk">1+e25</strong></p><p id="2261" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述方程可以借助于下面提到的四个不同的矩阵来写。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/027b8c31354d1fd288fcc75cff8851eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Mn9kvcnf7uE2dwquUl2_A.png"/></div></div></figure><p id="02f9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用上述四个矩阵，代数形式的线性回归方程可以写成:</p><p id="b5b2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">T87】Y = xβ+eT89】</strong></p><p id="6965" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了得到等式的右边，矩阵<strong class="ki jk"> <em class="mh"> X </em> </strong>是<strong class="ki jk"> <em class="mh"> </em> </strong>乘以<strong class="ki jk"><em class="mh"/></strong>向量，乘积加上误差向量<strong class="ki jk"> <em class="mh"> e </em> </strong>。众所周知，如果第一个矩阵的列数等于第二个矩阵的行数，则两个矩阵可以相乘。<strong class="ki jk"> <em class="mh"> </em> </strong>在这种情况下，<strong class="ki jk"> <em class="mh"> X </em> </strong>有 4 列，<strong class="ki jk"><em class="mh"/></strong>有 4 行。</p><p id="05ab" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">重新排列术语，误差向量表示为:</p><blockquote class="na nb nc"><p id="0fe4" class="kg kh mh ki b kj kk kl km kn ko kp kq nd ks kt ku ne kw kx ky nf la lb lc ld im bi translated"><strong class="ki jk"> e = Y - Xβ </strong></p></blockquote><p id="58d6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，很明显，错误，<strong class="ki jk"> <em class="mh"> e </em> </strong>是参数的函数，<strong class="ki jk"><em class="mh"/></strong>。在下一节中，推导矩阵形式的 MSE，并将其用作优化模型参数的目标函数。</p><h2 id="eb05" class="mi lf jj bd lg mj mk dn lk ml mm dp lo kr mn mo ls kv mp mq lw kz mr ms ma mt bi translated"><strong class="ak">矩阵形式的 MSE</strong></h2><p id="f9da" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">MSE 的计算方法是将所有观测值的<strong class="ki jk"> <em class="mh"> e </em> </strong>的平方和除以数据表中的观测值数量。数学上:</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/8908d69312926d90efa900589e32f7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*rEPy-3vxC6DZqfv2xQaOrg.png"/></div></figure><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/29da97d1edd1f6c3908c3fc8296048a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*myDTy4dmzSCxGKuMc9opvQ.png"/></div></figure><p id="e5b1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将方程中的<strong class="ki jk"> <em class="mh"> e </em> </strong>替换为<strong class="ki jk"> <em class="mh"> Y — Xβ </em> </strong>，MSE 改写为:</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/5b88fac11a4a82ac454fef39ddd7d0a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*_fsQ-E2iOoehMrIb7xZ8Ug.png"/></div></figure><p id="7675" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将上述等式展开如下:</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/4d1f301dca5e3e718febc187fe9c45be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*A9z_RzEhkhaXWy9azumCSg.png"/></div></figure><p id="e6ae" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述等式用作成本函数(优化问题中的目标函数)，需要最小化该函数以估计我们的回归模型中的最佳拟合参数。需要通过对参数向量<strong class="ki jk"><em class="mh"/></strong>求<strong class="ki jk"> <em class="mh"> MSE </em> </strong>函数的导数来估计梯度，并用于梯度下降优化。</p><h2 id="f564" class="mi lf jj bd lg mj mk dn lk ml mm dp lo kr mn mo ls kv mp mq lw kz mr ms ma mt bi translated">MSE 的梯度</h2><p id="b8af" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">如上所述，梯度表示为:</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/319cbe8508be4940d43908cbd7f6b04b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*rriUCiLrYNe7O_uFjY7oiQ.png"/></div></figure><p id="e668" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中，<strong class="ki jk"> <em class="mh"> ∇ </em> </strong>是用于梯度的微分算子。使用矩阵。微分规则，我们得到以下方程。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/506d21b8d4b8a4561a78ad71b49e7c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*Ddg50Wt1SpN3ATHpvDFdNQ.png"/></div></figure><p id="7a0d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述矩阵被称为<strong class="ki jk"> <em class="mh">雅可比矩阵</em> </strong>，用于梯度下降优化以及学习率(<strong class="ki jk"> <em class="mh"> lr </em> </strong>)更新模型参数。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ee3f8a964f2c8c574ca2c94fcc6d7298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*1BOpp7NWksjkqLBCCSrqOg.png"/></div></figure><h2 id="7fdc" class="mi lf jj bd lg mj mk dn lk ml mm dp lo kr mn mo ls kv mp mq lw kz mr ms ma mt bi translated">梯度下降法</h2><p id="4306" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">梯度下降法更新模型参数的公式如下所示。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1f202538f63138401cd8b189eef74820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*UfvJXEvjd2NPzsbaaATghA.png"/></div></figure><p id="b7a1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"><em class="mh">【βold】</em></strong>是初始化的参数向量，在每次迭代中更新，在每次迭代结束时<strong class="ki jk"> <em class="mh"> βold </em> </strong>等同于<strong class="ki jk"> <em class="mh"> βnew </em> </strong>。<strong class="ki jk"> <em class="mh"> lr </em> </strong>是学习率，代表步长，有助于防止超过误差面的最低点。迭代过程继续，直到<strong class="ki jk"> <em class="mh"> MSE </em> </strong>值变小变平。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/6a11abac8bfc4c40df5adeee038449da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rbga-ibelnhsbgys8VWqEg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Illustration of gradient descent method. Source: <a class="ae jg" href="http://www.claudiobellei.com/2018/01/06/backprop-word2vec/" rel="noopener ugc nofollow" target="_blank">http://www.claudiobellei.com/2018/01/06/backprop-word2vec/</a></figcaption></figure><h1 id="9577" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">示例数据</h1><p id="66fb" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在本节中，使用示例数据集开发了一个多元回归模型。应用梯度下降法估计模型参数<strong class="ki jk"><em class="mh">a</em></strong><strong class="ki jk"><em class="mh">b</em></strong><strong class="ki jk"><em class="mh">c</em></strong>和<strong class="ki jk"> <em class="mh"> d </em> </strong>。矩阵<em class="mh"> </em>的值<strong class="ki jk"> <em class="mh"> X </em> </strong>和<strong class="ki jk"> <em class="mh"> Y </em> </strong>从数据中已知，而<strong class="ki jk"> <em class="mh"> β </em> </strong>向量未知，需要估计。最初，计算<strong class="ki jk"><em class="mh"/></strong>MSE 和<strong class="ki jk"> <em class="mh"> MSE </em> </strong>的梯度，然后应用梯度下降法使<strong class="ki jk"> <em class="mh"> MSE </em> </strong>最小化。</p><h2 id="ee6f" class="mi lf jj bd lg mj mk dn lk ml mm dp lo kr mn mo ls kv mp mq lw kz mr ms ma mt bi translated">r 代码</h2><p id="37f5" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><strong class="ki jk"> <em class="mh">读取数据并初始化β: </em> </strong></p><pre class="mv mw mx my gt nk nl nm nn aw no bi"><span id="b6b1" class="mi lf jj nl b gy np nq l nr ns">dataLR &lt;- read.csv("C:\\Users\\Niranjan\\Downloads\\mlr03.csv", header = T)<br/>beta &lt;- c(0,0,0,0) ## beta initialized<br/>beta_T &lt;- t(beta)</span><span id="c156" class="mi lf jj nl b gy nt nq l nr ns">X = matrix(NA,nrow(dataLR),ncol = 4)</span><span id="a1a9" class="mi lf jj nl b gy nt nq l nr ns">X[,1] &lt;- dataLR$EXAM1<br/>X[,2] &lt;- dataLR$EXAM2<br/>X[,3] &lt;- dataLR$EXAM3<br/>X[,4] &lt;- 1</span><span id="3dc7" class="mi lf jj nl b gy nt nq l nr ns">XT &lt;- t(X)<br/>y &lt;- as.vector(dataLR$FINAL)<br/>yT &lt;- t(y)</span></pre><p id="c80c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> <em class="mh">计算 MSE 并更新β </em> </strong></p><pre class="mv mw mx my gt nk nl nm nn aw no bi"><span id="db31" class="mi lf jj nl b gy np nq l nr ns">mse &lt;- (1/nrow(dataLR))* (yT%*%y - 2 *  beta_T%*%XT%*%y + beta_T%*%XT%*%X%*%beta)<br/>betanew &lt;- beta - (lr *(2/nrow(dataLR)) * (XT%*%X%*%beta - XT%*%y))</span></pre><p id="d903" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> <em class="mh">参数估计的完整代码</em> </strong></p><pre class="mv mw mx my gt nk nl nm nn aw no bi"><span id="1566" class="mi lf jj nl b gy np nq l nr ns">##multivariate linear regression<br/>dataLR &lt;- read.csv("C:\\Users\\Niranjan\\Downloads\\mlr03.csv", header = T)<br/>beta &lt;- c(0,0,0,0)<br/>beta_T &lt;- t(beta)</span><span id="eaa5" class="mi lf jj nl b gy nt nq l nr ns">X = matrix(NA,nrow(dataLR),ncol = 4)</span><span id="1dfe" class="mi lf jj nl b gy nt nq l nr ns">X[,1] &lt;- dataLR$EXAM1<br/>X[,2] &lt;- dataLR$EXAM2<br/>X[,3] &lt;- dataLR$EXAM3<br/>X[,4] &lt;- 1</span><span id="fceb" class="mi lf jj nl b gy nt nq l nr ns">XT &lt;- t(X)<br/>y &lt;- as.vector(dataLR$FINAL)<br/>yT &lt;- t(y)</span><span id="38dc" class="mi lf jj nl b gy nt nq l nr ns">iteration &lt;- 1<br/>lr = 0.00001</span><span id="7bce" class="mi lf jj nl b gy nt nq l nr ns">msef = NULL<br/>while (iteration &lt; 10) {<br/>  mse &lt;- (1/nrow(dataLR))* (yT%*%y - 2 *  beta_T%*%XT%*%y + beta_T%*%XT%*%X%*%beta)<br/>  betanew &lt;- beta - (lr *(2/nrow(dataLR)) * (XT%*%X%*%beta - XT%*%y))<br/>  msef &lt;- rbind(msef,mse)<br/>  beta &lt;- betanew<br/>  beta_T &lt;- t(betanew)<br/>  iteration  &lt;-  iteration + 1<br/>}</span><span id="bd09" class="mi lf jj nl b gy nt nq l nr ns">plot(1:length(msef), msef, type = "l", lwd = 2, col = 'red', xlab = 'Iterations', ylab = 'MSE')<br/>grid(nx = 10, ny = 10)</span><span id="c433" class="mi lf jj nl b gy nt nq l nr ns">print(list(a = beta[1],b = beta[2], c = beta[3], d = beta[4]))</span></pre><p id="aec3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> <em class="mh">标绘输出的代码</em> </strong></p><pre class="mv mw mx my gt nk nl nm nn aw no bi"><span id="e275" class="mi lf jj nl b gy np nq l nr ns">library(plot3D)<br/>ymod &lt;- X%*%beta<br/>scatter3D(dataLR$EXAM1,dataLR$EXAM2,dataLR$EXAM3, colvar = ymod,<br/>          pch = 17, cex = 2,bty = "g",ticktype = "detailed",phi = 0,lwd=2.5, xlab = "Exam1", ylab = 'Exam2',zlab = 'Exam3')<br/>scatter3D(dataLR$EXAM1,dataLR$EXAM2,dataLR$EXAM3, colvar = dataLR$FINAL,<br/>          pch = 16, cex = 2,bty = "g",ticktype = "detailed",phi = 0,lwd=2.5, xlab = "Exam1", ylab = 'Exam2',zlab = 'Exam3',add = T)</span><span id="2e76" class="mi lf jj nl b gy nt nq l nr ns">plot(dataLR$FINAL, ymod, pch = 16, cex = 2, xlab = 'Data', ylab = 'Model')<br/>lines(ymod,ymod, lwd = 4, col = "green", lty = 6)<br/>grid(nx = 10, ny = 10)<br/>legend("topleft",c('Model-Data Points','Best fit line'), lty = c(NA,6), lwd = c(NA,4), col = c("black","green"), pch = c(16,NA))</span></pre><h2 id="7d01" class="mi lf jj bd lg mj mk dn lk ml mm dp lo kr mn mo ls kv mp mq lw kz mr ms ma mt bi translated">输出</h2><p id="d498" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">MSE 的值急剧减小，六次迭代后，它变得几乎平坦，如下图所示。相应的模型参数是最佳拟合值。</p><p id="f347" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mh">最小化</em><strong class="ki jk"><em class="mh">MSE</em></strong><em class="mh">:</em></p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/7254b684e3e9ff6a5230c84047c4f740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*Wzgt3vhquPcTbXIKjB0Tlw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">MSE change with iterations</figcaption></figure><p id="0f96" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mh">优化后的</em><strong class="ki jk"><em class="mh"/></strong><em class="mh">:</em></p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/f66386f5f4b87c1ee0ec8bd15865d19f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_Fp_w_5uIJPiYEEoKKBfA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Optimized model parameters</figcaption></figure><p id="b65a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将计算的最终分数与来自数据的最终分数进行比较。模型效率通过将模型输出与数据中的目标输出进行比较来可视化。决定系数估计为 0.978，以数值评估模型的性能。下图显示了模型和数据之间的比较，其中三个轴用于表示解释变量，如<strong class="ki jk"> <em class="mh">示例 1、示例 2、示例 3 </em> </strong>，颜色方案用于显示输出变量，即最终得分。</p><p id="5fc5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mh">模型输出与数据中目标的比较:</em></p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/54d2272615e1c5ab80211daf50a71062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*o4NBuo-ra88WslkbN7KbIw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Visualization of model out and target in the data</figcaption></figure><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/51d070e281a184f3ed84f5a0d365a68f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*czy-DSg06IqsxGCfTpg8Qw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Comparison between model output and target in the data</figcaption></figure><h1 id="e13c" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">基本矩阵规则</strong></h1><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/6a109bd2a69f1ef7a6c8004e326b41a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*TcO9YcQIIue8ZtAE2Wybfg.png"/></div></figure></div></div>    
</body>
</html>