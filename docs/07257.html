<html>
<head>
<title>Understanding Fixup initialization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解修正初始化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-fixup-initialization-6bf08d41b427?source=collection_archive---------17-----------------------#2019-10-12">https://towardsdatascience.com/understanding-fixup-initialization-6bf08d41b427?source=collection_archive---------17-----------------------#2019-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5adc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何在没有归一化层的情况下训练残差网络？</h2></div><h2 id="fd0d" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">我们为什么要关心初始化呢？</h2><p id="f67c" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">权重矩阵的正确初始化极其重要。根据杰瑞米·霍华德的说法，由于初始化不当，人们几十年来都无法训练神经网络。为了看到它，我们可以重现杰里米<a class="ae lu" href="https://www.youtube.com/watch?v=AcA8HAYh7IE&amp;feature=youtu.be&amp;t=1138&amp;fbclid=IwAR02htQ6jcHEROSaBt0RrSq2aIpHpWcgoOnpioWrstC7pBnJlbjt0itziFg" rel="noopener ugc nofollow" target="_blank">讲座</a>中的一个实验。让我们初始化一个随机权重矩阵和一个向量。我们可以通过取 x - &gt; Ax 并重复这个过程 n 次来模拟一个神经网络。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="37b5" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">因此，我们将看到，均值和标准差都是无穷大。错误在于我们的随机初始化。</p><p id="f535" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">幸运的是，还有其他初始化权重矩阵的方法。其中之一是修复。</p><h2 id="327b" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">什么是修正？</h2><p id="f217" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">fixup(fixup-update initial ization)是最近由、Yann N. Dauphin 和马腾宇等人提出的一种用于资源网的初始化方法。在他们的<a class="ae lu" href="https://arxiv.org/pdf/1901.09321.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中，作者展示了在没有批量范数层的情况下训练残差网络是可能的。此外，作者还在图像分类和机器翻译方面取得了很好的效果。</p><h2 id="4daa" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">修复的动机</strong></h2><p id="57bc" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">让我们考虑没有规范化层的 ResNet 的基本块。在这种网络的某些区域(正同质块)，该术语的详细描述在<a class="ae lu" href="https://arxiv.org/pdf/1901.09321.pdf" rel="noopener ugc nofollow" target="_blank">文章</a>中提供。)方差随深度呈指数增长:</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/a5562441d1f759033df4ee05100e85cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*XXLAoJhZIt7_uMzGldwSDQ.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Relationship between variance in the subsequent layers.</figcaption></figure><p id="2613" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">因此，可能会出现收敛、训练速度和推广能力方面的问题。标准化层是一个解决方案。近年来，人们认为这个解决方案是独一无二的。</p><p id="5588" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">然而，正如我们将看到的，这是不真实的。</p><h2 id="b7be" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">目标</h2><p id="7fb0" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">如果我们以直观的方式考虑分解梯度(如果梯度变得太大，那么很容易超过最小值)，我们可以注意到，如果在每一步中输出的变化是 O(η)，其中η是学习速率，则问题不会发生。更正式地说:</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi gj"><img src="../Images/50e1b01f554a2cf18be50e315cdd0fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zl2Gc8dtAvu7GuQ2AGbqJg.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Condition for change in f after each step.</figcaption></figure><p id="bf8d" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">因此，如果我们有 L 个剩余分支，我们希望每个分支平均更新 O(η/L)。</p><p id="ab31" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">现在，我们假设每个剩余分支有 m 层，其中 m 是一个小正整数(通常是 2 或 3)。</p><p id="cc55" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">作者证明了如果标度因子(除了最小的那个)的乘积为 O(η)，则 SGD 每一步的输出变化为 O(η)。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/a8bf6ad7f181b762a14c843cd318df24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*1iQG6bgwNr2Ocx62-ulceg.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">The constraint for the product of scaling factors.</figcaption></figure><p id="4207" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">因此，提出了以下公式:</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/ec2be399bee5dda12cd4ad68b435f5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*lawQU53J-7AY9GN7NbzYkg.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Scaling factors a_i.</figcaption></figure><h2 id="f7ea" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">加速修复</h2><p id="4164" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">现在，我们已经实现了我们的主要目标，我们可以考虑提高网络性能的东西。</p><p id="b95d" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">使用标量偏差和乘数是一种常见的做法，因为它们使每一层的输出适合后续层的激活。如果我们想改变平均值，我们只需添加一个偏差。类似地，乘数有助于操纵标准差。</p><p id="2131" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">此外，作者还发现:</p><blockquote class="mu mv mw"><p id="b15d" class="lb lc mx ld b le mc jr lg lh md ju lj my me ll lm mz mf lo lp na mg lr ls lt ij bi translated">“我们发现，在每个权重层和非线性激活层之前只插入一个标量偏差就可以显著提高训练性能。”</p><p id="4e61" class="lb lc mx ld b le mc jr lg lh md ju lj my me ll lm mz mf lo lp na mg lr ls lt ij bi translated">在具有乘数的分支中，这反过来导致乘数的增长，从而增加其他层的有效学习率。特别地，我们观察到，每个剩余分支仅插入一个标量乘数模仿了具有归一化的网络的权重范数动态，并且使我们免于搜索新的学习速率表。”</p></blockquote><p id="8191" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">记住所有这些，建议残差块的以下架构:</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nb"><img src="../Images/2c704272d4b29e6e73bbd2f225071039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQnR3AHEJlSIzKG3msF2NQ.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Basic residual blocks of ResNet, Fixup with and without bias.</figcaption></figure><p id="a3ff" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">总而言之，为了修复和初始化您的网络，您必须:</p><blockquote class="mu mv mw"><p id="0f38" class="lb lc mx ld b le mc jr lg lh md ju lj my me ll lm mz mf lo lp na mg lr ls lt ij bi translated">1.将每个残差分支的分类层和最后一层初始化为 0。</p><p id="953b" class="lb lc mx ld b le mc jr lg lh md ju lj my me ll lm mz mf lo lp na mg lr ls lt ij bi translated">2.使用标准方法(例如，何等人(2015))初始化每隔一个层，并通过 L^(1/(2m-2 仅缩放剩余分支内的权重层)</p><p id="b8b6" class="lb lc mx ld b le mc jr lg lh md ju lj my me ll lm mz mf lo lp na mg lr ls lt ij bi translated">3.在每个分支中添加一个标量乘数(初始化为 1 ),在每个卷积、线性和元素激活层之前添加一个标量偏差(初始化为 0)。</p></blockquote><p id="4db6" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">值得注意的是，第二点是在没有正则化的情况下训练深度神经网络所必须的，而其他两点可以提高其性能。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><p id="648a" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated"><strong class="ld ir"> PyTorch 实现</strong></p><p id="1b8c" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">论文的实现可以在作者的一个<a class="ae lu" href="https://github.com/hongyi-zhang/Fixup" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。我们将讨论最重要的部分。</p><p id="11be" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">首先，我们初始化两个基本的修正块:FixupBasicBlock 和 FixupBottleneck。如论文中所述，标量偏差和比例因子分别设置为 0 和 1。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="0459" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">与 FixupBasicBlock 相比，FixupBottleneck 多了一个卷积层和两个偏差。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="b91a" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">然后，我们准备构建我们的第一个修正初始化 ResNet。我们首先定义基底，如层数、卷积等。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="8a96" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">然后我们遍历网络的所有部分并初始化权重。请注意，比例因子是根据公式 L^(1/(2m-2)).计算的顶部的额外因子 2 有助于保持标准差等于 1。</p><p id="e2ec" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">由于 FixupBasicBlock 有 2 层，m=2，我们通过因子 L^(-0.5).来缩放权重类似地，FixupBottleneck 有三层，所以我们按 L^(-0.25 缩放)</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="9509" class="pw-post-body-paragraph lb lc iq ld b le mc jr lg lh md ju lj ko me ll lm ks mf lo lp kw mg lr ls lt ij bi translated">最后，我们可以定义我们的修正初始化 ResNets！</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ma mb l"/></div></figure><h2 id="3860" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结论</h2><p id="1451" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">修正初始化是初始化权重矩阵的强大工具。这是非常重要的，因为它是第一种方法之一，允许在没有批量范数层的情况下训练甚至非常深的神经网络。</p></div></div>    
</body>
</html>