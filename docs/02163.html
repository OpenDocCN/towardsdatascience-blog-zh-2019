<html>
<head>
<title>Create custom gym environments from scratch — A stock market example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始创建定制的健身房环境—股票市场示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e?source=collection_archive---------1-----------------------#2019-04-10">https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e?source=collection_archive---------1-----------------------#2019-04-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="2d07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">OpenAI 的<code class="fe ko kp kq kr b">gym</code>是一个很棒的包，允许你创建定制的强化学习代理。它配备了相当多的预建环境，如<a class="ae ks" href="https://gym.openai.com/envs/#classic_control" rel="noopener ugc nofollow" target="_blank">car pole</a>、<a class="ae ks" href="https://gym.openai.com/envs/#classic_control" rel="noopener ugc nofollow" target="_blank"> MountainCar </a>和<a class="ae ks" href="https://gym.openai.com/envs/#atari" rel="noopener ugc nofollow" target="_blank">大量免费的 Atari 游戏</a>以供试验。</p><p id="2c85" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些环境非常适合学习，但最终您会希望设置一个代理来解决自定义问题。为此，您需要创建一个定制的环境，特定于您的问题域。稍后，我们将创建一个自定义的股票市场环境来模拟股票交易。本文的所有代码都可以在我的<a class="ae ks" href="https://github.com/notadamking/Stock-Trading-Environment" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> GitHub </strong> </a>上获得。</p><p id="5d19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，让我们了解一下到底什么是环境。环境包含运行代理并允许其学习的所有必要功能。每个环境必须实现以下<em class="kt"> gym </em>接口:</p><pre class="ku kv kw kx gt ky kr kz la aw lb bi"><span id="1d16" class="lc ld it kr b gy le lf l lg lh"><strong class="kr iu">import</strong> gym<br/><strong class="kr iu">from</strong> gym <strong class="kr iu">import</strong> spaces<br/><br/><strong class="kr iu">class</strong> <strong class="kr iu">CustomEnv</strong>(gym.Env):<br/>  <em class="kt">"""Custom Environment that follows gym interface"""</em><br/>  metadata = {'render.modes': ['human']}<br/><br/>  <strong class="kr iu">def</strong> __init__(self, arg1, arg2, ...):<br/>    super(CustomEnv, self).__init__()</span><span id="a1b2" class="lc ld it kr b gy li lf l lg lh">    <em class="kt"># Define action and observation space</em><br/>    <em class="kt"># They must be gym.spaces objects</em></span><span id="f3bb" class="lc ld it kr b gy li lf l lg lh">    <em class="kt"># Example when using discrete actions:</em><br/>    self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)</span><span id="3019" class="lc ld it kr b gy li lf l lg lh"><em class="kt">    # Example for using image as input:</em><br/>    self.observation_space = spaces.Box(low=0, high=255, shape=<br/>                    (HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)<br/><br/>  <strong class="kr iu">def</strong> step(self, action):<br/>    # Execute one time step within the environment<br/>    ...</span><span id="a5b4" class="lc ld it kr b gy li lf l lg lh">  <strong class="kr iu">def</strong> reset(self):<br/>    # Reset the state of the environment to an initial state<br/>    ...</span><span id="1601" class="lc ld it kr b gy li lf l lg lh">  <strong class="kr iu">def</strong> render(self, mode='human', close=False):<br/>    # Render the environment to the screen<br/>    ...</span></pre><p id="7622" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在构造函数中，我们首先定义我们的<code class="fe ko kp kq kr b">action_space</code>的类型和形状，它将包含代理在环境中可能采取的所有动作。类似地，我们将定义<code class="fe ko kp kq kr b">observation_space</code>，它包含代理要观察的所有环境数据。</p><p id="1814" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的<code class="fe ko kp kq kr b">reset</code>方法将被调用来周期性地将环境重置为初始状态。接下来是通过环境的许多<code class="fe ko kp kq kr b">step</code>，其中一个动作将由模型提供，并且必须被执行，下一个观察结果被返回。这也是计算奖励的地方，稍后会详细介绍。</p><p id="00bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，可以定期调用<code class="fe ko kp kq kr b">render</code>方法来打印环境的再现。这可能像打印语句一样简单，也可能像使用 openGL 渲染 3D 环境一样复杂。对于这个例子，我们将坚持使用打印语句。</p><h1 id="f647" class="lj ld it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">股票交易环境</strong></h1><p id="4836" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">为了演示这一切是如何工作的，我们将创建一个股票交易环境。然后，我们将培训我们的代理，使其成为该环境中的盈利交易者。我们开始吧！</p><figure class="ku kv kw kx gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ml"><img src="../Images/d4b0e2a5980c174d1a7d8413bfcf8281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3UNtfZrmEiv3XFKs_xaRcA.jpeg"/></div></div></figure><p id="ad8b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要考虑的第一件事是人类交易者会如何看待他们的环境。在决定进行交易之前，他们会做哪些观察？</p><p id="16bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">交易者很可能会看一些股票价格走势图，上面可能会覆盖一些技术指标。从那时起，他们将把这些视觉信息与他们对类似价格行为的先验知识结合起来，对股票可能的走势做出明智的决定。</p><p id="bfd7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，让我们将此转化为我们的代理应该如何感知其环境。</p><p id="cafe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的<code class="fe ko kp kq kr b">observation_space</code>包含了我们希望经纪人在交易<em class="kt">或不交易</em>之前考虑的所有输入变量。在本例中，我们希望代理“看到”过去五天的股票数据点(开盘价、最高价、最低价、收盘价和日交易量)，以及其他一些数据点，如帐户余额、当前股票头寸和当前利润。</p><p id="b82b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里的直觉是，对于每个时间步，我们希望我们的代理考虑导致当前价格的价格行为，以及他们自己的投资组合的状态，以便为下一个行为做出明智的决定。</p><p id="5387" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦交易者意识到他们的环境，他们需要采取行动。在我们代理的例子中，它的<code class="fe ko kp kq kr b">action_space</code>将由三种可能性组成:买入一只股票，卖出一只股票，或者什么都不做。</p><p id="142d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是这还不够。我们需要知道每次买入或卖出的股票数量。使用 gym 的<code class="fe ko kp kq kr b">Box</code>空间，我们可以创建一个动作空间，该空间具有离散数量的动作类型(买入、卖出和持有)，以及连续的买入/卖出金额范围(分别为账户余额/头寸大小的 0-100%)。</p><p id="c783" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您会注意到，金额对于保留操作不是必需的，但无论如何都会提供。我们的代理最初并不知道这一点，但随着时间的推移，应该会知道该金额与此行为无关。</p><p id="8094" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在实施我们的环境之前，最后要考虑的是回报。我们希望激励长期持续的利润。在每一步，我们将把奖励设置为账户余额乘以到目前为止的时间步数的某个分数。</p><p id="c2e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这样做的目的是在早期阶段延迟对代理人的过快奖励，并允许它在过于深入地优化单个策略之前进行充分的探索。它还将奖励那些在更长时间内保持较高余额的代理商，而不是那些使用不可持续的策略迅速赚钱的代理商。</p><figure class="ku kv kw kx gt mm"><div class="bz fp l di"><div class="mt mu l"/></div></figure><h2 id="8430" class="lc ld it bd lk mv mw dn lo mx my dp ls kb mz na lw kf nb nc ma kj nd ne me nf bi translated">履行</h2><p id="a8a9" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">现在我们已经定义了我们的观察空间、行动空间和奖励，是时候实现我们的环境了。首先，我们需要在环境的构造函数中定义<code class="fe ko kp kq kr b">action_space</code>和<code class="fe ko kp kq kr b">observation_space</code>。环境期望传入一个包含要学习的股票数据的<code class="fe ko kp kq kr b">pandas</code>数据帧。<a class="ae ks" href="https://github.com/adamjking3/Stock-Trading-Environment" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> Github repo </strong> </a>中提供了一个例子。</p><pre class="ku kv kw kx gt ky kr kz la aw lb bi"><span id="aa16" class="lc ld it kr b gy le lf l lg lh"><strong class="kr iu"><em class="kt">class</em></strong> <strong class="kr iu">StockTradingEnvironment</strong>(<em class="kt">gym</em>.<em class="kt">Env</em>):<br/>  """A stock trading environment for OpenAI gym"""<br/>  metadata = {'render.modes': ['human']}</span><span id="abbf" class="lc ld it kr b gy li lf l lg lh"><strong class="kr iu">  def </strong>__init__(<em class="kt">self, df</em>):<br/>    <em class="kt">super</em>(StockTradingEnv, self).__init__()<br/>    self.df = df<br/>    self.reward_range = (0, MAX_ACCOUNT_BALANCE) </span><span id="dfd1" class="lc ld it kr b gy li lf l lg lh">    # Actions of the format Buy x%, Sell x%, Hold, etc.<br/>    self.action_space = spaces.Box(<br/>      <em class="kt">low</em>=np.array([0, 0]), <em class="kt">high</em>=np.array([3, 1]), <em class="kt">dtype</em>=np.float16)</span><span id="8d7c" class="lc ld it kr b gy li lf l lg lh">    # Prices contains the OHCL values for the last five prices<br/>    self.observation_space = spaces.Box(<br/>      <em class="kt">low</em>=0, <em class="kt">high</em>=1, <em class="kt">shape</em>=(6, 6), <em class="kt">dtype</em>=np.float16)</span></pre><p id="d782" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们将编写<code class="fe ko kp kq kr b">reset</code>方法，每当创建新环境或重置现有环境的状态时都会调用该方法。在这里，我们将设置每个代理的初始余额，并将其未平仓头寸初始化为一个空列表。</p><pre class="ku kv kw kx gt ky kr kz la aw lb bi"><span id="f9bd" class="lc ld it kr b gy le lf l lg lh"><strong class="kr iu">def </strong>reset(<em class="kt">self</em>):<br/>  # Reset the state of the environment to an initial state<br/>  self.balance = INITIAL_ACCOUNT_BALANCE<br/>  self.net_worth = INITIAL_ACCOUNT_BALANCE<br/>  self.max_net_worth = INITIAL_ACCOUNT_BALANCE<br/>  self.shares_held = 0<br/>  self.cost_basis = 0<br/>  self.total_shares_sold = 0<br/>  self.total_sales_value = 0<br/> <br/>  # Set the current step to a random point within the data frame<br/>  self.current_step = random.randint(0, len(self.df.loc[:, 'Open'].values) - 6)</span><span id="9151" class="lc ld it kr b gy li lf l lg lh">  <strong class="kr iu">return</strong> self._next_observation()</span></pre><p id="e33c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将当前步骤设置为数据帧中的随机点，因为它本质上为我们的代理提供了来自同一数据集的更独特的体验。<code class="fe ko kp kq kr b">_next_observation</code>方法编译最近五个时间步长的股票数据，附加代理的帐户信息，并将所有值调整到 0 到 1 之间。</p><pre class="ku kv kw kx gt ky kr kz la aw lb bi"><span id="a801" class="lc ld it kr b gy le lf l lg lh"><strong class="kr iu"><em class="kt">def</em></strong> _next_observation(<em class="kt">self</em>):<br/>  # Get the data points for the last 5 days and scale to between 0-1<br/>  frame = np.array([<br/>    self.df.loc[self.current_step: self.current_step +<br/>                5, 'Open'].values / MAX_SHARE_PRICE,<br/>    self.df.loc[self.current_step: self.current_step +<br/>                5, 'High'].values / MAX_SHARE_PRICE,<br/>    self.df.loc[self.current_step: self.current_step +<br/>                5, 'Low'].values / MAX_SHARE_PRICE,<br/>    self.df.loc[self.current_step: self.current_step +<br/>                5, 'Close'].values / MAX_SHARE_PRICE,<br/>    self.df.loc[self.current_step: self.current_step +<br/>                5, 'Volume'].values / MAX_NUM_SHARES,<br/>   ])</span><span id="7e4c" class="lc ld it kr b gy li lf l lg lh">  # Append additional data and scale each value to between 0-1<br/>  obs = np.append(frame, [[<br/>    self.balance / MAX_ACCOUNT_BALANCE,<br/>    self.max_net_worth / MAX_ACCOUNT_BALANCE,<br/>    self.shares_held / MAX_NUM_SHARES,<br/>    self.cost_basis / MAX_SHARE_PRICE,<br/>    self.total_shares_sold / MAX_NUM_SHARES,<br/>    self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),<br/>  ]], <em class="kt">axis</em>=0)</span><span id="1050" class="lc ld it kr b gy li lf l lg lh">  <strong class="kr iu">return</strong> obs</span></pre><p id="1465" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们的环境需要能够采取<code class="fe ko kp kq kr b">step</code>。在每一步，我们将采取指定的行动(由我们的模型选择)，计算奖励，并返回下一个观察结果。</p><pre class="ku kv kw kx gt ky kr kz la aw lb bi"><span id="937f" class="lc ld it kr b gy le lf l lg lh"><strong class="kr iu"><em class="kt">def</em></strong> step(<em class="kt">self</em>, <em class="kt">action</em>):<br/>  # Execute one time step within the environment<br/>  self._take_action(action)</span><span id="550b" class="lc ld it kr b gy li lf l lg lh">  self.current_step += 1</span><span id="5a5a" class="lc ld it kr b gy li lf l lg lh">  <strong class="kr iu">if</strong> self.current_step &gt; len(self.df.loc[:, 'Open'].values) - 6:<br/>    self.current_step = 0</span><span id="4b9e" class="lc ld it kr b gy li lf l lg lh">  delay_modifier = (self.current_step / MAX_STEPS)<br/>  <br/>  reward = self.balance * delay_modifier<br/>  done = self.net_worth &lt;= 0</span><span id="91a8" class="lc ld it kr b gy li lf l lg lh">  obs = self._next_observation()</span><span id="8463" class="lc ld it kr b gy li lf l lg lh">  <strong class="kr iu">return</strong> obs, reward, done, {}</span></pre><p id="bec8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们的<code class="fe ko kp kq kr b">_take_action</code>方法需要采取模型提供的动作，或者买入、卖出或者持有股票。</p><pre class="ku kv kw kx gt ky kr kz la aw lb bi"><span id="0f28" class="lc ld it kr b gy le lf l lg lh"><strong class="kr iu"><em class="kt">def</em></strong> _take_action(<em class="kt">self</em>, <em class="kt">action</em>):<br/>  # Set the current price to a random price within the time step<br/>  current_price = random.uniform(<br/>    self.df.loc[self.current_step, "Open"],<br/>    self.df.loc[self.current_step, "Close"])</span><span id="5ccd" class="lc ld it kr b gy li lf l lg lh">  action_type = action[0]<br/>  amount = action[1]</span><span id="6ee5" class="lc ld it kr b gy li lf l lg lh"><strong class="kr iu">  if</strong> action_type &lt; 1:<br/>    # Buy amount % of balance in shares<br/>    total_possible = self.balance / current_price<br/>    shares_bought = total_possible * amount<br/>    prev_cost = self.cost_basis * self.shares_held<br/>    additional_cost = shares_bought * current_price</span><span id="d0d4" class="lc ld it kr b gy li lf l lg lh">    self.balance -= additional_cost<br/>    self.cost_basis = (prev_cost + additional_cost) / <br/>                            (self.shares_held + shares_bought)<br/>    self.shares_held += shares_bought</span><span id="247d" class="lc ld it kr b gy li lf l lg lh">  <strong class="kr iu">elif</strong> actionType &lt; 2:<br/>    # Sell amount % of shares held<br/>    shares_sold = self.shares_held * amount . <br/>    self.balance += shares_sold * current_price<br/>    self.shares_held -= shares_sold<br/>    self.total_shares_sold += shares_sold<br/>    self.total_sales_value += shares_sold * current_price</span><span id="a65b" class="lc ld it kr b gy li lf l lg lh">  self.netWorth = self.balance + self.shares_held * current_price</span><span id="210a" class="lc ld it kr b gy li lf l lg lh">  <strong class="kr iu">if</strong> self.net_worth &gt; self.max_net_worth:<br/>    self.max_net_worth = net_worth</span><span id="6fbe" class="lc ld it kr b gy li lf l lg lh">  <strong class="kr iu">if</strong> self.shares_held == 0:<br/>    self.cost_basis = 0</span></pre><p id="58f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在唯一剩下要做的就是<code class="fe ko kp kq kr b">render</code>将环境调整到屏幕上。为了简单起见，我们将只呈现到目前为止的利润和一些其他有趣的指标。</p><pre class="ku kv kw kx gt ky kr kz la aw lb bi"><span id="095d" class="lc ld it kr b gy le lf l lg lh"><strong class="kr iu"><em class="kt">def</em></strong> render(<em class="kt">self</em>, <em class="kt">mode</em>='human', <em class="kt">close</em>=False):<br/>  # Render the environment to the screen<br/>  profit = self.net_worth - INITIAL_ACCOUNT_BALANCE</span><span id="c1b1" class="lc ld it kr b gy li lf l lg lh">  print(<em class="kt">f</em>'Step: {self.current_step}')<br/>  print(<em class="kt">f</em>'Balance: {self.balance}')<br/>  print(<em class="kt">f</em>'Shares held: {self.shares_held}<br/>          (Total sold: {self.total_shares_sold})')<br/>  print(<em class="kt">f</em>'Avg cost for held shares: {self.cost_basis}<br/>          (Total sales value: {self.total_sales_value})')<br/>  print(<em class="kt">f</em>'Net worth: {self.net_worth}<br/>          (Max net worth: {self.max_net_worth})')<br/>  print(<em class="kt">f</em>'Profit: {profit}')</span></pre><p id="647d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的环境是完整的。我们现在可以用数据框实例化一个<code class="fe ko kp kq kr b">StockTradingEnv</code>环境，并用来自<a class="ae ks" href="https://github.com/hill-a/stable-baselines" rel="noopener ugc nofollow" target="_blank">稳定基线</a>的模型测试它。</p><pre class="ku kv kw kx gt ky kr kz la aw lb bi"><span id="448e" class="lc ld it kr b gy le lf l lg lh"><strong class="kr iu">import</strong> gym<br/><strong class="kr iu">import</strong> json<br/><strong class="kr iu">import</strong> datetime <strong class="kr iu">as</strong> dt</span><span id="7e33" class="lc ld it kr b gy li lf l lg lh"><strong class="kr iu">from</strong> stable_baselines.common.policies <strong class="kr iu">import</strong> MlpPolicy<br/><strong class="kr iu">from</strong> stable_baselines.common.vec_env <strong class="kr iu">import</strong> DummyVecEnv<br/><strong class="kr iu">from</strong> stable_baselines <strong class="kr iu">import</strong> PPO2</span><span id="f349" class="lc ld it kr b gy li lf l lg lh"><strong class="kr iu">from</strong> env.StockTradingEnv <strong class="kr iu">import</strong> StockTradingEnv</span><span id="7ac0" class="lc ld it kr b gy li lf l lg lh"><strong class="kr iu">import</strong> pandas <strong class="kr iu">as</strong> pd</span><span id="1acb" class="lc ld it kr b gy li lf l lg lh">df = pd.read_csv('./data/AAPL.csv')<br/>df = df.sort_values('Date')</span><span id="9acb" class="lc ld it kr b gy li lf l lg lh"># The algorithms require a vectorized environment to run<br/>env = DummyVecEnv([<em class="kt">lambda</em>: StockTradingEnv(df)])</span><span id="3605" class="lc ld it kr b gy li lf l lg lh">model = PPO2(MlpPolicy, env, <em class="kt">verbose</em>=1)<br/>model.learn(<em class="kt">total_timesteps</em>=20000)</span><span id="f5d3" class="lc ld it kr b gy li lf l lg lh">obs = env.reset()<br/><strong class="kr iu">for</strong> i <strong class="kr iu">in</strong> range(2000):<br/>  action, _states = model.predict(obs)<br/>  obs, rewards, done, info = env.step(action)<br/>  env.render()</span></pre><p id="ca7c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当然，这只是为了好玩，来测试创建一个有趣的、定制的健身房环境，包括一些半复杂的动作、观察和奖励空间。如果我们真的想在股票市场上通过深度学习致富，这将需要更多的时间和努力…</p><p id="1ee8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请继续关注下周的文章，在那里我们将学习<a class="ae ks" rel="noopener" target="_blank" href="/visualizing-stock-trading-agents-using-matplotlib-and-gym-584c992bc6d4">为我们的环境创建简单而优雅的可视化效果</a>！</p><div class="ng nh gp gr ni nj"><a rel="noopener follow" target="_blank" href="/creating-bitcoin-trading-bots-that-dont-lose-money-2e7165fb0b29"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd iu gy z fp no fr fs np fu fw is bi translated">创造不赔钱的比特币交易机器人</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">让我们使用深度强化学习来制造有利可图的加密货币交易代理</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx mr nj"/></div></div></a></div></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="1bab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kt">感谢阅读！一如既往，本教程的所有代码都可以在我的</em> <a class="ae ks" href="https://github.com/notadamking/Stock-Trading-Environment" rel="noopener ugc nofollow" target="_blank"> <em class="kt"> GitHub </em> </a> <em class="kt">上找到。如果您有任何问题或反馈，请在下面留下评论，我很乐意收到您的来信！我也可以通过@notadamking 上的</em><a class="ae ks" href="https://twitter.com/notadamking" rel="noopener ugc nofollow" target="_blank"><em class="kt">Twitter</em></a><em class="kt">联系到。</em></p><p id="33fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kt">你也可以通过下面的链接在</em> <a class="ae ks" href="https://github.com/users/notadamking/sponsorship" rel="noopener ugc nofollow" target="_blank"> <em class="kt"> Github 赞助商</em> </a> <em class="kt">或者</em><a class="ae ks" href="https://www.patreon.com/join/notadamking" rel="noopener ugc nofollow" target="_blank"><em class="kt">Patreon</em></a><em class="kt">上赞助我。</em></p><div class="ng nh gp gr ni nj"><a href="https://github.com/users/notadamking/sponsorship" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd iu gy z fp no fr fs np fu fw is bi translated">GitHub 赞助商</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">嗨，我是亚当。我是一名开发人员、作家和企业家，尤其对深度…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">github.com</p></div></div><div class="ns l"><div class="of l nu nv nw ns nx mr nj"/></div></div></a></div><blockquote class="og oh oi"><p id="d9fb" class="jq jr kt js b jt ju jv jw jx jy jz ka oj kc kd ke ok kg kh ki ol kk kl km kn im bi translated">Github 赞助商目前正在 1:1 匹配所有捐款，最高可达 5000 美元！</p></blockquote><div class="ng nh gp gr ni nj"><a href="https://patreon.com/notadamking" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd iu gy z fp no fr fs np fu fw is bi translated">亚当·金正在创造改变世界的内容</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">嗨，我是亚当。我是一名开发人员、作家和企业家，尤其对深度…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">patreon.com</p></div></div><div class="ns l"><div class="om l nu nv nw ns nx mr nj"/></div></div></a></div></div></div>    
</body>
</html>