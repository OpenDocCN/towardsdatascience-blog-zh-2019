<html>
<head>
<title>A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Gensim Word2Vec 模型嵌入单词的初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92?source=collection_archive---------1-----------------------#2019-05-30">https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92?source=collection_archive---------1-----------------------#2019-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/eed40865dde2bc1613629f436cea45bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-em5mUlKP3fTaOHc3xaZhg.png"/></div></div></figure><blockquote class="kb kc kd"><p id="ec9b" class="ke kf kg kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">单词嵌入是自然语言处理(NLP)中最重要的技术之一，其中单词被映射到实数的向量。单词嵌入能够捕获文档中单词的含义、语义和句法相似性以及与其他单词的关系。它还被广泛用于推荐系统和文本分类。本教程将简要介绍 genism word2vec 模型，并给出一个为 vehicle make 模型生成单词嵌入的例子。</p></blockquote><h2 id="04e1" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">目录</h2><ul class=""><li id="7517" class="lz ma it kh b ki mb km mc lm md lq me lu mf lc mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/p/5970fa56cc92#702d" rel="noopener"> 1。Word2vec </a>简介</li><li id="d43c" class="lz ma it kh b ki ml km mm lm mn lq mo lu mp lc mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/p/5970fa56cc92#b513" rel="noopener"> 2。Gensim Python 库介绍</a></li><li id="16af" class="lz ma it kh b ki ml km mm lm mn lq mo lu mp lc mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/p/5970fa56cc92#9a6d" rel="noopener"> 3。用 Gensim Word2Vec 模型实现单词嵌入</a></li><li id="7e19" class="lz ma it kh b ki ml km mm lm mn lq mo lu mp lc mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/p/5970fa56cc92#ffb7" rel="noopener"> 3.1 数据预处理:</a></li><li id="92f8" class="lz ma it kh b ki ml km mm lm mn lq mo lu mp lc mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/p/5970fa56cc92#e71b" rel="noopener"> 3.2。Genism word2vec 模型训练</a></li><li id="049e" class="lz ma it kh b ki ml km mm lm mn lq mo lu mp lc mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/p/5970fa56cc92#9731" rel="noopener"> 4。计算相似度</a></li><li id="1760" class="lz ma it kh b ki ml km mm lm mn lq mo lu mp lc mg mh mi mj bi translated"><a class="ae mk" href="https://medium.com/p/5970fa56cc92#23fc" rel="noopener"> 5。T-SNE 可视化技术</a></li></ul></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="702d" class="mx le it bd lf my mz na li nb nc nd ll ne nf ng lp nh ni nj lt nk nl nm lx nn bi translated">1.Word2vec 简介</h1><p id="c16f" class="pw-post-body-paragraph ke kf it kh b ki mb kk kl km mc ko kp lm no ks kt lq np kw kx lu nq la lb lc im bi translated">Word2vec 是使用两层神经网络学习单词嵌入的最流行的技术之一。它的输入是一个文本语料库，输出是一组向量。通过 word2vec 嵌入单词可以使自然语言变得计算机可读，然后对单词进一步执行数学运算可以用来检测它们的相似性。一组训练有素的单词向量会将相似的单词彼此靠近地放置在那个空间中。例如，单词“女人”、“男人”和“人类”可能聚集在一个角落，而“黄色”、“红色”和“蓝色”则聚集在另一个角落。</p><figure class="ns nt nu nv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/73e26c035e07f705b66a4d446c406af6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmmFCZpKk3i4EvMYZ855tg.png"/></div></div></figure><p id="8417" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">word2vec 有两种主要的训练算法，一种是连续单词包(CBOW)，另一种叫做 skip-gram。这两种方法的主要区别在于 CBOW 使用上下文来预测目标单词，而 skip-gram 使用单词来预测目标上下文。通常，与 CBOW 方法相比，skip-gram 方法可以具有更好的性能，因为它可以捕获单个单词的两种语义。例如，它将有两个代表苹果的向量，一个代表公司，另一个代表水果。关于 word2vec 算法的更多细节，请查看<a class="ae mk" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="b513" class="mx le it bd lf my nw na li nb nx nd ll ne ny ng lp nh nz nj lt nk oa nm lx nn bi translated">2.Gensim Python 库简介</h1><p id="5e72" class="pw-post-body-paragraph ke kf it kh b ki mb kk kl km mc ko kp lm no ks kt lq np kw kx lu nq la lb lc im bi translated">Gensim 是一个用于自然语言处理的开源 python 库，由捷克自然语言处理研究员<a class="ae mk" href="https://www.linkedin.com/in/radimrehurek/" rel="noopener ugc nofollow" target="_blank">拉迪姆·řehůřek</a>开发和维护。Gensim 库将使我们能够通过在自定义语料库上训练我们自己的 word2vec 模型来开发单词嵌入，或者使用跳格算法的 CBOW。</p><p id="3ce7" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">首先，我们需要安装 genism 包。Gensim 可以在 Linux、Windows 和 Mac OS X 上运行，并且应该可以在任何其他支持 Python 2.7+和 NumPy 的平台上运行。Gensim 依赖于以下软件:</p><ul class=""><li id="db17" class="lz ma it kh b ki kj km kn lm ob lq oc lu od lc mg mh mi mj bi translated"><a class="ae mk" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kh iu"> Python </strong> </a> &gt; = 2.7(用 2.7、3.5、3.6 版本测试)</li><li id="cb2a" class="lz ma it kh b ki ml km mm lm mn lq mo lu mp lc mg mh mi mj bi translated"><a class="ae mk" href="http://www.numpy.org/" rel="noopener ugc nofollow" target="_blank"><strong class="kh iu"/></a>&gt;= 1 . 11 . 3</li><li id="628a" class="lz ma it kh b ki ml km mm lm mn lq mo lu mp lc mg mh mi mj bi translated"><a class="ae mk" href="https://www.scipy.org/" rel="noopener ugc nofollow" target="_blank"><strong class="kh iu"/></a>&gt;= 0 . 18 . 1</li><li id="2c08" class="lz ma it kh b ki ml km mm lm mn lq mo lu mp lc mg mh mi mj bi translated"><a class="ae mk" href="https://pypi.org/project/six/" rel="noopener ugc nofollow" target="_blank"> <strong class="kh iu">六</strong> </a> &gt; = 1.5.0</li><li id="a8e8" class="lz ma it kh b ki ml km mm lm mn lq mo lu mp lc mg mh mi mj bi translated"><a class="ae mk" href="https://pypi.org/project/smart_open/" rel="noopener ugc nofollow" target="_blank"><strong class="kh iu">smart _ open</strong></a>&gt;= 1 . 2 . 1</li></ul><p id="ce35" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">有两种安装方式。我们可以在终端上运行下面的代码来安装 genism 包。</p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="d18b" class="ld le it of b gy oj ok l ol om">pip install --upgrade gensim</span></pre><p id="8e16" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">或者，对于<em class="kg"> Conda </em>环境:</p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="f168" class="ld le it of b gy oj ok l ol om">conda install -c conda-forge gensim</span></pre><h1 id="9a6d" class="mx le it bd lf my nw na li nb nx nd ll ne ny ng lp nh nz nj lt nk oa nm lx nn bi translated">3.用 Gensim Word2Vec 模型实现单词嵌入</h1><p id="8c64" class="pw-post-body-paragraph ke kf it kh b ki mb kk kl km mc ko kp lm no ks kt lq np kw kx lu nq la lb lc im bi translated">在本教程中，我将通过一个具体的例子展示如何使用 genism 生成单词嵌入。我在本教程中使用的数据集来自<a class="ae mk" href="https://www.kaggle.com/CooperUnion/cardataset" rel="noopener ugc nofollow" target="_blank"> Kaggle 数据集</a>。</p><figure class="ns nt nu nv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/20674d42c620afee6310e2492b1f1d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDM48J-l72IURCG_dz5U1A.png"/></div></div></figure><p id="9e40" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">该车辆数据集包括汽车的品牌、型号、年份、发动机和其他属性等特征。我们将使用这些特征来为每个制造模型生成单词嵌入，然后比较不同制造模型之间的相似性。完整的 python 教程可以在这里找到<a class="ae mk" href="https://github.com/zhlli1/Genism-word2vec/blob/master/Genism%20Word2Vec%20Tutorial.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="74b9" class="ld le it of b gy oj ok l ol om">&gt;&gt;&gt; df = pd.read_csv('data.csv')<br/>&gt;&gt;&gt; df.head()</span></pre><figure class="ns nt nu nv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi on"><img src="../Images/44c0db20b85e251b0e2aa8dea86ba21a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9NhHvI7w6iIhYttWWcdZfw.png"/></div></div></figure><h2 id="ffb7" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">3.1 数据预处理:</h2><p id="d60f" class="pw-post-body-paragraph ke kf it kh b ki mb kk kl km mc ko kp lm no ks kt lq np kw kx lu nq la lb lc im bi translated">由于本教程的目的是学习如何使用 genism 库生成单词嵌入，为了简单起见，我们将不为 word2vec 模型进行 EDA 和特性选择。</p><p id="20e7" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">Genism word2vec 要求用于训练的“列表列表”格式，其中每个文档都包含在一个列表中，并且每个列表都包含该文档的标记列表。首先，我们需要生成一个“列表列表”的格式来训练 make 模型单词嵌入。更具体地说，每个制造模型都包含在一个列表中，每个列表都包含该制造模型的特征列表。</p><p id="6756" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">为了实现这一点，我们需要做以下事情:</p><p id="12bd" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">a.为制作模型创建新列</p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="6ba6" class="ld le it of b gy oj ok l ol om">&gt;&gt;&gt; df['Maker_Model']= df['Make']+ " " + df['Model']</span></pre><p id="29d8" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">b.为每个品牌车型生成一个具有以下特征的“列表列表”格式:发动机燃料类型、变速器类型、从动轮、市场类别、车辆尺寸、车型。</p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="755f" class="ld le it of b gy oj ok l ol om"># Select features from original dataset to form a new dataframe <br/>&gt;&gt;&gt; df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']]</span><span id="b539" class="ld le it of b gy oo ok l ol om"># For each row, combine all the columns into one column<br/>&gt;&gt;&gt; df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)</span><span id="4467" class="ld le it of b gy oo ok l ol om"># Store them in a pandas dataframe<br/>&gt;&gt;&gt; df_clean = pd.DataFrame({'clean': df2})</span><span id="3dae" class="ld le it of b gy oo ok l ol om"># Create the list of list format of the custom corpus for gensim modeling <br/>&gt;&gt;&gt; sent = [row.split(',') for row in df_clean['clean']]</span><span id="e7ec" class="ld le it of b gy oo ok l ol om"># show the example of list of list format of the custom corpus for gensim modeling <br/>&gt;&gt;&gt; sent[:2]<br/>[['premium unleaded (required)',<br/>  'MANUAL',<br/>  'rear wheel drive',<br/>  'Factory Tuner',<br/>  'Luxury',<br/>  'High-Performance',<br/>  'Compact',<br/>  'Coupe',<br/>  'BMW 1 Series M'],<br/> ['premium unleaded (required)',<br/>  'MANUAL',<br/>  'rear wheel drive',<br/>  'Luxury',<br/>  'Performance',<br/>  'Compact',<br/>  'Convertible',<br/>  'BMW 1 Series']]</span></pre><h2 id="e71b" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">3.2.Genism word2vec 模型培训</h2><p id="3795" class="pw-post-body-paragraph ke kf it kh b ki mb kk kl km mc ko kp lm no ks kt lq np kw kx lu nq la lb lc im bi translated">我们可以使用自己的自定义语料库训练 genism word2vec 模型，如下所示:</p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="593b" class="ld le it of b gy oj ok l ol om">&gt;&gt;&gt; model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)</span></pre><p id="acab" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">让我们试着去理解这个模型的超参数。</p><p id="474b" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated"><strong class="kh iu"> size </strong>:嵌入的维数，默认为 100。</p><p id="0187" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated"><strong class="kh iu">窗口</strong>:目标单词与其周围单词的最大距离。默认窗口是 5。</p><p id="d652" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated"><strong class="kh iu"> min_count </strong>:训练模型时要考虑的最小字数；出现次数少于此计数的单词将被忽略。min_count 的默认值为 5。</p><p id="01f7" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated"><strong class="kh iu">工人</strong>:培训时分区数量，默认工人为 3。</p><p id="b54b" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated"><strong class="kh iu"> sg </strong>:训练算法，CBOW(0)或 skip gram(1)。默认的训练算法是 CBOW。</p><p id="682d" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">在训练 word2vec 模型之后，我们可以直接从训练模型中获得单词嵌入，如下所示。</p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="7c95" class="ld le it of b gy oj ok l ol om">&gt;&gt;&gt; model['Toyota Camry']</span><span id="89a1" class="ld le it of b gy oo ok l ol om">array([-0.11884457,  0.03035539, -0.0248678 , -0.06297892, -0.01703234,<br/>       -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,<br/>       -0.07199778,  0.05235871,  0.21303181,  0.15767808, -0.1883737 ,<br/>        0.01938575, -0.24431638,  0.04261152,  0.11865819,  0.09881561,<br/>       -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,<br/>       -0.0040625 ,  0.16796461,  0.14578669,  0.04187112, -0.01436194,<br/>       -0.25554284,  0.25494182,  0.05522631,  0.19295982,  0.14461821,<br/>        0.14022525, -0.2065216 , -0.05020927, -0.08133671,  0.18031682,<br/>        0.35042757,  0.0245426 ,  0.15938364, -0.05617865,  0.00297452,<br/>        0.15442047, -0.01286271,  0.13923576,  0.085941  ,  0.18811756],<br/>      dtype=float32)</span></pre><h1 id="9731" class="mx le it bd lf my nw na li nb nx nd ll ne ny ng lp nh nz nj lt nk oa nm lx nn bi translated">4.计算相似度</h1><p id="55b9" class="pw-post-body-paragraph ke kf it kh b ki mb kk kl km mc ko kp lm no ks kt lq np kw kx lu nq la lb lc im bi translated">现在，我们甚至可以通过调用 model.similarity()并传入相关单词，使用 Word2vec 来计算词汇表中两个 Make 模型之间的相似性。例如，model . similarity(' Porsche 718 Cayman '，' Nissan Van ')这将为我们提供 Porsche 718 Cayman 和 Nissan Van 之间的欧几里得相似性。</p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="2d08" class="ld le it of b gy oj ok l ol om">&gt;&gt;&gt; model.similarity('Porsche 718 Cayman', 'Nissan Van')<br/>0.822824584626184</span><span id="36db" class="ld le it of b gy oo ok l ol om">&gt;&gt;&gt; model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')<br/>0.961089779453727</span></pre><p id="14fb" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">从上面的例子中，我们可以看出保时捷 718 Cayman 比日产 Van 更像奔驰 SLK 级。我们还可以使用内置函数 model.most_similar()来获得一组基于欧氏距离的最相似的 make 模型。</p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="d104" class="ld le it of b gy oj ok l ol om">&gt;&gt;&gt; model1.most_similar('Mercedes-Benz SLK-Class')[:5]</span><span id="ed3c" class="ld le it of b gy oo ok l ol om">[('BMW M4', 0.9959905743598938),<br/> ('Maserati Coupe', 0.9949707984924316),<br/> ('Porsche Cayman', 0.9945154190063477),<br/> ('Mercedes-Benz SLS AMG GT', 0.9944609999656677),<br/> ('Maserati Spyder', 0.9942780137062073)]</span></pre><p id="81bb" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">然而，欧几里德相似性不能很好地用于高维单词向量。这是因为欧几里德相似性会随着维数的增加而增加，即使嵌入这个词代表不同的意思。或者，我们可以使用余弦相似性来度量两个向量之间的相似性。在数学上，它测量的是在多维空间中投影的两个向量之间的角度余弦。余弦相似度捕捉单词向量的角度，而不是幅度。在余弦相似度下，没有相似度被表示为 90 度角，而总相似度 1 处于 0 度角。</p><figure class="ns nt nu nv gt ju gh gi paragraph-image"><div class="gh gi op"><img src="../Images/05d9a48813b9b1072015c35e13d94424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*_Bf9goaALQrS_0XkBozEiQ.png"/></div></figure><p id="8b45" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">以下函数显示了如何基于余弦相似度生成最相似的 make 模型。</p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="8443" class="ld le it of b gy oj ok l ol om">def cosine_distance (model, word,target_list , num) :<br/>    cosine_dict ={}<br/>    word_list = []<br/>    a = model[word]<br/>    for item in target_list :<br/>        if item != word :<br/>            b = model [item]<br/>            cos_sim = dot(a, b)/(norm(a)*norm(b))<br/>            cosine_dict[item] = cos_sim<br/>    dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order <br/>    for item in dist_sort:<br/>        word_list.append((item[0], item[1]))<br/>    return word_list[0:num]</span><span id="42d6" class="ld le it of b gy oo ok l ol om"># only get the unique Maker_Model<br/>&gt;&gt;&gt; Maker_Model = list(df.Maker_Model.unique()) </span><span id="cbf8" class="ld le it of b gy oo ok l ol om"># Show the most similar Mercedes-Benz SLK-Class by cosine distance <br/>&gt;&gt;&gt; cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5)</span><span id="1264" class="ld le it of b gy oo ok l ol om">[('Mercedes-Benz CLK-Class', 0.99737006),<br/> ('Aston Martin DB9', 0.99593246),<br/> ('Maserati Spyder', 0.99571854),<br/> ('Ferrari 458 Italia', 0.9952333),<br/> ('Maserati GranTurismo Convertible', 0.994994)]</span></pre><h1 id="23fc" class="mx le it bd lf my nw na li nb nx nd ll ne ny ng lp nh nz nj lt nk oa nm lx nn bi translated">5.T-SNE 可视化</h1><p id="6f94" class="pw-post-body-paragraph ke kf it kh b ki mb kk kl km mc ko kp lm no ks kt lq np kw kx lu nq la lb lc im bi translated">很难直接将嵌入这个词形象化，因为它们通常有 3 个以上的维度。T-SNE 是一种有用的工具，通过降维来可视化高维数据，同时保持点之间的相对成对距离。可以说，T-SNE 正在寻找一种新的数据表示方法，在这种方法中，邻域关系被保留下来。下面的代码显示了如何用 T-SNE 图绘制单词 embedding。</p><pre class="ns nt nu nv gt oe of og oh aw oi bi"><span id="8a32" class="ld le it of b gy oj ok l ol om">def display_closestwords_tsnescatterplot(model, word, size):<br/>    <br/>    arr = np.empty((0,size), dtype='f')<br/>    word_labels = [word]</span><span id="41cf" class="ld le it of b gy oo ok l ol om">close_words = model.similar_by_word(word)</span><span id="92d4" class="ld le it of b gy oo ok l ol om">arr = np.append(arr, np.array([model[word]]), axis=0)<br/>    for wrd_score in close_words:<br/>        wrd_vector = model[wrd_score[0]]<br/>        word_labels.append(wrd_score[0])<br/>        arr = np.append(arr, np.array([wrd_vector]), axis=0)<br/>        <br/>    tsne = TSNE(n_components=2, random_state=0)<br/>    np.set_printoptions(suppress=True)<br/>    Y = tsne.fit_transform(arr)</span><span id="315e" class="ld le it of b gy oo ok l ol om">x_coords = Y[:, 0]<br/>    y_coords = Y[:, 1]<br/>    plt.scatter(x_coords, y_coords)</span><span id="e5a2" class="ld le it of b gy oo ok l ol om">for label, x, y in zip(word_labels, x_coords, y_coords):<br/>        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')<br/>    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)<br/>    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)<br/>    plt.show()</span><span id="4d02" class="ld le it of b gy oo ok l ol om">&gt;&gt;&gt; display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) </span></pre><figure class="ns nt nu nv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/82af40f649d789c874f03f0e69675e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDFjJcD-Py65jNIXDFCqDA.png"/></div></div></figure><p id="41e5" class="pw-post-body-paragraph ke kf it kh b ki kj kk kl km kn ko kp lm kr ks kt lq kv kw kx lu kz la lb lc im bi translated">该 T-SNE 图以二维空间展示了与保时捷 718 Cayman 相似的前 10 款车型。</p><h1 id="b383" class="mx le it bd lf my nw na li nb nx nd ll ne ny ng lp nh nz nj lt nk oa nm lx nn bi translated">关于我</h1><p id="a75a" class="pw-post-body-paragraph ke kf it kh b ki mb kk kl km mc ko kp lm no ks kt lq np kw kx lu nq la lb lc im bi translated">我是旧金山大学数据科学专业的硕士生。我热衷于使用机器学习来解决商业挑战。也可以通过<a class="ae mk" href="https://www.linkedin.com/in/zhi--li/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>找到我。</p></div></div>    
</body>
</html>