<html>
<head>
<title>How to code Gaussian Mixture Models from scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 Python 从头开始编写高斯混合模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-code-gaussian-mixture-models-from-scratch-in-python-9e7975df5252?source=collection_archive---------2-----------------------#2019-09-03">https://towardsdatascience.com/how-to-code-gaussian-mixture-models-from-scratch-in-python-9e7975df5252?source=collection_archive---------2-----------------------#2019-09-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="35d1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 NumPy 的 GMMs 和最大似然优化</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/63140fb14b90220eb5fcd06bb0af82d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*fTS_6Ntq5EGu_i0nTUzGyw.gif"/></div></div></figure><p id="e4e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated"><span class="l lr ls lt bm lu lv lw lx ly di">在</span>无监督学习算法领域，高斯混合模型或 GMM 是特殊公民。GMM 基于这样的假设，即所有数据点都来自具有未知参数的高斯分布的精细混合。它们是试图了解真实数据分布的参数化生成模型。因此，一旦我们知道了高斯参数，我们就可以从与源相同的分布中生成数据。</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="6768" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以认为 GMMs 是 K-Means 聚类算法的软推广。像 K-Means 一样，GMM 也需要聚类的<strong class="kw iu">数 K </strong>作为学习算法的输入。然而，这两者之间有一个关键的区别。K-Means 只能学习圆形的聚类。另一方面，GMM 可以学习任何椭圆形状的簇。</p><p id="d0d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，K-Means 只允许一个观察值属于一个且仅属于一个聚类。<strong class="kw iu">不同的是，GMM 给出了将每个例子与给定的聚类相关联的概率</strong>。</p><blockquote class="mg"><p id="d4ab" class="mh mi it bd mj mk ml mm mn mo mp lp dk translated">换句话说，GMM 允许一个观测值属于多个聚类——具有一定程度的不确定性。</p></blockquote><p id="eb6d" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">对于每个观察，GMM 学习该例子属于每个聚类 k 的概率</p><p id="afd7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一般来说，GMM 试图将每个聚类学习为不同的高斯分布。它假设数据是从有限的<strong class="kw iu">高斯混合</strong>高斯产生的。</p><p id="8ebe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设一维数据和聚类数 K 等于 3，GMM 试图学习 9 个参数。</p><ul class=""><li id="1df4" class="mv mw it kw b kx ky la lb ld mx lh my ll mz lp na nb nc nd bi translated">平均值的 3 个参数</li><li id="c3b3" class="mv mw it kw b kx ne la nf ld ng lh nh ll ni lp na nb nc nd bi translated">方差的 3 个参数</li><li id="c708" class="mv mw it kw b kx ne la nf ld ng lh nh ll ni lp na nb nc nd bi translated">3 个缩放参数</li></ul><p id="f1f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，每个聚类由一个单独的高斯分布表示(在本例中，总共 3 个)。对于每个高斯，它从数据中学习<strong class="kw iu">一个均值</strong>和<strong class="kw iu">一个方差</strong>参数。3 个缩放参数(每个高斯 1 个)仅用于密度估计。</p><p id="bf1b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了学习这些参数，GMM 使用期望最大化(EM)算法来优化最大似然。在此过程中，GMM 使用贝叶斯定理计算给定观测值<strong class="kw iu"> xᵢ </strong>属于每个聚类 k 的概率，其中 k = 1，2，…，k</p><p id="6b7f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们来看一个例子。为了简单起见，让我们考虑一个合成的一维数据。但是，正如我们将在后面看到的，该算法很容易扩展到 D &gt; 1 的高维数据。你可以用这个<a class="ae nj" href="https://colab.research.google.com/drive/1PChVghOtJSjWwCYVoevnLGXjnbQNvFOY" rel="noopener ugc nofollow" target="_blank"> jupyter 笔记本</a>跟着做。</p><p id="3e4d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了建立一个玩具数据集，我们从从不同的高斯分布中采样点开始。每一个(有自己的均值和方差)代表我们合成数据中的一个不同的聚类。为了让事情更清楚，让我们用 K 等于 2。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/6eff1c4cb2d393147f440efff59feee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/0*NZUNoiiGqnuADEwA"/></div></figure><p id="bddc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面，你可以看到合成的数据。我们将使用它作为训练数据，使用 GMM(从数据中)学习这些聚类。请注意，某些值在某些时候确实会重叠。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5951f5d5094cdf675e3ddde8d690ed0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*qMLGi7lMiKkODCkIVegbQA.png"/></div></figure><p id="b5a0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以把 GMM 看作高斯分布的加权和。聚类数 K 定义了我们想要拟合的高斯数。</p><p id="04dc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们所说的，集群的数量需要预先定义。为简单起见，让我们假设我们知道聚类的数量，并将 K 定义为 2。在这种情况下，GMM 将尝试学习 2 个高斯分布。对于一维数据，我们需要了解每个高斯的均值和方差参数。</p><p id="eb0c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在开始运行 EM 之前，我们需要给出可学习参数的初始值。我们可以猜测均值和方差的值，并将权重参数初始化为 1/k。</p><p id="7978" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们可以使用 EM 算法开始最大似然优化。EM 可以简化为两个阶段:E(期望)和 M(最大化)步骤。</p><p id="c984" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在 e 步骤中，我们使用估计的参数计算每个观测值<strong class="kw iu"> xᵢ </strong>的可能性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/b46cc8875d4c8656f2d82b8f72e9d7d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NCTx6lqM2Kc-OWUQ"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">1-d gaussian distribution equation</figcaption></figure><p id="b4b1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于每个聚类 k = 1，2，3，…，K，我们使用均值和方差的估计值来计算数据的概率密度(pdf)。在这一点上，这些值仅仅是随机猜测。</p><p id="a6e4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们可以计算给定示例<strong class="kw iu"> xᵢ </strong>属于<strong class="kw iu"> kᵗʰ </strong>集群的可能性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/a287d3d620371a792d312ace87b3bfb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/0*gruxFBgDlZme-AbA"/></div></figure><p id="19ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">利用贝叶斯定理，我们得到了第 k 个高斯的后验概率来解释数据。那就是观测值<strong class="kw iu"> xᵢ </strong>由<strong class="kw iu"> kᵗʰ </strong>高斯产生的可能性。请注意，参数<strong class="kw iu">φ</strong>充当我们的先验信念，即一个例子来自我们正在建模的一个高斯模型。由于我们没有任何额外的信息来支持一个高斯模型而不是另一个，我们开始猜测一个例子来自每个高斯模型的概率相等。然而，在每次迭代中，我们改进我们的先验直到收敛。</p><p id="fd07" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，在最大化或 M 步骤中，我们如下重新估计我们的学习参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/ee46f86d40e2856b6b1e0b6ce4dc4b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gUFUIQGw-SjPPGVvHHPNbg.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Parameter update equations</figcaption></figure><p id="f2a3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，对于每个聚类，我们更新均值(<strong class="kw iu"> μₖ </strong>)、方差(<strong class="kw iu"> σ₂ </strong>)和缩放参数<strong class="kw iu">φₖ</strong>。为了更新平均值，请注意，我们使用条件概率<strong class="kw iu"> bₖ </strong>对每个观察值进行加权。</p><p id="98f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以重复这些步骤，直到收敛。这可能达到参数更新小于给定容限阈值的程度。在每一次迭代中，我们更新我们的参数，使其类似于真实的数据分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/11e032f59971215ad9c49731ee26b6b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*FAuNzpW8X5FTunwiiyJDoA.gif"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Gaussian Mixture Models for 1D data using K equals 2</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/f959034ff69f383759b6d687dfa6ff69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uRWeVw2a-8MHFzxEjv4rcg.gif"/></div></div></figure><p id="ba9b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于高维数据(D&gt;1)，只有很少的东西会改变。现在我们估算平均值和<strong class="kw iu">协方差</strong>，而不是估算每个高斯的平均值和方差。协方差是形状为(D，D)的方阵，其中 D 表示数据的维度。下面，我展示了一个不同的例子，其中一个 2-D 数据集被用来拟合不同数量的高斯混合。</p><p id="a61c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在查看<a class="ae nj" href="https://colab.research.google.com/drive/1Eb-G95_dd3XJ-0hm2qDqdtqMugLkSYE8" rel="noopener ugc nofollow" target="_blank"> jupyter 笔记本的二维数据。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/404103291bc2916df58609d82c32ff61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1xoPrNbbyQ0zXoueIal77w.gif"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Gaussian Mixture Models for 2D data using K equals 2</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/d15ac52cf8dd9a0cea754dee06cf83bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*EKRWlwYlrvjiEAx_7_EDhA.gif"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Gaussian Mixture Models for 2D data using K equals 3</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/79d5aa21a2effaa7eca1594837a4541c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ghHsFxL8Q9XQidDVDgHOOA.gif"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Gaussian Mixture Models for 2D data using K equals 4</figcaption></figure><p id="2ef1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请注意，上面的合成数据集是从 4 种不同的高斯分布中提取的。然而，GMM 为<strong class="kw iu">两个</strong>、<strong class="kw iu">三个</strong>和<strong class="kw iu">四个</strong>不同的集群提供了一个很好的案例。</p><p id="cf66" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是高斯混合模型。这是这篇文章的一些要点。</p><ul class=""><li id="96a6" class="mv mw it kw b kx ky la lb ld mx lh my ll mz lp na nb nc nd bi translated">GMM 是一族生成参数无监督模型，试图使用高斯分布对数据进行聚类。</li><li id="cb24" class="mv mw it kw b kx ne la nf ld ng lh nh ll ni lp na nb nc nd bi translated">和 K-Mean 一样，你还是需要定义你要学习的聚类数 K。</li><li id="4c48" class="mv mw it kw b kx ne la nf ld ng lh nh ll ni lp na nb nc nd bi translated">与 K-Means 不同，GMM 将聚类表示为概率分布。这允许一个数据点属于多个具有一定不确定性的聚类。</li></ul><p id="cb90" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">感谢阅读。</strong></p></div></div>    
</body>
</html>