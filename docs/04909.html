<html>
<head>
<title>Review: Deep Layer Cascade (LC) — Not All Pixels Are Equal (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:深层级联(LC) —并非所有像素都相等(语义分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-deep-layer-cascade-lc-not-all-pixels-are-equal-semantic-segmentation-cb29ec71b1a5?source=collection_archive---------18-----------------------#2019-07-24">https://towardsdatascience.com/review-deep-layer-cascade-lc-not-all-pixels-are-equal-semantic-segmentation-cb29ec71b1a5?source=collection_archive---------18-----------------------#2019-07-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1395" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">对于容易、中等和困难区域的分割，优于</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------"/><strong class="ak"/><a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------">CRF-RNN</a><strong class="ak"/><a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96?source=post_page---------------------------">SegNet</a><strong class="ak"/><a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5?source=post_page---------------------------">dilated net</a><strong class="ak"/><a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------">deeplabv 1&amp;deeplabv 2</a></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/7fbc7b58d94731a6e5a1bef97fd79078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-wQpqQF7Ua8ahkyFsn1pvA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Segmentation for Easy, Moderate and Hard Regions</strong></figcaption></figure><p id="f8c5" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di"> It </span>很久没有复习论文了，因为最近我一直在忙一些几何问题的研究工作，这不是我的强项。而且还在进行中。而昨天(23/07/2019)，我已经加入了 AWSome Day ( <a class="ae kf" href="https://www.linkedin.com/feed/update/urn:li:activity:6559395566839787520/" rel="noopener ugc nofollow" target="_blank">照片</a>)，同时抽时间看了一下<strong class="kz ir">深层级联(LC) </strong>论文，lol。因此，在这个故事中，我想谈谈这篇由中文大学(CUHK)和深圳先进技术研究院发表的<strong class="kz ir"> 2017 CVPR </strong>论文，这篇论文被引用了<strong class="kz ir"> 60 多次</strong>。(<a class="mc md ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----cb29ec71b1a5--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="ed56" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">动机:</p><ul class=""><li id="ea9d" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated">原本使用<strong class="kz ir">深度骨干网</strong>和<strong class="kz ir">高分辨率特征图</strong>，使得分割过程<strong class="kz ir">变慢</strong>。</li></ul><p id="0994" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">主要思想:</p><ul class=""><li id="087a" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated"><strong class="kz ir">早期地层划分的易发区</strong>。</li><li id="7074" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir">仅使用区域卷积(RC) </strong>通过后面/更深的层分类的硬/困难区域。</li><li id="921a" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">最后，<strong class="kz ir">分割精度提高</strong>而<strong class="kz ir">训练和推理时间减少</strong>。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="3b20" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">概述</h1><ol class=""><li id="d684" class="me mf iq kz b la nr ld ns lg nt lk nu lo nv ls nw mk ml mm bi translated"><strong class="kz ir">深层级联</strong></li><li id="a792" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls nw mk ml mm bi translated"><strong class="kz ir">区域卷积(RC) </strong></li><li id="ba66" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls nw mk ml mm bi translated"><strong class="kz ir">训练</strong></li><li id="aaa5" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls nw mk ml mm bi translated"><strong class="kz ir">消融研究</strong></li><li id="fcd5" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls nw mk ml mm bi translated"><strong class="kz ir">性能和速度分析</strong></li><li id="6fa1" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls nw mk ml mm bi translated"><strong class="kz ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="5b63" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated"><strong class="ak"> 1。深层级联(LC) </strong></h1><h2 id="18c6" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">1.1.将<a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="ak">Inception-ResNet(IRNet)</strong></a><strong class="ak">从图像分类目的转向语义分割目的</strong></h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oj"><img src="../Images/4635c1f06632829499edc8bec9197310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CKYKJPQ1OSqKqRD5CdDU4Q.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Original </strong><a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="bd kw">Inception-ResNet (IRNet)</strong></a><strong class="bd kw"> for Image Classification</strong></figcaption></figure><ul class=""><li id="7f72" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="kz ir">【Inception-ResNet(IRNet)</strong></a>，原本用于图像分类，如上图，修改后用于语义分割。</li><li id="7426" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">首先，为了<strong class="kz ir">增加预测</strong>的分辨率，IRNet 末端的池层被移除。并且通过减小“缩减-A/B”(从 2 到 1)中的卷积步长来扩大特征图的大小。因此，网络输出(标签图)的大小扩大了 4 倍。</li><li id="7e9b" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">其次，<strong class="kz ir">小批量的大小受限</strong>(例如 8)。</li></ul><h2 id="6071" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">1.2.从 IRNet 到 LC (IRNet-LC)</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ok"><img src="../Images/3ff1cb7abf17a82a89f9ec762efe1f4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10NXL0ak0ZGnlVWrxavT_g.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">IRNet After Layer Cascade (LC) (IRNet-LC)</strong></figcaption></figure><ul class=""><li id="5540" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated">如上图所示，IRNet-LC 有三个阶段<strong class="kz ir">和</strong>。这三个阶段分别用<strong class="kz ir">黄色</strong>、<strong class="kz ir">绿色</strong>和<strong class="kz ir">蓝色</strong>来区分。</li><li id="b846" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir">两个卷积层和一个 softmax 损耗被附加在每个阶段的末尾</strong>。<strong class="kz ir">每个阶段都有自己的损失函数</strong>。</li><li id="c68c" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">第一阶段预测 21×64×64 分割标记图 L1。</li><li id="df96" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">如果第<em class="ol"> i </em>个像素的最大分数大于阈值<em class="ol"> p </em>(例如，0.95)，则接受预测类，并且该像素不传播到阶段 2。</li><li id="a525" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">发现在<strong class="kz ir"> <em class="ol"> p </em> =0.95 </strong>的情况下，第一阶段<strong class="kz ir">中预测置信度&gt; 0.95 的像素占据了一幅图像近 40%的区域，包含了大量的易像素。</strong></li><li id="c11c" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">以及少量极硬像素，这些极硬像素具有被错误分类的高置信度。</li><li id="682a" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir">通过使更深的层能够聚焦于前景对象，从网络中移除那些容易的像素显著减少了计算并提高了准确性。</strong></li><li id="3bef" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">阶段 2 到阶段 3 的情况类似。</li><li id="16dc" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">在通过所有三个阶段传播图像之后，我们直接组合这些阶段的预测标签图作为最终预测。</li><li id="325f" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">最后，<strong class="kz ir"> stage-1 </strong>信任大多数<strong class="kz ir">背景</strong>中的预测。</li><li id="af42" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">阶段 2 和阶段 3 被学习来预测“较硬”区域，例如“人”和“马”。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="8686" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">2.<strong class="ak">区域卷积(RC) </strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi om"><img src="../Images/abce697bcc8197ac7fb34887269b68bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*tUJChlb8iLux5lIplAY28A.png"/></div></figure><ul class=""><li id="dc64" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated"><strong class="kz ir"> (a) </strong>:标准正则卷积。</li><li id="9e90" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir">(b)</strong>:<strong class="kz ir">区域卷积(RC)中的滤波器只卷积感兴趣的区域</strong>，记为<em class="ol"> M </em>，忽略其他区域，大大减少了计算量。<strong class="kz ir">其他区域的值直接设置为零</strong>。</li><li id="68f8" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">这意味着<strong class="kz ir"> <em class="ol"> M </em> </strong>被实现为<strong class="kz ir">二进制掩码</strong>。</li><li id="3c8e" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir"> (c) </strong> : <strong class="kz ir"> RC 上一个剩余模块</strong>，<em class="ol">h</em>(<em class="ol">I</em>)=<em class="ol">I</em>+conv(<em class="ol">I</em>)。这相当于学习了一个<strong class="kz ir">掩蔽残差表示</strong>。</li><li id="a291" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">因此，<strong class="kz ir">每个阶段只需要学习它所关注的区域的特征</strong>。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="2b43" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">3.培养</h1><p id="663f" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg on li lj lk oo lm ln lo op lq lr ls ij bi translated">网络由 ImageNet 中的<strong class="kz ir">预训练初始化。通过从正态分布中采样来初始化附加参数。而网络是<strong class="kz ir">先进行初始训练，再进行级联训练</strong>。</strong></p><h2 id="cc21" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">3.1.初步训练</h2><ul class=""><li id="643e" class="me mf iq kz b la nr ld ns lg nt lk nu lo nv ls mj mk ml mm bi translated">这一步<strong class="kz ir">类似于深度监督网络(DSN) </strong>，在网络的不同层有<strong class="kz ir">多个相同的损失函数</strong>。</li><li id="de23" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">它学习区分性和鲁棒性特征。</li><li id="4822" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">每一级都被训练成最小化逐像素的 softmax 损失函数。</li><li id="5fbf" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">通过使用反向传播(BP)和随机梯度下降(SGD)来联合优化这些损失函数。</li></ul><h2 id="fa27" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">3.2.级联训练</h2><ul class=""><li id="64f7" class="me mf iq kz b la nr ld ns lg nt lk nu lo nv ls mj mk ml mm bi translated">通过利用<em class="ol"> p </em>的级联策略对网络进行微调。</li><li id="a32a" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">更具体地说，<strong class="kz ir">BP 中的梯度仅传播到每个阶段的感兴趣区域</strong>。</li><li id="6a56" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">直观地说，当前阶段对在前一阶段中具有低置信度的像素进行微调，<strong class="kz ir">使得“更硬”的像素能够被更深的层捕获，以提高分割精度并减少计算</strong>。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="d715" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">4.<strong class="ak">消融研究</strong></h1><h2 id="19f0" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">4.1.<em class="oq"> p 的值</em></h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi or"><img src="../Images/db5d52cbd98f7666db47e440024c647c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*clnzc822lFM39ObsOsA30w.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Performances Using Different <em class="oq">p.</em></strong></figcaption></figure><ul class=""><li id="6d5d" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated">PASCAL VOC 2012 验证集用于消融研究。</li><li id="1a2f" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">如果<em class="ol"> p </em> =1，就跟 DSN 一样，比全卷积 IRNet 略胜一筹。</li><li id="848b" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">当<em class="ol"> p </em>减小时，更容易的区域在早期阶段被分类，而较难的区域由后期阶段逐步处理。</li><li id="15a5" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">如果<em class="ol"> p </em>太小，算法可能会变得过于乐观，即在早期阶段处理许多硬区域，并提前做出决策。性能将受到损害。</li><li id="f94f" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">当<strong class="kz ir"> <em class="ol"> p </em> = 0.985 </strong>时，即前期和<strong class="kz ir">的<strong class="kz ir"> 52%区域附近的 LC 过程达到最佳性能</strong>。该值用于以下所有实验。</strong></li></ul><h2 id="ee5a" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">4.2.层级联的有效性</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi os"><img src="../Images/980bcfc7643f783b0218441679a16e31.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*NxI21YXYhiBk_wfz2bsF7g.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Effectiveness of Layer Cascade (LC)</strong></figcaption></figure><ul class=""><li id="ae90" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated"><strong class="kz ir"> IRNet </strong>:全卷积 IRNet。</li><li id="5457" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir"> DSN </strong> : IRNet 只经过初步训练。</li><li id="8b66" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir"> DSN+Dropout </strong>:带 Dropout 的 DSN。</li><li id="30b3" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir">模型级联(MC) </strong> : MC 将 IRNet 分为三个阶段，每个阶段分别进行训练。当训练某一阶段时，所有先前阶段的参数都是固定的。</li><li id="6948" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">由此产生子模型，并使模型变浅，最终获得不良性能。</li><li id="86b8" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir">层叠(LC) </strong>:相反，LC 具有层叠的吸引人的性质，并且还保持整个模型的内在深度。</li><li id="a154" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">针对硬区域自适应地保持模型深度的能力使得 LC 在比较中表现突出。</li></ul><h2 id="351e" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">4.3.逐阶段分析</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ot"><img src="../Images/d8efd672ecb478b80a35308238b3f7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DFZPmf7nFurqc_Dn44X63A.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">(a) Change of label distribution in stage-2 and stage-3, (b) percentages of pixels at each stage for individual classes</strong></figcaption></figure><ul class=""><li id="07c6" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated"><strong class="kz ir"> (a) </strong>示出了<strong class="kz ir">像素数相对于阶段-2 和-3 中的每个类别</strong>如何变化。所有比率都增加了，属于 1 到 1.4 的范围。</li><li id="5f65" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">因为阶段 1 已经处理了容易的区域(即“背景”)并将困难的区域(即“前景”)留给阶段 2。</li><li id="a716" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">阶段 2 到阶段 3 的情况类似。第三阶段进一步关注更难的课程</li><li id="a453" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">(b)显示了像“椅子”和“桌子”这样的较难的类具有由较深的层处理的更多像素(阶段 3)。</li></ul><h2 id="47cf" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">4.4.逐阶段可视化</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ou"><img src="../Images/d54784d8b875a53460e6664cb7eff146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IXlP-f447c_ZrFrMpmgPnw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Visualization of different stages’ outputs on PASCAL VOC 2012 dataset</strong></figcaption></figure><ul class=""><li id="546b" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated">在 PASCAL VOC 2012 中，像“背景”和“人脸”这样的简单区域首先由 LC 中的 stage-1 标记。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ov"><img src="../Images/876266d2517be05b80d25665dbb6b771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tbZVYAvzQG-KO3YYMTlRDg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Visualization of different stages’ outputs on Cityscape dataset</strong></figcaption></figure><ul class=""><li id="ce2d" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated">类似地，在城市景观中，像“道路”和“建筑物”这样的简单区域首先由阶段 1 标记。其他的小物体和细节，如“柱子”和“行人”由阶段 2 和阶段 3 处理。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="1379" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">5.性能和<strong class="ak">速度分析</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/6e8f999e193864d3ab672efc9e2159f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*JF5e1fumbqKU_36AiHHizg.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Performance on PASCAL VOC 2012</strong></figcaption></figure><ul class=""><li id="50a8" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated">这里对<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------"> DeepLabv2 </a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96?source=post_page---------------------------"> SegNet </a>没有前后处理。</li><li id="6013" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------"> <strong class="kz ir"> DeepLabv2 </strong> </a>:使用超深<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------"> ResNet-101 </a>的 mIoU 为 20.42。因此，推断的速度很慢(7.1 FPS)。</li><li id="7d2d" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96?source=post_page---------------------------"> <strong class="kz ir"> SegNet </strong> </a>:速度更快(14.6 FPS)，但代价超过 1000 万。</li><li id="b048" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir"> LC </strong>:具有区域卷积的级联端到端可训练框架允许其以可接受的速度(14.7 FPS)实现最佳性能(73.91 mIoU)。</li><li id="805b" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated"><strong class="kz ir"> LC(快速)</strong>:0.8 的较小<em class="ol"> p </em>的 LC。它在 23.6 FPS 下仍显示出 66.95 的竞争 mIoU。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ox"><img src="../Images/b80d692dc1230d73a8c4433a4ef8473a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*itnxRg6nZZZlbrzsZZXyvQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Further Performance and Speed Trade-off</strong></figcaption></figure><ul class=""><li id="0941" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated">减小<em class="ol"> p </em>会稍微影响精度，但会大大减少计算时间。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="3ec9" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">6.与最先进方法的比较</h1><h2 id="f0e0" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">6.1.帕斯卡 VOC 2012</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oy"><img src="../Images/1e04c80f7d46ac18741b71fd95347640.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sVVP37TyxFskkljLkGooUQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">mIoU on PASCAL VOC 2012 Test Set (+: Pre-training on MS COCO)</strong></figcaption></figure><ul class=""><li id="9f92" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated">LC 实现了 80.3 的 mIoU，并通过 COCO 的预训练将 mIoU 进一步提高到 82.7，优于 SOTA 的方法，如<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------"> FCN </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------">DeepLabv1&amp;DeepLabv2</a>。</li><li id="5089" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">LC 赢得了 20 个前台类中的 16 个。</li><li id="e2ab" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">在一些特殊的类别中，如“自行车”、“椅子”、“植物”和“沙发”，可以观察到较大的收益。</li></ul><h2 id="b1c4" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">6.2.城市风光</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oz"><img src="../Images/136b0ebcfa7e0427b1874193768276c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2XMaB_4C8Svamqo4DFDz2A.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">mIoU on Cityscape Test Set</strong></figcaption></figure><ul class=""><li id="43c2" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated">[19]由于使用了更深的主干网络来探索更丰富的上下文信息，因此性能略优于 LC。</li><li id="690b" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">但是 LC 仍然在 19 个等级中的 9 个上获胜。</li><li id="0227" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">LC 在“传统上被视为”硬类别的类别中表现突出，例如，“栅栏”、“杆子”、“标志”、“卡车”、“公共汽车”和“自行车”，这些类别通常表现出灵活的形状和精细的细节。</li></ul><h2 id="802f" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">6.3.不同设置的更多比较</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pa"><img src="../Images/5e8ddf673d34b6539bb28981c2e59513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vld8SQ643YlnH3mC5BWi0g.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Comparisons with state-of-the-art methods on PASCAL VOC 2012 test set</strong></figcaption></figure><ul class=""><li id="98a6" class="me mf iq kz b la lb ld le lg mg lk mh lo mi ls mj mk ml mm bi translated">IRNet-LC 使用<a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet-v2(IRNet)</a>作为主干网络，比<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------"> ResNet-101 </a>要小(35.5M vs. 44.5M)。</li><li id="78b9" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">IRNet-LC 采用了<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------"> DeepLabv2 </a>中使用的阿特鲁空间金字塔池。</li><li id="0271" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">没有对 COCO 女士进行预训练的 IRNet-LC 获得最佳性能。</li><li id="9bec" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">当删除“COCO”、“multiscale”和“CRF”时，IRNet-LC 仍然获得了与<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------"> DeepLabv2 </a>相当的性能(78.2%对 79.7%)，但在 FPS 方面明显优于<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------">deeplabv 2</a>(14.3 FPS 对 0.9 fps)。</li><li id="b23c" class="me mf iq kz b la mn ld mo lg mp lk mq lo mr ls mj mk ml mm bi translated">IRNet-LC 在不采用任何预处理和后处理步骤的情况下，分别比最先进的系统如<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------"> CRF-RNN </a>和 DPN 高出 3.5%和 0.7%。</li></ul><h2 id="35f1" class="nx na iq bd nb ny nz dn nf oa ob dp nj lg oc od nl lk oe of nn lo og oh np oi bi translated">6.4.形象化</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pb"><img src="../Images/1880d5f7a3f21dd297f426b507437942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*if_HzWMBux0dEPV34PttoA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">PASCAL VOC 2012 Validation Set</strong></figcaption></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pc"><img src="../Images/b776ea46d2a306e606488ef18ae0f4f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LAEsc4jPCmATD80loFpg2A.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Cityscape Validation Set</strong></figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="ea5b" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">参考</h1><p id="0f78" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg on li lj lk oo lm ln lo op lq lr ls ij bi translated">【2017 CVPR】【LC】<br/><a class="ae kf" href="https://arxiv.org/abs/1704.01344" rel="noopener ugc nofollow" target="_blank">并非所有像素都相等:经由深层级联的难度感知语义分割</a></p><h1 id="cd10" class="mz na iq bd nb nc pd ne nf ng pe ni nj jw pf jx nl jz pg ka nn kc ph kd np nq bi translated">我以前的评论</h1><p id="65ec" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg on li lj lk oo lm ln lo op lq lr ls ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(是)(这)(些)(人)(,)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(但)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(想)(要)(让)(这)(些)(人)(都)(有)(这)(些)(情)(况)(,)(我)(们)(还)(不)(想)(要)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(就)(是)(这)(些)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(都)(是)(很)(强)(的)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(起)(来)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(</p><p id="3b25" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">物体检测</strong> [ <a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------" rel="noopener">过食</a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------" rel="noopener">快 R-CNN </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------">快 R-CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------">MR-CNN&amp;S-CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------">DeepID-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858?source=post_page---------------------------">CRAFT</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------">R-FCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------">离子</a><a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------"> [</a><a class="ae kf" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------">G-RMI</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------">yolo v1</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------">yolo v2/yolo 9000</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------">yolo v3</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------">FPN</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------">retina net</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------">DCN</a></p><p id="cd4b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">语义切分</strong>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------">FCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=post_page---------------------------">de convnet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------">deeplabv 1&amp;deeplabv 2</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------">CRF-RNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96?source=post_page---------------------------">SegNet</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990?source=post_page---------------------------" rel="noopener">parse net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5?source=post_page---------------------------">dilated net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5?source=post_page---------------------------">DRN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=post_page---------------------------">RefineNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-gcn-global-convolutional-network-large-kernel-matters-semantic-segmentation-c830073492d2?source=post_page---------------------------"/></p><p id="2cff" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生物医学图像分割</strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6?source=post_page---------------------------" rel="noopener">cumevision 1</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560?source=post_page---------------------------" rel="noopener">cumevision 2/DCAN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=post_page---------------------------">U-Net</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6?source=post_page---------------------------" rel="noopener">CFS-FCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43?source=post_page---------------------------" rel="noopener">U-Net+ResNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc?source=post_page---------------------------">多通道</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974?source=post_page---------------------------">V-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1?source=post_page---------------------------">3D U-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1?source=post_page---------------------------">M FCN</a>]</p><p id="9f3a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">实例分割</strong> [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b?source=post_page---------------------------" rel="noopener"> SDS </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979?source=post_page---------------------------">超列</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339?source=post_page---------------------------">深度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61?source=post_page---------------------------">清晰度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------">多路径网络</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34?source=post_page---------------------------"> MNC </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92?source=post_page---------------------------">实例中心</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2?source=post_page---------------------------"> FCIS </a></p><p id="dccb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">)( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )(</p><p id="be95" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> <a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=post_page---------------------------"> DeepPose </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=post_page---------------------------"> Tompson NIPS'14 </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------"> Tompson CVPR'15 </a> <a class="ae kf" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------" rel="noopener"> CPM </a></strong></p></div></div>    
</body>
</html>