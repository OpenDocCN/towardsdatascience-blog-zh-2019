<html>
<head>
<title>Lessons Learned from Building an AI Writing App [Guide, Open-Sourced]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建人工智能写作应用的经验教训[指南，开源]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lessons-learned-from-building-an-ai-writing-app-guide-open-sourced-6f661f9caec6?source=collection_archive---------41-----------------------#2019-10-15">https://towardsdatascience.com/lessons-learned-from-building-an-ai-writing-app-guide-open-sourced-6f661f9caec6?source=collection_archive---------41-----------------------#2019-10-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/daf90f290ee1ee1f7b9f9998ebbb9a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VziSZYsr9ElJrs23"/></div></div></figure><p id="3064" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://writeup.ai" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu"> writeup.ai </strong> </a> <strong class="kd iu">是一个开源的文本机器人，与你一起写作。</strong>它(大部分)由 OpenAI 的<a class="ae kz" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>驱动，并有额外的微调型号:</p><ul class=""><li id="8374" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">合法的</li><li id="97dd" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">文案和使命陈述</li><li id="4991" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">抒情诗</li><li id="1088" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">[文学作品]哈利波特</li><li id="0143" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">《权力的游戏》</li><li id="5642" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">学术研究文摘</li></ul><p id="13a3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">主要的技术挑战是创建一个能够快速<strong class="kd iu"/><strong class="kd iu"/><strong class="kd iu">支持 10-20 个重度用户的应用程序，该应用程序可以交付 OpenAI 的 GPT-2 媒体(一个生成文本的 ml 模型)。</strong></p><h1 id="1e3e" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">初始状态:</h1><ul class=""><li id="860d" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">开始是作为学习在 NLP(自然语言处理)中训练 ML 模型的借口。我最终主要学习了部署模型。</li><li id="baba" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">估计要一个月才能建成。不对。花了我三个月的时间。</li><li id="6f3c" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">工程师很难估计。过分自信的傻逼估计就更难了(咳)。</li><li id="1235" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">遗憾的是，我对训练模型了解不多(lol)。还是什么都不知道。</li><li id="a0aa" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">许多开源的训练脚本(<a class="ae kz" href="https://github.com/nshepperd/gpt-2/tree/finetuning" rel="noopener ugc nofollow" target="_blank"> nsheppard </a>)做了繁重的工作。发现 gwern 的<a class="ae kz" href="https://www.gwern.net/GPT-2" rel="noopener ugc nofollow" target="_blank"> GPT2 指南</a>对于培训教程来说非常有价值。另一个很棒的快速入门工具是 Max 的<a class="ae kz" href="https://github.com/minimaxir/gpt-2-simple" rel="noopener ugc nofollow" target="_blank"> gpt-2-simple </a> repo。</li><li id="84ca" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">writeup.ai 大部分都是开源的。我添加了相应的链接，从我的错误/失败中吸取教训。我还在 GitHub 中添加了代码的直接链接。</li></ul><h1 id="eef7" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">链接。它是开源的！</h1><p id="4f06" class="pw-post-body-paragraph kb kc it kd b ke mm kg kh ki mn kk kl km mr ko kp kq ms ks kt ku mt kw kx ky im bi translated">app—<a class="ae kz" href="https://writeup.ai" rel="noopener ugc nofollow" target="_blank">https://write up . ai</a><br/><a class="ae kz" href="https://github.com/jeffshek/writeup-frontend" rel="noopener ugc nofollow" target="_blank">前端回购</a> <br/> <a class="ae kz" href="https://github.com/jeffshek/open" rel="noopener ugc nofollow" target="_blank">后端回购</a></p><h1 id="ac03" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">背景:</h1><ul class=""><li id="575f" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">在<a class="ae kz" href="https://reactjs.org/" rel="noopener ugc nofollow" target="_blank"> React </a>、<a class="ae kz" href="https://www.djangoproject.com" rel="noopener ugc nofollow" target="_blank"> Django </a>、<a class="ae kz" href="https://palletsprojects.com/p/flask/" rel="noopener ugc nofollow" target="_blank"> Flask </a>做了太多年的网络应用。</li><li id="0a72" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">对机器学习(ML)和 MLOps(机器学习 devops)不熟悉，所以请以健康的怀疑态度阅读任何建议。</li></ul><h1 id="fb68" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">读者:</h1><ul class=""><li id="ded1" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">一些网络开发的背景是必要的，但是我积极地用行话来帮助链接。</li><li id="5b60" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">机器学习的基础知识是有帮助的。</li></ul><h1 id="7a0a" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">警告:</h1><ul class=""><li id="24f8" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">简明扼要的要点。</li><li id="0960" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">完整的短语和缩写。即。机器学习&gt;机器学习</li><li id="e64f" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">在大多数情况下，模型意味着机器学习模型。编写“ML 模型”是多余的。</li><li id="1a0a" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">供应商锁定是真实的。享受了<a class="ae kz" href="https://cloud.google.com" rel="noopener ugc nofollow" target="_blank">谷歌云平台</a> (GCP)这么多，从来没有打算离开。一些建议以 GCP 为中心。</li><li id="7515" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">GCP 部署和扩展 ML 资源比以前的 AWS 体验更好。</li><li id="979d" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">电子邮件，<a class="ae kz" href="https://twitter.com/shekkery" rel="noopener ugc nofollow" target="_blank">推特</a>，评论任何你想澄清的事情。</li></ul><h1 id="7ccd" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">技术架构:</h1><figure class="mv mw mx my gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mu"><img src="../Images/6d39fd53c9c1cc9b87daff04453a43fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qXUcVfHbOLF73JCv"/></div></div></figure><ul class=""><li id="bfc5" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">前端(ReactJS)加入后端(Django)上的 WebSocket。通过 WebSockets 与后端通信。<a class="ae kz" href="https://github.com/jeffshek/writeup-frontend/blob/8e2699cd7fffbe301673c32db272ff4510482b82/src/components/MainComponent/Main.js#L187" rel="noopener ugc nofollow" target="_blank">前端代码</a> | <a class="ae kz" href="https://github.com/jeffshek/open/blob/158e7095400e61688e62cde46e5565083d30b66a/open/core/writeup/consumers.py#L90" rel="noopener ugc nofollow" target="_blank">后端代码</a></li><li id="49b2" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">后端解析和序列化前端请求。将消息(文本、算法、设置)和 WebSocket 通道打包到 Google 负载平衡器。<a class="ae kz" href="https://github.com/jeffshek/open/blob/158e7095400e61688e62cde46e5565083d30b66a/open/core/writeup/consumers.py#L131" rel="noopener ugc nofollow" target="_blank">后端代码</a></li><li id="ca20" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">负载平衡器中继到适当的微服务(小型、中型、大型、哈利波特、法律等)。</li><li id="4bed" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">微服务定期用建议词实时更新 websocket。这就产生了“流”的效果。</li><li id="ac08" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">前端从微服务接收更新的 WebSocket 消息。</li><li id="d2e8" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">每个 ML 模型(小、中、大、哈利波特、法律、研究)都是一个微服务。利用率自动扩展。</li><li id="2f83" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">尝试了无数次迭代才做出 fast(er)。</li><li id="d03c" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">我通常不喜欢微服务架构(增加了额外的复杂性)。尽管尽了最大努力，微服务架构对于性能还是必要的。</li><li id="9a73" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">微服务的请求和计算成本与后端服务器有着本质的不同。传统的 web 服务器每秒可以轻松处理 500–5000 多个请求(参见<a class="ae kz" href="https://en.wikipedia.org/wiki/C10k_problem" rel="noopener ugc nofollow" target="_blank"> C10K </a>)。然而，对于一个运行 1gb 模型的实例来说，每秒 50 个请求生成 50-100 个单词就可以压垮一台机器。(*)</li><li id="8602" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">后端和微服务都是用 Python 3.6 写的。姜戈(DRF)负责后台。Django 的一个单独实例被用于微服务。</li><li id="dc1a" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">所有微服务实例都有一个附加的 GPU 或一个 Cascade Lake CPU 来运行 ML 模型。详情如下。</li><li id="bbce" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">后端和微服务托管在谷歌云平台上。</li><li id="b4b1" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">谷歌负载平衡器将所有流量路由到微服务。它基于 URL 后缀"/gpt2-medium，/gtp2-medium-hp 等进行路由..负载平衡器还运行健康检查来检查 CUDA 崩溃。</li></ul><p id="3eb8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">(*) —每当您必须证明您的微服务用例时，这可能意味着它不值得如此复杂。</p><h1 id="812c" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">秘鲁利马三周:</h1><ul class=""><li id="42a2" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">在秘鲁利马为期三周的旅行开始时开始认真编码。Trip 起到了催化剂的作用。</li><li id="a507" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">一些朋友在接近尾声时开始了 beta 测试。缓慢且经常失败。</li><li id="c031" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">我旅行的 80%时间都在一个<a class="ae kz" href="https://worx.pe" rel="noopener ugc nofollow" target="_blank">共同工作空间</a>中编码。</li><li id="d9d5" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">后端和开发运维两周，前端上周。</li><li id="707b" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">随着复杂性的增加，重写了 DevOps。</li><li id="d21d" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">在行程结束时，前端通过 POST 请求与后端通信，后端再转发给微服务。</li><li id="dd46" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">不算好看也不算快，但是看到第一条消息从前端→后端→微服务端到端，让我傻乎乎地兴奋起来。</li></ul><p id="cecb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> MVP 版本</strong></p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/4ae7673d31e3552a38bbc99372de2e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RLsICFCSEGKk3XTB"/></div></div></figure><h1 id="80e2" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">在利马取得的成就:</h1><ul class=""><li id="dfa7" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">前端的合理数量。它有一个简单的文本编辑器和由微服务填充的决策选项。</li><li id="543f" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">后端可以创建 WebSockets 来与前端通信。在第一次迭代中，后端通过 POST 请求与微服务通信，然后将消息转发到 WebSocket。我非常想让微服务保持沉默，不处理 WebSockets。</li><li id="fa8b" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">通过 Ansible 实现自动化部署(后来被重构/移除到 Google 启动脚本中)</li><li id="0031" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated"><strong class="kd iu">错误:提前发射！</strong>事后看来，我本应该在构建 4-5 周后就发布。到那时，它已经是一个很好的 MVP 了，但是我害怕，如果没有那些花里胡哨的东西，它就是一个笑话。</li></ul><figure class="mv mw mx my gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi na"><img src="../Images/d626631d3db33dba867e2bb5d6e7723f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GgErZgZ1phacXCCt"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk"><strong class="bd nf">Random</strong>: There’s something magical about flow, 2:00 AM and an empty coworking space. Anything feels possible.</figcaption></figure><h1 id="b4cf" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">90/90 法则:</h1><blockquote class="ng nh ni"><p id="34b1" class="kb kc nj kd b ke kf kg kh ki kj kk kl nk kn ko kp nl kr ks kt nm kv kw kx ky im bi translated"><em class="it">第</em><strong class="kd iu"><em class="it"/></strong><em class="it">百分之九十的代码占了第</em> <strong class="kd iu"> <em class="it">第</em> </strong> <em class="it">百分之九十的开发时间。剩下的</em> <strong class="kd iu"> <em class="it"> 10 </em> </strong> <em class="it">百分之十的代码占了其他</em> <strong class="kd iu"> <em class="it"> 90 </em> </strong> <em class="it">百分之十的开发时间。——</em><strong class="kd iu"><em class="it">汤姆·卡吉尔，贝尔实验室</em> </strong></p></blockquote><ul class=""><li id="2867" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">工程师不擅长估算。</li><li id="ad15" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">严重低估了机器学习 DevOps(孩子们称之为“MLOps”)的难度。</li><li id="b7f0" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">第二个主要的低估是管理我自己的特性蠕变。</li><li id="f1e3" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">让微服务在 Docker 容器上工作、扩展以及用合适的模型安装 CUDA 驱动程序出乎意料地困难。</li></ul><p id="3ded" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">为了让 ml-微服务在 Docker 容器中工作，我必须:</strong></p><ul class=""><li id="d23a" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">使用安装了 CUDA 的定制 TensorFlow 引导映像</li><li id="3454" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">向 Google 传递一个特殊的标志来安装 nvidia 驱动程序(对于某些图像并不总是必需的)</li><li id="44f0" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">在 GCP 实例上安装 Docker</li><li id="087f" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">将 docker 的默认运行时覆盖到 nvidia(使得使用 docker 和 docker-compose 更容易)。</li><li id="e5cc" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">确保 docker 没有删除我的配置更改或恢复配置。</li><li id="9985" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">将 GitHub 与 GCP 同步；推送时建立 Docker 图像。</li><li id="cf6a" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">将 ml 模型与谷歌云存储同步。GCP 存储和实例的读取速度快得惊人。比 AWS 快。</li><li id="9057" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">从 Google Cloud Build 中提取预构建的 Docker 映像。</li><li id="7d3b" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">从云桶中提取 ml 模型，并使用单独的适当文件夹安装在 Docker 中。</li><li id="3875" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">祈祷所有的需求(TensorFlow / PyTorch)都被正确安装。</li><li id="db9c" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">保存高清图像/快照，以便实例可以从图像快速冷启动。</li><li id="d0e7" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">其余的传统 DevOps (git、monitoring、start docker 容器等)。</li><li id="605d" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">在反思的时候，解决这些问题要容易得多，但是在那时，我不知道我让自己陷入了什么。</li></ul><p id="5498" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">上述步骤必须完全自动化，否则缩放会失败。将 bash 脚本作为自动化部署的一部分来编写(在 2019 年)感觉很脏，但在 Google <a class="ae kz" href="https://cloud.google.com/compute/docs/startupscript" rel="noopener ugc nofollow" target="_blank"> startup-scripts </a>上使用自动缩放时，这是必要的。Kubernetes 是另一个选择，但我没有聪明到用 K8。Google startup-scripts 在机器启动时运行一个 shell 脚本。自动缩放实例时很难使用 Ansible。</p><p id="f524" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">提示:</strong>使用<a class="ae kz" href="https://cloud.google.com/compute/docs/startupscript?hl=en_US&amp;_ga=2.127770893.-1480088751.1570398368#cloud-storage" rel="noopener ugc nofollow" target="_blank">启动-脚本-网址</a>！这告诉实例从定制的 bucket URL 运行脚本。这比将你的脚本复制/粘贴到 GCP 的 CLI/UI 中要好得多。您将会遇到许多对启动脚本的小改动。</p><ul class=""><li id="9708" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">设置后端很简单。这是我第一次使用 Django 通道，它配置 WebSockets。姜戈频道的道具。</li><li id="a93c" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">由于功能蔓延，前端花费了额外的时间。我一直在增加一个新功能，因为我担心它不够好。</li><li id="8bf8" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">微服务最初是用 Flask 编写的(因为每个人都这么建议)。然后我查看了基准测试，意识到如果我剥离它，我可以在 django-rest-framework 中获得相同的性能。在 django-rest-framework 中拥有一切对我来说要容易得多(我的背景是 django)。</li><li id="78ec" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">优化微服务需要一些时间。我试验了不同的显卡、CPU、内存配置和图像。稍后会详细介绍。</li></ul><h2 id="c29a" class="nn lp it bd lq no np dn lu nq nr dp ly km ns nt mc kq nu nv mg ku nw nx mk ny bi translated">令人震惊的事情:</h2><ul class=""><li id="abcc" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">直到两个月前，TensorFlow 图片上的默认 python 是 2.7</li><li id="f376" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">PyTorch 的 docker images 使用了 conda。</li><li id="be14" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">使 nvidia 运行时在 Docker 上工作所需的覆盖。</li><li id="6aef" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">ML 开源代码的例子比比皆是。很多意大利面和胶带。</li><li id="30ad" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">Google 的 TensorFlow Docker 图片都是这么优化的(！)他们跑 PyTorch 的速度比 PyTorch 官方图片上的 PyTorch 跑的还快。这可能是上游 PyTorch 图像中的一个错误，没有进行调查。</li><li id="be26" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">从 Docker 容器(TensorFlow / PyTorch)中提取时，构建可能会在上游中断。一切都变化得太快了，你会习惯的。</li></ul><p id="040c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">提示</strong>:尽量避免手动安装 CUDA。使用谷歌预装的引导镜像。<br/> <strong class="kd iu">提示</strong>:记下什么 CUDA 版本/其他配置。有助于谷歌没有 CUDA 版本+一些其他要求的问题。很多 CUDA 版本 Bug +框架版本的墓地。</p><p id="8bcb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">总的来说</strong>:一旦你知道什么配置(引导映像、Docker 映像、Docker 配置、CUDA)起作用，事情就简单了。难的是提前知道…</p><p id="4cde" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">提示</strong> : ML 有很多新术语/行话要掌握。保存并写一份术语备忘单是有帮助的。我推荐<a class="ae kz" href="https://senrigan.io/blog/chasing-10x-leveraging-a-poor-memory-in-software-engineering/" rel="noopener ugc nofollow" target="_blank">间隔重复和 Anki </a>。</p><h1 id="c341" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">推理剖析:GPU 的名字听起来吓人！</h1><ul class=""><li id="b6b0" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">当在网络上运行机器学习模型时，你有两种硬件选择:GPU(显卡)或 CPU。</li><li id="3820" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated"><strong class="kd iu">优点</strong>:GPU 更快，性能通常是 CPU 的 5-15 倍。<strong class="kd iu"> CONS </strong>:成本更高，增加部署复杂性。</li><li id="4d17" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">许多机器学习任务只需要几分之一秒(又名图像分类)。完全可以使用中央处理器。</li><li id="9cfb" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">大多数用户不会注意到异步任务中 0.05 秒和 0.5 秒的区别。你的网页应该加载很快，但是加载任务结果很慢。</li><li id="0f6b" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">在 CPU 上运行 gpt-2 中等型号(1.2-1.5 GB)并不快。平均 CPU 每秒产生大约 3-7 个单词，不是理想的 UX。</li><li id="42d9" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">在谷歌云上的 Cascade Lake(最新一代至强 CPU，针对 ML 优化)、K80s、V100s 或 P100s 之间做出决策。</li><li id="4843" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated"><strong class="kd iu">这些基准并不是科学基线。这更像是写在餐巾纸上的快速排序启发法。</strong></li><li id="383b" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">表格在 Medium 上显示不好(抱歉！).喀斯喀特湖:8-10 字每秒，K80:12-24 字每秒，P100:32-64 字每秒，V100:32-64 字每秒。</li><li id="35b6" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated"><strong class="kd iu">注意</strong>:这是在运行多个 PyTorch 实例时。我这样做是为了消除 CPU / GPU 阻塞操作的利用率。例如，在具有 GPU 的同一台机器上，由于消除了 CPU/GPU 瓶颈，两个 PyTorch 实例产生的结果可能是单个 PyTorch 实例的 1.5 倍。运行单个 PyTorch 应用程序的实例可能每秒生成 15 个单词，但是运行两个 Python 应用程序可能每秒生成 10 个单词。</li><li id="f994" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">注意:我犯了一个巨大的错误，但是我没有尝试安装最新的<a class="ae kz" href="https://github.com/intel/mkl-dnn" rel="noopener ugc nofollow" target="_blank"> MKL-DNN </a>驱动程序。你可能会看到一个很好的性能跳跃。或者你可能不会。</li><li id="a64c" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">随着文本输入的增加，更高的记忆力是有帮助的。</li><li id="8d44" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">从每周期成本来看，相比 GPU，Cascade Lakes 性价比更高。感觉喀斯喀特湖刚好低于 UX 的流速下限。Cascade Lakes 没有像我希望的那样快速生成提示。</li><li id="b1e6" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">我发现，在生成&lt;50 words at once.</li><li id="2341" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">Ended up using mostly Cascade Lakes and K80s except for GPT-2 Large. Cost.</li></ul><p id="6139" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">提示</strong>时，K80s 与 P100 的权衡对 UX 的影响是可以接受的:你可以让其中的大多数运行为可抢占的，其成本是前者的 1/2。除了产品发布期间，我大部分时间都在使用 preemptible。<br/> <strong class="kd iu">提示</strong>:如果使用 preemptible，Google 会每 24 小时强制重启一次。在凌晨 2 点这样的奇怪时间创建它们，这样对访问者的影响最小。提示:瀑布湖是一个完美的合理权衡。<br/> <strong class="kd iu">注意事项</strong>:这些“基准”仅用于推断(实时运行模型)。大多数训练应该在 GPU 上完成。</p><h1 id="0fad" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">汤姆森给第一次做望远镜的人的规则是:</h1><blockquote class="ng nh ni"><p id="f471" class="kb kc nj kd b ke kf kg kh ki kj kk kl nk kn ko kp nl kr ks kt nm kv kw kx ky im bi translated"><strong class="kd iu"> <em class="it">“做四寸镜比做六寸镜快。”</em> </strong> <em class="it"> —编程珍珠，美国计算机学会通讯，1985 年 9 月</em></p></blockquote><ul class=""><li id="4f03" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">从简单开始:API 端点从 gpt2-medium 生成单词。慢点。同步任务。用过的烧瓶。单端点。</li><li id="be33" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">添加了前端。会查询 API 端点。慢点。重复的请求可能会破坏 API。</li><li id="1a2e" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">添加后端作为 API 端点的看门人。</li><li id="e6c9" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">将烧瓶终点改写为姜戈-DRF。</li><li id="776d" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">集成 django-后端处理 Websockets 的通道。添加了 redis-cache，在转发到微服务之前检查重复请求。</li><li id="c23b" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">更改了前端以通过 WebSockets 进行通信。</li><li id="7346" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">重写了 Ansible 的部署脚本，以处理 Google Cloud 的启动脚本范例。</li><li id="095e" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">集成的微服务通过 WebSockets 进行通信，也就是允许“流式传输”。</li><li id="16dc" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">培训并添加了额外的微服务(小型、中型、大型、法律、写作、哈利波特、歌词、公司、xlnet)</li><li id="79fa" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">复杂性是从简单端点的最初概念逐渐发展而来的。</li></ul><p id="6d81" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">经过这些恶作剧之后，我在部署 ML 方面有了很大的提高。<br/> <strong class="kd iu">弊</strong>:与 GCP 的核心产品(特别是存储、云构建、自动扩展、映像)紧密结合。单一服务供应商上的紧密耦合并不总是理想的(技术上或战略上)。<br/> <strong class="kd iu">提示</strong>:如果你能接受与 GCP 产品的紧密耦合，你可以构建得更快。一旦我接受使用启动脚本，一切都变得容易了。<br/> <strong class="kd iu">总体而言</strong>:如果我知道最终架构的复杂性(以及我自己对 DevOps 的无知)，我可能会感到气馁/害怕。归因于缺乏规划和不知道自己不知道什么风险。在我的许多错误中，从简单的架构构建一个应用程序，然后逐渐用更复杂的方式重构它，是我做对的事情。</p><h1 id="1278" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">码头工人！我的显卡呢？！和其他部署困难。</h1><p id="21cd" class="pw-post-body-paragraph kb kc it kd b ke mm kg kh ki mn kk kl km mr ko kp kq ms ks kt ku mt kw kx ky im bi translated">注:GCP 和多克都有图像的概念。为了避免混淆，我将声明 GCP 的总是作为引导映像。</p><p id="e89c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一般来说，使用 Docker 容器有助于简化部署、服务配置和代码可复制性(“iuno，worked on my machine problems”)。</p><p id="ac73" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在 ML 中使用 Docker 更难。问题:</p><ul class=""><li id="5ab2" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">图像会变得非常大。官方 TensorFlow Docker 图片大小轻松 500mb-1.5gb。</li><li id="9273" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">大多数 GCP 机器学习引导映像都没有 Docker/Compose。</li><li id="d53e" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">计数器:许多包含 Docker 的引导映像没有 CUDA。</li><li id="613c" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">如果你有勇气从零开始安装 TensorFlow 和 CUDA，我为你鼓掌。</li><li id="d6af" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">诀窍是找到一个足够好的启动映像，并安装两者中难度较低的(CUDA，Docker)。大多数时候，Docker + Docker Tools 比 CUDA 更容易安装。</li><li id="6fcd" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">许多模型经常是 1gb 以上，对于源代码控制来说太大了。需要在启动/部署时同步大型模型的脚本。</li><li id="94f3" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">很容易忘记将 nvidia 运行时传递给 Docker 命令。</li><li id="4755" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">DevOps 中的反馈循环比编程慢得多。你可以做出改变，意识到你有一个打字错误，然后再花 10 分钟来部署。如果使用谷歌滚动部署，可能需要更长时间。</li></ul><p id="cca0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> PRO </strong>:一旦容器安装好，惊人的坚固。<br/> <strong class="kd iu">弊</strong> : Docker 增加了部署的复杂性。合理反驳:如果做了这么多，为什么不加 Kubernetes？回答:我对 Kubernetes 不够聪明。<br/> <strong class="kd iu">提示</strong>:小心谨慎，把你运行的每一个 shell 命令都放在一个颤动日志中(或者某种类型的记录保存)。您可能会多次复制和粘贴您的命令。稍后您将自动完成其中的大部分工作。如果你“有点”记得命令顺序，自动化就更难了。<br/> <strong class="kd iu">提示</strong>:以绝对路径运行/保存命令，以避免覆盖错误的目录。即。“rsync /path1 /path2”而不是“rsync path1 path2”，哦 f f. <br/> <strong class="kd iu">提示</strong>:如果你知道 Ansible，使用 Ansible 重新运行 google 的启动脚本。比 GCP 的滚动部署要快得多。</p><pre class="mv mw mx my gt nz oa ob oc aw od bi"><span id="7d6c" class="nn lp it oa b gy oe of l og oh">- name: Deploy to Open<br/>  # startup scripts does most of the hard work, but make sure <br/>  # you're only deploying to things that finished from startup scripts<br/>  hosts: open_django:&amp;finished_startup<br/>  gather_facts: true<br/>  become: true<br/><br/>  post_tasks:<br/>    - name: Run Startup Script<br/>      shell: |<br/>        google_metadata_script_runner --script-type startup --debug<br/>      args:<br/>        chdir: /<br/>      become: yes</span></pre><p id="87fd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">提示:花额外的时间写提纲</strong></p><ol class=""><li id="18c4" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky oi lg lh li bi translated">模型应该存放在哪个桶上。建议将培训和生产的云桶分开。</li><li id="df68" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky oi lg lh li bi translated">实例上应该同步桶/目录的位置。</li><li id="c112" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky oi lg lh li bi translated">如果可能的话，让实例共享与 docker 容器的挂载目录完全相同的位置。即。实例的/models 挂载到 docker 容器的/models 路径</li><li id="564f" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky oi lg lh li bi translated">将正确的 rsync 命令写入桶中。使用 rsync！(不是 cp)。重启时比通过 cp 拉同样的文件更有效。</li></ol><p id="6cab" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">提示</strong>:对 py torch(torch . cuda . is _ available)或 tensor flow(TF . test . is _ GPU _ available)进行快速自动检查，可以省去确保 Docker 使用 nvidia 的麻烦。<br/> <strong class="kd iu">总的来说</strong>:这个领域可能是许多 web 工程师在部署预先训练好的 ML 应用程序时努力的地方。</p><h1 id="5e3f" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">寻找瓶颈。你说我内存不足是什么意思？</h1><ul class=""><li id="366f" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">监控传统的 web 服务器负载通常是简单明了的。所有 GCP 页面上列出的 CPU 使用率百分比。对于内存，top 命令可以快速告诉程序使用了多少内存。谷歌的 StackDriver 会自动将内存使用情况转发到谷歌云。</li><li id="e275" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">几十年来，DevOps 一直关注对 cpu、内存、磁盘使用、网络的监控。</li><li id="0268" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">然而，唯一关心 GPU 使用的人是超频游戏玩家(又名 crysis-99-fps-water cooled-noobmaster)。自从 AlexNet(社区学会了使用 GPU 进行 ML)以来，生产 GPU 监控工具还没有完全达到标准。</li><li id="f5c3" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">为了正确地监控 GPU 的使用，你必须使用 nvidia-smi，按设定的时间间隔输出结果，编写一个供 Prometheus 读取的脚本，然后将其传递给 StackDriver。总之你要写一个微服务来监控一个微服务。</li><li id="3f90" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">在使用过程中，CPU 和 GPU 的使用量都呈线性增长。作为一个黑客，我发现 vcpu 的最低数量可以达到 80–100 %,并根据 CPU 的使用情况自动扩展。太多的 vcpu 和 CPU 使用率%不会让步，而 GPU 受到打击。</li><li id="e6c4" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">当 GPU 耗尽内存时，可能会出现问题。当用户传递较长的提示(&gt; 200 字)时，就会发生这种情况。PyTorch 引发了一个异常，但不幸的是包含了大量的内存泄漏。为了处理这个问题，我捕获了 PyTorch 异常并强制释放未使用的内存。nvidia-smi 没有用，因为内存使用统计数据不是实时精确的(IIRC，它只显示一个进程的内存使用峰值)。</li></ul><h1 id="ea83" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">培训模型</h1><ul class=""><li id="0618" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">微调了 gp T2-中型 P100 的附加型号。训练迭代(周期)从《权力的游戏》(GoT)和《哈利波特》(HP)上的 60k 到 600k(学术研究，在 200k 论文摘要上训练)。</li><li id="a564" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">用了 TensorFlow 1.13 来训练。</li><li id="806d" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">训练时间从几个小时(60k)到几天(600k)不等。</li><li id="e95c" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">交叉熵损失在 2-3 之间。过度训练时，公制没有用。</li><li id="f01d" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">分叉 nsheppard 的 gpt2 repo，做了一些小的修改来加速更大数据集的启动。</li><li id="2ff7" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">一旦你理解了 ML 的行话(尽管这可能是最难的部分)，遵循 gwern 的教程是非常简单的。</li><li id="ed3d" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">使用梯度检查点来处理内存问题。在没有内存问题的情况下，不可能在单个 GPU 上微调 gpt2-large (774M 参数，1.5gb)。</li><li id="1b57" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">寻找和清理数据集从轻微的麻木痛苦到乏味的挫折。</li><li id="1159" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">同样，数据清理是 80%的工作。</li><li id="5176" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">从 Kaggle，Google 和 misc. free 数据集抓取数据集进行填充。在清理过程中，数据集异常、新行(\r，\n\，回车符)、unicode 检测和语言检测等问题是最耗时的。</li><li id="5b01" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">Gwern 使用了大量 bash /命令行来清理他的莎士比亚文集。我推荐用 Python。更容易在不同的数据集上重用代码。</li><li id="2e62" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">无法使 16 位训练(apex)在 Docker 中正常工作。Nvidia 性能指标评测(尽管是营销..)显示 16 位可以将训练周期缩短 2 倍(甚至更多)。没有太努力(累)去做 16 位的作品。</li><li id="3411" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">训练后，使用 huggingface 脚本将模型转换为 PyTorch。在 pytorch-transformers 上部署非常简单。</li><li id="8ac9" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">想避免在哈利波特语料库上过度训练，但事后看来，感觉过度训练比训练不足更好。在平衡小数据集的过度/不足训练风险时，您的结果可能会有所不同。</li></ul><p id="1655" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">提示</strong>:当你有了原始的训练数据集后，做一个拷贝。不要修改原始数据集。将修改后的输出复制到单独的文件夹中。将修改过的数据集和原始数据集保存在不同的文件夹中，以避免错误/混淆。<br/> <strong class="kd iu">提示</strong>:如果你发现自己曾经清理过一个特定的数据集，后退一步，寻找一个没有问题的类似数据集。这发生在哈利波特数据集上。<br/> <strong class="kd iu">提示</strong>:学习 tmux！使用 tmux 可以更容易地在远程机器上开始训练，并且您可以放心地退出。<br/> <strong class="kd iu">提示</strong>:用箭袋来装你所有的命令。非常容易出现错别字。</p><h1 id="9e89" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">运行模型</h1><ul class=""><li id="8be0" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">二手 PyTorch。pytorch-transformers 为模型创建了方便的 API 调用点。模仿 huggingface 中 run_gpt2.py 的例子。然后应用大规模重构。</li><li id="7c47" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">在 PyTorch 中加载 GPT-2 模型很慢(1-2 分钟)。</li><li id="2494" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">为了缩短加载时间，当微服务启动时，WSGI 加载适当的模型(gp T2——小型、中型、大型等),并将 PyTorch 实例存储为单例。</li><li id="ec77" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">所有后续请求都使用 singleton PyTorch 实例。</li><li id="3458" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">基于模型大小，对运行的 WSGI 进程数量的配置限制。WSGI 进程太多，CUDA 内存不足。太少，GPU 利用不足。</li><li id="6813" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">当 PyTorch 耗尽内存时捕获异常；释放内存泄漏。</li></ul><pre class="mv mw mx my gt nz oa ob oc aw od bi"><span id="6c3d" class="nn lp it oa b gy oe of l og oh">def get_process_prompt_response(request, validated_data):<br/>    try:<br/>        output = generate_sequences_from_prompt(**validated_data)<br/>    except RuntimeError as exc:<br/>        if "out of memory" in str(exc):<br/>            logger.exception(<br/>                f"Ran Out of Memory When Running {validated_data}. Clearing Cache."<br/>            )<br/>            torch.cuda.empty_cache()<br/><br/>            oom_response = get_oom_response(validated_data)<br/>            return oom_response<br/><br/>    response = serialize_sequences_to_response(<br/>        output,<br/>        validated_data["prompt"],<br/>        validated_data["cache_key"],<br/>        WebsocketMessageTypes.COMPLETED_RESPONSE,<br/>        completed=validated_data["length"],<br/>        length=validated_data["length"],<br/>    )<br/><br/>    # clear cache on all responses (maybe this is overkill)<br/>    torch.cuda.empty_cache()<br/>    return response</span></pre><ul class=""><li id="8aed" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">95%的请求时间用于预测逻辑。另一个是来自前端-&gt;后端-&gt;负载平衡器的路由和反序列化。</li><li id="9310" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">每隔五个单词，微服务就用更新的文本更新 WebSocket。</li><li id="6d45" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">向后端添加缓存以防止重复请求很有帮助。</li><li id="4095" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated"><strong class="kd iu">为了简化来自不同实例的相同响应，我对所有请求使用 42 的种子。</strong></li></ul><h1 id="01f4" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">其他部署改进、提炼和想法</h1><ul class=""><li id="7b39" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">TensorFlow 有<a class="ae kz" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank"> TensorFlow Serve </a>和 PyTorch 有<a class="ae kz" href="https://pytorch.org/docs/stable/jit.html" rel="noopener ugc nofollow" target="_blank"> TorchScript </a>将模型转化为生产级。好处包括合理的速度提升(redditor 引用了 30%的提升)和在没有 Python 的设备上更容易部署。我在一些模型上追踪了(PyTorch 的转换过程),但是我发现速度上的好处并不明显，但是增加了更多的复杂性。</li><li id="f820" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">在过去的几个月中，模型的提取(在&lt;50% of the size and runtime) has picked up traction. Huggingface’s distillation of gpt2-small is 33% smaller and 2x faster.</li><li id="16de" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">There’s a recently published paper about <a class="ae kz" href="https://arxiv.org/abs/1909.11687" rel="noopener ugc nofollow" target="_blank">Extreme Language Model Compression 中提取 90–95%以上的模型，将 BERT 压缩了 60 倍</a>)。如果能在 GPT2 上应用，将会有很多影响！</li><li id="d370" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">有点反模式，但是 PyTorch 和 TensorFlow 在同一个 Docker 图像上非常有用。我能够更快地诊断和尝试潜在的解决方案。</li><li id="d691" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">我本来集成了<a class="ae kz" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet </a>，但是没有 GPT2 那么强的发现生成输出。我还试图让它建议单个单词(类似于它的屏蔽语言模型)，但我找不到一个好的写作用例/ UI。</li></ul><h1 id="356f" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">其他宝贵的工具</h1><ul class=""><li id="ac20" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">上面的一些重复。</li><li id="75c7" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">哨兵对于错误报告来说是无价的。与 ASGI (Django-Channels)一起使用比正常情况下稍微困难一些。</li><li id="8566" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated"><a class="ae kz" href="https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/" rel="noopener ugc nofollow" target="_blank"> tmux </a> —使用它来保持远程会话打开。另一种选择是屏蔽。</li><li id="9f75" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">使用 django-rest-framework 是一种乐趣。感觉像是作弊代码。</li><li id="fa1b" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">Netlify 非常适合部署。</li></ul><h1 id="cea6" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">应对倦怠</h1><figure class="mv mw mx my gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oj"><img src="../Images/26833ad3277c908e708b56cf59c9da73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7jVMay1fNXagvC_Z"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">How I Treat My Mental State Until It’s Too Late …</figcaption></figure><ul class=""><li id="5b30" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">在接近终点线时撞上了一堵墙。</li><li id="2394" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">在大约 2-2.5 个月内开始燃烧。</li><li id="f992" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">精神上的痛苦，因为我觉得我应该发射，但还不够好。痴迷于缺失的功能。</li><li id="d2e6" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">打电话给一个亲密的朋友倾诉真的很有帮助(谢谢詹姆斯 C)。</li><li id="d842" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">自我施加的压力“发射！”让我避免给家人打电话。那是个错误。<strong class="kd iu">我发现给妈妈打电话只是想问问她的生活让我又松了一口气。</strong></li><li id="eb24" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">很自豪我完成了这个。了解了许多关于 ML 部署的意想不到的事情。对我的下一个项目有用。</li></ul><h1 id="1d75" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">非常感谢</h1><ul class=""><li id="ec62" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">GPT2 的 OpenAIpytorch 变形金刚的拥抱脸。</li><li id="8b2e" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">GCP 要求学分，否则负担不起。有偏见，但我发现 GCP 指标比 AWS 全面(桶、网络、易用性)更好。</li><li id="db00" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">我的朋友们帮助我进行了测试，并给了我宝贵的反馈。感谢(随机排序):克里斯汀李，，凯特阿克塞，Zoltan 萨拉斯，斯蒂芬林森和哈里尼巴布，他们都给了宝贵的反馈。</li><li id="5dfb" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">许多 Redditors / ProductHunt 真的在推动和玩产品，还有很好的反馈和令人捧腹的写作提示。</li></ul><h1 id="8f0e" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">下一步是什么？</h1><ul class=""><li id="2cd8" class="la lb it kd b ke mm ki mn km mo kq mp ku mq ky lf lg lh li bi translated">致力于蒸馏 GPT 二号。能搞清楚的我就开源。</li><li id="c132" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">使用 GANs 制作营销图像。敬请期待！</li><li id="4a91" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">还不至于愚蠢到再次做出不准确的时间表估计。</li></ul></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="671f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="nj">最初发布于</em><a class="ae kz" href="https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/" rel="noopener ugc nofollow" target="_blank"><em class="nj">https://senri gan . io</em></a><em class="nj">。</em></p></div></div>    
</body>
</html>