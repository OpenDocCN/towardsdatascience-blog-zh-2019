<html>
<head>
<title>Spark MLlib Python Example — Machine Learning At Scale</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark MLlib Python 示例-大规模机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-at-scale-with-apache-spark-mllib-python-example-b32a9c74c610?source=collection_archive---------5-----------------------#2019-06-30">https://towardsdatascience.com/machine-learning-at-scale-with-apache-spark-mllib-python-example-b32a9c74c610?source=collection_archive---------5-----------------------#2019-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/1a11775919370975e335ffc88aa466ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/0*uPOoAK6fpqDWKEnY.png"/></div></figure><div class=""/><p id="9e88" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在其历史的大部分时间里，计算机处理器每年都变得更快。不幸的是，这种硬件趋势在 2005 年左右停止了。由于散热方面的限制，硬件开发人员不再提高单个处理器的时钟频率，而是选择并行 CPU 内核。这对于在台式电脑上玩视频游戏来说很好。然而，当涉及到处理数 Pb 的数据时，我们必须更进一步，将多台计算机的处理能力集中在一起，以便在任何合理的时间内完成任务。对水平扩展的需求催生了 Apache Hadoop 项目。Apache Hadoop 提供了一种方法来分解给定的任务，在一个集群内的多个节点上并发执行该任务，并聚合结果。</p><p id="f489" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">Apache Spark 始于 2009 年的加州大学伯克利分校 AMPlab。当时，Hadoop MapReduce 是集群的主流并行编程引擎。AMPlab 创建了 Apache Spark 来解决使用 Apache Hadoop 的一些缺点。Apache Hadoop 最显著的局限性之一是它将中间结果写入磁盘。相比之下，Spark 将所有内容都保存在内存中，因此速度更快。2013 年，该项目得到了广泛应用，有来自加州大学伯克利分校以外 30 多个组织的 100 多名贡献者。AMPlab 为 Apache 软件基金会贡献了 Spark。早期的 AMPlab 团队还成立了一家名为 Databricks 的公司来改进这个项目。</p><p id="64fe" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">尽管像 scikit-learn 这样的 Python 库对于 Kaggle 竞赛之类的比赛来说很棒，但是它们很少被大规模使用。根据我个人的经验，我遇到过只能加载一部分数据的情况，因为否则它会完全填满我的计算机的内存并使程序崩溃。Spark 有能力通过一个名为 MLlib 的内置库来大规模执行机器学习。MLlib API 虽然不如 scikit-learn 包容，但可以用于分类、回归和聚类问题。在前面的文章中，我们将使用传统的 scikit-learn/pandas 堆栈训练一个机器学习模型，然后使用 Spark 重复这个过程。</p><h1 id="2657" class="kv kw ja bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">Jupyter 笔记本</h1><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="a66b" class="mc kw ja ly b gy md me l mf mg">import pandas as pd</span></pre><p id="5d31" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在前面的例子中，我们将尝试根据人口普查数据预测一个成年人的收入是否超过 5 万美元/年。数据可以从<a class="ae mh" href="http://archive.ics.uci.edu/ml/datasets/Adult" rel="noopener ugc nofollow" target="_blank"> UC Irvine 机器学习库</a>下载。</p><p id="4ac0" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们正在处理的数据集包含 14 个要素和 1 个标注。默认情况下，csv 文件中不包含标题，因此，我们必须自己定义列名。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="4859" class="mc kw ja ly b gy md me l mf mg">column_names = [<br/>    'age',<br/>    'workclass',<br/>    'fnlwgt',<br/>    'education',<br/>    'education-num',<br/>    'marital-status',<br/>    'occupation',<br/>    'relationship',<br/>    'race',<br/>    'sex',<br/>    'capital-gain',<br/>    'capital-loss',<br/>    'hours-per-week',<br/>    'native-country',<br/>    'salary'<br/>]</span><span id="ffa2" class="mc kw ja ly b gy mi me l mf mg">train_df = pd.read_csv('adult.data', names=column_names)</span><span id="71e8" class="mc kw ja ly b gy mi me l mf mg">test_df = pd.read_csv('adult.test', names=column_names)</span></pre><p id="00c4" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">您会注意到每个特性都由逗号和空格分隔。虽然熊猫可以在引擎盖下处理这些，但 Spark 不能。因此，我们删除了空格。此外，我们从我们的训练集中删除了任何一行本国为<code class="fe mj mk ml ly b">Holand-Neitherlands</code>的行，因为在我们的测试集中没有任何实例，这将在我们对分类变量进行编码时引起问题。我们将生成的数据帧保存到一个 csv 文件中，这样我们可以在以后使用它。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="c523" class="mc kw ja ly b gy md me l mf mg">train_df = train_df.apply(lambda x: x.str.strip() if x.dtype == 'object' else x)</span><span id="32b8" class="mc kw ja ly b gy mi me l mf mg">train_df_cp = train_df.copy()</span><span id="36f6" class="mc kw ja ly b gy mi me l mf mg">train_df_cp = train_df_cp.loc[train_df_cp['native-country'] != 'Holand-Netherlands']</span><span id="40f9" class="mc kw ja ly b gy mi me l mf mg">train_df_cp.to_csv('train.csv', index=False, header=False)</span><span id="76f1" class="mc kw ja ly b gy mi me l mf mg">test_df = test_df.apply(lambda x: x.str.strip() if x.dtype == 'object' else x)</span><span id="e3ce" class="mc kw ja ly b gy mi me l mf mg">test_df.to_csv('test.csv', index=False, header=False)</span></pre><p id="e3f3" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">接下来，让我们来看看我们在做什么。训练集包含 3 万多行。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="c8b1" class="mc kw ja ly b gy md me l mf mg">print('Training data shape: ', train_df.shape)<br/>train_df.head()</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/223aac32b60bed906c25b06ef5d0f627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3mNk3AX_4l6I93Ba307-Ew.png"/></div></div></figure><p id="4932" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">另一方面，测试集包含 15000 多行。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="9aab" class="mc kw ja ly b gy md me l mf mg">print('Testing data shape: ', test_df.shape)<br/>test_df.head()</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/7e0c376ea0c023306a3cc849780b54d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QCwxLiQxOi5ERGKMHd8Hiw.png"/></div></div></figure><p id="805a" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">很多时候，我们必须在训练模型之前处理缺失的数据。以下行将返回每个要素的缺失值的数量。幸运的是，数据集是完整的。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="ffac" class="mc kw ja ly b gy md me l mf mg">train_df.isnull().sum()</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/08a8190ee47978356f2728bf14c87124.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*pN3_5pSMtY6IA7icmzCqXQ.png"/></div></figure><p id="82fd" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">分类变量的类型为<code class="fe mj mk ml ly b">object</code>。分类变量必须进行编码，以便由机器学习模型(而不是决策树)进行解释。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="91f5" class="mc kw ja ly b gy md me l mf mg">train_df.dtypes.value_counts()</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/8c6a52598e04a064d0bfbe0e92149d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*U5hwGaJXUnQ70Xc1Nz7DUg.png"/></div></figure><p id="83b5" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">以下代码打印每个分类变量的不同类别数。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="0d2e" class="mc kw ja ly b gy md me l mf mg">train_df.select_dtypes('object').apply(pd.Series.nunique, axis=0)</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f22695d4a35b71af5a83b7c0084d4f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*4qrBcpSHLp6egL8eZ7cgBw.png"/></div></figure><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="52dc" class="mc kw ja ly b gy md me l mf mg">test_df.select_dtypes('object').apply(pd.Series.nunique, axis=0)</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/a279d43d30ecf20c67187ff1cb3632aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*X_ClFMqvb0Isa9Rqs9aIoQ.png"/></div></figure><p id="7e9b" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们手动编码 salary，以避免在执行一个热编码时创建两列。在转换我们的数据之后，每个字符串都被替换为一个由<em class="mv">1</em>和<em class="mv">0</em>组成的数组，其中<em class="mv"> 1 </em>的位置对应于一个给定的类别。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="e2f7" class="mc kw ja ly b gy md me l mf mg">train_df['salary'] = train_df['salary'].apply(lambda x: 0 if x == ' &lt;=50K' else 1)<br/>test_df['salary'] = test_df['salary'].apply(lambda x: 0 if x == ' &lt;=50K' else 1)</span><span id="1214" class="mc kw ja ly b gy mi me l mf mg">train_df = pd.get_dummies(train_df)<br/>test_df = pd.get_dummies(test_df)</span><span id="b593" class="mc kw ja ly b gy mi me l mf mg">print('Training Features shape: ', train_df.shape)<br/>print('Testing Features shape: ', test_df.shape)</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mw"><img src="../Images/05f03731eda409776f8a081fadcde6f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*80ojjO3n8JaZWErcXDmZOA.png"/></div></div></figure><p id="3f45" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">测试集和训练集中<code class="fe mj mk ml ly b">native-country</code>类别的不同数量之间存在差异(测试集没有一个人的祖国是荷兰)。结果，当我们应用一个热编码时，我们得到了不同数量的特性。在使用逻辑回归之前，我们必须确保训练集和测试集中的特征数量相匹配。我们可以通过执行内部连接来做到这一点。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="ae10" class="mc kw ja ly b gy md me l mf mg"># Align the training and testing data, keep only columns present in both dataframes<br/>train_df, test_df = train_df.align(test_df, join = 'inner', axis = 1)</span><span id="d147" class="mc kw ja ly b gy mi me l mf mg">print('Training Features shape: ', train_df.shape)<br/>print('Testing Features shape: ', test_df.shape)</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mx"><img src="../Images/06b0e7de088e388ef3770e9e887b06b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*Jbl7uU2CdAswLHxdvi2Urg.png"/></div></div></figure><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="e74c" class="mc kw ja ly b gy md me l mf mg">train_df.head()</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi my"><img src="../Images/5e10a2c8cbf5424a74770c6eae9bc10e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ptwFWYoNgMf2g-CX-l0W3Q.png"/></div></div></figure><p id="819f" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">接下来，我们将数据帧分解成因变量和自变量。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="cae1" class="mc kw ja ly b gy md me l mf mg">X_train = train_df.drop('salary', axis=1)<br/>y_train = train_df['salary']</span><span id="e721" class="mc kw ja ly b gy mi me l mf mg">X_test = test_df.drop('salary', axis=1)<br/>y_test = test_df['salary']</span></pre><p id="d623" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">只要我们在解释系数时牢记单位，我们就不需要为正态逻辑回归调整变量。然而，默认情况下，逻辑回归的 scikit-learn 实现使用 L2 正则化。L2 正则化同等地惩罚所有参数的大值。因此，以米为单位的高度特征将比以毫米为单位的另一个特征受到更多的惩罚。因此，在通过我们的模型发送数据之前，我们对数据进行缩放。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="b783" class="mc kw ja ly b gy md me l mf mg">from sklearn.preprocessing import MinMaxScaler</span><span id="4e9a" class="mc kw ja ly b gy mi me l mf mg">scaler = MinMaxScaler(feature_range = (0, 1))</span><span id="4fb2" class="mc kw ja ly b gy mi me l mf mg">scaler.fit(X_train)<br/>X_train = scaler.transform(X_train)<br/>X_test = scaler.transform(X_test)</span></pre><p id="e47c" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">最后，我们可以训练我们的模型，并在测试集上测量它的性能。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="0e1e" class="mc kw ja ly b gy md me l mf mg">from sklearn.linear_model import LogisticRegression</span><span id="4055" class="mc kw ja ly b gy mi me l mf mg">lr = LogisticRegression()</span><span id="baf8" class="mc kw ja ly b gy mi me l mf mg">lr.fit(X_train, y_train)</span><span id="8e16" class="mc kw ja ly b gy mi me l mf mg">lr_pred = lr.predict(X_test)</span><span id="7463" class="mc kw ja ly b gy mi me l mf mg">from sklearn.metrics import accuracy_score</span><span id="cb15" class="mc kw ja ly b gy mi me l mf mg">accuracy_score(y_test, lr_pred)</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/24de97fb6dcceca471b95c0500831653.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*OBqwWj69vLJVAOiNgxGzsg.png"/></div></figure><h1 id="456d" class="kv kw ja bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">Pyspark</h1><p id="bcd0" class="pw-post-body-paragraph jx jy ja jz b ka na kc kd ke nb kg kh ki nc kk kl km nd ko kp kq ne ks kt ku im bi translated">让我们看看如何使用 Spark 来完成同样的事情。根据您的喜好，您可以用 Java、Scala 或 Python 编写 Spark 代码。鉴于大多数数据科学家习惯于使用 Python，我们将使用它。</p><p id="fa05" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">前面部分中的所有代码都将在我们的本地机器上运行。然而，如果我们要建立一个具有多个节点的 Spark 集群，这些操作将在集群中的每台计算机上并发运行，而无需对代码进行任何修改。</p><p id="bcf2" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">开始使用 Spark 最简单的方法是使用 Jupyter 提供的 Docker 容器。为了简单起见，我们创建一个包含以下内容的<code class="fe mj mk ml ly b">docker-compose.yml</code>文件。确保修改路径以匹配包含从 UCI 机器学习资源库下载的数据的目录。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="fad5" class="mc kw ja ly b gy md me l mf mg">version: '2'<br/>services:<br/>  spark:<br/>    image: jupyter/pyspark-notebook:latest<br/>    ports:<br/>      - 8888:8888<br/>    volumes:<br/>      - /home/cory/kaggle/adult:/home/jovyan/work</span></pre><p id="24c4" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然后，运行前面的命令。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="86ec" class="mc kw ja ly b gy md me l mf mg">docker-compose up</span></pre><p id="6fad" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">要访问 Jupyter 笔记本，请打开浏览器并进入<code class="fe mj mk ml ly b">localhost:8888</code>。</p><p id="4d80" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">继续导入下列库。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="fed5" class="mc kw ja ly b gy md me l mf mg">from pyspark import SparkConf, SparkContext<br/>from pyspark.sql import SparkSession<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler<br/>from pyspark.ml import Pipeline<br/>from pyspark.sql.types import StructType, StructField, IntegerType, StringType</span></pre><p id="d872" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在做任何事情之前，我们需要初始化一个 Spark 会话。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="24f6" class="mc kw ja ly b gy md me l mf mg">spark = SparkSession.builder.appName("Predict Adult Salary").getOrCreate()</span></pre><p id="272a" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">就像之前一样，我们定义了在读取数据时要使用的列名。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="34a6" class="mc kw ja ly b gy md me l mf mg">schema = StructType([<br/>    StructField("age", IntegerType(), True),<br/>    StructField("workclass", StringType(), True),<br/>    StructField("fnlwgt", IntegerType(), True),<br/>    StructField("education", StringType(), True),<br/>    StructField("education-num", IntegerType(), True),<br/>    StructField("marital-status", StringType(), True),<br/>    StructField("occupation", StringType(), True),<br/>    StructField("relationship", StringType(), True),<br/>    StructField("race", StringType(), True),<br/>    StructField("sex", StringType(), True),<br/>    StructField("capital-gain", IntegerType(), True),<br/>    StructField("capital-loss", IntegerType(), True),<br/>    StructField("hours-per-week", IntegerType(), True),<br/>    StructField("native-country", StringType(), True),<br/>    StructField("salary", StringType(), True)<br/>])</span></pre><p id="a851" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">像 Pandas 一样，Spark 提供了一个 API，用于将 csv 文件的内容加载到我们的程序中。我们使用开始时创建的文件。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="14e3" class="mc kw ja ly b gy md me l mf mg">train_df = spark.read.csv('train.csv', header=False, schema=schema)</span><span id="4a31" class="mc kw ja ly b gy mi me l mf mg">test_df = spark.read.csv('test.csv', header=False, schema=schema)</span></pre><p id="61c7" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们可以运行下面一行来查看前 5 行。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="784e" class="mc kw ja ly b gy md me l mf mg">train_df.head(5)</span></pre><p id="4a88" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">不管出于什么原因，如果你想把 Spark 数据帧转换成 Pandas 数据帧，你可以这样做。就我个人而言，我觉得输出更清晰，更容易阅读。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="99f8" class="mc kw ja ly b gy md me l mf mg">train_df.limit(5).toPandas()</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nf"><img src="../Images/429c1d0ab0aa74a3960783808ec6ea59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NUaAfFabjJQK2Jvj6rcnhA.png"/></div></div></figure><p id="58ee" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在继续前进之前，必须了解 Spark 和 Scikit-learn/Pandas 之间的几个重要区别。</p><ul class=""><li id="734c" class="ng nh ja jz b ka kb ke kf ki ni km nj kq nk ku nl nm nn no bi translated">Spark 数据帧是不可变的。因此，每当我们想要应用转换时，我们必须通过创建新的列来实现。</li><li id="93a8" class="ng nh ja jz b ka np ke nq ki nr km ns kq nt ku nl nm nn no bi translated">MLlib 希望所有功能都包含在一列中。</li></ul><p id="0b8c" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在前面的代码块中，我们对分类变量进行了所有必要的转换。</p><p id="813b" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><code class="fe mj mk ml ly b">StringIndexer</code>类执行标签编码，必须在<code class="fe mj mk ml ly b">OneHotEncoderEstimator</code>执行一次热编码之前应用。<code class="fe mj mk ml ly b">VectorAssembler</code>类接受多个列作为输入，输出一个列，其内容是一个数组，包含所有输入列的值。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="9649" class="mc kw ja ly b gy md me l mf mg">categorical_variables = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']</span><span id="cd28" class="mc kw ja ly b gy mi me l mf mg">indexers = [StringIndexer(inputCol=column, outputCol=column+"-index") for column in categorical_variables]</span><span id="c9c1" class="mc kw ja ly b gy mi me l mf mg">encoder = OneHotEncoderEstimator(<br/>    inputCols=[indexer.getOutputCol() for indexer in indexers],<br/>    outputCols=["{0}-encoded".format(indexer.getOutputCol()) for indexer in indexers]<br/>)</span><span id="917d" class="mc kw ja ly b gy mi me l mf mg">assembler = VectorAssembler(<br/>    inputCols=encoder.getOutputCols(),<br/>    outputCol="categorical-features"<br/>)</span><span id="0b6f" class="mc kw ja ly b gy mi me l mf mg">pipeline = Pipeline(stages=indexers + [encoder, assembler])</span><span id="abdc" class="mc kw ja ly b gy mi me l mf mg">train_df = pipeline.fit(train_df).transform(train_df)</span><span id="4fc2" class="mc kw ja ly b gy mi me l mf mg">test_df = pipeline.fit(test_df).transform(test_df)</span></pre><p id="0ca3" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">让我们查看在上一步中创建的所有不同的列。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="fa6b" class="mc kw ja ly b gy md me l mf mg">train_df.printSchema()</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/3be1235d2fe80947e27ad66f6f4ebaca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*iIngkKaY1Ms1X_np610Qyw.png"/></div></figure><p id="a457" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在应用转换之后，我们得到了一个包含每个编码分类变量的数组的单个列。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="4558" class="mc kw ja ly b gy md me l mf mg">df = train_df.limit(5).toPandas()<br/>df['scaled-categorical-features'][1]</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nv"><img src="../Images/ef78e1a46b16fcd6eaedeff55495dedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFxxO3Ai7MDYDnFMRqD88w.png"/></div></div></figure><p id="15ca" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们将连续变量和分类变量合并成一列。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="910d" class="mc kw ja ly b gy md me l mf mg">continuous_variables = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']</span><span id="f6ba" class="mc kw ja ly b gy mi me l mf mg">assembler = VectorAssembler(<br/>    inputCols=['categorical-features', *continuous_variables],<br/>    outputCol='features'<br/>)</span><span id="941b" class="mc kw ja ly b gy mi me l mf mg">train_df = assembler.transform(train_df)</span><span id="7e11" class="mc kw ja ly b gy mi me l mf mg">test_df = assembler.transform(test_df)</span></pre><p id="a143" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">让我们看看最后一列，我们将使用它来训练我们的模型。如您所见，它输出了一个 SparseVector。为了节省空间，稀疏向量不包含来自一个热编码的<strong class="jz jb"><em class="mv">0</em></strong>。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="c982" class="mc kw ja ly b gy md me l mf mg">train_df.limit(5).toPandas()['features'][0]</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nw"><img src="../Images/8c55d2bb0b608a08612805047a6735da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Lm3MGscaUMc-swjaDRGoQ.png"/></div></div></figure><p id="cdda" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">最后，我们对目标标签进行编码。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="7b35" class="mc kw ja ly b gy md me l mf mg">indexer = StringIndexer(inputCol='salary', outputCol='label')</span><span id="8b7a" class="mc kw ja ly b gy mi me l mf mg">train_df = indexer.fit(train_df).transform(train_df)</span><span id="d8e4" class="mc kw ja ly b gy mi me l mf mg">test_df = indexer.fit(test_df).transform(test_df)</span><span id="32e5" class="mc kw ja ly b gy mi me l mf mg">train_df.limit(10).toPandas()['label']</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/5b2868a5c8a08af902538806533af6b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*ZietovOAVgRZie1Q3uJcLw.png"/></div></figure><p id="91a7" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们适合并训练我们的模型。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="46d8" class="mc kw ja ly b gy md me l mf mg">lr = LogisticRegression(featuresCol='features', labelCol='label')</span><span id="3ebf" class="mc kw ja ly b gy mi me l mf mg">model = lr.fit(train_df)</span></pre><p id="e263" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><code class="fe mj mk ml ly b">transform</code>方法用于对测试集进行预测。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="56ff" class="mc kw ja ly b gy md me l mf mg">pred = model.transform(test_df)</span><span id="9fa0" class="mc kw ja ly b gy mi me l mf mg">pred.limit(10).toPandas()[['label', 'prediction']]</span></pre><figure class="lt lu lv lw gt iv gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/9a92fc0ee8c9f3490f198bfbb2a2b203.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*lIwqor7wtekcHDP_i6eixA.png"/></div></figure><h1 id="462d" class="kv kw ja bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">最后的想法</h1><p id="9378" class="pw-post-body-paragraph jx jy ja jz b ka na kc kd ke nb kg kh ki nc kk kl km nd ko kp kq ne ks kt ku im bi translated">Spark 是一个分布式计算平台，可用于在数据帧上执行操作，并大规模训练机器学习模型。</p></div></div>    
</body>
</html>