# 教育技术和算法透明性

> 原文：<https://towardsdatascience.com/edtech-algorithmic-transparency-76b7985fa018?source=collection_archive---------16----------------------->

## 对卡米拉·坎贝尔 SAT 成绩的调查提醒人们透明和可解释的人工智能的重要性

![](img/6b3446da70ec25ae718ceb9c3046c078.png)

Photo by [NeONBRAND](https://unsplash.com/@neonbrand?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

围绕佛罗里达州高中生卡米拉·坎贝尔的 SAT 分数调查的最近新闻为日益增长的教育技术领域中围绕算法透明度的问题提供了一个有趣的视角，该分数被大学委员会标记为可能作弊。虽然很可能只有一部分用于标记考试的过程是自动化的，但双方的回应凸显了在越来越被不透明算法主导的教育领域将变得司空见惯的问题。

# 背景

据 CNN 和其他媒体[报道，Campbell 的第二次 sat 考试取得了 330 分的进步，但在发现她的答案与其他学生的答案非常相似后，被大学委员会扣留并进行调查。由民权律师本·克伦普代理，坎贝尔请求及时公布她的 SAT 成绩，以便她被大学录取并申请奖学金。克伦普说:“【1230 分(分数)对她是否能进入梦想中的大学以及她是否负担得起有很大影响。](https://www.cnn.com/2019/01/02/us/florida-girl-sat-controversy/index.html)

大学委员会发布了一份声明，澄清了他们的审查过程，该过程着眼于各种因素，以确定学生的分数是否应该因涉嫌作弊而被保留或取消。作弊的证据可能包括一个学生的答案与一组其他学生的答案之间的高度相似性，存在(在该组中)以前被取消分数的学生，答案与被没收的“小抄”的相似性，以及学生的测试手册中没有草稿。大学委员会发给坎贝尔的这封信引用了“*(她)在考试的一个或多个得分部分的答案与其他考生的答案基本一致*”作为将考试标记为审查的原因。虽然在原始文章或评论声明中没有说明，但这个初始标志可能是该过程中唯一自动化的部分。

# 人工智能与教育技术的相关性

不管审查过程的结果如何，也不管坎贝尔对公布分数的要求如何，围绕这个故事的言论凸显了对教育科技行业来说很重要的几个问题，即[在人工智能和机器学习方面投入巨资](https://marketbrief.edweek.org/marketplace-k-12/artificial-intelligence-attracting-investors-inventors-academic-researchers-worldwide/?cmp=eml-enl-ii-news1&M=58716719&U=3143205&UUID=27ed230e65a2e0a769047ba5e8f3148e)作为其未来。

## 透明度

克伦普向美国有线电视新闻网明确表示，过程的透明度是一个关键问题，他说*“她现在被指控作弊。为什么呢？他们说，‘哦，你只需要相信我们的话，我们发现有些事情是错的，’”*，T4 补充道，“他们需要告诉我们(他们看到了什么)。”

这提醒我们，当大部分技能或能力评估完全自动化时，透明度是多么重要。大学委员会最初的信，以及他们后来发布的声明，试图通过概述他们的过程来解决这个问题，但它的不完整性留下了许多问题。—标记过程是自动化的吗？答案需要有多相似才能被标记出来？这个截止日期是如何选择的？高于此临界值的相似性的假阳性率是多少，它是如何确定的？—这些问题的答案是存在的，但是它们是否已经有效地传达给了相关的利益相关者？

欧洲委员会人工智能高级专家组(AI HLEG)已经为他们所谓的“值得信赖的人工智能”起草了道德准则，并且他们提供了关于透明度的特别建议。他们建议供应商“*以清晰和主动的方式向利益相关者提供关于人工智能系统的能力和局限性的信息……*”，进一步建议他们必须“*努力促进人工智能系统的可审计性，特别是在关键的环境或情况下。尽可能地设计你的系统，使之能够追踪个人决策到你的各种输入；数据、预先训练的模型等。此外，定义人工智能系统的解释方法。”*

一个担忧可能是:大学董事会能提供多少信息而不冒人们试图“玩弄系统”的风险？用 AI HLEG 的话说，“*要注意不同目标之间可能存在根本性的矛盾(透明度可能会为滥用敞开大门；识别和纠正偏见可能与隐私保护形成对比)。沟通并记录这些权衡。”*

Crump 选择关注透明度的事实很能说明问题，EdTech 的教训是，在部署评估算法之前，必须充分考虑这些问题。

## 有责任

Crump 公布分数的另一个重点是相互问责的问题。他认为“他们希望这些学生对他们负责，但是这个系统不对任何人负责。…好吧，这一次，他们也必须对此负责。”

这一信息应该会引起那些开发和部署算法解决方案来评估、评分、测量和预测人类技能和行为的人的强烈共鸣。机器所做决策的后果以潜在的重要方式影响着现实中的人，当出现问题时谁该负责的问题变得越来越恰当。 [Forrester Consulting 的一项调查(受毕马威国际公司委托)](https://assets.kpmg/content/dam/kpmg/xx/pdf/2018/02/guardians-of-trust.pdf)发现，62%的受访者将归咎于开发导致自动驾驶汽车事故的故障软件的公司，54%的人还将归咎于安装该软件的制造商，以及本可以手动控制的司机(这是一个“选择所有适用的”问题)。在 EdTech 中的类比是，创建人工智能软件的供应商在出现错误的情况下陷入困境，紧随其后的是购买该软件的学区或机构以及选择使用它来代替直接人工评估的教育工作者。

这并不意味着大学委员会做错了什么，或者人工智能没有令人难以置信的潜力来改革和民主化教育，但 Kamilah Campbell 案件提出的问题确实给了 EdTech 创新者停下来仔细考虑开发过程中每一步的透明度和问责制问题的理由。