<html>
<head>
<title>Deep Learning Explainability: Hints from Physics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的可解释性:来自物理学的提示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-explainability-hints-from-physics-2f316dc07727?source=collection_archive---------7-----------------------#2019-03-25">https://towardsdatascience.com/deep-learning-explainability-hints-from-physics-2f316dc07727?source=collection_archive---------7-----------------------#2019-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b7ab5cb2e48b793c805f597331d70926.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*suW_xGRohkXrZ5EiWehSuA.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Image by <a class="ae jg" href="https://pixabay.com/fr/users/realworkhard-23566/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=199054" rel="noopener ugc nofollow" target="_blank">Ralf Kunze</a> from <a class="ae jg" href="https://pixabay.com/fr/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=199054" rel="noopener ugc nofollow" target="_blank">Pixabay</a>.</figcaption></figure><h2 id="88f3" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内部 AI </a></h2><div class=""/><div class=""><h2 id="1a2a" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">从物理学角度看深层神经网络</h2></div><p id="667e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如今，人工智能几乎存在于我们生活的每一个部分。智能手机、社交媒体、推荐引擎、在线广告网络和导航工具是基于人工智能的应用的一些<a class="ae jg" href="https://beebom.com/examples-of-artificial-intelligence/" rel="noopener ugc nofollow" target="_blank">例子</a>，它们已经影响到我们的日常生活。在诸如<a class="ae jg" href="https://en.wikipedia.org/wiki/Speech_recognition" rel="noopener ugc nofollow" target="_blank">语音识别</a>、<a class="ae jg" href="https://en.wikipedia.org/wiki/Self-driving_car" rel="noopener ugc nofollow" target="_blank">自动驾驶</a>、<a class="ae jg" href="https://en.wikipedia.org/wiki/Machine_translation" rel="noopener ugc nofollow" target="_blank">机器翻译</a>、<a class="ae jg" href="https://en.wikipedia.org/wiki/Object_detection" rel="noopener ugc nofollow" target="_blank">视觉对象识别</a>等领域的深度学习已经系统地提高了艺术水平有一段时间了。</p><p id="96e7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，使深度神经网络(DNN)如此强大的原因仅仅是<a class="ae jg" href="https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/" rel="noopener ugc nofollow" target="_blank">启发式地</a> <a class="ae jg" href="https://arxiv.org/pdf/1608.08225.pdf\" rel="noopener ugc nofollow" target="_blank">理解</a>，即我们仅从经验中知道，我们可以通过使用大型数据集并遵循特定的训练协议来实现出色的结果。最近，有人提出了一种可能的解释，这种解释基于一种基于物理学的概念框架，称为<a class="ae jg" href="https://en.wikipedia.org/wiki/Renormalization" rel="noopener ugc nofollow" target="_blank">重整化群</a> (RG)和一种被称为<a class="ae jg" href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" rel="noopener ugc nofollow" target="_blank">受限玻尔兹曼机器</a> (RBM)的神经网络之间的显著相似性。</p><h2 id="90f7" class="md me jj bd mf mg mh dn mi mj mk dp ml lq mm mn mo lu mp mq mr ly ms mt mu jp bi translated">RG 和 RBMs 作为粗粒化过程</h2><p id="8064" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">重正化是一种用于研究物理系统在微观部分信息不可用时的行为的技术。这是一种“粗粒化”的方法，它展示了当我们缩小并检查不同长度尺度的物体时，物理规律是如何变化的，“戴上模糊的眼镜”。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/d155df3b7d1b798dba3c2b16c8eb4c40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*Ui-BOEvRIPHUjTiiFFECJA@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">When we change the length scale with which we observe a physical system (when we “zoom in”), our theories “<a class="ae jg" href="https://books.google.com.br/books?id=oElbxFaL2dIC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false" rel="noopener ugc nofollow" target="_blank">navigate the space</a>” of all possible theories (<a class="ae jg" href="https://books.google.com.br/books?id=oElbxFaL2dIC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><p id="9606" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">RG 理论的<strong class="lj jt">巨大重要性</strong>来自于它提供了一个强有力的框架，从本质上<strong class="lj jt">解释了为什么物理学本身是可能的</strong>。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/17cacffac3725d471e756a676e71ef93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfXmnHsOzRjzUBakHwp3Xg.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">To describe the motion of complex structures such as satellites, one does not need to take into account the motions of all its constituents. Picture by <a class="ae jg" href="https://www.shutterstock.com/g/3Dsculptor" rel="noopener ugc nofollow" target="_blank">3Dsculptor</a>/<a class="ae jg" href="https://www.shutterstock.com" rel="noopener ugc nofollow" target="_blank">Shutterstock.com</a>.</figcaption></figure><blockquote class="ng"><p id="5425" class="nh ni jj bd nj nk nl nm nn no np mc dk translated">RG 理论提供了一个强有力的框架来解释为什么物理学本身是可能的。</p></blockquote><p id="289e" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">例如，<a class="ae jg" href="https://books.google.com.br/books?id=oElbxFaL2dIC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false" rel="noopener ugc nofollow" target="_blank">要计算绕地球运行的卫星的轨道</a>，我们只需要运用牛顿运动定律。我们不需要考虑卫星微观成分极其复杂的行为来解释它的运动。我们在实践中所做的是对系统基本组件(在这种情况下是卫星)的详细行为进行某种“平均”。RG 理论解释了为什么这个过程如此有效。</p><p id="2130" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，RG 理论似乎表明，我们目前关于物理世界的所有理论都只是一些未知的“<a class="ae jg" href="https://books.google.com.br/books?id=oElbxFaL2dIC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false" rel="noopener ugc nofollow" target="_blank">真实理论</a>的近似(用更专业的术语来说，这种真实理论“生活”在物理学家所谓的标度变换的<a class="ae jg" href="http://www.nyu.edu/classes/tuckerman/stat.mech/lectures/lecture_28/node1.html" rel="noopener ugc nofollow" target="_blank">固定点</a>附近)。</p><blockquote class="ng"><p id="1c8a" class="nh ni jj bd nj nk nv nw nx ny nz mc dk translated">RG 理论似乎表明，我们目前关于物理世界的所有理论都只是一些未知的“真实理论”的近似。</p></blockquote><p id="ef08" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">当被研究的系统处于<a class="ae jg" href="https://en.wikipedia.org/wiki/Critical_point_(thermodynamics)" rel="noopener ugc nofollow" target="_blank">临界点</a>时，RG 工作良好，并表现出自相似性。一个自相似系统在它被观察的任何长度尺度下都是"<a class="ae jg" href="https://en.wikipedia.org/wiki/Self-similarity" rel="noopener ugc nofollow" target="_blank">完全或近似地类似于它自身的一部分</a>。显示自相似性的系统的例子有<a class="ae jg" href="https://en.wikipedia.org/wiki/Fractal" rel="noopener ugc nofollow" target="_blank">分形</a>。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/11182605837f255f93f853fedfb35f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*ctnk1jvanBWOvdCFLppo-A.gif"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Wikipedia animation showing the Mandelbrot set and we zoom in (<a class="ae jg" href="https://en.wikipedia.org/wiki/Mandelbrot_set" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><p id="afc9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">处于临界点的系统在相距甚远的部分之间表现出很强的相关性。所有的子部分影响整个系统，系统的物理性质变得完全独立于其微观结构。</p><p id="34a1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">人工神经网络也可以被视为一个粗粒度的迭代过程。人工神经网络由几个层组成，如下图所示，早期的层仅从输入数据中学习较低级别的特征(如边缘和颜色)，而较深的层将这些较低级别的特征(由早期的特征提供)组合成较高级别的特征。用<a class="ae jg" href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" rel="noopener ugc nofollow" target="_blank"> Geoffrey Hinton </a>的<a class="ae jg" href="https://www.quantamagazine.org/deep-learning-relies-on-renormalization-physicists-find-20141204/" rel="noopener ugc nofollow" target="_blank">话来说，</a><a class="ae jg" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>社区的领军人物之一:“你首先学习简单的特性，然后在此基础上学习更复杂的特性，这是分阶段进行的。”此外，与 RG 工艺的情况一样，更深的层<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank">只保留被认为相关的特征，不强调不相关的特征</a>。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/d994d8168d39767e6b37ae63dc491c8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ezjd9ZCAzmI-gatT4TXLXg@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Convolutional neural network (CNN). The complexity level of the forms recognized by the CNN is higher in later layers (<a class="ae jg" href="http://web.eecs.umich.edu/~honglak/cacm2011-researchHighlights-convDBN.pdf" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><h1 id="4450" class="oc me jj bd mf od oe of mi og oh oi ml ky oj kz mo lb ok lc mr le ol lf mu om bi translated">确切的联系</h1><p id="e699" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">物理学和机器学习都处理具有许多成分的系统。物理学研究包含许多(相互作用的)物体的系统。机器学习研究包含大量维度的复杂数据。此外，类似于物理学中的 RG，神经网络设法对数据进行分类，例如动物的图片，而不管它们的组成部分(例如大小和颜色)。</p><p id="5ccf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在 2014 年发表的一篇<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank">文章中，</a><a class="ae jg" href="http://physics.bu.edu/~pankajm/" rel="noopener ugc nofollow" target="_blank"> Pankaj Mehta </a>和<a class="ae jg" href="http://www.physics.northwestern.edu/people/personalpages/DavidSchwab.html" rel="noopener ugc nofollow" target="_blank"> David Schwab </a>两位物理学家基于重整化群理论对深度学习的性能进行了解释。他们表明 dnn 是如此强大的特征提取器，因为它们可以有效地“模仿”粗粒化过程，这是 RG 过程的特征。<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank">用他们的话说</a>“DNN 架构[……]可以被视为一种迭代的粗粒度方案，其中神经网络的每个新的高级层从数据中学习越来越抽象的高级特征”。事实上，在他们的论文中，他们设法证明了在 RG 和<a class="ae jg" href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" rel="noopener ugc nofollow" target="_blank">受限玻尔兹曼机器</a> (RBM)之间确实有一个<strong class="lj jt">精确的映射</strong>，这是一个构成 DNN 积木的双层神经网络。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/46960bdbdac57586838462b5c7eda498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J9zshq30uaZeeCUp-ZL_7g@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">From the <a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank">2014 paper by Mehta and Schwab</a> where they introduced the map between RG and DNNs built by stacking RBMs. More details are provided in the remaining sections of the present article (<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><p id="f4f5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">文献中还有许多其他工作将重正化和深度学习联系在一起，遵循不同的策略，具有不同的目标。尤其是基于<a class="ae jg" href="https://en.wikipedia.org/wiki/Information_bottleneck_method" rel="noopener ugc nofollow" target="_blank">信息瓶颈方法</a>的<a class="ae jg" href="http://naftali-tishby.strikingly.com/" rel="noopener ugc nofollow" target="_blank"> Naftali Tishby </a>及其合作者的<a class="ae jg" href="https://arxiv.org/abs/1503.02406" rel="noopener ugc nofollow" target="_blank">工作</a>引人入胜。另外，<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank"> Mehta 和 Schwab </a>只解释了一种神经网络的图谱，后续的工作已经<a class="ae jg" href="https://arxiv.org/abs/1605.05775" rel="noopener ugc nofollow" target="_blank">存在</a>。然而，为了简洁起见，我在这里将重点放在他们的<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank">原始论文</a>上，因为他们的见解导致了该主题的大量相关后续工作。</p><p id="cb25" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在给出这种关系的一个相对详细的描述之前(参见<a class="ae jg" href="https://www.quantamagazine.org/deep-learning-relies-on-renormalization-physicists-find-20141204/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>中一个很好的，虽然不太专业的描述),我将提供一些 RG 理论和 RBM 的一些事实。</p><h1 id="64e9" class="oc me jj bd mf od oe of mi og oh oi ml ky oj kz mo lb ok lc mr le ol lf mu om bi translated">重整化群理论:鸟瞰</h1><p id="1407" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">如上所述，重正化包括将粗粒化技术应用于物理系统。RG 理论是一个通用的概念框架，因此人们需要方法来操作这些概念。变分重整化群(VRG)就是这样一种方案，由<a class="ae jg" href="https://link.springer.com/article/10.1007/BF01011765" rel="noopener ugc nofollow" target="_blank"> Kadanoff，Houghton 和 Yalabik </a>于 1976 年提出。</p><p id="dc37" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了阐述清楚，我选择集中在一个特定类型的系统来说明 RG 是如何工作的，即<a class="ae jg" href="https://www.springer.com/br/book/9783642132896" rel="noopener ugc nofollow" target="_blank">量子自旋系统</a>，而不是进行全面的概括。但在钻研数学机械之前，我要给出一个“<a class="ae jg" href="https://en.wikipedia.org/wiki/Hand-waving" rel="noopener ugc nofollow" target="_blank">手挥动</a>”的解释<a class="ae jg" href="https://en.wikipedia.org/wiki/Spin_(physics)" rel="noopener ugc nofollow" target="_blank">自旋</a>在物理学中的意义。</p><h2 id="cf98" class="md me jj bd mf mg mh dn mi mj mk dp ml lq mm mn mo lu mp mq mr ly ms mt mu jp bi translated">物理学中的自旋概念</h2><p id="30db" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">在物理学中，自旋可以被<a class="ae jg" href="https://en.wikipedia.org/wiki/Spin_(physics)" rel="noopener ugc nofollow" target="_blank">定义为</a>“基本粒子、复合粒子和原子核所携带的角动量的一种内在形式。”尽管根据定义，自旋是一个没有经典对应物的<strong class="lj jt">量子力学概念</strong>，但具有自旋的粒子通常(尽管不正确)被描述为围绕自身轴旋转的小陀螺。自旋与<a class="ae jg" href="https://en.wikipedia.org/wiki/Magnetism" rel="noopener ugc nofollow" target="_blank">磁</a>现象密切相关。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/ce09b02b3ff7542e40f0b5e00d958e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*zL3Z4Z_p6SLtVs63vlJp6A@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">The particle spin (black arrow) and its associated magnetic field lines (<a class="ae jg" href="https://en.wikipedia.org/wiki/Spin_(physics)" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><h2 id="ae73" class="md me jj bd mf mg mh dn mi mj mk dp ml lq mm mn mo lu mp mq mr ly ms mt mu jp bi translated">重整化的数学</h2><p id="170a" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">让我们考虑一个由 N 个自旋组成的系统或系综。出于可视化的目的，假设它们可以放在一个网格上，如下图所示。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/5dd1ce9bedd2b38bbbe72392dddfaee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*5qSSrNov9u5_4dJ4vtS2WQ@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">A 2-dimensional lattice of spins (represented by the little arrows). The spheres are charged atoms (<a class="ae jg" href="https://quantumoptics.at/en/research/2darrays.html" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><p id="67d8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因为自旋可以向上或向下，所以它们与二进制变量相关联</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/2777edeaaead73e98342f4994a131f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*gVpLFK9S_gOJuL737kWHtQ@2x.png"/></div></figure><p id="7477" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">索引<em class="op"> i </em>可以用来标记晶格中自旋的位置。为了方便起见，我将用矢量<strong class="lj jt"> <em class="op"> v. </em> </strong>来表示自旋的构型</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/61d0ec6bbb25b882d812e8033831444c.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*pY3gciUREth9S0epjNTFBA@2x.png"/></div></figure><p id="6862" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于处于<a class="ae jg" href="https://en.wikipedia.org/wiki/Thermal_equilibrium" rel="noopener ugc nofollow" target="_blank">热平衡</a>的系统，与自旋组态<strong class="lj jt"> <em class="op"> v </em> </strong>相关的概率分布具有以下形式:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b98b7ba022a332a03a5e11594aa5ac76.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*3447ctp1a8YxEJezjQ8a6w@2x.png"/></div></figure><p id="838e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这就是无处不在的<a class="ae jg" href="https://en.wikipedia.org/wiki/Boltzmann_distribution" rel="noopener ugc nofollow" target="_blank">玻尔兹曼分布</a>(为方便起见，温度设为 1)。物体<em class="op">H</em>(<strong class="lj jt"><em class="op">v</em></strong>)就是系统所谓的哈密顿量，其中<a class="ae jg" href="https://en.wikipedia.org/wiki/Hamiltonian_(quantum_mechanics)" rel="noopener ugc nofollow" target="_blank">可以定义为</a>“系统中所有粒子的动能[和]势能之和对应的算符”。分母<em class="op"> Z </em>是一个归一化因子，称为<a class="ae jg" href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)" rel="noopener ugc nofollow" target="_blank">配分函数</a></p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/7d862306d0d710c3b2cd0d1aeeaf0ea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*ltxu0CkFRQiCAmEjdjQ3tw@2x.png"/></div></figure><p id="6090" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">系统的哈密顿量可以表示为对应于自旋之间相互作用的项的总和:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/6edd1d2d98dfa0ba4d9e1f15c0e04d8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*41p0YbDGkuJACTRbAtX-IA@2x.png"/></div></div></figure><p id="3125" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该组参数</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/771ff1e360f8b995e58ed40d04a573dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zS_24o8HcaBRXMk8H_TDwA@2x.png"/></div></div></figure><p id="2351" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">被称为<a class="ae jg" href="https://en.wikipedia.org/wiki/Coupling_constant" rel="noopener ugc nofollow" target="_blank">耦合常数</a>，它们决定了自旋之间(第二项)或自旋和外部磁场之间(第一项)的相互作用强度。</p><p id="6d7b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们需要考虑的另一个重要的量是自由能。自由能是一个最初来自热力学的概念，在热力学中，自由能被定义为物理系统中可以被转换来做功的能量。数学上，在我们的情况下，它由下式给出:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/cb5dec802046d6034c84236c81c16002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*9dsIMZlkJ-HT3ZkOfvk29A@2x.png"/></div></figure><p id="0751" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">符号“tr”代表<a class="ae jg" href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)" rel="noopener ugc nofollow" target="_blank">迹</a>(来自线性代数)。在本上下文中，它表示可见自旋<strong class="lj jt"> <em class="op"> v </em> </strong>的所有可能配置的总和。</p><p id="deca" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在重正化过程的每一步，系统在小尺度下的行为被平均化。粗粒度系统的哈密顿量用新的耦合常数表示</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/292329e3945949e2e6bca6bc2b207c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HRW2Hlgn23KVYvmugiI0Mw@2x.png"/></div></div></figure><p id="353e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">并且获得新的<strong class="lj jt">粗粒度变量</strong>。在我们的例子中，后者是块自旋<strong class="lj jt"> <em class="op"> h </em> </strong>而新的哈密顿量是:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/878fa3fffe642c70c0278066f3ebd8c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73JwfOq-vPdKjqLjqLYHMw@2x.png"/></div></div></figure><p id="a2e4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了更好地理解什么是块自旋，考虑下面的二维晶格。每个箭头代表一次旋转。现在把晶格分成正方形块，每块包含 2×2 个自旋。块旋转是对应于这些块中的每一个的平均旋转。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/b4c7c9eafb2ba3d6eee3d750ac942c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*mnmeyoqgi-AVB1XMM9rbFg@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">In block spin RG, the system is coarse-grained into new block variables describing the effective behavior of spin blocks (<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><p id="c57e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意，新的哈密顿量<strong class="lj jt">与原来的哈密顿量</strong>具有相同的结构，只是用自旋的<em class="op">块的配置代替了物理自旋。</em></p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/7376d7e8d110132e8dd8aebaf0ec8116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xxd88YJqskMGqgDBnRJKag@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Both Hamiltonians have the same structure but with different variables and couplings.</figcaption></figure><p id="61be" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">换句话说，模型的形式不会改变，但是当我们缩小时，模型的参数会改变。通过系统地重复这些步骤，可以获得理论的完全重正化。在几次 RG 迭代之后，一些参数将被丢弃，一些将被保留。剩下的被称为<a class="ae jg" href="https://en.wikipedia.org/wiki/Renormalization_group#Relevant_and_irrelevant_operators_and_universality_classes" rel="noopener ugc nofollow" target="_blank">相关运营商</a>。</p><p id="2bda" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这些哈密顿量之间的联系是通过自由能(上面描述的几行)在 RG 变换后不变的要求获得的。</p><h2 id="ea17" class="md me jj bd mf mg mh dn mi mj mk dp ml lq mm mn mo lu mp mq mr ly ms mt mu jp bi translated">变分重整化群(VRG)</h2><p id="93d5" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">如上所述，为了实现 RG 映射，可以使用变分重整化群(VRG)方案。在这个方案中，映射是由一个操作符实现的</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/35b647cf154713ccd939e34d6f8e5075.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*6H-m81EqsPkr5JGn3VA3sw@2x.png"/></div></figure><p id="1c75" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中<em class="op"> λ </em>是一组参数。该算子对隐藏自旋和输入(可见)自旋之间的耦合进行编码，并满足以下关系:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/074f34f718f91b95023c2a18bc413e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*MxuFLBGLkLfXHuGOaECKLg@2x.png"/></div></figure><p id="cbf6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其定义了上面给出的新的哈密顿量。尽管在精确的 RG 变换中，粗粒度系统将具有与原始系统完全相同的自由能，即</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/3be4529a1a865cba9b707641294ea833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jrkEpfcsj0eTtWN0F4RKHg@2x.png"/></div></div></figure><p id="caea" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这相当于以下条件</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/7a3b6adb9b772a647da4d4142c83ed03.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*zMruYkzsLk5HcfDAmZ_w3w@2x.png"/></div></figure><p id="7350" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在实践中，这个条件不能完全满足，变分方案被用来寻找使自由能之间的差异最小化的<em class="op"> λ </em></p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/24c44e569f7c22acff4fa06a7a3272e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mhkwFMAo7TaZtfLXXSXV4Q@2x.png"/></div></div></figure><p id="64d4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">或者等效地，近似精确的 RG 变换。</p><h1 id="d82c" class="oc me jj bd mf od oe of mi og oh oi ml ky oj kz mo lb ok lc mr le ol lf mu om bi translated">成果管理制概述</h1><p id="f5f6" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">我已经在<a class="ae jg" rel="noopener" target="_blank" href="/neural-quantum-states-4793fdf67b13">以前的文章</a>中详细描述了受限玻尔兹曼机器的内部工作原理。这里我将提供一个更简洁的解释。</p><div class="is it gp gr iu ph"><a rel="noopener follow" target="_blank" href="/neural-quantum-states-4793fdf67b13"><div class="pi ab fo"><div class="pj ab pk cl cj pl"><h2 class="bd jt gy z fp pm fr fs pn fu fw js bi translated">神经量子态</h2><div class="po l"><h3 class="bd b gy z fp pm fr fs pn fu fw dk translated">神经网络如何解决量子力学中高度复杂的问题</h3></div><div class="pp l"><p class="bd b dl z fp pm fr fs pn fu fw dk translated">towardsdatascience.com</p></div></div><div class="pq l"><div class="pr l ps pt pu pq pv ja ph"/></div></div></a></div><p id="9f4f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">受限玻尔兹曼机器(RBM)是生成式的，<a class="ae jg" href="http://www.iro.umontreal.ca/~bengioy/ift6266/H14/ftml-sec5.pdf" rel="noopener ugc nofollow" target="_blank">基于能量的模型</a>。用于<a class="ae jg" href="https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html'" rel="noopener ugc nofollow" target="_blank">非线性无监督特征学习</a>。它们最简单的版本只包含两层:</p><ul class=""><li id="a7e1" class="pw px jj lj b lk ll ln lo lq py lu pz ly qa mc qb qc qd qe bi translated">一层可见单元，用<strong class="lj jt"> <em class="op"> v </em> </strong>表示</li><li id="5584" class="pw px jj lj b lk qf ln qg lq qh lu qi ly qj mc qb qc qd qe bi translated">一个隐藏层，单位用<strong class="lj jt"> <em class="op"> h </em> </strong>表示</li></ul><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qk"><img src="../Images/064e35cf5007351f8dba72218e8b1644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVcPrQc15GdOe66A3IrrMA@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Illustration of a simple Restricted Boltzmann Machine (<a class="ae jg" rel="noopener" target="_blank" href="/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15">source</a>).</figcaption></figure><p id="d56d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我将再次考虑一个二进制可见数据集<strong class="lj jt"> <em class="op"> v </em> </strong>以及从一些概率分布中提取的<em class="op"> n </em>元素</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/729caf28c299663ce7d09898cb419baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*VeAIxN065D2KziwkKyeR_A@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Eq. 9: Probability distribution of the input or visible data.</figcaption></figure><p id="5082" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">RBM 中的隐藏单元(由矢量<strong class="lj jt"> <em class="op"> h </em> </strong>表示)耦合到可见单元，其相互作用能量由下式给出</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qm"><img src="../Images/38f13209f9d0dca3be48f8d2035839e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mlW8D_RG6T1_pktY38NfAQ@2x.png"/></div></div></figure><p id="6987" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">能量子指标<em class="op"> λ </em>代表<em class="op">变分</em>参数的集合{ <strong class="lj jt"> <em class="op"> c </em> </strong>，<strong class="lj jt"> <em class="op"> b，W </em> </strong> } <strong class="lj jt"> <em class="op">。</em> </strong>其中前两个元素是矢量，第三个是矩阵。RBMs 的目标是输出一个依赖于<em class="op"> λ- </em>的<em class="op"> </em>概率分布，它尽可能接近输入数据<em class="op">P</em>(<strong class="lj jt"><em class="op">v</em></strong>)的分布。</p><p id="f382" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">与配置(<strong class="lj jt"> <em class="op"> v </em> </strong>，<strong class="lj jt"> <em class="op"> h </em> </strong>)和参数<em class="op"> λ </em>相关的概率是该能量泛函的函数:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/41133f58d42d0f12d81c9ccbe3c7b02e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*9_7K2GYwGAFRBeqq6Lcq4g@2x.png"/></div></figure><p id="216c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">根据这个联合概率，通过对隐藏单元求和，可以容易地获得可见单元的变分(边缘化)分布。同样，隐藏单元的边缘化分布通过对可见单元求和来获得:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/44c20bf110b60bbef0757b051a65328f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*qYI3_E81aGml3UH8jmgH9w@2x.png"/></div></figure><p id="6a61" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以将 RBM 哈密顿量定义如下:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/f33b6a178d51a73e7c2d80c702a962f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*EMRmr6Jgou6Ns64AMiOuzQ@2x.png"/></div></figure><p id="dec2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">可以选择<em class="op"> λ </em>参数来优化所谓的<a class="ae jg" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> Kullback-Leibler (KL)散度或相对熵</a>，其测量两个概率分布的不同程度。在目前的情况下，我们感兴趣的是真实数据分布和 RBM 产生的可见单位的变化分布之间的 KL 散度。更具体地说:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qp"><img src="../Images/5b6a16bdeaf7817b74cf5e66f75b35ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y8XbIQ3h2wUqyV-4rEvzYQ@2x.png"/></div></div></figure><p id="ea89" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当两个分布相同时:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/0852c15d08be5dc3c379c10da4b37140.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*KVZ8aZB8DcOzBA1YPdGItQ@2x.png"/></div></figure><h1 id="e29f" class="oc me jj bd mf od oe of mi og oh oi ml ky oj kz mo lb ok lc mr le ol lf mu om bi translated">准确映射 RG 和 RBM</h1><p id="8ddf" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated"><a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank"> Mehta 和 Schwab </a>表明，为了建立 RG 和 RBMs 之间的精确映射，可以为变分算子选择以下表达式:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/2eb2e5ea4df3fd9f68f136ab8d0df2ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*J2t1A9Pqph3Gq0SaCxslQQ@2x.png"/></div></figure><p id="e975" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">回想一下，哈密顿量<em class="op"> H </em> ( <strong class="lj jt"> <em class="op"> v </em> </strong>)包含编码在其中的输入数据的概率分布。通过这种变分算子的选择，可以<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank">快速证明</a>隐藏层上的 RG 哈密顿量和 RBM 哈密顿量是相同的:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/207b0581fae1eb035603fab909177195.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*EobWlAMImV5ijvj6g-yF0A@2x.png"/></div></figure><p id="c465" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，当可以实现精确的 RG 变换时，真实的和变分的哈密顿量是相同的:</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/d83f37b833c7ce4f5eb63ea31be6b9a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*AE3JEhCd6OICme-gy-R9wg@2x.png"/></div></figure><p id="a594" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">于是我们看到，具有自旋<strong class="lj jt"> <em class="op"> v </em> </strong>和<strong class="lj jt"> <em class="op"> </em> </strong>的重整化群的一步——块自旋<strong class="lj jt"><em class="op"/></strong>可以精确地映射成由可见单元<strong class="lj jt"> <em class="op"> v </em> </strong>和隐藏单元<strong class="lj jt"><em class="op"/></strong>构成的双层 RBM。</p><p id="4914" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">随着我们堆叠越来越多的 RBM 层，我们实际上正在执行越来越多轮的 RG 转换。</p><h2 id="099f" class="md me jj bd mf mg mh dn mi mj mk dp ml lq mm mn mo lu mp mq mr ly ms mt mu jp bi translated">伊辛模型的应用</h2><p id="3e87" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">根据这个<em class="op">原理，</em>我们得出结论，RBMs，一种无监督的深度学习算法，实现了变分 RG 过程。这是一个非凡的对应，<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank">梅塔和施瓦布</a>通过在一个广为人知的<a class="ae jg" href="https://en.wikipedia.org/wiki/Ising_model" rel="noopener ugc nofollow" target="_blank">伊辛自旋模型</a>上实现堆叠 RBM 来展示他们的想法。他们将从伊辛模型中取样的自旋组态作为输入数据输入到 DNN 中。他们的结果表明，值得注意的是，DNNs 似乎正在执行(Kadanoff) <strong class="lj jt"> </strong>块自旋重正化。</p><p id="9f12" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">用作者的话说，“令人惊讶的是，这种局部块自旋结构从训练过程中出现，表明 DNN 正在自组织实施块自旋重整化……我对我们感到震惊的是，你不用手动输入，它就会学习”。</p><blockquote class="ng"><p id="d621" class="nh ni jj bd nj nk nv nw nx ny nz mc dk translated">他们的结果表明，值得注意的是，DNNs 似乎正在执行块自旋重正化。</p></blockquote><p id="496b" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">在他们的论文、<strong class="lj jt">和</strong>的下图<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank">中，展示了 DNN 的建筑。在<strong class="lj jt"> B </strong>中，学习参数 W 被绘制以显示隐藏和可见单元之间的交互。在<strong class="lj jt"> D </strong>中，当我们沿着 DNN 层移动时，我们看到了块自旋的逐渐形成(图中的斑点)。在<strong class="lj jt"> E </strong>中，显示了再现三个数据样本宏观结构的 RBM 重建。</a></p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qt"><img src="../Images/e7a11c105c1cd69220d267a99824a5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-IbnQbOn5g84JMtWgbmHfg@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Deep neural networks applied to the 2D Ising model. See the main text for a detailed description of each of the figures (<a class="ae jg" href="https://arxiv.org/pdf/1410.3831.pdf" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><h1 id="f604" class="oc me jj bd mf od oe of mi og oh oi ml ky oj kz mo lb ok lc mr le ol lf mu om bi translated">结论和展望</h1><p id="2050" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">2014 年<a class="ae jg" href="https://arxiv.org/abs/1410.3831" rel="noopener ugc nofollow" target="_blank"> Mehta 和 Schwab </a>表明，一种受限玻尔兹曼机器(RBM)，一种神经网络，与重正化群有关，重正化群是一个最初来自物理学的概念。在本文中，我回顾了他们的部分分析。正如之前所认识到的，RG 和深度神经网络都具有显著的“哲学相似性”:都将复杂的系统提取为相关的部分。这种 RG-RBM 映射是这种相似性的一种形式化。</p><p id="9a9f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于深度学习和生物学习过程有许多相似之处，因此假设我们的大脑也可能使用某种“类固醇上的重整化”来理解我们感知的现实并不算过分。</p><p id="d451" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如其中一位作者<a class="ae jg" href="https://www.quantamagazine.org/deep-learning-relies-on-renormalization-physicists-find-20141204/" rel="noopener ugc nofollow" target="_blank">建议</a>的那样，“也许对于如何从数据中挑选出相关的特征有一些普遍的逻辑，我会说这是一个暗示，也许有类似的东西存在。”</p><blockquote class="ng"><p id="d962" class="nh ni jj bd nj nk nv nw nx ny nz mc dk translated">假设我们的大脑也可能使用某种“类固醇上的重整化”来理解我们感知的现实，这并不是太夸张。</p></blockquote><p id="3d30" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">这方面的问题是，与 RG 工作良好的自相似系统(具有类似分形的行为)相比，自然界中的系统通常不是自相似的。正如神经科学家 Terrence Sejnowski 所指出的，突破这一限制的一种可能方式是，如果我们的大脑以某种方式在临界点运行，所有神经元都影响整个网络。但是那是另一篇文章的主题！</p></div><div class="ab cl qu qv hx qw" role="separator"><span class="qx bw bk qy qz ra"/><span class="qx bw bk qy qz ra"/><span class="qx bw bk qy qz"/></div><div class="im in io ip iq"><p id="918d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢您的阅读，再见！一如既往，我们随时欢迎建设性的批评和反馈！</p><p id="6d28" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我的<a class="ae jg" href="https://github.com/marcotav" rel="noopener ugc nofollow" target="_blank"> Github </a>和个人网站<a class="ae jg" href="https://marcotavora.me/" rel="noopener ugc nofollow" target="_blank"> www.marcotavora.me </a>有(希望)一些关于数据科学和物理学的其他有趣的东西。</p></div></div>    
</body>
</html>