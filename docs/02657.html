<html>
<head>
<title>The Significance and Applications of Covariance Matrix</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">协方差矩阵的意义及其应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-significance-and-applications-of-covariance-matrix-d021c17bce82?source=collection_archive---------4-----------------------#2019-05-01">https://towardsdatascience.com/the-significance-and-applications-of-covariance-matrix-d021c17bce82?source=collection_archive---------4-----------------------#2019-05-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a039" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">线性代数和各种应用之间的一个联系</h2></div><p id="927f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“数学的美妙之处在于简单的模型可以做伟大的事情。”现代数据科学中不乏花哨的算法和技术。技术容易学，但也容易落后。然而，从长远来看，数学基础是有益的。协方差矩阵是一个简单而有用的数学概念，广泛应用于金融工程、计量经济学以及机器学习中。鉴于它的实用性，我决定从我的记事本中总结一些要点和例子，并整理出一个有凝聚力的故事。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/abd878652203077589ca77e54e1d6ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9J5DbJ4XmTH0jWcT0DriUQ.jpeg"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Photo from Pixabay</figcaption></figure><p id="5097" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">协方差衡量两个随机变量在总体中一起变化的程度。当总体包含更高维度或更多随机变量时，用一个矩阵来描述不同维度之间的关系。用一种更容易理解的方式来说，协方差矩阵就是将整个维度上的关系定义为任意两个随机变量之间的关系。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lu"><img src="../Images/7748d4879bbacde4f297f31f02e46366.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-gVkrHRFE25-_ZiS4huYyQ.png"/></div></div></figure><h2 id="6133" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">用例 1:随机建模</h2><p id="a237" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">协方差矩阵最重要的特点就是正半定，这就带来了<a class="ae mt" href="https://en.wikipedia.org/wiki/Cholesky_decomposition" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"><em class="mu"/></strong></a>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mv"><img src="../Images/5fdfad1ca4ab2ea70f66bd6ac77836a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U3vydFrMM4Gzx2tlhRGnsg.png"/></div></div></figure><p id="b899" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简单地说，乔莱斯基分解就是将一个正定矩阵分解成一个下三角矩阵与其转置矩阵的乘积。在实践中，人们用它来生成相关随机变量，方法是将分解协方差矩阵得到的下三角乘以标准法线。此外，矩阵分解在许多方面是有帮助的，因为使用隐藏因子来表征矩阵揭示了通用的属性，并且我们不经常能够显式地执行矩阵计算。</p><p id="e406" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在金融工程中，蒙特卡洛模拟在期权定价中起着重要作用，在期权定价中，衍生品的收益取决于一篮子基础资产。给定股票价格演变的标准公式，该公式假设股票价格遵循几何布朗运动，相关股票价格可以通过将乔莱斯基分解应用于协方差矩阵来计算。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lu"><img src="../Images/8895b4ea86f7499241f552fc18cc7a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WgQtnBx1ILP7k-SvUvti5Q.png"/></div></div></figure><p id="96d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是我用 python 编写的一个简单例子，用这种方法模拟相关的股票价格路径。</p><pre class="lf lg lh li gt mw mx my mz aw na bi"><span id="1d75" class="lv lw it mx b gy nb nc l nd ne">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="85ee" class="lv lw it mx b gy nf nc l nd ne">mu_a, mu_b = 0.2, 0.3 # annual expected return for stock A and stock B<br/>sig_a, sig_b = 0.25, 0.35 # annual expected volatility<br/>s0_a, s0_b = 60, 55 # stock price at t0<br/>T = 1 # simulate price evolution for the next year<br/>delta_t = 0.001 <br/>steps = T/delta_t</span><span id="5195" class="lv lw it mx b gy nf nc l nd ne">rho = 0.2 # correlation between stock A and stock B<br/>cor_matrix = np.array([[1.0, rho],<br/>                       [rho, 1.0]])<br/>sd = np.diag([sig_a, sig_b]) <br/>cov_matrix = np.dot(sd, np.dot(cor_matrix, sd)) <br/>L = np.linalg.cholesky(cov_matrix) # Cholesky decomposition</span><span id="9b55" class="lv lw it mx b gy nf nc l nd ne">plt.figure(figsize = (12, 6))<br/>path_a = [s0_a]<br/>path_b = [s0_b]<br/>st_a, st_b = s0_a, s0_b<br/>for i in range(int(steps)):<br/>    V = L.dot(np.random.normal(0, 1, 2)) <br/>    st_a = st_a*np.exp((mu_a - 0.5*sig_a**2)*delta_t + sig_a*np.sqrt(delta_t)*V[0])<br/>    st_b = st_b*np.exp((mu_b - 0.5*sig_b**2)*delta_t + sig_b*np.sqrt(delta_t)*V[1])<br/>    path_a.append(st_a)<br/>    path_b.append(st_b)<br/>plt.plot(path_a, label = 'stock A', linewidth = 2)<br/>plt.plot(path_b, label = 'stock B', linewidth = 2)<br/>plt.legend()<br/>plt.title('Correlated Stock Movement Using Monte Carlo Simulation')<br/>plt.ylabel('stock price')<br/>plt.xlabel('steps')</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ng"><img src="../Images/c6d48b78f80765b63e6078c4ef39c642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qg_xpmONulZEEntg9ovynQ.png"/></div></div></figure><h2 id="3473" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">用例 2:主成分分析</h2><p id="c40f" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">PCA 是一种无监督的线性降维算法，将原始变量转化为这些独立变量的线性组合。它将整个数据集投影到不同的特征空间，在该空间中，它可以对解释数据最大差异的维度进行优先排序。机器学习实践者利用 PCA 通过降低低方差维度来降低计算复杂度，以及创建更好的可视化。</p><p id="ab17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PCA 和协方差方差是怎么联系起来的？<strong class="kk iu"> <em class="mu">特征分解</em> </strong></p><p id="878d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像乔莱斯基分解一样，特征分解是一种更直观的矩阵分解方式，通过使用矩阵的特征向量和特征值来表示矩阵。特征向量被定义为当对其应用线性变换时仅由标量改变的向量。如果 A 是表示线性变换的矩阵，<em class="mu"> v </em>是特征向量，λ是对应的特征值。可以表示为<em class="mu"> v </em> =λ <em class="mu"> v. </em>一个正方形<em class="mu"> </em>矩阵可以有和它的维数一样多的特征向量。如果把所有的特征向量作为矩阵 V 的列，把相应的特征值作为对角矩阵 L 的元素，上述方程可以推广到 AV = VL。在协方差矩阵的情况下，所有的特征向量彼此正交，它们是新特征空间的主分量。<strong class="kk iu">T15】</strong></p><p id="ea3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><ol class=""><li id="d4e7" class="nh ni it kk b kl km ko kp kr nj kv nk kz nl ld nm nn no np bi translated"><a class="ae mt" href="https://skymind.ai/wiki/eigenvector#code" rel="noopener ugc nofollow" target="_blank">https://skymind.ai/wiki/eigenvector#code</a></li><li id="778d" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><a class="ae mt" href="https://blog.csdn.net/thesnowboy_2/article/details/69564226" rel="noopener ugc nofollow" target="_blank">https://blog.csdn.net/thesnowboy_2/article/details/69564226</a></li><li id="8acd" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><a class="ae mt" href="https://datascienceplus.com/understanding-the-covariance-matrix/" rel="noopener ugc nofollow" target="_blank">https://datascienceplus . com/understanding-the-协方差矩阵/ </a></li></ol></div></div>    
</body>
</html>