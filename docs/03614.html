<html>
<head>
<title>Adding New Features by Probability Dimension</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过概率维度添加新特征</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adding-new-features-by-probability-dimension-bea2a4be45d5?source=collection_archive---------17-----------------------#2019-06-08">https://towardsdatascience.com/adding-new-features-by-probability-dimension-bea2a4be45d5?source=collection_archive---------17-----------------------#2019-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="32af" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">简介:</h1><p id="bf8f" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">特征工程是机器学习中的一个重要课题。如果我们想让我们的机器学习，我们需要给它有意义的信息。深度学习架构可能不需要创建良好的功能，因为它们实际上可以自己创建功能。但是，这将需要一个巨大的数据集，也需要大量的计算能力。</p><p id="2b0f" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">对于特征创建，我们需要了解机器学习将处理的主题。如果是信号处理，我们需要了解一下信号处理。如果是金融，我们需要了解一些金融知识，这样我们才会知道我们可以创造哪些特色。</p><h1 id="2239" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">方法:</h1><p id="9b0c" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">另一种可以应用于大多数领域的特征是概率。一个值与其他现有值一起存在于一个示例中的概率可能是一个有意义的特征。</p><h2 id="66cf" class="lr jr it bd js ls lt dn jw lu lv dp ka kz lw lx ke ld ly lz ki lh ma mb km mc bi translated">示例:假设我们有一个数字图像 0，如下所示。当其他值存在时，我们将创建每个值存在的概率。</h2><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/3c1015670b9f96f47398ec634eea316c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*amU04Ea7cqlwtKLJBhHxYQ.jpeg"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Digit 0 Image</figcaption></figure><p id="7553" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">我们将创建每个值的概率，并通过增加矩阵的维数来映射它。(表示是用 Python 语言编写的)</p><ol class=""><li id="02a5" class="mp mq it kq b kr lm kv ln kz mr ld ms lh mt ll mu mv mw mx bi translated">展平图像。(2D 至 1D) (imgFlat)</li><li id="46b8" class="mp mq it kq b kr my kv mz kz na ld nb lh nc ll mu mv mw mx bi translated">创建一个具有维度(valueRange，len(imgFlat))的零值矩阵。值域就是我们后面要确定的范围。</li><li id="368f" class="mp mq it kq b kr my kv mz kz na ld nb lh nc ll mu mv mw mx bi translated">将 imgFlat 值作为二进制矩阵映射到我们的零值矩阵(我们将在这里确定值的范围。假设我们的取值范围是 10。值范围是我们将映射图像值的范围。图像中的值将被映射到 0 到 10 之间的值。).我们的带有映射值的 imgFlat 是[ 0，3，10，7，2，0，0，6，8，5，8，1，1，7，2，1，7，1，1，5，1，7，1，0，6，3，5，6，0，3，9，7，1，0]。因此，让我们根据 imgFlat 值创建二元矩阵。</li></ol><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/f2591fdd2a26a674dfd7e588ee4a5ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*Hy-hRCXjvPwz6R4jVowL0Q.jpeg"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Binary Mapping</figcaption></figure><p id="5964" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">4.所以，我们拉平了我们的矩阵。然后，我们又增加了一个维度。在这个维度上，我们增加了概率。但是，我们还没有完成。根据我的启发式方法，最好将概率映射到接近概率 1 值的值。因此，我们将得到类似于[0 0.3 0.7 1 0.7 0.3]的值，而不是[0 0 0 1 0 0]。为此，我创建了一个新的概率分布函数。</p><h2 id="8c2b" class="lr jr it bd js ls lt dn jw lu lv dp ka kz lw lx ke ld ly lz ki lh ma mb km mc bi translated">概率分布函数:</h2><p id="15e7" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">假设我们需要一个有两个根的函数，这个函数的最大输出值是 1。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/764d1a0cd40dceaf4128bd8728138536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DVK4gnukvWIzwKK65MuSpg.jpeg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Probability Distribution Matrix</figcaption></figure><pre class="me mf mg mh gt nj nk nl nm aw nn bi"><span id="1692" class="lr jr it nk b gy no np l nq nr">def distFunc(argmax,xrange,valRange):<br/>    x = np.linspace(0,valRange,valRange+1)<br/>    a = argmax<br/>    b = xrange<br/>    y = ((x**2)-(2*x*a)+(a**2)-(b**2))/(valRange**2)<br/>    return 1-y<br/>yHist = exposure.rescale_intensity(y)#reason of this line is to distribute our values even more. (<a class="ae ns" href="https://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.rescale_intensity" rel="noopener ugc nofollow" target="_blank">skimage.exposure.rescale_intensity function</a>)</span></pre><p id="e22d" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">为了看清楚，我将数值四舍五入为十进制 1。正因为如此，一些值变成了 1。请忽略它。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nt"><img src="../Images/ca4b7f5049edd0e3ad200ed151a36d53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9WBsyH2QbD50MyJoP_PwNA.jpeg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Probability Distribution to Binary Mapping</figcaption></figure><p id="1039" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">6.概率分布之后，我们展平 2D 概率矩阵。它变得相当大。因此，我将在下面补充其中的一部分。</p><p id="abc4" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi">[1 0.9375 0 0 0.987654 1 1 0 0 0 0 1 1 0 0.987654 0.987654 0 0.987654 1 0 0.987654 0.987654 0 0.987654 1 0 0.816327 0 0 1 1 0.816327 0 0 1 …]</p><h2 id="d71f" class="lr jr it bd js ls lt dn jw lu lv dp ka kz lw lx ke ld ly lz ki lh ma mb km mc bi translated">这个结果意味着什么？</h2><p id="2ee7" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">毕竟，我们的矩阵代表了下面给出的陈述。</p><p id="462f" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">矩阵中第 0 列的值 0 以概率 1 存在，第 10 列的值 8 以概率 1 存在，而第 3 列的值 1 也以概率 0.3 存在，同时…</p><p id="50f1" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这给了我们一种情况，我们用它们存在的可能性来赋予我们的价值以意义。</p><h1 id="a2d9" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">基于 SVM 和概率维特征的数字分类；</h1><p id="3273" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">图书馆:</p><pre class="me mf mg mh gt nj nk nl nm aw nn bi"><span id="9abe" class="lr jr it nk b gy no np l nq nr">import numpy as np<br/>from skimage import exposure<br/>from sklearn.datasets import load_digits<br/>from sklearn import svm</span></pre><p id="7dd2" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">概率分布函数:</p><pre class="me mf mg mh gt nj nk nl nm aw nn bi"><span id="5fb5" class="lr jr it nk b gy no np l nq nr">def distFunc(argmax,xrange,valRange):<br/>    x = np.linspace(0,valRange,valRange+1)<br/>    a = argmax<br/>    b = xrange<br/>    y = ((x**2)-(2*x*a)+(a**2)-(b**2))/(valRange**2)<br/>    return 1-y</span></pre><p id="d564" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">概率维特征的创建:</p><pre class="me mf mg mh gt nj nk nl nm aw nn bi"><span id="d045" class="lr jr it nk b gy no np l nq nr">def createProbOfVal(kernel,valRange):<br/>    kernelFlat = kernel.flatten()<br/>    kernelFlat = kernelFlat/np.max(kernelFlat)*valRange<br/>    tempBefore = np.zeros((valRange+1,len(kernelFlat)))<br/>    tempAfter = np.zeros((valRange+1,len(kernelFlat)))<br/>    for cnt in range(len(kernelFlat)):<br/>        y = distFunc(int(kernelFlat[cnt]),1,valRange)<br/>        yhist = exposure.rescale_intensity(y)<br/>        tempBefore[int(kernelFlat[cnt]),cnt] = 1<br/>        tempAfter[:,cnt] = yhist<br/>    tempBeforeFlat = tempBefore.flatten()<br/>    tempAfterFlat = tempAfter.flatten()<br/>    return tempAfterFlat</span></pre><p id="afcd" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">数据集准备(图像到概率维度特征):</p><pre class="me mf mg mh gt nj nk nl nm aw nn bi"><span id="44d5" class="lr jr it nk b gy no np l nq nr">def prepareDataset(datasetImages,datasetLabels):<br/>    dataset = []<br/>    for cnt in range(len(datasetImages)):<br/>        processedImage = createProbOfVal(datasetImages[cnt],10)<br/>        dataset.append([processedImage,datasetLabels[cnt]])<br/>    return dataset</span></pre><p id="5792" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">加载并准备数据集:</p><pre class="me mf mg mh gt nj nk nl nm aw nn bi"><span id="ee19" class="lr jr it nk b gy no np l nq nr">digits = load_digits()<br/>dataset = prepareDataset(digits.images,digits.target)</span></pre><p id="51b3" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">创建分类器实例并训练它:</p><pre class="me mf mg mh gt nj nk nl nm aw nn bi"><span id="ccb4" class="lr jr it nk b gy no np l nq nr">clf = svm.SVC(gamma='scale')<br/>clf.fit([np.array(row[0]) for row in dataset[0:1500]],[np.array(row[1]) for row in dataset[0:1500]])</span></pre><p id="f140" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">测试:</p><pre class="me mf mg mh gt nj nk nl nm aw nn bi"><span id="d4a2" class="lr jr it nk b gy no np l nq nr">preds = clf.predict([np.array(row[0]) for row in dataset[1500:1796]])<br/>score = clf.score([np.array(row[0]) for row in dataset[1500:1796]],[np.array(row[1]) for row in dataset[1500:1796]])</span></pre><h2 id="dff7" class="lr jr it bd js ls lt dn jw lu lv dp ka kz lw lx ke ld ly lz ki lh ma mb km mc bi translated">准确率:92%</h2></div></div>    
</body>
</html>