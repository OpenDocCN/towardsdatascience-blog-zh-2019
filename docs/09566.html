<html>
<head>
<title>Process Wikipedia Using Apache Spark to Create Spicy Hot Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Apache Spark 处理维基百科，创建热点数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/process-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25?source=collection_archive---------22-----------------------#2019-12-16">https://towardsdatascience.com/process-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25?source=collection_archive---------22-----------------------#2019-12-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2bb1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从维基百科:名人数据集创建一个你选择的数据集</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7c7cb249e04697dbf914fbb561261e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jG8lS01f1BEQhM78.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Wikipedia, The Free Encyclopedia</figcaption></figure><p id="41a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们大多数人肯定都遇到过这样的情况，每当我们想了解某个国家的历史、纪念碑、电视剧和电影、名人的生活和职业生涯、过去的事件或当前的事件时，我们的首选是维基百科。聪明的人用它来获取各种各样的知识，听起来甚至更聪明。但是，你有没有想过它有多大？它有多少文件/文章？</p><p id="6204" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">维基百科目前有大约 598 万篇文章，并且每天都在增加。有各种各样来自各个领域的文章。我们可以使用这些数据来创建许多有趣的应用程序。如果您可以访问这些数据，您的下一个有趣的应用是什么？你如何得到整个数据集？即使你得到了它，处理它需要多少计算量？</p><p id="8087" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有这些问题都将得到解答。</p><p id="e4cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">在本文中，我想创建一个名人数据集。过去或现在所有在维基百科上有页面的受欢迎的人，像维拉特·科利、萨钦·坦杜尔卡尔、板球界的瑞奇·庞廷、布拉德·皮特、莱昂纳多·迪卡普里奥、电影界的阿米特巴·巴强、物理学家阿尔伯特·爱因斯坦、伊萨克·牛顿等等。我将使用 Apache Spark (PySpark)来处理这个庞大的数据集。</em>T3】</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="bb8c" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">1.获取维基百科转储</h1><p id="af53" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">为了开始这项任务，首先，我们需要 Wikipedia 转储。维基百科转储可以从<a class="ae mz" href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2" rel="noopener ugc nofollow" target="_blank">这里</a>以 XML 格式下载。它不断刷新，包含最新的维基百科数据。</p><h2 id="03df" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">解析维基百科 XML 的脚本</h2><p id="4802" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">接下来，可以使用免费提供的<a class="ae mz" href="https://github.com/attardi/wikiextractor" rel="noopener ugc nofollow" target="_blank"> Python 包</a>非常容易地解析这个下载的 XML。关于如何使用该软件包的更多细节可以在<a class="ae mz" href="https://spark-in.me/post/parsing-wikipedia-in-four-commands-for-nlp" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="2417" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦解析完成，这就是解析后的目录结构的外观。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/ea8e15d2c1629f017f13d86b66f10279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j34nNBtAxRPt9u9og8j11w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Parsed Directory Structure</figcaption></figure><p id="bdef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个目录由多个纯文本文件组成。文件内容的快照</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/366497024b769a040250f75c4cb9c06c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VcH-GZDjb4Pn-7dNZg5NvA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Snapshot of File Content</figcaption></figure><p id="6792" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个文件包含多个维基百科文章。每篇文章都以<doc>标签开始，以</doc>结束。接下来就是把这些档案都翻一遍，筛选出名人对应的文章。<strong class="la iu"> <em class="lu">如果我们使用一台机器来完成，可能需要几天时间，因为这是一项计算密集型任务。</em> </strong>我们将利用分布式系统框架 Apache Spark (PySpark)来执行这项任务，这项任务只需要 10 到 15 分钟的时间，并配备合理数量的执行者。</p><h1 id="462e" class="mc md it bd me mf no mh mi mj np ml mm jz nq ka mo kc nr kd mq kf ns kg ms mt bi translated">2.通过 Apache Spark (PySpark)处理维基百科解析数据</h1><h2 id="74fa" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">阿帕奇火花</h2><blockquote class="nt nu nv"><p id="f022" class="ky kz lu la b lb lc ju ld le lf jx lg nw li lj lk nx lm ln lo ny lq lr ls lt im bi translated"><a class="ae mz" href="https://en.wikipedia.org/wiki/Apache_Spark" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>是一种快如闪电的集群计算技术，专为快速计算而设计。它基于 Hadoop <a class="ae mz" href="https://en.wikipedia.org/wiki/MapReduce" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> MapReduce </strong> </a>并扩展了 MapReduce 模型，以有效地将其用于更多类型的计算，包括交互式查询和流处理。Spark 的主要特点是它的<strong class="la iu">内存集群计算</strong>提高了应用程序的处理速度。</p></blockquote><h2 id="cdc1" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">一些 Apache Spark 术语和转换函数(可选)</h2><p id="dc10" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">此部分是可选的。这一节是为 Spark 初学者或那些想在继续之前快速掌握一些 Spark 变换函数的人准备的。要了解关于这些功能的更多信息，请浏览此<a class="ae mz" href="https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><ul class=""><li id="7396" class="nz oa it la b lb lc le lf lh ob ll oc lp od lt oe of og oh bi translated"><strong class="la iu"> RDD </strong> : RDD(弹性分布式数据集)是 Apache Spark 的基本数据结构，它是在集群的不同节点上计算的不可变对象集合。RDD 是分布在集群中许多机器上的数据元素的分布式集合。</li><li id="53ab" class="nz oa it la b lb oi le oj lh ok ll ol lp om lt oe of og oh bi translated"><strong class="la iu">数据帧</strong>:数据帧是组织成指定列的数据的分布式集合。它在概念上相当于关系数据库中的一个表。</li><li id="a518" class="nz oa it la b lb oi le oj lh ok ll ol lp om lt oe of og oh bi translated"><strong class="la iu">map()</strong>:map 函数遍历 RDD 的每一条线，分割成新 RDD。使用 map()变换，我们接受任何函数，并将该函数应用于 RDD 的每个元素。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Usage of map() transformation function</figcaption></figure><ul class=""><li id="a4c4" class="nz oa it la b lb lc le lf lh ob ll oc lp od lt oe of og oh bi translated"><strong class="la iu"> flatMap() </strong>:借助 flatMap()函数，对于每个输入元素，我们在一个输出 RDD 中有很多元素。flatMap()最简单的用法是将每个输入字符串拆分成单词。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Usage of flatMap() transformation function</figcaption></figure><ul class=""><li id="c255" class="nz oa it la b lb lc le lf lh ob ll oc lp od lt oe of og oh bi translated"><strong class="la iu"> filter() </strong> : Spark RDD 过滤器()函数返回一个新的 RDD，只包含满足谓词的元素。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Usage of filter() transformation function</figcaption></figure><ul class=""><li id="9f77" class="nz oa it la b lb lc le lf lh ob ll oc lp od lt oe of og oh bi translated"><strong class="la iu">whole text files():</strong>whole text files 返回一个 PairRDD，其键是文件路径，值是文件内容。</li></ul><h2 id="bc61" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">开始执行任务</h2><p id="a3d9" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">首先，我们将获取 HDFS 中传输的所有数据，并使用 wholeTextFiles 读取数据。</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="9b73" class="na md it oq b gy ou ov l ow ox">data = sc.wholeTextFiles(“hdfs:///Data_w/*/*”)</span></pre><p id="69d7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出将具有键-值配对的 RDD，其中键是文件路径，内容是值。我们可以去掉文件路径，只处理内容。每个文件内容包含由<doc>标签分隔的多个维基百科文章。我们需要把所有这些文章作为单独的记录拿出来。为此，我们将使用一个 flatMap()函数。</doc></p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="9a86" class="na md it oq b gy ou ov l ow ox">pages = data.flatMap(lambda x :(x[1].split('&lt;/doc&gt;')))</span></pre><p id="6302" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们有了所有的文章，我们需要找到文章是关于什么的。每篇文章的第一行是标题，其余部分是内容。我们可以使用这些信息将每篇文章转换成一个键-值对，其中键是标题，值是内容。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/0a70b43ff36415b847b447aa9a795895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rCXyygp6YK_0KyGxlfp2qA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Snapshot of each article</figcaption></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Function to extract the title</figcaption></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Function to extract the content</figcaption></figure><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="d07a" class="na md it oq b gy ou ov l ow ox">pages = data.flatMap(lambda x :(x[1].split(‘&lt;/doc&gt;’))).map(lambda x : (get_title(x),get_content(x)))</span></pre><p id="23cc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，就是只过滤掉人物(名人)对应的文章。之前，写一些逻辑，让我们看看一些名人网页看起来如何。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/37eb98edafc5a9045fad816f9373e23c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EAQjBv1VfhBjtvy1ABATGQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Wiki Page of Sir Isaac Newton</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/c04b65c7e39f56243e5faf604f5f110b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQ3C07MGdSrO4plfEgHZCA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Wiki Page of Virat Kohli</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/2be9c66b6d662d378f31132727664ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6Op5jbeoBYUEzlzCqd5Uw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Wiki Page of Brad Pitt</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/9d8ac175d4fa2031c20e132752a9d415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RcECqAN3nzCb2dOLgXUzEw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Wiki Page of Michael Jackson</figcaption></figure><p id="653d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们也看看一些非名人的网页。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/827d2b4de6ac48f107307dface432708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0mm5WaoXIxrB0tgce5zLA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Wiki Page of the United States</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/d7549c49de9d2d06cec18e5219135a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ngnBGNGGD6yme7ecmV19Jw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Wiki Page of Taj Mahal, India</figcaption></figure><p id="8949" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">名人页面和非名人页面有什么不同？所有名人页面的共同点是什么？</em>T3】</strong></p><p id="1474" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大多数名人的页面在第一句话就包含了出生日期。在维基百科数据中，这个出生日期是以下两种格式之一:</p><ol class=""><li id="e530" class="nz oa it la b lb lc le lf lh ob ll oc lp od lt pf of og oh bi translated">年月日:如 1993 年 8 月 12 日</li><li id="4b64" class="nz oa it la b lb oi le oj lh ok ll ol lp om lt pf of og oh bi translated">年月日:如 1993 年 8 月 12 日</li></ol><p id="5971" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以利用这个事实来快速过滤掉所有的名人页面。我将使用 Regex 来查找这种格式。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Code to check if the Wiki page is a Celebrity Page or not</figcaption></figure><p id="b53b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们可以将输出保存为表格。完整的代码如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Entire Code for the project</figcaption></figure><h1 id="f566" class="mc md it bd me mf no mh mi mj np ml mm jz nq ka mo kc nr kd mq kf ns kg ms mt bi translated">3.验证创建的名人数据集</h1><p id="9dc3" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">维基百科总共有大约 598 万篇文章。我们的名人数据集有 138 万篇文章。所有名人的名单和获得的数据可以在<a class="ae mz" href="https://github.com/samread81/Wiki-Celebrity-DataSet" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="5b2d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该数据集包含了关于迈克尔·杰克逊、阿米特巴·巴强、布拉德·皮特、萨钦·坦杜尔卡尔、多尼女士以及所有我们能想到并核实的其他名人的文章。</p><h1 id="0fcb" class="mc md it bd me mf no mh mi mj np ml mm jz nq ka mo kc nr kd mq kf ns kg ms mt bi translated">4.结论</h1><p id="58c5" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">维基百科是互联网上所有可能信息的最佳去处之一。我们可以用它来创建许多有趣的应用程序，你的下一个大而有趣的 NLP 项目。随着 Apache Spark 的使用，处理这些海量数据变得很容易。用不到 20-25 行代码，我们就可以从中创建大多数有趣的数据集。</p><p id="21be" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">写一篇清晰易懂的好文章需要很多努力。我会继续努力做好我的工作。在<a class="ae mz" href="https://medium.com/@mungoliabhishek81" rel="noopener"> <strong class="la iu">中</strong> </a>关注我，查看我以前的帖子。我欢迎反馈和建设性的批评。所有名人和数据集的列表以及代码可以在这里找到。</p><p id="84bd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">我的 Youtube 频道获取更多内容:</em> </strong></p><div class="pg ph gp gr pi pj"><a href="https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA" rel="noopener  ugc nofollow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">阿布舍克·蒙戈利</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">嗨，伙计们，欢迎来到频道。该频道旨在涵盖各种主题，从机器学习，数据科学…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">www.youtube.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px ks pj"/></div></div></a></div><h1 id="ff25" class="mc md it bd me mf no mh mi mj np ml mm jz nq ka mo kc nr kd mq kf ns kg ms mt bi translated">5.参考</h1><ol class=""><li id="165f" class="nz oa it la b lb mu le mv lh py ll pz lp qa lt pf of og oh bi translated"><a class="ae mz" href="https://www.tutorialspoint.com/apache_spark/apache_spark_introduction.htm" rel="noopener ugc nofollow" target="_blank">https://www . tutorialspoint . com/Apache _ spark/Apache _ spark _ introduction . htm</a></li><li id="7b09" class="nz oa it la b lb oi le oj lh ok ll ol lp om lt pf of og oh bi translated">https://spark.apache.org/<a class="ae mz" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"/></li><li id="ddc2" class="nz oa it la b lb oi le oj lh ok ll ol lp om lt pf of og oh bi translated">【https://en.wikipedia.org/wiki/Apache_Spark T4】</li><li id="c26c" class="nz oa it la b lb oi le oj lh ok ll ol lp om lt pf of og oh bi translated"><a class="ae mz" href="https://en.wikipedia.org/wiki/MapReduce" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/MapReduce</a></li><li id="cd31" class="nz oa it la b lb oi le oj lh ok ll ol lp om lt pf of og oh bi translated"><a class="ae mz" href="https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/" rel="noopener ugc nofollow" target="_blank">https://data-flair . training/blogs/spark-rdd-operations-transformations-actions/</a></li><li id="c937" class="nz oa it la b lb oi le oj lh ok ll ol lp om lt pf of og oh bi translated"><a class="ae mz" href="https://data-flair.training/blogs/spark-rdd-tutorial/" rel="noopener ugc nofollow" target="_blank">https://data-flair.training/blogs/spark-rdd-tutorial/</a><br/><a class="ae mz" href="https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/" rel="noopener ugc nofollow" target="_blank">https://data-flair . training/博客/Apache-spark-rdd-vs-data frame-vs-dataset/</a></li><li id="9dd4" class="nz oa it la b lb oi le oj lh ok ll ol lp om lt pf of og oh bi translated"><a class="ae mz" href="https://www.analyticsvidhya.com/blog/2019/10/pyspark-for-beginners-first-steps-big-data-analysis/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2019/10/py spark-for-初学者-第一步-大数据-分析/ </a></li><li id="ab6e" class="nz oa it la b lb oi le oj lh ok ll ol lp om lt pf of og oh bi translated"><a class="ae mz" href="https://blog.softhints.com/python-regex-match-date/" rel="noopener ugc nofollow" target="_blank">https://blog.softhints.com/python-regex-match-date/</a></li></ol></div></div>    
</body>
</html>