<html>
<head>
<title>Classification and Regression Analysis with Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用决策树进行分类和回归分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/https-medium-com-lorrli-classification-and-regression-analysis-with-decision-trees-c43cdbc58054?source=collection_archive---------1-----------------------#2019-05-15">https://towardsdatascience.com/https-medium-com-lorrli-classification-and-regression-analysis-with-decision-trees-c43cdbc58054?source=collection_archive---------1-----------------------#2019-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4ebc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过理解决策树背后的基本概念和数学，学习构建分类和回归决策树！</h2></div><p id="663d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">决策树</strong>是一种受监督的机器学习模型，用于通过从特征中学习决策规则来预测目标。顾名思义，我们可以把这个模型看作是通过提出一系列问题来做出决定，从而分解我们的数据。</p><p id="af2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们考虑下面的例子，其中我们使用决策树来决定某一天的活动:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/9e685e71632ed6a80df330164f9b6883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WerHJ14JQAd3j8ASaVjAhw.jpeg"/></div></div></figure><p id="4b08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于我们训练集中的特征，决策树模型学习一系列问题来推断样本的类别标签。正如我们所见，如果我们关心可解释性，决策树是有吸引力的模型。</p><p id="bf8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然上图说明了基于分类目标的决策树的概念(<strong class="kk iu">分类</strong>)，但是如果我们的目标是实数(<strong class="kk iu">回归</strong>)，同样的概念也适用。</p><p id="0be6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本教程中，我们将讨论如何用 Python 的<code class="fe lq lr ls lt b">scikit-learn</code>库构建决策树模型。我们将涵盖:</p><ul class=""><li id="89ff" class="lu lv it kk b kl km ko kp kr lw kv lx kz ly ld lz ma mb mc bi translated">决策树的基本概念</li><li id="7e61" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated">决策树学习算法背后的数学原理</li><li id="90fc" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated">信息增益和杂质测量</li><li id="e243" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated">分类树</li><li id="2fe6" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated">回归树</li></ul><p id="d039" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们开始吧！</p><blockquote class="mi mj mk"><p id="a64c" class="ki kj ml kk b kl km ju kn ko kp jx kq mm ks kt ku mn kw kx ky mo la lb lc ld im bi translated">本教程改编自 Next Tech 的<strong class="kk iu"> Python 机器学习</strong>系列，带你从 0 到 100 的机器学习和深度学习算法。它包括一个浏览器内沙盒环境，预装了所有必要的软件和库，以及使用公共数据集的项目。这里可以开始<a class="ae mp" href="https://c.next.tech/2E7k6Dy" rel="noopener ugc nofollow" target="_blank">！</a></p></blockquote></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="8ba6" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">决策树的基础</h1><p id="0fa9" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">决策树是通过<strong class="kk iu">递归划分</strong>构建的——从根节点(称为第一个<strong class="kk iu">父节点</strong>)开始，每个节点可以拆分成左右<strong class="kk iu">子节点</strong>。然后，这些节点可以被进一步拆分，并且它们自己成为其结果子节点的父节点。</p><p id="30a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，看上面的图片，根节点是<code class="fe lq lr ls lt b">Work to do?</code>，并根据是否有工作要做而分成子节点<code class="fe lq lr ls lt b">Stay in</code>和<code class="fe lq lr ls lt b">Outlook</code>。<code class="fe lq lr ls lt b">Outlook</code>节点进一步分成三个子节点。</p><p id="ea21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么，我们如何知道每个节点的最佳分裂点是什么呢？</p><p id="d7cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从根开始，数据在导致最大<strong class="kk iu">信息增益</strong> ( <strong class="kk iu"> IG </strong>)的特征上被分割(下面更详细地解释)。在迭代过程中，我们然后在每个<strong class="kk iu">子节点</strong>处重复该分裂过程，直到叶子是纯的——即，每个节点处的样本都属于同一类。</p><p id="10b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在实践中，这会导致树非常深，有很多节点，这很容易导致过度拟合。因此，我们通常希望<strong class="kk iu">通过设置树的最大深度来修剪</strong>树。</p><h1 id="cc94" class="mx my it bd mz na nu nc nd ne nv ng nh jz nw ka nj kc nx kd nl kf ny kg nn no bi translated">最大化信息增益</h1><p id="66a2" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">为了在最具信息性的特征处分割节点，我们需要定义一个目标函数，我们希望通过树学习算法来优化该目标函数。这里，我们的目标函数是最大化每次分裂的信息增益，我们定义如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/1cb6543fcba5bf02e6fdfc6489ba1e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*CHsongM23EQUzvzzcWa2Pg.png"/></div></figure><p id="8fc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<em class="ml"> f </em>是执行拆分的特征，<em class="ml"> Dp </em>，<em class="ml"> Dleft </em>和<em class="ml"> Dright </em>是父节点和子节点的数据集，<em class="ml"> I </em>是<strong class="kk iu">杂质度量</strong>，<em class="ml"> Np </em>是父节点的样本总数，<em class="ml"> Nleft </em>和<em class="ml"> Nright </em>是</p><p id="d92a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在下面的例子中更详细地讨论分类和回归决策树的杂质度量。但就目前而言，只要明白信息增益简单来说就是父节点杂质和子节点杂质之和的差——子节点杂质越低，信息增益越大。</p><p id="f8c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，上面的等式是针对二元决策树的——每个父节点只被分成两个子节点。如果你有一个有多个节点的决策树，你可以简单地将所有节点的杂质相加。</p><h1 id="4cbe" class="mx my it bd mz na nu nc nd ne nv ng nh jz nw ka nj kc nx kd nl kf ny kg nn no bi translated">分类树</h1><p id="2f33" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">我们先来说说分类决策树(又称<strong class="kk iu">分类树</strong>)。对于这个例子，我们将使用机器学习领域的经典产品<a class="ae mp" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank"> <em class="ml"> Iris </em> </a>数据集。它包含了来自三个不同物种<em class="ml"> Setosa </em>、<em class="ml"> Versicolor </em>和<em class="ml"> Virginica </em>的 150 朵<em class="ml">鸢尾</em>花的尺寸。这些将是我们的<strong class="kk iu">目标</strong>。我们的目标是预测一朵鸢尾花属于哪一类。以厘米为单位的花瓣长度和宽度存储为列，我们也称之为数据集的<strong class="kk iu">特征</strong>。</p><p id="7b49" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先导入数据集，并将特征指定为<code class="fe lq lr ls lt b">X</code>，将目标指定为<code class="fe lq lr ls lt b">y</code>:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="db3d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<code class="fe lq lr ls lt b">scikit-learn</code>，我们现在将训练一个最大深度为 4 的决策树。代码如下:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="a28e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，我们将<code class="fe lq lr ls lt b">criterion</code>设置为“<strong class="kk iu">熵</strong>”。这一标准被称为杂质测量(在前一节中提到)。在分类中，熵是最常见的杂质测量或分裂标准。其定义如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/97306efc66fcea72f5f8996ccfa34920.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*TrGRS0Y7nokqAggvbdEClA.png"/></div></figure><p id="f792" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<em class="ml"> p(i|t) </em>是属于特定节点<em class="ml"> t </em>的类别<em class="ml"> c </em>的样本的比例。因此，如果一个节点上的所有样本都属于同一个类别，则熵为 0，如果我们具有均匀的类别分布，则熵最大。</p><p id="6b83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了更直观地理解熵，让我们为类别 1 的概率范围[0，1]绘制杂质指数。代码如下:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi od"><img src="../Images/d209b57d187666d07b4cb672b9dc5284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8BIEFvLcFv6e2WDC_P8gMA.png"/></div></div></figure><p id="abff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以看到，如果<em class="ml"> p(i=1|t) = 1 </em>，熵就是 0。如果类以<em class="ml"> p(i=1|t) = 0.5 </em>均匀分布，熵为 1。</p><p id="312f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，回到我们的虹膜例子，我们将可视化我们训练过的分类树，看看熵是如何决定每个分裂的。</p><p id="2af7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe lq lr ls lt b">scikit-learn</code>的一个很好的特性是，它允许我们在训练后将决策树导出为一个<code class="fe lq lr ls lt b">.dot</code>文件，例如，我们可以使用<a class="ae mp" href="http://www.graphviz.org/" rel="noopener ugc nofollow" target="_blank"> GraphViz </a>将其可视化。除了 GraphViz，我们将使用一个名为<code class="fe lq lr ls lt b">pydotplus</code>的 Python 库，它具有与 GraphViz 类似的功能，允许我们将<code class="fe lq lr ls lt b">.dot</code>数据文件转换为决策树图像文件。</p><p id="d58a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以通过在终端中执行以下命令来安装<code class="fe lq lr ls lt b">pydotplus</code>和<code class="fe lq lr ls lt b">graphviz</code>:</p><pre class="lf lg lh li gt oe lt of og aw oh bi"><span id="674b" class="oi my it lt b gy oj ok l ol om">pip3 install pydotplus<br/>apt install graphviz</span></pre><p id="d9b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下代码将创建一个 PNG 格式的决策树图像:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi on"><img src="../Images/4c3c73913cbe58c308b373a7d1e2b43a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z4zWVweDtHJzq4Ky5eOAHg.png"/></div></div><figcaption class="oo op gj gh gi oq or bd b be z dk">tree.png</figcaption></figure><p id="4aac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看保存在图像文件<code class="fe lq lr ls lt b">tree.png</code>中的结果决策树图，我们现在可以很好地追溯决策树从我们的训练数据集确定的分裂。我们从根处的 150 个样本开始，使用<strong class="kk iu">花瓣宽度</strong>截止值≤ 1.75 cm，将它们分成 50 个和 100 个样本的两个子节点。第一次拆分后，我们可以看到左边的子节点已经是纯的，只包含来自<code class="fe lq lr ls lt b">setosa</code>类的样本(熵= 0)。右侧的进一步分割用于从<code class="fe lq lr ls lt b">versicolor</code>和<code class="fe lq lr ls lt b">virginica</code>类别中分离样本。</p><p id="97a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看最终的熵，我们看到深度为 4 的决策树在分离花类方面做得非常好。</p><h1 id="6158" class="mx my it bd mz na nu nc nd ne nv ng nh jz nw ka nj kc nx kd nl kf ny kg nn no bi translated">回归树</h1><p id="29af" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">我们将使用<a class="ae mp" href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html" rel="noopener ugc nofollow" target="_blank"> <em class="ml">波士顿住房</em> </a>数据集作为我们的回归示例。这是另一个非常受欢迎的数据集，包含波士顿郊区的房屋信息。共有 506 个样本和 14 个属性。出于简单和直观的目的，我们将只使用两个目标值— <code class="fe lq lr ls lt b">MEDV</code>(以千美元计的自有住房的中值)和<code class="fe lq lr ls lt b">LSTAT</code>(人口中较低地位的百分比)作为特征。</p><p id="520f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先将必要的属性从<code class="fe lq lr ls lt b">scikit-learn</code>导入到<code class="fe lq lr ls lt b">pandas</code>数据框架中。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi os"><img src="../Images/2d2afe3a16fab2dd23b3fb26354bf4c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*1pSpTGA6A1rY4WCgIBUm0g.png"/></div></figure><p id="8e8f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用在<code class="fe lq lr ls lt b">scikit-learn</code>中实现的<code class="fe lq lr ls lt b">DecisionTreeRegressor</code>来训练一个回归树:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="b728" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我们的<code class="fe lq lr ls lt b">criterion</code>与我们用于分类树的不同。熵作为杂质的量度是分类的有用标准。然而，为了使用决策树进行回归，我们需要适用于连续变量的杂质度量，因此我们使用子节点的<strong class="kk iu">加权均方误差</strong> ( <strong class="kk iu"> MSE </strong>)来定义杂质度量:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/eff64c7e1477d4839403bee87a93fc98.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*6wRtInhGhxoX_Y7W9OR14Q.png"/></div></figure><p id="b330" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<em class="ml"> Nt </em>为节点<em class="ml"> t </em>的训练样本数，<em class="ml"> Dt </em>为节点<em class="ml"> t </em>的训练子集，<em class="ml"> y(i) </em>为真实目标值，<em class="ml"> ŷt </em>为预测目标值(样本均值):</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/51593def1b80dda6baaf037be9718ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*DylF2sjY6k3Cw7HE8vtYrA.png"/></div></figure><p id="1275" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们对<code class="fe lq lr ls lt b">MEDV</code>和<code class="fe lq lr ls lt b">LSTAT</code>之间的关系进行建模，看看回归树的直线拟合看起来像什么:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/20ef8c0192077d88f4a059f6fa1b0868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*Edq1ZIOo26Iqp3CuUTWotA.png"/></div></figure><p id="933d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们在结果图中看到的，深度为 3 的决策树捕捉到了数据的总体趋势。</p><p id="bdd4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这个关于决策树的教程！我们讨论了决策树的基本概念，最小化杂质的算法，以及如何为分类和回归构建决策树。</p><p id="cc90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在实践中，知道如何为树的深度选择一个合适的值以避免数据过拟合或欠拟合是很重要的。了解如何组合决策树以形成集合<strong class="kk iu">随机森林</strong>也很有用，因为由于随机性，它通常比单个决策树具有更好的泛化性能，这有助于减少模型的方差。它对数据集中的异常值也不太敏感，并且不需要太多的参数调整。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="bbcb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ml">我们在我们的</em> <strong class="kk iu"> <em class="ml"> Python 机器学习</em> </strong> <em class="ml">系列中涵盖了这些技术，以及其他机器学习模型，如感知器、Adaline、线性和多项式回归、逻辑回归、支持向量机、核支持向量机、k 近邻、情感分析模型、k 均值聚类、DBSCAN、卷积神经网络和递归神经网络。</em></p><p id="5b18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ml">我们还关注其他主题，如正则化、数据处理、特征选择和提取、降维、模型评估、集成学习技术以及部署机器学习模型。</em></p><p id="f382" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ml">这里</em>  <em class="ml">可以入门</em> <a class="ae mp" href="https://c.next.tech/2E7k6Dy" rel="noopener ugc nofollow" target="_blank"> <em class="ml">！</em></a></p></div></div>    
</body>
</html>