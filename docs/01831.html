<html>
<head>
<title>Language Models: N-Gram</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语言模型:N 元语法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9?source=collection_archive---------1-----------------------#2019-03-26">https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9?source=collection_archive---------1-----------------------#2019-03-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/41c42b5141fd233151e7a62a925c4349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FxnKedQ8ahuJhvFI_QXNXA.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@erikeae?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Erik Eastman</a> on <a class="ae jd" href="https://unsplash.com/search/photos/characters?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="098f" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph">介绍</h2><div class=""/><div class=""><h2 id="d345" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">统计语言建模的一个步骤</h2></div><h1 id="7662" class="le lf jg bd lg lh li lj lk ll lm ln lo kv lp kw lq ky lr kz ls lb lt lc lu lv bi translated">介绍</h1><p id="3e9b" class="pw-post-body-paragraph lw lx jg ly b lz ma kq mb mc md kt me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">统计语言模型本质上是一种为单词序列分配概率的模型。在本文中，我们将了解为句子和单词序列分配概率的最简单模型，即<strong class="ly jq"> <em class="ms"> n 元语法</em> </strong></p><p id="2f82" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">你可以把一个 N-gram 看作是 N 个单词的序列，根据这个概念，一个 2-gram(或二元模型)是两个单词的序列，如“请转”、“转你的”或“你的作业”，而一个 3-gram(或三元模型)是三个单词的序列，如“请转你的”或“转你的作业”</p><h1 id="89c1" class="le lf jg bd lg lh li lj lk ll lm ln lo kv lp kw lq ky lr kz ls lb lt lc lu lv bi translated">直觉公式</h1><p id="b0ee" class="pw-post-body-paragraph lw lx jg ly b lz ma kq mb mc md kt me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">先说方程 P(w|h)，给定一些历史，单词<em class="ms"> w </em>的概率，<em class="ms"> h </em>。举个例子，</p><figure class="mz na nb nc gt is gh gi paragraph-image"><div class="gh gi my"><img src="../Images/872bd4e187e26625269cb8427729c771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*NZKf7f0Mg3yGfSp7-vir3A.png"/></div></figure><blockquote class="nd ne nf"><p id="9812" class="lw lx ms ly b lz mt kq mb mc mu kt me ng mv mh mi nh mw ml mm ni mx mp mq mr ij bi translated">在这里，水是如此的透明</p></blockquote><p id="06e1" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">而且，估算上述概率函数的一种方法是通过相对频率计数法，在这里你会取一个相当大的语料库，统计你看到<strong class="ly jq"> <em class="ms">的次数，它的水是如此透明以至于</em> </strong>，然后统计它后面是<strong class="ly jq"><em class="ms"/></strong>的次数。换句话说，你在回答这个问题:</p><blockquote class="nd ne nf"><p id="17bc" class="lw lx ms ly b lz mt kq mb mc mu kt me ng mv mh mi nh mw ml mm ni mx mp mq mr ij bi translated">在你看到历史 h 的次数中，单词 w 跟随了多少次</p></blockquote><figure class="mz na nb nc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/36bc9c475ddde5ef85934f661b972524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aTyHzvyMHMVagA4yuSej_A.png"/></div></div></figure><p id="c32d" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">现在，你可以想象在整个语料库上执行这个是不可行的；尤其是它的尺寸很大。</p><p id="d32e" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">这一缺点和使用链式法则分解概率函数的方式充当了 N-gram 模型的基本直觉。在这里，你不是使用整个语料库来计算概率，而是用几个历史单词来近似计算概率</p><h1 id="bb2e" class="le lf jg bd lg lh li lj lk ll lm ln lo kv lp kw lq ky lr kz ls lb lt lc lu lv bi translated">二元模型</h1><p id="c36c" class="pw-post-body-paragraph lw lx jg ly b lz ma kq mb mc md kt me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">顾名思义，二元模型通过仅使用一个前面单词的条件概率来近似给定所有前面单词的单词的概率。换句话说，你用概率来近似它:P(the | that)</p><p id="2757" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">因此，当您使用二元模型来预测下一个单词的条件概率时，您会得出以下近似值:</p><figure class="mz na nb nc gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/97232faf5651e3666ffed0add7119caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/0*5IW1uofD_oWCISB4.png"/></div></figure><blockquote class="nd ne nf"><p id="4018" class="lw lx ms ly b lz mt kq mb mc mu kt me ng mv mh mi nh mw ml mm ni mx mp mq mr ij bi translated">这种一个词的概率只取决于前一个词的假设也被称为<strong class="ly jq">马尔可夫</strong>假设。</p><p id="8b82" class="lw lx ms ly b lz mt kq mb mc mu kt me ng mv mh mi nh mw ml mm ni mx mp mq mr ij bi translated">马尔可夫模型是一类概率模型，它假设我们可以预测某个未来单位的概率，而不用考虑太远的过去。</p></blockquote><p id="b070" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">您可以进一步将二元模型推广到<strong class="ly jq">三元模型</strong>，它查看过去的两个单词，因此可以进一步推广到<strong class="ly jq"> N 元模型</strong></p><h1 id="9185" class="le lf jg bd lg lh li lj lk ll lm ln lo kv lp kw lq ky lr kz ls lb lt lc lu lv bi translated">概率估计</h1><p id="3cec" class="pw-post-body-paragraph lw lx jg ly b lz ma kq mb mc md kt me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">现在，我们理解了 N 元模型的基础，你会想，我们如何估计概率函数。最直接和直观的方法之一是<strong class="ly jq">最大似然估计(MLE) </strong></p><p id="ac7c" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">例如，给定前一个单词<strong class="ly jq"> <em class="ms"> x </em> </strong>，计算单词<strong class="ly jq"> <em class="ms"> y </em> </strong>的特定二元模型概率，您可以确定二元模型 C(xy)的计数，并通过共享相同首词<strong class="ly jq"> <em class="ms"> x </em> </strong>的所有二元模型的总和对其进行归一化。</p><h1 id="2a6c" class="le lf jg bd lg lh li lj lk ll lm ln lo kv lp kw lq ky lr kz ls lb lt lc lu lv bi translated">挑战</h1><p id="3531" class="pw-post-body-paragraph lw lx jg ly b lz ma kq mb mc md kt me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">当然，每一种建模方法和评估方法都存在挑战。让我们看看影响 N-gram 模型的关键因素，以及 MLE 的使用</p><p id="8613" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated"><strong class="ly jq">对训练语料的敏感度</strong></p><p id="acf8" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">与许多统计模型一样，N-gram 模型非常依赖于训练语料库。因此，<strong class="ly jq">，</strong>概率通常编码关于给定训练语料库的特定事实。此外，N 元模型的性能随着 N 值的变化而变化。</p><p id="2332" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">此外，你可能有一个语言任务，其中你知道所有可能出现的单词，因此我们提前知道词汇量 V。封闭词汇表假设没有未知单词，这在实际场景中不太可能。</p><p id="400e" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated"><strong class="ly jq">平滑</strong></p><p id="cfbe" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">MLE 方法的一个显著问题是数据稀疏。也就是说，任何出现足够多次的 N-gram 都可能对其概率有一个合理的估计。但是因为任何语料库都是有限的，一些完全可以接受的英语单词序列必然会从其中缺失。</p><p id="40de" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">因此，任何训练语料库的 N-gram 矩阵必然具有大量假定的“零概率 N-gram”的情况</p><h1 id="b14a" class="le lf jg bd lg lh li lj lk ll lm ln lo kv lp kw lq ky lr kz ls lb lt lc lu lv bi translated"><strong class="ak">来源:</strong></h1><p id="5ed0" class="pw-post-body-paragraph lw lx jg ly b lz ma kq mb mc md kt me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">[1]章节草稿— | Stanford Lagunita。<a class="ae jd" href="https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf" rel="noopener ugc nofollow" target="_blank">https://lag unita . Stanford . edu/c4x/Engineering/CS-224n/asset/sl P4 . pdf</a></p><p id="96a4" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">[2]语音和语言处理:自然语言处理、计算语言学和语音导论</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><p id="31ad" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">感谢阅读。<em class="ms">如果您有任何反馈，请对本文发表评论，在</em><a class="ae jd" href="https://www.linkedin.com/in/shashankkapadia/" rel="noopener ugc nofollow" target="_blank"><em class="ms">LinkedIn</em></a><em class="ms">上给我发消息，或者给我发电子邮件(shmkapadia[at]gmail.com) </em></p><p id="07de" class="pw-post-body-paragraph lw lx jg ly b lz mt kq mb mc mu kt me mf mv mh mi mj mw ml mm mn mx mp mq mr ij bi translated">如果你喜欢这篇文章，请访问我的其他文章</p><div class="ip iq gp gr ir ns"><a rel="noopener follow" target="_blank" href="/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd jq gy z fp nx fr fs ny fu fw jp bi translated">Python 中的主题建模:潜在狄利克雷分配(LDA)</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">如何开始使用 Python 中的 LDA 进行主题建模</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og ix ns"/></div></div></a></div><div class="ip iq gp gr ir ns"><a rel="noopener follow" target="_blank" href="/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd jq gy z fp nx fr fs ny fu fw jp bi translated">评估主题模型:潜在狄利克雷分配(LDA)</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">构建可解释主题模型的分步指南</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="oh l od oe of ob og ix ns"/></div></div></a></div><div class="ip iq gp gr ir ns"><a rel="noopener follow" target="_blank" href="/building-blocks-text-pre-processing-641cae8ba3bf"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd jq gy z fp nx fr fs ny fu fw jp bi translated">构建块:文本预处理</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">本文是关于自然语言处理的后续文章的第二篇。这一系列…的目的</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="oi l od oe of ob og ix ns"/></div></div></a></div></div></div>    
</body>
</html>