<html>
<head>
<title>Review: MultiPathNet / MultiPath / MPN — 1st Runner Up in 2015 COCO Detection &amp; Segmentation (Object Detection / Instance Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">点评:多路径网络/多路径/MPN—2015 年 COCO 检测和分割(对象检测/实例分割)亚军</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=collection_archive---------24-----------------------#2019-01-03">https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=collection_archive---------24-----------------------#2019-01-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bcae" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">多个网络层，视网膜中央凹结构和整体损失，信息在网络中沿多条路径流动</h2></div><p id="f1e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事中，<strong class="kh ir">脸书 AI 研究</strong>的<strong class="kh ir">MultiPath net/MultiPath/MPN</strong>进行了回顾。这种方法在<a class="ae lk" href="https://github.com/facebookresearch/multipathnet" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中被命名为 MultiPathNet，在本文中称为 MultiPath。在<a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a>中它也被称为 MPN。<strong class="kh ir">对</strong>做了三处修改，以改进<a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>:</p><ol class=""><li id="dbfc" class="ll lm iq kh b ki kj kl km ko ln ks lo kw lp la lq lr ls lt bi translated"><strong class="kh ir">视网膜中央凹结构</strong>以多种物体分辨率利用物体环境。</li><li id="7b38" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la lq lr ls lt bi translated"><strong class="kh ir">跳过连接</strong>，该连接允许探测器访问多个网络层的功能。</li><li id="1f59" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la lq lr ls lt bi translated"><strong class="kh ir">积分损失函数</strong>和改善定位的相应网络调整。</li></ol><p id="29a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再加上<a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a>天体提议，组合系统<strong class="kh ir">将</strong>的结果提高到超过<strong class="kh ir">基线</strong> <a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener"> <strong class="kh ir">快速 R-CNN </strong> </a>探测器，选择性搜索<strong class="kh ir">总体提高 66% </strong>，小天体提高 4 倍<strong class="kh ir">。<strong class="kh ir">它在 COCO 2015 检测和细分挑战中均获得第二名。</strong>发表在<strong class="kh ir"> 2016 BMVC </strong>上，被<strong class="kh ir"> 100 多次引用</strong>。(<a class="lz ma ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----ea9741e7c413--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</strong></p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="d9ef" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">涵盖哪些内容</h1><ol class=""><li id="41be" class="ll lm iq kh b ki na kl nb ko nc ks nd kw ne la lq lr ls lt bi translated"><strong class="kh ir">视网膜中央凹结构</strong></li><li id="e74b" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la lq lr ls lt bi translated"><strong class="kh ir">跳过连接</strong></li><li id="be49" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la lq lr ls lt bi translated"><strong class="kh ir">积分损失函数</strong></li><li id="b08b" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la lq lr ls lt bi translated"><strong class="kh ir">消融研究</strong></li><li id="8f12" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la lq lr ls lt bi translated"><strong class="kh ir">结果</strong></li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nf"><img src="../Images/5e80b9dd8d991e640b8f369f2c704d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hmp86FwcA_q276llPiU3EA.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">MultiPath Architecture</strong></figcaption></figure><h1 id="d878" class="mi mj iq bd mk ml nw mn mo mp nx mr ms jw ny jx mu jz nz ka mw kc oa kd my mz bi translated"><strong class="ak"> 1。视网膜中央凹结构</strong></h1><p id="5e89" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko ob kq kr ks oc ku kv kw od ky kz la ij bi translated">除了原来的<strong class="kh ir">1×</strong>ROI 集中区域尺寸外，还开发了如上图所示的<strong class="kh ir">额外的 1.5×、2×和 4× ROI 集中区域</strong>。这提供了<strong class="kh ir">不同大小的视网膜中央凹区域</strong>。</p><p id="da26" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这四个 ROI 汇集区域经过完全连接的(FC)层(FC5 和 FC6)，然后<strong class="kh ir">连接成单个长特征向量(4096×4) </strong>。</p><h1 id="e0f5" class="mi mj iq bd mk ml nw mn mo mp nx mr ms jw ny jx mu jz nz ka mw kc oa kd my mz bi translated">2.跳过连接</h1><p id="1783" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko ob kq kr ks oc ku kv kw od ky kz la ij bi translated">在使用<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGG16 </a>作为主干的原始<a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>中，只有 conv5 层用于 ROI 合并。在此图层中，要素的缩减采样系数为 16。然而，<strong class="kh ir"> 40%的 COCO 对象的面积小于 32×32 像素，20%小于 16×16 像素</strong>，因此这些对象将在此阶段分别被<strong class="kh ir">缩减采样为 2×2 或 1×1。RoI-pooling 会将它们上采样到 7×7，但是由于对特征的 16 倍下采样，大多数空间信息将会丢失。</strong></p><p id="2562" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，由<a class="ae lk" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">或</a>建议的跳过池在 conv3 执行，con4 和 conv5 也用于 ROI 池。这个想法是<strong class="kh ir">早期层通常比后期层</strong>有更大的值，这在<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener"> ParseNet </a>中提到过。因此，在拼接之前，每个汇集的 ROI 被<strong class="kh ir"> L2 归一化</strong> <strong class="kh ir">并通过经验确定的比例重新按比例放大</strong>。之后，执行<strong class="kh ir"> 1×1 卷积</strong>以<strong class="kh ir">降低维度</strong>以适应分类器输入维度。</p><p id="6d15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些跳跃连接使分类器能够以多种分辨率访问来自要素的信息。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="b7a7" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">3.<strong class="ak">积分损失函数</strong></h1><p id="0fd6" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko ob kq kr ks oc ku kv kw od ky kz la ij bi translated">在 PASCAL 和 ImageNet 数据集中，评分标准仅关注 50 以上的交集(IoU ),即 AP⁵⁰.然而，COCO 数据集在从 50 到 95 的 IoU 范围内评估 AP。</p><p id="d83f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在最初的<a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>中，损失函数只关注优化 AP⁵⁰:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi gj"><img src="../Images/74c7dc6db6eef4b6a986c2217a78175d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FeSC0xxy-dcLgumMiTGF_g.png"/></div></div></figure><p id="9f7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一项<em class="oe"> Lcls </em>是分类对数损失，而第二项<em class="oe"> Lloc </em>是边界框定位损失。<em class="oe"> k* </em> ≥1 仅当 IoU 大于 50。否则<em class="oe"> k* </em> =0，忽略第二项损失。</p><p id="855f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总的来说，<em class="oe"> Lcls </em>被修改以适应 COCO 评估指标:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/80edb6b30c7d675138629a92bc70c0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*VcFZYDVbWkFGPJ5bDPXZ4Q.png"/></div></figure><p id="f8ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述等式将积分近似为一个和，其中<em class="oe"> du </em> = 5。</p><p id="0868" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具体来说，只考虑 6 个 IoU 阈值，从 50、55、…、到 75。修改后的损失变成:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi og"><img src="../Images/7413edb11ba3198d8a56c0371fa250cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k1ohofl_7KLmmomzTJjV8A.png"/></div></div></figure><p id="2c8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="oe"> n </em> =6，<em class="oe"> u </em>从 50，55，…，到 75。在培训期间，随着<em class="oe"> u </em>的增加，与实际情况相重叠的建议会减少。因此，它被限制为<em class="oe"> u </em> ≤75，否则，建议包含的用于训练的总正样本太少。</p><p id="11c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个积分损失函数显示在上图的右边。</p><h2 id="b8b5" class="oh mj iq bd mk oi oj dn mo ok ol dp ms ko om on mu ks oo op mw kw oq or my os bi translated">一些训练细节</h2><p id="88ff" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko ob kq kr ks oc ku kv kw od ky kz la ij bi translated">在训练期间，每批有 4 个图像，每个图像有 64 个对象提议。在 4 个 NVIDIA Titan X GPUs 上大概需要 3 天。使用每个图像 30，1000 个建议的非最大抑制阈值。并且没有重量衰减。该网络需要 150 毫秒来计算特征，350 毫秒来评估视网膜中央凹区域，因此每个 COCO 图像总共需要 500 毫秒。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="18ed" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">4.消融研究</h1><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ot"><img src="../Images/fe9fea6ec4b282562d91998ca6fff088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ueMiGwtV-yQeAouD0jSPOA.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">Left: Model improvements of our MultiPath network, Right: 4-region foveal setup versus the 10 regions used in multiregion</strong></figcaption></figure><ul class=""><li id="5932" class="ll lm iq kh b ki kj kl km ko ln ks lo kw lp la ou lr ls lt bi translated"><strong class="kh ir">左</strong>:采用视网膜中央凹结构和跳跃连接，AP⁵⁰.获得 46.4%的 mAP 积分损失后，AP⁵⁰的平均积分下降到了 44.8%。这是因为积分损失是专门为 COCO 评估指标设计的。因此，我们可以看到<strong class="kh ir">使用积分损失后，总 AP 从 27.0 提高到 27.9</strong>。</li><li id="41ae" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated"><strong class="kh ir">右</strong> : multiregion [9]在每个对象周围使用十个具有不同裁剪的上下文区域。在多路径中，仅使用 4 个视网膜中央凹区域。在没有积分损失的情况下，多径对于 AP⁵⁰.具有 45.2%的 mAP 在积分损失的情况下，获得了 26.9%的总 mAP。多路径总是比多区域好。</li></ul><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ov"><img src="../Images/a46b2d4ec1e6e6da0e8154bd8211f331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UAiXIihqr0IkdSuhAM0xPA.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">Left: MultiPath with different IoU thresholds and with Integral loss, Right: Integral loss with different number of u.</strong></figcaption></figure><ul class=""><li id="082d" class="ll lm iq kh b ki kj kl km ko ln ks lo kw lp la ou lr ls lt bi translated"><strong class="kh ir">左</strong>:每个标准模型在用于训练的阈值下表现最佳，而使用积分损失在所有设置下产生良好的结果。</li><li id="6080" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated"><strong class="kh ir">右</strong>:积分损失达到 6 头最佳 AP。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="b4fe" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">5.结果</h1><h2 id="45a9" class="oh mj iq bd mk oi oj dn mo ok ol dp ms ko om on mu ks oo op mw kw oq or my os bi translated">5.1.区域提议技术</h2><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ow"><img src="../Images/05f2b657d8af44c71fd6dd888e0b9b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ksJtvpauknOGHES00R0B2Q.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">AP⁵⁰ and overall AP versus number and type of proposals.</strong></figcaption></figure><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ox"><img src="../Images/9b8220fbb115bd71f12a0811273530ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0NqwoYFP3iIE9SpZilrY1A.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">AP⁵⁰ and overall AP with different approaches. (SS: Selective Search, DM: </strong><a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">DeepMask</a><strong class="bd nv">)</strong></figcaption></figure><ul class=""><li id="e2b1" class="ll lm iq kh b ki kj kl km ko ln ks lo kw lp la ou lr ls lt bi translated">在最初的<a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>中，第一步是使用选择性搜索(SelSearch)来生成多个区域提议。对于每个提案，ROI 池在 conv5 上执行，并通过 FC 层进行分类和定位。</li><li id="bd75" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated">因此，求婚技巧至关重要。</li><li id="a551" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated">结果是每张图片有大约 400 个<a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度蒙版</a>提议。</li><li id="61ef" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated">仅使用 50 个<a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度屏蔽</a>建议就可以匹配 2000 个选择性搜索建议的准确性。</li></ul><h2 id="7de3" class="oh mj iq bd mk oi oj dn mo ok ol dp ms ko om on mu ks oo op mw kw oq or my os bi translated">5.2.其他技术</h2><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/bf32172f600af6f25eeb6217d18e8a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*NAVGbs7IDw1HMjrEgp_wyg.png"/></div></figure><ul class=""><li id="64ab" class="ll lm iq kh b ki kj kl km ko ln ks lo kw lp la ou lr ls lt bi translated"><strong class="kh ir"> trainval </strong>:追加训练用 COCO 验证数据。</li><li id="3a81" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated"><strong class="kh ir"> hflip </strong>:水平翻转，平均结果。</li><li id="fa71" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated"><strong class="kh ir"> FMP </strong>:分数最大汇集，简而言之，是多个 ROI 汇集操作，具有扰动的汇集参数并平均 softmax 输出。</li><li id="d5ca" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated"><strong class="kh ir">组合</strong>:采用 6 模组合。</li><li id="f6ee" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated">通过以上 4 项技术，AP⁵⁰和整体 AP 都有了很大的提高。</li></ul><h2 id="7981" class="oh mj iq bd mk oi oj dn mo ok ol dp ms ko om on mu ks oo op mw kw oq or my os bi translated">5.3.COCO 2015 检测和分割</h2><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi oz"><img src="../Images/310af65d9ba9081dd31b5d89ddba3fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ov06p_6x3UrTbVm-ZhW20w.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">Top: Segmentation Results, Bottom: Detection Results</strong></figcaption></figure><ul class=""><li id="ce9e" class="ll lm iq kh b ki kj kl km ko ln ks lo kw lp la ou lr ls lt bi translated">多路径在检测和分段挑战中位居第二。</li><li id="6411" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated">小对象的整体 AP 提高了 4 倍，AP⁵⁰提高了 82%。</li><li id="adfe" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la ou lr ls lt bi translated">如果使用<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>主干网，AP 可以进一步提高。</li></ul><h2 id="91d5" class="oh mj iq bd mk oi oj dn mo ok ol dp ms ko om on mu ks oo op mw kw oq or my os bi translated">5.4.定性结果</h2><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi pa"><img src="../Images/3a3a6d750ad514a1ef21a9aa8ef7ec3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9obkYyJ0UX-d60-n6PhV_A.png"/></div></div></figure><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi pb"><img src="../Images/ce1f07958dc52bacffa9ec45c8e7b0fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Gy978-L9efX33B-zq0mug.png"/></div></div></figure><p id="ce0b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然存在遗漏对象和误报，但其中许多都相当不错。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="c2d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">论文和<a class="ae lk" href="http://cocodataset.org/#detection-leaderboard" rel="noopener ugc nofollow" target="_blank"> COCO 检测排行榜</a>中的结果略有不同。但是<a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a>中的结果与排行榜中的结果相同。(我不确定，但是)也许，在最后，<a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a>，一个改进的<a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a>，被用作具有多路径提交的区域提议。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="5963" class="oh mj iq bd mk oi oj dn mo ok ol dp ms ko om on mu ks oo op mw kw oq or my os bi translated">参考</h2><p id="b824" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko ob kq kr ks oc ku kv kw od ky kz la ij bi translated">【2016 BMVC】【多路径/MPN】<br/><a class="ae lk" href="https://arxiv.org/abs/1604.02135" rel="noopener ugc nofollow" target="_blank">用于物体检测的多路径网络</a></p><p id="e905" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">固态硬盘</a><a class="ae lk" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a><a class="ae lk" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约尔</a></p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong>[<a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a>]</p><p id="bd29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割<br/></strong><a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度遮罩</a><a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度遮罩</a></p></div></div>    
</body>
</html>