# åˆ†ç±»çš„è¦ç´ é€‰æ‹©æŠ€æœ¯åŠå…¶åº”ç”¨çš„ Python æŠ€å·§

> åŸæ–‡ï¼š<https://towardsdatascience.com/feature-selection-techniques-for-classification-and-python-tips-for-their-application-10c0ddd7918b?source=collection_archive---------1----------------------->

## å…³äºå¦‚ä½•ä½¿ç”¨æœ€å¸¸è§çš„ç‰¹å¾é€‰æ‹©æŠ€æœ¯è§£å†³åˆ†ç±»é—®é¢˜çš„æ•™ç¨‹

![](img/b3d1e7fa4cdd17fed9cf5b7c00ea9278.png)

é€‰æ‹©ä½¿ç”¨å“ªäº›åŠŸèƒ½æ˜¯ä»»ä½•æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­çš„å…³é”®ä¸€æ­¥ï¼Œä¹Ÿæ˜¯æ•°æ®ç§‘å­¦å®¶æ—¥å¸¸å·¥ä½œä¸­çš„ä¸€é¡¹ç»å¸¸æ€§ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å›é¡¾äº†åˆ†ç±»é—®é¢˜ä¸­æœ€å¸¸è§çš„ç‰¹å¾é€‰æ‹©æŠ€æœ¯ï¼Œå°†å®ƒä»¬åˆ†ä¸º 6 å¤§ç±»ã€‚æˆ‘æä¾›äº†å¦‚ä½•åœ¨æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­ä½¿ç”¨å®ƒä»¬çš„æŠ€å·§ï¼Œå¹¶å°½å¯èƒ½ç”¨ Python ä»£ç ç»™å‡ºä¾‹å­ã€‚ä½ å‡†å¤‡å¥½äº†å—ï¼Ÿ

# TLï¼›ç¾éš¾æ¢å¤-æ±‡æ€»è¡¨

ä¸‹è¡¨æ€»ç»“äº†ä¸»è¦æ–¹æ³•ï¼Œå¹¶åœ¨ä»¥ä¸‹éƒ¨åˆ†è¿›è¡Œäº†è®¨è®ºã€‚

![](img/d99f89869ed4ffd35b7fc4ec8417d390.png)

# ä»€ä¹ˆæ˜¯ç‰¹å¾é€‰æ‹©ï¼Œä¸ºä»€ä¹ˆæœ‰ç”¨ï¼Ÿ

æœºå™¨å­¦ä¹ ä¸­æœ€å¤§çš„ä¸¤ä¸ªé—®é¢˜æ˜¯**è¿‡æ‹Ÿåˆ**(æ‹Ÿåˆåœ¨æ•°æ®é›†ä¹‹å¤–ä¸å¯æ¦‚æ‹¬çš„æ•°æ®æ–¹é¢)å’Œ**ç»´æ•°ç¾éš¾**(é«˜ç»´æ•°æ®çš„éç›´è§‚å’Œç¨€ç–ç‰¹æ€§)ã€‚

é€šè¿‡å‡å°‘æ¨¡å‹ä¸­çš„ç‰¹å¾æ•°é‡ï¼Œå°è¯•ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œç‰¹å¾é€‰æ‹©æœ‰åŠ©äºé¿å…è¿™ä¸¤ä¸ªé—®é¢˜ã€‚è¿™æ ·åšï¼Œç‰¹æ€§é€‰æ‹©è¿˜æä¾›äº†ä¸€ä¸ªé¢å¤–çš„å¥½å¤„:**æ¨¡å‹**è§£é‡Šã€‚éšç€ç‰¹å¾çš„å‡å°‘ï¼Œè¾“å‡ºæ¨¡å‹å˜å¾—æ›´ç®€å•å’Œæ›´å®¹æ˜“è§£é‡Šï¼Œå¹¶ä¸”äººç±»æ›´æœ‰å¯èƒ½ç›¸ä¿¡æ¨¡å‹åšå‡ºçš„æœªæ¥é¢„æµ‹ã€‚

# æ— ç›‘ç£æ–¹æ³•

å‡å°‘ç‰¹å¾æ•°é‡çš„ä¸€ä¸ªç®€å•æ–¹æ³•æ˜¯å¯¹æ•°æ®åº”ç”¨**é™ç»´æŠ€æœ¯**ã€‚è¿™é€šå¸¸ä»¥æ— äººç›‘ç£çš„æ–¹å¼å®Œæˆï¼Œå³ä¸ä½¿ç”¨æ ‡ç­¾æœ¬èº«ã€‚

é™ç»´å®é™…ä¸Šå¹¶ä¸é€‰æ‹©ç‰¹å¾çš„å­é›†ï¼Œè€Œæ˜¯åœ¨ä½ç»´ç©ºé—´ä¸­äº§ç”Ÿä¸€ç»„æ–°çš„ç‰¹å¾ã€‚è¿™ä¸ªæ–°çš„é›†åˆå¯ä»¥ç”¨äºåˆ†ç±»è¿‡ç¨‹æœ¬èº«ã€‚

ä»¥ä¸‹ç¤ºä¾‹ä½¿ç”¨é™ç»´åçš„ç‰¹å¾è¿›è¡Œåˆ†ç±»ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒä½¿ç”¨ä¸»æˆåˆ†åˆ†æ**(PCA)çš„å‰ 2 ä¸ªæˆåˆ†ä½œä¸ºæ–°çš„ç‰¹å¾é›†ã€‚**

```
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormapimport numpy as np
h = .01
x_min, x_max = -4,4
y_min, y_max = -1.5,1.5# loading dataset
data = load_iris()
X, y = data.data, data.target# selecting first 2 components of PCA
X_pca = PCA().fit_transform(X)
X_selected = X_pca[:,:2]# training classifier and evaluating on the whole plane
clf = SVC(kernel='linear')
clf.fit(X_selected,y)
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)# Plotting
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
plt.figure(figsize=(10,5))
plt.pcolormesh(xx, yy, Z, alpha=.6,cmap=cmap_light)
plt.title('PCA - Iris dataset')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.scatter(X_pca[:,0],X_pca[:,1],c=data.target,cmap=cmap_bold)
plt.show()
```

**![](img/ce95fa2c69d5b8f99108db91828c6903.png)**

**åœ¨è¯„ä¼°ç‰¹å¾çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œé™ç»´çš„å¦ä¸€ä¸ªç”¨é€”æ˜¯ç”¨äºå¯è§†åŒ–:åœ¨è¾ƒä½ç»´åº¦çš„ç©ºé—´ä¸­ï¼Œæ›´å®¹æ˜“ä»è§†è§‰ä¸ŠéªŒè¯æ•°æ®æ˜¯å¦æ˜¯æ½œåœ¨å¯åˆ†çš„ï¼Œè¿™æœ‰åŠ©äºè®¾ç½®å¯¹åˆ†ç±»å‡†ç¡®æ€§çš„æœŸæœ›ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¯¹ç‰¹å¾çš„å­é›†æ‰§è¡Œç»´åº¦ç¼©å‡(ä¾‹å¦‚ PCA ),å¹¶æ£€æŸ¥æ ‡ç­¾å¦‚ä½•åˆ†å¸ƒåœ¨ç¼©å‡çš„ç©ºé—´ä¸­ã€‚å¦‚æœå®ƒä»¬çœ‹èµ·æ¥æ˜¯åˆ†å¼€çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜æ˜¾çš„è¿¹è±¡ï¼Œè¡¨æ˜ä½¿ç”¨è¿™ç»„ç‰¹å¾æ—¶ï¼Œé¢„æœŸä¼šæœ‰é«˜çš„åˆ†ç±»æ€§èƒ½ã€‚**

**åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œåœ¨ä¸€ä¸ª 2 ç»´çš„ç¼©å‡ç©ºé—´ä¸­ï¼Œä¸åŒçš„æ ‡ç­¾è¢«æ˜¾ç¤ºä¸ºæ˜¯ç›¸å½“å¯åˆ†çš„ã€‚è¿™è¡¨æ˜ï¼Œåœ¨è®­ç»ƒå’Œæµ‹è¯•åˆ†ç±»å™¨æ—¶ï¼Œäººä»¬å¯ä»¥æœŸå¾…é«˜æ€§èƒ½ã€‚**

```
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as pltfrom mlxtend.plotting import plot_pca_correlation_graphdata = load_iris()
X, y = data.data, data.targetplt.figure(figsize=(10,5))
X_pca = PCA().fit_transform(X)
plt.title('PCA - Iris dataset')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.scatter(X_pca[:,0],X_pca[:,1],c=data.target)
_ = plot_pca_correlation_graph(X,data.feature_names)
```

**![](img/ac5ae7ac9ea5304b194196e4e45c95e8.png)****![](img/afbc355101d608c32e6c4618cd66e03e.png)**

**é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘è¿˜ç»˜åˆ¶äº†**ç›¸å…³åœ†**ï¼Œå®ƒæ˜¾ç¤ºäº†æ¯ä¸ªåŸå§‹ç»´åº¦å’Œæ–° PCA ç»´åº¦ä¹‹é—´çš„ç›¸å…³æ€§ã€‚ç›´è§‚åœ°è¯´ï¼Œè¯¥å›¾æ˜¾ç¤ºäº†æ¯ä¸ªåŸå§‹ç‰¹å¾å¯¹æ–°åˆ›å»ºçš„ PCA æˆåˆ†çš„è´¡çŒ®ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼ŒèŠ±ç“£é•¿åº¦å’Œå®½åº¦ä¸ç¬¬ä¸€ä¸ªä¸»æˆåˆ†åˆ†æç»´åº¦é«˜åº¦ç›¸å…³ï¼Œè¼ç‰‡å®½åº¦å¯¹ç¬¬äºŒä¸ªç»´åº¦è´¡çŒ®å¾ˆå¤§ã€‚**

# **å•å˜é‡æ»¤æ³¢æ–¹æ³•**

**![](img/7e5e41f7fa200eb5108f5f35d08af28d.png)**

**è¿‡æ»¤æ–¹æ³•æ—¨åœ¨å¯¹ç‰¹å¾çš„é‡è¦æ€§è¿›è¡Œæ’åºï¼Œè€Œä¸ä½¿ç”¨ä»»ä½•ç±»å‹çš„åˆ†ç±»ç®—æ³•ã€‚**

**å•å˜é‡è¿‡æ»¤æ–¹æ³•å•ç‹¬è¯„ä¼°æ¯ä¸ªç‰¹å¾ï¼Œå¹¶ä¸”ä¸è€ƒè™‘ç‰¹å¾çš„ç›¸äº’ä½œç”¨ã€‚è¿™äº›æ–¹æ³•åŒ…æ‹¬ä¸ºæ¯ä¸ªç‰¹å¾æä¾›ä¸€ä¸ªåˆ†æ•°ï¼Œé€šå¸¸åŸºäºç»Ÿè®¡æµ‹è¯•ã€‚**

**åˆ†æ•°é€šå¸¸æˆ–è€…æµ‹é‡å› å˜é‡å’Œç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§(ä¾‹å¦‚ Chi2 å’Œç”¨äºå›å½’çš„ Pearls ç›¸å…³ç³»æ•°)ï¼Œæˆ–è€…æµ‹é‡ç»™å®šç±»åˆ«æ ‡ç­¾çš„ç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚(f æ£€éªŒå’Œ T æ£€éªŒ)ã€‚**

**åˆ†æ•°é€šå¸¸å¯¹åŸºç¡€æ•°æ®çš„ç»Ÿè®¡å±æ€§åšå‡ºå‡è®¾ã€‚ç†è§£è¿™äº›å‡è®¾å¯¹äºå†³å®šä½¿ç”¨å“ªç§æµ‹è¯•æ˜¯å¾ˆé‡è¦çš„ï¼Œå³ä½¿å…¶ä¸­ä¸€äº›å‡è®¾å¯¹äºè¿åå‡è®¾æ˜¯ç¨³å¥çš„ã€‚**

**åŸºäºç»Ÿè®¡æµ‹è¯•çš„åˆ†æ•°æä¾›äº†ä¸€ä¸ª **p å€¼**ï¼Œå¯ä»¥ç”¨æ¥æ’é™¤ä¸€äº›ç‰¹å¾ã€‚å¦‚æœ p å€¼é«˜äºæŸä¸ªé˜ˆå€¼(é€šå¸¸ä¸º 0.01 æˆ– 0.05)ï¼Œåˆ™ä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚**

**å¸¸è§æµ‹è¯•åŒ…æ‹¬:**

**![](img/7a2a3f018499cb9e3584353072ca51e6.png)**

**åŒ…`sklearn`å®ç°äº†ä¸€äº›è¿‡æ»¤æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºå¤§å¤šæ•°éƒ½æ˜¯åŸºäºç»Ÿè®¡æµ‹è¯•çš„ï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥ä½¿ç”¨ç»Ÿè®¡åŒ…(æ¯”å¦‚`statsmodels`)ã€‚**

**ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­:**

```
from sklearn.feature_selection import f_classif, chi2, mutual_info_classif
from statsmodels.stats.multicomp import pairwise_tukeyhsdfrom sklearn.datasets import load_irisdata = load_iris()
X,y = data.data, data.targetchi2_score, chi_2_p_value = chi2(X,y)
f_score, f_p_value = f_classif(X,y)
mut_info_score = mutual_info_classif(X,y)pairwise_tukeyhsd = [list(pairwise_tukeyhsd(X[:,i],y).reject) for i in range(4)]print('chi2 score        ', chi2_score)
print('chi2 p-value      ', chi_2_p_value)
print('F - score score   ', f_score)
print('F - score p-value ', f_p_value)
print('mutual info       ', mut_info_score)
print('pairwise_tukeyhsd',pairwise_tukeyhsd)Out:chi2 score         [ 10.82   3.71 116.31  67.05]
chi2 p-value       [0\.   0.16 0\.   0\.  ]
F - score score    [ 119.26   49.16 1180.16  960.01]
F - score p-value  [0\. 0\. 0\. 0.]
mutual info        [0.51 0.27 0.98 0.98]
pairwise_tukeyhsd [[True, True, True], [True, True, True], [True, True, True], [True, True, True]]
```

## **å¯¹ç‰¹å¾è¿›è¡Œåˆ†çº§çš„å¯è§†åŒ–æ–¹æ³•**

## **ç®±çº¿å›¾å’Œå°æç´å›¾**

**ç®±çº¿å›¾/å°æç´å›¾å¯èƒ½æœ‰åŠ©äºå¯è§†åŒ–ç»™å®šç±»åˆ«çš„ç‰¹å¾åˆ†å¸ƒã€‚å¯¹äº Iris æ•°æ®é›†ï¼Œä¸‹é¢æ˜¾ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹ã€‚**

**è¿™æ˜¯æœ‰ç”¨çš„ï¼Œå› ä¸ºç»Ÿè®¡æµ‹è¯•é€šå¸¸åªè¯„ä¼°è¿™ç§åˆ†å¸ƒçš„å¹³å‡å€¼ä¹‹é—´çš„å·®å¼‚ã€‚å› æ­¤ï¼Œè¿™äº›å›¾æä¾›äº†å…³äºç‰¹å¾è´¨é‡çš„æ›´å¤šä¿¡æ¯**

```
import pandas as pd
import seaborn as sns
sns.set()
df = pd.DataFrame(data.data,columns=data.feature_names)
df['target'] = data.targetdf_temp = pd.melt(df,id_vars='target',value_vars=list(df.columns)[:-1], 
                  var_name="Feature", value_name="Value")
g = sns.FacetGrid(data = df_temp, col="Feature", col_wrap=4, size=4.5,sharey = False)
g.map(sns.boxplot,"target", "Value");
g = sns.FacetGrid(data = df_temp, col="Feature", col_wrap=4, size=4.5,sharey = False)
g.map(sns.violinplot,"target", "Value");
```

**![](img/139b23735e4b1d14003bbc69ccbbe16e.png)****![](img/f728131dcfcc10716c9e271d6970e0b7.png)**

## **ç”¨ ROC æ›²çº¿è¿›è¡Œç‰¹å¾æ’åº**

**ROC æ›²çº¿å¯ç”¨äºæŒ‰é‡è¦æ€§é¡ºåºæ’åˆ—ç‰¹å¾ï¼Œè¿™ç»™å‡ºäº†æ’åˆ—ç‰¹å¾æ€§èƒ½çš„ç›´è§‚æ–¹å¼ã€‚**

**è¿™ç§æŠ€æœ¯æœ€é€‚åˆäºŒè¿›åˆ¶åˆ†ç±»ä»»åŠ¡ã€‚ä¸ºäº†åº”ç”¨äºå¤šç±»é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨**å¾®è§‚æˆ–å®è§‚**å¹³å‡å€¼æˆ–åŸºäºå¤šé‡æ¯”è¾ƒçš„æ ‡å‡†(ç±»ä¼¼äºæˆå¯¹ Tukey çš„èŒƒå›´æµ‹è¯•)ã€‚**

**ä»¥ä¸‹ç¤ºä¾‹ç»˜åˆ¶äº†å„ç§ç‰¹å¾çš„ ROC æ›²çº¿ã€‚**

```
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.metrics import auc
import numpy as np# loading dataset
data = load_iris()
X, y = data.data, data.targety_ = y == 2plt.figure(figsize=(13,7))
for col in range(X.shape[1]):
    tpr,fpr = [],[]
    for threshold in np.linspace(min(X[:,col]),max(X[:,col]),100):
        detP = X[:,col] < threshold
        tpr.append(sum(detP & y_)/sum(y_))# TP/P, aka recall
        fpr.append(sum(detP & (~y_))/sum((~y_)))# FP/N

    if auc(fpr,tpr) < .5:
        aux = tpr
        tpr = fpr
        fpr = aux
    plt.plot(fpr,tpr,label=data.feature_names[col] + ', auc = '\
                           + str(np.round(auc(fpr,tpr),decimals=3)))plt.title('ROC curve - Iris features')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()
```

**![](img/50b9e1db56b6eca835a735205ca0f145.png)**

# **å¤šå…ƒæ»¤æ³¢æ–¹æ³•**

**è¿™äº›æ–¹æ³•è€ƒè™‘äº†å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œè€Œæ²¡æœ‰è€ƒè™‘ä»»ä½•ç±»å‹çš„åˆ†ç±»ç®—æ³•ã€‚**

## **mRMR**

****mRMR(æœ€å°å†—ä½™æœ€å¤§ç›¸å…³æ€§)**æ˜¯ä¸€ç§å¯å‘å¼ç®—æ³•ï¼Œé€šè¿‡è€ƒè™‘ç‰¹å¾çš„é‡è¦æ€§å’Œå®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§æ¥å¯»æ‰¾æ¥è¿‘æœ€ä¼˜çš„ç‰¹å¾å­é›†ã€‚**

**å…¶æ€æƒ³æ˜¯ï¼Œå³ä½¿ä¸¤ä¸ªç‰¹å¾é«˜åº¦ç›¸å…³ï¼Œå¦‚æœå®ƒä»¬é«˜åº¦ç›¸å…³ï¼Œå°†å®ƒä»¬éƒ½æ·»åŠ åˆ°ç‰¹å¾é›†ä¸­å¯èƒ½ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ·»åŠ ä¸¤ä¸ªç‰¹å¾ä¼šå¢åŠ æ¨¡å‹çš„å¤æ‚æ€§(å¢åŠ è¿‡åº¦æ‹Ÿåˆçš„å¯èƒ½æ€§)ï¼Œä½†ç”±äºç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä¸ä¼šæ·»åŠ é‡è¦çš„ä¿¡æ¯ã€‚**

**åœ¨ä¸€ç»„ *N* ç‰¹å¾çš„ *S* ä¸­ï¼Œç‰¹å¾çš„ç›¸å…³æ€§( *D* )è®¡ç®—å¦‚ä¸‹:**

**![](img/c72ee438e7f2c782f06ea33ee612cc01.png)**

**å…¶ä¸­ *I* ä¸ºäº’ä¿¡æ¯ç®—å­ã€‚**

**ç‰¹å¾çš„å†—ä½™è¡¨ç¤ºå¦‚ä¸‹:**

**![](img/6f6bb753f5b3f35a1df3bd428489e603.png)**

**é›†åˆ *S* çš„ mRMR åˆ†æ•°å®šä¹‰ä¸º( *D - R)* ã€‚ç›®æ ‡æ˜¯æ‰¾åˆ°å…·æœ‰æœ€å¤§å€¼( *D-R)* çš„ç‰¹å¾å­é›†ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬æ‰§è¡Œå¢é‡æœç´¢(ä¹Ÿç§°ä¸ºå‰å‘é€‰æ‹©)ï¼Œåœ¨æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬æ·»åŠ äº§ç”Ÿæœ€å¤§ mRMR çš„ç‰¹å¾ã€‚**

**è¯¥ç®—æ³•ç”±ç®—æ³•ä½œè€…è‡ªå·±ç”¨ C å®ç°ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°è¿™ä¸ªåŒ…çš„æºä»£ç ï¼Œä»¥åŠåŸå§‹è®ºæ–‡[ã€‚](http://home.penglab.com/proj/mRMR/)**

**åœ¨åç§°`pymrmr`ä¸Šåˆ›å»ºäº†ä¸€ä¸ª(æœªç»´æŠ¤çš„)python åŒ…è£…å™¨ã€‚å¦‚æœ`pymrmr`æœ‰é—®é¢˜ï¼Œæˆ‘å»ºè®®ç›´æ¥è°ƒç”¨ C çº§å‡½æ•°ã€‚**

**ä¸‹é¢çš„ä»£ç ä¸¾ä¾‹è¯´æ˜äº†`pymrmr`çš„ç”¨æ³•ã€‚æ³¨æ„ï¼Œ`pandas`æ•°æ®å¸§çš„åˆ—åº”æŒ‰ç…§ C çº§åŒ…ä¸­çš„æè¿°è¿›è¡Œæ ¼å¼åŒ–(æ­¤å¤„[ä¸º](http://home.penglab.com/proj/mRMR/))ã€‚**

```
import pandas as pd
import pymrmrdf = pd.read_csv('some_df.csv')
# Pass a dataframe with a predetermined configuration. 
# Check http://home.penglab.com/proj/mRMR/ for the dataset requirements
pymrmr.mRMR(df, 'MIQ', 10)
```

**è¾“å‡º:**

```
*** This program and the respective minimum Redundancy Maximum Relevance (mRMR)
     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for
     the paper
     "Feature selection based on mutual information: criteria of
      max-dependency, max-relevance, and min-redundancy,"
      Hanchuan Peng, Fuhui Long, and Chris Ding,
      IEEE Transactions on Pattern Analysis and Machine Intelligence,
      Vol. 27, No. 8, pp.1226-1238, 2005.*** MaxRel features ***
 Order    Fea     Name    Score
 1        765     v765    0.375
 2        1423    v1423   0.337
 3        513     v513    0.321
 4        249     v249    0.309
 5        267     v267    0.304
 6        245     v245    0.304
 7        1582    v1582   0.280
 8        897     v897    0.269
 9        1771    v1771   0.269
 10       1772    v1772   0.269*** mRMR features ***
 Order    Fea     Name    Score
 1        765     v765    0.375
 2        1123    v1123   24.913
 3        1772    v1772   3.984
 4        286     v286    2.280
 5        467     v467    1.979
 6        377     v377    1.768
 7        513     v513    1.803
 8        1325    v1325   1.634
 9        1972    v1972   1.741
 10       1412    v1412   1.689
Out[1]:
 ['v765',
  'v1123',
  'v1772',
  'v286',
  'v467',
  'v377',
  'v513',
  'v1325',
  'v1972',
  'v1412']
```

# **åŒ…è£…æ–¹æ³•**

**![](img/3c32fccd789e2dc4e40fd41775af2147.png)**

**åŒ…è£…å™¨æ–¹æ³•èƒŒåçš„ä¸»è¦æ€æƒ³æ˜¯æœç´¢å“ªç»„ç‰¹æ€§æœ€é€‚åˆç‰¹å®šçš„åˆ†ç±»å™¨ã€‚è¿™äº›æ–¹æ³•å¯ä»¥æ€»ç»“å¦‚ä¸‹ï¼Œå¹¶ä¸”åœ¨æ‰€ä½¿ç”¨çš„æœç´¢ç®—æ³•æ–¹é¢æœ‰æ‰€ä¸åŒã€‚**

1.  **é€‰æ‹©ä¸€ä¸ªæ€§èƒ½æŒ‡æ ‡(å¯èƒ½æ€§ã€AICã€BICã€F1 åˆ†æ•°ã€å‡†ç¡®åº¦ã€MSEã€MAEâ€¦)ï¼Œè®°ä¸º **M.****
2.  **é€‰æ‹©ä¸€ä¸ªåˆ†ç±»å™¨/å›å½’å™¨/ â€¦ï¼Œåœ¨è¿™é‡Œè®°ä¸º **C** ã€‚**
3.  ****ç”¨ç»™å®šçš„æœç´¢æ–¹æ³•æœç´¢**ä¸åŒçš„ç‰¹å¾å­é›†ã€‚å¯¹äºæ¯ä¸ªå­é›† **Sï¼Œ**æ‰§è¡Œä»¥ä¸‹æ“ä½œ:**

*   **ä½¿ç”¨ **S** ä½œä¸ºåˆ†ç±»å™¨çš„ç‰¹å¾ï¼Œä»¥äº¤å‰éªŒè¯çš„æ–¹å¼è®­ç»ƒå’Œæµ‹è¯•**C**ï¼›**
*   **ä»äº¤å‰éªŒè¯ç¨‹åºä¸­è·å¾—å¹³å‡åˆ†æ•°(å¯¹äºæŒ‡æ ‡ **M** )ï¼Œå¹¶å°†è¯¥åˆ†æ•°åˆ†é…ç»™å­é›†**S**ï¼›**
*   **é€‰æ‹©ä¸€ä¸ªæ–°çš„å­é›†å¹¶é‡åšæ­¥éª¤*ä¸€ä¸ª*ã€‚**

## **è¯¦è¿°æ­¥éª¤ 3**

**ç¬¬ä¸‰æ­¥æœªæŒ‡å®šå°†ä½¿ç”¨å“ªç§ç±»å‹çš„æœç´¢æ–¹æ³•ã€‚å‡ ä¹åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œæµ‹è¯•æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾å­é›†éƒ½æ˜¯ç¦æ­¢çš„(**å¼ºåŠ›é€‰æ‹©**)ï¼Œå› ä¸ºè¿™å°†éœ€è¦æ‰§è¡Œæ­¥éª¤ 3 æŒ‡æ•°æ¬¡(ç‰¹å¾æ•°é‡çš„ 2 æ¬¡æ–¹)ã€‚é™¤äº†æ—¶é—´å¤æ‚æ€§ä¹‹å¤–ï¼Œç”±äºæœ‰å¦‚æ­¤å¤§é‡çš„å¯èƒ½æ€§ï¼Œå¾ˆå¯èƒ½æŸä¸ªç‰¹å¾ç»„åˆä»…ä»…æ˜¯éšæœºåœ°è¡¨ç°å¾—æœ€å¥½ï¼Œè¿™ä½¿å¾—å¼ºåŠ›è§£å†³æ–¹æ¡ˆæ›´å®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚**

**æœç´¢ç®—æ³•åœ¨å®è·µä¸­å¾€å¾€èƒ½å¾ˆå¥½åœ°è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å®ƒä»¬å€¾å‘äºå®ç°æ¥è¿‘è›®åŠ›è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ï¼Œå…·æœ‰æ›´å°‘çš„æ—¶é—´å¤æ‚åº¦å’Œæ›´å°‘çš„è¿‡æ‹Ÿåˆæœºä¼šã€‚**

****æ­£å‘é€‰æ‹©**å’Œ**åå‘é€‰æ‹©**(åˆå**ä¿®å‰ª**)åœ¨å®è·µä¸­è¢«å¤§é‡ä½¿ç”¨ï¼Œä»¥åŠå®ƒä»¬çš„æœç´¢è¿‡ç¨‹çš„ä¸€äº›å°å˜åŒ–ã€‚**

**åå‘é€‰æ‹©åŒ…æ‹¬ä»å…·æœ‰å…¨éƒ¨ç‰¹å¾çš„æ¨¡å‹å¼€å§‹ï¼Œå¹¶ä¸”åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç§»é™¤æ²¡æœ‰ç‰¹å¾çš„æ¨¡å‹å…·æœ‰æœ€é«˜åˆ†æ•°ã€‚æ­£å‘é€‰æ‹©ä»¥ç›¸åçš„æ–¹å¼è¿›è¡Œ:å®ƒä»ä¸€ç»„ç©ºçš„ç‰¹å¾å¼€å§‹ï¼Œå¹¶æ·»åŠ æœ€èƒ½æé«˜å½“å‰åˆ†æ•°çš„ç‰¹å¾ã€‚**

**å‘å‰/å‘åé€‰æ‹©ä»ç„¶å€¾å‘äºè¿‡åº¦æ‹Ÿåˆï¼Œå› ä¸ºé€šå¸¸ï¼Œåˆ†æ•°å€¾å‘äºé€šè¿‡æ·»åŠ æ›´å¤šç‰¹å¾æ¥æé«˜ã€‚é¿å…è¿™ç§æƒ…å†µçš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨æƒ©ç½šæ¨¡å‹å¤æ‚æ€§çš„åˆ†æ•°ï¼Œå¦‚ AIC æˆ– BICã€‚**

**åŒ…è£…æ–¹æ³•ç»“æ„çš„å›¾ç¤ºå¦‚ä¸‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç‰¹å¾é›†æ˜¯(1)é€šè¿‡**æœç´¢æ–¹æ³•**æ‰¾åˆ°çš„ï¼Œä»¥åŠ(2)åœ¨æ‰“ç®—ä½¿ç”¨çš„åŒä¸€åˆ†ç±»å™¨ä¸Šäº¤å‰éªŒè¯çš„**ã€‚****

**![](img/802383a82247dda57e760559b05928e8.png)**

**ç¬¬ä¸‰æ­¥è¿˜å¼€æ”¾äº†äº¤å‰éªŒè¯å‚æ•°ã€‚é€šå¸¸ï¼Œä½¿ç”¨ k-fold ç¨‹åºã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤§ k ä¼šç»™æ•´ä¸ªåŒ…è£…å™¨æ–¹æ³•å¸¦æ¥é¢å¤–çš„å¤æ‚æ€§ã€‚**

## **åŒ…è£…æ–¹æ³•çš„ Python åŒ…**

**([http://rasbt.github.io/mlxtend/](http://rasbt.github.io/mlxtend/))æ˜¯ä¸€ä¸ªç”¨äºå„ç§æ•°æ®ç§‘å­¦ç›¸å…³ä»»åŠ¡çš„æœ‰ç”¨åŒ…ã€‚è¿™ä¸ªåŒ…çš„åŒ…è£…æ–¹æ³•å¯ä»¥åœ¨ SequentialFeatureSelector ä¸Šæ‰¾åˆ°ã€‚å®ƒæä¾›å‘å‰å’Œå‘åçš„åŠŸèƒ½é€‰æ‹©ï¼Œæœ‰äº›å˜åŒ–ã€‚**

**è¯¥åŒ…è¿˜æä¾›äº†ä¸€ç§é€šè¿‡å‡½æ•° plot _ sequential _ feature _ selection å°†åˆ†æ•°å¯è§†åŒ–ä¸ºè¦ç´ æ•°é‡çš„å‡½æ•°çš„æ–¹æ³•ã€‚**

**ä¸‹é¢çš„ä¾‹å­æ‘˜è‡ªåŒ…çš„ä¸»é¡µã€‚**

```
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfsfrom sklearn.linear_model import LinearRegression
from sklearn.datasets import load_bostonboston = load_boston()
X, y = boston.data, boston.targetlr = LinearRegression()sfs = SFS(lr, 
          k_features=13, 
          forward=True, 
          floating=False, 
          scoring='neg_mean_squared_error',
          cv=10)sfs = sfs.fit(X, y)
fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')plt.title('Sequential Forward Selection (w. StdErr)')
plt.grid()
plt.show()
```

**![](img/e3c001c9151ac4fa0b510c5a02a8f137.png)**

# **åµŒå…¥å¼æ–¹æ³•**

**![](img/1036bc5f088c2e39c2d3dd3ed07587af.png)**

**è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨å½’ç»“ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­æˆ‘ä»¬è¯•å›¾æœ€å°åŒ–å…¶å‚æ•°çš„å‡½æ•°(è¿™é‡Œè®°ä¸ºğœƒ).è¿™ä¸ªå‡½æ•°è¢«ç§°ä¸º**æŸå¤±å‡½æ•°**(è®°ä¸ºğ¿(ğœƒ)).**

**åœ¨ä¸€ä¸ªæ›´ä¸€èˆ¬çš„æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›æœ€å°åŒ–ä¸€ä¸ª**ç›®æ ‡** **å‡½æ•°**ï¼Œå®ƒè€ƒè™‘äº†æŸå¤±å‡½æ•°å’Œå¯¹æ¨¡å‹å¤æ‚æ€§çš„**æƒ©ç½š**(æˆ–**æ­£åˆ™åŒ–**)(Ï‰(ğœƒ):**

****obj(ğœƒ)=ğ¿(ğœƒ)+Ï‰(ğœƒ)****

## **çº¿æ€§åˆ†ç±»å™¨çš„åµŒå…¥å¼æ–¹æ³•**

**å¯¹äºçº¿æ€§åˆ†ç±»å™¨(ä¾‹å¦‚çº¿æ€§ SVMã€é€»è¾‘å›å½’)ï¼ŒæŸå¤±å‡½æ•°è¡¨ç¤ºä¸º:**

**![](img/4ed5c415735e80b4ff00a69af9bba0ef.png)**

**å…¶ä¸­æ¯ä¸ª **xÊ²** å¯¹åº”ä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼Œè€Œ **Wáµ€xÊ²** è¡¨ç¤ºç³»æ•°å‘é‡ **(wâ‚,wâ‚‚,â€¦w_n)** ä¸æ¯ä¸ªæ ·æœ¬ä¸­çš„ç‰¹å¾çš„å†…ç§¯ã€‚**

**å¯¹äºçº¿æ€§ SVM å’Œé€»è¾‘å›å½’ï¼Œé“°é“¾å’Œé€»è¾‘æŸå¤±åˆ†åˆ«ä¸º:**

**![](img/374ed9d883368ffef81534d5ec71c1ba.png)**

**çº¿æ€§åˆ†ç±»å™¨çš„ä¸¤ä¸ªæœ€å¸¸è§çš„æƒ©ç½šæ˜¯ L-1 å’Œ L-2 æƒ©ç½š:**

**![](img/6345941828d0741f0287178b18657b41.png)**

****Î»** çš„å€¼è¶Šé«˜ï¼Œæƒ©ç½šè¶Šå¼ºï¼Œæœ€ä¼˜ç›®æ ‡å‡½æ•°å°†è¶‹å‘äºä»¥**æ”¶ç¼©**è¶Šæ¥è¶Šå¤šçš„ç³»æ•° **w_i** è€Œç»“æŸã€‚**

**ä¼—æ‰€å‘¨çŸ¥ï¼Œâ€œL1â€æƒ©ç½šä¼šåˆ›å»ºç¨€ç–æ¨¡å‹ï¼Œè¿™ç®€å•åœ°æ„å‘³ç€ï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä½¿ä¸€äº›ç³»æ•°ç­‰äºé›¶ï¼Œå®ƒå€¾å‘äº**ä»æ¨¡å‹ä¸­é€‰æ‹©ä¸€äº›ç‰¹å¾ã€‚****

**å¦ä¸€ä¸ªå¸¸è§çš„å¤„ç½šæ˜¯ L-2ã€‚è™½ç„¶ L-2 ç¼©å°äº†ç³»æ•°ï¼Œå› æ­¤æœ‰åŠ©äºé¿å…è¿‡æ‹Ÿåˆï¼Œä½†å®ƒä¸ä¼šåˆ›å»ºç¨€ç–æ¨¡å‹ï¼Œå› æ­¤å®ƒä¸é€‚åˆä½œä¸ºç‰¹å¾é€‰æ‹©æŠ€æœ¯ã€‚**

**å¯¹äºä¸€äº›çº¿æ€§åˆ†ç±»å™¨(çº¿æ€§ SVMï¼Œé€»è¾‘å›å½’)ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨ L-1 ç½šåˆ†ï¼Œè¿™æ„å‘³ç€æœ‰æœ‰æ•ˆçš„æ•°å€¼æ–¹æ³•æ¥ä¼˜åŒ–æœ€ç»ˆçš„ç›®æ ‡å‡½æ•°ã€‚å¯¹äºå…¶ä»–å‡ ä¸ªåˆ†ç±»å™¨(å„ç§æ ¸ SVM æ–¹æ³•ã€å†³ç­–æ ‘ç­‰ç­‰)ï¼Œæƒ…å†µå°±ä¸ä¸€æ ·äº†ã€‚å› æ­¤ï¼Œ**ä¸åŒçš„åˆ†ç±»å™¨åº”è¯¥ä½¿ç”¨ä¸åŒçš„æ­£åˆ™åŒ–æ–¹æ³•**ã€‚**

**å¸¦æœ‰æ­£åˆ™åŒ–çš„é€»è¾‘å›å½’ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œéšç€ C çš„å‡å°‘ï¼Œç®—æ³•æ’é™¤äº†ä¸€äº›ç‰¹å¾(æƒ³æƒ³å¦‚æœ **C** ä¸º **1/Î»** )ã€‚**

```
import numpy as np
import matplotlib.pyplot as pltfrom sklearn.svm import LinearSVC
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GridSearchCV
from sklearn.utils import check_random_state
from sklearn import datasets
from sklearn.linear_model import LogisticRegressionrnd = check_random_state(1)# set up dataset
n_samples = 3000
n_features = 15# l1 data (only 5 informative features)
X, y = datasets.make_classification(n_samples=n_samples,
                                        n_features=n_features, n_informative=5,
                                        random_state=1)cs = np.logspace(-2.3, 0, 50)coefs = []
for c in cs:
    clf = LogisticRegression(solver='liblinear',C=c,penalty='l1')
    # clf = LinearSVC(C=c,penalty='l1', loss='squared_hinge', dual=False, tol=1e-3)

    clf.fit(X,y)
    coefs.append(list(clf.coef_[0]))

coefs = np.array(coefs)
plt.figure(figsize=(10,5))
for i,col in enumerate(range(n_features)):
    plt.plot(cs,coefs[:,col])
plt.xscale('log')
plt.title('L1 penalty - Logistic regression')
plt.xlabel('C')
plt.ylabel('Coefficient value')
plt.show()
```

**![](img/a9a2db699c66748d1be9c91648960cc1.png)**

# **åŸºäºæ ‘çš„æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§**

**å¦ä¸€ç§å¸¸è§çš„ç‰¹å¾é€‰æ‹©æŠ€æœ¯åŒ…æ‹¬ä»åŸºäºæ ‘çš„æ¨¡å‹ä¸­æå–ç‰¹å¾é‡è¦æ€§ç­‰çº§ã€‚**

**ç‰¹å¾é‡è¦æ€§æœ¬è´¨ä¸Šæ˜¯ç”±æ¯ä¸ªå˜é‡äº§ç”Ÿçš„åˆ†è£‚æ ‡å‡†ä¸­çš„å•ä¸ªæ ‘çš„æ”¹è¿›çš„å¹³å‡å€¼ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯åœ¨ä½¿ç”¨ç‰¹å®šå˜é‡åˆ†å‰²æ ‘æ—¶åˆ†æ•°(å†³ç­–æ ‘ç¬¦å·ä¸Šæ‰€è°“çš„â€œæ‚è´¨â€)æé«˜äº†å¤šå°‘ã€‚**

**å®ƒä»¬å¯ç”¨äºå¯¹è¦ç´ è¿›è¡Œåˆ†çº§ï¼Œç„¶åé€‰æ‹©è¦ç´ çš„å­é›†ã€‚ç„¶è€Œï¼Œ**åº”å°å¿ƒä½¿ç”¨ç‰¹æ€§é‡è¦æ€§ï¼Œå› ä¸ºå®ƒä»¬ä¼šå—åˆ°åå·®å’Œçš„å½±å“ï¼Œå¹¶å‘ˆç°å‡ºä¸é«˜åº¦ç›¸å…³ç‰¹æ€§**ç›¸å…³çš„æ„å¤–è¡Œä¸ºï¼Œä¸ç®¡å®ƒä»¬æœ‰å¤šå¼ºã€‚**

**å¦‚æœ¬æ–‡ä¸­çš„[æ‰€ç¤ºï¼Œéšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§åå‘äºå…·æœ‰æ›´å¤šç±»åˆ«çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå¦‚æœä¸¤ä¸ªç‰¹å¾é«˜åº¦ç›¸å…³ï¼Œæ— è®ºç‰¹å¾çš„è´¨é‡å¦‚ä½•ï¼Œå®ƒä»¬çš„åˆ†æ•°éƒ½ä¼šå¤§å¤§é™ä½ã€‚](https://link.springer.com/content/pdf/10.1186%2F1471-2105-8-25.pdf)**

**ä»¥ä¸‹æ˜¯å¦‚ä½•ä»éšæœºæ£®æ—ä¸­æå–è¦ç´ é‡è¦æ€§çš„ç¤ºä¾‹ã€‚è™½ç„¶æ˜¯å›å½’å˜é‡ï¼Œä½†å¯¹äºåˆ†ç±»å™¨æ¥è¯´ï¼Œè¿‡ç¨‹æ˜¯ç›¸åŒçš„ã€‚**

```
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressorimport numpy as npboston = load_boston()
X = boston.data
Y = boston.target
feat_names = boston.feature_names 
rf = RandomForestRegressor()
rf.fit(X, Y)
print("Features sorted by their score:")
print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), feat_names), 
             reverse=True))
Out:
Features sorted by their score:
[(0.4334, 'LSTAT'), (0.3709, 'RM'), (0.0805, 'DIS'), (0.0314, 'CRIM'), (0.0225, 'NOX'), (0.0154, 'TAX'), (0.0133, 'PTRATIO'), (0.0115, 'AGE'), (0.011, 'B'), (0.0043, 'INDUS'), (0.0032, 'RAD'), (0.0016, 'CHAS'), (0.0009, 'ZN')]
```

## **é¢å¤–:æ ‘æ¨¡å‹çš„ä¸»è¦æ‚è´¨åˆ†æ•°**

**å¦‚ä¸Šæ‰€è¿°ï¼Œâ€œæ‚è´¨â€æ˜¯å†³ç­–æ ‘ç®—æ³•åœ¨å†³å®šåˆ†å‰²èŠ‚ç‚¹æ—¶ä½¿ç”¨çš„åˆ†æ•°ã€‚æœ‰è®¸å¤šå†³ç­–æ ‘ç®—æ³•(IDR3ã€C4.5ã€CART ç­‰)ï¼Œä½†ä¸€èˆ¬è§„åˆ™æ˜¯ï¼Œæˆ‘ä»¬ç”¨æ¥åˆ†å‰²æ ‘ä¸­èŠ‚ç‚¹çš„å˜é‡æ˜¯å¯¹æ‚è´¨äº§ç”Ÿæœ€é«˜æ”¹å–„çš„å˜é‡ã€‚**

**æœ€å¸¸è§çš„æ‚è´¨æ˜¯åŸºå°¼æ‚è´¨å’Œç†µã€‚åŸºå°¼ç³»æ•°æ‚è´¨çš„æ”¹è¿›è¢«ç§°ä¸ºâ€œ**åŸºå°¼ç³»æ•°é‡è¦æ€§**ï¼Œè€Œç†µçš„æ”¹è¿›æ˜¯**ä¿¡æ¯å¢ç›Šã€‚****

**![](img/bbf6e0c55f708886ac0c2cd66d5ce8db.png)**

## **SHAP:æ¥è‡ªæ ‘æ¨¡å‹çš„å¯é ç‰¹å¾é‡è¦æ€§**

**(æ„Ÿè°¢[æ©é‡Œå…‹Â·åŠ æ–¯å¸•é‡Œå°¼Â·è²ä¹Œè¨Â·å¤šçº³è¥¿é—¨æ‰˜](https://medium.com/u/7ba65e2cae1e?source=post_page-----10c0ddd7918b--------------------------------)çš„å»ºè®®ï¼)**

**SHAP å®é™…ä¸Šè¿œä¸æ­¢äºæ­¤ã€‚å®ƒæ˜¯ä¸€ç§ç®—æ³•ï¼Œæä¾›ä»»ä½•é¢„æµ‹æ¨¡å‹ä¹‹å¤–çš„æ¨¡å‹è§£é‡Šã€‚ç„¶è€Œï¼Œå¯¹äºåŸºäºæ ‘çš„æ¨¡å‹ï¼Œå®ƒç‰¹åˆ«æœ‰ç”¨:ä½œè€…ä¸ºè¿™ç§æ¨¡å‹å¼€å‘äº†é«˜é€Ÿå’Œç²¾ç¡®(ä¸ä»…ä»…æ˜¯å±€éƒ¨)çš„è§£é‡Šï¼Œä¸ X *GBoost* ã€ *LightGBM* ã€ *CatBoost* å’Œ *scikit-learn* æ ‘æ¨¡å‹å…¼å®¹ã€‚**

**æˆ‘é¼“åŠ±æ£€æŸ¥ä¸€ä¸‹ SHAP æä¾›çš„è§£é‡Šèƒ½åŠ›(æ¯”å¦‚**ç‰¹å¾ä¾èµ–ã€äº¤äº’æ•ˆæœã€æ¨¡å‹ç›‘æ§â€¦â€¦**)ã€‚ä¸‹é¢ï¼Œæˆ‘(ä»…)ç»˜åˆ¶äº† SHAP è¾“å‡ºçš„ç‰¹å¾é‡è¦æ€§ï¼Œå½“å¯¹å®ƒä»¬è¿›è¡Œæ’åºä»¥è¿›è¡Œç‰¹å¾é€‰æ‹©æ—¶ï¼Œè¿™äº›ç‰¹å¾é‡è¦æ€§æ¯”åŸå§‹æ ‘æ¨¡å‹è¾“å‡ºçš„ç‰¹å¾é‡è¦æ€§æ›´å¯é ã€‚è¿™ä¸ªä¾‹å­æ‘˜è‡ªä»–ä»¬çš„ [*github*](https://medium.com/p/10c0ddd7918b/edit) é¡µé¢ã€‚**

```
import xgboost
import shap# load JS visualization code to notebook
shap.initjs()# train XGBoost model
X,y = shap.datasets.boston()
model = xgboost.train({"learning_rate": 0.01}, xgboost.DMatrix(X, label=y), 100)# explain the model's predictions using SHAP values
# (same syntax works for LightGBM, CatBoost, and scikit-learn models)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)shap.summary_plot(shap_values, X, plot_type="bar")
```

**![](img/aee5d31bc5910f0c69129eb4f767aaf3.png)**

# **ç»“è®ºâ€”â€”ä½•æ—¶ä½¿ç”¨æ¯ç§æ–¹æ³•ï¼Ÿ**

**åµŒå…¥å¼æ–¹æ³•å¯¹äºé¿å…è¿‡åº¦æ‹Ÿåˆå’Œé€‰æ‹©æœ‰ç”¨çš„å˜é‡é€šå¸¸éå¸¸æœ‰æ•ˆã€‚å®ƒä»¬ä¹Ÿæ˜¯æ—¶é—´æœ‰æ•ˆçš„ï¼Œå› ä¸ºå®ƒä»¬åµŒå…¥åœ¨ç›®æ ‡å‡½æ•°ä¸­ã€‚å®ƒä»¬çš„ä¸»è¦ç¼ºç‚¹æ˜¯å®ƒä»¬å¯èƒ½æ— æ³•ç”¨äºæ‰€éœ€çš„åˆ†ç±»å™¨ã€‚**

**åŒ…è£…æ–¹æ³•åœ¨å®è·µä¸­å¾€å¾€å·¥ä½œå¾—å¾ˆå¥½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è®¡ç®—ä¸Šæ˜¯æ˜‚è´µçš„ï¼Œç‰¹åˆ«æ˜¯å½“å¤„ç†æ•°ç™¾ä¸ªç‰¹å¾æ—¶ã€‚ä½†æ˜¯å¦‚æœä½ æœ‰è®¡ç®—èµ„æºï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ã€‚**

**å¦‚æœç‰¹å¾é›†éå¸¸å¤§(å¤§çº¦æ•°ç™¾æˆ–æ•°åƒ)ï¼Œå› ä¸ºè¿‡æ»¤æ–¹æ³•å¾ˆå¿«ï¼Œå®ƒä»¬å¯ä»¥å¾ˆå¥½åœ°ä½œä¸ºé€‰æ‹©çš„ç¬¬ä¸€é˜¶æ®µï¼Œä»¥æ’é™¤ä¸€äº›å˜é‡ã€‚éšåï¼Œå¯ä»¥å°†å¦ä¸€ç§æ–¹æ³•åº”ç”¨äºå·²ç»ç¼©å‡çš„ç‰¹å¾é›†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³è¦åˆ›å»ºè¦ç´ çš„ç»„åˆï¼Œå°†å®ƒä»¬ç›¸ä¹˜æˆ–ç›¸é™¤ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚**

# **å‚è€ƒ**

**[å˜é‡å’Œç‰¹å¾é€‰æ‹©ä»‹ç»](http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf)**

**[éšæœºæ£®æ—å˜é‡é‡è¦æ€§æµ‹é‡ä¸­çš„åå·®:ä¾‹è¯ã€æ¥æºå’Œè§£å†³æ–¹æ¡ˆ](https://link.springer.com/content/pdf/10.1186%2F1471-2105-8-25.pdf)**

**[ç”¨äºåˆ†ç±»çš„ç‰¹å¾é€‰æ‹©:ç»¼è¿°](https://pdfs.semanticscholar.org/310e/a531640728702fce6c743c1dd680a23d2ef4.pdf)**