<html>
<head>
<title>My bet on causal reinforcement learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我赌因果强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-bet-on-causal-reinforcement-learning-d94fc9b37466?source=collection_archive---------16-----------------------#2019-09-03">https://towardsdatascience.com/my-bet-on-causal-reinforcement-learning-d94fc9b37466?source=collection_archive---------16-----------------------#2019-09-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cd12" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">营销科学的下一个杀手级应用</h2></div><p id="e606" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le"> Robert Osazuwa Ness，机器学习工程师博士，定期在</em> <a class="ae lf" href="https://altdeep.substack.com/" rel="noopener ugc nofollow" target="_blank"> <em class="le"> AltDeep </em> </a> <em class="le">撰写关于数据和决策科学、机器学习和人工智能的微趋势观察。</em></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/f71f8d44ac77fbde13736c896b13fda7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4QqbMADXJXuslpqv.png"/></div></div></figure><p id="60f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上周，我开始准备给一些数据科学研究生讲授一个特殊的主题— <strong class="kk iu">强化学习中的因果建模</strong>。</p><p id="bf77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">反思这个话题，我打个赌:<strong class="kk iu">因果强化学习将是未来十年内的 AI 杀手级营销 app</strong>。</p><h2 id="dbc3" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">RL 和因果建模的背景</h2><p id="81f4" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated"><a class="ae lf" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank">强化学习</a>关注的是软件代理应该如何在一个环境中采取行动，以最大化一些累积回报的概念。<a class="ae lf" href="https://en.wikipedia.org/wiki/Causal_model" rel="noopener ugc nofollow" target="_blank">因果建模</a>是建立可以明确表示和推理因果关系的模型(最近的科普书籍<a class="ae lf" href="https://amzn.to/2zIfPDM" rel="noopener ugc nofollow" target="_blank">《为什么的书》</a>提供了该主题的通俗易懂的介绍)。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/ec8f987046bc3bb2eaf48ded377f6470.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/0*QRQPKOksSz0TrbZo.png"/></div></figure><p id="443e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">近年来，机器学习研究社区对这两个领域都表现出了越来越大的兴趣。这种对强化学习的兴趣是由结合深度学习和强化学习以创建能够击败人类专家的代理的重大成就推动的。突出的例子包括<a class="ae lf" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="noopener ugc nofollow" target="_blank">古代战略游戏围棋</a>和<a class="ae lf" href="https://altdeep.substack.com/p/open-ai-five-dota-2-explained" rel="noopener ugc nofollow" target="_blank">基于团队的比赛在幻想电脑游戏 Dota 2 </a>。<a class="ae lf" href="https://venturebeat.com/2018/04/05/whats-hot-in-ai-deep-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">有人认为</a>深度强化学习是<a class="ae lf" href="https://en.m.wikipedia.org/wiki/Artificial_general_intelligence" rel="noopener ugc nofollow" target="_blank">广义 AI </a>的路径。</p><h2 id="79b1" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">因果建模如何适用</h2><p id="2647" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">说到玩游戏，人类代理人通过形成其环境的因果模型来“玩”活着的生命的“游戏”。这些是概念模型(“这是一个棒球，那是一扇窗户”)，对象之间有因果关系(“如果我把棒球扔向窗户，它就会粉碎”)。因果模型允许我们将知识转移到新的不熟悉的情况(“我敢打赌，如果我把这个奇怪的新的硬的重的东西扔向那个奇怪的新的脆的玻璃状的东西，它也会碎”)。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/92b21f201adb3b3cc03f38274172326e.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/0*yHiuYTloREVzdrEh.png"/></div></figure><p id="4e8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当决定做什么和不做什么时，人类用这些模型进行推理。你是否曾经思考过你在特定情况下的行为，并认为，“如果我做的事情不同，事情会变得更好。”那叫做<em class="le">反事实后悔</em>，是因果推理的一种形式。你在头脑中使用一个因果模型，在脑海中模拟如果你做出不同的决定，事情会如何发展。反事实后悔是这种模拟的可能结果和实际发生的结果之间的差异。当你基于对过去决策的因果推理做出你认为会避免遗憾结果的决定时，你正在利用强大的认知机制。</p><p id="0b44" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一些游戏代理，比如最近在无限注德州扑克中击败人类扑克专家的代理，通过模拟数百万场游戏，做了一个最大限度减少反事实后悔的暴力版本。这比人类玩家依赖更多的经验和计算资源。</p><h2 id="4cd2" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">缺乏实用的工具</h2><p id="5f64" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">我专注于执行。虽然网上有无穷无尽的深度强化编程教程。然而，在编程强化学习中引入因果建模的实际案例研究还很缺乏。有一些研究(例如由 Vicarious AI 开发的<a class="ae lf" href="https://www.vicarious.com/2017/08/07/general-game-playing-with-schema-networks/" rel="noopener ugc nofollow" target="_blank">模式网络)，但是我实际上还没有看到任何代码。例如，将因果建模中的编程抽象如</a><a class="ae lf" href="https://www.inference.vc/untitled/" rel="noopener ugc nofollow" target="_blank"> Pearl 的 do-operations </a>与强化学习中的<a class="ae lf" href="https://en.wikipedia.org/wiki/Dynamic_programming" rel="noopener ugc nofollow" target="_blank">动态编程</a>相结合的最佳方式是什么？我的希望是，将因果建模的理论和方法应用于编程 RL 可能会导致构建有用的编程抽象，用于构建更好的 RL 代理，特别是那些只需要一些真实或模拟经验就可以处理不熟悉情况的代理，就像我们人类一样。</p><h2 id="4a68" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">自动化决策科学的赌注</h2><p id="ac07" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">我说我在这个问题上“打赌”,而不是做预测，因为打赌是游戏中有皮肤的预测。当我可以在 deepfakes 或 transformer networks 上工作的时候，我正在解决这个问题(这两个项目都非常值得、有利可图，而且有些令人恐惧)，因为我认为这将会有回报。</p><p id="c1fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我看到了一些套利机会。深度强化学习的商业应用实际上是不存在的，除了向 OpenAI 和<a class="ae lf" href="https://www.bloomberg.com/news/articles/2019-08-07/alphabet-s-deepmind-takes-on-billion-dollar-debt-as-loss-spirals" rel="noopener ugc nofollow" target="_blank"> Deepmind </a>投入大量资金，以便他们可以玩视频游戏(这没什么错)。</p><p id="e4b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，RL 中的核心理论是为决策科学的各种元素提供动力的相同理论；顺序实验、最优化、决策理论、博弈论、拍卖设计等。将这些理论分支应用于决策问题正是数据科学家影响最大的领域，尤其是在科技领域。</p><p id="d49d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因果建模是连接所有这些领域的线索。它甚至引入了计算认知心理学，在那里人们模拟了人类头脑中的因果模型。这使得人们可以从关于人类行为的数据中模拟人类的非理性，如认知偏差和谬误。在我看来，广告商花了大量时间让人们不理性地花钱。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ms"><img src="../Images/0e209aa11609d4bc3012438aeede2bfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ybf_TVKaSxXWGn-QrUmH5Q.png"/></div></div></figure><p id="9ba2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我认为因果强化学习将是市场营销的圣杯。想象一下，在《黑镜》中，他们可以在数字口袋世界中制造出一个认知克隆体，并进行一百万次模拟，测试你会购买什么样的产品。就像那样。</p></div></div>    
</body>
</html>