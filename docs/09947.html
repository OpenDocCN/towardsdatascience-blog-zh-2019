<html>
<head>
<title>Long Short Term Memory and Gated Recurrent Unit’s Explained — ELI5 Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">长短期记忆和门控循环单位的解释——Eli 5 方式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/long-short-term-memory-and-gated-recurrent-units-explained-eli5-way-eff3d44f50dd?source=collection_archive---------11-----------------------#2019-12-29">https://towardsdatascience.com/long-short-term-memory-and-gated-recurrent-units-explained-eli5-way-eff3d44f50dd?source=collection_archive---------11-----------------------#2019-12-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7a24" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">ELI5 项目机器学习</h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/574e1f334159f5d5488d89d880a66b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_dRHT3tYxYl3qCPw"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk">Photo by <a class="ae ko" href="https://unsplash.com/@biglaughkitchen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Deva Williamson</a> on <a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="530e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">大家好，欢迎来到我的博客“<em class="ln">长短期记忆和门控循环单元的解释——Eli 5 Way</em>”这是我 2019 年的最后一篇博客。我叫<a class="ae ko" href="https://www.linkedin.com/feed/" rel="noopener ugc nofollow" target="_blank">尼兰詹·库马尔</a>，是好事达印度公司的高级数据科学顾问。</p><p id="6439" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">递归神经网络(RNN) </strong>是一种神经网络，前一步的输出作为当前步骤的输入。</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi lo"><img src="../Images/41478734854d05e3302efca2f9852698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ty-lHifMXIbYlR9E.png"/></div></div></figure><p id="8ff0" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">RNN 主要用于，</p><ul class=""><li id="fc9f" class="lt lu it kr b ks kt kw kx la lv le lw li lx lm ly lz ma mb bi translated"><strong class="kr jd">序列分类</strong> —情感分类&amp;视频分类</li><li id="e55d" class="lt lu it kr b ks mc kw md la me le mf li mg lm ly lz ma mb bi translated"><strong class="kr jd">序列标注</strong> —词性标注&amp;命名实体识别</li><li id="b10d" class="lt lu it kr b ks mc kw md la me le mf li mg lm ly lz ma mb bi translated"><strong class="kr jd">序列生成</strong> —机器翻译&amp;音译</li></ul><blockquote class="mh mi mj"><p id="0350" class="kp kq ln kr b ks kt ku kv kw kx ky kz mk lb lc ld ml lf lg lh mm lj lk ll lm im bi translated"><strong class="kr jd">引用说明:</strong>本文的内容和结构是基于我对四分之一实验室深度学习讲座的理解——<a class="ae ko" href="https://padhai.onefourthlabs.in/" rel="noopener ugc nofollow" target="_blank">pad hai</a>。</p></blockquote><p id="08e9" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在每个时间步的递归神经网络中，旧信息被当前输入改变。对于较长的句子，我们可以想象在“t”时间步之后，存储在时间步“t-k”的信息(k &lt;&lt; t) would have undergone a gradual process of transformation. During back-propagation, the information has to flow through the long chain of timesteps to update the parameters of the network to minimize the loss of the network.</p><p id="d94d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Consider a scenario, where we need to compute the loss of the network at time step four <strong class="kr jd"> L₄ </strong>)。假设损失是由于时间步<strong class="kr jd"> S₁ </strong>的隐藏表示的错误计算造成的。<strong class="kr jd"> S₁ </strong>处的错误是由于矢量<strong class="kr jd"> W 的参数不正确。</strong>该信息必须反向传播至<strong class="kr jd"> W </strong>，以便矢量修正其参数。</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/5291be7d0f0bf86b7557fe147fba1dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*-cLm3mpWdr_eTwBBKuPn2A.png"/></div></figure><p id="1943" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了将信息传播回向量 W，我们需要使用链式法则的概念。简而言之，链式法则归结为在特定时间步长上隐藏表示的所有偏导数的乘积。</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/c8b070167d428df56a3b7dc21ced64e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*eA0a9hoehxbaPOhGYnrgzA.png"/></div></figure><p id="23cc" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们有超过 100 个更长序列的隐藏表示，那么我们必须计算这些表示的乘积用于反向传播。假设偏导数之一变成一个大值，那么整个梯度值将爆炸，导致<strong class="kr jd">爆炸梯度的问题。</strong></p><p id="daea" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果偏导数之一是一个小值，那么整个梯度变得太小或消失，使得网络难以训练。<strong class="kr jd">消失渐变的问题</strong></p><h1 id="5b92" class="mp mq it bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">白板模拟</h1><p id="3f06" class="pw-post-body-paragraph kp kq it kr b ks nn ku kv kw no ky kz la np lc ld le nq lg lh li nr lk ll lm im bi translated">假设您有一个固定大小的白板，随着时间的推移，白板变得非常凌乱，您无法从中提取任何信息。在较长序列的 RNN 环境中，所计算的隐藏状态表示将变得混乱，并且难以从中提取相关信息。</p><p id="89f9" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因为 RNN 具有有限的状态大小，而不是从所有时间步长提取信息并计算隐藏状态表示。在从不同的时间步长提取信息时，我们需要遵循有选择地读、写和忘记的策略。</p><h1 id="9b0d" class="mp mq it bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">白板类比— RNN 示例</h1><p id="d09b" class="pw-post-body-paragraph kp kq it kr b ks nn ku kv kw no ky kz la np lc ld le nq lg lh li nr lk ll lm im bi translated">让我们以使用 RNN 的情感分析为例，来看看选择性读、写、忘策略是如何工作的。</p><p id="e2aa" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">回顾 : <em class="ln">电影的前半部分很枯燥，但后半部分真的加快了节奏。男主角的表演令人惊叹。</em></p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2f44a2ad251678888d0d3720858dd50d.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*yL8oet5DOoZcf6hqgOuFvQ.png"/></div></figure><p id="d39b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这部电影的评论从负面情绪开始，但从那时起，它变成了积极的回应。在选择性读取、写入和忘记的情况下:</p><ul class=""><li id="0bba" class="lt lu it kr b ks kt kw kx la lv le lw li lx lm ly lz ma mb bi translated">我们希望忘记由停用词(a、the、is 等)添加的信息。</li><li id="6648" class="lt lu it kr b ks mc kw md la me le mf li mg lm ly lz ma mb bi translated">有选择地阅读带有感情色彩的词语所添加的信息(惊人的、令人敬畏的等等)。</li><li id="f4a3" class="lt lu it kr b ks mc kw md la me le mf li mg lm ly lz ma mb bi translated">选择性地将隐藏状态表示信息从当前单词写入新的隐藏状态。</li></ul><p id="c0e3" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用选择性读取、写入和遗忘策略，我们可以控制信息流，从而使网络不会遭受短期记忆的问题，并且还可以确保有限大小的状态向量得到有效使用。</p><h1 id="c1b5" class="mp mq it bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">长短期记忆——LSTM</h1><p id="261b" class="pw-post-body-paragraph kp kq it kr b ks nn ku kv kw no ky kz la np lc ld le nq lg lh li nr lk ll lm im bi translated">LSTM 的引入是为了克服香草 RNN 的问题，如短期记忆和消失梯度。在 LSTM 的理论中，我们可以通过使用门来调节信息流，从而有选择地读、写和忘记信息。</p><p id="ddca" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="ln">在接下来的几节中，我们将讨论如何实现选择性读、写和忘记策略。我们还将讨论我们如何知道哪些信息应该阅读，哪些信息应该忘记。</em></p><h1 id="653d" class="mp mq it bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">选择性写入</h1><p id="771b" class="pw-post-body-paragraph kp kq it kr b ks nn ku kv kw no ky kz la np lc ld le nq lg lh li nr lk ll lm im bi translated">在普通 RNN 版本中，隐藏表示(<strong class="kr jd"> sₜ) </strong>被计算为先前时间步长隐藏表示(<strong class="kr jd"> sₜ₋₁ </strong>)和当前输入(<strong class="kr jd"> xₜ </strong>)以及偏差(<strong class="kr jd"> b </strong>)的输出的函数。</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c75f74600e9549a71b8076644d7d0fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*PtMRRZJu7qe_7auvw4AHxQ.png"/></div></figure><p id="2f47" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里，我们取 sₜ₋₁的所有值并计算当前时间的隐藏状态表示</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/91fa2226419aed2af415cb0b7c5707b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*Cj4ttUvnUCl4EmdVqqm0hg.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk">In Plain RNN Version</figcaption></figure><p id="bfd3" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在选择性写入中，不是将所有信息写入<strong class="kr jd"> sₜ₋₁ </strong>来计算隐藏表示(<strong class="kr jd"> sₜ </strong>)。我们可以只传递一些关于 sₜ₋₁的信息给下一个状态来计算 sₜ.一种方法是分配一个介于 0-1 之间的值，该值决定将当前状态信息的多少部分传递给下一个隐藏状态。</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nv"><img src="../Images/26c7e9ca38e7edf6cb5b4f65b222ff02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gr43iGadEY7WkuZFfFI2cg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk">Selective Write in RNN aka LSTM</figcaption></figure><p id="aee9" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们进行选择性写入的方式是，我们将 sₜ₋₁的每个元素乘以 0-1 之间的值来计算新的向量 hₜ₋₁。我们将使用这个新的向量来计算隐藏表示<strong class="kr jd"> sₜ.</strong></p><blockquote class="nw"><p id="b782" class="nx ny it bd nz oa ob oc od oe of lm dk translated">我们如何计算<strong class="ak"> oₜ₋₁？</strong></p></blockquote><p id="4b0d" class="pw-post-body-paragraph kp kq it kr b ks og ku kv kw oh ky kz la oi lc ld le oj lg lh li ok lk ll lm im bi translated">我们将从数据中学习<strong class="kr jd"> oₜ₋₁ </strong>，就像我们使用基于梯度下降优化的参数学习来学习其他参数一样，如<strong class="kr jd"> U </strong>和<strong class="kr jd"> W </strong>。<strong class="kr jd"> oₜ₋₁ </strong>的数学方程式如下:</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/d7ac1426d05035083479a76606ebce5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*qTcT72QRyypeFkayLTOgVA.png"/></div></figure><p id="fa6d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一旦我们从<strong class="kr jd"> oₜ₋₁那里得到</strong>数据，它就乘以<strong class="kr jd"> sₜ₋₁ </strong>得到一个新的矢量<strong class="kr jd"> hₜ₋₁.</strong>由于<strong class="kr jd"> oₜ₋₁ </strong>正在控制什么信息将进入下一个隐藏状态，所以它被称为<strong class="kr jd">输出门</strong>。</p><h1 id="10d0" class="mp mq it bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">选择性阅读</h1><p id="76b6" class="pw-post-body-paragraph kp kq it kr b ks nn ku kv kw no ky kz la np lc ld le nq lg lh li nr lk ll lm im bi translated">在计算了新的向量<strong class="kr jd"> hₜ₋₁ </strong>之后，我们将计算一个中间隐藏状态向量<strong class="kr jd">šₜ</strong>(用绿色标记)。在这一节中，我们将讨论如何实现选择性读取来获得我们的最终隐藏状态<strong class="kr jd"> sₜ.</strong></p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi om"><img src="../Images/4dc333fb1b27be0598d7102a4f917e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o6rduRgbtiXeRRfVUgre0w.png"/></div></div></figure><p id="2a7b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">ₜ的数学方程式如下:</strong></p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi on"><img src="../Images/2de3bafdc416918d03525955c2313803.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*JdgjNZx_aFKt27zShklF_A.png"/></div></figure><ul class=""><li id="a283" class="lt lu it kr b ks kt kw kx la lv le lw li lx lm ly lz ma mb bi translated"><strong class="kr jd">šₜ</strong>捕捉来自前一状态<strong class="kr jd"> hₜ₋₁ </strong>和当前输入<strong class="kr jd"> xₜ </strong>的所有信息。</li><li id="a2c9" class="lt lu it kr b ks mc kw md la me le mf li mg lm ly lz ma mb bi translated">然而，我们可能不想使用所有的新信息，而只是在构建新的单元结构之前有选择地从中读取。也就是说…我们只想从<strong class="kr jd">šₜ</strong>读取一些信息来计算<strong class="kr jd"> sₜ </strong>。</li></ul><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oo"><img src="../Images/1cde422c16fa5d7660ebf11c79a7b11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bG2oY1kJNLeVw09Iy0Wy8w.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk">Selective Read</figcaption></figure><p id="6c04" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">就像我们的输出门一样，这里我们用一个新的向量<strong class="kr jd"> iₜ </strong>乘以<strong class="kr jd">šₜ</strong>的每个元素，该向量包含 0-1 之间的值。由于矢量<strong class="kr jd"> iₜ </strong>控制着从当前输入流入的信息，它被称为<strong class="kr jd">输入门</strong>。</p><p id="b6c5" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd"> iₜ </strong>的数学方程式如下:</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi op"><img src="../Images/137564e778842edd0834f6d20b0076b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*_8LA3mRT-rBs8hof409BiQ.png"/></div></figure><p id="bd0f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在输入门中，我们将前一时间步隐藏状态信息<strong class="kr jd"> hₜ₋₁ </strong>和当前输入<strong class="kr jd"> xₜ </strong>连同偏置一起传递到一个 sigmoid 函数中。计算的输出将在 0-1 之间，它将决定什么信息从当前输入和先前的时间步长隐藏状态流入。0 表示不重要，1 表示重要。</p></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="dfbe" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">回顾一下我们到目前为止所学的，我们有先前的隐藏状态<strong class="kr jd"> sₜ₋₁ </strong>，我们的目标是使用选择性读取、写入和忘记策略来计算当前状态<strong class="kr jd"> sₜ </strong>。</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/78c4f9389f1c506ee60e208e0534088d.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*yXv_Cb49qN_0wrnD_zOyOQ.png"/></div></figure><h1 id="2df5" class="mp mq it bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">选择性遗忘</h1><p id="5b3f" class="pw-post-body-paragraph kp kq it kr b ks nn ku kv kw no ky kz la np lc ld le nq lg lh li nr lk ll lm im bi translated">在本节中，我们将讨论如何通过组合<strong class="kr jd"> sₜ₋₁ </strong>和<strong class="kr jd">šₜ.来计算当前状态向量<strong class="kr jd"> sₜ </strong></strong></p><p id="3da2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">遗忘门 fₜ </strong>决定从<strong class="kr jd"> sₜ₋₁ </strong>隐藏向量中保留或丢弃的信息部分。</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oy"><img src="../Images/c70e4740983657175f5d78f2106d25db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11CNK873ITYg0GT6fH8LcA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk">Selective Forget</figcaption></figure><p id="43bb" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">遗忘门<strong class="kr jd"> fₜ </strong>的数学方程式如下:</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/80168e04c61f0231861683225029da36.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*6VJwBJDm8nJX8LLmvgF93Q.png"/></div></figure><p id="102c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在“忘记门”中，我们将前一时间步隐藏状态信息<strong class="kr jd"> hₜ₋₁ </strong>和当前输入<strong class="kr jd"> xₜ </strong>连同一个偏置一起传递到一个 sigmoid 函数中。计算的输出将在 0-1 之间，它将决定保留或丢弃什么信息。如果该值更接近 0，则表示丢弃，如果更接近 1，则表示保留。</p></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="c1fc" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过结合遗忘门和输入门，我们可以计算当前的隐藏状态信息。</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/f4f4410baf630a492b86eb5e8b84afd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*4J9SuSI9UzsG9y670jEqOQ.png"/></div></figure><p id="6aaa" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后一幅插图如下所示:</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi pb"><img src="../Images/745a17b1a6f61b0d1537ce9f2525cdd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aKc2BBkeFXYZ4kZk44oBiA.png"/></div></div></figure><p id="b3a1" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">完整的方程组如下所示:</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi pc"><img src="../Images/1db5ca4c84d290116fcffafd8d1015b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aR_1Bj7dFhSAt_WMyg2vcA.png"/></div></div></figure><p id="0282" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">注:</strong><em class="ln">LSTM 架构的某些版本没有遗忘门，而是只有一个输出门和输入门来控制信息流。它将只实现选择性读取和选择性写入策略。</em></p><p id="b07a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们上面讨论的 LSTM 的变体是 LSTM 最流行的变体，三个门都控制信息。</p><h1 id="bab8" class="mp mq it bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">门控循环单元— GRU 氏病</h1><p id="5032" class="pw-post-body-paragraph kp kq it kr b ks nn ku kv kw no ky kz la np lc ld le nq lg lh li nr lk ll lm im bi translated">在本节中，我们将简要讨论 GRU 背后的直觉。门控循环单位是 LSTM 的另一个流行变体。GRU 使用较少的门。</p><p id="c76c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在 LSTM 这样的门控循环单元中，我们有一个输出门<strong class="kr jd"> oₜ₋₁ </strong>来控制什么信息进入下一个隐藏状态。类似地，我们也有一个输入门<strong class="kr jd"> iₜ </strong>控制什么信息从当前输入流入。</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi pd"><img src="../Images/9ec2e0f0f07c9a8a416fe748e3f6243a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kj3lCF28tiDfh5ZDrvdZIQ.png"/></div></div></figure><p id="f3fb" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">LSTM 和 GRU 之间的主要区别在于它们组合中间隐藏状态向量<strong class="kr jd">šₜ</strong>和先前隐藏状态表示向量<strong class="kr jd"> sₜ₋₁.的方式</strong>在 LSTM 中，我们忘记了确定要从<strong class="kr jd"> sₜ₋₁.那里保留多少部分信息</strong></p><p id="3091" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在 GRU 而不是遗忘门中，我们基于输入门向量的补充(1- <strong class="kr jd"> iₜ </strong>)来决定保留或丢弃多少过去的信息。</p><blockquote class="nw"><p id="64e4" class="nx ny it bd nz oa ob oc od oe of lm dk translated">忘记门= 1 —输入门向量(1- <strong class="ak"> iₜ </strong>)</p></blockquote><p id="aa6a" class="pw-post-body-paragraph kp kq it kr b ks og ku kv kw oh ky kz la oi lc ld le oj lg lh li ok lk ll lm im bi translated">GRU 的全套方程式如下所示:</p><figure class="lp lq lr ls gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi pe"><img src="../Images/fe8e6513a52b05f5a41178745723b767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fuZ_CqyyeXKBBq4I5up7hg.png"/></div></div></figure><p id="4ed1" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从等式中，我们可以注意到只有两个门(输入和输出),并且我们没有显式地计算隐藏状态向量<strong class="kr jd"> hₜ₋₁.</strong>所以我们没有在 GRU 中保持额外的状态向量，也就是说……比 LSTM 计算量更少，训练速度更快。</p></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><h1 id="fdd3" class="mp mq it bd mr ms pf mu mv mw pg my mz na ph nc nd ne pi ng nh ni pj nk nl nm bi translated">从这里去哪里？</h1><p id="c26f" class="pw-post-body-paragraph kp kq it kr b ks nn ku kv kw no ky kz la np lc ld le nq lg lh li nr lk ll lm im bi translated">如果想用 Keras &amp; Tensorflow 2.0 (Python 或 R)学习更多关于神经网络的知识。查看来自 Starttechacademy<a class="ae ko" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">的 Abhishek 和 Pukhraj 的</a><a class="ae ko" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>。他们以一种简单化的方式解释了深度学习的基础。</p></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><h1 id="4e0b" class="mp mq it bd mr ms pf mu mv mw pg my mz na ph nc nd ne pi ng nh ni pj nk nl nm bi translated">摘要</h1><p id="517a" class="pw-post-body-paragraph kp kq it kr b ks nn ku kv kw no ky kz la np lc ld le nq lg lh li nr lk ll lm im bi translated">本文讨论了递归神经网络在处理较长句子时的不足。RNN 受到短期记忆问题的困扰，也就是说，在信息变形之前，它只能存储有限数量的状态。之后，我们详细讨论了选择性读取、写入和遗忘策略在 LSTM 是如何通过使用门机制来控制信息流的。然后我们研究了 LSTM 的变体，称为门控循环单元，它比 LSTM 模型具有更少的门和更少的计算。</p><p id="8a0c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我的下一篇文章中，我们将深入讨论编码器-解码器模型。所以确保你在媒体上跟随着我，以便在它下跌时得到通知。</p><p id="ef84" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">直到那时，和平:)</p><p id="5c33" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">NK。</p><p id="2b01" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd"> <em class="ln">推荐阅读</em> </strong></p><p id="100b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用 Pytorch 开始练习 LSTM 和 GRU</p><div class="pk pl gp gr pm pn"><a href="https://www.marktechpost.com/2019/12/18/classifying-the-name-nationality-of-a-person-using-lstm-and-pytorch/" rel="noopener  ugc nofollow" target="_blank"><div class="po ab fo"><div class="pp ab pq cl cj pr"><h2 class="bd jd gy z fp ps fr fs pt fu fw jc bi translated">使用 LSTM 和 Pytorch 对人名国籍进行分类</h2><div class="pu l"><h3 class="bd b gy z fp ps fr fs pt fu fw dk translated">个人名字在不同的国家，甚至在同一个国家，都有不同的变化。通常情况下…</h3></div><div class="pv l"><p class="bd b dl z fp ps fr fs pt fu fw dk translated">www.marktechpost.com</p></div></div><div class="pw l"><div class="px l py pz qa pw qb ki pn"/></div></div></a></div><p id="0304" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">了解递归神经网络</p><div class="pk pl gp gr pm pn"><a rel="noopener follow" target="_blank" href="/recurrent-neural-networks-rnn-explained-the-eli5-way-3956887e8b75"><div class="po ab fo"><div class="pp ab pq cl cj pr"><h2 class="bd jd gy z fp ps fr fs pt fu fw jc bi translated">递归神经网络(RNN)解释 ELI5 方式</h2><div class="pu l"><h3 class="bd b gy z fp ps fr fs pt fu fw dk translated">使用 RNN 的序列标记和序列分类</h3></div><div class="pv l"><p class="bd b dl z fp ps fr fs pt fu fw dk translated">towardsdatascience.com</p></div></div><div class="pw l"><div class="qc l py pz qa pw qb ki pn"/></div></div></a></div><h1 id="2471" class="mp mq it bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">作者简介</h1><p id="9464" class="pw-post-body-paragraph kp kq it kr b ks nn ku kv kw no ky kz la np lc ld le nq lg lh li nr lk ll lm im bi translated"><a class="ae ko" href="https://medium.com/@niranjankumarc" rel="noopener"> Niranjan Kumar </a>是好事达印度公司的高级数据科学顾问。他对深度学习和人工智能充满热情。除了在媒体上写作，他还作为自由数据科学作家为 Marktechpost.com 写作。点击查看他的文章<a class="ae ko" href="https://www.marktechpost.com/author/niranjan-kumar/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="005c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可以在<a class="ae ko" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与他联系，或者在<a class="ae ko" href="https://twitter.com/Nkumar_283" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注他，了解关于深度学习和机器学习的最新文章。</p><h1 id="7966" class="mp mq it bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated"><strong class="ak">联系我:</strong></h1><ul class=""><li id="2fda" class="lt lu it kr b ks nn kw no la qd le qe li qf lm ly lz ma mb bi translated">领英—<a class="ae ko" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/niranjankumar-c/</a></li><li id="da38" class="lt lu it kr b ks mc kw md la me le mf li mg lm ly lz ma mb bi translated">GitHub—<a class="ae ko" href="https://github.com/Niranjankumar-c" rel="noopener ugc nofollow" target="_blank">https://github.com/Niranjankumar-c</a></li><li id="2c26" class="lt lu it kr b ks mc kw md la me le mf li mg lm ly lz ma mb bi translated">推特—<a class="ae ko" href="https://twitter.com/Nkumar_n" rel="noopener ugc nofollow" target="_blank">https://twitter.com/Nkumar_n</a></li><li id="a020" class="lt lu it kr b ks mc kw md la me le mf li mg lm ly lz ma mb bi translated">中—<a class="ae ko" href="https://medium.com/@niranjankumarc" rel="noopener">https://medium.com/@niranjankumarc</a></li></ul><h2 id="78af" class="qg mq it bd mr qh qi dn mv qj qk dp mz la ql qm nd le qn qo nh li qp qq nl iz bi translated"><strong class="ak">参考文献:</strong></h2><ol class=""><li id="9354" class="lt lu it kr b ks nn kw no la qd le qe li qf lm qr lz ma mb bi translated"><a class="ae ko" href="https://padhai.onefourthlabs.in/" rel="noopener ugc nofollow" target="_blank">pad hai 的深度学习</a></li><li id="1a50" class="lt lu it kr b ks mc kw md la me le mf li mg lm qr lz ma mb bi translated"><a class="ae ko" rel="noopener" target="_blank" href="/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">LSTM 和 GRU 的图解指南:一步一步的解释</a></li></ol><p id="c958" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">免责声明——这篇文章中可能有一些相关资源的附属链接。你可以以尽可能低的价格购买捆绑包。如果你购买这门课程，我会收到一小笔佣金。</p></div></div>    
</body>
</html>