<html>
<head>
<title>Word Embeddings for NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的单词嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4?source=collection_archive---------1-----------------------#2019-12-27">https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4?source=collection_archive---------1-----------------------#2019-12-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="20b4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解单词嵌入及其在深层自然语言处理中的应用</h2></div><p id="ccea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将了解如何处理文本，以便在机器学习算法中使用。什么是嵌入，为什么它们被用于文本处理？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/f3f6b4e4f233f4bebb0c81c204d371b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gcC7b_v7OKWutYN1NAHyMQ.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">word2vec and GloVe word embeddings</figcaption></figure><p id="13b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自然语言处理(NLP)是指旨在理解人类语言的计算机系统。人类语言，如英语或印地语，由单词和句子组成，NLP 试图从这些句子中提取信息。</p><p id="82b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">NLP 用于的一些任务</p><ul class=""><li id="8fe8" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">文本摘要:提取或抽象的文本摘要</li><li id="5c4d" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">情感分析</li><li id="1f89" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">从一种语言到另一种语言的翻译:神经机器翻译</li><li id="8961" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">聊天机器人</li></ul><p id="7dad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">机器学习和深度学习算法只接受数字输入，那么我们如何将文本转换成数字呢？</em> </strong></p><h2 id="20b8" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">单词袋(蝴蝶结)</h2><p id="4097" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">单词包是一种简单而流行的文本特征提取技术。<strong class="kk iu">单词包模型处理文本，找出每个单词在句子中出现的次数。这也称为矢量化。</strong></p><p id="d16c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">创建弓的步骤</p><ul class=""><li id="3f8b" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">将文本标记成句子</li><li id="ec75" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">把句子符号化成单词</li><li id="d1bf" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">删除标点符号或停用字词</li><li id="9dbc" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">将单词转换为较低的文本</li><li id="dda9" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">创建单词的频率分布</li></ul><p id="f11a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的代码中，我们使用<strong class="kk iu"> <em class="le"> CountVectorizer、</em> </strong> it <strong class="kk iu"> <em class="le"> </em> </strong>标记一组文本文档，构建一个已知单词的词汇表，并使用该词汇表对新文档进行编码。</p><pre class="lg lh li lj gt nh ni nj nk aw nl bi"><span id="2a52" class="mj mk it ni b gy nm nn l no np">#Creating frequency distribution of words using nltk<br/><strong class="ni iu">from nltk.tokenize import sent_tokenize<br/>from nltk.tokenize import word_tokenize<br/>from sklearn.feature_extraction.text import CountVectorizer</strong></span><span id="3835" class="mj mk it ni b gy nq nn l no np"><strong class="ni iu">text="""Achievers are not afraid of Challenges, rather they relish them, thrive in them, use them. Challenges makes is stronger.<br/>        Challenges makes us uncomfortable. If you get comfortable with uncomfort then you will grow. Challenge the challenge """</strong></span><span id="32e5" class="mj mk it ni b gy nq nn l no np">#Tokenize the sentences from the text corpus<br/><strong class="ni iu">tokenized_text=sent_tokenize(text)</strong></span><span id="ae99" class="mj mk it ni b gy nq nn l no np">#using CountVectorizer and removing stopwords in english language<br/><strong class="ni iu">cv1= CountVectorizer(lowercase=True,stop_words='english')</strong></span><span id="dd5c" class="mj mk it ni b gy nq nn l no np">#fitting the tonized senetnecs to the countvectorizer<br/><strong class="ni iu">text_counts=cv1.fit_transform(tokenized_text)</strong></span><span id="6d38" class="mj mk it ni b gy nq nn l no np"># printing the vocabulary and the frequency distribution pf vocabulary in tokinzed sentences<br/>print(cv1.vocabulary_)<br/>print(text_counts.toarray())</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nr"><img src="../Images/8619483be707ddc8b16945cd15596b74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OsEMsoI6BMOIiY6-l5hDCg.png"/></div></div></figure><p id="bc50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在文本分类问题中，我们有一组文本和它们各自的标签。我们使用单词袋模型来从文本中提取特征，并且我们通过将文本转换成文档中单词出现的矩阵来实现这一点。</p><p id="7fad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">c <a class="ae ns" href="https://medium.com/datadriveninvestor/simple-text-summarizer-using-nlp-d8aaf5828e68" rel="noopener">简单文本摘要的颂歌</a></p><p id="4f7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">包话有什么问题？</em> </strong></p><p id="8c6c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在单词袋模型中，每个文档被表示为一个单词计数向量。这些计数可以是二进制计数，一个单词可能出现在文本中，也可能不出现，或者将具有绝对计数。向量的大小等于词汇表中元素的数量。如果大多数元素都是零，那么单词包将是一个稀疏矩阵。</p><p id="fb21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在深度学习中，我们将拥有稀疏矩阵，因为我们将处理大量的训练数据。由于计算和信息的原因，稀疏表示更难建模。</p><p id="8f6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">巨大数量的权重:</strong>巨大的输入向量意味着神经网络的巨大数量的权重。</p><p id="23d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">计算量大:</strong>权重越大，训练和预测所需的计算量越大。</p><p id="b620" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺乏有意义的关系，不考虑词序:</strong> BOW 是在文本或句子中出现的具有字数统计的词的集合。单词包不考虑它们出现的顺序。</p><blockquote class="nt"><p id="49c7" class="nu nv it bd nw nx ny nz oa ob oc ld dk translated">单词嵌入是解决这些问题的方法</p></blockquote><p id="1645" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated"><strong class="kk iu">嵌入将大的稀疏向量转换到保持语义关系的低维空间</strong>。</p><p id="d756" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词嵌入是一种技术，其中领域或语言的单个单词被表示为低维空间中的实值向量。</p><p id="cd5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">通过将高维数据映射到一个更低维的空间来解决带有 BOW 的稀疏矩阵问题。</strong></p><p id="e8b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过将语义相似项的<strong class="kk iu">向量彼此靠近放置</strong>，解决了 BOW 缺乏有意义的关系的问题。这样，具有相似意思的单词在向量空间中具有相似的距离，如下所示。</p><p id="17c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">“国王对王后就像男人对女人一样”编码在向量空间中，动词时态和国家及其首都编码在低维空间中，保持语义关系。</em></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oi"><img src="../Images/f5a8b514f35caf8ea3780c2240d1b264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-aWU_fjKblzRG4OTgmCkA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">source: <a class="ae ns" href="https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space" rel="noopener ugc nofollow" target="_blank">https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space</a></figcaption></figure><p id="7abe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">语义相似的物品是如何靠近放置的？</em>T13】</strong></p><p id="b177" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">让我们用推荐引擎中使用的协同过滤来解释这一点。</em></p><p id="5678" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">推荐引擎基于具有相似兴趣的其他用户的历史购买来预测用户将购买什么。使用协同过滤</p><p id="bfcd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">亚马逊和网飞使用推荐引擎向用户推荐产品或电影</em></p><p id="5cb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">协同过滤</strong>是将多个顾客购买的所有相似产品嵌入到一个低维空间的方法。这个低维空间将包含彼此接近的相似产品，因此，它也被称为<strong class="kk iu">最近邻</strong>算法。</p><p id="1aaf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种最近邻技术用于将语义相似的项目彼此靠近放置</p><p id="055c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">我们如何将高维数据映射到一个更低维的空间？</em>T25】</strong></p><h2 id="167a" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated"><strong class="ak">使用标准降维技术</strong></h2><p id="999b" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated"><strong class="kk iu">像主成分分析(PCA)这样的标准降维技术可以用来创建单词嵌入</strong>。PCA 试图找到高度相关的维度，这些维度可以使用 BOW 折叠成一个维度。</p><h2 id="a43b" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated"><strong class="ak"> Word2Vec </strong></h2><p id="f12f" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">Word2vec 是 Google 发明的用于训练单词嵌入的算法。word2vec 依赖于<strong class="kk iu">分布假设。</strong>分布假设指出，经常具有相同相邻单词的单词倾向于语义相似。这有助于<strong class="kk iu">将语义相似的单词映射到几何上接近的嵌入向量。</strong></p><p id="bce1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">分布假设使用连续词袋(CBOW)或跳过克。</p><p id="1b87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> word2vec 模型是具有输入层、投影层和输出层的浅层神经网络。它被训练来重建单词的语言环境。</strong>word 2 vec 神经网络的输入层采用更大的文本语料库来生成向量空间，通常有数百个维度。文本语料库中的每个唯一单词被分配给空间中的相应向量。</p><p id="ffeb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种架构被称为连续单词包 CBOW，因为它使用上下文的连续分布式表示。它既考虑了历史上的语序，也考虑了未来的语序。</p><p id="9f9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这有助于语料库中的常见上下文单词向量在向量空间中彼此靠近。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oj"><img src="../Images/0b908c7589b5eb4de81faa42e29e6ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*cktg-1KKmTKFb7Qfyj4QWg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk"><a class="ae ns" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>: Efficient Estimation of Word Representations in Vector Space by Mikolov-2013</figcaption></figure><p id="91dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">跳过克</strong></p><p id="0e60" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Skip gram 不基于上下文预测当前单词，而是使用每个当前单词作为具有连续投影层的对数线性分类器的输入，并预测当前单词前后一定范围内的单词。</p><h2 id="c8f5" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated"><strong class="ak"> GloVe:单词表示的全局向量</strong></h2><p id="a70c" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">手套是由潘宁顿等人在斯坦福开发的。它被称为全局向量，因为全局语料库统计数据是由模型直接捕获的。</p><p id="6e24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它利用了这两者</p><ul class=""><li id="0435" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">用于生成低维单词表示的全局矩阵分解方法，如潜在语义分析(LSA)</li><li id="3577" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">局部上下文窗口方法，例如 Mikolov 等人的 skip-gram 模型</li></ul><p id="b95a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LSA 有效地利用了统计信息，但在单词类比方面做得不好，因此表明了次优的向量空间结构。</p><p id="06b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">像 skip-gram 这样的方法在类比任务上表现得更好，但是很少利用语料库的统计数据，因为它们没有在全局共现计数上进行训练。GloVe 使用特定的加权最小二乘模型来训练全局单词共现计数，以有效地利用统计数据。</p><p id="e38d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑热力学领域中的两个词<strong class="kk iu"><em class="le">I =冰，j =蒸汽</em> </strong>。这些词的关系可以通过研究它们与各种<strong class="kk iu"> <em class="le">探测词 k </em> </strong>共现概率的比值来考察。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/413705c1426a6f3a1ced1135625202ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:110/format:webp/1*w5l0aWdwST4WFa414LvR2w.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">ratio of co-occurrence probabilities</figcaption></figure><p id="46c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">探查像<strong class="kk iu"> <em class="le">水或者时尚</em> </strong>这种或者与<strong class="kk iu"> <em class="le">冰与蒸汽</em> </strong>都有关系，或者都没有关系的词，比例应该接近 1。探测词像<strong class="kk iu"> <em class="le">固体</em> </strong>与<strong class="kk iu"> <em class="le">冰</em> </strong>但不与<strong class="kk iu"> <em class="le">汽</em> </strong>有较大的比值</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ol"><img src="../Images/05d74b5797fdddec3e6ed7ddff8820a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dpoe4HmLFg7BJi7ApoV9Tw.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Source: <a class="ae ns" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">GloVe: Global Vectors for Word Representation</a> — Jeffrey Pennington</figcaption></figure><p id="0b02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与原始概率相比，该比率能够更好地区分相关的词(固体和气体)和不相关的词(水和时尚),并且还能够更好地区分两个相关的词。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi om"><img src="../Images/657b671f5790cbb7e096e4ea207702a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2HuruOHvhP7_gnW2DKB2FQ.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Source: <a class="ae ns" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a></figcaption></figure><p id="9ef7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">区分<em class="le">男人</em>和<em class="le">女人</em>的是性别，类似于词对，比如<em class="le">国王</em>和<em class="le">王后</em>或者<em class="le">兄弟</em>和<em class="le">姐妹</em>。从数学上来说，我们可能期望矢量差<em class="le">男人</em> : <em class="le">女人</em>，<em class="le">国王</em> : <em class="le">王后</em>，以及<em class="le">兄弟:姐妹</em>可能都大致相等。这个特性和其他有趣的模式可以在上面一组使用 GloVe 的可视化中观察到。</p><h2 id="f14a" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">结论:</h2><p id="3974" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">单词嵌入被认为是目前无监督学习的成功应用之一。它们不需要任何带注释的语料库。嵌入使用低维空间，同时保留语义关系。</p><h2 id="542b" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">参考资料:</h2><p id="305f" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">【https://nlp.stanford.edu/projects/glove/ T42】</p><p id="3c2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ns" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">手套:单词表示的全局向量</a></p><p id="f42b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ns" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的有效估计—托马斯·米科洛夫、程凯、格雷格·科拉多、杰弗里·迪恩</a></p><div class="on oo gp gr op oq"><a href="https://developers.google.com/machine-learning/crash-course/embeddings/categorical-input-data" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">嵌入:分类输入数据|机器学习速成班</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">估计时间:10 分钟分类数据指的是代表一个或多个离散项的输入特征…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">developers.google.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe lp oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">Word2vec</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">Word2vec 是一组用于产生单词嵌入的相关模型。这些模型很浅，有两层…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">en.wikipedia.org</p></div></div><div class="oz l"><div class="pf l pb pc pd oz pe lp oq"/></div></div></a></div><p id="9918" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ns" href="https://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/" rel="noopener ugc nofollow" target="_blank">https://blog . aylien . com/overview-word-embeddings-history-word 2 vec-cbow-glove/</a></p></div></div>    
</body>
</html>