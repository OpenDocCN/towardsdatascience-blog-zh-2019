<html>
<head>
<title>NanoNeuron — 7 simple JS functions that explain how machines learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">纳米神经元——解释机器如何学习的 7 个简单 JS 函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nanoneuron-7-simple-js-functions-that-explain-how-machines-learn-d2d647b21497?source=collection_archive---------30-----------------------#2019-12-06">https://towardsdatascience.com/nanoneuron-7-simple-js-functions-that-explain-how-machines-learn-d2d647b21497?source=collection_archive---------30-----------------------#2019-12-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn jo jp"><p id="1555" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated"><em class="iq"> 7 个简单的 JavaScript 函数，让你感受一下机器是如何“学习”的。</em></p></blockquote><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/789123c8248b49abb38d110066d1456e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gRqOM5iIhXuMxyBrXsd_qQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Image by <a class="ae lf" href="https://pixabay.com/users/mohamed_hassan-5229782/" rel="noopener ugc nofollow" target="_blank">mohamed_hassan</a> on <a class="ae lf" href="https://pixabay.com/vectors/nerve-cell-neuron-brain-neurons-3759541/" rel="noopener ugc nofollow" target="_blank">pixabay</a></figcaption></figure><h1 id="b7d3" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="e428" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated"><a class="ae lf" href="https://github.com/trekhleb/nano-neuron" rel="noopener ugc nofollow" target="_blank">纳米神经元</a>是神经网络中神经元概念的<em class="js">过度简化</em>版本。纳米神经元被训练成将温度值从摄氏温度转换成华氏温度。</p><p id="f44f" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated"><a class="ae lf" href="https://github.com/trekhleb/nano-neuron/blob/master/NanoNeuron.js" rel="noopener ugc nofollow" target="_blank"> NanoNeuron.js </a>代码示例包含 7 个简单的 JavaScript 函数(模型预测、成本计算、向前和向后传播、训练)，这些函数将让您感受到机器实际上是如何“学习”的。没有第三方库，没有外部数据集和依赖，只有纯粹简单的 JavaScript 函数。</p><p id="b035" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">☝🏻这些函数无论如何都不是机器学习的完整指南。很多机器学习的概念在那里被跳过，被过度简化！这种简化的目的是让读者对机器如何学习有一个真正基本的理解和感受，并最终使读者有可能称之为“机器学习魔法”而不是“机器学习数学”🤓。</p><blockquote class="jn jo jp"><p id="5498" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">对于更高级的机器学习示例(在 TensorFlow 和 Python 上实现递归和卷积神经网络),您可以继续🤖<a class="ae lf" href="https://github.com/trekhleb/machine-learning-experiments" rel="noopener ugc nofollow" target="_blank"> <strong class="jt ir">交互式机器学习实验</strong> </a> s 知识库。</p></blockquote><h1 id="59bb" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">纳米神经元会学习什么</h1><p id="228b" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">你可能在<a class="ae lf" href="https://en.wikipedia.org/wiki/Neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a>中听说过神经元。我们下面要实现的纳米神经元有点像它，但简单得多。为了简单起见，我们甚至不打算在纳米神经元上建立网络。我们将独自拥有它，为我们做一些神奇的预测。也就是说，我们将教会这个简单的纳米神经元将温度从摄氏温度转换(预测)为华氏温度。</p><p id="dab2" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">顺便说一下，把摄氏温度转换成华氏温度的公式是这样的:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/b63b2725e3a2bf690fcbd72ba20de2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/0*TkX0hbSmpIH9hiPA"/></div></figure><p id="c6aa" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">但是现在我们的纳米神经元还不知道它…</p><h1 id="e92d" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">纳米神经元模型</h1><p id="2e44" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">让我们实现我们的纳米神经元模型函数。它实现了<code class="fe mn mo mp mq b">x</code>和<code class="fe mn mo mp mq b">y</code>之间的基本线性依赖，看起来像<code class="fe mn mo mp mq b">y = w * x + b</code>。简单的说我们的纳米神经元就是一个可以在<code class="fe mn mo mp mq b">XY</code>坐标中画直线的“小孩”。</p><p id="9ce9" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">变量<code class="fe mn mo mp mq b">w</code>、<code class="fe mn mo mp mq b">b</code>是模型的参数。纳米神经元只知道线性函数的这两个参数。<br/>这些参数是纳米神经元在训练过程中将要“学习”的东西。</p><p id="47ff" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">纳米神经元唯一能做的就是模仿线性依赖。在其<code class="fe mn mo mp mq b">predict()</code>方法中，它接受一些输入<code class="fe mn mo mp mq b">x</code>并预测输出<code class="fe mn mo mp mq b">y</code>。这里没有魔法。</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="f7d9" class="mv lh iq mq b gy mw mx l my mz">function <strong class="mq ir">NanoNeuron</strong>(w, b) {<br/>  this.w = w;<br/>  this.b = b;<br/>  this.predict = (x) =&gt; {<br/>    return x * this.w + this.b;<br/>  }<br/>}</span></pre><p id="92b1" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated"><em class="js">(…等等… </em> <a class="ae lf" href="https://en.wikipedia.org/wiki/Linear_regression#:~:targetText=In%20statistics%2C%20linear%20regression%20is,is%20called%20simple%20linear%20regression." rel="noopener ugc nofollow" target="_blank"> <em class="js">线性回归</em> </a> <em class="js">是你吗？)</em> 🧐</p><h1 id="2964" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">摄氏到华氏的转换</h1><p id="cfe4" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">以摄氏度为单位的温度值可以使用以下公式转换为华氏温度:<code class="fe mn mo mp mq b">f = 1.8 * c + 32</code>，其中<code class="fe mn mo mp mq b">c</code>是以摄氏度为单位的温度，<code class="fe mn mo mp mq b">f</code>是以华氏温度为单位的计算温度。</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="aac9" class="mv lh iq mq b gy mw mx l my mz">function <strong class="mq ir">celsiusToFahrenheit</strong>(c) {<br/>  const w = 1.8;<br/>  const b = 32;<br/>  const f = c * w + b;<br/>  return f;<br/>};</span></pre><p id="84f7" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">最终，我们希望教会我们纳米神经元模仿这一功能(学习<code class="fe mn mo mp mq b">w = 1.8</code>和<code class="fe mn mo mp mq b">b = 32</code>)，而无需事先知道这些参数。</p><p id="7a77" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">这是摄氏到华氏转换函数的样子:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi na"><img src="../Images/5274e251f842da3d419b00a01e1549d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*ld3ND7sA8LClJmvT"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Celsius to Fahrenheit conversion function</figcaption></figure><h1 id="2108" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">生成数据集</h1><p id="0328" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">在训练之前，我们需要基于<code class="fe mn mo mp mq b">celsiusToFahrenheit()</code>函数生成<strong class="jt ir">训练</strong>和<strong class="jt ir">测试数据集</strong>。数据集由成对的输入值和正确标记的输出值组成。</p><blockquote class="jn jo jp"><p id="b496" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">在现实生活中，大多数情况下，这些数据是收集的，而不是生成的。例如，我们可能有一组手绘数字图像和一组相应的数字，这些数字解释了每张图片上写的是什么数字。</p></blockquote><p id="47dc" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">我们将使用训练样本数据来训练我们的纳米神经元。在我们的纳米神经元成长并能够自己做出决定之前，我们需要用训练样本教会它什么是对的，什么是错的。</p><p id="4203" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">我们将使用测试示例来评估我们的纳米神经元在训练期间没有看到的数据上的表现。在这一点上，我们可以看到我们的“孩子”已经长大，可以自己做决定了。</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="7809" class="mv lh iq mq b gy mw mx l my mz">function <strong class="mq ir">generateDataSets</strong>() {<br/>  // xTrain -&gt; [0, 1, 2, ...],<br/>  // yTrain -&gt; [32, 33.8, 35.6, ...]<br/>  const xTrain = [];<br/>  const yTrain = [];<br/>  for (let x = 0; x &lt; 100; x += 1) {<br/>    const y = celsiusToFahrenheit(x);<br/>    xTrain.push(x);<br/>    yTrain.push(y);<br/>  }</span><span id="f39b" class="mv lh iq mq b gy nb mx l my mz">  // xTest -&gt; [0.5, 1.5, 2.5, ...]<br/>  // yTest -&gt; [32.9, 34.7, 36.5, ...]<br/>  const xTest = [];<br/>  const yTest = [];<br/>  // By starting from 0.5 and using the same step of 1 as we have used for training set<br/>  // we make sure that test set has different data comparing to training set.<br/>  for (let x = 0.5; x &lt; 100; x += 1) {<br/>    const y = celsiusToFahrenheit(x);<br/>    xTest.push(x);<br/>    yTest.push(y);<br/>  }</span><span id="3f3b" class="mv lh iq mq b gy nb mx l my mz">  return [xTrain, yTrain, xTest, yTest];<br/>}</span></pre><h1 id="3588" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">预测的成本(误差)</h1><p id="75ab" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">我们需要一些指标来显示我们的模型预测与正确值的接近程度。纳米神经元产生的正确输出值<code class="fe mn mo mp mq b">y</code>和<code class="fe mn mo mp mq b">prediction</code>之间的成本(误差)的计算将使用以下公式:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/5777cd3ac314e2dc32ad0b084e00e342.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/0*DDlmRNtrmoOMFvcW"/></div></figure><p id="d308" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">这是两个值的简单区别。数值越接近，差异越小。我们在这里使用<code class="fe mn mo mp mq b">2</code>的能力只是为了去掉负数，这样<code class="fe mn mo mp mq b">(1 - 2) ^ 2</code>就和<code class="fe mn mo mp mq b">(2 - 1) ^ 2</code>一样了。除以<code class="fe mn mo mp mq b">2</code>只是为了进一步简化反向传播公式(见下文)。</p><p id="eb30" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">这种情况下的成本函数非常简单:</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="1f4b" class="mv lh iq mq b gy mw mx l my mz">function <strong class="mq ir">predictionCost</strong>(y, prediction) {<br/>  return (y - prediction) ** 2 / 2; // i.e. -&gt; 235.6<br/>}</span></pre><h1 id="86af" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">正向传播</h1><p id="d9f2" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">进行前向传播意味着对来自<code class="fe mn mo mp mq b">xTrain</code>和<code class="fe mn mo mp mq b">yTrain</code>数据集的所有训练样本进行预测，并计算这些预测的平均成本。</p><p id="da60" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">我们只是让我们的纳米神经元说出它在这一点上的意见，只是让他猜一猜如何换算温度。这可能是愚蠢的错误。平均成本将显示我们的模型现在是多么的错误。这个成本值非常有价值，因为通过改变纳米神经元参数<code class="fe mn mo mp mq b">w</code>和<code class="fe mn mo mp mq b">b</code>并再次进行正向传播，我们将能够评估纳米神经元在参数改变后是否变得更聪明。</p><p id="4027" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">将使用以下公式计算平均成本:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/50a9728f93452c18be8c75ae2f5099ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/0*8IZgMBIgGU2E2p37"/></div></figure><p id="201e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">其中<code class="fe mn mo mp mq b">m</code>是训练示例的数量(在我们的例子中是<code class="fe mn mo mp mq b">100</code>)。</p><p id="0285" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">下面是我们如何用代码实现它:</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="5a6d" class="mv lh iq mq b gy mw mx l my mz">function <strong class="mq ir">forwardPropagation</strong>(model, xTrain, yTrain) {<br/>  const m = xTrain.length;<br/>  const predictions = [];<br/>  let cost = 0;<br/>  for (let i = 0; i &lt; m; i += 1) {<br/>    const prediction = nanoNeuron.predict(xTrain[i]);<br/>    cost += predictionCost(yTrain[i], prediction);<br/>    predictions.push(prediction);<br/>  }<br/>  // We are interested in average cost.<br/>  cost /= m;<br/>  return [predictions, cost];<br/>}</span></pre><h1 id="ed27" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">反向传播</h1><p id="3100" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">现在，当我们知道纳米神经元的预测有多正确或错误(基于此时的平均成本)时，我们应该做些什么来使预测更精确呢？</p><p id="77bc" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">反向传播是这个问题的答案。反向传播是评估预测成本和调整纳米神经元参数<code class="fe mn mo mp mq b">w</code>和<code class="fe mn mo mp mq b">b</code>的过程，以便下一次预测更加精确。</p><p id="27e4" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">这就是机器学习看起来像魔法🧞‍♂️.的地方这里的关键概念是<strong class="jt ir">导数</strong>，它显示了采取什么步骤来接近成本函数最小值。</p><p id="0376" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">记住，找到一个成本函数的最小值是训练过程的最终目标。如果我们会发现<code class="fe mn mo mp mq b">w</code>和<code class="fe mn mo mp mq b">b</code>的值使得我们的平均成本函数很小，这将意味着纳米神经元模型做了非常好和精确的预测。</p><p id="ceda" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">衍生品是一个独立的大话题，我们不会在本文中讨论。MathIsFun 是一个很好的资源，可以让你对它有一个基本的了解。</p><p id="9722" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">关于导数，有一点可以帮助你理解反向传播是如何工作的，那就是导数的含义是函数曲线的切线，它指出了函数最小值的方向。</p><p id="07af" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated"><em class="js">图片来源:</em> <a class="ae lf" href="https://www.mathsisfun.com/calculus/derivatives-introduction.html" rel="noopener ugc nofollow" target="_blank"> <em class="js">马蒂斯芬</em> </a></p><p id="4260" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">例如，在上面的图中，你可以看到，如果我们在<code class="fe mn mo mp mq b">(x=2, y=4)</code>点，那么斜率告诉我们走<code class="fe mn mo mp mq b">left</code>和<code class="fe mn mo mp mq b">down</code>到达函数最小值。还要注意，斜率越大，我们向最小值移动的速度越快。</p><p id="1abe" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">我们的<code class="fe mn mo mp mq b">averageCost</code>函数对参数<code class="fe mn mo mp mq b">w</code>和<code class="fe mn mo mp mq b">b</code>的导数如下所示:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/a0b5328959a167234b7cef92e4efd22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/0*4EpQ3RjSimKE5FrJ"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/b2fbfccfcec2e60be7baad02de061a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/0*-IXvNtj0uUSMzG_P"/></div></figure><p id="659a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">其中<code class="fe mn mo mp mq b">m</code>是一些训练例子(在我们的例子中是<code class="fe mn mo mp mq b">100</code>)。</p><p id="733c" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated"><em class="js">你可以在这里</em>  <em class="js">阅读更多关于导数规则以及如何得到复杂函数的导数</em> <a class="ae lf" href="https://www.mathsisfun.com/calculus/derivatives-rules.html" rel="noopener ugc nofollow" target="_blank"> <em class="js">。</em></a></p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="b1fa" class="mv lh iq mq b gy mw mx l my mz">function <strong class="mq ir">backwardPropagation</strong>(predictions, xTrain, yTrain) {<br/>  const m = xTrain.length;<br/>  // At the beginning we don't know in which way our parameters 'w' and 'b' need to be changed.<br/>  // Therefore we're setting up the changing steps for each parameters to 0.<br/>  let dW = 0;<br/>  let dB = 0;<br/>  for (let i = 0; i &lt; m; i += 1) {<br/>    dW += (yTrain[i] - predictions[i]) * xTrain[i];<br/>    dB += yTrain[i] - predictions[i];<br/>  }<br/>  // We're interested in average deltas for each params.<br/>  dW /= m;<br/>  dB /= m;<br/>  return [dW, dB];<br/>}</span></pre><h1 id="0c51" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">训练模型</h1><p id="08aa" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">现在，我们知道如何评估我们的模型对于所有训练集示例的正确性(<em class="js">正向传播</em>)，我们也知道如何对纳米神经元模型的参数<code class="fe mn mo mp mq b">w</code>和<code class="fe mn mo mp mq b">b</code>(<em class="js">反向传播</em>)进行小的调整。但问题是，如果我们只运行一次前向传播和后向传播，我们的模型从训练数据中学习任何规律/趋势是不够的。你可以把它和给孩子上一天小学相比较。他/她应该去学校不是一次，而是日复一日，年复一年地学习一些东西。</p><p id="7d1e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">因此，我们需要为我们的模型多次重复向前和向后传播。这正是<code class="fe mn mo mp mq b">trainModel()</code>功能的作用。它就像是我们纳米神经元模型的“老师”:</p><ul class=""><li id="c929" class="ng nh iq jt b ju jv jy jz mg ni mi nj mk nk ko nl nm nn no bi translated">它将花一些时间(<code class="fe mn mo mp mq b">epochs</code>)在我们还有点愚蠢的纳米神经元模型上，并尝试训练/教授它，</li><li id="2f50" class="ng nh iq jt b ju np jy nq mg nr mi ns mk nt ko nl nm nn no bi translated">它将使用特定的“书籍”(<code class="fe mn mo mp mq b">xTrain</code>和<code class="fe mn mo mp mq b">yTrain</code>数据集)进行训练，</li><li id="32b9" class="ng nh iq jt b ju np jy nq mg nr mi ns mk nt ko nl nm nn no bi translated">它将通过使用学习率参数<code class="fe mn mo mp mq b">alpha</code>来推动我们的孩子更努力(更快)地学习</li></ul><p id="12e4" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">说几句学习率<code class="fe mn mo mp mq b">alpha</code>。这只是我们在反向传播期间计算的<code class="fe mn mo mp mq b">dW</code>和<code class="fe mn mo mp mq b">dB</code>值的乘数。因此，导数为我们指出了找到成本函数的最小值需要采取的方向(<code class="fe mn mo mp mq b">dW</code>和<code class="fe mn mo mp mq b">dB</code>符号)，它还为我们指出了需要多快到达那个方向(<code class="fe mn mo mp mq b">dW</code>和<code class="fe mn mo mp mq b">dB</code>绝对值)。现在，我们需要将这些步长乘以<code class="fe mn mo mp mq b">alpha</code>，以使我们的移动更快或更慢。有时，如果我们使用一个大值<code class="fe mn mo mp mq b">alpha</code>，我们可能会跳过最小值，永远找不到它。</p><p id="aa7c" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">与老师的类比是，他越是逼迫我们的“纳米孩子”，我们的“纳米孩子”就会学得越快，但是如果老师逼得太紧，“孩子”就会精神崩溃，什么也学不到🤯。</p><p id="b235" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">下面是我们如何更新模型的<code class="fe mn mo mp mq b">w</code>和<code class="fe mn mo mp mq b">b</code>参数:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/2a97d15ba479c9b19344477cfd048463.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/0*NBMCyhm3Oj-K4yLI"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/85f89e6549ab8dfbf19b1056e0ed1874.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*tXYiCrZRXUQlXvVmmQd7qw.png"/></div></figure><p id="1449" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">这是我们的培训师职能:</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="1628" class="mv lh iq mq b gy mw mx l my mz">function <strong class="mq ir">trainModel</strong>({model, epochs, alpha, xTrain, yTrain}) {<br/>  // The is the history array of how NanoNeuron learns.<br/>  const costHistory = [];</span><span id="4891" class="mv lh iq mq b gy nb mx l my mz">  // Let's start counting epochs.<br/>  for (let epoch = 0; epoch &lt; epochs; epoch += 1) {<br/>    // Forward propagation.<br/>    const [predictions, cost] = forwardPropagation(model, xTrain, yTrain);<br/>    costHistory.push(cost);</span><span id="0372" class="mv lh iq mq b gy nb mx l my mz">    // Backward propagation.<br/>    const [dW, dB] = backwardPropagation(predictions, xTrain, yTrain);<br/>    nanoNeuron.w += alpha * dW;<br/>    nanoNeuron.b += alpha * dB;<br/>  }</span><span id="4cc2" class="mv lh iq mq b gy nb mx l my mz">  return costHistory;<br/>}</span></pre><h1 id="82de" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">把所有的碎片放在一起</h1><p id="8d44" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">现在让我们使用上面创建的函数。</p><p id="fc71" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">让我们创建我们的纳米神经元模型实例。此时，纳米神经元不知道参数<code class="fe mn mo mp mq b">w</code>和<code class="fe mn mo mp mq b">b</code>应该设置什么值。所以我们随机设置<code class="fe mn mo mp mq b">w</code>和<code class="fe mn mo mp mq b">b</code>。</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="70eb" class="mv lh iq mq b gy mw mx l my mz">const w = Math.random(); // i.e. -&gt; 0.9492<br/>const b = Math.random(); // i.e. -&gt; 0.4570<br/>const nanoNeuron = new NanoNeuron(w, b);</span></pre><p id="23fe" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">生成训练和测试数据集。</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="25d5" class="mv lh iq mq b gy mw mx l my mz">const [xTrain, yTrain, xTest, yTest] = generateDataSets();</span></pre><p id="2a59" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">让我们在<code class="fe mn mo mp mq b">70000</code>时期用小的(<code class="fe mn mo mp mq b">0.0005</code>)步骤来训练模型。你可以摆弄这些参数，它们是根据经验定义的。</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="fb90" class="mv lh iq mq b gy mw mx l my mz">const epochs = 70000;<br/>const alpha = 0.0005;<br/>const trainingCostHistory = trainModel({model: nanoNeuron, epochs, alpha, xTrain, yTrain});</span></pre><p id="cd82" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">让我们检查一下成本函数在培训期间是如何变化的。我们期望培训后的费用会比以前低得多。这意味着纳米神经元变得更加聪明。相反的情况也是可能的。</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="6fc4" class="mv lh iq mq b gy mw mx l my mz">console.log('Cost before the training:', trainingCostHistory[0]); // i.e. -&gt; 4694.3335043<br/>console.log('Cost after the training:', trainingCostHistory[epochs - 1]); // i.e. -&gt; 0.0000024</span></pre><p id="cb05" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">这就是培训成本在不同时期的变化。在<code class="fe mn mo mp mq b">x</code>轴上是纪元编号 x1000。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/2dbb701bb2fb76c110c9157fe121a374.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*Gnq7d7_5eRJUo5MR"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Training cost change over the epochs</figcaption></figure><p id="4520" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">让我们来看看纳米神经元的参数，看看它学到了什么。我们期望纳米神经元参数<code class="fe mn mo mp mq b">w</code>和<code class="fe mn mo mp mq b">b</code>与我们在<code class="fe mn mo mp mq b">celsiusToFahrenheit()</code>函数中的参数<code class="fe mn mo mp mq b">w = 1.8</code>和<code class="fe mn mo mp mq b">b = 32</code>相似，因为我们的纳米神经元试图模仿它。</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="a613" class="mv lh iq mq b gy mw mx l my mz">console.log('NanoNeuron parameters:', {w: nanoNeuron.w, b: nanoNeuron.b}); // i.e. -&gt; {w: 1.8, b: 31.99}</span></pre><p id="83cc" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">评估我们的模型对测试数据集的准确性，看看我们的纳米神经元如何处理新的未知数据预测。对测试集进行预测的成本预计将接近训练成本。这将意味着纳米神经元在已知和未知数据上表现良好。</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="2873" class="mv lh iq mq b gy mw mx l my mz">[testPredictions, testCost] = forwardPropagation(nanoNeuron, xTest, yTest);<br/>console.log('Cost on new testing data:', testCost); // i.e. -&gt; 0.0000023</span></pre><p id="0f94" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">现在，由于我们看到我们的纳米神经元“孩子”在训练期间在“学校”表现良好，即使对于它没有看到的数据，他也可以正确地将摄氏温度转换为华氏温度，我们可以称之为“智能”，并问他一些问题。这是整个培训过程的最终目标。</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="ee96" class="mv lh iq mq b gy mw mx l my mz">const tempInCelsius = 70;<br/>const customPrediction = nanoNeuron.predict(tempInCelsius);<br/>console.log(`NanoNeuron "thinks" that ${tempInCelsius}°C in Fahrenheit is:`, customPrediction); // -&gt; 158.0002<br/>console.log('Correct answer is:', celsiusToFahrenheit(tempInCelsius)); // -&gt; 158</span></pre><p id="86f9" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">如此接近！和所有人一样，我们的纳米神经元是好的，但并不理想:)</p><p id="0bad" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">祝你学习愉快！</p><h1 id="0851" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">如何发射纳米神经元</h1><p id="ecfd" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">您可以克隆存储库并在本地运行它:</p><pre class="kq kr ks kt gt mr mq ms mt aw mu bi"><span id="8981" class="mv lh iq mq b gy mw mx l my mz">git clone https://github.com/trekhleb/nano-neuron.git<br/>cd nano-neuron</span><span id="665d" class="mv lh iq mq b gy nb mx l my mz">node ./NanoNeuron.js</span></pre><h1 id="9a7a" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">跳过机器学习概念</h1><p id="3cf1" class="pw-post-body-paragraph jq jr iq jt b ju me jw jx jy mf ka kb mg mh ke kf mi mj ki kj mk ml km kn ko ij bi translated">为了解释简单，跳过并简化了以下机器学习概念。</p><p id="71ac" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated"><strong class="jt ir">列车/测试装置拆分</strong></p><p id="1487" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">通常你有一大组数据。根据该集合中示例的数量，您可能希望对训练/测试集按 70/30 的比例进行拆分。在分割之前，应该随机打乱数据集中的数据。如果示例的数量很大(即数百万)，那么对于训练/测试数据集，拆分可能以更接近 90/10 或 95/5 的比例发生。</p><p id="c764" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated"><strong class="jt ir">网络带来力量</strong></p><p id="644e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">通常你不会注意到仅仅一个独立神经元的使用。力量就在这类神经元的<a class="ae lf" href="https://en.wikipedia.org/wiki/Neural_network" rel="noopener ugc nofollow" target="_blank">网络</a>中。网络可以学习更复杂的特性。纳米神经元本身看起来更像简单的线性回归，而不是神经网络。</p><p id="63e1" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated"><strong class="jt ir">输入归一化</strong></p><p id="e72b" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">在训练之前，最好将输入值<a class="ae lf" href="https://www.jeremyjordan.me/batch-normalization/" rel="noopener ugc nofollow" target="_blank">标准化</a>。</p><p id="31f9" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated"><strong class="jt ir">矢量化实现</strong></p><p id="be54" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">对于网络来说，矢量化(矩阵)计算比<code class="fe mn mo mp mq b">for</code>循环要快得多。通常，如果以矢量化形式实现并使用<a class="ae lf" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> Numpy </a> Python 库进行计算，前向/后向传播会工作得更快。</p><p id="2e8d" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated"><strong class="jt ir">成本函数的最小值</strong></p><p id="2118" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">我们在这个例子中使用的成本函数过于简化。它应该有<a class="ae lf" href="https://stackoverflow.com/questions/32986123/why-the-cost-function-of-logistic-regression-has-a-logarithmic-expression/32998675" rel="noopener ugc nofollow" target="_blank">对数分量</a>。改变成本函数也将改变其导数，因此反向传播步骤也将使用不同的公式。</p><p id="289f" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated"><strong class="jt ir">激活功能</strong></p><p id="8262" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb mg kd ke kf mi kh ki kj mk kl km kn ko ij bi translated">正常情况下，神经元的输出应该通过激活函数，如<a class="ae lf" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> Sigmoid </a>或<a class="ae lf" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>或其他。</p></div><div class="ab cl nx ny hu nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="ij ik il im in"><blockquote class="jn jo jp"><p id="30b5" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated"><em class="iq">更多更新和新文章</em> <a class="ae lf" href="https://twitter.com/Trekhleb" rel="noopener ugc nofollow" target="_blank"> <em class="iq">在 Twitter 上关注我</em> </a></p></blockquote></div></div>    
</body>
</html>