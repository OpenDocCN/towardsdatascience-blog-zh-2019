# 使用张量流执行的简单神经网络模型

> 原文：<https://towardsdatascience.com/simple-neural-network-model-using-tensorflow-eager-execution-997bbadef84a?source=collection_archive---------14----------------------->

![](img/81192ebd358abebd38821d39347cde49.png)

Reference: [https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Eager_Execution_Enabled.ipynb](https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Eager_Execution_Enabled.ipynb)

**简介**

渴望执行是 TensorFlow (TF)中一种从零开始构建深度学习模型的巧妙方法。它允许您构建原型模型，而没有 TF 通常使用的图形化方法带来的麻烦。

例如，使用急切执行，不需要启动图形会话来执行张量计算。这意味着更快的调试，因为您可以即时检查每一行计算，而不需要将计算包装在图形会话中。

然而，作为免责声明，使用急切执行需要一些关于深度学习中使用的矩阵代数概念的知识，特别是关于如何在神经网络中进行前向传递。如果你正在寻找一些更高级的现成的东西，我建议使用 TF 或 PyTorch 中的 Keras API。

本文将通过描述构建、训练和评估一个简单的多层感知器的过程，提供一个如何使用热切执行的例子。

**建筑和符号**

此示例中构建的神经网络由一个输入层、一个隐藏层和一个输出层组成。输入层包含 3 个节点，隐藏层包含 20 个节点，输出层包含 1 个节点。输出值是连续的(即神经网络执行回归)。

输入层、隐藏层和输出层的值以及层之间的权重可以表示为矩阵。对隐藏层和输出层的偏差可以表示为向量(具有一行或一列的矩阵的特殊情况)。下图显示了每个矩阵和向量的维度。

![](img/79937a5783864e0f97ee971b4a44046d.png)

Notations and dimensions for matrices and vectors

**开始急切执行**

在导入本例所需的依赖项(主要是 NumPy 和 TF)之后，如果您没有使用 TF 2.0，您将需要启用急切执行。下面的代码片段展示了如何启用急切执行。

**为培训和评估准备数据**

下一步是通过使用 NumPy 的 random 模块，随机生成一些用于训练和评估的数据(当然是为了说明的目的)。使用这种方法，我创建了两组独立的数据，一组用于训练，另一组用于评估。

每组数据包含一个输入数组和一个输出数组。输入数组的形状为(观测值的数量，要素的数量)，而输出数组的形状为(观测值的数量，每个观测值的输出值的数量)。要素数量对应于输入图层中的结点数量，而每个观测值的输出值数量对应于输出图层中的结点数量。

在生成数据之后，我将测试数据分成几批，以便进行更有效的评估。训练数据也将被分成批次，但是在训练过程本身中完成。

下面的片段显示了我如何准备数据。

**建立模型**

我在这里做的是创建一个 Python 类，它存储负责权重和偏差初始化、正向传递、反向传播以及权重和偏差更新的代码。

权重和偏差通过从标准正态分布中抽取随机值来初始化。权重的随机初始化通常优于用值 0 或 1 初始化权重，以便减少出现诸如消失梯度等问题的机会。

正向传递可由以下等式描述。relu()代表整流线性单位函数，它以非线性方式转换输入和偏差的线性组合。输出 Y 的等式没有变换函数，因为输出应该是连续值。顺便提一下，如果期望输出是分类的，则在第二个等式中将需要诸如 sigmoid 或 softmax 之类的非线性变换函数。

![](img/8b08433d7a409ae90f11322709df25d9.png)

Matrix algebra for the forward pass

损失的反向传播以及权重和偏差的更新只需要几行代码(分别在模型类的 loss()和 backward()方法中)。

下面相当长的片段展示了如何在一个类中实现模型构建过程。额外的 compute_output()方法是前向传递算法的包装器，便于用户选择用于模型训练和评估的硬件设备(CPU 或 GPU)。

您可能已经注意到了类中使用的函数 tf.cast()。原因是，由于前面代码片段中的 from_tensor_slices()方法返回 tf.float64 数据格式的张量，而矩阵运算(例如 tf.matmul())只能处理 tf.float32 数据格式的张量，所以会触发这个奇怪的错误。我没有在 TF 2.0 上尝试过急切执行，所以我不确定这个问题是否已经在这个新版本中解决了。我所知道的是，这个数据格式的问题肯定发生在我在这个例子中使用的 TF 版本(即 1.31.1)中，所以如果在 TF 的旧版本上使用急切执行，这是需要注意的。

**训练模特**

准备好数据，建立好模型之后，接下来就是训练模型了。模型训练非常简单，只需要几行代码。这里的基本思想是对每个时期的每批数据重复以下步骤:通过模型输入张量以获得预测张量，计算损失，反向传播损失，并更新权重和偏差。在每个时期，训练数据将被随机分成不同的批次，以提高模型训练的计算效率，并帮助模型更好地泛化。下面的代码片段说明了如何通过热切的执行来完成培训。

**评估模型**

最后一步是使用测试集评估模型。这样做的代码类似于训练的代码，但是没有反向传播以及权重和偏差的更新。

**结论**

虽然急切执行很容易使用，但我想再次强调，这是一种低级的方法。我建议不要使用渴望执行，除非:1)你正在做的工作需要你从头开始构建深度学习模型(例如，关于深度学习模型的研究/学术工作)，2)你正在试图理解深度学习背后正在发生的数学事情，或者 3)你只是喜欢从零开始构建东西。

话虽如此，但我认为急切执行是一种很好的方法，可以帮助你更好地理解我们进行深度学习时实际发生的事情，而不必摆弄复杂的图表或传统 TF 方法带来的其他令人困惑的东西。

我为这个例子创建的 Google Colab 笔记本可以在这里找到。