# 每个数据科学家都应该知道的检测异常值/异常的 5 种方法(Python 代码)

> 原文：<https://towardsdatascience.com/5-ways-to-detect-outliers-that-every-data-scientist-should-know-python-code-70a54335a623?source=collection_archive---------0----------------------->

## 检测异常对任何企业都至关重要，无论是通过识别故障还是采取主动。本文讨论了识别这些异常的 5 种不同方法。

![](img/6e21d16d04192386fb873aeab99fdced.png)

Photo by [Will Myers](https://unsplash.com/@will_myers?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

# 什么是异常/异常值？

![](img/a173ca77be332804c180085a0393744e.png)

**在统计学**中，离群值是不属于某个总体的数据点。这是一个远离其他价值观的反常观察。离群值是指与结构良好的数据相背离的观察结果。

例如，你可以清楚地看到这个列表中的异常值:[20，24，22，19，29，18， **4300** ，30，18]

当观察值只是一串数字并且是一维的时，很容易识别它，但是当你有成千上万的观察值或多维观察值时，你将需要更聪明的方法来检测这些值。这就是本文将要讨论的内容。

# 我们为什么要关心异常现象？

检测异常值是数据挖掘的核心问题之一。数据的新兴扩张和持续增长以及物联网设备的普及，促使我们重新思考处理异常的方式，以及通过观察这些异常可以构建的用例。

我们现在有智能手表和腕带，可以每隔几分钟检测一次我们的心跳。检测心跳数据的异常有助于预测心脏病。交通模式的异常有助于预测事故。它还可用于识别网络基础设施和服务器间流量的瓶颈。因此，基于检测异常的用例及解决方案是无限的。

我们需要检测异常的另一个原因是，在为机器学习模型准备数据集时，检测所有异常值并将其剔除或分析以了解为什么它们会出现在第一位置非常重要。

现在，让我们从最简单的方法开始，探索检测异常的 5 种常见方法。

# **方法 1 —标准偏差:**

在统计学中，如果数据分布近似正态，那么大约 68%的数据值位于平均值的一个标准偏差内，大约 95%位于两个标准偏差内，大约 99.7% 位于三个标准偏差内

![](img/13de1c30a00d3428da31ceaba8839b72.png)

因此，如果有任何数据点超过标准偏差的 3 倍，那么这些点很可能是异常点或异常值。

让我们看一些代码。

这段代码的输出是一个大于 80 小于-40 的值的列表。请注意，我传递的数据集是一维数据集。现在，让我们探索多维数据集的更高级的方法。

# 方法 2 —箱线图

![](img/1806da893c860d4b5bc5e9ab65417f27.png)

箱线图是数字数据通过其分位数的图形描述。这是一种非常简单但有效的可视化离群值的方法。将下方和上方的胡须视为数据分布的边界。显示在须状物之上或之下的任何数据点都可以被认为是异常值或异常值。以下是绘制箱线图的代码:

上面的代码显示了下图。如你所见，它认为所有高于 75 或低于-35 的都是异常值。结果非常接近上面的方法 1。

![](img/60fca1fba7592f161eeda6afaae3ff6a.png)

## 箱线图剖析:

I **四分位范围** ( **IQR** )的概念用于构建箱线图。IQR 是统计学中的一个概念，用于通过将数据集分成四分位数来测量统计离差和数据可变性。

简而言之，任何数据集或任何一组观察值都根据数据的值以及它们与整个数据集的比较情况被划分为四个定义的区间。四分位数将数据分为三个点和四个区间。

![](img/c78b29356480921ab5bbde3d6a011008.png)

四分位距(IQR)很重要，因为它用于定义异常值。它是第三个四分位数和第一个四分位数的差值(IQR = Q3 -Q1)。在这种情况下，异常值被定义为低于(Q1 1.5x IQR)或*箱线图下须*或高于(Q3 + 1.5x IQR)或*箱线图上须*的观测值。

![](img/3879d5789a0e96e72bcdf48527535e56.png)

Source: Wikipedia

# 方法 3— DBScan 聚类:

DBScan 是一种聚类算法，用于将数据分组。它还用作基于密度的异常检测方法，使用一维或多维数据。其他聚类算法，如 k-means 和层次聚类，也可以用来检测异常值。在这种情况下，我将向您展示一个使用 DBScan 的示例，但在开始之前，让我们先了解一些重要的概念。DBScan 有三个重要的概念:

*   ***核心点:*** 为了理解核心点的概念，我们需要访问一些用来定义 DBScan 作业的超参数。第一个超参数(HP)是 **min_samples。**这是形成一个聚类所需的核心点的最小数量**。**第二个重要的 HP 是 **eps。eps** 是两个样本之间的最大距离，它们被认为是在同一个聚类中。
*   ***边界点*** 与核心点在同一个群中，但是远离群的中心。

![](img/289c8554c419a57afce26e22ce295559.png)

Source:[https://stackoverflow.com/questions/34394641/dbscan-clustering-what-happens-when-border-point-of-one-cluster-is-considered](https://stackoverflow.com/questions/34394641/dbscan-clustering-what-happens-when-border-point-of-one-cluster-is-considered)

*   其他的都叫做 ***噪声点*** *，*那些是不属于任何聚类的数据点。它们可能是异常的，也可能是非异常的，需要进一步研究。现在，让我们看一些代码。

上面代码的输出是 **94。**这是噪音点的总数。SKLearn 将噪音点标记为(-1)。这种方法的缺点是维度越高，它变得越不精确。你还需要做一些假设，比如估算 T2 每股收益的正确价值，这可能很有挑战性。

# 方法 4—隔离林:

隔离森林是一种无监督学习算法，属于集成决策树家族。这种方法不同于所有以前的方法。所有以前的方法都是试图找到数据的正常区域，然后将这个定义区域之外的任何东西识别为异常值或异常值。

这种方法的工作原理不同。它通过给每个数据点分配一个分数来明确隔离异常，而不是描绘和构建正常点和区域。它利用了这样一个事实，即异常是少数数据点，并且它们具有与正常情况下非常不同的属性值。这种算法在处理非常高维的数据集时非常有效，并且被证明是一种非常有效的异常检测方法。因为本文关注的是实现而不是技术，所以我不会进一步讨论算法是如何工作的。然而，在这篇[论文](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)中涵盖了它如何工作的全部细节。

现在，让我们来探索代码:

这段代码将输出数组中每个数据点的预测。如果结果为-1，则意味着该特定数据点是异常值。如果结果为 1，则意味着该数据点不是异常值

# 方法 5—稳健随机采伐森林:

随机切割森林(RCF)算法是亚马逊的无监督算法，用于检测异常。它也通过关联异常分数来工作。低分值表示该数据点被视为“正常”高值表示数据中存在异常。“低”和“高”的定义取决于应用，但是通常的实践表明，超过平均分数的三个标准差的分数被认为是异常的。算法的细节可以在这篇[论文](http://proceedings.mlr.press/v48/guha16.pdf)中找到。

这个算法的伟大之处在于它可以处理非常高维的数据。它还可以处理实时流数据(内置于 AWS Kinesis Analytics)和离线数据。

我在下面的视频中详细解释了这个概念:

本文展示了与隔离林相比的一些性能基准。这是论文的结果，表明 RCF 比隔离森林更准确、更快速。

![](img/ca642eb434a344407414e4ba38846815.png)

完整的示例代码可在此处找到:

[](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/random_cut_forest) [## aw slabs/Amazon-sage maker-示例

### 展示如何在亚马逊 SageMaker 中应用机器学习和深度学习的示例笔记本…

github.com](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/random_cut_forest) 

## 结论:

我们生活在一个数据每秒都在变大的世界里。如果使用不当，数据的价值会随着时间的推移而降低。无论是在线发现数据流中的异常，还是离线发现数据集中的异常，对于识别业务中的问题或构建主动解决方案以在问题发生前发现问题至关重要，甚至在探索性数据分析(EDA)阶段为 ML 准备数据集也是如此。我希望这篇文章对你有用，请在下面的评论区告诉我你的想法。