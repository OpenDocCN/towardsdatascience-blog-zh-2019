<html>
<head>
<title>Dive Really Deep into YOLO v3: A Beginner’s Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入了解 YOLO v3:初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dive-really-deep-into-yolo-v3-a-beginners-guide-9e3d2666280e?source=collection_archive---------1-----------------------#2019-12-30">https://towardsdatascience.com/dive-really-deep-into-yolo-v3-a-beginners-guide-9e3d2666280e?source=collection_archive---------1-----------------------#2019-12-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/3f00d01f1a8ae84af8dd9cce35cbab57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V9N0HzCQaK7Co3jQlw9q0w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Screenshot from a video made by <a class="ae kc" href="https://www.youtube.com/channel/UClVWrKrmoeNM7A9nkEMeYzA" rel="noopener ugc nofollow" target="_blank">Joseph Redmon</a> on <a class="ae kc" href="https://www.youtube.com/watch?v=MPU2HistivI" rel="noopener ugc nofollow" target="_blank">Youtube</a></figcaption></figure><p id="adc5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">原载于 2019 年 12 月 30 日</em><a class="ae kc" href="https://yanjia.li/dive-really-deep-into-yolo-v3-a-beginners-guide/" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://www . yanjia . Li</em></a><em class="lb">。</em></p><blockquote class="lc ld le"><p id="d193" class="kd ke lb kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated"><em class="iq">完整源代码请前往</em><a class="ae kc" href="https://github.com/ethanyanjiali/deep-vision/tree/master/YOLO/tensorflow" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://github . com/ethanyangali/deep-vision/tree/master/YOLO/tensor flow</em></a><em class="iq">。我真的很感谢你的明星支持我的努力。</em></p></blockquote><h1 id="bdaa" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">序</h1><p id="4aed" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">当自动驾驶汽车在道路上行驶时，它是如何知道摄像头图像中其他车辆的位置的？当人工智能放射科医生阅读 x 光片时，它如何知道病变(异常组织)在哪里？今天，我将通过这个迷人的算法，它可以识别给定图像的类别，还可以定位感兴趣的区域。近年来，有许多算法被引入到深度学习方法中来解决对象检测问题，如 R-CNN，Faster-RCNN 和 Single Shot Detector。其中，我最感兴趣的是一个叫 YOLO 的模特——你只会看一眼。这款车型如此吸引我，不仅是因为它有趣的名字，还有一些对我来说真正有意义的实用设计。2018 年，该型号的最新 V3 已经发布，它实现了许多新的最先进的性能。因为我以前编写过一些 GANs 和图像分类网络，而且 Joseph Redmon 在论文中以一种非常简单的方式描述了它，所以我认为这个检测器只是 CNN 和 FC 层的另一个堆栈，只是神奇地工作得很好。</p><p id="de16" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但我错了。</p><p id="d2b1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也许是因为我比一般的工程师更笨，我发现对我来说把这个模型从纸上翻译成实际代码真的很难。即使我在几周内成功做到了这一点(有一次我放弃了，把它放了几个星期)，我发现让它工作起来对我来说更加困难。有很多关于 YOLO V3 的博客，GitHub repos，但大多数只是给出了架构的一个非常高层次的概述，不知何故他们就成功了。更糟糕的是，论文本身太冷了，它没有提供实现的许多关键细节，我不得不阅读作者最初的 C 实现(我最后一次编写 C 是什么时候？也许在大学？)来证实我的一些猜测。当有一个 bug 时，我通常不知道它为什么会出现。然后，我最终一步一步地手动调试它，并用我的小计算器计算那些公式。</p><p id="a569" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还好这次我没有放弃，终于成功了。但与此同时，我也强烈地感觉到，互联网上应该有一个更全面的指南来帮助像我这样的哑巴理解这个系统的每个细节。毕竟，如果一个细节出错，整个系统将很快崩溃。我敢肯定，如果我不写下来，我也会在几周内忘记所有这些。所以，我在这里，给你呈现这个“深入 YOLO V3:初学者指南”。我希望你会喜欢它。</p><h1 id="97fa" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">先决条件</h1><p id="7801" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">在进入网络本身之前，我需要先澄清一些先决条件。作为读者，你应该:</p><p id="9b0a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.了解卷积神经网络和深度学习的基础知识<br/> 2。了解物体探测任务<br/> 3 的思路。对算法内部如何工作有好奇心</p><p id="ea22" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你在前两项上需要帮助，有很多优秀的资源，如<a class="ae kc" href="https://www.udacity.com/course/computer-vision-nanodegree--nd891" rel="noopener ugc nofollow" target="_blank"> Udacity 计算机视觉纳米学位</a>、<a class="ae kc" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank"> Cousera 深度学习专业化</a>和<a class="ae kc" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank"> Stanford CS231n </a> <br/>如果你只是想构建一些东西来用你的自定义数据集快速检测一些对象，请查看这个<a class="ae kc" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank"> Tensorflow 对象检测 API </a></p><h1 id="09cb" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">YOLO V3</h1><p id="d789" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">YOLO V3 是对以前的 YOLO 检测网络的改进。与以前的版本相比，它具有多尺度检测，更强的特征提取网络，以及损失函数的一些变化。因此，这个网络现在可以探测到更多的目标，从大到小。当然，就像其他单次检测器一样，YOLO V3 运行速度也很快，并使实时推断在 GPU 设备上成为可能。嗯，作为一个物体检测的初学者，你可能没有一个清晰的图像，他们在这里意味着什么。但是你会在我后面的文章中逐渐理解它们。目前，只要记住 YOLO V3 是截至 2019 年实时对象检测方面最好的模型之一。</p><h2 id="3d7f" class="ml lj iq bd lk mm mn dn lo mo mp dp ls ko mq mr lw ks ms mt ma kw mu mv me mw bi translated"><strong class="ak">网络架构</strong></h2><figure class="my mz na nb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/6c2cebb93ef51eeb1219ac463f2fc1d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hULeMGlxnjjvuf7Fx7kuaA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Diagram by myself</figcaption></figure><p id="48fd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们来谈谈这个网络在高层次图中看起来是什么样子(尽管，网络架构是实现中最不耗时的部分)。整个系统可以分为两大部分:特征提取器和检测器；两者都是多尺度。当一幅新图像进来时，它首先通过特征提取器，这样我们可以在三个(或更多)不同的尺度上获得特征嵌入。然后，这些特征被馈入检测器的三个(或更多)分支，以获得包围盒和类别信息。</p><p id="85f0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">暗网-53 </strong></p><p id="52f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">YOLO V3 使用的特征提取器被称为 Darknet-53。你可能对 YOLO·V1 之前的暗网版本很熟悉，那里只有 19 层。但那是几年前的事了，图像分类网络已经从仅仅是深层次的堆叠发展了很多。ResNet 带来了跳过连接的想法，以帮助激活通过更深的层传播，而不会减少梯度。Darknet-53 借用了这一思想，成功地将网络从 19 层扩展到 53 层，从下图可以看出。</p><figure class="my mz na nb gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/41c4c47fcc40b27e2157ab107cdf64d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*a_hU7H_Av2YiUEFaui635w.jpeg"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Diagram from <code class="fe nd ne nf ng b"><a class="ae kc" href="https://arxiv.org/abs/1804.02767" rel="noopener ugc nofollow" target="_blank">YOLOv3: An Incremental Improvement</a></code></figcaption></figure><p id="df49" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个很好理解。将每个矩形中的层视为残余块。整个网络是一个由多个模块组成的链，在模块之间有两个 Conv 层以减少维度。在块内部，只有一个瓶颈结构(1x1 后跟 3x3)加上一个跳过连接。如果目标是像 ImageNet 一样进行多类分类，将添加一个平均池和 1000 路完全连接层以及 softmax 激活。</p><p id="a411" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，在对象检测的情况下，我们不会包括这个分类头。相反，我们将为这个特征提取器添加一个“检测”头。由于 YOLO V3 被设计为多尺度检测机，我们也需要多尺度的特征。因此，来自最后三个残差块的特征都被用于后面的检测。在下图中，我假设输入是 416x416，因此三个比例向量将是 52x52、26x26 和 13x13。请注意，如果输入尺寸不同，输出尺寸也会不同。</p><figure class="my mz na nb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/e94e44d6024e033585d8586b3b1416ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SzyNALdsE9pDCpCvtqH7ZQ.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Diagram by myself</figcaption></figure><p id="9064" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">多尺度探测器</strong></p><p id="35a0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们有了三个特征向量，我们现在可以将它们输入到检测器中。但是我们应该如何构造这个探测器呢？不幸的是，作者在这篇论文中没有解释这一部分。但是我们仍然可以看看他在 Github 上发布的源代码。通过这个<a class="ae kc" href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg#L549" rel="noopener ugc nofollow" target="_blank">配置文件</a>，在最终的 1x1 Conv 层之前使用多个 1x1 和 3x3 Conv 层来形成最终输出。对于中比例尺和小比例尺，它还会连接先前比例尺的要素。通过这样做，小规模检测也可以受益于大规模检测的结果。</p><figure class="my mz na nb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/6cd548cea8a67880059f84dda8d900ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kVFx54oUhBWzUdHzbFDsiw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Diagram by myself</figcaption></figure><p id="039e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设输入图像为(416，416，3)，检测器的最终输出将为<em class="lb"> [(52，52，3，(4 + 1 +数量类))，(26，26，3，(4 + 1 +数量类))，(13，13，3，(4 + 1 +数量类))]。</em>列表中的三个项目代表三种秤的检测。但是这个<em class="lb">52 x 52 x 3 x(4+1+num _ classes)</em>矩阵中的单元格是什么意思呢？好问题。这就把我们带到了 2019 年前物体检测算法中最重要的概念:锚盒(prior box)。</p><p id="9d46" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">锚箱</strong></p><p id="7b43" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">物体检测的目标是得到一个包围盒和它的类。包围盒通常以一种标准化的 xmin，ymin，xmax，ymax 格式表示。例如，0.5 xmin 和 0.5 ymin 表示框的左上角在图像的中间。直观上，如果我们想得到一个像 0.5 这样的数值，就面临着一个回归问题。我们还不如让网络预测值，并使用均方差来与实际情况进行比较。然而，由于盒子的比例和长宽比的差异很大，研究人员发现，如果我们只是使用这种“暴力”的方式来获得一个包围盒，网络真的很难收敛。因此，在 fast-RCNN 论文中，提出了锚盒的概念。</p><p id="ebb3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">锚定框是先前的框，其可以具有不同的预定义纵横比。这些纵横比在训练之前通过在整个数据集上运行 K-means 来确定。但是盒子固定在哪里呢？我们需要引入一个叫做网格的新概念。在“古老”的 2013 年，算法通过使用一个窗口滑过整个图像并在每个窗口上运行图像分类来检测对象。然而，这是如此低效，以至于研究人员提议使用 Conv 网一次性计算整个图像(从技术上讲，只有当你并行运行卷积核时。)由于卷积输出特征值的正方形矩阵(如 YOLO 的 13×13、26×26 和 52×52)，我们将该矩阵定义为“网格”，并为网格的每个单元分配锚框。换句话说，定位框定位到网格单元，它们共享同一个质心。一旦我们定义了这些锚，我们就可以确定基础事实框与锚框有多少重叠，并选择具有最佳 IOU 的一个并将它们耦合在一起。我猜你也可以声称地面真相箱锚定到这个锚定箱。在我们后面的训练中，我们现在可以预测这些边界框的偏移，而不是预测来自西部的坐标。这是因为我们的地面真相框应该看起来像我们选择的锚框，只需要细微的调整，这给了我们一个很好的训练开端。</p><figure class="my mz na nb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/dacdb8ad402b652952a8cc23cb33f321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yQFsd4vMDWVWikme3-Mb8g.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Diagram by myself</figcaption></figure><p id="f93d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 YOLO v3 中，每个网格单元有三个锚盒。我们有三个等级的网格。因此，我们将为每个秤配备 52x52x3、26x26x3 和 13x13x3 锚盒。对于每个锚盒，我们需要预测 3 件事:</p><p id="17eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.相对于锚箱的位置偏移:<em class="lb"> tx，ty，tw，th </em>。这有 4 个值。<br/> 2。指示此框是否包含对象的对象性分数。这有 1 个值。<br/> 3。类别概率告诉我们这个盒子属于哪个类别。这有 num_classes 个值。</p><p id="26bf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总的来说，我们为一个锚盒预测了<em class="lb"> 4 + 1 + num_classes </em>个值，这就是为什么我们的网络输出一个形状为<em class="lb">52x 52 x3x(4+1+num _ classes)</em>的矩阵，正如我之前提到的。<em class="lb"> tx，ty，tw，th </em>不是边界框的真实坐标。这只是相对于特定锚盒的相对偏移量。我会在后面的损失函数部分解释这三个预测。</p><p id="9aa9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Anchor box 不仅使检测器的实现变得更加困难和容易出错，而且如果你想得到最好的结果，它还在训练之前引入了一个额外的步骤。所以，就我个人而言，我非常讨厌它，觉得这个锚箱的想法更像是一个黑客，而不是一个真正的解决方案。2018 年和 2019 年，研究人员开始质疑锚盒的必要性。像 CornerNet、Object as Points 和 FCOS 这样的论文都讨论了在没有锚盒帮助的情况下从头训练对象检测器的可能性。</p><h2 id="fb29" class="ml lj iq bd lk mm mn dn lo mo mp dp ls ko mq mr lw ks ms mt ma kw mu mv me mw bi translated">损失函数</h2><p id="418b" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">有了最终的检测输出，我们现在可以计算相对于地面真实标签的损失。损失函数包括四个部分(或者五个，如果你分开 noobj 和 obj):质心(xy)损失、宽度和高度(wh)损失、对象(obj 和 noobj)损失和分类损失。放在一起时，公式是这样的:</p><pre class="my mz na nb gt nk ng nl nm aw nn bi"><span id="182a" class="ml lj iq ng b gy no np l nq nr">Loss = Lambda_Coord * Sum(Mean_Square_Error((tx, ty), (tx’, ty’) * obj_mask)<br/> + Lambda_Coord * Sum(Mean_Square_Error((tw, th), (tw’, th’) * obj_mask)<br/> + Sum(Binary_Cross_Entropy(obj, obj’) * obj_mask) + Lambda_Noobj * Sum(Binary_Cross_Entropy(obj, obj’) * (1 -obj_mask) * ignore_mask)<br/> + Sum(Binary_Cross_Entropy(class, class’))</span></pre><p id="a961" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这看起来很吓人，但是让我把它们一一分解并解释。</p><pre class="my mz na nb gt nk ng nl nm aw nn bi"><span id="8f7a" class="ml lj iq ng b gy no np l nq nr">xy_loss = Lambda_Coord * Sum(Mean_Square_Error((tx, ty), (tx’, ty’)) * obj_mask)</span></pre><p id="8be7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一部分是包围盒形心的损失。<em class="lb"> tx </em>和<em class="lb"> ty </em>是相对于地面的质心位置。<em class="lb"> tx' </em>和<em class="lb"> ty' </em>是直接来自检测器的质心预测。这种损失越小，预测和地面实况的质心就越接近。由于这是一个回归问题，我们在这里使用均方差。此外，如果对于某些细胞没有来自地面真理的对象，我们不需要将该细胞的损失包括在最终损失中。因此我们在这里也乘以<em class="lb"> obj_mask </em>。<em class="lb"> obj_mask </em>为 1 或 0，表示是否有物体。事实上，我们可以使用<em class="lb"> obj </em>作为<em class="lb"> obj_mask </em>，<em class="lb"> obj </em>是我将在后面介绍的客观性分数。需要注意的一点是，我们需要对地面实况进行一些计算，以得到这个<em class="lb"> tx </em>和<em class="lb"> ty </em>。所以，我们先来看看如何得到这个值。正如作者在论文中所说:</p><pre class="my mz na nb gt nk ng nl nm aw nn bi"><span id="5cb5" class="ml lj iq ng b gy no np l nq nr">bx = sigmoid(tx) + Cx<br/>by = sigmoid(ty) + Cy</span></pre><p id="356c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的<em class="lb"> bx </em>和<em class="lb"> by </em>是我们通常用来作为质心位置的绝对值。例如，<em class="lb"> bx = 0.5，by = 0.5 </em>表示这个盒子的质心就是整个图像的中心。然而，由于我们要计算锚点的质心，我们的网络实际上是预测相对于网格单元左上角的质心。为什么是网格单元？因为每个锚定框都绑定到一个网格单元，所以它们共享同一个质心。所以对网格单元的差异可以代表对锚盒的差异。在上面的公式中，<em class="lb"> sigmoid(tx) </em>和<em class="lb"> sigmoid(ty) </em>是相对于网格单元的质心位置。例如，<em class="lb"> sigmoid(tx) = 0.5，sigmoid(ty) = 0.5 </em>表示质心是当前网格单元的中心(但不是整个图像)。<em class="lb"> Cx </em>和<em class="lb"> Cy </em>代表当前网格单元左上角的绝对位置。因此，如果网格单元是网格 13x13 的第二行第二列中的单元，那么<em class="lb"> Cx = 1，Cy = 1 </em>。而如果我们将这个网格单元位置加上相对质心位置，我们将得到绝对质心位置<em class="lb"> bx = 0.5 + 1 </em>和<em class="lb"> by = 0.5 + 1 </em>。当然，作者不会告诉你，你也需要通过除以网格大小来归一化，所以真正的<em class="lb"> bx </em>将是<em class="lb"> 1.5/13 = 0.115 </em>。好了，现在我们理解了上面的公式，我们只需要把它反过来，这样我们就可以从<em class="lb"> bx </em>得到<em class="lb"> tx </em>，以便把我们原来的地面真相翻译成目标标签。最后，<em class="lb"> Lambda_Coord </em>是 Joe 在 YOLO v1 论文中引入的权重。这是更强调本地化而不是分类。他建议的值是 5。</p><figure class="my mz na nb gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/1ba6b9b6b5ad314ef8850747a31d6a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*a7seKlMjTN4yXvmTcTuVSw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Diagram from <code class="fe nd ne nf ng b"><a class="ae kc" href="https://arxiv.org/abs/1804.02767" rel="noopener ugc nofollow" target="_blank">YOLOv3: An Incremental Improvement</a></code></figcaption></figure><pre class="my mz na nb gt nk ng nl nm aw nn bi"><span id="b2b8" class="ml lj iq ng b gy no np l nq nr">wh_loss = Lambda_Coord * Sum(Mean_Square_Error((tw, th), (tw’, th’)) * obj_mask)</span></pre><p id="2e5d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一个是宽度和高度的损失。再次，作者说:</p><pre class="my mz na nb gt nk ng nl nm aw nn bi"><span id="22e9" class="ml lj iq ng b gy no np l nq nr">bw = exp(tw) * pw<br/>bh = exp(th) * ph</span></pre><p id="f5d6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里<em class="lb"> bw </em>和<em class="lb"> bh </em>仍然是整个图像的绝对宽度和高度。<em class="lb"> pw </em>和<em class="lb"> ph </em>是前一个盒子的宽度和高度(又名。锚箱，为什么有这么多名字)。我们在这里取<em class="lb"> e^(tw) </em>是因为<em class="lb"> tw </em>可能是负数，但是在现实世界中宽度不会是负数。所以这个<em class="lb"> exp() </em>会让它为正。并且我们乘以先前的框宽度<em class="lb"> pw </em>和<em class="lb"> ph </em>，因为预测<em class="lb"> exp(tw) </em>是基于锚框的。所以这个乘法给了我们真正的宽度。身高也一样。同样，我们可以在计算损耗时，将上面的公式反过来将<em class="lb"> bw </em>和<em class="lb"> bh </em>转化为<em class="lb"> tx </em>和<em class="lb"> th </em>。</p><pre class="my mz na nb gt nk ng nl nm aw nn bi"><span id="be0c" class="ml lj iq ng b gy no np l nq nr">obj_loss = Sum(Binary_Cross_Entropy(obj, obj’) * obj_mask)</span><span id="f838" class="ml lj iq ng b gy nt np l nq nr">noobj_loss = Lambda_Noobj * Sum(Binary_Cross_Entropy(obj, obj’) * (1 — obj_mask) * ignore_mask)</span></pre><p id="ad1f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第三项和第四项是客体性和非客体性得分损失。对象性表示当前单元格中有对象的可能性。与 YOLO v2 不同，这里我们将使用二进制交叉熵而不是均方误差。事实上，对于包含对象的单元格，objectness 始终为 1，对于不包含任何对象的单元格，object ness 始终为 0。通过测量这个<em class="lb"> obj_loss </em>，我们可以逐渐教会网络检测感兴趣的区域。在此期间，我们不希望网络通过到处提出对象来欺骗我们。因此，我们需要<em class="lb"> noobj_loss </em>来惩罚那些假阳性提议。我们通过用<em class="lb"> 1-obj_mask </em>屏蔽预测得到假阳性。`<em class="lb"> ignore_mask </em>`用于确保我们只在当前框与地面真相框没有太多重叠时进行惩罚。如果有，我们倾向于更柔和，因为它实际上非常接近答案。正如我们从论文中看到的，“如果边界框先验不是最好的，但确实与地面真实对象重叠超过某个阈值，我们忽略预测。”由于在我们的基本事实中有比 obj 多得多的 noobj，我们也需要这个 Lambda_Noobj = 0.5 来确保网络不会被没有对象的单元所控制。</p><pre class="my mz na nb gt nk ng nl nm aw nn bi"><span id="dd2e" class="ml lj iq ng b gy no np l nq nr">class_loss = Sum(Binary_Cross_Entropy(class, class’) * obj_mask)</span></pre><p id="c3b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一个损失是分类损失。如果总共有 80 个类，则<em class="lb">类</em>和<em class="lb">类'</em>将是具有 80 个值的独热编码向量。在 YOLO v3 中，它被更改为多标签分类，而不是多类分类。为什么？因为一些数据集可能包含分层或相关的标签，例如<em class="lb">女人</em>和<em class="lb">人</em>。因此，每个输出像元可能有不止一个类为真。相应地，我们也对每一类逐一应用二元交叉熵并求和，因为它们并不互斥。就像我们对其他损失所做的那样，我们也乘以这个<em class="lb"> obj_mask </em>，以便我们只计算那些具有基础真值对象的单元格。</p><p id="c245" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了充分理解这种损失是如何发生的，我建议您用一个真实的网络预测和地面实况来手动浏览它们。用计算器(或<em class="lb"> tf.math </em>)计算损失真的能帮你抓住所有的本质细节。我自己做的，这帮助我找到了很多错误。毕竟，细节决定成败。</p><h1 id="e357" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">履行</h1><p id="4420" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">如果我在这里停止写作，我的帖子将会像网络上的另一篇“YOLO v3 评论”。一旦您从上一节中理解了 YOLO v3 的一般概念，我们现在就可以开始探索我们 YOLO v3 之旅剩下的 90%了:实现。</p><h2 id="c383" class="ml lj iq bd lk mm mn dn lo mo mp dp ls ko mq mr lw ks ms mt ma kw mu mv me mw bi translated">结构</h2><p id="12aa" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">9 月底，谷歌终于发布了 TensorFlow 2.0.0。这对 TF 来说是一个迷人的里程碑。然而，新的设计并不一定意味着开发者的痛苦会减少。从 2019 年初开始，我就一直在玩 TF 2，因为我一直想以我为 PyTorch 所做的方式编写 TensorFlow 代码。如果不是因为 TensorFlow 强大的制作套件像 TF Serving，TF lite，还有 TF Board 等等。，我估计很多开发者不会为新项目选择 TF。因此，如果您对生产部署没有强烈的需求，我建议您在 PyTorch 甚至 MXNet 中实现 YOLO v3。然而，如果你下定决心坚持使用 TensorFlow，请继续阅读。</p><p id="1f57" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TensorFlow 2 正式让渴望模式成为一级公民。简而言之，您现在可以利用原生 Python 代码以动态模式运行图表，而不是使用 TensorFlow 特定的 API 在图表中进行计算。没有更多的图形编译和更容易的调试和控制流程。在性能更重要的情况下，还提供了一个方便的<em class="lb"> tf.function </em> decorator 来帮助将代码编译成静态图。但是，现实是，渴望模式和<em class="lb"> tf.function </em>仍然有问题，或者有时没有很好地记录，这使得你在像 YOLO v3 这样复杂的系统中的生活更加艰难。此外，Keras 模型不是很灵活，而定制的训练循环仍然是相当实验性的。所以你用 TF 2 写 YOLO v3 最好的策略就是先从一个最小的工作模板开始，逐渐给这个外壳增加更多的逻辑。通过这样做，我们可以在错误隐藏在一个巨大的嵌套图中之前尽早失败并修复它。</p><h2 id="9675" class="ml lj iq bd lk mm mn dn lo mo mp dp ls ko mq mr lw ks ms mt ma kw mu mv me mw bi translated">资料组</h2><p id="2bb3" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">除了要选择的框架，成功训练最重要的是数据集。在论文中，作者使用 MSCOCO 数据集来验证他的想法。事实上，这是一个很好的数据集，我们应该在这个基准数据集上为我们的模型争取一个好的精度。然而，像这样的大型数据集也可能隐藏代码中的一些错误。例如，如果损失没有下降，你如何知道它只是需要更多的时间来收敛，或者你的损失函数是错误的？即使使用 GPU，训练的速度仍然不够快，无法让你快速迭代和修复东西。因此，我建议您构建一个包含数十个图像的开发集，以确保您的代码首先看起来“工作”。另一种选择是使用 VOC 2007 数据集，它只有 2500 个训练图像。要使用 MSCOCO 或 VOC2007 数据集并创建 TF 记录，可以参考我这里的助手脚本:<a class="ae kc" href="https://github.com/ethanyanjiali/deep-vision/tree/master/Datasets/MSCOCO/" rel="noopener ugc nofollow" target="_blank"> MSCOCO </a>，<a class="ae kc" href="https://github.com/ethanyanjiali/deep-vision/tree/master/Datasets/VOC2007/" rel="noopener ugc nofollow" target="_blank"> VOC2007 </a></p><h2 id="e9c1" class="ml lj iq bd lk mm mn dn lo mo mp dp ls ko mq mr lw ks ms mt ma kw mu mv me mw bi translated">预处理</h2><p id="82a3" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">预处理是指将原始数据转换成合适的网络输入格式的操作。对于图像分类任务，我们通常只需要调整图像的大小，并对标签进行一次性编码。但是对于 YOLO v3 来说，事情有点复杂。还记得我说过网络的输出就像<em class="lb">52 x 52 x3x(4+1+num _ classes)</em>有三种不同的尺度吗？由于我们需要计算基础事实和预测之间的差值，我们还需要首先将基础事实格式化成这样的矩阵。</p><p id="7e05" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于每个真实边界框，我们需要选择最佳的比例和锚。例如，天空中的一个小风筝应该是小比例的(52x52)。如果风筝在图像中更像一个正方形，我们也应该选择那个比例中最正方形的锚。在 YOLO v3 中，作者为 3 个音阶提供了 9 个锚点。我们所需要做的就是选择一个最符合我们地面真相框的。当我实现这个时，我想我也需要锚盒的坐标来计算 IOU。事实上，你不需要。因为我们只是想知道哪个锚点最适合我们的基础真相框，所以我们可以假设所有锚点和基础真相框共享同一个质心。在这种假设下，匹配度就是重叠面积，可以通过<em class="lb">最小宽度*最小高度</em>来计算。</p><p id="7d30" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在转换过程中，还可以添加一些数据扩充，以增加虚拟训练集的多样性。例如，典型的增强包括随机翻转、随机裁剪和随机转换。然而，这些增强不会阻止您训练一个工作的检测器，所以我不会过多地讨论这个高级主题。</p><h2 id="a47f" class="ml lj iq bd lk mm mn dn lo mo mp dp ls ko mq mr lw ks ms mt ma kw mu mv me mw bi translated">培养</h2><p id="6bc6" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">经过这些讨论，你终于有机会运行“<em class="lb">python train . py”</em>开始你的模型训练了。这也是你遇到大多数 bug 的时候。当你被封锁时，你可以参考我的训练脚本<a class="ae kc" href="https://github.com/ethanyanjiali/deep-vision/tree/master/YOLO/tensorflow/train.py" rel="noopener ugc nofollow" target="_blank">这里</a>。同时，我想提供一些对我自己的训练有帮助的提示。</p><p id="c5ce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">楠的损失</strong></p><ol class=""><li id="f2c7" class="nu nv iq kf b kg kh kk kl ko nw ks nx kw ny la nz oa ob oc bi translated">检查你的学习率，确保它不会太高，以至于爆发你的梯度。</li><li id="498c" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">检查二进制交叉熵中的 0，因为 ln(0)不是数字。您可以从(epsilon，1-epsilon)中截取值。</li><li id="b8e0" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">找个例子，一步一步走过你的失落。找出你损失的哪一部分归南。例如，如果宽度/高度损失到 NaN，这可能是因为您从<em class="lb"> tw </em>到<em class="lb"> bw </em>的计算方法是错误的。</li></ol><p id="07d5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">亏损居高不下</strong></p><ol class=""><li id="f0d9" class="nu nv iq kf b kg kh kk kl ko nw ks nx kw ny la nz oa ob oc bi translated">试着提高自己的学习率，看看能不能降得更快。我的从 0.01 开始。但我也见过 1e-4 和 1e-5 的作品。</li><li id="94cc" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">想象你预处理过的基础事实，看看它是否有意义。我之前遇到的一个问题是，我的输出网格是在[y][x]而不是[x][y]，但是我的地面真相是反的。</li><li id="c794" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">再一次，用一个真实的例子来说明你的损失。我在计算对象和类别概率之间的交叉熵时犯了一个错误。</li><li id="4a69" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">我的损失也保持在 50 个 MSCOCO 时代后的 40 左右。然而，结果并没有那么糟糕。</li><li id="f46c" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">仔细检查代码中的坐标格式。YOLO 需要<em class="lb"> xywh </em>(质心 x，质心 y，宽度和高度)，但大部分数据集都是以<em class="lb"> x1y1x2y2 </em> (xmin，ymin，xmax，ymax)的形式出现。</li><li id="abf3" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">仔细检查你的网络架构。不要被一个名为“近距离观察 yolov 3-CyberAILab”的帖子中的图表误导。</li><li id="ab51" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated"><em class="lb">TF . keras . loss . binary _ cross entropy</em>不是你需要的二进制交叉熵之和。</li></ol><p id="ff48" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">损失较低，但预测失败</strong></p><ol class=""><li id="ee72" class="nu nv iq kf b kg kh kk kl ko nw ks nx kw ny la nz oa ob oc bi translated">根据您的观察将<em class="lb"> lambda_coord </em>或<em class="lb"> lambda_noobj </em>调整到损耗。</li><li id="b2af" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">如果你在自己的数据集上训练，并且数据集相对较小，那么损失函数中的 obj_mask 就不会错误地去掉必要的元素。</li><li id="091d" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">一次又一次，你的损失函数。计算损耗时，它使用单元格中的相对 xywh(也称为<em class="lb"> tx，ty，tw，th </em>)。不过，在计算忽略遮罩和 IOU 时，它会在整个图像中使用绝对 xywh。不要把它们混淆了。</li><li id="1404" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated"><strong class="kf ir">损失很低，但是没有预测</strong></li><li id="bbb8" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">如果您正在使用自定义数据集，请首先检查您的基础事实框的分布。盒子的数量和质量真的会影响网络学习(或欺骗)做什么。</li></ol><p id="4eb2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在您的训练集上进行预测，看看您的模型是否至少可以在训练集上过度拟合。</p><ol class=""><li id="3edd" class="nu nv iq kf b kg kh kk kl ko nw ks nx kw ny la nz oa ob oc bi translated"><strong class="kf ir">多 GPU 训练</strong></li><li id="e004" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la nz oa ob oc bi translated">由于对象检测网络有如此多的参数要训练，因此拥有更多的计算能力总是更好的。然而，TensorFlow 2.0 到目前为止还没有对多 GPU 训练提供很大的支持。要在 TF 中做到这一点，你需要选择一个训练策略，比如 MirroredStrategy，就像我在这里做的<a class="ae kc" href="https://github.com/ethanyanjiali/deep-vision/blob/master/YOLO/tensorflow/train.py#L281" rel="noopener ugc nofollow" target="_blank"/>。然后将数据集加载器也打包成分布式版本。对于分布式训练的一个警告是，每个批次产生的损失应该除以全局批次大小，因为我们将对所有 GPU 结果进行“减少总和”。例如，如果本地批量大小为 8，并且有 8 个 GPU，那么您的批量损失应该除以全局批量大小 64。一旦你把所有复制品的损耗加起来，最后的结果将是单个例子的平均损耗。</li></ol><p id="4aba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="ak">后处理</strong></p><p id="e252" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该检测系统的最后一个组件是后处理器。通常，后处理只是一些琐碎的事情，比如用人类可读的类文本替换机器可读的类 id。不过，在对象检测中，我们还有一个更关键的步骤要做，以获得最终的人类可读结果。这被称为非最大抑制。</p><h2 id="9067" class="ml lj iq bd lk mm mn dn lo mo mp dp ls ko mq mr lw ks ms mt ma kw mu mv me mw bi translated">让我们回忆一下我们的客观损失。当虚假提议与地面真相有很大重叠时，我们不会用<em class="lb"> noobj_loss </em>来惩罚它。这鼓励网络预测接近的结果，以便我们可以更容易地训练它。此外，虽然在 YOLO 没有使用，但当使用滑动窗口方法时，多个窗口可以预测同一物体。为了消除这些重复的结果，聪明的研究人员设计了一种叫做非最大抑制(NMS)的算法。</h2><p id="8b4a" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">NMS 的想法很简单。首先找出具有最佳置信度的检测框，将其添加到最终结果中，然后用该最佳框消除 IOU 超过特定阈值的所有其他框。接下来，你在剩下的盒子中选择另一个最有信心的盒子，重复做同样的事情，直到什么都没有了。在代码中，由于 TensorFlow 在大多数时候需要显式的形状，我们通常会定义一个最大的检测数，如果达到这个数就提前停止。在 YOLO v3 中，我们的分类不再相互排斥，一个检测可以有多个真实的类。然而，一些现有的 NMS 规范没有考虑到这一点，所以在使用时要小心。</p><p id="3663" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结论</p><figure class="my mz na nb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/f1f76f3ee0dbc227c1cc1c77d0dc5dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Me9PqpjFGoL03G5l7qU6NA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://medium.com/@pythonlessons0" rel="noopener">Python Lessons</a> from <a class="ae kc" href="https://medium.com/analytics-vidhya/yolo-v3-theory-explained-33100f6d193" rel="noopener">Analytics Vidhya</a></figcaption></figure><p id="1925" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">YOLO v3 是人工智能崛起时代的杰作，也是 2010 年代卷积神经网络技术和技巧的优秀总结。虽然有许多像 Detectron 这样的交钥匙解决方案来简化制作探测器的过程，但对机器学习工程师来说，编写如此复杂的探测器的实践经验确实是一个很好的学习机会，因为仅仅阅读论文是远远不够的。就像雷伊·达里奥谈到他的哲学时说的那样:</p><h1 id="18cc" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">痛苦加反思等于进步。</h1><p id="6ab8" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">我希望我的文章可以成为您在实施 YOLO v3 的痛苦旅程中的一座灯塔，也许您也可以在以后与我们分享这一令人愉快的进展。如果你喜欢我的文章或者我的 YOLO v3 源代码，请⭐star⭐我的<a class="ae kc" href="https://github.com/ethanyanjiali/deep-vision/" rel="noopener ugc nofollow" target="_blank"> repo </a>，那将是对我最大的支持。</p><blockquote class="oj"><p id="fe4c" class="ok ol iq bd om on oo op oq or os la dk translated">参考</p></blockquote><p id="15a2" class="pw-post-body-paragraph kd ke iq kf b kg ot ki kj kk ou km kn ko ov kq kr ks ow ku kv kw ox ky kz la ij bi translated">张子豪，<a class="ae kc" href="https://github.com/zzh8829/yolov3-tf2" rel="noopener ugc nofollow" target="_blank"> YoloV3 在 TensorFlow 2.0 </a>，Github 中实现</p><h1 id="72f3" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">杨云，<a class="ae kc" href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3" rel="noopener ugc nofollow" target="_blank"> TensorFlow2.x-YOLOv3 </a>，Github</h1><ul class=""><li id="d5d7" class="nu nv iq kf b kg mg kk mh ko oy ks oz kw pa la pb oa ob oc bi">逍遥王可爱, <a class="ae kc" href="https://zhuanlan.zhihu.com/p/49995236" rel="noopener ugc nofollow" target="_blank">史上最详细的 Yolov3 边框预测分析</a>, 知乎专栏</li><li id="8964" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la pb oa ob oc bi translated">Joseph Redmon，<a class="ae kc" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" rel="noopener ugc nofollow" target="_blank"> YOLOv3:增量改进</a></li><li id="ee67" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la pb oa ob oc bi translated">约瑟夫·雷德蒙，阿里·法尔哈迪，YOLO9000:更好，更快，更强</li><li id="8ec4" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la pb oa ob oc bi translated">约瑟夫·雷德蒙，桑托什·迪夫瓦拉，罗斯·吉斯克，阿里·法尔哈迪，<a class="ae kc" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的实时物体检测</a>，2016 年 IEEE 计算机视觉与模式识别会议</li><li id="44ff" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la pb oa ob oc bi translated">Ayoosh Kathuria，<a class="ae kc" rel="noopener" target="_blank" href="/yolo-v3-object-detection-53fb7d3bfe6b">YOLO v3 有什么新内容？</a>，走向数据科学</li><li id="346c" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la pb oa ob oc bi translated">Python 课程，<a class="ae kc" href="https://medium.com/analytics-vidhya/yolo-v3-theory-explained-33100f6d193" rel="noopener"> YOLO v3 理论讲解</a>，分析 Vidhya</li><li id="0951" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la pb oa ob oc bi">Ayoosh Kathuria, <a class="ae kc" rel="noopener" target="_blank" href="/yolo-v3-object-detection-53fb7d3bfe6b">What’s new in YOLO v3?</a>, Towards Data Science</li><li id="9da4" class="nu nv iq kf b kg od kk oe ko of ks og kw oh la pb oa ob oc bi">Python Lessons, <a class="ae kc" href="https://medium.com/analytics-vidhya/yolo-v3-theory-explained-33100f6d193" rel="noopener">YOLO v3 theory explained</a>, Analytics Vidhya</li></ul></div></div>    
</body>
</html>