<html>
<head>
<title>Imbalanced Classes: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不平衡的类:第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/imbalanced-class-sizes-and-classification-models-a-cautionary-tale-3648b8586e03?source=collection_archive---------9-----------------------#2019-03-08">https://towardsdatascience.com/imbalanced-class-sizes-and-classification-models-a-cautionary-tale-3648b8586e03?source=collection_archive---------9-----------------------#2019-03-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/95aa1b7f0780f063cbed0b5c46f3c39d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vGpjCzKHVGg8rGSAFX4n9A.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="1914" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">避免分类中的不平衡分类陷阱</h2></div><p id="5bd3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi lp translated">在最近的一个数据科学项目中，我开发了一个监督学习模型，对度假屋网站 Airbnb 的首次用户的预订位置进行分类。<a class="ae ly" href="https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings" rel="noopener ugc nofollow" target="_blank">作为 2015 年 Kaggle 竞赛的一部分，该数据集可在 Kaggle </a>上获得。</p><p id="791f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我的项目中，我决定将用户分为两组:一组是在美国和加拿大境内预订首次旅行的用户，另一组是在国际其他地方预订首次旅行的用户，这实质上是将问题转化为一个二元分类问题。听起来很简单，对吧？</p><p id="581d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">问题是分类目标(预订位置)非常不平衡。近 75%的首次用户预订了美国和加拿大境内的旅行。在我的初始模型显示了有希望的结果之后，对模型性能度量的进一步检查突出了一个关键问题，即当试图用不平衡的类大小执行二进制分类时。这篇文章旨在强调在构建具有不平衡类的分类模型时要注意的一些陷阱，并强调一些处理这些问题的方法。</p><p id="7d39" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">数据</strong></p><p id="f6fa" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这张图显示了我的目标群体中固有的严重不平衡:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="3f44" class="mi mj je me b gy mk ml l mm mn">import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import numpy as np<br/>import pickle<br/>import seaborn as sns</span><span id="3c2c" class="mi mj je me b gy mo ml l mm mn">df = pd.read_pickle('data_for_regression_20k.pkl')</span><span id="8782" class="mi mj je me b gy mo ml l mm mn">sns.set_style("white")<br/>dests2 = df.groupby('country_USA_World_bi').agg({'country_USA_World_bi':['count']}).reset_index()<br/>dests2.columns = ['dest', 'count']<br/>dests2['pct'] = dests2['count']*100/(sum(dests2['count']))</span><span id="20c9" class="mi mj je me b gy mo ml l mm mn">x = dests2['dest']<br/>y = dests2['pct']<br/>palette = ['olive','mediumvioletred']</span><span id="9d2a" class="mi mj je me b gy mo ml l mm mn">fig, ax = plt.subplots(figsize = (8,4))<br/>fig = sns.barplot(y, x, estimator = sum, ci = None, orient='h', palette=palette)<br/>y_lab = ['USA/Canada', 'International']<br/>ax.set_yticklabels(labels=y_lab, ha='right')</span><span id="c829" class="mi mj je me b gy mo ml l mm mn">for i, v in enumerate(y):<br/>    ax.text(v - 15, i + .05, str(int(v)+.5)+'%', color='white', fontweight='bold')</span><span id="b1dc" class="mi mj je me b gy mo ml l mm mn">plt.title('Country Destinations as Percent of Total Bookings',size = 16, weight = 'bold')<br/>plt.ylabel('Country')<br/>plt.xlabel('Percent of total');</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mp"><img src="../Images/b6f45b903e5a84f6e89bd41f7c1fdc30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqqU4b6BHJlCvqL4TJhK4Q.png"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">Close to 75% of users booked vacation rental in the U.S.A. and Canada</figcaption></figure><p id="9bdb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在将逻辑回归分类器应用于我的数据之前，我将数据分为训练集(80%)和测试集(20%)。由于国际旅行者的代表性不足，我使用了分层参数来确保两个目标类都在测试集中得到了体现。然后，我使用一个标准标量对训练和测试特性集进行了标准化。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="5de8" class="mi mj je me b gy mk ml l mm mn">from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, fbeta_score, make_scorer</span><span id="1ba1" class="mi mj je me b gy mo ml l mm mn">y = df['country_USA_World_bi']  <br/>X = df.drop(['country_dest_id','country_USA_World_bi','month_created', 'day_created', 'month_active', 'day_active'], axis = 1)</span><span id="76d5" class="mi mj je me b gy mo ml l mm mn">Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, stratify=y,random_state = 88)</span><span id="9605" class="mi mj je me b gy mo ml l mm mn">std_scale = StandardScaler()<br/>X_train_scaled = std_scale.fit_transform(Xtrain)<br/>X_test_scaled = std_scale.transform(Xtest)</span></pre><p id="391d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我运行的第一个模型是逻辑回归，因为逻辑回归包括特征系数(这有助于可解释性)。我用默认的 hypterparameters 拟合了一个初始模型，并惊喜地发现，在任何 hypter parameter 调整之前，该模型有 75%的准确性。很自然地，我发现自己在想这是不是好得不像真的。回想 75%的用户在美国/加拿大境内旅行，难道我每次只需猜测目的地就能有 75%的准确率吗？</p><p id="1ba9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">事实上，模型精度(预测的国际预订实际上是国际预订的比例)、模型召回率(模型正确识别的国际预订的比例)和 f1 分数(两者的平衡)都非常差。</p><p id="0e48" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">出于这个项目的目的，我对回忆分数最感兴趣，因为我认为模型能够准确预测将进行国际旅行的用户是最有用的(因为那些进行国际旅行的用户更有可能是具有较大旅行预算的狂热旅行者)。然而，鉴于最初的回忆分数只有 0.01，我还有很长的路要走，以改善这个模型！</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="af2f" class="mi mj je me b gy mk ml l mm mn">from sklearn.linear_model import LogisticRegression</span><span id="c046" class="mi mj je me b gy mo ml l mm mn">def fit_logistic_regression_classifier(X_training_set, y_training_set):<br/>    logreg = LogisticRegression(random_state=88)<br/>    model = logreg.fit(X_training_set, y_training_set)<br/>    y_pred = model.predict(X_test_scaled)<br/>    print('accuracy = ',model.score(X_test_scaled, ytest).round(2),<br/>          'precision = ',precision_score(ytest, y_pred).round(2), <br/>          'recall = ',recall_score(ytest, y_pred).round(2), <br/>          'f1_score = ',f1_score(ytest, y_pred).round(2)<br/>         )<br/>    return(y_pred)<br/>y_pred = fit_logistic_regression_classifier(X_train_scaled, ytrain)</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mu"><img src="../Images/a4172e49715a005690ed145f5863528d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8-kkBtAQcFYbTnu2ToVvQ.png"/></div></div></figure><p id="898b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">混淆矩阵是一个很好的工具，可以形象化模型被混淆的程度。在<em class="mv"> sklearn </em>中的混淆矩阵给出了根据实际类别预测的每个类别中观察值数量的原始值计数。plot_confusion_matrix()函数给出了每个实际类和预测类中值的百分比的可视化表示。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="4d91" class="mi mj je me b gy mk ml l mm mn">import itertools<br/>from sklearn.metrics import confusion_matrix<br/>def make_confusion_matrix(cm, classes,title='Confusion matrix',cmap=plt.cm.Blues):<br/>    print(cm)<br/>    # Normalize values<br/>    cm = cm.astype('float')*100 / cm.sum(axis=1)[:, np.newaxis]<br/>    plt.imshow(cm, interpolation='nearest', cmap=cmap)<br/>    plt.title(title)<br/>    plt.colorbar()<br/>    tick_marks = np.arange(len(classes))<br/>    plt.xticks(tick_marks, classes, rotation=45)<br/>    plt.yticks(tick_marks, classes)</span><span id="a249" class="mi mj je me b gy mo ml l mm mn">fmt = '.2f'<br/>    thresh = cm.max() / 2.<br/>    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):<br/>        plt.text(j, i, format(cm[i, j], fmt),<br/>                 horizontalalignment="center",<br/>                 color="white" if cm[i, j] &gt; 50 else "black")<br/>    plt.ylabel('True label')<br/>    plt.xlabel('Predicted label')<br/>    plt.tight_layout()<br/>def plot_confusion_matrix(y_test_set, y_pred):<br/>    class_names = ['Domestic','International']<br/>    cnf_matrix = confusion_matrix(y_test_set, y_pred)<br/>    np.set_printoptions(precision=2)<br/>    plt.figure()<br/>    make_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix with normalization');</span></pre><p id="89d3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">该图表明我的直觉是正确的——该模型将几乎 100%的观察结果分类为国内旅行者，因此有 75%的时间达到了目标！</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="56a9" class="mi mj je me b gy mk ml l mm mn">plot_confusion_matrix(ytest, y_pred)</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/005370cc2169ad50c5331cdf48daeee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*tA1NHMWULG2kTHy3g7SHCg.png"/></div></figure><p id="cf33" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf"> 1。随机过采样</strong></p><p id="543d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><a class="ae ly" href="https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html" rel="noopener ugc nofollow" target="_blank">不平衡学习库</a>包括多种方法来重新平衡类别，以获得更准确的预测能力。我尝试的方法叫做随机过采样。根据<a class="ae ly" href="https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html" rel="noopener ugc nofollow" target="_blank">文档</a>，“随机过采样可用于重复一些样本，并平衡数据集之间的样本数量。”基本上，这种重新平衡方法使用目标类的随机抽样和替换来获得训练集中每个类的平衡表示。事实上，在将随机抽样器应用于我的训练集之后，我在每个目标类中都有 12，743 个观察样本，而我的基线场景是 3，186 个国内预订和 1，088 个国际预订。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="bdf6" class="mi mj je me b gy mk ml l mm mn">from imblearn.over_sampling import RandomOverSampler</span><span id="cf1e" class="mi mj je me b gy mo ml l mm mn">ros = RandomOverSampler(random_state=88)<br/>X_resampled, y_resampled = ros.fit_sample(X_train_scaled, ytrain)</span><span id="2c34" class="mi mj je me b gy mo ml l mm mn">yvals, counts = np.unique(ytest, return_counts=True)<br/>yvals_ros, counts_ros = np.unique(y_resampled, return_counts=True)<br/>print('Classes in test set:',dict(zip(yvals, counts)),'\n',<br/>      'Classes in rebalanced test set:',dict(zip(yvals_ros, counts_ros)))</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/e956158e47787e7270757c1b7b27fc86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AgmebUmtFDpogpJNxL8xxw.png"/></div></div></figure><p id="ca06" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">和以前一样，我用默认参数拟合了一个逻辑回归分类器，并观察了模型的性能指标和混淆矩阵。由于该模型不再能在 75%的时间内正确猜测国内位置，其性能显著下降:准确率降至 54%。</p><p id="4a79" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然而，回想一下，我的兴趣指标增加了——从 0.01 增加到 0.58。在测试集中提供平衡的班级规模显著提高了模型预测少数民族班级的能力(在我的例子中，Airbnb 在国际地点的预订)。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="1106" class="mi mj je me b gy mk ml l mm mn">y_pred_ros = fit_logistic_regression_classifier(X_resampled, y_resampled)<br/>plot_confusion_matrix(ytest, y_pred_ros)</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/cb81024375871c9eb81b46e229cd180a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Jub_FVwYIjNeyENKCyl8Q.png"/></div></div></figure><p id="2e41" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf"> 2。SMOTE 和 ADASYN </strong></p><p id="bb5f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">合成少数过采样技术(SMOTE)和自适应合成(ADASYN)是对少数类进行过采样的另外两种方法。与对现有观测值进行过采样的随机过采样不同，SMOTE 和 ADASYN 使用插值在少数类的现有观测值附近创建新观测值。</p><p id="b9d8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">对于 SMOTE 重新平衡，我使用了 SMOTENC 对象，因为我的大多数特征(除了六个之外)都是非连续的(即分类的)特征。就像以前一样，我最终得到了一套平衡的训练。</p><p id="740f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">ADASYN 给了我一个新的培训集，其中约 49%的旅行者去了美国/加拿大，51%的旅行者去了国外。这种(微不足道的)不平衡是由于 ADASYN 根据难度的加权分布在困难点周围创建新数据点的方式造成的(<a class="ae ly" href="http://www.ele.uri.edu/faculty/he/PDFfiles/adasyn.pdf" rel="noopener ugc nofollow" target="_blank">见 he 等人，2008 </a>)。</p><p id="2ade" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">就像随机过采样一样，模型对<em class="mv">所有</em>目的地进行分类的能力(准确性)会随着过采样而下降。另一方面，SMOTE 和 ADASYN 都提高了模型对少数类的分类能力(回忆)。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="e9f7" class="mi mj je me b gy mk ml l mm mn">from imblearn.over_sampling import SMOTENC, ADASYN<br/>smote_nc = SMOTENC(categorical_features=list(np.arange(7,80)), random_state=88)<br/>X_smoted, y_smoted = smote_nc.fit_resample(X_train_scaled, ytrain)</span><span id="f1d7" class="mi mj je me b gy mo ml l mm mn">adasyn = ADASYN(random_state=88)<br/>X_adasyn, y_adasyn = adasyn.fit_resample(X_train_scaled, ytrain)</span><span id="2fc2" class="mi mj je me b gy mo ml l mm mn">yvals, counts = np.unique(ytest, return_counts=True)<br/>yvals_smt, counts_smt = np.unique(y_smoted, return_counts=True)<br/>yvals_ads, counts_ads = np.unique(y_adasyn, return_counts=True)</span><span id="15fa" class="mi mj je me b gy mo ml l mm mn">print('Classes in test set:',dict(zip(yvals, counts)),'\n',<br/>      'Classes in rebalanced test set with SMOTENC:',dict(zip(yvals_smt, counts_smt)),'\n',<br/>      'Classes in rebalanced test set with ADASYN:',dict(zip(yvals_ads, counts_ads)))</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/dff767b8e705a1f6dd1165baa79a5992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-wiZrIjKON8NcZfmkw3eiA.png"/></div></div></figure><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="49be" class="mi mj je me b gy mk ml l mm mn">y_pred_smt = fit_logistic_regression_classifier(X_smoted, y_smoted)<br/>plot_confusion_matrix(ytest, y_pred_smt)</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi na"><img src="../Images/a404fbe17a1ecd41decc8fa4137488e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7tAltIvNFHS3aHyj8ZtfVw.png"/></div></div></figure><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="e180" class="mi mj je me b gy mk ml l mm mn">y_pred_ads = fit_logistic_regression_classifier(X_adasyn, y_adasyn)<br/>plot_confusion_matrix(ytest, y_pred_ads)</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/28ff2717314aea14ef09f0b167143d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dk9lL0vjy96VUB1g5k43rA.png"/></div></div></figure><p id="e514" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf"> 3。平衡类网格搜索</strong></p><p id="b1f9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">由于采用 ADASYN 过采样的基线模型在召回率方面表现最佳，因此我对这个测试集进行了网格搜索，以找到进一步优化模型性能的参数。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="2b4c" class="mi mj je me b gy mk ml l mm mn">from sklearn.model_selection import GridSearchCV<br/>grid = {"C":np.logspace(-3,3,7), "penalty":["l1","l2"]}# l1 lasso l2 ridge<br/>logreg = LogisticRegression(random_state=88)<br/>logreg_cv = GridSearchCV(logreg,grid,cv=5,scoring='recall')<br/>logreg_cv.fit(X_adasyn, y_adasyn)<br/>print("tuned hpyerparameters :(best parameters) ", logreg_cv.best_params_)</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/2a9725ae83d608ffacc28f90ab7d9f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6DQ93Z3KWFPYgnTR6yWkjg.png"/></div></div></figure><p id="d3fb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">具有 0.001 的 C 参数和 L2 正则化惩罚的逻辑回归模型具有 0.65 的改进的回忆分数。这意味着该模型能够有效地抓住 65%的将在国际上预订 Airbnbs 的新用户。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7f7c" class="mi mj je me b gy mk ml l mm mn">y_pred_cv = logreg_cv.predict(X_test_scaled)<br/>print('accuracy = ',logreg_cv.score(X_test_scaled, ytest).round(2),<br/>        'precision = ',precision_score(ytest, y_pred_cv).round(2), <br/>        'recall = ',recall_score(ytest, y_pred_cv).round(2), <br/>        'f1_score = ',f1_score(ytest, y_pred_cv).round(2)<br/>        )<br/>plot_confusion_matrix(ytest, y_pred_cv)</span></pre><figure class="lz ma mb mc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nd"><img src="../Images/22df5cd24ec841d74c4316b282ec8d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ich3mDXP_eGRBIwZQsmt8g.png"/></div></div></figure><p id="b496" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">虽然平衡类和超参数调整显著提高了模型的召回分数，但模型精度仍然很低，只有 0.3。这意味着，只有 30%被归类为国际旅行者的用户实际上在国际上预订 Airbnbs。在商业环境中，像这样的模型可以用于根据预测的预订目的地通知度假屋的定向广告。这意味着 70%收到建议的用户，比如说，可以俯瞰埃菲尔铁塔的房子，实际上会考虑在国内旅行。这种错误定位不仅与该集团无关，而且未能向美国/加拿大集团传播相关广告可能意味着随着时间的推移会损失收入。</p><p id="ade7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，我已经通过对少数类进行过采样解决了模型性能的高估问题，接下来的步骤可能包括额外的特征工程，以梳理出更多的信号并拟合替代分类算法(如 K-最近邻或随机森林分类器)。</p><p id="43bd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">结论</strong></p><p id="25c9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这个例子中，一旦我重新平衡了目标类的大小，模型的准确性就会显著下降。即使在使用 gridsearch 交叉验证进行超参数调整后，逻辑回归模型的准确性也比具有不平衡类别的基线模型低 10 个百分点。</p><p id="52aa" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这个例子说明了考虑类别不平衡的重要性，以避免高估分类模型的准确性。我还用工作代码概述了通过过采样(随机过采样、SMOTE 和 ADASYN)重新平衡类的三种技术。关于每种技术的更多信息可以在<a class="ae ly" href="https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html" rel="noopener ugc nofollow" target="_blank">不平衡学习文档</a>中找到。</p></div></div>    
</body>
</html>