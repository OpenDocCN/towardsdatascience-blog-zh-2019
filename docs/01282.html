<html>
<head>
<title>Reinforcement Learning Tutorial Part 3: Basic Deep Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习教程第 3 部分:基本深度 Q 学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-tutorial-part-3-basic-deep-q-learning-186164c3bf4?source=collection_archive---------5-----------------------#2019-02-28">https://towardsdatascience.com/reinforcement-learning-tutorial-part-3-basic-deep-q-learning-186164c3bf4?source=collection_archive---------5-----------------------#2019-02-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/6092e154765c0ab162d3a1f261323449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RCwrJu7mG8AsmA85zQEU0w.jpeg"/></div></div></figure><p id="5ad6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在<a class="ae kz" href="https://blog.valohai.com/reinforcement-learning-tutorial-part-1-q-learning" rel="noopener ugc nofollow" target="_blank">第 1 部分</a>中，我们通过纸笔示例介绍了 Q-learning 的概念。</p><p id="25c6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在<a class="ae kz" href="https://blog.valohai.com/reinforcement-learning-tutorial-cloud-q-learning" rel="noopener ugc nofollow" target="_blank">第 2 部分</a>中，我们用代码实现了这个例子，并演示了如何在云中执行它。</p><p id="767e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在第三部分中，我们将把我们的 Q 学习方法从 Q 表转移到深度神经网络。</p><p id="be9d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用 Q-table，你的内存需求是一个由<em class="la">状态</em> x <em class="la">动作</em>组成的数组。对于状态空间 5 和动作空间 2，总的内存消耗是 2 x 5=10。但是国际象棋的状态空间大约是 10 ⁰，这意味着这种严格的电子表格方法不能扩展到现实世界。幸运的是，你可以从媒体压缩的世界里偷到一个窍门:用一些准确性换取内存。</p><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lb"><img src="../Images/b9c44cda350967a9c37ef6679c45ff46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-nL9XtEhH9hrNqfy8BkuBg.jpeg"/></div></div></figure><p id="f4a0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在无损压缩的情况下，以每秒 60 帧的速度存储 1080p 视频每秒大约需要 1gb。使用有损压缩的相同视频可以很容易地成为 1/10000 大小，而不会损失太多保真度。幸运的是，就像视频文件一样，用强化学习训练模型从来都不是 100%真实的，一些“足够好”或“比人类水平更好”的东西已经让数据科学家微笑了。因此，我们很乐意用准确性换取记忆。</p><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lg"><img src="../Images/57ca98b52c960dd0db0d1ad7a8a8c8d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M9skHIvVwxXuSAMQO1_htg.png"/></div></div></figure><p id="e4d0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们不是从 Q 表中取一个“完美的”值，而是训练一个神经网络来估计这个表。毕竟，神经网络只不过是一个美化了的权重和偏好表而已！</p><p id="4eaa" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们的示例游戏是如此简单，以至于我们很可能用神经网络比用 Q 表使用更多的内存！任何真实世界的场景都比这复杂得多，所以这只是我们试图保持示例简单的一个假象，而不是一个普遍趋势。</p><h1 id="c815" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">培养</h1><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mf"><img src="../Images/72a05229f426eb544b7fcf99754a7f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M7ayD65Cdw7DV21h"/></div></div></figure><p id="8d64" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们之前做 Q-learning 的时候，用的是上面的算法。用神经网络代替 Q 表，我们可以简化它。</p><p id="796a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">不再需要<em class="la">学习率</em>，因为我们的反向传播优化器已经有了。学习率只是一个全球油门踏板，你不需要两个。一旦去除了学习率，你意识到你也可以去除两个 Q(s，a)项，因为它们在去除学习率后相互抵消。</p><p id="ea2c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">强化学习通常被描述为独立于监督学习和非监督学习的一个类别，然而这里我们将从我们的监督表亲那里借用一些东西。据说强化学习不需要训练数据，但这只是部分正确。事先不需要训练数据，但是在探索模拟时收集训练数据，并且使用非常类似。</p><p id="4345" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当代理探索模拟时，它将记录体验。</p><p id="6897" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">单次体验=(旧状态，动作，奖励，新状态)</strong></p><p id="ea1e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">用单一体验训练我们的模型:</p><ol class=""><li id="5564" class="mg mh it kd b ke kf ki kj km mi kq mj ku mk ky ml mm mn mo bi translated">让模型估计旧状态的 Q 值</li><li id="f857" class="mg mh it kd b ke mp ki mq km mr kq ms ku mt ky ml mm mn mo bi translated">让模型估计新状态的 Q 值</li><li id="33c4" class="mg mh it kd b ke mp ki mq km mr kq ms ku mt ky ml mm mn mo bi translated">使用已知的奖励，计算行动的新目标 Q 值</li><li id="dea2" class="mg mh it kd b ke mp ki mq km mr kq ms ku mt ky ml mm mn mo bi translated">用输入=(旧状态)，输出=(目标 Q 值)训练模型</li></ol><p id="48a6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意:我们的网络不像 Q 学习函数 Q(s，a)那样得到(状态，动作)作为输入。这是因为我们没有复制 Q 学习作为一个整体，只是 Q 表。输入只是状态，输出是该状态下所有可能动作(向前、向后)的 Q 值。</p><h1 id="1171" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">代码</h1><p id="5211" class="pw-post-body-paragraph kb kc it kd b ke mu kg kh ki mv kk kl km mw ko kp kq mx ks kt ku my kw kx ky im bi translated">在上一部分中，我们足够聪明地将代理、模拟和编排作为单独的类分开。这意味着我们可以只引入一个新的代理，其余的代码基本保持不变。如果你想看剩下的代码，请看<a class="ae kz" href="https://blog.valohai.com/reinforcement-learning-tutorial-cloud-q-learning" rel="noopener ugc nofollow" target="_blank">第二部分</a>或者<a class="ae kz" href="https://github.com/valohai/qlearning-simple" rel="noopener ugc nofollow" target="_blank"> GitHub </a> repo。</p><figure class="lc ld le lf gt ju"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="3a42" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">定量</h1><p id="52dc" class="pw-post-body-paragraph kb kc it kd b ke mu kg kh ki mv kk kl km mw ko kp kq mx ks kt ku my kw kx ky im bi translated">在我们的例子中，我们在模拟的每一步之后重新训练模型，一次只有一次体验。这是为了保持代码简单。这种方法通常被称为<em class="la">在线培训。</em></p><p id="358b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">更常见的方法是将所有(或许多)经历收集到记忆日志中。然后，根据从日志中批量提取的多个随机经验来训练该模型。这被称为<em class="la">批量训练</em>或<em class="la">小批量训练</em>。它比强化学习更有效，并且通常提供更稳定的整体训练结果。将这个例子转化为批量训练是非常容易的，因为模型的输入和输出已经成形为支持批量训练。</p><h1 id="c1d9" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">结果</h1><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mf"><img src="../Images/74763ea9d975435a78f6900cb6e142d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JyeSPEk8WSH-ZVdGdFxCEA.png"/></div></div></figure><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nb"><img src="../Images/f130f0cf3abd4612c9bce637205cf977.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tCElEvJqynWKIqp2-_iCWQ.png"/></div></div></figure><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/07839da4ee15f2d575ad4ef198a1a344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w7tgoOMiB0OeZ-Si6zt_JQ.png"/></div></div></figure><p id="2fdb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里是一些使用 Valohai 贝叶斯优化器的不同学习率和折扣的训练运行。请注意，这里我们衡量的是绩效，而不是像前几部分那样衡量总薪酬。上升趋势是两件事的结果:学习和剥削。学习意味着模型像往常一样学习最小化损失和最大化回报。开发意味着，既然我们从赌博和探索开始，并越来越线性地转向开发，我们会在最后得到更好的结果，假设学习的策略已经开始在这个过程中有任何意义。</p><p id="3708" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">用深度神经网络训练这样的玩具模拟无论如何都不是最优的。模拟不是非常细致，奖励机制非常粗糙，深度网络通常在更复杂的场景中茁壮成长。通常在机器学习中，最简单的解决方案最终会成为最好的解决方案，所以在现实生活中不建议像我们这样用大锤砸坚果。</p><p id="1c81" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">既然我们已经学会了如何用神经网络来取代 Q-table，我们就可以处理更复杂的模拟，并在下一部分中充分利用<a class="ae kz" href="https://valohai.com/" rel="noopener ugc nofollow" target="_blank"> Valohai </a>深度学习平台。回头见！</p><h1 id="d6c1" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">在<a class="ae kz" href="https://github.com/valohai/qlearning-simple" rel="noopener ugc nofollow" target="_blank"> GitHub </a>开始这个 Q-learning 教程项目。</h1></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="b921" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://blog.valohai.com/reinforcement-learning-tutorial-part-1-q-learning" rel="noopener ugc nofollow" target="_blank">第一部分:Q-Learning </a></p><p id="5725" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://blog.valohai.com/reinforcement-learning-tutorial-cloud-q-learning" rel="noopener ugc nofollow" target="_blank">第二部分:云 Q 学习</a></p><p id="45ba" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://blog.valohai.com/reinforcement-learning-tutorial-basic-deep-q-learning" rel="noopener ugc nofollow" target="_blank">第三部分:基础深度 Q 学习</a></p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="ea07" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="la">原载于</em><a class="ae kz" href="https://blog.valohai.com/reinforcement-learning-tutorial-basic-deep-q-learning" rel="noopener ugc nofollow" target="_blank"><em class="la">blog.valohai.com</em></a><em class="la">。</em></p></div></div>    
</body>
</html>