# â€œç­‰å¼åˆ°ä»£ç â€æœºå™¨å­¦ä¹ é¡¹ç›®æ¼”ç»ƒâ€”ç¬¬ 2 éƒ¨åˆ†éçº¿æ€§å¯åˆ†é—®é¢˜

> åŸæ–‡ï¼š<https://towardsdatascience.com/an-equation-to-code-machine-learning-project-walk-through-in-python-part-2-non-linear-d193c3c23bac?source=collection_archive---------15----------------------->

## æ•°å­¦æ–¹ç¨‹å¼èƒŒåçš„è¯¦ç»†è§£é‡Šï¼Œä¸ºæ‚¨çš„æœºå™¨å­¦ä¹ æˆ–æ·±åº¦å­¦ä¹ ä¹‹æ—…å¥ å®šå®ç”¨çš„æ•°å­¦åŸºç¡€

![](img/621c0dfd0f6f25886c72ee0e85ee57bc.png)

å¤§å®¶å¥½ï¼è¿™æ˜¯â€œç­‰å¼åˆ°ä»£ç â€æ¼”ç»ƒçš„ç¬¬ 3 éƒ¨åˆ†ã€‚è¿™æ¬¡

åœ¨[ç¬¬ä¸€éƒ¨åˆ†](/an-equation-to-code-machine-learning-project-walk-through-in-python-part-1-linear-separable-fd0e19ed2d7?source=your_stories_page---------------------------)ä¸­ï¼Œæˆ‘ä»¬è°ˆåˆ°äº†å¦‚ä½•åˆ©ç”¨çº¿æ€§å›å½’è§£å†³**çº¿æ€§å¯åˆ†é—®é¢˜**ã€‚æˆ‘ä»¬å­¦ä¹ äº†å‘é‡è¡¨ç¤ºã€æ ‡å‡†åŒ–ã€æ·»åŠ åå·®ã€sigmoid å‡½æ•°ã€å¯¹æ•°ä¼¼ç„¶å‡½æ•°å’Œæ›´æ–°å‚æ•°ã€‚

è¿™æ¬¡æˆ‘ä»¬è¦è§£å†³ä¸€ä¸ª**éçº¿æ€§å¯åˆ†é—®é¢˜**ã€‚å¦‚æœä½ æ²¡æœ‰çœ‹è¿‡ç¬¬ä¸€éƒ¨åˆ†ï¼Œè¿™å®Œå…¨æ²¡é—®é¢˜ã€‚ç¬¬ 2 éƒ¨åˆ†æ˜¯ç‹¬ç«‹çš„ã€‚ä½†æ˜¯å¦‚æœä½ æƒ³æ›´å¥½åœ°ç†è§£ç¬¬ 2 éƒ¨åˆ†ï¼Œæœ€å¥½å…ˆè¯»ç¬¬ 1 éƒ¨åˆ†ã€‚

[](/an-equation-to-code-machine-learning-project-walk-through-in-python-part-1-linear-separable-fd0e19ed2d7) [## Python ä¸­çš„â€œç­‰å¼åˆ°ä»£ç â€æœºå™¨å­¦ä¹ é¡¹ç›®æ¼”ç»ƒâ€”ç¬¬ 1 éƒ¨åˆ†çº¿æ€§å¯åˆ†â€¦

### æ•°å­¦æ–¹ç¨‹å¼èƒŒåçš„è¯¦ç»†è§£é‡Šï¼Œä¸ºä½ çš„æœºå™¨å­¦ä¹ æˆ–å­¦ä¹ å»ºç«‹å®ç”¨çš„æ•°å­¦åŸºç¡€

towardsdatascience.com](/an-equation-to-code-machine-learning-project-walk-through-in-python-part-1-linear-separable-fd0e19ed2d7) 

ä¸‹é¢æ˜¯[æ•°æ®](https://gist.github.com/BrambleXu/52b0aaf10987015a078d36c97729dace)å’Œ[ä»£ç ](https://gist.github.com/BrambleXu/2640af09b1f43b93c2d951ba91ca3d5c)ã€‚

å†…å®¹ç»“æ„å¦‚ä¸‹ã€‚`*`è¡¨ç¤ºå¦‚æœæ‚¨å·²ç»å®Œæˆç¬¬ 1 éƒ¨åˆ†ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€æ­¥ã€‚

1.  çœ‹æ•°æ®
2.  **éçº¿æ€§å¯åˆ†é—®é¢˜**
3.  æ ‡å‡†åŒ–*
4.  **æ·»åŠ åå·®å’Œå¤šé¡¹å¼é¡¹**
5.  Sigmoid å‡½æ•°*
6.  ä¼¼ç„¶å‡½æ•°*
7.  æ›´æ–°å‚æ•°Î¸*
8.  ç»˜åˆ¶ç›´çº¿
9.  **ç²¾åº¦**
10.  æ‘˜è¦

# 1 çœ‹æ•°æ®

ä¸‹é¢æ˜¯æ•°æ®ï¼Œ [non_linear_data.csv](https://gist.github.com/BrambleXu/a64df128d6c0c26143f82f7b6e889983)

```
x1,x2,y
0.54508775,2.34541183,0
0.32769134,13.43066561,0
4.42748117,14.74150395,0
2.98189041,-1.81818172,1
4.02286274,8.90695686,1
2.26722613,-6.61287392,1
-2.66447221,5.05453871,1
-1.03482441,-1.95643469,1
4.06331548,1.70892541,1
2.89053966,6.07174283,0
2.26929206,10.59789814,0
4.68096051,13.01153161,1
1.27884366,-9.83826738,1
-0.1485496,12.99605136,0
-0.65113893,10.59417745,0
3.69145079,3.25209182,1
-0.63429623,11.6135625,0
0.17589959,5.84139826,0
0.98204409,-9.41271559,1
-0.11094911,6.27900499,0
```

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ç»˜åˆ¶è¿™äº›æ•°æ®ï¼Œçœ‹çœ‹å®ƒæ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª Python æ–‡ä»¶ï¼Œå¹¶å°†å…¶å‘½åä¸º non_logistic_regression.pyã€‚

```
import numpy as np
import matplotlib.pyplot as plt# read data
data = np.loadtxt("non_linear_data.csv", delimiter=',', skiprows=1)
train_x = data[:, 0:2]
train_y = data[:, 2]# plot data points
plt.plot(train_x[train_y == 1, 0], train_x[train_y == 1, 1], 'o')
plt.plot(train_x[train_y == 0, 0], train_x[train_y == 0, 1], 'x')
plt.show()
```

è¿è¡Œä¸Šé¢çš„è„šæœ¬åï¼Œæ‚¨åº”è¯¥ä¼šçœ‹åˆ°ä¸‹å›¾ã€‚

![](img/7ade453276139aebde2a779b35800e5f.png)

ä¼¼ä¹æˆ‘ä»¬ä¸èƒ½ç”¨ä¸€æ¡ç›´çº¿æ¥åˆ†ç¦» X å’Œ oã€‚æˆ‘ä»¬æŠŠè¿™æ ·çš„é—®é¢˜ç§°ä¸ºéçº¿æ€§å¯åˆ†é—®é¢˜ï¼Œå…¶ä¸­æ•°æ®ä¸æ˜¯çº¿æ€§å¯åˆ†çš„ã€‚

# 2 éçº¿æ€§å¯åˆ†é—®é¢˜

åœ¨ç¬¬ 1 éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨[çº¿æ€§å‡½æ•°](https://en.wikipedia.org/wiki/Linear_function_(calculus)?oldformat=true#Properties)æ¥è§£å†³çº¿æ€§å¯åˆ†é—®é¢˜ã€‚

![](img/8a7f6b94f05f89b47964230a43fb407f.png)

linear function

ä½†æ˜¯å¯¹äºéçº¿æ€§å¯åˆ†é—®é¢˜ï¼Œçº¿æ€§å‡½æ•°è¿‡äºç®€å•ï¼Œéš¾ä»¥å¤„ç†ã€‚æ‰€ä»¥æˆ‘ä»¬å¼•å…¥äº†å¤šé¡¹å¼é€»è¾‘å›å½’ï¼Œå®ƒåœ¨é€»è¾‘å›å½’ä¸­å¢åŠ äº†ä¸€ä¸ªå¤šé¡¹å¼é¡¹ã€‚

![](img/1fe7abc3aac2d5f70ec337406cae4fa1.png)

general form

æˆ‘ä»¬ç”¨Î¸æ¥è¡¨ç¤ºå‚æ•°ã€‚å·¦è¾¹çš„Î¸æ ‡è®°è¡¨ç¤ºå‡½æ•° f(x)æœ‰å‚æ•°Î¸ã€‚å³è¾¹çš„Î¸è¡¨ç¤ºæœ‰ä¸¤ä¸ªå‚æ•°ã€‚æœ€åä¸€é¡¹æ˜¯å¤šé¡¹å¼é¡¹ï¼Œå®ƒä½¿æ¨¡å‹æ¨å¹¿åˆ°éçº¿æ€§å¯åˆ†æ•°æ®ã€‚

æ³¨æ„æˆ‘ä»¬åœ¨ [non_linear_data.csv](https://gist.github.com/BrambleXu/a64df128d6c0c26143f82f7b6e889983) ä¸­æœ‰ x1 å’Œ x2 ä¸¤ä¸ªç‰¹å¾ã€‚æˆ‘ä»¬é€‰æ‹© x1 ä½œä¸ºå¤šé¡¹å¼é¡¹ã€‚æ‰€ä»¥åŠŸèƒ½åº”è¯¥å˜æˆä½äºå½¢å¼ã€‚

![](img/3f19fd852c6c8878549a3d6a524e359b.png)

a specific form fit to our data

æˆ‘ä»¬åˆå§‹åŒ– 4 ä¸ªå‚æ•°

```
import numpy as np
import matplotlib.pyplot as plt# read data
data = np.loadtxt("linear_data.csv", *delimiter*=',', *skiprows*=1)
train_x = data[:, 0:2]
train_y = data[:, 2]# initialize parameter
**theta = np.random.randn(4)**
```

# 3 æ ‡å‡†åŒ–

ä¸ºäº†ä½¿è®­ç»ƒå¿«é€Ÿæ”¶æ•›ï¼Œæˆ‘ä»¬ä½¿ç”¨[æ ‡å‡†åŒ–](https://stats.stackexchange.com/a/10298/116970)ï¼Œä¹Ÿå« **z** - **è¯„åˆ†ã€‚**æˆ‘ä»¬æ˜¯æŒ‰åˆ—æ¥åšçš„ã€‚

![](img/5c3f709c7b442c54fde9bf1fc63508c0.png)

*   ğœ‡åœ¨æ¯ä¸€æ éƒ½å¾ˆåˆ»è–„
*   ğœæ˜¯æ¯åˆ—çš„æ ‡å‡†åå·®

```
import numpy as np
import matplotlib.pyplot as plt# read data
data = np.loadtxt("linear_data.csv", *delimiter*=',', *skiprows*=1)
train_x = data[:, 0:2]
train_y = data[:, 2]# initialize parameter
theta = np.random.randn(4)**# standardization
mu = train_x.mean(axis=0)
sigma = train_x.std(axis=0)****def standardizer(x):
    return (x - mu) / sigma****std_x = standardizer(train_x)**
```

# 4 æ·»åŠ åå·®å’Œå¤šé¡¹å¼é¡¹

æˆ‘ä»¬éœ€è¦æ·»åŠ ä¸€ä¸ªåå·®å’Œå¤šé¡¹å¼é¡¹æ¥æ„å»ºæ•°æ®çŸ©é˜µã€‚æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªå¸¸æ•° x0=1ï¼Œä»¥ä¾¿å¯¹é½çŸ¢é‡è¡¨ç¤ºã€‚

![](img/3f19fd852c6c8878549a3d6a524e359b.png)

a specific form fit to our data

![](img/1c7d99d806b8250ca87ea521d4b67c99.png)

vector representation

æ‚¨å¯ä»¥åœ¨ç¬¬ 1 éƒ¨åˆ†æ‰¾åˆ°æ›´å¤šçš„çŸ¢é‡è¡¨ç¤ºç»†èŠ‚: [3 çŸ¢é‡è¡¨ç¤º](/an-equation-to-code-machine-learning-project-walk-through-in-python-part-1-linear-separable-fd0e19ed2d7)ã€‚

ä¸ºäº†ä½¿è®¡ç®—æ›´ç®€å•ï¼Œæˆ‘ä»¬æŠŠ x è½¬æ¢æˆçŸ©é˜µã€‚

```
import numpy as np
import matplotlib.pyplot as plt# read data
data = np.loadtxt("linear_data.csv", delimiter=',', skiprows=1)
train_x = data[:, 0:2]
train_y = data[:, 2]# initialize parameter
theta = np.random.randn(4)# standardization
mu = train_x.mean(axis=0)
sigma = train_x.std(axis=0)
def standardizer(x):
    return (x - mu) / sigma
std_x = standardizer(train_x)**# add x0 and x1^2 to get matrix
def to_matrix(x):
    x0 = np.ones([x.shape[0], 1]) 
    x3 = x[:, 0, np.newaxis] ** 2
    return np.hstack([x0, x, x3])****mat_x = to_matrix(std_x)** **# dot product
def f(x):
    return np.dot(x, theta)**
```

æˆ‘ä»¬ç”¨ x3 æ¥è¡¨ç¤º`x1*x1`ã€‚

`std_x`çš„å°ºå¯¸ä¸º`(20, 2)`ã€‚åœ¨`to_matrix(std_x)`ä¹‹åï¼Œ`mat_x`çš„å°ºå¯¸ä¸º`(20, 4)`ã€‚è‡³äºç‚¹ç§¯éƒ¨åˆ†ï¼Œç»“æœçš„ç»´åº¦æ˜¯`(4,)`ã€‚æ‰€ä»¥ç‚¹ç”Ÿæˆçš„ç»“æœåº”è¯¥æ˜¯`(20, 4) x (4,) -> (20,)`ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 20 ä¸ªæ ·æœ¬é¢„æµ‹çš„ä¸€ç»´æ•°ç»„ã€‚

# 5 Sigmoid å‡½æ•°

ä¸‹é¢æ˜¯çŸ¢é‡è¡¨ç¤º

![](img/10ce40c08977faa5b95da2864ca87a26.png)

ç„¶åæˆ‘ä»¬å°†åŸºäºå®ƒå»ºç«‹ä¸€ä¸ªæ›´å¼ºå¤§çš„é¢„æµ‹å‡½æ•°ï¼Œsigmoid å‡½æ•°ã€‚

![](img/6b8e85b53ae5c43c8ad4dff2f1299236.png)

æˆ‘ä»¬ç”¨ z æ¥è¡¨ç¤ºçº¿æ€§å‡½æ•°ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ sigmoid å‡½æ•°ã€‚sigmoid å‡½æ•°å°†ç»™å‡ºæ¯ä¸ªæ•°æ®æ ·æœ¬çš„æ¦‚ç‡ã€‚æˆ‘ä»¬çš„æ•°æ®ä¸­æœ‰ä¸¤ä¸ªç±»ï¼Œä¸€ä¸ªæ˜¯`1`ï¼Œå¦ä¸€ä¸ªæ˜¯`0`ã€‚

![](img/598ca2a33e63ef98b71a31237199f730.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¨¡å‹åŸºäºçº¿æ€§å‡½æ•°éƒ¨åˆ†é¢„æµ‹æ ·æœ¬ã€‚

![](img/76e6302b06c1921c101316eb5574518b.png)

æˆ‘ä»¬å¯ä»¥å†™ä¸‹é¢çš„ä»£ç 

```
import numpy as np
import matplotlib.pyplot as plt# read data
data = np.loadtxt("linear_data.csv", delimiter=',', skiprows=1)
train_x = data[:, 0:2]
train_y = data[:, 2]# initialize parameter
theta = np.random.randn(4)# standardization
mu = train_x.mean(axis=0)
sigma = train_x.std(axis=0)
def standardizer(x):
    return (x - mu) / sigma
std_x = standardizer(train_x)# add x0 and x1^2 to get matrix
def to_matrix(x):
    x0 = np.ones([x.shape[0], 1]) 
    x3 = x[:, 0, np.newaxis] ** 2
    return np.hstack([x0, x, x3])
mat_x = to_matrix(std_x)**# change dot production to sigmoid function
def f(x):
    return 1 / (1 + np.exp(-np.dot(x, theta)))**
```

# 6 ä¼¼ç„¶å‡½æ•°

> å¦‚æœæ‚¨å¯¹æ–¹ç¨‹çš„è§£é‡Šä¸æ„Ÿå…´è¶£ï¼Œæˆ–è€…æ‚¨å·²ç»åœ¨ç¬¬ 1 éƒ¨åˆ†ä¸­é˜…è¯»è¿‡ï¼Œé‚£ä¹ˆæ‚¨å¯ä»¥è·³è¿‡è¿™ä¸€æ­¥

å¥½äº†ï¼Œæˆ‘ä»¬å‡†å¤‡äº†æ•°æ®ã€æ¨¡å‹(sigmoid ),è¿˜éœ€è¦ä»€ä¹ˆï¼Ÿæ˜¯çš„ï¼Œä¸€ä¸ªç›®æ ‡å‡½æ•°ã€‚**ç›®æ ‡å‡½æ•°å¯ä»¥æŒ‡å¯¼æˆ‘ä»¬å¦‚ä½•ä»¥æ­£ç¡®çš„æ–¹å¼æ›´æ–°å‚æ•°ã€‚å¯¹äº sigmoid(é€»è¾‘å›å½’),æˆ‘ä»¬é€šå¸¸ä½¿ç”¨[å¯¹æ•°ä¼¼ç„¶](https://www.wikiwand.com/en/Likelihood_function#/Log-likelihood)ä½œä¸ºç›®æ ‡å‡½æ•°**

![](img/3de81529a87d49a1798074dc8c709b82.png)

ç­‰ç­‰ï¼Œç­‰ç­‰â€¦è¿™äº›ä¸œè¥¿åˆ°åº•æ˜¯æ€ä¹ˆå›äº‹ï¼

**ä¸è¦æ…Œã€‚å†·é™ç‚¹ã€‚**

è®©æˆ‘ä»¬æŠŠå®ƒæ‹†å¼€ã€‚

*   1->2(å¦‚ä½•ä»ç¬¬ 1 è¡Œåˆ°ç¬¬ 2 è¡Œ):`log(ab) = log a + log b`
*   2->3: `log(a)^b = b * log a`
*   3->4:ç”±äºæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªç±»ï¼Œy=0 å’Œ y=1ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ç­‰å¼:

![](img/7a8e5fda818e6cb17e20c614240fd8d9.png)

3->4

*   4->5:æˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„å˜æ¢ä½¿ç­‰å¼æ›´å…·å¯è¯»æ€§

![](img/d710cf2c40d17501bede7769043057d7.png)

æ‰€ä»¥æˆ‘ä»¬å¾—åˆ°äº†æœ€åä¸€éƒ¨åˆ†ã€‚

![](img/203b7f77765d76082d62d215395437a5.png)

åˆ«å¿˜äº†æˆ‘ä»¬ä¸ºä»€ä¹ˆå¼€å§‹è¿™ä¸ªã€‚**ç›®æ ‡å‡½æ•°å¯ä»¥æŒ‡å¯¼æˆ‘ä»¬å¦‚ä½•ä»¥æ­£ç¡®çš„æ–¹å¼æ›´æ–°å‚æ•°ã€‚**

æˆ‘ä»¬éœ€è¦ç”¨è¿™ä¸ªæ¥è®¡ç®—æŸè€—ï¼Œä»¥æ›´æ–°å‚æ•°ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„**å¯¼æ•°**ã€‚è¿™é‡Œæˆ‘ç›´æ¥ç»™å‡ºæœ€åçš„æ›´æ–°æ–¹ç¨‹å¼ã€‚(å¦‚æœä½ å¯¹å¦‚ä½•å¾—åˆ°è¿™ä¸ªæ–¹ç¨‹æ„Ÿå…´è¶£ï¼Œè¿™ä¸ª[è§†é¢‘](https://www.youtube.com/watch?v=SB2vz57eKgc)åº”è¯¥ä¼šæœ‰å¸®åŠ©)

![](img/92da534e72a3ead43346ba4017d5b243.png)

**ç¬¬å…­æ­¥ï¼Œæœ€é‡è¦çš„æ–¹ç¨‹å°±æ˜¯è¿™ä¸ªã€‚å¦‚æœä½ ä¸æ˜ç™½å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¿™æ˜¯å®Œå…¨å¯ä»¥çš„ã€‚æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯æŠŠå®ƒå†™æˆçœŸæ­£çš„ä»£ç ã€‚**

# 7 æ›´æ–°å‚æ•°Î¸

> å¦‚æœæ‚¨å·²ç»é˜…è¯»äº†ç¬¬ 1 éƒ¨åˆ†ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€æ­¥

è¿™ä¸€æ­¥éå¸¸é‡è¦ã€‚**ä¸è¦æ…Œ**ã€‚æˆ‘ä»¬ä¼šç ´è§£å®ƒã€‚

![](img/92da534e72a3ead43346ba4017d5b243.png)

Î¸j æ˜¯ç¬¬ j ä¸ªå‚æ•°ã€‚

*   Î·æ˜¯å­¦ä¹ ç‡ï¼Œæˆ‘ä»¬è®¾ä¸º 0.001 (1e-3)ã€‚
*   n æ˜¯æ•°æ®æ ·æœ¬çš„æ•°é‡ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æœ‰ 20 ä¸ªã€‚
*   I æ˜¯ç¬¬ I ä¸ªæ•°æ®æ ·æœ¬

å› ä¸ºæˆ‘ä»¬æœ‰ä¸‰ä¸ªå‚æ•°ï¼Œæ‰€ä»¥å¯ä»¥å†™æˆä¸‰ä¸ªæ–¹ç¨‹ã€‚æˆ‘ä»¬ç”¨ x3 æ¥ä»£è¡¨`x1*x1`ã€‚

![](img/3ff5eab945f0274f43a9ca12ea2cc1ff.png)

`:=`ç¬¦å·å°±åƒ`=`ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°è§£é‡Šã€‚

æœ€éš¾çš„éƒ¨åˆ†æ˜¯Ïƒ(æ±‚å’Œç¬¦å·)ï¼Œæ‰€ä»¥ä¸ºäº†æ›´å¥½åœ°ç†è§£ï¼Œæˆ‘æ‰©å±•äº†Ïƒã€‚

![](img/dff28b0186a5db02adafa21f12189b0d.png)

ä»”ç»†çœ‹ã€‚

![](img/a84dd60eb4bfff985c4de97f899c8ed8.png)

æˆ‘ç»™ç­‰å¼ä¸­çš„ä¸‰ä¸ªéƒ¨åˆ†æ¶‚ä¸Šé¢œè‰²ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç”¨çŸ©é˜µæ¥è¡¨ç¤ºå®ƒä»¬ã€‚çœ‹ç¬¬ä¸€è¡Œçº¢è‰²å’Œè“è‰²çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ›´æ–°äº†Î¸0ã€‚

![](img/db5e3e16ff2281e3fa518c2470f6e0b7.png)

æˆ‘ä»¬æŠŠçº¢è‰²éƒ¨åˆ†å’Œè“è‰²éƒ¨åˆ†å†™æˆåˆ—å‘é‡ã€‚

![](img/5eba89b5540ee2ca291c4f4d071858f4.png)

å› ä¸ºæˆ‘ä»¬æœ‰ 20 ä¸ªæ•°æ®æ ·æœ¬ï¼Œæ‰€ä»¥`f`çš„ç»´æ•°æ˜¯`(20,1)`ã€‚`x0`çš„å°ºå¯¸ä¸º`(20,1)`ã€‚æˆ‘ä»¬å¯ä»¥ç”¨è½¬ç½®å†™çŸ©é˜µä¹˜æ³•ã€‚

![](img/0cf3d8cd3910b07662571919f704f771.png)

æ‰€ä»¥ç»´åº¦åº”è¯¥æ˜¯`(1, 20) x (20, 1) -> (1,)`ã€‚æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæ ‡åº¦æ¥æ›´æ–°Î¸0ã€‚

`x1`å’Œ`x2`ä¹Ÿæ˜¯åˆ—å‘é‡ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå®ƒä»¬å†™æˆä¸€ä¸ª **X** çŸ©é˜µã€‚

![](img/e695e2e56a9bf00c1577f7db717cedb5.png)![](img/904ef77c5a1584b4b1981e5547f327b5.png)

Î¸æ˜¯ä¸€ä¸ªè¡Œå‘é‡

![](img/2d7bc9e4e313e6fbedfa95c3719c02ee.png)

å›åˆ°ç­‰å¼ã€‚

![](img/a84dd60eb4bfff985c4de97f899c8ed8.png)

æˆ‘ä»¬å¯ä»¥å†™ä¸º

![](img/5544dd9d089a5276415b6f09de77c8d3.png)

æŠŠå®ƒå†™æˆä¸€ä¸ªç­‰å¼ã€‚

![](img/cba3402100ab21cdce3de0cf1ab08c27.png)

ç±»ä¼¼ Numpy æ•°ç»„çš„ç‰ˆæœ¬å¯èƒ½å®¹æ˜“ç†è§£ã€‚

![](img/8548f80491498880aa47207a696be49e.png)

è®©æˆ‘ä»¬åšä¸€ç‚¹è®¡ç®—ï¼Œä»¥ç¡®ä¿å°ºå¯¸æ˜¯æ­£ç¡®çš„ã€‚

```
Î¸: (1, 4) 
f^T: (1, 20) 
x: (20, 4)dot production: (1, 20) x (20, 4) -> (1, 4)
```

ä¸€åˆ‡çœ‹èµ·æ¥éƒ½é‚£ä¹ˆæ­£ç¡®ã€‚è®©æˆ‘ä»¬å†™ä»£ç ã€‚å…¶å®å°±ä¸¤è¡Œã€‚

```
import numpy as np
import matplotlib.pyplot as plt# read data
data = np.loadtxt("linear_data.csv", delimiter=',', skiprows=1)
train_x = data[:, 0:2]
train_y = data[:, 2]# initialize parameter
theta = np.random.randn(4)# standardization
mu = train_x.mean(axis=0)
sigma = train_x.std(axis=0)
def standardizer(x):
    return (x - mu) / sigma
std_x = standardizer(train_x)# add x0 and x1^2 to get matrix
def to_matrix(x):
    x0 = np.ones([x.shape[0], 1]) 
    x3 = x[:, 0, np.newaxis] ** 2
    return np.hstack([x0, x, x3])
mat_x = to_matrix(std_x)# sigmoid function
def f(x):
    return 1 / (1 + np.exp(-np.dot(x, theta)))# update times
epoch = 2000# learning rate
ETA = 1e-3# update parameter
**for _ in range(epoch):
**    """
    f(mat_x) - train_y: (20,)
    mat_x: (20, 4)
    theta: (4,)

    dot production: (20,) x (20, 4) -> (4,)
    """
 **theta = theta - ETA * np.dot(f(mat_x) - train_y, mat_x)**
```

å¥‡æ€ªçš„äº‹ï¼Ÿè¿˜è®°å¾—æˆ‘ä»¬åœ¨ä»£ç å‰å†™äº†ä»€ä¹ˆå—ï¼Ÿ

```
dot production: (1, 20) x (20, 4) -> (1, 4)The dimension changes make sense here.
```

ä½†æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å†™ä»£ç çš„æ—¶å€™è¦ç”¨`(20,) x (20, 4) -> (4,)`ï¼Ÿ

å®é™…ä¸Šï¼Œè¿™ä¸æ˜¯çœŸæ­£çš„æ•°å­¦ç¬¦å·ï¼Œè¿™æ˜¯ Numpy ç¬¦å·ã€‚è€Œä¸”å¦‚æœä½ ç”¨çš„æ˜¯ TensorFlow æˆ–è€… PyTroch çš„è¯ï¼Œåº”è¯¥å¾ˆç†Ÿæ‚‰ã€‚

`(20,)`è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªåŒ…å« 20 ä¸ªæ•°å­—çš„ä¸€ç»´æ•°ç»„ã€‚å®ƒå¯ä»¥æ˜¯è¡Œå‘é‡ï¼Œä¹Ÿå¯ä»¥æ˜¯åˆ—å‘é‡ï¼Œå› ä¸ºå®ƒåªæœ‰ä¸€ç»´ã€‚å¦‚æœæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºäºŒç»´æ•°ç»„ï¼Œåƒ`(20, 1)`æˆ–`(1, 20)`ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°ç¡®å®š`(20, 1)`æ˜¯åˆ—å‘é‡è€Œ`(1, 20)`æ˜¯è¡Œå‘é‡ã€‚

**ä½†æ˜¯ä¸ºä»€ä¹ˆä¸æ˜¾å¼è®¾ç½®ç»´åº¦æ¥æ¶ˆé™¤æ­§ä¹‰å‘¢ï¼Ÿ**

å¥½å§ã€‚ç›¸ä¿¡æˆ‘ï¼Œæˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°è¿™ä¸ªçš„æ—¶å€™å°±æœ‰æ¥ç¼é—®é¢˜ã€‚ä½†æ˜¯ç»è¿‡ä¸€äº›ç¼–ç å®è·µï¼Œæˆ‘æƒ³æˆ‘çŸ¥é“åŸå› äº†ã€‚

**å› ä¸ºè¿™æ ·å¯ä»¥èŠ‚çœæˆ‘ä»¬çš„æ—¶é—´ï¼**

æˆ‘ä»¬ä»¥`(20,) x (20, 4) -> (4,)`ä¸ºä¾‹ã€‚å¦‚æœæˆ‘ä»¬æƒ³å¾—åˆ°`(1, 20) x (20, 4) -> (1, 4)`ï¼Œæˆ‘ä»¬éœ€è¦å¯¹`(20,) x (20, 4) -> (4,)`åšä»€ä¹ˆï¼Ÿ

*   å°†(20ï¼Œ)è½¬æ¢ä¸º(1ï¼Œ20)
*   è®¡ç®—(1ï¼Œ20) x (20ï¼Œ4) -> (1ï¼Œ4)
*   å› ä¸º(1ï¼Œ4)æ˜¯ä¸€ä¸ªäºŒç»´åˆ—å‘é‡ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºä¸€ç»´æ•°ç»„ã€‚(1,4) -> (4,)

è€å®è¯´ï¼Œè¿™å¾ˆä»¤äººæ²®ä¸§ã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸èƒ½ä¸€æ­¥åˆ°ä½ï¼Ÿ

å¯¹ï¼Œæ‰€ä»¥æˆ‘ä»¬æ‰èƒ½å†™`(20,) x (20, 4) -> (4,)`ã€‚

å¥½äº†ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ [numpy.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) doc æ˜¯æ€ä¹ˆè¯´çš„ã€‚

> [numpy.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) :å¦‚æœ *a* æ˜¯ä¸€ä¸ª N ç»´æ•°ç»„ï¼Œ *b* æ˜¯ä¸€ä¸ª 1 ç»´æ•°ç»„ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯ *a* å’Œ *b* æœ€åä¸€ä¸ªè½´ä¸Šçš„å’Œç§¯ã€‚

å—¯ï¼Œäº‹å®ä¸Šæˆ‘ä¸æ˜ç™½ã€‚ä½†æ˜¯ [np.matmul()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html#numpy.matmul) æè¿°äº†ä¸(20ï¼Œ1)æˆ–(1ï¼Œ20)çš„æ•´å½¢ç±»ä¼¼çš„è®¡ç®—ï¼Œä»¥æ‰§è¡Œæ ‡å‡†çš„ 2d çŸ©é˜µä¹˜ç§¯ã€‚ä¹Ÿè®¸æˆ‘ä»¬èƒ½å¾—åˆ°ä¸€äº›çµæ„Ÿã€‚

> [np.matmul()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html#numpy.matmul) :å¦‚æœç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ä¸€ç»´çš„ï¼Œé‚£ä¹ˆé€šè¿‡åœ¨å®ƒçš„ç»´æ•°å‰åŠ ä¸Š 1ï¼Œå®ƒè¢«æå‡ä¸ºä¸€ä¸ªçŸ©é˜µã€‚åœ¨çŸ©é˜µä¹˜æ³•ä¹‹åï¼Œå‰ç½®çš„ 1 è¢«ç§»é™¤ã€‚

å“ˆï¼Œè¿™å°±æ˜¯ç¼ºå¤±çš„éƒ¨åˆ†ï¼æ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œ`(20,)`å˜æˆäº†`(1, 20)`ï¼Œå› ä¸º`(20,4)`çš„ç¬¬ä¸€ç»´åº¦æ˜¯ 20ã€‚è¿˜æœ‰`(1, 20) * (20, 4) -> (1, 4)`ã€‚ç„¶åå‰ç½® 1 è¢«åˆ é™¤ï¼Œæ‰€ä»¥æˆ‘ä»¬å¾—åˆ°`(4,)`ã€‚ä¸€æ­¥åˆ°ä½ã€‚

# 8 ç”»è¿™æ¡çº¿

åœ¨æ›´æ–°å‚æ•° 2000 æ¬¡åï¼Œæˆ‘ä»¬åº”è¯¥ç»˜åˆ¶ç»“æœæ¥æŸ¥çœ‹æˆ‘ä»¬çš„æ¨¡å‹çš„æ€§èƒ½ã€‚

æˆ‘ä»¬å°†ä¸€äº›æ•°æ®ç‚¹åšä¸º x1ï¼Œæ ¹æ®æˆ‘ä»¬æ‰€å­¦çš„å‚æ•°è®¡ç®— x2ã€‚

![](img/e520e2e547555aba419cf39cfaa30146.png)

```
# plot line
x1 = np.linspace(-2, 2, 100)
**x2 = - (theta[0] + x1 * theta[1] + theta[3] * x1**2) / theta[2]**plt.plot(std_x[train_y == 1, 0], std_x[train_y == 1, 1], 'o') # train data of class 1
plt.plot(std_x[train_y == 0, 0], std_x[train_y == 0, 1], 'x') # train data of class 0
**plt.plot(x1, x2, linestyle='dashed') # plot the line we learned** plt.show()
```

![](img/ad808ef85ce8daef4628a584fb6490a6.png)

# 9 å‡†ç¡®æ€§

åœ¨ç¬¬ 2 éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å‡†ç¡®æ€§æ¥è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½å¦‚ä½•ã€‚

```
import numpy as np
import matplotlib.pyplot as plt# read data
data = np.loadtxt("linear_data.csv", delimiter=',', skiprows=1)
train_x = data[:, 0:2]
train_y = data[:, 2]# initialize parameter
theta = np.random.randn(4)# standardization
mu = train_x.mean(axis=0)
sigma = train_x.std(axis=0)
def standardizer(x):
    return (x - mu) / sigma
std_x = standardizer(train_x)# add x0 and x1^2 to get matrix
def to_matrix(x):
    x0 = np.ones([x.shape[0], 1]) 
    x3 = x[:, 0, np.newaxis] ** 2
    return np.hstack([x0, x, x3])
mat_x = to_matrix(std_x)# sigmoid function
def f(x):
    return 1 / (1 + np.exp(-np.dot(x, theta)))**# classify sample to 0 or 1
def classify(x): 
    return (f(x) >= 0.5).astype(np.int)**# update times
epoch = 2000# learning rate
ETA = 1e-3**# accuracy log
accuracies = []**# update parameter
for _ in range(epoch):    theta = theta - ETA * np.dot(f(mat_x) - train_y, mat_x)    **result = classify(mat_x) == train_y 
    accuracy = sum(result) / len(result) 
    accuracies.append(accuracy)****# plot accuracy line
x = np.arange(len(accuracies))
plt.plot(x, accuracies)
plt.show()**
```

*   `classify(x)`:å¦‚æœæ¦‚ç‡å¤§äº 0.5ï¼Œæˆ‘ä»¬è®¤ä¸ºæ˜¯çœŸçš„
*   `result`:åŒ…å«åˆ—è¡¨å½¢å¼çš„é¢„æµ‹ï¼Œ[çœŸï¼Œå‡ï¼Œâ€¦]
*   `accuracy = sum(result) / len(result)`:è®¡ç®—å½“å‰å†å…ƒä¸­é¢„æµ‹çš„æ­£ç¡®æ ·æœ¬æ•°ã€‚

æœ€åï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†ç²¾åº¦çº¿ã€‚

![](img/30b99968cfabb804ac95fff22487d0b6.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™æ¡çº¿åœ¨ 1000 ä¸ªå‘¨æœŸåå˜å¾—ç¨³å®šã€‚

# 10 æ‘˜è¦

å¦‚æœä½ å·²ç»çœ‹è¿‡ç¬¬ 1 éƒ¨åˆ†ï¼Œä½ ä¼šå‘ç°ç¬¬ 2 éƒ¨åˆ†å¾ˆå®¹æ˜“ç†è§£ã€‚ä½ å¯ä»¥åœ¨ä¸‹é¢æ‰¾åˆ°å®Œæ•´çš„ä»£ç ã€‚ç•™ä¸‹è¯„è®ºè®©æˆ‘çŸ¥é“æˆ‘çš„æ–‡ç« æ˜¯å¦æ˜“æ‡‚ã€‚è¯·ç»§ç»­å…³æ³¨æˆ‘çš„ä¸‹ä¸€ç¯‡å…³äºéšæœºæ¢¯åº¦ä¸‹é™çš„æ–‡ç« ã€‚

> ***æŸ¥çœ‹æˆ‘çš„å…¶ä»–å¸–å­*** [***ä¸­***](https://medium.com/@bramblexu) ***åŒ*** [***ä¸€ä¸ªåˆ†ç±»æŸ¥çœ‹***](https://bramblexu.com/posts/eb7bd472/) ***ï¼
> GitHub:***[***bramble Xu***](https://github.com/BrambleXu) ***LinkedIn:***[***å¾äº®***](https://www.linkedin.com/in/xu-liang-99356891/) ***åšå®¢:***[***bramble Xu***](https://bramblexu.com)