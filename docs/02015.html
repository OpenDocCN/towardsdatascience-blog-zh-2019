<html>
<head>
<title>Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的权重初始化:从基础到明凯的旅程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79?source=collection_archive---------1-----------------------#2019-04-03">https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79?source=collection_archive---------1-----------------------#2019-04-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4960" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我想邀请你和我一起探索初始化神经网络层权重的不同方法。一步一步地，通过各种简短的实验和思考练习，我们将发现为什么足够的权重初始化在训练深度神经网络中如此重要。在这一过程中，我们将涵盖研究人员多年来提出的各种方法，并最终深入探讨最适合您最有可能使用的当代网络架构的方法。</p><p id="5721" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的例子来自我自己对一套笔记本的<a class="ae kl" href="https://nbviewer.jupyter.org/github/jamesdellinger/fastai_deep_learning_course_part2_v3/blob/master/02_fully_connected_my_reimplementation.ipynb?flush_cache=true" rel="noopener ugc nofollow" target="_blank">重新实现</a>，这套笔记本是<a class="ae kl" href="https://www.usfca.edu/faculty/jeremy-howard" rel="noopener ugc nofollow" target="_blank">杰瑞米·霍华德</a>在<a class="ae kl" href="https://www.fast.ai/2019/03/06/fastai-swift/" rel="noopener ugc nofollow" target="_blank">fast . ai 深度学习第二部分课程的最新版本</a>中涵盖的，目前<a class="ae kl" href="https://www.usfca.edu/data-institute/certificates/deep-learning-part-two" rel="noopener ugc nofollow" target="_blank">正在 2019 年春天在 USF 的数据研究所</a>举行。</p><h2 id="a36f" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">为什么要初始化权重</h2><p id="f4a5" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">权重初始化的目的是防止层激活输出在正向通过深度神经网络的过程中爆炸或消失。如果出现任何一种情况，损耗梯度要么过大，要么过小，不利于回流，网络将需要更长时间才能收敛，如果网络能够收敛的话。</p><p id="d79e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">矩阵乘法是神经网络的基本数学运算。在具有几层的深度神经网络中，一次正向传递只需要在每层的输入和权重矩阵之间执行连续的矩阵乘法。一层的乘法结果成为下一层的输入，以此类推。</p><p id="a600" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了说明这一点，让我们假设我们有一个包含一些网络输入的向量<strong class="jp ir"> x </strong>。在训练神经网络时，这是标准做法，以确保我们的输入值按比例调整，使它们落入均值为 0、标准差为 1 的正态分布中。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/e6d7930b24e4679fdf5021abb41af457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ne6gE3vogzJp53lwAYxtGw.png"/></div></div></figure><p id="f011" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们假设我们有一个简单的 100 层网络，没有激活，并且每一层都有一个矩阵<strong class="jp ir"> a </strong>包含该层的权重。为了完成一次前向传递，我们必须在 100 层中的每一层执行层输入和权重之间的矩阵乘法，这将产生总计<em class="lw"> 100 </em>次连续矩阵乘法。</p><p id="efc6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实证明，从我们将输入缩放到的同一标准正态分布初始化图层权重值从来都不是一个好主意。为了了解原因，我们可以通过我们假设的网络模拟一次向前传递。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/fb61a7b54be0ec9d9055d511e1372398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IK15xc15E1LJGlDP1FEXFA.png"/></div></div></figure><p id="6f3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">哇哦。在这 100 次乘法中的某处，层输出变得如此之大，以至于计算机都无法识别它们的标准差和平均值。我们实际上可以看到这发生了多长时间。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/ac59820bdd7863eea5e2f6a34c2b984f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_yr6lnXY-1aXVUWb1MJIYA.png"/></div></div></figure><p id="0ba2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">激活输出在我们网络的 29 层内爆炸。我们显然将权重初始化得太大了。</p><p id="ab92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不幸的是，我们还必须担心防止图层输出消失。为了了解当我们将网络权重初始化得过小时会发生什么情况，我们将调整权重值，这样，虽然它们仍处于平均值为 0 的正态分布中，但它们的标准偏差为 0.01。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/2d405f61c01e1cc50653f49e82c1ed0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z89-cC4jCGPOkp3cnbyr8A.png"/></div></div></figure><p id="6271" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上述假设的向前传递过程中，激活输出完全消失。</p><p id="08c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">综上所述，如果权重初始化过大，网络学习效果不会很好。当权重初始化过小时，也会发生同样的情况。</p><h2 id="da92" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">如何才能找到最佳点？</h2><p id="7698" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">请记住，如上所述，完成神经网络正向传递所需的数学只需要一系列矩阵乘法。如果我们有一个输出<strong class="jp ir"> y </strong>，它是我们的输入向量<strong class="jp ir"> x </strong>和权重矩阵<strong class="jp ir"> a </strong>之间的矩阵乘法的乘积，则<strong class="jp ir"> y </strong>中的每个元素<em class="lw"> i </em>被定义为</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/b39ba6272323b32c1ce70d332a41db93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K3AIFVUelCr0z646zrQjPw.png"/></div></div></figure><p id="15e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="lw"> i </em>是权重矩阵<strong class="jp ir"> a </strong>的给定行索引，<em class="lw"> k </em>既是权重矩阵<strong class="jp ir"> a </strong>的给定列索引，也是输入向量<strong class="jp ir"> x </strong>的元素索引，<em class="lw"> n </em>是<strong class="jp ir"> x </strong>中元素的范围或总数。这也可以在 Python 中定义为:</p><pre class="ll lm ln lo gt lx ly lz ma aw mb bi"><span id="316e" class="km kn iq ly b gy mc md l me mf">y[i] = sum([c*d for c,d in zip(a[i], x)])</span></pre><p id="d276" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以证明，在给定的层，我们从标准正态分布初始化的输入矩阵<strong class="jp ir"> x </strong>和权重矩阵<strong class="jp ir"> a </strong>的矩阵乘积，平均起来，具有非常接近输入连接数量的平方根的标准偏差，在我们的示例中为√512。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/8ab796c9a3713e4888da1668bb51ff91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oJDRa2HOPhe5JV7co5Ig-A.png"/></div></div></figure><p id="6b2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们从矩阵乘法是如何定义的角度来看待它，这一特性并不令人惊讶:为了计算<strong class="jp ir"> y </strong>，我们对输入<strong class="jp ir"> x </strong>的一个元素与一列权重<strong class="jp ir"> a </strong>的逐元素乘法的 512 个乘积求和。在我们的示例中，使用标准正态分布对<strong class="jp ir"> x </strong>和<strong class="jp ir"> a </strong>进行初始化，这 512 个产品中的每一个的平均值为 0，标准差为 1。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/ccfdd1a985be65a3309e05bd4cc53a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wMTdWrSPOXh8C6XxoSO7pg.png"/></div></div></figure><p id="0a28" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，这 512 个产品的<em class="lw">和</em>的平均值为 0，方差为 512，因此标准差为√512。</p><p id="b501" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是为什么在上面的例子中，我们看到我们的层输出在 29 次连续矩阵乘法后爆炸。在我们最基本的 100 层网络架构的情况下，我们希望每层的输出具有大约 1 的标准偏差。可以想象，这将允许我们在尽可能多的网络层上重复矩阵乘法，而不会出现激活爆炸或消失。</p><p id="95b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们首先通过将所有随机选择的值除以√512 来缩放权重矩阵<strong class="jp ir"> a </strong>，填充输出<strong class="jp ir"> y </strong>的一个元素的逐元素乘法现在平均起来只有 1/√512 的方差。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/101a46f04dc79a5708bec4f653c348ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDpw22Z8XSDZe5MLLOLUew.png"/></div></div></figure><p id="ac76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这意味着矩阵<strong class="jp ir"> y </strong>的标准偏差将是 1，该矩阵包含通过输入<strong class="jp ir"> x </strong>和权重<strong class="jp ir"> a </strong>之间的矩阵乘法生成的 512 个值中的每一个。让我们用实验来证实这一点。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/3fcc8851bc53a703a3ce2fb7d070d08f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5LAW2b1nDhPB9k7gt68yZg.png"/></div></div></figure><p id="a5c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们重新运行我们的快速和肮脏的 100 层网络。与之前一样，我们首先从[-1，1]内的标准正态分布中随机选择层权重，但这次我们将这些权重缩放 1/√ <em class="lw"> n </em>，其中<em class="lw"> n </em>是一层的网络输入连接数，在我们的示例中为 512。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/00a829b4ddacd07b9a2c6e3fb2f9c301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8-f9bmfeVVWwLLqSoQADSA.png"/></div></div></figure><p id="b143" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">成功！我们的层输出既没有爆炸也没有消失，即使在我们假设的 100 层之后。</p><p id="57e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然乍一看，我们似乎可以到此为止了，但现实世界的神经网络并不像我们的第一个例子所显示的那样简单。为了简单起见，省略了激活函数。然而，我们在现实生活中从来不会这样做。正是由于在网络层的末端放置了这些非线性激活函数，深度神经网络能够创建描述现实世界现象的复杂函数的近似逼近，然后这些函数可以用于生成令人震惊的预测，例如手写样本的分类。</p><h2 id="169e" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">Xavier 初始化</h2><p id="3231" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">直到几年前，最常用的激活函数都是关于一个给定值对称的，并且其范围渐近地接近与该中点正/负一定距离的值。双曲线正切函数和软设计函数就是这类激活的例子。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/b6b3322858270a45b5761f2fd1c8c5c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*39Dm-zzV98YO-WKCfXgeVg.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Tanh and softsign activation functions. Credit: Sefik Ilkin Serengil’s <a class="ae kl" href="https://sefiks.com/2017/11/10/softsign-as-a-neural-networks-activation-function/" rel="noopener ugc nofollow" target="_blank">blog.</a></figcaption></figure><p id="5a52" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将在假设的 100 层网络的每一层后添加一个双曲正切激活函数，然后看看当我们使用我们自己开发的权重初始化方案时会发生什么，其中层权重按 1/√ <em class="lw"> n. </em></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/73afde260787fe9dca09ce9b0c528db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Ko4_sp9-58wK_hFL2GCHQ.png"/></div></div></figure><p id="d3a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第 100 层的激活输出的标准偏差下降到大约 0.06。这绝对是小的方面，但至少激活没有完全消失！</p><p id="fafa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想起来，发现我们的本土体重初始化策略的旅程似乎很直观，但您可能会惊讶地听到，直到 2010 年，这还不是初始化体重层的常规方法。</p><p id="04d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当 Xavier Glorot 和 Yoshua Bengio 发表了他们题为<a class="ae kl" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lw">理解训练深度前馈神经网络</em> </a>的难度的里程碑式论文时，他们将其实验与“常用的启发式方法”进行比较，该方法是从[-1，1]中的<em class="lw">均匀</em>分布初始化权重，然后按 1/√ <em class="lw"> n </em>进行缩放。</p><p id="7aad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实证明，这种“标准”方法实际上并不那么有效。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/d93cf734d0c4f9a42b6d255748cd301b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RJ2AxAObnSszBlJ6_LZKrQ.png"/></div></div></figure><p id="33ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用“标准”权重初始化重新运行我们的 100 层双曲正切网络导致激活梯度变得极小——它们几乎就像消失了一样。</p><p id="43db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种糟糕的性能实际上促使 Glorot 和 Bengio 提出了他们自己的权重初始化策略，他们在论文中称之为“规范化初始化”，现在被普遍称为“Xavier 初始化”。</p><p id="54ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Xavier 初始化将层的权重设置为从随机均匀分布中选择的值，该分布介于</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/f3444132a23163c40c5bf3db1dbf171a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6t3yYBLlinNRUwmL-d7vw.png"/></div></div></figure><p id="9c07" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="lw"> nᵢ </em>是该层的传入网络连接数，或称“扇入”，而<em class="lw"> nᵢ₊₁ </em>是该层的传出网络连接数，也称为“扇出”</p><p id="9fb4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Glorot 和 Bengio 认为 Xavier 权重初始化将保持激活的方差和反向传播的梯度沿着网络的层向上或向下。在他们的实验中，他们观察到 Xavier 初始化使 5 层网络能够在各层之间保持几乎相同的权重梯度方差。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/79e4aa7169f85ae2f8c7f3f69134e69e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gelf2ZcKYowsLf5FT4n0BQ.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">With Xavier init. Credit: <a class="ae kl" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank">Glorot &amp; Bengio</a>.</figcaption></figure><p id="b727" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相反，使用“标准”初始化导致网络较低层的权重梯度(较高)和最高层的权重梯度(接近于零)之间的方差差距更大。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/04e49a93f07bccb5ac2490ab29966131.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mDilNz4ADDbbr8Qb4d4RqQ.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Without Xavier init. Credit: <a class="ae kl" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank">Glorot &amp; Bengio</a>.</figcaption></figure><p id="fc04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了证明这一点，Glorot 和 Bengio 证明了用 Xavier 初始化的网络在<a class="ae kl" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 图像分类任务</a>中实现了更快的收敛和更高的精度。</p><p id="4c2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们再次运行我们的 100 层 tanh 网络，这次使用 Xavier 初始化:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/c32783f78c1b5237582611197e8d8250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3NiBW8yi-gYrOpsy70PNEg.png"/></div></div></figure><p id="92d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们的实验网络中，Xavier 初始化的执行与我们之前导出的自制方法完全相同，在该方法中，我们从随机正态分布中采样值，并根据传入网络连接数的平方根进行缩放，<em class="lw"> n </em>。</p><h2 id="9dba" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">明凯初始化</h2><p id="e8d9" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">从概念上讲，当使用关于零对称且输出在[-1，1]内的激活函数(如 softsign 和 tanh)时，我们希望每层的激活输出平均值为 0，平均标准偏差约为 1。这正是我们自己开发的方法和 Xavier 所实现的。</p><p id="863f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是如果我们使用 ReLU 激活函数呢？想要以同样的方式缩放随机初始权重值还有意义吗？</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/61feb87faaae38518f47d08b8658e26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*njuH4XVXf-l9pR_RorUOrA.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">ReLU activation function. Credit: Kanchan Sarkar’s <a class="ae kl" href="https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec" rel="noopener">blog</a>.</figcaption></figure><p id="6233" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了了解会发生什么，让我们在假设的网络层中使用 ReLU 激活代替 tanh，并观察其输出的预期标准偏差。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/b93925becdeb5f5531059c0b59594a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0C5Beclgsv-_HEaOfV8eJA.png"/></div></div></figure><p id="1765" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实证明，当使用 ReLU 激活时，单个层的平均标准偏差非常接近输入连接数目的平方根<em class="lw">除以两个</em>的平方根，或者我们示例中的√512/√2。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/c761e39aca79e10c043bef4d157d0189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZ5UcHx2RTeLXL46YxKgbw.png"/></div></div></figure><p id="edc4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过这个数字缩放权重矩阵<strong class="jp ir"> a </strong>的值将导致每个单独的 ReLU 层平均具有 1 的标准偏差。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/754fa20527b57f55c0087efbe4194526.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1G5dke8TAGumS88OKa3TfA.png"/></div></div></figure><p id="f1cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们之前所展示的，将层激活的标准偏差保持在 1 左右将允许我们在深度神经网络中堆叠几个层，而不会出现梯度爆炸或消失。</p><p id="3f29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种对如何最好地初始化具有类 ReLU 激活的网络中的权重的探索是明凯等人的动机。艾尔。为了<a class="ae kl" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank">提出他们自己的初始化方案</a>，这是为使用这种非对称、非线性激活的深度神经网络定制的。</p><p id="f0db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在他们 2015 年的论文中，何等人。艾尔。证明了如果采用以下输入权重初始化策略，深度网络(例如 22 层 CNN)将更早收敛:</p><ol class=""><li id="1f94" class="mm mn iq jp b jq jr ju jv jy mo kc mp kg mq kk mr ms mt mu bi translated">创建一个张量，其维数适合给定层的权重矩阵，并使用从标准正态分布中随机选择的数字填充它。</li><li id="e8ea" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">将每个随机选择的数字乘以<em class="lw"> √ </em> 2/ <em class="lw"> √n </em>，其中<em class="lw"> n </em>是从前一层的输出进入给定层的传入连接数(也称为“扇入”)。</li><li id="0e8d" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">偏置张量被初始化为零。</li></ol><p id="bc86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以按照这些指导来实现我们自己版本的明凯初始化，并验证如果在我们假设的 100 层网络的所有层使用 ReLU，它确实可以防止激活输出爆炸或消失。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/16f3d8680e10b793639cb8dac0fd0f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RD_c4ayThCkRvODAIMfOEQ.png"/></div></div></figure><p id="dbc9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为最后的比较，下面是如果我们使用 Xavier 初始化会发生什么。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/0554c249cdaf9e2e16dce06d93e501ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_1RKMYJbwsIa4SYAj8ol-A.png"/></div></div></figure><p id="ff7b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">哎哟！当使用 Xavier 初始化权重时，激活输出在第 100 层时几乎完全消失了！</p><p id="944b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">顺便提一下，当他们训练使用 ReLUs 的更深层次的网络时，他等。艾尔。发现一个使用 Xavier 初始化的 30 层 CNN 完全失速，根本不学习。然而，当同一个网络按照上述三步程序初始化时，它的收敛速度大大提高。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/60326038faa33d3406cfb02df6b77553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AcZIzXFAJm_ZafRKleF_0g.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Convergence of a <strong class="bd na">30-layer</strong> CNN thanks to Kaiming init. Credit: <a class="ae kl" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank">He et. al.</a></figcaption></figure><p id="049e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个故事对我们的启示是，我们从头开始训练的任何网络，尤其是用于计算机视觉应用的网络，几乎肯定会包含 ReLU 激活功能，并且有几层深度。在这种情况下，明凯应该是我们的首要策略。</p><h2 id="e55f" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">是的，你也可以成为一名研究员</h2><p id="343f" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">更重要的是，我不羞于承认，当我第一次看到泽维尔和明凯公式时，我感到害怕。他们各自的平方根是 6 和 2，我不禁觉得他们一定是某种神谕智慧的结果，我自己无法理解。让我们面对现实吧，有时候深度学习论文中的数学看起来很像象形文字，只是没有 T2 罗塞塔石碑来帮助翻译。</p><p id="4889" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但我认为，我们在这里的旅程向我们表明，这种受到威胁的下意识反应虽然完全可以理解，但绝不是不可避免的。尽管明凯和(尤其是)Xavier 的论文确实包含了相当多的数学内容，但我们亲眼目睹了实验、经验观察和一些简单的常识如何足以帮助推导出支撑当前最广泛使用的权重初始化方案的核心原则集。</p><p id="c773" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说:当有疑问时，鼓起勇气，尝试一下，看看会发生什么！</p></div></div>    
</body>
</html>