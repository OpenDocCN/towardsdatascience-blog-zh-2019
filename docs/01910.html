<html>
<head>
<title>Innate Knowledge and Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">先天知识和深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/innate-knowledge-and-deep-learning-8e9405741ee2?source=collection_archive---------12-----------------------#2019-03-29">https://towardsdatascience.com/innate-knowledge-and-deep-learning-8e9405741ee2?source=collection_archive---------12-----------------------#2019-03-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3595" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们生来就有某种先天知识吗？天赋论正在获得神经科学证据，并可能塑造人工智能和深度学习的下一个 R&amp;D 步骤。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4b8bdd43750b05c0cab3be5939e6a3ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rvW-3vfzRi7sXgPiVBr27g.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ku"><img src="../Images/7d2ed8d7a3d88ea58c75548355993afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1uCWwaR-UfMqallssz0TBg.jpeg"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Figure 1: An elder <a class="ae kz" href="https://en.wikipedia.org/wiki/Plato" rel="noopener ugc nofollow" target="_blank">Plato</a> walks alongside <a class="ae kz" href="https://en.wikipedia.org/wiki/Aristotle" rel="noopener ugc nofollow" target="_blank">Aristotle</a>, <a class="ae kz" href="https://en.wikipedia.org/wiki/The_School_of_Athens" rel="noopener ugc nofollow" target="_blank">The School of Athens, Raphael</a></figcaption></figure><p id="b067" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="lw">“天赋论”..]认为人类的头脑生来就有想法或知识。这种信念，最著名的是由柏拉图作为他的形式理论提出的，后来由笛卡尔在他的《沉思录》中提出，目前正在获得神经科学的证据，可以证实我们生来就有关于我们世界的先天知识的信念。</em></p><p id="9cb9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">天赋论与“纯粹主义”的机器学习方法相冲突，在“纯粹主义”的机器学习方法中，机器学习算法只从数据中学习，而没有显式编程或配备预编程的计算和逻辑模块。“思想的实际内容极其复杂，不可救药；我们应该停止试图寻找简单的方法来思考头脑的内容，例如思考空间、物体、多重代理或对称的简单方法。所有这些都是任意的、内在复杂的外部世界的一部分。它们不是应该内置的，因为它们的复杂性是无穷无尽的；相反，我们应该只构建能够发现和捕获这种任意复杂性的元方法。 " ( <a class="ae kz" href="http://bit.ly/2Uj1h9C" rel="noopener ugc nofollow" target="_blank">来源</a>)</p><p id="4c06" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">截然不同的是，一个不同的思想流派建议将符号人工智能技术与深度学习相结合。</p><h1 id="51f8" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">深度学习的未来</h1><p id="eaf1" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">纽约大学教授加里·马库斯(Gary Marcus)等人提出了一种想法，即深度学习需要与更古老的象征性人工智能技术相结合，以达到人类水平的智能，辛顿对此表示蔑视。辛顿将此比作仅使用电动机来驱动汽油发动机的燃料喷射器，尽管电的能效要高得多。 " ( <a class="ae kz" href="https://bloom.bg/2uw5kB3" rel="noopener ugc nofollow" target="_blank">来源</a>)</p><p id="60a8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">与此同时，混合模型可能会解决深度学习的明显限制，特别是“<em class="lw">深度学习目前缺乏一种通过明确的口头定义来学习抽象的机制，当有数千、数百万甚至数十亿个训练示例时效果最佳</em>”。(<a class="ae kz" href="http://bit.ly/2HZ7QHJ" rel="noopener ugc nofollow" target="_blank">来源</a>)</p><p id="81dd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">是否应该更好地将 GOFAI 融入深度学习？这场争论正在激烈地进行着。</p><h1 id="aec9" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">新的神经学证据</h1><p id="b66c" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">在我看来，讨论最终归结为一个问题——我们人类是从经验中学习一切，还是天生就具备某种形式的先天知识？</p><p id="67b0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">发表在《美国国家科学院院刊》(PNAS)上的一项研究称，“<em class="lw">发现了一种突触组织原理，它以一种在动物中常见的方式对神经元进行分组，因此独立于个体经历</em>”(<a class="ae kz" href="http://bit.ly/2JQRpji" rel="noopener ugc nofollow" target="_blank">来源</a>)。这样的集群包含了物理世界中某些简单工作的表现。<em class="lw">神经元群或细胞集合体，在动物的新皮质中不断出现，本质上是细胞的“积木”。那么，在许多动物中，学习、感知和记忆可能是将这些片段放在一起的结果，而不是形成新的细胞组合</em>(<a class="ae kz" href="http://bit.ly/2FAHDg3" rel="noopener ugc nofollow" target="_blank">来源</a>)。</p><h1 id="63c8" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">一条狭窄的分界线</h1><p id="0638" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">鉴于越来越多的神经学证据支持先天知识的存在，为深度学习配备“先天”计算模块或原语可能是有意义的。很可能一些这样的原语将基于借鉴或受 GOFAI 启发的想法。</p><p id="9e1e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">另一方面，很难预见深度学习架构在未来会是什么样子。Yoshua Bengio 自己承认，“然而，在神经网络能够与人脑拥有的一般智能相匹配之前，需要深度学习的新架构”。</p><p id="5493" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我看来，符号操作很可能会与神经架构深度耦合和纠缠在一起，而不是简单的并列，例如，神经后端和符号前端(如图 2 所示)。“M <em class="lw">模型更接近于通用计算机程序，建立在比我们当前的可区分层丰富得多的原语之上——这就是我们将如何达到推理和抽象，当前模型的根本弱点</em>”(<a class="ae kz" href="http://bit.ly/2NPB3Ez" rel="noopener ugc nofollow" target="_blank">来源</a> <a class="ae kz" href="http://bit.ly/2NPB3Ez)." rel="noopener ugc nofollow" target="_blank">)。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/5f43f396334343179ded0bba44a57591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Nbr_0fAW_YSgrQKXOV5Mw.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Figure 2: Deep Symbolic Reinforcement Learning, the neural back end learns to map raw sensor data into a symbolic representation, which is used by the symbolic front end to learn an effective policy (<a class="ae kz" href="https://arxiv.org/pdf/1609.05518.pdf" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="5b4e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这表明两种方法之间的分界线，“纯粹型”和“混合型”之间的界限非常模糊。因此，我认为，观点上的差异更多的是侧重点的差异，而不是根本性的差异。</p></div></div>    
</body>
</html>