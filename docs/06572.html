<html>
<head>
<title>Sentiment Analysis / Text Classification Using CNN (Convolutional Neural Network)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 CNN(卷积神经网络)的情感分析/文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7?source=collection_archive---------3-----------------------#2019-09-20">https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7?source=collection_archive---------3-----------------------#2019-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/89e2487ad0e57edbb804a75677fbd2a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PFI22lMXZFyPpM3wm-IzeQ.jpeg"/></div></div></figure><div class=""/><p id="ef3b" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi kz translated">文本分类有很多应用。例如，仇恨言论检测、意图分类和组织新闻文章。本文的重点是情感分析，这是一个文本分类问题。我们将 IMDB 意见分为两类，即正面和负面。</p><p id="cda5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们使用<a class="ae li" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> Python </a>和<a class="ae li" href="http://jupyter.org/" rel="noopener ugc nofollow" target="_blank"> Jupyter Notebook </a>来开发我们的系统，我们将使用的库包括<a class="ae li" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras、</a> <a class="ae li" href="https://pypi.org/project/gensim/" rel="noopener ugc nofollow" target="_blank"> Gensim、</a> <a class="ae li" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> Numpy、</a> <a class="ae li" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> Pandas </a>、<a class="ae li" href="https://docs.python.org/2/library/re.html" rel="noopener ugc nofollow" target="_blank"> Regex </a> (re)和<a class="ae li" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK </a>。我们还将使用 Google News Word2Vec <a class="ae li" href="https://github.com/mmihaltz/word2vec-GoogleNews-vectors" rel="noopener ugc nofollow" target="_blank">型号</a>。完整的代码和数据可以从<a class="ae li" href="https://github.com/saadarshad102/Sentiment-Analysis-CNN" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><h2 id="00e6" class="lj lk je bd ll lm ln dn lo lp lq dp lr km ls lt lu kq lv lw lx ku ly lz ma mb bi translated">数据探索</h2><p id="2dd5" class="pw-post-body-paragraph kb kc je kd b ke mc kg kh ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky im bi translated">首先，我们看一下我们的数据。由于数据文件是制表符分隔文件(tsv ),我们将使用 pandas 来读取它，并传递参数来告诉函数分隔符是制表符，并且在我们的数据文件中没有标题。然后，我们设置数据帧的报头。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="b12b" class="lj lk je mm b gy mq mr l ms mt">import pandas as pd<br/>data = pd.read_csv('imdb_labelled.tsv', <br/>                   header = None, <br/>                   delimiter='\t')<br/>data.columns = ['Text', 'Label']<br/>df.head()</span></pre><figure class="mh mi mj mk gt iv gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/909d7a53ff50844e918d1ba7e5c8ebd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*6mnj_g6MF9VurFJqfmEyXA.jpeg"/></div></figure><p id="2808" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后我们检查数据的形状</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="420a" class="lj lk je mm b gy mq mr l ms mt">data.shape</span></pre><p id="5c7c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们看到了阶级分布。我们有 386 个正面和 362 个负面的例子。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="a8c0" class="lj lk je mm b gy mq mr l ms mt">data.Label.value_counts()</span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="7278" class="lj lk je bd ll lm ln dn lo lp lq dp lr km ls lt lu kq lv lw lx ku ly lz ma mb bi translated">数据清理</h2><p id="ceae" class="pw-post-body-paragraph kb kc je kd b ke mc kg kh ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky im bi translated">数据清理的第一步是删除标点符号。我们只需使用正则表达式就可以了。删除标点符号后，数据将保存在同一数据框中。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="333f" class="lj lk je mm b gy mq mr l ms mt">import re</span><span id="b0fe" class="lj lk je mm b gy nc mr l ms mt">def remove_punct(text):<br/>    text_nopunct = ''<br/>    text_nopunct = re.sub('['+string.punctuation+']', '', text)<br/>    return text_nopunct</span><span id="06a4" class="lj lk je mm b gy nc mr l ms mt">data['Text_Clean'] = data['Text'].apply(lambda x: remove_punct(x))</span></pre><p id="c561" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在下一步中，我们通过使用 NLTK 的 word_tokenize 来标记注释。如果我们传递一个字符串‘Tokenizing is easy’给 word_tokenize。输出是['标记化'，'是'，'简单']</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="7dd0" class="lj lk je mm b gy mq mr l ms mt">from nltk import word_tokenize</span><span id="a1dc" class="lj lk je mm b gy nc mr l ms mt">tokens = [word_tokenize(sen) for sen in data.Text_Clean]</span></pre><p id="573d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后我们用小写字母表示数据。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="bc18" class="lj lk je mm b gy mq mr l ms mt">def lower_token(tokens): <br/>    return [w.lower() for w in tokens]    <br/>    <br/>lower_tokens = [lower_token(token) for token in tokens]</span></pre><p id="8e76" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在对数据进行小写处理后，使用 NLTK 的停止字从数据中删除停止字。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="9e19" class="lj lk je mm b gy mq mr l ms mt">from nltk.corpus import stopwords</span><span id="e893" class="lj lk je mm b gy nc mr l ms mt">stoplist = stopwords.words('english')</span><span id="0685" class="lj lk je mm b gy nc mr l ms mt">def removeStopWords(tokens): <br/>    return [word for word in tokens if word not in stoplist]</span><span id="e53f" class="lj lk je mm b gy nc mr l ms mt">filtered_words = [removeStopWords(sen) for sen in lower_tokens]</span><span id="eede" class="lj lk je mm b gy nc mr l ms mt">data['Text_Final'] = [' '.join(sen) for sen in filtered_words]<br/>data['tokens'] = filtered_words</span></pre><p id="b5a1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因为我们的问题是二元分类。我们需要给我们的模型传递一个二维输出向量。为此，我们在数据框中添加了两个 one hot 编码列。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="2261" class="lj lk je mm b gy mq mr l ms mt">pos = []<br/>neg = []<br/>for l in data.Label:<br/>    if l == 0:<br/>        pos.append(0)<br/>        neg.append(1)<br/>    elif l == 1:<br/>        pos.append(1)<br/>        neg.append(0)</span><span id="a626" class="lj lk je mm b gy nc mr l ms mt">data['Pos']= pos<br/>data['Neg']= neg<br/><br/>data = data[['Text_Final', 'tokens', 'Label', 'Pos', 'Neg']]<br/>data.head()</span></pre><figure class="mh mi mj mk gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b08736b0137301d7663a6db67b36674d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*2xo7YQSYTMarm8hJQ1eiUA.jpeg"/></div></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="ebb1" class="lj lk je bd ll lm ln dn lo lp lq dp lr km ls lt lu kq lv lw lx ku ly lz ma mb bi translated">将数据分为测试和训练</h2><p id="991c" class="pw-post-body-paragraph kb kc je kd b ke mc kg kh ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky im bi translated">现在，我们将数据集分为训练集和测试集。我们将使用 90 %的数据进行训练，10 %的数据进行测试。我们使用随机状态，所以每次我们都得到相同的训练和测试数据。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="bb25" class="lj lk je mm b gy mq mr l ms mt">data_train, data_test = train_test_split(data, <br/>                                         test_size=0.10, <br/>                                         random_state=42)</span></pre><p id="05b5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后构建训练词汇，得到最大训练句子长度和总字数的训练数据。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="2f31" class="lj lk je mm b gy mq mr l ms mt">all_training_words = [word for tokens in data_train["tokens"] for word in tokens]<br/>training_sentence_lengths = [len(tokens) for tokens in data_train["tokens"]]<br/>TRAINING_VOCAB = sorted(list(set(all_training_words)))<br/>print("%s words total, with a vocabulary size of %s" % (len(all_training_words), len(TRAINING_VOCAB)))<br/>print("Max sentence length is %s" % max(training_sentence_lengths))</span></pre><p id="54f6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后建立测试词汇，得到测试数据中最大的测试句子长度和总字数。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="20fa" class="lj lk je mm b gy mq mr l ms mt">all_test_words = [word for tokens in data_test[“tokens”] for word in tokens]<br/>test_sentence_lengths = [len(tokens) for tokens in data_test[“tokens”]]<br/>TEST_VOCAB = sorted(list(set(all_test_words)))<br/>print(“%s words total, with a vocabulary size of %s” % (len(all_test_words), len(TEST_VOCAB)))<br/>print(“Max sentence length is %s” % max(test_sentence_lengths))</span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="eda6" class="lj lk je bd ll lm ln dn lo lp lq dp lr km ls lt lu kq lv lw lx ku ly lz ma mb bi translated">正在加载 Google 新闻 Word2Vec 模型</h2><p id="d825" class="pw-post-body-paragraph kb kc je kd b ke mc kg kh ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky im bi translated">现在我们将加载 Google News Word2Vec 模型。这一步可能需要一些时间。如果您有足够的数据量，您可以使用任何其他预先训练的单词嵌入或训练您自己的单词嵌入。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="60e9" class="lj lk je mm b gy mq mr l ms mt">word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'<br/>word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)</span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="621e" class="lj lk je bd ll lm ln dn lo lp lq dp lr km ls lt lu kq lv lw lx ku ly lz ma mb bi translated">标记化和填充序列</h2><p id="c114" class="pw-post-body-paragraph kb kc je kd b ke mc kg kh ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky im bi translated">每个单词被赋予一个整数，这个整数被放在一个列表中。因为所有的训练句子必须具有相同的输入形状，所以我们填充句子。</p><p id="0161" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">例如，如果我们有一个句子“文本如何排序和填充”。每个单词都有一个编号。我们假设 how = 1，text = 2，to = 3，sequence =4，and = 5，padding = 6，works = 7。调用 texts_to_sequences 后，我们的句子看起来会像[1，2，3，4，5，6，7 ]。现在我们假设我们的最大序列长度= 10。填充后，我们的句子将看起来像[0，0，0，1，2，3，4，5，6，7 ]</p><p id="5219" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们对测试数据也做同样的事情。欲了解完整代码<a class="ae li" href="https://github.com/saadarshad102/Sentiment-Analysis-CNN" rel="noopener ugc nofollow" target="_blank">，请访问</a>。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="2347" class="lj lk je mm b gy mq mr l ms mt">tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)<br/>tokenizer.fit_on_texts(data_train[“Text_Final”].tolist())<br/>training_sequences = tokenizer.texts_to_sequences(data_train[“Text_Final”].tolist())</span><span id="bb24" class="lj lk je mm b gy nc mr l ms mt">train_word_index = tokenizer.word_index<br/>print(‘Found %s unique tokens.’ % len(train_word_index))</span><span id="9ce0" class="lj lk je mm b gy nc mr l ms mt">train_cnn_data = pad_sequences(training_sequences, <br/>                               maxlen=MAX_SEQUENCE_LENGTH)</span></pre><p id="37f6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，我们将从 Google News Word2Vec 模型中获取嵌入内容，并根据我们分配给每个单词的序列号保存它们。如果我们不能得到嵌入，我们为这个词保存一个随机向量。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="bc36" class="lj lk je mm b gy mq mr l ms mt">train_embedding_weights = np.zeros((len(train_word_index)+1, <br/> EMBEDDING_DIM))</span><span id="72e2" class="lj lk je mm b gy nc mr l ms mt">for word,index in train_word_index.items():<br/> train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)</span><span id="25da" class="lj lk je mm b gy nc mr l ms mt">print(train_embedding_weights.shape)</span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="0289" class="lj lk je bd ll lm ln dn lo lp lq dp lr km ls lt lu kq lv lw lx ku ly lz ma mb bi translated">定义 CNN</h2><p id="a6dd" class="pw-post-body-paragraph kb kc je kd b ke mc kg kh ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky im bi translated">文本作为一个序列被传递给 CNN。嵌入矩阵被传递给嵌入层。五种不同的过滤器大小应用于每个评论，GlobalMaxPooling1D 层应用于每个层。所有的输出然后被连接。然后施加脱落层、致密层、脱落层和最终致密层。</p><p id="ed20" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">model.summary()将打印所有图层的简要摘要以及输出的形状。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="221e" class="lj lk je mm b gy mq mr l ms mt">def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):<br/> <br/>    embedding_layer = Embedding(num_words,<br/>                            embedding_dim,<br/>                            weights=[embeddings],<br/>                            input_length=max_sequence_length,<br/>                            trainable=False)<br/>    <br/>    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')<br/>    embedded_sequences = embedding_layer(sequence_input)</span><span id="bd3e" class="lj lk je mm b gy nc mr l ms mt">    convs = []<br/>    filter_sizes = [2,3,4,5,6]</span><span id="9367" class="lj lk je mm b gy nc mr l ms mt">    for filter_size in filter_sizes:<br/>        l_conv = Conv1D(filters=200, <br/>                        kernel_size=filter_size, <br/>                        activation='relu')(embedded_sequences)<br/>        l_pool = GlobalMaxPooling1D()(l_conv)<br/>        convs.append(l_pool)</span><span id="4af2" class="lj lk je mm b gy nc mr l ms mt">    l_merge = concatenate(convs, axis=1)</span><span id="4915" class="lj lk je mm b gy nc mr l ms mt">    x = Dropout(0.1)(l_merge)  <br/>    x = Dense(128, activation='relu')(x)<br/>    x = Dropout(0.2)(x)<br/>    preds = Dense(labels_index, activation='sigmoid')(x)</span><span id="bfca" class="lj lk je mm b gy nc mr l ms mt">    model = Model(sequence_input, preds)<br/>    model.compile(loss='binary_crossentropy',<br/>                  optimizer='adam',<br/>                  metrics=['acc'])<br/>    model.summary()<br/>    return model</span></pre><p id="2870" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们将执行该函数。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="2081" class="lj lk je mm b gy mq mr l ms mt">model = ConvNet(train_embedding_weights, <br/>                MAX_SEQUENCE_LENGTH, <br/>                len(train_word_index)+1, <br/>                EMBEDDING_DIM, <br/>                len(list(label_names)))</span></pre><figure class="mh mi mj mk gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/390d94aaaa27a46f1564d40efa709d29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*2EGafixFFGNfRvi7vR05QA.jpeg"/></div></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="10d8" class="lj lk je bd ll lm ln dn lo lp lq dp lr km ls lt lu kq lv lw lx ku ly lz ma mb bi translated">训练 CNN</h2><p id="e69a" class="pw-post-body-paragraph kb kc je kd b ke mc kg kh ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky im bi translated">历元数是您的模型将循环和学习的数量，批量大小是您的模型在单个时间看到的数据量。因为我们只在几个时期内对小数据集进行训练，所以模型会过度拟合。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="7b6e" class="lj lk je mm b gy mq mr l ms mt">num_epochs = 3<br/>batch_size = 32<br/>hist = model.fit(x_train, <br/>                 y_tr, <br/>                 epochs=num_epochs, <br/>                 validation_split=0.1, <br/>                 shuffle=True, <br/>                 batch_size=batch_size)</span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="f8f3" class="lj lk je bd ll lm ln dn lo lp lq dp lr km ls lt lu kq lv lw lx ku ly lz ma mb bi translated">测试模型</h2><p id="49cc" class="pw-post-body-paragraph kb kc je kd b ke mc kg kh ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky im bi translated">哇！仅用三次迭代和一个小数据集，我们就能获得 84 %的准确率。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="7d24" class="lj lk je mm b gy mq mr l ms mt">predictions = model.predict(test_cnn_data, <br/>                            batch_size=1024, <br/>                            verbose=1)<br/>labels = [1, 0]<br/>prediction_labels=[]<br/>for p in predictions:<br/>    prediction_labels.append(labels[np.argmax(p)])</span><span id="b777" class="lj lk je mm b gy nc mr l ms mt">sum(data_test.Label==prediction_labels)/len(prediction_labels)</span></pre></div></div>    
</body>
</html>