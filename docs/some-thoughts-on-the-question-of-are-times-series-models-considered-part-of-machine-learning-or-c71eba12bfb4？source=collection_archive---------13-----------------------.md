# 关于“时间序列模型是否被认为是机器学习的一部分？”

> 原文：<https://towardsdatascience.com/some-thoughts-on-the-question-of-are-times-series-models-considered-part-of-machine-learning-or-c71eba12bfb4?source=collection_archive---------13----------------------->

![](img/081dbd62709921ae4937f12da32c3585.png)

Forecasting the Air Passenger time series: ARIMA (top) vs. LSTM (bottom)

这是一篇非常简短的笔记，在我再次遇到某种形式的问题“时间序列分析是机器学习的一部分吗？/时间序列分析算不算监督学习？”在论坛上。

这个问题显然是一个非常宽泛的问题，并且在某种程度上是主观的。几个月前，我和一位同事在[的工作中发生了一场争论](https://stats.stackexchange.com/q/371619/89649)，争论的焦点是拟合一个指数平滑模型是否构成“从数据中学习”，尽管我们对模型拟合的方式和我们使用的技术完全一致。我不想假装公正地对待这个宽泛的问题，或者对它提供一个全面的答案。相反，在这里我将介绍两种非常有用的方法来看待时间序列分析和 ML 方法的问题。我认为这两种看待问题的方式是有用的，因为记住这两种方式可以让你在试图解决时间序列分析问题时更好地决定使用什么方法。此外，我在这里描述的区别避免了沿着“线性回归被认为是机器学习吗？”等等…

## 统计方法是参数化的，而 ML 方法不是:

在时间序列社区，大多数人(f [跟随例如 Makridakis、Spiliotis 和 Assimakopoulos](https://journals.plos.org/plosone/article/file?type=printable&id=10.1371/journal.pone.0194889) )将时间序列分析技术分为两类:统计方法，如 ARIMA、霍尔特-温特斯、Theta 等，以及 ML 方法，如神经网络、支持向量回归或随机森林。使用这种分类，两个类别之间的主要区别在于，在前一种情况下，模型是参数化的，即假设一个已知的函数来表示数据(例如指数平滑:**Y*(t)=αY(t-1)+(1-α)(Y *(t-1))**，(Y(t)为实际值，Y *(t)为预测值)，我们只需要将正确的参数(在这种情况下为α)拟合到该函数。

在后一种情况下，我们不对代表我们数据的函数的形状做出任何假设，我们依靠[我们算法的通用近似](https://en.wikipedia.org/wiki/Universal_approximation_theorem)属性来找到我们时间序列的最佳拟合(严格地说，大多数 ML 模型也是参数化的，但它们是一种更松散、更广泛的参数化形式。在这种情况下，我们可以认为它们是非参数的，因为它们可以以任意的精度和复杂度来近似任何函数。

这为什么有用？当选择是用统计方法还是用 ML 时，你应该问自己:

*   我的时间序列数据背后的随机过程是否足够复杂，以至于需要某种通用近似器来建模？
*   我的时间序列中是否有足够的数据点和足够高的信噪比，以便我可以拟合复杂的非参数模型？
*   由于拟合简单的参数模型需要较少的计算，可用的计算资源允许基于 ML 的方法吗？(如果您计划自动化您的流程并在生产环境中运行它，这一点尤其重要)

## 序贯方法与纯自回归方法:

第二个有趣的分类是由伯格梅尔、海德曼和古提出的。在他们的论文中，他们将时间序列方法分为序列方法和纯自回归方法:

*   序列模型，如指数平滑模型:这里预测值 **Y*(t)** 和过去值 **Y(t-1)，Y(t-2)** 等之间的关系是递归的，并且随着每个新的时间步长，模型“消耗”一个额外的滞后值。如果想要用数据完整地表达模型，最终需要使用整个时间序列。我们可以用简单的指数平滑法来看这个例子:**Y *(t)=αY(t-1)+(1-α)Y *(t-1)**。如果我们想根据可用的数据扩展这个表达式，我们必须解开表达式:**Y *(t)=αY(t-1)+(1-α)(αY(t-2)+(1-α)Y *(t-2))**等等…直到我们最终得到一个形式为:**Y *(t)=αY(t-1)+(1-α)(αY(t-2)+(1-α)(αY(t-3))的模型..**(1-α)^(t-1)y*(0)。(注意，Y*(0)是一个不可观测的变量，这就是为什么这些方法通常使用最大似然估计量)。你在这里看到了为什么模型被认为是“连续的”:使参数适合数据的唯一方法是按顺序计算一切以适合我们的模型。指数平滑模型、具有 MA 成分的 ARIMA 模型、大多数状态空间模型都属于序列类别。典型地，你用最大似然法和/或卡尔曼滤波器来拟合这样的模型。
*   “纯”自回归方法是任何时间序列模型，其中预测值 **Y*(t)** 是时间序列变量 **Y(t): Y*(t) = f(Y(t-1)，Y(t-2)，…Y(t-n))** 的固定数量的 **n** 过去滞后的函数。 **f(…)** 可以是线性的，也可以是非线性的，关键是用于预测的过去滞后数总是固定的。最大似然模型都属于这一类别，但一些 ARIMA 模型也是如此(即 **ARIMA(p.d.q)** 模型，其中 **q=0** ，因此没有移动平均分量)。您不需要整个序列来估计 **f(Y(t-1)，Y(t-2)，…Y(t-n))** ，并且您仅使用可观察的数据点来拟合/训练您的模型，因此可以使用 OLS 或梯度下降。

这为什么有用？Bergmeir、Hyndman 和 Koo 的论文的主要结果是，对于序列模型，正常的交叉验证不适用，因为模型查看数据的顺序对模型如何拟合至关重要。你将不得不求助于时间序列交叉验证，这是比较棘手的，或者放弃交叉验证，转而采用另一种模型选择方法。另一方面，对于纯自回归方法，如果模型中的误差是不相关的，正常的交叉验证是适用的。因此，是否可以使用交叉验证将取决于您使用的方法。

顺便说一句，使用“纯”自回归模型是因为 ARIMA 或 BSTS 可以有自回归成分，但总体上仍然是序列模型。

最后一点:脸书越来越受欢迎的 Prophet 模型不符合 Bergmeir *等人*提出的任何一个类别，因为它是一个 GAM 风格的模型，直接适用于时间变量。它在精神上更接近于序列模型，在某种意义上，正常的交叉验证不会很好地与它一起工作，因为该模型具有明确的时间依赖性。

Prophet 也不太适合我在第一个分类中提到的任何一个类别:一方面，它是参数化的，本质上是统计性的，基于 [GAMs](https://en.wikipedia.org/wiki/Generalized_additive_model) ，另一方面，Prophet 模型中的第二项是傅立叶展开，这使得它在本质上有点类似于一些基于核的 ML 方法。也许这就是它变得如此受欢迎的原因，它有一种“两全其美”的氛围。

## 参考资料:

1.  南 Makridakis，E. Spiliotis，V. Assimakopoulos，“统计和机器学习预测方法:关注点和前进方向”， *PloS one，* 2018。
2.  C.Bergmeir，R. J. Hyndman，B. Koo，“关于评估时间序列预测的交叉验证有效性的说明”，*莫纳什大学计量经济学和商业统计系工作论文*，2015 年。
3.  S.J. Taylor，B. Latham，《大规模预测》，*美国统计学家，* 2018。