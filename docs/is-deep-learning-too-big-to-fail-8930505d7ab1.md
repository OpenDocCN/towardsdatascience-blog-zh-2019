# 深度学习是不是大到不能倒？

> 原文：<https://towardsdatascience.com/is-deep-learning-too-big-to-fail-8930505d7ab1?source=collection_archive---------45----------------------->

![](img/131611f1e1a0ceda536ef068ebc93488.png)

[*“Occupy Portland: Day 1”*](https://www.flickr.com/photos/80547277@N00/6218867557) *by* [*eliduke*](https://www.flickr.com/photos/80547277@N00) *is licensed under* [*CC BY-SA 2.0*](https://creativecommons.org/licenses/by-sa/2.0/?ref=ccsearch&atype=rich)

上周， [OpenAI 发布了他们去年的人工智能和计算分析的更新](https://openai.com/blog/ai-and-compute/)，其中人们可以读到人工智能的计算“以 3.4 个月的倍增时间呈指数增长(相比之下，摩尔定律的倍增周期为 2 年)。自 2012 年以来，这一指标增长了 300，000 倍以上(2 年的翻一番只会产生 7 倍的增长)。”

鉴于这种令人难以置信的增长率，一些问题出现了:**最先进的人工智能是否从中受益？这种增长速度可持续吗？**如果不是，我们是否正在迎来一个新的人工智能冬天？

嗯，看起来**深度学习模型变得越来越庞大，而规模准确性却没有从中受益太多**。正如 Jameson Toole 在他的文章中令人难以置信地解释的那样，[深度学习有一个规模问题](https://heartbeat.fritz.ai/deep-learning-has-a-size-problem-ea601304cd8)。它还有一个尺寸问题，因为如果我们希望人工智能能够扩展并集成到企业流程中，人工智能模型应该设计为适合普通的计算能力设备，而不是最先进的执行 GPU。仅举几个图尔提到的例子:

> “今年早些时候，NVIDIA 的研究人员宣布了 MegatronLM ，这是一个拥有 83 亿个参数(比 BERT 大 24 倍)的大型变压器模型，在各种语言任务中实现了最先进的性能。虽然这无疑是一项令人印象深刻的技术成就，但我不禁问自己:深度学习的方向正确吗？
> 
> 仅参数一项在磁盘上的重量就超过 33 GB。训练最终模型用了 512 个 V100 GPUs 连续运行 9.2 天。给定每张卡的电力需求，信封背面的估计显示，用于训练该模型的能量是普通美国人年能量消耗的 3 倍以上。"

看看这些数字，这似乎是一个很好的例子，说明当涉及到人工智能的普通消费者时，巨大的力量展示并不会真正成比例。

退一步说，我们必须认识到人工智能可以通过改善这三个轴中的一些来改善:算法创新，更好的数据和计算能力，我完全同意 Jameson Toole 的观点，即**我们应该把效率放在我们的首要任务**(如果不是首要任务的话)**算法创新**当最大化计算能力时，如果我们希望人工智能民主化。

最后，我们不要忘记**更多的数据并不一定意味着更好的数据**。**我们需要高质量的数据**:公正和多样化的数据，这些数据实际上可以帮助人工智能造福于许多社区，这些社区远未获得像玩 AlphaStar 所需的最先进的计算能力。

只有当我们使用高效且具有积极社会影响的算法(因此大多数公民都可以使用)并通过无偏见和多样化的数据进行训练时，深度学习才会“大到不能倒”。它太大了，因为它将为那些大到不能倒的人服务:人民。

*如果你喜欢读这篇文章，请* [*考虑成为会员*](https://dpereirapaz.medium.com/membership) *以便在支持我和媒体上的其他作者的同时获得每个故事的完整访问权限。*