<html>
<head>
<title>Navigating the Hyperparameter Space</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">导航超参数空间</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/navigating-the-hyperparameter-space-32f48be2c12d?source=collection_archive---------27-----------------------#2019-08-19">https://towardsdatascience.com/navigating-the-hyperparameter-space-32f48be2c12d?source=collection_archive---------27-----------------------#2019-08-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="62bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">开始应用机器学习有时会令人困惑。有许多术语需要学习，其中许多术语的使用并不一致，尤其是从一个研究领域到另一个领域——其中一个术语可能意味着两个不同领域中的两种不同事物。</p><p id="d29c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">今天我们来讲一个这样的术语:<strong class="js iu">模型超参数</strong>。</p><h1 id="ae3b" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">参数 vs .超参数:有什么区别？</h1><p id="4d9c" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">让我们从最基本的区别开始，即参数和超参数之间的区别。</p><p id="1d73" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">模型参数</strong>是模型内部的属性，并且是在训练阶段学习到的，并且是模型进行预测所需要的。</p><p id="24af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一方面，<a class="ae lr" href="https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">模型超参数</strong>不能在训练期间学习，而是预先设定</a>。它们的值可以使用经验法则或试错法来设置；但事实上，我们无法知道特定问题的最佳模型超参数— <strong class="js iu">，因此我们调整超参数以发现具有最佳性能的模型参数</strong>。</p><blockquote class="ls"><p id="7630" class="lt lu it bd lv lw lx ly lz ma mb kn dk translated">在某种程度上，超参数是我们可以用来调整模型的旋钮。</p></blockquote><h1 id="9b4f" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz mc lb lc ld md lf lg lh me lj lk ll bi translated">超参数调谐</h1><p id="0665" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">也叫<strong class="js iu">超参数优化</strong>，是<a class="ae lr" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" rel="noopener ugc nofollow" target="_blank"> <em class="mf">为一个特定的学习算法</em> </a>寻找性能最好的超参数集合的问题。</p><p id="7fcc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个重要的步骤，因为<strong class="js iu">使用正确的超参数将会发现模型的参数，从而产生最巧妙的预测</strong>；这也是我们最终想要的。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/6c412631ef2552bb57ff9c6c2ec192d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*wmjUZxavf1NkqlHwFjOePQ.gif"/></div></figure><p id="7515" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种性质的优化问题有三个基本组成部分:(1)一个<strong class="js iu">目标函数</strong>，这是我们想要最大化或最小化的模型的主要目的；(2)控制目标函数的一组变量<strong class="js iu">；(3)一组<strong class="js iu">约束</strong>，允许变量取某些值，同时排除其他值。</strong></p><blockquote class="ls"><p id="95cf" class="lt lu it bd lv lw lx ly lz ma mb kn dk translated">因此，优化问题是找到最大化或最小化目标函数同时满足约束集的一组变量值。</p></blockquote><p id="79e8" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">解决优化问题有不同的方法，也有许多实现这些方法的开源软件选项。在本文中，我们将探索<strong class="js iu">网格搜索、随机搜索和贝叶斯优化</strong>。</p><h1 id="8c41" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">网格搜索</h1><p id="7d43" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">也被称为<strong class="js iu">参数扫描</strong> <em class="mf">，</em>它被认为是最简单的超参数优化算法。它包括彻底搜索手动指定的一组参数，这意味着为指定子集的所有可能组合训练模型。</p><p id="343a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果模型可以快速训练，这种方法可能是一个很好的选择，否则，模型将需要很长时间来训练。这就是为什么<a class="ae lr" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html" rel="noopener ugc nofollow" target="_blank">不认为使用<strong class="js iu">网格搜索</strong>来调整神经网络</a>的超参数是最佳实践。</p><p id="73d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个流行的实现是<a class="ae lr" href="https://scikit-learn.org/stable/modules/grid_search.html#grid-search" rel="noopener ugc nofollow" target="_blank"> <em class="mf"> Scikit-Learn </em> </a>的<a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" rel="noopener ugc nofollow" target="_blank"> <em class="mf"> GridSearchCV </em> </a>。其他实现还有<a class="ae lr" href="https://github.com/autonomio/talos" rel="noopener ugc nofollow" target="_blank"> <em class="mf"> Talos </em> </a>，一个包括<a class="ae lr" rel="noopener" target="_blank" href="/hyperparameter-optimization-with-keras-b82e6364ca53"><em class="mf"/></a>用于<a class="ae lr" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> <em class="mf"> Keras </em> </a>和<a class="ae lr" href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html" rel="noopener ugc nofollow" target="_blank"> <em class="mf"> H2O </em> </a>的库，一个为不同的机器学习算法提供<a class="ae lr" href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html" rel="noopener ugc nofollow" target="_blank"><em class="mf"/></a><em class="mf"/>网格搜索实现的平台。</p><h1 id="884f" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">随机搜索</h1><p id="506c" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated"><strong class="js iu">随机搜索</strong>背后的思想与<strong class="js iu">网格搜索</strong>非常相似，除了它不是穷尽搜索手动指定参数集的所有可能组合，而是<a class="ae lr" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html" rel="noopener ugc nofollow" target="_blank">选择组合的随机子集来尝试</a>。</p><blockquote class="ls"><p id="35e5" class="lt lu it bd lv lw lx ly lz ma mb kn dk translated">这种方法大大减少了运行超参数优化的时间，但是有一个警告:没有<a class="ae lr" rel="noopener" target="_blank" href="/hyperparameter-tuning-c5619e7e6624">保证会找到最优的超参数集</a>。</p></blockquote><p id="4f43" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">一些流行的实现有<a class="ae lr" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="mf"> Scikit-Learn </em> </a>的<a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV" rel="noopener ugc nofollow" target="_blank"><em class="mf">RandomizedSearchCV</em></a><a class="ae lr" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank"><em class="mf">hyperpt</em></a>和<a class="ae lr" href="https://github.com/autonomio/talos" rel="noopener ugc nofollow" target="_blank"> <em class="mf"> Talos </em> </a>。</p><h1 id="d9fa" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">贝叶斯优化</h1><p id="a842" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">我们已经确定<strong class="js iu">随机搜索</strong>和<strong class="js iu">网格搜索</strong>建立在相似的想法上，它们的另一个共同点是<strong class="js iu">不使用以前的结果来通知每个超参数子集的评估</strong>，并且反过来<strong class="js iu">、</strong>它们<strong class="js iu">花费时间评估不到最优选项</strong>。</p><p id="d4be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">相比之下，<strong class="js iu">贝叶斯优化</strong>会跟踪过去的评估结果，使其成为一种自适应的优化方法。寻找满足目标函数的变量的值是一种<a class="ae lr" href="https://arxiv.org/pdf/1012.2599.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mf">强有力的策略，该目标函数评估</em> </a>是昂贵的。在某种程度上，当我们需要<strong class="js iu">最小化我们在试图<a class="ae lr" href="https://arxiv.org/pdf/1012.2599.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">找到全局最优</strong> </a>时所采取的步骤</strong>的数量时，贝叶斯技术是最有用的。</p><blockquote class="ls"><p id="7264" class="lt lu it bd lv lw lx ly lz ma mb kn dk translated"><a class="ae lr" href="http://krasserm.github.io/2018/03/21/bayesian-optimization/" rel="noopener ugc nofollow" target="_blank">贝叶斯优化结合了关于目标函数的先验信念(<strong class="ak"><em class="mt">【f】</em></strong>)，并使用从<strong class="ak"><em class="mt">【f】</em></strong>中提取的样本更新先验，以获得更好地逼近的后验概率(<strong class="ak"><em class="mt">f)</em></strong></a><strong class="ak"><em class="mt">——</em></strong>马丁·克拉瑟(2018)贝叶斯优化。</p></blockquote><p id="57bf" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">为了实现这一点，<strong class="js iu">贝叶斯优化</strong>利用了另外两个概念:一个<strong class="js iu">代理模型</strong>和一个<strong class="js iu">获取函数</strong>。第一个是指用于<a class="ae lr" href="https://arxiv.org/pdf/1012.2599.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">逼近目标函数</strong> </a>的概率模型，第二个是用于<a class="ae lr" href="https://arxiv.org/pdf/1012.2599.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">确定采样域内的新位置</strong> </a>，在该位置最有可能对当前最佳评估结果进行改进。这是<strong class="js iu">贝叶斯优化</strong>模型效率背后的两个关键因素。</p><p id="1cf4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">替代模型</strong>有几种不同的选择，最常见的有<a class="ae lr" href="https://app.sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">高斯过程、随机森林回归和树 Parzen 估计器</strong> </a>。至于采集函数，最常见的有<a class="ae lr" href="https://arxiv.org/pdf/1012.2599.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"/></a><strong class="js iu"/><a class="ae lr" href="http://krasserm.github.io/2018/03/21/bayesian-optimization/" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">最大概率改善，置信上限</strong> </a>。</p><p id="7697" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一些流行的实现有<a class="ae lr" href="https://github.com/scikit-optimize/scikit-optimize" rel="noopener ugc nofollow" target="_blank"><em class="mf">Scikit-Optimize</em></a><em class="mf"><a class="ae lr" href="https://scikit-optimize.github.io/notebooks/bayesian-optimization.html" rel="noopener ugc nofollow" target="_blank"><em class="mf">贝叶斯优化</em></a><a class="ae lr" href="https://github.com/automl/SMAC3" rel="noopener ugc nofollow" target="_blank"><em class="mf">SMAC</em></a><a class="ae lr" href="https://github.com/JasperSnoek/spearmint" rel="noopener ugc nofollow" target="_blank"><em class="mf">兰香</em></a><a class="ae lr" href="https://github.com/Yelp/MOE" rel="noopener ugc nofollow" target="_blank"><em class="mf">MOE</em></a><a class="ae lr" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank"><em class="mf">hyperpt</em></a>。</em></p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="4480" class="ko kp it bd kq kr nb kt ku kv nc kx ky kz nd lb lc ld ne lf lg lh nf lj lk ll bi translated">结论</h1><p id="02c8" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">超参数优化对于机器学习任务来说是一件大事。这是提高模型性能的重要一步，但不是实现这一目标的唯一策略。</p><p id="f4de" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们探索的所有方法中，不同的从业者和专家会根据他们的经验提倡不同的方法，但是他们中的许多人也会同意实际上<a class="ae lr" href="https://stats.stackexchange.com/questions/302891/hyper-parameters-tuning-random-search-vs-bayesian-optimization" rel="noopener ugc nofollow" target="_blank">取决于数据</a>、问题和其他项目考虑因素。最重要的一点是，这是寻找最佳表演模特的<strong class="js iu">关键步骤。</strong></p></div></div>    
</body>
</html>