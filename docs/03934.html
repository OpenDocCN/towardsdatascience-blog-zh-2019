<html>
<head>
<title>[CVPR 2017] OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[CVPR 2017] OpenPose:使用部分亲和力场的实时多人 2D 姿势估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cvpr-2017-openpose-realtime-multi-person-2d-pose-estimation-using-part-affinity-fields-f2ce18d720e8?source=collection_archive---------2-----------------------#2019-06-21">https://towardsdatascience.com/cvpr-2017-openpose-realtime-multi-person-2d-pose-estimation-using-part-affinity-fields-f2ce18d720e8?source=collection_archive---------2-----------------------#2019-06-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="48f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">今天的话题是来自<strong class="jp ir"> CVPR 2017 </strong>的一篇名为<strong class="jp ir">“利用局部亲和场的实时多人 2D 姿态估计”</strong>的论文。这项工作对计算机视觉社区有着非凡的贡献，因为:</p><ul class=""><li id="35b1" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">它基于其<strong class="jp ir">自底向上的方法</strong>而不是其他作品中基于检测的方法，为多人 2D 姿态估计提供了一种<strong class="jp ir">实时方法。</strong></li><li id="7dda" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">作者开源并将他们的工作扩展到<strong class="jp ir">第一个实时多人系统，以联合检测单一图像上的人体、手、面部和脚关键点(总共 135 个关键点)——open pose</strong>。今天，这个库被广泛用于各种研究工作和生产应用。</li></ul><p id="93ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于这个 OpenPose 系统有很多帖子。然而，我们的帖子有一些不同之处:</p><ul class=""><li id="cdb1" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">我们将回顾作者的<a class="ae kz" href="https://arxiv.org/abs/1812.08008" rel="noopener ugc nofollow" target="_blank">期刊文章，2018 年发表在 arXiv </a>上，比<a class="ae kz" href="https://arxiv.org/abs/1611.08050" rel="noopener ugc nofollow" target="_blank"> CVPR 2017 版</a>更准确，速度更快。</li><li id="0676" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">对于其他博文中通常会跳过的论文后处理步骤，我们会提供<strong class="jp ir">一个详细的解释。</strong></li></ul><p id="383e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该文件的一些要点:</p><ul class=""><li id="4674" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">通过<strong class="jp ir">零件亲缘关系字段(PAF)</strong>的关联分数的<strong class="jp ir">第一次自下而上展示</strong>。</li><li id="fa1e" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">不变运行时间</strong>到图像中的<strong class="jp ir">人数</strong>。</li><li id="e487" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">精细化网络使速度和精度分别提高了<strong class="jp ir"> 200% </strong>和<strong class="jp ir">7%</strong>(2018 年期刊版)</li><li id="1223" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">可以将<strong class="jp ir">推广到任何关键点关联任务</strong>如车辆关键点检测。</li></ul><h1 id="808c" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">概述</h1><ul class=""><li id="4c44" class="kl km iq jp b jq ly ju lz jy ma kc mb kg mc kk kq kr ks kt bi translated">介绍</li><li id="4c42" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">体系结构</li><li id="ce0e" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">置信图</li><li id="8f40" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">零件亲缘关系字段(PAF)</li><li id="05d4" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">多级 CNN</li><li id="b2c6" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">使用 PAFs 的多人解析</li><li id="7c3f" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">车辆姿态估计</li><li id="a7d2" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">履行</li><li id="442d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">参考</li></ul><h1 id="210c" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">介绍</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/0c884062a7373d081f4c989a162b8d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/0*VOAvEFqZiQrIWyeR.png"/></div></figure><p id="f7c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">人体姿态估计</strong>是理解图像和视频中人的核心问题。在单人姿态估计中，通过假设图像只有一个人，问题被简化。多人姿态估计更困难，因为在一幅图像中有多人。多年来，有许多工作致力于解决这个问题。</p><p id="222d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一种常见的方法是遵循<strong class="jp ir">两步框架</strong>，该框架使用人体检测器并解决每个人体的姿态估计。这种方法的运行时间往往会随着图像中人数的增加而增加，并使实时性能成为一个挑战。</p><p id="3877" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本文中，作者提供了一种<strong class="jp ir">自底向上的方法</strong>，其中通过模型检测身体部位，并使用最终解析来提取姿态估计结果。这种方法可以<strong class="jp ir">将运行时间复杂度从图像中的人数</strong>中分离出来。</p><p id="371d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者也在<a class="ae kz" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> OpenPose 库</a>开源他们的作品。这个 OpenPose 系统提供了<strong class="jp ir">易于使用的管道，带有命令行接口、Python API、Unity 插件</strong>。系统支持<strong class="jp ir">NVIDIA GPU(CUDA)、AMD GPU(OpenCL)和纯 CPU</strong>。他们甚至提供<strong class="jp ir">可移植的可执行二进制文件</strong>，可以简单地下载并在 Windows 上使用。</p><p id="772c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是社区分析的一些视频:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="ml mm l"/></div></figure><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mn mm l"/></div></figure><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mn mm l"/></div></figure><h1 id="b8dd" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">体系结构</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/92aa5c6fd43a2d735b16a0c4c99e400e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RiQ9rkWEtPR0uVfx.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Overall Pipeline of the OpenPose architecture. (<a class="ae kz" href="https://arxiv.org/abs/1812.08008" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="aa76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图是 OpenPose 的整体流水线。有几个步骤:</p><ul class=""><li id="78ef" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">首先，图像通过基线网络提取特征图。在论文中，作者使用了 VGG-19 模型的前 10 层。</li><li id="f825" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">然后，用多级 CNN 处理特征图，以生成:<strong class="jp ir"> 1)一组零件置信度图</strong>和<strong class="jp ir"> 2)一组零件亲和域(PAF)</strong></li><li id="2e50" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">部位置信度图</strong>:一组身体部位位置的 2D 置信度图<strong class="jp ir"> S </strong>。每个关节位置都有一张地图。</li><li id="c7ce" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">零件亲和域(PAF)</strong>:一组 2D 矢量场<strong class="jp ir"> L </strong>，对零件之间的关联程度进行编码。</li><li id="aaf6" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">最后，通过贪婪算法处理<strong class="jp ir">置信图</strong>和<strong class="jp ir">部分亲和场</strong>，以获得图像中每个人的姿态。</li></ul><h1 id="0b53" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">置信图</h1><p id="5608" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">置信图是一种 2D 表示法，表示特定身体部位可以在任何给定像素中定位的信念。</p><p id="7e6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设 J 为身体部位(关节)的数量。然后，<strong class="jp ir">置信图</strong>是:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi na"><img src="../Images/5ae35baa3b5fc9a0544efd84e602a363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*OX6b1zDDwlUIx85pdD_d3w.png"/></div></figure><p id="f269" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">总之，每个地图对应一个关节，并且与输入图像</strong>大小相同。</p><h1 id="9aa3" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">零件亲缘关系字段(PAF)</h1><p id="c8c6" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><strong class="jp ir">部位亲和场(PAF) </strong>是对身体部位之间的非结构化成对关系进行编码的一组流场。</p><p id="7f3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每对身体部位都有一个<strong class="jp ir"> (PAF) </strong>，即脖子、鼻子、手肘等。</p><p id="565a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设 CC 为身体部位的对数。然后，<strong class="jp ir">部分关联字段(PAF)</strong>是:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nb"><img src="../Images/bfbf657bc09d979ac32e08395ce0890f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5VAvHsmD_6bacQJiduxgw.png"/></div></div></figure><p id="25fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果一个像素在一个肢体(身体部位)上，该像素的<strong class="jp ir"> Lc </strong>中的值是从开始关节到结束关节的 2D 单位向量。</p><h1 id="81f1" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">多级 CNN</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nc"><img src="../Images/0fa67d5a30b12c7fd97a4a8e189c57b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*83LSLGoIrMqCATlZ.png"/></div></div></figure><p id="8a36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是来自 OpenPose 的 2018 journal 版本的多级 CNN 的架构。有如下几个步骤:</p><ul class=""><li id="75e2" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">阶段<strong class="jp ir"> 1 </strong>:从基网络的特征图中计算<strong class="jp ir">零件亲和域(PAF)</strong>、<strong class="jp ir"> L1 </strong>、<strong class="jp ir"> F </strong>。让ϕ1 成为第一阶段的 CNN。</li></ul><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/bc3b6b53935621d1255bb061b325916e.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*DsuY3S2cKaloTedB0ATNOA.png"/></div></figure><ul class=""><li id="7047" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">阶段<strong class="jp ir"> t </strong>到阶段<strong class="jp ir"> Tp </strong>:使用特征图<strong class="jp ir"> F </strong>和先前的 PAF<strong class="jp ir">Lt 1</strong>改进先前阶段的 PAF 预测。让ϕt 成为舞台上的 CNN</li></ul><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/14bd8fe37770a8ebd733dee37ec5ec3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*MT4zI2eoC3A91T9_5zVQ5A.png"/></div></figure><ul class=""><li id="971d" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">在<strong class="jp ir"> Tp </strong>迭代之后，从<strong class="jp ir">最近更新的 PAF 预测</strong>开始，对<strong class="jp ir">置信图检测</strong>重复该过程。设<strong class="jp ir"> ρt </strong>为阶段<strong class="jp ir"> t </strong>的 CNN。该过程重复进行<strong class="jp ir"> Tc </strong>迭代。</li></ul><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/99a57c1a859d09a2c6bd7435378b05bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*kh_EtaVG6mPxtAStJjigYg.png"/></div></figure><ul class=""><li id="8f7e" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">最后的<strong class="jp ir"> S </strong>和<strong class="jp ir"> L </strong>是<strong class="jp ir">置信图</strong>和<strong class="jp ir">零件亲和域(PAF)</strong>，它们将由贪婪算法进一步处理。</li></ul><p id="ec83" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:</p><ul class=""><li id="9fa3" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">这个多级 CNN 来自 2018 期刊版。在最初的 CVPR 2017 版本中，他们在每个阶段都改进了<strong class="jp ir">信心图</strong>和<strong class="jp ir">零件亲和域(PAF)</strong>。因此，它们在每个阶段都需要更多的计算和时间。在新方法中，作者发现新方法在速度和精度上分别提高了<strong class="jp ir"> 200% </strong>和<strong class="jp ir"> 7% </strong>。</li></ul><h1 id="16b0" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">使用 PAFs 的多人解析</h1><p id="7cfc" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">在本节中，我们将向您概述贪婪算法，该算法用于从<strong class="jp ir">置信度图</strong>和<strong class="jp ir">零件相似性字段</strong>中解析多人的姿势。</p><p id="24c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">许多其他博客帖子显示了这个问题有多难。然而，没有很多详细的解释这个步骤。</p><p id="93c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">幸运的是，我们发现了一个由 tensorboy 完成的这篇论文的优秀实现，它有详细的文档和简单明了的代码。我们建议您检查一下这个存储库，并亲自尝试一下。</p><p id="24f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">解析过程可以总结为三个步骤:</p><ul class=""><li id="e354" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">步骤 1 </strong>:使用<strong class="jp ir">置信图</strong>找到<strong class="jp ir">所有关节</strong>的位置。</li><li id="cc3f" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">第 2 步</strong>:使用第 1 步中的<strong class="jp ir">部件亲和域</strong>和关节，找到哪些关节组合在一起<strong class="jp ir">形成肢体(身体部位)</strong>。</li><li id="2cf0" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">第三步</strong> : <strong class="jp ir">关联属于同一个人的肢体</strong>，得到最终的人体姿态列表。</li></ul><h1 id="ec70" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">步骤 1:使用置信图找到所有关节位置。</h1><p id="a6ab" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><strong class="jp ir">输入</strong>:</p><ul class=""><li id="5038" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">置信图:</strong></li></ul><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi na"><img src="../Images/5ae35baa3b5fc9a0544efd84e602a363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*OX6b1zDDwlUIx85pdD_d3w.png"/></div></figure><ul class=""><li id="cc24" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">上采样比例</strong>:输入图像和置信图的宽/高差异。</li></ul><p id="2f91" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">输出</strong>:</p><ul class=""><li id="25b5" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir"> joints_list </strong>:大小为 JJ 的关节位置列表，其中每一项都是峰值列表(x，y，概率)。</li><li id="b3cc" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">例如，<strong class="jp ir">关节 _ 列表</strong>长度为 18，对应 18 个关节位置(鼻子、脖子等)。)并且<strong class="jp ir"> joints_list </strong>中的项目是不同长度的列表，其存储每个关节位置的峰值信息(x，y 位置和概率分数)。</li></ul><p id="e43d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">流程</strong>:</p><p id="49de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于从 1 到 J 的每个关节:</p><ul class=""><li id="d8c6" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">在<strong class="jp ir">置信图</strong>中获取相应的接头 2D 热图。</li><li id="47b0" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">通过对 2D 热图进行阈值处理找到峰值。</li><li id="a942" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">对于每个峰值:</li><li id="a7fd" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">-在堆中的峰周围取一小块</li><li id="6513" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">-使用<strong class="jp ir">上采样标尺</strong>放大补丁。</li><li id="ce9b" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">-在放大的补丁中获得最大峰值位置。</li><li id="f7b3" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">-将峰信息添加到接头的峰列表中</li></ul><h1 id="fe51" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">第 2 步:使用第 1 步中的“部分相似性”字段和关节，找到哪些关节组合在一起形成肢体(身体部分)。</h1><p id="e3a6" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><strong class="jp ir">输入</strong>:</p><ul class=""><li id="c5ba" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">关节 _ 列表</strong>:第一步的输出。</li><li id="921b" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">零件关联性字段(PAF)</strong>:</li></ul><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nb"><img src="../Images/bfbf657bc09d979ac32e08395ce0890f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5VAvHsmD_6bacQJiduxgw.png"/></div></div></figure><ul class=""><li id="8e56" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">上采样比例</strong>:输入图像和 PAFs 贴图的宽高不同。</li><li id="bc71" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">中间点数</strong>:获取 PAFs 值的源关节和目的关节之间的中间点数。</li></ul><p id="d8d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">输出</strong>:</p><ul class=""><li id="a3b1" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir"> connected_limbs </strong>:大小为 CC 的连通分支列表，其中每一项都是找到的该类型的所有分支的列表。</li><li id="47f1" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">每个肢体信息包含:源关节的 id、目标关节的 id 和连接程度的分数。</li></ul><p id="df7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">流程</strong>:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi ng"><img src="../Images/d4f9f22e527ed6a6b94419e40f39917a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v2bC8MXnKU9fnjuBoUYF7w.png"/></div></div></figure><h1 id="b04b" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">第三步:关联属于同一个人的肢体，得到最终的人体姿态列表。</h1><p id="cb32" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><strong class="jp ir">输入</strong>:</p><ul class=""><li id="3058" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">关节 _ 列表</strong>:来自步骤 1</li><li id="726c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">连接 _ 肢体</strong>:来自步骤 2</li></ul><p id="59ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">输出</strong>:</p><ul class=""><li id="4a0a" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">姿势</strong>:图像中每个人的姿势列表。每个项目都包含该人的关节位置。</li></ul><p id="c325" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">流程</strong>:</p><p id="4fe0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每个肢体类型和该类型的<strong class="jp ir">连接肢体</strong>中的每个连接:</p><ul class=""><li id="a9ac" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">找到与当前连接的任一关节相关联的人</li><li id="a2d9" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果<strong class="jp ir">没有</strong>人；用当前连接创建一个新的人</li><li id="3386" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果有<strong class="jp ir"> 01 </strong>人:将当前连接添加到该人</li><li id="5402" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果有<strong class="jp ir"> 02 </strong>人:将这 02 人合并成 01 人。</li></ul><p id="40b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">移除关节很少的人。</p><h1 id="5d20" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">车辆姿态估计</h1><p id="7b5f" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">在 2018 年的期刊版本中，作者证明了这种方法可以应用于任何关键点标注任务。下图是车辆关键点数据集的结果:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nh"><img src="../Images/aeaaa580f20267ada49e4b726e01b3c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H-Kt0HtNT6unx6l7.png"/></div></div></figure><h1 id="17f2" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">履行</h1><ul class=""><li id="ae14" class="kl km iq jp b jq ly ju lz jy ma kc mb kg mc kk kq kr ks kt bi translated">OpenPose 的最佳实现来自用 C++实现的<a class="ae kz" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> OpenPose 库</a>。这个 OpenPose 系统提供了<strong class="jp ir">易于使用的管道，带有命令行接口、Python API、Unity 插件</strong>。系统支持<strong class="jp ir">NVIDIA GPU(CUDA)、AMD GPU(OpenCL)和纯 CPU</strong>。他们甚至提供<strong class="jp ir">可移植的可执行二进制文件</strong>，可以简单地下载并在 Windows 上使用。</li><li id="2e51" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">一个优秀的 PyTorch 实现，由 tensor boy<a class="ae kz" href="https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation" rel="noopener ugc nofollow" target="_blank">实现，有详细的文档和简单的代码。</a></li><li id="82fb" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><a class="ae kz" href="https://github.com/ildoonet/tf-pose-estimation" rel="noopener ugc nofollow" target="_blank"> ildoonet </a>提供了具有多个基础网络的 Tensorflow 实现。</li></ul><h1 id="d7b6" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">参考</h1><ul class=""><li id="0ae9" class="kl km iq jp b jq ly ju lz jy ma kc mb kg mc kk kq kr ks kt bi translated"><a class="ae kz" href="https://arxiv.org/abs/1611.08050" rel="noopener ugc nofollow" target="_blank">【1】曹哲，托马斯·西蒙，施-韦恩，亚塞尔·谢赫，利用部分亲和场的实时多人 2D 姿态估计(2017)，CVPR 2017 口述</a></li><li id="6186" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><a class="ae kz" href="https://arxiv.org/abs/1611.08050" rel="noopener ugc nofollow" target="_blank">【2】曹哲，托马斯·西蒙，施-韦恩，亚塞尔·谢赫，OpenPose:使用部分亲和场的实时多人 2D 姿态估计(2018)，a </a> rXiv</li><li id="df31" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">[3] <a class="ae kz" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> OpenPose:用于身体、面部、手和脚估计的实时多人关键点检测库</a></li><li id="3714" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">[4] <a class="ae kz" href="https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation" rel="noopener ugc nofollow" target="_blank"> tensorboy 的实现</a></li><li id="6e8c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">[5] <a class="ae kz" href="https://github.com/ildoonet/tf-pose-estimation" rel="noopener ugc nofollow" target="_blank"> ildoonet 的实现</a></li></ul><h1 id="eacf" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">我的评论</h1><p id="8236" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">图像分类:<a class="ae kz" href="https://medium.com/p/alexnet-review-and-implementation-e37a8e4dab54" rel="noopener">【NIPS 2012】AlexNet</a><br/>图像分割:<a class="ae kz" rel="noopener" target="_blank" href="/cvpr-2019-pose2seg-detection-free-human-instance-segmentation-61e4948ba6db">【CVPR 2019】Pose 2 seg</a>T5】姿态估计:<a class="ae kz" rel="noopener" target="_blank" href="/cvpr-2017-openpose-realtime-multi-person-2d-pose-estimation-using-part-affinity-fields-f2ce18d720e8">【CVPR 2017】open Pose</a><br/>姿态跟踪:<a class="ae kz" rel="noopener" target="_blank" href="/cvpr-2019-efficient-online-multi-person-2d-pose-tracking-with-recurrent-spatio-temporal-affinity-25c4914e5f6">【CVPR 2019】STAF</a></p></div></div>    
</body>
</html>