<html>
<head>
<title>How to find Feature importances for BlackBox Models?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何找到黑盒模型的特征重要性？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-find-feature-importances-for-blackbox-models-c418b694659d?source=collection_archive---------17-----------------------#2019-10-04">https://towardsdatascience.com/how-to-find-feature-importances-for-blackbox-models-c418b694659d?source=collection_archive---------17-----------------------#2019-10-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bea0fad9bcade5d0366a1c7008091298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ox8Cxbdvyhcxd7VGL49ERw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Source: <a class="ae jg" href="https://pixabay.com/users/DavidRockDesign-2595351/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1430105" rel="noopener ugc nofollow" target="_blank">DavidRockDesign</a>, Randomness-the lifeblood of many algorithms</figcaption></figure><h2 id="6107" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">DS 算法</h2><div class=""/><div class=""><h2 id="e98f" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">置换重要性作为一种特征选择方法</h2></div><p id="f26f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">数据科学是对算法的研究。</p><p id="8bc1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我每天都在努力学习许多算法，所以我想列出一些最常见和最常用的算法，这些算法将在这个新的<a class="ae jg" href="https://towardsdatascience.com/tagged/ds-algorithms" rel="noopener" target="_blank"> DS 算法系列</a>中使用。</p><p id="cb5d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有多少次，当你创建了很多功能，然后你需要想办法减少功能的数量？</p><p id="e59f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上次我写了一篇名为“<a class="ae jg" rel="noopener" target="_blank" href="/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2">每个数据科学家都应该知道的 5 种特征选择算法</a>”的文章，其中我谈到了使用相关性或基于树的方法，并在特征选择过程中添加一些结构。</p><p id="b431" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最近，有人向我介绍了另一种新颖的特征选择方式，叫做<strong class="lj jt"> <em class="md">排列重要性</em> </strong>，我非常喜欢这种方式。</p><p id="fc3c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，这篇文章解释了排列重要性是如何工作的，以及我们如何用 ELI5 来编码它。T11】</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="b38b" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">什么是排列重要性？</h1><p id="f29f" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">简单地说，我们可以根据我们的评估指标(F1、准确性 AUC 等)来确定某个特征的重要性。)在我们从数据集中移除特定要素时会发生变化。</p><p id="c023" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这可能非常简单—我们从数据集中移除一个要素，训练分类器，然后查看评估指标如何变化。我们对所有功能都这样做。</p><p id="384a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，我们至少拟合我们的模型 n 次，其中 n 是模型中特征的数量。这是如此多的工作和计算。<strong class="lj jt"> <em class="md">我们能做得更好吗？</em>T15】</strong></p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/bd4240caa3bd086543173c805f1b2f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9krghn2aVDz9Y0wu.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://www.kaggle.com/dansbecker/permutation-importance" rel="noopener ugc nofollow" target="_blank">Source</a>: We permute a feature and predict using the updated dataset. Intuitively, if our accuracy or any evaluation metric doesn’t take a hit, we can say that the feature is not important. If our accuracy does take a hit, we consider this feature important.</figcaption></figure><p id="0545" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">是的，我们可以。为了计算排列重要性，我们对单个要素的值进行混洗/排列，并使用生成的数据集进行预测。</p><p id="0c98" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，这些预测用于计算我们的评估指标。直观地说，如果我们的准确性或任何评估指标没有受到影响，我们可以说这个特性不重要。如果我们的准确性受到影响，我们认为这个特性很重要。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="ffd5" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">数据</h1><p id="7e0c" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">我们将尝试使用数据集来更好地理解它。</p><p id="ffdb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我将使用一个足球运动员数据集，并尝试使用它找出最重要的特征。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/109c38d754329c454a202f269943bfbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1qEzkD3R90N_sZFc.png"/></div></div></figure><p id="826a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你不懂足球术语，也不用担心。我会尽量把它保持在最低限度。</p><p id="ce8b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你可以在这个<a class="ae jg" href="https://www.kaggle.com/mlwhiz/permutation-feature-selection-using-football-data" rel="noopener ugc nofollow" target="_blank"> Kaggle 内核</a>中看到完整的代码。</p><h2 id="ce7f" class="no mm jj bd mn np nq dn mr nr ns dp mv lq nt nu mx lu nv nw mz ly nx ny nb jp bi translated">一些简单的数据预处理</h2><p id="f849" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">我们已经做了一些基本的预处理，如删除空值和一个热编码。我们还使用以下公式将问题转化为分类问题:</p><pre class="nj nk nl nm gt nz oa ob oc aw od bi"><span id="3d5f" class="no mm jj oa b gy oe of l og oh">y = traindf['Overall']&gt;=80</span></pre><p id="9176" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这里，我们用高总体来代表一个伟大的球员。我们的数据集(X)如下所示，有 223 列。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/b519751d546716cc4492b2dc914c8a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gA-rgdP_HIqvnY7s.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">train Data X</figcaption></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="d499" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">履行</h1><h2 id="0e17" class="no mm jj bd mn np nq dn mr nr ns dp mv lq nt nu mx lu nv nw mz ly nx ny nb jp bi translated">1.对于 sklearn 型号</h2><p id="c279" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">ELI5 库让我们很容易对<code class="fe oj ok ol oa b">sklearn</code>模型使用排列重要性。首先，我们训练我们的模型。</p><pre class="nj nk nl nm gt nz oa ob oc aw od bi"><span id="d523" class="no mm jj oa b gy oe of l og oh">from sklearn.ensemble import RandomForestClassifier<br/>my_model = RandomForestClassifier(n_estimators=100,<br/>                                  random_state=0).fit(X, y)</span></pre><p id="d07f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后我们使用来自<code class="fe oj ok ol oa b">eli5.sklearn</code>模块的函数<code class="fe oj ok ol oa b">PermutationImportance</code>。</p><pre class="nj nk nl nm gt nz oa ob oc aw od bi"><span id="b81b" class="no mm jj oa b gy oe of l og oh">from eli5.sklearn import PermutationImportance<br/>import eli5<br/>perm = PermutationImportance(my_model,n_iter=2).fit(X, y)<br/>eli5.show_weights(perm, feature_names = X.columns.tolist())</span></pre><p id="363a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">结果看起来像:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/1438ad54857c1295952215f7a3307fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*yBsyX6ioYf2kUrIQ-HDqGg.png"/></div></figure><p id="6960" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里我们注意到反应、拦截和控球是衡量一名球员水平的最重要的特征。</p><h2 id="3318" class="no mm jj bd mn np nq dn mr nr ns dp mv lq nt nu mx lu nv nw mz ly nx ny nb jp bi translated">2.对于黑盒模型或非 sklearn 模型</h2><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/3483a4580e754dcb4b48b7a2c96eca14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-5BpqwBny7KMBfAo.jpg"/></div></div></figure><p id="5531" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们还可以使用 eli5 来计算非 scikit-learn 模型的特征重要性。这里我们训练一个 LightGBM 模型。</p><pre class="nj nk nl nm gt nz oa ob oc aw od bi"><span id="d417" class="no mm jj oa b gy oe of l og oh">import numpy as np</span><span id="65ab" class="no mm jj oa b gy oo of l og oh">from lightgbm import LGBMClassifier</span><span id="ea6e" class="no mm jj oa b gy oo of l og oh">lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,<br/>            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)</span><span id="66fe" class="no mm jj oa b gy oo of l og oh">lgbc.fit(X,y)</span></pre><p id="81ef" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们需要为分数函数创建一个包装器来计算我们的评估指标。</p><pre class="nj nk nl nm gt nz oa ob oc aw od bi"><span id="b340" class="no mm jj oa b gy oe of l og oh">from sklearn.metrics import accuracy_score</span><span id="499e" class="no mm jj oa b gy oo of l og oh">#define a score function. In this case I use accuracy<br/>def score(X, y):<br/>    y_pred = lgbc.predict(X)<br/>    return accuracy_score(y, y_pred)</span></pre><p id="8bad" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们现在可以使用来自<code class="fe oj ok ol oa b">eli5.permutation_importance</code>的<code class="fe oj ok ol oa b">get_score_importances</code>函数来获得最终的特征重要性。</p><pre class="nj nk nl nm gt nz oa ob oc aw od bi"><span id="d67f" class="no mm jj oa b gy oe of l og oh">from eli5.permutation_importance import get_score_importances</span><span id="4d42" class="no mm jj oa b gy oo of l og oh"># This function takes only numpy arrays as inputs<br/>base_score, score_decreases = get_score_importances(score, np.array(X), y)</span><span id="b2e7" class="no mm jj oa b gy oo of l og oh">feature_importances = np.mean(score_decreases, axis=0)</span></pre><p id="655b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以使用以下工具来查看前 5 大功能:</p><pre class="nj nk nl nm gt nz oa ob oc aw od bi"><span id="a7e3" class="no mm jj oa b gy oe of l og oh">feature_importance_dict = {}<br/>for i, feature_name in enumerate(X.columns):<br/>    feature_importance_dict[feature_name]=feature_importances[i]</span><span id="f27e" class="no mm jj oa b gy oo of l og oh">print(dict(sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)[:5]))</span><span id="3a30" class="no mm jj oa b gy oo of l og oh">---------------------------------------------------------------<br/>{'Reactions': 0.019626631422435127,<br/> 'Interceptions': 0.004075114268406832,<br/> 'BallControl': 0.0025001376727793235,<br/> 'ShortPassing': 0.0012996310369513431,<br/> 'Strength': 0.0009251610771518149}</span></pre></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="5bc5" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">结论</h1><p id="5867" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/the-hitchhikers-guide-to-feature-extraction-b4c157e96631">特征工程</a>和特征选择是任何机器学习管道的关键部分。</p><p id="c9a8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们力求模型的准确性，如果不一次又一次地重温这些作品，就不可能达到良好的准确性。</p><p id="faa4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这篇文章中，我试图解释排列作为一种特征选择方法的重要性。它帮助我们找到任何黑盒模型的特征重要性，不像我之前的<a class="ae jg" rel="noopener" target="_blank" href="/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2">文章</a>中关于特征选择的技术。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="c726" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你想了解更多关于特征工程/选择的知识，我想在 Kazanova 的<a class="ae jg" href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="noopener ugc nofollow" target="_blank">高级机器学习专业化</a>课程中喊出<a class="ae jg" href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="noopener ugc nofollow" target="_blank">如何赢得数据科学竞赛:向顶级 Kagglers </a>学习。本课程讲述了许多使用有用的特征工程/选择技术来改进模型的直观方法。绝对推荐。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="9f5d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" rel="noopener"><strong class="lj jt"/></a>关注我或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter <a class="ae jg" href="https://twitter.com/MLWhiz?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> @mlwhiz </a>联系。</p></div></div>    
</body>
</html>