<html>
<head>
<title>How to make your own deep learning accelerator chip!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何自制深度学习加速器芯片！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-make-your-own-deep-learning-accelerator-chip-1ff69b78ece4?source=collection_archive---------1-----------------------#2019-09-18">https://towardsdatascience.com/how-to-make-your-own-deep-learning-accelerator-chip-1ff69b78ece4?source=collection_archive---------1-----------------------#2019-09-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="c756" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">目前，全球有超过 100 家公司正在构建面向深度学习应用的 ASICs(专用集成电路)或 SOC(片上系统)。这里有一长串的公司。除了谷歌(TPU)、脸书、亚马逊(<a class="ae ko" href="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener ugc nofollow" target="_blank">推理</a>)、<a class="ae ko" href="https://techcrunch.com/2019/04/22/tesla-vaunts-creation-of-the-best-chip-in-the-world-for-self-driving/" rel="noopener ugc nofollow" target="_blank">特斯拉</a>等这些创业大公司都在开发定制的 ASIC，用于深度学习训练和推理。这些可以分为两种类型—</p><ol class=""><li id="b252" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">训练和推理——这些 ASIC 设计用于处理深度神经网络的训练和执行推理。训练像 Resnet-50 这样的大型神经网络是一项涉及梯度下降和反向传播的更加计算密集型的任务。与训练相比，推理非常简单，需要较少的计算。今天最流行的深度学习的 NVidia GPU 既可以做训练，也可以做推理。其他一些例子还有<a class="ae ko" href="https://www.graphcore.ai/technology" rel="noopener ugc nofollow" target="_blank"> Graphcore IPU </a>、<a class="ae ko" href="https://cloud.google.com/tpu/" rel="noopener ugc nofollow" target="_blank">谷歌 TPU V3 </a>、<a class="ae ko" href="https://www.cerebras.net/wafer-scale-deep-learning-hot-chips-2019-presentation/" rel="noopener ugc nofollow" target="_blank">脑波强化器</a>等。OpenAI 有很棒的<a class="ae ko" href="https://openai.com/blog/ai-and-compute/" rel="noopener ugc nofollow" target="_blank">分析</a>显示最近训练大型网络所需的计算增加。</li><li id="1045" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">推论——这些 ASIC 被设计为运行 DNN(深度神经网络),这些网络已经在 GPU 或其他 ASIC 上进行过训练，然后经过训练的网络被修改(量化、修剪等)以在不同的 ASIC 上运行(如谷歌珊瑚边缘 TPU、英伟达杰特森纳米)。大多数人都说深度学习推理的<a class="ae ko" href="https://www.mckinsey.com/~/media/McKinsey/Industries/Semiconductors/Our%20Insights/Artificial%20intelligence%20hardware%20New%20opportunities%20for%20semiconductor%20companies/Artificial-intelligence-hardware.ashx" rel="noopener ugc nofollow" target="_blank">市场</a>比训练大得多。如<a class="ae ko" href="https://www.tensorflow.org/lite" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite </a>团队所示，即使是基于 ARM Cortex 的非常小的微控制器(MCU)——M0、M3、M4 等也可以进行推理。</li></ol><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/bb1e1e5caa86cd57f57aaeec8abbb972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y6fXBUTKqlDcoGxOPpHz_A.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">AI Landscape by Shan Tang : <a class="ae ko" href="https://github.com/basicmi/AI-Chip" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="47d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">制造任何芯片(ASIC、SOC 等)都是一个昂贵、困难和漫长的过程，通常由 10 到 1000 人的团队完成，具体取决于芯片的大小和复杂程度。这里我只提供一个针对深度学习<strong class="js iu"> <em class="ma">推理</em> </strong>加速器的简要概述。如果你已经设计了芯片，你会发现这太简单了。如果你仍然感兴趣，请继续阅读！如果你喜欢它，分享和👏。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="0e38" class="mb mc it bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated"><strong class="ak">现有 ASIC 的架构</strong></h1><p id="fc89" class="pw-post-body-paragraph jq jr it js b jt mz jv jw jx na jz ka kb nb kd ke kf nc kh ki kj nd kl km kn im bi translated">让我们首先来看看目前正在开发的一些加速器的高层架构。</p><p id="ade8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://habana.ai/inference/" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">Habana Goya</strong></a>—<a class="ae ko" href="https://habana.ai/" rel="noopener ugc nofollow" target="_blank">Habana labs</a>是一家初创公司，正在开发用于训练的独立芯片——高迪和推理——Goya。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ne"><img src="../Images/b2a3e7072121e3662313d71659088e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ww2YM3cy-I53xLe-S4X1KA.jpeg"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Habana Goya High-Level Architecture: <a class="ae ko" href="https://www.electronicdesign.com/industrial-automation/habana-enters-machine-learning-derby-goya-platform" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="67e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GEMM 引擎——通用矩阵和乘法引擎。矩阵乘法是所有 DNN 中的核心运算——卷积可以表示为矩阵乘法，全连接层是直接的矩阵乘法。</p><p id="c046" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">TPC——张量处理核心——这是一个实际执行乘法或乘加(MAC)运算的模块。</p><p id="36bb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本地内存和共享内存—这些都是某种形式的高速缓存，通常使用<a class="ae ko" href="https://en.wikipedia.org/wiki/Static_random-access_memory" rel="noopener ugc nofollow" target="_blank"> SRAM </a>(静态随机存取存储器)和<a class="ae ko" href="https://en.wikipedia.org/wiki/Register_file" rel="noopener ugc nofollow" target="_blank">寄存器文件</a>(也是一种静态易失性存储器，只是密度比 SRAM 小)。</p><p id="d78c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="http://eyeriss.mit.edu/" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">Eyeriss</strong></a><strong class="js iu"/>——来自麻省理工学院的 eye riss 团队一直在研究深度学习推理加速器，并发表了几篇关于他们的两个芯片的论文，即 Eyeriss V1 和<a class="ae ko" href="http://www.rle.mit.edu/eems/wp-content/uploads/2019/04/2019_jetcas_eyerissv2.pdf" rel="noopener ugc nofollow" target="_blank"> V2 </a>。你可以在这里找到好的教程<a class="ae ko" href="http://eyeriss.mit.edu/tutorial.html" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/c4be4270ce741bf4dd867b34f10d7770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*B0cbZ090-385BMCYB-eUuQ.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Eyeriss V2 top-level architecture: <a class="ae ko" href="https://www.semanticscholar.org/paper/Eyeriss-v2%3A-A-Flexible-Accelerator-for-Emerging-on-Chen-Yang/0682bfa5cca15726aab6c00ecfac91eb44379626" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="98b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="http://nvdla.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">英伟达深度学习加速器(NVDLA) </strong> </a></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/74f0313af6355265d993a0069a13619a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*e8b7fNeYtbbz0s3A1v1OCA.jpeg"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">NVDLA : <a class="ae ko" href="http://nvdla.org/primer.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="dd22" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://en.wikipedia.org/wiki/Dataflow_architecture" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">数据流架构</strong> </a> <strong class="js iu"> </strong> —数据流架构至少从 20 世纪 70 年代就开始研究了。<a class="ae ko" href="https://wavecomp.ai/" rel="noopener ugc nofollow" target="_blank"> Wave Computing </a>想出了<a class="ae ko" href="https://www.hotchips.org/wp-content/uploads/hc_archives/hc29/HC29.22-Tuesday-Pub/HC29.22.60-NeuralNet1-Pub/HC29.22.610-Dataflow-Deep-Nicol-Wave-07012017.pdf" rel="noopener ugc nofollow" target="_blank">数据流处理单元</a> (DPU)来加速 DNN 的训练。<a class="ae ko" href="https://www.hailo.ai/" rel="noopener ugc nofollow" target="_blank"> Hailo </a>也使用了某种形式的<a class="ae ko" href="https://www.slideshare.net/embeddedvision/emerging-processor-architectures-for-deep-learning-options-and-tradeoffs-a-presentation-from-hailo" rel="noopener ugc nofollow" target="_blank">数据流架构</a>。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/491564a3e082508d39e4bc1d56dcf573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*7Njs6sntpB_xkKuMWFTEYw.jpeg"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Hailo — Embedded Vision Summit — <a class="ae ko" href="https://www.slideshare.net/embeddedvision/emerging-processor-architectures-for-deep-learning-options-and-tradeoffs-a-presentation-from-hailo" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="6365" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://www.gyrfalcontech.ai/" rel="noopener ugc nofollow" target="_blank"> Gyrfalcon </a> —他们已经发布了一些针对低功耗边缘人工智能应用的芯片，如<a class="ae ko" href="https://www.gyrfalcontech.ai/solutions/2801s/" rel="noopener ugc nofollow" target="_blank"> Lightspeeur 2801S </a>。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ni"><img src="../Images/303940d10071c60f67bb29933ec5894e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RY0P6wAZWhmVcsfLLzIPeg.jpeg"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Matrix Processing Engine (MPE) — <a class="ae ko" href="https://www.gyrfalcontech.ai/about-us/company-overview/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="c31f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">谷歌 TPU </strong> </a>也有脉动数据流引擎。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nj"><img src="../Images/4a0a24a20f169feb322b81cd06ba47ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l4AtQE8zmxX_f85DoddoDA.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Matrix Multiplier on TPU —<a class="ae ko" href="https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu" rel="noopener ugc nofollow" target="_blank"> Source</a></figcaption></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/6e4e00eec9ac86033fdb1945302502b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*yAl85NvKgwKEqhU5MhexIQ.jpeg"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">TPU Floor plan — <a class="ae ko" href="https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="c2b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">统一缓冲区—这基本上是本地内存/缓存，可能使用 SRAM 实现。</p><p id="2e24" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">DRAM —这些是访问外部 DRAM 的接口，使用其中两个接口，您可以访问两倍的数据。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="843c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">关键模块</strong></p><p id="d66d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">基于上面的一些例子，我们可以说下面是制造深度学习推理加速器所需的关键组件。此外，我们将只关注<a class="ae ko" href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener ugc nofollow" target="_blank"> 8 位推理</a>引擎，该引擎已被证明对许多应用足够好。</p><p id="6da9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">矩阵乘法单元——它有不同的名称，如 TPC(张量处理核心)、PE 等。GEMM 参与了 DNN 的核心计算，要了解更多关于 GEMM 阅读这篇<a class="ae ko" href="https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/" rel="noopener ugc nofollow" target="_blank">伟大的帖子</a>。</p><p id="e246" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">SRAM —这是用于存储权重或中间输出/激活的本地存储器。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/c0cdcdf3fec4ff9a3e0a6506d2d18de9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*vrwxsVxqF6iZi25_zZipZA.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Data movement Energy Vs Compute — Source —<a class="ae ko" href="https://arxiv.org/abs/1703.09039" rel="noopener ugc nofollow" target="_blank"> Efficient Processing of Deep Neural Networks: A Tutorial and Survey</a></figcaption></figure><p id="5666" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了减少能量消耗，存储器应该尽可能靠近处理单元，并且应该尽可能少地被访问。</p><p id="21ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">互连/结构—这是连接所有不同处理单元和内存的逻辑，以便一个层或模块的输出可以传输到下一个模块。也称为片上网络(NoC)。</p><p id="89ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接口(DDR、PCIE) —需要这些模块来连接外部内存(DRAM)和外部处理器。</p><p id="67d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">控制器——这可以是 RISC-V 或 ARM 处理器或定制逻辑，用于控制所有其他模块和外部处理器并与之通信。</p><p id="7964" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">架构和指令集</strong></p><p id="20ee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们观察所有架构，我们会发现内存总是尽可能靠近计算。原因是移动数据比计算消耗更多的能量。让我们来看看<a class="ae ko" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet 架构</a>所涉及的计算和内存，它在 2012 年打破了 ImageNet 记录——</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/5af1b7b249252328ed36d99785a49769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*ZV4poPG977D20rQN3DOHLA.jpeg"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">AlexNet Layers and Parameter —<a class="ae ko" href="https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/" rel="noopener ugc nofollow" target="_blank"> Source</a></figcaption></figure><p id="8c7a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">AlexNet 由 5 个构成层和 3 个全连接层组成。AlexNet 的参数/权重总数约为 6200 万。假设在<a class="ae ko" href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener ugc nofollow" target="_blank">权重量化</a>之后，每个权重被存储为 8 位值，因此如果我们想要将所有权重保存在片内存储器中，则至少需要 62 MB 的 SRAM 或 62*8 兆位= 4.96 亿 SRAM 单元。如果我们使用 6T(六晶体管)SRAM 单元，仅存储器就需要 496M * 6 ~ 2.9 亿个晶体管。因此，在决定架构时，我们必须记住在不增加片外重量(这会增加功耗)的情况下，我们可以支持哪些 DNN 架构。由于这个原因，许多创业公司展示了使用更新的架构，如<a class="ae ko" href="https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html" rel="noopener ugc nofollow" target="_blank"> MobileNetV2 </a>，它使用更少的参数和更少的计算，例如，ImageNet 上前 5 名准确率为 92.5%的 MobileNetV2 的一个<a class="ae ko" href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" rel="noopener ugc nofollow" target="_blank">检查点</a>只有 6.06M 个参数，在单个图像推断期间执行 582M 次 MAC(乘和累加)操作。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nn"><img src="../Images/32001cfa8d79abb17ee79459b58ccc5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f57O6E5hQ61JmSJIemGZzg.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Accuracy Vs Model Size — <a class="ae ko" href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="b3da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://www.tensorflow.org/model_optimization/guide/pruning" rel="noopener ugc nofollow" target="_blank">权重修剪</a>是另一种可以用来减少模型大小(从而减少内存占用)的技术。参见<a class="ae ko" href="https://community.cadence.com/cadence_blogs_8/b/breakfast-bytes/posts/ai-processing" rel="noopener ugc nofollow" target="_blank">模型压缩</a>的结果。</p><p id="1ee2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">MobileNetV2 使用不同于传统卷积的深度方向可分离卷积，因此加速器架构必须足够灵活，以便如果研究人员提出不同的操作，它们仍然可以根据加速器上可用的指令集来表示。</p><p id="7488" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以为我们简单的加速器想出一套非常简单的指令，就像—</p><ol class=""><li id="021a" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">加载数据—获取源地址和目标地址</li><li id="cfcd" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">MAC(乘加)—假设数据已经在本地寄存器中。</li><li id="765b" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">存储结果—存储中间结果</li><li id="07fb" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">填充—添加零</li></ol></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="aba1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">硬件加速器编译器</strong></p><p id="2267" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">编译器将使用 PyTorch 或 Tensorflow 用 python 编写的高级代码转换为特定芯片的指令集。下面是开发/使用这些定制 ASIC 的一些框架。这个过程可能非常困难和复杂，因为不同的 ASIC 支持不同的指令集，如果编译器没有生成优化的代码，那么您可能没有充分利用 ASIC 的功能。</p><p id="ba4a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://github.com/pytorch/glow" rel="noopener ugc nofollow" target="_blank">脸书 Glow </a> —哈瓦那实验室<a class="ae ko" href="https://engineering.fb.com/open-source/glow-habana/" rel="noopener ugc nofollow" target="_blank">利用 Glow 框架为他们的 ASIC 开发了</a>后端。</p><p id="cfe8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://tvm.ai/" rel="noopener ugc nofollow" target="_blank"> TVM </a> —这是一个开源的深度学习编译器堆栈，由华盛顿大学的研究人员发起。TVM 框架还包括<a class="ae ko" href="https://tvm.ai/vta" rel="noopener ugc nofollow" target="_blank">多功能张量加速器</a> (VTA)，这是一个可编程的独立加速器。<a class="ae ko" href="https://aws.amazon.com/sagemaker/neo/" rel="noopener ugc nofollow" target="_blank">亚马逊 Sagemaker Neo </a>使用 TVM 编译深度学习模型，部署在不同的硬件上。</p><p id="0144" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://github.com/tensorflow/mlir" rel="noopener ugc nofollow" target="_blank">TensorFlow MLIR</a>—<a class="ae ko" href="https://medium.com/tensorflow/mlir-a-new-intermediate-representation-and-compiler-framework-beba999ed18d" rel="noopener">MLIR</a>是 Google 为 tensor flow 提供的编译器基础设施，最近已经成为<a class="ae ko" href="https://llvm.org/" rel="noopener ugc nofollow" target="_blank"> LLVM </a>项目的<a class="ae ko" href="https://www.blog.google/technology/ai/mlir-accelerating-ai-open-source-infrastructure/" rel="noopener ugc nofollow" target="_blank">部分。</a></p><p id="e0a0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://github.com/NervanaSystems/ngraph" rel="noopener ugc nofollow" target="_blank">英特尔 ngraph </a> —这是由 Nervana 开发的，用于 nerv ana/英特尔深度学习加速器。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="aca0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> EDA 工具和高级综合</strong></p><p id="faad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Chisel 是一种硬件构造/描述语言，最初由伯克利的研究人员开发。它实际上是用 Scala 编写的，用于许多基于 RISC-V 的处理器的设计。</p><p id="21c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">综合、时序和布局——RTL 综合是将 Verilog/VHDL 等语言编写的高级代码转换成逻辑门的过程。时序工具使用逻辑门和布线的布局前和布局后延迟信息来确保设计正确。在时序设计中，一切都与时钟沿有关，因此时序非常重要。布局工具从合成的网表生成布局。<a class="ae ko" href="https://www.synopsys.com/implementation-and-signoff/rtl-synthesis-test.html" rel="noopener ugc nofollow" target="_blank"> Synopsys </a>(设计编译器，黄金时间)和 Cadence 工具最常用于这些步骤。</p><p id="0380" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://en.wikipedia.org/wiki/High-level_synthesis" rel="noopener ugc nofollow" target="_blank">高级综合</a>(HLS)——HLS 是指用 C/C++等高级语言描述硬件，然后转换成 VHDL/Verilog 等 RTL(寄存器传输级)语言的过程。甚至还有一个 python 包<a class="ae ko" href="http://www.myhdl.org/" rel="noopener ugc nofollow" target="_blank">http://www.myhdl.org/</a>——将 python 代码转换成 Verilog 或 VHDL。<a class="ae ko" href="https://www.cadence.com/content/cadence-www/global/en_US/home/tools/digital-design-and-signoff/synthesis/stratus-high-level-synthesis.html" rel="noopener ugc nofollow" target="_blank"> Cadence </a>拥有支持 C/C++等的商业工具，这些工具对定制设计非常有帮助。Google 使用 Mentor Graphics Catapult HLS 工具开发了<a class="ae ko" href="https://www.mentor.com/hls-lp/success/google-inc" rel="noopener ugc nofollow" target="_blank"> WebM 解压 IP </a>。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="4be6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">可用 IP </strong></p><p id="ac4b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们已经确定了所需的关键模块，让我们看看我们使用什么现有的 IP(免费或付费)。</p><p id="3bc1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="http://nvdla.org/" rel="noopener ugc nofollow" target="_blank"> Nvidia 深度学习加速器(NVDLA) </a> — NVDLA 是 Nvidia 发布的免费开放架构，用于深度学习推理加速器的设计。源代码、驱动程序、文档等可在<a class="ae ko" href="https://github.com/nvdla/" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p><p id="5f4f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">SRAM——不同类型的 SRAM IP——单端口、双端口、低功耗、高速等，可从<a class="ae ko" href="https://www.synopsys.com/dw/ipdir.php?ds=dwc_sram_memory_compilers" rel="noopener ugc nofollow" target="_blank"> Synopsys </a>和其他公司获得。通常，它们提供 SRAM 编译器，用于根据芯片要求生成特定的 SRAM 模块。</p><p id="6105" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">寄存器文件—该 IP 也可从<a class="ae ko" href="https://www.synopsys.com/dw/ipdir.php?ds=dwc_sram_memory_compilers" rel="noopener ugc nofollow" target="_blank"> Synopsys </a>和各种类型的逻辑<a class="ae ko" href="https://www.synopsys.com/dw/ipdir.php?ds=dwc_standard_cell" rel="noopener ugc nofollow" target="_blank">标准单元</a>获得。</p><p id="33b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">互连/结构/NoC——这个 IP 的一个选项是<a class="ae ko" href="http://www.arteris.com/" rel="noopener ugc nofollow" target="_blank"> Arteris </a>，他们有针对深度学习加速器的<a class="ae ko" href="http://www.arteris.com/flexnoc-ai-package" rel="noopener ugc nofollow" target="_blank"> FlexNoC AI 包</a>。</p><p id="517b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">处理器——各种<a class="ae ko" href="https://riscv.org/" rel="noopener ugc nofollow" target="_blank"> RISC-V </a>处理器内核可以免费获得。甚至 ARM 也免费或以非常低的前期成本向初创公司提供<a class="ae ko" href="https://developer.arm.com/ip-products/designstart" rel="noopener ugc nofollow" target="_blank">许可</a>。<a class="ae ko" href="https://developer.arm.com/ip-products/processors/machine-learning/arm-ethos-n/ethos-n77" rel="noopener ugc nofollow" target="_blank">ARM Ethos</a>npu 是专门为神经网络设计的——Ethos N37、N57、N77。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f9cfba9205f6aea944ed10aa5ad56c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*Kv0Jr5UMvCzt_Ko90Hx8Og.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">AMR Ethos NPU — <a class="ae ko" href="https://developer.arm.com/ip-products/processors/machine-learning/arm-ethos-n/ethos-n77" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="4a80" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://ip.cadence.com/ai" rel="noopener ugc nofollow" target="_blank">Cadence Tensilica DNA 100</a>—根据我们的目标应用/行业，Cadence 提供的 IP 可配置为 0.5 至 100 的 TMAC 操作。</p><p id="bd22" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有很多其他可用的知识产权，所以我的建议是在设计自己的知识产权之前，从像<a class="ae ko" href="https://www.arm.com/" rel="noopener ugc nofollow" target="_blank"> ARM </a>、<a class="ae ko" href="https://www.ceva-dsp.com/product/ceva-deep-neural-network-cdnn/" rel="noopener ugc nofollow" target="_blank"> Ceva </a>、<a class="ae ko" href="https://www.nxp.com/products/product-information/ip-block-licensing/starcore-dsp:STARCORE-DSP" rel="noopener ugc nofollow" target="_blank">恩智浦</a>等公司寻找已经测试过的知识产权。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="7f40" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">设计流程</strong></p><p id="6047" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有很多关于 ASIC 设计流程、数字设计过程等的<a class="ae ko" href="https://en.wikipedia.org/wiki/Physical_design_(electronics)" rel="noopener ugc nofollow" target="_blank">资源</a>(书籍、讲座等)，所以我就不多讲了。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi np"><img src="../Images/19be0da0ce8f9816d93e1e61739e70c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*gcFCfBIy2i56ka9Ogb2CKg.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">ASIC Flow from Wikipedia — <a class="ae ko" href="https://en.wikipedia.org/wiki/Physical_design_(electronics)" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="da63" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">代工厂和工艺技术</strong></p><p id="2f48" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">芯片的制造是在大型晶圆厂(制造厂或代工厂)完成的，目前，很少有公司像英特尔、三星、德州仪器、恩智浦等拥有自己的晶圆厂。甚至像高通、AMD 等大公司也使用外部代工厂，所有这样的公司都被称为无晶圆厂。以下是一些最大的半导体代工厂</p><p id="83b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">TSMC(台积电) — TSMC 是世界上最大的代工厂，为高通、苹果等公司制造芯片。对于小型创业公司来说，在 TSMC 进行生产可能具有挑战性，因为他们的大部分生产能力都被大公司利用了。</p><p id="534c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="http://www.umc.com/English/about/index.asp" rel="noopener ugc nofollow" target="_blank"> UMC(联合微电子公司)</a>——UMC 也与包括小型创业公司在内的大量客户合作。目前，UMC 可用的最小工艺是 14 纳米。</p><p id="20e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">还有其他几家代工厂，如<a class="ae ko" href="https://www.globalfoundries.com/" rel="noopener ugc nofollow" target="_blank">全球代工厂</a>、<a class="ae ko" href="https://www.samsungfoundry.com/foundry/identity/anonymous/ssoLogin.do" rel="noopener ugc nofollow" target="_blank">三星代工厂</a>等</p><p id="0b86" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">流程选择</strong></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/abf0093e8ea4886c146b948306fc5bec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*bOQf7QcKkzL4b3Ql.PNG"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><a class="ae ko" href="https://en.wikipedia.org/wiki/CMOS" rel="noopener ugc nofollow" target="_blank">Cross-section of two transistors in a CMOS gate, in an N-well CMOS process</a></figcaption></figure><p id="7a35" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">IC 制造工艺是通过晶体管的尺寸和金属连接的宽度来衡量的。长期以来，工艺尺寸一直在下降(<a class="ae ko" href="https://en.wikipedia.org/wiki/Moore%27s_law" rel="noopener ugc nofollow" target="_blank">摩尔定律</a>)，这就是现代 IC 每年包含越来越多的晶体管(这曾经是由<a class="ae ko" href="https://en.wikipedia.org/wiki/Moore%27s_law" rel="noopener ugc nofollow" target="_blank">摩尔定律</a>决定的)。目前，最先进的工艺节点是 7 纳米，使用 7 纳米工艺的产品仅在 2019 年推出。所以目前大部分产品都是使用 14 纳米/16 纳米工艺制作的芯片。工艺越先进，成本就越高，因此大多数小型创业公司最初会使用稍微老一点的工艺来保持低成本。许多开发深度学习加速器的创业公司都在使用<a class="ae ko" href="http://www.umc.com/English/process/a.asp" rel="noopener ugc nofollow" target="_blank">28 纳米</a>处理器，在某些情况下，甚至是 40 纳米工艺。<a class="ae ko" href="http://courses.ece.ubc.ca/579/579.lect6.leakagepower.08.pdf" rel="noopener ugc nofollow" target="_blank">泄漏</a>是现代工艺中的一个大问题，如果芯片设计不当，可能会导致巨大的功耗。</p><p id="2440" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">简单成本估算</strong></p><p id="56f6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">晶圆成本取决于工艺节点和各种其他因素，如加工步骤的数量(使用的层数)。成本从相对较老的工艺的几千美元到最新的工艺节点的几千美元不等，这很大程度上取决于一个人要购买多少晶片等等。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0e252520a8694848629f0a7136ce01c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*G-4gGeg5WpE05hNGsjJGtw.jpeg"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">CMOS Wafer —<a class="ae ko" href="https://en.wikipedia.org/wiki/Wafer_(electronics)" rel="noopener ugc nofollow" target="_blank"> Source</a></figcaption></figure><p id="7405" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">大多数代工厂生产 300 毫米(约 12 英寸)直径的晶圆用于数字工艺。让我们简单计算一下 12 英寸晶片的芯片成本</p><p id="d3da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总面积~ π * r (r =晶圆半径)~ 70，650 mm</p><p id="99b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">晶圆总成本约 1000 美元(仅用作示例)</p><p id="6384" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">芯片面积约 10mm×10mm ~ 100mm(<a class="ae ko" href="https://en.wikipedia.org/wiki/Tensor_processing_unit" rel="noopener ugc nofollow" target="_blank">TPU V1 芯片尺寸约 331 mm </a>，<a class="ae ko" href="https://en.wikipedia.org/wiki/32_nanometer" rel="noopener ugc nofollow" target="_blank"> SRAM 单元面积约 32nm ~ 0.18um</a></p><p id="90b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每个晶片的芯片数约为 70，650 / 100 ~ 706(由于边缘缺陷等原因，实际上更少)</p><p id="a02d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">实际上好的模具有<a class="ae ko" href="https://en.wikichip.org/wiki/yield" rel="noopener ugc nofollow" target="_blank"> 95%的成品率</a> ~ 0.95 * 706 ~ 670</p><p id="9e75" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ma">单个模具成本约 1000 元/670 元~ 1.5 元</em> </strong></p><p id="68c0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">包装和测试也会增加最终成本。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="0c8e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个巨大的领域，这篇文章只是触及了其中一些话题的表面。还有很多其他的东西要涵盖，比如用于深度学习的<a class="ae ko" href="https://aws.amazon.com/ec2/instance-types/f1/" rel="noopener ugc nofollow" target="_blank">FPGA</a>，布局，测试，成品率，低功耗设计等等。如果人们喜欢这篇文章，我可能会再写一篇。</p><p id="3f1e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我热衷于构建生产机器学习系统来解决具有挑战性的现实世界问题。我正在积极寻找 ML/AI 工程师职位，你可以在这里联系我<a class="ae ko" href="https://www.linkedin.com/in/manusuryavansh/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="fa73" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">链接</strong></p><p id="24af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://cs217.stanford.edu/" rel="noopener ugc nofollow" target="_blank">斯坦福 CS 271 —机器学习的硬件加速器</a></p><p id="1aaf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">麻省理工学院的教程</p><p id="92cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://web.stanford.edu/group/mast/cgi-bin/drupal/content/plasticine-reconfigurable-architecture-parallel-patterns" rel="noopener ugc nofollow" target="_blank">橡皮泥</a>和<a class="ae ko" href="https://spatial-lang.org/" rel="noopener ugc nofollow" target="_blank">空间</a></p><div class="ns nt gp gr nu nv"><a href="https://medium.com/tensorflow/mlir-a-new-intermediate-representation-and-compiler-framework-beba999ed18d" rel="noopener follow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd iu gy z fp oa fr fs ob fu fw is bi translated">MLIR:一种新的中间表示和编译框架</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">发布者:TensorFlow MLIR 团队</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">medium.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj ln nv"/></div></div></a></div><div class="ns nt gp gr nu nv"><a rel="noopener follow" target="_blank" href="/google-coral-edge-tpu-board-vs-nvidia-jetson-nano-dev-board-hardware-comparison-31660a8bda88"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd iu gy z fp oa fr fs ob fu fw is bi translated">Google Coral Edge TPU 主板与 NVIDIA Jetson Nano 开发主板—硬件比较</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">NVidia 和 Google 最近都发布了针对 EdgeAI 的开发板，并且以低廉的价格吸引了…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">towardsdatascience.com</p></div></div><div class="oe l"><div class="ok l og oh oi oe oj ln nv"/></div></div></a></div></div></div>    
</body>
</html>