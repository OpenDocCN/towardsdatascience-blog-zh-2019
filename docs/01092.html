<html>
<head>
<title>Machine Learning Classification Project: Finding Donors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习分类项目:寻找捐赠者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classification-project-finding-donors-853db66fbb8c?source=collection_archive---------3-----------------------#2019-02-20">https://towardsdatascience.com/classification-project-finding-donors-853db66fbb8c?source=collection_archive---------3-----------------------#2019-02-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f24b0f50ca6b1caff4b3589f0bdf6fa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OgxmUpUV1OfgoCPP"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Picture from <a class="ae kf" href="https://unsplash.com/photos/npxXWgQ33ZQ" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="152e" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="4121" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这个项目中，我们将使用许多不同的监督算法，使用从 1994 年美国人口普查中收集的数据来精确预测个人收入。</p><p id="7072" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">然后，我们将从初步结果中选择最佳候选算法，并进一步优化该算法以最佳地模拟数据。我们实现的目标是构建一个模型，准确预测个人收入是否超过 50，000 美元。</p><p id="fbda" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这种任务可能出现在非营利机构中，这些机构依靠捐赠生存。了解个人的收入可以帮助非营利组织更好地了解需要多少捐款，或者他们是否应该从一开始就伸出援手。根据我们之前的研究，我们发现最有可能向慈善机构捐款的人是那些年收入超过 5 万美元的人。</p><p id="b0a5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这个项目的数据集来源于<a class="ae kf" href="https://archive.ics.uci.edu/ml/datasets/Census+Income" rel="noopener ugc nofollow" target="_blank"> UCI 机器学习库</a>。该数据集由 Ron Kohavi 和 Barry Becker 捐赠，发表在文章<em class="mh">“提高朴素贝叶斯分类器的准确性:决策树混合”</em>中。你可以在网上找到罗恩·科哈维的文章。我们在这里研究的数据由对原始数据集的微小更改组成，比如删除<code class="fe mi mj mk ml b">'fnlwgt'</code>特性和带有缺失或格式错误条目的记录。</p><h1 id="542f" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">数据</h1><p id="553a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">修改后的人口普查数据集由大约 32，000 个数据点组成，每个数据点有 13 个特征。该数据集是 Ron Kohavi 在论文<em class="mh">“提高朴素贝叶斯分类器的准确性:决策树混合”中发表的数据集的修改版本。你可以在网上找到这篇论文<a class="ae kf" href="https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf" rel="noopener ugc nofollow" target="_blank">，原始数据集存放在</a><a class="ae kf" href="https://archive.ics.uci.edu/ml/datasets/Census+Income" rel="noopener ugc nofollow" target="_blank"> UCI </a>上。</em></p><p id="1e13" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">特性</strong></p><ul class=""><li id="d25a" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">age</code>:年龄</li><li id="94c7" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">workclass</code>:工人阶级(私营、自营企业、自营企业、联邦政府、地方政府、州政府、无薪、从未工作)</li><li id="4383" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">education_level</code>:教育水平(学士、部分大学、11 年级、HS-grad、Prof-school、Assoc-acdm、Assoc-voc、9 年级、7-8 年级、12 年级、硕士、1-4 年级、10 年级、博士、5-6 年级、学前班)</li><li id="b1ca" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">education-num</code>:完成的教育年数</li><li id="d073" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">婚姻状况(已婚-同居-配偶、离婚、未婚、分居、丧偶、已婚-配偶缺席、已婚-配偶)</li><li id="4d8f" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">occupation</code>:工作职业(技术支持、工艺维修、其他服务、销售、行政管理、专业教授、搬运工人、清洁工、机器操作员、行政文员、农业-渔业、运输-搬运、私人服务、保护服务、武装部队)</li><li id="0454" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">relationship</code>:关系状态(妻子、亲生子女、丈夫、非家庭成员、其他亲属、未婚)</li><li id="93ca" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">race</code>:种族(白人、亚洲太平洋岛民、美洲印第安爱斯基摩人、其他人、黑人)</li><li id="c57e" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">sex</code>:性别(女，男)</li><li id="9162" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">capital-gain</code>:货币资本收益</li><li id="0e7a" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">capital-loss</code>:货币资金损失</li><li id="432e" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">hours-per-week</code>:每周平均工作时间</li><li id="6597" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">native-country</code>:本土国家(美国、柬埔寨、英国、波多黎各、加拿大、德国、美国外围地区(关岛-USVI 等)、印度、日本、希腊、中国、古巴、伊朗、洪都拉斯、菲律宾、意大利、波兰、牙买加、越南、墨西哥、葡萄牙、爱尔兰、法国、多米尼加共和国、老挝、厄瓜多尔、台湾、海地、哥伦比亚、匈牙利、危地马拉、尼加拉瓜、苏格兰、泰国、南斯拉夫、萨尔瓦多、特立尼达和多巴哥&amp;多巴哥、秘鲁、香港、荷兰)</li></ul><p id="0d0c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">目标变量</strong></p><ul class=""><li id="b2ae" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated"><code class="fe mi mj mk ml b">income</code>:收入阶层(&lt; =50K，&gt; 50K)</li></ul><h1 id="04b4" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">导入库并加载数据</h1><p id="f0cf" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将首先加载将要使用的 Python 库，以及人口普查数据。最后一列将是我们的目标变量，“收入”，其余的将是功能。</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="6ba6" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Import libraries necessary for this project</em></strong><br/>import numpy as np<br/>import pandas as pd<br/>from time import time<br/>from IPython.display import display <strong class="ml iu"><br/></strong><br/><strong class="ml iu"><em class="mh"># Import supplementary visualization code visuals.py</em></strong><br/>import visuals as vs<br/><br/><strong class="ml iu"><em class="mh"># Pretty display for notebooks</em></strong><br/>%matplotlib inline<br/><br/><strong class="ml iu"><em class="mh"># Load the Census dataset</em></strong><br/>data = pd.read_csv("census.csv")<br/><br/><strong class="ml iu"><em class="mh"># Success - Display the first record</em></strong><br/>display(data.head(n=1))</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/50874e8bb7ebafe6703228b62114d113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hRFSzxL-R7XPKgRlUY2qhg.png"/></div></div></figure><h1 id="7f72" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">探索性数据分析</h1><p id="c87f" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对数据集的初步研究将向我们展示每个群体中有多少人，以及他们中收入超过 50，000 美元的比例。</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="f8d2" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Total number of records</em></strong><br/>n_records = data.shape[0]<br/><br/><strong class="ml iu"><em class="mh"># Number of records where individual's income is more than $50,000</em></strong><br/>n_greater_50k = data[data['income'] == '&gt;50K'].shape[0]<br/><br/><strong class="ml iu"><em class="mh"># Number of records where individual's income is at most $50,000</em></strong><br/>n_at_most_50k = data[data['income'] == '&lt;=50K'].shape[0]<br/><br/><strong class="ml iu"><em class="mh"># Percentage of individuals whose income is more than $50,000</em></strong><br/>greater_percent = (n_greater_50k / n_records) * 100<br/><br/><strong class="ml iu"><em class="mh"># Print the results</em></strong><br/>print("Total number of records: {}".format(n_records))<br/>print("Individuals making more than $50,000: {}".format(n_greater_50k))<br/>print("Individuals making at most $50,000: {}".format(n_at_most_50k))<br/>print("Percentage of individuals making more than $50,000: {}%".format(greater_percent))</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9d05984e4c44ac0bcb38d32bd4b4628c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*lzLprjN5OLO-GMNd3A_R2Q.png"/></div></figure><h1 id="c7ce" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">准备数据</h1><p id="dd48" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">数据必须经过预处理才能用于机器学习算法。这个预处理阶段包括数据的清理、格式化和重构。</p><p id="a485" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">对于此数据集，既没有空条目也没有无效条目，但是有一些必须调整的要素。这项任务将极大地改善我们模型的结果和预测能力。</p><p id="1c2d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">转换偏斜连续特征</strong></p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/38fe23661cfb4de23cd90f51a06e7d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*LupzGQ5Si0burPxsEeo8mg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure by Author</figcaption></figure><p id="575a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果未对范围进行归一化，则要素值的偏态分布可能会使算法表现不佳。</p><p id="178d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在我们的数据集中，此分布有两个特征:</p><ul class=""><li id="68de" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">资本收益</li><li id="fc2d" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">资本损失</li></ul><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="a513" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Split the data into features and target label</em></strong><br/>income_raw = data['income']<br/>features_raw = data.drop('income', axis = 1)<br/><br/><strong class="ml iu"><em class="mh"># Visualize skewed continuous features of original data</em></strong><br/>vs.distribution(data)</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/1566a71228faf57832bd73b834b87173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IP6Y0sxrz7q2IO1Y9w0B6A.png"/></div></div></figure><p id="156f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">对于这种类型的分布，对数据应用对数变换是非常典型的，因此异常值不会对机器学习模型的性能产生负面影响。</p><p id="a098" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">但是，我们应该小心对待 0 值，因为 log(0)是未定义的，所以我们将这些值转换为大于 0 的一个小数值，以成功应用对数。</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="60ee" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Log-transform the skewed features</em></strong><br/>skewed = ['capital-gain', 'capital-loss']<br/>features_log_transformed = pd.DataFrame(data = features_raw)<br/>features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))<br/><br/><strong class="ml iu"><em class="mh"># Visualize the new log distributions</em></strong><br/>vs.distribution(features_log_transformed, transformed = True)</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/74822eda54b6c6fc5bd057a2b68f8168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oRXNPcmFb7N2cTWmnlNqWA.png"/></div></div></figure><p id="92e8" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">归一化数字特征</strong></p><p id="0ae3" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">建议对数字特征执行某种类型的缩放。这种缩放不会改变要素分布的形状，但可以确保在应用监督模型时平等对待每个要素。</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="b339" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Import sklearn.preprocessing.StandardScaler</em></strong><br/>from sklearn.preprocessing import MinMaxScaler<br/><br/><strong class="ml iu"><em class="mh"># Initialize a scaler, then apply it to the features</em></strong><br/>scaler = MinMaxScaler() <em class="mh"># default=(0, 1)</em><br/>numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']<br/><br/>features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)<br/>features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])<br/><br/><strong class="ml iu"><em class="mh"># Show an example of a record with scaling applied</em></strong><br/>display(features_log_minmax_transform.head(n = 5))</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ns"><img src="../Images/ca15b6d2b6826e7faae0eb4abea2a2cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-LjYYZsfE38Sp7IdkbKsQ.png"/></div></div></figure><p id="3bff" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">预处理类别特征</strong></p><p id="9fe3" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果我们看一下前面的表格，我们可以看到有一些特征，如“职业”或“种族”，它们不是数字，而是类别。机器学习算法期望与数值一起工作，因此这些分类特征应该被转换。</p><p id="578f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">最流行的分类转换之一叫做“一键编码”。在一次性编码中，为分类特征的每个可能类别创建一个“虚拟”变量。</p><p id="7e26" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为了更好地理解，请看下表:</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/0b1f95790cd202dc37ac4e0923cb9631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3jmoarkrXar4nyXHFJlIJg.png"/></div></div></figure><p id="336d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">此外，我们应该转换我们的目标变量“收入”。它只能取可能的值，“&lt;=50K” and “&gt; 50K”，所以我们将避免一次性编码，并将类别分别编码为 0 和 1。</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="716d" class="ni kh it ml b gy nj nk l nl nm">features_log_minmax_transform.head(1)</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nu"><img src="../Images/6d12cdeacf27b9ee974c5c60c02e7e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jek2-o9_lb0sKh7OGQnFHw.png"/></div></div></figure><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="023c" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># One-hot encode the 'features_log_minmax_transform' data </em></strong><br/>features_final = pd.get_dummies(features_log_minmax_transform)<br/><br/><strong class="ml iu"><em class="mh"># Encode the 'income_raw' data to numerical values</em></strong><br/>income = income_raw.map({'&lt;=50K':0,'&gt;50K':1})<br/><br/><strong class="ml iu"><em class="mh"># Print the number of features after one-hot encoding</em></strong><br/>encoded = list(features_final.columns)<br/>print("<strong class="ml iu">{}</strong> total features after one-hot encoding.".format(len(encoded)))<br/><br/><strong class="ml iu"><em class="mh"># See the encoded feature names</em></strong><br/>print (encoded)</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/56a10717f58e919be8983a4f306a84db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v-HgwY_w5BCjjEwnBM-aRQ.png"/></div></div></figure><p id="76b3" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">混洗和分割数据</strong></p><p id="ec8a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">当所有的分类变量被转换，所有的数字特征被标准化后，我们需要将数据分成训练集和测试集。我们将 80%的数据用于训练，20%用于测试，</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="9a00" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Import train_test_split</em> <br/></strong>from sklearn.model_selection import train_test_split  </span><span id="89af" class="ni kh it ml b gy nw nk l nl nm"><strong class="ml iu"><em class="mh"># Split the 'features' and 'income' data into training and testing sets</em></strong> <br/>X_train, X_test, y_train, y_test = train_test_split(features_final,                                                      income,                                                      test_size = 0.2,                                                      random_state = 0)  </span><span id="1eb1" class="ni kh it ml b gy nw nk l nl nm"><strong class="ml iu"><em class="mh"># Show the results of the split</em> </strong><br/>print("Training set has <strong class="ml iu">{}</strong> samples.".format(X_train.shape[0])) print("Testing set has <strong class="ml iu">{}</strong> samples.".format(X_test.shape[0]))</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/4fdce07bd6e87f2ca4e98a549b4e1653.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*S5-vL6XdQExZ-bEzVP_edw.png"/></div></figure><h1 id="e133" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">模型性能评估</h1><p id="898a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这一部分中，我们将研究四种不同的算法，并确定最擅长建模和预测数据的算法。这些算法中的一个将是天真的预测器，它将作为性能的基线，另外三个将是受监督的学习器。</p><p id="e027" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">指标和朴素预测器</strong></p><p id="7274" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">该项目的目标是正确识别每年收入超过 5 万美元的个人，因为他们是最有可能向慈善机构捐款的群体。因此，我们应该明智地选择我们的评估指标。</p><p id="8985" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> <em class="mh">注:评估指标提醒</em> </strong></p><p id="1b24" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">当对事件进行预测时，我们可以得到 4 种结果:真阳性、真阴性、假阳性和假阴性。所有这些都表示在下面的分类矩阵中:</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/df97bbef7edb6133253d4e0264918f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/0*ryvmZJ40k8yVnlr_.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure by Author</figcaption></figure><p id="a15c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> <em class="mh">准确度</em> </strong> <em class="mh">衡量分类器做出正确预测的频率。它是正确预测的数量与预测总数(测试数据点的数量)的比率。</em></p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/234c01f04e1ac99234c203e70ecb066f.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*_WCAZ2LmAYT3FJS59kFiJQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure by Author</figcaption></figure><p id="041d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> <em class="mh">精密</em> </strong> <em class="mh">告诉我们被我们归类为某一类的事件的比例，实际上都是那个类。它是真阳性与所有阳性的比率。</em></p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/5a4c5b810ef94da02c9e5fdbe9cb4c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*vIeuuORD9gXKdroLoIbC0Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure by Author</figcaption></figure><p id="031f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> <em class="mh">【回忆(敏感度)</em> </strong> <em class="mh">告诉我们实际上属于某一类的事件有多大比例被我们归类为该类。它是真阳性与所有阳性的比率。</em></p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2c3ff40172f228b3c2543a4e6ad59a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*0jS_lWQ3q0shAorbCivZ6w.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure by Author</figcaption></figure><p id="742d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">对于像我们这样分类分布有偏差的分类问题，准确性本身并不是一个合适的度量。反而精度和召回率更有代表性。</p><p id="d9ac" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="mh">这两个指标可以组合得到</em> <strong class="lg iu"> <em class="mh"> F1 得分</em> </strong> <em class="mh">，这是精度和召回得分的加权平均值(调和平均值)。这个分数的范围从 0 到 1，1 是可能的最佳 F1 分数(我们在处理比率时采用调和平均值)。</em></p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/6599e3f35dfc0c1cdb4bd0c7c5a4a7c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*SaP3n4_vtVbSo3o9r_wDcQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure by Author</figcaption></figure><p id="d4d7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="mh">此外，由于我们正在搜索愿意捐赠的个人，模型精确预测那些收入超过 5 万美元的个人的能力比模型回忆这些个人的能力更重要。我们可以使用 F-beta 分数作为同时考虑精确度和召回率的度量。</em></p><p id="c298" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="mh">具体来说，对于β = 0.5，更重视精度。</em></p><p id="b3ae" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果我们看一下“收入”变量的等级分布，很明显，年收入最多 5 万美元的人比年收入更多的人多得多。因此，我们可以做一个天真的预测，随机抽取一个人，预测他/她每年的收入不会超过 5 万美元。它被称为天真，因为我们没有考虑任何相关信息来证实这一说法。</p><p id="e768" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这个天真的预测将作为一个基准来确定我们的模型是否表现良好。重要的是要注意，单独使用这种类型的预测是没有意义的，因为所有个体都将被归类为非供体。</p><p id="845f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">天真预测者的表现</strong></p><p id="a677" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们将运行以下代码来确定我们的朴素预测器性能:</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="dc5d" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Counting the ones as this is the naive case. Note that 'income' is the 'income_raw' data encoded to numerical values done in the data preprocessing step.</em></strong><br/>TP = np.sum(income) </span><span id="54db" class="ni kh it ml b gy nw nk l nl nm"><strong class="ml iu"><em class="mh"># Specific to the naive case<br/></em></strong>FP = income.count() - TP</span><span id="395e" class="ni kh it ml b gy nw nk l nl nm"><strong class="ml iu"><em class="mh"># No predicted negatives in the naive case</em></strong><br/>TN = 0 <br/>FN = 0 <br/><br/><strong class="ml iu"><em class="mh"># Calculate accuracy, precision and recall</em></strong><br/>accuracy = TP / (TP + FP + TN + FN)<br/>recall = TP / (TP + FN)<br/>precision = TP / (TP + FP)<br/><br/><strong class="ml iu"><em class="mh"># Calculate F-score using the formula above for beta = 0.5 and correct values for precision and recall.</em></strong><br/>beta = 0.5<br/>fscore = (1 + beta**2) * ((precision * recall) / ((beta**2) * precision + recall))<br/><br/><strong class="ml iu"><em class="mh"># Print the results </em></strong><br/>print("Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]".format(accuracy, fscore))</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/9662cceef3801cddf6d106e39eefa841.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*pvCGk8oNy9FBDh6qQaiTWA.png"/></div></figure><h1 id="fcd7" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">监督学习模型</strong></h1><p id="66e2" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">考虑到我们数据的形状:</p><ul class=""><li id="40cf" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">45222 个数据点</li><li id="838a" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">103 个特征</li></ul><p id="1b76" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">数据点的数量不是很大，但是有大量的特征，并且不是所有的监督算法都适合适当地处理数量。</p><p id="3a8e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">scikit-learn 中的一些可用分类算法:</p><ul class=""><li id="3a25" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">高斯朴素贝叶斯</li><li id="fae8" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">决策树</li><li id="382f" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">集成方法(Bagging、AdaBoost、随机森林、梯度增强)</li><li id="74e1" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">k-最近邻(近邻)</li><li id="b7b7" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">随机梯度下降分类器(SGDC)</li><li id="d4a3" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">支持向量机(SVM)</li><li id="b5d2" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">逻辑回归</li></ul><p id="c29a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">考虑到他们的特点和我们的数据集，我们将选择以下三个:</p><p id="5892" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> a)高斯朴素贝叶斯</strong></p><ul class=""><li id="380d" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">该模型的<strong class="lg iu">优势</strong>是:它是一个简单快速的分类器，只需对模型的超参数进行很小的调整就能提供良好的结果。此外，它不需要大量的数据来进行适当的训练。</li><li id="d075" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">该模型的<strong class="lg iu">弱点</strong>是:它具有很强的特征独立性假设。如果我们没有同时出现类别标签和某个属性值(例如，class =“nice”，shape =“sphere”)，那么基于频率的概率估计将为零，因此给定条件独立性假设，当所有概率相乘时，我们将得到零，这将影响后验概率估计。</li><li id="aa5e" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">该模型可以应用的一个可能的真实世界应用是文本学习。</li><li id="0d54" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">它是一个很好的候选对象，因为它是一个高效的模型，可以处理许多特征(数据集包含 98 个特征)。</li></ul><p id="6ad8" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> b)随机森林</strong></p><ul class=""><li id="da56" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">该模型的<strong class="lg iu">优势</strong>是:它可以很好地处理二元特征，因为它是决策树的集合。它不期望线性特征。它适用于高维空间和大量的训练样本。</li><li id="8dcc" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">主要的<strong class="lg iu">弱点</strong>是在处理噪声数据时可能会过拟合。</li><li id="e063" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">该模型的一个可能的实际应用是预测股票市场价格。</li><li id="af7a" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">它是一个很好的候选者，因为它通常是一个非常准确的分类器，并且能够很好地处理二元要素和高维数据集。</li></ul><p id="146b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> c)支持向量机分类器</strong></p><ul class=""><li id="408e" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">该模型的<strong class="lg iu">优势</strong>是:它在没有线性可分数据和高维空间的情况下工作良好。</li><li id="7e1d" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">主要的<strong class="lg iu">缺点</strong>是训练效率可能相当低，因此不适合“工业规模”应用。</li><li id="391c" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">该模型可以应用的一个可能的真实世界应用是对患有和不患有常见疾病的人进行分类。</li><li id="c505" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">它是一个很好的候选者，因为它通常是一个非常准确的分类器，并且能够很好地处理二元要素和高维数据集。</li></ul><p id="6d6e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">创建训练和预测管道</strong></p><p id="86ab" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为了正确评估每个模型的性能，我们将创建一个训练和预测管道，使我们能够使用各种大小的训练数据快速有效地训练模型，并对测试数据执行预测。</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="e210" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Import two metrics from sklearn - fbeta_score and accuracy_score</em><br/></strong>def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): <br/>    <em class="mh">'''</em><br/><em class="mh">    inputs:</em><br/><em class="mh">       - learner: the learning algorithm to be trained and predicted on</em><br/><em class="mh">       - sample_size: the size of samples (number) to be drawn from training set</em><br/><em class="mh">       - X_train: features training set</em><br/><em class="mh">       - y_train: income training set</em><br/><em class="mh">       - X_test: features testing set</em><br/><em class="mh">       - y_test: income testing set</em><br/><em class="mh">    '''</em><br/>    <br/>    results = {}<br/>    <br/><strong class="ml iu">    <em class="mh"># Fit the learner to the training data </em></strong><br/>    start = time() <em class="mh"># Get start time</em><br/>    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])<br/>    end = time() <em class="mh"># Get end time</em><br/>    <br/>    <strong class="ml iu"><em class="mh"># Calculate the training time</em></strong><br/>    results['train_time'] = end - start <br/>        <br/>   <strong class="ml iu"> <em class="mh"># Get the predictions on the test set</em></strong><br/>    start = time() <strong class="ml iu"><em class="mh"># Get start time</em></strong><br/>    predictions_test = learner.predict(X_test)<br/>    predictions_train = learner.predict(X_train[:300])<br/>    end = time() <strong class="ml iu"><em class="mh"># Get end time</em></strong><br/>    <br/>    <strong class="ml iu"><em class="mh"># Calculate the total prediction time</em></strong><br/>    results['pred_time'] = end -start<br/>            <br/> <strong class="ml iu">   <em class="mh"># Compute accuracy on the first 300 training samples </em></strong><br/>    results['acc_train'] = accuracy_score(y_train[:300], predictions_train)<br/>        <br/>    <strong class="ml iu"><em class="mh"># Compute accuracy on test set using accuracy_score()</em></strong><br/>    results['acc_test'] = accuracy_score(y_test, predictions_test)<br/>    <br/>    <strong class="ml iu"><em class="mh"># Compute F-score on the the first 300 training samples</em></strong><br/>    results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta=0.5)<br/>        <br/>    <strong class="ml iu"><em class="mh"># Compute F-score on the test set which is y_test</em></strong><br/>    results['f_test'] = fbeta_score(y_test, predictions_test, beta=0.5)<br/>       <br/>    <strong class="ml iu"><em class="mh"># Success</em></strong><br/>    print("{} trained on {} samples.".format(learner.__class__.__name__, sample_size))<br/>        <br/>    <strong class="ml iu"><em class="mh"># Return the results</em></strong><br/>    return results</span></pre><h1 id="cbaf" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">初始模型评估</strong></h1><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="bb8f" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Import the three supervised learning models from sklearn</em></strong><br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.svm import SVC<br/><br/><strong class="ml iu"><em class="mh"># Initialize the three models</em></strong><br/>random_state = 42<br/><br/>clf_A = RandomForestClassifier(random_state=random_state)<br/>clf_B = GaussianNB()<br/>clf_C = SVC(random_state=random_state)<br/><br/><strong class="ml iu"><em class="mh"># Calculate the number of samples for 1%, 10%, and 100% of the training data</em></strong><br/>samples_100 = len(y_train)<br/>samples_10 = int(len(y_train)/10)<br/>samples_1 = int(len(y_train)/100)<br/><strong class="ml iu"><br/><em class="mh"># Collect results on the learners</em></strong><br/>results = {}<br/>for clf in [clf_A, clf_B, clf_C]:<br/>    clf_name = clf.__class__.__name__<br/>    results[clf_name] = {}<br/>    for i, samples in enumerate([samples_1, samples_10,   samples_100]):<br/>        results[clf_name][i] = \<br/>        train_predict(clf, samples, X_train, y_train, X_test, y_test)<br/><br/><strong class="ml iu"><em class="mh"># Run metrics visualization for the three supervised learning models chosen</em></strong><br/>vs.evaluate(results, accuracy, fscore)</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/8af4b35afd98395e225d4bb2e8fa9c82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AwD4DFGdCZkIABd3EQUkPQ.png"/></div></div></figure><h1 id="b6f0" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">改善结果</h1><p id="74ca" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最后，在本节中，我们将选择最佳模型用于我们的数据，然后，通过调整参数来提高模型的 F 值，在整个训练集(X_train 和 y_train)上对模型执行网格搜索优化。</p><p id="0f54" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">选择最佳模特</strong></p><p id="5678" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">根据评估结果，识别潜在捐赠者的最合适的模型是随机森林分类器，因为它产生与支持向量分类器相同的 F 分数，但时间要少得多。</p><p id="ac55" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这与我们对算法的了解是一致的，因为当处理高维数据集时，这是一个非常好的选择，换句话说，是具有大量特征的数据集。</p><p id="b5d6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">因此，在评估的模型中，这是最有效的一个，也是最适合处理我们的数据集的。</p><p id="4228" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">通俗地描述随机森林</strong></p><p id="f002" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为了理解随机森林分类器，我们需要首先介绍决策树的概念。决策树是一种类似流程图的结构，其中每个内部节点代表对数据集属性的测试，每个品牌代表测试的结果，每个叶子代表一个类别标签。因此，算法将对数据进行测试，找出数据集的哪些特征与预测某个结果最相关，并相应地分离数据集。</p><p id="a0c5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">下面是一个决策树的例子，用来决定你是否把你的车借给别人:</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="ab gu cl of"><img src="../Images/d54ee1f2a552de51a7eb7dd8a15bebab.png" data-original-src="https://miro.medium.com/v2/format:webp/1*s6bohT7lyP4MZCNBSrJHgg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure by Author</figcaption></figure><p id="a147" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">随机森林是一种元估计器，它在数据集的各种子样本上拟合多个决策树分类器，并使用平均来提高模型的预测准确性，并通过防止模型变得过于复杂和无法对看不见的数据进行归纳来控制过度拟合。它随机选择一些特征，并在每个特征子集中训练每个决策树分类器。然后，它通过让每个决策树为正确的标签投票来做出预测。</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="ab gu cl of"><img src="../Images/d5ef9c4239e4b15ef31556dde5a24241.png" data-original-src="https://miro.medium.com/v2/format:webp/1*i0o8mjFfCn-uD79-F1Cqkw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure by Author</figcaption></figure><p id="f4f0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">随机森林分类器因其使用简单、高效和良好的预测精度而在分类问题中得到广泛应用。</p><p id="0021" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">模型调谐</strong></p><p id="7c50" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们现在将使用 GridSearchCV 微调所选的模型。</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="68fa" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Import the necessary libraries</em></strong><br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.metrics import make_scorer<br/><br/><strong class="ml iu"><em class="mh"># Initialize the classifier</em></strong><br/>clf = RandomForestClassifier(random_state = 42)<br/><br/><br/><strong class="ml iu"><em class="mh"># Create the parameters list </em></strong><br/>parameters =  {<br/>    'max_depth': [10,20,30,40],<br/>    'max_features': [2, 3],<br/>    'min_samples_leaf': [3, 4, 5],<br/>    'min_samples_split': [8, 10, 12],<br/>    'n_estimators': [50,100,150]}<br/><br/><strong class="ml iu"><em class="mh"># Make an fbeta_score scoring object using make_scorer()</em></strong><br/>scorer = make_scorer(fbeta_score, beta=0.5)<br/><br/><strong class="ml iu"><em class="mh"># Perform grid search on the classifier </em></strong><br/>grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=scorer)<br/><br/><strong class="ml iu"><em class="mh"># Fit the grid search object to the training data </em></strong><br/>grid_fit = grid_obj.fit(X_train, y_train)<br/><br/><strong class="ml iu"><em class="mh"># Get the estimator</em></strong><br/>best_clf = grid_fit.best_estimator_<br/><br/><strong class="ml iu"><em class="mh"># Make predictions using the unoptimized and model</em></strong><br/>predictions = (clf.fit(X_train, y_train)).predict(X_test)<br/>best_predictions = best_clf.predict(X_test)<br/><br/><strong class="ml iu"><em class="mh"># Report the before-and-afterscores</em></strong><br/>print("Unoptimized model\n------")<br/>print("Accuracy score on testing data {:.4f}".format(accuracy_score(y_test, predictions)))<br/>print("F-score on testing data: {:.4f}".format(fbeta_score(y_test, predictions, beta = 0.5)))<br/>print("\nOptimized Model\n------")<br/>print("Final accuracy score on the testing data: {:.4f}".format(accuracy_score(y_test, best_predictions)))<br/>print("Final F-score on the testing data: {:.4f}".format(fbeta_score(y_test, best_predictions, beta = 0.5)))</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/51faf1ee2448f90028db2675366ed340.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*eM2WORgAldwHwPp4aBd9MQ.png"/></div></figure><h1 id="368e" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">最终模型评估</h1><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="2fa1" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Show the best classifier hyperparameters</em></strong><br/>best_clf</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/02b79e2ac43badfd079f9732a6c59e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*CwjDESDj0WdMqZaan7Fa4w.png"/></div></figure><p id="a200" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">观察结果</strong></p><ul class=""><li id="b769" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">优化后的模型对测试数据的准确率和 F 值分别为:84.8%和 71.38%。</li><li id="d48b" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">这些分数比未优化模型的分数稍好，但是计算时间要长得多。</li><li id="59e6" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">精度和 F 值的朴素预测器基准分别为 24.78%和 29.27%，比用训练模型获得的结果差得多。</li></ul><p id="070e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> <em class="mh">附加:特征重要性</em> </strong></p><p id="8625" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在像他这样的数据集上执行监督学习时，一个重要的任务是确定哪些特征提供了最强的预测能力。通过只关注少数关键特征和目标标签之间的关系，我们简化了对现象的理解，这通常是一件有用的事情。</p><p id="6503" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这个项目的例子中，这意味着我们希望确定少量的特征，这些特征最强有力地预测一个人的收入是最多 50，000 美元还是超过 50，000 美元。</p><p id="f01b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">凭直觉，在原始数据的 13 个可用特征中，我们可以推断出预测收入最重要的特征是:</p><p id="e600" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">1)年龄</p><p id="3b8f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">2)教育</p><p id="91e4" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">3)母国</p><p id="f726" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">4)职业</p><p id="fbb8" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">5)每周小时数</p><p id="ee05" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">等级的顺序逻辑如下:</p><ul class=""><li id="4960" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">一个人的收入可能会随着时间的推移而增加，老年人往往比年轻人挣得更多。</li><li id="78e7" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">此外，受过高等教育的人往往会获得收入更高的工作，这也是一个与本国密切相关的因素，因为通常情况下，来自经济实力较强国家的人往往有机会接受高等教育。</li><li id="68d5" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">职业是一个需要考虑的重要因素，因为年收入会因行业和部门的不同而有很大差异。</li><li id="0306" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">最后，每周工作时间通常情况下，工作时间越长的人收入越高。</li></ul><p id="4177" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们将通过一个具有<em class="mh"> feature_importance_ </em>方法的模型来检查我们的逻辑的准确性:</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="4194" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Import Ada Boost Classifier</em></strong><br/>from sklearn.ensemble import AdaBoostClassifier<br/><br/><strong class="ml iu"><em class="mh"># Train the supervised model on the training </em></strong><br/>model = AdaBoostClassifier().fit(X_train, y_train)<br/><br/><strong class="ml iu"><em class="mh"># Extract the feature importances using .feature_importances</em></strong><em class="mh">_ </em><br/>importances = model.feature_importances_<br/><br/><strong class="ml iu"><em class="mh"># Plot</em></strong><br/>vs.feature_plot(importances, X_train, y_train)</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/abf1a9df4d82fc49fbcfe47fb28aaf7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*PTPP2j4SIh6tkqGJzHMLlQ.png"/></div></figure><p id="1a08" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">上一节中的观点部分正确，因为 AdaBoost 测试表明，年龄、每周工作时间和教育程度等特征与预测收入密切相关。然而，我们没有确定资本损失和资本收益。</p><p id="17d9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">功能选择</strong></p><p id="cd13" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在有理由提出以下问题:</p><p id="48f4" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="mh">如果我们只使用数据中所有可用特征的一个子集，一个模型表现如何？</em></p><p id="2521" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">由于需要训练的特征更少，因此期望训练和预测时间更少——以性能指标为代价。从上面的可视化中，我们看到前五个最重要的特征占数据中所有特征重要性的一半以上。这暗示我们可以尝试<em class="mh">减少特征空间</em>并简化模型学习所需的信息。</p><pre class="na nb nc nd gt ne ml nf ng aw nh bi"><span id="6f7e" class="ni kh it ml b gy nj nk l nl nm"><strong class="ml iu"><em class="mh"># Import functionality for cloning a model</em></strong><br/>from sklearn.base import clone<strong class="ml iu"><br/></strong><br/><strong class="ml iu"><em class="mh"># Reduce the feature space</em></strong><br/>X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]<br/>X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]]<br/><br/><strong class="ml iu"><em class="mh"># Train on the "best" model found from grid search earlier</em></strong><br/>clf = (clone(best_clf)).fit(X_train_reduced, y_train)<br/><br/><strong class="ml iu"><em class="mh"># Make new predictions</em></strong><br/>reduced_predictions = clf.predict(X_test_reduced)<br/><br/><strong class="ml iu"><em class="mh"># Report scores from the final model using both versions of data</em></strong><br/>print("Final Model trained on full data\n------")<br/>print("Accuracy on testing data: {:.4f}".format(accuracy_score(y_test, best_predictions)))<br/>print("F-score on testing data: {:.4f}".format(fbeta_score(y_test, best_predictions, beta = 0.5)))<br/>print("\nFinal Model trained on reduced data\n------")<br/>print("Accuracy on testing data: {:.4f}".format(accuracy_score(y_test, reduced_predictions)))<br/>print("F-score on testing data: {:.4f}".format(fbeta_score(y_test, reduced_predictions, beta = 0.5)))</span></pre><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/612e471d5e0552a3bfc2b44d4bfe3733.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*ylwV1fDMjSz3RtynVFfngw.png"/></div></figure><p id="3468" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">对特征选择效果的观察</strong></p><ul class=""><li id="7120" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">精简数据的精度和 f 值都低于原始数据集。特别是 f 分数 71.38%对 67.57%</li><li id="03fe" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">考虑到在对默认模型、优化模型和精简数据集进行评估时获得的指标，最佳选择是使用带有完整数据集的默认模型版本，因为它在良好的训练时间内产生了准确性和 f 分数的良好组合。</li></ul><h1 id="c1ae" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><p id="6efc" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在整篇文章中，我们做了一个端到端的机器学习分类项目，我们了解并获得了关于分类模型的一些见解以及开发一个具有良好性能的分类模型的关键。</p><p id="86ef" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是本系列中解释的第三个机器学习项目。如果你喜欢，看看之前的:</p><ul class=""><li id="5abd" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">回归项目</li><li id="ee0a" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">朴素贝叶斯项目</li></ul><p id="a50f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">敬请期待下一篇文章！这将是关于无监督学习的理论和概念的介绍。</p><p id="f0bb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="mh">如果你喜欢这篇文章，那么你可以看看我关于数据科学和机器学习的其他文章</em> <a class="ae kf" href="https://medium.com/@rromanss23" rel="noopener"> <em class="mh">这里</em> </a> <em class="mh">。</em></p><p id="d3b8" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="mh">如果你想了解更多关于机器学习、数据科学和人工智能的知识</em> <strong class="lg iu"> <em class="mh">请在 Medium </em> </strong> <em class="mh">上关注我，敬请关注我的下一篇帖子！</em></p></div></div>    
</body>
</html>