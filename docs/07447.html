<html>
<head>
<title>Practical Coding in TensorFlow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2.0 中的实用编码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-coding-in-tensorflow-2-0-fafd2d3863f6?source=collection_archive---------15-----------------------#2019-10-18">https://towardsdatascience.com/practical-coding-in-tensorflow-2-0-fafd2d3863f6?source=collection_archive---------15-----------------------#2019-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6b33" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于数据集和 TFRecord 数据格式的所有信息</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f0de80c98aabc3d4ffa6a286d4204fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LjHpPMV3SXU54lQaZjhPcQ.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@thefredyjacob?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Fredy Jacob</a> on <a class="ae kv" href="https://unsplash.com/s/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="dc39" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">摘要</h1><p id="9f42" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这篇博文中，我将展示如何在 TF 2 中使用<code class="fe mk ml mm mn b">tf.dataset</code>。此外，我们从 numpy 数组创建一个数据集，并学习如何将图像和数组写入 TFRecord 文件或从 TF record 文件中读取图像和数组。</p><h1 id="5aa2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">装置</h1><p id="e344" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">要安装 TF 2.x，请访问这个<a class="ae kv" href="https://www.tensorflow.org/install" rel="noopener ugc nofollow" target="_blank">页面</a>。</p><p id="bc51" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">要检查您的当前版本:</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="8c5c" class="mx kx iq mn b gy my mz l na nb">import tensorflow as tf<br/>print(tf.__version__)</span></pre><h1 id="9d52" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">TF 2 中的数据集</h1><p id="feee" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">向网络提供数据的最简单和最有效的方法是使用<code class="fe mk ml mm mn b">tf.dataset</code>。这个职业在 TF 2 中经历了显著的变化。数据集本身现在是一个迭代器，可以用 for 循环进行迭代。</p><p id="1379" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">我们先做一个数据集。对于可以存储在内存中的少量数据，我们可以从 numpy 数组中创建一个数据集:</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="2191" class="mx kx iq mn b gy my mz l na nb">import numpy as np<br/>np.random.seed(0)</span><span id="8440" class="mx kx iq mn b gy nc mz l na nb">data = np.random.randn(256, 8, 8, 3)<br/>dataset = tf.data.Dataset.from_tensor_slices(data)<br/>print(dataset)</span><span id="0aad" class="mx kx iq mn b gy nc mz l na nb">...<br/>&lt;TensorSliceDataset shapes: (8, 8, 3), types: tf.float64&gt;</span></pre><p id="b20c" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">按照惯例，第一个维度是训练样本的数量。迭代器可以产生任何批量。默认情况下，它会生成单个训练示例，即批量大小为 1。</p><p id="7ff7" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">for 循环在数据集的末尾停止。要多次迭代数据集，请使用<code class="fe mk ml mm mn b">.repeat()</code>。</p><p id="298c" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">我们可以使用 Python 的枚举器或内置方法来枚举每个批处理。前者产生一个张量，推荐使用。</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="e361" class="mx kx iq mn b gy my mz l na nb">for i, batch in enumerate(dataset):<br/>  if i == 255 or i == 256:<br/>  print(i, batch.shape)</span><span id="51e7" class="mx kx iq mn b gy nc mz l na nb">...<br/>255 (8, 8, 3)<br/>...</span><span id="cc48" class="mx kx iq mn b gy nc mz l na nb">for i, batch in dataset.enumerate():<br/>  if i == 255 or i == 256:<br/>  print(i, batch.shape)<br/>  print(i.numpy(), batch.shape)</span><span id="8859" class="mx kx iq mn b gy nc mz l na nb">...<br/>tf.Tensor(255, shape=(), dtype=int64) (8, 8, 3) 255 (8, 8, 3)<br/>...</span><span id="41a9" class="mx kx iq mn b gy nc mz l na nb">for i, batch in dataset.repeat(2).enumerate():<br/>  if i == 255 or i == 256:<br/>  print(i.numpy(), batch.shape)</span><span id="f9d8" class="mx kx iq mn b gy nc mz l na nb">...<br/>255 (8, 8, 3)<br/>256 (8, 8, 3)</span></pre><p id="5195" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">如果不需要整个数据集，可以从中提取所需数量的批次:</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="3bb5" class="mx kx iq mn b gy my mz l na nb">for batch in dataset.take(3):<br/>  print(batch.shape)</span><span id="f821" class="mx kx iq mn b gy nc mz l na nb">...<br/>(8, 8, 3)<br/>(8, 8, 3)<br/>(8, 8, 3)</span></pre><p id="436a" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">要迭代大于 1 的批处理，必须指定批处理大小。</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="f35f" class="mx kx iq mn b gy my mz l na nb">dataset = dataset.batch(16)</span><span id="856b" class="mx kx iq mn b gy nc mz l na nb">for batch in dataset.take(3):<br/>  print(batch.shape)</span><span id="788c" class="mx kx iq mn b gy nc mz l na nb">...<br/>(16, 8, 8, 3)<br/>(16, 8, 8, 3)<br/>(16, 8, 8, 3)</span></pre><p id="3c65" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">我们也可以在将数据输入网络之前对其进行洗牌。这会用<code class="fe mk ml mm mn b">buffer_size</code>元素填充一个缓冲区，然后从这个缓冲区中随机抽取元素，用新元素替换选中的元素。为了实现完美的混洗，缓冲区大小应该等于数据集的完整大小。<code class="fe mk ml mm mn b">buffer_size</code> 5 有一个例子。在实践中，我们使用等于几个批量大小的缓冲区大小。</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="6ab1" class="mx kx iq mn b gy my mz l na nb">dataset = tf.data.Dataset.from_tensor_slices(np.arange(19))</span><span id="6fcf" class="mx kx iq mn b gy nc mz l na nb">for batch in dataset.batch(5):<br/>  print(batch)</span><span id="9fc5" class="mx kx iq mn b gy nc mz l na nb">...<br/>tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)<br/>tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)<br/>tf.Tensor([10 11 12 13 14], shape=(5,), dtype=int64)<br/>tf.Tensor([15 16 17 18], shape=(4,), dtype=int64)<br/>...</span><span id="ee56" class="mx kx iq mn b gy nc mz l na nb">for batch in dataset.shuffle(5).batch(5):<br/>  print(batch)</span><span id="5c70" class="mx kx iq mn b gy nc mz l na nb">...<br/>tf.Tensor([2 5 0 4 1], shape=(5,), dtype=int64)<br/>tf.Tensor([ 6  9  3 12 10], shape=(5,), dtype=int64)<br/>tf.Tensor([13  8 15 17 11], shape=(5,), dtype=int64)<br/>tf.Tensor([18 16 14  7], shape=(4,), dtype=int64)</span></pre><p id="ca9c" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">你可以看到，结果并不统一，但足够好。</p><p id="449c" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">但是，如果您以不同的顺序应用这些方法，将会得到意想不到的结果。它打乱了批次，而不是元素。</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="6758" class="mx kx iq mn b gy my mz l na nb">for batch in dataset.batch(5).shuffle(5):<br/>  print(batch)</span><span id="5111" class="mx kx iq mn b gy nc mz l na nb">...<br/>tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)<br/>tf.Tensor([15 16 17 18], shape=(4,), dtype=int64)<br/>tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)<br/>tf.Tensor([10 11 12 13 14], shape=(5,), dtype=int64)</span></pre><p id="5e09" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">您也可以在使用<code class="fe mk ml mm mn b">.map()</code>从数据集中读取数据的同时准备数据。函数顺序在这里是至关重要的。比如现在<code class="fe mk ml mm mn b">transform</code>应用于每批。</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="3f52" class="mx kx iq mn b gy my mz l na nb">def tranform(data):<br/>  mean = tf.reduce_mean(data)<br/>  return data - mean</span><span id="f83b" class="mx kx iq mn b gy nc mz l na nb">for batch in dataset.shuffle(5).batch(5).map(tranform):<br/>  print(batch)</span><span id="7e7f" class="mx kx iq mn b gy nc mz l na nb">...<br/>tf.Tensor([ 2  3 -1  0 -2], shape=(5,), dtype=int64)<br/>tf.Tensor([-2 -5  2  3  4], shape=(5,), dtype=int64)<br/>tf.Tensor([-1  1  2 -5  3], shape=(5,), dtype=int64)<br/>tf.Tensor([ 3 -3  7 -4], shape=(4,), dtype=int64)</span></pre><p id="fcfa" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">读取和处理数据集通常非常耗时。为了让训练有一个稳定的数据流，从而让你的 GPU 保持忙碌，你可以预取几个批次。为此，您必须将<code class="fe mk ml mm mn b">.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)</code>作为最后一个方法。通过使用<code class="fe mk ml mm mn b">autotune</code>，您允许 TF 找到最佳的缓冲区大小。</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="2803" class="mx kx iq mn b gy my mz l na nb">dataset.shuffle(5).batch(5).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)</span></pre><h1 id="c0c5" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">为图像制作 TFRecord 文件</h1><p id="8561" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">只有当数组相当小并且可以存储在内存中时，才可以从 numpy 数组生成数据集。对于更大的数据集，比如图像，我建议使用 TF 自制的数据格式<code class="fe mk ml mm mn b">tfrecord</code>。</p><p id="88a9" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated"><em class="nd">注</em>:我尝试过使用<code class="fe mk ml mm mn b">h5</code>，但是支持微乎其微，数据读取较慢。因此，我们现在坚持使用<code class="fe mk ml mm mn b">tfrecord</code>。</p><p id="7568" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">假设您希望将一个图像和一个包含动作的数组存储为一条记录:一个训练示例。</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="dcec" class="mx kx iq mn b gy my mz l na nb">imgs = (np.random.randn(256, 8, 8, 3) * 255).astype(np.uint8)<br/>acts = np.random.randn(256, 4).astype(np.float32)</span></pre><p id="a072" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">首先，我们必须定义一个助手函数:</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="14a3" class="mx kx iq mn b gy my mz l na nb">def _bytes_feature(value):<br/>  <br/>  if isinstance(value, type(tf.constant(0))):<br/>    value = value.numpy()</span><span id="ffeb" class="mx kx iq mn b gy nc mz l na nb">return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))</span></pre><p id="f284" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">从图像和动作中生成一个字节串。</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="4f78" class="mx kx iq mn b gy my mz l na nb">def serialize_example(image, action):</span><span id="9fa3" class="mx kx iq mn b gy nc mz l na nb">image = tf.image.encode_png(image)</span><span id="2c18" class="mx kx iq mn b gy nc mz l na nb">feature = {<br/>    'image': _bytes_feature(image),<br/>    'action': _bytes_feature(tf.io.serialize_tensor(action)),<br/>  }</span><span id="6212" class="mx kx iq mn b gy nc mz l na nb">return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()</span></pre><p id="d239" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated"><code class="fe mk ml mm mn b">tf.train.Example</code>是一个容易引起误解的记录名称。</p><p id="1ef0" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">现在，我们可以将所有示例写入一个文件:</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="3a7e" class="mx kx iq mn b gy my mz l na nb">with tf.io.TFRecordWriter('test.tfrecord') as writer:</span><span id="efe5" class="mx kx iq mn b gy nc mz l na nb">for xi, ai in zip(imgs, acts):<br/>    example = serialize_example(xi, ai)<br/>    writer.write(example)</span></pre><h1 id="a0d5" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">读取 TFRecord 文件</h1><p id="4515" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">要读取文件，我们必须以相反的顺序执行我们的操作。我们需要两个辅助函数:第一个用于将字节串解析为张量，第二个用于解码图像。</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="274d" class="mx kx iq mn b gy my mz l na nb">def parse_image_function(example_proto):<br/>  image_feature_description = {<br/>    'image': tf.io.FixedLenFeature([], tf.string),<br/>    'action': tf.io.FixedLenFeature([], tf.string),<br/>  }</span><span id="dd05" class="mx kx iq mn b gy nc mz l na nb">return tf.io.parse_single_example(example_proto, image_feature_description)</span><span id="e7df" class="mx kx iq mn b gy nc mz l na nb">def decode_image_function(record):<br/>  record['image'] = tf.cast(tf.image.decode_image(record['image']), tf.float32) / 255.</span><span id="0758" class="mx kx iq mn b gy nc mz l na nb">record['action'] = tf.io.parse_tensor(record['action'], out_type=tf.float32)</span><span id="ed81" class="mx kx iq mn b gy nc mz l na nb">return record['image'], record['action']</span></pre><p id="aa63" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">现在我们可以从文件中读取。</p><pre class="kg kh ki kj gt mt mn mu mv aw mw bi"><span id="a068" class="mx kx iq mn b gy my mz l na nb">dataset = tf.data.TFRecordDataset('test.tfrecord')</span><span id="cacd" class="mx kx iq mn b gy nc mz l na nb">dataset = dataset.map(parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><span id="d31a" class="mx kx iq mn b gy nc mz l na nb">dataset = dataset.map(decode_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><span id="282d" class="mx kx iq mn b gy nc mz l na nb">for img, act in dataset.batch(16).take(3):<br/>  print(img.shape, act.shape)</span><span id="61cf" class="mx kx iq mn b gy nc mz l na nb">...<br/>(16, 8, 8, 3) (16, 4)<br/>(16, 8, 8, 3) (16, 4)<br/>(16, 8, 8, 3) (16, 4)</span></pre><p id="9d3e" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated">此外，如果需要的话，您可以应用任何想要的混排或映射。</p><p id="e5aa" class="pw-post-body-paragraph lo lp iq lq b lr mo jr lt lu mp ju lw lx mq lz ma mb mr md me mf ms mh mi mj ij bi translated"><em class="nd">注</em>:我试过用<code class="fe mk ml mm mn b">h5</code>但是它的支持非常有限，读取速度也比较慢。因此，我们现在坚持使用<code class="fe mk ml mm mn b">tfrecord</code>。</p></div></div>    
</body>
</html>