<html>
<head>
<title>Automatic Differentiation, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动微分，解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automatic-differentiation-explained-b4ba8e60c2ad?source=collection_archive---------5-----------------------#2019-03-03">https://towardsdatascience.com/automatic-differentiation-explained-b4ba8e60c2ad?source=collection_archive---------5-----------------------#2019-03-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0d8c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">计算机如何计算导数？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0c854cf64b9eefdd0cc7c060f32ed5f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rXu2CpQbBWraj2s4sByjAA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Title image: <a class="ae ky" href="https://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Pure-mathematics-formul%C3%A6-blackboard.jpg/1280px-Pure-mathematics-formul%C3%A6-blackboard.jpg" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="241e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过<a class="ae ky" rel="noopener" target="_blank" href="/the-beginners-guide-to-gradient-descent-c23534f808fd">梯度下降</a>的过程，神经网络能够在每次训练中逐渐提高精确度。在梯度下降中，我们的目标是通过调整权重和偏差来最小化损失(即模型有多不准确)。</p><p id="40ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如在<a class="ae ky" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9">之前的系列文章</a>中所解释的，通过找到损失函数的偏导数，我们知道我们必须调整多少(和在什么方向上)我们的权重和偏差来减少损失。在该系列中，我们手工计算了单神经元神经网络的导数均方误差损失函数。</p><p id="067d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，神经网络——计算机——是如何计算表达式的偏导数的呢？答案在于一个被称为<strong class="lb iu">自动微分</strong>的过程。让我用上一个系列中的成本函数来说明它，但是做了调整，使它是标量形式的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/bcf57a269fcf61d84e83dd16452437c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jKiFjnroX-WuQyuBPqQmJA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image 1: The cost function in scalar form</figcaption></figure><p id="e4ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，由于自动微分只能计算表达式在某一点上的偏导数，所以我们必须为每个变量赋予初始值。我们假设:y = 5；w = 2；x = 1；并且 b=1。</p><p id="1dc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来求函数的导数吧！</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="4aaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们开始推导这个表达式之前，必须把它转换成一个计算图形。一个计算图简单地将每个操作变成一个<strong class="lb iu">节点</strong>，并通过<strong class="lb iu"> </strong>线将它们连接起来，称为<strong class="lb iu">边</strong>。我们的示例函数的计算图如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/973ee8557207b458ecd76365f1902901.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*W6-39saZm_QqL-wQvGESGQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 2: Computation graph for the scalar cost function</figcaption></figure><p id="495d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们计算每个节点的值，从底部(输入变量)传播到顶部(输出函数)。这是我们得到的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/fe3d4af2ac79c0aae864ffe464910b55.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*yStTdUqFp2FcYbY9UVAXrw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 3: Values of each node</figcaption></figure><p id="ce4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要计算运算之间每个连接的偏导数，用边来表示。这些是每条边的偏角的计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mf"><img src="../Images/7410a187b3afa71ceb196b616185e7ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sBhdw3Dycs6hV7HhHrtBWg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 4: Values of each edge</figcaption></figure><p id="2e4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意 max(0，x)分段函数的部分也是分段函数。该函数将所有负值转换为零，并保持所有正值不变。函数的部分(或者用图形表示，它的斜率)，应该在它的图上很清楚:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/b6bda5ad1989bb60cf208a8d3f067ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*-_GDqRTH-5E7PQC027eA2g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 5: max(0, x) function. As seen, there is a slope of 1 when x&gt;0, and a slope of 0 when x&lt;0. The slope is undefined when x=0.</figcaption></figure><p id="2600" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以继续计算偏导数了！让我们找出相对于重量的偏导数。如图 6 所示，只有一条线将结果与权重联系起来。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/96cda590e095095954378795d7ea7188.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*ESfQYuqE-CWl58Xaun9H4g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 6: The path that connects the function to the weights</figcaption></figure><p id="7f95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们简单地将这些边相乘:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mh"><img src="../Images/cd5178f4cead92a829e07411a9d24c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*53HDeNScHx2zwkLPZ1vEhA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 7: Partial</figcaption></figure><p id="b002" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们的部分！</p><p id="148e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这整个过程可以自动完成，并允许计算机准确快速地计算函数值的偏导数。正是这个过程，让 AI 能够像今天这样高效。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="30bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你喜欢这篇文章，或者有任何问题或建议，欢迎在下面留下评论！</p></div></div>    
</body>
</html>