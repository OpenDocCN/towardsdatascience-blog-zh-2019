<html>
<head>
<title>Review: Tompson NIPS’14 — Joint Training of CNN and Graphical Model (Human Pose Estimation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:Tompson NIPS ' 14—CNN 和图形模型的联合训练(人体姿态估计)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=collection_archive---------18-----------------------#2019-03-08">https://towardsdatascience.com/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=collection_archive---------18-----------------------#2019-03-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7c1a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">又名{多人/人/人/人体} {姿势估计/关键点检测}，NYU</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4707fc80db32de5b9007f1b2b614a156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZCDGtjV-I0ObzfVe.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">FLIC (Frames Labeled In Cinema) Dataset for Human Pose Estimation (</strong><a class="ae kw" href="https://bensapp.github.io/flic-dataset.html" rel="noopener ugc nofollow" target="_blank">https://bensapp.github.io/flic-dataset.html</a>)</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/11c7c30b3766bcb56050e711dc756627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xFgQfc_34JobhZHx.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">FLIC Plus Dataset for Human Pose Estimation</strong></figcaption></figure><p id="3fa9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di">在</span>这个故事中，<strong class="kz ir"> NYU </strong>的《<strong class="kz ir">卷积网络和人体姿态估计图形模型的联合训练</strong>》做了简要回顾。本文中的方法似乎没有简写形式。由于在 2014 NIPS 的<strong class="kz ir">论文中第一作者的名字是汤普逊，所以我就在标题上称之为<strong class="kz ir">汤普逊 NIPS’14</strong>。你可能会注意到，这是 NYU 写的，也是勒村教授的论文之一。而这是一篇超过<strong class="kz ir"> 600 次引用</strong>的论文。(<a class="mc md ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----95016bc510c--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</strong></p><p id="aa88" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">人体姿态估计的目标是定位人体关节。</strong>有很多困难，比如关节遮挡、体型变化、服装、灯光、视角等。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="639b" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">概述</h1><ol class=""><li id="0719" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls nk nl nm nn bi translated"><strong class="kz ir">零件检测器:模型架构</strong></li><li id="308f" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">空间模型:关节间的消息传递</strong></li><li id="8450" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">结果</strong></li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="3d2b" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">1.零件检测器:模型架构</h1><h2 id="564f" class="nt mm iq bd mn nu nv dn mr nw nx dp mv lg ny nz mx lk oa ob mz lo oc od nb oe bi translated">1.1.从滑动窗口到全图像卷积—单一分辨率</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/deecfdf58ebdb34f77731f26ca3ef726.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TswuixxYwZgHP5qunDVevw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">From Sliding Window to Full Image Convolution — Single Resolution</strong></figcaption></figure><ul class=""><li id="dd2f" class="nd ne iq kz b la lb ld le lg og lk oh lo oi ls oj nl nm nn bi translated">作者通过<strong class="kz ir">对完整图像</strong>执行卷积而不是使用滑动窗口卷积来改进 2013 ICLR 架构。</li><li id="93b5" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls oj nl nm nn bi translated">此外，不是使用全连接层来预测 4 个接头(红、黄、绿和蓝)，而是使用<strong class="kz ir"> 1×1 卷积</strong>来替换全连接层，以将特征图的数量从 512 个减少到 4 个。因此，<strong class="kz ir">最后，为 4 个关节</strong>生成了 4 个热图。</li></ul><h2 id="68b3" class="nt mm iq bd mn nu nv dn mr nw nx dp mv lg ny nz mx lk oa ob mz lo oc od nb oe bi translated">1.2.从滑动窗口到全图像卷积—多分辨率</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/05d0809922d4e5d374378c4d18c2d3c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o33hSSm3xI2slqX03zAyDg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">From Sliding Window to Full Image Convolution — Multi-Resolution</strong></figcaption></figure><ul class=""><li id="a498" class="nd ne iq kz b la lb ld le lg og lk oh lo oi ls oj nl nm nn bi translated">此外，为了提高网络的鲁棒性，使用了多分辨率输入图像。与顶部的相比，底部不需要两次滑动窗口卷积。对于底部的网络，在对输入图像进行下采样之后，我们得到 2 个输入图像，网络可以一次性执行。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="5174" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 2。空间模型:关节间的消息传递</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/31c4ca91cb244f007d5322fb6dcdd4f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dgx6nBSx2nnUHMU0LLKrjw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Message Passing Between Face and Shoulder Joints</strong></figcaption></figure><ul class=""><li id="3f5d" class="nd ne iq kz b la lb ld le lg og lk oh lo oi ls oj nl nm nn bi translated">一个关节位置可以帮助细化另一个关节位置，例如，通过肩部位置，我们可以知道在正常情况下面部应该在肩部之上。基于这一思想，可以消除假阳性。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/d2bd9e6b076045c478220bc7fe47b83f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vx2QuS5h6U8gAF5wLRRr6g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Single Round Message Passing Network</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c1b39595bacffeef7059c2f19f55f6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/0*cNhNPbptVq8NpjtC.gif"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">SoftPlus and ReLU</strong></figcaption></figure><ul class=""><li id="0e9f" class="nd ne iq kz b la lb ld le lg og lk oh lo oi ls oj nl nm nn bi translated"><strong class="kz ir">作者设计了一个消息传递网络来逼近马尔可夫随机场(MRF)。</strong>该网络用于<strong class="kz ir">模拟一元电位和成对电位</strong>的估计，以完善结果。</li><li id="1cd1" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls oj nl nm nn bi translated">使用 SoftPlus 代替 ReLU。两者具有相似的效果，但是 SofPlus 可以提供平滑的曲线。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="d330" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">3.结果</h1><h2 id="b1c8" class="nt mm iq bd mn nu nv dn mr nw nx dp mv lg ny nz mx lk oa ob mz lo oc od nb oe bi translated">3.1.培养</h2><ul class=""><li id="213b" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls oj nl nm nn bi translated">首先单独训练零件检测器，并存储热图输出。然后，这些热图被用来训练一个空间模型。最后，将经过训练的部分检测器和空间模型结合起来，用于整个网络的反向传播。</li></ul><h2 id="d6fe" class="nt mm iq bd mn nu nv dn mr nw nx dp mv lg ny nz mx lk oa ob mz lo oc od nb oe bi translated">3.2.新的 FLIC <strong class="ak">(电影中标记的帧)</strong>加上数据集</h2><ul class=""><li id="0455" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls oj nl nm nn bi translated">为了公平起见，作者对原始 FLIC 数据集进行了微调，以生成新的 FLIC Plus 数据集，使其不与测试集重叠。</li></ul><h2 id="9caa" class="nt mm iq bd mn nu nv dn mr nw nx dp mv lg ny nz mx lk oa ob mz lo oc od nb oe bi translated">3.3.估价</h2><ul class=""><li id="a6b3" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls oj nl nm nn bi translated">对于给定的归一化像素半径，评估测试集中预测关节位置到地面实况位置的距离落在给定半径内的图像的数量。</li></ul><h2 id="7346" class="nt mm iq bd mn nu nv dn mr nw nx dp mv lg ny nz mx lk oa ob mz lo oc od nb oe bi translated">3.4.FLIC 测试集和 LSP 测试集</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/2e0d4c47e7ea840f8e522ed3b5f3e557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RTSVGRZPwCS9pzDX1U9vCA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">FLIC Test Set (Left &amp; Middle) &amp; LSP (Leeds Sports Pose) Test Set (</strong><a class="ae kw" href="http://sam.johnson.io/research/lsp.html" rel="noopener ugc nofollow" target="_blank">http://sam.johnson.io/research/lsp.html</a>) <strong class="bd kv">(Right)</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/8bc816aa50fdc315dbab33ec1e192167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HfQUE0FLQjjLhj7sq2BROw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">LSP Test Set (Left) &amp; Further Study on Proposed Models</strong></figcaption></figure><ul class=""><li id="1124" class="nd ne iq kz b la lb ld le lg og lk oh lo oi ls oj nl nm nn bi translated">曲线越高，模型越好。</li><li id="7028" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls oj nl nm nn bi translated">我们可以看到，对于 FLIC(顶行，左和中间)和 LSP(右上和左下)测试集，与其他方法相比，所提出的方法(我们的方法)具有较大的优势。</li><li id="8745" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls oj nl nm nn bi translated">进一步研究空间模型和联合训练的有效性(底部中间)，我们可以看到两个网络的联合训练具有最高的检测率。</li><li id="0c27" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls oj nl nm nn bi translated">此外，使用 3 种分辨率的图像输入网络会产生最佳结果(右下角)。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/3cf5bdc32df4cf39ee68c9b17c4f7208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7l8hgBH0yAaxiYuINmWNAQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Predicted Joint Positions, Top Row: FLIC Test-Set, Bottom Row: LSP Test-Set</strong></figcaption></figure></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="ab25" class="nt mm iq bd mn nu nv dn mr nw nx dp mv lg ny nz mx lk oa ob mz lo oc od nb oe bi translated">参考</h2><p id="abdb" class="pw-post-body-paragraph kx ky iq kz b la nf jr lc ld ng ju lf lg or li lj lk os lm ln lo ot lq lr ls ij bi translated">【2014 NIPS】【Tompson NIPS’14】<br/><a class="ae kw" href="https://arxiv.org/abs/1406.2984" rel="noopener ugc nofollow" target="_blank">卷积网络和人体姿态估计图形模型的联合训练</a></p><h2 id="803d" class="nt mm iq bd mn nu nv dn mr nw nx dp mv lg ny nz mx lk oa ob mz lo oc od nb oe bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kx ky iq kz b la nf jr lc ld ng ju lf lg or li lj lk os lm ln lo ot lq lr ls ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(是)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(情)(况)(。</p><p id="8b77" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">物体检测<br/></strong><a class="ae kw" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae kw" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae kw" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae kw" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae kw" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae kw" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae kw" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a><a class="ae kw" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">ION</a><a class="ae kw" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">multipath Net</a>【T21 [ <a class="ae kw" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae kw" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae kw" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3 </a> ] [ <a class="ae kw" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a> ] [ <a class="ae kw" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a> ] [ <a class="ae kw" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44"> DCN </a> ]</p><p id="6582" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">语义切分<br/></strong><a class="ae kw" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae kw" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae kw" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a>】<a class="ae kw" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae kw" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae kw" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae kw" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae kw" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a><a class="ae kw" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a>]</p><p id="fc65" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生物医学图像分割<br/></strong>[<a class="ae kw" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae kw" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae kw" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae kw" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae kw" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae kw" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a></p><p id="3134" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> 实例分段 <br/> </strong> <a class="ae kw" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a> <a class="ae kw" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a> <a class="ae kw" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413"> MultiPathNet </a> <a class="ae kw" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> <a class="ae kw" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> InstanceFCN </a> <a class="ae kw" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a>】</p><p id="58de" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p></div></div>    
</body>
</html>