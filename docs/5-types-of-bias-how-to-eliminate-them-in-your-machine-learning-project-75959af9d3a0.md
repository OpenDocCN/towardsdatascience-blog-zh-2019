# 5 种偏见&如何在你的机器学习项目中消除它们

> 原文：<https://towardsdatascience.com/5-types-of-bias-how-to-eliminate-them-in-your-machine-learning-project-75959af9d3a0?source=collection_archive---------5----------------------->

## [**现实世界中的数据科学**](https://medium.com/towards-data-science/data-science-in-the-real-world/home)

## 样本，排除，观察者，偏见，测量偏差。每一个的介绍和例子！

![](img/be5681c27189075268674669969c96a2.png)

Photo by rawpixel.com from Pexels

以下是发生在现实生活中的关于有偏见的机器学习程序的毁灭性事实。可以肯定地说，以下是种族主义仍然存在的原因的一个例子。我想以此开始，向你展示在你的人工智能程序中修正任何偏见是多么重要。

# 康巴丝

*由一家名为 Equivant(前身为 Northpointe)的私人公司开发。* Compas 是一种预测被告犯罪可能性的机器学习算法，已经证明它对谁更有可能再次犯罪做出了有偏见的预测。来自 ProPublica 的研究发现，该工具错误地将黑人被告称为重新犯罪高风险的可能性是 2 倍，错误地预测白人被告重新犯罪低风险的可能性也是 2 倍。
*在这里* *找到 ProPublica* [*的完整分析。*](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)

美国超过 12 个州的法官使用 Compas，它被用作许多事情的工具，如确定监狱中的人是否应该在审判前保释，对囚犯的监督类型，以及对人们刑期的影响。
**这种算法的后果非常真实。**

![](img/030ab0b7e0199531be50a9567309c37f.png)

Photo by rawpixel.com from Pexels

当然，我们不能说这是故意的。建造 Compas 的工程师不太可能在系统中加入偏见，而更有可能的是 Compas 是在一个没有暴露于包括肤色在内的不同面部的数据集上训练的。

假设并应用其创造者或其数据的偏见的编程是有偏见的编程。更危险的是，人们发现很多这种“偏见”是无意识的，因为你不一定知道当你制作算法时，它会产生不正确的结果。许多算法也是一个黑盒，你只是使用现成的，这意味着如果你不直接开发算法，你不知道里面有什么，你肯定无法评估它们是否无偏。

# 史前古器物

工件是由数据收集过程中的缺陷引起的人为模式。
**数据分析尽量区分事实和工件。**

# 5 种常见的偏见

# 单样本偏差

**当收集的数据不能准确代表程序预期运行的环境时发生。**
没有一种算法可以在整个数据宇宙上训练，而不是在一个精心选择的子集上训练。

选择这个足够大和足够有代表性的子集来减轻样本偏差是有科学依据的。

***示例:安全摄像机***如果您的目标是创建一个可以在白天和夜间操作安全摄像机的模型，但只对夜间数据进行训练。你在模型中引入了样本偏差。

***样本偏差可以通过*** 减少或消除

*   在白天和晚上训练你的模型。
*   涵盖了所有你期望你的模型会遇到的情况。这可以通过检查每个特性的域来完成，并确保我们拥有覆盖所有特性的平衡均匀分布的数据。否则，您将面临错误的结果，并且会产生没有意义的输出。

# 2-排除偏差

**通常在清理数据的保护伞下，从我们的数据集中排除一些特征而发生。** 我们删除了一些特征，认为它们与我们基于已有信念的标签/输出无关。

***举例:泰坦尼克号生存预测*** 在著名的泰坦尼克号问题中我们预测谁幸存了谁没了。人们可能会忽略旅行者的乘客 id，因为他们可能认为这与他们是否幸存完全无关。他们不知道泰坦尼克号上的乘客是根据他们的身份证被分配房间的。身份证号码越小，他们被分配的房间离救生艇越近，这使得那些人能够比那些在泰坦尼克号中心深处的人更快地到达救生艇。因此，随着 id 的增加，存活率降低。
*id 影响标签的假设并不是基于实际的数据集，我只是公式化的举个例子。*

***排除偏倚可以通过*** 来减少或消除

*   通过对特征进行充分的分析，在丢弃特征之前进行调查。
*   请一位同事调查你正在考虑放弃的特征，一双全新的眼睛肯定会有所帮助。
*   如果时间/资源不足，需要通过丢弃要素来缩减数据集的大小。在删除任何内容之前，请确保搜索该功能与您的标签之间的关系。最有可能的是，你会找到相似的解决方案，调查它们是否考虑了相似的特性，然后做出决定。
*   比这更好，因为人类容易受到偏见的影响。**有工具可以帮助**。看看这篇[文章](/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e)(通过一个随机森林的例子解释特征重要性)，包含了计算特征重要性的各种方法。包含不需要大量计算资源的方法的方法。

# 3-观察者偏差(又名实验者偏差)

**倾向于看到我们期望看到的，或者我们想看到的。**研究者在研究某个群体时，通常是带着对被研究群体的先验知识和主观感受来进行实验的。换句话说，他们带着有意识或无意识的偏见来到谈判桌前。

***举例:智力受地位影响吗？——伯特事件*** 观察者偏见的一个著名例子是心理学家[西里尔·伯特](https://en.wikipedia.org/wiki/Cyril_Burt)的工作，他以研究智商的遗传性而闻名。他认为，与社会经济地位较高的儿童相比，社会经济地位较低的家庭的儿童(即工人阶级的儿童)也更有可能智力较低。他所谓的科学的智力测试方法是革命性的，据说证明了工人阶级的孩子普遍不太聪明。这导致了 20 世纪 60 年代英国双重教育体系的产生，中产阶级和上层阶级的孩子被送到精英学校，而工人阶级的孩子被送到不太理想的学校。当然，伯特的研究后来被揭穿，结论是他伪造了数据。现在人们公认智力是遗传的而不是遗传的。

***观察者偏差可以通过*** 来减少或消除

*   确保观察者(进行实验的人)训练有素。
*   筛选潜在偏见的观察者。
*   有明确的实验规则和程序。
*   确保行为被清楚地定义。

![](img/12c2a8ce61a83f54fe62e6377b862a2e.png)

Source: [Pixabay](http://pixabay.com)

# 4-偏见

文化影响或刻板印象的结果。当我们在现实中不喜欢的东西，如根据外表判断，社会阶级，地位，性别等等，在我们的机器学习模型中不固定时。当这个模型应用现实生活中由于偏见数据而存在的相同的刻板印象时，它就被灌输了。

***举例:检测工作中的人的计算机视觉程序***如果你的目标是检测工作中的人。你的模型已经输入了成千上万的训练数据，男人在编码，女人在做饭。算法很可能学习到编码员是男的，女的是厨师。这是错误的，因为女人可以编码，男人可以做饭。

这里的问题是，数据有意无意地反映了刻板印象。

***偏见偏见可以通过*** 来减少或消除

*   忽略了性别和职业之间的统计关系。
*   将算法暴露给更公平的样本分布。

# 5-测量偏差

当用于观察或测量的设备出现问题时，会发生系统值失真**。这种偏差往往会使数据向某一特定方向倾斜。**

***示例:用增加亮度的相机拍摄图像数据。*** 这个乱七八糟的测量工具没能复制出模型将要运行的环境，换句话说，它把它的训练数据搞得乱七八糟，以至于它不再代表它启动时将要处理的真实数据。

这种偏差不是简单的通过收集更多的数据就能避免的。

***测量偏差可以通过*** 减少或消除

*   具有多个测量装置。
*   雇佣受过训练的人来比较这些设备的输出。

# 在产品开发周期中增加偏差测试

## 1- FairML

预测建模偏差诊断工具箱。它审核它们并确定输入的重要性
[查看更多](https://dspace.mit.edu/handle/1721.1/108212)

## 2-石灰

这里的是一个很棒的介绍

> 扰乱输入，看看预测如何变化。这在解释能力方面证明是有益的，因为我们可以通过改变对人类有意义的组件(例如，单词或图像的一部分)来干扰输入，即使模型使用更复杂的组件作为特征(例如，单词嵌入)。