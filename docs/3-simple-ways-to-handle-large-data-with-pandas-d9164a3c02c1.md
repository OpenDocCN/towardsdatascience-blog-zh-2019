# 用熊猫处理大量数据的 3 种简单方法

> 原文：<https://towardsdatascience.com/3-simple-ways-to-handle-large-data-with-pandas-d9164a3c02c1?source=collection_archive---------4----------------------->

## 熊猫喜欢吃数据

![](img/440610af6551c9448c4981cd4d2b0dbb.png)

Pandas love eating data

> 想获得灵感？快来加入我的 [**超级行情快讯**](https://www.superquotes.co/?utm_source=mediumtech&utm_medium=web&utm_campaign=sharing) 。😎

熊猫已经成为最受欢迎的数据科学图书馆之一。它易于使用，文档非常棒，功能非常强大。

然而，无论使用什么库，大型数据集总是带来额外的挑战，需要小心处理。

你开始遇到硬件障碍，因为你没有足够的内存来保存所有的数据。企业公司存储高达数百甚至数千 GB 的数据集。

即使你碰巧买了一台有足够内存来存储所有数据的机器，只是把它读入内存也是非常慢的。

但是熊猫图书馆将再次帮助我们。本文将讨论 3 种可以用来减少大型数据集的内存占用和读入时间的技术。我将这些技术用于大小超过 100GB 的数据集，将它们压缩到具有 64 GB(有时是 32GB)RAM 的机器上。

让我们检查一下！

# (1)将你的数据分块

CSV 格式是一种非常方便的数据存储方式，既易于书写，又易于阅读。此外，还有一个很好的 pandas 函数`read_csv()`用于加载以 CSV 格式存储的数据。

但是当你的 CSV 大到内存不足的时候会发生什么呢？

有一个非常简单的熊猫技巧来解决这个问题！我们不会试图一次性处理所有数据，而是将数据分成几部分来处理。通常，这些块被称为*块*。

一个块只是我们数据集的一部分。我们可以随心所欲地将这一大块变大或变小。这取决于我们有多少内存。

然后，该过程如下进行:

1.  整体阅读
2.  处理大块
3.  保存块的结果
4.  重复步骤 1 到 3，直到我们得到所有的块结果
5.  合并块结果

我们可以使用一个叫做 **chunksize** 的`read_csv()`函数的便利变量来执行上述所有步骤。chunksize 指的是熊猫一次将读取多少 CSV 行。这当然取决于你有多少内存和每行有多大。

如果我们认为我们的数据有一个非常容易处理的分布，比如高斯分布，那么我们可以一次对一个数据块进行我们想要的处理和可视化，而不会损失太多的准确性。

如果我们的分布有点像泊松分布那样复杂，那么最好在行进之前过滤每个块并将所有的小块放在一起。大多数情况下，您最终会删除许多不相关的列或缺少值的行。我们可以对每个数据块都这样做，使它们变得更小，然后将它们放在一起，在最终的数据帧上执行我们的数据分析。

下面的代码执行所有这些步骤。

# (2)丢弃数据

有时，我们会马上知道我们要分析数据集的哪些列。事实上，经常会出现这样的情况，有几个或几个我们并不关心的列，如姓名、账号等。

在读入数据之前直接跳过列可以节省大量内存。Pandas 允许我们指定想要阅读的栏目:

丢弃包含无用信息的列将是您最大的内存节省之一。

我们可以做的另一件事是过滤掉任何缺少值或 NA 值的行。使用`dropna()`功能最简单:

有几个真正有用的变量我们可以传递给`dropna()`:

*   **如何:**这将允许您指定“any”(如果某一行的任何列是 NA，则删除该行)或“all”(仅当某一行的所有列都是 NA 时，才删除该行)
*   **thresh:** 设置一个阈值，确定一行需要删除多少 NA 值
*   **subset:** 选择检查 NA 值时要考虑的列的子集

您可以使用这些参数，尤其是 *thresh* 和 *subset* 来具体确定哪些行将被删除。

Pandas 没有像对列那样在读取时实现这一点的方法，但是我们可以像上面那样在每个块上实现这一点。

# (3)为每一列设置具体的数据类型

对于许多初学数据的科学家来说，数据类型并没有被考虑太多。但是一旦开始处理非常大的数据集，处理数据类型就变得至关重要。

标准的做法是读入数据帧，然后根据需要转换列的数据类型。但是对于一个大的数据集，我们真的需要注意内存空间。

在我们的 CSV 中可能会有一些列，比如浮点数，它们会占据比实际需要更多的空间。例如，如果我们下载了一个预测股票价格的数据集，我们的价格可能会保存为 32 位浮点！

但是我们*真的*需要 32 浮动吗？大多数时候，股票是以两位小数指定的价格买入的。即使我们想做到真正的精确，float16 已经足够了。

因此，我们不是用列的原始数据类型读入数据集，而是指定我们希望 pandas 在我们的列中使用的数据类型。这样，我们就不会用完超过实际需要的内存。

这很容易通过使用`read_csv()`函数中的 **dtype** 参数来完成。我们可以指定一个字典，其中每个键都是数据集中的一列，每个值都是我们希望使用该键的数据类型。

这里有一个熊猫的例子:

我们的教程到此结束！希望这三个建议能帮你节省很多时间和内存！

# 喜欢学习？

在 [twitter](https://twitter.com/GeorgeSeif94) 上关注我，我会在那里发布所有最新最棒的人工智能、技术和科学！也在 LinkedIn 上与我联系！