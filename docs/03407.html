<html>
<head>
<title>Bias in the AI court decision making — spot it before you fight it</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能法庭决策中的偏见——在对抗之前发现它</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bias-in-the-ai-court-decision-making-spot-it-before-you-fight-it-52acf8903b11?source=collection_archive---------12-----------------------#2019-05-31">https://towardsdatascience.com/bias-in-the-ai-court-decision-making-spot-it-before-you-fight-it-52acf8903b11?source=collection_archive---------12-----------------------#2019-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1abea25a271c646650ea93401a53dc45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BakEuqq3VKibn7ESzTDZfA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Image by <a class="ae kf" href="https://pixabay.com/users/TPHeinz-2541163/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3667391" rel="noopener ugc nofollow" target="_blank">TPHeinz</a> from <a class="ae kf" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3667391" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><h1 id="fb26" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">法院判决中的机器学习</h1><p id="9d0c" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在不同的决策过程中，包括在司法实践中，机器学习的使用越来越频繁。由于法院判决对个人的个人和职业生活以及整个社会都有很大的影响，因此能够识别并理想地纠正人工智能(AI)系统中的偏见以避免该模型做出不公平或不准确的决定，从而潜在地放大我们社会中现有的不平等是很重要的。</p><h2 id="e0bd" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">优势</h2><p id="e374" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在法庭决策中使用机器学习的目标应该是使决策和决策过程更好，即更准确、公正、更快和成本更低。或许令人惊讶的是，AI 模型实际上可以帮助法官发现并打击自己的偏见。该系统可以根据历史统计数据，在法官的语言中检测到他或她注意力不集中、即将做出仓促决定或缺乏同情心时，向法官发出警告。该模型可以通过加权外部因素来实现这一点，这些因素可能会影响决策，如一天中的时间、温度甚至选举期即将到来的事实。正如丹尼尔·卡内曼在他的《思考，快与慢》中所描述的那样，在对以色列假释法官的研究中，这些外部因素对法官决策的影响已经显现出来。被观察的法官将遭受所谓的葡萄糖耗竭，这导致他们更倾向于在用餐休息后批准假释，每次用餐后批准率达到 65 %，而平均只有 35 %的假释请求获得批准。</p><h2 id="f9bf" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">现状</h2><p id="f657" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">显然，在目前的技术阶段，只有某些法院判决或部分法院判决可以由算法做出。人工智能已被用于所谓的预测性警务，在这种情况下，基于可用的数据，算法可以帮助警方或法院就案件的特定方面做出决定。例如，这可以是批准假释、决定保释和确定适当的刑期。例如，法院可以使用这种软件来评估被告在假释期间再次犯罪的风险，如果获准保释，他是否会出庭，或者是否应该考虑缓刑。此外，机器学习也被用于判决的实际渲染。这些案件通常涉及小的民事法律纠纷，包括推翻或决定停车罚款。爱沙尼亚最近推出了试点机器人法官，将裁决涉及小额索赔的纠纷。</p><h1 id="e858" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">人工智能法院判决中的偏见</h1><p id="f9f8" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">尽管优势很多，并且随着技术的进步，可能是无限的，但在法庭决策中部署人工智能的风险规避方法是至关重要的。在启动任何算法来代替人类法官做出裁决之前，我们必须确保它做出的决定至少与人类法官一样公正合理。欧盟委员会人工智能高级专家组认为，当一个人工智能系统合法、符合道德规范且强健时，它就是值得信赖的。</p><p id="e812" class="pw-post-body-paragraph le lf it lg b lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb im bi translated">当谈论道德人工智能时，需要考虑的最大问题是算法本身或数据中是否存在<strong class="lg iu">偏差</strong>，无论是有意识的还是无意识的，因为它会影响和扭曲计算和预测过程。</p><ul class=""><li id="9e3a" class="mt mu it lg b lh mo ll mp lp mv lt mw lx mx mb my mz na nb bi translated"><strong class="lg iu">偏差可能是采样和测量中的错误造成的，导致数据不完整，基于太少的数据或仅仅是错误的。</strong>这是一种由于数据收集或生产过程的疏忽而导致使用错误数据的情况。这种偏差在理论上可以通过重新操作数据收集和生产系统并包括丢失的数据或替换坏的数据来纠正。然而，如果数据丢失是因为它根本就不存在，那么纠正偏差将是一项更加困难的任务。例如，当某些类型的犯罪实际上没有受到调查，因而一群罪犯由于警察的偏见做法而没有受到起诉时，就会发生这种情况。</li><li id="70f8" class="mt mu it lg b lh nc ll nd lp ne lt nf lx ng mb my mz na nb bi translated"><strong class="lg iu">这些数据本身也可能带有反映社会不平等的偏见。目前最基本的偏见涉及种族和性别不平等，以及与个人的社会背景和性取向有关的不平等。这可以反映在内容本身以及数据的语言中，但不必明确提及。例如，案件事实和情况或被告行为的描述方式可能带有被告种族或社会阶层的信息偏见。根据种族不容忍程度较高的地区的历史数据预测与某一被告行为相关的风险的算法，可能反映出执法部门不成比例地针对非裔美国人，导致在收集数据的最终民意调查中这种数据的比例过高。</strong></li><li id="c889" class="mt mu it lg b lh nc ll nd lp ne lt nf lx ng mb my mz na nb bi translated">此外，<strong class="lg iu">当数据反映了欺诈性信息、伪造的文件、伪造的证据或其他被操纵或非法的事实或受其影响时，不干净的数据(与干净的高质量数据相反)也可能导致偏差</strong>。这种偏见，如果被发现，会将整个人工智能系统的使用转移到一个潜在的非法领域，并增加开发者和用户的责任风险。</li></ul><h1 id="6703" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">对抗偏见</h1><p id="bf4b" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一个建立在并使用坏的或肮脏的数据之上的模型有可能通过增加其工作产出(决策)与社会价值(平等待遇、公正程序等)之间的脱节，在社会中进一步传播歧视和不平等。)，最终做出不公正或不准确的决定。因此，在人工智能系统部署之前或在更糟的情况下部署之后，发现并消除不公平的偏见(与故意引入的偏见相反)至关重要。</p><p id="1550" class="pw-post-body-paragraph le lf it lg b lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb im bi translated">由于包括法院在内的公共领域使用的人工智能系统主要是由私营公司开发的，除非以这种方式明确编程，否则它们并不带有保护司法或人权的内在承诺。换句话说，这个系统是不道德的，除非它是如此。</p><h2 id="2b26" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">AI 模型尽职调查</h2><p id="ff09" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">理想的情况是，由于人工智能对我们的生活和我们的基本人权具有巨大的潜在影响，人工智能将受到与其他重要部门类似(如果不是更严格的话)的监管，例如空中交通、卫生系统或法律实践。目前，在国家、欧洲和国际层面上有许多倡议正在进行，以界定人工智能监管的关键原则(见经合组织人工智能原则)。然而，目前任何定义明确的监管制度与其说是短期内可以实现的，不如说是乌托邦。与此同时，为了确保现在部署的人工智能系统能够尽可能公正和准确地做出决定，软件必须接受 T2 持续的审计。</p><p id="4b64" class="pw-post-body-paragraph le lf it lg b lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb im bi translated">无论它在首次部署时看起来有多准确，使用它的法院都必须持续确保它始终如一地执行，并做出公平的裁决，将质量与人类法官的裁决进行比较。虽然引入一种纠正措施来消除已识别的偏差在理论上是可行的，但<a class="ae kf" href="https://poseidon01.ssrn.com/delivery.php?ID=816116074064003023081023089074028081024036042028091005105067127026092075091074087126009003116014006009030064112028104107096087114026028080092007066103116104102084090081005076082069121126024108092103083065120073119123079123095096079090102108101006007068&amp;EXT=pdf" rel="noopener ugc nofollow" target="_blank">的实践表明</a>通过允许人为引入一些变量来使人工智能系统合乎伦理是非常困难的。</p><h2 id="a4f8" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">个案尽职调查</h2><p id="3ed7" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在对人工智能模型本身进行审核之后，整个人工智能法院决策模型的第二层是确保它从各个相关方的角度来看是可验证的。我们不应该忘记，人工智能系统没有感觉或关心，因此，如果它做出不公正的决定，它不会意识到这一点。此外，它也没有说明为什么会有这样或那样的裁决。因此，从本质上来说，它缺乏在案件当事人眼中值得信赖的核心价值——可解释性。</p><p id="0f11" class="pw-post-body-paragraph le lf it lg b lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb im bi translated"><strong class="lg iu"> AI 可解释性或可解释性</strong>指的是一个尽职调查过程，该过程使有关各方能够要求对机器学习决策背后有法律或其他重大影响的解释，并可能对其提出质疑。尽管这是一般审计义务的一部分，但这一措施也要求各方有权在可能和合理的范围内访问人工智能模型使用的数据和生成的信息。</p><p id="7f74" class="pw-post-body-paragraph le lf it lg b lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb im bi translated">在实践中，揭示人工智能模型做出的决策的推理过程可能并不总是容易，甚至是不可能的。通常，可解释性最高级别的预测模型，如决策树和分类规则，缺乏预测准确性，反之亦然-神经网络通常非常准确，但对于它们如何进行计算却非常不透明。然而，即使在使用深度学习时，对 AI 模型的整体审计以及确保尽可能高的决策过程可追溯性将提供某种程度的透明度，从而具有可解释性。</p><h2 id="d2b6" class="mc kh it bd ki md me dn km mf mg dp kq lp mh mi ku lt mj mk ky lx ml mm lc mn bi translated">人工智能培训</h2><p id="9056" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">用于法院决策的人工智能系统通常不是由法院自己开发或实现的，而是由外包开发商或供应商开发或实现的。因此，决策者和相关方往往不了解和不理解系统是如何工作的，以及它是根据什么标准做出决策的。此外，人工智能系统可能不会完全取代人类法官和律师，而只是对他们进行补充。</p><p id="bd4d" class="pw-post-body-paragraph le lf it lg b lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb im bi translated">在这两种情况下，如果法官和律师对所使用的人工智能模型、输入变量和预测方法有很好的理解，或者至少有一些理解，这将是有益的。现在用神经网络训练所有的律师和法官既不可行，也没有必要。然而，作为第一步，我们可以集中精力进行培训，以发现常规决策中的偏见和肮脏数据，从而使法律专业人员对歧视性语言或欺诈性数据的使用更加警觉。更高的偏见意识，结合对机器学习过程及其优势和局限性的基本理解，可以帮助相关各方对通过尽职调查提供的信息有所了解，并改善迄今为止不完善的人工智能模型在决策中的使用。</p><h1 id="345d" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">系统性变化</h1><p id="0e3c" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">即使在软件部署之前和使用期间进行深入的尽职调查，以发现和修复任何潜在的偏差，也不足以做出完美、准确和公正的决策。收集到的数据中的偏差很有可能反映出社会中现存的不公正和不平等。除非在创建数据集时特别关注这种文化和社会规范和陈规定型观念，或者错误和不道德的执法做法和政策，除非有目的地纠正这些做法和政策，否则我们只会面临进一步扩大其偏见的风险。</p><p id="243e" class="pw-post-body-paragraph le lf it lg b lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb im bi translated">只有在数据收集、制作和标记过程以及算法的使用有明确的规则、得到理解和监督的情况下，才能确保参与决策的所有行为者，无论是法官、律师、书记员还是执法当局，都努力确保最高水平的准确性和公正性。</p><p id="831f" class="pw-post-body-paragraph le lf it lg b lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb im bi translated">如果我们无法部署一个无偏见的系统，或者一个让各方有足够理由相信它达成了公平决定的系统，一个中间解决方案可能是使用人工智能系统来补充人类决策。这样，我们可以加快司法程序，更深入地分析案件事实，或者在保护司法的同时节省费用。仅仅因为一项技术是可用的，并不意味着它应该取代现有的政策。</p></div></div>    
</body>
</html>