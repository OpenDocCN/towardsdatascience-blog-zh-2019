<html>
<head>
<title>Understanding Word N-grams and N-gram Probability in Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解自然语言处理中的单词 N 元语法和 N 元语法概率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058?source=collection_archive---------1-----------------------#2019-11-27">https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058?source=collection_archive---------1-----------------------#2019-11-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c55c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/the-fasttext-series" rel="noopener" target="_blank">快速文本系列</a></h2><div class=""/><div class=""><h2 id="67d0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">这并没有听起来那么难。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/761a5f636d5daa90027e96e11c0679b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WHJPETD3QbwufExsyMl1tw.jpeg"/></div></div></figure><blockquote class="la lb lc"><p id="bcc9" class="ld le lf lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最初发表于<a class="ae ma" href="https://blog.contactsunny.com/data-science/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing" rel="noopener ugc nofollow" target="_blank">我的博客</a>。</p></blockquote><p id="8ef3" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">N-gram 大概是整个机器学习领域最容易理解的概念了吧，我猜。一个 N-gram 意味着 N 个单词的序列。举例来说，“中型博客”是一个 2-gram(二元模型)，“中型博客文章”是一个 4-gram，而“写在介质上”是一个 3-gram(三元模型)。嗯，那不是很有趣或令人兴奋。没错，但是我们还是要看看 n-gram 使用的概率，这很有趣。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="a573" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">为什么是 N-gram 呢？</h1><p id="3e85" class="pw-post-body-paragraph ld le iq lg b lh nd ka lj lk ne kd lm mb nf lp lq mc ng lt lu md nh lx ly lz ij bi translated">在我们继续讲概率的东西之前，让我们先回答这个问题。为什么我们需要学习 n-gram 和相关的概率？嗯，在自然语言处理中，简称 NLP，n-grams 有多种用途。一些例子包括自动完成句子(如我们最近在 Gmail 中看到的)，自动拼写检查(是的，我们也可以这样做)，在一定程度上，我们可以检查给定句子的语法。我们将在后面的文章中看到一些例子，当我们谈论给 n 元文法分配概率的时候。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="8a72" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">n 元概率</h1><p id="e974" class="pw-post-body-paragraph ld le iq lg b lh nd ka lj lk ne kd lm mb nf lp lq mc ng lt lu md nh lx ly lz ij bi translated">让我们以一个句子完成系统为例。这个系统建议在给定的句子中接下来可以使用的单词。假设我给系统一句话“非常感谢你的”,并期望系统预测下一个单词是什么。现在你我都知道，下一个词是“救命”的概率非常大。但是系统怎么知道呢？</p><p id="94ff" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">这里需要注意的一点是，与任何其他人工智能或机器学习模型一样，我们需要用庞大的数据语料库来训练模型。一旦我们做到了这一点，系统或 NLP 模型将对某个单词在某个单词之后出现的“概率”有一个非常好的了解。因此，希望我们已经用大量数据训练了我们的模型，我们将假设模型给了我们正确的答案。</p><p id="9d15" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">我在这里讲了一点概率，但现在让我们在此基础上继续。当我们建立预测句子中单词的 NLP 模型时，单词在单词序列中出现的概率才是最重要的。我们如何衡量呢？假设我们正在使用一个二元模型，我们有以下句子作为训练语料:</p><ol class=""><li id="6bcd" class="ni nj iq lg b lh li lk ll mb nk mc nl md nm lz nn no np nq bi translated">非常感谢你的帮助。</li><li id="6454" class="ni nj iq lg b lh nr lk ns mb nt mc nu md nv lz nn no np nq bi translated">我非常感谢你的帮助。</li><li id="a59e" class="ni nj iq lg b lh nr lk ns mb nt mc nu md nv lz nn no np nq bi translated">对不起，你知道现在几点了吗？</li><li id="0787" class="ni nj iq lg b lh nr lk ns mb nt mc nu md nv lz nn no np nq bi translated">我真的很抱歉没有邀请你。</li><li id="0e0e" class="ni nj iq lg b lh nr lk ns mb nt mc nu md nv lz nn no np nq bi translated">我真的很喜欢你的手表。</li></ol><p id="a03e" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">让我们假设在用这些数据训练了我们的模型之后，我想写下句子“我真的很喜欢你的花园。”因为这是一个二元模型，该模型将学习每两个单词的出现，以确定某个单词出现在某个单词之后的概率。例如，从上面例子中的第二句、第四句和第五句，我们知道在单词“really”之后，我们可以看到单词“appreciate”、“sorry”或单词“like”出现。所以模型会计算这些序列中每一个的概率。</p><p id="f436" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">假设我们正在计算单词“w1”出现在单词“w2”之后的概率，则公式如下:</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="b1c4" class="ob mm iq nx b gy oc od l oe of"><em class="lf">count(w2 w1) / count(w2)</em></span></pre><p id="2731" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">其是单词在所需序列中出现的次数，除以该单词在预期单词在语料库中出现之前的次数。</p><p id="7448" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">从我们的例句中，让我们来计算“like”这个词出现在“really”这个词之后的概率:</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="e1f7" class="ob mm iq nx b gy oc od l oe of">count(really like) / count(really)<br/>= 1 / 3<br/>= 0.33</span></pre><p id="c7b6" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">同样，对于另外两种可能性:</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="0ef4" class="ob mm iq nx b gy oc od l oe of">count(really appreciate) / count(really)<br/>= 1 / 3<br/>= 0.33</span><span id="b20b" class="ob mm iq nx b gy og od l oe of">count(really sorry) / count(really)<br/>= 1 / 3<br/>= 0.33</span></pre><p id="b55d" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">因此，当我键入短语“我真的”，并期望模型建议下一个单词时，它只会在三次中得到一次正确答案，因为正确答案的概率只有 1/3。</p><p id="d6e3" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">作为另一个例子，如果我对模型的输入句子是“谢谢你的邀请”，并且我期望模型建议下一个单词，它将会给我单词“你”，因为例句 4。这是模型知道的唯一例子。你可以想象，如果我们给模型一个更大的语料库(或更大的数据集)来训练，预测将会改善很多。同样，我们在这里只使用二元模型。我们可以使用三元模型甚至四元模型来提高模型对概率的理解。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="a453" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">使用这些 n 元语法和某些单词在某些序列中出现的概率可以改进自动完成系统的预测。同样，我们使用 can NLP 和 n-grams 来训练基于语音的个人助理机器人。例如，使用 3-gram 或 trigram 训练模型，机器人将能够理解句子之间的差异，如“温度是多少？”和“设定温度”</p><p id="31e2" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">我希望这是一个足够清晰的解释，可以理解自然语言处理中 n 元语法这个非常简单的概念。我们将使用这种 n 元语法的知识，并使用它来<a class="ae ma" href="https://blog.contactsunny.com/data-science/optimising-a-fasttext-model-for-better-accuracy" rel="noopener ugc nofollow" target="_blank">优化我们的机器学习模型</a>用于文本分类，我们在早些时候的<a class="ae ma" href="https://blog.contactsunny.com/data-science/an-intro-to-text-classification-with-facebooks-fasttext-natural-language-processing" rel="noopener ugc nofollow" target="_blank">介绍 fastText 库</a>帖子中构建了该模型。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><blockquote class="la lb lc"><p id="8a51" class="ld le lf lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在<a class="ae ma" href="https://twitter.com/contactsunny" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我，了解更多<a class="ae ma" href="https://blog.contactsunny.com/tag/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>、<a class="ae ma" href="https://blog.contactsunny.com/tag/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，以及通用<a class="ae ma" href="https://blog.contactsunny.com/category/tech" rel="noopener ugc nofollow" target="_blank">技术更新</a>。还有，你可以<a class="ae ma" href="https://blog.contactsunny.com/" rel="noopener ugc nofollow" target="_blank">关注我的个人博客</a>。</p></blockquote><p id="78fa" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm mb lo lp lq mc ls lt lu md lw lx ly lz ij bi translated">如果你喜欢我在 Medium 或我的个人博客上的帖子，并希望我继续做这项工作，请考虑<a class="ae ma" href="https://www.patreon.com/bePatron?u=28955887" rel="noopener ugc nofollow" target="_blank">在 Patreon </a>上支持我。</p></div></div>    
</body>
</html>