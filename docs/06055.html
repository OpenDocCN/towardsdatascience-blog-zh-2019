<html>
<head>
<title>Modeling and Output Layers in BiDAF — an Illustrated Guide with Minions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BiDAF 中的建模和输出层——带 Minions 的图解指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/modeling-and-output-layers-in-bidaf-an-illustrated-guide-with-minions-f2e101a10d83?source=collection_archive---------14-----------------------#2019-09-03">https://towardsdatascience.com/modeling-and-output-layers-in-bidaf-an-illustrated-guide-with-minions-f2e101a10d83?source=collection_archive---------14-----------------------#2019-09-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="475a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">BiDAF 的权威指南——第 4 部分，共 4 部分</h2><div class=""/><div class=""><h2 id="61a2" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">BiDAF 是一个流行的机器学习模型，用于问答任务。本文借助一些可爱的小喽啰来解释 BiDAF 的建模层。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/92895ab50bc48d49e0fc51a164487eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cUrRfbRQ2Sr75fFz"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Source: <a class="ae lh" href="https://unsplash.com/photos/ltR0yi_YwFI" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5005" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">他的文章是一系列四篇文章中的最后一篇，旨在说明<a class="ae lh" href="https://arxiv.org/abs/1611.01603" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">【双向注意力流】</strong></a><strong class="lk jd"/>一种流行的问答机器学习模型(Q &amp; A)的工作原理。</p><p id="d592" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">概括地说，BiDAF 是一个<em class="mn">封闭领域，抽取的</em> Q &amp; A 模型。这意味着为了能够回答一个<strong class="lk jd">查询</strong>，BiDAF 需要查阅一个附带的文本，该文本包含回答查询所需的信息。这个附带的文本被称为<strong class="lk jd">上下文。</strong> BiDAF 的工作方式是提取上下文中最能回答查询的子串——这就是我们所说的对查询的<strong class="lk jd">回答</strong> <em class="mn"> </em>。我有意将“查询”、“上下文”和“回答”这几个词大写，以表明我是在用它们的专业技术能力来使用它们。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/fe68c61984cb40db0dec7c3625d3be9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*zSsrgM-btCoduYDHR6MsMg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">An example of Context, Query and Answer</figcaption></figure><p id="51ed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对前面三篇文章的简要总结如下:</p><ul class=""><li id="24bd" class="mp mq it lk b ll lm lo lp lr mr lv ms lz mt md mu mv mw mx bi translated"><a class="ae lh" href="https://medium.com/@meraldo.antonio/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b" rel="noopener">本系列的第 1 部分</a>提供了 BiDAF 的高级概述。</li><li id="7b0d" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated">第 2 部分解释了 BiDAF 如何使用 3 种嵌入算法来获得上下文和查询中单词的向量表示。</li><li id="7891" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/the-definitive-guide-to-bidaf-part-3-attention-92352bbdcb07">第 3 部分</a>探讨了 BiDAF 的注意力机制，它结合了来自上下文和查询的信息。</li></ul><p id="3832" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">前述注意步骤的输出是一个名为<strong class="lk jd"> G </strong>的巨型矩阵。<strong class="lk jd"> G </strong>是一个<strong class="lk jd"> 8d </strong> -by- <strong class="lk jd"> T </strong>矩阵，它对上下文单词的查询感知表示进行编码。G 是建模层的输入，这将是本文的重点。</p><h1 id="c821" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">我们一直在做什么？G 实际上代表什么？<em class="nv">喽啰来救援了！</em></h1><p id="ee43" class="pw-post-body-paragraph li lj it lk b ll nw kd ln lo nx kg lq lr ny lt lu lv nz lx ly lz oa mb mc md im bi translated">好的，我知道在过去的三篇文章中我们已经经历了很多步骤。人们很容易迷失在无数的符号和方程式中，尤其是考虑到 BiDAF 论文中的符号选择并不是那么“用户友好”我的意思是，你还记得<strong class="lk jd"> H，u，ĥ</strong>和<strong class="lk jd">ũ</strong>分别代表什么吗？</p><p id="4ce1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，现在让我们后退一步，试着得到我们到目前为止所做的所有这些矩阵运算背后的直觉。</p><blockquote class="ob oc od"><p id="1c8a" class="li lj mn lk b ll lm kd ln lo lp kg lq oe ls lt lu of lw lx ly og ma mb mc md im bi translated"><em class="it">实际上，前面所有的步骤都可以分解为两个步骤集合:</em> <strong class="lk jd"> <em class="it">嵌入步骤</em> </strong> <em class="it">和</em> <strong class="lk jd"> <em class="it">注意步骤</em> </strong> <em class="it">。如上所述，所有这些步骤的结果是一个名为<strong class="lk jd"> G </strong>的<strong class="lk jd"> 8d </strong> -by- <strong class="lk jd"> T </strong>矩阵。</em></p></blockquote><p id="0172" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面可以看到一个<strong class="lk jd"> G </strong>的例子。<strong class="lk jd"> G </strong>的每一列都是一个单词在上下文中的<strong class="lk jd">8d</strong>by-<strong class="lk jd">1</strong>向量表示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/70cf1926c15f337c53e677ef4779fae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*0cF6HuD4WlKq7HcEH2Arww.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">An example of <strong class="bd oi">G</strong>. The length of the matrix, <strong class="bd oi">T</strong>, equals the number of words in the Context (9 in this example). Its height is <strong class="bd oi">8d</strong>; <strong class="bd oi">d </strong>is a number that we preset in the word embedding and character embedding steps.</figcaption></figure><p id="1760" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在让我们玩一个小游戏，(希望)能帮助你理解之前文章中所有的数学术语。具体来说，<strong class="lk jd">让我们把上下文中的单词想象成一群有序的奴才。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/13dd8ea0f3b7fc698843f8512d7d57c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sxtJALXTcR5v39kxy2FDsw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Think of our Context as a bunch of Minions, with each Context word corresponding to one Minion</figcaption></figure><p id="2603" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的每个小喽啰都有一个大脑，可以储存一些信息。现在，我们的奴才们的大脑已经很拥挤了。<strong class="lk jd">每个 Minion 的当前脑内容相当于 Minion 所代表的上下文单词的 8d 乘 1 列向量。在这里，我展示了“新加坡”奴才的大脑扫描图:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/969b919829bd57f66d8c576116ae7c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*tYGi1OwYGpjAYlVKv0j2_g.png"/></div></figure><p id="96ba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">奴才们的脑子可不是一直这么满的！事实上，当它们出现时，它们的大脑几乎是空的。现在让我们回到过去，想想那些奴才们为了获得他们当前的知识状态而经历了什么样的“课程”。</p><p id="ba05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">小黄人上的前两节课是“单词嵌入”和“字符嵌入”在这些课程中，奴才们了解了自己的身份。“单词嵌入”课的老师 GloVe 教授教小喽啰们关于他们身份的基本信息。另一方面，“角色嵌入”课程是一门解剖学课程，在这门课程中，小喽啰们通过反复扫描来了解自己的身体结构。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/13dd577ee269a01c1dafbb6ba0178839.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*_fnYbhMSeIDa4whe.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Minions in the “Character Embedding” class</figcaption></figure><p id="798a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是这两节课后“新加坡”奴才的脑部扫描。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/738b273fe44fac0170fcfb508dee913a.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*mfrZNAza-Vi5GMuOocuECQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">The “Singapore” Minion understands his identity after attending the “Word Embedding” and “Character Embedding” lessons</figcaption></figure><p id="769f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">紧接着，小喽啰们继续前进，参加了“情境嵌入”课程。本课是一堂会话课，在课上，小喽啰们必须通过一款名为 bi-LS 的 messenger 应用程序相互交流。bi-LS 协助的 convo 允许奴才们了解彼此的身份<strong class="lk jd"/>——这是他们在前两节课中学到的。很整洁，是吧？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e68443d8ebc5048989f4308bd3565303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*hfo4osImRi7Unequ.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Two Minions having a fun conversation through bi-LS™, sharing information about themselves. Source: Giphy</figcaption></figure><p id="6d01" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我在“情境嵌入”课结束后，对“新加坡”的小跟班进行了另一次核磁共振扫描。如你所见，现在我们的小家伙知道更多的东西了！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/8cac09b5d216523f4ce6a3dd056d5365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*PKE4EIRDg9BP1A9YAyHsgA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Now “Singapore” knows both his and his neighbors’ identities!</figcaption></figure><p id="366a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的奴才们正在愉快地学习，突然一个人闯进了他们的学校😱原来他的名字是<strong class="lk jd">提问先生</strong>，他是一名记者。他在这里:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/fccd831f4e537db4cc7f595d7e9e80ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/0*grdcUkvOuO7FnAh2"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">The inquisitive Mr. Query. He has an urgent question<strong class="bd oi"> —</strong><em class="nv">”Where is Singapore situated”</em> — and he knows some of our Minions hold relevant information for this question.</figcaption></figure><p id="bce5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Query 先生急需为他正在写的一篇文章收集一些信息。具体来说，他想知道"<em class="mn">新加坡位于哪里【Query 先生知道<em class="mn">我们的一些奴才</em>在大脑中保存着这些信息。</em></p><p id="442b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的奴才们虽然乐于助人，但还是想帮 Query 先生一把。为此，他们需要选择几名团队成员与 Query 先生会面，并提供他所寻求的信息。<strong class="lk jd">这一群拥有查询先生相关信息并将被派遣给他的爪牙被称为回答帮。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/74b198ecd2905dd8b6f1ebe986ed25bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i0dJoGxq2dV-NrLHAx5lNw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">The Answer Gang, which collectively holds the answer to Mr. Query’s question. Only relevant Minions can join the Answer Gang!</figcaption></figure><p id="e320" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">现在，我们的跟班们有一项任务要做——他们需要集体决定谁应该、谁不应该加入回答帮。</strong>他们这样做时需要小心！如果他们从答案帮中漏掉了太多本应包括在内的爪牙，查询先生就不会得到他需要的所有信息。这种情况被称为<em class="mn">低回忆</em>，Query 先生对此深恶痛绝。</p><p id="f4c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一方面，如果太多不必要的爪牙加入回答帮，查询先生就会被多余的信息淹没。他称这种情况为低精度，他也不喜欢这样！众所周知，Query 先生有一些愤怒管理问题👺所以给查询先生提供适量的信息对我们的下属最有利。</p><p id="346e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mn">那么喽啰们怎么知道他们中的哪一个应该加入答题帮呢？</em></p><p id="f5ba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">解决这个问题的方法是组织几次会面，统称为“关注”在这些会议中，每个仆人都可以单独与查询先生交谈，了解他的需求。换句话说，注意力会议允许下属衡量他们对提问者的问题的重要性。</p><p id="5f3a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是“新加坡”奴才在注意力会议上闲逛时大脑的核磁共振扫描。这相当于我展示的第一张大脑扫描图像。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/cd5b2d3500b8d72a0d2ac8d3fa4b4801.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*7Rgj1vy10-mFVD7oap8GXA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Singapore’s current brain content. He knows quite a bit— but he is still missing one thing!</figcaption></figure><p id="8048" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如我们所看到的，我们的奴才们的大脑现在已经很满了。以奴才们目前的知识水平，他们现在可以开始选择答案帮的成员了吗？不，不完全是！他们仍然缺少一条关键信息。我们的每个爪牙都知道他对查询先生的重要性。然而，在做出这个重要的决定之前，他们还需要意识到对方对 Query 先生的重要性。</p><p id="30f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如你可能已经猜到的，这意味着奴才们必须第二次互相交谈！现在你知道这个对话是通过 bi-LS 应用<strong class="lk jd">完成的。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi os"><img src="../Images/0f6c6116888ec296388e580cd44ce6d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*zz_4Hb8ppc3AaJoO.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">The Minions during the <strong class="bd oi">modeling step </strong>meeting. Here, they talk to each other through bi-LS™ and share their relative importance to Mr. Query. Source: <a class="ae lh" href="https://www.freepnglogos.com/images/minions-9701.html" rel="noopener ugc nofollow" target="_blank">Free PNG Logo</a></figcaption></figure><p id="f500" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">这种由 bi-LS 促成的对话也被称为“建模步骤”,是我们当前文章的重点。</strong>现在让我们详细了解这一步骤！</p><h1 id="d5d4" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated"><strong class="ak">第十步。建模层</strong></h1><p id="3603" class="pw-post-body-paragraph li lj it lk b ll nw kd ln lo nx kg lq lr ny lt lu lv nz lx ly lz oa mb mc md im bi translated">好吧，让我们暂时离开我们的奴才，回到符号和方程上来，好吗？没有<em class="mn">那么</em>复杂，我保证！</p><p id="23dd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">建模层相对简单。它由两层双 LSTM 组成。如上所述，建模层的输入是<strong class="lk jd">G。</strong>第一个双 LSTM 层将<strong class="lk jd"> G </strong>转换为<strong class="lk jd"> 2d </strong> -by- <strong class="lk jd"> T </strong>矩阵，称为<strong class="lk jd"> M1 </strong>。</p><p id="3f6d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> M1 </strong>然后作为第二个双 LSTM 层的输入，第二个双层将其转换为另一个<strong class="lk jd"> 2d </strong> -by- <strong class="lk jd"> T </strong>矩阵，称为<strong class="lk jd"> M2 </strong>。</p><p id="d9b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从<strong class="lk jd"> G </strong>到<strong class="lk jd"> M1 </strong>和<strong class="lk jd"> M2 </strong>的形成如下图所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/03d94b24e81c91803f8c4a1c47de98db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ojdqFQ74YY0aC-_rkCSsOQ.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Step 10. In the modeling layer, <strong class="bd oi">G </strong>is passed through two bi-LSTM layers to form <strong class="bd oi">M1 </strong>and <strong class="bd oi">M2</strong></figcaption></figure><p id="c693" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> M1 </strong>和<strong class="lk jd"> M2 </strong>是上下文单词的另一种矩阵表示。<strong class="lk jd"> M1 </strong>和<strong class="lk jd"> M2 </strong>与之前的语境词表示的区别在于<strong class="lk jd"> M1 </strong>和<strong class="lk jd"> M2 </strong>在其中嵌入了关于整个语境段落以及查询的信息。</p><p id="3f92" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用奴才的话来说，这意味着我们的奴才现在拥有了所有他们需要的信息来决定谁应该加入回答帮。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/2dee4cec347ed914271fc0832ca6e8ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*misbSI-B7odDdxVpOp8JVA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">The “Singapore” guy now has all he needs to decide if he should join the Answer Gang.</figcaption></figure><h1 id="6948" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated"><strong class="ak">第十一步。输出层</strong></h1><p id="ccb6" class="pw-post-body-paragraph li lj it lk b ll nw kd ln lo nx kg lq lr ny lt lu lv nz lx ly lz oa mb mc md im bi translated">好了，现在我们到了终曲！只要再走一步，我们就完成了！</p><p id="0bd7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于上下文中的每个单词，我们都有两个数值向量来编码单词与查询的相关性。太棒了。我们需要做的最后一件事是<strong class="lk jd">将这些数字向量转换成两个概率值，这样我们就可以比较所有上下文单词的查询相关性</strong>。这正是输出层所做的。</p><p id="455a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在输出层中，<strong class="lk jd"> M1 </strong>和<strong class="lk jd"> M2 </strong>先与<strong class="lk jd"> G </strong>纵向串接形成<strong class="lk jd">G</strong>；<strong class="lk jd"> M1 </strong>和<strong class="lk jd">克</strong>；<strong class="lk jd"> M2 </strong>。两者都[<strong class="lk jd">G</strong>；<strong class="lk jd"> M1 </strong>和<strong class="lk jd">克</strong>；<strong class="lk jd"> M2 </strong>的尺寸为<strong class="lk jd"> 10d </strong> -by- <strong class="lk jd"> T </strong>。</p><p id="1bc2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们通过以下步骤获得<strong class="lk jd"> p1 </strong>，即整个上下文中开始索引的概率分布:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/4ba0f931a0bd100feff513294bb6d5a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ol4_FDFa_IVm653fNVbfOQ.png"/></div></div></figure><p id="7806" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">类似地，我们通过以下步骤获得最终指数的概率分布<strong class="lk jd"> p2 </strong>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/cb502283e2e280b574c1864c1335c9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uiHEJ0Esl-rIryS2JRM12Q.png"/></div></div></figure><p id="8abc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">获取<strong class="lk jd"> p1 </strong>和<strong class="lk jd"> p2 </strong>的步骤如下图所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/5f6264174fa368dd57b9fbf1c4102cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IohHc3HjsJSOuTj5zsil4g.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Step 11. The output layer, which converts <strong class="bd oi">M1 </strong>and <strong class="bd oi">M2 </strong>to two vector of probabilities, <strong class="bd oi">p1 </strong>and <strong class="bd oi">p2</strong>.</figcaption></figure><p id="bf96" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> p1 </strong>和<strong class="lk jd"> p2 </strong>然后被用来寻找最佳答案跨度。最佳答案跨度只是具有最高<em class="mn">跨度分数</em>的上下文的子串。依次，跨度分数是该跨度中第一个单词的<strong class="lk jd"> p1 </strong>分数和该跨度中最后一个单词的<strong class="lk jd"> p2 </strong>分数的简单乘积。然后，我们返回具有最高 span 分数的 span 作为我们的答案。</p><p id="bef9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">举个例子就能说明这一点。如您所知，我们目前正在处理以下查询/上下文对:</p><ul class=""><li id="131f" class="mp mq it lk b ll lm lo lp lr mr lv ms lz mt md mu mv mw mx bi translated"><strong class="lk jd">语境</strong> : <em class="mn">“新加坡是位于东南亚的一个小国。”</em> ( <strong class="lk jd"> T </strong> = 9)</li><li id="a34d" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd">提问</strong> : <em class="mn">“新加坡位于哪里？”</em> ( <strong class="lk jd"> J </strong> = 4)</li></ul><p id="64da" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在通过 BiDAF 运行这个查询/上下文对之后，我们获得了两个概率向量— <strong class="lk jd"> p1 </strong>和<strong class="lk jd"> p2。</strong></p><p id="bd3f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上下文中的每个单词都与一个<strong class="lk jd"> p1 </strong>值和一个<strong class="lk jd"> p2 </strong>值相关联。<strong class="lk jd"> p1 </strong>值表示单词成为答案范围开始单词的概率。以下是我们示例中的<strong class="lk jd"> p1 </strong>值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/792703540339a5588e56fc6187929017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qmMO65RS5jADlOY21nbxQg.png"/></div></div></figure><p id="1863" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们看到模型认为我们的答案跨度最有可能的起始词是“东南”</p><p id="239d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> p2 </strong>值表示单词成为答案范围的最后一个单词的概率。以下是我们示例中的<strong class="lk jd"> p2 </strong>值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/01fc9c8ba8c38289ae7cb169433290a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0IAbK-YSscu8mc6jiuZjEQ.png"/></div></div></figure><p id="6458" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们看到我们的模型非常确定，几乎 100%确定，我们的答案跨度最有可能的结尾词是“亚洲”</p><p id="c2eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果在原始上下文中，具有最高<strong class="lk jd"> p1 </strong>的单词在具有最高<strong class="lk jd"> p2 </strong>的单词之前出现<em class="mn">，那么我们已经有了我们的最佳答案范围——它将简单地以前者开始，以后者结束。这就是我们例子中的情况。因此，模型返回的答案将简单地是“东南亚”</em></p><p id="70c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就这样，女士们先生们——经过 11 个漫长的步骤，我们终于得到了我们问题的答案！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/a326ba4fa3516079631d941c8435feb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*RAsDiEatQ1s-WwRN_KTeug.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Here is Mr. Query with “Southeast” and “Asia”, both of whom have been selected to join the Answer Gang. It turns out that the information provided by “Southeast” and “Asia” is just what Mr. Query needs! Mr. Query is happy🎊</figcaption></figure><p id="2075" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好吧，在我们结束这个系列之前有一个警告。假设具有最高<strong class="lk jd"> p1 </strong>的上下文单词在具有最高<strong class="lk jd"> p2 </strong>的上下文单词之后出现<em class="mn">，我们还有一点工作要做。在这种情况下，我们需要生成<em class="mn">所有</em>可能的答案范围，并计算每个答案的范围分数。以下是我们的查询/上下文对的可能答案范围的一些示例:</em></p><ul class=""><li id="03ad" class="mp mq it lk b ll lm lo lp lr mr lv ms lz mt md mu mv mw mx bi translated"><strong class="lk jd">可能答案跨度:“</strong>新加坡”；<strong class="lk jd">跨度分数</strong> : 0.0000031</li><li id="c6ea" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd">可能答案跨度:“</strong>新加波是”；<strong class="lk jd">跨度分数</strong> : 0.00000006</li><li id="f6c5" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd">可能答案跨度:“</strong>新加坡是 a”；<strong class="lk jd">跨度分数</strong> : 0.0000000026</li><li id="607f" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd">可能答案跨度:“</strong>新加坡是个小”；<strong class="lk jd">跨度分数</strong> : 0.0000316</li></ul><p id="691f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们将具有最高跨度分数的跨度作为我们的答案。</p></div><div class="ab cl pb pc hx pd" role="separator"><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg"/></div><div class="im in io ip iq"><p id="c880" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是它 BiDAF 中每一步的详细说明，从开始到结束(撒上健康剂量的 Minion-joy)。我希望这个系列能帮助你理解这个迷人的 NLP 模型！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/739fe5b861964a5b071f544d671b579d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*Ak1LA_uH_Wco8SOG"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Source: <a class="ae lh" href="https://giphy.com/gifs/minions-iUOzkJmvnFfqM" rel="noopener ugc nofollow" target="_blank">Giphy</a></figcaption></figure><p id="0b9e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你对这篇文章有任何问题/评论，或者想联系我，请随时通过 LinkedIn 或 gmail DOT com 发送电子邮件至 meraldo.antonio。</p></div><div class="ab cl pb pc hx pd" role="separator"><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg"/></div><div class="im in io ip iq"><h1 id="7d93" class="nd ne it bd nf ng pj ni nj nk pk nm nn ki pl kj np kl pm km nr ko pn kp nt nu bi translated">词汇表</h1><ul class=""><li id="4fc5" class="mp mq it lk b ll nw lo nx lr po lv pp lz pq md mu mv mw mx bi translated"><strong class="lk jd">上下文:</strong>包含查询答案的查询附带文本。</li><li id="b808" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd">查询:</strong>模型应该给出答案的问题。</li><li id="954a" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd">答案:</strong>包含可以回答查询的信息的上下文的子字符串。这个子串将由模型提取出来。</li><li id="b035" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd">跨度得分:</strong>一个答案跨度中第一个单词的<strong class="lk jd"> p1 值</strong>与该答案跨度中最后一个单词的<strong class="lk jd"> p2 值</strong>的乘积。</li><li id="36cb" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd"> T </strong>:上下文中的字数。</li><li id="e04d" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd"> J </strong>:查询的字数。</li><li id="43ff" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd"> G </strong>:一个大的、<strong class="lk jd"> 8d </strong> -by- <strong class="lk jd"> T </strong>矩阵，包含查询感知的上下文表示。G 是建模层的输入。</li><li id="6be6" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd"> M1: </strong>将<strong class="lk jd"> G </strong>通过双 LSTM 得到的一个<strong class="lk jd"> 2d </strong> -by- <strong class="lk jd"> T </strong>矩阵。<strong class="lk jd"> M1 </strong>包含上下文单词的向量表示，其具有关于整个上下文段落以及查询的信息。</li><li id="7840" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd"> M2: </strong>一个<strong class="lk jd"> 2d </strong> -by- <strong class="lk jd"> T </strong>矩阵，通过一个 bi-LSTM 传递<strong class="lk jd"> M1 </strong>得到。<strong class="lk jd"> M2 </strong>，就像<strong class="lk jd"> M1 一样，</strong>包含上下文单词的向量表示，其具有关于整个上下文段落以及查询的信息。</li><li id="59b5" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd"> p1: </strong>一个长度为<strong class="lk jd"> T </strong>的概率向量。每个上下文单词都有自己的<strong class="lk jd"> p1 值。</strong>该<strong class="lk jd"> p1 值</strong>表示该单词成为答案区间中<strong class="lk jd">第一个单词</strong>的概率。</li><li id="dc8c" class="mp mq it lk b ll my lo mz lr na lv nb lz nc md mu mv mw mx bi translated"><strong class="lk jd"> p2: </strong>一个长度为<strong class="lk jd"> T </strong>的概率向量。每个上下文单词都有自己的<strong class="lk jd"> p2 值。</strong>该<strong class="lk jd"> p2 值</strong>表示该单词成为答案区间中最后一个单词<strong class="lk jd">单词</strong>的概率。</li></ul></div><div class="ab cl pb pc hx pd" role="separator"><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg"/></div><div class="im in io ip iq"><h1 id="7305" class="nd ne it bd nf ng pj ni nj nk pk nm nn ki pl kj np kl pm km nr ko pn kp nt nu bi translated">参考</h1><p id="04f7" class="pw-post-body-paragraph li lj it lk b ll nw kd ln lo nx kg lq lr ny lt lu lv nz lx ly lz oa mb mc md im bi translated">[1] <a class="ae lh" href="https://arxiv.org/abs/1611.01603" rel="noopener ugc nofollow" target="_blank">机器理解的双向注意力流(闵俊 Seo <em class="mn"> et。艾尔</em>，2017) </a></p></div><div class="ab cl pb pc hx pd" role="separator"><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg"/></div><div class="im in io ip iq"><p id="66c3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你对这篇文章有任何意见或者想联系我，请随时通过 LinkedIn 给我发一个联系方式。此外，如果你能通过我的推荐链接支持我成为一名中级会员，我将非常感激。作为一名会员，你可以阅读我所有关于数据科学和个人发展的文章，并可以完全访问所有媒体上的故事。</p></div></div>    
</body>
</html>