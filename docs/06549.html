<html>
<head>
<title>How to use a clustering technique for synthetic data generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用聚类技术生成合成数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-a-clustering-technique-for-synthetic-data-generation-7c84b6b678ea?source=collection_archive---------13-----------------------#2019-09-19">https://towardsdatascience.com/how-to-use-a-clustering-technique-for-synthetic-data-generation-7c84b6b678ea?source=collection_archive---------13-----------------------#2019-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ecf1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们展示了如何使用高斯混合模型(GMM)，一个强大的聚类算法，合成数据生成。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/13dc63692f2a904804c51978393541c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rYvA7NyczCtewj76TO91YQ.jpeg"/></div></div></figure><h1 id="1650" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">聚类和 k-均值</h1><p id="a919" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">对于利用数据科学的商业或科学企业来说，聚类是机器学习管道</a>的重要组成部分。顾名思义，它有助于识别数据块中密切相关(通过某种距离度量)的数据点的集合，否则很难理解。</p><p id="50eb" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><a class="ae mi" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank"> k-means 算法</a>在未标记的多维数据集中搜索预定数量的聚类。它使用最佳聚类的简单概念来实现这一点:</p><ul class=""><li id="d3b3" class="mo mp it lo b lp mj ls mk lv mq lz mr md ms mh mt mu mv mw bi translated"><em class="mx">“聚类中心”</em>是属于该聚类的所有点的算术平均值。</li><li id="64ec" class="mo mp it lo b lp my ls mz lv na lz nb md nc mh mt mu mv mw bi translated">每个点离自己的聚类中心比离其他聚类中心更近。</li></ul><p id="bcdb" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它通过从随机分配聚类中心开始，然后分阶段迭代来完成聚类，其中，在每个阶段，它为每个聚类中心分配特定的点，然后根据这些点的算术平均值来重新计算聚类中心的位置。这里有一个很好的视频演示了这个过程。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h1 id="d1cd" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">k 均值和高斯混合模型(GMM)的局限性</h1><p id="d453" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">k-means 算法的一个关键优势是其简单性，这使得它成为快速处理大规模数据的流行选择。但是这种简单性也导致了一些关键的实际挑战。</p><p id="9e8a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">特别是，k-means 的<strong class="lo iu">非概率特性及其采用简单的径向距离度量来分配集群成员资格，导致许多现实情况下性能不佳。</strong></p><p id="ae32" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">例如，想象以下数据点。在这里，它们由它们的标签来着色，但是在无监督的机器学习设置中，我们不会有类别标签。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/f3c75636581c64337d6ebad142e5e9f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*Jp8-hOuL98cayCk_benpSA.png"/></div></figure><p id="7b42" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">如果我们运行一个简单的 k-means，我们很可能会得到下面的最终聚类排列，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/17b07732e4b0e595207937da481a12d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*jVhln0U-Qw1Fg0BJwBv-5A.png"/></div></figure><p id="1eb9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">两个中间的聚类看起来有重叠，即<strong class="lo iu">我们可能对重叠区域中点的聚类分配没有完全的信心</strong>。不幸的是，<strong class="lo iu"> k-means 模型没有对聚类分配的概率或不确定性</strong>的内在度量。</p><blockquote class="nh"><p id="8386" class="ni nj it bd nk nl nm nn no np nq mh dk translated">k-means 从设计上来说是非概率的。</p></blockquote><p id="be3d" class="pw-post-body-paragraph lm ln it lo b lp nr ju lr ls ns jx lu lv nt lx ly lz nu mb mc md nv mf mg mh im bi translated">此外，对于 k-means，当数据分布的形状是圆形时，它工作得最好。这是因为 k-means 没有考虑长方形或椭圆形簇的内置方法。因此，举例来说，如果我们获取相同的数据，并通过向一个方向拉伸来转换它，则聚类分配会变得次优。</p><p id="3c87" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">k-means 不够灵活，无法解释这种“<em class="mx">拉长的数据</em>，并试图<strong class="lo iu">将数据强行拟合到四个圆形聚类中，</strong>这些聚类并不代表数据点的真实形状。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/7b31fd96a1c6b136c1d7de261e43d346.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*9YN2BNlkA5sFX3OC2UVgTg.png"/></div></figure><blockquote class="nh"><p id="dae2" class="ni nj it bd nk nl nx ny nz oa ob mh dk translated">k-means 在“非圆形”数据(即长方形或椭圆形聚类)上表现不佳</p></blockquote><p id="3f98" class="pw-post-body-paragraph lm ln it lo b lp nr ju lr ls ns jx lu lv nt lx ly lz nu mb mc md nv mf mg mh im bi translated"><strong class="lo iu"> G </strong>澳大利亚<strong class="lo iu"> M </strong> ixture <strong class="lo iu"> M </strong>模型(GMMs)，可以看作是 k-means 背后思想的延伸。GMM 试图将给定数据建模为多个多维高斯概率分布的混合物。</p><p id="752d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它基于<a class="ae mi" rel="noopener" target="_blank" href="/inference-using-em-algorithm-d71cccb647bc"> <strong class="lo iu">期望最大化(E-M)算法</strong> </a>工作，并从本质上处理生成数据的概率性质。</p><p id="372f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在最简单的情况下，GMM 可以像 k-means 一样用于寻找聚类。然而，它对于非圆形数据斑点执行得完美无缺，因为它可以用参数(std。dev 和 mean)适合于拟合数据的特定形状。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ad8a622a46d5d4aaa1ff1f2bccc1c253.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*ytZYLtTTkhHbtOhe63jstA.png"/></div></figure><p id="4eef" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在本文中可以找到对这些模型的全面总结，</p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/gaussian-mixture-models-explained-6986aaf5a95"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">高斯混合模型解释</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">在机器学习领域，我们可以区分两个主要领域:监督学习和非监督学习。主要的…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ks of"/></div></div></a></div><h1 id="d0f3" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">作为生成模型的 GMM</h1><p id="5e90" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">GMM 最流行的分类是聚类算法。然而，从根本上说，它是一种用于<a class="ae mi" href="https://en.wikipedia.org/wiki/Density_estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="lo iu">密度估计</strong> </a>的算法，属于<a class="ae mi" href="https://stats.stackexchange.com/questions/12421/generative-vs-discriminative" rel="noopener ugc nofollow" target="_blank"> <strong class="lo iu">生成模型</strong> </a>家族。</p><p id="6f67" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这意味着，当 GMM 适合某些数据时，生成的模型不仅仅是一组固定聚类的描述，而是描述数据真实分布的生成概率模型。</p><blockquote class="nh"><p id="f704" class="ni nj it bd nk nl nm nn no np nq mh dk translated">GMM 是一种生成式建模技术，可以生成接近拟合数据分布的合成数据。</p></blockquote><p id="2a4c" class="pw-post-body-paragraph lm ln it lo b lp nr ju lr ls ns jx lu lv nt lx ly lz nu mb mc md nv mf mg mh im bi translated">这里有一个关于生成模型的很好的介绍，</p><div class="oc od gp gr oe of"><a href="https://medium.com/@mlengineer/generative-and-discriminative-models-af5637a66a3" rel="noopener follow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">生成模型与判别模型</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">我想以一个故事开始这篇文章。</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">medium.com</p></div></div></div></a></div><h1 id="6633" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">使用 GMM 的合成数据生成</h1><p id="4f17" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">假设我们有一些数据点的月牙形聚类排列。这是一个流行的玩具例子，经常用来展示 k 均值的局限性。一种称为谱聚类的特殊聚类方法最适合这种类型的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/88053265af03cb592da99cfc415e6f4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*EZQ81Q99Wtaa_9hGimCr7g.png"/></div></figure><p id="73e0" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu"> <em class="mx">有多少个自然集群？</em> </strong></p><p id="b182" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">显然是两个。如果我们选择两个集群运行 GMM，我们将得到类似这样的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/16c65b782ec9a0458d329af2dbcdc6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*oir9Yef62d5hKiMkqK0uqA.png"/></div></figure><p id="ec50" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">印象不深，是吗？对于这种特殊形式的数据，它显示了与 k-means 相同的限制。</p><p id="f710" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">但是假设我们对聚类不感兴趣，但是<strong class="lo iu">基于给定的数据样本</strong>生成更多的合成数据。</p><p id="ad2a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">为此，我们需要大量的高斯星团。看看当我们选择 4、8 和 16 个集群运行相同的 GMM 算法时会发生什么。</p><div class="kj kk kl km gt ab cb"><figure class="ow kn ox oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/79feea82b171a0e0f6d884b3ed22a69d.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*FHKmbchu0uLBrKvh7GyrOg.png"/></div></figure><figure class="ow kn pc oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/b4f58502bb28c91d0180cd1e07da7c16.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*qctXDymO7CmHqv8ktXld6g.png"/></div></figure><figure class="ow kn pd oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/f12f9250de4b669c8f0e56e095496c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*xnoVWjwUEzHBhGtSrFvibQ.png"/></div></figure></div><p id="91e9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这些聚类似乎紧密地跟随数据的形状。GMM 的伟大之处在于我们有一种<code class="fe pe pf pg ph b">sample</code>方法。这意味着它已经学会了数据的分布，而不仅仅是对数据进行聚类，并且可以从分布中生成任意数量的样本。</p><blockquote class="nh"><p id="0799" class="ni nj it bd nk nl nm nn no np nq mh dk translated">GMM 的标准算法实现有一个生成数据的<code class="fe pe pf pg ph b">sampling</code>方法。k-means 没有这样的方法。</p></blockquote><p id="9210" class="pw-post-body-paragraph lm ln it lo b lp nr ju lr ls ns jx lu lv nt lx ly lz nu mb mc md nv mf mg mh im bi translated">看看当我在这些模型(4、8 和 16 集群模型)上调用<code class="fe pe pf pg ph b">predict</code>方法时会发生什么。</p><div class="kj kk kl km gt ab cb"><figure class="ow kn pi oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/c1bae8ccac15151e34792d95358473e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*wSobdNVJeUXCUR6F5GXs6A.png"/></div></figure><figure class="ow kn pj oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/11a4e429f2231e36d1900a4ff720668b.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*9RQ-vFCJUCJIdhXldXhoIw.png"/></div></figure><figure class="ow kn pk oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/89b8064c63d077e51acad9cd0acebfdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*G5Zf5Co5NYGKTgngdwBr0w.png"/></div></figure></div><p id="4125" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">它能够生成新的数据，紧密跟随它所聚类的原始数据的形状！</strong></p><p id="9fa8" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">很明显，4 簇模型不太擅长模拟原始数据，但是 16 簇模型非常接近原始数据，不是吗？</p><p id="9796" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">当然，<strong class="lo iu">这种能力并不局限于任何特定的形状或二维空间，而是通常扩展到多变量情况</strong>。下面是一个 4 簇数据集的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/f2a92b6d6ad10649d9ede4953fffce03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vlC9fXtKAaY-JP4HweqOgw.png"/></div></div></figure><h1 id="4b54" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">警告和含义</h1><p id="26e3" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">就像任何生成技术一样，这容易受到过度拟合和泛化能力差的影响。您可以无限制地增加高斯分量的数量，但这将开始适应噪声，而不是代表真实的分布。</p><p id="ed8f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">一般来说，必须对高斯分量、初始化、容差和迭代次数的选择进行仔细的实验。</p><blockquote class="pm pn po"><p id="d16a" class="lm ln mx lo b lp mj ju lr ls mk jx lu pp ml lx ly pq mm mb mc pr mn mf mg mh im bi translated">增加高斯分量的数量，直到开始获得足够接近的原始数据集表示，但不再增加。遵守<a class="ae mi" href="https://www.teradata.com/Blogs/Occam%E2%80%99s-razor-and-machine-learning" rel="noopener ugc nofollow" target="_blank">奥卡姆剃刀</a>的原则，并尝试<strong class="lo iu">在足以满足您的应用</strong>的最小复杂度处停止。</p></blockquote><p id="d083" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这种技术的一个潜在应用是直接用于商业分析和市场细分案例。通常，市场调查产生的数据点数量有限。如果您的内部聚类算法或其他数据处理技术需要更多的数据点，该怎么办？这是一种接近模拟真实数据集分布并为进一步处理生成相似聚类的潜在方法。</p><blockquote class="nh"><p id="14b2" class="ni nj it bd nk nl nm nn no np nq mh dk translated">业务分析可以使用这种合成数据生成技术从有限的真实数据样本中创建人工聚类。</p></blockquote><p id="c10c" class="pw-post-body-paragraph lm ln it lo b lp nr ju lr ls ns jx lu lv nt lx ly lz nu mb mc md nv mf mg mh im bi translated">关于这个<a class="ae mi" href="https://github.com/tirthajyoti/Machine-Learning-with-Python/blob/master/Clustering-Dimensionality-Reduction/k-means_clustering_GMM.ipynb" rel="noopener ugc nofollow" target="_blank">的练习 Jupyter 笔记本可以在这里</a>找到。</p></div><div class="ab cl ps pt hx pu" role="separator"><span class="pv bw bk pw px py"/><span class="pv bw bk pw px py"/><span class="pv bw bk pw px"/></div><div class="im in io ip iq"><p id="eef9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi pz translated"><span class="l qa qb qc bm qd qe qf qg qh di">如果</span>您有任何问题或想法要分享，请通过<a class="ae mi" href="mailto:tirthajyoti@gmail.com" rel="noopener ugc nofollow" target="_blank"><strong class="lo iu">tirthajyoti【AT】Gmail . com</strong></a>联系作者。此外，您可以查看作者的<a class="ae mi" href="https://github.com/tirthajyoti?tab=repositories" rel="noopener ugc nofollow" target="_blank"> <strong class="lo iu"> GitHub </strong> </a> <strong class="lo iu">资源库</strong>，了解 Python、R 和机器学习资源中其他有趣的代码片段。如果你像我一样对机器学习/数据科学充满热情，请随时<a class="ae mi" href="https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/" rel="noopener ugc nofollow" target="_blank">在 LinkedIn 上添加我</a>或<a class="ae mi" href="https://twitter.com/tirthajyotiS" rel="noopener ugc nofollow" target="_blank">在 Twitter 上关注我。</a></p><div class="oc od gp gr oe of"><a href="https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">Tirthajyoti Sarkar - Sr .首席工程师-半导体、人工智能、机器学习- ON…</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">通过写作使数据科学/ML 概念易于理解:https://medium.com/@tirthajyoti 开源和有趣…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">www.linkedin.com</p></div></div><div class="oo l"><div class="qi l oq or os oo ot ks of"/></div></div></a></div></div></div>    
</body>
</html>