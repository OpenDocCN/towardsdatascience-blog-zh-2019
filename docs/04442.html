<html>
<head>
<title>Dynamic Meta Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">动态元嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dynamic-meta-embeddings-f97e2c682187?source=collection_archive---------16-----------------------#2019-07-09">https://towardsdatascience.com/dynamic-meta-embeddings-f97e2c682187?source=collection_archive---------16-----------------------#2019-07-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="54cf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你在过去的几个月里阅读过任何与 NLP 相关的书籍，你不可能没有听说过<a class="ae ko" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae ko" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>或<a class="ae ko" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank">埃尔莫</a>这些人进一步拓展了 NLP 任务的范围。尽管这些技术很棒，但是有很多 NLP 问题设置你无法使用它们。你的问题可能不是真正的语言，而是可以用记号和序列来表达。你可能正在处理一种没有预先训练选项的语言，也没有资源来自己训练它。<br/>也许你只是在处理一个非常特殊的领域(但是你知道<a class="ae ko" href="https://github.com/dmis-lab/biobert" rel="noopener ugc nofollow" target="_blank"> BioBERT </a>吗？).</p><p id="3ca6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，您可以选择处理预先训练好的嵌入(不是由上述模块生成的)。今年早些时候的 Kaggle <a class="ae ko" href="https://www.kaggle.com/c/quora-insincere-questions-classification" rel="noopener ugc nofollow" target="_blank"> Quora 虚假问题分类</a>就是这种情况，参赛者不能使用 BERT/GPT-2/ELMo，但只能获得四组预训练的嵌入。这个设置引发了一个有趣的讨论，即如何组合不同的嵌入，而不是只选择一个。<br/>在这篇文章中，我们将回顾一些将不同的嵌入组合成单一表示的最新技术，这被称为“元嵌入”。<br/>一般来说，我们可以将元嵌入技术分为两个不同的类别。第一个是(1)当创建元嵌入的过程与它们将被用于的任务分离时，以及(2)当训练元嵌入与实际任务同步时。<br/>我不会在这篇文章中介绍使用第一个提到的过程(1)的方法，它们中的大多数都非常简单，包括嵌入维度的平均、连接等。然而，不要把简单误认为是弱，因为提到的 Kaggle 竞赛<a class="ae ko" href="https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568#latest-564470" rel="noopener ugc nofollow" target="_blank">获胜解决方案</a>使用了嵌入的加权平均。<br/>如果你想了解更多关于这类方法的信息，你可以在下面两篇文章中找到。<a class="ae ko" href="https://arxiv.org/abs/1804.05262" rel="noopener ugc nofollow" target="_blank">简单得令人沮丧的元嵌入——通过平均源词嵌入来计算元嵌入</a> <br/> <strong class="js iu"> b </strong>。<a class="ae ko" href="https://arxiv.org/abs/1508.04257" rel="noopener ugc nofollow" target="_blank">通过使用嵌入集合的系综学习元嵌入</a> <br/>在下一节中，我们将介绍使用第二个提到的过程(2)的两种技术。</p><h1 id="ff43" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">(上下文)动态元嵌入</h1><p id="decd" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">本节中的技术都来自脸书人工智能研究论文“用于改进句子表示的动态元嵌入”，并提出了两种新技术，“T2”动态元嵌入(DME) 和“T4”上下文动态元嵌入(CDME) 。这两种技术都是附加在网络开始处的模块，并且具有可训练的参数，这些参数从与网络其余部分相同的梯度中更新。这两种技术的共同步骤是原始嵌入的线性投影。下图提供了投影的可视化效果；虚线表示学习的参数。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/f14f59cbf0054bd919513ae9750c453a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bi00PiJ2X2RV3o4d7d_foQ.jpeg"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Figure 1: Projection, learned parameters in dashed line</figcaption></figure><p id="7e2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用由此产生的投影，将计算关注系数，以使用这些投影的加权和。DME 和 CDME 计算这些系数的方式不同。</p><h2 id="ff68" class="mj kq it bd kr mk ml dn kv mm mn dp kz kb mo mp ld kf mq mr lh kj ms mt ll mu bi translated">测距装置(Distance Measuring Equipment)</h2><p id="11b8" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">DME 使用的机制只依赖于单词 projections 本身。每个单词投影乘以一个学习向量<em class="ls"> a </em>，这产生一个标量——因此对于<em class="ls"> N </em>个不同的投影(对应于<em class="ls"> N </em>个不同的嵌入集),我们将有<em class="ls"> N </em>个标量。这些标量然后通过 softmax 函数传递，结果是注意系数。然后，这些系数用于创建元嵌入，这是投影的加权和(使用系数加权)。<br/>该步骤如下所示(虚线表示已学习的参数)</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mv"><img src="../Images/1de5a26812c934409628c46021261867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RJ2pU8HUwsT2H2fdQo3eHw.jpeg"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Figure 2: Creating the DME embedding, learned parameters in dashed line</figcaption></figure><p id="1ae2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你说 TensorFlow(或 Keras)，你可能更喜欢在代码中看到它——下面是 DME 的 Github 要点(精简版),完整代码可以在这里找到<a class="ae ko" href="https://github.com/eliorc/tavolo/blob/master/tavolo/embeddings.py#L125" rel="noopener ugc nofollow" target="_blank"/></p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h2 id="909d" class="mj kq it bd kr mk ml dn kv mm mn dp kz kb mo mp ld kf mq mr lh kj ms mt ll mu bi translated">CDME</h2><p id="eb9a" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">CDME 使用双向 LSTM (BiLSTM)将上下文添加到混合中。如果你对这个术语感到不舒服，我不会在这里详述 LSTMs，回顾一下关于它们的经典的<a class="ae ko" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> colah post </a>。</p><p id="07e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与 DME 的唯一区别在于注意力系数是如何计算的。<br/>就像 DME 一样，首先序列被投影，然后投影的序列通过 BiLSTM。<br/>然后，不使用单词本身，而是使用正向和反向 LSTMs 的级联隐藏状态(在单词的对应索引中)和向量<em class="ls"> a </em>来计算关注系数。<br/>这个过程的可视化:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi my"><img src="../Images/f1d11d46fb7974085daed15ecaae3a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zji-3qrlbhNYOeQACNEvuQ.jpeg"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Figure 3: Creating the CDME embedding, learned parameters in dashed line</figcaption></figure><p id="41ba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，如果你喜欢阅读下面的代码是 Github gist 的精简版，完整版在<a class="ae ko" href="https://github.com/eliorc/tavolo/blob/master/tavolo/embeddings.py#L269" rel="noopener ugc nofollow" target="_blank">这里</a></p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="mw mx l"/></div></figure></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="8a2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就这些，希望你学到了新的东西，欢迎发表你的想法和问题。👋</p><p id="9266" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<a class="ae ko" href="https://www.datascience.co.il/blog" rel="noopener ugc nofollow" target="_blank">https://www.datascience.co.il/blog</a>查看更多内容、博客和新闻</p></div></div>    
</body>
</html>