<html>
<head>
<title>Too powerful NLP model (GPT-2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">过于强大的自然语言处理模型(GPT-2)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655?source=collection_archive---------7-----------------------#2019-02-21">https://towardsdatascience.com/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655?source=collection_archive---------7-----------------------#2019-02-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d1eb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">什么是生成性预训练</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/56970f66005bc60b3e5644f6986f2e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AUG1xC9lv8BMtZVq"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edward Ma</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="39db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">OpenAI 发布了<a class="ae ky" rel="noopener" target="_blank" href="/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b">生成式预训练模型</a> (GPT)，该模型在 2018 年的许多 NLP 任务中取得了最先进的结果。GPT 利用 transformer 来执行无监督学习和有监督学习，以学习 NLP 下游任务的文本表示。</p><p id="c65e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了展示这种模式的成功，OpenAI 对其进行了增强，并于 2019 年 2 月发布了一款 GPT-2。GPT-2 被训练为基于 40GB 文本预测下一个单词。与其他模型和实践不同，OpenAI 没有发布完整版本的模型，而是发布了一个轻量级版本。他们在自己的<a class="ae ky" href="https://blog.openai.com/better-language-models/" rel="noopener ugc nofollow" target="_blank">博客</a>中提到了这件事:</p><blockquote class="lv lw lx"><p id="d87d" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">由于我们担心该技术的恶意应用，我们不会发布经过训练的模型。作为负责任披露的一项实验，我们发布了一个小得多的模型供研究人员进行实验，以及一篇技术论文。</p></blockquote><p id="72a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于这个原因，它制造了许多关于没有最新型号和源代码可供公众使用的<a class="ae ky" href="https://techcrunch.com/2019/02/17/openai-text-generator-dangerous/?utm_source=tcfbpage&amp;sr_share=facebook&amp;fbclid=IwAR3HuAaJasah3ZsxcFPDg73pNse0dFHtYbIGx8L9TczSEhJoXDfKLrLxhDw" rel="noopener ugc nofollow" target="_blank">噪音</a>。应该研究开放模型和源代码吗？OpenAI 确实引发了很多讨论，但似乎大多数反馈都是负面的。忽略它是否应该开放，这个故事将讨论关于<a class="ae ky" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是无监督的多任务学习者</a>(拉德福德等人，2019)，并将涵盖以下内容:</p><ul class=""><li id="ebf5" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">数据</li><li id="3d44" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">体系结构</li><li id="064f" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">实验</li><li id="a589" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">经验</li></ul><h1 id="cd49" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">数据</h1><h2 id="520d" class="ni mr it bd ms nj nk dn mw nl nm dp na li nn no nc lm np nq ne lq nr ns ng nt bi translated">资料组</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/0e06b2eee2f1a7c4c8a599d4f25a220b.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*SunaL9dBFUQfOQhbL1Ux8w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Reddit Logo</figcaption></figure><p id="404a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">OpenAI 没有使用现有的数据集，而是选择建立一个新的强调文档质量的网络抓取工具。所有文字来自出站克林从<a class="ae ky" href="https://www.reddit.com/" rel="noopener ugc nofollow" target="_blank"> Reddit </a>职位和职位必须被评为至少 3 因果报应。换句话说，它是由人类确认的有趣的、有教育意义的或有意义的事情。</p><h2 id="f298" class="ni mr it bd ms nj nk dn mw nl nm dp na li nn no nc lm np nq ne lq nr ns ng nt bi translated">数据预处理</h2><p id="159d" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">不需要预处理步骤。换句话说，小写、标记化和其他步骤被跳过，因为作者认为这些预处理步骤限制了模型的能力，并且它能够评估所有语言模型基准。</p><h1 id="21a8" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">GPT 2 号的建筑</h1><h2 id="e504" class="ni mr it bd ms nj nk dn mw nl nm dp na li nn no nc lm np nq ne lq nr ns ng nt bi translated">输入表示</h2><p id="c341" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">文本表示法是一种很好的表示方法，一个词在神经网络中无疑是真实的。然而，拉德福德等人既不适用<a class="ae ky" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">字级</a>也不适用<a class="ae ky" rel="noopener" target="_blank" href="/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10">字级</a>。他们选择中间的一个子词。子字可以通过字节对编码(BPE)算法获得。</p><p id="96da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"><em class="ly">【BPE】</em></strong>字节对编码</p><p id="f407" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BPE 本来就是压缩的方式。将使用以下算法计算子字列表。</p><ul class=""><li id="4eac" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">将单词拆分成字符序列。</li><li id="750a" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">加入最高频率模式</li><li id="4f01" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">继续执行上一步，直到达到预定义的最大迭代子词数。</li></ul><p id="0076" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以“低:5”、“低:2”、“最新:6”和“最宽:3”为例，在每次迭代中提取最高频率的子字:</p><ol class=""><li id="20bc" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu oa mi mj mk bi translated">9 个频率的“es”</li><li id="c36f" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu oa mi mj mk bi translated">9 个频率的“est”</li><li id="ad2c" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu oa mi mj mk bi translated">等等</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/a8eafc4e45a04a8a187de2a53fdcaf2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*_bpIUb6YZr6DOMLAeSU2WA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Algorithm of BPE (Sennrich et al., 2015)</figcaption></figure><h2 id="fea9" class="ni mr it bd ms nj nk dn mw nl nm dp na li nn no nc lm np nq ne lq nr ns ng nt bi translated">网络体系结构</h2><p id="09da" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">同<a class="ae ky" rel="noopener" target="_blank" href="/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b"> GPT </a>，GPT-2 利用变压器模型。而<a class="ae ky" rel="noopener" target="_blank" href="/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b"> GPT </a>和 GPT-2 的区别在于:</p><ul class=""><li id="4a4e" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">将归一化层移动到每个子块的输入端</li><li id="9c4a" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">在最终自我关注模型后添加标准化层</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oc"><img src="../Images/642d752795691798a78bcce6119359fd.png" data-original-src="https://miro.medium.com/v2/format:webp/1*jbcwhhB8PEpJRk781rML_g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Architecture of GPT (Radford et al., 2018)</figcaption></figure><p id="b699" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了适应不同场景，训练了具有不同参数的 4 个模型</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/0100d584f1698434fdf404c97e9af6d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*zyFctt4KAKh6UOJlkRiqWA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Architecture Hyperparameters (Radford et al., 2019)</figcaption></figure><h2 id="7a88" class="ni mr it bd ms nj nk dn mw nl nm dp na li nn no nc lm np nq ne lq nr ns ng nt bi translated">模特培训</h2><p id="2bce" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">GPT-2 使用无监督学习方法来训练语言模型。不同于其他模型，如<a class="ae ky" rel="noopener" target="_blank" href="/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f"> ELMo </a>和<a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb"> BERT </a>需要两个阶段的培训，即预培训和微调阶段。GPT-2 没有微调阶段。</p><p id="4b60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">没有 GPT-2 的定制培训。OpenAI 不发布训练 GPT-2 的源代码(截至 2019 年 2 月 15 日)。因此，我们只能将训练好的模型用于研究或采用。同时，唯一发布的训练模型是最小的模型，有 117 个参数。要下载这个模型，你可以按照<a class="ae ky" href="https://github.com/openai/gpt-2" rel="noopener ugc nofollow" target="_blank"> GPT-2 Github </a>中的说明进行。</p><h1 id="d0ae" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">实验</h1><p id="f63c" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">拉德福德等人证明了最大的模型(即 1542M 参数)达到了 8 个最先进的结果，而最小的模型达到了 4 个最先进的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/d93dd8f866c10894801134fa937454b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pYw22GoiQxBhILLlu2bEww.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Result of 4 models in different dataset (Radford et al., 2019)</figcaption></figure><h1 id="9f87" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">经验</h1><p id="04f1" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">很容易尝试 GPT-2 小型模型。你只需要遵循来自<a class="ae ky" href="https://github.com/openai/gpt-2" rel="noopener ugc nofollow" target="_blank"> GPT-2 Github </a>的简单指令。下载源代码和模型并安装库后，可以使用<code class="fe of og oh oi b">unconditional sample generation</code>或<code class="fe of og oh oi b">conditional sample generation</code>生成文本。</p><h2 id="fc59" class="ni mr it bd ms nj nk dn mw nl nm dp na li nn no nc lm np nq ne lq nr ns ng nt bi translated">无条件样本生成</h2><p id="c372" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">第一种模式是<code class="fe of og oh oi b">Unconditional Sample Generation</code>。它意味着无条件地生成文本。</p><pre class="kj kk kl km gt oj oi ok ol aw om bi"><span id="7a94" class="ni mr it oi b gy on oo l op oq">python src/generate_unconditional_samples.py</span></pre><p id="716b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过一会儿，会生成一些文本，这里是一个例子:</p><pre class="kj kk kl km gt oj oi ok ol aw om bi"><span id="a00f" class="ni mr it oi b gy on oo l op oq">Most people think that when a warship runs aground it doesn't just kill people and then sink or burn all of society. That would make L.S. Kaminsky blush. Yet Kaminsky is doing one thing right: the CREAPH presidency. Whoever he is that fired the salt gun after getting thrown out of the Senate tossup race here in Richmond, he runs the "war," real, that is, guys like Alvin Dream, Dennis Hastert and Vijay Swarup. Given that Ed Gillespie, the GOP nominee barely a month into the campaign, on May 2 earned 45 points from the Tea Partiers, secessionists and nativities, right much everyone under 30 has been cheering the idea of "the war." Elliot Abrams, one of the Campus Reform editorial staff writers, also called the "war" mundane in the broadest terms. "Oh, well with them suiting it up voting be vigilant. And that produces 14 Rand Paul a grand total of 50 but Johnson 53. Two just finished with a 45 for Johnson 46. 'Well I hope it keeps getting led!' No, it's to save your mates from gun sin," wrote James Hernandez in New York to figure out what was going on. Ditto Bob Corker, who greeted the notion this far by saying it was "a dip in accuracy." As for the Rand Paul and Marco Rubio brilliant running mates like Thad Execury (FML) — who are now both running for president estranged from their father, John Doe III, and miscarried by accident — it's just another rebel outside cover.</span></pre><p id="d0c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您也可以通过更改默认配置来生成文本。温度是随机化的水平。较低的值很有可能从 WebText 的测试集中输出数据。</p><pre class="kj kk kl km gt oj oi ok ol aw om bi"><span id="1799" class="ni mr it oi b gy on oo l op oq">python src/generate_unconditional_samples.py --top_k 1 --temperature 0.1</span></pre><p id="133e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出可以是:</p><pre class="kj kk kl km gt oj oi ok ol aw om bi"><span id="f578" class="ni mr it oi b gy on oo l op oq">The first time I saw the new version of the game, I was so excited. I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game, I was so excited to see the new version of the game,</span></pre><h2 id="83b5" class="ni mr it bd ms nj nk dn mw nl nm dp na li nn no nc lm np nq ne lq nr ns ng nt bi translated">条件样本生成</h2><p id="8ef2" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">在无条件文本生成之后，我们将尝试有条件文本生成。</p><pre class="kj kk kl km gt oj oi ok ol aw om bi"><span id="c6d7" class="ni mr it oi b gy on oo l op oq">python <!-- -->src/interactive_conditional_samples.py --top_k 40</span></pre><p id="645c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们试试香港乐队(法玛)的一首歌词</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/849156c765cca2e7a76e5370093815fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lt9xueHeTLLBcxyzgv6Wcg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Reply of “I go to school by bus”</figcaption></figure><p id="8b35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们试试来自爱莉安娜·格兰德的 7 枚戒指:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/7bcf99be2442229045df5b6fa3f2b0f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yF0_l16tA8rcRqUbOsf5QA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Reply of “Yeah, breakfast at Tiffany’s and bottles of bubbles.”</figcaption></figure><h1 id="0b51" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">拿走</h1><ul class=""><li id="f658" class="mc md it lb b lc nv lf nw li ot lm ou lq ov lu mh mi mj mk bi translated">使用子词(BPE)而不是使用字符和单词嵌入。也许单词嵌入的层次太高，而纯字符嵌入的层次太低。BPE 包括字符级、子词级和词级嵌入。</li><li id="022a" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">数据很重要，但标记数据的成本很高。拉德福德等人使用高质量的数据进行无监督学习，这样他们可以避免有限的标记数据问题。如前所述，至少选择 3 个因果报应数据。换句话说，这是一种人群过滤。</li><li id="a2ae" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">通过对预先训练好的模型进行多次尝试，得到了令人印象深刻的结果。这是 OpenAI 不公开发布所有内容的原因吗？不知道，但有一点是肯定的，这是一个非常好的营销，OpenAI 忽略了许多负面反馈。期待最大的模型和源代码。</li></ul><h1 id="c81b" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae ky" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="a23a" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">延伸阅读</h1><ul class=""><li id="4876" class="mc md it lb b lc nv lf nw li ot lm ou lq ov lu mh mi mj mk bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b">创成式预培训(GPT) </a></li><li id="0180" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">单词嵌入</a></li><li id="fc04" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10">字符嵌入</a></li><li id="1090" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb">来自变压器(BERT)的双向编码器表示</a></li><li id="93a3" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f">来自语言模型(ELMo)的嵌入</a></li><li id="d99c" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><a class="ae ky" href="https://github.com/openai/gpt-2" rel="noopener ugc nofollow" target="_blank"> GPT-2 Github </a></li></ul><h1 id="9b17" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">参考</h1><ul class=""><li id="fb4e" class="mc md it lb b lc nv lf nw li ot lm ou lq ov lu mh mi mj mk bi translated">A.、吴、蔡尔德、栾、阿莫代和苏茨基弗。<a class="ae ky" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是无人监督的多任务学习者</a>。2019</li><li id="3206" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">A.拉德福德、K. Narasimhan、T. Salimans 和 I. Sutskever。<a class="ae ky" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">通过生成性预训练提高语言理解</a>。2018</li><li id="80e5" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">R.森里奇 b .哈多和 a .伯奇。<a class="ae ky" href="http://aclweb.org/anthology/P16-1162" rel="noopener ugc nofollow" target="_blank">具有子词单元的生僻字的神经机器翻译</a>。2015</li></ul></div></div>    
</body>
</html>