<html>
<head>
<title>Lasso regularization on linear regression and deep learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于线性回归和深度学习的 Lasso 正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lasso-regularization-on-linear-regression-and-other-models-70f65efda40c?source=collection_archive---------28-----------------------#2019-09-30">https://towardsdatascience.com/lasso-regularization-on-linear-regression-and-other-models-70f65efda40c?source=collection_archive---------28-----------------------#2019-09-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="75bb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对套索惩罚的影响及其对线性回归的影响的数学分析，包括对深度学习的可能扩展</h2></div><h1 id="b20b" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><h2 id="0c1d" class="la kj it bd kk lb lc dn ko ld le dp ks lf lg lh ku li lj lk kw ll lm ln ky lo bi translated">曲线拟合-欠拟合和过拟合</h2><p id="e20b" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx lf ly lz ma li mb mc md ll me mf mg mh im bi translated">正如我在<a class="ae mi" href="https://medium.com/@snaveenmathew/a-short-note-on-regularization-42ee07c65d90" rel="noopener">上一篇文章</a>中所讨论的，当问题不适当时，就会出现“曲线拟合”的问题。不足通常不是一个大问题，因为我们可以通过获取/设计新特性来扩展特性集。然而，过度拟合并不容易处理。</p><h2 id="6cd0" class="la kj it bd kk lb lc dn ko ld le dp ks lf lg lh ku li lj lk kw ll lm ln ky lo bi translated">回归中的最佳子集选择</h2><p id="f92f" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx lf ly lz ma li mb mc md ll me mf mg mh im bi translated">考虑一个有 p 个预测变量的线性回归。假设整个数据集用于训练。众所周知，训练集 R 不会随着特征的增加而减少。因此，R 不是一个很好的拟合优度的度量。调整后的-R，马洛的 Cₚ，AIC，BIC 等。是用来衡量拟合优度的。然而，在添加/移除预测变量时，不存在关于这些测量值的变化的先验知识。因此，可能需要所有 2ᵖ-1 独特模型来判断为结果变量建模所需的特征的“最佳子集”。然而，这在计算上非常昂贵。这就需要一种适当的方法来减少变量，而不需要建立指数级的大量模型。套索惩罚有助于部分实现这一目标。</p><h1 id="4f18" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">制定</h1><h2 id="ff67" class="la kj it bd kk lb lc dn ko ld le dp ks lf lg lh ku li lj lk kw ll lm ln ky lo bi translated">线性回归和正规方程</h2><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/576cb2178c2747ab7c609a6f18bda498.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*CkQkKUvXyBmPPLMC7GRToA.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Regression equation</figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/804238842762ed9bdd08b7673979a384.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*ngB1v_Qa1MlENc6nF5u0YQ.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Linear regression estimated on sample</figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/f4775302ee5ca2152c30ba99951849e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*gC2BHrX6rmi_M2U3yBv5eQ.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">OLS solution — same as MLE under certain conditions</figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/39b3c87f937a51264f0b2d8919ba13c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*tb0x05C43zlk_eRljVkXrg.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Normal equation</figcaption></figure><p id="5046" class="pw-post-body-paragraph lp lq it lr b ls my ju lu lv mz jx lx lf na lz ma li nb mc md ll nc mf mg mh im bi translated">我们观察到，如果协方差矩阵是不可逆的，则 OLS 解不存在(可能不是唯一的)。</p><h2 id="e667" class="la kj it bd kk lb lc dn ko ld le dp ks lf lg lh ku li lj lk kw ll lm ln ky lo bi translated">套索公式</h2><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/576cb2178c2747ab7c609a6f18bda498.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*CkQkKUvXyBmPPLMC7GRToA.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Regression equation</figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/8df514b2adf45217117148596dab8810.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*UVlYo6Pf_CbTbm-D04U7zQ.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Lasso solution estimated on sample</figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ni"><img src="../Images/901d523fc4c31a80236b3be287bc86e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScMmJYjYm-iERIXNQ0i2Rw.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Lasso solution</figcaption></figure><p id="18e0" class="pw-post-body-paragraph lp lq it lr b ls my ju lu lv mz jx lx lf na lz ma li nb mc md ll nc mf mg mh im bi translated">对于这种最小化，不存在解析解。梯度下降也不能保证收敛于这个损失函数，即使它是凸的。需要另一个公式来计算解决这个问题。</p><h1 id="8383" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">解决办法</h1><h2 id="56a4" class="la kj it bd kk lb lc dn ko ld le dp ks lf lg lh ku li lj lk kw ll lm ln ky lo bi translated">正交协方差的软阈值处理</h2><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nj"><img src="../Images/8f69291c69458ee8bae94656474b53d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1vMDMHr4RBRcwflP-F-jOA.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Equations copied from my personal notes</figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nk"><img src="../Images/3cad0543ec2b0d9c0e6bfe849a33c3fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_3t4hrmcVRKgdyM7vWliyA.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Assumption to simplify the analysis</figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8ebdcc1c609b9ef4500a6c395675402a.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*kdZ0h7-c6yFGGtIv9K0NFg.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Separating out the dimensions</figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/93f7cd9d4150276bb4ba853d773ba5c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*DEEDmMlrWUvMu9F-BmmfaA.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Soft thresholding</figcaption></figure><p id="cb20" class="pw-post-body-paragraph lp lq it lr b ls my ju lu lv mz jx lx lf na lz ma li nb mc md ll nc mf mg mh im bi translated">可以在每个维度上单独执行软阈值处理。这种更新将收敛到最佳β，因为它们是独立的。这种想法类似于<a class="ae mi" href="https://en.wikipedia.org/wiki/Likelihood_function#Profile_likelihood" rel="noopener ugc nofollow" target="_blank">轮廓似然</a> —关键参数的估计是通过描绘出(固定)噪声参数并最大化似然(假设为凸的)来完成的，然后通过将关键参数固定在其计算出的最佳值来估计噪声参数:这在参数独立时有效。</p><p id="1913" class="pw-post-body-paragraph lp lq it lr b ls my ju lu lv mz jx lx lf na lz ma li nb mc md ll nc mf mg mh im bi translated">这种一次更新一个参数的方法称为坐标下降法。</p><h2 id="d878" class="la kj it bd kk lb lc dn ko ld le dp ks lf lg lh ku li lj lk kw ll lm ln ky lo bi translated">坐标下降:一般情况</h2><p id="47fb" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx lf ly lz ma li mb mc md ll me mf mg mh im bi translated">对于没有正交设计的一般情况，坐标下降可总结如下:</p><ul class=""><li id="f7e6" class="nn no it lr b ls my lv mz lf np li nq ll nr mh ns nt nu nv bi translated">从猜测套索解开始，设置收敛参数ϵ</li><li id="1a5b" class="nn no it lr b ls nw lv nx lf ny li nz ll oa mh ns nt nu nv bi translated">迭代 I，从 1 到 max_iter:</li><li id="19c6" class="nn no it lr b ls nw lv nx lf ny li nz ll oa mh ns nt nu nv bi translated">—从 1 到 p 迭代 j:</li><li id="3a7d" class="nn no it lr b ls nw lv nx lf ny li nz ll oa mh ns nt nu nv bi translated">— —通过设置βⱼ = 0 来计算残差</li><li id="7af1" class="nn no it lr b ls nw lv nx lf ny li nz ll oa mh ns nt nu nv bi translated">— —回归第 j 个预测因子上的残差，得到第 j 个β的 OLS 解</li><li id="5a20" class="nn no it lr b ls nw lv nx lf ny li nz ll oa mh ns nt nu nv bi translated">— —对 OLS 解应用软阈值，获得第 j 个β的套索解</li><li id="309d" class="nn no it lr b ls nw lv nx lf ny li nz ll oa mh ns nt nu nv bi translated">— —检查收敛:β &lt; ϵ?</li><li id="1cf5" class="nn no it lr b ls nw lv nx lf ny li nz ll oa mh ns nt nu nv bi translated">Return final β. Did β converge in max_iter iterations?</li></ul><p id="3725" class="pw-post-body-paragraph lp lq it lr b ls my ju lu lv mz jx lx lf na lz ma li nb mc md ll nc mf mg mh im bi translated">Coordinate descent is guaranteed to converge in one iteration for orthogonal design. It is not guaranteed to converge in 1 iteration if the design matrix is not orthogonal, but it will converge in finite number of iterations.</p><h1 id="7e66" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">Geometric interpretation</h1><h2 id="10c4" class="la kj it bd kk lb lc dn ko ld le dp ks lf lg lh ku li lj lk kw ll lm ln ky lo bi translated">Dual form of optimization</h2><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8875fefb0545e2082a28bf1751c25b21.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*hO4BzvV8vFXzooXHBlaXVw.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Lasso loss</figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/0f3287c09ad7207c4d633851a66b6be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*wptS8Y6QsgPTgU3H46FfEA.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Dual form of optimization</figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi od"><img src="../Images/a4206adecf1182e3dd9d943a41b70533.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*k5hMt0uFu6VISYPiuNMtHw.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Contour plot for lasso. Image credits: <a class="ae mi" href="https://stats.stackexchange.com/questions/30456/geometric-interpretation-of-penalized-linear-regression" rel="noopener ugc nofollow" target="_blank">https://stats.stackexchange.com/questions/30456/geometric-interpretation-of-penalized-linear-regression</a></figcaption></figure><p id="c056" class="pw-post-body-paragraph lp lq it lr b ls my ju lu lv mz jx lx lf na lz ma li nb mc md ll nc mf mg mh im bi translated">The blue squares correspond to |β|₁ ≤ s for different s, where |β|₁ = constant along a square. Increasing λ decreases the size of the square. The red ellipses correspond to different distinct values of (y-xβ)₂² where (y-xβ)₂² = constant along an ellipse. For a fixed λ the value of s is fixed: this corresponds to one of the blue squares.</p><p id="e617" class="pw-post-body-paragraph lp lq it lr b ls my ju lu lv mz jx lx lf na lz ma li nb mc md ll nc mf mg mh im bi translated">The minimum value of (y-xβ)₂² in unconstrained case occurs at the center of the ellipse. However, under the constrained case of |β|₁ ≤ s the solution will be displaced towards the origin.</p><p id="bfcd" class="pw-post-body-paragraph lp lq it lr b ls my ju lu lv mz jx lx lf na lz ma li nb mc md ll nc mf mg mh im bi translated">The unique lasso solution is located at the point where these two ‘curves’ touch. Since the curve |β|₁ ≤ s is non-differentiable at few points the lasso solution for few βᵢs can be zero. On increasing λ (decreasing s) these βᵢs remain 0; other βᵢs tend to 0. This causes sparsity in the coefficients of lasso.</p><h1 id="8833" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">Extension to deep learning</h1><p id="af54" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx lf ly lz ma li mb mc md ll me mf mg mh im bi translated">Deep learning networks inevitably have fully-connected layers. These layers perform linear transformation on the input and apply an activation on the transformed variables. When the transformed outputs are small in magnitude (typically less than 1) the non-linearity can be ignored. With lasso penalty on the weights the estimation can be viewed in the same way as a linear regression with lasso penalty. The geometric interpretation suggests that for λ &gt; λ₁中更新的 L1 范数(最小λ，其中只有一个β估计为 0)我们将至少有一个权重= 0。这造成了权重的稀疏性。这一论点也适用于具有较大转换输入值的非线性情况。</p><h1 id="5cc2" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="5cfb" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx lf ly lz ma li mb mc md ll me mf mg mh im bi translated">套索惩罚通过将一些系数驱动到 0 来创建系数的稀疏性。这适用于线性回归和深度神经网络中的全连接层。因此，对于合适的λ值，lasso 惩罚可以降低深度学习模型的复杂性。然而，它不是所有问题的解决方案。</p><p id="5ed5" class="pw-post-body-paragraph lp lq it lr b ls my ju lu lv mz jx lx lf na lz ma li nb mc md ll nc mf mg mh im bi translated">如果基础模型是线性的(Y = Xβ + ϵ)，非零λ会导致套索解(E(βˡᵃˢˢᵒ)中的偏差≠ β，但估计量的方差低于最大似然法)，因此，即使对于简单的情况，它也不能同时实现估计和选择的一致性。尽管有这个缺点，但它是对有限样本过度拟合的一个好的解决方案，特别是在具有大量参数的深度神经网络中，这些参数往往会过度拟合。</p></div></div>    
</body>
</html>