<html>
<head>
<title>Turbocharging SVD with JAX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带 JAX 的涡轮增压 SVD</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/turbocharging-svd-with-jax-749ae12f93af?source=collection_archive---------22-----------------------#2019-09-05">https://towardsdatascience.com/turbocharging-svd-with-jax-749ae12f93af?source=collection_archive---------22-----------------------#2019-09-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="3cee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<a class="ae ko" rel="noopener" target="_blank" href="/pca-and-svd-explained-with-numpy-5d13b0d2a4d8">之前的文章</a>中，我写了两种常用的降维方法的基本原理，奇异值分解(SVD)和主成分分析(PCA)。我还用 numpy 解释了他们的关系。快速回顾一下，一个以 0 为中心的矩阵<strong class="js iu"> <em class="kp"> X </em> </strong> <em class="kp"> (n 个样本× m 个特征</em>)的奇异值(<strong class="js iu"><em class="kp">∑</em></strong>)<strong class="js iu"><em class="kp"/></strong>)，等于其特征值的平方根(<strong class="js iu"><em class="kp">λ</em></strong>)，使得使用 SVD 计算 PCA 成为可能，这通常更高效。</p><p id="af8c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，我将探索一些 SVD 算法，并用 Python 对平方稠密矩阵(<strong class="js iu"> <em class="kp"> X </em> </strong>，<em class="kp"> n×n </em>)进行基准测试。我还有一个非常令人兴奋的发现:最近开发的由<a class="ae ko" href="https://www.tensorflow.org/xla" rel="noopener ugc nofollow" target="_blank"> XLA 编译器</a>支持的<a class="ae ko" href="https://github.com/google/jax" rel="noopener ugc nofollow" target="_blank"> JAX 库</a>能够显著加速 SVD 计算。我们开始吧！</p><h1 id="da96" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">SVD 怎么算？</h1><p id="76c1" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">奇异值分解是探索性数据分析和机器学习中一种常见的基本技术，但我们实际上如何计算出左右酉矩阵(<strong class="js iu"> <em class="kp"> U </em> </strong>，<strong class="js iu"> <em class="kp"> V </em> </strong> *)和奇异值呢？通过我远非详尽的文献综述，至少有四种算法可以求解稠密矩阵的完全奇异值分解。甚至更多的算法可用于稀疏矩阵的截断/缩减 SVD，为了效率，其仅计算最大的<em class="kp"> k </em>奇异值。在本帖中，我将重点介绍密集矩阵和完整的 SVD 算法:</p><ol class=""><li id="cb66" class="lt lu it js b jt ju jx jy kb lv kf lw kj lx kn ly lz ma mb bi translated"><strong class="js iu">分治法(GESDD) </strong>:两阶段算法。它首先通过<a class="ae ko" href="http://mlwiki.org/index.php/Householder_Transformation" rel="noopener ugc nofollow" target="_blank"> Householder 反射</a>将输入矩阵简化为双对角形式，然后将双对角矩阵分成更小的矩阵，以计算奇异值和酉矩阵 U 和 v。最后，整个输入矩阵的奇异值就是更小的子矩阵的奇异值。在<a class="ae ko" href="https://en.wikipedia.org/wiki/Divide-and-conquer_eigenvalue_algorithm" rel="noopener ugc nofollow" target="_blank">这里</a>可以看到这个算法更深入的讲解。这种算法在概念上类似于<a class="ae ko" href="https://en.wikipedia.org/wiki/MapReduce" rel="noopener ugc nofollow" target="_blank"> MapReduce </a>，其中大块数据被分成多个块进行并行处理，每个块的结果可以使用简单的函数进行聚合，例如求和和连接。</li><li id="73b6" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu">通用矩形方法(GESVD) </strong>:也是一种两阶段算法，其中第一步与 GESDD 相同。第二步可以使用迭代<a class="ae ko" href="https://en.wikipedia.org/wiki/QR_decomposition" rel="noopener ugc nofollow" target="_blank"> QR 分解</a>得到奇异值。更多数学细节可在<a class="ae ko" href="https://web.stanford.edu/class/cme335/lecture6.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</li></ol><p id="bc58" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这两种算法都在用 Fortran 编写的经典线性代数库<a class="ae ko" href="http://www.netlib.org/lapack/" rel="noopener ugc nofollow" target="_blank"> LAPACK </a>中实现。分治法<a class="ae ko" href="https://www.netlib.org/lapack/lug/node32.html" rel="noopener ugc nofollow" target="_blank">被证明</a>比一般的矩形法快得多，但是占用更多的内存。</p><h1 id="2f72" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">Python 中的 SVD 实现</h1><p id="56b4" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">Scipy 和 Numpy 都包含在各自的线性代数子模块下计算 SVD 的方法:</p><ul class=""><li id="eafe" class="lt lu it js b jt ju jx jy kb lv kf lw kj lx kn mh lz ma mb bi translated"><a class="ae ko" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html" rel="noopener ugc nofollow" target="_blank"> numpy.linalg.svd </a>:“使用 LAPACK 例程` _gesdd '执行分解”。但通过查看这个函数的源代码，似乎 numpy 是通过一些用 c 编写的调用宏来调用 LAPACK 的，这样设计的目的很可能是为了规避构建 numpy 时对一个 Fortran 编译器的要求。</li><li id="11a8" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn mh lz ma mb bi translated"><a class="ae ko" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html" rel="noopener ugc nofollow" target="_blank"> scipy.linalg.svd </a>:提供更多参数，如<code class="fe mi mj mk ml b">overwrite_a</code>、<code class="fe mi mj mk ml b">check_finite</code>，为 svd 计算提供潜在的加速。它还有一个参数<code class="fe mi mj mk ml b">lapack_driver</code>，用于在 LAPACK 中的 GESDD 和 GESVD 算法之间进行选择。</li><li id="a0d3" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn mh lz ma mb bi translated"><a class="ae ko" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html" rel="noopener ugc nofollow" target="_blank">scipy . sparse . Lina LG . svds</a>:执行截断 SVD，计算稀疏矩阵的最大<em class="kp"> k </em>奇异向量。虽然这个函数也能够计算完整的 SVD，但是它的效率比 scipy.linalg.svd 低得多。</li></ul><p id="19ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可能想知道为什么在 numpy 和 scipy 中都有<code class="fe mi mj mk ml b">linalg.svd</code>的实现。<code class="fe mi mj mk ml b">np.linalg.svd</code>和<code class="fe mi mj mk ml b">scipy.linalg.svd</code>有什么区别？我在<a class="ae ko" href="https://www.scipy.org/scipylib/faq.html#why-both-numpy-linalg-and-scipy-linalg-what-s-the-difference" rel="noopener ugc nofollow" target="_blank"> scipy 的 FAQ </a>中找到了这个问题的答案:<code class="fe mi mj mk ml b">scipy.linalg</code>是使用 f2py 对 Fortran LAPACK 更完整的包装，而 numpy 则试图独立于 LAPACK。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/96891459d0826470e0a2a17215f06925.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*0BSHtInThjbHktxm7fExYA.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Timings for numpy/scipy SVD methods as a function of matrix size <em class="my">n</em></figcaption></figure><p id="3b04" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了比较不同 SVD 实现的速度，我建立了一个非常简单的基准，通过改变大小为<em class="kp"> n </em>的方阵的大小来测量 numpy 和 scipy 中 SVD 实现的执行时间。如上图所示，scipy ( <code class="fe mi mj mk ml b">scipy_svd</code>)和 numpy ( <code class="fe mi mj mk ml b">np_svd</code>)中的分治法(GESDD)比 QR 分解法(GESVD)快一个数量级以上(<code class="fe mi mj mk ml b">scipy_gesvd</code>)。scipy.linalg.svd 也比它的 numpy 版本快得多，这可能是因为它与 LAPACK 的通信效率更高。</p><h1 id="7fd9" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">JAX 和 XLA</h1><p id="d80d" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">让我们绕一小段路到 JAX，这是谷歌最近开发的开源库，旨在加速线性代数计算。<a class="ae ko" href="https://github.com/google/jax" rel="noopener ugc nofollow" target="_blank"> JAX </a>是一个“非官方”的谷歌项目，配备了<a class="ae ko" href="https://github.com/hips/autograd" rel="noopener ugc nofollow" target="_blank">亲笔签名的</a>和<a class="ae ko" href="https://www.tensorflow.org/xla" rel="noopener ugc nofollow" target="_blank"> XLA </a>编译器，汇集在一起进行高性能机器学习研究。XLA(加速线性代数)是 Tensorflow 背后的编译器，用于在不改变源代码的情况下加速模型计算。它的工作原理是将多个简单操作“融合”到一个内核中。JAX 在<a class="ae ko" href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="noopener ugc nofollow" target="_blank"> jit </a>(实时)函数中实现了 XLA 编译器，以允许显式编译 Python 函数。下面是 JAX 简单用法的快速浏览，展示了如何用 jit 编译器加速原始 Python 函数。</p><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="953d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">JAX 的另一个显著特点是，XLA 编译的代码会被自动缓存，如果重复运行可以重用，以进一步加快进程。这在<a class="ae ko" href="https://github.com/google/jax#a-brief-tour" rel="noopener ugc nofollow" target="_blank">教程</a>中有演示。</p><p id="014f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，JAX 还在子模块<code class="fe mi mj mk ml b">jax.numpy</code>和<code class="fe mi mj mk ml b">jax.scipy</code>下提供了 XLA 加速器支持的 numpy 和 scipy 函数，尽管目前并不支持所有的 numpy/scipy 函数。幸运的是，<code class="fe mi mj mk ml b">linalg.svd</code>对于 numpy 和 scipy 都已经在 JAX 实现了。</p><h1 id="cb74" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">密集矩阵的奇异值分解函数基准测试</h1><p id="65c1" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">随着 JAX 加速 numpy 和 scipy，我可以比较他们的表现与他们的香草同行。我使用了与之前相同的计时函数来获得以下结果。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/c6ee60d5406284f7e1f642156827163b.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*f-yskzjUe2uAwM2y02fXHA.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">First attempt in timing JAX powered SVD implementations</figcaption></figure><p id="f039" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">乍一看，这看起来很惊人:JAX 驱动的 numpy 和 scipy 都大大超过了 SVD 的简单的 numpy 和 scipy 实现。但是，这看起来好得令人难以置信:JAX 驱动的奇异值分解怎么可能快四个数量级，并且几乎有一个<strong class="js iu"><em class="kp">O</em></strong>【1】的复杂度呢？在检查了我的计时功能后，我发现 JAX 可能已经找到了一种方法，通过缓存以前运行的结果来作弊。为了防止 JAX 这样做，我编写了另一个计时函数，在每次试验中生成一个全新的随机密集矩阵，从而得到下面这个更真实的结果。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/05f8e8d9c948a7beed9fcc44502c8d1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*oXyPvqAratL01y_ZE6M8IQ.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Timings of SVD methods with and without JAX acceleration as a function of matrix size n</figcaption></figure><p id="30c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上图显示，numpy ( <code class="fe mi mj mk ml b">jnp_svd</code>)中 JAX 支持的 SVD 函数在速度上明显优于所有其他计数器，而 JAX 支持的 scipy 实现(<code class="fe mi mj mk ml b">jsp_svd</code>)与其普通的 scipy 对应物没有任何差异。值得更多地研究实现，找出原因。这可能与 scipy 和 numpy 与 LAPACK 交互的不同方法有关，这可能会影响它们被 XLA 编译器进一步优化的能力。</p><p id="3485" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">还有必要提一下，所有的基准测试结果都是从 CPU 运行时生成的。我还试图通过 Google Colab 在单个 GPU 和单个 TPU 运行时上运行它们。而且看起来 GPU 并没有促进<code class="fe mi mj mk ml b">jax.numpy</code>的加速，而 TPU 从 CPU 产生几乎相同的结果。这些观察结果也需要进一步研究。</p><p id="81b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是从所有这些基准测试结果中得到的好消息是，您可以通过切换到<code class="fe mi mj mk ml b">jax.numpy </code>来轻松加速您的机器学习工作流，而无需进行重大的代码更改！</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="4815" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以上所有基准测试的 Jupyter 笔记本都可以在这里找到:<a class="ae ko" href="https://gist.github.com/wangz10/d6f43db76ca260516d149d92b8834bf9" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/Wang 10/d6f 43 db 76 ca 260516d 149d 92 b 8834 bf9</a></p><h1 id="f1db" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">参考</h1><ul class=""><li id="061c" class="lt lu it js b jt lo jx lp kb nj kf nk kj nl kn mh lz ma mb bi translated"><a class="ae ko" href="http://fa.bianp.net/blog/2012/singular-value-decomposition-in-scipy/" rel="noopener ugc nofollow" target="_blank">SciPy 中的奇异值分解</a></li><li id="d065" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn mh lz ma mb bi translated"><a class="ae ko" href="https://jakevdp.github.io/blog/2012/12/19/sparse-svds-in-python/" rel="noopener ugc nofollow" target="_blank">Python 中的稀疏 SVDs】</a></li><li id="ab8a" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn mh lz ma mb bi translated"><a class="ae ko" href="https://www.scipy.org/scipylib/faq.html#why-both-numpy-linalg-and-scipy-linalg-what-s-the-difference" rel="noopener ugc nofollow" target="_blank">为什么既有 numpy.linalg 又有 scipy.linalg？有什么区别</a></li><li id="da55" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn mh lz ma mb bi translated"><a class="ae ko" href="http://www.math.pitt.edu/~sussmanm/2071Spring08/lab09/lab09.pdf" rel="noopener ugc nofollow" target="_blank">数学 2071:实验室#9:奇异值分解</a></li><li id="5587" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn mh lz ma mb bi translated"><a class="ae ko" href="https://en.wikipedia.org/wiki/Divide-and-conquer_eigenvalue_algorithm" rel="noopener ugc nofollow" target="_blank">分治特征值算法</a></li><li id="9d08" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn mh lz ma mb bi translated"><a class="ae ko" href="https://www.netlib.org/lapack/lug/node32.html" rel="noopener ugc nofollow" target="_blank">SVD 的 LAPACK 文档</a></li></ul></div></div>    
</body>
</html>