<html>
<head>
<title>Don’t Ever Ignore Reinforcement Learning Again</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不要再忽视强化学习了</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dont-ever-ignore-reinforcement-learning-again-4d026ee81371?source=collection_archive---------15-----------------------#2019-10-27">https://towardsdatascience.com/dont-ever-ignore-reinforcement-learning-again-4d026ee81371?source=collection_archive---------15-----------------------#2019-10-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="516a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">监督或无监督学习并不代表一切。每个人都知道。开始使用 OpenAI 健身房。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/48b79d9fee9a3269a3af77265d849fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vb-JsJ7ZywHZ9ZjOiEEvng.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Credits: Sophie Madeleine</figcaption></figure><p id="bb0e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你想在直升机上创造自动飞行特技动作吗？还是你在管理一个投资组合？</p><p id="2b18" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你想接管一个发电站吗？或者你的目标是控制人形机器人运动的动力？</p><blockquote class="lu"><p id="930a" class="lv lw it bd lx ly lz ma mb mc md lt dk translated">你想打败一个国际象棋、双陆棋或围棋的世界冠军吗？</p></blockquote><p id="84d5" class="pw-post-body-paragraph ky kz it la b lb me ju ld le mf jx lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">有一个地方你会解决这样的问题:强化学习。</p><h1 id="cc66" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">什么是强化学习？</h1><p id="4a71" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">强化学习是关于学习在一个环境中做出连续的决策，同时最大化在这个过程中获得的整体回报。</p><p id="0fed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">没有监管，只是环境的一个奖励信号。时间问题和行动会影响后续数据。这使得有监督和无监督的机器学习变得困难。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d9f93cded25ab11c16778ad605be53dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*71B3lRrFyHdqwXdPFFAi0A.png"/></div></figure><p id="29ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下面的例子中，老鼠试图找到尽可能多的食物，同时尽可能避免电击。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/389f0c93f515eed3771916cc64aff1a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*8Vf8davEBLciKu-2lLQs3w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">P. Protopapas, Harvard IACS</figcaption></figure><p id="c3b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这只老鼠可能很勇敢，为了到达有大量奶酪的地方而遭到电击。这将是一个比停滞不前、一无所获更好的结果。</p><p id="74fb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">老鼠不想在每一个特定的情况下都采取最佳行动。这样会想太多，不灵活。</p><p id="8bdb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">强化学习提供了一些神奇的方法，让我们的老鼠能够自己学习如何避免电和收集尽可能多的食物。</p><p id="a411" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">鼠标是<strong class="la iu">代理</strong>。有墙、奶酪和电的迷宫就是<strong class="la iu">环境</strong>。鼠标可以左右上下移动；这些是<strong class="la iu">动作</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/15fb96adc1951de6abbc4d30b0e54cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*hAJpJvdGzq9lKvyBx_Z4ag.png"/></div></figure><p id="dfee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">老鼠想要食物而不是电击；这些是<strong class="la iu">奖励</strong>。老鼠可以观察环境；这些是<strong class="la iu">观察结果</strong>。</p><h1 id="5638" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">冰上强化学习</h1><p id="bd8e" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">让我们把老鼠留在迷宫里，到冰上去。“冬天来了。当你和你的朋友在公园里扔飞盘时，你把飞盘扔到了湖中央。水大部分都结冰了，但有几个洞的冰已经融化了。”(<a class="ae nj" href="https://gym.openai.com/envs/FrozenLake-v0/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/755ff91c244a799dc7a88e0b17e81a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*LKM8B3-7VqIvvbneaYHfVw.png"/></div></figure><p id="08a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">“如果你踏入其中一个洞，你会掉进冰冷的水里。在这个时候，有一个国际飞盘短缺，所以它是绝对必要的，你航行穿过湖和检索光盘。”(<a class="ae nj" href="https://gym.openai.com/envs/FrozenLake-v0/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p><h2 id="0303" class="nl mk it bd ml nm nn dn mp no np dp mt lh nq nr mv ll ns nt mx lp nu nv mz nw bi translated">你如何处理这种情况？</h2><p id="5793" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">这是一个强化学习的问题。代理控制网格世界中角色的移动。网格的一些瓦片是可行走的，其他的导致代理人掉进水里。代理人因找到一条通往目标方块的可行走路径而获得奖励。</p><p id="c614" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以使用<a class="ae nj" href="https://github.com/openai/gym" rel="noopener ugc nofollow" target="_blank"> OpenAI Gym </a>对这个环境进行建模，这是一个开发和比较强化学习算法的工具包。它提供了对一组标准化环境的访问，比如我们例子中的那个，叫做<a class="ae nj" href="https://gym.openai.com/envs/FrozenLake-v0/" rel="noopener ugc nofollow" target="_blank">冰湖</a>。它是一个基于文本的玩具环境，只需要几行代码就可以加载。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="4f56" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们需要一个框架，允许我们系统地处理强化学习问题。</p><h1 id="db48" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">马尔可夫决策过程</h1><p id="1700" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">在我们的例子中，因为<em class="nz">代理</em>控制由格子中的瓷砖组成的格子世界中的角色的移动，这被称为<em class="nz">完全可观察环境。</em></p><p id="f699" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于给定当前瓦片，未来瓦片独立于过去瓦片(即，遵循<strong class="la iu">马尔可夫特性</strong>的随机状态序列)，因此我们正在处理所谓的<a class="ae nj" href="https://en.wikipedia.org/wiki/Markov_chain" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">马尔可夫过程</strong> </a>。</p><p id="78ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当前状态包含了决定未来行动所需的一切，不需要记忆。</p><p id="8c9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个时间步长，代理都处于一个状态，它选择一个动作(以一定的概率导致下一个状态)，环境返回一个观察和一个奖励。</p><p id="669e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在马尔可夫过程中加入报酬函数和折扣因子，我们得到了所谓的<em class="nz">马尔可夫报酬过程</em>。通过包含这组动作，我们获得一个<strong class="la iu"><em class="nz">【MDP】</em></strong>马尔可夫决策过程。下面更详细地定义了 MDP 的组件。</p><h2 id="959f" class="nl mk it bd ml nm nn dn mp no np dp mt lh nq nr mv ll ns nt mx lp nu nv mz nw bi translated"><em class="oa">状态</em></h2><p id="3180" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">状态是环境的一部分，是代理在特定时刻在环境中观察到的事物的数字表示，即湖的棋盘状态。这里，S 是起点，G 是目标，F 是代理人可以站立的固体冰，H 是如果代理人去，它会掉下来的洞。我们在 4x4 网格环境中有 16 个状态，或者在 8x8 环境中有 64 个状态。下面我们用 OpenAI Gym 渲染一个 4x4 网格环境的例子。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/670d751bc01e23dd62761d166cb57ba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:82/format:webp/1*OpgCQ1wUV9HOWsrbH77Bag.png"/></div></figure><h2 id="a86c" class="nl mk it bd ml nm nn dn mp no np dp mt lh nq nr mv ll ns nt mx lp nu nv mz nw bi translated"><em class="oa">动作</em></h2><p id="b2a0" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">代理有 4 种可能的移动，在环境中分别表示为 0、1、2、3，分别表示向左、向右、向下、向上。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1db52cf8e6b40656582bf3c35daf644c.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*ITmlnYcxYT74WXKbj4gs2g.png"/></div></figure><h2 id="cc3d" class="nl mk it bd ml nm nn dn mp no np dp mt lh nq nr mv ll ns nt mx lp nu nv mz nw bi translated"><em class="oa">状态转换模型</em></h2><p id="418e" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">状态转换模型描述了当代理根据当前状态执行动作时，环境状态如何变化。</p><p id="55dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它通常由转移概率来描述，转移概率表示为 N×N 大小的正方形转移矩阵，其中 N 是我们模型中的状态数。下面举例说明天气环境。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d0593a302b57899f011fc42b849b5d7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*VPfcvJVO3yzkNzi-3kV9uQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">P. Protopapas, Harvard IACS</figcaption></figure><p id="01bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在冰湖环境的情况下，我们假设湖是不滑的。如果我们向右走，我们只向右走。因此所有的概率都是相等的。</p><p id="7d81" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">“左”将代理 1 平铺向左移动，或者如果代理在左边界，则保持在当前位置。</p><p id="a2c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果代理在右边界，向右移动 1 格或保持在当前位置。</p><p id="52f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果代理位于上边框，则向上移动 1 格或保持在当前位置。</p><p id="e361" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">“向下”将它向下移动 1 格，或者如果代理在下边框，则保持在当前位置。</p><h2 id="e450" class="nl mk it bd ml nm nn dn mp no np dp mt lh nq nr mv ll ns nt mx lp nu nv mz nw bi translated"><em class="oa">奖励</em></h2><p id="dbca" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">对于每个状态 F，代理人获得 0 奖励，对于状态 H，它获得-1 奖励，因为在状态 H 中，代理人将死亡，并且在达到目标时，代理人获得+1 奖励。</p><p id="f3b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为状态转移模型和报酬模型都是确定性函数，这使得环境<em class="nz">确定性</em>。</p><h2 id="00b0" class="nl mk it bd ml nm nn dn mp no np dp mt lh nq nr mv ll ns nt mx lp nu nv mz nw bi translated"><em class="oa">折扣</em></h2><p id="8dfa" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">折扣是控制未来奖励重要性的可选因素。它的值介于 0 和 1 之间。其目的是防止总报酬趋于无穷大。</p><p id="57c9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">折扣也模拟了代理人的行为，当代理人更喜欢即时的回报，而不是可能在遥远的未来收到的回报。</p><h2 id="4747" class="nl mk it bd ml nm nn dn mp no np dp mt lh nq nr mv ll ns nt mx lp nu nv mz nw bi translated"><em class="oa">值</em></h2><p id="9b62" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">一个州的价值是该州的预期长期回报加上折扣。</p><h2 id="fa4a" class="nl mk it bd ml nm nn dn mp no np dp mt lh nq nr mv ll ns nt mx lp nu nv mz nw bi translated"><em class="oa">政策(π) </em></h2><p id="bf26" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">代理用来决定下一个动作的策略称为策略。在采取的所有政策中，最优政策是最大化一生中获得或预期获得的回报金额的政策。</p><h2 id="e262" class="nl mk it bd ml nm nn dn mp no np dp mt lh nq nr mv ll ns nt mx lp nu nv mz nw bi translated"><em class="oa">第一集</em></h2><p id="4e7f" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">当代理人在起始牌上时，一集开始，当代理人落入洞中或到达目标牌时结束。</p><h1 id="3284" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">让我们想象一下这一切</h1><p id="ab8c" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">在回顾了马尔可夫决策过程中涉及的所有概念之后，我们现在可以使用 OpenAI Gym 在 16x16 的环境中模拟一些随机行为。每次，代理都会选择一个随机操作并执行它。系统计算奖励，并显示环境的新状态。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a15a015d3ff83b7510fdd50cacb8e379.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*akfpiVGfTftaa9LcuW8xfA.jpeg"/></div></figure><h1 id="b6b6" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结论</h1><p id="1892" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">在本文中，我们简要介绍了强化学习中的关键概念。一个玩具示例提供了对 OpenAI Gym toolkit 的深入了解，这使得用预构建的环境进行实验变得很容易。</p><p id="26de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的下一篇文章<a class="ae nj" rel="noopener" target="_blank" href="/this-is-how-reinforcement-learning-works-5080b3a335d6">中，我们将说明如何设计和实现策略，这些策略将允许代理做出一系列必要的行动来达到目标并获得奖励，例如击败世界冠军。</a></p><div class="om on gp gr oo op"><a rel="noopener follow" target="_blank" href="/this-is-how-reinforcement-learning-works-5080b3a335d6"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd iu gy z fp ou fr fs ov fu fw is bi translated">这就是强化学习的工作原理</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">(什么会让你建立你的第一个人工智能)</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ks op"/></div></div></a></div><p id="3636" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读。</p></div></div>    
</body>
</html>