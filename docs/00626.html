<html>
<head>
<title>Principal Component Analysis (PCA) 101, using R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析(PCA) 101，使用 R</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff?source=collection_archive---------0-----------------------#2019-01-29">https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff?source=collection_archive---------0-----------------------#2019-01-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="5bc5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一次提高一个维度的可预测性和分类能力！使用 2D 图“可视化”30 个维度！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/7a182547fb60859db49fe8644e86d0bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oSOHZMoS-ZfmuAWiF8jY8Q.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Basic 2D PCA-plot showing clustering of “Benign” and “Malignant” tumors across 30 features.</figcaption></figure><p id="35d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你喜欢这篇文章并想看更多，请务必关注我的简介  <strong class="js iu"> </strong>！</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="fc46" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">设置</h1><p id="3bc6" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">对于本文，我们将使用来自<a class="ae le" href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29" rel="noopener ugc nofollow" target="_blank"> <em class="mp"> UCI 机器学习报告</em> </a>的乳腺癌威斯康星州数据集作为我们的数据。如果您想继续学习，请继续为自己加载:</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="b0a1" class="mv ln it mr b gy mw mx l my mz"><strong class="mr iu">wdbc </strong>&lt;- read.csv("wdbc.csv", header = F)</span><span id="51d7" class="mv ln it mr b gy na mx l my mz"><strong class="mr iu">features </strong>&lt;- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity", "concave_points", "symmetry", "fractal_dimension")</span><span id="faa4" class="mv ln it mr b gy na mx l my mz">names(<strong class="mr iu">wdbc</strong>) &lt;- c("<strong class="mr iu">id</strong>", "<strong class="mr iu">diagnosis</strong>", paste0(<strong class="mr iu">features</strong>,"<strong class="mr iu">_mean</strong>"), paste0(<strong class="mr iu">features</strong>,"<strong class="mr iu">_se</strong>"), paste0(<strong class="mr iu">features</strong>,"<strong class="mr iu">_worst</strong>"))</span></pre><p id="f32d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的代码将简单地加载数据并命名所有 32 个变量。<strong class="js iu"> ID </strong>、<strong class="js iu">诊断</strong>和十(30)个不同的特征。来自 UCI:</p><p id="3f5a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="mp"/><strong class="js iu"><em class="mp">表示</em> </strong> <em class="mp">，</em> <strong class="js iu"> <em class="mp">标准误差</em> </strong> <em class="mp">，</em> <strong class="js iu"> <em class="mp">最差</em> </strong> <em class="mp">或者最大(三个最大值的平均值)这些特征被计算出来，从而得到</em> <strong class="js iu"> <em class="mp"> 30 个特征</em> </strong> <em class="mp">。例如，字段 3 是平均半径，字段 13 是半径 SE，字段 23 是最差半径。”</em></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="aa46" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">为什么是 PCA？</h1><p id="c40b" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">对，现在我们已经加载了数据，发现自己有 30 个变量(因此排除了我们的响应“诊断”和不相关的 ID 变量)。</p><p id="a57e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在你们中的一些人可能会说“30 个变量太多了”，一些人可能会说“呸..才 30？我和成千上万的人合作过！!"但是请放心，这同样适用于任何一种场景..！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/99cb7a40851ee0960b9fb73e9bf34f6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/0*f5Raws3k935kqQCc.gif"/></div></figure><p id="bdb3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用 PCA 有几个很好的理由。本文开头的图是如何使用 PCA 绘制多维数据的一个很好的例子，我们实际上通过使用那些<strong class="js iu">两个主成分</strong>捕获了整个数据集中的<strong class="js iu">63.3%</strong>(dim 1 44.3%+dim 2 19%)的方差，当考虑到原始数据由<strong class="js iu"> 30 个特征</strong>组成，而这些特征不可能以任何有意义的方式绘制时，这是非常好的。</p><p id="4a19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个非常重要的考虑是承认<strong class="js iu">我们从未在我们的 PCA 图中指定反应变量</strong>或任何其他东西来指示肿瘤是“<em class="mp">良性</em>还是“<em class="mp">恶性</em>”。事实证明，当我们试图使用 PCA 的线性组合来描述数据中的差异时，我们发现“<em class="mp">良性</em>”和“<em class="mp">恶性</em>”肿瘤之间有一些非常明显的聚类和分离！这为基于我们的特征开发分类模型提供了一个很好的案例！</p><p id="1d89" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">PCA 的另一个主要“特性”(没有双关语的意思)是，它实际上可以直接提高模型的性能，请阅读这篇伟大的文章以了解更多信息:</p><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/dimensionality-reduction-does-pca-really-improve-classification-outcome-6e9ba21f0a32"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">降维——主成分分析真的能改善分类结果吗？</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">介绍</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt ky nf"/></div></div></a></div></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="6264" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">什么是 PCA，它是如何工作的？</h1><p id="e6b9" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">让我们立即解决一些问题，PCA 的主要目的是<strong class="js iu">而不是</strong>作为一种功能移除的方式！主成分分析可以减少维度，但是<strong class="js iu">它不会减少数据中的特征/变量的数量。</strong>这意味着您可能会发现，仅使用 3 个主成分就可以解释 1000 个特征数据集中 99%的差异，但您仍然需要这 1000 个特征来构建这 3 个主成分，这也意味着在预测未来数据的情况下，您仍然需要新观测数据中的 1000 个特征来构建相应的主成分。</p><h2 id="b2dc" class="mv ln it bd lo nu nv dn ls nw nx dp lw kb ny nz ma kf oa ob me kj oc od mi oe bi translated">好了，够了，它是怎么工作的？</h2><p id="9414" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">由于这纯粹是介绍性的，我将跳过数学，给你一个 PCA 工作的快速纲要:</p><ul class=""><li id="6412" class="of og it js b jt ju jx jy kb oh kf oi kj oj kn ok ol om on bi translated"><strong class="js iu">标准化数据</strong>(中心和刻度)。</li><li id="177a" class="of og it js b jt oo jx op kb oq kf or kj os kn ok ol om on bi translated"><strong class="js iu">从协方差矩阵或相关矩阵</strong>计算特征向量和特征值 <strong class="js iu">(也可以使用奇异向量分解)。</strong></li><li id="fc37" class="of og it js b jt oo jx op kb oq kf or kj os kn ok ol om on bi translated"><strong class="js iu">对特征值进行降序排序，选择<em class="mp"> K 个</em>最大特征向量</strong>(其中<em class="mp"> K </em>为新特征子空间 k ≤ d 的期望维数)。</li><li id="b2b3" class="of og it js b jt oo jx op kb oq kf or kj os kn ok ol om on bi translated"><strong class="js iu">从选择的<em class="mp"> K </em>个特征向量中构造投影矩阵 W </strong> <strong class="js iu">。</strong></li><li id="0b6b" class="of og it js b jt oo jx op kb oq kf or kj os kn ok ol om on bi translated"><strong class="js iu">通过 W</strong>T24】变换原始数据集 X，得到一个<em class="mp"> K </em>维特征子空间 y</li></ul><p id="e714" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你没有上过代数课，这听起来可能有点复杂，但它的要点是将我们的数据从它的初始状态<strong class="js iu"> X </strong>转换到一个具有<strong class="js iu"><em class="mp"/></strong>维度的子空间<strong class="js iu"> Y </strong>，其中<strong class="js iu"> <em class="mp"> K </em> </strong>通常是<em class="mp">——小于<strong class="js iu"> X </strong>的原始维度。幸运的是，使用 R 很容易做到这一点！</em></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="1d3d" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">肿瘤数据的主成分分析</h1><p id="bdfe" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">现在我们对 PCA 的工作原理有了一点了解，这就足够了。让我们实际尝试一下:</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="181a" class="mv ln it mr b gy mw mx l my mz">wdbc.pr &lt;- prcomp(wdbc[c(3:32)], center = TRUE, scale = TRUE)<br/>summary(wdbc.pr)</span></pre><p id="0cdf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是不言自明的，'<em class="mp"> prcomp </em>'函数对我们提供的数据运行 PCA，在我们的例子中，这是'<em class="mp">wdbc[c(3:32)]【T47]'数据，不包括 ID 和诊断变量，然后我们告诉 R 对我们的数据进行居中和缩放(因此<strong class="js iu">标准化</strong>数据)。最后我们呼吁总结一下:</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ot"><img src="../Images/4c6cc1ac0ce461ba253abcd9840be5aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Ljjg9rQSLrribGmySVmFg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">The values of the first 10 principal components</figcaption></figure><p id="3ad6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">回想一下，PCA 的一个特性是，我们的分量根据它们的标准差(<strong class="js iu">特征值</strong>)从最大到最小排序。所以让我们来理解这些:</p><ul class=""><li id="d96e" class="of og it js b jt ju jx jy kb oh kf oi kj oj kn ok ol om on bi translated"><strong class="js iu"> <em class="mp">标准差:</em> </strong>在我们的例子中，这只是<strong class="js iu">特征值</strong>，因为数据已经被居中和缩放(<strong class="js iu">标准化</strong>)</li><li id="ecf8" class="of og it js b jt oo jx op kb oq kf or kj os kn ok ol om on bi translated"><strong class="js iu"> <em class="mp">差异比例</em> </strong>:该组件在数据中所占的差异量，即。<strong class="js iu"> PC1 </strong>仅在数据中就占<strong class="js iu"> &gt;总方差</strong>的 44%！</li><li id="6a46" class="of og it js b jt oo jx op kb oq kf or kj os kn ok ol om on bi translated"><strong class="js iu"><em class="mp"/></strong>累计比例:简单来说就是解释方差的累计量，即。如果我们使用<strong class="js iu">前 10 个成分</strong>，我们将能够解释数据中总方差的<strong class="js iu"> &gt; 95%。</strong></li></ul><p id="699c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">好的，那么我们需要多少组件？我们显然希望能够解释尽可能多的差异，但要做到这一点，我们需要所有 30 个组件，同时我们希望减少维度的数量，所以我们肯定希望少于 30 个！</p><p id="86b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于我们对数据进行了标准化，现在我们有了每台电脑的相应特征值，我们实际上可以用它们来为我们画一个边界。因为一个<strong class="js iu">特征值&lt; 1 </strong>将意味着该分量实际上解释了少于单个解释变量，所以我们想要丢弃这些。如果我们的数据非常适合<strong class="js iu"> PCA </strong>，我们应该能够丢弃这些成分，同时保留至少<strong class="js iu">70–80%的累积方差</strong>。让我们绘制并观察:</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="f8ca" class="mv ln it mr b gy mw mx l my mz">screeplot(wdbc.pr, type = "l", npcs = 15, main = "Screeplot of the first 10 PCs")<br/>abline(h = 1, col="red", lty=5)<br/>legend("topright", legend=c("Eigenvalue = 1"),<br/>       col=c("red"), lty=5, cex=0.6)</span><span id="e861" class="mv ln it mr b gy na mx l my mz">cumpro &lt;- cumsum(wdbc.pr$sdev^2 / sum(wdbc.pr$sdev^2))<br/>plot(cumpro[0:15], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")<br/>abline(v = 6, col="blue", lty=5)<br/>abline(h = 0.88759, col="blue", lty=5)<br/>legend("topleft", legend=c("Cut-off @ PC6"),<br/>       col=c("blue"), lty=5, cex=0.6)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/55799c19e609fa4ab4f88a5bdbd36035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vEZjZOscRQV0uuksWK_ryw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd ov">Screeplot</strong> of the Eigenvalues of the first 15 PCs (<em class="ow">left</em>) &amp; <strong class="bd ov">Cumulative variance plot</strong> (right)</figcaption></figure><p id="66a6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们注意到<strong class="js iu">的前 6 个分量</strong>有一个<strong class="js iu">特征值&gt; 1 </strong>并且解释了几乎<strong class="js iu"> 90%的方差</strong>，这太棒了！我们可以有效地<strong class="js iu">将维度从 30 个减少到 6 个</strong>，同时只“损失”大约 10%的方差！</p><p id="4d6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们还注意到，仅用前两个成分，我们实际上就可以解释超过 60%的方差。让我们试着画出这些:</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="1e26" class="mv ln it mr b gy mw mx l my mz">plot(wdbc.pr$x[,1],wdbc.pr$x[,2], xlab="PC1 (44.3%)", ylab = "PC2 (19%)", main = "PC1 / PC2 - plot")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/dc16dab2fa45b6a09a785a8131844bae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sv08OjZ8QR1MeT06NhB_pg.png"/></div></div></figure><p id="632c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">好吧，这真的不太能说明问题，但是考虑一下，这代表了 30 维数据集中 60%以上的方差<strong class="js iu">。但是我们从中看到了什么？在<strong class="js iu">的上/中右有一些<strong class="js iu">群集</strong>正在进行。让我们也考虑一下这个分析的实际目标是什么。我们想解释一下<strong class="js iu">恶性</strong>和<strong class="js iu">良性</strong>肿瘤的区别。让我们将<strong class="js iu">响应变量</strong> ( <em class="mp">诊断</em>)添加到图表中，看看我们是否能更好地理解它:</strong></strong></p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="c015" class="mv ln it mr b gy mw mx l my mz">library("factoextra")<br/>fviz_pca_ind(wdbc.pr, geom.ind = "point", pointshape = 21, <br/>             pointsize = 2, <br/>             fill.ind = wdbc$diagnosis, <br/>             col.ind = "black", <br/>             palette = "jco", <br/>             addEllipses = TRUE,<br/>             label = "var",<br/>             col.var = "black",<br/>             repel = TRUE,<br/>             legend.title = "Diagnosis") +<br/>  ggtitle("2D PCA-plot from 30 feature dataset") +<br/>  theme(plot.title = element_text(hjust = 0.5))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/7a182547fb60859db49fe8644e86d0bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oSOHZMoS-ZfmuAWiF8jY8Q.png"/></div></div></figure><p id="ffe5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这基本上是完全相同的图，带有一些花哨的椭圆和颜色，对应于受试者的诊断，现在我们看到了<strong class="js iu">PCA</strong>的美妙之处。仅通过前两个组成部分，我们可以清楚地看到良性肿瘤<strong class="js iu">和恶性肿瘤<strong class="js iu">之间的一些区别。这清楚地表明数据非常适合某种分类模型</strong>(如<strong class="js iu">判别分析</strong>)。</strong></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="5e69" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">下一步是什么？</h1><p id="271d" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">我们的下一个直接目标是使用前 6 个主要成分构建某种模型来预测肿瘤是良性还是恶性，然后将其与使用原始 30 个变量的模型进行比较。</p><p id="459c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将在下一篇文章中讨论这个问题:</p><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/linear-discriminant-analysis-lda-101-using-r-6a97217a55a6"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">线性判别分析(LDA) 101，使用 R</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">决策边界、分离、分类等等。让我们潜入 LDA！</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="ox l nq nr ns no nt ky nf"/></div></div></a></div></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="cfc3" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">结束语</h1><p id="774e" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">如果你想看和了解更多，一定要关注我的<a class="ae le" href="https://medium.com/@peter.nistrup" rel="noopener"> <strong class="js iu">媒体</strong> </a>🔍<strong class="js iu"/><a class="ae le" href="https://twitter.com/peternistrup" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">碎碎念</strong> </a> <strong class="js iu"> </strong>🐦</p><div class="nc nd gp gr ne nf"><a href="https://medium.com/@peter.nistrup" rel="noopener follow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">彼得·尼斯特鲁普-中等</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">阅读彼得·尼斯特拉普在媒介上的作品。数据科学、统计和人工智能...推特:@PeterNistrup，LinkedIn…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">medium.com</p></div></div><div class="no l"><div class="oy l nq nr ns no nt ky nf"/></div></div></a></div></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="cecf" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">其他资源:</h1><div class="nc nd gp gr ne nf"><a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">理解主成分分析、特征向量和特征值</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">想象一下一次家庭聚餐，每个人都开始问你关于 PCA 的事情。首先你向你的…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">stats.stackexchange.com</p></div></div><div class="no l"><div class="oz l nq nr ns no nt ky nf"/></div></div></a></div><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/dive-into-pca-principal-component-analysis-with-python-43ded13ead21"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">用 Python 理解 PCA(主成分分析)</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">陷入变量的海洋来分析你的数据？在决定选择哪些功能时感到迷茫，以便…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="pa l nq nr ns no nt ky nf"/></div></div></a></div><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">主成分分析的一站式商店</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">在我用于研究生统计理论课的教科书的开始，作者(乔治·卡塞拉和罗杰…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="pb l nq nr ns no nt ky nf"/></div></div></a></div><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/https-medium-com-abdullatif-h-dimensionality-reduction-for-dummies-part-1-a8c9ec7b7e79"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">假人的降维第 1 部分:直觉</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">基于主成分分析和奇异值分解的维数约简。以简单、直观的方式解释。从大局到…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="pc l nq nr ns no nt ky nf"/></div></div></a></div><div class="nc nd gp gr ne nf"><a href="https://www.kaggle.com/shravank/predicting-breast-cancer-using-pca-lda-in-r" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">在 R | Kaggle 中使用 PCA + LDA 预测乳腺癌</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">编辑描述</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">www.kaggle.com</p></div></div></div></a></div><div class="nc nd gp gr ne nf"><a href="https://stats.stackexchange.com/questions/2592/how-to-project-a-new-vector-onto-pca-space" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">如何将新向量投影到 PCA 空间？</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">感谢您为交叉验证提供答案！请务必回答问题。提供详细信息并分享…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">stats.stackexchange.com</p></div></div><div class="no l"><div class="pd l nq nr ns no nt ky nf"/></div></div></a></div><div class="nc nd gp gr ne nf"><a href="http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">PCA -主成分分析要点</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">用于数据分析和可视化的统计工具</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">www.sthda.com</p></div></div><div class="no l"><div class="pe l nq nr ns no nt ky nf"/></div></div></a></div></div></div>    
</body>
</html>