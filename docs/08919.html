<html>
<head>
<title>EfficientDet: Scalable and Efficient Object Detection Review</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EfficientDet:可扩展和高效的对象检测综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/efficientdet-scalable-and-efficient-object-detection-review-4472ffc34fd9?source=collection_archive---------10-----------------------#2019-11-28">https://towardsdatascience.com/efficientdet-scalable-and-efficient-object-detection-review-4472ffc34fd9?source=collection_archive---------10-----------------------#2019-11-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="957e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">谷歌大脑团队最近发表了另一篇基于他们预览工作的对象检测论文，EfficientNet。该论文的作者提出了一种可扩展的检测架构，同时在广泛的资源限制范围内实现更高的准确性和效率。与其他检测网络相比，EfficientDet 网络系列可实现类似的性能，同时减少参数和触发器的数量。</p><h1 id="a78a" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">效率净值汇总</h1><p id="b142" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">在深入研究 EfficientDet 纸之前，我们首先需要了解 EfficientNet 纸。与 EfficientDet 不同，该网络旨在提升 ImageNet 域中的网络性能。EfficientNet 论文中的核心概念是网络在多个维度(输入分辨率、深度、宽度)上的复合扩展。EfficientNet 不是只扩展网络的一个或两个维度，而是扩展网络的所有维度。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/9da7aacd70c5687a6beb07b9bb5db3e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tGMIa1NR5pzTtOrjqIWIzw.png"/></div></div></figure><p id="52ea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如上图所示，传统方法只增加了网络的一个维度(宽度、深度或分辨率)。这可以通过手动调整缩放系数来提高模型的性能，但是找到正确值的过程是一个冗长的过程，并且经常导致次优的精度和效率。</p><p id="78ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当在 EfficientNet 中执行复合缩放时，使用固定的缩放系数统一执行每个维度中的调整。例如，当给定 2^N 计算资源时，我们可以通过α^N 增加网络深度，通过β^N 增加宽度，通过γ^N 增加图像大小，其中α、β和γ是通过在原始小模型上的小网格搜索确定的常系数。使用多目标神经架构搜索来执行找到基础效率 Net-B0 的过程。然后，从找到的基线网络开始，应用复合缩放方法来放大网络的维度。</p><p id="72f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要了解更多关于 EfficientNet 论文的信息，我建议在这里阅读论文<a class="ae md" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="a831" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">效率检测</h1><p id="99fe" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">在 EfficientDet 论文中，做出了两个主要贡献:(a) BiFPN 允许双向快速多尺度特征融合。(b)一种新的复合缩放方法联合放大主干、特征网络、盒/类网络和分辨率。使用 BiFPN 和新的复合缩放，形成了一系列高效的 Det 网络，在准确性和效率方面优于以前的对象检测器。</p><h2 id="b666" class="me kp it bd kq mf mg dn ku mh mi dp ky kb mj mk lc kf ml mm lg kj mn mo lk mp bi translated">BiFPN</h2><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mq"><img src="../Images/df9fb2e6564fa9bf4c66540088d3cbda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*funGGmYqT6f2VkRnhqd2rA.png"/></div></div></figure><p id="6604" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">BiFPN 的目标是以自上而下的方式有效地聚合多尺度特征。上图显示了基于 FPN 的不同汇总方法的比较。从(a)开始，典型的 FPN 网络融合了从 3 级到 7 级的多尺度特征。PANet 架构(b)在 FPN 之上增加了一个额外的自底向上的路径，以有效地利用较低层次的特性。节点之间的连接也可以通过使用神经结构搜索来找到。如(c)所示，这些连接不同于人类构建的连接。</p><p id="ef9d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过比较这三种架构，作者观察到 PANet 比 FPN 和 NAS-FPN 实现了更好的准确性，但代价是更多的参数和计算。从 PANet 架构开始，为了提高模型效率，EfficientDet 的作者首先删除只有一个输入边的节点，并创建 PANet 架构的简化版本，如(e)所示。直觉是，如果一个节点只有一条输入边而没有特征融合，那么它对旨在融合不同特征的特征网络的贡献将会更小。然后，如果节点处于同一级别，作者会从原始输入向输出节点添加一条额外的边。这样做是为了在不增加太多成本的情况下融合更多功能。最后，与只有一个自上而下和一个自下而上路径的 PANet 不同，双向(自上而下和自下而上)路径被视为一个要素网络图层，并多次重复同一图层以实现更高级别的要素融合。最终提出的 BiFPN 架构如(f)所示。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/695be7abff34aeb7366834889d44f516.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*HxIG_qCpgnlIB14-gaDxGA.png"/></div></figure><p id="72b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当特征被融合在一起时，利用<strong class="js iu">加权特征融合</strong>。由于不同分辨率下的不同输入特征对输出特征的贡献不相等，因此在进行特征融合时，每个输入的额外权重充当比例因子。具体而言，使用上式所示的<strong class="js iu">快速归一化融合</strong>。通过在每个ω后应用 relu 来确保每个ω的正定性，并添加ϵ以避免数值不稳定。</p><h2 id="d140" class="me kp it bd kq mf mg dn ku mh mi dp ky kb mj mk lc kf ml mm lg kj mn mo lk mp bi translated">复合缩放</h2><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi ms"><img src="../Images/847c666ef2cc7dd897017d234a75de33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5WTFs68ox6ibMC8CCxDuvg.png"/></div></div></figure><p id="9d77" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">网络复合扩展的目的是扩大基线模型(EfficientDet D0)的规模，以涵盖广泛的资源限制。一个简单的复合系数φ联合放大了主干网络(BiFPN)、类/箱网络和分辨率的所有维度。在 EfficientNet 论文中使用了网格搜索，但使用了基于启发式的缩放方法，因为对象检测器比图像分类模型具有更多的缩放维度。</p><p id="7da3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> (a)主干网络:</strong>使用与 B6 efficient net-B0 相同的宽度/深度比例系数来恢复 ImageNet-pretrained 检查点。</p><p id="9b86" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> (b) BiFPN 网络:</strong>使用等式(1)，BiFPN 网络的宽度以指数方式增长，深度以线性方式增加，以将深度值舍入为小整数。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/23fd79bea7361e94c7069ba1a6f207f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*rL1kXrz4IEhzvcHH72QWtw.png"/></div></figure><p id="26f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> (c)盒/类预测网络:</strong>预测网络的宽度固定为总是与 BiFPN 相同，但是预测网络的深度使用等式(2)线性增加。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/52a8e4a4da4240dc4334e689f7609cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*M3RCyXsbIDCVbFJJhFIm1w.png"/></div></figure><p id="76ee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> (d)输入图像分辨率:</strong>由于在 BiFPN 中使用特征级别 3–7，输入分辨率必须能被 2⁷ = 128 整除，因此使用等式(3)图像的分辨率线性增加。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/4cdb10e79012aaa1328fecea451906b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*PTtxeOZEojFRdLejA0s0eg.png"/></div></figure><h1 id="467a" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">结果</h1><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mw"><img src="../Images/78b82e7e027d141072f88fec8e79ecf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A-Jazqi-KZCQyfRNtzIduw.png"/></div></div></figure><p id="00b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用复合比例方法，可生成一系列 EfficientDet 网络(efficient det D0–07)。上表显示了 COCO 数据集上模型的单尺度性能。与以前的检测机相比，EfficientNet 型号在各种精度或资源限制下实现了更高的精度和效率。请注意，EfficientDet-D0 模型实现了与 YOLOv3 相似的精度，但触发器数量减少了 28 倍。此外，与 RetinaNet 和 Mask-RCNN 相比，EfficientDet-D1 实现了相似的精度，但参数减少了 8 倍，触发器减少了 25 倍。</p><p id="1980" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于那些想了解 EfficientDet 更多信息的人，我建议阅读整篇文章。下面提供了链接。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="8d78" class="ko kp it bd kq kr ne kt ku kv nf kx ky kz ng lb lc ld nh lf lg lh ni lj lk ll bi translated">参考</h1><div class="nj nk gp gr nl nm"><a href="https://arxiv.org/abs/1905.11946" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">EfficientNet:重新思考卷积神经网络的模型缩放</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">卷积神经网络(ConvNets)通常是在固定的资源预算下开发的，然后按比例放大用于…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="nj nk gp gr nl nm"><a href="https://arxiv.org/abs/1911.09070" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">EfficientDet:可扩展且高效的对象检测</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">模型效率在计算机视觉中变得越来越重要。在本文中，我们系统地研究了各种模型</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">arxiv.org</p></div></div></div></a></div></div></div>    
</body>
</html>