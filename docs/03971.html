<html>
<head>
<title>How XLNet combines the best of GPT and BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XLNet 如何结合 GPT 和伯特的精华</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-the-difference-of-gpt-bert-and-xlnet-in-2-min-8aa917330ad1?source=collection_archive---------11-----------------------#2019-06-22">https://towardsdatascience.com/understanding-the-difference-of-gpt-bert-and-xlnet-in-2-min-8aa917330ad1?source=collection_archive---------11-----------------------#2019-06-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="be59" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在 3 分钟内理解 GPT、伯特和 XLNet 的概念差异</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2eda3106d7e707c07deec804e28f1285.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ogCHmKPP6ZP2TR8lFtli8Q.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Centre Pompidou, Paris, France</figcaption></figure><p id="9c6b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">XLNet 是一个新的预训练模型，它在 20 个任务上持续优于 BERT，通常是大幅度优于 BERT。</p><p id="2b0e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">什么？！为什么呢？</strong></p><p id="67f5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果不了解机器学习，就不难认为我们捕捉到的上下文越多，预测就越准确。因此，一个模型能够最深入、最有效地捕捉上下文的能力就是制胜的秘诀。</p><p id="d0f3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们来玩一个游戏——在下面的语境中<strong class="kx ir">【猜测 1】</strong>和<strong class="kx ir">【猜测 2】</strong>是什么？</p><p id="1b01" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">['自然'，'语言'，'处理'，'是'，'一'，'婚姻'，'的'，<strong class="kx ir">【guess 1】</strong>，<strong class="kx ir">【guess 2】</strong>，'和'，'语言学']</p><p id="65ea" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">考虑到 3 分钟的时间限制，让我来揭晓答案，相反，我会问你:你认为哪个模型(GPT、伯特、XLNet)最有助于找到答案。</p><p id="feda" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">回答:</strong> ['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，<strong class="kx ir">，'机器'，</strong>，<strong class="kx ir">，'学习'，</strong>，'和'，'语言学']</p><p id="e5c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将继续使用 Pr(Guess|Context)符号。字面意思是根据上下文猜测的概率。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="cb12" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">GPT——我们从左向右阅读，所以我们不知道“机器”、“学习”之后的上下文:</strong></p><p id="a739" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Pr('机器' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'])</p><p id="491a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Pr('学习' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'机器'])</p><p id="30b1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">知道“机器”实际上有助于你猜测“学习”，因为随着机器学习的流行，“学习”经常跟在“机器”后面。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="6a28" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">伯特——我们知道与 GPT 相反的两个方面，但我们是基于相同的上下文猜测“机器”和“学习”:</strong></p><p id="b4af" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Pr('机器' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'和'，'语言学'])</p><p id="931e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Pr('学习' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'和'，'语言学'])</p><p id="23e6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">拥有“语言学”实际上可以帮助你猜测“机器”的“学习”，因为你知道自然语言处理是机器学习和语言学的完美结合。即使你不知道，有了“语言学”的存在，你至少知道它不是“语言学”。</p><p id="f9a8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以看到 BERT 的明显缺点是，它不能说明“机器”和“学习”是一个相当常见的术语。</p><p id="3951" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们如何结合 GPT 和伯特的优点？</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="996c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">XLNet——两者之长:</strong></p><p id="d657" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">排列！排列的力量在于，即使我们只从左向右阅读，排列也能让我们捕捉到两边的上下文(从左向右阅读，从右向左阅读)。</p><p id="7589" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">允许我们捕捉双方语境的排列之一:['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'和'，'语言学'，<strong class="kx ir">，'机器'</strong>，<strong class="kx ir">，'学习'</strong> ]</p><p id="96cf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Pr('机器' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'和'，'语言学'])</p><p id="b99e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Pr('学习' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'和'，'语言学'，'机器'])</p><p id="3dc1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这一次，你有了完整的上下文，在猜测“机器”之后，你可以立即猜测“学习”。你可以清楚地看到 XLNet 结合了 GPT 和伯特的优点。</p><p id="aba8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">仅此而已，希望这只是一个 3 分钟的阅读。如果你喜欢这篇文章，请鼓掌并分享！当然，如果你想知道更多，请阅读 XLNet <a class="ae ly" href="https://arxiv.org/pdf/1906.08237.pdf" rel="noopener ugc nofollow" target="_blank">的论文</a>。</p></div></div>    
</body>
</html>