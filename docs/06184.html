<html>
<head>
<title>Stochastic Gradient Descent — Clearly Explained !!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机梯度下降——解释清楚！！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31?source=collection_archive---------0-----------------------#2019-09-07">https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31?source=collection_archive---------0-----------------------#2019-09-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/4e318fc54f8e9567e2a5c9df75714026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a8eqycckkKZ7G_dYG0AFyQ.png"/></div></div></figure><p id="fba9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随机梯度下降是各种机器学习算法中非常流行和常用的算法，最重要的是形成了神经网络的基础。在这篇文章中，我尽力用简单的语言详细解释它。我强烈建议在阅读本文之前，先浏览一下<a class="ae kw" rel="noopener" target="_blank" href="/regression-explained-in-simple-terms-dccbcad96f61">线性回归</a>。</p><h2 id="dc78" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">梯度下降的目的是什么？</h2><p id="40d9" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">简单地说，坡度意味着一个表面的斜度。所以梯度下降字面上的意思是沿着斜坡下降，到达表面的最低点。让我们想象一个二维图形，比如下图中的抛物线。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/1bc082589e39fa8574d34735f9cd8958.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*D-G0RuBASeMkZQRQZRHuwA.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">A parabolic function with two dimensions (x,y)</figcaption></figure><p id="7f78" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上图中，抛物线的最低点出现在 x = 1 处。梯度下降算法的目标是找到“x”的值，使得“y”最小。“y”在这里被称为梯度下降算法操作的目标函数，以下降到最低点。</p><p id="25c8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在继续下一步之前，理解以上内容是很重要的。</p><h1 id="efba" class="me ky iq bd kz mf mg mh lc mi mj mk lf ml mm mn li mo mp mq ll mr ms mt lo mu bi translated">梯度下降——算法</h1><p id="328c" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">我用线性回归问题来解释梯度下降算法。正如我们从<a class="ae kw" rel="noopener" target="_blank" href="/regression-explained-in-simple-terms-dccbcad96f61">这篇</a>文章中回忆的，回归的目标是最小化残差平方和。我们知道，当斜率等于 0 时，函数达到最小值。通过使用该技术，我们解决了线性回归问题并学习了权重向量。同样的问题可以用梯度下降技术解决。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="10b4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">“梯度下降是一种迭代算法，它从一个函数上的随机点开始，一步一步地沿着它的斜率向下移动，直到它到达那个函数的最低点。”</strong></p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="4d4b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在通过将函数的斜率等于 0 而找不到最佳点的情况下，该算法很有用。在线性回归的情况下，您可以在脑海中将残差平方和映射为函数“y ”,将权重向量映射为上面抛物线中的“x”。</p><h2 id="f7b2" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">如何分步下移？</h2><p id="e385" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">这是算法的关键。总的想法是从一个随机的点开始(在我们的抛物线例子中，从一个随机的“x”开始)，并找到一种方法在每次迭代中更新这个点，这样我们就可以沿着斜坡下降。</p><p id="91d7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该算法的步骤是</p><ol class=""><li id="2805" class="nc nd iq ka b kb kc kf kg kj ne kn nf kr ng kv nh ni nj nk bi translated">找到目标函数<strong class="ka ir">相对于每个参数/特征</strong>的斜率。换句话说，计算函数的梯度。</li><li id="4244" class="nc nd iq ka b kb nl kf nm kj nn kn no kr np kv nh ni nj nk bi translated">为参数选择一个随机的初始值。(为了澄清，在抛物线的例子中，区分“y”和“x”。如果我们有更多像 x1、x2 等功能。我们对每个特征取“y”的偏导数。)</li><li id="3923" class="nc nd iq ka b kb nl kf nm kj nn kn no kr np kv nh ni nj nk bi translated">通过插入参数值来更新梯度函数。</li><li id="2670" class="nc nd iq ka b kb nl kf nm kj nn kn no kr np kv nh ni nj nk bi translated">计算每个特征的步长:<strong class="ka ir">步长=梯度*学习率。</strong></li><li id="6320" class="nc nd iq ka b kb nl kf nm kj nn kn no kr np kv nh ni nj nk bi translated">计算新参数如下:<strong class="ka ir">新参数=旧参数-步长</strong></li><li id="72af" class="nc nd iq ka b kb nl kf nm kj nn kn no kr np kv nh ni nj nk bi translated">重复步骤 3 至 5，直到梯度几乎为 0。</li></ol><p id="420d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上面提到的“学习率”是一个灵活的参数，它严重影响算法的收敛性。较大的学习率使得算法在斜坡上向下迈出巨大的步伐，并且它可能跳过最小点，从而错过它。所以，坚持 0.01 这样的低学习率总是好的。从数学上也可以看出，如果起点较高，梯度下降算法会在斜坡上向下迈出较大的步伐，当它接近目的地时会迈出较小的步伐，以确保不会错过它，并且足够快。</p><h1 id="89ff" class="me ky iq bd kz mf mg mh lc mi mj mk lf ml mm mn li mo mp mq ll mr ms mt lo mu bi translated">随机梯度下降</h1><p id="eb60" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">梯度下降算法有一些缺点。我们需要更仔细地看看我们为算法的每次迭代所做的计算量。</p><p id="58d1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们有 10，000 个数据点和 10 个特征。残差平方和由与数据点一样多的项组成，因此在我们的例子中有 10000 项。我们需要计算这个函数相对于每个特征的导数，所以实际上我们每次迭代要做 10000 * 10 = 100，000 次计算。通常需要 1000 次迭代，实际上我们有 100，000 * 1000 = 100000000 次计算来完成算法。这是相当大的开销，因此梯度下降在大数据上是缓慢的。</p><p id="3bb1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随机梯度下降来救我们了！！“随机”，简单来说就是“随机”的意思。</p><p id="2b12" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">在梯度下降算法中，我们可以在哪里引入随机性？？</strong></p><p id="6d9c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">是的，你可能猜对了！！在每一步选择数据点来计算导数。SGD 在每次迭代中从整个数据集中随机选取一个数据点，以极大地减少计算量。</p><p id="7c3e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通常在每一步采样少量的数据点，而不是一个点，这被称为“小批量”梯度下降。Mini-batch 试图在梯度下降的优势和 SGD 的速度之间取得平衡。</p><h1 id="e7b3" class="me ky iq bd kz mf mg mh lc mi mj mk lf ml mm mn li mo mp mq ll mr ms mt lo mu bi translated">结论</h1><p id="677b" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">我希望这篇文章有助于掌握该算法。敬请关注更多文章。请在下面留下您的评论/疑问。</p></div></div>    
</body>
</html>