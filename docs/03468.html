<html>
<head>
<title>Gaussian Mixture Models Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高斯混合模型解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95?source=collection_archive---------0-----------------------#2019-06-03">https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95?source=collection_archive---------0-----------------------#2019-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7efc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从直觉到实施</h2></div><p id="66a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在机器学习领域，我们可以区分两个主要领域:监督学习和非监督学习。两者的主要区别在于数据的性质以及处理数据的方法。聚类是一个无监督的学习问题，我们希望在数据集中找到具有一些共同特征的点的聚类。假设我们有一个类似这样的数据集:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/92c1b7254aacd8706a7203a016bc4916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*s_Ht2xwPeC54j_yIDbb8aw.png"/></div></figure><p id="d79e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的工作是找到看起来很接近的几组点。在这种情况下，我们可以清楚地识别出两组点，我们将分别将它们着色为蓝色和红色:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/f8efd63dee0a1fe58675e18fea2cf825.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*1kqIsUwCRhibCKEQgf9pKQ.png"/></div></figure><p id="31f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我们现在引入了一些额外的符号。这里，μ1 和μ2 是每个聚类的质心，并且是识别每个聚类的参数。一种流行的聚类算法被称为 K-means，它将遵循迭代方法来更新每个聚类的参数。更具体地说，它要做的是计算每个聚类的均值(或质心)，然后计算它们到每个数据点的距离。后者随后被标记为由其最近质心识别的聚类的一部分。重复该过程，直到满足某种收敛标准，例如当我们看不到集群分配中的进一步变化时。</p><p id="6e32" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">K-means 的一个重要特点是它是一个<em class="lm">硬聚类方法</em>，也就是说它会将每个点关联到一个且仅一个聚类。这种方法的一个局限是没有不确定性度量或<em class="lm">概率</em>来告诉我们一个数据点与一个特定的聚类有多少关联。那么，使用软集群而不是硬集群怎么样呢？这正是高斯混合模型(简称 GMM)试图做的事情。现在让我们进一步讨论这个方法。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="10fe" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">定义</h1><p id="65b3" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">一个<em class="lm">高斯混合</em>是一个由几个高斯组成的函数，每个高斯由<em class="lm"> k </em> ∈ {1，…，<em class="lm"> K </em> }标识，其中<em class="lm"> K </em>是我们数据集的聚类数。混合物中的每个高斯 k 由以下参数组成:</p><ul class=""><li id="767a" class="mr ms it kk b kl km ko kp kr mt kv mu kz mv ld mw mx my mz bi translated">定义其中心的平均值μ。</li><li id="0afd" class="mr ms it kk b kl na ko nb kr nc kv nd kz ne ld mw mx my mz bi translated">定义其宽度的协方差σ。这相当于多变量场景中椭球体的维度。</li><li id="389c" class="mr ms it kk b kl na ko nb kr nc kv nd kz ne ld mw mx my mz bi translated">定义高斯函数大小的混合概率π。</li></ul><p id="38f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们用图表来说明这些参数:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nf"><img src="../Images/5e6c1cc25348430f9c36b9298ea92ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTv7e4Cdlp738X_WFZyZHA.png"/></div></div></figure><p id="fb49" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们可以看到有三个高斯函数，因此<em class="lm"> K </em> = 3。每个高斯解释了三个可用分类中包含的数据。混合系数本身是概率，必须满足以下条件:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/e08c707b97e71d253d9a9bc1a7de3bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*FT0gHnR60NXRPi3VMgOoeA.png"/></div></figure><p id="2804" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们如何确定这些参数的最佳值呢？为了实现这一点，我们必须确保每个高斯拟合属于每个聚类的数据点。这正是最大似然法的作用。</p><p id="4359" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，高斯密度函数由下式给出:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nl"><img src="../Images/57e8ec45cddfaa93a19864b72558d8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qUy5tdKD3JF8SBpGfN9TpQ.png"/></div></div></figure><p id="6bb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="kk iu"> x </strong>代表我们的数据点，<em class="lm"> D </em>是每个数据点的维数。μ和σ分别是均值和协方差。如果我们有一个由<em class="lm"> N </em> = 1000 个三维点(<em class="lm"> D </em> = 3)组成的数据集，那么<strong class="kk iu"> x </strong>将是一个 1000 × 3 的矩阵。μ将是 1 × 3 的向量，σ将是 3 × 3 的矩阵。为了后面的目的，我们还会发现取这个等式的对数是有用的，它由下式给出:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nm"><img src="../Images/49cc70cb759f1c89beb72cb14f7fe79d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xzCG9T8h0hTVyK0dmTRXeA.png"/></div></div></figure><p id="3829" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们对该方程的均值和协方差进行微分，然后使其等于零，那么我们将能够找到这些参数的最优值，并且解将对应于该设置的最大似然估计(MLE)。然而，因为我们处理的不是一个，而是许多个高斯函数，当我们找到整个混合物的参数时，事情会变得有点复杂。在这方面，我们需要引入一些额外的方面，我们将在下一节中讨论。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="cc9f" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">初始导数</h1><p id="6955" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">我们现在要引入一些额外的符号。只是一句警告。数学来了！别担心。为了更好地理解推导，我将尽量保持符号的简洁。首先假设我们想知道一个数据点<strong class="kk iu">x</strong>n 来自高斯<em class="lm"> k </em>的概率是多少。我们可以这样表达:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/39fb64f7625951c00e5cb6aec3a2b1ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*1QPoMlE66eNz7XEc5rDNCg.png"/></div></figure><p id="3a05" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面写着“<em class="lm">给定一个数据点</em> <strong class="kk iu"> x </strong> <em class="lm">，它来自高斯 k 的概率是多少？”</em>在这种情况下，<em class="lm"> z </em>是一个<em class="lm">潜在变量</em>，它只取两个可能的值。当<strong class="kk iu"> x </strong>来自高斯<em class="lm"> k </em>时为 1，否则为零。实际上，我们在现实中看不到这个<em class="lm"> z </em>变量，但是知道它出现的概率将有助于我们确定高斯混合参数，正如我们稍后讨论的。</p><p id="65e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，我们可以陈述如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/c67e433b86f8dcaccadae8c94ce7902c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*sbk2KHLJ_N2YznI-nLFVsA.png"/></div></figure><p id="b414" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着观察到来自高斯<em class="lm"> k </em>的点的总概率实际上等于该高斯的混合系数。这是有意义的，因为高斯越大，我们期望的概率就越高。现在让<strong class="kk iu"> z </strong>成为所有可能的潜在变量<em class="lm"> z </em>的集合，因此:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/a02c8461323311e924669f6c1158e2e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*0ZXQljahC7qtdWcgpF3cHw.png"/></div></figure><p id="89da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们预先知道每个<em class="lm"> z </em>独立于其他出现，并且当<em class="lm"> k </em>等于该点来自的集群时，它们只能取值 1。因此:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e7bbde533b46e3bf52e674d535034e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*iEtghc45ARgqoPDbXYgkcQ.png"/></div></figure><p id="69a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，假设我们的数据来自高斯函数<em class="lm"> k </em>，那么如何找到观察数据的概率呢？原来它其实就是高斯函数本身！遵循我们用来定义<em class="lm"> p </em> ( <strong class="kk iu"> z </strong>)的相同逻辑，我们可以声明:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d1fe7c555dfd1426d81ff26c565e4957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*a0ar35yudBa53OEKwmKM_Q.png"/></div></figure><p id="f180" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好吧，现在你可能会问，我们为什么要做这些？还记得我们最初的目标是确定给定我们的观测值<strong class="kk iu"> x </strong>时<em class="lm"> z </em>的概率吗？事实证明，我们刚刚推导出的方程，以及贝叶斯法则，将帮助我们确定这个概率。根据概率的乘积法则，我们知道</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/fcea3fd85b7326f3207dd0e47db7bc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*Q1o34nS66t0nRCy3AdOfRA.png"/></div></figure><p id="2b4f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，看起来我们现在有所进展了。右边的操作数是我们刚刚找到的。也许你们中的一些人会预料到，我们会用贝叶斯法则来得到我们最终需要的概率。但是，首先我们会需要<strong class="kk iu"><em class="lm">p</em>(x</strong><em class="lm">n</em><strong class="kk iu">)</strong>，而不是<strong class="kk iu"><em class="lm">p</em>(x</strong><em class="lm">n</em>，<strong class="kk iu"> z) </strong>。那么我们这里怎么去掉<strong class="kk iu"> z </strong>呢？是的，你猜对了。边缘化！我们只需要总结一下 z 轴上的术语，因此</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/ebc83e356f9fed0fe25d721e70ddf81f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*yke-ln6cCHEbL4KcieYjmA.png"/></div></figure><p id="5d37" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是定义高斯混合的方程，你可以清楚地看到，它取决于我们之前提到的所有参数！为了确定这些的最优值，我们需要确定模型的最大似然。我们可以找到所有观测值的联合概率<strong class="kk iu">x</strong>n 的可能性，定义如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/fe80d3214b69f9761333329d8e5a3db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*0qbBe3o8UCGYN5kcb9Lqeg.png"/></div></figure><p id="95b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像我们对原始高斯密度函数所做的那样，让我们将对数应用于等式的每一侧:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/3e6291cc4fe3f70b1b6750fe2c33a2be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*7cOQv0wHAiN9zm5MBfCT1A.png"/></div></figure><p id="476d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太好了！现在为了找到高斯混合的最佳参数，我们要做的就是对这个方程的参数求导，这样就完成了，对吗？等等！没那么快。我们这里有一个问题。我们可以看到，有一个对数正在影响第二次求和。计算这个表达式的导数，然后求解参数，会非常困难！</p><p id="901f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们能做什么？嗯，我们需要用迭代法来估计参数。但是首先，记住我们应该找到给定<strong class="kk iu"> x </strong>的<em class="lm"> z </em>的概率？好，让我们这样做，因为在这一点上，我们已经有了定义这个概率的一切。</p><p id="d7a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据贝叶斯法则，我们知道</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a3858eb7849efccf5d75016fe36a7942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*Tt1J84V6Qao24yYHLRDB9A.png"/></div></figure><p id="2bb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从我们之前的推导中，我们了解到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6af53535d6739b5af19c73aa8fc2ae6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*u9tZEOBHwWsUjyFUQjQ8OA.png"/></div></figure><p id="1626" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们将这些代入上一个等式:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi no"><img src="../Images/02bc1484f5a814a7dd618e6aa7d319d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*riCgo81TEX0-tWFyi1IeWg.png"/></div></figure><p id="da2d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是我们一直在寻找的！接下来，我们会经常看到这个表达。接下来，我们将继续讨论一种方法，这种方法将帮助我们容易地确定高斯混合的参数。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h2 id="896d" class="np lv it bd lw nq nr dn ma ns nt dp me kr nu nv mg kv nw nx mi kz ny nz mk oa bi translated">期望值最大化算法</h2><p id="9e6e" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">好了，在这一点上，我们已经得到了一些概率的表达式，我们会发现这些表达式对确定模型的参数很有用。然而，在上一节中，我们可以看到，简单地评估(3)来找到这样的参数将被证明是非常困难的。幸运的是，我们可以使用一种迭代方法来达到这个目的。它被称为<em class="lm">期望——最大化</em>，或者简称为<em class="lm"> EM 算法</em>。它广泛用于目标函数复杂的优化问题，比如我们刚刚遇到的 GMM 问题。</p><p id="befa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们的模型的参数</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/18e976cb4cddd65fe07e21cf621e025d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*rVAJtuaMXEgcdklaSBRzQw.png"/></div></figure><p id="1836" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们定义一般 EM 算法将遵循的步骤。</p><p id="42e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">步骤 1: </strong>相应地初始化<em class="lm"> θ </em>。例如，我们可以使用前一次 K-Means 运行获得的结果作为我们算法的良好起点。</p><p id="4b82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第二步(期望步骤):</strong>评估</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/7c8ca675983586f8866fea777950c947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*uXrWst1bgPCMCDInvA8nZA.png"/></div></figure><p id="6b08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，其实我们已经找到了<em class="lm"> p </em> ( <strong class="kk iu"> Z </strong> | <strong class="kk iu"> X，</strong> <em class="lm"> θ </em>)。还记得上一节我们最后用的γ表达式吗？为了更好地理解，让我们将之前的等式(4)放在这里:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi no"><img src="../Images/896a5fe1b7e45cf428332647b626adca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*nrAnmM6XPi6XqlBAH9WaOA.png"/></div></figure><p id="1696" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于高斯混合模型，期望步骤归结为通过使用旧的参数值来计算(4)中γ的值。现在，如果我们将(4)替换为(5)，我们将得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/c0f8777464cc7fb6b5f4bfa50ac43c29.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*rB6TclaQ8uUseyMjP3tENw.png"/></div></figure><p id="5790" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">听起来不错，但是我们还缺少<em class="lm"> p </em> ( <strong class="kk iu"> X </strong>，<strong class="kk iu"> Z </strong> | <em class="lm"> θ* </em>)。怎么才能找到呢？嗯，实际上并没有那么难。就是模型的完全似然，既包括<strong class="kk iu"> X </strong>又包括<strong class="kk iu"> Z </strong>，我们可以用下面的表达式求出:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/04029b820badf828d5cf8d9414056ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*pQ1SZu-YP4NeoPUpQbx-Xw.png"/></div></figure><p id="c683" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是计算所有观测值和潜在变量的联合概率的结果，是我们对<em class="lm"> p </em> ( <strong class="kk iu"> x </strong>)的初始推导的扩展。该表达式的对数由下式给出</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nm"><img src="../Images/5faacf19eb9edca36ae0edf1829dc7a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6DkOhWaEaaw1GJk6kvrHSQ.png"/></div></div></figure><p id="fd63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不错！我们终于摆脱了影响(3)中求和的这个麻烦的对数。有了所有这些，我们通过最大化关于参数的<em class="lm"> Q </em>来估计参数就容易多了，但是我们将在<em class="lm">最大化步骤</em>中处理这个问题。此外，请记住，每次求和时，潜在变量<em class="lm"> z </em>将<strong class="kk iu">仅</strong>为 1。有了这些知识，我们就可以根据推导的需要轻松地去掉它。</p><p id="b799" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们可以将(6)中的(7)替换为:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/54e4fe80a76cba19cb47a3197e6e7810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*UsK-RCuxpvErgoBdyOT18g.png"/></div></figure><p id="9ded" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在最大化步骤中，我们将找到混合物的修正参数。出于这个目的，我们将需要使 Q 成为一个受限的最大化问题，因此我们将向(8)添加一个拉格朗日乘数。现在让我们回顾一下最大化步骤。</p><p id="1d5e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">步骤 3(最大化步骤):</strong>使用以下公式找到修正参数<em class="lm"> θ </em> *</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/2143b3b69020f0b13119af077e958325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*95xfzKBzaqYC36b33lVgAw.png"/></div></figure><p id="ccf7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在哪里</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/54e4fe80a76cba19cb47a3197e6e7810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*UsK-RCuxpvErgoBdyOT18g.png"/></div></figure><p id="5440" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是我们上一步的结果。但是，Q 还应考虑所有π值总和应为 1 的限制。为此，我们需要添加一个合适的拉格朗日乘数。因此，我们应该这样重写(8):</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nm"><img src="../Images/333ced42e105e46ee9806de3bbd7c0d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sy0tvlMp_9N9PvLwW77KGw.png"/></div></div></figure><p id="d2ae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以很容易地用最大似然法来确定参数。现在让我们对π求<em class="lm"> Q </em>的导数，并将其设为 0:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/9b0af7b896728858b81779c07cf7352a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*LOU3Ys_pFF7QlkdiUrbEbQ.png"/></div></figure><p id="a1aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，通过重新排列各项，并对等式两边的<em class="lm"> k </em>求和，<em class="lm"> </em>我们得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/085c70b7319c8959cb5350f1bfab195a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*CMbE92fQDmJZmgHawXFVaw.png"/></div></figure><p id="6818" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从(1)中，我们知道所有混合系数的总和π等于 1。此外，我们知道，将概率γ与 k 相加也会得到 1。因此我们得到<em class="lm"> λ </em> = <em class="lm"> N </em>。利用这个结果，我们可以求解<em class="lm"> π </em>:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/08b93d8c1ef5fba761b0df5861b00198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*VuTnZ5klSeqrYY1TK92DiQ.png"/></div></figure><p id="1194" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，如果我们将<em class="lm"> Q </em>对μ和σ求导，使导数等于零，然后利用我们定义的对数似然方程(2)求解参数，我们得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nm"><img src="../Images/286af72ae7d02c541ed693448beebb9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VL-9JyHWZF-zZzZRPATbEQ.png"/></div></div></figure><p id="c53b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就是这样！然后，我们将在下一次 EM 迭代中使用这些修正值来确定γ，以此类推，直到我们看到似然值有所收敛。我们可以使用等式(3)来监控每一步中的对数似然，并且我们总是保证达到局部最大值。</p><p id="fc27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果能看到我们如何用编程语言实现这个算法就好了，不是吗？接下来，我们将看到我提供的 Jupyter 笔记本的一部分，这样您就可以看到 GMMs 在 Python 中的工作实现。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h2 id="260c" class="np lv it bd lw nq nr dn ma ns nt dp me kr nu nv mg kv nw nx mi kz ny nz mk oa bi translated">用 Python 实现</h2><p id="bffb" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">顺便提一下，完整的实现可以在 https://bit.ly/2MpiZp4 的 Jupyter 笔记本上获得</p><p id="8e38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在这个练习中使用了 Iris 数据集，主要是为了简单和快速训练。根据我们之前的推导，我们说明了 EM 算法遵循迭代方法来寻找高斯混合模型的参数。我们的第一步是初始化我们的参数。在这种情况下，我们可以使用 K-means 的值来满足这个目的。这方面的 Python 代码如下所示:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="936b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们执行期望步骤。我们在这里计算</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/84689f11d529e791c23e39dc2e79d4bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*fuTitoDTz9DBQ2SZ8yLhgg.png"/></div></figure><p id="69ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相应的 Python 代码如下所示:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="177c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，为了计算总和，我们只需利用分子中的项，然后进行相应的除法运算。</p><p id="2451" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后是最大化步骤，我们计算</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/a4a961c31c453ce9e993a507a9fe8fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*yrm_xupJbeQWfL11LpdcTA.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nm"><img src="../Images/286af72ae7d02c541ed693448beebb9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VL-9JyHWZF-zZzZRPATbEQ.png"/></div></div></figure><p id="eba0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相应的 Python 代码如下所示:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="b7ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，为了稍微简化计算，我们使用了:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/bafcf5f5bebcac25decdebc3e51db01b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*ASLkhCUNGkZFHf1rqzQIkQ.png"/></div></figure><p id="1141" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们还有对数似然计算，由下式给出</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3ee889042b9aa848dd5bd6f5979d7075.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*YOEEcpEOWLyeVvdq6ePJ0g.png"/></div></figure><p id="52dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这方面的 Python 代码应该是</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="7401" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经在期望步骤中预先计算了第二个求和的值，所以我们在这里只是利用它。此外，创建图表来查看可能性的进展情况总是很有用的。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi og"><img src="../Images/5ada923a67cde35bbedec5d5555b6fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*_mWYbTTwAtD-3_LoOO_qMA.png"/></div></figure><p id="798b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以清楚地看到，算法在大约 20 个历元后收敛。EM 保证在过程的给定迭代次数之后将达到局部最大值。</p><p id="1526" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，作为实现的一部分，我们还生成一个动画，向我们展示每次迭代后集群设置是如何改进的。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nl"><img src="../Images/07472e3090237adb6a4a99698d435cae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*I0WTzTOyyDVwfPyMSZPzWQ.gif"/></div></div></figure><p id="38e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意 GMM 是如何改进 K-means 估计的质心的。当我们收敛时，每个集群的参数值不再改变。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h2 id="4236" class="np lv it bd lw nq nr dn ma ns nt dp me kr nu nv mg kv nw nx mi kz ny nz mk oa bi translated">结束语</h2><p id="98a5" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">高斯混合模型是一种非常强大的工具，广泛用于涉及数据聚类的各种任务。我希望这篇文章对你有用！请随意提出问题或评论。我也强烈建议您亲自尝试这些派生，并深入研究代码。我期待着尽快创作出更多这样的材料。</p><p id="011f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽情享受吧！</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="82db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1] Bishop，Christopher M. <em class="lm">模式识别和机器学习</em> (2006)施普林格出版社柏林，海德堡。</p><p id="671e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]墨菲，凯文·p .<em class="lm">机器学习:一种概率视角</em> (2012) <em class="lm"> </em>麻省理工学院出版社，剑桥，麻省，</p></div></div>    
</body>
</html>