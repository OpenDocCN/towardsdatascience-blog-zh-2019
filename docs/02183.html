<html>
<head>
<title>[Overview]: Ensemble Learning made simple</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【概述】:让集成学习变得简单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overview-ensemble-learning-made-simple-d4ac0d13cb96?source=collection_archive---------21-----------------------#2019-04-10">https://towardsdatascience.com/overview-ensemble-learning-made-simple-d4ac0d13cb96?source=collection_archive---------21-----------------------#2019-04-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/888cbcf761c7449c8448c7a5a792f769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*jPjEBf2r26YlJbKdkN6bPQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk"><a class="ae jy" href="https://www.xiaomitoday.com/ask-these-questions-before-buying-a-mobile-phone-as-a-gift/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="af92" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当你想购买一部手机的时候，你会直接走到商店然后在网上转然后挑选任何一部手机吗？最常见的做法是浏览互联网上的评论，比较不同的型号，规格，功能和价格。你可能会向你的同伴询问购买建议，并以结论结束。总的来说，你没有直接得出结论，而是考虑了其他来源的选择。</p><p id="78a5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在本文中，我将向您介绍机器学习中的一种称为“集成学习”的技术，以及使用这种技术的算法。</p><h2 id="50db" class="kx ky iq bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo lp bi translated">内容列表</h2><ol class=""><li id="faf8" class="lq lr iq kb b kc ls kg lt kk lu ko lv ks lw kw lx ly lz ma bi translated"><strong class="kb ir"> <em class="mb">多样的算法</em> </strong></li><li id="0812" class="lq lr iq kb b kc mc kg md kk me ko mf ks mg kw lx ly lz ma bi translated"><strong class="kb ir"> <em class="mb">不同实例上每个预测器的算法相同</em> </strong></li><li id="f779" class="lq lr iq kb b kc mc kg md kk me ko mf ks mg kw lx ly lz ma bi translated"><strong class="kb ir"> <em class="mb">相同的算法对不同的特性集&amp;实例</em> </strong></li><li id="474b" class="lq lr iq kb b kc mc kg md kk me ko mf ks mg kw lx ly lz ma bi translated"><strong class="kb ir"> <em class="mb">升压</em> </strong></li><li id="b730" class="lq lr iq kb b kc mc kg md kk me ko mf ks mg kw lx ly lz ma bi translated"><strong class="kb ir"> <em class="mb">堆叠</em> </strong></li></ol><p id="1a6a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">集成模型的思想是训练多个模型，每个模型的目标是预测或分类一组结果。</p><p id="9760" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">集成学习背后的主要原理是将弱学习者分组在一起以形成一个强学习者。</p><blockquote class="mh mi mj"><p id="a7c2" class="jz ka mb kb b kc kd ke kf kg kh ki kj mk kl km kn ml kp kq kr mm kt ku kv kw ij bi translated">集成学习:集成是一组被训练并用于预测的预测器</p><p id="320f" class="jz ka mb kb b kc kd ke kf kg kh ki kj mk kl km kn ml kp kq kr mm kt ku kv kw ij bi translated"><strong class="kb ir">集成算法</strong>:集成算法的目标是将几个基础估计器的预测与给定的学习算法结合起来，以提高单个估计器的鲁棒性</p></blockquote><p id="3ec6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">用训练数据的子集训练一组决策树。使用个别树的预测，预测得到最大值的类。一些投票。这样的决策树集合——<strong class="kb ir">随机森林</strong>。</p><p id="30ec" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">集成学习方法通常区分如下</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/1655702143d2d6dd19f7ef49ef0d2a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*wVXEofAuuRGyMMEWpDUohw.png"/></div></figure><h2 id="120b" class="kx ky iq bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo lp bi translated"><strong class="ak">集成学习的类型</strong></h2><p id="ef5f" class="pw-post-body-paragraph jz ka iq kb b kc ls ke kf kg lt ki kj kk ms km kn ko mt kq kr ks mu ku kv kw ij bi translated">a.)一套多样的<strong class="kb ir"> <em class="mb">算法</em> </strong></p><p id="0565" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">基于我们的目标(回归/分类),我们选择一组不同的模型，训练它们，汇总这些模型的结果并得出结论。这是使用投票分类器完成的。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/5a5b620662ecb66c7061f52b7d1ad379.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*Z6M1YaQ2k-0clMBpLEruOg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Source: Hands-on machine learning with sci-kit-learn and tensorflow</figcaption></figure><p id="835c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="mb">硬投票分类器:</em>聚合各个分类器的预测，预测得到票数最多的类。只有当分类器相互独立时，集成才能比单个低性能分类器表现更好。但是他们接受了相同的数据训练</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/3192da544a4d238dcd9c6cfb5d31b5b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4PLflcDkTTaxGYjhHwDLKw.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Source: Hands-on machine learning with sci-kit-learn and tensorflow</figcaption></figure><p id="318b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="mb">软投票分类器</em>:如果所有分类器都能够估计类的概率(predict_proba()方法)，那么预测具有最高类概率的类，在单个分类器上平均。软投票通常比硬投票表现更好。软投票考虑每个分类器的确定程度</p><p id="d6aa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在让我们试着用一种简单的方式来理解这一点。</p><p id="548d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">举例:假设你有 3 个分类器(1，2，3)，两个类(A，B)，经过训练，你在预测单个点的类。</p><p id="3bc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">硬投票</strong></p><p id="84bc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="mb">预测</em>:</p><p id="8e1e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">分类器 1 预测类别 A</p><p id="90fa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">分类器 2 预测 B 类</p><p id="4dfd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">分类器 3 预测 B 类</p><p id="0aa1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">2/3 分类器预测 B 类，所以<strong class="kb ir">B 类是集成决策</strong>。</p><p id="95a8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">软投票</strong></p><p id="61c5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="mb">预测</em>(与之前的例子相同，但现在是根据概率。此处仅显示 A 类，因为问题是二进制的):</p><p id="2ddd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">分类器 1 以 99%的概率预测类别 A</p><p id="d33d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">分类器 2 以 49%的概率预测类别 A</p><p id="850f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">分类器 3 以 49%的概率预测类别 A</p><p id="804d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">跨分类器属于 A 类的平均概率为(99 + 49 + 49) / 3 = 65.67%。因此，<strong class="kb ir">A 类是系综决策</strong>。</p><h2 id="a364" class="kx ky iq bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo lp bi translated"><strong class="ak"> b.) </strong> <strong class="ak"> <em class="nb">不同情况下每个预测器的算法相同</em> </strong></h2><p id="4392" class="pw-post-body-paragraph jz ka iq kb b kc ls ke kf kg lt ki kj kk ms km kn ko mt kq kr ks mu ku kv kw ij bi translated">到目前为止，我们已经看到了在相同实例上训练的不同算法。现在让我们看看如何使用不同的实例使用相同的算法进行分类。</p><p id="50f5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果样本选择是通过替换- <strong class="kb ir"> <em class="mb">装袋</em> </strong>(引导汇总)完成的</p><p id="434f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果样品选择完成而没有替换— <strong class="kb ir"> <em class="mb">粘贴</em> </strong></p><p id="3553" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">只有 bagging 允许为同一个预测器对训练实例进行多次采样</p><p id="74c0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">一旦模型被训练，集合将聚集来自所有模型的预测。</p><p id="5d77" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">聚集-模式:分类</p><p id="71e2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">聚合-平均值:回归</p><p id="bbcc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">聚合减少了偏倚和方差。</p><p id="49b7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在 Sci-Learn 中，为了执行 bagging，我们使用 BaggingClassifier():如果基本分类器有 predict_proba()方法，则自动执行软投票。</p><p id="8313" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">Bagging —较高的偏差，低方差结果。首选整体装袋</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/4e77bf3306f30af2f1585f354e2f64c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*sfboqjqxHoNxx9ZgfQGzhg.png"/></div></figure><h2 id="9f81" class="kx ky iq bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo lp bi translated">c.)<strong class="ak"> <em class="nb">同一算法上的一组多样特性&amp;实例</em> </strong></h2><p id="d4fc" class="pw-post-body-paragraph jz ka iq kb b kc ls ke kf kg lt ki kj kk ms km kn ko mt kq kr ks mu ku kv kw ij bi translated">到目前为止，我们已经看到使用采样数据训练模型。现在是时候根据所选的特性训练模型了。</p><p id="6727" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> <em class="mb">随机面片和随机子空间</em> </strong></p><p id="7fad" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在 Bagging 分类器()中，用于实例采样和特征采样的参数如下</p><p id="f884" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">实例采样— max_samples，引导</p><p id="f351" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要素采样-最大要素，引导要素</p><p id="4871" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">采样训练实例和特征— <strong class="kb ir"> <em class="mb">随机补丁方法</em> </strong></p><p id="fb6b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">保留所有训练实例(即 bootstrap=False 和 max_sam ples=1.0)但采样特征(即 bootstrap_features=True 和/或 max_features 小于 1.0)称为<strong class="kb ir"> <em class="mb">随机子空间方法</em> </strong></p><h2 id="b58d" class="kx ky iq bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo lp bi translated">d.)助推</h2><p id="c5ca" class="pw-post-body-paragraph jz ka iq kb b kc ls ke kf kg lt ki kj kk ms km kn ko mt kq kr ks mu ku kv kw ij bi translated">提升是一种集成技术，它试图从多个弱分类器中创建一个强分类器。这是一个循序渐进的过程，每个后续模型都试图修复其前一个模型的错误。</p><p id="e5c5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">AdaBoost(自适应增强)，梯度增强。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/402db89e2562f378a640da847eb75693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*j6U6LpyDHEA_s4qLnZRfPg.png"/></div></figure><p id="b24d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"><em class="mb">AdaBoost</em></strong><em class="mb">:</em>在每个模型预测的最后，我们最终提高了错误分类实例的权重，以便下一个模型对它们做得更好，等等。顺序学习的一个主要缺点是该过程不能并行化，因为预测器可以一个接一个地训练。如果 AdaBoost 集成过拟合训练集，则减少估计器的数量或调整基本估计器</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/6a94605e8657c03c4e7f7b06400badc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*3r4weQAGLI-JwRsJagJW5Q.png"/></div></figure><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ne"><img src="../Images/291f7273fbdaf9160e6a766e88ddcc57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sGDoUrsglxZl84WnTINbVA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Source: Hands-on machine learning with sci-kit-learn and tensorflow</figcaption></figure><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nf"><img src="../Images/e28d52157bf133bab6cfa139b2534aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XGfickpWOfK3zlLH-0zOQ.png"/></div></div></figure><p id="1a08" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">AdaBoost 通过将(每棵树的)权重相加乘以(每棵树的)预测来进行新的预测。显然，权重较高的树将有更大的权力来影响最终决策</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="ab gu cl ng"><img src="../Images/b14d695f5c9b616372ff273038c67d85.png" data-original-src="https://miro.medium.com/v2/format:webp/1*MwIT3Gu-dhICzEou7he3OQ.png"/></div></figure><p id="681a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"><em class="mb">【GBM】</em></strong></p><p id="b9d7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">梯度增强直接从误差——残差中学习，而不是更新数据点的权重。</p><p id="8918" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">下面是梯度推进决策树(GBDT)的 Python 代码</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/3b54261d01a61e1b2aaa03dde20cd171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*zu2goMliI4t0DrB8n0mppg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Source: Hands-on machine learning with sci-kit-learn and tensorflow</figcaption></figure><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/64a14055fb633706122d7063f25071a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*8Ty0grO-Lg9WdywVXY3Erw.png"/></div></figure><p id="b845" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">梯度推进通过简单地将(所有树的)预测相加来进行新的预测</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="ab gu cl ng"><img src="../Images/a301b34b91dfb8dac6cc9a2145380283.png" data-original-src="https://miro.medium.com/v2/format:webp/1*r3VlUhPBPfHP9zDU97GJjQ.png"/></div></figure><h2 id="5988" class="kx ky iq bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo lp bi translated">e.)堆叠(<strong class="ak">堆叠概括</strong></h2><p id="37a0" class="pw-post-body-paragraph jz ka iq kb b kc ls ke kf kg lt ki kj kk ms km kn ko mt kq kr ks mu ku kv kw ij bi translated">堆叠背后的主要思想是，与其使用琐碎的函数(如硬投票)来聚合集合中所有预测者的预测，不如我们训练一个模型来执行这种聚合。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/f3e98394acb2e17513b116934f54376f.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*CectbKYf28NT03gijngUqA.png"/></div></figure><p id="1d07" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">集成对新实例执行回归任务。底部三个预测器中的每一个都预测不同的值(3.1、2.7 和 2.9)，然后最终预测器(称为混合器或元学习器)将这些预测作为输入，并做出最终预测(3.0)</p><p id="9b83" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">T13】结论:T15】</strong></p><p id="d4b0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">总的来说，集成学习是一种用于提高模型预测能力/估计准确性的强大技术。</p><p id="9686" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">请在下面留下任何评论、问题或建议。</p><p id="b9f5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">谢谢大家！</p><p id="63ad" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">快乐学习！</p></div></div>    
</body>
</html>