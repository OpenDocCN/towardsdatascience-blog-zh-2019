<html>
<head>
<title>Predictive Modeling: Picking the Best Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测建模:选择最佳模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predictive-modeling-picking-the-best-model-69ad407e1ee7?source=collection_archive---------5-----------------------#2019-02-08">https://towardsdatascience.com/predictive-modeling-picking-the-best-model-69ad407e1ee7?source=collection_archive---------5-----------------------#2019-02-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="34c4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在相同的数据上测试不同类型的模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a76505b5d0516017ea75ca16925bd629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntFprcp8f_zCat6LJdOD1g.jpeg"/></div></div></figure><p id="f976" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">无论您是在办公室环境中预测数据，还是只是在 Kaggle 比赛中竞争，测试不同的模型以找到最适合您正在处理的数据都是非常重要的。</p><p id="dc4f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最近，我有机会在一个私人的 Kaggle 比赛中与一些非常聪明的同事竞争，预测坦桑尼亚有问题的水泵。在做了一些数据清理后，我运行了以下模型，我将向您展示结果。</p><ul class=""><li id="8f39" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">逻辑回归</li><li id="e1d0" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">随机森林</li><li id="9d50" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">里脊回归</li><li id="0c9f" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">k-最近邻</li><li id="3d46" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">XGBoost</li></ul><h2 id="3553" class="mb mc iq bd md me mf dn mg mh mi dp mj la mk ml mm le mn mo mp li mq mr ms mt bi translated">加载数据</h2><p id="b832" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">首先，我们需要看看我们正在处理的数据。在这个特定的数据集中，要素与标注位于不同的文件中。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="81af" class="mb mc iq na b gy ne nf l ng nh">import pandas as pd<br/>pd.set_option('display.max_columns', None)</span><span id="ce03" class="mb mc iq na b gy ni nf l ng nh">X_df = pd.read_csv('./train_features.csv')<br/>X_df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/7999dfd1220c2fabea4789dd39d53522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JTyzHQnDnrr93h9vqJekXA.png"/></div></div></figure><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="a376" class="mb mc iq na b gy ne nf l ng nh">y_df = pd.read_csv('./train_labels.csv')<br/>y_df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/14439df266bb99831e4f0e5550be0a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uo63QKmFsL6nZp7pv2IUIA.png"/></div></div></figure><p id="bcba" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以看到 status_group 或 target 标签是一个字符串，有些模型不需要修改就可以工作，但有些模型不需要。稍后我们会做些什么。让我们来看看我们的目标标签的分布。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="9153" class="mb mc iq na b gy ne nf l ng nh">y_df['status_group'].value_counts(normalize=True)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/c77170bd3aba870409aa9440d39b1f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*96URHsWFaSEPxpjsH9wEKw.png"/></div></div></figure><p id="de46" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这种分离表明我们在标签中正好有 3 个类，因此我们有一个多类分类。大多数类是“functional ”,所以如果我们只是将 functional 分配给所有的实例，我们的模型在这个训练集上将是. 54。这被称为多数类基线，是我们运行的模型要达到的目标。</p><h2 id="e5e5" class="mb mc iq bd md me mf dn mg mh mi dp mj la mk ml mm le mn mo mp li mq mr ms mt bi translated">数据清理和特征工程</h2><p id="860b" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">这个数据集中有许多功能，所以我不会详细介绍我所做的每一件事，但我会一步一步地从高层次进行介绍。</p><p id="7dea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">首先，我们希望通过查看所有的特性和数据类型来检查事物。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="f09d" class="mb mc iq na b gy ne nf l ng nh">X_df.info()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/31c4d3fc2d096320188b00f72a2d5cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPsN0WbToK5T3TQfJvRxcw.png"/></div></div></figure><p id="ae9e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了能够在模型中使用它们，我们需要使用 30 个对象特征。int 和 float 对象可以直接使用。</p><p id="231e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">另一个需要关注的是高基数特性。如果我们对这些特性的每一个都有超过 100 个类别，那么使用它们就没什么用了。这会给我们的数据集增加维度，我们不想这么做。</p><p id="c2d5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我们删除这些高基数列之前，我看到 date_recorded 是一个对象，它肯定会随着我们的高基数特性而被删除，所以我在此基础上创建了一些特性。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="6abc" class="mb mc iq na b gy ne nf l ng nh">#So date doesn't get dropped in next step</span><span id="faf2" class="mb mc iq na b gy ni nf l ng nh">X_df['date_recorded'] = pd.to_datetime(X_df['date_recorded'])</span><span id="bf27" class="mb mc iq na b gy ni nf l ng nh">X_df['YearMonth'] = X_df['date_recorded'].map(lambda x: 100*x.year + x.month)</span><span id="c950" class="mb mc iq na b gy ni nf l ng nh">X_df['Year'] = X_df['date_recorded'].map(lambda x: x.year)</span><span id="329d" class="mb mc iq na b gy ni nf l ng nh">X_df['Month'] = X_df['date_recorded'].map(lambda x: x.month)</span></pre><p id="3f1f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">既然我们已经对日期进行了分类，我们可以检查高基数并删除那些特性。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="bec0" class="mb mc iq na b gy ne nf l ng nh">max_cardinality = 100</span><span id="ea0c" class="mb mc iq na b gy ni nf l ng nh">high_cardinality = [col for col in X_df.select_dtypes(exclude=np.number)<br/>                   if X_df[col].nunique() &gt; max_cardinality]</span><span id="ab14" class="mb mc iq na b gy ni nf l ng nh">X_df = X_df.drop(columns=high_cardinality)</span><span id="d167" class="mb mc iq na b gy ni nf l ng nh">X_df.info()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/53b18f0a3db51c7dd0edf089bb67637c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*txPVzMDE__nZb13-VpoZNQ.png"/></div></div></figure><p id="81ff" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">所以，我们放弃了 8 个高基数的特性。现在我们可以使用 OneHotEncoder 或 Pandas get_dummies()将这些对象更改为 int。</p><p id="95aa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我们所有的特征都是数字的，让我们进入模型吧！</p><h2 id="1810" class="mb mc iq bd md me mf dn mg mh mi dp mj la mk ml mm le mn mo mp li mq mr ms mt bi translated">逻辑回归</h2><p id="e7fa" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">逻辑回归对于多类分类非常有用，因为如果目标标签是字符串，Scikit-learn 会自动对其进行编码。</p><p id="cb70" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">首先，我们需要将数据分为训练和测试。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="ddba" class="mb mc iq na b gy ne nf l ng nh">from sklearn.preprocessing import scale<br/>from sklearn.model_selection import train_test_split</span><span id="e68c" class="mb mc iq na b gy ni nf l ng nh">X = X.drop(columns='id') #id is our index and won't help our model<br/>X = scale(X) </span><span id="7328" class="mb mc iq na b gy ni nf l ng nh">X_train, X_test, y_train, y_test = train_test_split(<br/>        X, y, train_size=0.75, test_size=0.25, random_state=42, shuffle=True)</span></pre><p id="1c38" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当你使用一个学习模型时，重要的是<strong class="kt ir">缩放</strong>特征到一个以零为中心的范围。缩放将确保特征的方差在相同的范围内。</p><p id="917a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，我们将在训练和测试中运行该模型，并查看我们的准确度得分。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="c392" class="mb mc iq na b gy ne nf l ng nh">from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score</span><span id="6f20" class="mb mc iq na b gy ni nf l ng nh">logreg = LogisticRegression()<br/>logreg.fit(X_train,y_train)<br/>y_pred = logreg.predict(X_train)<br/>print('Train accuracy score:',accuracy_score(y_train,y_pred))<br/>print('Test accuracy score:', accuracy_score(y_test,logreg.predict(X_test)))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/fef9139bb56f6e70f04b246cee774992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1kV2GS8Mp9XsLPbRd4lz-w.png"/></div></div></figure><p id="5718" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在训练和测试中，我们以 0.73 的成绩打破了大多数班级 0.54 的基线。让我们看看另一种模式是否能做得更好。</p><h2 id="cb65" class="mb mc iq bd md me mf dn mg mh mi dp mj la mk ml mm le mn mo mp li mq mr ms mt bi translated">随机森林</h2><p id="a7dc" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">随机森林也可以将字符串作为我们的目标标签，因此我们可以使用相同的训练测试分割来运行模型。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="bea6" class="mb mc iq na b gy ne nf l ng nh">from sklearn.ensemble import RandomForestClassifier as RFC</span><span id="c9f5" class="mb mc iq na b gy ni nf l ng nh">rfc_b = RFC()</span><span id="1db1" class="mb mc iq na b gy ni nf l ng nh">rfc_b.fit(X_train,y_train)<br/>y_pred = rfc_b.predict(X_train)<br/>print('Train accuracy score:',accuracy_score(y_train,y_pred))<br/>print('Test accuracy score:', accuracy_score(y_test,rfc_b.predict(X_test)))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/c93629d8e63adfa531ed6af76bd88411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kIG-mtSz10NFu7aK6sCHhQ.png"/></div></div></figure><p id="3d87" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随机森林在训练和测试中击败了逻辑回归，训练中为 0.97，测试中为 0.79。</p><h2 id="43db" class="mb mc iq bd md me mf dn mg mh mi dp mj la mk ml mm le mn mo mp li mq mr ms mt bi translated">里脊回归</h2><p id="bd36" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">对于岭回归，我们需要在运行模型之前对目标标签进行编码。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="712d" class="mb mc iq na b gy ne nf l ng nh">X = X_df.drop(columns=['id'])<br/>X = scale(X)</span><span id="a259" class="mb mc iq na b gy ni nf l ng nh">y = y_df.drop(columns='id')<br/>y = y.replace({'functional':0, 'non functional':2,'functional needs repair':1 })</span><span id="2f25" class="mb mc iq na b gy ni nf l ng nh">X_train, X_test, y_train, y_test = train_test_split(<br/>        X, y, train_size=0.75, test_size=0.25, random_state=42, shuffle=True)</span></pre><p id="2297" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我们运行这个模型。Ridge 在其 predict()方法中输出一个概率，所以我们必须用 numpy 更新它，以便得到实际的预测。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="f3d2" class="mb mc iq na b gy ne nf l ng nh">from sklearn.linear_model import Ridge<br/>import numpy as np</span><span id="e8e4" class="mb mc iq na b gy ni nf l ng nh">ridge = Ridge()</span><span id="fb45" class="mb mc iq na b gy ni nf l ng nh">ridge.fit(X_train,y_train)<br/>y_prob = ridge.predict(X_train)<br/>y_pred = np.asarray([np.argmax(line) for line in y_prob])<br/>yp_test = ridge.predict(X_test)<br/>test_preds = np.asarray([np.argmax(line) for line in yp_test])<br/>print(accuracy_score(y_train,y_pred))<br/>print(accuracy_score(y_test,test_preds))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/40c886825df851aa6520ed61c68a8887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8zvtVbi21JkVEz6dIkUpcw.png"/></div></div></figure><p id="30b7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">所以，岭回归对于这些数据来说不是一个好的模型。</p><h2 id="68f2" class="mb mc iq bd md me mf dn mg mh mi dp mj la mk ml mm le mn mo mp li mq mr ms mt bi translated">k-最近邻</h2><p id="588e" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">对于 K-最近邻，我们将使用与岭相同的训练测试分割。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="f576" class="mb mc iq na b gy ne nf l ng nh">from sklearn.neighbors import KNeighborsClassifier</span><span id="220c" class="mb mc iq na b gy ni nf l ng nh">knn = KNeighborsClassifier()</span><span id="3548" class="mb mc iq na b gy ni nf l ng nh">knn.fit(X_train,y_train)<br/>y_pred = knn.predict(X_train)<br/>print('Train accuracy score:',accuracy_score(y_train,y_pred))<br/>print('Test accuracy score:',accuracy_score(y_test,knn.predict(X_test)))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/433f133b215baae5f6dc6a1f437f9e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F7KiiR9lPvVHjI371C7dFg.png"/></div></div></figure><p id="0c95" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这些分数看起来比 Ridge 好很多，但仍然不是我们最好的分数。</p><h2 id="ffd3" class="mb mc iq bd md me mf dn mg mh mi dp mj la mk ml mm le mn mo mp li mq mr ms mt bi translated">XGBoost</h2><p id="1253" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">XGBoost 是一种算法，在应用机器学习和针对结构化或表格数据的 Kaggle 竞赛中非常流行。</p><p id="de2e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是一个梯度增强决策树的实现，旨在提高速度和性能。</p><p id="114f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你想了解更多，请查看这里的文档<a class="ae ns" href="https://xgboost.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"/>。当运行这个模型时，我对这些参数进行了一些调整，这些参数是我运行的数据中最好的。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="c801" class="mb mc iq na b gy ne nf l ng nh">xg_train = xgb.DMatrix(X_train, label=y_train)<br/>xg_test = xgb.DMatrix(X_test, label=y_test)</span><span id="a877" class="mb mc iq na b gy ni nf l ng nh">xg_train.save_binary('train.buffer')<br/>xg_test.save_binary('train.buffer')</span><span id="6078" class="mb mc iq na b gy ni nf l ng nh"># setup parameters for xgboost<br/>param = {}<br/># use softmax multi-class classification<br/>param['objective'] = 'multi:softmax'<br/>param['silent'] = 1 # cleans up the output<br/>param['num_class'] = 3 # number of classes in target label<br/></span><span id="9839" class="mb mc iq na b gy ni nf l ng nh">watchlist = [(xg_train, 'train'), (xg_test, 'test')]<br/>num_round = 30<br/>bst = xgb.train(param, xg_train, num_round, watchlist)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/efc1937c143681b0b7530aaabc4d20c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yE4aieM6McOy5RRdIyy-qA.png"/></div></div></figure><p id="2158" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">XGBoost 分类器的输出输出一个错误，该错误被定义为</p><blockquote class="nu nv nw"><p id="3231" class="kr ks nx kt b ku kv jr kw kx ky ju kz ny lb lc ld nz lf lg lh oa lj lk ll lm ij bi translated"><code class="fe ob oc od na b">merror</code>:多类分类错误率。它被计算为<code class="fe ob oc od na b">#(wrong cases)/#(all cases)</code></p></blockquote><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="4b4f" class="mb mc iq na b gy ne nf l ng nh"># get prediction<br/>y_pred1 = bst.predict(xg_train)<br/>y_pred2 = bst.predict(xg_test)<br/>print('Train accuracy score:',accuracy_score(y_train,y_pred1))<br/>print('Test accuracy score:',accuracy_score(y_test,bst.predict(xg_test)))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/cbefd6d3e76f24ac097315e61e302620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fyg7YM0ZDPmkYtrXijT3Eg.png"/></div></div></figure><p id="7c6e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们在训练中得到 0.79 分，在测试中得到 0.78 分，这也不是我们最好的成绩，但和随机森林差不多。</p><h2 id="f19a" class="mb mc iq bd md me mf dn mg mh mi dp mj la mk ml mm le mn mo mp li mq mr ms mt bi translated">结论</h2><p id="e8d8" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">出于我的目的，我选择使用 XGBoost 并修改了参数。我在上面使用的训练测试分割数据中的分数在训练中是. 97，在测试中是. 81。在给出的测试数据上，我的 Kaggle 分数以. 795 结束。</p><p id="4d70" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">一旦您找到了最适合您所拥有的数据的模型，您就可以使用模型接受的参数，看看您是否可以获得更好的分数。</p><p id="d53f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我希望这有助于您的预测建模工作！</p></div></div>    
</body>
</html>