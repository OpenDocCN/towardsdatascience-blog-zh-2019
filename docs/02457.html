<html>
<head>
<title>ReFocus: Making Out-of-Focus Microscopy Images In-Focus Again</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">重新聚焦:使失焦的显微图像重新聚焦</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/refocus-making-out-of-focus-microscopy-images-in-focus-again-90e1fe98ead4?source=collection_archive---------18-----------------------#2019-04-22">https://towardsdatascience.com/refocus-making-out-of-focus-microscopy-images-in-focus-again-90e1fe98ead4?source=collection_archive---------18-----------------------#2019-04-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/0aaca5069f3a51288f6560adf594addd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLx2oLFhBsnTBTaNat2Sug.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Image source: <a class="ae kf" href="https://www.thebisforboss.com/blog/2017/10/16/the-power-of-refocusing-resetting" rel="noopener ugc nofollow" target="_blank">https://www.thebisforboss.com/blog/2017/10/16/the-power-of-refocusing-resetting</a></figcaption></figure><p id="eb8a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显微图像广泛用于诊断各种疾病，例如感染和癌症。此外，它们促进了基础生物医学研究，这些研究不断产生对人类疾病原因的新见解。因此，显微图像对改善我们的健康非常重要。然而，获得高质量的聚焦显微镜图像是显微镜领域最大的挑战之一。例如，某些组织(如肺和肠)是不均匀的，会导致图像失焦。在这篇文章中，我们将通过使用深度学习来重新聚焦失焦的显微镜图像来解决这个问题。换句话说，我们将使用深度学习将失焦的显微镜图像变成对焦的图像(见下图)。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/8b24187737eda8de727a27745e46b54e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2tFvmSBPgMRcfPU4agdInQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Left: out of focus. Right: in focus (Image source: <a class="ae kf" href="https://data.broadinstitute.org/bbbc/BBBC006/" rel="noopener ugc nofollow" target="_blank">https://data.broadinstitute.org/bbbc/BBBC006/</a>)</figcaption></figure><h1 id="eaa6" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">数据</h1><p id="f618" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">我们将使用<a class="ae kf" href="https://data.broadinstitute.org/bbbc/BBBC006/" rel="noopener ugc nofollow" target="_blank">Broad bio image Benchmark Collection 006(bbbc 006)</a>图像集，该图像集是从一个 384 孔微孔板中获取的，该微孔板包含细胞核被 Hoechst 染色剂标记的人类细胞。为 768 个视场(384 个孔，每个孔 2 个视场)中的每一个拍摄 32 个图像的 z 叠置体(最佳焦平面处 z = 16，焦平面上方 15 个图像，下方 16 个图像)。</p><h1 id="a0d7" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">方法</h1><p id="0e82" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">总体策略是建立一个卷积神经网络，将离焦图像作为输入，生成聚焦图像作为输出。我们将把我们的神经网络建立在<strong class="ki iu"> U-net </strong>架构上。此外，我们将使用<strong class="ki iu">特征损失</strong>(最初由 Johnson 等人称为<a class="ae kf" href="https://arxiv.org/pdf/1603.08155.pdf" rel="noopener ugc nofollow" target="_blank">感知损失)作为损失函数，以量化神经网络的输出与其在 z = 16 处的相应最佳焦平面图像或目标之间的差异。</a></p><h2 id="5739" class="mm lk it bd ll mn mo dn lp mp mq dp lt kr mr ms lx kv mt mu mb kz mv mw mf mx bi translated">优信网</h2><p id="8ba9" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated"><a class="ae kf" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U-net </a>最初是 Ronneberge 等人针对生物医学图像分割问题开发的。U-net 本质上由三个组件组成:减小图像尺寸的下采样路径、增大图像尺寸的后续上采样路径、以及将激活从下采样路径的所选部分转移到上采样路径中的相应部分的交叉连接。交叉连接用来自下采样路径的信息补充上采样路径，并且是使 U-net 表现如此好的主要发明。我们将使用在 ImageNet 上预训练的<a class="ae kf" href="https://en.wikipedia.org/wiki/Residual_neural_network" rel="noopener ugc nofollow" target="_blank"> ResNet </a> -34 作为下采样路径，它利用了被称为<a class="ae kf" href="https://en.wikipedia.org/wiki/Transfer_learning" rel="noopener ugc nofollow" target="_blank">迁移学习</a>的技术。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/fb768710e9c2b3b3e1849eb4824163c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ukVE4J5N_2zvU3W7jH1ANA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">U-net-based architecture (adapted from <a class="ae kf" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1505.04597.pdf</a>)</figcaption></figure><h2 id="1cae" class="mm lk it bd ll mn mo dn lp mp mq dp lt kr mr ms lx kv mt mu mb kz mv mw mf mx bi translated">特征损失</h2><p id="f233" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">特征损失有助于重建图像中的精细细节，非常适合于<a class="ae kf" href="https://en.wikipedia.org/wiki/Neural_Style_Transfer" rel="noopener ugc nofollow" target="_blank">风格转移</a>和<a class="ae kf" href="https://en.wikipedia.org/wiki/Super-resolution_imaging" rel="noopener ugc nofollow" target="_blank">超分辨率成像</a>应用。如下图所示，特征损失的基本思想是将输出和目标放入同一个 ImageNet 模型中(在我们的例子中是<a class="ae kf" href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener ugc nofollow" target="_blank"> VGG </a> -16)，然后在选定的中间层而不是最终层比较它们的激活。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/02a25a9f478dc770b38269dfa8d1082d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aH5Ydp3pUmO6cEKE46v-wQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Feature Loss (adapted from <a class="ae kf" href="https://arxiv.org/pdf/1603.08155.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1603.08155.pdf</a>)</figcaption></figure><h1 id="7739" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">使用 fastai 实现</h1><p id="4fa3" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">我们的基于 U-net 的神经网络的实现只是使用如下所示的<a class="ae kf" href="https://github.com/fastai/fastai" rel="noopener ugc nofollow" target="_blank"> fastai </a>库的一行代码。我们只需要提供数据、下采样架构(ResNet-34)、损失函数(特征损失)和一些附加参数。</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">U-net-based model in fastai</figcaption></figure><p id="8595" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用杰瑞米·霍华德的特征损失的实现，如下所示:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Feature loss implementation by Jeremy Howard</figcaption></figure><h1 id="e345" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">培训和测试</h1><p id="5663" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">我们将使用 z = 16 作为基本事实(y ),用在 z = 1(最差)、5(中间)和 10(接近焦点)的焦平面上方的失焦图像来训练我们的神经网络。我们将留出这些图像的一小部分作为验证集，并使用它们来指导和评估训练过程。训练后，我们将进一步评估模型在 z = 32 的焦平面下方的失焦图像上的性能。虽然两者都没有对焦，但焦平面上方或下方的图像看起来不同，因为它们到镜头的距离不同。因此，我们的策略将允许我们测试训练模型的可推广性。</p><h1 id="2dbb" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">结果</h1><p id="cb39" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">我们首先评估我们的模型在不同失焦水平的验证集上的性能。如下所示，该模型在区分模糊细胞方面做得很好，如果我们像预期的那样使用较少模糊的图像，性能会得到提高。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/43dad99ec872e19133b3fa84a34f6f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*kHYSHlhjyo9deSGt-haCxg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Model performance on the validation sets</figcaption></figure><p id="fbee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们在 z = 32 的测试集上检查了模型的性能。如下所示，模型生成的图像非常类似于 z = 16 时的最佳焦平面图像。因此，我们用焦平面上方的图像训练的模型在焦平面下方的图像上也表现良好。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/979a6034d2409f79bc8812146f1715e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KbXAI44spRKg_hNIvDJN9g.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Model performance on the test set at z = 32</figcaption></figure><h1 id="be03" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">总结和未来方向</h1><p id="e5cb" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">总之，我们成功地建立了一个神经网络，可以重新聚焦模糊的失焦显微图像。这项工作提高了显微图像的质量，并将促进人类疾病的研究。由于计算能力有限，我只能处理非常小的图像(128*128)。理想情况下，神经网络应该能够处理整个幻灯片图像，可能是通过将它们分成小块。此外，神经网络应该能够处理具有不同染色的不同种类的显微图像。最后，我们可以将神经网络集成到采集平台中，以实时重新聚焦失焦图像，并消除采集后固定的需要。</p><h1 id="f805" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">感谢</h1><p id="8cf8" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">我要感谢拉荷亚免疫学研究所(LJI)影像中心的 Paola Marcovecchio、Sara McArdle 和 Zbigniew Mikulski 博士的有益讨论。</p></div></div>    
</body>
</html>