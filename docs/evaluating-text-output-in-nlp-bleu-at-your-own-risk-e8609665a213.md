# 评估 NLP: BLEU 中的文本输出风险自担

> 原文：<https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213?source=collection_archive---------5----------------------->

刚进入 NLP 的人经常问我一个问题，当系统的输出是文本，而不是输入文本的某种分类时，如何评估系统。这种类型的问题，你把一些文本放入你的模型，并从中获得一些其他文本，被称为**序列到序列**或**字符串转换**问题。

这些问题真的很有趣！序列到序列建模的一般任务是 NLP 中一些最困难任务的核心，包括:

*   文本摘要
*   文本简化
*   问题回答
*   聊天机器人
*   机器翻译

这种技术完全出自科幻小说。有了如此广泛的令人兴奋的应用，很容易理解为什么序列到序列建模比以往任何时候都更受欢迎。真正不容易的是评估这些系统。

不幸的是，对于那些刚刚起步的人来说，对于应该使用什么样的指标来评估你的模型，没有简单的答案。更糟糕的是，评估序列到序列任务的最流行的度量标准之一 BLEU 有重大缺陷，特别是当应用于它从未打算评估的任务时。

幸运的是*你*，你已经找到了这篇有深度的博文！在这篇文章中，我将介绍这个流行的度量标准是如何工作的(别担心，我会尽量减少方程)。我们将看看 BLEU 的一些问题，最后，你如何在你自己的工作中最小化这些问题。

![](img/169135514ed246c1aab80e82d5e12a7f.png)

Orange painted blue on a blue background. (There, uh, aren’t a lot of eye-catching photos for NLP evaluation metrics as it turns out.)

# 一个非常困难的问题

BLEU 最初是为了测量机器翻译而开发的，所以让我们来看一个翻译示例。下面是语言 A(又名“法语”)中的一段文本:

> 你好，曼杰特罗伊斯·费尔伯特。

这里有一些 B 语言(又名“英语”)的参考译文。(注意，一些说英语的人称榛子为“filberts”，所以这两个都是非常好的翻译。)

> 我已经吃了三颗榛子了。
> 
> 我吃了三块榛子。

这里的*是一个生成的“神经”翻译。(在这种情况下，“神经”是“雷切尔用她的大脑想出了一个可能的翻译”，但假装这是由你正在训练的网络生成的。)*

> 我吃了三颗榛子。

现在，这里有一个非常困难的问题: ***我如何给这个翻译分配一个单一的数字分数，告诉我们仅使用提供的参考句子和神经输出，它有多“好”？***

> **为什么需要单个数值分数？**很棒的问题！如果我们想使用机器学习来建立一个机器翻译系统，我们需要一个单一的实数分数放入我们的损失函数。如果我们还知道潜在的最佳得分，我们就可以计算两者之间的距离。这使我们能够在系统训练时向系统提供反馈，即潜在的变化是否会通过使分数更接近理想分数来改善翻译，并通过查看相同任务的分数来比较经过训练的系统。

您可以做的一件事是查看输出句子中的每个单词，如果它出现在任何参考句子中，则给它打分 1，如果没有出现，则给 0。然后，为了标准化该计数，使其始终介于 0 和 1 之间，您可以将某个参考译文中出现的单词数除以输出句子中的总单词数。这给了我们一个叫做**的度量单位精度**。

所以，对于我们的例子，“我吃了三个榛子”，我们在至少一个参考句子中看到输出句子中的所有单词。除以输出中的单词数，4，您最终得到这个翻译的得分为 1。到目前为止一切顺利！但是这句话呢？

> 三三三三。

使用同样的标准，我们*也会*得出 1 分。这并不好:我们需要某种方式来告诉我们正在训练的系统，像第一个这样的句子比第二个这样的句子更好。

你可以根据每个单词在任何参考句子中出现的最高次数，通过限制次数来调整分数。使用这种方法，我们的第一句仍然会得到 1 分，而我们的第二句只会得到 0.25 分。

这让我们摆脱了“三三三三”的问题，但对于像这样的句子没有帮助，因为某种原因，单词是按字母顺序排列的:

> 我吃了三颗榛子

用我们现在的方法，这句话得 1 分，最高分！我们可以通过计数来解决这个问题，不是单个单词，而是相邻出现的单词。这些被称为 **n-grams** ，其中 n 是每组的字数。单字、双字、三字和四字分别由一个、两个、三个和四个单词组成。

对于这个例子，让我们使用二元模型。一般来说，BLEU 分数是基于一元、二元、三元和四元精度的平均值，但为了简单起见，我们在这里只坚持二元。同样为了简单起见，我们不会添加一个“单词”来告诉我们在句子的开头和结尾有一个句子边界。根据这些原则，单词按字母顺序排序示例中的二元模型是:

> [吃了榛子]
> 
> [榛子一号]
> 
> [我三]

如果我们用同样的方法计算单词，我们现在得到的分数是 0；最差的分数。我们的“三三三三”示例也得到 0 分，而不是现在的 0.25 分，而第一个示例“我吃了三颗榛子”的得分是 1 分。不幸的是，这个例子也是如此:

> 我吃过了。

解决这个问题的一个方法是将我们目前的分数乘以一个惩罚比我们的参考译文短的句子的标准。我们可以通过比较它和长度最接近的参考句子的长度来做到这一点。这就是简短的代价。

如果我们的输出和任何参考句子一样长或者更长，惩罚是 1。因为我们用它乘以我们的分数，这不会改变最终的输出。

另一方面，如果我们的输出比任何参考句子都短，我们用最接近的句子的长度除以我们的输出长度，从中减去 1，然后将 e 提高到整个句子的幂。**基本上，最短的参考句子越长，我们的输出越短，简短惩罚就越接近于零。**

在我们的“我吃了”例子中，输出句子是两个单词长，最接近的参考句子是四个单词。这给了我们 0.36 的简洁性损失，当乘以我们的双字母精度分数 1 时，我们的最终分数下降到只有 0.36。

这种方法着眼于输出和参考翻译之间的 n-grams 重叠，对较短的输出进行惩罚，被称为 **BLEU** (“双语评估替角”的缩写，人们实际上只在解释缩写词时才会说)，由 IBM 的 [Kishore Papineni、Salim Roukos、Todd Ward 和 Wei-朱婧于 2002 年开发](https://www.aclweb.org/anthology/P02-1040.pdf)。这是 NLP 中一个非常流行的度量，特别是对于系统输出是文本字符串而不是分类的任务。这包括机器翻译和越来越多的自然语言生成。这是我在这篇文章开始时提出的一个非常困难的问题的解决方案:开发一种方法，给一个翻译分配一个单一的数字分数，告诉我们它有多“好”。

它也有很大的缺陷。

# 布鲁的问题

此时，您可能会疑惑，“ *Rachael，如果这个指标有这么大的缺陷，您为什么要向我们介绍如何计算它呢？*“主要是给你看这个指标有多合理。这是相当直观的，其基本思想是，你可以通过与参考翻译进行比较来评估机器翻译系统的输出，这在 NLP 中具有极大的影响力(尽管并非没有批评者)。

BLEU 也*当然*有一些长处。在 NLP 工作的人们最关心的是它对研究人员来说有多方便。

*   计算起来既快又容易，尤其是与让人工翻译来评估模型输出相比。
*   它无处不在。这使得将您的模型与相同任务的基准进行比较变得容易。

不幸的是，这种便利性导致人们过度使用它，即使对于它不是度量标准的最佳选择的任务。

> 尽管我只举了一个句子的例子，BLEU 一直被认为是一个语料库级别的测量方法。获取语料库中每个句子的 BLEU 分数，然后对它们进行平均，这将人为地抬高你的分数，如果你试图在你做的地方发表作品，你肯定会被评论者批评。

即使它没有被过度应用，在你选择花费时间和计算追求更好的 BLEU 分数之前，你应该知道这个指标有严重的限制。虽然有很多关于 BLEU 缺点的讨论，但我认为最重要的四个问题是:

*   它不考虑意义
*   它不直接考虑句子结构
*   它不能很好地处理形态丰富的语言
*   它不太符合人类的判断

让我们逐一讨论这些问题，这样我可以向您展示为什么我认为这些是问题。

## BLEU 不考虑意义

对我来说，这是不仅仅依靠 BLEU 来评估机器翻译(MT)系统的最有说服力的理由。作为机器翻译的人类用户，我的主要目标是准确理解原文中文本的潜在含义。我很乐意接受输出句子中的一些句法或语法怪异，只要它符合原文的意思。

BLEU 不衡量意义。它只奖励在参考系统中有精确匹配的 n 元文法的系统。这意味着一个虚词(如“an”或“on”)的差异与一个更重要的实词的差异一样严重。这也意味着一个翻译有一个完全有效的同义词，只是没有出现在参考翻译中，这将受到惩罚。

让我们看一个例子，这样你就能明白为什么这是一个问题。

> **原文(法语):** J'ai mangé la pomme。
> 
> 参考译文:我吃了苹果。

基于 BLEU，这些都是“同样糟糕”的输出句子。

> 我吃掉了苹果。
> 
> 我吃了一个苹果。
> 
> 我吃了土豆。

作为一个机器翻译系统的最终用户，我实际上对前两句没什么意见。尽管它们与参考译文不完全相同，但它们表达了意思。然而，第三句完全不能接受；它完全改变了原来的意思。

基于 BLEU 的指标之一， [NIST](http://www.mt-archive.info/HLT-2002-Doddington.pdf) ，通过对不匹配的 n-grams 进行加权来解决这个问题。因此，在更常见的 n-gram(如“of of”)上的错配将受到更低的惩罚，而在更罕见的 n-gram 上的错配(如“[水牛](http://mentalfloss.com/article/18209/buffalo-buffalo-buffalo-buffalo-buffalo-buffalo-buffalo-buffalo)”)将受到更高的惩罚。但是，虽然这解决了给予虚词太多权重的问题，但它实际上使惩罚同义词的问题(如“ambled”代表“walked”)*变得更糟*，因为这些同义词只出现在更罕见的 r-gram 中，因此被赋予更高的惩罚。

## BLEU 不考虑句子结构

也许你不相信“即使你弄乱了几个关键词，完全改变了句子的意思，你仍然可以得到很好的 BLEU 分数”这句话。也许一些语法会说服你？

> **句法**是对句子结构的研究。这是一个研究领域，它允许我们正式地模仿像“我用望远镜看到了狗”这样的句子，这可能意味着要么我在用望远镜看狗，要么狗拿着望远镜。这两种意思之间的差异只能通过模拟句子中的单词可以彼此具有不同关系的事实来捕捉。

我不是世界上最伟大的句法学家(差得远)，但即使是我也知道自然语言中有很多重要的内部句法结构，如果你随机打乱一个句子中单词的顺序，你要么得到 1)无意义的单词，要么得到 2)意义非常不同的东西。

幸运的是，在开发系统以自动建模该结构方面已经做了大量的工作，这被称为**解析**。

不幸的是，BLEU 并没有建立在这些研究之上。我可以理解你为什么想要避免它，因为解析往往是相当计算密集型的，每次求值都必须解析所有的输出确实会增加一些开销。(尽管有一些度量标准，如 STM 或子树度量标准，直接比较引用和输出翻译的解析。)

然而，不考虑句法结构的结果意味着表面词序完全混乱的输出可以得到与那些更加连贯的输出相同的分数。

Callison-Burch 等人(2006 年)对此有一个很好的说明。对于这组参考句子:

> Orejuela 在被带到美国飞机上时显得很平静，这架飞机将把他带到佛罗里达州的迈阿密。
> 
> Orejuela 在被护送到飞往佛罗里达州迈阿密的飞机上时显得很平静。
> 
> Orejuela 看起来很平静，因为他正被带到美国飞机上，飞机将带他去佛罗里达州的迈阿密。
> 
> Orejuela 看起来很平静，因为他被带到了美国飞机，将带他到佛罗里达州的迈阿密。

他们生成了这个机器翻译。

> 当他被带上飞往佛罗里达州迈阿密的美国飞机时，显得很平静。

这不是一个完美的翻译——这个人的名字被省略了，在句子的后半部分“will”后面没有动词——但也不是完全没有意义。然而，这个例子是:

> 在飞往佛罗里达州迈阿密的美国飞机上，他看起来很平静。

踢球者？第一个和第二个输出得到完全相同的 BLEU 分数，尽管第一个输出显然是更好的英语翻译。

## BLEU 不能很好地处理形态丰富的语言

如果像地球上的大多数人一样，你碰巧使用英语以外的语言，你可能已经发现了这个指标的问题:它是基于单词级匹配的。对于形态丰富的语言来说，这很快就会成为一个问题。

> **语素**是语言中最小的意义单位，组合在一起构成单词。英语中的一个例子是“cats”中的“s ”,它告诉我们不止有一只猫。一些语言，如土耳其语，在一个单词中有很多语素，而其他语言，如英语，通常每个单词的语素较少。

想想秘鲁人所说的 Shipibo 语中的下列句子。(这些例子来自 Pilar Valenzuela 的《[Shi pibo-koni bo 中的言据性，以及 Panoan](https://books.google.com/books?hl=en&lr=&id=GWk9AAAAQBAJ&oi=fnd&pg=PA33&dq=info:q5ttUx4ZjpAJ:scholar.google.com&ots=p_ThJSA1nj&sig=TmEBbdwVxZP8lj0xt4DHmHeKZ84#v=onepage&q&f=false) 中的类别比较概述。)

> 你好，我好。
> 
> Jawen jemaronki ani iki。

这两个都是英语句子“她的村庄很大”的完全可以接受的翻译你可能会注意到，以“jemar-”开头的中间词在两个句子中有不同的结尾。不同的词尾是不同的词素，表明说话者对村庄很大这一事实有多确定；最上面的意思是他们确实去过那里，最下面的是他们从别人那里听说的。

这种特殊类型的语素被称为“据实性标记”，而英语没有这种标记。然而，在 Shipibo 中，你*需要*这两个语素中的一个来使句子合乎语法，所以我们的参考译文肯定会有这两个语素中的一个。但是，如果我们没有碰巧生成我们在参考句子中的单词的精确形式，BLEU 会为此惩罚它…即使两个句子都很好地捕捉了英语的意思。

## 蓝色不太符合人类的判断

如果你的眼睛开始呆滞，当我进入语法位，现在是时候调回来了。

构建机器翻译、聊天机器人或问答系统的最终目标是什么？你最终希望人们使用它，对吗？如果一个系统不能提供有用的输出，人们就不会使用它。因此，你*实际上*想要优化的是使用你的系统的人喜欢它的程度，这是有道理的。我们使用的几乎所有指标都被设计成不同的近似方式。

当 BLEU 第一次被提出时，作者[确实做了一些行为测试](https://www.aclweb.org/anthology/P02-1040.pdf)，以确保该测量与人类判断相关。(并为此向他们表示祝贺！不幸的是，随着研究人员做了更多的实验来比较 BLEU 分数和人类的判断，他们发现这种相关性并不总是很强，其他指标往往更接近人类的判断，这取决于具体的任务。

例如，Turian 等人(2003 年)发现，在三种度量中，BLEU 与机器翻译的人类判断的相关性最差，简单 F1 与人类判断的相关性最强，其次是 NIST。 [Callison-Burch 等人](http://www.aclweb.org/anthology/E06-1032) (2006)研究了为共享任务开发的系统(就像学术上的 Kaggle 竞赛，但没有奖金),发现这些系统的相对排名非常不同，这取决于你是在看 BLEU 分数还是人类评估员的判断。而[孙](http://www.lrec-conf.org/proceedings/lrec2010/pdf/87_Paper.pdf) (2010)比较了三个不同的指标——BLEU、GTM 和 TER——再次发现，BLEU 评分与人类判断的相关性最小。

换句话说:如果你想让人们喜欢使用你的系统，你不应该只关注获得更高的 BLEU 分数。

## 我不是唯一有所保留的人

也许你仍然不相信 BLEU 并不总是这项工作的合适工具。那也行；事实上，我赞成你的怀疑态度！然而，我并不是唯一一个不热衷于这一指标的 NLP 从业者。这里有一些同行评议论文的快速链接，这些论文对 BLEU 的其他一些缺点进行了更多的讨论。

**同行评审论文:**

*   [Reiter](http://aclweb.org/anthology/J18-3002) (2018)对使用 BLEU 和人类判断进行评估的 ACL 论文进行了元综述，并发现它们仅在机器翻译系统的系统级审查中一起使用。
*   [Sulem 等人](http://aclweb.org/anthology/D18-1081) (2018)建议不要使用 BLEU 进行文本简化。他们发现，BLEU 分数不能很好地反映语法或意义保留。
*   [Novikova 等人](https://www.aclweb.org/anthology/D17-1238) (2017)表明，BLEU 以及其他一些常用的指标，在评估 NLG(自然语言生成)任务时，不能很好地映射到人类的判断。
*   [Ananthakrishnan 等人](https://core.ac.uk/download/pdf/23798335.pdf) (2006)列出了几种对 BLEU 的具体反对意见，并对 BLEU 得分较高的英语/印地语翻译中的具体错误进行了深入探究。

这里有一些未经同行评审的资源。(虽然它们可能不会让看你写的研究论文的同行评议者信服，但它们可能更容易说服你的老板。)

**其他资源:**

*   Amazon Research 的 Matt Post 对预处理对 BLEU 分数的影响进行了精彩的讨论。
*   本文由从事翻译工作的 Kirti Vashee 撰写，从译者的角度讨论了 BLEU 的问题。
*   Yoav Goldberg 做了一个非常好的演讲，其中包括在 2018 年自然语言生成国际会议上讨论为什么不应该为 NLG 使用 BLEU。您可以[在此处找到幻灯片](https://inlg2018.uvt.nl/wp-content/uploads/2018/11/INLG2018-YoavGoldberg.pdf)(搜索“BLEU 可能会误导”以获得相关幻灯片)。特别是，他和他的合作者发现，他们的句子简化模型**即使通过添加、删除或重复信息**也能获得很高的 BLEU 分数。
*   Ana Marasovi 的博客文章" [NLP 的泛化问题，以及研究人员如何解决它](https://thegradient.pub/frontiers-of-generalization-in-natural-language-processing/)"讨论了包括 BLEU 在内的个体指标如何无法捕捉模型处理与他们在训练期间接触到的数据不同的数据的能力。

# 那么你应该用什么来代替呢？

**我想让你在评估以文本为输出的系统时使用的主要东西是小心**，尤其是当你正在构建最终可能会投入生产的东西的时候。对于 NLP 从业者来说，思考我们的工作将如何被应用，以及什么可能出错，这真的很重要。想想[这个因为脸书把一篇说“早上好”的帖子翻译成“攻击他们”而被捕的巴勒斯坦人](https://www.theguardian.com/technology/2017/oct/24/facebook-palestine-israel-translates-good-morning-attack-them-arrest)！我不是特别针对脸书，我只是想指出，NLP 产品的风险可能比我们有时意识到的要高。

仔细挑选我们优化的指标是确保我们工作的系统实际可用的重要部分。例如，对于像机器翻译这样的任务，我个人认为惩罚意义上的巨大变化是非常重要的。

也就是说，有很多自动评估指标可以替代 BLEU。它们中的一些将更适合不同的任务，因此值得花一些时间来评估什么是适合您的特定项目的最佳选择。

两种流行的方法实际上是 BLEU 的衍生，旨在帮助解决它的一些缺点。

*   [NIST](http://www.mt-archive.info/HLT-2002-Doddington.pdf) ，正如我上面提到的，根据其稀有程度对 n-grams 进行加权。这意味着正确匹配一个罕见的 n-gram 比正确匹配一个常见的 n-gram 更能提高你的分数。
*   [ROUGE](http://www.aclweb.org/anthology/N03-1020) 是 BLEU 的修改版，侧重于召回而非精确。换句话说，它查看参考翻译中有多少 n 元语法出现在输出中，而不是相反。

还有许多方法可以用来评估不基于 BLEU 的序列对序列模型。其中一些是从机器学习的 NLP 的其他领域采用的方法。

*   [困惑](https://en.wikipedia.org/wiki/Perplexity)是信息论中的一种度量，更常用于语言建模。它衡量单词的学习概率分布与输入文本的匹配程度。
*   单词错误率，或称 [WER](https://en.wikipedia.org/wiki/Word_error_rate) ，是语音识别中常用的指标。它测量在给定参考输入的情况下，输出序列中的替换(“an”代表“the”)、删除和插入的数量。
*   [F 值](https://en.wikipedia.org/wiki/F1_score)，也就是通常所说的 F1，是精确度(有多少预测是正确的)和召回率(有多少可能的正确预测被做出)的平均值。

其他的是专门为序列到序列任务开发的。

*   [STM，或子树度量](http://aclweb.org/anthology/W05-0904)(我在上面提到过)比较参考和输出翻译的解析，并惩罚具有不同语法结构的输出。
*   METEOR 类似于 BLEU，但包括额外的步骤，如考虑同义词和比较词干(这样“running”和“runs”将被视为匹配)。与 BLEU 不同的是，它被明确设计用来比较句子而不是语料库。
*   [之三](http://www.cs.umd.edu/~snover/pub/amta06/ter_amta.pdf)，或翻译错误率，衡量将原始输出翻译转变为可接受的人工翻译所需的编辑次数。
*   TERp ，或 TER-plus，是 TER 的扩展，它还考虑了释义、词干和同义词。
*   hLEPOR 是一种度量标准，更适合于像土耳其语或捷克语这样形态复杂的语言。除其他因素外，它还考虑词类(名词、动词等)。)可以帮助捕获语法信息。
*   像赫莱波里一样，里伯斯并不依赖与英语具有相同品质的语言。它旨在为亚洲语言——如日语和汉语——提供更多信息，而不仅仅是单词边界。
*   MEWR 可能是列表中最新的指标，我发现它特别令人兴奋:它不需要参考翻译！(这对于可能没有大型并行语料库的低资源语言来说非常好。)它结合使用[单词和句子嵌入](https://www.kaggle.com/learn/embeddings?utm_medium=blog&utm_source=medium&utm_campaign=bleu)(捕捉意义的某些方面)和困惑来给翻译评分。

当然，我这里没有足够的空间来涵盖研究人员在这里开发的所有自动化指标。尽管如此，请随意在评论中加入一些你最喜欢的，以及你为什么喜欢它们！

# 所以你的意思是…这很复杂？

这几乎是问题的核心。语言是复杂的，这意味着自动测量语言是困难的。我个人认为，为自然语言生成开发评估指标可能是目前 NLP 中最困难的问题。(如果你和我一样感兴趣的话，实际上在 NAACL 2019 上有一个 [n 即将举行的研讨会。)](https://neuralgen.io/)

也就是说，有一个很好的方法可以确保你的系统确实在做人类喜欢的事情方面做得更好:你可以问真实的人他们是怎么想的。人工评估曾经是机器翻译的标准，我认为它仍然有一席之地。是的，它很贵，而且，是的，它需要更长的时间。但至少对于即将投入生产的系统，我认为你应该至少和人类专家进行一轮系统评估。

不过，在你进入那一轮之前，你可能需要使用至少一个自动评估指标。我强烈建议你使用 BLEU 当且仅当:

1.  **你在做机器翻译**
2.  **你在评估整个语料库和**
3.  **你知道指标的局限性，并准备接受它们。**

否则，我会花时间找到一个更适合您的特定问题的指标。

**关于作者**:

Rachael 是 Kaggle 的数据科学家(有趣的是，ka ggle 从未举办过使用 BLEU 作为评估指标的比赛)。她有语言学博士学位，还有一只名叫古斯塔夫的刺猬。如果你有兴趣看更多她的 NLP 教程和项目，[你可以在这里查看。](https://www.kaggle.com/rtatman/kernels?sortBy=voteCount&group=everyone&pageSize=20&userId=1162990&tagIds=11208)