<html>
<head>
<title>From Zero to Hero in XGBoost Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost 调优从零到英雄</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-zero-to-hero-in-xgboost-tuning-e48b59bfaf58?source=collection_archive---------1-----------------------#2019-05-21">https://towardsdatascience.com/from-zero-to-hero-in-xgboost-tuning-e48b59bfaf58?source=collection_archive---------1-----------------------#2019-05-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5b98" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">走过一些最常见的(也不太常见！)XGBoost 的超参数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4eaa164f4be97cf95a94ed33dc0ec833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qwa7Z0lIyYyAIdV8NB6r4A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Boosted Trees by <a class="ae ky" href="http://cican17.com/an-overview-of-boosted-trees/" rel="noopener ugc nofollow" target="_blank">Chen Shikun</a>.</figcaption></figure><p id="1a24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/dmlc/xgboost" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>或极端梯度增强是梯度增强算法的优化实现。自 2014 年<a class="ae ky" href="https://github.com/dmlc/xgboost" rel="noopener ugc nofollow" target="_blank">推出以来，XGBoost </a>因其预测性能和处理时间而成为机器学习黑客马拉松和竞赛的宠儿。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="04b0" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">这一切都始于助推…</h1><p id="5e5a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">增强是一种合奏技术。集成学习借鉴了<a class="ae ky" href="https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem" rel="noopener ugc nofollow" target="_blank">孔多塞的陪审团定理</a>和<em class="mz">群体智慧</em>的思想。因此，集成技术结合了不同模型的结果，以提高整体结果和性能</p><blockquote class="na"><p id="ccc6" class="nb nc it bd nd ne nf ng nh ni nj lu dk translated">多数票正确分类的概率比任何人(模型)都高，随着人(模型)的数量变大，多数票的准确率接近 100% — Scott Page，模型思考者。</p></blockquote><p id="d03e" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">在基于决策树的机器学习中，Boosting 算法实现了一个顺序过程，其中每个模型都试图纠正以前模型的错误。这个想法是将许多<em class="mz">弱学习者</em>转化为一个<em class="mz">强学习者。</em>梯度推进采用梯度下降算法来最小化顺序模型中的误差。梯度推进的主要低效之处在于它一次只创建一个决策树。</p><p id="8d85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了克服这一点，陈天奇和卡洛斯·盖斯特林建立了一个可扩展的树增强系统。它具有并行树构建、<a class="ae ky" href="https://stackoverflow.com/questions/473137/a-simple-example-of-a-cache-aware-algorithm" rel="noopener ugc nofollow" target="_blank">缓存感知</a>访问、稀疏感知、正则化和加权分位数草图，作为其<a class="ae ky" rel="noopener" target="_blank" href="/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d">系统优化和算法增强</a>的一部分。</p><blockquote class="na"><p id="d0cc" class="nb nc it bd nd ne nf ng nh ni nj lu dk translated">如有疑问，请使用欧文·张的智慧之言。</p></blockquote><p id="e193" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">为了向您介绍<a class="ae ky" href="https://github.com/dmlc/xgboost" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>及其超参数，我们将使用<a class="ae ky" href="https://www.kaggle.com/zalando-research/fashionmnist" rel="noopener ugc nofollow" target="_blank">时尚 MNIST </a>数据集构建一个简单的分类模型。</p><h1 id="e3d0" class="mc md it bd me mf np mh mi mj nq ml mm jz nr ka mo kc ns kd mq kf nt kg ms mt bi translated">预处理数据</h1><p id="f748" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">时尚 MNIST 数据集由与 10 个分类标签相关联的 60，000 幅 28x28 灰度图像的训练集和 10，000 幅图像的测试集组成。标签如下:</p><ul class=""><li id="a36f" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">0 T 恤/上衣</li><li id="7605" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">1 条裤子</li><li id="3ac2" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">2 件套头衫</li><li id="13e1" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">3 连衣裙</li><li id="6e17" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">4 件外套</li><li id="ae4d" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">5 凉鞋</li><li id="fefb" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">6 衬衫</li><li id="e084" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">7 运动鞋</li><li id="d1ce" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">8 袋</li><li id="6778" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">9 踝靴</li></ul><p id="85f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看第一个和最后一个图像。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/8009f397b33d505ed3550b9b8b3e0c73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6CFJ_S7OJhoRHarGZT1r9w.png"/></div></div></figure><p id="b61b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集中的每个实例(图像)有 784 个特征(每个像素一个)，每个特征中的值范围从 0 到 255，因此我们将使用<a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn 的</a> <code class="fe ol om on oo b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">StandardScaler</a></code>将这些值重新调整到一个更小的范围，平均值为零，单位方差。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h1 id="1eef" class="mc md it bd me mf np mh mi mj nq ml mm jz nr ka mo kc ns kd mq kf nt kg ms mt bi translated">基线模型</h1><p id="ad98" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们将使用一个<code class="fe ol om on oo b"><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" rel="noopener ugc nofollow" target="_blank">XGBoostClassifier</a></code>来预测给定图像的标签。建造一个<code class="fe ol om on oo b"><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" rel="noopener ugc nofollow" target="_blank">XGBoostClassifier</a></code>非常简单。我们将从一个简单的基线模型开始，然后继续前进。</p><p id="fd11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于第一次迭代，我们将只指定一个超参数:objective，并将其设置为<code class="fe ol om on oo b">“multi:softmax”</code>。<code class="fe ol om on oo b">objective</code>是学习任务超参数的一部分，它指定了要使用的学习任务(回归、分类、排序等)和函数。</p><p id="ee98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们回顾一下。什么是超参数？-它们是在训练模型之前初始化的参数，因为它们无法从算法中学习。它们控制定型算法的行为，并对模型的性能有很大影响。典型的比喻是这样的:<em class="mz">超参数是人们用来调整机器学习模型的旋钮</em>。它们对于优化和改进评估指标是必不可少的。</p><p id="e250" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们看看我们的模型和结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><pre class="kj kk kl km gt op oo oq or aw os bi"><span id="4d26" class="ot md it oo b gy ou ov l ow ox">Training F1 Micro Average:  0.8794833333333333<br/>Test F1 Micro Average:  0.8674<br/>Test Accuracy:  0.8674</span></pre><p id="db3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不算太寒酸！但是我们可以通过一些调整和<em class="mz">旋钮转动</em>来尝试打破这些最初的分数。</p><h1 id="434a" class="mc md it bd me mf np mh mi mj nq ml mm jz nr ka mo kc ns kd mq kf nt kg ms mt bi translated">像老板一样调音！</h1><p id="7852" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="https://github.com/dmlc/xgboost" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>的最大优势之一是可定制的程度，<em class="mz">即</em>一个令人生畏的长列表，您可以调整这些参数，它们主要是为了<a class="ae ky" href="https://gabrieltseng.github.io/2018/02/25/XGB.html" rel="noopener ugc nofollow" target="_blank">防止过度调整</a>。</p><p id="f1ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，最重要的问题是调整什么以及如何调整？对于什么是理想的超参数没有基准，因为这些将取决于您的具体问题、您的数据和您优化的目标。但是一旦你理解了甚至是最模糊的超参数的概念，你就可以像老板一样<strong class="lb iu">调优了</strong>！</p><p id="b4fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了找到我们的最佳超参数，我们可以使用<a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn 的</a>或<code class="fe ol om on oo b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">GridSearchCV</a></code>。两者之间的区别是分别在<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html" rel="noopener ugc nofollow" target="_blank">较低的运行时间和更好的性能</a>之间进行权衡。考虑数据集的大小<code class="fe ol om on oo b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank">RandomizedSearchCV</a></code>是这一次要走的路。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="0f95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先创建一个<code class="fe ol om on oo b"><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" rel="noopener ugc nofollow" target="_blank">XGBClassifier</a></code>对象，就像基线模型一样，除了<code class="fe ol om on oo b">objective</code>之外，我们还直接传递超参数<code class="fe ol om on oo b">tree_method</code>、<code class="fe ol om on oo b">predictor</code>、<code class="fe ol om on oo b">verbosity</code>和<code class="fe ol om on oo b">eval_metric</code>，而不是通过参数网格。前两个允许我们直接访问 GPU 功能，<code class="fe ol om on oo b">verbosity</code>让我们实时了解模型正在运行什么，<code class="fe ol om on oo b">eval_metric</code>是用于验证数据的评估指标——如您所见，您可以以<code class="fe ol om on oo b">Python list</code>的形式传递多个评估指标。</p><p id="b3ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，使用<code class="fe ol om on oo b">tree_method</code>和<code class="fe ol om on oo b">predictor</code>我们不断得到同样的错误。您可以在此处跟踪该错误<a class="ae ky" href="https://github.com/dmlc/xgboost/issues/4287#issuecomment-494055233" rel="noopener ugc nofollow" target="_blank">的状态。</a></p><pre class="kj kk kl km gt op oo oq or aw os bi"><span id="a3de" class="ot md it oo b gy ou ov l ow ox">Kernel error:<br/>In: /workspace/include/xgboost/./../../src/common/span.h, 	line: 489<br/>	T &amp;xgboost::common::Span&lt;T, Extent&gt;::operator[](long) const [with T = xgboost::detail::GradientPairInternal&lt;float&gt;, Extent = -1L]<br/>	Expecting: _idx &gt;= 0 &amp;&amp; _idx &lt; size()<br/>terminate called after throwing an instance of 'thrust::system::system_error'<br/>  what():  function_attributes(): after cudaFuncGetAttributes: unspecified launch failure</span></pre><p id="3fb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">鉴于我们无法修复根本问题(<strong class="lb iu">双关语为</strong>)，该模型必须在 CPU 上运行。所以我们的<code class="fe ol om on oo b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank">RandomizedSearchCV</a></code>的最终代码看起来像这样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="39a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ol om on oo b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank">RandomizedSearchCV</a></code>允许我们从参数网格给出的选项中找到超参数的最佳组合。然后我们可以通过<code class="fe ol om on oo b">model_xgboost.best_estimator_.get_params()</code>访问它们，这样我们就可以在模型的下一次迭代中使用它们。下面是这个模型的最佳估计。</p><pre class="kj kk kl km gt op oo oq or aw os bi"><span id="df6b" class="ot md it oo b gy ou ov l ow ox">Learning Rate: 0.1<br/>Gamma: 0.1<br/>Max Depth: 4<br/>Subsample: 0.7<br/>Max Features at Split: 1<br/>Alpha: 0<br/>Lambda: 1<br/>Minimum Sum of the Instance Weight Hessian to Make Child: 7<br/>Number of Trees: 100<br/>Accuracy Score: 0.883</span></pre><p id="1f34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的准确率提高了 2%！对于一个运行了 60 多个小时的模型来说，这已经不错了！</p><p id="d6c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们单独看看每个超参数。</p><ul class=""><li id="fef2" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">首先，让我们澄清一下这个学习率和梯度下降中的学习率是不一样的。在梯度推进的情况下，学习率意味着<a class="ae ky" href="https://stats.stackexchange.com/questions/354484/why-does-xgboost-have-a-learning-rate" rel="noopener ugc nofollow" target="_blank"> <em class="mz">减少</em>每个附加树</a>对模型的影响。在他们的论文<a class="ae ky" href="https://arxiv.org/pdf/1603.02754.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mz">A Scalable Tree Boosting System</em></a><em class="mz"/>Tianchi Chen 和 Carlos Guestrin 将这种正则化技术称为<em class="mz">收缩</em>，它是一种防止过拟合的附加方法。学习率越低，模型在防止过度拟合方面<a class="ae ky" href="https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f" rel="noopener ugc nofollow" target="_blank">就越稳健</a>。</li><li id="ac91" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">gamma</code>:数学上，这被称为<a class="ae ky" href="https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6" rel="noopener"><em class="mz"/></a>拉格朗日乘数，其目的是控制复杂度。它是损失函数的伪正则项；它代表<a class="ae ky" href="https://gabrieltseng.github.io/appendix/2018-02-25-XGB.html" rel="noopener ugc nofollow" target="_blank">在考虑分割</a>时，为了使分割发生，损失必须减少多少。</li><li id="c5cd" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">max_depth</code>:指一棵树的深度。它设置了在根和最远的叶子之间可以存在的最大节点数。请记住，较深的树容易过度拟合。</li><li id="f582" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">colsample_bytreee</code>:表示在构建每棵树时要考虑的列(<em class="mz">特性</em>)的一部分，因此它在构建每棵树时出现一次。在陈天琦和 Carlos Guestrin 的论文<a class="ae ky" href="https://arxiv.org/pdf/1603.02754.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mz">中提到了一种可扩展的树提升系统</em> </a> <em class="mz">，作为防止过拟合和提高计算速度的另一种主要技术。</em></li><li id="dee3" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">subsample</code>:表示在构建每个子树时要考虑的行(<em class="mz">观察值</em>)的一部分。陈天琦和卡洛斯·盖斯特林在他们的论文<a class="ae ky" href="https://arxiv.org/pdf/1603.02754.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mz">中提出了一种可扩展的树提升系统</em> </a> <em class="mz"> </em>推荐<code class="fe ol om on oo b">colsample_bytree</code>而不是<code class="fe ol om on oo b">subsample</code>来防止过拟合，因为他们发现前者在这方面更有效。</li><li id="0b68" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">reg_alpha</code> : L1 正则项。L1 正则化鼓励稀疏性(意味着将权重拉至 0)。当<code class="fe ol om on oo b">objective</code>是逻辑回归时，它会更有用，因为您可能需要在特征选择方面得到帮助。</li><li id="cd61" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">reg_lambda</code> : L2 正则项。L2 鼓励更小的权重，这种方法在树模型中更有用，在树模型中归零特性可能没有太大意义。</li><li id="cec3" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">min_child_weight</code>:类似于<code class="fe ol om on oo b">gamma</code>，因为它在分割步骤执行规则化。这是创建一个新节点所需的最小 Hessian 权重。黑森是二阶导数。</li><li id="ab9b" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">n_estimators</code>:适合的树数。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/7a9228a959119361eba17bb2f3f30c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*urP-acdQbOfCMdpIaozIag.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The Ugly Truth</figcaption></figure><p id="726a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是等等！还有更多。根据您试图解决的问题或您试图优化的内容，您还可以修改其他超参数:</p><ul class=""><li id="197f" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated"><code class="fe ol om on oo b">booster</code>:允许您选择使用哪个助推器:<code class="fe ol om on oo b">gbtree</code>、<code class="fe ol om on oo b">gblinear</code>或<code class="fe ol om on oo b">dart</code>。我们一直在使用<code class="fe ol om on oo b">gbtree</code>，但是<code class="fe ol om on oo b">dart</code>和<code class="fe ol om on oo b">gblinear</code>也有它们自己额外的超参数需要探索。</li><li id="b5e0" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">scale_pos_weight</code>:正负权重之间的平衡，在数据呈现高等级不平衡的情况下绝对应该使用。</li><li id="594c" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">importance_type</code>:指<code class="fe ol om on oo b"><a class="ae ky" href="https://datascience.stackexchange.com/questions/12318/how-do-i-interpret-the-output-of-xgboost-importance" rel="noopener ugc nofollow" target="_blank">feature_importances_</a></code> <a class="ae ky" href="https://datascience.stackexchange.com/questions/12318/how-do-i-interpret-the-output-of-xgboost-importance" rel="noopener ugc nofollow" target="_blank">方法</a>要使用的特征重要性类型。<code class="fe ol om on oo b">gain</code>计算一个特征对模型中所有树的相对贡献(相对增益越高，该特征越相关)。<code class="fe ol om on oo b">cover</code>当用于决定叶节点时，计算与特征相关的观察值的相对数量。<code class="fe ol om on oo b">weight</code>测量某个特征被用于在模型的所有树上分割数据的相对次数。</li><li id="08bb" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">base_score</code>:全局偏差。这个参数在处理高等级不平衡时很有用。</li><li id="4102" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe ol om on oo b">max_delta_step</code>:设置重量的最大可能绝对值。在处理不平衡的类时也很有用。</li></ul><p id="39d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意:</strong>我们使用<a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a> <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" rel="noopener ugc nofollow" target="_blank">包装器接口</a>为<a class="ae ky" href="https://github.com/dmlc/xgboost" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>执行了<code class="fe ol om on oo b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank">RandomizedSearchCV</a></code>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4d28" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">接下来是什么</h1><p id="8a8d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">一旦你有了模型的最佳估值器，你就可以做许多不同的事情。我建议将它们用作 XGBoost 内置交叉验证的超参数，这样您就可以利用<code class="fe ol om on oo b">early_stopping_rounds</code>功能——这是优化和防止过度拟合的又一步。但那是以后的事了！</p></div></div>    
</body>
</html>