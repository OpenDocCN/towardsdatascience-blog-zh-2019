<html>
<head>
<title>Classify Toxic Online Comments with LSTM and GloVe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 LSTM 和手套对有毒的网络评论进行分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classify-toxic-online-comments-with-lstm-and-glove-e455a58da9c7?source=collection_archive---------7-----------------------#2019-09-23">https://towardsdatascience.com/classify-toxic-online-comments-with-lstm-and-glove-e455a58da9c7?source=collection_archive---------7-----------------------#2019-09-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/a07c7ff4f37f7daee9c6ad63b35e8be9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IRtKZ02VB9SnNyiJ533PRg.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo credit: Pixabay</figcaption></figure><div class=""/><div class=""><h2 id="6df1" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">深度学习，文本分类，自然语言处理</h2></div><p id="0be9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这篇文章展示了如何使用一个简单的<a class="ae lq" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank"> LSTM </a>和一个预先训练好的<a class="ae lq" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>文件为<a class="ae lq" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank">有毒评论分类问题</a>创建一个强大的基线。</p><p id="4bbe" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">本文由四个主要部分组成:</p><ul class=""><li id="0846" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">准备数据</li><li id="27be" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">实现一个简单的 LSTM (RNN)模型</li><li id="a3b3" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">训练模型</li><li id="77ce" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">评估模型</li></ul><h1 id="273c" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">数据</h1><p id="7e8d" class="pw-post-body-paragraph ku kv jf kw b kx mx kg kz la my kj lc ld mz lf lg lh na lj lk ll nb ln lo lp ij bi translated">在下面的步骤中，我们将设置关键模型参数并拆分数据。</p><ul class=""><li id="45df" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">"<strong class="kw jg"> <em class="nc"> MAX_NB_WORDS </em> </strong>"设置被视为 tokenizer 特征的最大字数。</li><li id="86db" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">"<strong class="kw jg"><em class="nc">MAX _ SEQUENCE _ LENGTH</em></strong>"在此字数之后(在<strong class="kw jg"> <em class="nc"> MAX_NB_WORDS </em> </strong>最常用的字中)切断文本。</li><li id="ba49" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><strong class="kw jg"><em class="nc">VALIDATION _ SPLIT</em></strong>设置一部分数据用于验证，不用于训练。</li><li id="8112" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><strong class="kw jg"><em class="nc">EMBEDDING _ DIM</em></strong>定义了“矢量空间”的大小。</li><li id="0108" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><strong class="kw jg"> <em class="nc"> GLOVE_DIR </em> </strong>定义了手套文件的目录。</li><li id="3cac" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">将数据分为文本和标签。</li></ul><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">toxic_data.py</figcaption></figure><h1 id="9705" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">文本预处理</h1><p id="b910" class="pw-post-body-paragraph ku kv jf kw b kx mx kg kz la my kj lc ld mz lf lg lh na lj lk ll nb ln lo lp ij bi translated">在下面的步骤中，我们删除停用词，标点符号，并使一切小写。</p><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">preprocessing_toxic.py</figcaption></figure><p id="dd0f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">看一看样本数据。</p><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="e166" class="no mg jf nk b gy np nq l nr ns">print('Sample data:', texts[1], y[1])</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/ddf84f1d6fd9cc526c92e14ca1d32324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uBM87gz2Gltx3yVMnIcxLA.png"/></div></div></figure><ul class=""><li id="3456" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">我们创建一个分词器，配置成只考虑到<strong class="kw jg"> <em class="nc"> MAX_NB_WORDS </em> </strong>最常见的单词。</li><li id="1dcb" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">我们建立单词索引。</li><li id="2a0c" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">我们可以恢复计算出的单词索引。</li></ul><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="4000" class="no mg jf nk b gy np nq l nr ns">tokenizer = Tokenizer(num_words=MAX_NB_WORDS)<br/>tokenizer.fit_on_texts(texts)<br/>sequences = tokenizer.texts_to_sequences(texts)<br/>word_index = tokenizer.word_index<br/>print('Vocabulary size:', len(word_index))</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/40c773bb09960993646a85deda2738a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*NTTb1F5XkctADgLa5ufbmA.png"/></div></div></figure><ul class=""><li id="9f50" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">将整数列表转换为形状的 2D 整数张量(samples，maxlen)</li><li id="7031" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">每个序列后填充。</li></ul><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="921b" class="no mg jf nk b gy np nq l nr ns">data = pad_sequences(sequences, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)</span><span id="477f" class="no mg jf nk b gy nv nq l nr ns">print('Shape of data tensor:', data.shape)<br/>print('Shape of label tensor:', y.shape)</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/dd48fe89ae21ae781a5dc654df92aa06.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*eWXt9VSAA_2V7brJWjo4ug.png"/></div></figure><ul class=""><li id="3bae" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">打乱数据。</li></ul><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="373e" class="no mg jf nk b gy np nq l nr ns">indices = np.arange(data.shape[0])<br/>np.random.shuffle(indices)<br/>data = data[indices]<br/>labels = y[indices]</span></pre><p id="7cfb" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">创建培训验证分解。</p><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="3911" class="no mg jf nk b gy np nq l nr ns">num_validation_samples = int(VALIDATION_SPLIT*data.shape[0])<br/>x_train = data[: -num_validation_samples]<br/>y_train = labels[: -num_validation_samples]<br/>x_val = data[-num_validation_samples: ]<br/>y_val = labels[-num_validation_samples: ]</span><span id="30ac" class="no mg jf nk b gy nv nq l nr ns">print('Number of entries in each category:')<br/>print('training: ', y_train.sum(axis=0))<br/>print('validation: ', y_val.sum(axis=0))</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ead94f83d4d98291e69449da674ac941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*r-wJMBibN2YiLmpXW7wQ6w.png"/></div></figure><p id="936d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">数据看起来是这样的:</p><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="9a5a" class="no mg jf nk b gy np nq l nr ns">print('Tokenized sentences: \n', data[10])<br/>print('One hot label: \n', labels[10])</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/170bfeb8451cd795eea3d3c46b7bafb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pmuTy9hw8wRzRZ3vZ_Lpzw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 1</figcaption></figure><h1 id="36b4" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">创建模型</h1><ul class=""><li id="0502" class="lr ls jf kw b kx mx la my ld nz lh oa ll ob lp lw lx ly lz bi translated">我们将使用来自斯坦福的<a class="ae lq" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">预训练手套向量</a>通过解析预训练嵌入的数据转储来创建映射到已知嵌入的单词索引。</li><li id="cfdc" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">然后将单词嵌入加载到一个<code class="fe oc od oe nk b"><strong class="kw jg">embeddings_index</strong></code></li></ul><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">embedding_index.py</figcaption></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi of"><img src="../Images/a5e63d8152ac4c407a41fedc107d614f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*H0NUyoJNFT2ZNyGMyotNuw.png"/></div></figure><ul class=""><li id="ca09" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">创建嵌入层。</li><li id="677b" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">指定嵌入层的最大输入长度。</li><li id="0a17" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">利用来自先前嵌入层的输出，该嵌入层将 3-D 张量输出到 LSTM 层。</li><li id="8785" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">使用<a class="ae lq" href="https://keras.io/layers/pooling/" rel="noopener ugc nofollow" target="_blank">全局最大池层</a>将 3D 张量重塑为 2D 张量。</li><li id="2bf9" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">我们设置丢弃层来丢弃 10%的节点。</li><li id="1917" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">我们定义密集层以产生 50 的输出尺寸。</li><li id="be40" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">我们再次将输出馈入一个漏失层。</li><li id="2c3f" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">最后，我们将输出送入一个“Sigmoid”层。</li></ul><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">embedding_layers.py</figcaption></figure><p id="f920" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">是时候将模型编译成静态图进行训练了。</p><ul class=""><li id="68dc" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">定义输入、输出并配置学习过程。</li><li id="60ed" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">使用“Adam”优化器设置模型以优化我们的损失函数，将损失函数定义为“binary_crossentropy”。</li></ul><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="6c05" class="no mg jf nk b gy np nq l nr ns">model = Model(sequence_input, preds)<br/>model.compile(loss = 'binary_crossentropy',<br/>             optimizer='adam',<br/>             metrics = ['accuracy'])</span></pre><ul class=""><li id="cfb2" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">我们可以想象模型的设计师。</li></ul><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="d121" class="no mg jf nk b gy np nq l nr ns">tf.keras.utils.plot_model(model)</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a43385c063d7d8c98929c5051c796d9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*3FfepQcT4Ly1svedd2K2Mg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 2</figcaption></figure><h1 id="40d1" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">培养</h1><ul class=""><li id="a37b" class="lr ls jf kw b kx mx la my ld nz lh oa ll ob lp lw lx ly lz bi translated">为每一批输入 32 个填充的索引句子。验证集将用于评估模型是否过度拟合。</li><li id="c452" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">该模型将运行 2 个时期，因为即使 2 个时期也足以过度拟合。</li></ul><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="18ee" class="no mg jf nk b gy np nq l nr ns">print('Training progress:')<br/>history = model.fit(x_train, y_train, epochs = 2, batch_size=32, validation_data=(x_val, y_val))</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/e73c085533b44aa8bf12f61ea241e595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aBoQaY4FkxBUnT62mdSUyg.png"/></div></div></figure><h1 id="aaa6" class="mf mg jf bd mh mi mj mk ml mm mn mo mp kl mq km mr ko ms kp mt kr mu ks mv mw bi translated">评估模型</h1><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="e429" class="no mg jf nk b gy np nq l nr ns">loss = history.history['loss']<br/>val_loss = history.history['val_loss']</span><span id="c89d" class="no mg jf nk b gy nv nq l nr ns">epochs = range(1, len(loss)+1)</span><span id="70cd" class="no mg jf nk b gy nv nq l nr ns">plt.plot(epochs, loss, label='Training loss')<br/>plt.plot(epochs, val_loss, label='Validation loss')<br/>plt.title('Training and validation loss')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show();</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/6011df6e1f37e016690e08adc712b0da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*QWXhEAiQLhPy3Jfp0e_9RQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 3</figcaption></figure><pre class="nd ne nf ng gt nj nk nl nm aw nn bi"><span id="262d" class="no mg jf nk b gy np nq l nr ns">accuracy = history.history['accuracy']<br/>val_accuracy = history.history['val_accuracy']</span><span id="6e89" class="no mg jf nk b gy nv nq l nr ns">plt.plot(epochs, accuracy, label='Training accuracy')<br/>plt.plot(epochs, val_accuracy, label='Validation accuracy')<br/>plt.title('Training and validation accuracy')<br/>plt.ylabel('Accuracy')<br/>plt.xlabel('Epochs')<br/>plt.legend()<br/>plt.show();</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/9aab492659cde89760adec0fd946fd04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*-bYwMK0bqwTH8iO0RWRNgw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 4</figcaption></figure><p id="cc06" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Jupyter 笔记本可以在<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Toxic%20Comments%20LSTM%20GloVe.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。周一快乐！</p></div></div>    
</body>
</html>