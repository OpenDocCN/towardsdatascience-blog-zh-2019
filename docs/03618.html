<html>
<head>
<title>Create your first ETL Pipeline in Apache Spark and Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Apache Spark 和 Python 创建您的第一个 ETL 管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/create-your-first-etl-pipeline-in-apache-spark-and-python-ec3d12e2c169?source=collection_archive---------1-----------------------#2019-06-09">https://towardsdatascience.com/create-your-first-etl-pipeline-in-apache-spark-and-python-ec3d12e2c169?source=collection_archive---------1-----------------------#2019-06-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/87dc2eb2363d4885be12cab75930deba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*HL7Nig0G8VgKn0njpCWPbQ.jpeg"/></div></figure><p id="d5c5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这篇文章中，我将讨论 Apache Spark 以及如何在其中创建简单而健壮的 ETL 管道。您将了解 Spark 如何提供 API 来将不同的数据格式转换成数据框架和 SQL 以便进行分析，以及如何将一个数据源转换成另一个数据源。</p><h1 id="b629" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">什么是阿帕奇火花？</h1><p id="2821" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">据<a class="ae lv" href="https://en.wikipedia.org/wiki/Apache_Spark" rel="noopener ugc nofollow" target="_blank">维基百科</a>:</p><blockquote class="lw lx ly"><p id="0df9" class="ju jv lz jw b jx jy jz ka kb kc kd ke ma kg kh ki mb kk kl km mc ko kp kq kr ij bi translated">Apache Spark 是一个开源的分布式通用集群计算框架。Spark 提供了一个接口，通过隐式数据并行和容错对整个集群进行编程。</p></blockquote><p id="e520" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">来自<a class="ae lv" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">官网</a>:</p><blockquote class="lw lx ly"><p id="678c" class="ju jv lz jw b jx jy jz ka kb kc kd ke ma kg kh ki mb kk kl km mc ko kp kq kr ij bi translated">Apache Spark 是用于大规模数据处理的统一分析引擎。</p></blockquote><p id="53a0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">简而言之，Apache Spark 是一个用于处理、查询和分析大数据的框架。由于计算是在内存中完成的，因此它比竞争对手如 MapReduce 等快几倍。每天产生万亿字节数据的速度，需要一种能够高速提供实时分析的解决方案。Spark 的一些功能包括:</p><ul class=""><li id="922c" class="md me iq jw b jx jy kb kc kf mf kj mg kn mh kr mi mj mk ml bi translated">比传统的大规模数据处理框架快 100 倍。</li><li id="ad1f" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated">易于使用，因为您可以用 Python、R 和 Scala 编写 Spark 应用程序。</li><li id="4068" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated">它为 SQL、流和图形计算提供了库。</li></ul><h1 id="4697" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">Apache Spark 组件</h1><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/562d66a6972db269eadd11a1d0689af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*bvK44gaNIK763gl3.jpg"/></div></figure><h1 id="178a" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">火花核心</h1><p id="71d7" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">它包含 Spark 的基本功能，如任务调度、内存管理、与存储的交互等。</p><h1 id="24a9" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">Spark SQL</h1><p id="d7d7" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">它是一组用于与结构化数据交互的库。它使用类似 SQL 的接口与各种格式的数据进行交互，如 CSV、JSON、Parquet 等。</p><h1 id="a81c" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">火花流</h1><p id="3415" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">Spark 流是一个 Spark 组件，支持实时数据流的处理。实时流，如股票数据、天气数据、日志和各种其他内容。</p><h1 id="9ea6" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">MLib</h1><p id="a413" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">MLib 是 Spark 提供的一组机器学习算法，用于监督和非监督学习</p><h1 id="85c7" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">GraphX</h1><p id="a1a9" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">它是 Apache Spark 用于图形和图形并行计算的 API。它扩展了 Spark RDD API，允许我们创建一个带有附加到每个顶点和边的任意属性的有向图。它为 ETL、探索性分析和迭代图计算提供了统一的工具。</p><h1 id="e9c2" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">Spark 集群管理器</h1><p id="a5e1" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">Spark 支持以下资源/集群管理器:</p><ul class=""><li id="e6fd" class="md me iq jw b jx jy kb kc kf mf kj mg kn mh kr mi mj mk ml bi translated"><strong class="jw ir">Spark Standalone</strong>—Spark 附带的一个简单的集群管理器</li><li id="5c5e" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated">Apache Mesos  —一个通用的集群管理器，也可以运行 Hadoop 应用。</li><li id="2556" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated"><strong class="jw ir">Apache Hadoop YARN</strong>—Hadoop 2 中的资源管理器</li><li id="62c5" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated">Kubernetes  —一个用于自动化部署、扩展和管理容器化应用程序的开源系统。</li></ul><h1 id="f355" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">设置和安装</h1><p id="7bc7" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">从<a class="ae lv" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">这里</a>下载 Apache Spark 的二进制文件。您必须在系统上安装 Scala，并且还应该设置它的路径。</p><p id="e101" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于本教程，我们使用的是 2019 年 5 月发布的 2.4.3 版本。移动<code class="fe mw mx my mz b">/usr/local</code>中的文件夹</p><p id="8284" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe mw mx my mz b">mv spark-2.4.3-bin-hadoop2.7 /usr/local/spark</code></p><p id="d848" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">然后导出 Scala 和 Spark 的路径。</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="9d56" class="ne kt iq mz b gy nf ng l nh ni">#Scala Path<br/>export PATH="/usr/local/scala/bin:$PATH"</span><span id="924c" class="ne kt iq mz b gy nj ng l nh ni">#Apache Spark path<br/>export PATH="/usr/local/spark/bin:$PATH"</span></pre><p id="4525" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">通过在终端上运行<code class="fe mw mx my mz b">spark-shell</code>命令来调用 Spark Shell。如果一切顺利，您将看到如下内容:</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nk"><img src="../Images/c1d462afb017880f9928dda129186f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xLfoBHAp3JCeTpaQ.png"/></div></div></figure><p id="94b0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">它加载基于 Scala 的 shell。既然我们要使用 Python 语言，那么我们必须安装<strong class="jw ir"> PySpark </strong>。</p><p id="715a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe mw mx my mz b">pip install pyspark</code></p><p id="f727" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一旦安装完成，你可以在你的终端上运行命令<code class="fe mw mx my mz b">pyspark</code>来调用它:</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi np"><img src="../Images/422c813238e71cdc542c86ca519235f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9ePjdi7bzgeS1VPk.png"/></div></div></figure><p id="9dd1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">您会发现一个典型的 Python shell，但是它加载了 Spark 库。</p><h1 id="ba27" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">Python 开发</h1><p id="fc0a" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">让我们开始写我们的第一个程序。</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="027a" class="ne kt iq mz b gy nf ng l nh ni">from pyspark.sql import SparkSession<br/>from pyspark.sql import SQLContext</span><span id="78c2" class="ne kt iq mz b gy nj ng l nh ni">if __name__ == '__main__':<br/>    scSpark = SparkSession \<br/>        .builder \<br/>        .appName("reading csv") \<br/>        .getOrCreate()</span></pre><p id="7528" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们导入了两个库:<code class="fe mw mx my mz b">SparkSession</code>和<code class="fe mw mx my mz b">SQLContext</code>。</p><p id="cb79" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">SparkSession 是编程 Spark 应用程序的入口点。它允许您与 Spark 提供的<code class="fe mw mx my mz b">DataSet</code>和<code class="fe mw mx my mz b">DataFrame</code>API 进行交互。我们通过调用<code class="fe mw mx my mz b">appName</code>来设置应用程序名称。<code class="fe mw mx my mz b">getOrCreate()</code>方法要么返回应用程序的新 SparkSession，要么返回现有的 spark session。</p><p id="ad98" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们的下一个目标是读取 CSV 文件。我已经创建了一个示例 CSV 文件，名为<code class="fe mw mx my mz b">data.csv</code>，如下所示:</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="9b59" class="ne kt iq mz b gy nf ng l nh ni">name,age,country<br/>adnan,40,Pakistan<br/>maaz,9,Pakistan<br/>musab,4,Pakistan<br/>ayesha,32,Pakistan</span></pre><p id="3b94" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">代码是:</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="d09e" class="ne kt iq mz b gy nf ng l nh ni">if __name__ == '__main__':<br/>    scSpark = SparkSession \<br/>        .builder \<br/>        .appName("reading csv") \<br/>        .getOrCreate()</span><span id="8787" class="ne kt iq mz b gy nj ng l nh ni">data_file = '/Development/PetProjects/LearningSpark/data.csv'<br/>    sdfData = scSpark.read.csv(data_file, header=True, sep=",").cache()<br/>    print('Total Records = {}'.format(sdfData.count()))<br/>    sdfData.show()</span></pre><p id="3e72" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我设置了文件路径，然后调用<code class="fe mw mx my mz b">.read.csv</code>来读取 CSV 文件。参数是不言自明的。<code class="fe mw mx my mz b">.cache()</code>缓存返回的结果集，从而提高性能。当我运行该程序时，它会返回如下内容:</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nq"><img src="../Images/ac9f8e6a1df1f4ae7e3e62e0778c919e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UWfdmFPifEWsP1Xw.png"/></div></div></figure><p id="ca92" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">看起来很有趣，不是吗？现在，如果我想读取一个数据帧中的多个文件呢？让我们创建另一个文件，我将其命名为<code class="fe mw mx my mz b">data1.csv</code>，如下所示:</p><p id="3d0f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi">1</p><p id="a869" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi">2</p><p id="a811" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi">3</p><p id="a85a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi">4</p><p id="7732" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi">5</p><p id="4e6d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">姓名，年龄，国家</p><p id="6c68" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">诺琳，23 岁，英国</p><p id="a081" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">阿米尔，9 岁，巴基斯坦</p><p id="6d3e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">诺曼，4 岁，巴基斯坦</p><p id="3670" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">拉希德，12 岁，巴基斯坦</p><p id="064f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我要做的就是:</p><p id="fb8d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe mw mx my mz b">data_file = '/Development/PetProjects/LearningSpark/data*.csv'</code>它将读取所有以<em class="lz">数据</em>开始的 CSV 类型的文件。</p><p id="1d6b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">它将读取所有匹配模式的 CSV 文件并转储结果:</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nr"><img src="../Images/31ab7b53d2ae840d38bfa5d011b2f546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_FdLWZpdVJ2jNjsv.png"/></div></div></figure><p id="4ef4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如您所见，它将 CSV 中的所有数据转储到一个数据帧中。很酷吧。</p><p id="debe" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">但是有一点，只有当所有的 CSV 都遵循特定的模式时，这种转储才会起作用。如果您有一个不同列名的 CSV，那么它将返回以下消息。</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="3ded" class="ne kt iq mz b gy nf ng l nh ni">19/06/04 18:59:05 WARN CSVDataSource: Number of column in CSV header is not equal to number of fields in the schema:<br/> Header length: 3, schema size: 17<br/>CSV file: file:///Development/PetProjects/LearningSpark/data.csv</span></pre><p id="d45e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如您所见，Spark 抱怨 CSV 文件不一致，无法处理。</p><p id="fe91" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">您可以使用 DataFrame 执行许多操作，但是 Spark 为您提供了更简单、更熟悉的界面来使用<code class="fe mw mx my mz b">SQLContext</code>操作数据。它是 SparkSQL 的网关，允许您使用类似 SQL 的查询来获得想要的结果。</p><p id="cd49" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在我们进一步讨论之前，让我们先玩一些真实的数据。为此，我们使用我从<a class="ae lv" href="https://www.kaggle.com/aungpyaeap/supermarket-sales" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>获得的超市销售数据。在尝试 SQL 查询之前，让我们尝试按性别对记录进行分组。我们在这里处理 ETL 的<strong class="jw ir">提取</strong>部分。</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="bdff" class="ne kt iq mz b gy nf ng l nh ni">data_file = '/Development/PetProjects/LearningSpark/supermarket_sales.csv'<br/>sdfData = scSpark.read.csv(data_file, header=True, sep=",").cache()</span><span id="dd47" class="ne kt iq mz b gy nj ng l nh ni">gender = sdfData.groupBy('Gender').count()<br/>print(gender.show())</span></pre><p id="4b3a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">当您运行时，它会返回如下内容:</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ns"><img src="../Images/06f81bbce73fd18e37e84db68a86db81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ceEVQBMunNz4Yy2h.png"/></div></div></figure><p id="7b22" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe mw mx my mz b">groupBy()</code>按给定的列对数据进行分组。在我们的例子中，它是性别栏。</p><p id="da09" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">SparkSQL 允许您使用类似 SQL 的查询来访问数据。</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="cf4f" class="ne kt iq mz b gy nf ng l nh ni">sdfData.registerTempTable("sales")<br/>output =  scSpark.sql('SELECT * from sales')<br/>output.show()</span></pre><p id="e750" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，我们从数据帧中创建一个临时表。为此，使用了<code class="fe mw mx my mz b">registerTampTable</code>。在我们的例子中，表名是<strong class="jw ir">销售</strong>。一旦完成，你就可以对它使用典型的 SQL 查询。在我们的例子中，它是<strong class="jw ir"> Select * from sales </strong>。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nt"><img src="../Images/bf2c1fc4821850160564b42c495480d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0y_ONNQ2ozDGUo3c.png"/></div></div></figure><p id="7938" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">或者类似下面的内容:</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="1c69" class="ne kt iq mz b gy nf ng l nh ni">output = scSpark.sql('SELECT * from sales WHERE `Unit Price` &lt; 15 AND Quantity &lt; 10')<br/>output.show()</span></pre><p id="f390" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">或者甚至是聚合值。</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="c0d4" class="ne kt iq mz b gy nf ng l nh ni">output = scSpark.sql('SELECT COUNT(*) as total, City from sales GROUP BY City')<br/>output.show()</span></pre><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nu"><img src="../Images/a54ff9a636b4a1b8cce1db01e24b5ccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Zs7Ukg3_Ko2ibUdN.png"/></div></div></figure><p id="4de3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">很灵活，对吧？</p><p id="a280" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们刚刚完成了 ETL 的<strong class="jw ir">转换</strong>部分。</p><p id="86bb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最后<strong class="jw ir">加载 ETL 的</strong>部分。如果您想保存这些转换后的数据，该怎么办？你有很多选择，RDBMS，XML 或者 JSON。</p><p id="0f1a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe mw mx my mz b">output.write.format('json').save('filtered.json')</code></p><p id="574b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">当您运行它时，Sparks 会创建以下文件夹/文件结构。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4272b36aa9ebe7a30942ae5cca519546.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*fRqZhsjdK21TJNpC.png"/></div></figure><p id="0784" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">它用文件名创建了一个文件夹，在我们的例子中是<strong class="jw ir"> filtered.json </strong>。然后，一个名为<strong class="jw ir"> _SUCCESS </strong>的文件会告知操作是否成功。如果失败，将生成一个名为<strong class="jw ir"> _FAILURE </strong>的文件。然后，您会在这里找到多个文件。之所以有多个文件，是因为每个工作都涉及到写入文件的操作。如果你想创建一个单独的文件(不推荐)，那么可以使用<code class="fe mw mx my mz b">coalesce</code>,从所有分区收集数据并减少到一个单独的数据帧中。</p><p id="607c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe mw mx my mz b">output.coalesce(1).write.format('json').save('filtered.json')</code></p><p id="03b1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">它将输出以下数据:</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="d362" class="ne kt iq mz b gy nf ng l nh ni">{"total":328,"City":"Naypyitaw"}<br/>{"total":332,"City":"Mandalay"}<br/>{"total":340,"City":"Yangon"}</span></pre><h1 id="4b8c" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">MySQL 和 Apache 火花集成</h1><p id="98a1" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">上面的数据帧包含转换后的数据。我们希望将这些数据加载到 MYSQL 中，以便进一步使用，如可视化或在应用程序上显示。</p><p id="c862" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，我们需要 MySQL 连接器库来与 Spark 交互。我们将从<a class="ae lv" href="https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-8.0.16.tar.gz" rel="noopener ugc nofollow" target="_blank"> MySQL 网站</a>下载连接器，并放在一个文件夹中。我们将修改<code class="fe mw mx my mz b">SparkSession</code>以包含 JAR 文件。</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="4a65" class="ne kt iq mz b gy nf ng l nh ni">scSpark = SparkSession \<br/>        .builder \<br/>        .appName("reading csv") \<br/>        .config("spark.driver.extraClassPath", "/usr/local/spark/jars/mysql-connector-java-8.0.16.jar") \<br/>        .getOrCreate()</span></pre><p id="114a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe mw mx my mz b">output</code>现在看起来如下:</p><pre class="ms mt mu mv gt na mz nb nc aw nd bi"><span id="4ead" class="ne kt iq mz b gy nf ng l nh ni">output = scSpark.sql('SELECT COUNT(*) as total, City from sales GROUP BY City')<br/>    output.show()<br/>    output.write.format('jdbc').options(<br/>        url='jdbc:mysql://localhost/spark',<br/>        driver='com.mysql.cj.jdbc.Driver',<br/>        dbtable='city_info',<br/>        user='root',<br/>        password='root').mode('append').save()</span></pre><p id="620b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在运行脚本之前，我在 DB 中创建了所需的 Db 和表。如果一切顺利，您应该会看到如下结果:</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nw"><img src="../Images/8bdf309b50f03ffbe48acf55b91b2414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hhQtNU2R-QE6UAPJ.png"/></div></div></figure><p id="31f4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如您所见，Spark 使得从一个数据源向另一个数据源传输数据变得更加容易。</p><h1 id="0ce0" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">结论</h1><p id="a7c9" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">Apache Spark 是一个非常苛刻和有用的大数据工具，它有助于非常容易地编写 ETL。您可以加载数 Pb 的数据，并通过建立一个包含多个节点的集群来轻松处理这些数据。本教程只是让您对 Apache Spark 编写 ETL 的方式有一个基本的了解。您应该查看文档和其他资源来深入了解。</p><p id="faaa" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="lz">本帖原载</em> <a class="ae lv" href="http://blog.adnansiddiqi.me/create-your-first-etl-pipeline-in-spark-and-python/" rel="noopener ugc nofollow" target="_blank"> <em class="lz">此处</em> </a> <em class="lz">。</em></p></div></div>    
</body>
</html>