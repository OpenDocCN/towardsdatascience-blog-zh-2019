<html>
<head>
<title>Learning to Drive Smoothly in Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">几分钟内学会平稳驾驶</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4?source=collection_archive---------4-----------------------#2019-01-27">https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4?source=collection_archive---------4-----------------------#2019-01-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f175" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">小型赛车上的强化学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e3713abe60565ffef777d3f5c38333fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CBCj13inKdUEJmwkQerXGg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Donkey Car in action with teleoperation control panel in the unity simulator</figcaption></figure><p id="bd7a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我们将看到如何在几分钟内训练一辆自动驾驶赛车，以及如何平稳地控制它。这种基于强化学习(RL)的方法，在这里的模拟(驴车模拟器)中提出，被设计成适用于现实世界。它建立在一家名为<a class="ae lr" href="https://wayve.ai/" rel="noopener ugc nofollow" target="_blank"> Wayve.ai </a>的专注于自动驾驶的初创公司的工作基础上。</p><p id="a6db" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文中使用的代码和模拟器是开放源代码和公共的。更多信息请查看<a class="ae lr" href="https://github.com/araffin/learning-to-drive-in-5-minutes" rel="noopener ugc nofollow" target="_blank">关联的 GitHub 库</a>；)(预先训练的控制器也可以下载)</p><p id="85c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">重要提示:如需最新版本(使用</strong><a class="ae lr" href="https://github.com/DLR-RM/stable-baselines3" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir">Stable-baselines 3</strong></a><strong class="kx ir">和 PyTorch)，请查看</strong><a class="ae lr" href="https://github.com/araffin/aae-train-donkeycar/releases/tag/live-twitch-2" rel="noopener ugc nofollow" target="_blank">https://github . com/araffin/aae-train-donkey car/releases/tag/live-twitch-2</a></p><p id="87c1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">2022 年 4 月更新</strong>:我用强化学习做了一系列学习赛车的视频:<a class="ae lr" href="https://www.youtube.com/watch?v=ngK33h00iBE&amp;list=PL42jkf1t1F7dFXE7f0VTeFLhW0ZEQ4XJV" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=ngK33h00iBE&amp;list = pl 42 JK f1 t1 f 7 dfxe 7 f 0 vteflhw 0 ze Q4 XV</a></p><h2 id="c59d" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">录像</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><h2 id="1668" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">GitHub 库:重现结果</h2><div class="mn mo gp gr mp mq"><a href="https://github.com/araffin/learning-to-drive-in-5-minutes" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">araffin/5 分钟学会驾驶</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">实施强化学习方法，使汽车在几分钟内学会平稳驾驶——5 分钟内学会驾驶</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">github.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne kp mq"/></div></div></a></div><h2 id="cf54" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">赛车比赛</h2><p id="21d9" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">自从几年前 DIY Robocars 诞生以来，现在已经有了无数的自主赛车比赛(例如<a class="ae lr" href="http://toulouse-robot-race.org/" rel="noopener ugc nofollow" target="_blank">图卢兹机器人大赛</a>、<a class="ae lr" href="http://www.ironcar.org/" rel="noopener ugc nofollow" target="_blank">铁车、</a>……)。在这些项目中，目标很简单:你有一辆赛车，它必须在赛道上尽可能快地行驶，只给它车载摄像头的图像作为输入。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/4de4ee9de4ed151a7f05d6d133853174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*z7xGwQwzLI-5PoyxsfvqrQ.gif"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The Warehouse level, inspired by DIY Robocars</figcaption></figure><p id="47c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">自驾挑战是进入机器人领域的好方法。为了方便学习，开发了开源的自动驾驶平台<a class="ae lr" href="http://www.donkeycar.com/" rel="noopener ugc nofollow" target="_blank">驴车</a>。在其生态系统中，现在有一个以那个小机器人为特色的<a class="ae lr" href="https://github.com/tawnkramer/sdsandbox/tree/donkey" rel="noopener ugc nofollow" target="_blank"> unity 模拟器</a>。我们将在这辆驴车上测试提议的方法。</p><h2 id="36a5" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">概述</h2><p id="342b" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">在简要回顾了小型自动驾驶汽车比赛中使用的不同方法后，我们将介绍什么是强化学习，然后详细介绍我们的方法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/04c3dfea2991b2067b33f1e76a1308b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cv0mRiU_J9ji_gq_3Bl4cQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae lr" href="https://becominghuman.ai/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63" rel="noopener ugc nofollow" target="_blank">Autonomous Racing Robot With an Arduino, a Raspberry Pi and a Pi Camera</a></figcaption></figure><h2 id="186f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">自驾车比赛中使用的方法:路线跟踪和行为克隆</h2><p id="4712" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">在介绍 RL 之前，我们将首先快速回顾一下目前在 RC 赛车比赛中使用的不同解决方案。</p><p id="2b90" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在<a class="ae lr" href="https://becominghuman.ai/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63" rel="noopener ugc nofollow" target="_blank">之前的博客文章</a>中，我描述了第一种自动驾驶的方法，它结合了<em class="nm">计算机视觉和 PID 控制器</em>。虽然这个想法很简单，适用于许多设置，但它需要手动标记数据(以告诉汽车赛道的中心在哪里)，这既费钱又费力(相信我，手动标记并不好玩！).</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/27e0864a254117f2f7b40f9280600152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYn2p9RDul0Iiy1yqAfJ3w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae lr" href="https://www.youtube.com/watch?v=xhI71ZdSh6k" rel="noopener ugc nofollow" target="_blank">Predicting where is the center of the track</a></figcaption></figure><p id="0a9b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">作为另一种方法，许多竞争对手使用监督学习来<em class="nm">再现人类驾驶员行为</em>。为此，人类需要在几圈期间手动驾驶汽车，记录相机图像和来自操纵杆的相关控制输入。然后，训练一个模型来再现人类驾驶。然而，这种技术并不是真正健壮的，需要每条赛道的同质驾驶和再训练，因为它的泛化能力相当差。</p><h2 id="9dd1" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">什么是强化学习(RL)，我们为什么要使用它？</h2><p id="40a0" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">鉴于上述问题，强化学习(RL)似乎是一个有趣的选择。</p><p id="c476" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在强化学习环境中，一个代理(或机器人)作用于它的环境，并接收一个奖励作为反馈。它可以是积极的奖励(机器人做了好事)或消极的奖励(机器人应该受到惩罚)。</p><p id="1463" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nm">机器人的目标是累积奖励最大化。</em>为了做到这一点，它通过与世界的交互来学习所谓的策略(或行为/控制器),将它的感官输入映射到行动。</p><p id="839e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的例子中，<em class="nm">输入是摄像机图像，动作是油门和转向角度</em>。因此，如果我们以这样一种方式模拟奖励，即赛车保持在赛道上并使其速度最大化，我们就完成了！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c73c9281f30e2aeefc8da04bb6e29fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*vhcFaoSKKU4hLsnCVqY-8g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae lr" href="https://github.com/hill-a/stable-baselines" rel="noopener ugc nofollow" target="_blank">Stable-Baselines: an easy to use reinforcement learning library</a></figcaption></figure><p id="6bd7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是<a class="ae lr" href="https://spinningup.openai.com/en/latest/" rel="noopener ugc nofollow" target="_blank">强化学习</a>的美妙之处，你只需要很少的假设(这里只设计一个奖励函数),它会直接优化你想要的(在赛道上快速前进，赢得比赛！).</p><p id="2db0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意:这不是第一篇关于小型无人驾驶汽车强化学习的博客文章，但与<a class="ae lr" href="https://flyyufelix.github.io/2018/09/11/donkey-rl-simulation.html" rel="noopener ugc nofollow" target="_blank">以前的方法</a>、<em class="nm">相比，本文介绍的技术只需要几分钟</em>、<strong class="kx ir">、</strong>(而不是几个小时)就可以学会一个良好而平滑的控制策略(对于一个平滑的控制器来说大约需要 5 到 10 分钟，对于一个非常平滑的控制器来说大约需要 20 分钟)。</p><p id="08ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们已经简要介绍了什么是 RL，我们将进入细节，从剖析 Wayve.ai 方法开始，这是我们方法的基础。</p><h2 id="1c59" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">一天学会驾驶——way ve . ai 方法的关键要素</h2><p id="1a37" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated"><a class="ae lr" href="https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning" rel="noopener ugc nofollow" target="_blank"> Wayve.ai </a>描述了一种在简单道路上训练现实世界中自动驾驶汽车的方法。这种方法由几个关键要素组成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a580601d609ab575dd2e2d76ce413ee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*tf2jFvzymcIakY3vfaXoag.jpeg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae lr" href="https://www.youtube.com/watch?v=eRwTbRtnT1I" rel="noopener ugc nofollow" target="_blank">Wayve.ai approach</a>: learning to drive in a day</figcaption></figure><p id="8848" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，他们<em class="nm">训练一个特征提取器</em>(这里是一个可变自动编码器或<em class="nm"> VAE </em>)将图像压缩到一个更低维度的空间。该模型被训练来重建输入图像，但是包含迫使其压缩信息的瓶颈。</p><p id="7dfd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">这个从原始数据中提取相关信息的步骤叫做</strong> <a class="ae lr" href="https://github.com/araffin/robotics-rl-srl" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">状态表征学习(SRL) </strong> </a>，是我主要的<a class="ae lr" href="https://openreview.net/forum?id=Hkl-di09FQ" rel="noopener ugc nofollow" target="_blank">研究课题</a>。这显著地允许减少搜索空间，并因此加速训练。下图显示了 SRL 和端到端强化学习之间的联系，也就是说，直接从像素学习控制策略。</p><p id="e4e1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意:训练自动编码器<a class="ae lr" href="https://arxiv.org/pdf/1802.04181.pdf" rel="noopener ugc nofollow" target="_blank">并不是提取有用特征的唯一解决方案</a>，你也可以训练例如<a class="ae lr" href="https://github.com/araffin/srl-zoo" rel="noopener ugc nofollow" target="_blank">逆动力学模型</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/f507dcffa84821df6392e6b5f9569280.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1y7Z_TxH7j6fDhSQexl1Iw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae lr" href="https://openreview.net/forum?id=Hkl-di09FQ" rel="noopener ugc nofollow" target="_blank">Decoupling Feature Extraction from Policy Learning</a></figcaption></figure><p id="64e8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第二个关键要素是使用名为<a class="ae lr" href="https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html" rel="noopener ugc nofollow" target="_blank">深度确定性策略梯度(DDPG) </a>的 RL 算法，该算法使用 VAE 特性作为输入来学习控制策略。这个政策每集之后都会更新。该算法的一个重要方面是<em class="nm">它有一个内存，称为重放缓冲区</em><strong class="kx ir"/>，在这里它与环境的交互被记录下来，并可以在以后“重放”。因此，即使汽车不与世界互动，它也可以从这个缓冲区中采样经验来更新它的策略。</p><p id="dc05" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在人类干预之前，汽车被训练成最大化行驶的米数。这是最后一个关键因素:一旦汽车开始偏离道路，人类操作员就结束这一集。这个<strong class="kx ir"> </strong> <em class="nm">提前终止</em>真的很重要(如<a class="ae lr" href="https://xbpeng.github.io/projects/DeepMimic/index.html" rel="noopener ugc nofollow" target="_blank">深度模仿</a>所示)，防止汽车探索不感兴趣的区域来解决任务。</p></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><p id="81bf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">到目前为止，没有什么新的东西被提出来，我们只是总结了 Wayve.ai 的方法。以下是我对基本技术的所有修改。</p></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><h2 id="ef78" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">几分钟内学会驾驶——最新方法</h2><p id="99a7" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">虽然 Wayve.ai 技术在原理上可能行得通，但要将其应用于自动驾驶的遥控汽车，还需要解决一些问题。</p><p id="b570" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，因为特征提取器(VAE)在每集之后被训练，所以特征的分布不是固定的。也就是说，<em class="nm">特征随着时间的推移而变化</em>，并可能导致策略训练的不稳定性。此外，在笔记本电脑上训练 VAE(没有 GPU)相当慢，所以我们希望避免在每一集后重新训练 VAE。</p><p id="709c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了解决这两个问题，我决定<em class="nm">事先训练一只 VAE</em>并使用谷歌<a class="ae lr" href="https://colab.research.google.com/drive/1mF2abRb_yi4UNqYXVBF-t4FuCy6fl1c1#scrollTo=9bIR_N7R11XI" rel="noopener ugc nofollow" target="_blank"> Colab 笔记本</a>来保存我的电脑。通过这种方式，使用固定的特征提取器来训练策略。</p><p id="0912" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下面的图片中，我们<a class="ae lr" href="https://github.com/araffin/srl-zoo" rel="noopener ugc nofollow" target="_blank">探索 VAE 学到了什么</a>。我们在它的潜在空间中导航(使用滑块)并观察重建的图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/dd08a7178f11ba8a76ea406fd3f4c2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*EvSBN0zynU-RuWbhXWfdhQ.gif"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae lr" href="https://github.com/araffin/srl-zoo" rel="noopener ugc nofollow" target="_blank">Exploring the latent space</a> learned by the VAE</figcaption></figure><p id="37d4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，众所周知，DDPG 是不稳定的(在某种意义上，它的表现会在训练中灾难性地下降)，并且很难调整。幸运的是，最近一个名为<a class="ae lr" href="https://stable-baselines.readthedocs.io/en/master/modules/sac.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">的软演员评论家</strong> </a> <strong class="kx ir"> (SAC)的算法具有相当的性能，并且更容易调整</strong> *。</p><p id="5066" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">*在我的实验中，我尝试了 PPO、SAC 和 DDPG。DDPG 和 SAC 在几集内给出了最好的结果，但 SAC 更容易调整。</p><p id="58a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于这个项目，我使用了我为<a class="ae lr" href="https://github.com/hill-a/stable-baselines" rel="noopener ugc nofollow" target="_blank">stable-baselines</a><strong class="kx ir"/>编写的软 Actor-Critic (SAC)实现(如果你正在使用 RL，我肯定推荐你看一看；) )，里面有算法的<a class="ae lr" href="https://bair.berkeley.edu/blog/2018/12/14/sac/" rel="noopener ugc nofollow" target="_blank">最新改进</a>。</p><div class="mn mo gp gr mp mq"><a href="https://github.com/hill-a/stable-baselines" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">丘陵/稳定基线</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">OpenAI 基线的一个分支，强化学习算法的实现- hill-a/stable-baselines</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">github.com</p></div></div><div class="mz l"><div class="ny l nb nc nd mz ne kp mq"/></div></div></a></div><p id="8221" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我更新了奖励函数和动作空间，以平滑控制和最大化速度。</p><h2 id="4a3b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">奖励功能:走得快但留在赛道上！</h2><p id="1d1f" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">机器人汽车没有任何里程计(也没有速度传感器)，因此行驶的米数(也没有速度)不能作为奖励。</p><p id="077d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，我决定在每个时间步给予“生命奖励”(即停留在赛道上的+1 奖励)，并对机器人进行惩罚，对离开赛道的使用<strong class="kx ir"/>碰撞惩罚<strong class="kx ir"/>(-10 奖励)。此外，我发现惩罚开得太快的车也是有益的:与油门成比例的额外负奖励被加到撞车惩罚上。</p><p id="38ef" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，因为<strong class="kx ir"> </strong>我们想跑得快，因为它是一辆赛车，我添加了一个与当前油门成比例的“油门加成”。这样，<strong class="kx ir">机器人会尽量待在轨道上，同时最大化速度</strong>。</p><p id="d7bc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">总结一下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/3b4cd4be26f3f19a2c13e9c93ddf4289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yay32inVCySHAV9sspqQAw.png"/></div></div></figure><p id="19e3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中 w1 和 w2 是常数，允许平衡目标(w1 &lt;&lt; 10，w2 &lt;&lt; 1，因为它们是次要目标)</p><h2 id="c2ab" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">避免摇晃控制:学习平稳驾驶</h2><blockquote class="oa ob oc"><p id="a09b" class="kv kw nm kx b ky kz jr la lb lc ju ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated">世界并不是随机的。如果你注意到了——机器人不会自发地开始颤抖。除非你给它接上一个 RL 算法。—埃莫·托多洛夫</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a322fb12a155612a7649ac6753fae454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*20LZ6UZUHUjaCarRgNHfEw.gif"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Left: Shaky Control — Right: Smooth Control using the proposed technique</figcaption></figure><p id="f8ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果到目前为止你应用了所介绍的方法，它将会起作用:赛车将会停留在赛道上，并且会试着跑得更快。然而，<strong class="kx ir">你很可能会以一个不稳定的控制结束:</strong>汽车会如上图所示振荡，因为它没有动力不这样做，它只是试图最大化它的回报。</p><p id="9a5a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">平滑控制的解决方案是约束转向角的变化，同时用先前命令的历史增加输入</strong>(转向和油门)。这样，<em class="nm">你就能在转向中保持连贯性。</em></p><p id="4247" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">举例来说，如果当前汽车转向角度为 0，并且它突然尝试以 90°转向，则连续性约束将仅允许它以 40°转向。因此，两个连续转向命令之间的差异保持在给定的范围内。这种额外的约束是以多一点培训为代价的。</p><p id="97e2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在找到令人满意的解决方案之前，我花了几天时间试图解决这个问题<strong class="kx ir">，以下是我尝试过但没有成功的方法</strong>:</p><ul class=""><li id="b6a0" class="oh oi iq kx b ky kz lb lc le oj li ok lm ol lq om on oo op bi translated">输出相对转向而不是绝对转向:产生较低频率的振荡</li><li id="37a6" class="oh oi iq kx b ky oq lb or le os li ot lm ou lq om on oo op bi translated">添加一个连续性惩罚(惩罚机器人转向的高变化):机器人没有优化正确的事情，它有时会工作，但后来不会留在轨道上。如果惩罚的成本太低，它就忽略它。</li><li id="1c2e" class="oh oi iq kx b ky oq lb or le os li ot lm ou lq om on oo op bi translated">限制最大转向:在急转弯时，汽车不能再停留在赛道上</li><li id="2fad" class="oh oi iq kx b ky oq lb or le os li ot lm ou lq om on oo op bi translated"><a class="ae lr" href="https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/" rel="noopener ugc nofollow" target="_blank">堆叠几帧</a>以给出一些速度信息:产生较低频率的振荡</li></ul><p id="9d09" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注:最近，<a class="ae lr" href="http://robotics.sciencemag.org/content/4/26/eaau5872" rel="noopener ugc nofollow" target="_blank">苏黎世联邦理工学院的研究人员</a>建议利用课程学习进行持续的节能控制。这可能是第二个解决方案(尽管有点难以调整)。</p><h2 id="5df6" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">概述该方法</h2><p id="dfb3" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">在我们的方法中，我们将策略学习从特征提取中分离出来，并添加一个额外的约束来平滑控制。</p><p id="4f61" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，人类通过手动驾驶汽车来收集数据(手动驾驶约 5 分钟可获得 10k 张图像)。这些图像被用来训练 VAE。</p><p id="59e4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，我们在探索阶段(使用随机策略)和策略培训(当人类将汽车放回赛道以优化花费的时间时完成)之间交替。</p><p id="a2eb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了训练该策略，首先使用 VAE(这里具有 64 维的潜在空间)对图像进行编码，并将其与最近十次采取的动作(油门和转向)的历史连接，从而创建 84D 特征向量。</p><p id="3691" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">控制策略由神经网络(32 和 16 个单元的两个全连接层，具有 ReLU 或 eLU 激活功能)表示。</p><p id="5efc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该控制器输出转向角和油门。我们将油门限制在给定的范围内，并且还限制了当前转向角和先前转向角之间的差异。</p><h2 id="d18f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">结论</h2><p id="0914" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">在这篇文章中，我们提出了一种方法，只使用一个摄像头，在几分钟内学习驴车的平稳控制策略。</p><p id="aeb4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于该方法旨在应用于现实世界，这肯定是我在这个项目中的下一步:在一辆真实的 RC 汽车上测试该方法*(见下文)。这将需要缩小 VAE 模型(政策网络已经很小了)，以便让它在树莓派上运行。</p><p id="f5af" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">今天到此为止，不要犹豫测试代码，评论或提问，记住，分享是关怀；)!</p><p id="91a9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">* Roma Sokolkov 在一辆真实的遥控汽车上复制了 wayve.ai 方法<a class="ae lr" href="https://www.youtube.com/watch?v=6JUjDw9tfD4" rel="noopener ugc nofollow" target="_blank">，但是这不包括平滑控制的最新改进</a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><h2 id="793a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">感谢</h2><p id="fecb" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">如果没有<a class="ae lr" href="https://github.com/r7vme/learning-to-drive-in-a-day" rel="noopener ugc nofollow" target="_blank"> Roma Sokolkov </a>对 Wayve.ai 方法的重新实现、<a class="ae lr" href="https://github.com/tawnkramer" rel="noopener ugc nofollow" target="_blank"> Tawn Kramer </a>的驴车模拟器、<a class="ae lr" href="https://flyyufelix.github.io/2018/09/11/donkey-rl-simulation.html" rel="noopener ugc nofollow" target="_blank"> Felix Yu </a>的博客文章获得灵感、<a class="ae lr" href="https://github.com/hardmaru/WorldModelsExperiments" rel="noopener ugc nofollow" target="_blank"> David Ha </a>在 VAE 的实现、<a class="ae lr" href="https://github.com/hill-a/stable-baselines" rel="noopener ugc nofollow" target="_blank"> Stable-Baselines </a>及其<a class="ae lr" href="https://github.com/araffin/rl-baselines-zoo" rel="noopener ugc nofollow" target="_blank"> model zoo </a>用于 SAC 实现和训练脚本、用于遥控操作的<a class="ae lr" href="https://github.com/sergionr2/RacingRobot" rel="noopener ugc nofollow" target="_blank">赛车机器人项目</a>和<a class="ae lr" href="https://github.com/araffin/robotics-rl-srl" rel="noopener ugc nofollow" target="_blank">就不可能完成这项工作</a></p><p id="ba86" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我也想感谢罗马，塞巴斯蒂安，塔恩，佛罗伦萨，约翰内斯，乔纳斯，加布里埃尔，阿尔瓦罗，阿瑟和塞尔吉奥的反馈。</p><h2 id="d47b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">附录:学习状态表示</h2><p id="f09f" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated"><em class="nm">潜在空间维度和样本数的影响</em></p><p id="c032" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">VAE 的潜在空间维度只需要足够大，以便 VAE 能够重建输入图像的重要部分。例如，64D 和 512D VAE 之间的最终控制策略没有巨大差异。</p><p id="d900" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">重要的不是样本的数量，而是样本的多样性和代表性。如果你的训练图像没有覆盖所有的环境多样性，那么你需要更多的样本。</p><p id="52de" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们能从随机特征中学习控制策略吗？</p><p id="e971" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我试图在初始化后立即修正 VAE 的权重，然后学习关于那些随机特征的策略。然而，这并没有奏效。</p><p id="935a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nm">与像素学习的比较</em></p><p id="9528" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我没有时间(因为我的笔记本电脑没有 GPU)来比较直接从像素学习策略的方法。然而，如果有人能使用我的代码库做到这一点，我会对结果感兴趣。</p><p id="0ae2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nm">有效的最低限度政策是什么？</em></p><p id="b636" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">单层 mlp 有效。我也尝试了线性策略，但是，我没有成功地获得一个好的控制器。</p></div></div>    
</body>
</html>