<html>
<head>
<title>Principal Component Analysis from Statistical and Machine Learning Perspectives (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">统计和机器学习视角下的主成分分析(上)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-ceb42ed04d77?source=collection_archive---------4-----------------------#2019-01-05">https://towardsdatascience.com/principal-component-analysis-ceb42ed04d77?source=collection_archive---------4-----------------------#2019-01-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/3b690f983c472dadf5c1796751755311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*buvbTe-AwrPNVeGmslAK9Q.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="https://www.jpl.nasa.gov/images/technology/20150522/Iceberg20150522.jpg" rel="noopener ugc nofollow" target="_blank">https://www.jpl.nasa.gov/images/technology/20150522/Iceberg20150522.jpg</a></figcaption></figure><p id="ed6e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">复杂数据分析中的一个常见问题来自于大量的变量，这需要大量的内存和计算能力。这就是主成分分析(PCA)的用武之地。通过 <a class="ae kf" href="https://en.wikipedia.org/wiki/Feature_extraction" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu">特征提取</strong> </a>降低特征空间的维数是一种<strong class="ki iu">技术。例如，如果我们有 10 个变量，在特征提取中，我们通过组合旧的 10 个变量来创建新的自变量。通过创建新的变量，看起来好像引入了更多的维度，但我们只从新创建的变量中按重要性顺序选择了几个变量。那么这些被选择的变量的数量就比我们开始时的少，这就是我们如何降低维数的。</strong></p><p id="4cb0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看待 PCA 有多种方式。在本文中，我将从统计学的角度讨论 PCA，本文的第二部分是机器学习的角度(最小误差公式和最大化特征值)，链接如下:<a class="ae kf" rel="noopener" target="_blank" href="/principal-component-analysis-the-machine-learning-perspective-part-2-a2630fa3b89e">https://towardsdatascience . com/principal-component-analysis-the-machine-learning-perspective-part-2-a 2630 fa 3b 89e</a>。为了阅读第一部分和第二部分，熟悉以下一些内容会让本文更容易理解:<a class="ae kf" href="https://www.amazon.com/Elementary-Algebra-Classics-Advanced-Mathematics/dp/013468947X" rel="noopener ugc nofollow" target="_blank">线性代数(矩阵、特征向量/特征值、对角化、正交性)和统计学/机器学习(标准化、方差、协方差、独立性、线性回归</a>)。</p><h1 id="90ae" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">PCA 统计观点</h1><p id="89df" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">为了理解 PCA，我们可以从这本<a class="ae kf" href="https://www.amazon.com/Elementary-Algebra-Classics-Advanced-Mathematics/dp/013468947X" rel="noopener ugc nofollow" target="_blank">线性代数教科书</a>中的一个例子开始，其中作者从他的 14 名学生的荣誉微积分班上收集了一组测试数据。下面的四个变量是 ACT(全国测试的分数，范围为 1 到 36)、FinalExam(范围为 0 到 200 的期末考试分数)、QuizAvg(八个测验分数的平均值，每个分数的范围为 0 到 100)和 TestAvg(三个测试分数的平均值，每个分数的范围为 0 到 100)。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/bee8b98a230b196980c88ab550244b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*tXw7AlHItN7rcYjnyoptEg.png"/></div></div></figure><p id="6fce" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有所有这些考试分数，但有些分数是多余的，很难想象。但是，不能随便掉一些考试分数。这就是降维发挥作用的地方，在不损失太多信息的情况下，减少变量的数量。</p><p id="6cf4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的另一个问题是，很难解释这些考试分数，因为它们是基于不同的范围和尺度。变量，如上面的测试分数，在不同的尺度上或在一个共同的尺度上测量，有很大不同的范围，通常是标准化的，以引用相同的标准。</p><h1 id="d369" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">标准化</strong></h1><p id="1077" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><strong class="ki iu"> Z 评分</strong>是一种用于标准化/规范化数据的常用方法:从每个值中减去数据的平均值，然后除以标准偏差。</p><p id="ae23" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="mm">的意思是</em> </strong>被定义为</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mn"><img src="../Images/0628d9c8955413a3e6828a1fcf84df8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFGEW9oioJP8E_NZP_V9hw.png"/></div></div></figure><p id="ab30" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集 x 的<strong class="ki iu">方差</strong>为</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/26fd875204a31b95b77f5dc58ca7505c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZKgu6prggEB4csT64cnv8A.png"/></div></div></figure><p id="c240" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">计算数据方差的平均值和平方根后，即<strong class="ki iu">标准差</strong>:</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mp"><img src="../Images/117cff5308e5876a20ca58c9deab4c27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hIdenmfd8XTbND2jzunOdg.png"/></div></div></figure><p id="be4f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，通过减去平均值并除以标准差(即方差的平方根),最终可以对数据进行 z 评分/归一化。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/ae6b5a86d67f9907456fb4102ae4b7d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQNBvyzgWPVkZ1n1fVEyKQ.png"/></div></div></figure><h1 id="2603" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">协方差/相关性</h1><p id="8b68" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">为了了解所有这些变量之间的关系，使用了协方差。协方差告诉你两个变量之间的线性关系:两个变量是如何密切相关的。</p><p id="8b0d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">向量 x 和 y 的<strong class="ki iu"> <em class="mm">样本协方差</em> </strong>定义为</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mr"><img src="../Images/6e7584853ce32720178f5189dd97b93a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CKCZTL36_K4KFoXLWNuDgg.png"/></div></div></figure><p id="1f87" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，使用协方差的一个问题是它受测量单位的影响。例如，如果向量 x 以千克为单位，y 以英尺为单位，那么协方差现在是千克-英尺。为了使单位消失，使用了<strong class="ki iu"> <em class="mm">样本相关性</em> </strong>(向量)，它被定义为协方差除以标准偏差:</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ms"><img src="../Images/d4b288bee0e174a5239d8142754c3539.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXVhdu6Y9t63NcYhQVExNA.png"/></div></div></figure><p id="8ad8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">定义样本相关性(矩阵)的另一种方法是使用 z 得分数据，该数据已经除以标准差。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mt"><img src="../Images/47395749750f4cc4165bdadf817d0a37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-X3YM30FbwHNwtZhawj0A.png"/></div></div></figure><p id="335f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是数据集的相关矩阵:</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mu"><img src="../Images/c2a7575fb068b7dcfa70abfde499e1f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1kAvTsqlL97TVW8WbuoM7A.png"/></div></div></figure><p id="52a8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到第三次考试成绩(小考平均分)和第一次考试成绩(ACT)的相关性最高，为 0.82780343。第二高的相关值显示在第一和第四:ACT 和 Test 平均值之间。因此，我们证实了我们之前亲眼所见，这些测试分数是相互关联的，其中一些是多余的。</p><p id="1273" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们将开始我们的主成分分析。为了提出新的维度，我们将经历两个过程:</p><ol class=""><li id="69af" class="mv mw it ki b kj kk kn ko kr mx kv my kz mz ld na nb nc nd bi translated"><strong class="ki iu">数据集的转换:</strong>定义新的变量替换现有的变量</li><li id="6ad5" class="mv mw it ki b kj ne kn nf kr ng kv nh kz ni ld na nb nc nd bi translated"><strong class="ki iu">数据集的选择:</strong>衡量新变量代表原变量的程度。</li></ol><p id="d090" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是每一步的细节。</p><h1 id="56ad" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak"> <em class="nj">变换步骤</em> </strong></h1><p id="980d" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们将从上面计算的相关矩阵开始。重要的是要看到，这个矩阵必须是对称的，因为它是由 Z 和 Z 的转置相乘得到的。也就是说，这里将使用对称矩阵的一个定理。<a class="ae kf" href="https://www.amazon.com/Elementary-Algebra-Classics-Advanced-Mathematics/dp/013468947X" rel="noopener ugc nofollow" target="_blank">一个 n×n 矩阵 A 是对称的当且仅当存在一个由 A 的特征向量组成的正交基，在这种情况下，存在一个正交矩阵 P 和一个对角矩阵 D 使得 A = PDP_transpose </a>。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/01787ba4da10ae7397aae0af27fe297b.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*2tpr5vS3J52XNdkx_v8LVA.png"/></div></figure><p id="cd4f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以得到列为 A 的特征向量或特征值的相关矩阵和对角矩阵 D 的正交矩阵 P。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/56945b28a34ef36716ebf80fc7f72298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MHZ2xj5axjWWtXN3zLNOkA.png"/></div></div></figure><p id="4baf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这里开始，我们将通过执行与 P: <strong class="ki iu"> ZP </strong>的矩阵乘法来转换我们的原始 Z 得分数据</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/0a79d4ed0a14c4b5e782a8eaad41ef8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rwLvzs1MtctR4-S5PDoexA.png"/></div></div></figure><p id="413e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这组新数据的形状与原始数据集的形状没有任何不同。现在，我们已经成功地将原始数据转换为新数据，在下一步中，我们将降低原始数据集的维度。</p><h1 id="abb1" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak"> <em class="nj">选择步骤</em> </strong></h1><p id="494d" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">现在，我们将从<strong class="ki iu"> ZP </strong>中选择向量，以实际降低维度，但是我们如何知道哪些要保留，哪些要丢弃呢？将基于特征值做出决定。我们将以一种可读的格式重写 ZP，其中 ZP 的每个列向量表示为 y_i</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/083e39895823c3eb6f17959e9c7fc9b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*unlHklRfCIM0av05mdixVQ.png"/></div></div></figure><p id="7c42" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> y1 </strong>为<strong class="ki iu">第一主成分</strong>，定义为新的向量，其系数为最大特征值为 2.978867 的相关矩阵的特征向量。那么<strong class="ki iu">第二主成分</strong>就是 y2，因为它的系数对应的是第二大特征值 0.762946。</p><p id="f904" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，最后两个特征值无关紧要，因为与前两个相比，它们不够大。因此，我们将选择特征向量为系数的 ZP 向量对应前两个最大的特征值，即<strong class="ki iu"> y1 </strong>和<strong class="ki iu"> y2 </strong>。这两个向量是我们的新向量，我们将把它们作为降维的结果。</p><p id="2be9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在问题来了，为什么我们要做 ZP 来选择一组新的向量？为什么 ZP 如此重要？这是因为 ZP 的协方差是对角矩阵。</p><p id="a17f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是为什么的证据(<em class="mm">这是前半段</em>最重要的部分):</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/97f6d1a9a6c878fcdbbe2560936c9ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLUf8-XFvY-qETJrSIRjfA.png"/></div></div></figure><h1 id="affb" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">为什么我们要把 Z 乘以 P？</h1><p id="9e6b" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这解释了我们将归一化数据 Z 乘以 p 的原因。ZP 的协方差是对角矩阵的事实表明，除了与自身相关的变量之外，变量之间没有线性关系。换句话说，我们创建的新变量之间没有关系，新变量彼此独立。<strong class="ki iu">因此，我们已经成功地提出了一组新的数据集(转换后的数据集),它们之间没有任何相关性，我们可以根据方差选择变量，方差由特征值表示。</strong></p><p id="be9a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们更多地讨论特征值/方差。这里重要的是原始数据集的总方差与转换后数据集的总方差相同。也就是说，<em class="mm">对角矩阵 D 的元素之和的元素之和就是 Z 得分的方差之和</em>。当 ZP 的协方差矩阵为 D 时，ZP 的每个向量代表变换数据的方差，并且通过选择对应于最高特征值的向量，方差被最大化。因此，<strong class="ki iu">通过选择对应于最高特征值的向量，我们选择了新的变量，其具有被原始数据集的总方差分割的变换数据集的方差的高分数</strong>。</p><h1 id="aa19" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">从统计角度看主成分分析的结论</h1><p id="5719" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">当用较少的新变量替换原始变量时，试图捕捉尽可能多的方差是统计学中的常见做法"<a class="ae kf" href="https://www.amazon.com/Elementary-Algebra-Classics-Advanced-Mathematics/dp/013468947X" rel="noopener ugc nofollow" target="_blank">，以说明原始数据集中方差的高百分比。</a>“这在直觉上是有意义的，因为当我们考虑降低数据集的维度时，我们希望丢弃相似的特征，但只保留最大不相似的特征。在这种情况下，我们可以看到，一些测试分数彼此高度相关，因此一些测试分数是冗余的。但是我们想从数学上理解为什么保持尽可能多的方差是有益的。这个问题的答案在机器学习视角的 PCA 中更容易看到，这将是本文的第 2 部分。</p><p id="87b9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">参考文献:</strong> 1。<a class="ae kf" href="https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf" rel="noopener ugc nofollow" target="_blank">https://www . stat . CMU . edu/~ cshalizi/uADA/12/lections/ch18 . pdf</a>2。<a class="ae kf" rel="noopener" target="_blank" href="/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c">https://towards data science . com/a-一站式主成分分析-5582fb7e0a9c </a>，3 .<a class="ae kf" href="https://www.amazon.com/Elementary-Algebra-Classics-Advanced-Mathematics/dp/013468947X" rel="noopener ugc nofollow" target="_blank">https://www . Amazon . com/Elementary-Algebra-Classics-Advanced-Mathematics/DP/013468947 x</a>4 .<a class="ae kf" href="http://www.cs.columbia.edu/~verma/teaching.html" rel="noopener ugc nofollow" target="_blank">http://www.cs.columbia.edu/~verma/teaching.html</a></p></div></div>    
</body>
</html>