# 如何初始化神经网络

> 原文：<https://towardsdatascience.com/how-to-initialize-a-neural-network-27564cfb5ffc?source=collection_archive---------4----------------------->

## 权重初始化技术的实际应用

![](img/58c6e344195af062a2bef95a0914e3c7.png)

Image source: [https://pixabay.com/photos/sport-tracks-running-run-sprint-1201014/](https://pixabay.com/photos/sport-tracks-running-run-sprint-1201014/)

训练神经网络远不是一项简单的任务，因为最轻微的错误都会在没有任何警告的情况下导致非最佳结果。训练取决于许多因素和参数，因此需要[深思熟虑的方法](http://karpathy.github.io/2019/04/25/recipe/)。

众所周知，训练的开始(即最初的几次迭代)[非常重要](https://arxiv.org/pdf/1901.09321.pdf)。如果操作不当，结果会很糟糕——有时，网络甚至什么也学不到！因此，初始化神经网络权重的方式是良好训练的关键因素之一。

本文的目标是解释为什么初始化会产生影响，并给出有效实现它的不同方法。我们将通过实际例子来检验我们的方法。
代码使用了 [fastai 库](https://github.com/fastai)(基于 pytorch)。所有的实验笔记本都可以在这个 github 库中找到。

# 为什么初始化很重要？

神经网络训练基本上包括重复以下两个步骤:

*   前进的一步在于权重和输入/激活之间的大量矩阵乘法(我们称*激活为*将成为下一层输入的层的输出，即隐藏激活)
*   向后的步骤，包括更新网络的权重，以便最小化损失函数(使用参数的梯度)

在向前的步骤中，激活(然后是梯度)可以很快变得很大或很小——这是因为我们重复了许多矩阵乘法。更具体地说，我们可能会得到:

*   非常大的激活，因此大梯度射向无限
*   非常小的激活，因此梯度极小，由于数值精度，梯度可能被抵消为零

这两种影响对训练都是致命的。下面是在第一次向前传递时，使用随机初始化的权重进行爆炸的示例。

![](img/7c814bd4708193e2410e550da9bdc473.png)

在这个特殊的例子中，平均值和标准偏差在第 10 层已经很大了！

让事情变得更加棘手的是，在实践中，即使在避免爆炸或消失效果的同时，经过长时间的训练，你仍然可以获得非最佳的结果。这在下面一个简单的 convnet 上有所说明(实验将在文章的第二部分详述):

![](img/caaa9626e25c9595960df1504ec0db00.png)

请注意，默认的 pytorch 方法并不是最好的方法，随机初始化并不能学到很多东西(另外:这只是一个 5 层网络，这意味着更深层次的网络不会学到任何东西)。

# 如何初始化您的网络

回想一下，良好初始化的目标是:

*   获取随机权重
*   在第一个正向过程中，将激活保持在*良好范围*内(反向过程中的梯度也是如此)

什么是*练好的靶场*？定量地说，这意味着使矩阵与输入向量相乘的输出产生平均值接近 0 且标准偏差接近 1 的输出向量(即激活)。那么每一层将在所有层中传播这些统计数据。
即使在一个深度网络中，你也会在第一次迭代中获得稳定的统计数据。

我们现在讨论两种实现方法。

# 数学方法:明凯初始化

让我们想象一下这个问题。如果初始化的权重在训练开始时太大，那么每个矩阵乘法将指数地增加激活，导致我们所说的*梯度爆炸*。
相反，如果权重太小，那么每次矩阵乘法将减少激活，直到它们完全消失。

因此，这里的关键是缩放权重矩阵，以获得均值约为 0、标准差为 1 的矩阵乘法输出。

但是如何定义权重的范围呢？因为每个权重(以及输入)都是独立的，并且按照正态分布分布，所以我们可以通过计算得到帮助。

两篇著名论文基于这一思想提出了一个很好的初始化方案:

*   “Xavier 初始化”，在 2010 年的论文[中提出理解训练深度前馈神经网络的难度](http://proceedings.mlr.press/v9/glorot10a.html)
*   “明凯初始化”，2015 年在论文[中提出，深入研究整流器:在 ImageNet 分类上超越人类水平的性能](https://arxiv.org/abs/1502.01852)

实际上，这两种方案非常相似:“主要”区别在于明凯初始化考虑了每次矩阵乘法之后的 ReLU 激活函数。

现在，大多数神经网络使用 ReLU(或类似 leaky ReLU 的类似函数)。这里，我们只关注明凯初始化。

简化公式(对于标准 ReLU)是通过以下方式对随机权重(取自标准分布)进行缩放:

![](img/77447072cc9e7b03c76d0ed617aa83c3.png)

例如，如果我们有一个大小为 512 的输入:

![](img/55e69c88b476c447f5f9e829715dd4ba.png)

此外，所有偏置参数应初始化为零。

注意，对于 Leaky ReLU，公式有一个额外的部分，我们在这里没有考虑(我们让读者参考原始论文)。

让我们看看这个方法在前面的例子中是如何工作的:

![](img/c800c9691bddc0ab3e0df9c69f21c6d0.png)

请注意，现在我们在初始化后获得了平均值为 0.64、标准偏差为 0.87 的激活。显然，这并不完美(怎么可能是随机数呢？)，但比正态分布的随机权重好得多。

50 层之后，我们得到的平均值为 0.27，标准差为 0.464，因此不再有爆炸或消失的效果。

## 可选:明凯公式的快速解释

明凯的论文中提供了导致 math.sqrt 的神奇缩放数(2 /输入向量的大小)的数学推导。此外，我们在下面提供了一些有用的代码，读者可以完全跳过这些代码进入下一节。注意，代码要求理解如何做矩阵乘法以及什么是方差/标准差。

为了理解公式，我们可以考虑矩阵乘法结果的方差是什么。在本例中，我们将一个 512 矢量乘以一个 512x512 矩阵，得到一个 512 矢量的输出。

![](img/fe7e06834b135abf3f6d2ed5426cfd8a.png)

在我们的例子中，矩阵乘法输出的方差大约等于输入向量的大小。根据定义，标准差是它的平方根。

这就是为什么将权重矩阵除以输入向量大小的平方根(本例中为 512)会得到标准差为 1 的结果。

但是“2”的分子从何而来？这只是考虑到了 ReLU 层。

如你所知，ReLU 将负数设置为 0(只有 max(0，input))。所以，因为我们有以平均值 0 为中心的数字，它基本上消除了一半的方差。这就是为什么我们加一个分子 2。

## 明凯 init 的缺点是

明凯 init 在实践中非常有效，那么为什么要考虑另一种方法呢？事实证明，Kaming init 有一些缺点:

*   一层之后的平均值不是 0，而是大约 0.5。这是因为 ReLU 激活函数，它删除了所有的负数，有效地改变了它的平均值
*   明凯初始化只适用于 ReLU 激活函数。因此，如果你有一个更复杂的架构(不仅仅是 matmult → ReLU 层)，那么这将不能在所有层上保持大约 1 的标准偏差
*   一层之后的标准偏差不是 1，而是接近 1。在深度网络中，这不足以使标准差始终接近 1。

# 算法途径:LSUV

那么，在不为更复杂的架构手动定制明凯初始化的情况下，我们能做些什么来获得一个好的初始化方案呢？

2015 年的论文[展示了一种有趣的方法。它被称为 LSUV(层序单位方差)。](https://arxiv.org/abs/1511.06422)

解决方案在于使用简单的算法:首先，用正交初始化来初始化所有层。然后，取一个小批量输入，并为每一层计算其输出的标准偏差。将每层除以产生的偏差，然后将其重置为 1。下面是论文中解释的算法:

![](img/5deeb92b639e2c5b2f155f72ae453461.png)

经过一些测试，我发现正交初始化比在 ReLU 之前进行明凯初始化给出了相似(有时更差)的结果。

杰瑞米·霍华德在 [fastai MOOC](https://course.fast.ai/) 中展示了另一种实现方式，它对权重进行了更新，以保持均值在 0 左右。在我的实验中，我还发现将平均值保持在 0 左右会得到更好的结果。

现在让我们比较这两种方法的结果。

# 初始化方案的性能

我们将在两个架构上检查不同初始化方案的性能:一个具有 5 层的“简单”convnet 和一个更复杂的类似 resnet 的架构。
任务是在[Imagenet 数据集](https://github.com/fastai/imagenette)(Imagenet 数据集的 10 个类的子集)上进行图像分类。

## 简单的建筑

这个实验可以在这个笔记本里找到[。请注意，由于随机性，每次的结果可能会略有不同(但不会改变顺序和整体情况)。](https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Simple%20model.ipynb)

它使用一个简单的模型，定义为:

![](img/b2bf5e318b67b39a081d5f43f852579a.png)

> #ConvLayer 是一个 Conv2D 层，后跟一个 ReLU
> nn。Sequential(ConvLayer(3，32，ks=5)，ConvLayer(32，64)，ConvLayer(64，128)，ConvLayer(128，128)，nn。AdaptiveAvgPool2d(1)，Flatten()，nn。线性(128，data.c))

下面是 3 个初始化方案的比较:Pytorch 默认的初始化(它是一个明凯初始化，但有一些特定的参数)，明凯初始化和 LSUV 初始化。

![](img/c05c83b846e6c8adefa8ee1ea37a30a5.png)

请注意，随机初始化的性能太差了，我们从下面的结果中删除了它。

【init 后的激活统计
第一个问题是在第一次迭代向前传递后的激活统计是什么？我们越接近平均值 0 和标准差 1，就越好。

该图显示了初始化后(训练前)每一层的激活状态。

![](img/7c328734a07969c121ffe58e6b753ebb.png)

对于标准差(右图)，LSUV 和明凯初始值都接近 1(LSUV 更接近)。但是对于 pytorch 默认值，标准差要低得多。

但是对于平均值，明凯初始结果更差。这是可以理解的，因为明凯 init 没有考虑平均的 ReLU 效应。所以平均值在 0.5 左右而不是 0。

## 复杂架构(resnet50)

现在让我们检查一下在一个更复杂的架构上是否得到了类似的结果。

架构是 xresnet-50，[在 fastai 库](https://github.com/fastai/fastai/blob/master/fastai/vision/models/xresnet.py)中实现。它比我们之前的简单模型多了 10 倍的层数。

我们将分两步进行检查:

*   没有规范化层:batchnorm 将被禁用。因为这一层将逐批修改 stats mini，所以它应该会降低初始化的影响
*   使用规范化图层:batchnorm 将被启用

**第一步:没有 batchnorm** 这个实验可以在这个笔记本里找到[。](https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Complex%20model%20-%20no%20batchnorm.ipynb)

如果没有 batchnorm，10 个时期的结果是:

![](img/0b142fa4625b3499a184df8d3eae2784.png)

该图显示 LSUV 的精度(y 轴)为 67%,明凯初始化为 57 %, py torch 默认为 48%。差别是巨大的！

让我们在训练前检查激活统计:

![](img/8c749082317ffb824abc25af27063c89.png)

让我们缩放以获得更好的比例:

![](img/feb7e0b1276500692fa10bc1701b9080.png)

我们看到一些层的 stats 为 0:这是 xresnet50 的设计，与 init 方案无关。这是论文[中的一个技巧，用于使用卷积神经网络](https://arxiv.org/abs/1812.01187)进行图像分类(在 fastai 库中实现)。

我们认为:

*   Pytorch 默认初始化:标准差和均值接近于 0。这是不好的，显示了一个正在消失的问题
*   明凯·尼特:我们得到了一个很大的均值和标准差
*   LSUV init:我们得到了很好的统计数据，虽然不完美，但比其他方案要好

我们看到，即使在 10 个完整的时期之后，这个例子的最佳初始化方案对于完整的训练给出了好得多的结果。这显示了在第一次迭代中保持各层良好统计的重要性。

## 步骤 2:使用 batchnorm 图层

这个实验可以在这个笔记本里找到[。](https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Complex%20model%20-%20with%20batchnorm.ipynb)

因为 batchnorm 正在规范化一个层的输出，我们应该期望 init 方案的影响较小。

![](img/0561619b965e874bdee55e535354244d.png)

结果显示，所有 init 方案的准确率接近 88%。请注意，在每次运行时，最佳初始化方案可能会根据随机生成器而变化。

它表明 batchnom 层使网络对初始化方案不太敏感。

培训前的激活统计如下:

![](img/676bc76fbbb6f8806a2aa3c9797e1cb6.png)

像以前一样，最好的似乎是 LSUV init(只有它能保持平均值在 0 左右，标准差接近 1)。

但是结果显示这对准确性没有影响，至少对于这个体系结构和这个数据集是这样。不过它证实了一件事:batchnorm 使网络对初始化的质量不太敏感。

# 结论

从这篇文章中要记住什么？

*   **第一次迭代非常重要**,会对整个培训产生持久的影响。
*   一个好的初始化方案应该在网络的所有层(对于第一次迭代)保持关于激活的输入统计(平均值为 0，标准偏差为 1)。
*   批处理层降低了神经网络对初始化方案的敏感性。
*   **使用明凯 init + LSUV 似乎是一个不错的方法**，尤其是当网络缺少标准化层的时候。
*   其他类型的架构在初始化方面可能有不同的行为。