<html>
<head>
<title>Feature Selection Using Regularisation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用正则化的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-using-regularisation-a3678b71e499?source=collection_archive---------4-----------------------#2019-02-04">https://towardsdatascience.com/feature-selection-using-regularisation-a3678b71e499?source=collection_archive---------4-----------------------#2019-02-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1c63" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">拉索来救援了</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/79db6c3b19fdcee1d4ce371c0775e210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XTOBgv_H4C1ufmHU"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="20cb" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="da6f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">正则化包括向机器学习模型的不同参数添加惩罚，以减少模型的自由度，换句话说，避免过度拟合。在线性模型正则化中，惩罚应用于乘以每个预测值的系数。从不同类型的正则化，套索或 L1 的属性，能够缩小一些系数为零。因此，可以从模型中删除该特征。</p><p id="5f14" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在本帖中，我将演示如何使用套索正则化分类问题来选择要素。对于分类，我将使用来自 Kaggle 的<a class="ae kv" href="https://www.kaggle.com/c/bnp-paribas-cardif-claims-management" rel="noopener ugc nofollow" target="_blank"> Paribas 索赔数据集</a>。</p><ol class=""><li id="6460" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated"><strong class="lq ir"> <em class="my">导入重要库</em> </strong></li></ol><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="9035" class="ne kx iq na b gy nf ng l nh ni">import pandas as pd<br/>import numpy as np</span><span id="0ae8" class="ne kx iq na b gy nj ng l nh ni">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>%matplotlib inline</span><span id="26b5" class="ne kx iq na b gy nj ng l nh ni">from sklearn.model_selection import train_test_split</span><span id="9206" class="ne kx iq na b gy nj ng l nh ni">from sklearn.linear_model import Lasso, LogisticRegression<br/>from sklearn.feature_selection import SelectFromModel<br/>from sklearn.preprocessing import StandardScaler</span></pre><p id="1439" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">2<em class="my">2。加载数据集</em> </strong></p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="952b" class="ne kx iq na b gy nf ng l nh ni">data = pd.read_csv(‘paribas.csv’, nrows=50000)<br/>data.shape</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/baf76f35d76adf76af0d592e81511a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*kELd3BvdvJG_9yBJkmfXNA.png"/></div></figure><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="f0d7" class="ne kx iq na b gy nf ng l nh ni">data.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/97396c94f2a8fa14ef936844d80aeeaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ruq8Y-khUQQ9sep2wh5dsw.png"/></div></div></figure><p id="f5cb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="my"> 3。选择数值列</em>和</strong></p><p id="c4fc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在实践中，特征选择应该在数据预处理之后进行，所以理想情况下，所有分类变量都被编码成数字，然后我们可以评估它们对目标的确定性，这里为了简单起见，我将只使用数字变量来选择数字列:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="962b" class="ne kx iq na b gy nf ng l nh ni">numerics = ['int16','int32','int64','float16','float32','float64']<br/>numerical_vars = list(data.select_dtypes(include=numerics).columns)<br/>data = data[numerical_vars]<br/>data.shape</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/2f50c9d01ed11cf898cdce93962dd064.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*pO8uNCZGBhuCZCVrUAqz2w.png"/></div></figure><p id="c4f5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="my"> 4。将数据分成训练集和测试集</em> </strong></p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="e034" class="ne kx iq na b gy nf ng l nh ni">X_train, X_test, y_train, y_test = train_test_split(<br/>    data.drop(labels=['target', 'ID'], axis=1),<br/>    data['target'],<br/>    test_size=0.3,<br/>    random_state=0)</span><span id="9e98" class="ne kx iq na b gy nj ng l nh ni">X_train.shape, X_test.shape</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/848a3251ebd12e4a192e7d74fed762b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*FZnHeqskHqEta_zyAEZ3ig.png"/></div></figure><p id="410c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="my"> 5 </em> </strong>。<strong class="lq ir"> <em class="my">缩放数据，因为线性模型受益于特征缩放</em> </strong></p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="b503" class="ne kx iq na b gy nf ng l nh ni">scaler = StandardScaler()<br/>scaler.fit(X_train.fillna(0))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/246de4156f705c644489500f316059a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*u55rfAb3O_gttR1I_09tgQ.png"/></div></figure><p id="2972" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="my"> 6。使用</em> </strong> <code class="fe np nq nr na b"><strong class="lq ir"><em class="my">SelectFromModel</em></strong></code>使用套索正则化选择特征</p><p id="2535" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这里，我将在一行代码中完成模型拟合和特征选择。首先，我指定了逻辑回归模型，并确保选择了套索(L1)惩罚。然后我使用来自<code class="fe np nq nr na b">sklearn</code>的<code class="fe np nq nr na b">selectFromModel</code>对象，它将在理论上选择系数非零的特征。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="9f58" class="ne kx iq na b gy nf ng l nh ni">sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1'))<br/>sel_.fit(scaler.transform(X_train.fillna(0)), y_train)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/85ceff70a6f0eaeddf6c6a0a60d7e564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dGOxdHlDYV38zd_aciBzPg.png"/></div></div></figure><p id="96f1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="my"> 7。可视化套索正则化保留的特征</em> </strong></p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="e39d" class="ne kx iq na b gy nf ng l nh ni">sel_.get_support()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/51f3292a23b1276757ecf5c7a0ef4c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*kK6iu8H9zV9NFyr9-q6Rqw.png"/></div></figure><p id="e027" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在上面的输出中，输出标签是索引方式的。因此<code class="fe np nq nr na b">True</code>是针对 lasso 认为重要的特征(非零特征)，而<code class="fe np nq nr na b">False</code>是针对权重收缩为零且 Lasso 认为不重要的特征。</p><p id="c6b1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="my"> 8。列出具有所选功能的。</em> </strong></p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="f421" class="ne kx iq na b gy nf ng l nh ni">selected_feat = X_train.columns[(sel_.get_support())]</span><span id="0fc3" class="ne kx iq na b gy nj ng l nh ni">print('total features: {}'.format((X_train.shape[1])))<br/>print('selected features: {}'.format(len(selected_feat)))<br/>print('features with coefficients shrank to zero: {}'.format(<br/>      np.sum(sel_.estimator_.coef_ == 0)))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/13137a4f0eec87cf85e991d1208aabf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*P3XAhHzWkXV_ixM4N63jPA.png"/></div></figure><p id="7437" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="my">系数收缩为零的特征数:</em> </strong></p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="d528" class="ne kx iq na b gy nf ng l nh ni">np.sum(sel_.estimator_.coef_ == 0)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/2c3c48088d7c91c95fc2517d041f3440.png" data-original-src="https://miro.medium.com/v2/resize:fit:110/format:webp/1*HU788Cr3TeOvWcMFsdRa_Q.png"/></div></figure><p id="f2fb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">9<em class="my">。识别被移除的特征</em>和</strong></p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="fce5" class="ne kx iq na b gy nf ng l nh ni">removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]</span><span id="4b92" class="ne kx iq na b gy nj ng l nh ni">removed_feats</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/2db7bf8eef888e4aea8acd97851e421d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*bgVTYnLhA7bZTiXuT29eQQ.png"/></div></figure><p id="df34" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="my"> 10。从训练测试集中移除特征</em> </strong></p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="51de" class="ne kx iq na b gy nf ng l nh ni">X_train_selected = sel_.transform(X_train.fillna(0))<br/>X_test_selected = sel_.transform(X_test.fillna(0))</span><span id="22ef" class="ne kx iq na b gy nj ng l nh ni">X_train_selected.shape, X_test_selected.shape</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f2cbcf7f30b506572de96557971d4b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*0X0oDnkPDvHymx-Q8si7VA.png"/></div></figure><h2 id="168b" class="ne kx iq bd ky ny nz dn lc oa ob dp lg lx oc od li mb oe of lk mf og oh lm oi bi translated">注意:</h2><blockquote class="oj"><p id="bbc2" class="ok ol iq bd om on oo op oq or os mj dk translated">L2 正则化不会将系数缩小到零</p></blockquote><pre class="ot ou ov ow ox mz na nb nc aw nd bi"><span id="dc3a" class="ne kx iq na b gy nf ng l nh ni"># Separating the data into train and test set <br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    data.drop(labels=['target', 'ID'], axis=1),<br/>    data['target'],<br/>    test_size=0.3,<br/>    random_state=0)</span><span id="17ab" class="ne kx iq na b gy nj ng l nh ni">X_train.shape, X_test.shape</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/81078dd2d4c77ecfc9c3e5ea77358e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*rRz6ZJJqlKWRjTKUZPM3aA.png"/></div></figure><p id="7b1a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了比较，我将使用岭正则化拟合逻辑回归，并评估系数:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="f4e1" class="ne kx iq na b gy nf ng l nh ni">l1_logit = LogisticRegression(C=1, penalty='l2')<br/>l1_logit.fit(scaler.transform(X_train.fillna(0)), y_train)</span></pre><p id="de53" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，让我们计算零值系数的数量:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="076d" class="ne kx iq na b gy nf ng l nh ni">np.sum(l1_logit.coef_ == 0)</span></pre><p id="ab61" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以，现在零值系数的数量是零。因此，现在很清楚，脊正则化(L2 正则化)不会将系数缩小到零。</p><h2 id="bbb3" class="ne kx iq bd ky ny nz dn lc oa ob dp lg lx oc od li mb oe of lk mf og oh lm oi bi translated"><strong class="ak">结论:</strong></h2><p id="50e6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">正如我们所见，我们用于套索正则化的逻辑回归从数据集中移除了不重要的要素。请记住，增加惩罚<code class="fe np nq nr na b">c</code>会增加移除的特征数量。因此，我们需要保持警惕，不要将惩罚设置得太高，以至于删除甚至重要的功能，或者设置得太低，以至于不删除不重要的功能。</p><p id="aa98" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">对于使用随机森林的特征选择:</strong></p><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/feature-selection-using-random-forest-26d7b747597f"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd ir gy z fp ph fr fs pi fu fw ip bi translated">使用随机森林的特征选择</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">随机森林是最流行的机器学习算法之一。他们如此成功是因为他们提供了…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq kp pc"/></div></div></a></div></div></div>    
</body>
</html>