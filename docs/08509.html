<html>
<head>
<title>Illustrated: Self-Attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">插图:自我关注</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a?source=collection_archive---------0-----------------------#2019-11-18">https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a?source=collection_archive---------0-----------------------#2019-11-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/505538e8eb484732176378ddee2c773f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*_92bnsMJy8Bl539G4v93yg.gif"/></div></div></figure><h2 id="f9fa" class="jc jd je bd b dl jf jg jh ji jj jk dk jl translated" aria-label="kicker paragraph">内部人工智能</h2><div class=""/><div class=""><h2 id="e0a7" class="pw-subtitle-paragraph kk jn je bd b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dk translated">带插图和代码的自我关注循序渐进指南</h2></div><p id="b308" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">插图最好在桌面上看。一个 Colab 版本可以在这里找到<a class="ae lz" href="https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF" rel="noopener ugc nofollow" target="_blank"><em class="ly"/></a><em class="ly">(感谢</em> <a class="ma mb ep" href="https://medium.com/u/3f2bb9b4510b?source=post_page-----2d627e33b20a--------------------------------" rel="noopener" target="_blank"> <em class="ly">罗梅洛</em> </a> <em class="ly">！).</em></p><p id="0dec" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><em class="ly">变更日志:<br/>2022 年 12 月 30 日—使用 Medium 的新代码块突出显示语法<br/>2022 年 1 月 12 日—提高清晰度<br/>2022 年 1 月 5 日—修复错别字并提高清晰度</em></p><p id="318a" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi mc translated">伯特、罗伯塔、艾伯特、斯潘伯特、迪尔伯特、塞姆斯伯特、森伯特、西伯特、比奥伯特、莫比尔伯特、蒂尼伯特和卡门伯特有什么共同点？我不是在寻找答案“伯特”🤭。</p><p id="dfee" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">回答:自我关注🤗</strong>。我们不仅在谈论名为“BERT”的架构，更准确地说，是基于<strong class="le jo">变压器的</strong>架构。基于转换器的架构主要用于建模语言理解任务，避免了神经网络中的递归，而是完全依靠<strong class="le jo">自我关注</strong>机制来绘制输入和输出之间的全局依赖关系。但是这背后的数学原理是什么呢？</p><p id="b9f7" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这就是我们今天要发现的。这篇文章的主要内容是引导你完成自我关注模块中的数学运算。到本文结束时，您应该能够从头开始编写或编码自我关注模块。</p><p id="d989" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这篇文章的目的不是提供自我关注模块中不同的数字表示和数学运算背后的直觉和解释。它也没有试图证明变形金刚中自我关注的原因和方式(我相信已经有很多了)。注意，本文中也没有详细说明注意力和自我注意力之间的区别。</p><h2 id="e39c" class="ml mm je bd mn mo mp dn mq mr ms dp mt ll mu mv mw lp mx my mz lt na nb nc jk bi translated">内容</h2><ol class=""><li id="6345" class="nd ne je le b lf nf li ng ll nh lp ni lt nj lx nk nl nm nn bi translated"><a class="ae lz" href="#570c" rel="noopener ugc nofollow">插图</a></li><li id="88a3" class="nd ne je le b lf no li np ll nq lp nr lt ns lx nk nl nm nn bi translated"><a class="ae lz" href="#8481" rel="noopener ugc nofollow">代码</a></li><li id="df33" class="nd ne je le b lf no li np ll nq lp nr lt ns lx nk nl nm nn bi translated"><a class="ae lz" href="#faae" rel="noopener ugc nofollow">延伸至变压器</a></li></ol><p id="66c1" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">现在让我们开始吧！</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h2 id="570c" class="ml mm je bd mn mo mp dn mq mr ms dp mt ll mu mv mw lp mx my mz lt na nb nc jk bi translated">0.什么是自我关注？</h2><p id="59b8" class="pw-post-body-paragraph lc ld je le b lf nf ko lh li ng kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">如果你觉得自我关注也差不多，那么答案是肯定的！它们从根本上共享相同的概念和许多共同的数学运算。</p><p id="66af" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">自关注模块接收<em class="ly"> n </em>个输入，并返回<em class="ly"> n </em>个输出。在这个模块中发生了什么？通俗地说，自我注意机制就是让输入相互作用(“自我”)，找出自己应该更注意谁(“注意”)。输出是这些交互和注意力分数的集合。</p><h2 id="3140" class="ml mm je bd mn mo mp dn mq mr ms dp mt ll mu mv mw lp mx my mz lt na nb nc jk bi translated">1.插图</h2><p id="46c5" class="pw-post-body-paragraph lc ld je le b lf nf ko lh li ng kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">插图分为以下几个步骤:</p><ol class=""><li id="aacd" class="nd ne je le b lf lg li lj ll od lp oe lt of lx nk nl nm nn bi translated">准备输入</li><li id="aaf2" class="nd ne je le b lf no li np ll nq lp nr lt ns lx nk nl nm nn bi translated">初始化重量</li><li id="c673" class="nd ne je le b lf no li np ll nq lp nr lt ns lx nk nl nm nn bi translated">导出<strong class="le jo">键</strong>、<strong class="le jo">查询</strong>和<strong class="le jo">值</strong></li><li id="2f82" class="nd ne je le b lf no li np ll nq lp nr lt ns lx nk nl nm nn bi translated">计算输入 1 的注意力分数</li><li id="5319" class="nd ne je le b lf no li np ll nq lp nr lt ns lx nk nl nm nn bi translated">计算 softmax</li><li id="342e" class="nd ne je le b lf no li np ll nq lp nr lt ns lx nk nl nm nn bi translated">将分数乘以<strong class="le jo">值</strong></li><li id="b758" class="nd ne je le b lf no li np ll nq lp nr lt ns lx nk nl nm nn bi translated">对<strong class="le jo">加权</strong>和<strong class="le jo">值</strong>求和，得到输出 1</li><li id="1654" class="nd ne je le b lf no li np ll nq lp nr lt ns lx nk nl nm nn bi translated">对输入 2 和输入 3 重复步骤 4-7</li></ol><blockquote class="og oh oi"><p id="400c" class="lc ld ly le b lf lg ko lh li lj kr lk oj lm ln lo ok lq lr ls ol lu lv lw lx im bi translated"><strong class="le jo"> <em class="je">注</em> </strong> <em class="je"> <br/>实际上，数学运算是矢量化的，即所有的输入一起进行数学运算。我们将在后面的代码部分看到这一点。</em></p></blockquote><p id="a0b2" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">步骤 1:准备输入</strong></p><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/60b30f333ad9f23985d6fa5b1ba60b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hmvdDXrxhJsGhOQClQdkBA.png"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">Fig. 1.1: Prepare inputs</figcaption></figure><p id="3e28" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">对于本教程，我们从 3 个输入开始，每个输入的维度为 4。</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="4f7a" class="oz mm je ov b be pa pb l pc pd">Input 1: [1, 0, 1, 0] <br/>Input 2: [0, 2, 0, 2]<br/>Input 3: [1, 1, 1, 1]</span></pre><p id="d825" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">步骤 2:初始化砝码</strong></p><p id="f1bf" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">每个输入必须有三个表示(见下图)。这些表示被称为<strong class="le jo">键</strong>(橙色)<strong class="le jo">查询</strong>(红色)，以及<strong class="le jo">值</strong>(紫色)。对于这个例子，让我们假设我们希望这些表示具有 3 维。因为每个输入的维数为 4，所以每组权重的形状必须为 4×3。</p><blockquote class="og oh oi"><p id="8594" class="lc ld ly le b lf lg ko lh li lj kr lk oj lm ln lo ok lq lr ls ol lu lv lw lx im bi translated"><strong class="le jo"> <em class="je">注</em> </strong> <em class="je"> <br/>我们后面会看到</em> <strong class="le jo"> <em class="je">值</em> </strong> <em class="je">的维度也是输出维度。</em></p></blockquote><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/cb15e2b40359cd27848b9f8fa658edc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*VPvXYMGjv0kRuoYqgFvCag.gif"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">Fig. 1.2: Deriving <strong class="bd pf">key</strong>, <strong class="bd pf">query</strong> and <strong class="bd pf">value</strong> representations from each input</figcaption></figure><p id="654a" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">为了获得这些表示，每个输入(绿色)都要乘以一组<strong class="le jo">键</strong>的权重、一组<strong class="le jo">查询</strong>的权重(我知道这不是正确的拼写)，以及一组<strong class="le jo">值</strong>的权重。在我们的例子中，我们如下初始化三组权重。</p><p id="ac7b" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">键</strong>的重量:</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="bebe" class="oz mm je ov b be pa pb l pg pd">[[0, 0, 1],<br/> [1, 1, 0],<br/> [0, 1, 0],<br/> [1, 1, 0]]</span></pre><p id="835f" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">查询</strong>的权重:</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="0c2e" class="oz mm je ov b be pa pb l pg pd">[[1, 0, 1],<br/> [1, 0, 0],<br/> [0, 0, 1],<br/> [0, 1, 1]]</span></pre><p id="c5b8" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">值</strong>的权重:</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="f1bb" class="oz mm je ov b be pa pb l pg pd">[[0, 2, 0],<br/> [0, 3, 0],<br/> [1, 0, 3],<br/> [1, 1, 0]]</span></pre><blockquote class="og oh oi"><p id="f0c4" class="lc ld ly le b lf lg ko lh li lj kr lk oj lm ln lo ok lq lr ls ol lu lv lw lx im bi translated"><strong class="le jo"> <em class="je">注释<br/> </em> </strong> <em class="je">在神经网络设置中，这些权重通常是小数字，使用适当的随机分布(如高斯、泽维尔和明凯分布)进行随机初始化。该初始化在训练前进行一次。</em></p></blockquote><p id="ee49" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">第三步:导出关键字、查询和值</strong></p><p id="38f2" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">现在我们有了三组权重，让我们获得每个输入的<strong class="le jo">键</strong>、<strong class="le jo">查询</strong>和<strong class="le jo">值</strong>表示。</p><p id="3eb7" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">输入 1 的<strong class="le jo">键</strong>表示:</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="f4d5" class="oz mm je ov b be pa pb l pg pd">               [0, 0, 1]<br/>[1, 0, 1, 0] x [1, 1, 0] = [0, 1, 1]<br/>               [0, 1, 0]<br/>               [1, 1, 0]</span></pre><p id="4444" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">使用相同的一组权重来获得输入 2 的<strong class="le jo">键</strong>表示:</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="fb9a" class="oz mm je ov b be pa pb l pg pd">               [0, 0, 1]<br/>[0, 2, 0, 2] x [1, 1, 0] = [4, 4, 0]<br/>               [0, 1, 0]<br/>               [1, 1, 0] </span></pre><p id="036b" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">使用相同的一组权重来获得输入 3 的<strong class="le jo">键</strong>表示:</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="d2ef" class="oz mm je ov b be pa pb l pg pd">               [0, 0, 1]<br/>[1, 1, 1, 1] x [1, 1, 0] = [2, 3, 1]<br/>               [0, 1, 0]<br/>               [1, 1, 0]</span></pre><p id="bd7e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">更快的方法是将上述操作矢量化:</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="ea06" class="oz mm je ov b be pa pb l pg pd">               [0, 0, 1]<br/>[1, 0, 1, 0]   [1, 1, 0]   [0, 1, 1]<br/>[0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0]<br/>[1, 1, 1, 1]   [1, 1, 0]   [2, 3, 1]</span></pre><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/9e57705ff723532f45ea6a0a2fc0bcd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*dr6NIaTfTxEWzxB2rc0JWg.gif"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">Fig. 1.3a: Derive <strong class="bd pf">key</strong> representations from each input</figcaption></figure><p id="e69c" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">让我们做同样的事情来获得每个输入的<strong class="le jo">值</strong>表示:</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="65d2" class="oz mm je ov b be pa pb l pg pd">               [0, 2, 0]<br/>[1, 0, 1, 0]   [0, 3, 0]   [1, 2, 3] <br/>[0, 2, 0, 2] x [1, 0, 3] = [2, 8, 0]<br/>[1, 1, 1, 1]   [1, 1, 0]   [2, 6, 3]</span></pre><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/644ff16ae04111dc0e7eecc4342c7077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*5kqW7yEwvcC0tjDOW3Ia-A.gif"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">Fig. 1.3b: Derive <strong class="bd pf">value</strong> representations from each input</figcaption></figure><p id="b167" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">最后是<strong class="le jo">查询</strong>交涉:</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="04f8" class="oz mm je ov b be pa pb l pg pd">               [1, 0, 1]<br/>[1, 0, 1, 0]   [1, 0, 0]   [1, 0, 2]<br/>[0, 2, 0, 2] x [0, 0, 1] = [2, 2, 2]<br/>[1, 1, 1, 1]   [0, 1, 1]   [2, 1, 3]</span></pre><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/895ab4ec49fd6130c1dff8e47ecf29d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wO_UqfkWkv3WmGQVHvrMJw.gif"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">Fig. 1.3c: Derive <strong class="bd pf">query</strong> representations from each input</figcaption></figure><blockquote class="og oh oi"><p id="9180" class="lc ld ly le b lf lg ko lh li lj kr lk oj lm ln lo ok lq lr ls ol lu lv lw lx im bi translated"><strong class="le jo"> <em class="je">注意事项</em> </strong> <em class="je"> <br/>实际上，一个</em>偏置向量<em class="je">可能被加到矩阵乘法的乘积上。</em></p></blockquote><p id="57fe" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">步骤 4:计算输入 1 的关注度分数</strong></p><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f192c9ba7c6ed1bae217baf201bf6675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*u27nhUppoWYIGkRDmYFN2A.gif"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">Fig. 1.4: Calculating attention scores (blue) from query 1</figcaption></figure><p id="2ede" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">为了获得<em class="ly">注意力分数</em>，我们首先在输入 1 的<strong class="le jo">查询</strong>(红色)和所有<strong class="le jo">键</strong>(橙色)之间取点积，包括它本身。由于有 3 个<strong class="le jo">键</strong>表示(因为我们有 3 个输入)，我们获得 3 个注意力分数(蓝色)。</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="2b55" class="oz mm je ov b be pa pb l pg pd">            [0, 4, 2]<br/>[1, 0, 2] x [1, 4, 3] = [2, 4, 4]<br/>            [1, 0, 1]</span></pre><p id="e48a" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">注意，我们只使用输入 1 中的<strong class="le jo">查询</strong>。稍后，我们将对其他<strong class="le jo">查询</strong>重复同样的步骤。</p><blockquote class="og oh oi"><p id="444a" class="lc ld ly le b lf lg ko lh li lj kr lk oj lm ln lo ok lq lr ls ol lu lv lw lx im bi translated"><strong class="le jo"> <em class="je">注意</em> </strong> <em class="je"> <br/>以上操作就是众所周知的</em>点积注意<em class="je">，其中的几个</em> <a class="ae lz" rel="noopener" target="_blank" href="/attn-illustrated-attention-5ec4ad276ee3#ba24"> <strong class="le jo"> <em class="je">得分功能</em> </strong> </a>。<em class="je">其他得分功能包括</em>缩放点积<em class="je">和</em>加法/串联。</p></blockquote><p id="7439" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">第五步:计算 softmax </strong></p><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/79370ceb31d518993f1f7e344c2e2117.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*jf__2D8RNCzefwS0TP1Kyg.gif"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">Fig. 1.5: Softmax the attention scores (blue)</figcaption></figure><p id="0b14" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">将<a class="ae lz" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax </a>穿过这些注意力得分(蓝色)。</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="2bce" class="oz mm je ov b be pa pb l pc pd">softmax([2, 4, 4]) = [0.0, 0.5, 0.5]</span></pre><p id="731f" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">注意，为了可读性，我们在这里四舍五入到小数点后 1 位。</p><p id="8aa6" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">第六步:将分数与数值相乘</strong></p><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1cb7fb230690b49aa5cde40866912ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*9cTaJGgXPbiJ4AOCc6QHyA.gif"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">Fig. 1.6: Derive <strong class="bd pf">weighted</strong> <strong class="bd pf">value</strong> representation (yellow) from multiply <strong class="bd pf">value </strong>(purple) and score (blue)</figcaption></figure><p id="ca19" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">每个输入(蓝色)的最大关注分数乘以其相应的<strong class="le jo">值</strong>(紫色)。这产生了 3 个<em class="ly">对准矢量</em>(黄色)。在本教程中，我们将它们称为<strong class="le jo">加权值</strong>。</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="004d" class="oz mm je ov b be pa pb l pg pd">1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]<br/>2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]<br/>3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]</span></pre><p id="9d4e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">步骤 7:加权值求和得到输出 1 </strong></p><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ac277424c439fe252ea36bc91071f9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1je5TwhVAwwnIeDFvww3ew.gif"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">Fig. 1.7: Sum all <strong class="bd pf">weighted values</strong> (yellow) to get Output 1 (dark green)</figcaption></figure><p id="d1da" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">取所有<strong class="le jo">加权</strong> <strong class="le jo">值</strong>(黄色)并按元素求和:</p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="c645" class="oz mm je ov b be pa pb l pg pd">  [0.0, 0.0, 0.0]<br/>+ [1.0, 4.0, 0.0]<br/>+ [1.0, 3.0, 1.5]<br/>-----------------<br/>= [2.0, 7.0, 1.5]</span></pre><p id="5133" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">结果向量[2.0，7.0，1.5](深绿色)是输出 1，它基于来自输入 1 的<strong class="le jo">查询表示</strong>与所有其他键交互，包括它自己。</p><p id="c14b" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">第 8 步:重复输入 2 &amp;输入 3 </strong></p><p id="7a43" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">现在我们已经完成了输出 1，我们对输出 2 和输出 3 重复步骤 4 到 7。我相信我可以让你自己解决这个问题👍🏼。</p><figure class="om on oo op gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9737f6a14e24bdf08e881df05854196d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*G8thyDVqeD8WHim_QzjvFg.gif"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">Fig. 1.8: Repeat previous steps for Input 2 &amp; Input 3</figcaption></figure><blockquote class="og oh oi"><p id="f2f3" class="lc ld ly le b lf lg ko lh li lj kr lk oj lm ln lo ok lq lr ls ol lu lv lw lx im bi translated"><strong class="le jo"> <em class="je">备注<br/> </em> </strong> <em class="je">维度</em> <strong class="le jo"> <em class="je">查询</em> </strong> <em class="je">和</em> <strong class="le jo"> <em class="je">键</em> </strong> <em class="je">因为点积得分函数的关系必须始终相同。但是</em> <strong class="le jo"> <em class="je">值</em> </strong> <em class="je">的维度可能与</em> <strong class="le jo"> <em class="je">查询</em></strong><em class="je"/><strong class="le jo"><em class="je">键</em> </strong> <em class="je">不同。结果输出将遵循</em> <strong class="le jo"> <em class="je">值</em> </strong> <em class="je">的尺寸。</em></p></blockquote><h2 id="8481" class="ml mm je bd mn mo mp dn mq mr ms dp mt ll mu mv mw lp mx my mz lt na nb nc jk bi translated">2.密码</h2><p id="f24e" class="pw-post-body-paragraph lc ld je le b lf nf ko lh li ng kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">下面是<a class="ae lz" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>中的代码🤗，Python 中流行的深度学习框架。要在下面的代码片段中享受用于<code class="fe ph pi pj ov b">@</code>操作符、<code class="fe ph pi pj ov b">.T</code>和<code class="fe ph pi pj ov b"><strong class="le jo">None</strong></code>索引的 API，请确保您使用的是 Python≥3.6 和 PyTorch 1.3.1。只需跟着做，将这些复制粘贴到 Python/IPython REPL 或 Jupyter 笔记本中。</p><p id="8ff4" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">步骤 1:准备输入</strong></p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="d731" class="oz mm je ov b be pa pb l pg pd">&gt;&gt;&gt; import torch<br/><br/>&gt;&gt;&gt; x = [<br/>...   [1, 0, 1, 0], # Input 1<br/>...   [0, 2, 0, 2], # Input 2<br/>...   [1, 1, 1, 1], # Input 3<br/>... ]<br/>&gt;&gt;&gt; x = torch.tensor(x, dtype=torch.float32)</span></pre><p id="313d" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">步骤 2:初始化砝码</strong></p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="0f67" class="oz mm je ov b be pa pb l pg pd">&gt;&gt;&gt; w_key = [<br/>...   [0, 0, 1],<br/>...   [1, 1, 0],<br/>...   [0, 1, 0],<br/>...   [1, 1, 0],<br/>... ]<br/>&gt;&gt;&gt; w_query = [<br/>...   [1, 0, 1],<br/>...   [1, 0, 0],<br/>...   [0, 0, 1],<br/>...   [0, 1, 1],<br/>... ]<br/>&gt;&gt;&gt; w_value = [<br/>...   [0, 2, 0],<br/>...   [0, 3, 0],<br/>...   [1, 0, 3],<br/>...   [1, 1, 0],<br/>... ]<br/><br/>&gt;&gt;&gt; w_key = torch.tensor(w_key, dtype=torch.float32)<br/>&gt;&gt;&gt; w_query = torch.tensor(w_query, dtype=torch.float32)<br/>&gt;&gt;&gt; w_value = torch.tensor(w_value, dtype=torch.float32)</span></pre><p id="5c4a" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">步骤 3:导出密钥、查询和值</strong></p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="fe08" class="oz mm je ov b be pa pb l pg pd">&gt;&gt;&gt; keys = x @ w_key<br/>&gt;&gt;&gt; querys = x @ w_query<br/>&gt;&gt;&gt; values = x @ w_value<br/><br/>&gt;&gt;&gt; keys<br/>tensor([[0., 1., 1.],<br/>        [4., 4., 0.],<br/>        [2., 3., 1.]])<br/><br/>&gt;&gt;&gt; querys<br/>tensor([[1., 0., 2.],<br/>        [2., 2., 2.],<br/>        [2., 1., 3.]])<br/><br/>&gt;&gt;&gt; values<br/>tensor([[1., 2., 3.],<br/>        [2., 8., 0.],<br/>        [2., 6., 3.]])</span></pre><p id="442b" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">第四步:计算注意力得分</strong></p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="365d" class="oz mm je ov b be pa pb l pg pd">&gt;&gt;&gt; attn_scores = querys @ keys.T<br/>&gt;&gt;&gt; attn_scores<br/>tensor([[ 2.,  4.,  4.],  # attention scores from Query 1<br/>        [ 4., 16., 12.],  # attention scores from Query 2<br/>        [ 4., 12., 10.]]) # attention scores from Query 3</span></pre><p id="a04c" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">第五步:计算 softmax </strong></p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="d963" class="oz mm je ov b be pa pb l pg pd">&gt;&gt;&gt; from torch.nn.functional import softmax<br/><br/>&gt;&gt;&gt; attn_scores_softmax = softmax(attn_scores, dim=-1)<br/>tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],<br/>        [6.0337e-06, 9.8201e-01, 1.7986e-02],<br/>        [2.9539e-04, 8.8054e-01, 1.1917e-01]])<br/><br/>&gt;&gt;&gt; # For readability, approximate the above as follows<br/>&gt;&gt;&gt; attn_scores_softmax = [<br/>...   [0.0, 0.5, 0.5],<br/>...   [0.0, 1.0, 0.0],<br/>...   [0.0, 0.9, 0.1],<br/>...  ]<br/>&gt;&gt;&gt; attn_scores_softmax = torch.tensor(attn_scores_softmax)</span></pre><p id="686c" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">第六步:将分数与数值相乘</strong></p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="a211" class="oz mm je ov b be pa pb l pg pd">&gt;&gt;&gt; weighted_values = values[:,None] * attn_scores_softmax.T[:,:,None]<br/>&gt;&gt;&gt; weighted_values<br/>tensor([[[0.0000, 0.0000, 0.0000],<br/>         [0.0000, 0.0000, 0.0000],<br/>         [0.0000, 0.0000, 0.0000]],<br/><br/>        [[1.0000, 4.0000, 0.0000],<br/>         [2.0000, 8.0000, 0.0000],<br/>         [1.8000, 7.2000, 0.0000]],<br/><br/>        [[1.0000, 3.0000, 1.5000],<br/>         [0.0000, 0.0000, 0.0000],<br/>         [0.2000, 0.6000, 0.3000]]])</span></pre><p id="1110" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">步骤 7:加权值求和</strong></p><pre class="om on oo op gt ou ov ow bn ox oy bi"><span id="4e1a" class="oz mm je ov b be pa pb l pg pd">&gt;&gt;&gt; outputs = weighted_values.sum(dim=0)<br/>&gt;&gt;&gt; outputs<br/>tensor([[2.0000, 7.0000, 1.5000],  # Output 1<br/>        [2.0000, 8.0000, 0.0000],  # Output 2<br/>        [2.0000, 7.8000, 0.3000]]) # Output 3</span></pre><blockquote class="og oh oi"><p id="5256" class="lc ld ly le b lf lg ko lh li lj kr lk oj lm ln lo ok lq lr ls ol lu lv lw lx im bi translated"><strong class="le jo"> <em class="je">注</em> </strong> <em class="je"> <br/> PyTorch 为此提供了一个 API 叫做</em> <code class="fe ph pi pj ov b"><a class="ae lz" href="https://pytorch.org/docs/master/nn.html#multiheadattention" rel="noopener ugc nofollow" target="_blank"><em class="je">nn.MultiheadAttention</em></a></code> <em class="je">。但是，这个 API 要求您输入键、查询和值 PyTorch 张量。此外，该模块的输出经历线性变换。</em></p></blockquote><h2 id="faae" class="ml mm je bd mn mo mp dn mq mr ms dp mt ll mu mv mw lp mx my mz lt na nb nc jk bi translated">3.延伸到变压器</h2><p id="9163" class="pw-post-body-paragraph lc ld je le b lf nf ko lh li ng kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">那么，我们该何去何从？变形金刚！事实上，我们生活在深度学习研究和高计算资源的激动人心的时代。变形金刚是来自<a class="ae lz" href="#9abf" rel="noopener ugc nofollow">注意力就是一切</a>的化身，原本是为了执行<a class="ae lz" href="https://en.wikipedia.org/wiki/Neural_machine_translation" rel="noopener ugc nofollow" target="_blank">神经机器翻译</a>而生。研究人员从这里开始，重新组装，切割，添加和扩展部件，并将其应用于更多的语言任务。</p><p id="ab0e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在这里，我将简要地提到我们如何将自我关注扩展到一个 Transformer 架构。</p><p id="9183" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在自我关注模块中:</p><ul class=""><li id="bfec" class="nd ne je le b lf lg li lj ll od lp oe lt of lx pk nl nm nn bi translated">尺寸</li><li id="915a" class="nd ne je le b lf no li np ll nq lp nr lt ns lx pk nl nm nn bi translated">偏见</li></ul><p id="5743" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">自我关注模块的输入:</p><ul class=""><li id="d096" class="nd ne je le b lf lg li lj ll od lp oe lt of lx pk nl nm nn bi translated">嵌入模块</li><li id="a754" class="nd ne je le b lf no li np ll nq lp nr lt ns lx pk nl nm nn bi translated">位置编码</li><li id="60ce" class="nd ne je le b lf no li np ll nq lp nr lt ns lx pk nl nm nn bi translated">缩短</li><li id="79ef" class="nd ne je le b lf no li np ll nq lp nr lt ns lx pk nl nm nn bi translated">掩饰</li></ul><p id="c16c" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">添加更多自我关注模块:</p><ul class=""><li id="26ec" class="nd ne je le b lf lg li lj ll od lp oe lt of lx pk nl nm nn bi translated">多传感头</li><li id="7c37" class="nd ne je le b lf no li np ll nq lp nr lt ns lx pk nl nm nn bi translated">层堆叠</li></ul><p id="0a2c" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">自我关注模块之间的模块:</p><ul class=""><li id="15c1" class="nd ne je le b lf lg li lj ll od lp oe lt of lx pk nl nm nn bi translated">线性变换</li><li id="31a5" class="nd ne je le b lf no li np ll nq lp nr lt ns lx pk nl nm nn bi translated">层状</li></ul></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><p id="3ef7" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">那都是乡亲们！希望你觉得内容容易消化。你认为我应该在这篇文章中进一步补充或阐述什么吗？请留言！此外，一定要看看下面我创作的一幅插图</p><h2 id="9abf" class="ml mm je bd mn mo mp dn mq mr ms dp mt ll mu mv mw lp mx my mz lt na nb nc jk bi translated">参考</h2><p id="214a" class="pw-post-body-paragraph lc ld je le b lf nf ko lh li ng kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">你所需要的就是关注(arxiv.org)</p><p id="0400" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图解变压器</a> (jalammar.github.io)</p><h2 id="51ff" class="ml mm je bd mn mo mp dn mq mr ms dp mt ll mu mv mw lp mx my mz lt na nb nc jk bi translated">相关文章</h2><p id="e122" class="pw-post-body-paragraph lc ld je le b lf nf ko lh li ng kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">【towardsdatascience.com】经办人:图文并茂</p><p id="edfc" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><em class="ly">如果你喜欢我的内容并且还没有订阅 Medium，请通过我的推荐链接</em> <a class="ae lz" href="https://medium.com/@remykarem/membership" rel="noopener"> <em class="ly">这里</em> </a> <em class="ly">订阅！注意:你的会员费的一部分将作为介绍费分配给我。</em></p><p id="2e2a" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><em class="ly">特别感谢 Xin Jie、Serene、任杰、Kevin 和 Wei Yih 为本文提供想法、建议和更正。</em></p><p id="4d00" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><em class="ly">在 Twitter 上关注我</em><a class="ae lz" href="https://twitter.com/remykarem" rel="noopener ugc nofollow" target="_blank"><em class="ly">@ remykarem</em></a><em class="ly">关于 AI、ML、深度学习和 Python 的消化文章和其他推文。</em></p></div></div>    
</body>
</html>