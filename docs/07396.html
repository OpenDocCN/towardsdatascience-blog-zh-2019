<html>
<head>
<title>Top three mistakes with K-Means Clustering during data analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据分析期间 K 均值聚类的三大错误</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/top-three-mistakes-with-k-means-clustering-during-data-analysis-b984fda0a0d6?source=collection_archive---------29-----------------------#2019-10-16">https://towardsdatascience.com/top-three-mistakes-with-k-means-clustering-during-data-analysis-b984fda0a0d6?source=collection_archive---------29-----------------------#2019-10-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="2774" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">介绍</h1><p id="0822" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在本帖中，我们将看看 KMC 算法表现不佳或可能产生不直观结果的一些情况。具体来说，我们将查看以下场景:</p><ol class=""><li id="70eb" class="lm ln it kq b kr lo kv lp kz lq ld lr lh ls ll lt lu lv lw bi translated">我们对(真实)星团数量的猜测是错误的。</li><li id="6092" class="lm ln it kq b kr lx kv ly kz lz ld ma lh mb ll lt lu lv lw bi translated">特征空间是高度多维的。</li><li id="1aa5" class="lm ln it kq b kr lx kv ly kz lz ld ma lh mb ll lt lu lv lw bi translated">这些簇以奇怪或不规则的形状出现。</li></ol><p id="9daa" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">所有这些情况都会导致 K-Means 出现问题，所以让我们来看看。</p><h1 id="3514" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">错误的集群数量</h1><p id="23da" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了简单起见，让我们定义一个助手函数<code class="fe mf mg mh mi b">compare</code>，它将为我们创建并解决聚类问题，然后比较结果。</p><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="2018" class="mr jr it mi b gy ms mt l mu mv">from sklearn import datasets<br/>from sklearn.cluster import KMeans<br/>from sklearn.datasets import make_blobs, make_circles, make_moons<br/>from mpl_toolkits.mplot3d import Axes3D<br/><br/>import numpy as np<br/>import pandas as pd<br/>import itertools<br/><br/><br/>def compare(N_features, C_centers, K_clusters, dims=[0, 1],*args):<br/>    data, targets = make_blobs(<br/>      n_samples=n_samples if 'n_samples' in args else 400,<br/>      n_features=N_features,<br/>      centers=C_centers,<br/>      cluster_std=cluster_std if 'cluster_std' in args else 0.5,<br/>      shuffle=True,<br/>      random_state=random_state if 'random_state' in args else 0)<br/><br/>    FEATS = ['x' + str(x) for x in range(N_features)]<br/>    X = pd.DataFrame(data, columns=FEATS)<br/>    X['cluster'] = \<br/>		KMeans(n_clusters=K_clusters, random_state=0).fit_predict(X)<br/><br/>    fig, axs = plt.subplots(1, 2, figsize=(12, 4))<br/>    axs[0].scatter(data[:, dims[0]], data[:, dims[1]],<br/>        c='white', marker='o', edgecolor='black', s=20)<br/>    axs[0].set_xlabel('x{} [a.u.]'.format(dims[0]))<br/>    axs[0].set_ylabel('x{} [a.u.]'.format(dims[1]))<br/>    axs[0].set_title('Original dataset')<br/>    axs[1].set_xlabel('x{} [a.u.]'.format(dims[0]))<br/>    axs[1].set_ylabel('x{} [a.u.]'.format(dims[1]))<br/>    axs[1].set_title('Applying clustering')<br/><br/>    colors = itertools.cycle(['r', 'g', 'b', 'm', 'c', 'y'])<br/>    for k in range(K_clusters):<br/>        x = X[X['cluster'] == k][FEATS].to_numpy()<br/>        axs[1].scatter(<br/>		x[:, dims[0]], <br/>		x[:, dims[1]], <br/>		color=next(colors),<br/>		edgecolor='k', <br/>		alpha=0.5<br/>	)<br/>    plt.show()</span></pre><h1 id="3ea2" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">集群太少</h1><figure class="mj mk ml mm gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mw"><img src="../Images/9b5634603b6f6e1c26c57b675fd8567c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IosV6VdqDFxxnofJ.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Figure 1a. Example of a 2-dimensional dataset with 4 centres, requesting 3 clusters (<code class="fe mf mg mh mi b">compare(2, 4, 3)</code>).</figcaption></figure><p id="2602" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">尽管数据中有不同的聚类，但我们低估了它们的数量。因此，一些不相交的数据组被迫放入一个更大的群集中。</p><h1 id="7f3d" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">太多集群</h1><figure class="mj mk ml mm gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mw"><img src="../Images/ac7efe7123f86959f575a8b40341143a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QUitlt-UonzXUzKH.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Figure 1b. Example of a 2-dimensional dataset with 2 centres, requesting 4 clusters (<code class="fe mf mg mh mi b">compare(2, 2, 4)</code>).</figcaption></figure><p id="65a4" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">与最后一种情况相反，试图将数据包装到太多的集群中会在真实的数据集群中产生人为的边界。</p><h1 id="0383" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">高维数据</h1><p id="3086" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在我们开始发现问题之前，数据集不需要有那么高的维数。尽管可视化和对高维数据的分析已经很有挑战性了(现在开始诅咒…)，但是 KMC 经常被用来洞察数据，它并不能帮助你理解数据的模糊性。</p><p id="4329" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">为了解释这一点，让我们生成一个具有明显不同聚类的三维数据集。</p><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="eeb8" class="mr jr it mi b gy ms mt l mu mv">fig = plt.figure(figsize=(14, 8))<br/>ax = fig.add_subplot(111, projection='3d')<br/><br/>data, targets = make_blobs(<br/>    n_samples=400,<br/>    n_features=3,<br/>    centers=3,<br/>    cluster_std=0.5,<br/>    shuffle=True,<br/>    random_state=0)<br/><br/>ax.scatter(data[:, 0], data[:, 1], <br/>    zs=data[:, 2], zdir='z', s=25, c='black', depthshade=True)<br/>ax.set_xlabel('x0 [a.u.]')<br/>ax.set_ylabel('x1 [a.u.]')<br/>ax.set_zlabel('x2 [a.u.]')<br/>ax.set_title('Original distribution.')<br/>plt.grid()<br/>plt.show()</span></pre><figure class="mj mk ml mm gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ni"><img src="../Images/79cdbc0d0655e1f5ded499769762a7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RgzPm9ak3fWBd7to.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Figure 2. Example of a 3-dimensional dataset with 3 centers.</figcaption></figure><p id="5b00" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">尽管有无限多种方法可以将这个 3D 数据集投影到 2D 上，但是有三个主要的正交子空间:</p><p id="49c0" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">查看<code class="fe mf mg mh mi b">x2 : x0</code>投影，数据集看起来好像只有两个集群。右下角的“超星系团”实际上是两个不同的群组，即使我们猜对了<em class="nj">K</em><em class="nj">(K = 3)</em>，这看起来也是一个明显的错误，尽管这些星系团非常局限。</p><figure class="mj mk ml mm gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nk"><img src="../Images/f0dffaacd59c2387ecdb76f2e04385e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g4JHXAdSUa97VKTW.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Figure 3a. Projection on `x0 : x2` shows spurious result (<code class="fe mf mg mh mi b">compare(2, 2, 4, dims=[0, 2])</code>).</figcaption></figure><p id="3ad8" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">可以肯定的是，我们必须从不同的角度来看剩下的预测，才能真正理解这个问题。</p><figure class="mj mk ml mm gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nk"><img src="../Images/5f129d0ce4e71ba0ab2764f890c060cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dU4oCpziJjJRnQlI.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Figure 3b. Projection on `x1 : x2` resolves the ambiguity (<code class="fe mf mg mh mi b">compare(2, 2, 4, dims=[1, 2])</code>).</figcaption></figure><figure class="mj mk ml mm gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nl"><img src="../Images/6d1c78c45357291b9a2d0643c4ae5f49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ipDCLDNfM__n1mlC.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Figure 3c. Projection on `x0 : x1` resolves the ambiguity (<code class="fe mf mg mh mi b">compare(2, 2, 4, dims=[0, 1])</code>).</figcaption></figure><p id="f45c" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">这更有道理！</p><p id="fcdb" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">另一方面，我们有不可思议的优势。首先，有了三维，我们能够绘制整个数据集。其次，数据集中存在的聚类非常独特，因此很容易识别。最后，有了三维数据集，我们只需要面对三个标准的 2D 投影。</p><p id="db2c" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">在<em class="nj"> N，N &gt; 3 </em>特征的情况下，我们将<strong class="kq iu">不能绘制整个数据集</strong>，并且 2D 投影的数量将与<em class="nj"> N </em>成二次比例:</p><p id="ef62" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">更不用说数据集可能会有形状奇怪或者非本地化的聚类，这是我们的下一个挑战。</p><h1 id="63c5" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">不规则数据集</h1><p id="d937" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">到目前为止，我们提到了“我们这边”的问题。我们查看了一个非常“表现良好”的数据集，并讨论了分析方面的问题。但是，如果数据集不适合我们的解决方案，或者我们的<strong class="kq iu">解决方案不适合问题，该怎么办？</strong>正是这种情况，数据分布以奇怪或不规则的形状出现。</p><p id="931e" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">仅仅看到这张图，我们可能会误以为数据中只有两个聚类。然而，当绘制剩余的投影时，我们很快发现这是不正确的。</p><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="e2c5" class="mr jr it mi b gy ms mt l mu mv">fig, axs = plt.subplots(1, 3, figsize=(14, 4))<br/><br/># unequal variance<br/>X, y = make_blobs(n_samples=1400,<br/>    cluster_std=[1.0, 2.5, 0.2],<br/>    random_state=2)<br/>y_pred = KMeans(n_clusters=3, random_state=2).fit_predict(X)<br/>colors = [['r', 'g', 'b'][c] for c in y_pred]<br/><br/>axs[0].scatter(X[:, 0], X[:, 1], <br/>	color=colors, edgecolor='k', alpha=0.5)<br/>axs[0].set_title("Unequal Variance")<br/><br/># anisotropically distributed data<br/>X, y = make_blobs(n_samples=1400, random_state=156)<br/>transformation = [<br/>	[0.60834549, -0.63667341],<br/>	[-0.40887718, 0.85253229]<br/>]<br/>X = np.dot(X, transformation)<br/>y_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X)<br/>colors = [['r', 'g', 'b'][c] for c in y_pred]<br/><br/>axs[1].scatter(X[:, 0], X[:, 1], <br/>	color=colors, edgecolor='k', alpha=0.5)<br/>axs[1].set_title("Anisotropicly Distributed Blobs")<br/><br/># irregular shaped data<br/>X, y = make_moons(n_samples=1400, shuffle=True, <br/>	noise=0.1, random_state=120)<br/>y_pred = KMeans(n_clusters=2, random_state=0).fit_predict(X)<br/>colors = [['r', 'g', 'b'][c] for c in y_pred]<br/><br/>axs[2].scatter(X[:, 0], X[:, 1], <br/>	color=colors, edgecolor='k', alpha=0.5)<br/>axs[2].set_title("Irregular Shaped Data")<br/><br/>plt.show()</span></pre><figure class="mj mk ml mm gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nm"><img src="../Images/9353356454d806b6458bb3f0fc6914d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XfKavIpTGoMpy4lq.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Figure 4. Misleading clustering results are shown on irregular datasets.</figcaption></figure><p id="a880" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">左图显示了其分布(虽然是高斯分布)不具有相等标准偏差的数据。中间的图表显示了<em class="nj">各向异性</em>数据，即沿着特定轴拉长的数据。最后，右图显示了完全非高斯的数据，尽管这些数据被组织成清晰的簇。</p><p id="b949" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">在这两种情况下，不规则性使得 KMC 算法表现不佳。由于算法平等地对待每一个数据点，并且完全独立于其他点，算法<strong class="kq iu">未能发现一个聚类</strong>内任何可能的连续性或局部变化。它所做的只是采用相同的指标，并将其应用于每一点。因此，KMC 算法可能会在数据中产生奇怪的或违反直觉的聚类，即使我们正确地猜测了<em class="nj"> K </em>并且特征<em class="nj"> N </em>不是很多。</p><h1 id="1889" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">结论</h1><p id="4908" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在这篇文章中，我们讨论了 K-Means 聚类算法给我们错误答案的三个主要原因。</p><ul class=""><li id="255c" class="lm ln it kq b kr lo kv lp kz lq ld lr lh ls ll nn lu lv lw bi translated">首先，由于需要先验地确定聚类数<em class="nj"> K </em>，因此我们很有可能会错误地猜测它。</li><li id="10c3" class="lm ln it kq b kr lx kv ly kz lz ld ma lh mb ll nn lu lv lw bi translated">其次，从分析的角度来看，高维空间中的聚类变得很麻烦，在这种情况下，KMC 将为我们提供可能具有误导性的见解。</li><li id="143c" class="lm ln it kq b kr lx kv ly kz lz ld ma lh mb ll nn lu lv lw bi translated">最后，对于任何形状不规则的数据，KMC 很可能会人为地进行不符合常识的聚类。</li></ul><p id="aa4d" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">知道了这三个谬误，KMC 仍然是一个有用的工具，尤其是在检查数据或构造标签的时候。</p><h1 id="48e0" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">还会有更多…</h1><p id="f1b4" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我计划把文章带到下一个层次，并提供简短的视频教程。</p><p id="b613" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated">如果您想了解关于视频和未来文章的更新，<strong class="kq iu">订阅我的</strong> <a class="ae no" href="https://landing.mailerlite.com/webforms/landing/j5y2q1" rel="noopener ugc nofollow" target="_blank"> <strong class="kq iu">简讯</strong> </a> <strong class="kq iu">。你也可以通过填写<a class="ae no" href="https://forms.gle/bNpf9aqZJGLgaU589" rel="noopener ugc nofollow" target="_blank">表格</a>让我知道你的期望。回头见！</strong></p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="9834" class="pw-post-body-paragraph ko kp it kq b kr lo kt ku kv lp kx ky kz mc lb lc ld md lf lg lh me lj lk ll im bi translated"><em class="nj">最初发表于</em><a class="ae no" href="https://zerowithdot.com/mistakes-with-k-means-clustering/" rel="noopener ugc nofollow" target="_blank"><em class="nj">【https://zerowithdot.com】</em></a><em class="nj">。</em></p></div></div>    
</body>
</html>