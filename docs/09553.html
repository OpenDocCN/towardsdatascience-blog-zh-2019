<html>
<head>
<title>Biomedical Image Segmentation: UNet++</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生物医学图像分割:UNet++</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/biomedical-image-segmentation-unet-991d075a3a4b?source=collection_archive---------9-----------------------#2019-12-16">https://towardsdatascience.com/biomedical-image-segmentation-unet-991d075a3a4b?source=collection_archive---------9-----------------------#2019-12-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b027" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过一系列嵌套、密集的跳过路径提高分段准确性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/919b5026498d110b656f4df7d828064b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pxsNisW2-bbXZ7xOL1z3pA.png"/></div></div></figure><p id="7943" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这篇文章中，我们将探索由美国亚利桑那州立大学的周等人编写的用于医学图像分割的嵌套 U-Net 架构。这篇文章是<a class="ae lq" rel="noopener" target="_blank" href="/biomedical-image-segmentation-u-net-a787741837fa"> U-Net 文章</a>的延续，我们将比较 UNet++和 Ronneberger 等人的原始 U-Net。</p><p id="b5dc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">UNet++旨在通过在编码器和解码器之间包含密集块和卷积层来提高分割精度。</p><p id="d195" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">分割精度对于医学图像是至关重要的，因为边际分割误差会导致不可靠的结果；因此将被拒绝用于临床设置。</p><p id="6fc6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管数据样本较少，但为医学成像设计的算法必须实现高性能和高精度。获取这些样本图像来训练模型可能是一个消耗资源的过程，因为需要由专业人员审查的高质量的未压缩和精确注释的图像。</p><h1 id="a54f" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">UNet++有什么新功能？</h1><p id="2d19" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">下面是 UNet++和 U-Net 架构的图解。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/ed7b86ea25b8d3958c68ecdb163dd0ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XmqyKSM3I68GWGJg3V5ZkQ.jpeg"/></div></div></figure><p id="e045" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">UNet++在原有的 U-Net 基础上增加了 3 项功能:</p><ol class=""><li id="c265" class="mq mr it kw b kx ky la lb ld ms lh mt ll mu lp mv mw mx my bi translated">重新设计的跳过路径(显示为绿色)</li><li id="9655" class="mq mr it kw b kx mz la na ld nb lh nc ll nd lp mv mw mx my bi translated">密集跳跃连接(以蓝色显示)</li><li id="92b3" class="mq mr it kw b kx mz la na ld nb lh nc ll nd lp mv mw mx my bi translated">深度监督(以红色显示)</li></ol><h2 id="9cca" class="ne lt it bd lu nf ng dn ly nh ni dp mc ld nj nk me lh nl nm mg ll nn no mi np bi translated">重新设计的跳过路径</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/f4a90b283dba575b21bc6f9c81a12fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PqtyWOC9StSUBFw1SLMe4g.png"/></div></div></figure><p id="aefc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在 UNet++中，重新设计的跳过路径(显示为绿色)被添加进来，以弥合编码器和解码器子路径之间的语义鸿沟。</p><p id="01fa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些卷积层的目的是减少编码器和解码器子网的特征图之间的语义差距。因此，对于优化者来说，这可能是一个更直接的优化问题。</p><p id="2bb6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">U-Net 中使用的 Skip 连接直接连接编码器和解码器之间的特征映射，从而融合语义不同的特征映射。</p><p id="f9bf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，使用 UNet++时，来自同一密集块的先前卷积层的输出与较低密集块的相应上采样输出融合。这使得编码特征的语义级别更接近在解码器中等待的特征映射的语义级别；因此，当接收到语义相似的特征图时，优化更容易。</p><p id="24cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">skip 路径上的所有卷积层都使用大小为 3×3 的核。</p><h2 id="6cb5" class="ne lt it bd lu nf ng dn ly nh ni dp mc ld nj nk me lh nl nm mg ll nn no mi np bi translated">密集跳跃连接</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/f4a90b283dba575b21bc6f9c81a12fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PqtyWOC9StSUBFw1SLMe4g.png"/></div></div></figure><p id="21b5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在 UNet++中，密集跳过连接(蓝色显示)在编码器和解码器之间实现了跳过路径。这些密集块受<a class="ae lq" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNet </a>的启发，目的是提高分割精度并改善梯度流。</p><p id="a4f9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">密集跳跃连接确保所有先前的特征图被累积并到达当前节点，因为沿着每个跳跃路径的密集卷积块。这在多个语义级别生成全分辨率特征图。</p><h2 id="6a79" class="ne lt it bd lu nf ng dn ly nh ni dp mc ld nj nk me lh nl nm mg ll nn no mi np bi translated">深度监督</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/9f278b0e24614ad70d77b2ea7acd763b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1CedzEo8ruw1UTs0EafpQ.png"/></div></div></figure><p id="2cf2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在 UNet++中，增加了深度监督(红色显示)，这样就可以修剪模型来调整模型复杂度，平衡<em class="lr">速度</em>(推理时间)和<em class="lr">性能</em>。</p><p id="dc00" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于<em class="lr">精确</em>模式，所有分割分支的输出被平均。</p><p id="025e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于<em class="lr">快速</em>模式，从一个分割分支中选择最终分割图。</p><p id="ac46" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">周等人进行了实验以确定具有不同修剪水平的最佳分割性能。使用的度量标准是<em class="lr">交集/并集</em>和<em class="lr">推理时间</em>。</p><p id="fad9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">他们对四个分割任务进行了实验:a)细胞核，b)结肠息肉，c)肝脏，以及 d)肺结节。结果如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/855bb215a735bf9590768de4858efe92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oekmlu51SVp6EmFDYtPvwA.png"/></div></div></figure><p id="4500" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">与 L4 相比，L3 的<em class="lr">推理时间</em>平均减少了 32.2%，同时略微降低了 Union 的<em class="lr">交集。</em></p><p id="2709" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">L1 和 L2 等更激进的修剪方法可以进一步减少<em class="lr">推理时间</em>，但代价是显著的分割性能。</p><p id="c537" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当使用 UNet++时，我们可以调整用例的层数。</p><h1 id="0a03" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">我在 UNet++上的实验</h1><p id="69da" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我将使用<a class="ae lq" href="https://cvit.iiit.ac.in/projects/mip/drishti-gs/mip-dataset2/Home.php" rel="noopener ugc nofollow" target="_blank"> Drishti-GS 数据集</a>，这与 Ronneberger 等人在论文中使用的数据集不同。该数据集包含 101 个视网膜图像，以及光盘和视杯的注释掩模，用于检测青光眼，青光眼是世界上失明的主要原因之一。50 幅图像将用于训练，51 幅用于验证。</p><h2 id="12fc" class="ne lt it bd lu nf ng dn ly nh ni dp mc ld nj nk me lh nl nm mg ll nn no mi np bi translated">韵律学</h2><p id="fe38" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我们需要一组指标来比较不同的模型，这里我们有二元交叉熵、Dice 系数和交集。</p><p id="aba1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">二元交叉熵<br/> </strong>二元分类常用的度量和损失函数，用于度量误分类的概率。</p><p id="37e4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将使用 PyTorch 的 binary _ cross _ entropy _ with _ logits<a class="ae lq" href="https://pytorch.org/docs/stable/nn.functional.html#binary-cross-entropy-with-logits" rel="noopener ugc nofollow" target="_blank">。与 Dice 系数一起用作训练模型的损失函数。</a></p><p id="0099" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">骰子系数</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/d812b74b97d20082ad1a454659e44d58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNoa0X7NYtKyFD3-Tw8w3A.jpeg"/></div></div></figure><p id="0c4f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">预测值和实际值之间重叠的常用度量标准。计算是 2 *重叠面积(<em class="lr">预测值和实际值</em>之间)除以总面积(<em class="lr">预测值和实际值的组合</em>)。</p><p id="05d4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该度量的范围在 0 和 1 之间，其中 1 表示完美和完全的重叠。</p><p id="ca4e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我将使用这个度量和二进制交叉熵作为训练模型的损失函数。</p><p id="f7ac" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">并集上的交集</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/0010cdcc908c8d1686f71068cb548593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vsx1tBly1KnY7e8IKJvvQQ.jpeg"/></div></div></figure><p id="2996" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一个简单(然而有效！)用于计算预测遮罩与地面真实遮罩的准确度的度量。计算重叠面积<em class="lr"> </em> ( <em class="lr">在预测值和实际值</em>之间)并除以并集面积<em class="lr"> </em> ( <em class="lr">预测值和实际值</em>)的计算。</p><p id="f9b7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">类似于 Dice 系数，该度量的范围从 0 到 1，其中 0 表示没有重叠，而 1 表示预测值和实际值之间完全重叠。</p><h2 id="ad70" class="ne lt it bd lu nf ng dn ly nh ni dp mc ld nj nk me lh nl nm mg ll nn no mi np bi translated">培训和结果</h2><p id="b440" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">为了优化该模型，训练超过 50 个时期，使用具有 1e-4 学习率的<a class="ae lq" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam" rel="noopener ugc nofollow" target="_blank"> Adam 优化器</a>，以及每 10 个时期具有 0.1 衰减(伽马)的<a class="ae lq" href="https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR" rel="noopener ugc nofollow" target="_blank"> Step LR </a>。损失函数是二进制交叉熵和 Dice 系数的组合。</p><p id="3eb4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">模型在 27 分钟内完成了 36.6M 可训练参数的训练；每个时期大约需要 32 秒。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/7520cd48a11c3c3dcbe3c3f745d952c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*siHLGfbXyCMy152mESYFBA.png"/></div></div></figure><p id="24d4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">具有最佳性能的纪元是纪元# 45(50 个中的一个)。</p><ul class=""><li id="c96f" class="mq mr it kw b kx ky la lb ld ms lh mt ll mu lp nw mw mx my bi translated">二元交叉熵:0.2650</li><li id="8e4b" class="mq mr it kw b kx mz la na ld nb lh nc ll nd lp nw mw mx my bi translated">骰子系数:0.8104</li><li id="8a1d" class="mq mr it kw b kx mz la na ld nb lh nc ll nd lp nw mw mx my bi translated">并集上的交点:0.8580</li></ul><p id="d51d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用于比较的几个 U-Net 模型之间的度量，如下所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="0502" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">测试从模型处理一些看不见的样本开始，以预测光盘(红色)和光学杯(黄色)。以下是 UNet++和 U-Net 的测试结果，以供比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/05e714502b374a17bcdf7ef96048f251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jhyAYAhCHizWt2qkGJmkEg.jpeg"/></div></div></figure><p id="e7e9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从指标表来看，UNet++在<em class="lr">交集方面超过了 U-Net，但在<em class="lr">骰子系数</em>方面落后。从定性测试结果来看，UNet++已经成功地正确分割了第一幅图像，而 U-Net 做得并不太好。同样由于 UNet++的复杂性，训练时间是 U-Net 的两倍。人们必须根据它们的数据集来评估每种方法。</em></p><h1 id="496a" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">结论</h1><p id="b2e7" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">UNet++旨在通过一系列嵌套的密集跳过路径来提高分割精度。</p><p id="0016" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">重新设计的跳过路径使得语义相似的特征图的优化更加容易。</p><p id="29e9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">密集跳跃连接提高了分割精度并改善了梯度流。</p><p id="6e99" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">深度监督允许模型复杂性调整，以平衡速度和性能优化。</p><p id="9776" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">阅读另一个 U-Net:</p><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/biomedical-image-segmentation-u-net-a787741837fa"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">生物医学图像分割:U-Net</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">使用非常少的训练图像，并产生更精确的分割。</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq ks oc"/></div></div></a></div><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/biomedical-image-segmentation-attention-u-net-29b6f0827405"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">生物医学图像分割:注意力 U 网</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">通过在标准 U-Net 上附加注意门来提高模型的灵敏度和准确性</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="or l on oo op ol oq ks oc"/></div></div></a></div></div><div class="ab cl os ot hx ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="im in io ip iq"><div class="kj kk kl km gt oc"><a rel="noopener follow" target="_blank" href="/data-scientist-the-dirtiest-job-of-the-21st-century-7f0c8215e845"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">数据科学家:21 世纪最肮脏的工作</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">40%的吸尘器，40%的看门人，20%的算命师。</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="oz l on oo op ol oq ks oc"/></div></div></a></div><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/7-essential-ai-youtube-channels-d545ab401c4"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">7 个必不可少的人工智能 YouTube 频道</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">如何跟上最新最酷的机器学习进展</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="pa l on oo op ol oq ks oc"/></div></div></a></div><div class="kj kk kl km gt ab cb"><figure class="pb kn pc pd pe pf pg paragraph-image"><a href="https://www.linkedin.com/in/jingles/"><img src="../Images/7820823f18c088b934fefc4fcbe5e6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*fPTPd_WxZ4Ey7iOVElxwJQ.png"/></a></figure><figure class="pb kn ph pd pe pf pg paragraph-image"><a href="https://towardsdatascience.com/@jinglesnote"><img src="../Images/ed2857d42868ce52ed8f717376bc4cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*i2NzU4j49rZ36Mxz4gp4Sg.png"/></a></figure><figure class="pb kn ph pd pe pf pg paragraph-image"><a href="https://jingles.substack.com/subscribe"><img src="../Images/c6faf13786230940c1756ff46938c471.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*oENDSDMTwXi2CJdO1gryug.png"/></a></figure></div><p id="6ed4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下是 UNet++架构的 PyTorch 代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure></div></div>    
</body>
</html>