<html>
<head>
<title>Residual Networks: Implementing ResNet in Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">剩余网络:在 Pytorch 中实现 ResNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278?source=collection_archive---------1-----------------------#2019-07-03">https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278?source=collection_archive---------1-----------------------#2019-07-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/a476c54194e79e53c6dd27d6f9f6b646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1JCuiYW2fTqbQVrjMXAFUA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Image by the Author</figcaption></figure><div class=""/><p id="d1e5" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj"> <em class="ld">我在</em></strong><a class="ae le" href="https://www.linkedin.com/in/francesco-saverio-zuppichini-94659a150/?originalSubdomain=ch" rel="noopener ugc nofollow" target="_blank"><strong class="kh jj"><em class="ld">LinkedIn</em></strong></a><strong class="kh jj"><em class="ld">，快来打个招呼</em> </strong>👋</p><p id="c9db" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">今天我们将在<a class="ae le" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>中实现何等人(微软研究院)著名的 ResNet。它在 ILSVRC 2015 分类任务中获得第一名。</p><p id="5df0" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj"> ResNet 及其所有变种已经在我的库中实现</strong> <a class="ae le" href="https://github.com/FrancescoSaverioZuppichini/glasses" rel="noopener ugc nofollow" target="_blank"> <strong class="kh jj">眼镜</strong> </a></p><p id="0404" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">代码是<a class="ae le" href="https://github.com/FrancescoSaverioZuppichini/ResNet" rel="noopener ugc nofollow" target="_blank">这里</a>，这篇文章的互动版可以在<a class="ae le" href="https://github.com/FrancescoSaverioZuppichini/ResNet/blob/master/ResNet.ipynb" rel="noopener ugc nofollow" target="_blank">这里下载</a>原文可以从<a class="ae le" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">这里阅读</a>(很容易理解)附加材料可以在这个<a class="ae le" href="https://www.quora.com/" rel="noopener ugc nofollow" target="_blank"> quora 答案</a>中找到</p><figure class="lg lh li lj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lf"><img src="../Images/bd90dfc8a364eb8cdb7ff9e44356f967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uhZbfvWvZVjn8GRoTCj2GA.png"/></div></div></figure><h1 id="842f" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">介绍</h1><p id="f748" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">这不是一篇技术文章，我也没有聪明到比原作者更好地解释剩余连接。因此我们将仅限于快速概述。</p><p id="ff46" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="ld">越深的神经网络越难训练。</em>为什么？深层网络的一个大问题是消失梯度问题。基本上是越深越难练。</p><p id="c488" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了解决这个问题，作者建议使用对前一层的引用来计算给定层的输出。在 ResNet 中，上一层的输出(称为残差)被添加到当前层的输出中。下图显示了这一操作</p><p id="d34a" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们将使用大多数数据科学家都不知道的东西:面向对象编程，使我们的实现尽可能具有可伸缩性</p><h1 id="3d04" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">基本块</h1><p id="04de" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">好的，首先要考虑我们需要什么。首先，我们必须有一个卷积层，因为 PyTorch 在 Conv2d 中没有“自动”填充，我们必须自己编码！</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="d886" class="mu ll ji mq b gy mv mw l mx my">Conv2dAuto(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span></pre><p id="1eb8" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我们使用<code class="fe mz na nb mq b">ModuleDict</code>创建一个具有不同激活功能的字典，这在以后会很方便。</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="7215" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果你对<code class="fe mz na nb mq b">ModuleDict</code>不熟悉，我建议阅读我以前的文章<a class="ae le" rel="noopener" target="_blank" href="/pytorch-how-and-when-to-use-module-sequential-modulelist-and-moduledict-7a54597b5f17"> Pytorch:如何以及何时使用模块、顺序、模块列表和模块指令</a></p><h1 id="765e" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">残余块</h1><p id="f20a" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">创建干净的代码必须考虑应用程序的主要构件，或者在我们的例子中是网络的主要构件。残差块采用带有<code class="fe mz na nb mq b">in_channels</code>的输入，应用卷积层的一些块将其减少到<code class="fe mz na nb mq b">out_channels</code>，并将其加起来作为原始输入。如果它们的大小不匹配，那么输入进入<code class="fe mz na nb mq b">identity</code>。我们可以抽象这个过程，并创建一个可扩展的接口。</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="dad0" class="mu ll ji mq b gy mv mw l mx my">ResidualBlock(<br/>  (blocks): Identity()<br/>  (activate): ReLU(inplace)<br/>  (shortcut): Identity()<br/>)</span></pre><p id="b23b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们用一个 1 的虚拟向量来测试它，我们应该得到一个 2 的向量</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="bfe8" class="mu ll ji mq b gy mv mw l mx my">tensor([[[[2.]]]])</span></pre><p id="22f7" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在 ResNet 中，每个块都有一个扩展参数，以便在需要时增加<code class="fe mz na nb mq b">out_channels</code>。同样，身份被定义为一个卷积，后跟一个 BatchNorm 层，这被称为<code class="fe mz na nb mq b">shortcut</code>。然后，我们可以扩展<code class="fe mz na nb mq b">ResidualBlock</code>并定义<code class="fe mz na nb mq b">shortcut</code>函数。</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="da8e" class="mu ll ji mq b gy mv mw l mx my">ResNetResidualBlock(<br/>  (blocks): Identity()<br/>  (activate): ReLU(inplace)<br/>  (shortcut): Sequential(<br/>    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>  )<br/>)</span></pre><h1 id="80ab" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">基本块</h1><p id="d51c" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">一个基本的 ResNet 块由两层<code class="fe mz na nb mq b">3x3</code> conv/batchnorm/relu 组成。图中，线条代表剩余运算。虚线表示应用了快捷方式来匹配输入和输出维度。</p><figure class="lg lh li lj gt iv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/e6b778594d916d25df48c9a86e8c0f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*E6ptpDYIVmdlJO6hC-KzCQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Basic ResNet Block</figcaption></figure><p id="6e94" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们首先创建一个方便的函数来堆叠一个 conv 和 batchnorm 层</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="0ecf" class="mu ll ji mq b gy mv mw l mx my">ResNetBasicBlock(<br/>  (blocks): Sequential(<br/>    (0): Sequential(<br/>      (0): Conv2dAuto(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (1): ReLU(inplace)<br/>    (2): Sequential(<br/>      (0): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (activate): ReLU(inplace)<br/>  (shortcut): Sequential(<br/>    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>  )<br/>)</span></pre><h1 id="6e95" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">瓶颈</h1><p id="d499" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">为了增加网络深度，同时保持参数大小尽可能低，作者定义了一个瓶颈块，即“三层是 1x1、3x3 和 1x1 卷积，其中 1×1 层负责减少然后增加(恢复)维度，而 3×3 层是具有较小输入/输出维度的瓶颈。”我们可以扩展<code class="fe mz na nb mq b">ResNetResidualBlock</code>并创建这些块。</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="8ef9" class="mu ll ji mq b gy mv mw l mx my">ResNetBottleNeckBlock(<br/>  (blocks): Sequential(<br/>    (0): Sequential(<br/>      (0): Conv2dAuto(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (1): ReLU(inplace)<br/>    (2): Sequential(<br/>      (0): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (3): ReLU(inplace)<br/>    (4): Sequential(<br/>      (0): Conv2dAuto(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (activate): ReLU(inplace)<br/>  (shortcut): Sequential(<br/>    (0): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>  )<br/>)</span></pre><h1 id="f50f" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">层</h1><p id="c4e9" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">ResNet 的层由一个接一个堆叠的相同块组成。</p><figure class="lg lh li lj gt iv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/fb543a9524e6be4f3fa5211d23feedf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*6_ex5WTe7ziTK-A5ix1HOQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">ResNet Layer</figcaption></figure><p id="3934" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们可以通过一个接一个地粘贴<code class="fe mz na nb mq b">n</code>块来轻松定义它，只需记住第一个卷积块的步长为 2，因为“我们通过步长为 2 的卷积层直接执行下采样”。</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="f355" class="mu ll ji mq b gy mv mw l mx my">torch.Size([1, 128, 24, 24])</span></pre><h1 id="01f4" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">编码器</h1><p id="b18d" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">类似地，编码器由特征尺寸逐渐增加的多层组成。</p><figure class="lg lh li lj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nd"><img src="../Images/a2ece30661ab2c61d7092da7e8f84de9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QEVnqrWM2ydj29QTPcugiA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">ResNet Encoder</figcaption></figure><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><h1 id="0dc4" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">解码器</h1><p id="5983" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">解码器是我们创建完整网络所需的最后一块。它是一个完全连接的层，将网络学习到的特征映射到它们各自的类。很容易，我们可以将其定义为:</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><h1 id="b49f" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">雷斯内特</h1><p id="9fed" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">最后，我们可以将所有的部分放在一起，创建最终的模型。</p><figure class="lg lh li lj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lf"><img src="../Images/bd90dfc8a364eb8cdb7ff9e44356f967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uhZbfvWvZVjn8GRoTCj2GA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">ResNet34</figcaption></figure><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="0bc4" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们现在可以定义作者提出的五个模型，<code class="fe mz na nb mq b">resnet18,34,50,101,152</code></p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="1a85" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们用<a class="ae le" href="https://github.com/sksq96/pytorch-summary" rel="noopener ugc nofollow" target="_blank">火炬概要</a>来测试这个模型</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="bb98" class="mu ll ji mq b gy mv mw l mx my">----------------------------------------------------------------<br/>        Layer (type)               Output Shape         Param #<br/>================================================================<br/>            Conv2d-1         [-1, 64, 112, 112]           9,408<br/>       BatchNorm2d-2         [-1, 64, 112, 112]             128<br/>              ReLU-3         [-1, 64, 112, 112]               0<br/>         MaxPool2d-4           [-1, 64, 56, 56]               0<br/>        Conv2dAuto-5           [-1, 64, 56, 56]          36,864<br/>       BatchNorm2d-6           [-1, 64, 56, 56]             128<br/>              ReLU-7           [-1, 64, 56, 56]               0<br/>        Conv2dAuto-8           [-1, 64, 56, 56]          36,864<br/>       BatchNorm2d-9           [-1, 64, 56, 56]             128<br/>             ReLU-10           [-1, 64, 56, 56]               0<br/> ResNetBasicBlock-11           [-1, 64, 56, 56]               0<br/>       Conv2dAuto-12           [-1, 64, 56, 56]          36,864<br/>      BatchNorm2d-13           [-1, 64, 56, 56]             128<br/>             ReLU-14           [-1, 64, 56, 56]               0<br/>       Conv2dAuto-15           [-1, 64, 56, 56]          36,864<br/>      BatchNorm2d-16           [-1, 64, 56, 56]             128<br/>             ReLU-17           [-1, 64, 56, 56]               0<br/> ResNetBasicBlock-18           [-1, 64, 56, 56]               0<br/>      ResNetLayer-19           [-1, 64, 56, 56]               0<br/>           Conv2d-20          [-1, 128, 28, 28]           8,192<br/>      BatchNorm2d-21          [-1, 128, 28, 28]             256<br/>       Conv2dAuto-22          [-1, 128, 28, 28]          73,728<br/>      BatchNorm2d-23          [-1, 128, 28, 28]             256<br/>             ReLU-24          [-1, 128, 28, 28]               0<br/>       Conv2dAuto-25          [-1, 128, 28, 28]         147,456<br/>      BatchNorm2d-26          [-1, 128, 28, 28]             256<br/>             ReLU-27          [-1, 128, 28, 28]               0<br/> ResNetBasicBlock-28          [-1, 128, 28, 28]               0<br/>       Conv2dAuto-29          [-1, 128, 28, 28]         147,456<br/>      BatchNorm2d-30          [-1, 128, 28, 28]             256<br/>             ReLU-31          [-1, 128, 28, 28]               0<br/>       Conv2dAuto-32          [-1, 128, 28, 28]         147,456<br/>      BatchNorm2d-33          [-1, 128, 28, 28]             256<br/>             ReLU-34          [-1, 128, 28, 28]               0<br/> ResNetBasicBlock-35          [-1, 128, 28, 28]               0<br/>      ResNetLayer-36          [-1, 128, 28, 28]               0<br/>           Conv2d-37          [-1, 256, 14, 14]          32,768<br/>      BatchNorm2d-38          [-1, 256, 14, 14]             512<br/>       Conv2dAuto-39          [-1, 256, 14, 14]         294,912<br/>      BatchNorm2d-40          [-1, 256, 14, 14]             512<br/>             ReLU-41          [-1, 256, 14, 14]               0<br/>       Conv2dAuto-42          [-1, 256, 14, 14]         589,824<br/>      BatchNorm2d-43          [-1, 256, 14, 14]             512<br/>             ReLU-44          [-1, 256, 14, 14]               0<br/> ResNetBasicBlock-45          [-1, 256, 14, 14]               0<br/>       Conv2dAuto-46          [-1, 256, 14, 14]         589,824<br/>      BatchNorm2d-47          [-1, 256, 14, 14]             512<br/>             ReLU-48          [-1, 256, 14, 14]               0<br/>       Conv2dAuto-49          [-1, 256, 14, 14]         589,824<br/>      BatchNorm2d-50          [-1, 256, 14, 14]             512<br/>             ReLU-51          [-1, 256, 14, 14]               0<br/> ResNetBasicBlock-52          [-1, 256, 14, 14]               0<br/>      ResNetLayer-53          [-1, 256, 14, 14]               0<br/>           Conv2d-54            [-1, 512, 7, 7]         131,072<br/>      BatchNorm2d-55            [-1, 512, 7, 7]           1,024<br/>       Conv2dAuto-56            [-1, 512, 7, 7]       1,179,648<br/>      BatchNorm2d-57            [-1, 512, 7, 7]           1,024<br/>             ReLU-58            [-1, 512, 7, 7]               0<br/>       Conv2dAuto-59            [-1, 512, 7, 7]       2,359,296<br/>      BatchNorm2d-60            [-1, 512, 7, 7]           1,024<br/>             ReLU-61            [-1, 512, 7, 7]               0<br/> ResNetBasicBlock-62            [-1, 512, 7, 7]               0<br/>       Conv2dAuto-63            [-1, 512, 7, 7]       2,359,296<br/>      BatchNorm2d-64            [-1, 512, 7, 7]           1,024<br/>             ReLU-65            [-1, 512, 7, 7]               0<br/>       Conv2dAuto-66            [-1, 512, 7, 7]       2,359,296<br/>      BatchNorm2d-67            [-1, 512, 7, 7]           1,024<br/>             ReLU-68            [-1, 512, 7, 7]               0<br/> ResNetBasicBlock-69            [-1, 512, 7, 7]               0<br/>      ResNetLayer-70            [-1, 512, 7, 7]               0<br/>    ResNetEncoder-71            [-1, 512, 7, 7]               0<br/>AdaptiveAvgPool2d-72            [-1, 512, 1, 1]               0<br/>           Linear-73                 [-1, 1000]         513,000<br/>    ResnetDecoder-74                 [-1, 1000]               0<br/>================================================================<br/>Total params: 11,689,512<br/>Trainable params: 11,689,512<br/>Non-trainable params: 0<br/>----------------------------------------------------------------<br/>Input size (MB): 0.57<br/>Forward/backward pass size (MB): 65.86<br/>Params size (MB): 44.59<br/>Estimated Total Size (MB): 111.03<br/>----------------------------------------------------------------</span></pre><p id="cbce" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了检查正确性，让我们看看原始实现的参数数量</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="3685" class="mu ll ji mq b gy mv mw l mx my">----------------------------------------------------------------<br/>        Layer (type)               Output Shape         Param #<br/>================================================================<br/>            Conv2d-1         [-1, 64, 112, 112]           9,408<br/>       BatchNorm2d-2         [-1, 64, 112, 112]             128<br/>              ReLU-3         [-1, 64, 112, 112]               0<br/>         MaxPool2d-4           [-1, 64, 56, 56]               0<br/>            Conv2d-5           [-1, 64, 56, 56]          36,864<br/>       BatchNorm2d-6           [-1, 64, 56, 56]             128<br/>              ReLU-7           [-1, 64, 56, 56]               0<br/>            Conv2d-8           [-1, 64, 56, 56]          36,864<br/>       BatchNorm2d-9           [-1, 64, 56, 56]             128<br/>             ReLU-10           [-1, 64, 56, 56]               0<br/>       BasicBlock-11           [-1, 64, 56, 56]               0<br/>           Conv2d-12           [-1, 64, 56, 56]          36,864<br/>      BatchNorm2d-13           [-1, 64, 56, 56]             128<br/>             ReLU-14           [-1, 64, 56, 56]               0<br/>           Conv2d-15           [-1, 64, 56, 56]          36,864<br/>      BatchNorm2d-16           [-1, 64, 56, 56]             128<br/>             ReLU-17           [-1, 64, 56, 56]               0<br/>       BasicBlock-18           [-1, 64, 56, 56]               0<br/>           Conv2d-19          [-1, 128, 28, 28]          73,728<br/>      BatchNorm2d-20          [-1, 128, 28, 28]             256<br/>             ReLU-21          [-1, 128, 28, 28]               0<br/>           Conv2d-22          [-1, 128, 28, 28]         147,456<br/>      BatchNorm2d-23          [-1, 128, 28, 28]             256<br/>           Conv2d-24          [-1, 128, 28, 28]           8,192<br/>      BatchNorm2d-25          [-1, 128, 28, 28]             256<br/>             ReLU-26          [-1, 128, 28, 28]               0<br/>       BasicBlock-27          [-1, 128, 28, 28]               0<br/>           Conv2d-28          [-1, 128, 28, 28]         147,456<br/>      BatchNorm2d-29          [-1, 128, 28, 28]             256<br/>             ReLU-30          [-1, 128, 28, 28]               0<br/>           Conv2d-31          [-1, 128, 28, 28]         147,456<br/>      BatchNorm2d-32          [-1, 128, 28, 28]             256<br/>             ReLU-33          [-1, 128, 28, 28]               0<br/>       BasicBlock-34          [-1, 128, 28, 28]               0<br/>           Conv2d-35          [-1, 256, 14, 14]         294,912<br/>      BatchNorm2d-36          [-1, 256, 14, 14]             512<br/>             ReLU-37          [-1, 256, 14, 14]               0<br/>           Conv2d-38          [-1, 256, 14, 14]         589,824<br/>      BatchNorm2d-39          [-1, 256, 14, 14]             512<br/>           Conv2d-40          [-1, 256, 14, 14]          32,768<br/>      BatchNorm2d-41          [-1, 256, 14, 14]             512<br/>             ReLU-42          [-1, 256, 14, 14]               0<br/>       BasicBlock-43          [-1, 256, 14, 14]               0<br/>           Conv2d-44          [-1, 256, 14, 14]         589,824<br/>      BatchNorm2d-45          [-1, 256, 14, 14]             512<br/>             ReLU-46          [-1, 256, 14, 14]               0<br/>           Conv2d-47          [-1, 256, 14, 14]         589,824<br/>      BatchNorm2d-48          [-1, 256, 14, 14]             512<br/>             ReLU-49          [-1, 256, 14, 14]               0<br/>       BasicBlock-50          [-1, 256, 14, 14]               0<br/>           Conv2d-51            [-1, 512, 7, 7]       1,179,648<br/>      BatchNorm2d-52            [-1, 512, 7, 7]           1,024<br/>             ReLU-53            [-1, 512, 7, 7]               0<br/>           Conv2d-54            [-1, 512, 7, 7]       2,359,296<br/>      BatchNorm2d-55            [-1, 512, 7, 7]           1,024<br/>           Conv2d-56            [-1, 512, 7, 7]         131,072<br/>      BatchNorm2d-57            [-1, 512, 7, 7]           1,024<br/>             ReLU-58            [-1, 512, 7, 7]               0<br/>       BasicBlock-59            [-1, 512, 7, 7]               0<br/>           Conv2d-60            [-1, 512, 7, 7]       2,359,296<br/>      BatchNorm2d-61            [-1, 512, 7, 7]           1,024<br/>             ReLU-62            [-1, 512, 7, 7]               0<br/>           Conv2d-63            [-1, 512, 7, 7]       2,359,296<br/>      BatchNorm2d-64            [-1, 512, 7, 7]           1,024<br/>             ReLU-65            [-1, 512, 7, 7]               0<br/>       BasicBlock-66            [-1, 512, 7, 7]               0<br/>AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0<br/>           Linear-68                 [-1, 1000]         513,000<br/>================================================================<br/>Total params: 11,689,512<br/>Trainable params: 11,689,512<br/>Non-trainable params: 0<br/>----------------------------------------------------------------<br/>Input size (MB): 0.57<br/>Forward/backward pass size (MB): 62.79<br/>Params size (MB): 44.59<br/>Estimated Total Size (MB): 107.96<br/>----------------------------------------------------------------</span></pre><p id="f66b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">是一样的！</p><h1 id="54db" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">用户化</h1><p id="6c87" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">面向对象编程的一个优点是我们可以很容易地定制我们的网络。</p><h1 id="fc66" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">改变街区</h1><p id="7a47" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">如果我们想使用不同的基本块呢？也许我们只想要一个 3x3 的 conv，也许还要退学？。在这种情况下，我们可以子类化<code class="fe mz na nb mq b">ResNetResidualBlock</code>并改变<code class="fe mz na nb mq b">.blocks</code>字段！</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="5969" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们把这个新的区块交给<code class="fe mz na nb mq b">resnet18</code>，创建一个新的架构！</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="ccb9" class="mu ll ji mq b gy mv mw l mx my">----------------------------------------------------------------<br/>        Layer (type)               Output Shape         Param #<br/>================================================================<br/>            Conv2d-1         [-1, 64, 112, 112]           9,408<br/>       BatchNorm2d-2         [-1, 64, 112, 112]             128<br/>              ReLU-3         [-1, 64, 112, 112]               0<br/>         MaxPool2d-4           [-1, 64, 56, 56]               0<br/>        Conv2dAuto-5           [-1, 64, 56, 56]          36,864<br/>         Dropout2d-6           [-1, 64, 56, 56]               0<br/>              ReLU-7           [-1, 64, 56, 56]               0<br/>              ReLU-8           [-1, 64, 56, 56]               0<br/>AnOtherResNetBlock-9           [-1, 64, 56, 56]               0<br/>       Conv2dAuto-10           [-1, 64, 56, 56]          36,864<br/>        Dropout2d-11           [-1, 64, 56, 56]               0<br/>             ReLU-12           [-1, 64, 56, 56]               0<br/>             ReLU-13           [-1, 64, 56, 56]               0<br/>AnOtherResNetBlock-14           [-1, 64, 56, 56]               0<br/>      ResNetLayer-15           [-1, 64, 56, 56]               0<br/>           Conv2d-16          [-1, 128, 28, 28]           8,192<br/>      BatchNorm2d-17          [-1, 128, 28, 28]             256<br/>       Conv2dAuto-18          [-1, 128, 28, 28]          73,728<br/>        Dropout2d-19          [-1, 128, 28, 28]               0<br/>             ReLU-20          [-1, 128, 28, 28]               0<br/>             ReLU-21          [-1, 128, 28, 28]               0<br/>AnOtherResNetBlock-22          [-1, 128, 28, 28]               0<br/>       Conv2dAuto-23          [-1, 128, 28, 28]         147,456<br/>        Dropout2d-24          [-1, 128, 28, 28]               0<br/>             ReLU-25          [-1, 128, 28, 28]               0<br/>             ReLU-26          [-1, 128, 28, 28]               0<br/>AnOtherResNetBlock-27          [-1, 128, 28, 28]               0<br/>      ResNetLayer-28          [-1, 128, 28, 28]               0<br/>           Conv2d-29          [-1, 256, 14, 14]          32,768<br/>      BatchNorm2d-30          [-1, 256, 14, 14]             512<br/>       Conv2dAuto-31          [-1, 256, 14, 14]         294,912<br/>        Dropout2d-32          [-1, 256, 14, 14]               0<br/>             ReLU-33          [-1, 256, 14, 14]               0<br/>             ReLU-34          [-1, 256, 14, 14]               0<br/>AnOtherResNetBlock-35          [-1, 256, 14, 14]               0<br/>       Conv2dAuto-36          [-1, 256, 14, 14]         589,824<br/>        Dropout2d-37          [-1, 256, 14, 14]               0<br/>             ReLU-38          [-1, 256, 14, 14]               0<br/>             ReLU-39          [-1, 256, 14, 14]               0<br/>AnOtherResNetBlock-40          [-1, 256, 14, 14]               0<br/>      ResNetLayer-41          [-1, 256, 14, 14]               0<br/>           Conv2d-42            [-1, 512, 7, 7]         131,072<br/>      BatchNorm2d-43            [-1, 512, 7, 7]           1,024<br/>       Conv2dAuto-44            [-1, 512, 7, 7]       1,179,648<br/>        Dropout2d-45            [-1, 512, 7, 7]               0<br/>             ReLU-46            [-1, 512, 7, 7]               0<br/>             ReLU-47            [-1, 512, 7, 7]               0<br/>AnOtherResNetBlock-48            [-1, 512, 7, 7]               0<br/>       Conv2dAuto-49            [-1, 512, 7, 7]       2,359,296<br/>        Dropout2d-50            [-1, 512, 7, 7]               0<br/>             ReLU-51            [-1, 512, 7, 7]               0<br/>             ReLU-52            [-1, 512, 7, 7]               0<br/>AnOtherResNetBlock-53            [-1, 512, 7, 7]               0<br/>      ResNetLayer-54            [-1, 512, 7, 7]               0<br/>    ResNetEncoder-55            [-1, 512, 7, 7]               0<br/>AdaptiveAvgPool2d-56            [-1, 512, 1, 1]               0<br/>           Linear-57                 [-1, 1000]         513,000<br/>    ResnetDecoder-58                 [-1, 1000]               0<br/>================================================================<br/>Total params: 5,414,952<br/>Trainable params: 5,414,952<br/>Non-trainable params: 0<br/>----------------------------------------------------------------<br/>Input size (MB): 0.57<br/>Forward/backward pass size (MB): 54.38<br/>Params size (MB): 20.66<br/>Estimated Total Size (MB): 75.61<br/>----------------------------------------------------------------</span></pre><h1 id="a99e" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">改变激活功能</h1><p id="7f33" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">容易的事</p><figure class="lg lh li lj gt iv"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="lg lh li lj gt mp mq mr ms aw mt bi"><span id="4bd4" class="mu ll ji mq b gy mv mw l mx my">----------------------------------------------------------------<br/>        Layer (type)               Output Shape         Param #<br/>================================================================<br/>            Conv2d-1         [-1, 64, 112, 112]           9,408<br/>       BatchNorm2d-2         [-1, 64, 112, 112]             128<br/>         LeakyReLU-3         [-1, 64, 112, 112]               0<br/>         MaxPool2d-4           [-1, 64, 56, 56]               0<br/>        Conv2dAuto-5           [-1, 64, 56, 56]          36,864<br/>       BatchNorm2d-6           [-1, 64, 56, 56]             128<br/>         LeakyReLU-7           [-1, 64, 56, 56]               0<br/>        Conv2dAuto-8           [-1, 64, 56, 56]          36,864<br/>       BatchNorm2d-9           [-1, 64, 56, 56]             128<br/>        LeakyReLU-10           [-1, 64, 56, 56]               0<br/> ResNetBasicBlock-11           [-1, 64, 56, 56]               0<br/>       Conv2dAuto-12           [-1, 64, 56, 56]          36,864<br/>      BatchNorm2d-13           [-1, 64, 56, 56]             128<br/>        LeakyReLU-14           [-1, 64, 56, 56]               0<br/>       Conv2dAuto-15           [-1, 64, 56, 56]          36,864<br/>      BatchNorm2d-16           [-1, 64, 56, 56]             128<br/>        LeakyReLU-17           [-1, 64, 56, 56]               0<br/> ResNetBasicBlock-18           [-1, 64, 56, 56]               0<br/>      ResNetLayer-19           [-1, 64, 56, 56]               0<br/>           Conv2d-20          [-1, 128, 28, 28]           8,192<br/>      BatchNorm2d-21          [-1, 128, 28, 28]             256<br/>       Conv2dAuto-22          [-1, 128, 28, 28]          73,728<br/>      BatchNorm2d-23          [-1, 128, 28, 28]             256<br/>        LeakyReLU-24          [-1, 128, 28, 28]               0<br/>       Conv2dAuto-25          [-1, 128, 28, 28]         147,456<br/>      BatchNorm2d-26          [-1, 128, 28, 28]             256<br/>        LeakyReLU-27          [-1, 128, 28, 28]               0<br/> ResNetBasicBlock-28          [-1, 128, 28, 28]               0<br/>       Conv2dAuto-29          [-1, 128, 28, 28]         147,456<br/>      BatchNorm2d-30          [-1, 128, 28, 28]             256<br/>        LeakyReLU-31          [-1, 128, 28, 28]               0<br/>       Conv2dAuto-32          [-1, 128, 28, 28]         147,456<br/>      BatchNorm2d-33          [-1, 128, 28, 28]             256<br/>        LeakyReLU-34          [-1, 128, 28, 28]               0<br/> ResNetBasicBlock-35          [-1, 128, 28, 28]               0<br/>      ResNetLayer-36          [-1, 128, 28, 28]               0<br/>           Conv2d-37          [-1, 256, 14, 14]          32,768<br/>      BatchNorm2d-38          [-1, 256, 14, 14]             512<br/>       Conv2dAuto-39          [-1, 256, 14, 14]         294,912<br/>      BatchNorm2d-40          [-1, 256, 14, 14]             512<br/>        LeakyReLU-41          [-1, 256, 14, 14]               0<br/>       Conv2dAuto-42          [-1, 256, 14, 14]         589,824<br/>      BatchNorm2d-43          [-1, 256, 14, 14]             512<br/>        LeakyReLU-44          [-1, 256, 14, 14]               0<br/> ResNetBasicBlock-45          [-1, 256, 14, 14]               0<br/>       Conv2dAuto-46          [-1, 256, 14, 14]         589,824<br/>      BatchNorm2d-47          [-1, 256, 14, 14]             512<br/>        LeakyReLU-48          [-1, 256, 14, 14]               0<br/>       Conv2dAuto-49          [-1, 256, 14, 14]         589,824<br/>      BatchNorm2d-50          [-1, 256, 14, 14]             512<br/>        LeakyReLU-51          [-1, 256, 14, 14]               0<br/> ResNetBasicBlock-52          [-1, 256, 14, 14]               0<br/>      ResNetLayer-53          [-1, 256, 14, 14]               0<br/>           Conv2d-54            [-1, 512, 7, 7]         131,072<br/>      BatchNorm2d-55            [-1, 512, 7, 7]           1,024<br/>       Conv2dAuto-56            [-1, 512, 7, 7]       1,179,648<br/>      BatchNorm2d-57            [-1, 512, 7, 7]           1,024<br/>        LeakyReLU-58            [-1, 512, 7, 7]               0<br/>       Conv2dAuto-59            [-1, 512, 7, 7]       2,359,296<br/>      BatchNorm2d-60            [-1, 512, 7, 7]           1,024<br/>        LeakyReLU-61            [-1, 512, 7, 7]               0<br/> ResNetBasicBlock-62            [-1, 512, 7, 7]               0<br/>       Conv2dAuto-63            [-1, 512, 7, 7]       2,359,296<br/>      BatchNorm2d-64            [-1, 512, 7, 7]           1,024<br/>        LeakyReLU-65            [-1, 512, 7, 7]               0<br/>       Conv2dAuto-66            [-1, 512, 7, 7]       2,359,296<br/>      BatchNorm2d-67            [-1, 512, 7, 7]           1,024<br/>        LeakyReLU-68            [-1, 512, 7, 7]               0<br/> ResNetBasicBlock-69            [-1, 512, 7, 7]               0<br/>      ResNetLayer-70            [-1, 512, 7, 7]               0<br/>    ResNetEncoder-71            [-1, 512, 7, 7]               0<br/>AdaptiveAvgPool2d-72            [-1, 512, 1, 1]               0<br/>           Linear-73                 [-1, 1000]         513,000<br/>    ResnetDecoder-74                 [-1, 1000]               0<br/>================================================================<br/>Total params: 11,689,512<br/>Trainable params: 11,689,512<br/>Non-trainable params: 0<br/>----------------------------------------------------------------<br/>Input size (MB): 0.57<br/>Forward/backward pass size (MB): 65.86<br/>Params size (MB): 44.59<br/>Estimated Total Size (MB): 111.03<br/>----------------------------------------------------------------</span></pre><h1 id="21ad" class="lk ll ji bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">结论</h1><p id="1274" class="pw-post-body-paragraph kf kg ji kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">在本文中，我们看到了如何以一种良好的、可伸缩的和可定制的方式实现 ResNet。在下一篇文章中，我们将进一步扩展这个架构，训练它并使用另外两个技巧:预激活和挤压和激励。</p><p id="fa49" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这里所有的代码都是<a class="ae le" href="https://github.com/FrancescoSaverioZuppichini/ResNet" rel="noopener ugc nofollow" target="_blank">这里是</a></p><p id="bd9c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果你对理解更好的神经网络感兴趣，我建议你读一读我写的另一篇文章</p><div class="is it gp gr iu ne"><a rel="noopener follow" target="_blank" href="/a-journey-into-convolutional-neural-network-visualization-1abc71605209"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jj gy z fp nj fr fs nk fu fw jh bi translated">卷积神经网络可视化之旅</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">弗朗西斯科·萨维里奥·祖皮奇尼</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ja ne"/></div></div></a></div><p id="c0eb" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">想知道如何实现 RepVGG？ResNet 的更好版本？</p><div class="is it gp gr iu ne"><a rel="noopener follow" target="_blank" href="/implementing-repvgg-in-pytorch-fc8562be58f9"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jj gy z fp nj fr fs nk fu fw jh bi translated">在 PyTorch 中实现 RepVGG</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">让您的 CNN 速度快 100 倍以上</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="nt l np nq nr nn ns ja ne"/></div></div></a></div><p id="f735" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">了解 PyTorch 中的非最大抑制</p><div class="is it gp gr iu ne"><a href="https://medium.com/@FrancescoZ/non-max-suppression-nms-in-pytorch-35f77397a0aa" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jj gy z fp nj fr fs nk fu fw jh bi translated">PyTorch 中的非最大抑制(NMS)</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">在 PyTorch 中实现非最大抑制</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">medium.com</p></div></div><div class="nn l"><div class="nu l np nq nr nn ns ja ne"/></div></div></a></div><p id="a9f4" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">感谢您的阅读</p><p id="d2cf" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">弗朗西斯科·萨维里奥·祖皮奇尼</p></div></div>    
</body>
</html>