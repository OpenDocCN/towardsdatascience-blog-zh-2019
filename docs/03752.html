<html>
<head>
<title>LSTM Recurrent Neural Network Keras Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM 递归神经网络实例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-recurrent-neural-networks-and-long-short-term-memory-lstm-python-keras-example-86001ceaaebc?source=collection_archive---------1-----------------------#2019-06-14">https://towardsdatascience.com/machine-learning-recurrent-neural-networks-and-long-short-term-memory-lstm-python-keras-example-86001ceaaebc?source=collection_archive---------1-----------------------#2019-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="ir is gp gr it iu gh gi paragraph-image"><div class="ab gu cl iv"><img src="../Images/6b60f5385a7256264bddf46f4aadcb98.png" data-original-src="https://miro.medium.com/v2/format:webp/1*6xj691fPWf3S-mWUCbxSJg.jpeg"/></div></figure><div class=""/><p id="693e" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">递归神经网络具有广泛的应用。这些包括时间序列分析、文档分类、语音和声音识别。与前馈<strong class="jz jb"> </strong>人工神经网络相反，递归神经网络做出的预测依赖于先前的预测。</p><p id="06cf" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">具体来说，想象我们决定遵循一个锻炼程序，每天在举重、游泳和瑜伽之间交替进行。然后，我们可以建立一个递归神经网络来预测今天的锻炼，因为我们昨天做了什么。例如，如果我们昨天举重，那么我们今天就去游泳。</p><p id="88ea" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">通常情况下，你将在现实世界中处理的问题是当前状态和其他输入的函数。例如，假设我们每周注册一次曲棍球。如果我们在应该举重的同一天打曲棍球，那么我们可能会决定不去健身房。因此，我们的模型现在必须区分昨天上瑜伽课但我们没有打曲棍球的情况，以及昨天上瑜伽课但我们今天打曲棍球的情况，在这种情况下我们会直接跳到游泳。</p><h1 id="e31e" class="kv kw ja bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">长短期记忆(LSTM)</h1><p id="78ca" class="pw-post-body-paragraph jx jy ja jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">在实践中，我们很少看到使用常规的递归神经网络。递归神经网络有一些缺点，使它们不实用。例如，假设我们增加了一个休息日。休息日只应在运动两天后休。如果我们使用递归神经网络来尝试和预测我们明天将做什么活动，它可能会陷入循环。</p><p id="76f8" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">假设我们有以下场景。</p><ul class=""><li id="5b88" class="ly lz ja jz b ka kb ke kf ki ma km mb kq mc ku md me mf mg bi translated">第一天:举重</li><li id="c1a9" class="ly lz ja jz b ka mh ke mi ki mj km mk kq ml ku md me mf mg bi translated">第二天:游泳</li><li id="3da4" class="ly lz ja jz b ka mh ke mi ki mj km mk kq ml ku md me mf mg bi translated">第三天:在这一点上，我们的模型必须决定我们是否应该休息一天或瑜伽。不幸的是，它只能访问前一天。换句话说，它知道我们昨天游泳了，但不知道我们前一天是否休息过。所以，最后可以预测瑜伽。</li></ul><p id="b835" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">发明 LSTMs 是为了解决这个问题。顾名思义，LSTMs 是有记忆的。就像人类可以在短期记忆中存储大约 7 位信息一样，LSTMs 理论上可以记住几个状态的信息。然而，这提出了一个问题，即他们应该记得多久以前的事情。什么时候信息变得无关紧要了？例如，在我们的练习例子中，我们不应该需要回溯两天以上来决定我们是否应该休息一下。</p><p id="d8b4" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">LSTMs 没有深入研究太多细节，而是通过使用专用神经网络来遗忘和选择信息来解决这个问题。单个 LTSM 层由以特殊方式交互的四个神经网络层组成。</p><figure class="mn mo mp mq gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mm"><img src="../Images/0fef9b70e87b8d107e82b41754aeb311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60YWl37JyS8YEBa1_dSDgw.png"/></div></div></figure><p id="faaa" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果你有兴趣了解更多关于 LSTM 网络的内部情况，我强烈建议你去看看下面的链接。</p><div class="ir is gp gr it mv"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd jb gy z fp na fr fs nb fu fw iz bi translated">了解 LSTM 网络——colah 的博客</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">这些循环使得循环神经网络看起来有点神秘。然而，如果你想得更多一点，事实证明…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">colah.github.io</p></div></div></div></a></div><p id="7690" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">关于自然语言处理的一个棘手的事情是，单词的含义会根据上下文而变化。在情感分析的情况下，我们不能仅仅关闭一些单词的出现，比如<strong class="jz jb"><em class="ne"/></strong>，因为<strong class="jz jb"> <em class="ne"> </em> </strong>如果前面是<strong class="jz jb"> <em class="ne">而不是</em> </strong>就像<strong class="jz jb"> <em class="ne">不好一样，它的意思完全改变了。计算机也很难识别讽刺之类的东西，因为它们需要从字里行间去解读。事实证明，LSTM 网络特别适合解决这类问题，因为它们能记住与问题相关的所有单词。</em></strong></p><h1 id="8e65" class="kv kw ja bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">密码</h1><p id="6cba" class="pw-post-body-paragraph jx jy ja jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">在接下来的部分中，我们回顾了我对 Kaggle 竞赛的解决方案，该竞赛的目标是对电影评论集进行情感分析。我们被要求在 0 到 4 的范围内给每个短语贴上标签。对应于每个标签的情感是:</p><ul class=""><li id="43f4" class="ly lz ja jz b ka kb ke kf ki ma km mb kq mc ku md me mf mg bi translated">0:负</li><li id="70d8" class="ly lz ja jz b ka mh ke mi ki mj km mk kq ml ku md me mf mg bi translated">1:有些消极</li><li id="d74c" class="ly lz ja jz b ka mh ke mi ki mj km mk kq ml ku md me mf mg bi translated">2:中性</li><li id="b7ab" class="ly lz ja jz b ka mh ke mi ki mj km mk kq ml ku md me mf mg bi translated">3:有点积极</li><li id="b9d5" class="ly lz ja jz b ka mh ke mi ki mj km mk kq ml ku md me mf mg bi translated">4:积极</li></ul><p id="f86f" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果您想继续学习，可以从下面的链接获得数据集。</p><div class="ir is gp gr it mv"><a href="https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd jb gy z fp na fr fs nb fu fw iz bi translated">电影评论情感分析(仅限内核)</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">从烂番茄数据集中对句子的情感进行分类</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">www.kaggle.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk iw mv"/></div></div></a></div><p id="b855" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们将使用以下库。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="7b31" class="nq kw ja nm b gy nr ns l nt nu">import numpy as np<br/>import pandas as pd<br/>from matplotlib import pyplot as plt<br/>plt.style.use('dark_background')<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.preprocessing.sequence import pad_sequences<br/>from sklearn.model_selection import train_test_split<br/>from keras.utils import to_categorical<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D</span></pre><p id="a0bd" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">语料库包含超过 150，000 个训练样本。如你所见，有些短语不完整，有些重复。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="a81f" class="nq kw ja nm b gy nr ns l nt nu">df_train = pd.read_csv('train.tsv', sep='\t')</span><span id="3fce" class="nq kw ja nm b gy nv ns l nt nu">print('train set: {0}'.format(df_train.shape))<br/>df_train.head(10)</span></pre><figure class="mn mo mp mq gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nw"><img src="../Images/7251737a012a872d694c46a895b8bff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ccqhhFm8no_weZfQZ27g-w.png"/></div></div></figure><p id="2212" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">测试集包括超过 60，000 个样本。每一行都有一个<code class="fe nx ny nz nm b">PhraseId</code>和一个<code class="fe nx ny nz nm b">SentenceId</code>，Kaggle 将使用这两个值来评估提交文件中模型所做的预测。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="64b8" class="nq kw ja nm b gy nr ns l nt nu">df_test = pd.read_csv('test.tsv', sep='\t')</span><span id="6e8a" class="nq kw ja nm b gy nv ns l nt nu">print('test set: {0}'.format(df_test.shape))<br/>df_test.head(10)</span></pre><figure class="mn mo mp mq gt iu gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/fc74ad160dfcc6d906337bef1c8ae253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*RzRnCPDvBQPMFZuLJKhNTg.png"/></div></figure><p id="6455" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">ASCII 字符最终被计算机解释为十六进制。因此，对计算机来说，“A”和“A”是不同的。因此，我们想把所有的字符都变成小写。因为我们要根据空格把句子分割成单个的单词，所以后面有句号的单词不等同于后面没有句号的单词(<strong class="jz jb"> <em class="ne"> happy)。</em>T24】！= <strong class="jz jb"> <em class="ne">开心</em> </strong>)。此外，收缩将被解释为不同于原来的，这将对模型产生影响(我！=我就是)。因此，我们使用 proceeding 函数替换所有事件。</strong></p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="90d1" class="nq kw ja nm b gy nr ns l nt nu">replace_list = {r"i'm": 'i am',<br/>                r"'re": ' are',<br/>                r"let’s": 'let us',<br/>                r"'s":  ' is',<br/>                r"'ve": ' have',<br/>                r"can't": 'can not',<br/>                r"cannot": 'can not',<br/>                r"shan’t": 'shall not',<br/>                r"n't": ' not',<br/>                r"'d": ' would',<br/>                r"'ll": ' will',<br/>                r"'scuse": 'excuse',<br/>                ',': ' ,',<br/>                '.': ' .',<br/>                '!': ' !',<br/>                '?': ' ?',<br/>                '\s+': ' '}</span><span id="6661" class="nq kw ja nm b gy nv ns l nt nu">def clean_text(text):<br/>    text = text.lower()<br/>    for s in replace_list:<br/>        text = text.replace(s, replace_list[s])<br/>    text = ' '.join(text.split())<br/>    return text</span></pre><p id="a117" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们可以使用<code class="fe nx ny nz nm b">apply</code>将函数应用到序列中的每一行。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="66ab" class="nq kw ja nm b gy nr ns l nt nu">X_train = df_train['Phrase'].apply(lambda p: clean_text(p))</span></pre><p id="6799" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">让我们看看语料库中每个短语的单独长度。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="7c90" class="nq kw ja nm b gy nr ns l nt nu">phrase_len = X_train.apply(lambda p: len(p.split(' ')))<br/>max_phrase_len = phrase_len.max()<br/>print('max phrase len: {0}'.format(max_phrase_len))</span><span id="482e" class="nq kw ja nm b gy nv ns l nt nu">plt.figure(figsize = (10, 8))<br/>plt.hist(phrase_len, alpha = 0.2, density = True)<br/>plt.xlabel('phrase len')<br/>plt.ylabel('probability')<br/>plt.grid(alpha = 0.25)</span></pre><figure class="mn mo mp mq gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ob"><img src="../Images/5256c154f1ca98e287b7626bd9fa1e30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3k0AsKGOAO3GtzSPcRxBg.png"/></div></div></figure><p id="1f41" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">神经网络的所有输入必须长度相同。因此，我们将最长的长度存储为一个变量，稍后我们将使用它来定义模型的输入。</p><p id="8f95" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">接下来，我们为目标标签创建一个单独的数据帧。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="6a54" class="nq kw ja nm b gy nr ns l nt nu">y_train = df_train['Sentiment']</span></pre><p id="523a" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">计算机不理解单词，更不用说句子了，因此，我们使用分词器来解析短语。在指定<code class="fe nx ny nz nm b">num_words</code>时，只保留最常用的<code class="fe nx ny nz nm b">num_words-1</code>字。我们使用过滤器来去除特殊字符。默认情况下，所有标点符号都被删除，将文本转换为空格分隔的单词序列。然后，这些记号被矢量化。我们所说的矢量化是指它们被映射到整数。<code class="fe nx ny nz nm b">0</code>是一个保留索引，不会分配给任何单词。</p><p id="6e66" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><code class="fe nx ny nz nm b">pad_sequence</code>用于确保所有短语长度相同。比<code class="fe nx ny nz nm b">maxlen</code>短的序列在末尾用<code class="fe nx ny nz nm b">value</code>(默认为<code class="fe nx ny nz nm b">0</code>)填充。</p><p id="df77" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">每当我们处理分类数据时，我们都不想让它成为整数，因为模型会将数字越大的样本解释为更有意义。<code class="fe nx ny nz nm b">to_categorical</code>是对数据进行编码的快速而肮脏的方式。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="f603" class="nq kw ja nm b gy nr ns l nt nu">max_words = 8192</span><span id="e639" class="nq kw ja nm b gy nv ns l nt nu">tokenizer = Tokenizer(<br/>    num_words = max_words,<br/>    filters = '"#$%&amp;()*+-/:;&lt;=&gt;@[\]^_`{|}~'<br/>)</span><span id="d03f" class="nq kw ja nm b gy nv ns l nt nu">tokenizer.fit_on_texts(X_train)</span><span id="1090" class="nq kw ja nm b gy nv ns l nt nu">X_train = tokenizer.texts_to_sequences(X_train)<br/>X_train = pad_sequences(X_train, maxlen = max_phrase_len)<br/>y_train = to_categorical(y_train)</span></pre><p id="cc8f" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们为超参数定义变量。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="00e9" class="nq kw ja nm b gy nr ns l nt nu">batch_size = 512<br/>epochs = 8</span></pre><p id="3431" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然后，我们建立我们的模型使用 LSTM 层。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="cf96" class="nq kw ja nm b gy nr ns l nt nu">model_lstm = Sequential()</span><span id="b240" class="nq kw ja nm b gy nv ns l nt nu">model_lstm.add(Embedding(input_dim = max_words, output_dim = 256, input_length = max_phrase_len))<br/>model_lstm.add(SpatialDropout1D(0.3))<br/>model_lstm.add(LSTM(256, dropout = 0.3, recurrent_dropout = 0.3))<br/>model_lstm.add(Dense(256, activation = 'relu'))<br/>model_lstm.add(Dropout(0.3))<br/>model_lstm.add(Dense(5, activation = 'softmax'))</span><span id="5efa" class="nq kw ja nm b gy nv ns l nt nu">model_lstm.compile(<br/>    loss='categorical_crossentropy',<br/>    optimizer='Adam',<br/>    metrics=['accuracy']<br/>)</span></pre><p id="eb33" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果你不明白<strong class="jz jb">嵌入层</strong>在做什么，我建议你去看看我写的一篇关于这个主题的文章。</p><div class="ir is gp gr it mv"><a rel="noopener follow" target="_blank" href="/machine-learning-sentiment-analysis-and-word-embeddings-python-keras-example-4dfb93c5a6cf"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd jb gy z fp na fr fs nb fu fw iz bi translated">机器学习情感分析和单词嵌入 Python Keras 示例</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">机器学习的主要应用之一是情感分析。情绪分析就是判断语气…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="oc l nh ni nj nf nk iw mv"/></div></div></a></div><p id="69f6" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们使用辍学来防止过度拟合。</p><p id="2806" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们留出 10%的数据进行验证。在我们向解决方案迈出一步之前，每个时代都有 512 条评论通过网络。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="eeb1" class="nq kw ja nm b gy nr ns l nt nu">history = model_lstm.fit(<br/>    X_train,<br/>    y_train,<br/>    validation_split = 0.1,<br/>    epochs = 8,<br/>    batch_size = 512<br/>)</span></pre><figure class="mn mo mp mq gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi od"><img src="../Images/4759ff62b8494e26bbd720c5f2afc959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5g9CwBmyFMHWAInsx9n0g.png"/></div></div></figure><p id="5009" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们可以通过使用由<em class="ne"> fit </em>函数返回的历史变量来绘制每个时期的训练和验证准确度和损失。</p><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="a3e7" class="nq kw ja nm b gy nr ns l nt nu">plt.clf()<br/>loss = history.history['loss']<br/>val_loss = history.history['val_loss']<br/>epochs = range(1, len(loss) + 1)<br/>plt.plot(epochs, loss, 'g', label='Training loss')<br/>plt.plot(epochs, val_loss, 'y', label='Validation loss')<br/>plt.title('Training and validation loss')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mn mo mp mq gt iu gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/30d299d6f0a56b875d00d9784d5c0d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*I-mgVe1xBUz1xMgoaxK7Ow.png"/></div></figure><pre class="mn mo mp mq gt nl nm nn no aw np bi"><span id="8cce" class="nq kw ja nm b gy nr ns l nt nu">plt.clf()<br/>acc = history.history['acc']<br/>val_acc = history.history['val_acc']<br/>plt.plot(epochs, acc, 'g', label='Training acc')<br/>plt.plot(epochs, val_acc, 'y', label='Validation acc')<br/>plt.title('Training and validation accuracy')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Accuracy')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mn mo mp mq gt iu gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0bc93b650450e8a0042310dfc4567e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*iTS2UzqvAm5tLgZyHKT6Qg.png"/></div></figure><p id="44f1" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">由于数据集是作为 Kaggle 竞赛的一部分获得的，我们没有得到与测试集中的短语相对应的情感。</p><h1 id="f2e2" class="kv kw ja bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">最后的想法</h1><p id="49cc" class="pw-post-body-paragraph jx jy ja jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">递归神经网络可用于建模任何依赖于其先前状态的现象。我们在本文中提到的例子是语义的例子。换句话说，一个句子的意思随着它的发展而变化。我们可以使用更复杂的方法来捕捉单词之间的相互作用(例如，电影不是好的)，而不是试图基于某个单词(例如，好的)的出现来分类文档。</p></div></div>    
</body>
</html>