<html>
<head>
<title>DCGANs — Generating Dog Images with Tensorflow and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DCGANs —使用 Tensorflow 和 Keras 生成狗的图像</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dcgans-generating-dog-images-with-tensorflow-and-keras-fb51a1071432?source=collection_archive---------3-----------------------#2019-08-29">https://towardsdatascience.com/dcgans-generating-dog-images-with-tensorflow-and-keras-fb51a1071432?source=collection_archive---------3-----------------------#2019-08-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="aa02" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">基于 Kaggle 的生殖狗比赛(2019)</h1><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="kq kr l"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">DCGAN Dog Generation over epochs (~8 hours of runtime on Kaggle)</figcaption></figure><p id="204e" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这篇文章是一个关于<strong class="ky ir"> DCGANs </strong>有效性背后的基本思想的教程，以及一些提高他们性能的方法/技巧。这些方法都是我在<strong class="ky ir"> Kaggle 的</strong> <a class="ae lu" href="https://www.kaggle.com/c/generative-dog-images" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">生财狗大赛</strong> </a> <strong class="ky ir">期间的经验。教程也有，要么在我原来的</strong> <a class="ae lu" href="https://www.kaggle.com/jadeblue/dcgans-and-techniques-to-optimize-them" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">内核</strong> </a> <strong class="ky ir">上笔记本格式，要么在</strong><a class="ae lu" href="https://github.com/JadeBlue96/DCGAN-Dog-Generator" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">GitHub</strong></a><strong class="ky ir">上。</strong></p><p id="6827" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">要在本地或使用<strong class="ky ir"> Colab </strong>运行这个例子，您将需要一个<strong class="ky ir"> Kaggle 帐户</strong>，以便检索其 API 密钥并使用所提供的数据集。完整教程可在<a class="ae lu" href="https://medium.com/@yvettewu.dw/tutorial-kaggle-api-google-colaboratory-1a054a382de0" rel="noopener"> <strong class="ky ir">这里</strong> </a>。</p><h1 id="6c0c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">生成对抗网络</h1><p id="004a" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">与大多数流行的神经网络架构不同，<strong class="ky ir"> GANs </strong>被训练来同时解决两个问题——<strong class="ky ir">辨别</strong>(有效地将真实图像与虚假图像分开)和<strong class="ky ir">“真实的”虚假数据生成</strong>(有效地生成被认为是真实的样本)。正如我们所看到的，这些任务是完全对立的，但是如果我们把它们分成不同的模型，会发生什么呢？</p><p id="1233" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">嗯，这些模型的通称是<strong class="ky ir">发生器(G) </strong>和<strong class="ky ir">鉴别器(D) </strong>，被认为是<strong class="ky ir">甘斯</strong>理论背后的构建模块。</p><p id="e9c9" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir">发生器</strong>网络将简单的随机噪声 N 维向量作为输入，并根据学习的目标分布对其进行转换。其输出也是 N 维的。另一方面，<strong class="ky ir">鉴别器</strong>模拟概率分布函数(类似于分类器)并输出输入图像是真实还是虚假的概率<strong class="ky ir">【0，1】</strong>。考虑到这一点，我们可以定义生成任务的两个主要目标:</p><p id="1ba5" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir"> 1。训练 G 使 D 的最终分类误差最大化。</strong>(使得生成的图像被感知为真实)。</p><p id="9172" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir"> 2。训练 D 以最小化最终分类误差。</strong>(以便正确区分真实数据和虚假数据)。</p><p id="d164" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">为了实现这一点，在<strong class="ky ir">反向传播</strong>期间，<strong class="ky ir"> G </strong>的权重将使用<strong class="ky ir">梯度上升</strong>进行更新，以使误差最大化，而<strong class="ky ir"> D </strong>将使用<strong class="ky ir">梯度下降</strong>将其最小化。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ma"><img src="../Images/9f32261d9656d91213410ab10dd7b317.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*efj9Bgi86fdiyfDq.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">GAN inputs and outputs — (note that the two networks don’t use the true distribution of images directly during training but instead use each other’s outputs to estimate their performance)</figcaption></figure><p id="fc88" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">那么我们如何定义一个<strong class="ky ir">损失函数</strong>来估计两个网络的累积性能呢？嗯，我们可以使用<strong class="ky ir">绝对误差</strong>来估计<strong class="ky ir"> D </strong>的误差，然后我们可以对<strong class="ky ir"> G </strong>重复使用相同的函数，但要最大化:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/a10693fdd125ca5a118b483cf2f1f4de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*Ip0v_LDojrOQTX3ikMojNg.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Mean Absolute Error — the distance between the real and fake distributions of images</figcaption></figure><p id="4dcb" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在这种情况下，<strong class="ky ir"> <em class="mi"> p_t </em> </strong>代表图像的真实分布，而<strong class="ky ir"> <em class="mi"> p_g </em> </strong>则是由<strong class="ky ir"> G </strong>创建的分布。</p><p id="68b6" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们可以观察到这个理论是基于<strong class="ky ir">强化学习</strong>的一些关键概念。它可以被认为是一个两个玩家的极大极小游戏，其中两个玩家相互竞争，从而在各自的任务中逐步提高。</p><p id="2758" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们看到了<strong class="ky ir">甘斯</strong>理论背后的基本思想。现在让我们更进一步，通过应用来自<strong class="ky ir">卷积神经网络</strong>的思想和方法来学习<strong class="ky ir">DC gan</strong>是如何工作的。</p><h1 id="58fc" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">深度卷积生成对抗网络</h1><p id="a839" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated"><strong class="ky ir">DC gan</strong>利用了<strong class="ky ir">CNN</strong>的一些基本原理，并因此成为实践中使用最广泛的架构之一，这是因为它们收敛速度快，并且还因为它们可以非常容易地适应更复杂的变体(使用标签作为条件，应用残差块等等)。以下是<strong class="ky ir"> DCGANs </strong>解决的一些更重要的问题:</p><ul class=""><li id="9a8d" class="mj mk iq ky b kz la ld le lh ml ll mm lp mn lt mo mp mq mr bi translated"><strong class="ky ir"> D 是这样创建的，它基本上解决了一个有监督的图像分类任务。</strong>(对于这种情况狗还是不狗)</li><li id="f9ef" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">GAN 学习的过滤器可用于在生成的图像中绘制特定对象。</strong></li><li id="a7f2" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir"> G 包含可以学习对象非常复杂的语义表示的矢量化属性。</strong></li></ul><p id="9357" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">以下是创建稳定的<strong class="ky ir"> DCGAN </strong>时要考虑的一些核心准则，与标准的<strong class="ky ir"> CNN </strong>(摘自官方<a class="ae lu" href="https://arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>)相对照:</p><ul class=""><li id="5533" class="mj mk iq ky b kz la ld le lh ml ll mm lp mn lt mo mp mq mr bi translated">用步长卷积代替池函数。(这允许<strong class="ky ir"> D </strong>学习其自己的空间下采样和<strong class="ky ir"> G </strong>其各自的上采样，而不会给模型增加任何偏差)</li><li id="e57e" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">使用 BatchNorm </strong>(它通过标准化每个单元的输入来稳定学习，使平均值和单元方差为零，这也有助于创建更健壮的深度模型，而不会出现梯度发散)</li><li id="4c58" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">避免使用全连接隐藏层(非输出)。</strong>(这方面的例子是全局平均池，这似乎会影响收敛速度)</li><li id="4563" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">对于 G-使用 ReLU 激活和 Tanh 进行输出。</strong>(当您将图像作为输出时，tanh 通常是更优选的激活，因为它的范围为[-1，1])</li><li id="c1fc" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">对于 D-使用泄漏激活(以及输出概率的 sigmoid 函数)。</strong>(这是经过经验测试的，似乎适用于更高分辨率的建模)</li></ul><p id="9fa2" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">下面是一个<strong class="ky ir"> DCGAN 发生器</strong>的最标准结构:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mx"><img src="../Images/966e02604449f1d59baac5d015c7b632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QsYQLn7wfIIeZ49XU7SLuw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">DCGAN Generator structure</figcaption></figure><p id="b3d0" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">正如我们所看到的，它的初始输入只是一个<strong class="ky ir"> (1，100) </strong>噪声向量，它通过<strong class="ky ir"> 4 个卷积</strong>层进行上采样，并以<strong class="ky ir"> 2 </strong>的步长产生大小为(64，64，3)的结果 RGB 图像。为了实现这一点，输入向量被投影到 1024 维的输出上，以匹配第一个<strong class="ky ir"> Conv </strong>层的输入，我们将在后面看到更多。</p><p id="b240" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">标准的鉴别器是什么样的？好吧，关于你所期望的，让我们来看看:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi my"><img src="../Images/096bf42ee3932883753d8c17c56babb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*bjMzwBibh-KAb_06QV32bA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">DCGAN Discriminator structure</figcaption></figure><p id="bd2b" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这一次我们有一个<strong class="ky ir"> (64，64，3) </strong>的输入图像，与<strong class="ky ir"> G </strong>的输出相同。我们将它传递给<strong class="ky ir"> 4 标准下采样 Conv 层</strong>，再次以<strong class="ky ir"> 2 </strong>的步幅。在最终的输出层中，图像被展平为一个向量，该向量通常被馈送给一个 sigmoid 函数，然后该函数输出该图像的<strong class="ky ir"> D </strong>的预测<strong class="ky ir">(一个表示概率在[0，1] — dog = 1 或 no dog = 0) </strong>范围内的单个值)。</p><p id="f43b" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">好了，现在你看到了<strong class="ky ir"> GANs </strong>和<strong class="ky ir"> DCGANs </strong>背后的基本思想，所以现在我们可以继续使用<strong class="ky ir"> Tensorflow 和 Keras </strong>:)生成一些狗。</p><h1 id="57dc" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">图像预处理和 EDA(探索性数据分析)</h1><p id="de55" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">在我们继续创建<strong class="ky ir">甘</strong>模型之前，让我们先快速浏览一下我们将要使用的<strong class="ky ir">斯坦福狗</strong>数据集。因为我们也有每张图片的注释，我们可以用它们将每张狗图片映射到它各自的品种。为此，我们可以首先创建一个字典，将文件名中的品种代码映射到实际的品种名称。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="kq kr l"/></div></figure><p id="31ea" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">接下来，我将使用<strong class="ky ir"> OpenCV </strong>作为快速函数来读取图像并将其转换为<strong class="ky ir"> RGB </strong>。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="kq kr l"/></div></figure><p id="fe86" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如果我们查看数据集，我们可以看到每个注释文件夹都包含一个<strong class="ky ir"> xml </strong>文件列表。这些文件与特定的图像相关联，包含非常有用的信息，主要是图像中每只狗周围的边界框。也有图像中有多只狗，这使我们能够准确地裁剪它们，并制作一个只包含<strong class="ky ir">单身狗的数据集</strong>图像。</p><p id="787c" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在这里，我们可以利用<strong class="ky ir"> xml </strong>库来创建一个树，并为该注释找到相关的元素。对于每个对象，我们可以提取边界框坐标，裁剪图像，并根据结果<strong class="ky ir">图像宽度</strong>通过<strong class="ky ir">收缩</strong>或<strong class="ky ir">扩展</strong>来标准化裁剪。最后，我们将图像保存在一个 numpy 数组中。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="kq kr l"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Use ET to find the annotations of each dog in the image</figcaption></figure><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="df30" class="ne jo iq na b gy nf ng l nh ni">for o in objects:                           <br/>   bndbox = o.find('bndbox')                            <br/>   xmin = int(bndbox.find('xmin').text)                           <br/>   ymin = int(bndbox.find('ymin').text)                           <br/>   xmax = int(bndbox.find('xmax').text)                           <br/>   ymax = int(bndbox.find('ymax').text)                                 .. Add some margins and adjust the width ..                             # Crop the image                           <br/>   img_cropped = img[ymin:ymin+w, xmin:xmin+w, :]      # [h,w,c]                           .. Interpolation step ..                           <br/># Resize the image                           <br/>   img_cropped = cv2.resize(img_cropped, (image_width, image_height), interpolation=interpolation)<br/># Save the images and labels<br/>dog_images_np[curIdx,:,:,:] = np.asarray(img_cropped)<br/>dog_breed_name = dog_breed_dict[dog_ann.split('_')[0]]<br/>breeds.append(dog_breed_name)<br/>curIdx += 1<br/>                <br/>return dog_images_np, breeds</span></pre><p id="d3c6" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="mi">加载这些功能大约需要 2-3 分钟。</em></p><p id="578e" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">通过试验，我们可以得出只有单条狗的结果图像是<strong class="ky ir"> 22125 </strong>，因此我们可以指定 numpy 数组的确切大小。有<strong class="ky ir"> 120 </strong>不同的狗品种。</p><p id="21b7" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">现在我们有了要素和标注，我们可以在正方形网格中绘制它们，以查看作物的外观并确保它们的标注正确。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="8cd7" class="ne jo iq na b gy nf ng l nh ni">def plot_features(features, labels, image_width=image_width, image_height=image_height, image_channels=image_channels,<br/>examples=25, disp_labels=True): <br/>  <br/>    if not math.sqrt(examples).is_integer():<br/>        print('Please select a valid number of examples.')<br/>        return<br/>    imgs = []<br/>    classes = []<br/>    for i in range(examples):<br/>        rnd_idx = np.random.randint(0, len(labels))<br/>        imgs.append(features[rnd_idx, :, :, :])<br/>        classes.append(labels[rnd_idx])</span><span id="78fa" class="ne jo iq na b gy nj ng l nh ni">    fig, axes = plt.subplots(round(math.sqrt(examples)), round(math.sqrt(examples)),figsize=(15,15),<br/>    subplot_kw = {'xticks':[], 'yticks':[]},<br/>    gridspec_kw = dict(hspace=0.3, wspace=0.01))<br/>    <br/>    for i, ax in enumerate(axes.flat):<br/>        if disp_labels == True:<br/>            ax.title.set_text(classes[i])<br/>        ax.imshow(imgs[i])</span></pre><p id="d7e3" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">注意，我们需要归一化像素值，以确保狗被正确绘制。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="3903" class="ne jo iq na b gy nf ng l nh ni">plot_features(dog_images_np / 255., breeds, examples=25, disp_labels=True)</span></pre><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nk"><img src="../Images/6be4e1627f16da560b9f3a78b871f236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9LwQfdpKPB_OvuQq92YMlg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Visualizing the image features</figcaption></figure><h1 id="79f4" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">模型超参数列表</h1><p id="4ec7" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">这里我们有一个超参数的完整列表，您可以调整并尝试改进该模型。我主要是从研究论文中收集这些值，并对它们进行了一点调整，这就是我最终得到的结果。以下是你可以尝试的一些事情:</p><ul class=""><li id="9edb" class="mj mk iq ky b kz la ld le lh ml ll mm lp mn lt mo mp mq mr bi translated"><strong class="ky ir">样本大小</strong> —特征的数量</li><li id="2742" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">批量大小</strong> — 64 或 32 可以提高性能，但是计算量很大，并且只能运行少量时期的模型</li><li id="53fd" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">权重初始标准值和平均值</strong> —这些值来自研究论文，似乎可以稳定模型训练</li><li id="b123" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">泄漏 ReLU 斜率</strong>—<strong class="ky ir">D</strong>激活的阈值看起来也很稳定</li><li id="e495" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">缩小因子和比例因子</strong>——设置使得<strong class="ky ir"> G </strong>的噪声向量可以被整形为(4，4，512)，其他组合也可能有效</li><li id="8b66" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">漏失</strong> —漏失层的数量、位置和速率可以提高性能。</li><li id="f9a5" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">学习率和学习率衰减</strong> —对模型收敛非常重要，很难精确调整，<strong class="ky ir"> G </strong>和<strong class="ky ir"> D </strong>可以有不同的学习率。</li><li id="d6d0" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">噪声矢量形状</strong>——通常 128 或 100 似乎就足够了</li></ul><h1 id="2f60" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">创建影像数据集</h1><p id="b5ce" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">现在让我们使用我们的 numpy 特性数组来构造一个<strong class="ky ir"> Tensorflow </strong> dataset 对象。首先，我们可以将数据类型转换为<strong class="ky ir"> float32 </strong>，这总是有助于保留一些内存。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="20e5" class="ne jo iq na b gy nf ng l nh ni">dog_features_tf = tf.cast(dog_images_np, 'float32')</span></pre><p id="ab61" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们还可以将<strong class="ky ir">数据扩充</strong>应用到我们的数据集。这包括随机的<strong class="ky ir">水平翻转</strong>，在随机区域缩放和裁剪图像。在这些方法中，我发现只有第一种方法对向数据集添加更多的方差有点用，因为其他方法会引入很多噪声。</p><p id="6647" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">因此，在这种情况下，我们数据集中的图像将有<strong class="ky ir"> 50% </strong>的机会从左向右翻转。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="8650" class="ne jo iq na b gy nf ng l nh ni">def flip(x: tf.Tensor) -&gt; (tf.Tensor):<br/>    x = tf.image.random_flip_left_right(x)<br/>    return x</span></pre><p id="2cf0" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">现在，我们可以使用<strong class="ky ir"> Tensorflow </strong>来创建数据集，方法是将它混洗，应用一些增强，最后将它分成指定<strong class="ky ir">批处理大小</strong>的批处理。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="510c" class="ne jo iq na b gy nf ng l nh ni">dog_features_data = tf.data.Dataset.from_tensor_slices(dog_features_tf).shuffle(sample_size).map(flip).batch(batch_size, drop_remainder=True)</span></pre><h1 id="3db6" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">标准化技术</h1><p id="1f4b" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">在我们实际制作<strong class="ky ir">生成器</strong>之前，让我们来看看一些可以逐渐加快<strong class="ky ir"> DCGAN </strong>收敛速度的规范化。</p><h1 id="00f3" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">重量初始化</h1><p id="80c1" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">其中一种方法是<strong class="ky ir">权重初始化</strong>。原来训练稳定的<strong class="ky ir">甘斯</strong>还是蛮重要的。首先，模型重量需要在<strong class="ky ir">零中心</strong>上稍微增加<strong class="ky ir">标准值</strong> (0.02)。这在训练期间稳定了<strong class="ky ir"> D </strong>和<strong class="ky ir"> G </strong>，并防止模型梯度消失或爆炸。这是每种情况下的关键一步，我们必须在模型中使用随机变量(随机噪声向量)。</p><p id="8d89" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">下面是一个关于<strong class="ky ir">权重初始化</strong>如何严重影响神经网络学习过程的例子。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nl"><img src="../Images/21903a070ab477bb45cad5b9462b223c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PaTir4OPRGRMpWCCwv6eXQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">The impact of weight initialization on model training</figcaption></figure><p id="f3af" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们也可以使用<strong class="ky ir"> Keras </strong>应用<strong class="ky ir">截尾正态分布</strong>，这将丢弃超过平均值 2 个标准偏差的<strong class="ky ir">值。这也许可以在训练期间消除一些异常点。</strong></p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="54f6" class="ne jo iq na b gy nf ng l nh ni">weight_initializer = tf.keras.initializers.TruncatedNormal(stddev=weight_init_std, mean=weight_init_mean, seed=42)</span></pre><h1 id="db72" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">光谱归一化</h1><p id="33e4" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated"><strong class="ky ir">谱归一化</strong>是一种新型的权重初始化，专门针对<strong class="ky ir"> GANs </strong>设计的，似乎可以进一步稳定模型训练(你可以从这篇<a class="ae lu" href="https://arxiv.org/pdf/1802.05957.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中读到更多)。关于<strong class="ky ir">光谱归一化</strong>的更详细的解释以及它为什么工作也值得看看这篇<a class="ae lu" href="https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html" rel="noopener ugc nofollow" target="_blank">文章</a>，它有非常直观的例子。</p><p id="262c" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir">我们网络中单个权重的频谱归一化</strong>可定义如下:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/bd40c894671e5f85b127c6f21b808e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*szrUCXyXqK5uIk1xUxKvfQ.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Applying Spectral Normalization on a single weight</figcaption></figure><p id="afaf" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里<em class="mi"> u </em>和<em class="mi"> v </em>是相同大小的简单随机向量。对于每个学习步骤，它们被用来对特定权重执行所谓的<strong class="ky ir">幂迭代</strong>操作，并且它被证明比简单地惩罚梯度在计算上高效得多。</p><p id="6d25" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">之后，在反向传播步骤中，我们使用<em class="mi"> WSN(W) </em>代替<em class="mi"> W </em>来更新权重。</p><p id="7d4e" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">对于这个项目，我将重用一些由<strong class="ky ir"> IShengFang </strong> ( <a class="ae lu" href="https://github.com/IShengFang/SpectralNormalizationKeras" rel="noopener ugc nofollow" target="_blank">官方代码</a>)实现的自定义<strong class="ky ir"> Keras </strong>图层，在<strong class="ky ir"> Conv </strong>和<strong class="ky ir">密集</strong>图层之上应用<strong class="ky ir">光谱归一化</strong>。</p><p id="c0c9" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里还有一个关于<strong class="ky ir">光谱归一化</strong>效果的好例子:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/64e68cdfcf9a88f026b5e2b0250ec6cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rd7XVN8zTgZB3gGgqFkC6w.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">DCGAN training with SN</figcaption></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/18fc4e2014c0363a7b8e2b2b48d43672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*h80WCznlsGXifDDxrbUAow.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">DCGAN training with standard layers</figcaption></figure><p id="2baf" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">让我们也在<strong class="ky ir"> Keras </strong>中定义一些模板层，以便我们稍后可以更容易地创建<strong class="ky ir"> G </strong>和<strong class="ky ir"> D </strong>。各层的标准模式将是:</p><p id="6aab" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir">TP _ conv _ Block =[(conv(SN)2d transpose(上采样))-&gt;(batch norm)-&gt;(ReLU)]</strong></p><p id="2ba0" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir">conv _ 布洛克= [(Conv(SN)2D(下采样))-&gt;(batch norm)-&gt;(leaky relu)】</strong></p><p id="d041" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">对于本例，我将在 G 中使用标准<strong class="ky ir">Conv2D 转置模块，在 d 中使用频谱归一化 conv 2d 层</strong></p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="24d9" class="ne jo iq na b gy nf ng l nh ni">def transposed_conv(model, out_channels, ksize, stride_size, ptype='same'):<br/>    model.add(Conv2DTranspose(out_channels, (ksize, ksize),<br/>                              strides=(stride_size, stride_size), padding=ptype, <br/>                              kernel_initializer=weight_initializer, use_bias=False))<br/>    model.add(BatchNormalization())<br/>    model.add(ReLU())<br/>    return model<br/><br/>def convSN(model, out_channels, ksize, stride_size):<br/>    model.add(ConvSN2D(out_channels, (ksize, ksize), strides=(stride_size, stride_size), padding='same',<br/>                     kernel_initializer=weight_initializer, use_bias=False))<br/>    model.add(BatchNormalization())<br/>    model.add(LeakyReLU(alpha=leaky_relu_slope))<br/>    #model.add(Dropout(dropout_rate))<br/>    return model<br/></span></pre><h1 id="286a" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">发电机</h1><p id="cb03" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">我们终于可以定义我们的生成器了。模型结构主要基于官方的 DCGAN <a class="ae lu" href="https://arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，做了一些我认为对性能有益的调整。总体结构如下:</p><p id="f761" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir">【输入(128，1)——&gt;密集(2048，)——&gt;重塑(4，4，128)——&gt;TP _ conv _ Block(深度=512，K = 5×5，S = 1×1)——&gt;Dropout(0.5)-&gt;TP _ conv _ Block(深度=256，K = 5×5，S = 2×2)-&gt;Dropout(0.5)-&gt;TP _ conv _ Block(深度=128，K</strong></p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="fd99" class="ne jo iq na b gy nf ng l nh ni">def DogGenerator():<br/>    model = Sequential()<br/>    model.add(Dense(image_width // scale_factor * image_height // scale_factor * 128,<br/>                    input_shape=(noise_dim,), kernel_initializer=weight_initializer))<br/>    #model.add(BatchNormalization(epsilon=BN_EPSILON, momentum=BN_MOMENTUM))<br/>    #model.add(LeakyReLU(alpha=leaky_relu_slope))<br/>    model.add(Reshape((image_height // scale_factor, image_width // scale_factor, 128)))<br/>    <br/>    model = transposed_conv(model, 512, ksize=5, stride_size=1)<br/>    model.add(Dropout(dropout_rate))<br/>    model = transposed_conv(model, 256, ksize=5, stride_size=2)<br/>    model.add(Dropout(dropout_rate))<br/>    model = transposed_conv(model, 128, ksize=5, stride_size=2)<br/>    model = transposed_conv(model, 64, ksize=5, stride_size=2)<br/>    model = transposed_conv(model, 32, ksize=5, stride_size=2)<br/>    <br/>    model.add(Dense(3, activation='tanh', kernel_initializer=weight_initializer))<br/><br/>    return model</span></pre><h1 id="ca4f" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">鉴别器</h1><p id="9518" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated"><strong class="ky ir">鉴别器</strong>相对更容易实现，因为它基本上是一个小型的<strong class="ky ir"> CNN </strong>两类分类器。我们可以选择是否应用<strong class="ky ir">光谱归一化</strong>并观察性能效果。对于这个例子，我将尝试只在<strong class="ky ir"> D </strong>中应用<strong class="ky ir"> SN </strong>。下面是<strong class="ky ir"> D </strong>的结构:</p><p id="eef5" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir">【输入(128，128，3)——&gt;conv(SN)2D(深度=64，K=5x5，S=1x1，相同)——&gt;leaky relu——&gt;conv _ Block(深度=64，K=5x5，S = 2 x2)——&gt;conv _ Block(深度=128，K=5x5，S = 2 x2)——&gt;conv _ Block(深度=256，K=5x5，S = 2 x2)——【T57</strong></p><p id="c22e" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">还要注意，所有的<strong class="ky ir"> Conv </strong>和<strong class="ky ir">密集</strong>层都用上面定义的<strong class="ky ir">截断正态</strong>分布初始化。另一件事是<strong class="ky ir">偏差</strong>项从<strong class="ky ir"> Conv </strong>层中移除，这也稍微稳定了模型。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="169e" class="ne jo iq na b gy nf ng l nh ni">def DogDiscriminator(spectral_normalization=True):<br/>    model = Sequential()<br/>    if spectral_normalization:<br/>        model.add(ConvSN2D(64, (5, 5), strides=(1,1), padding='same', use_bias=False,<br/>                         input_shape=[image_height, image_width, image_channels], <br/>                         kernel_initializer=weight_initializer))<br/>        #model.add(BatchNormalization(epsilon=BN_EPSILON, momentum=BN_MOMENTUM))<br/>        model.add(LeakyReLU(alpha=leaky_relu_slope))<br/>        #model.add(Dropout(dropout_rate))<br/>        <br/>        model = convSN(model, 64, ksize=5, stride_size=2)<br/>        #model = convSN(model, 128, ksize=3, stride_size=1)<br/>        model = convSN(model, 128, ksize=5, stride_size=2)<br/>        #model = convSN(model, 256, ksize=3, stride_size=1)<br/>        model = convSN(model, 256, ksize=5, stride_size=2)<br/>        #model = convSN(model, 512, ksize=3, stride_size=1)<br/>        #model.add(Dropout(dropout_rate))<br/><br/>        model.add(Flatten())<br/>        model.add(DenseSN(1, activation='sigmoid'))<br/>    else:<br/>        ...<br/>    return model</span><span id="390c" class="ne jo iq na b gy nj ng l nh ni">dog_discriminator = DogDiscriminator(spectral_normalization=True)</span></pre><h1 id="3d4e" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">标签平滑</h1><p id="2765" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">一种可以在训练期间应用的正则化方法被称为<strong class="ky ir">标签平滑</strong>。这实际上是防止了<strong class="ky ir"> D </strong>在其预测中过于自信或过于自信。如果<strong class="ky ir"> D </strong>变得过于确定在特定的图像中有一只狗，那么<strong class="ky ir"> G </strong>可以利用这个事实，并且持续地开始只生成那个种类的图像，并且依次停止改进。我们可以通过将负类的类标签设置在范围<strong class="ky ir">【0，0.3】</strong>内，将正类的类标签设置在范围<strong class="ky ir">【0.7，1】</strong>内来解决这个问题。</p><p id="9f90" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这将防止总体概率非常接近两个阈值。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="7c38" class="ne jo iq na b gy nf ng l nh ni"># Label smoothing -- technique from GAN hacks, instead of assigning 1/0 as class labels, we assign a random integer in range [0.7, 1.0] for positive class<br/># and [0.0, 0.3] for negative class<br/><br/>def smooth_positive_labels(y):<br/>    return y - 0.3 + (np.random.random(y.shape) * 0.5)<br/><br/>def smooth_negative_labels(y):<br/>    return y + np.random.random(y.shape) * 0.3</span></pre><h1 id="4d9e" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">将噪声引入标签</h1><p id="0604" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">这种技术也被称为<strong class="ky ir">实例噪声</strong>。通过给标签增加少量误差(假设 5%)，这往往会使真实分布和预测分布更加分散，从而开始相互重叠。这反过来使得在学习过程中拟合生成图像的定制分布更加容易。</p><p id="c71b" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">下面是一个很好的例子，展示了使用这些技术时这两个发行版的样子:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi no"><img src="../Images/30b09a76d32468006e15444a9bb56662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EA95dVM9vFHBTxWjrwYJCQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">The impact of Insance Noise and Label smoothing on the real and fake distributions of images</figcaption></figure><p id="b8d6" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">下面是我们如何实现<strong class="ky ir">实例噪声</strong>:</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="2776" class="ne jo iq na b gy nf ng l nh ni"># randomly flip some labels<br/>def noisy_labels(y, p_flip):<br/>    # determine the number of labels to flip<br/>    n_select = int(p_flip * int(y.shape[0]))<br/>    # choose labels to flip<br/>    flip_ix = np.random.choice([i for i in range(int(y.shape[0]))], size=n_select)<br/>    <br/>    op_list = []<br/>    # invert the labels in place<br/>    #y_np[flip_ix] = 1 - y_np[flip_ix]<br/>    for i in range(int(y.shape[0])):<br/>        if i in flip_ix:<br/>            op_list.append(tf.subtract(1, y[i]))<br/>        else:<br/>            op_list.append(y[i])<br/>    <br/>    outputs = tf.stack(op_list)<br/>    return outputs</span></pre><h1 id="25f4" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">优化者</h1><p id="60d6" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">这个任务的最佳验证优化算法是<strong class="ky ir"> Adam </strong>，两个模型的标准学习率为<strong class="ky ir"> 0.0002 </strong>，beta 为<strong class="ky ir"> 0.5 </strong>。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="e831" class="ne jo iq na b gy nf ng l nh ni">generator_optimizer = tf.train.AdamOptimizer(learning_rate=lr_initial_g, beta1=0.5)<br/>discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=lr_initial_d, beta1=0.5)</span></pre><h1 id="d2aa" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">定义损失函数</h1><p id="98c6" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">最近优化<strong class="ky ir"> GANs </strong>的另一个新趋势是应用<strong class="ky ir">相对论</strong>损失函数，而不是标准损失函数。这些函数测量真实数据比生成的数据更“真实”的概率。其中比较流行的相对论函数选择有<strong class="ky ir"> RaLSGAN(相对论平均最小二乘法)、RaSGAN(相对论平均标准)和 RaHinge(相对论铰链损耗)</strong>。</p><p id="8ffc" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">但在这一切之前，让我们先定义一下标准<strong class="ky ir">甘</strong>损失:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ef0592ce9725bc7c1ded85ce083c301c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*o8VrOm7QexoDjOOoi_sHpg.png"/></div></figure><p id="b0e9" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">正如我们所观察到的，这基本上是用于分类任务的标准<strong class="ky ir">二元交叉熵损失</strong>或者真实分布和生成分布之间的<strong class="ky ir">逻辑损失</strong>。在<strong class="ky ir">张量流</strong>中，这可以定义如下:</p><p id="79ed" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">相比之下，这里是一个<strong class="ky ir"> RSGAN(相对论标准)</strong>损耗的样子:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nq"><img src="../Images/c80822fd09e8f49f2a9f71cba0a37d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*vrDKykPN7Fwny5ffWvV6Ew.png"/></div></div></figure><p id="60c2" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在这种情况下，任务是不同的，测量<strong class="ky ir">真实(r) </strong>和<strong class="ky ir">虚假(f) </strong>数据分布之间的相似性。RSGAN 在 D(x) = 0.5 时达到最优点(即<em class="mi">C</em>(<em class="mi">xr</em>)=<em class="mi">C</em>(<em class="mi">xf</em>))。有许多相对论损失函数变体，它们都包含不同的方法来测量这种相似性。在这个项目中，我尝试了似乎拥有最佳记录的<strong class="ky ir"> MIFID </strong>得分<strong class="ky ir"> (RaLSGAN、RaSGAN 和 RaHinge) </strong>的<strong class="ky ir"> 3 </strong>。随意给自己尝试不同的损耗，看看是否能提高性能；).</p><p id="4121" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里有一个最常用的列表:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mx"><img src="../Images/a4e5bdc672102d0dcaafb03107cc3579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R6cGS9l48aQcT9SEDvLFlA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Standard and Relativistic GAN losses</figcaption></figure><p id="23e8" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在针对这个特定问题的多次试验中，我没有发现通过切换到<strong class="ky ir">相对论性</strong>损耗来提高性能，所以我决定坚持使用标准的<strong class="ky ir"> GAN </strong>损耗函数，因为它更容易估计，尽管在某些情况下，这些损耗确实可以加快模型的收敛。</p><p id="237c" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里是应用了<strong class="ky ir">标签平滑和实例噪声</strong>的<strong class="ky ir">鉴别器</strong>损失函数的样子。它基本上是两个子损失的总和<strong class="ky ir">(假的——根据 G 的图像，真实的——根据实际的训练图像)</strong>。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="b60a" class="ne jo iq na b gy nf ng l nh ni">def discriminator_loss(real_output, fake_output, loss_func, apply_label_smoothing=True, label_noise=True):<br/>    if label_noise and apply_label_smoothing:<br/>        real_output_noise = noisy_labels(tf.ones_like(real_output), 0.05)<br/>        fake_output_noise = noisy_labels(tf.zeros_like(fake_output), 0.05)<br/>        real_output_smooth = smooth_positive_labels(real_output_noise)<br/>        fake_output_smooth = smooth_negative_labels(fake_output_noise)<br/>        if loss_func == 'gan': <br/>            real_loss = cross_entropy(tf.ones_like(real_output_smooth), real_output)<br/>            fake_loss = cross_entropy(tf.zeros_like(fake_output_smooth), fake_output)</span><span id="5546" class="ne jo iq na b gy nj ng l nh ni">        else:<br/>... other loss function variants</span><span id="0424" class="ne jo iq na b gy nj ng l nh ni">        loss = fake_loss + real_loss<br/>        return loss</span></pre><p id="4f9a" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">而<strong class="ky ir">生成器</strong>损失函数<strong class="ky ir">(应用了标签平滑)</strong>只是一个标准的<strong class="ky ir">物流损失</strong>:</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="bb7a" class="ne jo iq na b gy nf ng l nh ni">def generator_loss(real_output, fake_output, loss_func, apply_label_smoothing=True):<br/>    if apply_label_smoothing:<br/>        fake_output_smooth = smooth_negative_labels(tf.ones_like(fake_output))<br/>        if loss_func == 'gan':<br/>            return cross_entropy(tf.ones_like(fake_output_smooth), fake_output)<br/>        else:<br/>... other loss function variants</span><span id="4c29" class="ne jo iq na b gy nj ng l nh ni">    return loss</span></pre><h1 id="26ec" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">主训练循环</h1><p id="a458" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">让我们也确定用于训练的时期的数量和提供给<strong class="ky ir">生成器</strong>用于可视化中间结果的图像的数量。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="2971" class="ne jo iq na b gy nf ng l nh ni">EPOCHS = 280<br/>num_examples_to_generate = 64<br/>seed = tf.random.normal([num_examples_to_generate, noise_dim])</span></pre><p id="bf16" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir"> DCGAN </strong>的一个训练步骤由三个标准步骤组成:</p><ol class=""><li id="4af3" class="mj mk iq ky b kz la ld le lh ml ll mm lp mn lt nr mp mq mr bi translated"><strong class="ky ir">前进道具</strong> — <strong class="ky ir"> G </strong>制造一批假像；这和一批真实图像一起被传送到<strong class="ky ir"> D </strong>。</li><li id="f5f9" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt nr mp mq mr bi translated"><strong class="ky ir">计算 G 和 D 的损失函数</strong>。</li><li id="6421" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt nr mp mq mr bi translated"><strong class="ky ir">反投影</strong> —计算<strong class="ky ir"> G </strong>和<strong class="ky ir"> D </strong>的梯度，优化权重。</li></ol><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="6a65" class="ne jo iq na b gy nf ng l nh ni">def train_step(images, loss_type='gan'):<br/>    noise = tf.random.normal([batch_size, noise_dim])<br/><br/>    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:<br/>        generated_images = dog_generator(noise, training=True)<br/>        <br/>        real_output = dog_discriminator(images, training=True)<br/>        fake_output = dog_discriminator(generated_images, training=True)<br/>        <br/>        gen_loss = generator_loss(real_output, fake_output, loss_type, apply_label_smoothing=True)<br/>        disc_loss = discriminator_loss(real_output, fake_output, loss_type, <br/>                                       apply_label_smoothing=True, label_noise=True)<br/> <br/>    gradients_of_generator = gen_tape.gradient(gen_loss, dog_generator.trainable_variables)<br/>    gradients_of_discriminator = disc_tape.gradient(disc_loss, dog_discriminator.trainable_variables)<br/><br/>    generator_optimizer.apply_gradients(zip(gradients_of_generator, dog_generator.trainable_variables))<br/>    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, dog_discriminator.trainable_variables))<br/>    <br/>    return gen_loss, disc_loss</span></pre><p id="070f" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">让我们也定义一些函数来直观显示按时期和作为一个整体的模型损失。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="dc23" class="ne jo iq na b gy nf ng l nh ni">def plot_losses(G_losses, D_losses, all_gl, all_dl, epoch):<br/>    plt.figure(figsize=(10,5))<br/>    plt.title("Generator and Discriminator Loss - EPOCH {}".format(epoch))<br/>    plt.plot(G_losses,label="G")<br/>    plt.plot(D_losses,label="D")<br/>    plt.xlabel("Iterations")<br/>    plt.ylabel("Loss")<br/>    plt.legend()<br/>    ymax = plt.ylim()[1]<br/>    plt.show()<br/>    <br/>    plt.figure(figsize=(10,5))<br/>    plt.plot(np.arange(len(all_gl)),all_gl,label='G')<br/>    plt.plot(np.arange(len(all_dl)),all_dl,label='D')<br/>    plt.legend()<br/>    #plt.ylim((0,np.min([1.1*np.max(all_gl),2*ymax])))<br/>    plt.title('All Time Loss')<br/>    plt.show()</span></pre><p id="09dc" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们还可以使用下面的函数来绘制生成图像的网格。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="f9c3" class="ne jo iq na b gy nf ng l nh ni">def generate_and_save_images(model, epoch, test_input, rows, cols):<br/>    # Notice `training` is set to False.<br/>    # This is so all layers run in inference mode (batchnorm).<br/>    predictions = model(test_input, training=False)<br/>    fig = plt.figure(figsize=(14,14))<br/>    for i in range(predictions.shape[0]):<br/>        plt.subplot(rows, cols, i+1)<br/>        plt.imshow((predictions[i, :, :, :] * 127.5 + 127.5) / 255.)<br/>        plt.axis('off') <br/>        <br/>    plt.subplots_adjust(wspace=0, hspace=0)<br/>    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))<br/>    plt.show()</span></pre><p id="de35" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">为了生成单个测试图像，我们也可以重用相同的方法。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="6d5f" class="ne jo iq na b gy nf ng l nh ni">def generate_test_image(model, noise_dim=noise_dim):<br/>    test_input = tf.random.normal([1, noise_dim])<br/>    # Notice `training` is set to False.<br/>    # This is so all layers run in inference mode (batchnorm).<br/>    predictions = model(test_input, training=False)<br/>    fig = plt.figure(figsize=(5,5))<br/>    plt.imshow((predictions[0, :, :, :] * 127.5 + 127.5) / 255.)<br/>    plt.axis('off') <br/>    plt.show()</span></pre><h1 id="35ba" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">评估 GANs</h1><p id="d04b" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">我们还没提到一个<strong class="ky ir">甘</strong>平时是怎么评价的。大多数使用基准来评估一个<strong class="ky ir">甘</strong>表现如何的研究论文通常基于所谓的<strong class="ky ir">初始分数</strong>。这测量输入图像的两个主要特征:</p><ul class=""><li id="3511" class="mj mk iq ky b kz la ld le lh ml ll mm lp mn lt mo mp mq mr bi translated"><strong class="ky ir">品种(例如生成不同类型的狗品种)</strong></li><li id="908d" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">图像的区别(或质量)</strong></li></ul><p id="3457" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如果两件事都是真的，分数就高。如果其中一个或两个都是错误的，分数将会很低。分数越高越好。这意味着您的 GAN 可以生成许多不同的清晰图像。最低分可能是零分。数学上的最高可能分数是无穷大，尽管实际上可能会出现一个非无穷大的上限。</p><p id="0903" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir"> Inception Score </strong>来源于 Google 的<a class="ae lu" rel="noopener" target="_blank" href="/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202"> Inception Network </a>，是最先进的图像分类深度架构之一。通过将来自我们的 GAN 的图像通过分类器，我们可以测量我们生成的图像的属性。为了产生分数，我们需要计算图像的真实和虚假分布之间的相似性/距离。这是使用<strong class="ky ir">KL(kull back–lei bler)散度公式</strong>完成的:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f9e700662bac10af59c4908a2802842c.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*S6Cp770aAUtMJBK66zLgFw.png"/></div></figure><p id="c9e6" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里，<strong class="ky ir"> P 和 Q </strong>是两个测量分布。在这种情况下，<strong class="ky ir">更高的 KL 散度</strong>意味着更好的结果——图像的质量是相似的，并且存在各种各样的标签。在相反的情况下，<strong class="ky ir">低 KL 偏差</strong>可能是由于标签质量低或种类少:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nt"><img src="../Images/ffec0ed2c73b3abba0ef09c39c25907d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NbPx_RCrZv0G7nRIA_cseg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Measuring the performance with KL divergence</figcaption></figure><h1 id="f1d9" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">弗雷歇起始距离</h1><p id="f9bf" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated"><strong class="ky ir">的一个缺点是</strong>如果每个类只生成一个图像，它可能会歪曲性能。为了解决这个问题，我们可以使用<strong class="ky ir"> FID(弗雷歇初始距离)</strong>。这种方法将前面两种类型的图像定义为具有平均值<em class="mi"> μ </em>和协方差σ(适马)的多元<strong class="ky ir">高斯分布</strong>。让我们看看这个距离是如何计算的:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/4a7cc6bc0a483143f79a87fdc1dbe820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*c159Dp06UeZUzAj_5JKonw.png"/></div></figure><p id="79e9" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里，<em class="mi"> x </em>和<em class="mi"> g </em>代表图像的真假分布，而<em class="mi"> Tr </em>是结果的对角元素之和。</p><blockquote class="nv nw nx"><p id="b9bc" class="kw kx mi ky b kz la lb lc ld le lf lg ny li lj lk nz lm ln lo oa lq lr ls lt ij bi translated"><strong class="ky ir"><em class="iq">FID 值越低，图像质量和多样性越好。</em> </strong></p></blockquote><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ob"><img src="../Images/2c1ab9c0c77e4ae696afe2bba43a6156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYEKvgxy1WKo6Tn4EE624g.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Example of how the diversity factor impacts the scoring on different datasets</figcaption></figure><p id="a474" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">以下是一些有用的注释，说明为什么<strong class="ky ir"> FID </strong>是一个好的衡量标准:</p><ul class=""><li id="c0c2" class="mj mk iq ky b kz la ld le lh ml ll mm lp mn lt mo mp mq mr bi translated">FID 比 is 对噪声更鲁棒。</li><li id="72ed" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">如果模型每类只生成一个图像，距离会很高。因此，FID 是一种更好的图像多样性度量。</strong></li><li id="cb6d" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir">通过计算训练数据集和测试数据集之间的 FID，我们应该期望 FID 为零，因为两者都是真实图像。(虽然通常会有少量误差)</strong></li><li id="8dc7" class="mj mk iq ky b kz ms ld mt lh mu ll mv lp mw lt mo mp mq mr bi translated"><strong class="ky ir"> FID 和 IS 基于特征提取(特征的有无)。</strong></li></ul><h1 id="049e" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">记忆通知弗雷歇起始距离</h1><p id="32ab" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">以下是 Kaggle 对生殖狗比赛的官方评估工作流程:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e005a62cb792f709ce7ed235d01aa941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*EPkyoJAjlDC2QHAKGyHcyg.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">There are two stages of evaluation — public and private on different datasets. The MIFID metric is calculated using the standard FID, combined with a Memorization Score.</figcaption></figure><p id="1874" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">正如我们所看到的，除了<strong class="ky ir"> FID </strong>指标之外，还有一个额外的<strong class="ky ir">记忆分数</strong>被添加到计算中。这基本上是一个<strong class="ky ir">余弦距离</strong>公式，用于测量真实(来自私人数据集的图像)和虚假图像之间的相似性。我猜这样做是为了确保提供给评估内核的图像实际上是由<strong class="ky ir"> GAN </strong>生成的，而不仅仅是从真实数据集复制或修改的。</p><p id="052b" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">谢天谢地，<strong class="ky ir"> MIFID </strong>赋值器已经被 Kaggle 团队<a class="ae lu" href="https://www.kaggle.com/wendykan/demo-mifid-metric-for-dog-image-generation-comp" rel="noopener ugc nofollow" target="_blank">(此处)</a>实现了，我们不用担心这个问题。</p><h1 id="4dc0" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">图像压缩和保存功能</h1><p id="f735" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">我将再添加两个，用于压缩最终的<strong class="ky ir"> 10K </strong>图像以供提交，并生成临时图像以计算训练期间特定时期之间的<strong class="ky ir"> MIFID </strong>。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="fbed" class="ne jo iq na b gy nf ng l nh ni">def zip_images(filename='images.zip'):<br/>    # SAVE TO ZIP FILE NAMED IMAGES.ZIP<br/>    z = zipfile.PyZipFile(filename, mode='w')<br/>    for k in range(image_sample_size):<br/>        generated_image = dog_generator(tf.random.normal([1, noise_dim]), training=False)<br/>        f = str(k)+'.png'<br/>        img = np.array(generated_image)<br/>        img = (img[0, :, :, :] + 1.) / 2.<br/>        img = Image.fromarray((255*img).astype('uint8').reshape((image_height,image_width,image_channels)))<br/>        img.save(f,'PNG')<br/>        z.write(f)<br/>        os.remove(f)<br/>        #if k % 1000==0: print(k)<br/>    z.close()<br/>    print('Saved final images for submission.')<br/>    <br/>def save_images(directory=OUT_DIR):<br/>    for k in range(image_sample_size):<br/>        generated_image = dog_generator(tf.random.normal([1, noise_dim]), training=False)<br/>        f = str(k)+'.png'<br/>        f = os.path.join(directory, f)<br/>        img = np.array(generated_image)<br/>        img = (img[0, :, :, :] + 1.) / 2.<br/>        img = Image.fromarray((255*img).astype('uint8').reshape((image_height,image_width,image_channels)))<br/>        img.save(f,'PNG')<br/>        #if k % 1000==0: print(k)<br/>    print('Saved temporary images for evaluation.')</span></pre><p id="73cf" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">终于到了实现最终培训功能的时候了，该功能总结了整个过程。这里还有一些我还没有提到的技巧。让我们看看它们是什么。</p><h1 id="e50a" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">学习率衰减</h1><p id="3fda" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">这是一个实验，并不总是有助于提高性能，但我不认为它会伤害任何一种方式。这里的想法是，对于每个训练步骤，将<strong class="ky ir">学习率</strong>降低一个非常小的量，以便稳定训练过程并加速收敛(并逃离局部最小值)。对于这个项目，我使用<strong class="ky ir"> Tensorflow </strong>中的<a class="ae lu" href="https://www.tensorflow.org/api_docs/python/tf/train/cosine_decay" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">余弦学习率衰减</strong> </a> <strong class="ky ir"> </strong>来降低每<code class="fe od oe of na b">decay_step</code>次迭代的学习率。</p><h1 id="7772" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">处理模式崩溃</h1><p id="8a64" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">除了<strong class="ky ir">不收敛</strong>和<strong class="ky ir">消失和爆炸梯度</strong>之外，<strong class="ky ir"> GANs </strong>有时还会遇到另一个主要问题，叫做<strong class="ky ir">模式崩溃</strong>。当开始生产有限种类的样品时，就会发生这种情况。下面是一个在<strong class="ky ir"> MNIST </strong>数据集上训练的<strong class="ky ir">甘</strong>的<strong class="ky ir">模式折叠</strong>的好例子，其中<strong class="ky ir"> G </strong>连续地仅产生单个类标签的图像:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi og"><img src="../Images/23455fb38bb5cc5b3b49b09cce794391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t5Vr12P6BnrwiQbEbvpUfQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">G learns to fool D by only generating samples from a single class, which causes the model to lose diversity</figcaption></figure><p id="dc80" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们已经看到了一些可能消除<strong class="ky ir">模式崩溃</strong>的方法，如<strong class="ky ir">标签平滑</strong>、<strong class="ky ir">实例噪声</strong>、<strong class="ky ir">权重初始化</strong>等等。我们可以在培训中应用的另一种方法叫做<strong class="ky ir">体验回放</strong>。</p><p id="af44" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir">体验回放</strong>在内存中保存一些最近生成的图像。对于每一次<code class="fe od oe of na b">replay_step</code>迭代，我们在那些先前的图像上训练<strong class="ky ir"> D </strong>，以“提醒”先前世代的网络，从而减少<strong class="ky ir">在训练期间过度拟合</strong>到数据批次的特定实例的机会。在这个例子中，我使用了稍微不同形式的<strong class="ky ir">经验重放</strong>，在这个意义上，我为每个训练步骤生成一个新的额外图像以存储在一个列表中，而不是从以前的迭代中馈送实际生成的图像，因为在<strong class="ky ir">急切执行</strong>期间存储数据不是一件容易的任务。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="43aa" class="ne jo iq na b gy nf ng l nh ni">'''<br/>        generated_image = dog_generator(tf.random.normal([1, noise_dim]), training=False)<br/>        exp_replay.append(generated_image)<br/>        if len(exp_replay) == replay_step:<br/>            print('Executing experience replay..')<br/>            replay_images = np.array([p[0] for p in exp_replay])<br/>            dog_discriminator(replay_images, training=True)<br/>            exp_replay = []    <br/>'''</span></pre><p id="8e5e" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">由于 Kaggle 在运行了大约 7-8 个小时后遇到了内存问题，我决定不使用体验回放。如果您找到了解决方法，请告诉我</p><h1 id="3e8f" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">训练功能</strong></h1><p id="0856" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">总而言之，训练过程相当简单。显示中间结果(如图像、损耗)和计算<strong class="ky ir"> MIFID </strong>还有其他步骤。在学习过程的最后，我们打印出最终评估和最终图像的较大网格。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="b19d" class="ne jo iq na b gy nf ng l nh ni">display_results = 40<br/>calculate_mifid = 100<br/>replay_step = 50<br/>decay_step = 50<br/><br/>def train(dataset, epochs):<br/>    all_gl = np.array([]); all_dl = np.array([])<br/>    for epoch in tqdm(range(epochs)):<br/>        <br/>        G_loss = []; D_loss = []<br/>        <br/>        start = time.time()<br/>        new_lr_d = lr_initial_d<br/>        new_lr_g = lr_initial_g<br/>        global_step = 0<br/>        <br/>        for image_batch in dataset:<br/>            g_loss, d_loss = train_step(image_batch)<br/>            global_step = global_step + 1<br/>            G_loss.append(g_loss); D_loss.append(d_loss)<br/>            all_gl = np.append(all_gl,np.array([G_loss]))<br/>            all_dl = np.append(all_dl,np.array([D_loss]))<br/><br/>        if (epoch + 1) % display_results == 0 or epoch == 0:<br/>            plot_losses(G_loss, D_loss, all_gl, all_dl, epoch + 1)<br/>            generate_and_save_images(dog_generator, epoch + 1, seed, rows=8, cols=8)<br/>        <br/>        if (epoch + 1) % calculate_mifid == 0:            <br/>            OUT_DIR.mkdir(exist_ok=True)<br/>            save_images(OUT_DIR)<br/>            evaluator = MiFIDEvaluator(MODEL_PATH, TRAIN_DIR)<br/>            fid_value, distance, mi_fid_score = evaluator.evaluate(OUT_DIR)<br/>            print(f'FID: {fid_value:.5f}')<br/>            print(f'distance: {distance:.5f}')<br/>            print(f'MiFID: {mi_fid_score:.5f}')<br/>            shutil.rmtree(OUT_DIR)<br/>            print('Removed temporary image directory.')<br/>        <br/>        # Cosine learning rate decay<br/>        if (epoch + 1) % decay_step == 0:<br/>            new_lr_d = tf.train.cosine_decay(new_lr_d, min(global_step, lr_decay_steps), lr_decay_steps)<br/>            new_lr_g = tf.train.cosine_decay(new_lr_g, min(global_step, lr_decay_steps), lr_decay_steps)<br/>            generator_optimizer = tf.train.AdamOptimizer(learning_rate=new_lr_d, beta1=0.5)<br/>            discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=new_lr_g, beta1=0.5)          <br/><br/>        print('Epoch: {} computed for {} sec'.format(epoch + 1, time.time() - start))<br/>        print('Gen_loss mean: ', np.mean(G_loss),' std: ', np.std(G_loss))<br/>        print('Disc_loss mean: ', np.mean(D_loss),' std: ', np.std(D_loss))<br/><br/>    # Generate after the final epoch and repeat the process<br/>    generate_and_save_images(dog_generator, epochs, seed, rows=8, cols=8)<br/>    checkpoint.save(file_prefix = checkpoint_prefix)<br/>   <br/>    OUT_DIR.mkdir(exist_ok=True)<br/>    save_images(OUT_DIR)<br/>    evaluator = MiFIDEvaluator(MODEL_PATH, TRAIN_DIR)<br/>    fid_value, distance, mi_fid_score = evaluator.evaluate(OUT_DIR)<br/>    print(f'FID: {fid_value:.5f}')<br/>    print(f'distance: {distance:.5f}')<br/>    print(f'MiFID: {mi_fid_score:.5f}')<br/>    shutil.rmtree(OUT_DIR)<br/>    print('Removed temporary image directory.')<br/>    <br/>  <br/>    print('Final epoch.')</span></pre><p id="e743" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">以下是在训练过程中生成的一些狗的图像:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi oh"><img src="../Images/37ee0d1f850d536c8361072b654f203b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByRDA7N5DSEBEAyNGtGlGQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Epoch 120 — MIFID ~ 90.5</figcaption></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi oh"><img src="../Images/551dc938c0f2689553d67e9fab848ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u0XOmQ8_5nSBItT7f7HI7A.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Epoch 200 — MIFID ~ 64.8</figcaption></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi oh"><img src="../Images/8b40923770ef14f96972f81b81dad842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkG-eUZOBIiW0U6kWDdY3A.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Epoch 280 (final) — MIFID ~ 60.99</figcaption></figure><p id="d7bc" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">正如我们所观察到的，MIFID 在 280 个周期(约 8 小时)内稳步提高。我在比赛中使用这个模型取得的最好成绩是<strong class="ky ir"> 55.87 </strong>。学习过程确实有点随机，所以我认为在<strong class="ky ir">【50，65】</strong>附近的分数应该是现实的。如果你有时间的话，可以继续训练这个模型，因为它有不断改进的潜力:)。</p><p id="40d3" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">最后，我将向您展示如何制作一个有趣的<strong class="ky ir"> GIF </strong>来查看<strong class="ky ir"> DCGAN </strong>学习过程的一个漂亮的小模拟(代码来自 Tensorflow 的<a class="ae lu" href="https://www.tensorflow.org/beta/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank"> DCGAN 教程</a>)。</p><pre class="kl km kn ko gt mz na nb nc aw nd bi"><span id="aacb" class="ne jo iq na b gy nf ng l nh ni">anim_file = 'dcgan.gif'<br/><br/>with imageio.get_writer(anim_file, mode='I') as writer:<br/>    filenames = glob.glob('image*.png')<br/>    filenames = sorted(filenames)<br/>    last = -1<br/>    for i,filename in enumerate(filenames):<br/>        frame = 1*(i**2)<br/>        if round(frame) &gt; round(last):<br/>            last = frame<br/>        else:<br/>            continue<br/>        image = imageio.imread(filename)<br/>        writer.append_data(image)<br/>    image = imageio.imread(filename)<br/>    writer.append_data(image)<br/><br/>import IPython<br/>if IPython.version_info &gt; (6,2,0,''):<br/>    IPython.display.Image(filename=anim_file)</span></pre><h1 id="d12f" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论</h1><p id="c4ac" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">综上所述，<strong class="ky ir"> DCGANs </strong>似乎对超参数选择极其敏感，在训练过程中会出现很多问题，包括<strong class="ky ir">模式崩溃</strong>。它们也是非常计算密集型的，并且难以置信地难以为运行时间<strong class="ky ir"> ~9 小时</strong>构建高分模型。幸运的是，有一个完整的列表，列出了可能的方法和技术，这些方法和技术都有很好的文档记录，可以很容易地应用到您的模型中，以稳定培训过程。</p><p id="ad31" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">对我个人来说，试验这些技术并在这个过程中打破一些核心真的很有趣:d .随时在评论中留下任何建议(改进模型或修复我搞砸的东西)。</p><p id="7c1f" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">非常感谢<strong class="ky ir">克里斯·德奥特、那那西、查德·马拉和尼尔哈·罗伊</strong>在比赛中提供的 Kaggle 内核和示例。我会在下面留下他们的链接。</p><p id="fdec" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">总的来说，这是我参加的第一个<strong class="ky ir">卡格尔</strong>比赛，这是一个非常有趣的了解<strong class="ky ir">甘斯</strong>并和他们一起玩耍的方法。这似乎是 Kaggle 的第一次涉及生成建模的比赛，让我们希望未来会有更多类似这样令人兴奋的挑战出现；).</p><h1 id="5300" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">参考</h1><h1 id="76a5" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">有用的内核和笔记本</h1><p id="43de" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">[1].我之前在<a class="ae lu" href="https://www.kaggle.com/jadeblue/dog-generator-starter-eda-preprocessing" rel="noopener ugc nofollow" target="_blank"> EDA 上的内核和图像预处理</a></p><p id="5490" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[2].<a class="ae lu" href="https://www.kaggle.com/paulorzp/show-annotations-and-breeds" rel="noopener ugc nofollow" target="_blank"> Xml 解析并裁剪到指定的边界框</a></p><p id="0664" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[3].<a class="ae lu" href="https://www.kaggle.com/amanooo/wgan-gp-keras" rel="noopener ugc nofollow" target="_blank">带插值的图像裁剪方法</a></p><p id="bd2d" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[4].<a class="ae lu" href="https://www.kaggle.com/cmalla94/dcgan-generating-dog-images-with-tensorflow" rel="noopener ugc nofollow" target="_blank">Chad Malla 的另一个基于 Keras 的 DCGAN 方法</a></p><p id="d40d" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[5].<a class="ae lu" href="https://www.kaggle.com/c/generative-dog-images/discussion/98595" rel="noopener ugc nofollow" target="_blank"> DCGAN 帮助您提高模型性能的方法</a></p><p id="f0ee" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[6].<a class="ae lu" href="https://www.tensorflow.org/beta/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank"> Tensorflow DCGAN 教程</a></p><p id="cf9a" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[7].<a class="ae lu" href="https://www.kaggle.com/jesucristo/introducing-dcgan-dogs-images" rel="noopener ugc nofollow" target="_blank">那那西拍摄的 DCGAN 狗图片</a></p><p id="6cae" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[8].<a class="ae lu" href="https://www.kaggle.com/phoenix9032/gan-dogs-starter-24-jul-custom-layers" rel="noopener ugc nofollow" target="_blank">甘犬首发 24-7 月 Nirjhar Roy 定制层</a></p><p id="2b6d" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[9].<a class="ae lu" href="https://www.kaggle.com/cdeotte/supervised-generative-dog-net" rel="noopener ugc nofollow" target="_blank">克里斯·德奥特监督的生殖狗网</a></p><p id="01bd" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[10].<a class="ae lu" href="https://www.kaggle.com/jadeblue/dogdcgan-v6-ksize" rel="noopener ugc nofollow" target="_blank">我的最佳参赛作品</a></p><h1 id="e742" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">研究论文、帖子和讨论</h1><p id="a70f" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">[1].Ian Goodfellow，J. Pouget-Abadie，M. Mirza，B. Xu，S. Ozair，Y. Bengio <strong class="ky ir">，</strong> <a class="ae lu" href="https://arxiv.org/pdf/1406.2661.pdf" rel="noopener ugc nofollow" target="_blank">生成性对抗网络</a> (2014)</p><p id="bf2b" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[2].罗卡，<a class="ae lu" rel="noopener" target="_blank" href="/understanding-generative-adversarial-networks-gans-cd6e4651a29">理解生成性对抗网络</a></p><p id="1535" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[3].J. Brownlee，<a class="ae lu" href="https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/" rel="noopener ugc nofollow" target="_blank">生成性对抗网络的温和介绍</a></p><p id="9e5f" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[4].a .拉德福德，l .梅斯，s .钦塔拉，<a class="ae lu" href="https://arxiv.org/abs/1511.06434" rel="noopener ugc nofollow" target="_blank">深度卷积生成对抗网络的无监督表示学习</a> (2015)</p><p id="6b62" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[5].惠，<a class="ae lu" href="https://medium.com/@jonathan_hui/gan-dcgan-deep-convolutional-generative-adversarial-networks-df855c438f" rel="noopener"> GAN — DCGAN(深度卷积生成对抗网络)</a></p><p id="78df" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[6].S. Yadav，<a class="ae lu" rel="noopener" target="_blank" href="/weight-initialization-techniques-in-neural-networks-26c649eb3b78">神经网络中的权重初始化技术</a></p><p id="1c87" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[7].T. Miyato，T. Kataoka，M. Koyama，Y. Yoshida，<a class="ae lu" href="https://arxiv.org/pdf/1802.05957.pdf" rel="noopener ugc nofollow" target="_blank">生成性对抗网络的谱归一化</a> (2018)</p><p id="e8af" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[8].IShengFang，<a class="ae lu" href="https://github.com/IShengFang/SpectralNormalizationKeras" rel="noopener ugc nofollow" target="_blank">在 Keras 实现光谱归一化</a></p><p id="93f1" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[9].c .科斯格罗维，<a class="ae lu" href="https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html" rel="noopener ugc nofollow" target="_blank">光谱归一化解释</a></p><p id="d884" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[10].许军，<a class="ae lu" rel="noopener" target="_blank" href="/gan-ways-to-improve-gan-performance-acf37f9f59b">氮化镓——提高氮化镓性能的方法</a></p><p id="410e" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[11].J. Brownlee，<a class="ae lu" href="https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/" rel="noopener ugc nofollow" target="_blank">如何在 Keras 中实现 GAN Hacks 来训练稳定的模型</a></p><p id="53d5" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[12].焦裕禄，<a class="ae lu" href="https://lanpartis.github.io/deep%20learning/2018/03/12/tricks-of-gans.html" rel="noopener ugc nofollow" target="_blank">GANS 的诡计</a></p><p id="c02f" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[13].C. K. Sø nderby，<a class="ae lu" href="https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/" rel="noopener ugc nofollow" target="_blank">实例噪声:稳定 GAN 训练的技巧</a></p><p id="da2a" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[14].J. Hui，<a class="ae lu" href="https://medium.com/@jonathan_hui/gan-rsgan-ragan-a-new-generation-of-cost-function-84c5374d3c6e" rel="noopener"> GAN — RSGAN &amp; RaGAN(新一代代价函数。)</a></p><p id="cb9f" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[15].D. Mack，<a class="ae lu" href="https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a" rel="noopener">对初始得分的简单解释</a></p><p id="c411" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[16].J. Hui，<a class="ae lu" href="https://medium.com/@jonathan_hui/gan-how-to-measure-gan-performance-64b988c47732" rel="noopener">GAN——如何衡量 GAN 的性能？</a></p><p id="b9f2" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[17].你所需要的就是甘的黑客</p><p id="e92e" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[18].如何训练你的敏感神经——这些方法似乎很有效。</p><p id="04e1" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[19].<a class="ae lu" href="https://www.kaggle.com/c/generative-dog-images/discussion/97809#latest-591866" rel="noopener ugc nofollow" target="_blank">解释公制 FID </a></p></div></div>    
</body>
</html>