# 机器学习，第 4 部分:强化学习

> 原文：<https://towardsdatascience.com/machine-learning-part-4-reinforcement-learning-43070cbd83ab?source=collection_archive---------13----------------------->

![](img/f655c439c1f39dde41681f4558b230c9.png)

Image by the author

# 机器学习基础

1.  [基本概念](/machine-learning-part-1-essential-concepts-c2556fb2f3e1)
2.  [监督学习](/machine-learning-part-2-supervised-learning-632621f77188)
3.  [无监督学习](/machine-learning-part-3-unsupervised-learning-6d9e59924c34)
4.  强化学习(你在这里)

强化学习在机器学习问题的世界中占有一个有趣的位置。一方面，它使用了一个反馈和改进系统，看起来类似于梯度下降的监督学习。另一方面，我们通常在解决强化学习问题时使用数据集。鉴于我们之前的所有方法都完全依赖于数据集，这种新问题如何联系起来似乎令人困惑。

强化学习和我们之前所做的当然有区别；在很多方面，强化学习本身更类似于更广泛的人工智能领域的问题。然而，在一天结束时，仍然有教师和学习者的因素，我们仍然通过反馈来学习我们问题的逻辑。强化学习是一个很好的例子，说明了整体“学习”方法可以有多广泛。

# 问题是

想象一下，你坐在桌边玩 21 点。你从来没有玩过 21 点，你不知道任何规则，也不允许任何人告诉你(奇怪，我知道)。庄家告诉你“你的总数是硬 19，庄家的上牌是 10，你可以站着或打。”即使你不知道那是什么意思，你还是选择了“打”，庄家告诉你你输了，并拿走你的钱。

你从中学到了什么？很可能下一次当你面对同样的手牌和选择击打或站立时，你会选择站立，而不是上次失败的那个选项。不是很多，但比你之前知道的要多。从根本上来说，这是**强化学习**，在这里我们学习根据类似情况下先前行动的结果选择正确的行动。

强化学习在我们事先不知道一个特定行为是“好”还是“坏”的情况下很有效，但是我们*可以*测量该行为的结果，并在事后弄清楚。这类问题出奇的普遍，计算机非常适合学习这类模式。强化学习仍然是一种学习算法，但它与我们之前讨论的有许多不同之处，这需要定义一些新的术语。

## 强化学习术语

第一任是**代理**。“代理”一词可以粗略地定义为“作用于自身环境的能力”在之前的例子中，我们的模型只是被动地对呈现给它们的数据做出反应，但在强化学习中，我们正在训练能够*主动影响*它们环境的算法。换句话说，我们用代理，也就是“代理”，来训练算法

代理通过选择一个影响他们周围环境的**动作**来完成这个任务。他们通过检查**状态**来决定这个动作，状态是与他们相关的“环境状态”。状态通过例子更容易理解(我们稍后会讲到)，但是现在假设状态包含了选择一个动作所需的所有信息。

我们将使用 **choice** 来指代在该特定状态下从所有选项中选择的动作，我们的代理做出的所有选择的组合定义了它的**行为。**我们最终试图学习最佳行为。

在强化学习中，这是通过教师元素完成的，它为每个动作提供一个分数**和**。积极的分数表示一个好的选择，而消极的分数表示一个坏的选择。可以有多个好的选择，也可以有多个坏的选择。在给定的状态下，也可以有零个好的或坏的选择。最终目标是从这些选项中找到最佳选择。分数是帮助我们找到它的反馈。

将这些术语放在上面例子的上下文中。你是代理人，因为你在做决定。你的手牌总数(19)和庄家的升牌(10)组成了*状态*。您可用的*动作*是“击中”或“站立”，在本例中，您的*选择*是“击中”那个选择的分数是输掉你下注的所有钱。如果你玩了数百手牌，你所有选择的组合将定义你在玩 21 点时的行为。

## 关注结果，而不是选择

我们之前提到了一些我们应该扩展的东西:*当我们在强化学习中得分时，我们想要判断选择的结果，而不是选择本身。*判断结果而不是行动有三个主要原因:

首先，我们对结果的了解比我们对每个行动的价值的了解要多得多。事实上，行动的价值是我们正在努力学习的，我们不知道它们的事实是隐含的。如果我们确实知道每个状态下每个动作的价值，那就根本没有强化学习的理由了。

第二，即使状态保持不变，一些动作的值也可以改变；通常这是随机的结果。这不会影响我们的栅栏绘画示例，但会影响像 21 点这样的游戏。在 21 点游戏中，在总共`15`上选择`hit`有时会赢，有时不会。稍后我们会看到更多。

最后，试图奖励*的具体行动*而不是结果会导致怪异的行为。想象一下，我们正在训练一个机器人去画栅栏:每次机器人选择“画”作为它的动作时，我们似乎应该奖励它。对一个人来说，这是有意义的，因为我们理解问题的背景:我们希望整个围栏都被油漆。

然而，机器所看到的是，它因为绘画而得到奖励，所以它可能会无休止地绘画栅栏的同一部分，直到它关闭。这看起来可能很奇怪，因为机器人是*而不是*因为移动而得到奖励，这实际上是我们评分系统的*最优*策略。

如果我们改变奖励，使其基于*状态*——换句话说，我们奖励结果而不是行动——那么我们就会得到正确的行为。从国家的角度来看，呆在一个地方画画什么也做不了，因为栅栏和以前画的一样。然而，移动和绘制一个新的部分更接近目标(并导致更高的分数)。通过正确地构建问题，我们得到了我们想要的行为。

# 与监督分类的比较

当我们看强化学习的核心循环时，我们有:做出决定(行动)，获得反馈(评分)，使用反馈来改进逻辑。与监督学习相比，我们有:作出决定(预测)，获得反馈(误差度量)，使用反馈来改善逻辑。

他们看起来非常相似，因为就他们的学习过程而言，他们是。他们都使用来自知识丰富的来源的反馈来改进他们的逻辑。然而，除此之外，他们两人之间的情况就不同了。主要的区别是他们为了学习如何接收和处理数据。

在监督学习中，我们有一个示例数据集，用正确的输出进行标记。该模型使用这些示例和标签来查找可用于预测响应值的趋势和模式。监督学习模型“知道”的一切都来自这个训练数据集。训练也是完全被动的:没有概念模型需要做任何事情来生成或访问新的训练数据。

在强化学习中，情况并非如此。使用强化学习而不是标记的训练数据，我们得到的(通常)是一组规则。我们有规则来确定某些动作何时有效，例如“当机器人靠近栅栏时，`paint`是一个有效的动作。”我们还有其他规则来规定状态如何响应动作而改变。例如“如果栅栏在当前状态下是`unpainted`，并且代理选择`paint`作为动作，那么栅栏现在是`painted`我们给每个动作打分的方式本身通常是一套规则。从`unpainted`到`painted`的栅栏可能值`+10`，但是从`painted`到`painted`的栅栏一文不值。

此外，我们无法访问列出所有可能的状态、动作和结果组合的整个数据集。相反，这些需要通过选择一个动作并使用规则集转换状态来发现。这意味着代理必须**通过选择动作、转换状态和接收反馈来探索**。换句话说，学习过程需要智能体积极地做事情，不像我们迄今为止看到的任何其他学习算法。

你可能想知道为什么我们不能创建一个包含所有可能的状态、动作和反馈的数据集，并以这种方式学习呢？我们可以，如果有可能列出每种组合的话。然而，大多数时候，枚举每一个可能的状态是*而不是*不可能的，因为它们的数量巨大，可能是无限的。我们最终会得到一个包含数万亿(或更多)例子的数据集。也可能是这样的情况，如果不选择一些起点并采取行动来观察会出现什么样的新状态，我们甚至不会真正知道哪些状态是可能的。

无论哪种方式，结果都是一样的:我们需要一个代理通过**探索**来学习状态、动作和反馈。它必须选择随机行动，看看会发生什么。这里值得强调的是，许多状态只能通过执行某些动作来达到，代理只能通过首先到达这些状态来了解这些状态:例如，如果围栏油漆机器人想要了解油漆围栏所导致的动作，但整个围栏开始未油漆，那么它需要选择首先油漆一部分。事实上，有些状态只能通过特定的动作组合才能达到，这使得学习所有正确的动作变得更加困难。

这与我们以前见过的任何东西都非常不同，需要更多的术语来描述它。这种状态和动作的互联网络的概念被称为**状态空间**。术语“空间”表明了一个事实，即代理需要通过动作从一个状态“移动”到下一个状态。围绕状态空间本身移动的行为被称为**遍历**。

# 新挑战

可以想象，这种问题会带来一系列全新的挑战。最大的问题之一就是状态空间本身的大小。许多强化学习问题的状态空间包含数十亿、数万亿甚至无限数量的状态。这些巨大的状态空间通常是因为 1)具有大量变量的非常复杂的状态，2)涉及随机性的状态，因此单个动作可能导致许多不同的结果，或者两者都有。

更专业的术语是**组合爆炸**。有时这真的无法处理，其他时候我们可以通过忽略不会影响行为的事情来减少状态空间的大小。我们有时也可以避免探索可能不属于任何最优行为的状态，因此我们可能永远不会在实践中达到它们。我们将在后面的示例中更多地讨论这两者。

这就引出了**探索与**开发的相关概念。当一个代理第一次开始学习时，它将看到状态，在大多数时间里它几乎没有数据。然后尝试随机行动，看看会发生什么/会导致什么结果，这是有意义的。这被称为**探索**，它帮助代理建立问题知识。

一旦代理获得了这些早期状态的一些知识，就值得只关注那些看起来是最佳的状态和动作。在这种情况下，代理需要在每个状态下做出“正确”的选择。这个过程被称为**利用**，因为代理正在利用它对问题的了解来做出选择。利用使我们能够改进对最佳选择的估计，并遍历一系列我们认为是最佳的状态，但可能对其了解不够。最优选择本身在这里被称为**【贪婪】**选择，因为它是当时的最佳选择，忽略了未来的任何后果。

有些选择并不贪婪，它们让我们想到了最后一个我们不会多谈的因素。不是因为它不重要，而是因为它太复杂，无法在这里深入讨论。在一个特定的问题中，最优行为可能包括做出非贪婪的选择，仅仅是为了达到一种状态，这种状态下的行动会有很大的回报。

换句话说，我们需要提前计划，以达到一种有巨大回报的状态，这种回报可以补偿达到这种状态所需的次优选择。这些问题被称为**规划**问题，需要一种方法来说明*未来*的回报以及当前的回报。正如我所说的，这本身就是一个复杂的问题，有一整个领域的不同方法，所以我们不会深入研究这个问题，但知道它的存在是很好的。

这在短时间内提供了很多信息，但这里的基本思想是，在强化学习中，我们有一个状态空间，代理只能通过遍历状态空间并从与其选择相关的反馈中学习来改进。这与我们迄今为止讨论过的任何内容都有很大不同，但幸运的是，如果你理解了这个概念，你就已经攻克了强化学习中最令人困惑的部分。学习过程本身非常简单:代理获得关于其动作的即时反馈，只需记住状态/动作/反馈的组合。

# 方法

讨论了强化学习的不同方面后，我们现在可以考虑一种方法。我们需要解决几个问题:

1.  **状态表示:**我们需要以一种方式来表示状态，这种方式包括我们需要的所有信息，排除我们不需要的信息，并且如果可能的话，我们应该减少状态空间的大小。
2.  列举选择:无论如何，我们可以做出的选择可能都是一样的。然而，在一些问题中，选择会根据状态而改变，所以当我们向代理呈现状态及其动作选择时，我们需要一种方法来确定哪些选择是有效的。
3.  **评分:**我们需要一种方法来给代理的动作评分。在某些情况下，这可能是显而易见的——例如，在赌博游戏中，我们可以使用赢/输的钱数——在其他情况下，我们需要自己构建分数。正如我们在上面提到的画栅栏机器人的例子，我们如何给动作打分对算法的成功极其重要。
4.  **探索:**我们需要算法探索状态空间的方法。至少，这很简单。如果我们已经实现了上面的所有东西，我们可以通过简单地从呈现给我们的动作中选择一个随机动作来探索。
5.  **剥削:**剥削有点难。我们需要能够存储我们之前选择的行为的值。这包括记录行动的选择和我们选择行动时的状态。如果行动的价值永远不变，那么我们只需要经历一次每个选择。如果动作*的值可以*改变，那么我们需要反复尝试每一个动作，并且我们需要存储我们选择的所有时间的平均值。选择贪婪行为的实际行为仅仅包括查找在该状态下可用的每个行为的值，并选择具有最高期望分数的一个。如果我们需要实施计划，而*不是*总是选择贪婪的行动，这需要改变，但像以前一样，我们不会在这里深入讨论。

列表中的问题 1、2 和 3 取决于具体的问题，因此没有处理它们的通用方法。然而，解决探索-开发权衡确实有一些通用的方法，其中最流行的是**ε-贪婪**算法。

在 epsilon-greedy 中，我们将 epsilon 设置为 0.0 到 1.0 之间的值，并选择以概率 epsilon 进行探索。所以如果我们 90%的时间都在探索。随着时间的推移，我们降低了ε的值，因此我们越来越经常地做出贪婪的选择。在需要一长串正确动作的问题中，较低的ε值意味着我们更有可能沿着有用的动作路径前进，而不是像我们随机探索那样完全忽略探索。这对于大型复杂的状态空间自然是最有用的。

# 例子

在这篇文章的介绍中，我谈到了只用强化学习来学习 21 点。让我们更具体地证明*可以真正做到*。我们将使用一个简化版的游戏，去掉了分裂、翻倍、投降和保险。如果你不知道这些术语的意思，不要担心，因为我们不会用到它们。对于那些想要快速复习剩余规则的人，您可以在这里找到。

## 国家代表权

让我们仔细研究一下上面列出的问题。首先，我们需要代表国家，顺便提一下，这可能是这个具体问题中最困难的部分。如上所述，带有随机元素的问题——就像这个——可能有大量的状态。四张卡的初始交易有 6，497，400 个可能的状态，所以我们希望尽可能减少。

我们还提到，如果我们能够将状态简化为必要的状态，那么组合爆炸问题将会得到解决。这是我们可以在这里做的事情。首先，代理人在做决定时不会知道经销商的“向下牌”(面朝下的牌)的值，所以从代理人的角度来看，我们可以忽略它。我们现在只有 132，600 个州，州的表示如下:

```
{
    Agent: [5 of Diamonds, 10 of Clubs],
    Dealer: [Jack of Spades]
}
```

这里有几件事值得一提。在 21 点中，一张牌的花色并不重要，许多牌值 10，所以我们应该只使用*牌值，而不是其他任何值。卡片的顺序同样不重要:一手`[5, 10]`牌和一手`[10, 5]`牌是一样的，所以我们应该确保代理意识到这一点。*

我们可以通过将状态更改为仅从玩家的角度显示一手牌的总价值来实现这一点，甚至更多。这要求我们确保自动处理 a，可以是 1 或 11，但这并不难做到(在大多数赌场中，庄家会选择对玩家更好的值)。这让我们看起来像是:

```
{ Agent: 15, Dealer:10 }
```

现在像`[10, 5]`和`[5, 10]`这样的手是一样的。这也使得像`[8, 7]`这样的手相当于`[11, 4]`，因为它们都是总计`15`。我们也忽略了经销商的降牌——尽管不同的降牌在技术上是不同的州——因为我们无论如何也不会知道。请记住，我们希望代理学习玩家的行为，所以我们需要状态来准确地表示*只有*玩家会知道什么。

通过这些改变，庄家的单卡可以是从`2`到`11`的任何值(9 个可能值)，而玩家的手牌可以是从`4`到`21`的任何值(18 个可能值)。这给了我们`9 * 18 = 162`个可能状态的总数。考虑到我们从超过 600 万个初始状态开始，这是一个巨大的进步。

## 列举选择

现在我们继续列举动作，这可能是这个问题最简单的部分。在任何时候，我们都可以选择`hit`或`stand`，不多也不少(回想一下，我们删除了允许更多操作的规则)。所以列举动作很容易。

## 得分

对于计分系统，我们只考虑这手牌是赢、输还是平(21 点术语中的“推”)。换句话说，我们对待代理人就好像它每手都下相同的赌注。这简化了评分，足以使用:

```
Winning a hand = +1
Losing a hand  = -1
Tie (Push)     =  0
```

这里实际上有一个问题。我们需要对我们的代理人做出的每一个选择给予反馈，但是这个评分系统只考虑已经完成的手牌。在我们打了牌但没有打爆的牌上，我们有一个不能用这个系统来判断的状态。如何解决这个问题？规划是一个显而易见的答案，我们可以等待这手牌结束，并将这手牌的最终价值分配给导致这手牌的每一个选择。

这是可行的，但是有两个原因使得它在这里看起来不理想。首先，这可能导致对某些选择的不合逻辑的反馈。假设开始手牌是 10，代理选择`hit`。这在客观上是正确的选择，因为当总数为 10 时，没有办法破产。点击后，我们的总数是 20，代理再次选择`hit`。毫不奇怪，这是半身像，手是一个损失。即使第一个选择`hit`是个好选择，我们还是要把损失分配给两个选择吗？

从长远来看，即使我们这样做了，它也可能会成功，但我想大多数人都会同意这个分数没有多大意义。第二个独立的问题是，将计划添加到这样一个简单的方法中会增加很多额外的复杂性。如果我们能避免，我们应该避免。

我们可以做到这一点，只要考虑到任何时候我们在不破坏的情况下增加总量通常都是一件好事。不如赢一手，但还是聊胜于无；我们将这个结果称为`Hit/increase`。为了处理最后一个边缘情况，我们需要考虑当我们击中并且*减少*我们的总数时会发生什么，这可能发生在软牌值变硬的情况下(这手牌包含被计为 11 的 a，但是在击中后变为 1)；我们称之为`Hit/decrease`。自然，这比`Hit/increase`差，也比失败好，所以我们应该在`-1`和`0.5`之间选择一个值。很难说达到和减少总数是好的、坏的还是中性的，但我将继续给它赋值`-0.2`。这给了我们最后的分数:

```
Winning a hand = +1
Losing a hand  = -1
Tie (Push)     =  0
Hit/increase   = +0.5
Hit/decrease   = -0.2
```

这些值可能不太准确，可能需要进一步测试，以找到产生最佳策略的值。不管怎样，这确实让我们可以在任何一手牌上得分，而不需要等着看最后的结果。我们通过增加得分来避免额外的计划复杂性。

## 勘探和开发

最后，我们到了问题的探索/开发部分。探索很容易，我们可以通过简单的抛硬币机制在`hit`和`stand`之间随机选择。剥削更加复杂，但也不复杂多少。我们只需要一个结构(比如 hashmap/dictionary ),它可以记录状态和动作的每种组合，并平均出我们在执行时看到的结果的值。选择贪婪的选择只是挑选估计分数最高的选择。

## 最终算法

我们现在有了一个可以学习 21 点的代理。我们可以遵循 epsilon-greedy 方法，随着时间的推移减少探索的数量，但在这种情况下，相对较少的总状态数和我们不需要担心规划的事实意味着，如果我们通过足够多的手牌随机探索，我们会获得同样好的策略(为此，我们可以轻松地模拟数百万手牌)。尽管对规则一无所知，也不需要任何复杂的规划或战略编程，这种简单的方法可以在几秒钟内找到一种几乎最佳的纸牌游戏方法。就效率而言，这种方法可能会击败除了最勤奋的人类玩家之外的所有人。

# 结论

强化学习介绍了两件事，我认为这对任何机器学习从业者都是有用的。首先，它显示了反馈和改进机制在生成逻辑时是多么灵活，因为这个问题以一种与我们以前见过的完全不同的方式出现。

其次，它引入了通过搜索状态空间来解决问题的概念。事实证明，AI 中的很多问题都可以用同样的方式概念化。这甚至可以应用于我们已经讨论过的其他形式的机器学习。比方说，我们为监督学习模型生成的参数只是所有可能参数的状态空间中的一组参数。通过在每一步对参数进行微小改变的方法(如梯度下降)找到它们，非常类似于在状态空间中搜索。

这不是一个需要理解的必要联系，但有时意识到这些不同的算法很有趣，其中大多数看起来非常独特和不同，很大程度上只是处理同一基础抽象问题的不同方式。

# 系列结论

我真诚地感谢所有阅读了本系列部分或全部内容的人。我希望它对机器学习中正在发生的事情提供了一个很好的高层次的观点，并有助于揭开这个有时感觉像魔法的领域的神秘面纱。如果对未来的帖子有任何要求或建议，请在评论中留下。