<html>
<head>
<title>Review: IDW-CNN — Learning from Image Descriptions in the Wild Dataset Boosts the Accuracy (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:IDW-CNN——从野生数据集中的图像描述中学习提高了准确性(语义分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-idw-cnn-learning-from-image-descriptions-in-the-wild-dataset-boosts-the-accuracy-807eb5ffe371?source=collection_archive---------33-----------------------#2019-07-30">https://towardsdatascience.com/review-idw-cnn-learning-from-image-descriptions-in-the-wild-dataset-boosts-the-accuracy-807eb5ffe371?source=collection_archive---------33-----------------------#2019-07-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5520" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">胜过<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------"> FCN </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------"> CRF-RNN </a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------"> DeepLabv2 </a></h2></div><p id="3e1a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">本文对中山大学、香港中文大学和商汤集团(有限公司)联合制作的<strong class="ki ir"> IDW 有线电视新闻网</strong>进行简要回顾。</p><ul class=""><li id="c76a" class="lc ld iq ki b kj kk km kn kp le kt lf kx lg lb lh li lj lk bi translated"><strong class="ki ir">通过从野生(IDW)数据集中的图像描述进行学习，提高了分割精度。</strong></li><li id="abd4" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">与以前的图像字幕数据集不同，在以前的图像字幕数据集中，字幕是手动密集注释的，<strong class="ki ir">IDW 的图像及其描述是自动从互联网上下载的，无需任何手动清理和提炼。</strong></li></ul><p id="63c6" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这是一篇<strong class="ki ir"> 2017 年 CVPR </strong>论文，引用<strong class="ki ir">数十次</strong>。(<a class="lq lr ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----807eb5ffe371--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="8bba" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">概述</h1><ol class=""><li id="4b4b" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb mw li lj lk bi translated"><strong class="ki ir">在野外(IDW)数据集中构建图像描述</strong></li><li id="ede3" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb mw li lj lk bi translated"><strong class="ki ir"> IDW-CNN 架构</strong></li><li id="3695" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb mw li lj lk bi translated"><strong class="ki ir">培训方式</strong></li><li id="4996" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb mw li lj lk bi translated"><strong class="ki ir">实验结果</strong></li></ol></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="3130" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated"><strong class="ak"> 1。在野外(IDW)数据集构建图像描述</strong></h1><ul class=""><li id="8f65" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated">IDW 分两级建造。</li></ul><h2 id="008f" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">1.1.第一阶段的</h2><ul class=""><li id="3add" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated"><strong class="ki ir">准备了</strong>中经常出现的 21 个介词和动词，如“hold”、“play with”、“hug”、“ride”和“stand near”，以及来自 VOC12 的 20 个对象类别，如“person”、“cow”、“bike”、“sheep”和“table”，<strong class="ki ir">。</strong></li><li id="b7a4" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><strong class="ki ir">他们在‘主语+动词/prep’方面的组合。“+ object”导致 20×21×20 = 8400 个不同的短语</strong>，例如“骑自行车的人”、“坐在自行车附近的人”和“站在自行车附近的人”。</li><li id="b20e" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">有些在实践中很少出现，例如“牛抱羊”。</li><li id="fbfe" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">在去除无意义的短语之后，收集了数百个有意义的短语。</li></ul><h2 id="be70" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">1.2.第二级</h2><ul class=""><li id="26c1" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated">这些短语被用作从因特网上搜索图像及其周围文本的关键词。</li><li id="c7ee" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><strong class="ki ir">无效短语，如“人骑牛”，如果其检索图像的数量小于 150，则被丢弃</strong>，以防止可能导致训练中过拟合的罕见情况或异常值。</li><li id="d047" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">结果，获得了 59 个有效短语。最后，<strong class="ki ir"> IDW 有 41421 张图片和描述</strong>。</li></ul><figure class="nk nl nm nn gt no gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/7bf2ac0c19121c1d53e89f637de2968c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*-YZ_m12nPdwIvt9S4sT9UA.png"/></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">The number of images in IDW with respect to each object category in PASCAL VOC 2012</strong></figcaption></figure><ul class=""><li id="f53f" class="lc ld iq ki b kj kk km kn kp le kt lf kx lg lb lh li lj lk bi translated">上面的直方图揭示了现实世界中这些物体的图像分布，没有任何手动清理和细化。</li></ul><h2 id="969b" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">1.3.图像描述表示</h2><ul class=""><li id="5b9f" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated">每一个图像描述都被自动转换成一个解析树，在这里我们选择有用的物体(例如名词)和动作(例如动词)作为训练过程中的监督。</li><li id="6e06" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">两个对象的每个配置和它们之间的动作可以被认为是对象交互，这对于图像分割是有价值的信息，但是在 VOC12 的 labelmaps 中没有给出。</li></ul><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi nw"><img src="../Images/2129f88ae2b98159cd84281325847d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0LmMf7YcRSEZW2fztSocaw.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">The constituency tree generated by language parser</strong></figcaption></figure><ul class=""><li id="cbc8" class="lc ld iq ki b kj kk km kn kp le kt lf kx lg lb lh li lj lk bi translated">首先，<strong class="ki ir">斯坦福解析器用于解析图像描述并生成选区树</strong>，如上所述。<strong class="ki ir">但是它仍然包含不相关的词</strong>，既不描述对象类别也不描述交互。</li><li id="ff45" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">然后需要<strong class="ki ir">将选区树转换成语义树</strong>，语义树只包含对象及其交互。1)根据词类过滤叶节点，仅保留名词作为对象候选，保留动词和介词作为动作候选。2)名词转宾语。WordNet 中统一同义词的词汇关系数据。不属于 20 个对象类别的名词将从树中移除。3)使用 word2vec 将动词映射到定义的 21 个动作。4)通过节点从语义树中提取对象交互。</li><li id="1ad6" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">如‘姑娘玩小羊，牵着小羊’先被过滤出描写，再进一步转移成‘人玩小羊，牵着小羊’。</li><li id="5971" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">在解析了 IDW 中的所有图像描述之后，总共获得了 62，100 个对象交互。</li></ul><figure class="nk nl nm nn gt no gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/aa8ee7ebd7cc736e13b8ef0c7f5e521e.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*FDTYNRDLq0mSB9NsFJrrIA.png"/></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">The number of images with respect to the number of interactions, showing that each image has 1.5 interactions on average.</strong></figcaption></figure><blockquote class="oc"><p id="a023" class="od oe iq bd of og oh oi oj ok ol lb dk translated">与之前的数据集相比，IDW 的构建无需人工干预且费用极低。</p></blockquote><figure class="on oo op oq or no gh gi paragraph-image"><div class="gh gi om"><img src="../Images/39b8d6fba18d1253bc74519ee168958c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*mZtOBM3B7kuOyKTR1L-B4Q.png"/></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">The constituency tree after POS tag filtering (Left), and object interactions (Right)</strong></figcaption></figure><h2 id="1f54" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">1.4.三个测试集</h2><ul class=""><li id="04fe" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated"><strong class="ki ir"> int-IDW </strong>:从 IDW 随机选择<strong class="ki ir">1440 张图像作为物体交互预测的测试集。</strong></li><li id="215e" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><strong class="ki ir"> seg-IDW </strong>:在 int-IDW 中为每幅图像标注<strong class="ki ir">每像素 labelmap，产生一个分割测试集。就每幅图像中的物体多样性而言，seg-IDW 比 VOC12 更具挑战性。</strong></li><li id="06b5" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><strong class="ki ir">零 IDW </strong> : <strong class="ki ir">零拍摄测试集包括 1000 张看不见的物体相互作用的图像</strong>。例如,“人骑牛”的形象很少见(如在斗牛中),在训练中不会出现。</li></ul></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="22a1" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">2.<strong class="ak"> IDW-CNN 架构</strong></h1><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi os"><img src="../Images/9a06a881cb618849bbfd50c7914697d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Flvhemyv2I0O07NgO_qvqw.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">(a) IDW-CNN, which has two streams, (b) Each subnet has the same network structure.</strong></figcaption></figure><ul class=""><li id="4147" class="lc ld iq ki b kj kk km kn kp le kt lf kx lg lb lh li lj lk bi translated">该网络可以分为三个主要部分。</li></ul><h2 id="7da6" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">2.1.特征抽出</h2><ul class=""><li id="3e99" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated">IDW-CNN 采用<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------"> DeepLabv2 </a>作为特征提取的构建模块。</li><li id="3621" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><strong class="ki ir"> IDW-CNN 仅从</strong> <a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------"> <strong class="ki ir"> DeepLabv2 </strong> </a>继承了<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------"><strong class="ki ir">ResNet-101</strong></a><strong class="ki ir">，而<strong class="ki ir">则去除了多尺度融合和 CRF </strong>等其他组件。</strong></li><li id="cb3e" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">给定一幅图像<em class="ot"> I </em>，<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------"> ResNet-101 </a>产生<strong class="ki ir"> 2048 通道</strong>的特征。每个通道的尺寸为<strong class="ki ir"> 45 × 45 </strong>。</li></ul><h2 id="0de9" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">2.2.分段流</h2><ul class=""><li id="33ca" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated">上述特征被卷积层用来预测<strong class="ki ir">分割标签图</strong>(表示为 Is)，其大小为<strong class="ki ir"> 21×45×45 </strong>。</li></ul><h2 id="97de" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">2.3.内部流</h2><ul class=""><li id="3af5" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated">这条河流有三个阶段。</li><li id="52f9" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">在<strong class="ki ir">第一个</strong>阶段，我们<strong class="ki ir">通过一个卷积层将特征通道的数量从 2048 个减少到 512 个</strong>，记为<strong class="ki ir"> <em class="ot"> h </em> </strong>，以减少后续阶段的计算量。</li><li id="1de6" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><em class="ot"> hm </em>、<em class="ot"> hm_i </em>中的每个特征图都是通过预成型<em class="ot"> h </em>和<em class="ot"> Is </em>每个通道之间的元素乘积(“⊗”)得到的，代表一个掩膜。因此，512×45×45 的每个<em class="ot"> hm_i </em> ∈ <em class="ot"> R </em>代表第<em class="ot"> i </em>个对象类的被屏蔽特征。</li><li id="8a2f" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">在<strong class="ki ir">第二</strong>阶段，利用每个<em class="ot"> hm_i </em>作为输入来训练相应的对象子网，该子网输出表征对象<em class="ot"> i </em>是否出现在图像<em class="ot"> I </em>中的概率。</li><li id="4dec" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><strong class="ki ir">训练了 21 个对象子网</strong>，这些子网具有相同的网络结构，但其参数不共享，除非全连接层共享。在上图右侧的<strong class="ki ir">橙色</strong>处。</li><li id="4b05" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><strong class="ki ir">总体来说，第二阶段决定了哪些对象出现在<em class="ot"> I </em>中。</strong></li><li id="6fc2" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">在<strong class="ki ir">第三</strong>阶段，<strong class="ki ir">训练了 22 个动作子网，</strong>每个动作子网预测两个出现的对象之间的动作。上图右边的蓝色部分是。</li><li id="a640" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">例如，如果“人”和“自行车”都出现在<em class="ot"> I </em>中，则它们的特征组合，512×45×45 的<em class="ot">hm _ person</em>⊕<em class="ot">hm _ bike</em>∈<em class="ot">r</em>被传播到所有动作子网。</li><li id="c835" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">最大的响应很可能是由以下动作子网之一产生的:“乘坐”、“坐在附近”和“站在附近”。</li></ul><h2 id="f86c" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">2.4.对象对选择</h2><ul class=""><li id="1bd0" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated">OPS <strong class="ki ir">合并所呈现对象的特征</strong>。在上图左侧的<strong class="ki ir">紫色</strong>处。</li><li id="99c4" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">例如，如果“人”、“自行车”和“汽车”的对象子网具有高响应，则<em class="ot"> hm_person </em>、<em class="ot"> hm_bike </em>和<em class="ot"> hm_car </em>中的每对特征被元素地加在一起，产生三个组合特征，分别表示为<em class="ot"> hm_person+bike </em>、<em class="ot"> hm_person+car </em>和<em class="ot"> hm_bike+car </em>。</li><li id="6f55" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">每个合并的特征然后被转发到所有 22 个动作子网。</li></ul><h2 id="7ce1" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">2.5.精炼</h2><ul class=""><li id="f886" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated">第<em class="ot"> i </em>个对象子网产生一个分数(概率)，所有 21 个分数串接成一个向量。</li><li id="3ad2" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">它被当作一个滤波器来使用卷积来改进分割图<em class="ot">和</em>。</li></ul></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="d620" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">3.培训方法</h2><ul class=""><li id="6c70" class="lc ld iq ki b kj mr km ms kp mt kt mu kx mv lb lh li lj lk bi translated">IDW 的每张图片都包含了物体间的相互作用，但是没有标签地图。</li><li id="3b5e" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">VOC12 中的每个图像都有 labelmap，但没有交互。</li><li id="8596" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">IDW-CNN 为每个样本估计一个伪标签，并将其视为 BP 中的基本事实。</li><li id="1f53" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">对于<strong class="ki ir"> Seg-stream </strong>，通过<strong class="ki ir">结合预测分割图、<em class="ot"> Is_idw </em>和预测对象标签、<em class="ot"> lo_idw </em> </strong>，将潜在的<em class="ot"> Is_idw </em>估计为“<strong class="ki ir">伪地真值</strong>”。</li><li id="127a" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">对于<strong class="ki ir"> Int-stream </strong>，针对每对对象之间的动作获得一个<strong class="ki ir">先验分布</strong>。对于“自行车”和“人”，这种先验产生的概率比上述四个动作高，比其他动作低。在训练阶段，如果预测动作是上述之一，损失函数提供<strong class="ki ir">低惩罚，否则提供高惩罚。</strong></li></ul></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="9715" class="mx ma iq bd mb my mz dn mf na nb dp mj kp nc nd ml kt ne nf mn kx ng nh mp ni bi translated">4.实验结果</h2><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi ou"><img src="../Images/da8f4a437e7041b49f8e6afa52dc199e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MbQ1snUOfvMsStK0Rz8Wzg.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">VOC12 test set</strong></figcaption></figure><ul class=""><li id="9d0a" class="lc ld iq ki b kj kk km kn kp le kt lf kx lg lb lh li lj lk bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------"><strong class="ki ir">ResNet-101</strong></a>:74.2% mIoU。</li><li id="1aa9" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><strong class="ki ir"> IDW-CNN(10k) </strong> : 10k IDW 训练图像，81.8% mIoU。</li><li id="a156" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><strong class="ki ir"> IDW-CNN(20k) </strong> : 20k IDW 训练图像，85.2% mIoU。</li><li id="4e0d" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated"><strong class="ki ir"> IDW-CNN(40k) </strong> : 40k IDW 图像用于训练，86.3% mIoU。并且它胜过 SOTA 的方法，如<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------"> FCN </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------"> CRF-RNN </a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------"> DeepLabv2 </a>。</li></ul><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi ov"><img src="../Images/b7a054a5c2457e337bb351575dfa9a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1QcBic_W-iROix7FsToFsw.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">seg-IDW dataset</strong></figcaption></figure><ul class=""><li id="7216" class="lc ld iq ki b kj kk km kn kp le kt lf kx lg lb lh li lj lk bi translated">类似地，在赛格-IDW 数据集上，IDW-CNN(40k)具有最好的性能。</li></ul><figure class="nk nl nm nn gt no gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/9b7d37342117d105f0143dc203af944d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*isDoCpF2cc4Mddu0bT-0Cg.png"/></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">Recall of object interaction prediction</strong></figcaption></figure><ul class=""><li id="7ada" class="lc ld iq ki b kj kk km kn kp le kt lf kx lg lb lh li lj lk bi translated">回忆- <em class="ot"> n </em> ( <em class="ot"> n </em> = 5，10)，衡量真实交互在前 5 或 10 个预测交互中的可能性。</li><li id="d0aa" class="lc ld iq ki b kj ll km lm kp ln kt lo kx lp lb lh li lj lk bi translated">IDW-CNN 在 5 次召回中领先其他公司 3%。</li></ul><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi ox"><img src="../Images/8f331e7fe8b5de8b39376e0f2aceb1c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*VjVz9mVtL4So7CaIZhpCXw.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">Recall on zero-IDW</strong></figcaption></figure><ul class=""><li id="1fec" class="lc ld iq ki b kj kk km kn kp le kt lf kx lg lb lh li lj lk bi translated">IDW-CNN 全模式仍然获得最高召回率。</li></ul><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi oy"><img src="../Images/49db130e93a94489028dfb50ff70c12f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nzaHHVK2CxDE2LqsOgk9xQ.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><strong class="bd nv">Some Visualizations</strong></figcaption></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="0967" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">参考</h1><p id="2327" class="pw-post-body-paragraph kg kh iq ki b kj mr jr kl km ms ju ko kp oz kr ks kt pa kv kw kx pb kz la lb ij bi translated">【2017 CVPR】【IDW-CNN】<br/><a class="ae kf" href="http://personal.ie.cuhk.edu.hk/~pluo/pdf/wangLLWcvpr17.pdf" rel="noopener ugc nofollow" target="_blank">用于语义图像分割的学习对象交互和描述</a></p><h1 id="a72d" class="lz ma iq bd mb mc pc me mf mg pd mi mj jw pe jx ml jz pf ka mn kc pg kd mp mq bi translated">我以前的评论</h1><p id="8c57" class="pw-post-body-paragraph kg kh iq ki b kj mr jr kl km ms ju ko kp oz kr ks kt pa kv kw kx pb kz la lb ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(是)(这)(些)(人)(,)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(但)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(想)(要)(让)(这)(些)(人)(都)(有)(这)(些)(情)(况)(,)(我)(们)(还)(不)(想)(要)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(就)(是)(这)(些)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(都)(是)(很)(强)(的)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(起)(来)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(</p><p id="2347" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">物体检测</strong> [ <a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------" rel="noopener">过食</a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------" rel="noopener">快 R-CNN </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------">快 R-CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------">MR-CNN&amp;S-CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------">DeepID-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858?source=post_page---------------------------">CRAFT</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------">R-FCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------">离子</a><a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------"> [</a><a class="ae kf" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------">G-RMI</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------">yolo v1</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------">yolo v2/yolo 9000</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------">yolo v3</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------">FPN</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------">retina net</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------">DCN</a></p><p id="923e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">语义切分</strong>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------">FCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=post_page---------------------------">de convnet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------">deeplabv 1&amp;deeplabv 2</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------">CRF-RNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96?source=post_page---------------------------">SegNet</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990?source=post_page---------------------------" rel="noopener">parse net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5?source=post_page---------------------------">dilated net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5?source=post_page---------------------------">DRN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=post_page---------------------------">RefineNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-gcn-global-convolutional-network-large-kernel-matters-semantic-segmentation-c830073492d2?source=post_page---------------------------"/></p><p id="e573" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">生物医学图像分割</strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6?source=post_page---------------------------" rel="noopener">cumevision 1</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560?source=post_page---------------------------" rel="noopener">cumevision 2/DCAN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=post_page---------------------------">U-Net</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6?source=post_page---------------------------" rel="noopener">CFS-FCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43?source=post_page---------------------------" rel="noopener">U-Net+ResNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc?source=post_page---------------------------">多通道</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974?source=post_page---------------------------">V-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1?source=post_page---------------------------">3D U-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1?source=post_page---------------------------">M FCN</a>]</p><p id="d25b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">实例分割</strong> [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b?source=post_page---------------------------" rel="noopener"> SDS </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979?source=post_page---------------------------">超列</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339?source=post_page---------------------------">深度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61?source=post_page---------------------------">清晰度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------">多路径网络</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34?source=post_page---------------------------"> MNC </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92?source=post_page---------------------------">实例中心</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2?source=post_page---------------------------"> FCIS </a></p><p id="bec0" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">)( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )(</p><p id="9a2e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=post_page---------------------------"> DeepPose </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=post_page---------------------------"> Tompson NIPS'14 </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------"> Tompson CVPR'15 </a> <a class="ae kf" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------" rel="noopener"> CPM </a></strong></p></div></div>    
</body>
</html>