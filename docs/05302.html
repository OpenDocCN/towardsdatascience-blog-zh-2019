<html>
<head>
<title>Feature selection using Python for classification problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 解决分类问题的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-using-python-for-classification-problem-b5f00a1c7028?source=collection_archive---------5-----------------------#2019-08-07">https://towardsdatascience.com/feature-selection-using-python-for-classification-problem-b5f00a1c7028?source=collection_archive---------5-----------------------#2019-08-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div class="gh gi jx"><img src="../Images/cf946b96d257b98c46a5d789f1a0b223.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*EGVBYESy1g1xWIz4pUSmjg.png"/></div></figure><h1 id="ff87" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">介绍</h1><p id="4245" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在模型中包含更多要素会使模型更加复杂，并且模型可能会过度拟合数据。一些特征可能是噪声，并可能损坏模型。通过移除那些不重要的特征，该模型可以更好地概括。</p><p id="f8ee" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">Sklearn 网站列出了不同的功能选择方法。这篇文章主要是基于那个网站的主题。然而，我收集了关于这些方法背后的理论的不同资源，并将它们添加到本文中。此外，我在同一个数据集上应用了不同的特征选择方法来比较它们的性能。</p><p id="7bba" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">阅读完这篇文章和我在文章中提供的参考资料后，您应该能够理解特性选择背后的理论以及如何使用 python 来完成它。</p><h2 id="a3f1" class="mg kg it bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated">获取数据</h2><p id="1422" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我们使用虹膜数据集。有关此数据的更多信息:</p><p id="9abe" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><a class="ae ms" href="https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/datasets/plot _ iris _ dataset . html</a></p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="cc2f" class="mg kg it mu b gy my mz l na nb">iris = load_iris()<br/>X = iris.data<br/>y = iris.target<br/>print(X[: 5, :])<br/>print(X.shape)</span><span id="cd55" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output: <br/></strong>[[5.1 3.5 1.4 0.2]<br/> [4.9 3.  1.4 0.2]<br/> [4.7 3.2 1.3 0.2]<br/> [4.6 3.1 1.5 0.2]<br/> [5.  3.6 1.4 0.2]]</span><span id="b9bf" class="mg kg it mu b gy nc mz l na nb">(150, 4)</span></pre><p id="b180" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">这些数据有四个特征。为了测试不同特征选择方法的有效性，我们向数据集添加了一些噪声特征。</p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="6870" class="mg kg it mu b gy my mz l na nb">np.random.seed(100)<br/>E = np.random.uniform(0, 1, size=(len(X), 10))<br/>X = np.hstack((X, E))<br/>print(X.shape)</span><span id="aa64" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>(150, 14)</span></pre><p id="2d05" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">数据现在有 14 个特征。</p><p id="4c28" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">在应用特征选择方法之前，我们需要首先分割数据。原因是我们只基于来自训练集的信息选择特征，而不是基于整个数据集。我们应该拿出整个数据集的一部分作为测试集来评估特征选择和模型的性能。因此，在我们进行特征选择和训练模型时，无法看到来自测试集的信息。</p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="51d4" class="mg kg it mu b gy my mz l na nb">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100, test_size=0.3)<br/>print(X_train.shape)</span><span id="b506" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>(105, 14)</span></pre><p id="dd3b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">我们将应用基于 X_train 和 y_train 的特征选择。</p><h2 id="6a56" class="mg kg it bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated">I .移除具有低方差的特征</h2><p id="c96a" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><em class="nd">变量阈值</em>是一种简单的特征选择基线方法。它会移除方差未达到某个阈值的所有要素。默认情况下，它会移除所有零方差特征。<br/><a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold" rel="noopener ugc nofollow" target="_blank">https://scikit-learn . org/stable/modules/generated/sk learn . feature _ selection。variance threshold . html # sk learn . feature _ selection。变量阈值</a></p><p id="7607" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">显然，我们的数据没有任何零方差特征。但是为了演示的目的，我仍然在这里应用这个方法。</p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="92d2" class="mg kg it mu b gy my mz l na nb">sel_variance_threshold = VarianceThreshold() <br/>X_train_remove_variance = sel_variance_threshold.fit_transform(X_train)<br/>print(X_train_remove_variance.shape)</span><span id="4f97" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>(105, 14)</span></pre><p id="2c47" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">数据仍然有 14 个特征，没有一个特征被删除。</p><h2 id="ee9c" class="mg kg it bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated">二。单变量特征选择</h2><p id="c741" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">单变量特征选择通过基于单变量统计测试选择最佳特征来工作。我们将每个特征与目标变量进行比较，以查看它们之间是否有任何统计上的显著关系。它也被称为方差分析(ANOVA)。当我们分析一个特征和目标变量之间的关系时，我们忽略了其他特征。这就是它被称为“单变量”的原因。每个特性都有它的测试分数。<br/>最后，比较所有的测试分数，将选择分数最高的特征。</p><p id="7fd0" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu"> (1)使用卡方检验。</strong></p><p id="b5ec" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">有关卡方检验的更多信息，请阅读:</p><ul class=""><li id="b875" class="ne nf it lf b lg mb lk mc lo ng ls nh lw ni ma nj nk nl nm bi translated">统计:<a class="ae ms" href="http://vassarstats.net/textbook/," rel="noopener ugc nofollow" target="_blank">http://vassarstats.net/textbook/,</a>第八章</li><li id="5b42" class="ne nf it lf b lg nn lk no lo np ls nq lw nr ma nj nk nl nm bi translated">sk learn:<a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ selection . chi 2 . html # sk learn . feature _ selection . chi 2</a></li></ul><p id="0b1b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">如果你想了解更多关于 Sklearn 如何应用卡方检验的信息，请阅读源代码:<a class="ae ms" href="https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/feature_selection/univariate_selection.py#L172" rel="noopener ugc nofollow" target="_blank">https://github . com/scikit-learn/scikit-learn/blob/1495 f 6924/sk learn/feature _ selection/univariate _ selection . py # L172</a></p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="c26b" class="mg kg it mu b gy my mz l na nb">sel_chi2 = SelectKBest(chi2, k=4)    # select 4 features<br/>X_train_chi2 = sel_chi2.fit_transform(X_train, y_train)<br/>print(sel_chi2.get_support())</span><span id="0fe5" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:</strong></span><span id="74cf" class="mg kg it mu b gy nc mz l na nb">[ True  True  True  True False False False False False False False False False False]</span></pre><p id="9e15" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">数组中的前四个元素为真，这意味着前四个特征是通过这种方法选择的。由于这些特征是数据中的原始特征，卡方检验表现良好。</p><p id="8a2c" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu"> (2)使用 f 检验</strong></p><ul class=""><li id="55e6" class="ne nf it lf b lg mb lk mc lo ng ls nh lw ni ma nj nk nl nm bi translated">统计数字</li></ul><p id="0e88" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">单向方差分析:</p><p id="12ec" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><a class="ae ms" href="http://vassarstats.net/textbook/," rel="noopener ugc nofollow" target="_blank">http://vassarstats.net/textbook/,</a>第十四章</p><p id="070b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">回归方差分析:</p><p id="4500" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><a class="ae ms" href="http://facweb.cs.depaul.edu/sjost/csc423/documents/f-test-reg.htm" rel="noopener ugc nofollow" target="_blank">http://fac web . cs . de Paul . edu/shost/CSC 423/documents/f-test-reg . htm</a></p><ul class=""><li id="6d71" class="ne nf it lf b lg mb lk mc lo ng ls nh lw ni ma nj nk nl nm bi translated">Sklearn</li></ul><p id="7843" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">分类测试:</p><p id="78eb" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ selection . f _ classif . html # sk learn . feature _ selection . f _ classif</a></p><p id="1d74" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">回归检验:</p><p id="4814" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ selection . f _ regression . html # sk learn . feature _ selection . f _ regression</a></p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="bf59" class="mg kg it mu b gy my mz l na nb">sel_f = SelectKBest(f_classif, k=4)<br/>X_train_f = sel_f.fit_transform(X_train, y_train)<br/>print(sel_f.get_support())</span><span id="715d" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>[ True  True  True  True False False False False False False False False False False]</span></pre><p id="e4f9" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">f 检验也能正确选择原始特征。</p><p id="3317" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu"> (3)使用 mutual_info_classif 测试</strong></p><ul class=""><li id="cb44" class="ne nf it lf b lg mb lk mc lo ng ls nh lw ni ma nj nk nl nm bi translated">用于分类:<a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ selection . mutual _ info _ class if . html # sk learn . feature _ selection . mutual _ info _ class if</a></li><li id="1a87" class="ne nf it lf b lg nn lk no lo np ls nq lw nr ma nj nk nl nm bi translated">对于回归:<a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ selection . mutual _ info _ regression . html # sk learn . feature _ selection . mutual _ info _ regression</a></li></ul><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="f280" class="mg kg it mu b gy my mz l na nb">sel_mutual = SelectKBest(mutual_info_classif, k=4)<br/>X_train_mutual = sel_mutual.fit_transform(X_train, y_train)<br/>print(sel_mutual.get_support())</span><span id="0df0" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>[ True  True  True  True False False False False False False False False False False]</span></pre><p id="17b9" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">总之，三种单变量特征选择方法产生相同的结果。</p><p id="665e" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">我们使用虹膜数据作为分类问题。对于回归问题，同样，我们可以使用 f_regression，mutual_info_regression 来进行特征选择。</p><h2 id="ab03" class="mg kg it bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated">三。递归特征消除</h2><p id="1e0c" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">给定将权重分配给特征(例如，线性模型的系数)的外部估计器，递归特征消除(RFE)是通过递归地考虑越来越小的特征集来选择特征。首先，在初始特征集上训练估计器，并且通过<em class="nd"> coef_ </em>属性或通过<em class="nd"> feature_importances_ </em>属性获得每个特征的重要性。然后，从当前特征集中删除最不重要的特征。该过程在删减集上递归重复，直到最终达到要选择的特征的期望数量。</p><p id="2ac9" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">关于 Sklearn 中的 RFE:<a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ selection。rfe . html # sk learn . feature _ selection。RFE </a></p><p id="31f1" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu"> (1)使用逻辑回归作为模型</strong></p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="07f0" class="mg kg it mu b gy my mz l na nb">model_logistic = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)<br/>sel_rfe_logistic = RFE(estimator=model_logistic, n_features_to_select=4, step=1)<br/>X_train_rfe_logistic = sel_rfe_logistic.fit_transform(X_train, y_train)</span><span id="6e23" class="mg kg it mu b gy nc mz l na nb">print(sel_rfe_logistic.get_support())</span><span id="0bb7" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>[False  True  True  True False False False False False False False False False  True]</span><span id="3cbc" class="mg kg it mu b gy nc mz l na nb">print(sel_rfe_logistic.ranking_)</span><span id="d0fe" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>array([ 3,  1,  1,  1,  9,  8,  7,  6,  5,  4,  2, 11, 10,  1])</span></pre><p id="c326" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">所选择的特征是等级 1。结果表明，递归特征消除仅选择了部分原始特征和一个噪声特征。这不是我们想要的理想结果。让我们试试另一个模型:</p><p id="24ad" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu"> (2)使用随机森林作为模型</strong></p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="3d07" class="mg kg it mu b gy my mz l na nb">model_tree = RandomForestClassifier(random_state=100, n_estimators=50)<br/>sel_rfe_tree = RFE(estimator=model_tree, n_features_to_select=4, step=1)<br/>X_train_rfe_tree = sel_rfe_tree.fit_transform(X_train, y_train)<br/>print(sel_rfe_tree.get_support())</span><span id="9423" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>[ True  True  True  True False False False False False False False False False False]</span></pre><p id="1030" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">在递归特征选择中使用随机森林作为模型可以在这种情况下选择正确的特征。</p><h2 id="3c36" class="mg kg it bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated">四。使用 SelectFromModel 进行特征选择</h2><p id="fdbe" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">SelectFromModel 是一个元转换器，可以与拟合后具有<em class="nd"> coef_ </em>或<em class="nd"> feature_importances_ </em>属性的任何估计器一起使用。如果相应的<em class="nd"> coef_ </em>或<em class="nd"> feature_importances_ </em>值低于提供的阈值参数，则这些特征被视为不重要并被移除。</p><p id="b63e" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">与单变量特征选择相比，基于模型的特征选择一次考虑所有特征，因此可以捕捉交互。用于特征选择的模型不需要与稍后用于训练的模型相同。</p><p id="e026" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">sk learn:<a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ selection。select from model . html # sk learn . feature _ selection。从模型中选择</a></p><p id="7931" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu"> (1)基于 L1 的特征选择</strong></p><p id="1770" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">具有 L1 惩罚的线性模型可以消除一些特征，因此可以在使用另一个模型来拟合数据之前充当特征选择方法。</p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="db49" class="mg kg it mu b gy my mz l na nb">model_logistic = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=10000, penalty='l1')<br/>sel_model_logistic = SelectFromModel(estimator=model_logistic)<br/>X_train_sfm_l1 = sel_model_logistic.fit_transform(X_train, y_train)<br/>print(sel_model_logistic.get_support())</span><span id="397f" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>[ True False  True  True False False False False False  True False False False False]</span></pre><p id="8402" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">它选择了错误的功能！选择噪声中的一个要素时，原始数据集中的第二个要素被忽略。</p><p id="65c5" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu"> (2)基于树的特征选择</strong></p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="8252" class="mg kg it mu b gy my mz l na nb">model_tree = RandomForestClassifier(random_state=100, n_estimators=50)<br/>model_tree.fit(X_train, y_train)<br/>print(model_tree.feature_importances_)</span><span id="ab1a" class="mg kg it mu b gy nc mz l na nb">sel_model_tree = SelectFromModel(estimator=model_tree, prefit=True, threshold='mean')  <br/>      # since we already fit the data, we specify prefit option here<br/>      # Features whose importance is greater or equal to the threshold are kept while the others are discarded.</span><span id="021a" class="mg kg it mu b gy nc mz l na nb">X_train_sfm_tree = sel_model_tree.transform(X_train)<br/>print(sel_model_tree.get_support())</span><span id="6bd5" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:</strong></span><span id="6151" class="mg kg it mu b gy nc mz l na nb">[0.11844633 0.09434048 0.21340848 0.33708242 0.02019553 0.03081254<br/> 0.02317242 0.01962394 0.02407251 0.02193159 0.03289007 0.01836624<br/> 0.01811144 0.027546  ]</span><span id="dc28" class="mg kg it mu b gy nc mz l na nb">[ True  True  True  True False False False False False False False False False False]</span></pre><p id="0505" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">它选择正确的原始特征。</p><p id="ebed" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu">综上所述的所有特征选择方法:</strong>对于这种特定的数据集，使用 logistic 模型作为递归特征剔除或模型选择会错误地选择特征。另一方面，所有其他特征选择方法都正确地选择了前四个特征。</p><h2 id="0bad" class="mg kg it bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated">让我们比较特性选择前后的性能</h2><p id="ce8a" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lf iu"> (1)特征选择前</strong></p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="ebd1" class="mg kg it mu b gy my mz l na nb">model_logistic = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=10000)<br/>model_logistic.fit(X_train, y_train)<br/>predict = model_logistic.predict(X_test)<br/>print(confusion_matrix(y_test, predict))<br/>print(classification_report(y_test, predict))</span><span id="0c8a" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>[[16  0  0]<br/> [ 0 11  0]<br/> [ 0  2 16]]<br/>              precision    recall  f1-score   support</span><span id="8d9b" class="mg kg it mu b gy nc mz l na nb">           0       1.00      1.00      1.00        16<br/>           1       0.85      1.00      0.92        11<br/>           2       1.00      0.89      0.94        18</span><span id="cba0" class="mg kg it mu b gy nc mz l na nb">    accuracy                           0.96        45<br/>   macro avg       0.95      0.96      0.95        45<br/>weighted avg       0.96      0.96      0.96        45</span></pre><p id="08ba" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu"> (2)特征选择后</strong></p><p id="656e" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">我们使用基于卡方检验的特征选择的结果。<br/> <em class="nd"> X_train_chi2 </em>是特征选择后馈入模型的数据。</p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="3e4d" class="mg kg it mu b gy my mz l na nb">model_logistic = LogisticRegression(solver=’saga’, multi_class=’multinomial’, max_iter=10000)<br/>model_logistic.fit(X_train_chi2, y_train)</span></pre><p id="d233" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">我们还需要转换测试数据，因为特性的数量发生了变化。</p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="96ba" class="mg kg it mu b gy my mz l na nb">X_test_chi2 = sel_chi2.transform(X_test)<br/>print(X_test.shape)<br/>print(X_test_chi2.shape)</span><span id="b27b" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>(45, 14)<br/>(45, 4)</span></pre><p id="e36a" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">仅使用测试集中与训练集中的剩余特征相对应的特征。这种情况下有四个特征。</p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="d140" class="mg kg it mu b gy my mz l na nb">predict = model_logistic.predict(X_test_chi2)<br/>print(confusion_matrix(y_test, predict))<br/>print(classification_report(y_test, predict))</span><span id="e295" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">output:<br/></strong>[[16  0  0]<br/> [ 0 11  0]<br/> [ 0  1 17]]<br/>              precision    recall  f1-score   support</span><span id="6ebe" class="mg kg it mu b gy nc mz l na nb">           0       1.00      1.00      1.00        16<br/>           1       0.92      1.00      0.96        11<br/>           2       1.00      0.94      0.97        18</span><span id="d98d" class="mg kg it mu b gy nc mz l na nb">    accuracy                           0.98        45<br/>   macro avg       0.97      0.98      0.98        45<br/>weighted avg       0.98      0.98      0.98        45</span></pre><p id="1f41" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu">总之，</strong>特征选择去除了噪声，更好地泛化了模型，从而提高了模型性能。</p><h2 id="862b" class="mg kg it bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated">关于噪音</h2><p id="e6c9" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我们可以尝试添加不同的噪声。例如:</p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="ca02" class="mg kg it mu b gy my mz l na nb">E = np.random.uniform(0, 10, size=(len(X), 20))</span></pre><p id="8028" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">如果噪声特征的数量增加到 20，并且噪声的幅度更大，则特征选择可能会选择错误的特征。不过性能还是提升了！</p><pre class="jy jz ka kb gt mt mu mv mw aw mx bi"><span id="527d" class="mg kg it mu b gy my mz l na nb"><strong class="mu iu">Before feature selection</strong></span><span id="28a0" class="mg kg it mu b gy nc mz l na nb">[[16  0  0]<br/> [ 0 10  1]<br/> [ 0  4 14]]<br/>              precision    recall  f1-score   support</span><span id="ecd6" class="mg kg it mu b gy nc mz l na nb">0                  1.00      1.00      1.00        16<br/>           1       0.71      0.91      0.80        11<br/>           2       0.93      0.78      0.85        18</span><span id="ebb5" class="mg kg it mu b gy nc mz l na nb">accuracy                               0.89        45<br/>   macro avg       0.88      0.90      0.88        45<br/>weighted avg       0.90      0.89      0.89        45</span><span id="8537" class="mg kg it mu b gy nc mz l na nb"><strong class="mu iu">After feature selection:</strong></span><span id="38d8" class="mg kg it mu b gy nc mz l na nb">[[16  0  0]<br/> [ 0 10  1]<br/> [ 0  1 17]]<br/>              precision    recall  f1-score   support</span><span id="1447" class="mg kg it mu b gy nc mz l na nb">0                  1.00      1.00      1.00        16<br/>           1       0.91      0.91      0.91        11<br/>           2       0.94      0.94      0.94        18</span><span id="8e76" class="mg kg it mu b gy nc mz l na nb">accuracy                               0.96        45<br/>   macro avg       0.95      0.95      0.95        45<br/>weighted avg       0.96      0.96      0.96        45</span></pre><h1 id="f81f" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">摘要</h1><p id="5065" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在本文中，我对相同的数据使用了不同的特征选择方法。然后我评价他们的表现。我首先向虹膜数据集添加噪声以形成新的数据集。然后，不同的特征选择方法被应用于这个新的数据集。如果噪声与原始数据相比较小，并且噪声特征的数量相对较少，那么大多数方法可以正确地发现原始特征。</p><p id="77d2" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">比较使用所有特征(原始特征+噪声特征)来训练模型，如果我们仅使用特征选择之后剩余的特征，则模型表现更好。</p><p id="3b93" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><strong class="lf iu">注:</strong>完整的木星笔记本可以在这里下载:</p><p id="ad75" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><a class="ae ms" href="https://github.com/musicpiano/data-science-project/blob/master/compare-different-feature-selection-methods.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/music piano/data-science-project/blob/master/compare-different-feature-selection-methods . ipynb</a></p><h1 id="c4e5" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">参考:</h1><div class="ns nt gp gr nu nv"><a href="https://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd iu gy z fp oa fr fs ob fu fw is bi translated">[1] 1.13.功能选择-sci kit-了解 0.21.3 文档</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">sklearn.feature_selection 模块中的类可用于…上的特征选择/维度缩减</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">scikit-learn.org</p></div></div><div class="oe l"><div class="of l og oh oi oe oj kd nv"/></div></div></a></div><p id="4040" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">[2]安德烈亚斯·c·米勒，萨拉·圭多(2016)。Python 机器学习导论:数据科学家指南。奥赖利</p></div></div>    
</body>
</html>