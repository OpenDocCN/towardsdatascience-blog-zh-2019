<html>
<head>
<title>We are ready for Machine Learning Explainability?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我们准备好机器学习了吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/we-are-ready-to-ml-explainability-2e7960cb950d?source=collection_archive---------20-----------------------#2019-04-08">https://towardsdatascience.com/we-are-ready-to-ml-explainability-2e7960cb950d?source=collection_archive---------20-----------------------#2019-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="28a4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ko translated">新的欧盟通用数据保护条例(<a class="ae kx" href="https://eugdpr.org/" rel="noopener ugc nofollow" target="_blank"> GPDR </a>，通用数据保护条例)包括如何使用机器学习的规定。这些规定旨在将个人数据的控制权交给用户，并引入解释权。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi ky"><img src="../Images/c222893b2d77847ca967b42c17d3d0f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZIZad4lfDyGr2F47R0uFkw.jpeg"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">European Union HQ</figcaption></figure><p id="204f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="lo">解释权</em> </strong>是欧盟为了让人工智能更加透明和道德而提出的要求。这一规定促进了算法的建立，以确保对每一个机器学习决策的解释。</p><p id="0702" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ko translated">可解释性对于一个解释应该是什么样子仍然没有共识。例如，传统的 ML 模型，在大多数情况下，仅限于产生二进制输出或评分输出(准确性，F1-评分…)给定一个 ML 模型来授予信用，其二进制输出为是:批准信用/否:拒绝信用。另一方面，一个更容易解释的输出将告诉我们为什么信贷被批准或拒绝(图 1)</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/159231f806f69d647ec8b29b817bce1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*BAJNgqu4pBvA0Bwk3y8kuw.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Figure 1: Example of an explainable output</figcaption></figure><p id="dc3c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用决策树或分类规则可以很容易地生成前面的输出。但是很难从像神经网络或梯度提升分类器(这些算法也被称为黑盒算法)这样的高精度 ML 算法中生成/提取这种解释。</p><p id="7f52" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们限制 ML 模型的可解释性，例如决策树或分类规则；欧洲会输掉<a class="ae kx" href="https://www.euronews.com/2019/02/05/the-eu-s-softball-approach-to-artificial-intelligence-will-lose-to-china-s-hardball-view" rel="noopener ugc nofollow" target="_blank">人工智能竞赛</a>。这种可解释系统的缺乏转化为欧盟对研究可解释系统的迫切需求:在不牺牲可解释性的情况下获得高预测精度。</p><p id="02ee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi">­</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/f0fcf669be5862084695eefd00b53cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*oSz01gdZeGM_ex3upwbLlg.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Figure 2 : Prediction accuracy versus Explainability</figcaption></figure><p id="65de" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ko translated">可解释的机器学习是一个非常广泛的话题，还没有正式的定义。快速浏览一下谷歌学术就足以看出可解释性的不同定义。</p><p id="bb16" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">定义 1 </strong>，理解模型做了什么或可能做了什么的科学【Leilani H. et altrum 2019】</p><p id="f02a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">定义 2，</strong>向人类解释或呈现可理解术语的能力【压轴多希-维勒兹和贝内金 2017】</p><p id="0762" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">定义 3 </strong>，使用机器学习模型提取数据中包含的领域关系相关知识【W. James Murdoch et altrum 2019】</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi lr"><img src="../Images/9edc147322b2e536b0a7a8f936e4c269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AJnC_QgHIp-eDRuCx0tCCQ.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Figure 3: Explainability Machine Learning as a Post Hoc Analysis</figcaption></figure><p id="92be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">即使是定义的细微差别也很容易理解什么是可解释的机器学习(至少从理论的角度来看)。近年来出现了像<a class="ae kx" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank">莱姆</a>和<a class="ae kx" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> SHAP </a>这样的算法，试图提供事后分析的解释(图 3)。</p><p id="f833" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这两个工具是开始研究可解释性的一个很好的起点，但不足以完成 GPDR 框架。此外，它不存在解释的正式输出。没有一个比较解释的标准可以肯定解释 A 比解释 b 更好。</p><p id="b5cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">直到一个可解释性的一般框架仍然是一条漫长的道路。可解释性框架需要完成的一些洞见如下【压轴多希-维勒兹和 Been Kim 2017】:</p><p id="506a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">- <strong class="js iu">公平</strong>:受保护的群体不会受到某种形式的歧视(明示或暗示)。</p><p id="efb5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">- <strong class="js iu">隐私</strong>:该方法保护敏感信息，每个预测都独立于其他观察</p><p id="5910" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">- <strong class="js iu">可靠性和稳健性</strong>:解释必须根据输入变化而发展。</p><p id="6f3d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">- <strong class="js iu">因果关系</strong>:暗示由于扰动导致的预测输出变化将发生在真实系统中。</p><p id="7a82" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">- <strong class="js iu">可用和可信</strong>:框架协助用户完成任务。在其他任务中，可信指的是拥有用户的信任。</p><h1 id="cbb7" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">参考</h1><p id="0163" class="pw-post-body-paragraph jq jr it js b jt mq jv jw jx mr jz ka kb ms kd ke kf mt kh ki kj mu kl km kn im bi translated">走向可解释机器学习的严格科学(终曲多希-维勒兹和贝金，2017 年)</p><p id="db72" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可解释的机器学习:定义、方法和应用(W. James Murdoch et altrum，2019)</p><p id="f2b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">解释解释:机器学习的可解释性概述(Leilani H. Gilpin et altrum，2019)</p><p id="d1a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">解释模型预测的统一方法(Scott M. Lundberg 和 Su-In Lee，2017 年)</p></div></div>    
</body>
</html>