<html>
<head>
<title>Breaking down the Lottery Ticket Hypothesis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">打破彩票假说</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/breaking-down-the-lottery-ticket-hypothesis-ca1c053b3e58?source=collection_archive---------13-----------------------#2019-11-26">https://towardsdatascience.com/breaking-down-the-lottery-ticket-hypothesis-ca1c053b3e58?source=collection_archive---------13-----------------------#2019-11-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="647c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从麻省理工学院 CSAIL 有趣的论文中提炼思想:“彩票假说:寻找稀疏、可训练的神经网络”。</h2></div><p id="568c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我最近读过的最有趣的论文之一是“彩票假说:寻找稀疏的、可训练的神经网络”(<a class="ae lb" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1803.03635</a>)。我已经将内容浓缩成一篇简短的博文，这可能会帮助你快速理解文章中的观点。</p><h1 id="4309" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">动机</h1><p id="f1c0" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">神经网络的剪枝技术可以将训练网络的参数数量减少 90%以上，而不会影响精度。具有较少参数的网络降低了存储需求，并且可以花费较少的时间来执行推断。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/6dabbc137736edae14f707ecf150bb4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wr_48GOYCwQh4bkcuVb1Zg.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Simplified example of pruning a trained network. Removing a parameter is equivalent to removing a connection between neurons.</figcaption></figure><p id="94a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">听起来不错，给我报名吧！但是训练大型网络可能非常昂贵。如果我们可以训练一个与修剪后的网络具有相同拓扑(形状)的网络，这不是很好吗？不幸的是，用修剪过的网络的拓扑来训练网络产生的网络比原始的未修剪过的网络具有更低的精度。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mp"><img src="../Images/25ffcbf0ab196bbb5dfb6a6ebaeab373.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3QhCbJ_sIqyo5SF7pzwZ1A.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Simplified example of pruning a trained network (above). Simplified example of training a network with the topology of the pruned network using a new random weight initialization (below)</figcaption></figure><p id="e257" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇论文的作者想问<strong class="kh ir">为什么</strong>用修剪过的网络拓扑来训练网络会产生更差的性能。</p><h1 id="a1bc" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">彩票假说</h1><blockquote class="mq mr ms"><p id="d484" class="kf kg mt kh b ki kj jr kk kl km ju kn mu kp kq kr mv kt ku kv mw kx ky kz la ij bi translated">“一个随机初始化的密集神经网络包含一个子网络，该子网络被初始化，以便在隔离训练时，它可以在最多相同次数的迭代训练后匹配原始网络的测试精度。”</p></blockquote><p id="e605" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他们发现，通过<strong class="kh ir">保留来自未修剪网络</strong>的原始权重初始化，可以用修剪后的网络拓扑来训练网络，并在相同次数的训练迭代内实现相同或更好的测试精度。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mx"><img src="../Images/eb84c19212ab66967126aafb2e8a33e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j9jp9aUwNtm8_Ku7_Kav0Q.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Simplified example of pruning a trained network (above). Simplified example of training a network with the topology of the pruned network using the original weight initialization from(below)</figcaption></figure><h1 id="db29" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">彩票类比</h1><p id="7977" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">好吧，看起来很简单。但是彩票是怎么回事呢？假设你的目标是找到一张中奖彩票。如果你买了一张票，赢得钱的机会很小。但是如果你买了一百万张票，很可能你的一些票会赢得一些钱(虽然这可能不盈利)。</p><p id="6bee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个类比中，购买大量门票就像为您的任务设置了一个过度参数化的神经网络。神经网络通常有超过一百万个参数！彩票中奖就相当于训练了一个神经网络，对你的任务有很高的准确率。最后，获胜的标签指的是达到高精度的修剪子网的权重初始化。</p><h1 id="dbc1" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">识别中奖彩票</h1><p id="7b04" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">本文提出了两种剪枝策略，可以找到获胜的票(达到高精度的子网)。</p><p id="ced8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">一次性修剪</strong></p><ol class=""><li id="0fd4" class="my mz iq kh b ki kj kl km ko na ks nb kw nc la nd ne nf ng bi translated">随机初始化神经网络</li><li id="c137" class="my mz iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">训练网络</li><li id="b2ce" class="my mz iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">将每个层中具有最低幅度的权重的 p%设置为 0(这是修剪)</li><li id="d533" class="my mz iq kh b ki nh kl ni ko nj ks nk kw nl la nd ne nf ng bi translated">将修剪后的网络权重重置为其原始随机初始化</li></ol><p id="aba1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">应该注意到，到输出神经元的连接以正常修剪速率的一半被修剪。</p><p id="5835" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">迭代剪枝</strong></p><p id="62a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">迭代修剪只是迭代地应用一次性修剪的步骤。作者发现迭代修剪比一次性修剪产生更小的修剪子网络。</p><h1 id="dbcf" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结果</h1><p id="e03d" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">迭代修剪策略被应用于全连接架构、卷积架构和具有剩余连接的卷积架构(ResNet)。作者在 MNIST 和 CIFAR-10 图像分类数据集上研究了迭代剪枝技术。他们的修剪策略也适用于不同的优化器(SGD、momentum、Adam)、dropout、weight decay、batchnorm 和 residual connections。</p><p id="adb3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">修剪后的子网络比原来的小 10-20 %,并且在最多相同的迭代次数下达到或超过了原来的测试精度。这些结果支持彩票假说。修剪后的子网络也具有更好的泛化能力，因为训练和测试精度之间的差异更小。</p><h1 id="8e92" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">讨论</h1><p id="b3a0" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">为什么网络会收敛到使用其总参数的一小部分来解决一个任务？“获胜”的初始化已经接近完全训练的值了吗？没有。实际上，在训练过程中，它们比其他参数变化更大！作者推测，获胜的初始化可能落在损失景观中特别适合优化的<br/>区域。他们提出了“彩票猜想”，即随机梯度下降在超参数化网络中寻找并训练一张中奖彩票(子网)。</p><h1 id="942b" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">限制</h1><p id="093a" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这篇论文展示了有趣的结果，这些结果有助于更好地理解深度神经网络是如何学习的。不幸的是，他们提出的迭代修剪策略没有提供显著的实际好处。</p><p id="0d2d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">迭代修剪在计算上是昂贵的，因为它涉及每次尝试训练网络 15 次。这使得作者很难研究像 ImageNet 这样的大型数据集。作为未来的工作，作者希望找到更有效的剪枝策略。</p><p id="34df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过将参数数量减少 80–90 %,降低了网络的存储需求。权重矩阵仍然具有相同的维数，但是更加稀疏。用于修剪后的子网络的前馈步骤仍然需要以相同的计算复杂度计算矩阵乘法，因此训练和推断并没有明显更有效。作为未来的工作，他们建议基于非数量级的剪枝策略可以找到更小的剪枝子网。</p><p id="70b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>