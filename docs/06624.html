<html>
<head>
<title>Similarity Measures — Scoring Textual Articles</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">相似性度量—对文本文章评分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/similarity-measures-e3dbd4e58660?source=collection_archive---------7-----------------------#2019-09-22">https://towardsdatascience.com/similarity-measures-e3dbd4e58660?source=collection_archive---------7-----------------------#2019-09-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="1feb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">许多现实世界的应用程序利用相似性度量来查看两个对象是如何联系在一起的。例如，我们可以在涉及计算机视觉和自然语言处理的应用程序中使用这些度量来查找和映射相似的文档。对于企业来说，一个重要的用例是将简历与职位描述相匹配，从而为招聘人员节省大量时间。另一个重要的用例是使用 K 均值聚类算法(也使用相似性度量)为营销活动细分不同的客户。</p><p id="7439" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">相似性通常是介于 0(无相似性)和 1(完全相似性)之间的正值。我们将具体讨论两个重要的相似性度量，即欧几里德和余弦，以及处理维基百科文章的编码示例。</p><h2 id="a757" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">欧几里得度量</h2><p id="df36" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">你还记得毕达哥拉斯定理吗？？毕达哥拉斯定理用于计算两点之间的距离，如下图所示。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/75229a0ffc3113ee17406484078a8d0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*755XtHJ7YMQwoqUHdUplBQ.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk"><a class="ae ly" href="http://rosalind.info/glossary/euclidean-distance/" rel="noopener ugc nofollow" target="_blank">http://rosalind.info/glossary/euclidean-distance/</a></figcaption></figure><p id="56a3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在图中，我们有两个数据点(x1，y1)和(x2，y2 ),我们感兴趣的是计算这两个点之间的距离或接近程度。为了计算距离，我们需要先从 x1 到 x2 水平移动，然后从 y1 到 y2 垂直向上移动。这就组成了一个直角三角形。我们对计算斜边 d 感兴趣，使用毕达哥拉斯定理可以很容易地计算出斜边 d。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/66bba06a2871b08c11bd902582ba6395.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*2WjAedRw2hSiV1mosOXcuA.png"/></div></figure><p id="3c77" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中 b 是直角三角形的底边，p 是直角三角形的垂线。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/f0b536feb457f857bc4f2a07d58c393c.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*Mi3o9AHJDySeHwh2WlIy9g.png"/></div></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/190f94e8a36228d9cdec2aeacf9e5fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*FFfiOmjswoX6DZ0oed_1TA.png"/></div></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/5e7ec973678dd0f87e5b3bb42d64ab56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*-qXzZGIhCmsp6KXi0v8N6w.png"/></div></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi md"><img src="../Images/b162a9188785bd8208d450a77e1cb8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*TSH52hk8P7RUGmkK8bodBw.png"/></div></figure><p id="c405" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就完成了我们二维空间中两点的欧氏距离公式。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/8b4d5bd0b68594a053b4d363abe6c35b.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*f9MfUWdeqhf35fpEN3vaLw.png"/></div></figure><p id="2318" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这定义了一维、二维、三维或更高维空间中两点之间的欧几里德距离，其中 n 是维数，x_k 和 y_k 分别是 x 和 y 的分量。</p><p id="fdde" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">定义欧氏距离的 Python 函数</strong></p><pre class="ln lo lp lq gt mf mg mh mi aw mj bi"><span id="de6b" class="ko kp it mg b gy mk ml l mm mn">def euclidean_distance(x, y):   <br/>    return np.sqrt(np.sum((x - y) ** 2))</span></pre><p id="8d7e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里 x 和 y 是两个向量。</p><p id="fd23" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">也可以使用 sklearn 库来计算欧氏距离。这个函数在计算上更有效。</p><pre class="ln lo lp lq gt mf mg mh mi aw mj bi"><span id="38d4" class="ko kp it mg b gy mk ml l mm mn">from sklearn.metrics.pairwise import euclidean_distances</span></pre><p id="0ca9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">距离越大，两个对象之间的相似性越低；距离越小，两个对象之间的相似度越高。要将这个距离度量转换为相似性度量，我们可以用最大距离除以对象的距离，然后减去 1，以得到 0 和 1 之间的相似性得分。在讨论了余弦度量之后，我们将看看这个例子。</p><h2 id="952d" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated"><strong class="ak">余弦</strong>公制</h2><p id="0bb6" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">这是专门为文档寻找相似性的另一个度量。如图所示，此指标用于测量 x 和 y 之间的角度，当矢量的大小无关紧要时使用。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/ede4e0dc3336f1fa075a666542735ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*mEH9e99eESIyOHqz.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk"><a class="ae ly" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Cosine_similarity</a></figcaption></figure><p id="b01b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果 v 和 w 之间的角度是 0 度，那么余弦相似度=1(完全相似)。</p><p id="0b21" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">点积余弦公式:</strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/3b0ef792a088479a7eda5796b21dde6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*hT80mNov40zWVhctfR-gig.png"/></div></div></figure><p id="eb9d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">‖⋅‖表示矢量长度</p><p id="62d2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">𝐯和𝐰.之间的𝜃:角这里我们感兴趣的是测量 v 和 w 之间的相似性。</p><p id="6851" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你想知道如何推导点积的余弦公式，请参考本页:<a class="ae ly" href="https://proofwiki.org/wiki/Cosine_Formula_for_Dot_Product" rel="noopener ugc nofollow" target="_blank">https://proofwiki.org/wiki/Cosine_Formula_for_Dot_Product.</a>这个点积公式是从你们在学校学过的余弦定律推导出来的。</p><p id="96d7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">让我们用符号 x 和 y 代替 v 和 w。</strong></p><p id="edf2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">示例:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/17b224ce60fd0fb8e95c0d035b4e6cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*CEIzJYM3YetMzQyTZJrE7g.png"/></div></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mu"><img src="../Images/b37d74fb3bd28615228c36ba91009de7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VezBjbECU5UmCdq--AAJug.png"/></div></div></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mv"><img src="../Images/a09f67182da149f68972771778156a49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OdKdEH56qU-ge8uIEkPfSg.png"/></div></div></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mw"><img src="../Images/d42b4981a7f536bd1e776f7ff274c686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kroN3o1A4Y_5zWgUWb7nhA.png"/></div></div></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/d353540ee3f54c01f6204e88102b9735.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*rPwkLH9d4Ikxyjm-Tc-1Mg.png"/></div></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/bc15cea1af0cbb4534569c4120fdb0b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*89ZCbKvAdHXL5F4pcUrInw.png"/></div></figure><p id="da9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这两个向量具有低相似性，由 0.11 的值来解释。该值接近 0，这意味着 x 和 y 之间的角度接近 90 度。如果该值接近 1，那么这将是角度接近 0 度的非常相似的对象。</p><p id="4c5e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将 x 和 y 除以它们的长度，将其长度归一化为 1，这就是所谓的单位向量。这表明余弦相似性不考虑 x 和 y 的大小。当我们需要考虑大小时，欧几里得可能是更好的选择。</p><p id="aacb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们已经有了长度为 1 的向量，余弦相似性可以很容易地使用简单的点积来计算。因此，建议首先归一化矢量，使其具有单位长度，以减少计算时间。</p><p id="766a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> Python 函数定义余弦相似度</strong></p><pre class="ln lo lp lq gt mf mg mh mi aw mj bi"><span id="de89" class="ko kp it mg b gy mk ml l mm mn">def cosine_similarity(x, y):<br/>    return np.dot(x, y) / (np.sqrt(np.dot(x, x)) * np.sqrt(np.dot(y, y)))</span></pre><h2 id="7f0f" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">我应该使用哪个指标<strong class="ak">？</strong></h2><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mz"><img src="../Images/606f2ca779ab12b6e95888afa8883fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_F8Vfe2S9lp4Lt5Evz1jYQ.png"/></div></div></figure><p id="c1e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该图总结了欧几里德度量和余弦度量。</p><p id="2e0e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">余弦着眼于两个向量之间的角度，忽略大小，而欧几里得着眼于直线距离，考虑向量的大小。</p><p id="5508" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">文本挖掘是我们可以利用余弦相似性来映射相似文档的领域之一。我们还可以使用它来根据给定的单词向量对文档进行排序，简历入围就是我们可以利用余弦相似度的用例之一。</p><p id="5192" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看一下我们需要匹配相似文档的问题，我们首先创建文档术语矩阵，它包含术语在文档集合中出现的频率。我们通常有不均匀长度的文档，例如维基百科的文章。假设单词 math 在文档 1 中比在文档 2 中出现得更多，在这种情况下，余弦相似度将是一个完美的选择，因为我们不关心文档的长度，而是它包含的内容。如果我们要考虑长度，那么欧几里得可能是一个完美的选择。</p><h2 id="7979" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">现在我们将使用 Python 编写简短的代码来映射类似的维基百科文章。</h2><pre class="ln lo lp lq gt mf mg mh mi aw mj bi"><span id="c144" class="ko kp it mg b gy mk ml l mm mn">import wikipedia</span><span id="e24d" class="ko kp it mg b gy na ml l mm mn">articles=['Data Mining','Machine Learning','Cricket','Swimming','Tennis']<br/>wiki_lst=[]</span><span id="7dde" class="ko kp it mg b gy na ml l mm mn">for article in articles:<br/>    print(article)<br/>    wiki_lst.append(wikipedia.page(article).content)</span></pre><p id="9eba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这里，我们刚刚从 5 篇维基百科文章中提取了内容。</p><pre class="ln lo lp lq gt mf mg mh mi aw mj bi"><span id="b907" class="ko kp it mg b gy mk ml l mm mn">from sklearn.feature_extraction.text import CountVectorizer<br/>cv = CountVectorizer()<br/>X=cv.fit_transform(wiki_lst)</span></pre><p id="20c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">计数矢量器只是将文档集合转换成字数矩阵。</p><p id="23e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们使用欧几里德度量来找出我们的文章是相似的。这里，我们将这些欧几里得距离归一化为 0 到 1，然后减去 1，使其范围在 0 到 1 之间。因此，它越接近 1，相似性越高。</p><pre class="ln lo lp lq gt mf mg mh mi aw mj bi"><span id="54e0" class="ko kp it mg b gy mk ml l mm mn">from sklearn.metrics.pairwise import euclidean_distances<br/>print("Data Mining and Machine Learning",1-euclidean_distances(X[0],X[1])/np.max((euclidean_distances(X))))<br/>print("Data Mining and Cricket",1-euclidean_distances(X[0],X[2])/np.max((euclidean_distances(X))))<br/>print("Data Mining and Swimming",1-euclidean_distances(X[0],X[3])/np.max((euclidean_distances(X))))<br/>print("Data Mining and Tennis",1-euclidean_distances(X[0],X[4])/np.max((euclidean_distances(X))))</span></pre><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nb"><img src="../Images/64d5231a8a55ca38d2b5615acfbd282d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X_fuL-FjYfNLAHrSU8FxFQ.png"/></div></div></figure><p id="b335" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这两篇文章(数据挖掘游泳)怎么最像？这没有任何意义。现在让我们试试余弦相似度。</p><pre class="ln lo lp lq gt mf mg mh mi aw mj bi"><span id="94af" class="ko kp it mg b gy mk ml l mm mn">from sklearn.metrics.pairwise import cosine_similarity<br/>print("Data Mining and Machine Learning",cosine_similarity(X[0],X[1]))<br/>print("Data Mining and Cricket",cosine_similarity(X[0],X[2]))<br/>print("Data Mining and Swimming",cosine_similarity(X[0],X[3]))<br/>print("Data Mining and Tennis",cosine_similarity(X[0],X[4]))</span></pre><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nc"><img src="../Images/8761d084d0ef6157273f7a0eac6ff8f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lmQngOJJRVvDRH0YKJxVhw.png"/></div></div></figure><p id="d984" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这对我们来说是有意义的。数据挖掘非常接近机器学习。</p><h2 id="9acf" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated"><strong class="ak">结论</strong></h2><p id="dfbf" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">欧几里得度量在这里似乎集中于长度而不是内容，而余弦集中于内容而忽略了幅度。</p><p id="4986" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">参考文献</strong></p><p id="a306" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【https://proofwiki.org/wiki/Cosine_Formula_for_Dot_Product?】[1]T4source = post _ page-E3 DBD 4 e 58660-</p><p id="f43a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ly" href="https://cmry.github.io/notes/euclidean-v-cosine?source=post_page-----e3dbd4e58660----------------------" rel="noopener ugc nofollow" target="_blank">【2】https://cmry.github.io/notes/euclidean-v-cosine?source = post _ page-E3 DBD 4 e 58660-</a></p></div></div>    
</body>
</html>