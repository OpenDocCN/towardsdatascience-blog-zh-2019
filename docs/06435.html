<html>
<head>
<title>How to compile TensorFlow 1.12 on Ubuntu 16.04 using Docker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用 Docker 在 Ubuntu 16.04 上编译 TensorFlow 1.12</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-compile-tensorflow-1-12-on-ubuntu-16-04-using-docker-6ca2d60d7567?source=collection_archive---------17-----------------------#2019-09-15">https://towardsdatascience.com/how-to-compile-tensorflow-1-12-on-ubuntu-16-04-using-docker-6ca2d60d7567?source=collection_archive---------17-----------------------#2019-09-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3c3b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">本教程将帮助您在 Ubuntu 16.04 上使用 Docker 和 nvidia-docker 的 GPU 设置 TensorFlow 1.12。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/141f51dbe65d7315c35732a41064a41a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z1CK6ku3sbNIMQ9MGs95qw.png"/></div></div></figure><p id="3144" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">TensorFlow 是最受欢迎的深度学习库之一。它由谷歌创建，并于 2015 年作为一个开源项目发布。TensorFlow 用于研究和生产环境。安装 TensorFlow 可能很麻烦。难度因环境限制而异，如果你是一名只想构建神经网络的数据科学家，难度会更大。</p><p id="2af3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在 GPU 上使用 TensorFlow 时—设置需要几个步骤。在下面的教程中，我们将回顾设置 TensorFlow 所需的过程。</p><h1 id="9d71" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">要求:</h1><ul class=""><li id="6209" class="mg mh iq kt b ku mi kx mj la mk le ml li mm lm mn mo mp mq bi translated">NVIDIA GPU 机器</li><li id="b1df" class="mg mh iq kt b ku mr kx ms la mt le mu li mv lm mn mo mp mq bi translated">Docker 和 Nvidia-Docker 已安装。(阅读我们博客中的“<a class="ae ln" href="https://cnvrg.io/how-to-setup-docker-and-nvidia-docker-2-0-on-ubuntu-18-04/" rel="noopener ugc nofollow" target="_blank">如何安装 docker 和 nvidia-docker”)</a></li><li id="23c6" class="mg mh iq kt b ku mr kx ms la mt le mu li mv lm mn mo mp mq bi translated">Ubuntu 16.04</li></ul><h2 id="640a" class="mw lp iq bd lq mx my dn lu mz na dp ly la nb nc ma le nd ne mc li nf ng me nh bi translated">步骤 1-使用 Docker 和 Nvidia-Docker 准备您的环境</h2><p id="9a39" class="pw-post-body-paragraph kr ks iq kt b ku mi jr kw kx mj ju kz la ni lc ld le nj lg lh li nk lk ll lm ij bi translated"><a class="ae ln" href="https://github.com/docker/docker" rel="noopener ugc nofollow" target="_blank"> Docker </a>是一个工具，旨在通过使用容器来简化应用程序的创建、部署和运行。容器到底是什么？容器允许数据科学家和开发人员用环境需要的所有部分——比如库和其他依赖项——包装环境，并将其全部打包。</p><p id="77fb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要将 docker 与 GPU 配合使用，并能够在您的应用程序中使用 TensorFlow，您需要安装 Docker 和 Nvidia-Docker。如果您已经安装了这些，请转到下一步。否则，你可以按照我们之前的指南<a class="ae ln" href="https://cnvrg.io/how-to-setup-docker-and-nvidia-docker-2-0-on-ubuntu-18-04/" rel="noopener ugc nofollow" target="_blank">安装 nvidia docker </a>。</p><h1 id="9789" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated"><strong class="ak">先决条件:</strong></h1><h2 id="957b" class="mw lp iq bd lq mx my dn lu mz na dp ly la nb nc ma le nd ne mc li nf ng me nh bi translated">第 2 步— Dockerfile</h2><p id="197a" class="pw-post-body-paragraph kr ks iq kt b ku mi jr kw kx mj ju kz la ni lc ld le nj lg lh li nk lk ll lm ij bi translated">Docker 可以通过读取 docker 文件中的指令来自动构建映像(环境)。Dockerfile 是一个文本文档，它包含用户可以在命令行上调用的所有命令来组合一个图像。</p><p id="3c52" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我们的例子中，这些命令将描述 Python 3.6、CUDA 9 和 CUDNN 7.2.1 的安装—当然还有从源代码安装 TensorFlow 1.12。</p><p id="6b8d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于这种环境，我们将使用以下 docker 文件-</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h2 id="927f" class="mw lp iq bd lq mx my dn lu mz na dp ly la nb nc ma le nd ne mc li nf ng me nh bi translated">步骤 3 —运行 Dockerfile</h2><p id="f757" class="pw-post-body-paragraph kr ks iq kt b ku mi jr kw kx mj ju kz la ni lc ld le nj lg lh li nk lk ll lm ij bi translated">要从 docker 文件构建映像，只需运行 docker build 命令。请记住，这个构建过程可能需要几个小时才能完成。我们建议使用<a class="ae ln" href="https://www.maketecheasier.com/nohup-and-uses/" rel="noopener ugc nofollow" target="_blank"> nohup 实用程序</a>,这样，如果您的终端挂起，它仍将运行。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="3097" class="mw lp iq no b gy ns nt l nu nv">$ docker build -t deeplearning -f Dockerfile</span></pre><p id="f9c8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这将输出设置过程，并以类似于以下内容结束:</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="d2b4" class="mw lp iq no b gy ns nt l nu nv">&gt;&gt; Successfully built deeplearning (= the image ID)</span></pre><p id="bd9c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您的图像已经可以使用了。要启动环境，只需输入以下命令。但是，不要忘记更换您的图像 id:</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="c8e2" class="mw lp iq no b gy ns nt l nu nv">$ docker run --runtime=nvidia -it deeplearning /bin/bash<!-- -->Step 4 — Validating TensorFlow &amp; start building!</span></pre><p id="388d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">验证 TensorFlow 确实在您的 docker 文件中运行</strong></p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="b9e1" class="mw lp iq no b gy ns nt l nu nv">$ python<br/>import tensorflow as tf<br/>sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))<br/>2019-02-23 07:34:14.592926: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports <br/>instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2019-02-23 07:34:17.452780: I <br/>tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA<br/>node read from SysFS had negative value (-1), but there must be at leastone NUMA node, so returning NUMA node zero<br/>2019-02-23 07:34:17.453267: I<br/>tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with <br/>properties: <br/>name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235pciBusID: 0000:00:1e.0<br/>totalMemory: 11.17GiB freeMemory: 11.10GiB<br/>2019-02-23 07:34:17.453306: I<br/>tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu<br/>devices: 0<br/>2019-02-23 07:34:17.772969: I<br/>tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect<br/>StreamExecutor with strength 1 edge matrix:<br/>2019-02-23 07:34:17.773032: I<br/>tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0<br/>2019-02-23 07:34:17.773054: I<br/>tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N<br/>2019-02-23 07:34:17.773403: I<br/>tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow<br/>device (/job:localhost/replica:0/task:0/device:GPU:0 with 10757 MB memory)<br/>-&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0,<br/>compute capability: 3.7)<br/>Device mapping:<br/>/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device<br/>/job:localhost/replica:0/task:0/device:XLA_GPU:0 -&gt; device: XLA_GPU device<br/>/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla K80,<br/>pci bus id: 0000:00:1e.0, compute capability: 3.7<br/>2019-02-23 07:34:17.774289: I<br/>tensorflow/core/common_runtime/direct_session.cc:307] Device mapping:<br/>/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device<br/>/job:localhost/replica:0/task:0/device:XLA_GPU:0 -&gt; device: XLA_GPU device<br/>/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla K80, pci bus id: 0000:0</span></pre><p id="5482" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">恭喜你。您的新 TensorFlow 环境已经设置完毕，可以开始训练、测试和部署您的深度学习模型了！</p><h2 id="c993" class="mw lp iq bd lq mx my dn lu mz na dp ly la nb nc ma le nd ne mc li nf ng me nh bi translated">结论</h2><p id="9e03" class="pw-post-body-paragraph kr ks iq kt b ku mi jr kw kx mj ju kz la ni lc ld le nj lg lh li nk lk ll lm ij bi translated">Tensorflow 通过提供一种大规模构建生产就绪模型的解决方案，真正颠覆了机器学习世界。但是 Tensorflow 并不总是最用户友好的。很难顺利地融入你的机器学习管道。<a class="ae ln" href="https://cnvrg.io" rel="noopener ugc nofollow" target="_blank"> cnvrg.io </a>数据科学平台利用 Tensorflow 和其他开源工具，以便数据科学家可以专注于算法这一魔法。你可以找到更多关于如何轻松利用开源工具的教程，如<a class="ae ln" href="https://blog.cnvrg.io/how-to-compile-tensorflow-1-12-on-ubuntu-16-04-using-docker" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>、<a class="ae ln" href="https://info.cnvrg.io/kubernetes-for-machinelearning" rel="noopener ugc nofollow" target="_blank">如何为你的机器学习工作流设置 Kubernetes</a>，以及<a class="ae ln" href="https://info.cnvrg.io/spark-on-kubernetes" rel="noopener ugc nofollow" target="_blank">如何在 Kubernetes </a>上运行 Spark。找到简单的方法来集成这些有用的工具将使您的模型更接近生产。</p></div></div>    
</body>
</html>