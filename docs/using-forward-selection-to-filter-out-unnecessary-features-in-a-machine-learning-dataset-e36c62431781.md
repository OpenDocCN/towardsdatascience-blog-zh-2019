# 使用正向选择过滤掉机器学习数据集中不必要的特征

> 原文：<https://towardsdatascience.com/using-forward-selection-to-filter-out-unnecessary-features-in-a-machine-learning-dataset-e36c62431781?source=collection_archive---------22----------------------->

![](img/552eaa69f6da8f0ae8bbdd60094a4920.png)

在我们之前的帖子中，我们看到了[如何执行反向消除](/backward-elimination-for-feature-selection-in-machine-learning-c6a3a8f8cef4)作为一种特征选择算法，从我们的数据集中剔除无关紧要的特征。在这篇文章中，我们将探讨特征选择的下一种方法，即前向选择。你可能已经猜到了，这是逆向淘汰的反义词。但在此之前，请确保您熟悉[P 值](/null-hypothesis-and-the-p-value-fdc129db6502)的概念。

类似于逆向淘汰，即使在这里我们也有几个步骤可以遵循。我们像往常一样一个一个去。但是在进入之前，你需要知道这将是一个比逆向消去法更乏味的工作，因为你必须在这里创建一堆简单的线性回归模型。根据数据集中要素的数量，需要创建的线性回归模型的数量可能会很快增长到一个巨大的数字。记住这一点，让我们开始吧。

# 第一步

第一步非常类似于逆向淘汰。这里，我们选择一个显著性水平或 P 值。正如你已经知道的，5%的显著性水平或 0.05 的 P 值是常见的。所以让我们坚持下去。

# 第二步

这是一个非常繁琐的步骤。在第二步中，我们为数据集中的每个要素创建一个简单的回归模型。所以如果有 100 个特征，我们就创建 100 个简单的线性回归模型。因此，根据数据集中要素的数量，这可能会变得非常枯燥和复杂。但这也是这个过程中最重要的一步。一旦我们拟合了所有简单的线性回归模型，我们将计算所有模型的 P 值，并确定具有最低**P 值的特征。**

# 第三步

在上一步中，我们确定了具有最低 P 值的要素。我们会将该特性添加到所有其他特性的简单线性回归模型中。所以在第二步，我们有简单的回归模型，每个模型有一个特征。在这一步中，我们将少一个线性回归模型，但每个模型都有两个特征。完成后，我们将再次拟合模型并计算 P 值。

# 第四步

在这一步中，我们获得了上一步中创建的所有模型的 P 值。我们再次识别具有最低 P 值的特征。我们检查这个最低的 P 值是否小于显著性水平，或者在我们的例子中是 0.05。如果是这样，我们将把这个新特性作为一个特性添加到所有其他模型中。所以基本上，我们用一个新的特性重复步骤 3。我们将继续这个循环，直到我们从模型中得到的最低 P 值不再小于显著性水平。一旦我们到达这个阶段，我们就打破了这个循环。

一旦我们打破了这个循环，我们将得到我们想要的模型，这是我们在打破循环的迭代之前的迭代中创建的模型。让我解释一下。假设我们让循环运行 10 次迭代。在第 10 次迭代中，我们发现最低 P 值大于显著性水平。我们将在这个模型之前考虑模型，这个模型是来自第 9 次迭代的模型。我们不考虑最后一个模型，因为它没有意义，因为 P 值大于 0.05。我希望你能理解。

无论如何，你现在有了你要找的模型。这种向前选择方法的唯一问题是迭代的次数和您最终构建的模型的数量，这很容易变得难以维护和监控。但这是这个过程的必要部分。我希望我解释得够清楚了。请在下面的评论中告诉我是否有任何遗漏，或者你是否需要我做更多的解释。

> 在 [Twitter](https://twitter.com/contactsunny) 上关注我，了解更多[数据科学](https://blog.contactsunny.com/tag/data-science)、[机器学习](https://blog.contactsunny.com/tag/machine-learning)，以及通用[技术更新](https://blog.contactsunny.com/category/tech)。还有，你可以[关注我的个人博客](https://blog.contactsunny.com/)。