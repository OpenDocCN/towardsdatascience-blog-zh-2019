<html>
<head>
<title>Skip-Gram: NLP context words prediction algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Skip-Gram:自然语言处理上下文词预测算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c?source=collection_archive---------1-----------------------#2019-03-16">https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c?source=collection_archive---------1-----------------------#2019-03-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1e4d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">NLP 是人工智能的一个领域，我们试图将人类语言作为文本或语音进行处理，以使计算机与人类相似。人类有大量的数据是以非常无组织的格式编写的。因此，任何机器都很难从原始文本中找到意义。</p><p id="e2fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了让机器从原始文本中学习，我们需要将这些数据转换成向量格式，这样我们的计算机就可以很容易地处理这些数据。这种原始文本到矢量格式的转换被称为单词表示。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/b7fbc9450b8606461ec9eb42871c315e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DowVC1x_3K0aLflh.jpg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">word embeddings</figcaption></figure><p id="0578" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">单词表示在向量空间中表示单词，因此如果单词向量彼此接近，则意味着这些单词彼此相关。在给定的图像中，我们可以看到在女性中常见的各种单词聚集在左侧，而与男性相关的单词聚集在右侧。因此，如果我们传递单词 like 耳环，计算机会将其与女性联系起来，这在逻辑上是正确的。</p><p id="0d79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于任何语言的词汇量都很大，并且不能被人类标注，因此我们需要无监督的学习技术，它可以自己学习任何单词的上下文。Skip-gram 是一种无监督学习技术，用于查找给定单词的最相关单词。</p><p id="28f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">跳跃语法用于预测给定目标单词的上下文单词。这是 CBOW 算法的逆运算。这里，目标单词被输入，而上下文单词被输出。因为要预测不止一个上下文单词，这使得这个问题变得困难。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lb"><img src="../Images/a291f14834997a57823a5b3fe26a30ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yxs3JKs5bKc4c_i8.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">skip-gram example</figcaption></figure><p id="d11d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将给出单词<code class="fe lc ld le lf b">sat</code>,我们将分别尝试预测位置<code class="fe lc ld le lf b">-1 and 3</code>处的单词<code class="fe lc ld le lf b">cat, mat</code>,假设<code class="fe lc ld le lf b">sat</code>位于位置<code class="fe lc ld le lf b">0</code>。我们不预测常用词或停用词，如<code class="fe lc ld le lf b">the</code>。</p><h1 id="096e" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">体系结构</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi me"><img src="../Images/994d8cc0e31f94bae5dbf6f0dbce36c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*3xy5IOpScN0aQwwfFbCmGQ.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The Skip-gram model architecture (Source: <a class="ae mf" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1301.3781.pdf</a> Mikolov el al.)</figcaption></figure><p id="269c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到 w(t)是给定的目标单词或输入。有一个隐藏层执行权重矩阵和输入向量 w(t)之间的点积。隐藏层中不使用激活函数。现在，隐藏层的点积结果被传递到输出层。输出层计算隐藏层的输出向量和输出层的权重矩阵之间的点积。然后，我们应用 softmax 激活函数来计算单词在给定的上下文位置出现在 w(t)的上下文中的概率。</p><h1 id="a555" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">我们将使用的变量</h1><ol class=""><li id="01f5" class="mg mh iq jp b jq mi ju mj jy mk kc ml kg mm kk mn mo mp mq bi translated">存在于我们的数据集或文本中的唯一单词的字典。该词典被称为词汇表，并且是系统已知的单词。词汇用‘V’来表示。</li><li id="415b" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><code class="fe lc ld le lf b">N</code>是隐藏层中存在的神经元数量。</li><li id="fb82" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">窗口大小是需要预测单词的最大上下文位置。窗口大小由 c 表示。例如，在给定的架构图像中，窗口大小是 2，因此，我们将预测上下文位置(t-2)、(t-1)、(t+1)和(t+2)处的单词。</li><li id="2ece" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">上下文窗口是在给定单词的范围内可能出现的要预测的单词的数量。上下文窗口的值是 2*c 的窗口大小的两倍，并且由 k 表示。对于给定的图像，上下文窗口的值是 4。</li><li id="9c09" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">输入向量的维数等于|V|。每个单词使用一个热编码进行编码。</li><li id="794a" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">隐藏层(W)的权重矩阵的维数为[|V|，N]。||是返回数组大小的模数函数。</li><li id="deed" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">隐藏层的输出向量是 H[N]。</li><li id="77b9" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">隐藏层和输出层(W’)之间的权重矩阵的维数为[N，|V|]。</li><li id="efa2" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">W '和 H 之间的点积给出了输出向量 U[|v|]。</li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mw"><img src="../Images/dd824afe2480c64f124b0836381561e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ygnAlYq2zMy9SrJl0GfdPA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">N = context window</figcaption></figure><h1 id="8752" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">工作步骤</h1><ol class=""><li id="4f89" class="mg mh iq jp b jq mi ju mj jy mk kc ml kg mm kk mn mo mp mq bi translated">使用一种热编码将单词转换成向量。这些向量的维数是[1，|v|]。</li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mx"><img src="../Images/9bd1dd4534835c9858c6c30314a0fea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2eJwousisjdsKvA0Gw0QJA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">one hot encoding</figcaption></figure><p id="b3e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.单词 w(t)从|v|神经元传递到隐藏层。</p><p id="aab7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.隐藏层执行权重向量 W[|v|，N]和输入向量 w(t)之间的点积。在此，我们可以断定 W[|v|，N]的第(t)行将是输出(H[1，N])。</p><p id="8e8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.请记住，隐藏层没有使用激活函数，因此 H[1，k]将直接传递到输出层。</p><p id="99ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">5.输出层将应用 H[1，N]和 W'[N，|v|]之间的点积，并给出向量 u。</p><p id="83e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">6.现在，为了找到每个向量的概率，我们将使用 softmax 函数。因为每次迭代给出一种热编码类型的输出向量 U。</p><p id="b3db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">7.具有最高概率的单词是结果，并且如果给定上下文位置的预测单词是错误的，那么我们将使用反向传播来修改我们的权重向量 W 和 W’。</p><p id="30a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将对词汇表中存在的每个单词 w(t)执行这些步骤。并且每个单词 w(t)将被传递 k 次。因此，我们可以看到，在每个时段中，前向传播将被处理|v|*k 次。</p><h1 id="690f" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">概率函数</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/e22f2aeafb7539a334497d209c83c608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rDCdrW0_c2bdCxKb.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">softmax probability</figcaption></figure><p id="8a20" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">w(c，j)是在第<code class="fe lc ld le lf b">c</code>个上下文位置预测的第<code class="fe lc ld le lf b">j</code>个字；w(O，c)是出现在第<code class="fe lc ld le lf b">c</code>个上下文位置的实际单词；w(I)是唯一的输入字；并且 u(c，j)是当预测第<code class="fe lc ld le lf b">c</code>个上下文位置的单词时 U 向量中的第<code class="fe lc ld le lf b">j</code>个值。</p><h1 id="e780" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">损失函数</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi my"><img src="../Images/627482f117b4799d84f2583bb72c5b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EcqcX6aZDQWjj0_fnj7jOA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Loss function</figcaption></figure><p id="c409" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为我们想要最大化在第<code class="fe lc ld le lf b">c</code>个上下文位置上预测 w(c，j)的概率，所以我们可以表示损失函数<code class="fe lc ld le lf b">L</code>。</p><h1 id="6734" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">优势</h1><ol class=""><li id="f569" class="mg mh iq jp b jq mi ju mj jy mk kc ml kg mm kk mn mo mp mq bi translated">这是无监督的学习，因此可以对任何给定的原始文本。</li><li id="24ad" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">与矢量表示相比，它需要更少的内存。</li><li id="cef5" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">它需要两个维数分别为[N，|v|]的权重矩阵，而不是[|v|，|v|]。通常，N 大约为 300，而|v|以百万为单位。因此，我们可以看到使用这种算法的优势。</li></ol><h1 id="7291" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">不足之处</h1><ol class=""><li id="3bbc" class="mg mh iq jp b jq mi ju mj jy mk kc ml kg mm kk mn mo mp mq bi translated">找到<code class="fe lc ld le lf b">N</code>和<code class="fe lc ld le lf b">c</code>的最佳值是困难的。</li><li id="13f9" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">Softmax 函数的计算开销很大。</li><li id="026d" class="mg mh iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated">训练该算法所需的时间很长。</li></ol></div></div>    
</body>
</html>