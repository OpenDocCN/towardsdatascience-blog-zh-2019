<html>
<head>
<title>Understanding language modelling(NLP) and Using ULMFIT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解语言建模(NLP)和使用 ULMFIT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-language-modelling-nlp-part-1-ulmfit-b557a63a672b?source=collection_archive---------9-----------------------#2019-09-04">https://towardsdatascience.com/understanding-language-modelling-nlp-part-1-ulmfit-b557a63a672b?source=collection_archive---------9-----------------------#2019-09-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="deb1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用带代码的 ULMFIT 进行文本分类</h2></div><p id="0ebe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">语言模型是单词序列的概率分布。这种语言模型被用作各种自然语言处理任务的基础模型，包括文本分类、摘要、文本生成等等。</p><p id="0108" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">建议先对深度学习和迁移学习有一个基本的认识，再继续这个博客。</p><p id="effe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LMs 使得映射上下文更加容易。男人对女人的映射相当于叔叔对阿姨，国王对女王。这种映射被转换成词汇间的联合概率分布。这被称为统计 LM。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/e5d55b264877c1fa9ee913236a538d30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*t_uNclZ6Fm28EY8iWAbQCg.png"/></div></figure><p id="ed96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">给定这样一个序列，比如长度为<em class="lm"> m </em>，LM 给整个序列分配一个概率 P{w1，w2，…，wm }。</strong></p><p id="717a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于词汇表的长度，统计 LMs 面临数据稀疏的问题。而神经语言模型(NLM)仅将单词序列作为输入，这解决了数据稀疏的问题。</p><p id="adac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在不深入语言模型数学的情况下，让我们理解 ULMFIT 是如何工作的，这将给出一个关于神经语言模型的想法。</p><p id="2c13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通用语言模型微调(ULMFIT)是一种迁移学习技术，可以帮助完成各种 NLP 任务。很长一段时间以来，它一直是最先进的 NLP 技术，但后来它被 BERT(最近在文本分类方面被 XLNet 取代)取代了。我们将在博客的下一部分详细研究 BERT。</p><blockquote class="ln lo lp"><p id="f5da" class="ki kj lm kk b kl km ju kn ko kp jx kq lq ks kt ku lr kw kx ky ls la lb lc ld im bi translated">乌尔菲特的优点</p></blockquote><p id="4316" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">深度学习需要大量数据集。具体来说，在进行迁移学习时，我们有一个大型数据集，我们的基础模型就是在这个数据集上构建的，我们将学习神经网络的参数迁移到我们的领域特定数据集。当我们有一个更小的特定领域数据集时，模型会过拟合。为了解决这个问题，杰瑞米·霍华德和塞巴斯蒂安·鲁德在他们的论文《关于文本分类的通用语言模型微调的<a class="ae lt" href="https://arxiv.org/pdf/1801.06146.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lm"/></a><em class="lm"/>中提出了 3 种不同的技术，用于针对 NLP 特定任务的迁移学习 LMs 中的微调</p><ul class=""><li id="75ea" class="lu lv it kk b kl km ko kp kr lw kv lx kz ly ld lz ma mb mc bi translated"><strong class="kk iu">区别微调</strong></li><li id="6e57" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated"><strong class="kk iu">倾斜三角形学习率</strong></li><li id="a23e" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated"><strong class="kk iu">逐步解冻</strong></li></ul><p id="f0b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们了解使用 ULMFiT 创建文本分类器的各个阶段，通过它我们将了解作者建议的 3 种新技术。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mi"><img src="../Images/5cf81c381f782065c558dc4c6e21a11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QS7HQRBB8Y_bJi6BLaoDgg.png"/></div></div></figure><p id="8f19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ULMFiT 包括 3 个主要阶段:LM 预训练、LM 微调和分类器微调。</p><p id="c113" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该方法是通用的，因为它满足这些实用标准:</p><ol class=""><li id="6589" class="lu lv it kk b kl km ko kp kr lw kv lx kz ly ld mn ma mb mc bi translated">它适用于不同文档大小、数量和标签类型的任务。</li><li id="ae32" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld mn ma mb mc bi translated">它使用单一的架构和培训流程。</li><li id="4a7c" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld mn ma mb mc bi translated">它不需要定制的特征工程或预处理。</li><li id="a95e" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld mn ma mb mc bi translated">它不需要额外的域内文档或标签。</li></ol><p id="1e9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AWD-LSTM 是一个<a class="ae lt" href="https://arxiv.org/abs/1708.02182" rel="noopener ugc nofollow" target="_blank"> <em class="lm">最先进的</em> </a>语言模型，一个常规的 LSTM(没有关注、快捷连接或其他复杂的添加)，具有各种调整过的丢失超参数。作者使用 AWD-LSTM 作为他们建筑中的 LM。</p><h1 id="0b25" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">(a)法律硕士职前培训</h1><p id="38b6" class="pw-post-body-paragraph ki kj it kk b kl ng ju kn ko nh jx kq kr ni kt ku kv nj kx ky kz nk lb lc ld im bi translated">LM 在一个<strong class="kk iu">通用领域语料库</strong>上接受训练，以捕获不同层次语言的一般特征</p><p id="20b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在大型通用领域语料库上预先训练 LM，并使用新技术在目标任务上对其进行微调。因此，作者们使用了 Wikitext-103，这是一个由 2.8 万篇预处理文章组成的数据集，包含 1.03 亿个单词。一般来说，数据集应该非常大，以至于 LM 可以学习语言的所有属性。就计算资源和时间而言，这也是最昂贵的。因此我们只做一次。</p><h1 id="fb67" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated"><strong class="ak"> (b) LM 微调</strong></h1><p id="d0ba" class="pw-post-body-paragraph ki kj it kk b kl ng ju kn ko nh jx kq kr ni kt ku kv nj kx ky kz nk lb lc ld im bi translated">在几乎所有情况下，目标任务数据集相对于一般领域语料库将具有不同的分布。在这个阶段，我们通过使用<strong class="kk iu"> <em class="lm">判别微调和倾斜三角形学习率，在目标任务数据集上微调模型以学习其分布。</em>T11】</strong></p><p id="7411" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于不同的层次掌握不同的信息，作者建议对每一层进行不同程度的微调。</p><p id="950c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在随机梯度下降中，我们在每个时间步长 t 更新θ。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8544f226fb508d8441bfc67d8e2cc985.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*Bx5y3GK5YTysfr404t1C6w.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk">Regular SGD</figcaption></figure><p id="ec92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在区别微调中，我们使用θ1、θ2、… θL，而不是各个 L 层的单个θ值。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/0e337c9555735069d12a4137aecc975b.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*qKPGsJVNlvMawVwIy5RBcg.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk">Discriminative Learning rate</figcaption></figure><blockquote class="ln lo lp"><p id="9f42" class="ki kj lm kk b kl km ju kn ko kp jx kq lq ks kt ku lr kw kx ky ls la lb lc ld im bi translated">U <!-- -->在整个培训过程中保持相同的学习率(LR)或一个稳定的学习率并不是实现这种行为的最佳方式。相反，我们提出倾斜的三角学习率(STLR)</p></blockquote><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0f1d30611c9a6f936e1fcfef83a2a3c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*snyVzCZjBdvFY_HNFMJK1A.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk">The slanted triangular learning rate schedule used for ULMFiT as a function of the number of training iterations.</figcaption></figure><p id="1b0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 STLR，作者建议线性增加学习速率，并以下列方式衰减它。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e0f2dc50ad10c58f14396661da13cbbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*5uK4WSWIFfAZJW1eAFolEg.png"/></div></figure><p id="92d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在哪里，</p><ul class=""><li id="e4bc" class="lu lv it kk b kl km ko kp kr lw kv lx kz ly ld lz ma mb mc bi translated"><strong class="kk iu"> T </strong>是训练迭代次数</li><li id="5337" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated"><strong class="kk iu"> cut_frac </strong>是迭代的分数</li><li id="3044" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated"><strong class="kk iu"> cut </strong>是我们从增加 LR 切换到减少 LR 时的迭代</li><li id="17b8" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated">p 是我们分别增加或减少 LR 的迭代次数的分数</li><li id="beef" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated"><strong class="kk iu">比率</strong>指定最低 LR 比最大 LR ηmax 小多少</li><li id="0626" class="lu lv it kk b kl md ko me kr mf kv mg kz mh ld lz ma mb mc bi translated"><strong class="kk iu"> ηt </strong>是迭代 t 时的学习率</li></ul><p id="2bbf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lm"> STLR 曾经用</em> </strong> <a class="ae lt" href="https://arxiv.org/abs/1608.03983" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="lm">达到最先进的</em> </strong> </a> <strong class="kk iu"> <em class="lm">成绩 CV </em> </strong></p><h1 id="b2ac" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated"><strong class="ak"> (c)选粉机微调</strong></h1><p id="482d" class="pw-post-body-paragraph ki kj it kk b kl ng ju kn ko nh jx kq kr ni kt ku kv nj kx ky kz nk lb lc ld im bi translated">微调是迁移学习的最重要的状态，需要尽最大的努力。因为过于激进的微调会使我们的模型过拟合，反之也会使我们的模型欠拟合。作者建议采用逐步解冻的方法来解决这一重大问题。</p><p id="d50b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们从解冻最后一层开始，因为它包含了最基本的知识。在微调一个时期的未冻结层之后，我们继续下一个较低层，并重复直到我们完成所有层，直到在最后一次迭代中收敛。</p><p id="a42b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用于文本分类的<strong class="kk iu"/><strong class="kk iu"/>(BPT3C)语言模型通过时间反向传播(BPTT)来训练，以实现大输入序列的梯度传播。为了使针对大文档的分类器的微调变得可行，作者提出了用于文本分类的<strong class="kk iu">BPTT(BPT3C):</strong>将文档分成大小为 b 的固定长度的批次，在每个批次开始时，用前一批次的最终状态来初始化模型；跟踪平均池和最大池的隐藏状态；梯度被反向传播到其隐藏状态对最终预测有贡献的批次。在实践中，作者建议使用可变长度反向传播序列。</p><p id="7c54" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，现在我们完成了压倒性的理论。让我们深入研究代码！！</p><p id="7f19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在 quora 的问题文本上实现文本分类，找出不真诚的问题。数据集在<a class="ae lt" href="https://www.kaggle.com/c/quora-insincere-questions-classification/overview" rel="noopener ugc nofollow" target="_blank"> <em class="lm"> kaggle </em> </a>可用。</p><p id="fc07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代码即将发布。在此之前，请亲自尝试。</p></div></div>    
</body>
</html>