# 使用石头剪刀布和 Tensorflow.js 进行强化学习的初学者指南

> 原文：<https://towardsdatascience.com/a-beginners-guide-to-reinforcement-learning-using-rock-paper-scissors-and-tensorflow-js-37d42b6197b5?source=collection_archive---------17----------------------->

Tensorflow.js 刚刚发布，我刚刚参加了一个关于它在浏览器中的用例的讲座。我有在 Python 中广泛使用 Tensorflow 的经验，但我很好奇在浏览器中构建一个小型强化学习示例有多困难，在这个示例中，您可以看到一个代理随着时间的推移而学习。

我选择了石头剪子布，出于简洁和懒惰的原因，从现在开始它将被称为 RPS，因为它的游戏规则很简单，并且因为我知道学习只需要很少的例子。

本教程面向谁:任何对强化学习(RL)和神经网络的直观非数学教程感兴趣的人。他们应该有 JavaScript 和 html 的基本知识来理解代码，但这不是理解概念部分所必需的。

目标是:构建一个能够使用强化学习和神经网络来学习 RPS 规则的代理。这意味着如果用户选择剪刀，我们希望代理能够选择石头。

强化学习直观上可以描述如下:

循环:基于信念做某事→获得积极或消极的奖励→基于奖励更新信念

因此，当构建强化学习问题时，你必须记住上面的循环。所以我们可以用同样的方式分解 RPS 游戏。

1.  当用户选择一步棋时，代理有一些关于它应该选择哪一步棋的信念。即，当用户选择纸张时，代理认为它应该选择石头。
2.  用户对代理选择的移动给予积极或消极的奖励。
3.  当用户选择纸张时，代理更新它关于是否应该选择石头的信念。

记住这一点，本教程分为两个部分:

1.  script.js，它将包含执行强化学习周期的代码。
2.  index.html 将允许用户与学习代理交互，并实时可视化学习。

我们将从 script.js 开始。

Choose Move

学习周期的开始是“根据代理的信念做一些事情”。在这个上下文中，这意味着根据用户的移动选择一个移动。

第 6–12 行:在评估阶段，我们想看看代理学得有多好。因此，选择的移动是具有最高值的移动。即代理最有信心的移动。这是借助神经网络完成的。

旁白:神经网络

神经网络是一种可以调整的功能。它接受一些输入，对输入做一些事情，然后输出一些结果。

![](img/ae4b03ffe9ba40fd6b67a57d034ffd6e.png)

Function

在 RL 和 RPS 的上下文中，使用神经网络来表示代理的信念。网络的输入是用户的移动，输出是代理在选择石头、布、剪刀时的信心。

![](img/836ab1b58cac1cb49dca4be58994661e.png)

Neural Network for Agent’s Beliefs

从上面可以看出，用户的举动是摇滚。代理人 0.7 自信应该选剪刀，0.1 自信选石头，0.2 自信选纸。如果我们处于评估阶段，代理将选择价值最高/最有信心的行动。即剪刀。

第 8–12 行:将移动转换为神经网络可以理解的格式，向网络请求其输出，然后选择具有最高值的移动。

第 14–16 行:在训练阶段，不是基于信念选择移动，而是随机选择移动。这使得代理可以探索所有的移动，而不是总是选择相同的移动。

Update beliefs

强化循环的下一阶段是从用户那里获得奖励，奖励将在 index.html 处理。收到奖励后，代理可以更新它的信念。

第 5–7 行:处理获取代理对用户移动应该选择的移动的信心

第 10-12 行:用奖励更新代理的信念。这是使用神经网络完成的。

旁白:更新神经网络

![](img/f328f81baaeb6eacb7dcbb6e8f9f8095.png)

Updating a Neural Network

在这个场景中，用户选择了石头，代理选择了剪刀。用户给出了-100 的奖励，因为这一步棋是错误的。然后将-100 加到 0.7 上，并送回网络。这将告诉网络 0.7 的值太高，应该降低一些。这个过程叫做反向传播。这是更新代理人信念的行为。在这种情况下，代理应该对选择剪刀不太有信心，而对选择石头或布更有信心。

第 13 行:一旦网络被更新，新的信念就被绘制出来。

Plot the beliefs of the Agent

这种方法遍历用户可以选择的所有移动，并为用户的每个移动绘制代理的信念。

第 10 行:应用一个函数使神经网络的输出总和为 1。这使得用户能够更容易地理解神经网络的输出。

第 25 行:使用 Plotly.js 绘制 index.html 每个 div 中的数据集

![](img/473a401dba60a0470df5162d2db9d5ea.png)

Example output of the Network

现在我们已经完成了强化循环和可视化部分的所有部分。接下来我们将描述 index.html。这需要做几件事:

1.  允许用户选择移动。
2.  用户应该能够说出代理的移动是好是坏。
3.  在学习代理随机选择移动和选择它最有信心的移动之间切换的能力。
4.  可视化代理的信念，这意味着当用户选择一个移动时，显示代理在选择一个特定移动时有多自信

以下节选自 index.html，反映了上述观点。

User interface

第 1–3 行:有三个按钮是用户移动的，单击它们将调用 chooseMove 函数并向它传递按钮的值。这将使代理选择并返回一个移动。

第 5–6 行:当用户决定代理选择的移动是好是坏时，这些按钮将调用训练功能。这将告诉代理积极或消极地更新它的信念。

第 8–11 行:切换代理是正在学习还是正在评估其所学内容

第 13-17 行:包含 div，这些 div 将用于描绘代理对每个动作的信念。

至此，本教程到此结束。要查看所有代码，请访问这个[库](https://github.com/sachag678/freeCodeCamp)。你可以随意克隆它并使用代码。克隆完成后，在浏览器中打开 index.html。您将看到下图所示的设置。

![](img/364acb7db58af1a37825bc0a6f6c5314.png)

最上面的按钮是用户的移动。一旦您选择了移动，代理将随机选择一个移动。然后，用户可以点击正面奖励或负面奖励按钮，信念将实时更新。单击新的移动并重复该过程。一旦你对学习到的行为感到满意，将开关从训练切换到评估，然后代理选择的移动将是每个直方图中最高值的移动。

我希望本教程对您有所帮助，并让您直观地了解如何构建强化学习问题，以及如何使用神经网络来帮助解决这个问题。

请继续关注神经网络和强化学习基础的进一步教程。

感谢阅读。