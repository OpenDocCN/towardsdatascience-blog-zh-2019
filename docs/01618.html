<html>
<head>
<title>Hyper-parameter Tuning Techniques in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的超参数调整技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8?source=collection_archive---------2-----------------------#2019-03-16">https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8?source=collection_archive---------2-----------------------#2019-03-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="30e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设置超参数的过程需要专业知识和大量的反复试验。没有简单易行的方法来设置超参数，特别是学习率、批量、动量和重量衰减。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/561842ff0dc065b035203a7c48ba7397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZIVv73msFAsWheL82C9QQ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://unsplash.com/photos/RT3QngqeIEc" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><blockquote class="lc"><p id="45c7" class="ld le iq bd lf lg lh li lj lk ll kk dk translated">深度学习模型充满了超参数，在如此高维的空间中找到这些参数的最佳配置不是一个微不足道的挑战。</p></blockquote><p id="ea70" class="pw-post-body-paragraph jn jo iq jp b jq lm js jt ju ln jw jx jy lo ka kb kc lp ke kf kg lq ki kj kk ij bi translated">在讨论寻找最优超参数的方法之前，让我们先了解这些超参数:<strong class="jp ir">学习率</strong>、<strong class="jp ir">批量</strong>、<strong class="jp ir">动量</strong>、<strong class="jp ir">权重衰减</strong>。这些超参数充当旋钮，可以在模型训练期间调整。为了让我们的模型提供最佳结果，我们需要找到这些<strong class="jp ir">超参数的最佳值。</strong></p><h2 id="8f00" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">梯度下降</h2><p id="153f" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">梯度下降是一种常用于训练机器学习算法的优化技术。训练 ML 算法的主要目的是调整权重<code class="fe mp mq mr ms b"><em class="mt">w</em></code>以最小化损失或成本。这个成本是我们的模型做得有多好的度量，我们用<em class="mt"> </em> <code class="fe mp mq mr ms b"><em class="mt">J(w)</em></code>来表示这个成本。因此，通过最小化成本函数，我们可以找到产生最佳模型性能的最佳参数[1]。</p><p id="8701" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回归问题的典型损失函数图是碗形的，如下所示。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="ab gu cl mu"><img src="../Images/e25d51f913415e644001dfd2abdebabc.png" data-original-src="https://miro.medium.com/v2/format:webp/1*9j2Vj8L8jm55NpM4LsE8Ew.png"/></div></figure><p id="d569" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在梯度下降算法中，我们从随机模型参数开始，计算每次学习迭代的误差，不断更新模型参数，以更接近产生最小成本的值。详情请参考我的<a class="ae lb" rel="noopener" target="_blank" href="/machine-learning-basics-part-1-a36d38c7916">帖子</a>。梯度下降算法将梯度(斜率)乘以一个称为<strong class="jp ir">学习速率</strong>(或<strong class="jp ir">步长</strong>)的标量，以确定下一个点。此参数说明了在渐变方向上移动权重的距离。</p><p id="8c1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们将<code class="fe mp mq mr ms b">dw</code>和<code class="fe mp mq mr ms b">db</code>表示为梯度，以更新梯度下降算法的参数<code class="fe mp mq mr ms b">W</code>和<code class="fe mp mq mr ms b">b</code>，如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="ab gu cl mu"><img src="../Images/06f6650233ef57e6e8de7de9bc6ef5d9.png" data-original-src="https://miro.medium.com/v2/format:webp/1*J6d0-3brWTtq1YhLrSyEMA.png"/></div></figure><p id="9994" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果学习率小，那么训练更可靠，但是它将花费大量时间，因为向损失函数的最小值的步骤很小。</p><p id="4e85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果学习率很高，那么训练可能不收敛，甚至发散。重量变化可能如此之大，以至于优化器超过了最小值，使损失更严重。因此，我们的目标是找到能够快速找到最小损失的最优学习速率。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mv"><img src="../Images/1cf2a6f9621710a70f550ae70066dd03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9zqj3nwIEU-L0-9pYitcRA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://www.jeremyjordan.me/nn-learning-rate/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="394b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于更一般的模型，您可以将梯度下降想象为一个球在山谷中滚动。我们希望它坐落在大山的最深处，然而，很容易看出事情可能会出错。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/e19dc26a376a6efb67490445311d4d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*SFU_7bF7P-5I-z9NaqvQkw.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" rel="noopener" target="_blank" href="/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645">Source</a></figcaption></figure><p id="e9e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据球开始滚动的位置，它可能会停在谷底。但不是在最低的一个。这被称为局部最小值。我们初始化模型权重的方式可能导致它停留在局部最小值。为了避免这种情况，我们用随机分布的值初始化权重向量。</p><p id="3772" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以用二维表示损失面，如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mx"><img src="../Images/d0e2da584238c1a89a0cc0633c80cc64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ir4daeF9OVL6Gz1G.png"/></div></div></figure><p id="dbb4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">红点是全局最小值，我们想要到达那个点。使用梯度下降，更新将如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi my"><img src="../Images/cb1d8799e6413537154b6e362c3991de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VV9CjHyPr8QsIZKD.png"/></div></div></figure><p id="ecba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着梯度下降的每一次迭代，我们随着上下振荡向局部最优移动。如果我们使用更大的学习率，那么垂直振荡将具有更高的幅度。所以，这种垂直振荡减缓了我们的梯度下降，并阻止我们使用大得多的学习率。此外，学习率太小会使梯度下降更慢。</p><blockquote class="mz na nb"><p id="308e" class="jn jo mt jp b jq jr js jt ju jv jw jx nc jz ka kb nd kd ke kf ne kh ki kj kk ij bi translated">我们希望在垂直方向上进行较慢的学习，在水平方向上进行较快的学习，这将帮助我们更快地达到全局最小值。</p></blockquote><p id="e967" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了帮助我们实现这一点，我们使用<strong class="jp ir">梯度下降与动量</strong>【2】。</p><p id="196f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们从梯度下降开始:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="ab gu cl mu"><img src="../Images/06f6650233ef57e6e8de7de9bc6ef5d9.png" data-original-src="https://miro.medium.com/v2/format:webp/1*J6d0-3brWTtq1YhLrSyEMA.png"/></div></figure><p id="c42d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在动量项中，我们采用 dw 和 db 的指数加权平均值，而不是对每个历元独立使用 dw 和 db。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nf"><img src="../Images/7b581e2380c76d77fd75c4e72eec618e.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*XRRuu9cUkLbRlvaYlYMPuA.png"/></div></div></figure><p id="6cf5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中，β是另一个称为动量的超参数，范围从 0 到 1。它设置以前值的平均值和当前值之间的权重，以计算新的加权平均值。</p><p id="9ea9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在计算指数加权平均值后，我们将更新我们的参数。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/983f9b6c1141f7b731dfb7e799626b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*-tsqOL94claZYkM7RkwIPg.png"/></div></figure><p id="8ded" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过使用 dw 和 db 的指数加权平均值，我们倾向于使垂直方向上的振荡趋于零。然而，在水平方向上，所有的导数都指向水平方向的右边，所以水平方向上的平均值仍然很大。它允许我们的算法采取更直接的路径走向局部最优，并抑制垂直振荡。由于这个原因，该算法将通过几次迭代而在局部最优处结束。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nh"><img src="../Images/0c22e1d16bf492532223769394083ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXKymgKu9bv2gJIgJYENmw.png"/></div></div></figure><p id="9c8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要直观地了解这是如何工作的，可以考虑一个球滚下山的例子——vᵈʷ和 Vᵈᵇ为球提供了速度，使它运动得更快。我们不希望我们的球加速太快，以至于错过了全局最小值，因此β作为摩擦力。</p><p id="fe15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降有三种方式:</p><p id="fb7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">批量梯度下降:</strong></p><ul class=""><li id="7c58" class="ni nj iq jp b jq jr ju jv jy nk kc nl kg nm kk nn no np nq bi translated">一次所有示例:在每次迭代中使用所有训练实例来更新模型参数。</li><li id="a024" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">随着误差梯度的精确估计而缓慢收敛。</li></ul><p id="50f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">随机梯度下降(SGD): </strong></p><ul class=""><li id="4a71" class="ni nj iq jp b jq jr ju jv jy nk kc nl kg nm kk nn no np nq bi translated"><em class="mt">一次一个例子</em>:在每次迭代中仅使用单个训练实例更新参数。训练实例通常是随机选择的。</li><li id="e843" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">在误差梯度的噪声估计下快速收敛。</li></ul><p id="afe0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">小批量梯度下降:</strong></p><ul class=""><li id="2892" class="ni nj iq jp b jq jr ju jv jy nk kc nl kg nm kk nn no np nq bi translated">一次 b 个样本:小批量梯度下降不是使用所有的样本，而是将训练集分成更小的规模，称为批量，用“b”表示。因此，小批量“b”用于在每次迭代中更新模型参数。</li></ul><blockquote class="lc"><p id="c378" class="ld le iq bd lf lg lh li lj lk ll kk dk translated"><em class="nw">小批量梯度下降寻求在随机梯度下降的鲁棒性和批量梯度下降的效率之间找到平衡。</em></p></blockquote><p id="edbd" class="pw-post-body-paragraph jn jo iq jp b jq lm js jt ju ln jw jx jy lo ka kb kc lp ke kf kg lq ki kj kk ij bi translated">小批量梯度下降是深度学习领域中最常见的梯度下降实现。Mini-batch 的缺点是它为学习算法增加了一个额外的超参数“批量大小”或“b”。</p><h1 id="3d62" class="nx ls iq bd lt ny nz oa lw ob oc od lz oe of og mc oh oi oj mf ok ol om mi on bi translated">寻找最佳配置的方法:网格搜索和随机搜索</h1><h2 id="bc06" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">网格搜索</h2><p id="e44c" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">在网格搜索[3]中，我们尝试了每个可能的参数配置。</p><p id="3330" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">步骤:</p><ul class=""><li id="aaa1" class="ni nj iq jp b jq jr ju jv jy nk kc nl kg nm kk nn no np nq bi translated">在<em class="mt"> n </em>维上定义一个网格，每个网格映射一个超参数。例如<em class="mt"> n </em> = (learning_rate，，batch_size)</li><li id="07eb" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">对于每个维度，定义可能值的范围:例如，batch_size = [4，8，16，32]，learning_rate =[0.1，0.01，0.0001]</li><li id="6194" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">搜索所有可能的配置，并等待结果以建立最佳配置:例如<em class="mt"> C1 </em> = (0.1，4) - &gt; acc = 92%，<em class="mt"> C2 </em> = (0.01，4) - &gt; acc = 92.3%，等等</li></ul><p id="f121" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们看到的，维度越多，搜索的时间复杂度就越高。当维度小于或等于 4 时，通常使用这种方法。虽然它保证最终找到最佳配置，但它仍然不是首选。相反，最好使用随机搜索</p><h2 id="a920" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">随机搜索</h2><p id="8ec2" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">随机搜索[4]在<em class="mt">步骤 1 </em>中进行随机搜索，从配置空间中随机选取一个点。它如何更好地工作的直觉是，我们可以通过随机搜索更广泛地探索超参数空间(特别是对于更重要的变量)。这将帮助我们在更少的迭代中找到最佳配置。例如，请参见下图:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oo"><img src="../Images/9c95974e6de3f6022c725884aa9b5efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*t1EKsWXpTqyhg9nV.png"/></div></div></figure><p id="069d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在网格布局中，很容易注意到，即使我们训练了 9 个(n=3)模型，每个变量也只使用了 3 个值。然而，在随机布局中，我们不太可能多次选择相同的变量。最后，使用第二种方法，我们将为每个变量使用 9 个不同的值来训练 9 个模型。关于网格 vs 随机的详细分析，请参考这篇<a class="ae lb" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><p id="3f59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">即使随机搜索比网格搜索执行得更好，这两种方法仍然计算量大且耗时。<strong class="jp ir"><em class="mt">2018 年，Leslie N. Smith 在其经典的</em> </strong> <a class="ae lb" href="https://arxiv.org/pdf/1803.09820.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="mt">论文</em> </strong> </a> <strong class="jp ir">中，就确定最优超参数的各种方法发表了详细的报告。</strong>我们将快速浏览 Smith 建议的方法【5】。该方法基于通过检查训练的测试/验证损失来寻找欠拟合和过拟合的线索，从而找到欠拟合和过拟合之间的平衡，以便努力获得超参数的最优集合。</p><blockquote class="lc"><p id="412a" class="ld le iq bd lf lg op oq or os ot kk dk translated">超参数调整过程是一个走钢丝的过程，目的是在欠适应和过适应之间取得平衡。</p></blockquote><p id="63a2" class="pw-post-body-paragraph jn jo iq jp b jq lm js jt ju ln jw jx jy lo ka kb kc lp ke kf kg lq ki kj kk ij bi translated"><strong class="jp ir">欠拟合</strong>是指机器学习模型无法减少测试集或训练集的误差。不合适的模型不足以适应数据分布的潜在复杂性。</p><p id="2a03" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">过度拟合</strong>发生在机器学习模型强大到与训练集拟合得太好，泛化误差增大的时候。上图显示了这种欠拟合和过拟合的权衡。</p><h2 id="8dd0" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">方法</h2><ol class=""><li id="7bc5" class="ni nj iq jp b jq mk ju ml jy ou kc ov kg ow kk ox no np nq bi translated">通过<strong class="jp ir">监控验证/测试损失</strong>观察和理解培训期间可用的线索。在培训的早期，通过几个时期的短期运行来调整您的架构和超参数。</li><li id="1c16" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk ox no np nq bi translated">测试的<strong class="jp ir"> <em class="mt">欠拟合</em> </strong>或<strong class="jp ir"> <em class="mt">过拟合</em> </strong>的迹象或训练过程早期的验证损失对于调整超参数是有用的。</li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oy"><img src="../Images/c4eeb4526b18e5ce45ef3785ba4999a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WyncMcJhXa7yPYebXyRq-Q.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Model complexity refers to the capacity of the machine learning model. The figure shows the optimal capacity that falls between underfitting and overfitting.</figcaption></figure><h1 id="07cc" class="nx ls iq bd lt ny nz oa lw ob oc od lz oe of og mc oh oi oj mf ok ol om mi on bi translated">寻找最佳超参数</h1><h2 id="0fc8" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">学习率</h2><p id="8af1" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">如果学习率(LR)太小，可能会出现过拟合。大的学习率有助于使训练规律化，但是如果学习率太大，训练就会分散。因此，对短期运行进行网格搜索以找到收敛或发散的学习率是可能的，但我们有另一种方法，称为 Leslie N. Smith 的“<a class="ae lb" href="https://arxiv.org/pdf/1506.01186.pdf" rel="noopener ugc nofollow" target="_blank">循环学习率(CLR) </a>”。</p><p id="d5dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Leslie 的实验表明，在训练期间改变学习速率总体上是有益的，因此建议在一个值范围内循环地改变学习速率，而不是将其设置为固定值。这种学习率政策的本质来自于这样一种观察，即提高学习率可能会产生短期的负面影响，但却能产生长期的有益影响。这一观察引出了让学习率在一个值的范围内变化的想法，而不是采用逐步的、固定的或指数递减的值。也就是说，设置最小和最大边界，学习率在这些边界之间循环变化。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/084780729b904f1f00abddd810364112.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*RaBzl8jctejDxpbZnVlnVg.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://arxiv.org/pdf/1506.01186.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="0d6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如何估计合理的最小和最大边界值？</p><p id="7cfd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> LR 范围测试:</strong>在让学习率在低 LR 值和高 LR 值之间线性增加的同时，运行你的模型几个时期。每当你面对一个新的架构或数据集时，这个测试都是非常有价值的。对于浅 3 层架构，large 是 0.01，而对于 resnet，large 是 3.0，您可以尝试多个最大值。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/7c2413a469caf5b5dca2496f8a12d0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*dc8ADXWZieBWzIXjyzyQXQ.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">From my previous <a class="ae lb" rel="noopener" target="_blank" href="/machine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d">post</a>, using fast.ai library to do a LR test</figcaption></figure><p id="62ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 1 周期 LR 策略，通过 LR 范围测试确定最大学习率，最大学习率的十分之一的最小学习率似乎工作良好[6]。</p><h2 id="2e2f" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">批量</h2><p id="0c0a" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">与学习率超参数(其值不影响计算时间)不同，批量大小必须结合训练的执行时间进行检查。批量大小受硬件内存的限制，而学习速率不受限制。Leslie 建议使用适合硬件内存的批处理大小，并允许使用更大的学习速率。</p><p id="3107" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您的服务器有多个 GPU，总批处理大小是 GPU 上的批处理大小乘以 GPU 的数量。如果体系结构很小，或者您的硬件允许非常大的批量，那么您可以比较不同批量的性能。此外，请记住，小批量增加正则化，而大批量增加较少，因此在平衡正则化的适当数量时利用这一点。使用较大的批量通常更好，这样可以使用较大的学习速率。</p><h2 id="849a" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated"><strong class="ak">周期性势头</strong></h2><p id="fb94" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">动量和学习率密切相关。最佳学习速率取决于动量，动量取决于学习速率。由于学习率被认为是最重要的超参数，因此动量也很重要。像学习率一样，在不导致训练不稳定的情况下，将动量设置得尽可能大是很有价值的。</p><h2 id="aa68" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">寻找学习率和动量组合的步骤</h2><ul class=""><li id="b630" class="ni nj iq jp b jq mk ju ml jy ou kc ov kg ow kk nn no np nq bi translated"><strong class="jp ir">使用循环学习率:</strong>最佳训练程序是一个递增的循环学习率和递减的循环动量的组合，前者初始的小学习率允许收敛开始，后者递减的动量允许学习率在训练的早期到中期变大。当学习率增加时，使用递减的循环动量提供了更快的初始收敛，并稳定了训练以允许更大的学习率。</li></ul><blockquote class="lc"><p id="d03e" class="ld le iq bd lf lg lh li lj lk ll kk dk translated">循环动量对于以大动量开始并在学习速率增加时减小动量是有用的，因为它提高了测试精度，并使训练对大学习速率更鲁棒。</p></blockquote><p id="1f8a" class="pw-post-body-paragraph jn jo iq jp b jq lm js jt ju ln jw jx jy lo ka kb kc lp ke kf kg lq ki kj kk ij bi translated">下面的图来自我的<a class="ae lb" rel="noopener" target="_blank" href="/machine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d">帖子</a>典型地显示了学习率和动量在一个训练周期(一个时期)中是如何变化的。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi pb"><img src="../Images/96c279bf414258d0c2461ae9247cbf99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WL5k-6uVwxXzHHjq1vkKXw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">left: learning rate one cycle, right:momentum for one cycle</figcaption></figure><ul class=""><li id="5c92" class="ni nj iq jp b jq jr ju jv jy nk kc nl kg nm kk nn no np nq bi translated"><strong class="jp ir">使用恒定学习率:</strong>如果使用循环学习率，则相反方向的循环动量是有意义的，但是<strong class="jp ir">当学习率恒定时，最佳动量是什么？</strong>在这里周期性的动量并不比一个好的常量值强。如果使用恒定的学习速率，那么大的恒定动量(即 0.9-0.99)将表现为伪递增的学习速率，并将加速训练。然而，使用过大的动量值会导致较差的训练结果，这在训练的早期是可见的，并且这可以被快速测试。</li></ul><blockquote class="lc"><p id="15fd" class="ld le iq bd lf lg lh li lj lk ll kk dk translated">对于循环学习率或恒定学习率，一个好的程序是测试 0.9 到 0.99 范围内的动量值，并选择一个表现最佳的值。</p></blockquote><h2 id="6d2b" class="lr ls iq bd lt lu pc dn lw lx pd dp lz jy pe mb mc kc pf me mf kg pg mh mi mj bi translated">重量衰减</h2><p id="8412" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">权重衰减是正则化的一种形式，它在训练中起着重要的作用，因此需要适当地设置它的值[7]。<strong class="jp ir">权重衰减</strong>被定义为将每个历元的梯度下降中的每个权重乘以因子λ [0 &lt; λ &lt; 1】。</p><p id="a461" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Leslie 的实验表明，重量衰减不同于学习速率或动量，最佳值应在训练过程中保持不变(即<strong class="jp ir">周期性重量衰减没有用</strong>)。</p><p id="671f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你不知道重量衰减的合理值，测试 1/10，1/10⁴，1/10⁵和 0。较小的数据集和架构似乎需要较大的权重衰减值，而较大的数据集和较深的架构似乎需要较小的值。我们的假设是，复杂数据提供了自己的正则化，其他正则化应该减少。</p><p id="0c1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你用一个恒定的学习速率和使用一个学习速率范围进行搜索，最佳权重衰减是不同的。这符合我们的直觉，因为较大的学习率提供了正则化，所以较小的权重衰减值是最佳的。</p><h1 id="2036" class="nx ls iq bd lt ny nz oa lw ob oc od lz oe of og mc oh oi oj mf ok ol om mi on bi translated">主要调查结果摘要</h1><h2 id="a647" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated"><strong class="ak">学习率(LR): </strong></h2><ul class=""><li id="4e1a" class="ni nj iq jp b jq mk ju ml jy ou kc ov kg ow kk nn no np nq bi translated">执行学习率范围测试，以确定“大”学习率。</li><li id="91e2" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">使用 1 循环 LR 策略，最大学习率由 LR 范围测试确定，将最小学习率设置为最大学习率的十分之一。</li></ul><h2 id="ff96" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated"><strong class="ak">气势</strong>:</h2><ul class=""><li id="047b" class="ni nj iq jp b jq mk ju ml jy ou kc ov kg ow kk nn no np nq bi translated">使用动量值 0.99、0.97、0.95 和 0.9 进行短期测试，以获得动量的最佳值。</li><li id="6670" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">如果使用 1 周期学习率时间表，最好使用一个循环动量(CM ),它从这个最大动量值开始，并随着学习率的增加而减少到 0.8 或 0.85。</li></ul><h2 id="fac8" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated"><strong class="ak">批量:</strong></h2><ul class=""><li id="4600" class="ni nj iq jp b jq mk ju ml jy ou kc ov kg ow kk nn no np nq bi translated">使用尽可能大的批处理大小来适应您的内存，然后比较不同批处理大小的性能。</li><li id="862d" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">小批量增加正规化，而大批量增加较少，所以利用这一点，同时平衡适当数量的正规化。</li><li id="0691" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">使用较大的批量通常更好，这样可以使用较大的学习速率。</li></ul><h2 id="7798" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated"><strong class="ak">重量衰减:</strong></h2><ul class=""><li id="db2e" class="ni nj iq jp b jq mk ju ml jy ou kc ov kg ow kk nn no np nq bi translated">确定适当震级的网格搜索，但通常不要求超过一个有效数字的精度。</li><li id="b6d6" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">更复杂的数据集需要更少的正则化，因此测试更小的权重衰减值，例如 104、105、106、0。</li><li id="643e" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">浅架构需要更多的正则化，因此测试更大的权重衰减值，如 102、103、104。</li></ul><p id="9351" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢您的阅读。</p><h2 id="c674" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">参考资料:</h2><p id="0e25" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">[1]<a class="ae lb" href="https://www.jeremyjordan.me/gradient-descent/" rel="noopener ugc nofollow" target="_blank">https://www.jeremyjordan.me/gradient-descent/</a></p><p id="6b44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="https://engmrk.com/gradient-descent-with-momentum/" rel="noopener ugc nofollow" target="_blank">https://engmrk.com/gradient-descent-with-momentum/</a></p><p id="6f9c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3]<a class="ae lb" href="https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/" rel="noopener ugc nofollow" target="_blank">https://blog . Floyd hub . com/guide-to-hyperparameters-search-for-deep-learning-models/</a></p><p id="b04c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[4]<a class="ae lb" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">http://www . jmlr . org/papers/volume 13/bergstra 12a/bergstra 12a . pdf</a></p><p id="7310" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="https://arxiv.org/pdf/1803.09820.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1803.09820.pdf</a></p><p id="4b1d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[6]<a class="ae lb" href="https://arxiv.org/pdf/1506.01186.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.01186.pdf</a></p><p id="fdd8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[7]<a class="ae lb" href="https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/563-a-simple-weight-decay-can-improve-generalization . pdf</a></p><h2 id="8096" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">其他参考文献</h2><p id="43f2" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated"><a class="ae lb" href="https://www.analyticsvidhya.com/blog/2018/11/neural-networks-hyperparameter-tuning-regularization-deeplearning/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/11/neural-networks-hyperparameter-tuning-regulatory-deep learning/</a></p></div></div>    
</body>
</html>