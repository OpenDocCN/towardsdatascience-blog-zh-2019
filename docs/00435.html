<html>
<head>
<title>How to do Deep Learning on Graphs with Graph Convolutional Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用图卷积网络在图上做深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0?source=collection_archive---------1-----------------------#2019-01-20">https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0?source=collection_archive---------1-----------------------#2019-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8156" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第 2 部分:谱图卷积的半监督学习</h2></div><p id="ee70" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图的机器学习是一项困难的任务，因为图的结构非常复杂，但也能提供丰富的信息。本文是关于如何使用图形卷积网络(GCNs)对图形进行深度学习的系列文章中的第二篇，GCNs 是一种强大的神经网络，旨在直接对图形进行处理并利用其结构信息。我将对上一篇文章做一个简要的回顾，但是你可以在这里找到这个系列的其他部分:</p><ol class=""><li id="f8e0" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780">图卷积网络的高级介绍</a></li><li id="5c91" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">谱图卷积的半监督学习(this)</li></ol><p id="c82b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上一篇文章中，我对 GCNs 进行了高层次的介绍，并展示了如何根据相邻节点的表示来更新节点的表示。在这篇文章中，我们首先深入了解在上一篇文章中讨论的相当简单的图卷积中执行的聚合。然后我们继续讨论最近发布的图卷积传播规则，我展示了如何在小型社交网络 Zachary's 空手道俱乐部的社区预测任务中实现和使用它进行<a class="ae lk" href="https://en.wikipedia.org/wiki/Semi-supervised_learning" rel="noopener ugc nofollow" target="_blank">半监督学习</a>。如下所示，GCN 能够学习每个节点的潜在特征表示，该潜在特征表示将两个社区分成两个合理内聚和分离的聚类，尽管每个社区仅使用一个训练示例。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/f077e55af46ecad0e7090819ad77be8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*hLSeJjZHD3k_pcgHDQbC9g.gif"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Latent node representations of Zachary’s Karate Club in a GCN at each training epoch.</figcaption></figure><h1 id="7e78" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">简要回顾</h1><p id="b3c5" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">在我上一篇关于 GCNs 的文章中，我们看到了一个简单的数学框架来表达 GCNs 中的传播。简而言之，给定一个<em class="mz"> N × F⁰ </em>特征矩阵<strong class="kh ir">t5】xT7】和一个图结构的矩阵表示，例如<em class="mz"> G </em>的<em class="mz"> N </em> × <em class="mz"> N </em>邻接矩阵<strong class="kh ir"> <em class="mz"> A </em> </strong>，GCN 中的每一个隐层可以表示为<strong class="kh ir"><em class="mz">hⁱ</em></strong><em class="mz">= f(</em>【t24) <strong class="kh ir"><em class="mz">a</em></strong><em class="mz">)</em>其中<strong class="kh ir"><em class="mz">h</em></strong><em class="mz">⁰=</em><strong class="kh ir"><em class="mz">x</em></strong>和<em class="mz"> f </em>是一个传播规则。 每层<strong class="kh ir"> <em class="mz"> Hⁱ </em> </strong>对应一个<em class="mz">n</em>×<em class="mz">f</em><strong class="kh ir"><em class="mz">ⁱ</em></strong>特征矩阵，其中每一行都是一个节点的特征表示。</strong></p><p id="0daf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们看到了以下形式的传播规则</p><ol class=""><li id="34c4" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated"><em class="mz">f(</em><strong class="kh ir"><em class="mz">hⁱ</em></strong><em class="mz">，</em><strong class="kh ir"><em class="mz">a</em></strong><em class="mz">)=σ(</em><strong class="kh ir"><em class="mz">ahⁱwⁱ</em></strong><em class="mz">)，</em>和</li><li id="429e" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated"><em class="mz">f(</em><strong class="kh ir"><em class="mz"/></strong><em class="mz">，</em><strong class="kh ir"><em class="mz">a</em></strong><em class="mz">)=σ(</em><strong class="kh ir"><em class="mz">d</em></strong><em class="mz">⁻</em><strong class="kh ir"><em class="mz">âhⁱwⁱ</em></strong><em class="mz">)</em>凡<em class="mz"> </em> <strong class="kh ir"> <em class="mz"/></strong></li></ol><p id="6020" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些规则在通过应用权重<strong class="kh ir"><em class="mz">【wⁱ</em></strong>和激活函数<em class="mz"> σ </em>进行变换之前，将节点的特征表示计算为其邻居 <strong class="kh ir"> </strong>的特征表示的集合。我们可以通过将上面的传播规则 1 和 2 表示为<em class="mz">f(</em><strong class="kh ir"><em class="mz">hⁱ</em></strong><em class="mz">，</em><strong class="kh ir"><em class="mz">a</em></strong><em class="mz">)= transform(aggregate(</em><strong class="kh ir"><em class="mz">【a,hⁱ</em></strong><em class="mz">)，</em><strong class="kh ir"><em class="mz">wⁱ</em></strong><em class="mz"/>)其中 <strong class="kh ir"><em class="mz"/></strong><em class="mz">)</em><strong class="kh ir"><em class="mz">=</em></strong><em class="mz">【σ】(</em><strong class="kh ir"><em class="mz">mwⁱ</em></strong><em class="mz">)</em>和<em class="mz">合计(</em><strong class="kh ir"><em class="mz"/></strong><em class="mz">)=</em><strong class="kh ir"><em class="mz">ahⁱ<em class="mz"/></em></strong></p><p id="cd4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们在上一篇文章中所讨论的，规则 1 中的聚合将结点表示为其相邻要素表示的总和，这有两个明显的缺点:</p><ul class=""><li id="750a" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la na lh li lj bi translated">节点的聚集表示不包括它自己的特征，并且</li><li id="1a42" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la na lh li lj bi translated">度数较大的结点在其要素制图表达中将具有较大的值，而度数较小的结点将具有较小的值，这可能会导致梯度爆炸的问题，并使得使用对要素缩放敏感的随机梯度下降等算法进行训练变得更加困难。</li></ul><p id="7feb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了解决这两个问题，规则 2 首先通过将单位矩阵添加到<strong class="kh ir"><em class="mz"/></strong>来实施自循环，并使用转换后的邻接矩阵<strong class="kh ir"><em class="mz">= A</em></strong><em class="mz">+</em><strong class="kh ir"><em class="mz">I</em></strong>进行聚合。接下来，通过与逆度矩阵<strong class="kh ir"><em class="mz">d</em></strong><em class="mz">⁻</em>相乘来归一化特征表示，将集合变成平均值，其中集合的特征表示的比例对于节点度是不变的。</p><p id="4d7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下文中，我将把规则 1 称为<em class="mz">求和规则</em>，把规则 2 称为平均规则。</p><h1 id="692c" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">光谱图卷积</h1><p id="c534" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">Kipf 和 Welling 最近的一篇论文提出了使用光谱传播规则的快速近似光谱图卷积[1]:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/dbf7355a7decdd404b9dbfe558411a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*mJQRxmEz2XJ1Qgm3wkuaNQ@2x.png"/></div></figure><p id="d14b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与上一篇文章中讨论的求和规则和均值规则相比，谱规则的不同之处仅在于聚合函数的选择。虽然它有点类似于均值规则，因为它使用度数矩阵<strong class="kh ir"> <em class="mz"> D </em> </strong>的负幂来标准化聚集，但是标准化是不对称的。让我们试一试，看看它能做什么。</p><h2 id="e52c" class="nc md iq bd me nd ne dn mi nf ng dp mm ko nh ni mo ks nj nk mq kw nl nm ms nn bi translated">作为加权和的聚合</h2><p id="0c4a" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我们可以将我到目前为止介绍的聚合函数理解为加权和，其中每个聚合规则的不同之处仅在于它们对权重的选择。在继续讨论谱规则之前，我们首先来看看如何将相对简单的和与平均规则表达为加权和。</p><p id="c016" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">求和规则<br/> </strong>为了了解如何使用求和规则计算第<em class="mz"> i </em>个节点的集合特征表示，我们来看看如何计算集合中的第<em class="mz"> i </em>行。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi no"><img src="../Images/8fe8c909d63a889a5b5a5931784b2891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ne9RA0BGI04TRNepjBSu7A@2x.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">The Sum Rule as a Weighted Sum</figcaption></figure><p id="7b10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如以上等式 1a 所示，我们可以将第<em class="mz"> i </em>个节点的集合特征表示计算为向量矩阵乘积。我们可以将这个向量矩阵乘积公式化为一个简单的加权和，如等式 1b 所示，其中我们对<strong class="kh ir"> <em class="mz"> X </em> </strong>中的<em class="mz"> N </em>行中的每一行求和。</p><p id="98aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">等式 1b 中聚集的第<em class="mz"> j </em>个节点的贡献由第<strong class="kh ir"> <em class="mz"> A </em> </strong>的第<em class="mz"> i </em>行的第<em class="mz"> j </em>列的值确定。由于<strong class="kh ir"><em class="mz"/></strong>是邻接矩阵，如果第<em class="mz"> j </em>个节点是第<em class="mz"> i </em>个节点的邻居，则该值为 1，否则为 0。因此，等式 1b 对应于对第<em class="mz"> i </em>个节点的邻居的特征表示求和。这证实了前一篇文章中的非正式观察。</p><p id="2c3a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，每个邻居的贡献仅取决于由邻接矩阵<strong class="kh ir"><em class="mz"/></strong>定义的邻居。</p><p id="29fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">平均值规则<br/> </strong>为了查看平均值规则如何聚合节点表示，我们再次查看如何计算聚合中的第<em class="mz"> i </em>行，现在使用平均值规则。为了简单起见，我们只考虑“原始”邻接矩阵上的平均规则，而不考虑<strong class="kh ir"><em class="mz"/></strong>和单位矩阵<strong class="kh ir"> <em class="mz"> I </em> </strong>之间的加法，这简单地对应于将自循环添加到图中。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nt"><img src="../Images/7fbbcc596f02f8b4980089f40ed760c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c8cNoBQ6y7ERNIUbVRYwyQ@2x.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">The Mean Rule as a Weighted Sum</figcaption></figure><p id="1268" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从上面的等式中可以看出，现在求导的时间稍微长了一点。在等式 2a 中，我们现在首先通过将邻接矩阵<strong class="kh ir"> <em class="mz"> A </em> </strong>乘以度矩阵<strong class="kh ir"> <em class="mz"> D </em> </strong>的逆矩阵来对其进行变换。这个计算在等式 2b 中变得更加清楚。逆度矩阵是一个<a class="ae lk" href="https://en.wikipedia.org/wiki/Diagonal_matrix" rel="noopener ugc nofollow" target="_blank">对角矩阵</a>，其中沿对角线的值是逆节点度 s.t .位置(<em class="mz"> i，i) </em>的值是第<em class="mz"> i </em>个节点的逆度。因此，我们可以去掉求和符号之一，得到等式 2c。方程 2c 可以进一步简化，得到方程 2d 和 2e。</p><p id="1732" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如等式 2e 所示，我们现在再次对邻接矩阵<strong class="kh ir"><em class="mz"/></strong>中的<em class="mz"> N </em>行中的每一行求和。正如在讨论求和规则时提到的，这对应于对每个第<em class="mz"> i </em>个节点的邻居求和。然而，等式 2e 中的加权和中的权重现在被保证用第<em class="mz"> i </em>个节点的度求和为 1。因此，等式 2e 对应于第<em class="mz"> i </em>个节点的邻居的特征表示的平均值。</p><p id="9b96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">求和规则仅依赖于由邻接矩阵<strong class="kh ir"><em class="mz"/></strong>定义的邻域，而平均值规则也依赖于节点度数。</p><p id="3300" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">光谱规则我们现在有了一个有用的框架来分析光谱规则。让我们看看它会把我们带到哪里！</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nu"><img src="../Images/3a6f8d15902007928dc327493b2b90e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2qWQawkWlpnQziHPzKNtfA@2x.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">The Spectral Rule as a Weighted Sum</figcaption></figure><p id="93cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与均值规则一样，我们使用度矩阵 d 来变换邻接矩阵 A。然而，如等式 3a 所示，我们将度矩阵提升到-0.5 的幂，并将其乘以<strong class="kh ir"> <em class="mz"> A </em> </strong>的每一侧。该操作可以分解为等式 3b 所示。再次回忆，度矩阵(及其幂)是对角。因此，我们可以进一步简化方程 3b，直到得到方程 3d 中的表达式。</p><p id="f02c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">等式 3e 显示了一些非常有趣的东西。当计算第 I 个节点的聚集特征表示时，我们不仅考虑第 I 个节点的度，还考虑第 j 个节点的度。</p><p id="22c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与平均值规则类似，光谱规则对聚合 s.t .进行归一化。聚合要素制图表达与输入要素保持大致相同的比例。但是，谱规则在加权和中对低度数邻居的权重较高，对高度数邻居的权重较低。当低度相邻比高度相邻提供更多有用信息时，这可能是有用的。</p><h1 id="6009" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">基于 GCNs 的半监督分类</h1><p id="f125" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">除了谱规则，Kipf 和 Welling 还演示了 GCNs 如何用于半监督分类[1]。在<a class="ae lk" href="https://en.wikipedia.org/wiki/Semi-supervised_learning" rel="noopener ugc nofollow" target="_blank">半监督学习</a>中，我们希望利用有标签和无标签的例子。到目前为止，我们已经隐含地假设整个图是可用的，也就是说，我们处于<a class="ae lk" href="https://en.wikipedia.org/wiki/Transduction_(machine_learning)" rel="noopener ugc nofollow" target="_blank">转导</a>设置中。换句话说，我们知道所有的节点，但不知道所有的节点标签。</p><p id="8102" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们看到的所有规则中，我们在结点邻域上进行聚合，因此共享相邻结点的结点往往具有相似的要素表示。如果图表现出<a class="ae lk" href="https://en.wikipedia.org/wiki/Homophily" rel="noopener ugc nofollow" target="_blank">同质性</a>，即连接的节点倾向于相似(例如具有相同的标签)，则该属性非常有用。同向性存在于许多现实网络中，尤其是社交网络表现出强烈的同向性。</p><p id="55a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们在<a class="ae lk" rel="noopener" target="_blank" href="/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780">上一篇</a>中看到的，即使是<em class="mz">一个随机初始化的 GCN </em>仅仅通过使用图结构就可以实现同形图中节点的特征表示之间的良好分离。我们可以通过在标记的节点上训练 GCN 来更进一步，通过更新在所有节点上共享的权重矩阵来有效地将节点标记信息传播到未标记的节点。这可以按如下方式完成[1]:</p><ol class=""><li id="3bd9" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">通过 GCN 执行前向传播。</li><li id="8b84" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">在 GCN 的最后一层按行应用 sigmoid 函数。</li><li id="6d5d" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">计算已知节点标签的交叉熵损失。</li><li id="26b4" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">反向传播损失并更新每层中的权重矩阵 W。</li></ol><h1 id="9596" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">扎卡里空手道俱乐部中的社区预测</h1><p id="e87d" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">让我们看看谱规则如何使用半监督学习将节点标签信息传播到未标签节点。正如在<a class="ae lk" rel="noopener" target="_blank" href="/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780">上一篇</a>中一样，我们将以扎卡里的空手道俱乐部为例。</p><p id="7540" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mz">如果您想继续学习，您可以在 Jupyter 笔记本上找到数据集，其中包含训练和评估 GCN </em> <a class="ae lk" href="https://github.com/TobiasSkovgaardJepsen/posts/tree/master/HowToDoDeepLearningOnGraphsWithGraphConvolutionalNetworks/Part2_SemiSupervisedLearningWithSpectralGraphConvolutions" rel="noopener ugc nofollow" target="_blank"> <em class="mz">的代码，请点击这里</em> </a> <em class="mz">。</em></p><h2 id="d4b8" class="nc md iq bd me nd ne dn mi nf ng dp mm ko nh ni mo ks nj nk mq kw nl nm ms nn bi translated">扎卡里空手道俱乐部</h2><p id="9a6d" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">简而言之，<a class="ae lk" href="https://en.wikipedia.org/wiki/Zachary%27s_karate_club" rel="noopener ugc nofollow" target="_blank"> Zachary 的空手道俱乐部</a>是一个小型社交网络，在这里，空手道俱乐部的管理员和教练之间发生了冲突。任务是预测空手道俱乐部的每个成员选择冲突的哪一方。网络的图示如下所示。每个节点代表一个空手道俱乐部的成员，成员之间的链接表明他们在俱乐部之外进行互动。管理员和讲师分别标有 A 和 I。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/912a309708b9a3c73201bfa40c7bafb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*OBbxUmrC0Jg7G4ugg0lF8Q.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Zachary’s Karate Club</figcaption></figure><h2 id="1d9c" class="nc md iq bd me nd ne dn mi nf ng dp mm ko nh ni mo ks nj nk mq kw nl nm ms nn bi translated">MXNet 中的谱图卷积</h2><p id="3aad" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我在<a class="ae lk" href="https://mxnet.apache.org/" rel="noopener ugc nofollow" target="_blank"> MXNet </a>中实现了谱法则，这是一个易用且<a class="ae lk" href="https://medium.com/@julsimon/keras-shoot-out-tensorflow-vs-mxnet-51ae2b30a9c0" rel="noopener">高效的</a>深度学习框架。实现如下:</p><pre class="lr ls lt lu gt nv nw nx ny aw nz bi"><span id="0005" class="nc md iq nw b gy oa ob l oc od">class SpectralRule(HybridBlock):<br/>    def __init__(self,<br/>                 A, in_units, out_units,<br/>                 activation, **kwargs):<br/>        super().__init__(**kwargs)</span><span id="0582" class="nc md iq nw b gy oe ob l oc od">        I = nd.eye(*A.shape)<br/>        A_hat = A.copy() + I</span><span id="1489" class="nc md iq nw b gy oe ob l oc od">        D = nd.sum(A_hat, axis=0)<br/>        D_inv = D**-0.5<br/>        D_inv = nd.diag(D_inv)</span><span id="4c33" class="nc md iq nw b gy oe ob l oc od">        A_hat = D_inv * A_hat * D_inv<br/>        <br/>        self.in_units, self.out_units = in_units, out_units<br/>        <br/>        with self.name_scope():<br/>            self.A_hat = self.params.get_constant('A_hat', A_hat)<br/>            self.W = self.params.get(<br/>                'W', shape=(self.in_units, self.out_units)<br/>            )<br/>            if activation == 'ident':<br/>                self.activation = lambda X: X<br/>            else:<br/>                self.activation = Activation(activation)</span><span id="305b" class="nc md iq nw b gy oe ob l oc od">    def hybrid_forward(self, F, X, A_hat, W):<br/>        aggregate = F.dot(A_hat, X)<br/>        propagate = self.activation(<br/>            F.dot(aggregate, W))<br/>        return propagate</span></pre><p id="162e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe of og oh nw b">__init__</code>将邻接矩阵<code class="fe of og oh nw b">A</code>以及来自图卷积层的每个节点的特征表示的输入和输出维度作为输入；分别为<code class="fe of og oh nw b">in_units</code>和<code class="fe of og oh nw b">out_units</code>。通过与单位矩阵<code class="fe of og oh nw b">I</code>相加，将自循环添加到邻接矩阵<code class="fe of og oh nw b">A</code>中，计算度矩阵<code class="fe of og oh nw b">D</code>，并将邻接矩阵<code class="fe of og oh nw b">A</code>转换为谱规则指定的<code class="fe of og oh nw b">A_hat</code>。这种变换不是严格必需的，但是在计算上更有效，因为否则变换将在层的每次向前传递期间执行。</p><p id="5bcc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，在<code class="fe of og oh nw b">__init__</code>的<code class="fe of og oh nw b">with</code>子句中，我们存储了两个模型参数— <code class="fe of og oh nw b">A_hat</code>存储为常量，权重矩阵<code class="fe of og oh nw b">W</code>存储为可训练参数。</p><p id="8a06" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">奇迹就发生在这里。在前向传递中，我们使用以下输入执行该方法:<code class="fe of og oh nw b">X</code>，前一层的输出，以及我们在构造函数<code class="fe of og oh nw b">__init__</code>中定义的参数<code class="fe of og oh nw b">A_hat</code>和<code class="fe of og oh nw b">W</code>。</p><h2 id="e542" class="nc md iq bd me nd ne dn mi nf ng dp mm ko nh ni mo ks nj nk mq kw nl nm ms nn bi translated">构建图形卷积网络</h2><p id="b060" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">现在我们有了光谱规则的实现，我们可以将这样的层堆叠在彼此之上。我们使用类似于上一篇文章中的两层架构，其中第一个隐藏层有 4 个单元，第二个隐藏层有 2 个单元。这种体系结构可以很容易地将产生的二维嵌入可视化。它与前一篇文章中的架构有三点不同:</p><ul class=""><li id="ed56" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la na lh li lj bi translated">我们使用光谱法则而不是平均值法则。</li><li id="ef98" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la na lh li lj bi translated">我们使用不同的激活函数:在第一层中使用 tanh 激活函数，因为否则死亡神经元的概率会非常高，并且第二层使用同一性函数，因为我们使用最后一层来分类节点。</li></ul><p id="d9e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们在 GCN 之上添加一个逻辑回归层用于节点分类。</p><p id="a0e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述架构的 Python 实现如下。</p><pre class="lr ls lt lu gt nv nw nx ny aw nz bi"><span id="4f0a" class="nc md iq nw b gy oa ob l oc od">def build_model(A, X):<br/>    model = HybridSequential()</span><span id="a747" class="nc md iq nw b gy oe ob l oc od">    with model.name_scope():<br/>        features = build_features(A, X)<br/>        model.add(features)</span><span id="fafb" class="nc md iq nw b gy oe ob l oc od">        classifier = LogisticRegressor()<br/>        model.add(classifier)</span><span id="094a" class="nc md iq nw b gy oe ob l oc od">        model.initialize(Uniform(1))</span><span id="a9b7" class="nc md iq nw b gy oe ob l oc od">    return model, features</span></pre><p id="42fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我已经将包含图卷积层的网络的特征学习部分分成了一个<code class="fe of og oh nw b">features</code>组件，将分类部分分成了<code class="fe of og oh nw b">classifier</code>组件。独立的<code class="fe of og oh nw b">features</code>组件使得稍后可视化这些层的激活更加容易。<code class="fe of og oh nw b">LogisticRegressor</code>作为分类层，通过对最后一个图形卷积层提供的每个节点的特征求和并对该和应用<a class="ae lk" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid 函数</a>来执行<a class="ae lk" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>。</p><p id="2bf7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mz">你可以在</em> <a class="ae lk" href="https://github.com/TobiasSkovgaardJepsen/posts/blob/master/HowToDoDeepLearningOnGraphsWithGraphConvolutionalNetworks/Part2_SemiSupervisedLearningWithSpectralGraphConvolutions/notebook.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="mz">中找到构造</em> <code class="fe of og oh nw b"><em class="mz">features</em></code> <em class="mz">组件的代码和</em> <code class="fe of og oh nw b"><em class="mz">LogisticRegressor</em></code> <em class="mz">组件的代码。</em></a></p><h2 id="ff3a" class="nc md iq bd me nd ne dn mi nf ng dp mm ko nh ni mo ks nj nk mq kw nl nm ms nn bi translated">训练 GCN</h2><p id="2869" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">用于训练 GCN 模型的代码如下所示。简而言之，我初始化一个二元交叉熵损失函数<code class="fe of og oh nw b">cross_entropy</code>和一个 SGD 优化器<code class="fe of og oh nw b">trainer</code>来学习网络参数。然后，该模型被训练特定数量的时期，其中为每个训练示例计算<code class="fe of og oh nw b">loss</code>，并且使用<code class="fe of og oh nw b">loss.backward()</code>反向传播误差。然后调用<code class="fe of og oh nw b">trainer.step</code>来更新模型参数。在每个时期之后，由 GCN 层构建的特征表示被存储在<code class="fe of og oh nw b">feature_representations</code>列表中，我们将很快对此进行检查。</p><pre class="lr ls lt lu gt nv nw nx ny aw nz bi"><span id="2616" class="nc md iq nw b gy oa ob l oc od">def train(model, features, X, X_train, y_train, epochs):<br/>    cross_entropy = SigmoidBinaryCrossEntropyLoss(from_sigmoid=True)<br/>    trainer = Trainer(<br/>        model.collect_params(), 'sgd',<br/>        {'learning_rate': 0.001, 'momentum': 1})</span><span id="bf5d" class="nc md iq nw b gy oe ob l oc od">    feature_representations = [features(X).asnumpy()]</span><span id="10ca" class="nc md iq nw b gy oe ob l oc od">    for e in range(1, epochs + 1):<br/>        for i, x in enumerate(X_train):<br/>            y = array(y_train)[i]<br/>            with autograd.record():<br/>                pred = model(X)[x] # Get prediction for sample x<br/>                loss = cross_entropy(pred, y)<br/>            loss.backward()<br/>            trainer.step(1)</span><span id="5105" class="nc md iq nw b gy oe ob l oc od">        feature_representations.append(features(X).asnumpy())</span><span id="150b" class="nc md iq nw b gy oe ob l oc od">    return feature_representations</span></pre><p id="435f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">至关重要的是，只有教师和管理员的标签被标记，网络中的其余节点是已知的，但未被标记！GCN 可以在图卷积期间找到标记和未标记节点的表示，并且可以在训练期间利用这两种信息源来执行半监督学习。</p><p id="7d7d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具体来说，半监督学习发生在 GCN 中，因为它通过聚集节点的标记和未标记邻居来产生节点的潜在特征表示。在训练期间，我们然后反向传播监督的二进制交叉熵损失，以更新所有节点共享的权重。然而，这种损失取决于标记节点的潜在特征表示，而潜在特征表示又取决于标记节点和未标记节点。因此，学习变成半监督的。</p><h2 id="e691" class="nc md iq bd me nd ne dn mi nf ng dp mm ko nh ni mo ks nj nk mq kw nl nm ms nn bi translated">可视化特征</h2><p id="4296" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">如上所述，存储每个时期的特征表示，这允许我们看到特征表示在训练期间如何变化。在下文中，我考虑两种输入特征表示。</p><p id="4af7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">表示 1 <br/> </strong>在第一种表示中，我们简单地用稀疏的 34 × 34 <a class="ae lk" href="https://en.wikipedia.org/wiki/Identity_matrix" rel="noopener ugc nofollow" target="_blank">单位矩阵</a>，<em class="mz"> I </em>作为特征矩阵<code class="fe of og oh nw b">X</code>，即<em class="mz">对图</em>中每个节点的一键编码。这种表示的优点在于，它可以用于任何图中，但是导致网络中每个节点的输入参数，这需要大量的存储器和计算能力用于大型网络上的训练，并且可能导致过拟合。谢天谢地，空手道俱乐部网络相当小。使用这种表示对网络进行 5000 个时期的训练。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/7d4d76eaf3c153be4addcf2c10bfa33e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*aT5fAkPaGrrP94ft7l6n9A.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Classification Errors in the Karate Club using Representation 1</figcaption></figure><p id="beed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过对网络中的所有节点进行集体分类，我们得到了网络中的误差分布，如上图所示。这里，黑色表示分类错误。尽管有将近一半(41%)的节点被错误分类，但是与管理员或教师(但不是两者)都有密切联系的节点。)倾向于正确分类。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/3c0b61c0e0e83b8a88fe6bc2db9c5a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*LASPbLe96wER6rYmO-RyWw.gif"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Changes in Feature Representation during Training using Representation 1</figcaption></figure><p id="f8fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在左侧，我已经说明了特征表示在训练过程中是如何变化的。节点最初紧密地聚集在一起，但是随着训练的进行，教师和管理员被拉开，拖着一些节点。</p><p id="f2c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管管理员和教师被给予完全不同的表示，但是他们所拖动的节点不一定属于他们的社区。这是因为图卷积在特征空间中将共享邻居的节点紧密地嵌入在一起，但是共享邻居的两个节点可能不平等地连接到管理员和教师。特别地，使用单位矩阵作为特征矩阵导致每个节点的高度局部表示，即，属于图的相同区域的节点可能紧密地嵌入在一起。这使得网络难以以归纳的方式在遥远的区域之间共享公共知识。</p><p id="86ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">表示 2 </strong> <br/>我们将通过添加两个不特定于网络的任何节点或区域的特征来改进表示 1，这两个特征测量与管理员和教师的连通性。为此，我们计算从网络中的每个节点到管理员和教师的最短路径距离，并将这两个特征连接到之前的表示。</p><p id="c74a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">On 可能会认为这是一种欺骗，因为我们注入了关于图中每个节点位置的全局信息；应该(理想地)由<code class="fe of og oh nw b">features</code>组件中的图形卷积层捕获的信息。然而，图卷积层总是具有局部视角，并且捕获这种信息的能力有限。尽管如此，它仍然是理解 gcn 的有用工具。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/06a4d0963a78615bcd3ba353c04fa1cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*NT-YBzbiTjGsC6jkmIIDCg.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Classification Errors in the Karate Club using Representation 1</figcaption></figure><p id="50fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如前所述，我们对网络中的所有节点进行分类，并绘制网络中的误差分布图，如上图所示。这一次，只有四个节点被错误分类；相对于表示 1 的显著改进！根据对特征矩阵的更仔细的检查，这些节点或者与教师和管理员等距(在最短路径意义上),或者更靠近管理员但属于教师团体。使用表示 2 对 GCN 进行 250 个时期的训练。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/f077e55af46ecad0e7090819ad77be8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*hLSeJjZHD3k_pcgHDQbC9g.gif"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Changes in Feature Representation during Training using Representation 2</figcaption></figure><p id="5271" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如左图所示，节点最初还是非常紧密地聚集在一起，但是在训练开始之前就已经分成了几个社区！随着训练的进行，社区之间的距离增加了。</p><h1 id="865b" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">下一步是什么？</h1><p id="c648" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">在这篇文章中，我已经深入解释了 gcn 中的聚合是如何执行的，并以均值、求和以及谱规则为例，展示了如何将其表示为加权和。我真诚的希望你会发现这个框架对于考虑在你自己的图卷积网络中聚合时你可能想要的权重是有用的。</p><p id="db1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我还展示了如何在 MXNet 中实现和训练一个 GCN，使用谱图卷积在图上执行半监督分类，并以 Zachary 的空手道俱乐部作为一个简单的示例网络。我们看到了仅使用两个带标签的节点，GCN 仍有可能在表示空间中实现两个网络社区之间的高度分离。</p><p id="e038" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然还有很多关于图卷积网络的知识需要学习，我希望将来有时间与你分享，但这是(目前)这个系列的最后一篇文章。如果你对进一步阅读感兴趣，我想以下列我认为相当有趣的论文作为结束:</p><ol class=""><li id="b11f" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated"><a class="ae lk" href="https://arxiv.org/abs/1706.02216" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">大型图上的归纳表示学习</strong> </a> <br/>在这篇论文中，Hamilton 等人提出了几个新的聚合函数，例如，使用 max/mean pooling 或多层感知器。此外，他们还提出了一种简单的方法对 GCNs 进行小批量训练，大大提高了训练速度。</li><li id="fac7" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated"><a class="ae lk" href="https://arxiv.org/pdf/1801.10247.pdf" rel="noopener ugc nofollow" target="_blank">陈等人。al 提出了他们的 FastGCN 方法，该方法通过独立地执行图卷积层的批量训练来解决这个缺点。</a></li><li id="01f1" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated"><a class="ae lk" href="https://arxiv.org/pdf/1802.08888.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> N-GCN:用于半监督节点分类的多尺度图卷积</strong> </a> <br/>其中 FastGCN 解决了训练递归图卷积网络的问题，N-GCN 挑战了 GCNs 完全需要递归的前提！相反，Abu-El-Haija 等人提出了一种具有多个(N 个)gcn 的扁平架构，其输出被连接在一起。每个 GCN 在不同的距离捕获邻域(基于随机行走语义)，从而避免递归聚集。感谢 Binny Mathew 让我注意到这一点。</li></ol></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><p id="f736" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">喜欢你读的书吗？考虑在 <a class="ae lk" href="https://twitter.com/TobiasSJepsen" rel="noopener ugc nofollow" target="_blank"> <em class="mz"> Twitter </em> </a> <em class="mz">上关注我，在那里，除了我自己的帖子之外，我还分享与数据科学和机器学习的实践、理论和伦理相关的论文、视频和文章。</em></p><p id="524c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mz">如需专业咨询，请在</em><a class="ae lk" href="https://www.linkedin.com/in/tobias-skovgaard-jepsen/" rel="noopener ugc nofollow" target="_blank"><em class="mz">LinkedIn</em></a><em class="mz">上联系我，或在</em><a class="ae lk" href="https://twitter.com/TobiasSJepsen" rel="noopener ugc nofollow" target="_blank"><em class="mz">Twitter</em></a><em class="mz">上直接留言。</em></p><h1 id="c798" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">参考</h1><p id="d8e3" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">[1] <a class="ae lk" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">论文</a>名为<em class="mz">带有图卷积网络的半监督分类</em>作者 Thomas Kipf 和 Max Welling。</p></div></div>    
</body>
</html>