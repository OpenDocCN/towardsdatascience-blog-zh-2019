<html>
<head>
<title>Eligibility Traces in Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习中的资格痕迹</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/eligibility-traces-in-reinforcement-learning-a6b458c019d6?source=collection_archive---------5-----------------------#2019-06-04">https://towardsdatascience.com/eligibility-traces-in-reinforcement-learning-a6b458c019d6?source=collection_archive---------5-----------------------#2019-06-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="adfd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">有时候向后看并没有那么糟糕</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5b68c23f03f5ea0267ffe172275e5d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uAS9N38_ZA_y8fvR"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@lampe_91?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Didier Provost</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="8a5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新</strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae ky" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="2db7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是资格痕迹？</h1><p id="971b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">简而言之，合格轨迹是一种数学技巧，可以提高强化学习中<a class="ae ky" rel="noopener" target="_blank" href="/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce">时间差</a>方法的性能。</p><p id="d856" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">资格跟踪的好处如下:</p><ul class=""><li id="b11a" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">提供一种以在线方式(不要等待剧集结束)和在没有剧集的问题上实现蒙特卡罗的方法。</li><li id="ce0b" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">提供一种使用短期记忆向量的算法机制。</li><li id="be7f" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">通过<strong class="lb iu">存储单个向量</strong> <strong class="lb iu">存储器</strong>而不是特征向量列表来提高计算效率。</li><li id="2cc2" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">学习是不断进行的，而不是在一集结束时等待结果。</li></ul><h1 id="28b6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">前瞻性的观点</h1><p id="1c5e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">请记住，在<a class="ae ky" rel="noopener" target="_blank" href="/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce">中，时间差</a>和<a class="ae ky" href="https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511" rel="noopener">蒙特卡罗</a>方法基于未来奖励更新状态。这可以通过直接向前看一步或者等待一集结束来实现。</p><p id="2e9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法被称为前瞻。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/cd5079e3a38deb0c2351dd37f2e6ae17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4v5glg1xdjI18oxzW_e4pA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">In Forward View we look ahead n steps for future rewards</figcaption></figure><p id="09f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 TD(0)中，我们向前看一步，而在蒙特卡洛中，我们向前看，直到这一集结束，我们收集贴现结果。然而，有一个中间地带，我们向前看 n 步。</p><h1 id="5119" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">n 步前瞻视图</h1><p id="1252" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">正如上一节所解释的，展望未来可能会有所不同，从向前一步到一集结束，就像蒙特卡洛的情况一样。因此，n 步是某种中间地带。<br/>请记住，在蒙特卡洛中，我们执行剧集，获取它们的回报 G <em class="nh"> i </em>并对这些回报进行平均，以计算状态值。<br/>请注意，每集的长度(步数)可能会因集而异。它不是常数！</p><p id="14c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，我们将对 n 步前瞻进行同样的操作。就像在蒙特卡洛一样，每集的步数不一定相同。<br/> <em class="nh"> NB。在本节中，我们不再将一个情节称为必须通过达到终止状态而终止的多个步骤，而是简单地称为多个步骤(终止状态不再是一个要求)。</em></p><p id="3173" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，让我们定义所有这些迭代的平均回报，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e267b6989b8ebbd2a9ead574a78a4a2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*eWM47GD0qVcQk9fMiWFPCw.png"/></div></figure><p id="64c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中 G( 𝛌，t)是所有收益 G(t，t+n)的加权平均值，g(t，t+n)是单个剧集的收益，其中每集从 t 开始，到 t+n 结束，n 从 1 到无穷大。𝛌是一个值在[0，1]之间的权重。<br/>在所有加权平均中，权重之和必须为 1，这是因为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/720d2e9ab99b73415a9f2c21510fd885.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*e-YA75COVlMY4coBqNj1WA.png"/></div></figure><p id="e395" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重要备注:很容易注意到，对于 n 较大的剧集，𝛌对 n 次方的贡献变小，G(t+n)的贡献也会很小。<br/>但是嘿！我们已经知道，通过使用收益定义中的贴现因子γ(ɣ):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/aa17574b1c826d10b1b849e81f76907f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_BIU_nEnb8i6GbwKHb8cOQ.png"/></div></div></figure><p id="4918" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是请记住，这是一个不同的问题，例如在蒙特卡洛，V(s)是从已经播放的剧集中计算出的所有回报 Gi 的<strong class="lb iu">平均值</strong>。所有的地理标志对 V(s)的贡献是相等的，即使奖励会根据它们与州 s 的距离而打折扣。</p><p id="0e81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，问题是不同的，所有 Gi 对 V(s)的贡献并不相同，但是它们是加权的，并且每个权重随着每集内的步数而变小。</p><p id="c4ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图显示了每集如何根据长度进行加权。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/f5bd71f9658f22a531c948548bda4a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vj0AFM6K_Y8km0UL-npKMA.png"/></div></div></figure><p id="8b1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这张照片显示了重量如何随着时间(或 n 步)的增加而变小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/710d16b37681b86009155c7a28952219.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K_wMnWttvex42ENhBbYNBw.png"/></div></div></figure><p id="94c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，如果一个情节在 3 步之后终止，与其回报相关联的权重远远大于在 T 步处终止的情节(其中 T 远大于 3)。<br/>同样重要的是注意到重量呈指数下降。</p><p id="550d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们重写当剧集在时间 T 终止时的返回。<br/>注意，时间步长 T 之后的返回总是 Gt(这是时间步长 T 时的返回),这仅仅是因为不再有状态，并且最后看到的返回是在 T。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/d95420cb679a32e9291b6e3f9f19d4e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kDQLp3ncTDpamdy_-XM8ZA.png"/></div></div></figure><p id="ca5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当𝛌为 0 时，Gt 的表达式显示的是 TD(0)的公式(注意这个上下文中的 0 到幂零按照惯例是 1，更多信息见这个<a class="ae ky" href="https://en.wikipedia.org/wiki/Zero_to_the_power_of_zero" rel="noopener ugc nofollow" target="_blank">维基百科文章</a>)，当𝛌为 1 时，Gt 就变成了蒙特卡洛的公式。</p><h1 id="411b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">问题还没有解决！</h1><p id="a51c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">所有这些细节，但我们仍然没有解决问题…</p><p id="1a88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为每个状态的更新依赖于当前不可用的后来的事件或奖励，所以前瞻视图实现起来有些复杂。</p><p id="d12b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但到目前为止，我们一直在这么做！<br/>我们正在向前看 n 步… <br/>然而，这将通过采用一种新的方法来改变:向后看。</p><h1 id="5e49" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">后视 TD( 𝛌)</h1><p id="8c87" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">假设一个代理人在一个环境中随机行走，发现了一个宝藏。然后他停下来向后看，试图知道是什么让他找到了这个宝藏？自然，靠近宝藏的台阶比远离宝藏的台阶更有价值。因此，较近的位置比较远的位置更有价值，因此它们被赋予更大的价值</p><p id="d462" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是如何实现的，是通过一个叫做合格痕迹的向量<strong class="lb iu"><em class="nh">【E】</em></strong>。<br/>具体地，合格痕迹是状态<strong class="lb iu"> <em class="nh"> E(s) </em> </strong>或状态动作<strong class="lb iu"> <em class="nh"> E(s，a) </em> </strong>的函数，并保存 V(s)的衰减值。</p><p id="1469" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，我们如何从向前看过渡到向后看，资格跟踪在其中的作用是什么？</p><p id="aff7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还记得我们说过的前视，每个情节对当前状态的贡献随着情节中的步数(n)呈指数衰减(𝛌的 n 次方)。<br/>使用相同的逻辑，当我们处于状态 s 时，我们不是向前看，看到一集的衰减回报(Gt)向我们走来，而是简单地使用我们拥有的值，并使用相同的衰减机制将其向后抛出。</p><p id="62df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在 TD(0)中，我们将 TD 误差定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/cd7b1f7d10b7020e51bb1bb80f28b7de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NDgA1pxaiUSfGdjJVro3Rg.png"/></div></div></figure><p id="0189" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该误差将向后传播，但是以衰减的方式。<br/>类似于随着距离渐行渐远的声音。<br/>我们实现这一点的方法是将𝜹乘以每个州的资格跟踪。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/c77df4c59c6b536e367258d4c9c9460e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KSaKJjF8NP60iP6kRNNdYg.png"/></div></div></figure><p id="b902" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中 Et 更新如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/a88ba14e762131f17fadd6dde14acfe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NwEaZg8mIam9uSQ6V5dnMg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/825291182dd89b21c601ca15b5186551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QHbSid7K3SgUBGb55v9nmA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Backward View propagates the error δ to previous states</figcaption></figure><p id="caf9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">符号 1(St = s)意味着，当我们处于状态 s 时，我们分配完整的值，当它向后传播时，它以指数形式衰减。</p><p id="09b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于所有状态，合格的跟踪更新从 E(s) = 0 开始，然后当我们经过每个状态(<strong class="lb iu">由于执行动作</strong>)时，我们递增 E(s)以增加状态的值，然后对于所有的，我们通过ɣ𝛌 (E(s) = ɣ𝛌 E(s)衰减 E(s)</p><p id="443a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与 n 步前向视图相比，合格轨迹的主要优点是只需要一个轨迹向量，而不是存储最后 n 个特征向量。</p><h1 id="17da" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">使用资格跟踪的算法</h1><p id="aaf5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">以下是几个使用资格跟踪的算法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/11f8ebd7cffac14e69f9815ddedee209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oUHFsUenCGaaYGpt4UVe6g.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/16fd7d5be8480b8b5c21538327c2a22e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uNd9u_L8JNKCD3sGnSSWaA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/15064b10e62c96144207f1453d18ec50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NrfbzndokXpK2rcQu8VOLA.png"/></div></div></figure><h1 id="956a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="c996" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">资格跟踪是一种在时间差“目标”和蒙特卡罗“回报”之间进行加权的方式。这意味着我们不使用一步 TD 目标，而是使用 TD(λ)目标。<br/>换句话说，它微调目标以获得更好的学习性能。</p><h2 id="36f1" class="nu lw it bd lx nv nw dn mb nx ny dp mf li nz oa mh lm ob oc mj lq od oe ml of bi translated">资源</h2><p id="fcff" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">《强化学习:导论》，萨顿&amp;巴尔托著</p></div></div>    
</body>
</html>