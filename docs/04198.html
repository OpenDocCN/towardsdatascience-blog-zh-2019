<html>
<head>
<title>Uncovering what neural nets “see” with FlashTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用闪光灯揭开神经网络“看到”了什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-visualisation-in-pytorch-saliency-maps-a3f99d08f78a?source=collection_archive---------17-----------------------#2019-07-01">https://towardsdatascience.com/feature-visualisation-in-pytorch-saliency-maps-a3f99d08f78a?source=collection_archive---------17-----------------------#2019-07-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7879" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">py torch 中用于神经网络的开源特征可视化工具包</strong></h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a31390556f28a46cd5e8cbba427cd02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQi30PyEa2fvHA_lPWCf_A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Visualisation of what AlexNet “sees” in these images of birds, using FlashTorch. <a class="ae kv" href="https://github.com/MisaOgura/flashtorch" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="fd2e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">设置场景</h1><p id="1e6f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">几周前，作为伦敦科技周的一部分，我在由 AnitaB.org<a class="ae kv" href="https://anitab.org/" rel="noopener ugc nofollow" target="_blank">组织的伦敦<strong class="lq ir">hopper x1</strong>T7]上做了一次演讲。滑梯可在</a><a class="ae kv" href="https://misaogura.github.io/flashtorch/presentations/Hopperx1London/#/" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="989c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我在演讲后收到了如此积极的反馈，所以我决定写一个稍微长一点的演讲版本，以便以前向世界介绍【T0:)</p><p id="02d8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该软件包可以通过<code class="fe mp mq mr ms b">pip</code>进行安装。查看源代码的<a class="ae kv" href="https://github.com/MisaOgura/flashtorch" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir"> GitHub repo </strong> </a>。你也可以在 Google Colab 的笔记本上使用它，而不需要安装任何东西！</p><p id="86d8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是首先，我将简要回顾一下特征可视化的历史，给你一个关于<em class="mt">什么&amp;为什么</em>的更好的背景。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="ab43" class="kw kx iq bd ky kz nb lb lc ld nc lf lg jw nd jx li jz ne ka lk kc nf kd lm ln bi translated">特征可视化介绍</h1><p id="c90a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><a class="ae kv" href="https://distill.pub/2017/feature-visualization/" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">特征可视化</strong> </a>是一个活跃的研究领域，旨在通过探索我们可以“通过他们的眼睛”看东西的方式来理解神经网络<em class="mt">如何感知</em>图像。它的出现和发展是为了响应让神经网络更容易被人类理解的日益增长的愿望。</p><p id="efaa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最早的工作包括分析神经网络在输入图像中关注什么。例如，<a class="ae kv" href="https://arxiv.org/abs/1312.6034" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">特定于图像的类别显著性图</strong> </a>通过经由反向传播计算类别输出相对于输入图像<em class="mt">的梯度，使输入图像内对相应输出贡献最大</em>的<em class="mt">区域可视化(稍后将在帖子中详细介绍显著性图)。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/73e41090fb2b4b14c2571d02d87ac941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GCCN7vaE0dZoaExIKO1j-w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Earliest work on image-specific class saliency maps. <a class="ae kv" href="https://arxiv.org/abs/1312.6034" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="29fa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">特征可视化的另一个技巧是<a class="ae kv" href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html" rel="noopener ugc nofollow" target="_blank"><strong class="lq ir"/></a>。这允许我们迭代地更新输入图像(最初由一些随机噪声产生)以生成最大限度地激活目标神经元的<em class="mt">图像。它提供了一些关于<strong class="lq ir">个体神经元</strong>如何对输入做出反应的直觉。这是所谓的<a class="ae kv" href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html" rel="noopener ugc nofollow" target="_blank">深度梦</a>背后的技术，由谷歌推广。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/cd5c41d39c8e5136947b3ac1cf6b080b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rNKcM_c1Y8lziLi9OLYZxA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Deep Dream: what does the network sees in the sky? <a class="ae kv" href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="db38" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是一个巨大的进步，但有缺点，因为它没有提供足够的洞察力来了解整个网络是如何运行的，因为神经元不是孤立运行的。这导致了可视化神经元之间相互作用的努力。Olah 等人通过两个神经元之间的相加或插值，演示了<a class="ae kv" href="https://stackoverflow.com/a/47269896" rel="noopener ugc nofollow" target="_blank">激活空间</a>的<a class="ae kv" href="https://distill.pub/2017/feature-visualization/#interaction" rel="noopener ugc nofollow" target="_blank"> <em class="mt">算术性质</em> </a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/30fc47a866ce63151f8cd68a119994b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pKGTms_23t7BHOEI1tVTPw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Neuron arithmetic. <a class="ae kv" href="https://distill.pub/2017/feature-visualization/#interaction" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="d11f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后奥拉在阿尔。通过分析给定特定输入时每个中子在隐藏层中发射的数量，进一步定义了更有意义的可视化单位。<a class="ae kv" href="https://distill.pub/2018/building-blocks/#CubeNatural" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">将一组被强烈激活在一起的神经元可视化</strong> </a>揭示出似乎有一组神经元负责捕捉诸如耷拉的耳朵、毛茸茸的腿和草等概念。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/13e92361a5f40ca9e059413440164070.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2k_alWQdMaBB-WPn2DQM1g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">A group of neurons detecting floppy ears. <a class="ae kv" href="https://distill.pub/2018/building-blocks/#CubeNatural" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="1bdb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该领域内的最新发展之一是<a class="ae kv" href="https://distill.pub/2019/activation-atlas/" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">激活图谱</strong> </a>(卡特等人，2019)。在这项研究中，作者解决了可视化过滤器激活的一个主要弱点，因为它仅给出了网络如何响应<em class="mt">单输入</em>的有限视图。为了更好地了解网络如何感知无数的对象，以及这些对象在网络的世界中如何相互关联，他们设计了一种方法，通过显示神经元的常见组合来创建“通过网络之眼看到的全球地图”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/6dec0cdd796160c94c5e144d0fc01ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QwIstjjHDQ5KLcyAalcTbA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Different ways to visualise the network. <a class="ae kv" href="https://distill.pub/2019/activation-atlas/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="8abc" class="kw kx iq bd ky kz nb lb lc ld nc lf lg jw nd jx li jz ne ka lk kc nf kd lm ln bi translated">手电筒背后的动机</h1><p id="97a2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">当我发现特征可视化的世界时，我立即被它在使神经网络更易解释和说明方面的潜力所吸引。然后我很快意识到，没有工具可以轻松地将这些技术应用于我在 PyTorch 中构建的神经网络。</p><p id="1ffd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以我决定建一个— <code class="fe mp mq mr ms b"><a class="ae kv" href="https://github.com/MisaOgura/flashtorch" rel="noopener ugc nofollow" target="_blank">FlashTorch</a></code>，现在<em class="mt">可以通过</em> <code class="fe mp mq mr ms b"><em class="mt">pip</em></code>安装！我实现的第一个特征可视化技术是<strong class="lq ir">显著图</strong>。</p><p id="a49c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将在下面更详细地看看什么是显著图，以及如何使用<code class="fe mp mq mr ms b">FlashTorch</code>在你的神经网络中实现它们。</p><h1 id="8dda" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">显著图</h1><p id="a21e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><strong class="lq ir">显著性</strong>，在人类视觉感知中，是一种主观品质<em class="mt">使视野中的某些事物突出</em>并抓住我们的注意力。<strong class="lq ir">计算机视觉中的显著图</strong>可以给出图像中<em class="mt">最显著区域</em>的指示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/c502795d4ceca3a5f38804849aeeb24c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ComhghtpwLKD4fpApUzJFA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Examples of saliency maps. <a class="ae kv" href="https://www.mathworks.com/matlabcentral/fileexchange/43558-roi-selection-for-saliency-maps" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="fec1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从卷积神经网络(CNN)创建显著图的方法最早于 2013 年在论文<a class="ae kv" href="https://arxiv.org/abs/1312.6034" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">深入卷积网络内部:可视化图像分类模型和显著图</strong> </a>中提出。作者报告称，通过<strong class="lq ir">计算目标类别</strong>相对于输入图像的梯度，我们可以可视化输入图像内的<em class="mt">区域，这些区域对该类别的预测值有影响</em>。</p><h1 id="d64b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">使用闪光灯的显著性图</h1><p id="c294" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">事不宜迟，让我们使用<code class="fe mp mq mr ms b">FlashTorch</code>并自己可视化显著图！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/d2aad0d0620da432132a63a820a41e41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rgVXXbYBRxnuyhSv2QeTrw.png"/></div></div></figure><p id="fe55" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe mp mq mr ms b">FlashTorch</code>还附带了一些<code class="fe mp mq mr ms b">utils</code>功能，让数据处理变得更加简单。我们将用这张<code class="fe mp mq mr ms b">great grey owl</code>的图片作为例子。</p><p id="061b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，我们将对图像应用一些变换，使它的<em class="mt">形状、类型和值</em>适合作为 CNN 的输入。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/4349b64a42df78c60a317d5157c544ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KWtC9IXdQuzCC5AUvd-doA.png"/></div></div></figure><p id="f0f8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我将使用已经用<code class="fe mp mq mr ms b">ImageNet</code>分类数据集预训练过<em class="mt">的<code class="fe mp mq mr ms b">AlexNet</code>进行可视化。事实上，<code class="fe mp mq mr ms b">FlashTorch</code>支持<em class="mt">所有自带<code class="fe mp mq mr ms b">torchvision</code>的型号</em>，所以我鼓励你也尝试其他型号！</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/44742d39e0be7fadc84f10078234942a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W-eMH9FiB0mWsFFkSj4JFw.png"/></div></div></figure><p id="88db" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe mp mq mr ms b">Backprop</code>类是创建显著图的<em class="mt">核心</em>。</p><p id="81ae" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在实例化时，它接受一个模型<code class="fe mp mq mr ms b">Backprop(model)</code>并且<em class="mt">将自定义挂钩</em>注册到网络中感兴趣的层，这样我们就可以<strong class="lq ir">从计算图</strong>中抓取中间梯度以便可视化。由于<code class="fe mp mq mr ms b">PyTorch</code>的设计方式，这些中间梯度<em class="mt">不会立即</em>提供给我们。<code class="fe mp mq mr ms b">FlashTorch</code>帮你整理一下:)</p><p id="1faf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，在计算梯度之前，我们需要的最后一件事是<strong class="lq ir">目标类索引</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/eec050f01bfda205a742efd9031f12e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ct8sBSsQPecmgi9ox-eH5w.png"/></div></div></figure><p id="bf89" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">概括地说，我们对目标类相对于输入图像的<em class="mt">梯度感兴趣。然而，该模型是用<code class="fe mp mq mr ms b">ImageNet</code>数据集预先训练的，因此其预测被提供为<em class="mt">1000 个类别的概率分布</em>。我们希望从这 1000 个值中找出目标类的值(在我们的例子中是<code class="fe mp mq mr ms b">great grey owl</code>)，以避免不必要的计算，并且只关注输入图像和目标类</em>之间的<em class="mt">关系。</em></p><p id="e82d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为此，我还实现了一个名为<code class="fe mp mq mr ms b">ImageNetIndex</code>的类。如果你<em class="mt">不想下载整个数据集</em>，只想根据类名找出<em class="mt">类索引，这是一个方便的工具。如果你给它一个类名，它会找到对应的类索引<code class="fe mp mq mr ms b">target_class = imagenet['great grey owl']</code>。如果您<em class="mt">确实</em>想要下载数据集，请使用最新版本<code class="fe mp mq mr ms b">torchvision==0.3.0</code>中提供的<code class="fe mp mq mr ms b"><a class="ae kv" href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagenet" rel="noopener ugc nofollow" target="_blank">ImageNet</a></code> <a class="ae kv" href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagenet" rel="noopener ugc nofollow" target="_blank">类</a>。</em></p><p id="c1e3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，我们有了输入图像和目标类索引(<code class="fe mp mq mr ms b">24</code>)，所以我们<strong class="lq ir">准备好计算渐变</strong>！</p><p id="6cb0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这两行是关键:</p><pre class="kg kh ki kj gt nq ms nr ns aw nt bi"><span id="c420" class="nu kx iq ms b gy nv nw l nx ny">gradients = backprop.calculate_gradients(input_, target_class)<br/><br/>max_gradients = backprop.calculate_gradients(input_, target_class, take_max=<strong class="ms ir">True</strong>)</span></pre><p id="1592" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">默认情况下，将为每个颜色通道计算渐变<em class="mt">，因此它的形状将与输入图像相同——在我们的例子中为<code class="fe mp mq mr ms b">(3, 224, 224)</code>。有时，如果我们采用颜色通道<em class="mt">的最大梯度</em>，会更容易看到梯度。我们可以通过将<code class="fe mp mq mr ms b">take_max=True</code>传递给方法调用来实现。渐变的形状将是<code class="fe mp mq mr ms b">(1, 224, 224)</code>。</em></p><p id="747c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，让我们想象一下我们得到了什么！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/12b7318cfff159714d3b296fa359ab1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R6nY2laTjzXLE6da0ji-_Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">From far left: input image, gradients across colour channels, max gradients, an overlay of input image and max gradients</figcaption></figure><p id="6f29" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们可以理解，动物所在区域的<em class="mt">像素</em>对预测值的影响最大。</p><p id="8c60" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但这是一种噪音…信号是传播的，它没有告诉我们多少关于神经网络对猫头鹰的感知。</p><p id="1570" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">有办法改善这一点吗？</p><h1 id="7dab" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">引导返回救援</h1><p id="fe03" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">答案是肯定的！</p><p id="9a87" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在论文<a class="ae kv" href="https://arxiv.org/abs/1412.6806" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">力求简单:全卷积网</strong> </a>中，作者介绍了一种降低梯度计算中噪声的巧妙方法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/d5bf4d0def29fe22c5f499ee7a59f88f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PdtlEkX9nnjKGjq5B16j_Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Guided backpropagation. <a class="ae kv" href="https://arxiv.org/abs/1412.6806" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="19c7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">本质上，在<strong class="lq ir">导向反向传播</strong>中，对目标类的预测值没有<em class="mt">影响或负面影响</em>的神经元被<em class="mt">屏蔽掉</em>并被忽略。通过这样做，我们可以阻止梯度流通过这样的神经元，从而减少噪音。</p><p id="2f82" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">您可以通过将<code class="fe mp mq mr ms b">guided=True</code>传递给<code class="fe mp mq mr ms b">calculate_gradients</code>的方法调用，在<code class="fe mp mq mr ms b">FlashTorch</code>中使用引导式反向传播，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/10fe3fbd2eec575fcbe18bcb37d8b4ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hLXoGqof7HdGKs3aGoGgWg.png"/></div></div></figure><p id="3b1b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们想象引导梯度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/b3400e4f06c9e86a54c306219d7cc6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FnW4jsQxJkv3CQ2Csl8Qrw.png"/></div></div></figure><p id="ebc3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">差别是惊人的！</p><p id="5272" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我们可以<em class="mt">清楚的</em>看到网络在关注一只猫头鹰的<strong class="lq ir">凹陷的眼睛</strong>圆头<em class="mt"> </em>。这些都是“说服”网络将对象归类为<code class="fe mp mq mr ms b">great grey owl</code>的特征。</p><p id="dc5b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是它并不总是聚焦在眼睛或头部…</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/aa3cbee8d5b293b453fb2fe08e2c090f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vj8XtjFhmYzfUT7fp91_FQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/066a6d8101d73f3f24e7ff58856038e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Z1BPATHeSsK_W2FbVi89A.png"/></div></div></figure><p id="e465" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如你所看到的，这个网络已经学会关注那些与我们认为这些鸟最有区别的特征相一致的特征。</p><h1 id="cf01" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">特征可视化的应用</h1><p id="f70e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">有了特征可视化，我们不仅可以更好地理解神经网络对物体的了解，而且我们<em class="mt">可以更好地</em>进行:</p><ul class=""><li id="02dd" class="oc od iq lq b lr mk lu ml lx oe mb of mf og mj oh oi oj ok bi translated">诊断网络出了什么问题以及<em class="mt">为什么</em></li><li id="83d1" class="oc od iq lq b lr ol lu om lx on mb oo mf op mj oh oi oj ok bi translated">发现并纠正算法中的<em class="mt">偏差</em></li><li id="8781" class="oc od iq lq b lr ol lu om lx on mb oo mf op mj oh oi oj ok bi translated">从只看准确性向前迈一步</li><li id="4ef3" class="oc od iq lq b lr ol lu om lx on mb oo mf op mj oh oi oj ok bi translated">理解网络行为的原因</li><li id="739a" class="oc od iq lq b lr ol lu om lx on mb oo mf op mj oh oi oj ok bi translated">阐明神经网络如何学习的机制</li></ul><h1 id="296b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">今天就使用闪光灯吧！</h1><p id="ae9e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">如果你有在 PyTorch 中使用 CNN 的项目，<code class="fe mp mq mr ms b">FlashTorch</code>可以帮助你<em class="mt">使你的项目更具可解释性和可解释性</em>。</p><p id="b4ea" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果用了请告诉我你的想法！我将非常感谢您的建设性意见、反馈和建议🙏</p><p id="2e96" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">谢谢，祝编码愉快！</p></div></div>    
</body>
</html>