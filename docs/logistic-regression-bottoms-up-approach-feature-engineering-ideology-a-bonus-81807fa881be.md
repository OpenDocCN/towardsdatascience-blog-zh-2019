# 逻辑回归:自下而上的方法。

> 原文：<https://towardsdatascience.com/logistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be?source=collection_archive---------32----------------------->

## (特征工程思想——额外收获)

**快速提问？**

我们可以使用 RMSE 或 MSE 作为分类的评估指标吗？如果有，什么时候可以用？如果不是，为什么我们不能使用它？

我最后会透露答案。

**对你有什么好处？**

这篇文章将带你了解逻辑回归的细微差别，并让你熟悉特性工程的思想。我想澄清的第一件事是，逻辑回归的核心是回归，但它是一种分类算法。

**我的告白:**

这是我仅次于决策树的第二喜欢的算法，仅仅是因为它提供的简单性和健壮性。尽管有大量的分类算法，但这种过时的算法经受住了时间的考验，你可以看到它与神经网络的无缝集成是它非常特殊的地方。

**我们来谈谈数据**

为了达到目标，我挑选了一个[数据集](https://www.kaggle.com/bhavikbb/password-strength-classifier-dataset)，它将允许我讨论特征工程。这是一个密码强度分类数据集。它有 3 个等级，0-弱，1-中等和 2-强。它只有两列，分别是密码和力量等级。该数据集有大约 700，000 条记录，因此可以肯定它不是一个玩具数据集。

**免责声明！**

这篇文章期望你能提供一些概率概念。请刷新 [**概率**](https://www.youtube.com/watch?v=o4QmoNfW3bI) (5 分钟)和 [**赔率**](https://www.youtube.com/watch?v=GxbXQjX7fC0) (8 分钟)。阅读 BetterExplained 的这些优秀文章: [**指数函数直观指南& e**](http://betterexplained.com/articles/an-intuitive-guide-to-exponential-functions-e/) 和 [**揭秘自然对数(ln)**](http://betterexplained.com/articles/demystifying-the-natural-logarithm-ln/) 。然后，复习一下这个 [**对指数函数和对数的**](http://nbviewer.jupyter.org/github/justmarkham/DAT8/blob/master/notebooks/12_e_log_examples.ipynb) 的简要总结。

**OMG！我没有正确的数据。**

这里可以关注代码[。](https://nbviewer.jupyter.org/github/hdev7/medium-article-logistic-regression-follow-up-notebook/blob/master/logistic%20regression%20bottoms%20up%20approach.%20%28Feature%20engineering%20ideology%20-%20a%20bonus%29.ipynb)

陷阱 1:在密码数据集中可能有逗号(，)，并且您正在读取 CSV 文件，这是一个常见的分隔值文件。您可能会读取过多的逗号，并且可能会出现异常。所以，你可以通过设置 *error_bad_lines = False* 来避开这个问题。

![](img/bc3b7b116b92f373401389f3c7eb5216.png)

Distribution of passwords based on their strength

我们拥有的数据是密码列表，熊猫将其识别为“对象类型”。但是，为了使用 ML 模型，我们需要数字特征。让我们从现有的文本中生成新的特征。这就是特征工程拯救我们的地方。特征工程通常基于直觉。做这件事没有固定的正确方法。通常，在行业中，人们倾向于依赖领域专业知识来识别新特性。根据我们的数据，你是领域专家。您知道理想密码的关键要素，我们将从这里开始。

创建的特征:

*   密码中的字符数
*   密码中的数字个数
*   密码中的字母数
*   密码中元音的数量
*   密码中辅音的数量(越多意味着密码的意义越小)

![](img/d92bf3c23ccf880b1abc6318424e4c97.png)

New features. We will have to see if they actually contribute something.

# 逻辑函数

因此，我们希望返回一个介于 0 和 1 之间的值，以确保我们实际上表示的是一个概率。为此，我们将利用逻辑功能。逻辑函数在数学上看起来像这样:

![](img/027c06fdb31bcb91236a35e9b1b10b79.png)

让我们来看看剧情

![](img/d71dbc0994b074d24a73e7c047358ec6.png)

你可以看到为什么这是一个伟大的概率测量函数。y 值表示概率，范围仅在 0 和 1 之间。同样，对于 0 的 x 值，你得到 0.5 的概率，当你得到更多的正 x 值时，你得到更高的概率，更多的负 x 值时，你得到更低的概率。

# 利用我们的数据

好吧——这很好，但是我们到底该如何使用它呢？我们知道我们有五个属性——char _ count，numerics，alpha，元音，辅音——我们需要在逻辑函数中使用它们。我们可以做的一件显而易见的事情是:

![](img/1b3afe523a53a148342866c515ae9ac4.png)

其中 CC 是 char_count 的值，N 是 numerics 的值，A 是 alpha 的值，V 是元音的值，CO 是辅音的值。对于那些熟悉[线性回归](https://medium.com/@hemanthsaid7/linear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323)的人来说，这看起来很熟悉。基本上，我们假设 x 是我们的数据加上截距的线性组合。例如，假设我们有一个密码，其 char_count 为 8，numerics 为 4，alpha 为 4，元音为 1，辅音为 3。一些神谕告诉我们，𝛽0=1、𝛽1=2、𝛽2=4、𝛽3=6、𝛽4=8 和𝛽5=10.这意味着:

x = 1+(2 * 8)+(4 * 4)+(6 * 4)+(8 * 1)+(10 * 3)= 95

将此代入我们的物流功能，得出:

![](img/ddec27cb8e342bc45d446ca1f05d153d.png)

因此，我们认为具有这些特征的密码有 100%的可能性是中等(我取了第一行数据)

# 学问

好吧——有道理。但是是谁给了我们𝛽价值观呢？好问题！这就是机器学习中学习的用武之地:)。我们将了解我们的𝛽价值观。

# 步骤 1-定义你的成本函数

为了简单起见，我们将去掉一个类，使之成为一个二元分类问题。在本文的后面，我们将看到如何进行多重分类。现在，我把中等和弱混合作为一个类。如果你一直在研究机器学习，你可能会听到“成本函数”这个词。不过，在此之前，让我们思考一下。我们正在尝试选择𝛽值，以最大化正确分类密码的概率。这就是我们问题的定义。假设有人给了我们一些𝛽价值观，我们如何确定它们是否是好的价值观？我们在上面看到了如何获得一个例子的概率。现在想象一下，我们对所有的密码观察—所有的 700k 都这样做了。我们现在会有 70 万的概率分数。我们希望强密码的概率值接近 1，弱密码的概率值接近 0。

但是我们并不关心只得到一个观察值的正确概率，我们想要正确地分类我们所有的观察值。如果我们假设我们的数据是[独立且同分布的](http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)，我们可以取所有我们单独计算的概率的乘积，这就是我们想要最大化的值。所以在数学中，如果我们定义逻辑函数和 x 为:

![](img/f29356a3c8616561a0316bcc729b7b85.png)![](img/1b3afe523a53a148342866c515ae9ac4.png)

这可以简化为:

![](img/55ba5cce923d49df8322861baae8991f.png)

∏符号表示将产品归类为该密码进行观察。在这里，我们利用我们的数据被标记的事实，所以这被称为监督学习。另外，你会注意到，对于弱观测值，我们取 1 减去逻辑函数。这是因为我们试图找到一个最大化的值，由于弱观测值的概率应该接近于零，1 减去概率应该接近于 1。所以我们现在有了一个我们试图最大化的价值。通常，人们通过使其为负来将其转换为最小化:

![](img/96b9e859897e02a8173d8e6a1f54fb0f.png)

注意:最小化负面和最大化正面是一样的。上面的公式将被称为我们的成本函数。

# 步骤 2 —渐变

现在我们有一个要最小化的值，但是我们实际上如何找到最小化我们的成本函数的𝛽β值呢？我们要不要试一堆？这似乎不是一个好主意…

这就是[凸优化](http://en.wikipedia.org/wiki/Convex_optimization)发挥作用的地方。我们知道物流成本函数是[凸](http://en.wikipedia.org/wiki/Convex_function)——请相信我。因为它是凸的，它有一个全局最小值，我们可以使用[梯度下降](http://en.wikipedia.org/wiki/Gradient_descent)收敛到这个最小值。

这是一个凸函数的图像:

![](img/fb3c43312e45e1f53c66445a2b822b54.png)

source: utexas.edu

现在你可以想象，这条曲线是我们上面定义的成本函数，如果我们在曲线上选择一个点，然后沿着它下降到最小值，我们最终会达到最小值，这就是我们的目标。这里的是一个动画。这就是梯度下降背后的思想。

所以我们跟踪曲线的方法是，计算梯度，或者成本函数对每个𝛽.的一阶导数所以让我们做些数学计算。首先认识到，我们也可以将成本函数定义为:

![](img/341e895fdbf6fedc7e979dc97d958482.png)

这是因为当我们取对数时，我们的乘积变成了一个和。参见[日志规则](http://www.mathwords.com/l/logarithm_rules.htm)。如果我们定义𝑦𝑖在强观察时为 1，弱观察时为 0，那么我们只对强观察做 h(x ),对弱观察做 1 — h(x)。让我们对这个新版本的成本函数，对𝛽0.求导记住我们的𝛽0 在我们的 x 值里。所以记住 log(x)的导数是 1/𝑥，所以我们得到(对于每次观察):

![](img/0959772ff9fb9858656b254051e0a0c2.png)

使用[商法则](https://www.math.hmc.edu/calculus/tutorials/quotient_rule/)我们看到 h(x)的导数是:

![](img/fb0d4d2e054d2d75ab43975c51956a89.png)

x 对𝛽0 的导数是 1。综上所述，我们得到:

![](img/7fe9e02073d9132b4138a465f81d742d.png)

简化为:

![](img/bc16cdbab103ad7ae0025021ea022410.png)

引入负数和，我们得到对𝛽0 的偏导数是:

![](img/a1673a0d9141744cb70276057ca5afdc.png)

现在其他的偏导数就简单了。唯一的变化是𝑥𝑖的导数不再是 1。对𝛽1 来说是 CC𝑖，对𝛽2 来说是倪。所以𝛽1 的偏导数是:

![](img/8dd56503068d7a2038f1e2dabecffc35.png)

为了𝛽2:

![](img/b54062c42fe8b04398a8c0204f8d7528.png)

# 步骤 3 —梯度下降

现在我们有了梯度，我们可以使用梯度下降算法来找到最小化成本函数的𝛽s 值。梯度下降算法非常简单:

*   最初猜测您的𝛽值的任何值
*   重复直到收敛:
*   相对于𝛽𝑖的𝛽𝑖=𝛽𝑖−(𝛼∗梯度)

这里𝛼是我们的学习率。基本上，我们的成本曲线需要多大的步骤。我们现在做的是取当前的𝛽值，然后减去梯度的一部分。我们做减法是因为梯度是最大增加的方向，但是我们想要最大减少的方向，所以我们做减法。换句话说，我们在成本曲线上选取一个随机点，通过使用梯度的负值来检查我们需要向哪个方向靠近最小值，然后更新我们的𝛽值以向最小值靠近。重复直到收敛意味着不断更新我们的𝛽值，直到我们的成本值收敛或停止下降，这意味着我们已经达到了最小值。此外，同时更新所有𝛽值也很重要。这意味着您使用相同的先前的𝛽值来更新所有接下来的𝛽值。

# 梯度下降技巧

标准化变量:

*   这意味着每个变量减去平均值并除以标准差。
*   学习率:如果不收敛，学习率需要更小——但是收敛需要更长的时间。值得尝试的好值…，. 001，. 003，. 01，. 03，. 1，. 3，1，3，…
*   如果成本下降小于 10^−3，则声明收敛(这只是一个不错的建议)
*   绘制收敛图作为检查

遵循代码来实现。从零开始建立模型后，我们得到了 76%的准确率，我使用了 sklearn 软件包，得到了 99%的准确率，这相当不错。

# 高级优化

所以梯度下降是学习𝛽值的一种方法，但是还有其他的方法。基本上，这些是更高级的算法，我不会解释，但一旦你定义了你的成本函数和梯度，就可以很容易地在 Python 中运行。这些算法是:

*   [BFGS](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_bfgs.html)
*   [L-BFGS](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html) :类似 BFGS，但使用有限的内存
*   [共轭梯度](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html)

我会把它留在这里让你去探索。我建议没有多少人知道这些先进的优化技术，而是他们很好地工作与梯度下降及其变种，因为他们做了相当不错的工作。

# 评估分类

现在，我们已经花了一些时间来理解逻辑回归背后的数学，让我们看看如何评估分类问题。我们现在可以拟合一个逻辑回归模型。逻辑回归也可以利用我们在线性回归文章中使用的正则化技术。它们的使用方式完全相同——我们通过向成本函数添加 L1 或 L2 范数来惩罚大系数。

请注意，我们不会像线性回归一样在这里演示超参数优化的 CV，但这仍然适用。事实上，我们在上一篇文章中讨论的几乎所有技术都可以在这里使用。我们将讨论新的话题。我们已经知道了前一篇文章中的训练和测试数据。让我们看看这个模型在测试数据上表现如何。

# 准确(性)

分类时最容易理解的度量之一是准确性:您正确预测的分数是多少。

![](img/a3d9ea84687f434fada3024e5c03d0f2.png)

Excellent!

准确性的一个缺点是，当你有**不平衡的**类时，这是一个相当**可怕的**度量。让我们来看看我们的阶级平衡:

![](img/a8f1b72090678974109e0f7334d39cf3.png)

Class Imbalance

因此，我们大约 88%的数据是负面的——因此，如果我们只是预测一切都是负面的，我们的准确率将是 88%左右！还不错。想象一下其他数据，其中负类只出现了 1%的时间——你可以通过预测所有事物都是正的来获得 99%的准确率。这种准确性可能感觉很好，但如果这是你的模型，它是非常没有价值的。

# 精确度、召回率和 F1

为了处理其中的一些问题，您经常会看到使用精度、召回率和 F1 来评估分类任务。

**精确度**是真阳性的数量除以所有的阳性预测(真阳性加上假阳性):基本上，当我们预测某事为阳性时，我们的预测有多精确。

![](img/f6ea41c8fae6f57ee83142a14b4b3330.png)

Precision

**回忆**是真阳性的数量除以所有实际阳性的数量(真阳性加上假阴性):基本上，我们实际预测了所有阳性的多少。

![](img/ee64b2dd69d29ec41c949f786798f581.png)

Recall

这两个指标相互之间有所取舍。对于同一个模型，你可以以召回为代价提高精度，反之亦然。通常，您必须确定对于手头的问题哪个更重要。我们希望我们的预测是精确的还是高召回率的？

什么是真/假阳性/阴性？我们来看一个 [**混淆矩阵**](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) 。

![](img/f879c9147e681af11adaf60cec2b74fe.png)

Confusion matrix

这是我们测试预测的混淆矩阵。这些行就是真相。因此，第 0 行是实际的 0 标签，第 1 行是实际的 1 标签。列是预测。因此，单元格 0，0 计算我们正确得到的 0 类的数量。这被称为真正的否定(假设我们认为标签 0 是否定标签)。单元格 1，1 计算我们得到正确的 1 类的数量—真阳性。单元格 0，1 是我们的假阳性，单元格 1，0 是假阴性。

正如你可能知道的，混淆矩阵对于错误分析是非常有用的工具。在这个例子中，我们有非常对称的错误——两个类中都有四个错误。情况并不总是这样，通常会有两个以上的类，因此了解哪些类会让其他人感到困惑，对于改进模型非常有用。

有时候你只想在精确度和召回率之间取得平衡。如果这是你的目标， **F1** 是一个非常常见的指标。这是精确和回忆的调和平均值:

![](img/89dc7c45ca67f846ffa98f9a31e36024.png)

Harmonic mean of precision and recall

使用调和平均值是因为它强烈倾向于被平均的最小元素。如果 precision 或 recall 是 0，那么 F1 是 0。最好的 F1 成绩是 1。

# 精度/召回曲线

对于之前的所有指标，我们一直使用二元预测。但是真正的逻辑回归返回概率，这是非常有用的。

现在每个预测不是一个 1/0 的数字，我们有两个数字:被分类为 0 的概率和被分类为 1 的概率。这两个数字的和必须是 1，所以 1 的概率= 0 的概率。一件有用的事情是确保这些概率实际上似乎与类别相关联:

![](img/2ef7e331084624d9b9c2f111fc51b5c1.png)

这张图向我们展示的是，对于我们的训练数据，我们的积极类确实有很高的概率是积极的，而我们的消极类有很高的概率是消极的。这基本上是我们设计算法的目的，所以这是可以预料的。检查验证集是否如此也是很好的。

Sklearn 的 predict 函数只是把概率最高的类作为预测类，这个方法还不错。然而，人们可能想要一个非常精确的模型来进行积极的预测。我们可以这样做的一个方法是，只有当概率超过 90%时，才称之为肯定的。为了理解不同截止值下精确度和召回率之间的权衡，我们可以绘制它们:

![](img/ab5208d0cbadb6d01fe6f2d5071329a0.png)

不错！当我们调整阈值时，我们可以清楚地看到 P 和 R 之间的权衡。事实上，我们可以找到最大化 F1 的截止点(**注**:我们在这里使用测试数据，这在实践中并不好。我们希望使用验证集来选择临界值)

# 受试者工作特征曲线

ROC 曲线是评估分类任务的另一种流行方法。它是一个图，y 轴是召回值(或真阳性率)，x 轴是假阳性率(假阳性除以所有实际阴性)。因此，它显示了您的模型如何在这两个值之间进行权衡。您的值越靠近图表的左上角越好。让我们来看看我们的测试数据:

![](img/15bef3e4a70ba5ba64b94041a5eead93.png)

ROC curve USP: It would work even with highly imbalanced data

显然，我们的模型做得非常好。您在右下角看到的 AUC(曲线下面积)分数是 ROC 曲线下的面积，1 是最好的值。我们有一个。如果您知道想要在 TPR 和 FPR 之间做出的权衡，该曲线也可用于选择阈值。

# 多类

逻辑回归也可以处理 2 个以上的类。有两种方法可以做到这一点:

*   [One-vs-rest](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier) 方法:使用这种方法，我们可以为每个类训练一个分类器，当该类是活动的时，正值，而对于所有其他类，负值。
*   [交叉熵](https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation)损失:我们也可以改变我们的损失函数来合并多个类。最常见的方法是使用交叉熵损失:

![](img/7d3151e675865e38216b3842e809d181.png)

Cross entropy loss

这种损失采用正确类别的预测概率的负平均值。因此，优化它试图获得尽可能高的正确类别的预测概率。

# 解释逻辑回归

解释逻辑回归的系数比解释线性回归的系数要稍微复杂一些。让我们来看看我们的系数:

我们最正的系数是 char_counts 的值 5(粗略估计)。由于我们使用对数损失，我们需要通过 e^5 来转换这个值，即 147.52。也就是说，char_counts 每增加 1 个单位，密码是强密码的几率就会增加 14752%，因为我们现在已经转换为一个几率比。关于这个的更多细节，请看这个[的帖子](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/)。

**问题答案**

我想用一个例子来说明。假设一个 2 类分类问题和 6 个实例。

假设，真实概率= [1，0，0，0，0]

**案例 1:** 预测概率= [0.2，0.16，0.16，0.16，0.16，0.16，0.16]

**情况二:**预测概率= [0.4，0.5，0.1，0，0，0]

情况 1 和情况 2 的均方误差分别为 **0.128** 和 **0.1033** 。

尽管如果 0.5 被认为是阈值，情况 1 在预测该实例的类时更正确，但是情况 1 中的损失高于情况 2 中的损失。这是为什么呢？MSE 是一种度量标准，用于测量预测值与实际值的标准偏差。而且它的应用可能仅限于二元分类。它仍然可能在特定的情况下使用。RMSE 是衡量分类器性能的一种方式。错误率(或错误分类的数量)是另一个因素。如果我们的目标是一个低错误率的分类器，RMSE 是不合适的，反之亦然。这解释了为什么我们不使用 MSE/RMSE 作为分类问题的评估指标。其他度量比 MSE 提供了更多的灵活性和对分类器性能的理解。

**代号:** [Github](https://nbviewer.jupyter.org/github/hdev7/medium-article-logistic-regression-follow-up-notebook/blob/master/logistic%20regression%20bottoms%20up%20approach.%20%28Feature%20engineering%20ideology%20-%20a%20bonus%29.ipynb)

我的文章关于[线性回归](https://medium.com/@hemanthsaid7/linear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323)

如果你需要任何帮助，你可以联系: [LinkedIn](https://www.linkedin.com/in/hemanthsaid/)

参考资料:

Geron Aurelien 的动手机器学习