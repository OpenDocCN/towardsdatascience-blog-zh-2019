# 强大的图形对数据科学有多重要？

> 原文：<https://towardsdatascience.com/how-vital-are-powerful-graphics-for-data-science-5ead7a54d50c?source=collection_archive---------21----------------------->

![](img/6e28861138f1f302cd60fde5c5d1c446.png)

当谈到深度学习时，通常你可以将模型带到的规范与你机器内部的硬件绑定在一起。我们都见过强化的深度学习机器，它们配有巨大的鼓风机式冷却器、一体机，以及大量的内存插槽，只有最快的 DDR4 才能填满。考虑到这一点，我们的 GPU 对机器学习有多重要？当我们拟合模型和构建神经网络时，我们可以期望我们的 GPU 为我们执行什么样的角色？

> (确切地说)不重要。

硬件对标准深度学习的影响可以像这样映射出来。

*   处理器
*   随机存取存储器
*   互换
*   读写速度(在某些情况下)
*   图形卡

随着这些信息在你的大脑中跳跃，你可能会思考 GPU 在日常 ML 中可能扮演的角色。首先，虽然渲染可视化只依赖于处理器，但是 GPU 仍然需要估算所有的单独向量，向量 2，以及你放入屏幕的任何其他几何图形。因此，对于你绘制数百万对数百万的可视化来说，一个能够支持计算的显卡是必不可少的。

然而，有了股票包的经验，我们注定要 CPU 来做我们所有的计算，并拉和推一切来回到内存，并最终到我们的显卡。

# 人们添加它们的原因

GPU 的可取之处在于它以包的形式设计了 CUDA。大多数统计机器学习操作涉及以数字矩阵的形式移动大量数据，并实时计算值。幸运的是，这正是显卡的设计目的。

![](img/cdf4f5a625b42e42626ab7b93296565b.png)

虽然在您的包中使用 CUDA 有一些缺点，但是好处远远超过这些问题。CUDA 允许你的 GPU 和 CPU 互换通信，其中你的显卡非常擅长矩阵乘法和矢量渲染，你的 CPU 非常擅长发送、接收和存储数据。当然，从优化的角度来看，这是一个粗略的话题。在我使用 CUDAnative.jl 在 Julia 中进行的测试中，结果肯定比使用原始 CPU 能力来提供所有结果要快。这是一个严重的缺点，我们称之为频率步进。在典型的情况下，中央处理器更小，更不专业。而图形处理单元更专用于较低的核心时钟。因此，在一起计算时，GPU 和 CPU 可以有效地交换意见。

这不是问题，问题是跨硬件 IRQ 不平衡:当一个硬件必须等待另一个硬件时，时间就被浪费了，整个几百 mHz 都因为这个问题而丢失了。

考虑到这一点，CPU 绝对擅长处理这个问题，这个问题以任何明显的瓶颈的形式出现，所以一个平衡的系统可能是理想的。

![](img/addc22628a926acbc466acc6b3d46348.png)

所以回到一个系统上的四个 GPU，通常这对于机器学习来说是一个低效的设置，因为尽管图形能力非常棒，但单个 CPU 的能力根本不足以支持这样的事情。这就是为什么我们通常会在数据科学 ThinkStation 中看到匹配良好的双 CPU 和双 GPU。在这个特定的使用案例中，我们充分利用了两个显卡的全部潜力，并拥有支持每个显卡的处理速度。

没有人知道未来几年处理器、显卡和中央处理器会怎么样。就我个人而言，当我们看到英特尔试图对抗新的锐龙处理器，同时试图推出一款产品来击败 amd 即将推出的 5 纳米台式机(而不是服务器)CPU 时，我非常兴奋地看到明年的“巨人的冲突”。我能确定的是，2020 年对计算机、人工智能和技术爱好者来说肯定会是激动人心的一年。