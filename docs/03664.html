<html>
<head>
<title>A Gentle Introduction to Maximum Likelihood Estimation and Maximum A Posteriori Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大似然估计和最大后验估计简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-and-maximum-a-posteriori-estimation-d7c318f9d22d?source=collection_archive---------1-----------------------#2019-06-11">https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-and-maximum-a-posteriori-estimation-d7c318f9d22d?source=collection_archive---------1-----------------------#2019-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ec2a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">以足球为例获得最大似然法和地图的直觉</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7870cb15f3b1c447b2895083e3b7e1ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sR-wO1B9Slwo4HSI"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@focusmitch?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mitch Rosen</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e513" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最大似然估计(MLE)和最大后验估计(MAP)是估计统计模型参数的方法。</p><p id="3c5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管这些方法背后有一点高等数学，但 MLE 和 MAP 的思想非常简单，直观易懂。在这篇文章中，我将解释什么是 MLE 和 MAP，重点是方法的直觉以及背后的数学。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ce3d" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">例子:利物浦足球俱乐部在下个赛季赢得比赛的概率</h2><p id="e159" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">2018-19 赛季，利物浦 FC 在英超 38 场比赛中赢了 30 场。有了这些数据，我们想猜测下赛季利物浦赢得比赛的概率。</p><p id="6efe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里最简单的猜测是<em class="na"> 30/38 = 79% </em>，这是基于数据的最佳猜测。这实际上是用<strong class="lb iu"> MLE </strong>方法估计的。</p><p id="681d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，假设我们知道利物浦在过去几个赛季的胜率在 50%左右。你认为我们最好的猜测还是 79%吗？考虑到先前的知识以及本赛季的数据，我认为 50%到 79%之间的某个值会更现实。这是用<strong class="lb iu">图</strong>法估算的。</p><p id="0927" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我相信上面的想法很简单。但是为了更精确的理解，我将在下面的部分中阐述 MLE 和 MAP 的数学细节。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="dc55" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">模型和参数</h2><p id="49cd" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在讨论每种方法之前，让我先澄清一下这个例子中的模型和参数，因为 MLE 和 MAP 是估计统计模型的<strong class="lb iu">参数的方法。</strong></p><p id="f099" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，我们简化为利物浦在所有赛季的所有比赛中只有一个获胜概率(姑且称之为<em class="na"> θ </em>)，而不考虑每场比赛的独特性和真实足球比赛的任何复杂因素。换句话说，我们假设利物浦的每场比赛都是伯努利试验，获胜概率为<em class="na"> θ </em>。</p><p id="70d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这个假设，我们就可以描述对于任意给定数量的<em class="na"> k </em>和<em class="na"> n </em> ( <em class="na"> k≤n </em>)，利物浦在<em class="na"> n </em>场比赛中赢<em class="na"> k </em>次的概率。更准确的说，我们假设利物浦的胜数服从参数为<em class="na"> θ </em>的<strong class="lb iu">二项分布</strong>。给定获胜概率<em class="na"> θ </em>，利物浦在<em class="na"> n </em>场比赛中赢<em class="na"> k </em>次的概率公式如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/eef88801d112dc55c186a0b36b4f0593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nSxC3iyny6wSAiy7R8USvA.png"/></div></div></figure><p id="4b71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种简化(仅使用单个参数<em class="na"> θ </em>来描述概率，而不考虑现实世界的复杂性)是该示例的统计建模，并且<em class="na"> θ </em>是待估计的参数。</p><p id="5067" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从下一节开始，我们用 MLE 和 MAP 来估计这个<em class="na"> θ </em>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="6935" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">最大似然估计</h2><p id="5027" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在上一节中，我们得到了对于给定的<em class="na"> θ </em>，利物浦在<em class="na"> n </em>场比赛中赢<em class="na"> k </em>次的概率公式。<br/>由于我们有本赛季的观测数据，即<em class="na">38 场比赛中的 30 场胜利</em>(姑且称此数据为<em class="na"> D </em>)，我们可以计算出<em class="na">P(D |θ)——</em>对于给定的<em class="na"> θ </em>，观测到此数据<em class="na"> D </em>的概率。让我们以<em class="na"> θ=0.1 </em>和<em class="na"> θ=0.7 </em>为例计算<em class="na"> P(D|θ) </em>。</p><p id="5434" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当利物浦获胜概率<em class="na"> θ = 0.1 </em>时，观察到这个数据<em class="na">D</em>(<em class="na">38 场比赛 30 胜</em>)的概率如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/f4627f69ec00f7585ed510d17e667a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QeRVGqCCREhoUI3Nci5d1Q.png"/></div></div></figure><p id="41ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="na">P(D |θ)= 0.000000000000000000211</em>。所以，如果利物浦的获胜概率<em class="na"> θ </em>实际上是<em class="na"> 0.1 </em>，这个数据<em class="na">D</em>(<em class="na">38 场比赛 30 胜</em>)是极不可能被观测到的。那如果<em class="na"> θ = 0.7 呢？</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/327dc450e15bc6ee8311d81fc84400bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfyck-rx1z7ZVkqSA3Yhag.png"/></div></div></figure><p id="0178" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">远高于之前的。所以如果利物浦的获胜概率<em class="na"> θ </em>为 0.7，这个数据<em class="na"> D </em>比<em class="na"> θ = 0.1 </em>时更容易被观察到。</p><p id="e522" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于这种比较，考虑到实际观测数据<em class="na"> D </em>，我们可以说<em class="na"> θ </em>更有可能是<em class="na"> 0.7 </em>而不是<em class="na"> 0.1 </em>。<br/>这里，我们一直在计算对于每个<em class="na"> θ </em>观察到<em class="na"> D </em>的概率，但同时，我们也可以说，我们一直在根据观察到的数据检查<em class="na"> θ </em>的每个值的可能性。正因为如此，<em class="na"> P(D|θ) </em>也被认为是<em class="na"> θ </em>的<strong class="lb iu">可能性</strong>。这里的下一个问题是，最大化可能性<em class="na"> P(D|θ) </em>的<em class="na"> θ </em>的精确值是什么？没错，这就是最大似然估计！</p><p id="b13d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最大化似然性的<em class="na"> θ </em>的值可以通过使似然函数对<em class="na"> θ </em>求导并将其设置为零来获得。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/a487f4351e9835efaa25aa34fe2f38ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wIiE6nsBZy7qyaLnLaa8WA.png"/></div></div></figure><p id="bcb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决这个，<em class="na"> θ = 0，1 或者 k/n </em>。由于当<em class="na"> θ= 0 或 1 </em>时似然性为零，因此<em class="na"> θ </em>的值使似然性最大化为<em class="na"> k/n </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/14500dcbb601944855eeaaa77cbb402e.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*aw135gzMg-4cUEbCvCpQvw.png"/></div></figure><p id="363e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本例中，用 MLE 估计时，<em class="na"> θ </em>的估计值为<em class="na"> 30/38 = 78.9% </em>。</p><h2 id="7cab" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">最大后验估计</h2><p id="f80b" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">当你有足够的数据时，MLE 是强大的。然而，当观察到的数据量很小时，这种方法效果不好。例如，如果利物浦只有 2 场比赛，他们赢了这 2 场比赛，那么 MLE 估计的<em class="na"> θ </em>的值是<em class="na"> 2/2 = 1 </em>。意思是估计说利物浦赢<em class="na"> 100% </em>，这是不切实际的估计。地图可以帮助处理这个问题。</p><p id="8589" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们事先知道利物浦过去几个赛季的胜率在 50%左右。<br/>然后，没有这个赛季的数据，我们已经对<em class="na"> θ </em>的潜在价值有了一些概念。(仅)基于先验知识，<em class="na"> θ </em>的值最有可能是<em class="na"> 0.5 </em>，不太可能是<em class="na"> 0 或 1 </em>。换句话说，<em class="na"> θ=0.5 </em>的概率高于<em class="na"> θ=0 或 1 </em>。称之为<strong class="lb iu">先验概率</strong> <em class="na"> P(θ)，</em>如果我们将它形象化，它将如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/6e4244b11eeae592c33df87839fc1f07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sDmq4gqhxo1EQQ20p2M8bA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 1. A visualisation of P(<em class="nh">θ</em>) expressing the example prior knowledge</figcaption></figure><p id="49c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，有了本赛季的观察数据<em class="na">D(38 场比赛中 30 胜)</em>，我们可以更新这个仅基于先验知识的<em class="na"> P(θ) </em>。给定<em class="na"> D </em>的<em class="na"> θ </em>的更新概率表示为<em class="na"> P(θ|D) </em>，称为<strong class="lb iu">后验概率</strong>。<br/>现在，考虑到我们的先验知识和观测数据，我们想知道<em class="na"> θ </em>的最佳猜测。这意味着最大化<em class="na"> P(θ|D) </em>并且这是 MAP 估计。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d4ca4d3cc9ee13a40fd20290235d934f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*62Lf0sgsvPlRM5k9Rsj1ug.png"/></div></figure><p id="8ca8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的问题是，如何计算<em class="na"> P(θ|D) </em>？到目前为止，在本文中，我们检查了计算<em class="na"> P(D|θ) </em>的方法，但还没有看到计算<em class="na"> P(θ|D) </em>的方法。为此，我们需要使用下面的贝叶斯定理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/3d55c4a270ec0a342e799bafd6646d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L61Vh8oNChtksR-GXNBIhA.png"/></div></div></figure><p id="08b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文不深入讨论贝叶斯定理，但有了这个定理，我们可以利用似然<em class="na"> P(D|θ) </em>和先验概率<em class="na"> P(θ) </em>计算后验概率<em class="na"> P(θ|D) </em>。</p><p id="f59e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">等式中有<em class="na"> P(D) </em>，但<em class="na"> P(D) </em>与<em class="na"> θ </em>的值无关。由于我们只对寻找最大化<em class="na"> P(θ|D) </em>的<em class="na"> θ </em>感兴趣，我们可以在最大化中忽略<em class="na"> P(D) </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/b92f686050cc7e9ad74821cf0877082b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mm96Re-AKM1-tPZJ4nB6EQ.png"/></div></div></figure><p id="5575" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的等式意味着关于<em class="na"> θ </em>的后验概率<em class="na"> P(θ|D) </em>的最大化等于关于<em class="na"> θ </em>的似然<em class="na"> P(D|θ) </em>和先验概率<em class="na"> P(θ) </em>的乘积的最大化。</p><p id="f76f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在本节的前面讨论了<em class="na"> P(θ) </em>的含义，但是我们还没有进入公式。本质上，我们可以用任何一个将概率分布描述为<em class="na"> P(θ) </em>的公式来很好地表达我们的先验知识。然而，为了计算的简单性，使用对应于可能性的概率分布的特定概率分布。叫做<strong class="lb iu">共轭先验分布</strong>。</p><p id="9b37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，可能性<em class="na"> P(D|θ) </em>遵循二项式分布。由于二项分布的共轭先验是贝塔分布，所以我们这里用贝塔分布来表示<em class="na"> P(θ) </em>。贝塔分布描述如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/301b9f6f57d1bc737814d27e2a7ba072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5QkPiTyCK6Ns6nMfrV4OEQ.png"/></div></div></figure><p id="6fc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中，<em class="na"> α </em>和<em class="na"> β </em>称为<strong class="lb iu">超参数</strong>，无法通过数据确定。相反，我们主观地设置它们是为了更好地表达我们的先验知识。例如，下图是不同值的<em class="na"> α </em>和<em class="na"> β </em>的贝塔分布的可视化。你可以看到左上角的图是我们在上面的例子中使用的图(表示<em class="na"> θ=0.5 </em>是基于先验知识的最可能的值)，右上角的图也表示相同的先验知识，但这是为相信过去几个赛季的结果很好地反映了利物浦的真实能力的人准备的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/fe8420fe0cd9441744ce2df13c336e28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TdAPtdZMOkyZGp_hzIqTvg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 2. Visualisations of Beta distribution with different values of <em class="nh">α</em> and <em class="nh">β</em></figcaption></figure><p id="45b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个关于右下图的说明:当<em class="na"> α=1 </em>和<em class="na"> β=1 </em>时，意味着我们对<em class="na"> θ </em>没有任何先验知识。在这种情况下，估计将与 MLE 的估计完全相同。</p><p id="4832" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，现在我们有了所有的组件来计算<em class="na"> P(D|θ)P(θ) </em>以使其最大化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/01288ef69e31ab456ad691dcf1d9e0da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*apbMK3YGujmqFsbe0WmE_Q.png"/></div></div></figure><p id="d8a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与 MLE 一样，我们可以通过对这个函数求关于<em class="na"> θ </em>的导数，并将其设置为零，从而使<em class="na"> θ </em>最大化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/e5f47c9729d6e6e52fdc2b40e5cab14f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LHlIHEpBTl-vLIT0HgZMPg.png"/></div></div></figure><p id="de06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过解决这个问题，我们得到以下结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/038372887c39eb3e5c50d3c5fd48712c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*__rBAOTIFRxmYyO76bgpZQ.png"/></div></figure><p id="1a58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，假设我们用<em class="na"> α=10 </em>和<em class="na"> β=10 </em>，那么<em class="na">θ=(30+10–1)/(38+10+10–2)= 39/56 = 69.6%</em></p><h2 id="83a5" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">结论</h2><p id="823d" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">从上面的例子可以看出，MLE 和 MAP 复杂的数学方程背后的思想出奇的简单。我在本文中使用了二项分布作为例子，但是 MLE 和 MAP 也适用于其他统计模型。希望这篇文章能帮助你理解 MLE 和 MAP。</p></div></div>    
</body>
</html>