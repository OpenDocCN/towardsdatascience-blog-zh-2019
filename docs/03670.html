<html>
<head>
<title>Transfer Learning for Time Series Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于时间序列预测的迁移学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transfer-learning-for-time-series-prediction-4697f061f000?source=collection_archive---------7-----------------------#2019-06-11">https://towardsdatascience.com/transfer-learning-for-time-series-prediction-4697f061f000?source=collection_archive---------7-----------------------#2019-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="d4ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> LSTM </strong>递归神经网络被证明是时间序列预测任务的一个很好的选择，然而该算法依赖于我们有足够的来自同一分布的训练和测试数据的假设。挑战在于时间序列数据通常表现出时变特征，这可能导致新旧数据之间的很大差异。在这篇博客中，我们想测试<strong class="js iu">迁移学习</strong>和模型的一般领域训练在多大程度上有助于解决上述问题。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a3f5b5a66ad146461400092bf65078e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HQ1F6hlk-aAjTUCiNiOsSA.jpeg"/></div></div></figure><blockquote class="la lb lc"><p id="1878" class="jq jr ld js b jt ju jv jw jx jy jz ka le kc kd ke lf kg kh ki lg kk kl km kn im bi translated"><em class="it">迁移学习(migration learning)是通过从相关任务中迁移已经学习过的知识来提高新任务中的学习。</em> <!-- -->第十一章:迁移学习<!-- -->，<a class="ae lh" href="http://amzn.to/2fgeVro" rel="noopener ugc nofollow" target="_blank">机器学习应用研究手册</a>，2009。</p></blockquote><p id="7919" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">学习迁移有几种方法，它们的区别在于可迁移的知识。在这篇博客中，我们将使用<em class="ld">参数传递</em>方法:</p><ol class=""><li id="6446" class="li lj it js b jt ju jx jy kb lk kf ll kj lm kn ln lo lp lq bi translated">我们将在基线数据集(作为源域)上建立一个模型，然后转移训练过的权重，作为要在目标域上训练的模型的初始化(标准转移学习)。</li><li id="1161" class="li lj it js b jt lr jx ls kb lt kf lu kj lv kn ln lo lp lq bi translated">我们将在来自两个源(源和目标域)的组合时间序列数据上构建一个模型到一个一般域中，在一般域数据上“预训练”该模型，仅在一个目标域上调整该模型，并将后一个模型的性能与仅在一般域和目标数据上训练的另外两个模型进行比较。</li></ol><p id="009d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里是<a class="ae lh" href="https://github.com/NshanPotikyan/TransferLearningTimeSeries.git" rel="noopener ugc nofollow" target="_blank"> GitHub 链接</a>到项目的仓库。</p><p id="8518" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们用下面显示的随机噪声生成 6 个合成时间序列。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lw"><img src="../Images/93ecf51b765f7e486e59e7cd8ba3fd08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5OwgU0RcrBBXu8JoV5SkA.jpeg"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">The generated synthetic time series</figcaption></figure><p id="a09d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们有了我们的实验时间序列，让我们在第一个时间序列上训练一个模型，并把它作为一个源任务。这是代表模型拟合和评估流程的流程图。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mb"><img src="../Images/abe2350130ed46ca0a967fc84c4354fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LO4eJZoed-mINR3uroZJzQ.jpeg"/></div></div></figure><p id="276b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为 LSTM 网络期望输入/输出序列，在阶段 2 中，我们以如下方式将缩放的序列转换成 X(输入)和 Y(输出)序列:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mc"><img src="../Images/56308c50c4d08496df8538ea331feedc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFyQSB5pXi45tXFzBufwEQ.jpeg"/></div></div></figure><p id="b8b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了找到模型的合适架构和超参数值，进行了网格搜索，结果得到了具有以下配置的模型:</p><p id="3766" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">具有 100 和 50 个神经元的 2 个 LSTM 层(无脱落)</strong></p><p id="b79e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">学习率=0.00004 的 Adam 优化器</strong></p><p id="ce97" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输入序列的长度是在自相关图(相关图)的帮助下确定的，因为它显示了相对于时间滞后的序列中的重复模式。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi md"><img src="../Images/5fb3d2a069619316938d5cc81726a7dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*0SPV61GdIBEzjD05OU5rXA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Correlogram of Sine Wave</figcaption></figure><p id="e594" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在生成正弦波的情况下，所需的输入(滚动窗口)大小约为 70(因为模式从那里开始重复)，这一选择也通过在一些其他候选值上运行网格搜索来证明。以下是模型训练和评估结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi me"><img src="../Images/11097bb7ccbf408bd777cc5a0760a3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*pYGDpo9d0kqSZlTa09ShRA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Training and Validation Loss per Epoch</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/d1edce1ebf1e4184490d28b58ba9c5d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*cE1kCa7iPmb_guY9TF6m5w.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Baseline model prediction results</figcaption></figure><p id="b45c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">MSE = 0.1</p><p id="2eef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图上的预测对应于模型的 50 倍预测，该预测是这样迭代完成的:</p><ol class=""><li id="666e" class="li lj it js b jt ju jx jy kb lk kf ll kj lm kn ln lo lp lq bi translated">X_test(用于测试的输入数据集)中的第一个可用序列用于预测序列的下一个值，例如 f(x1，…x10 )=x11</li><li id="874f" class="li lj it js b jt lr jx ls kb lt kf lu kj lv kn ln lo lp lq bi translated">预测值 x11 被附加到 x2，…，x9 序列以形成新的序列，并且我们使用 x2，…，x11 预测 x12。</li><li id="4d1e" class="li lj it js b jt lr jx ls kb lt kf lu kj lv kn ln lo lp lq bi translated">第二步重复用户定义的次数(n_ahead-1)</li></ol><h2 id="910e" class="mg mh it bd mi mj mk dn ml mm mn dp mo kb mp mq mr kf ms mt mu kj mv mw mx my bi translated">迁移学习</h2><p id="241c" class="pw-post-body-paragraph jq jr it js b jt mz jv jw jx na jz ka kb nb kd ke kf nc kh ki kj nd kl km kn im bi translated">现在我们有了一个训练好的模型，我们可以开始试验知识转移是否有助于在目标领域(除了 Sine 之外的所有其他系列)获得更好的执行模型。我们进行了两项独立的试验:</p><p id="5726" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">1)在目标序列的最后 100 个数据点上训练 2 个模型(有和没有迁移学习),并预测 10 个超前值</p><p id="dba9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2)在目标系列的整个数据集(2000 点)上训练 2 个模型，并预测 50 个提前值。</p><p id="84f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这背后的直觉是看迁移学习在数据稀缺的情况下是否有帮助。</p><p id="8e49" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是 RMSE 实验的结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ne"><img src="../Images/08ec8f32e3373752acb0da7ed90a1e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PT9iwEUGgQqLKbr8nHcg0g.jpeg"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">RMSE results from the predictions</figcaption></figure><p id="f422" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是一些结果的可视化。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lw"><img src="../Images/75523380de19e7fe2be2a31fbea5f138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cVxslliTHEzJ94OpyrKgOQ.jpeg"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Positive example of Transfer Learning on Cosine series</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lw"><img src="../Images/e85ed378e113d72723431db43fc3aa48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5hlcfR3EVNLYU2KpQCI3g.jpeg"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Negative example of Transfer Learning on Absolute Sine series</figcaption></figure><p id="69bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从 RMSE 的结果中，我们可以得出结论，一般来说，迁移学习有助于一半情况下的提高，但是如果我们仔细观察，我们可以注意到<strong class="js iu">在目标领域</strong>缺乏数据时，它在 3/5 的情况下是有帮助的。</p><h2 id="92f1" class="mg mh it bd mi mj mk dn ml mm mn dp mo kb mp mq mr kf ms mt mu kj mv mw mx my bi translated">常规和域内调整</h2><p id="1924" class="pw-post-body-paragraph jq jr it js b jt mz jv jw jx na jz ka kb nb kd ke kf nc kh ki kj nd kl km kn im bi translated">正如开头所解释的，这里我们将安装 3 个模型:</p><ol class=""><li id="925e" class="li lj it js b jt ju jx jy kb lk kf ll kj lm kn ln lo lp lq bi translated">预调优—基于通用域数据</li><li id="905a" class="li lj it js b jt lr jx ls kb lt kf lu kj lv kn ln lo lp lq bi translated">tuned——既在一般领域上，然后在目标领域上传递知识和拟合</li><li id="bdec" class="li lj it js b jt lr jx ls kb lt kf lu kj lv kn ln lo lp lq bi translated">仅针对目标—顾名思义，此功能仅适用于目标域</li></ol><p id="a5e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">时间序列被附加在一起形成一个通用的领域数据集。</p><p id="32ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是与上一节相似的实验结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/62bc9dcbc133581fbb5a09c6f9af3a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zt9g42rPUpNmPhRpsa-NLA.jpeg"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">RMSE results from the predictions</figcaption></figure><p id="cf96" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到，在“调整”的情况下，所提出的方法只提高了一次性能。可以预料，目标领域的训练比其他两种情况表现更好，而“预调优”的情况在 10 种情况中有 3 种表现更好。</p><p id="6414" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是一些结果的可视化。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ng"><img src="../Images/1d33169b6dac61f13ad843014ff39a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NoNHcjDxz91cML7djTEnDw.jpeg"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Positive example of “Pre-tuned” case outperformance</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nh"><img src="../Images/3f2f08345cf06746b434400372f66361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LumA8v_8ZZHppRZGIpxdXg.jpeg"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Positive example of “Tuned” case outperformance.</figcaption></figure><p id="d5e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">结论</strong></p><p id="c45f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">基于所获得的结果，迁移学习似乎在缺乏时间序列数据的情况下以及当我们对类似任务有微调的模型时是有用的。</p><p id="4217" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">至于模型的一般领域调优，需要进一步探索，以发现是否有其他组合技术可以提高“调优”模型的性能。</p></div></div>    
</body>
</html>