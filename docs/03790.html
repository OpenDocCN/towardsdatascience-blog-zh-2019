<html>
<head>
<title>A new Tool to your Toolkit, KL Divergence at Work</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的工具箱中的一个新工具，KL Divergence at Work</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/part-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d?source=collection_archive---------10-----------------------#2019-06-15">https://towardsdatascience.com/part-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d?source=collection_archive---------10-----------------------#2019-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0e72" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最后，将 KL 散度应用于真实数据集</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1fd984e12059e1c2a118d10d3eca2532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b217uqv8HdLI0Uld.jpg"/></div></div></figure><p id="ef59" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我的<a class="ae lq" rel="noopener" target="_blank" href="/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e">上一篇</a>中，我们对<a class="ae lq" href="https://en.wikipedia.org/wiki/Entropy" rel="noopener ugc nofollow" target="_blank">熵</a>、<a class="ae lq" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">交叉熵</a>、<a class="ae lq" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> KL 散度</a>有了一个直观的了解，也通过实例计算了它们的值。如果你错过了，请<a class="ae lq" rel="noopener" target="_blank" href="/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e">在进入最后一集之前再看一遍</a>。</p><p id="317e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本帖中，我们将应用这些概念，并在真实的数据集中检查结果。此外，它将为我们提供良好的直觉，告诉我们如何在建模各种日常机器学习问题时使用这些概念。那么，我们开始吧。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="eb37" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">1.分析数据集</h1><p id="bb92" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">数据集包括两个潜在特征‘f1’和‘F2’以及数据点所属的类，即正类或负类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/6ad5ef3e3526e6378262ab9447e64388.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*Sza4FIF0b47XXbjUPU1iJA.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Dataset</figcaption></figure><h2 id="4e48" class="na lz it bd ma nb nc dn me nd ne dp mi ld nf ng mk lh nh ni mm ll nj nk mo nl bi translated">数据集可视化</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/dd00c0eb5a3bac25cb17cefd3ead1245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VXJg0kT_BSyC6ejXn4laGA.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Visualizing the Data with a scatterplot</figcaption></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Code used for Visualisation</figcaption></figure><p id="fd27" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，我们的数据点有两个潜在特征，“f1”和“f2”。数据点属于'+'类(红色)和'-'类(蓝色)。</p><h1 id="8359" class="ly lz it bd ma mb np md me mf nq mh mi jz nr ka mk kc ns kd mm kf nt kg mo mp bi translated">2.定义目的</h1><p id="5cee" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">我们的目的是为数据集的正负类定义一个理想的分布。到目前为止，我们还不知道它会是什么样子，它的概率密度函数会是怎样的，但是我们可以定义一些好的性质。</p><h2 id="38e9" class="na lz it bd ma nb nc dn me nd ne dp mi ld nf ng mk lh nh ni mm ll nj nk mo nl bi translated">理想分布的性质</h2><ol class=""><li id="5dd7" class="nu nv it kw b kx mq la mr ld nw lh nx ll ny lp nz oa ob oc bi translated">正类的分布应该使得任何数据点属于正类的概率应该是 1，而负类的概率应该是 0。</li><li id="8ee8" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">负类的分布应该使得任何数据点属于正类的概率应该是 0，负类的概率应该是 1。</li></ol><h1 id="0563" class="ly lz it bd ma mb np md me mf nq mh mi jz nr ka mk kc ns kd mm kf nt kg mo mp bi translated">3.如何估计上述理想分布</h1><p id="2ec8" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">现在好戏开始了。如何估计理想分布。这是一个开放式的问题，可以尝试许多技术。但是对于这个博客的帖子，我将保持事情简单，不会偏离原始主题太多，将 KL 散度应用于日常的机器学习问题。</p><h2 id="9c8d" class="na lz it bd ma nb nc dn me nd ne dp mi ld nf ng mk lh nh ni mm ll nj nk mo nl bi translated">高斯/正态分布救援</h2><p id="3d6a" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">估计分布的一种方法是使用高斯分布。我们将尝试为正类拟合一个高斯，为负类拟合另一个高斯。有可用的软件包可以为我们找到这些拟合的高斯函数的适当参数。但是，如果你有兴趣了解它是如何做到的，那么你可以在这里阅读更多关于它的<a class="ae lq" href="http://www.aishack.in/tutorials/expectation-maximization-gaussian-mixture-model-mixtures/" rel="noopener ugc nofollow" target="_blank">。一种叫做</a><a class="ae lq" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">期望最大化</a>的算法被用于此。也许，我会在另一篇博文中写下它。让我们使用 python 中可用的 GaussianMixture 包来拟合发行版。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/d08e587806a4f25cd812ac0e4f71d81a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uHSApwRdaaNXMrJahoz-BA.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">PDF of a multivariate Gaussian Distribution</figcaption></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Fitting the Distribution and visualizing the results</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/084e0657b88e6a8d0a2d87870e27b9ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*dhzY9WLtbyowQIH6awYJ5w.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Fitted Gaussians for the positive and negative class</figcaption></figure><p id="5914" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从视觉上看，在完成分配的任务时，分配看起来很好。一个高斯模型适合正类，另一个适合负类。接下来，我们将计算每个数据点属于正负类分布的概率。</p><h1 id="902f" class="ly lz it bd ma mb np md me mf nq mh mi jz nr ka mk kc ns kd mm kf nt kg mo mp bi translated">4.寻找每个数据点的概率(可选)</h1><p id="0844" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">在这一部分中，我们将看到，一旦完成了正类和负类的多元高斯拟合，如何计算每个数据点的最终概率。这将是一个数学密集型和可选的。它可以作为黑箱，得到最终的概率。但是万一，你有兴趣了解背后的数学，你可以按照章节 else 跳到下一个。</p><p id="70c4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于任何数据点' x ',属于分布概率由下式给出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/85da97c6c9ccee6f00f385cdf0497ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yblX_WVELhTPbEJqolx5RQ.png"/></div></div></figure><p id="e9e0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用上面的公式，我们可以找到可能性，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/7ae78f444a96bea74af4c88a079dd306.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*6uLv_U6lGaUl11lRhdoMaw.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Probability of datapoint given + distribution</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a02429fe761c713223bb0c14325ee651.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*4i6Mz19y8GbYUFiK8t__eQ.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Probability of datapoint given - distribution</figcaption></figure><p id="810d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来，我们可以找到类概率或先验，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/53f4db58785b8b4f0f1b3de57646dfc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GlzWla6G_-RqHMOPCNHRfA.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Probability of + distribution</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/9b56869ca06c3f0258aa77e5f42a430b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KN4IgIX0TYrP_KmcZ_KgeQ.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Probability of - distribution</figcaption></figure><p id="1471" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中 n 是数据点的总数。</p><p id="5622" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦我们有了可能性和先验，最后一步就是找到后验，即数据点的概率。我们可以用<a class="ae lq" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>来计算。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/fb67d3f76d70f0fc73387cd9a1cdfa78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ls77ZoXXkKTN2swN-JSLWA.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Probability of datapoint belonging to the + distribution</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/a908060375061d4c750d5c6728a8b9eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eog--NzTjPftH949GIfOHg.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Probability of datapoint belonging to the - distribution</figcaption></figure><p id="7350" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以利用上面的后验，求出每个数据点属于+ve 或-ve 分布的概率。</p><h1 id="6015" class="ly lz it bd ma mb np md me mf nq mh mi jz nr ka mk kc ns kd mm kf nt kg mo mp bi translated">5.评估拟合优度</h1><p id="ab36" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">现在，一旦我们拟合了分布，并且计算了每个数据点属于正负分布的概率。我们可以看到这个拟合的分布与我们的理想分布有多大的不同。</p><p id="ae52" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们如何检查呢？当然，使用我们最喜欢的度量标准，<a class="ae lq" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> KL 散度</a>(kull back–lei bler 散度)。</p><p id="3626" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">只是重申，KL 散度只是拟合分布和实际分布之间的差异，即交叉熵和熵之间的差异。还可以看出这两种分布有多大差异。</p><h2 id="215c" class="na lz it bd ma nb nc dn me nd ne dp mi ld nf ng mk lh nh ni mm ll nj nk mo nl bi translated">计算 KL 散度</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oq"><img src="../Images/3f3e92e5c632dd9c7d16e518239877f8.png" data-original-src="https://miro.medium.com/v2/format:webp/1*wzgJUCDsBgtleCIGmCMo5Q.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">KL Divergence</figcaption></figure><p id="e444" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中 H(p，q)是交叉熵，H(p)是系统的熵，其中<em class="or"> pᵢ </em>是第<em class="or">个</em>事件的实际概率，q <em class="or"> ᵢ </em>是第 I 个事件的估计概率。</p><h2 id="bb80" class="na lz it bd ma nb nc dn me nd ne dp mi ld nf ng mk lh nh ni mm ll nj nk mo nl bi translated">重申我们理想分布的性质</h2><ol class=""><li id="5f22" class="nu nv it kw b kx mq la mr ld nw lh nx ll ny lp nz oa ob oc bi translated">正类的分布应该使得任何数据点属于正类的概率应该是 1，而负类的概率应该是 0。</li><li id="3e2a" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">负类的分布应该使得任何数据点属于正类的概率应该是 0，负类的概率应该是 1。</li></ol><p id="cf91" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="or"> pᵢ </em>是来自理想分布特性的事件的实际概率。q <em class="or"> ᵢ </em>是事件的估计概率，使用拟合/估计分布计算。我们用这些概率来寻找 KL 散度。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Calculating the probabilities and KL Divergence</figcaption></figure><p id="5215" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">KL 散度出来是<strong class="kw iu"> 5.74 </strong>，这表示拟合的分布非常接近理想值。但是我们能做得更好吗？</p><h1 id="5f21" class="ly lz it bd ma mb np md me mf nq mh mi jz nr ka mk kc ns kd mm kf nt kg mo mp bi translated">6.努力接近理想的分布</h1><p id="53b2" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">每类一条高斯曲线可能不足以模拟整个分布。我们可以拟合高斯混合分布，看看结果。有多少高斯人？直到我们的 KL 散度接近 0，即理想分布和拟合分布之间没有差异或差异最小。让我们试试那个。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Try fitting more than one Gaussian per Class</figcaption></figure><h2 id="497e" class="na lz it bd ma nb nc dn me nd ne dp mi ld nf ng mk lh nh ni mm ll nj nk mo nl bi translated">结果</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/5f8340372986f3ee4a3a0991ed4dddf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*vmTHcJ2QxzriTHhVOIjhPw.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">KL Divergence on increasing the number of Gaussians per Class</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/5ff29221b1e892c9af25e527a22a9b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*rjnCGlTpBnEMEzgFgdEmWg.jpeg"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">1 Gaussian per Class, KL Divergence = 5.74</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/c7c95cedeb069f64f56a5df7ce8176ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*A3aY003xZvvjpWC7QkTL4g.jpeg"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">2 Gaussian per Class, KL Divergence = 3.18</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/28ba344e088a5aa29511a69deaa03900.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*tQKDBc0xgBrizgszp3Dwsg.jpeg"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">3 Gaussian per Class, KL Divergence = 1.81</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/5abea81b1c5a2d92c5e620651767dc80.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*y3mqqSndluJSziyQSVY8qA.jpeg"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">4 Gaussian per Class, KL Divergence = 0.77</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/3f8f3df92b94bf848f2b5fe7fc11ba24.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*XBuTs0Mb0vGMzMgm9C6Fyw.jpeg"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">5 Gaussian per Class, KL Divergence = 0.20</figcaption></figure><h2 id="8992" class="na lz it bd ma nb nc dn me nd ne dp mi ld nf ng mk lh nh ni mm ll nj nk mo nl bi translated">外卖食品</h2><p id="94d3" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">每类四个高斯分布就足够了，并且非常接近地模拟了 KL 散度几乎为 0 的理想分布。下面的情节也清楚地表明了这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/5abea81b1c5a2d92c5e620651767dc80.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*y3mqqSndluJSziyQSVY8qA.jpeg"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">4 Gaussian per Class, KL Divergence approaches 0</figcaption></figure><h1 id="4186" class="ly lz it bd ma mb np md me mf nq mh mi jz nr ka mk kc ns kd mm kf nt kg mo mp bi translated">7.结论</h1><p id="bad9" class="pw-post-body-paragraph ku kv it kw b kx mq ju kz la mr jx lc ld ms lf lg lh mt lj lk ll mu ln lo lp im bi translated">我们选择了一个包含两个不同类的数据集。我们想找到这两个类的基本分布。所以，我们首先定义了什么是好的，有理想分布的性质，并且能够非常接近地模拟理想分布。这样，我们总是可以尝试找到数据的基本分布，并使用 KL 散度来查看拟合度。希望它为这个主题带来了所需的清晰度，并为其在日常机器学习工作中的应用带来了新的视野。</p><p id="cc2f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="or">我的 Youtube 频道获取更多内容:</em> </strong></p><div class="ot ou gp gr ov ow"><a href="https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd iu gy z fp pb fr fs pc fu fw is bi translated">阿布舍克·蒙戈利</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">嗨，伙计们，欢迎来到频道。该频道旨在涵盖各种主题，从机器学习，数据科学…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">www.youtube.com</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk ks ow"/></div></div></a></div><p id="68fa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">写一篇清晰易懂的好文章需要很多努力。我会继续努力做好我的工作。在<a class="ae lq" href="https://medium.com/@mungoliabhishek81" rel="noopener"> <strong class="kw iu">中</strong> </a>关注我，查看我以前的帖子。我欢迎反馈和建设性的批评。任务的完整代码可以从<a class="ae lq" href="https://github.com/samread81/KL-Divergence" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><h1 id="7074" class="ly lz it bd ma mb np md me mf nq mh mi jz nr ka mk kc ns kd mm kf nt kg mo mp bi translated">8.参考</h1><ol class=""><li id="2c3d" class="nu nv it kw b kx mq la mr ld nw lh nx ll ny lp nz oa ob oc bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Probability_density_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Probability_density_function</a></li><li id="855e" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">https://en.wikipedia.org/wiki/Entropy<a class="ae lq" href="https://en.wikipedia.org/wiki/Entropy" rel="noopener ugc nofollow" target="_blank"/></li><li id="1ae4" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">【https://en.wikipedia.org/wiki/Cross_entropy T4】</li><li id="aea4" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/kull back % E2 % 80% 93 lei bler _ divergence</a></li><li id="30bd" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><a class="ae lq" href="https://scikit-learn.org/stable/modules/mixture.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/mixture.html</a></li><li id="3be5" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e">https://towards data science . com/part-I-a-new-tool-to-your-toolkit-KL-divergence-5b 887 b5 b420 e</a></li><li id="5f43" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/demystifying-entropy-f2c3221e2550">https://towards data science . com/demystifying-entropy-f2c 3221 e 2550</a></li><li id="a5a5" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/demystifying-cross-entropy-e80e3ad54a8">https://towards data science . com/demystifying-cross-entropy-e 80 e 3 ad 54 a 8</a></li><li id="b85e" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><a class="ae lq" href="http://www.aishack.in/tutorials/expectation-maximization-gaussian-mixture-model-mixtures/" rel="noopener ugc nofollow" target="_blank">http://www . ai shack . in/tutorials/expect-maximization-Gaussian-mixture-model-mixtures/</a></li><li id="5546" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Expectation % E2 % 80% 93 最大化 _ 算法</a></li><li id="813c" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><a class="ae lq" href="https://brilliant.org/wiki/gaussian-mixture-model/" rel="noopener ugc nofollow" target="_blank">https://brilliant.org/wiki/gaussian-mixture-model/</a></li><li id="d4ae" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Bayes%27_theorem</a></li><li id="bb5b" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/kull back % E2 % 80% 93 lei bler _ divergence</a></li></ol></div></div>    
</body>
</html>