<html>
<head>
<title>Inference using EM algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 EM 算法的推理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/inference-using-em-algorithm-d71cccb647bc?source=collection_archive---------10-----------------------#2019-02-11">https://towardsdatascience.com/inference-using-em-algorithm-d71cccb647bc?source=collection_archive---------10-----------------------#2019-02-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7a9c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深入了解 EM 算法的魔力，并开始训练您自己的图形模型</h2></div><h1 id="3985" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="5287" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这篇文章的目标是解释统计分析中一个强大的算法:期望最大化(EM)算法。它之所以强大，是因为它有能力处理缺失数据和未观察到的特性，这些用例在许多现实应用程序中经常出现。</p><blockquote class="lt"><p id="2649" class="lu lv iq bd lw lx ly lz ma mb mc ls dk translated">完成这篇文章后，你将能够理解许多使用概率图形模型解决有趣问题的研究论文，并且获得自己开发和训练这种模型的能力。</p></blockquote><h2 id="6d5c" class="md kg iq bd kh me mf dn kl mg mh dp kp lg mi mj kr lk mk ml kt lo mm mn kv mo bi translated">先决条件</h2><ul class=""><li id="dd04" class="mp mq iq kz b la lb ld le lg mr lk ms lo mt ls mu mv mw mx bi translated">耐心(因为这篇文章很详细，并且深入数学概念)。</li><li id="acf5" class="mp mq iq kz b la my ld mz lg na lk nb lo nc ls mu mv mw mx bi translated">基本概率概念。</li><li id="943b" class="mp mq iq kz b la my ld mz lg na lk nb lo nc ls mu mv mw mx bi translated">逻辑回归。</li><li id="5f1c" class="mp mq iq kz b la my ld mz lg na lk nb lo nc ls mu mv mw mx bi translated">迭代优化技术，如梯度上升。</li></ul><h1 id="9540" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">观察特征的情况</h1><p id="68d4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在深入研究复杂用例之前，先了解简单用例中的参数是如何估计的会有所帮助。因此，我们首先考虑我们的数据不包含任何缺失值并且我们的模型没有任何潜在特征的情况。为此，我们将借助逻辑回归模型。</p><p id="8755" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">假设我们有一个数据集，每个数据点由一个<strong class="kz ir"> <em class="ni"> d </em> </strong>维特征向量<strong class="kz ir"> <em class="ni"> X </em> </strong> <em class="ni"> </em>和一个相关联的目标变量<strong class="kz ir"><em class="ni">Y</em></strong><em class="ni"/>∈{ 0，1}组成。该模型的图示如下:</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nj"><img src="../Images/2c7ecbac2ab2a537222dd58c2618acfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U9Rbn8TXnrjser4b0GgvNw@2x.png"/></div></div></figure><p id="1a28" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">众所周知，逻辑回归中目标变量的预测概率由如下 sigmoid 函数给出:</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nv"><img src="../Images/838d2a36159790cfecd567f09ba72860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ko_LPZHSYLyvn09gbFRhyQ@2x.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">equation 1</strong></figcaption></figure><p id="f60a" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">其中<strong class="kz ir"> <em class="ni"> w </em> </strong>为待估计的权重向量。一旦我们估计了参数<strong class="kz ir"><em class="ni">【w】</em></strong>、<strong class="kz ir"> <em class="ni"> </em> </strong>，我们就可以根据<strong class="kz ir"><em class="ni">【Y】</em></strong>= 0 或<strong class="kz ir"> <em class="ni"> Y </em> </strong> =1 是否根据<code class="fe ob oc od oe b">equation 1</code>获得更大的概率来产生未观测数据点的输出。然而，这里的核心问题是:我们怎样才能逼近<strong class="kz ir"> <em class="ni"> w </em> </strong>从而产生准确的预测？为此，我们将使用最大似然法。</p><blockquote class="of og oh"><p id="56cb" class="kx ky ni kz b la nd jr lc ld ne ju lf oi nf li lj oj ng lm ln ok nh lq lr ls ij bi translated">最大似然法的前提很简单:找到使观测数据的似然性(或概率)最大化的参数<strong class="kz ir"> w <em class="iq"> </em> </strong>。</p></blockquote><p id="3876" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">为了<a class="ae ol" href="https://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution" rel="noopener ugc nofollow" target="_blank">数学上的便利</a>，我们将考虑最大化可能性的<strong class="kz ir"> <em class="ni">对数</em> </strong>。由<strong class="kz ir"> <em class="ni"> w </em> </strong>参数化，对数似然可以写成:</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi om"><img src="../Images/963164900184567ae5b127d78fa84689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s3tAvNT8BSm4RJ_ubxVDKw@2x.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">equation 2</figcaption></figure><p id="42e5" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">这里，在数据样本独立同分布的假设下(所谓的<a class="ae ol" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank"> <em class="ni"> i.i.d .假设</em> </a>)，观察数据的似然性被表示为各个数据点的似然性的乘积。我们还利用了这样一个性质:一个<strong class="kz ir"> <em class="ni">对数</em> </strong>表达式中各项的乘积相当于单个项的<strong class="kz ir"> <em class="ni">对数</em> </strong>的总和。将<code class="fe ob oc od oe b">equation 2</code>中的<code class="fe ob oc od oe b">equation 1</code>替换为:</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi on"><img src="../Images/60dec80b5a42a4e2de960f0c02283e16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uw7EZIfONaWvdHnB3ncj2A@2x.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">equation 3</figcaption></figure><p id="2348" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">这里我们用到了这样的知识:当<strong class="kz ir"> <em class="ni"> y_i </em> </strong> =1，else<strong class="kz ir"><em class="ni">σ</em>(<em class="ni">w . X _ I</em>)w . X _ I</strong></p><blockquote class="of og oh"><p id="af31" class="kx ky ni kz b la nd jr lc ld ne ju lf oi nf li lj oj ng lm ln ok nh lq lr ls ij bi translated">在这一点上，我们应该注意到 log-likelihood，<strong class="kz ir">L<em class="iq">(</em>w<em class="iq">)</em></strong><em class="iq">，</em> <strong class="kz ir"> </strong>可以方便地分解为每个实例的形式，并且不同参数之间没有耦合，这大大简化了优化。我们将在后面看到，这在许多现实场景中可能无法实现。</p></blockquote><p id="2d36" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">由于<strong class="kz ir"><em class="ni">L</em>(<em class="ni">w</em>)</strong>是<strong class="kz ir"> <em class="ni"> w </em> </strong>的一个函数，所以我们对<code class="fe ob oc od oe b">equation 3</code>没有任何封闭形式的解。因此，我们将不得不使用<em class="ni">迭代优化方法</em>像梯度上升找到<strong class="kz ir"> <em class="ni"> w </em> </strong>。梯度上升法的更新如下所示:</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oo"><img src="../Images/ccbaf6e498792b0d1cab35992f34eb6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xw4LWSqh3CQ7CmiXr7jODg@2x.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">equation 4</figcaption></figure><p id="3e2e" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">其中<strong class="kz ir"> <em class="ni"> η </em> </strong>为学习率。我们重复<code class="fe ob oc od oe b">equation 4</code>中的过程，直到收敛，最后得到的<strong class="kz ir"><em class="ni"/></strong>称为<em class="ni">最大似然估计</em>。</p><h1 id="844e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">潜在特征的情况</h1><p id="9135" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在我们准备深入更复杂和更现实的用例。在大多数现实场景中，数据集中通常会有缺失值，或者选择具有与数据中观察到的特征相关的潜在(未观察到的)特征的复杂模型。</p><blockquote class="of og oh"><p id="7a40" class="kx ky ni kz b la nd jr lc ld ne ju lf oi nf li lj oj ng lm ln ok nh lq lr ls ij bi translated">一个使用潜在特征的模型是<a class="ae ol" href="https://en.wikipedia.org/wiki/Hidden_Markov_model" rel="noopener ugc nofollow" target="_blank">隐马尔可夫模型</a>。它被广泛使用，并具有一系列真实世界的应用，如语音识别、手写识别、手势识别、词性标注、乐谱跟踪、时间序列分析等。</p></blockquote><p id="79b7" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">然而，问题是:<em class="ni">具有潜在特征对参数的估计有影响吗</em>？原来，<em class="ni">是的</em>。如果涉及潜在特征(或缺失数据)，估计模型参数确实有点棘手。我们来看看为什么。</p><h2 id="a0e8" class="md kg iq bd kh me op dn kl mg oq dp kp lg or mj kr lk os ml kt lo ot mn kv mo bi translated">问题</h2><p id="e2f7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">设<strong class="kz ir"> <em class="ni"> V </em> </strong>为观察变量集合，<strong class="kz ir"> <em class="ni"> Z </em> </strong>为潜在变量集合，<strong class="kz ir"> <em class="ni"> θ </em> </strong>为模型参数集合。如果我们考虑参数估计的最大似然法，我们的目标 1 函数将是:</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ou"><img src="../Images/cb6cf21cd880a5836976b1e58bfeab7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p8AB-mw8CJRcC6kVZjuTzw@2x.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">equation 5</figcaption></figure><blockquote class="of og oh"><p id="d259" class="kx ky ni kz b la nd jr lc ld ne ju lf oi nf li lj oj ng lm ln ok nh lq lr ls ij bi translated">比较<code class="fe ob oc od oe b">equation 5</code>和<code class="fe ob oc od oe b">equation 2</code>，我们可以看到，在后一种情况下，由于<strong class="kz ir">测井</strong>内的求和，参数是耦合的。这使得使用梯度上升的优化(或一般的任何迭代优化技术)变得难以处理。这意味着许多现实场景需要更强大的技术来推断参数。</p></blockquote><h1 id="53b7" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">EM 算法拯救世界</h1><p id="60a7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">谢天谢地，研究人员已经想出了这样一个强大的技术，它被称为<strong class="kz ir">期望最大化</strong> <strong class="kz ir"> (EM)算法</strong>。它利用了这样一个事实，即当我们知道<strong class="kz ir"><em class="ni">【Z】</em></strong>的值时，优化完整数据对数似然<strong class="kz ir"> <em class="ni"> P </em> ( <em class="ni"> V，Z | θ </em> )* </strong>要容易得多(因此，从<strong class="kz ir"> <em class="ni"> log </em> </strong>内部移除求和)。</p><blockquote class="of og oh"><p id="291f" class="kx ky ni kz b la nd jr lc ld ne ju lf oi nf li lj oj ng lm ln ok nh lq lr ls ij bi translated"><strong class="kz ir"> <em class="iq"> * </em> </strong>向前看，为了标记简单起见，我们将<strong class="kz ir"> Y <em class="iq"> </em> </strong>视为<strong class="kz ir"> V <em class="iq"> </em> </strong>的一部分。</p></blockquote><p id="ed58" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">然而，由于知道<strong class="kz ir"> <em class="ni"> Z </em> </strong>的值的唯一方法是通过后验<strong class="kz ir"> <em class="ni"> P </em> ( <em class="ni"> Z | V，θ </em> ) </strong>，我们转而考虑潜变量后验分布下的完全数据对数似然的期望值。这个寻找期望值的步骤被称为<strong class="kz ir">电子步骤</strong>。在随后的<strong class="kz ir"> M 步</strong>中，我们最大化这个期望来优化<strong class="kz ir"> <em class="ni"> θ </em> </strong>。</p><p id="73a5" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">形式上，EM 算法可以写成:</p><pre class="nk nl nm nn gt ov oe ow ox aw oy bi"><span id="ea70" class="md kg iq oe b gy oz pa l pb pc">1. Choose initial setting for the parameters <strong class="oe ir"><em class="ni">θ^old</em></strong></span><span id="af1c" class="md kg iq oe b gy pd pa l pb pc">2. <strong class="oe ir">E Step</strong> Evaluate <strong class="oe ir"><em class="ni">P(Z | V, θ^old)</em></strong></span><span id="e1d1" class="md kg iq oe b gy pd pa l pb pc">3. <strong class="oe ir">M step</strong> Evaluate <strong class="oe ir"><em class="ni">θ^new</em></strong> given by</span></pre><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pe"><img src="../Images/7ef24883b76d82fa57093991efd17e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D98AZcPsLNDzevf_6Hoq6A@2x.png"/></div></div></figure><pre class="nk nl nm nn gt ov oe ow ox aw oy bi"><span id="8355" class="md kg iq oe b gy oz pa l pb pc">4. Check for convergence of log likelihood or parameter values. If not converged, then <strong class="oe ir"><em class="ni">θ^old</em></strong>=<strong class="oe ir"><em class="ni">θ^new</em></strong> and return to the E step.</span></pre><h1 id="ed5e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">深入新兴市场</h1><p id="bf23" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在深入研究之前，首先，我们将推导一个在解释 E 和 M 步骤时会派上用场的属性。</p><p id="c985" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">让我们考虑潜在变量的分布<strong class="kz ir"><em class="ni">q</em>(<em class="ni">Z</em>)</strong>。独立于<strong class="kz ir"><em class="ni">q</em>(<em class="ni">Z</em>)</strong>的选择，我们可以按以下方式分解观察数据的可能性:</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pf"><img src="../Images/213deabe768e7a7fb7f9b4885840e2fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QCr9SjskKR15gK83HHfolA@2x.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">equation 6</figcaption></figure><blockquote class="of og oh"><p id="7fe3" class="kx ky ni kz b la nd jr lc ld ne ju lf oi nf li lj oj ng lm ln ok nh lq lr ls ij bi translated">*第一步，我们分别应用概率边际化概念和贝叶斯定理。</p><p id="f9fc" class="kx ky ni kz b la nd jr lc ld ne ju lf oi nf li lj oj ng lm ln ok nh lq lr ls ij bi translated">*第二步，我们将<strong class="kz ir">日志</strong>中的<strong class="kz ir">q<em class="iq">(</em>Z<em class="iq">)</em></strong>相乘，并分别应用“乘以<strong class="kz ir">日志</strong>中的项相当于项的<strong class="kz ir">日志</strong>的和”属性。</p></blockquote><p id="7146" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated"><code class="fe ob oc od oe b">equation 6</code>中的第二项是两个分布之间众所周知的距离度量，称为<a class="ae ol" href="https://rishabhmisra.github.io/Machine-Learning-Glossary/#KLD" rel="noopener ugc nofollow" target="_blank"> Kullback-Leibler 散度</a>。</p><p id="7e33" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">此时，我们应该仔细研究一下<code class="fe ob oc od oe b">equation 6</code>的形式。第一项包含在<strong class="kz ir"> <em class="ni"> V </em> </strong>和<strong class="kz ir"> <em class="ni"> Z </em> </strong>上的联合分布，而第二项包含给定<strong class="kz ir"> <em class="ni"> V </em> </strong>的<strong class="kz ir"> <em class="ni"> Z </em> </strong>的条件分布。KL 散度的一个性质是它总是非负的。利用<code class="fe ob oc od oe b">equation 6</code>中的这个性质，我们可以推导出<strong class="kz ir"> <em class="ni"> L </em> ( <em class="ni"> q，θ</em>)<em class="ni"/></strong>≤<strong class="kz ir"><em class="ni">ln p</em>(<em class="ni">V |θ</em>)</strong>。</p><blockquote class="lt"><p id="52f3" class="lu lv iq bd lw lx ly lz ma mb mc ls dk translated">这意味着<strong class="ak"> L <em class="pg"> ( </em> q，θ <em class="pg"> ) </em> </strong>作为观察数据的对数似然的下界。</p></blockquote><p id="fe09" class="pw-post-body-paragraph kx ky iq kz b la ph jr lc ld pi ju lf lg pj li lj lk pk lm ln lo pl lq lr ls ij bi translated">这个观察，很快，将有助于证明 EM 算法确实最大化了对数似然。</p><h2 id="2ba0" class="md kg iq bd kh me op dn kl mg oq dp kp lg or mj kr lk os ml kt lo ot mn kv mo bi translated">电子步骤</h2><p id="fbe6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">假设参数向量<strong class="kz ir"> <em class="ni"> θ </em> </strong>的初始值为<code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old</em></strong></code><strong class="kz ir"><em class="ni">——(步骤 1) </em> </strong>。记住由<code class="fe ob oc od oe b">equation 6</code>给出的关系，E 步试图最大化下限<strong class="kz ir"><em class="ni">L</em>(<em class="ni"/></strong><code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old</em></strong></code><strong class="kz ir">)</strong>相对于<strong class="kz ir"> <em class="ni"> q </em> </strong>，同时保持<code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old</em></strong></code>固定。注意到<strong class="kz ir"><em class="ni">ln p</em>(<em class="ni">V |θ</em>)</strong>的值不依赖于<strong class="kz ir"><em class="ni">q</em>(<em class="ni">Z</em>)</strong>，因此<strong class="kz ir"> <em class="ni"> L </em> ( <em class="ni"> q，</em> </strong> <code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old</em></strong></code> <strong class="kz ir"> ) </strong>的最大值将在 KL 背离消失时出现 或者换句话说当<strong class="kz ir"><em class="ni">q</em>(<em class="ni">Z</em>)</strong>等于后验分布<strong class="kz ir"> <em class="ni"> p(Z | V，</em></strong><code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old</em></strong></code><strong class="kz ir"><em class="ni">)</em></strong>(因为<strong class="kz ir"> <em class="ni"> ln </em> </strong> 1 求值为 0)。</p><p id="191c" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">因此，E-step 涉及评估<strong class="kz ir"> <em class="ni"> p(Z | V，</em></strong><code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old</em></strong></code><strong class="kz ir"><em class="ni">)——(Step 2)</em></strong></p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pm"><img src="../Images/e9fb17dd02f969988a9296816d5f4415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jamMOiu7pyeu7Myk.JPG"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Illustration of E-step. [1]</figcaption></figure><h2 id="4a48" class="md kg iq bd kh me op dn kl mg oq dp kp lg or mj kr lk os ml kt lo ot mn kv mo bi translated">m 步</h2><p id="3513" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在该步骤中，分配<strong class="kz ir"><em class="ni">q</em>(<em class="ni">Z</em>)</strong>保持固定。如果我们把 E-step 中的<strong class="kz ir"><em class="ni">q</em>(<em class="ni">Z</em>)</strong>=<strong class="kz ir"><em class="ni">p(Z | V，</em></strong><code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old</em></strong></code><strong class="kz ir"><em class="ni">)</em></strong>代入表达式中的<strong class="kz ir"> <em class="ni"> L </em> ( <em class="ni"> q，θ </em> ) </strong> ( <code class="fe ob oc od oe b">equation 6</code>，我们看到，下界</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pn"><img src="../Images/999137cfafee389f2cf6331016930e94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9VeYfdKRnm1h0nsvu9nTNg@2x.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">equation 7</figcaption></figure><p id="364d" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">其中该常数仅仅是<strong class="kz ir"> <em class="ni"> q </em> </strong>分布的负熵，因此与<strong class="kz ir"> <em class="ni"> θ </em> </strong>无关。</p><p id="1b42" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">所以，在 M 步中，我们最大化下界<strong class="kz ir"> <em class="ni"> L </em> ( <em class="ni"> q，θ </em> ) </strong>相对于<strong class="kz ir"> <em class="ni"> θ </em> </strong>给出一些新值<code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^new</em></strong></code> <strong class="kz ir"> <em class="ni">。</em> </strong>这将引起下界<strong class="kz ir"> <em class="ni"> L </em> ( <em class="ni"> q，θ </em> ) </strong>增大，必然引起相应的对数似然函数增大。<strong class="kz ir"><em class="ni">——</em></strong>(第三步)</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi po"><img src="../Images/6d739e981c0ed178c800104e327e3bf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ig-dt5tIKgN0FQ4C.JPG"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Illustration of M-step. [1]</figcaption></figure><p id="9e27" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">由于分布<strong class="kz ir"> <em class="ni"> q </em> </strong>在 M 步期间保持固定，所以它将不等于新的后验分布<strong class="kz ir"> <em class="ni"> p(Z | V，</em></strong><code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^new</em></strong></code><strong class="kz ir"><em class="ni">)</em></strong>)，因此将存在非零 KL 散度。因此，我们再次重复 E 和 M 步骤，直到收敛。<strong class="kz ir"><em class="ni">——</em></strong>(第四步)</p><h2 id="d19d" class="md kg iq bd kh me op dn kl mg oq dp kp lg or mj kr lk os ml kt lo ot mn kv mo bi translated">把所有的放在一起</h2><p id="d663" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我知道一下子很难消化。因此，我将尝试在下图的帮助下总结讨论，这应该有助于将这些点联系起来。</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pp"><img src="../Images/70172043c303d9e8f3ee551262a40912.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OziyuPrVfZ_qBc33lNaeFA@2x.jpeg"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">How E and M step maximize the likelihood. [1]</figcaption></figure><p id="e958" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated"><strong class="kz ir">红色</strong>曲线描绘的是不完全数据对数似然性，<strong class="kz ir"><em class="ni">ln p</em>(<em class="ni">V |θ</em>)</strong>，我们希望最大化。在第一个 E 步骤中，我们从一些初始参数值<code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old.</em></strong></code>开始，我们评估潜在变量的后验分布，<strong class="kz ir"><em class="ni">【p(Z | V，</em></strong><code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old</em></strong></code><strong class="kz ir"><em class="ni">)</em></strong>，这产生了一个下界<strong class="kz ir"><em class="ni">L</em>(<em class="ni"/></strong><code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old</em></strong></code>【T71)，其值等于在 M 步中，界限被最大化，给出值<code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^new</em></strong></code>，这给出了比<code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^old</em></strong></code>更大的对数似然值。随后的 E 步骤构建一个在<code class="fe ob oc od oe b"><strong class="kz ir"><em class="ni">θ^new</em></strong></code>相切的边界，如绿色<strong class="kz ir">曲线</strong>所示。</p><blockquote class="of og oh"><p id="d951" class="kx ky ni kz b la nd jr lc ld ne ju lf oi nf li lj oj ng lm ln ok nh lq lr ls ij bi translated">在每一步中，我们看到所获得的参数增加了对数似然，并且该过程一直持续到收敛。</p></blockquote><h1 id="96c4" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结束语</h1><p id="bdf5" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这结束了一篇关于 EM 算法的相当激烈的数学文章。然而，深入理解这个算法是一个很好的时间投资，如果您想开发自己的图形模型来解决具有挑战性的问题，它确实证明是有帮助的。如果你正在寻找一个示例问题和代码，你可以参考我的这个<a class="ae ol" href="https://github.com/rishabhmisra/T-JMARS" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>,在那里我们解决了将方面、评级和情绪与评级预测任务的时间动态联合建模的挑战。</p><h2 id="c3bc" class="md kg iq bd kh me op dn kl mg oq dp kp lg or mj kr lk os ml kt lo ot mn kv mo bi translated">参考</h2><p id="2374" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[1]纳斯拉巴迪 NM。<strong class="kz ir">模式识别和机器学习</strong>。电子成像杂志。2007 年十月；16(4):049901.</p></div><div class="ab cl pq pr hu ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="ij ik il im in"><p id="209d" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">原贴@<a class="ae ol" href="https://rishabhmisra.github.io/Maximum-Likelihood-Estimates-Motivation-For-EM-Algorithm/" rel="noopener ugc nofollow" target="_blank">https://rishabhmisra . github . io/Maximum-Likelihood-Estimates-Motivation-For-EM-Algorithm/</a></p></div><div class="ab cl pq pr hu ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="ij ik il im in"><h2 id="0322" class="md kg iq bd kh me op dn kl mg oq dp kp lg or mj kr lk os ml kt lo ot mn kv mo bi translated"><strong class="ak">引文</strong></h2><p id="8b53" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果您在研究中使用了此处介绍的作品，请使用以下格式之一引用它:</p><p id="3b13" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated"><strong class="kz ir">文本格式:</strong></p><p id="d560" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">米斯拉里沙卜。"使用 EM 算法进行推理."<em class="ni">中</em>，<em class="ni">向数据科学</em> (2019)。</p><p id="58c6" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated"><strong class="kz ir"> BibTex 格式:</strong></p><p id="7503" class="pw-post-body-paragraph kx ky iq kz b la nd jr lc ld ne ju lf lg nf li lj lk ng lm ln lo nh lq lr ls ij bi translated">@misc{misrainference2019，<br/> author={Misra，Rishabh}，<br/>title = {推理使用 EM 算法}，<br/>URL = { https://Medium . com/forward-Data-Science/推理使用-em-algorithm-d71cccb647bc}，<br/> journal={Medium}，<br/>publisher = { forward Data Science }，<br/> year={2019}，<br/> month={Feb} <br/> }</p></div></div>    
</body>
</html>