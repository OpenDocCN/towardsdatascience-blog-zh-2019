# 数据科学生产化:规模

> 原文：<https://towardsdatascience.com/data-science-productionization-scale-1884ca4e969e?source=collection_archive---------26----------------------->

## [DS 生产化](https://towardsdatascience.com/tagged/ds-productionization)

## 你可以等到你对意外感到惊讶，或者你可以建立系统来限制意外对你的伤害程度。

![](img/556abe3ecbc5ca23c662918021daca37.png)

这是关于数据科学生产的五部分系列的第四部分:

1.  [数据科学的“生产化”意味着什么？](/what-does-it-mean-to-productionize-data-science-82e2e78f044c)
2.  [便携性](/data-science-productionization-portability-f5d1a1f2f45b)
3.  [维护](/data-science-productionization-maintenance-af59ce6c958)
4.  [刻度](/data-science-productionization-scale-1884ca4e969e)
5.  [信任](/data-science-productionization-trust-b37f10b8f426)

让我们再看一遍单词规范化代码(来自我的前两篇文章)。

我们之前写的代码对于一个单词来说很好。它甚至可以很好地处理几千个单词。但是如果我们需要规范化数百万或数十亿个单词，这将花费比我们可能想花费的更多的时间。这就是可伸缩性的问题:您必须处理的数据量通常与处理数据所需的时间有直接关系。企业是有期限的——如果你不能在你需要的时间框架内得到结果，那么这些结果对你没有任何好处。

在示例中，我们通过并行化来扩展代码。有很多方法可以做到这一点——我使用 Spark 是因为它是目前大规模处理数据最流行的方法之一。我首先创建一个 DataFrame，它或多或少是 Python 版本的电子表格。它有两列——id 和 words——每一列都有三个值。然后，我将数据帧转换为 Spark 数据帧，这样就可以分别并行处理每条记录。然后，我将我们的“normalize_word”函数转换为 Spark“用户定义函数”(UDF)，这允许将函数分配给不同的虚拟机来处理数据。出于同样的原因，我将我们的停靠点列表转换成一个 Spark 数组。

一旦我将所有不可伸缩的代码转换成可伸缩的代码，我就可以使用 UDF 处理数据帧了。这将数据帧的每一行输出给一个“执行程序”,执行程序使用提供的函数处理记录，并给出结果:

```
**+---+--------+
|ids|   norms|
+---+--------+
|  1|      rr|
|  2|houspous|
|  3|    shzm|
+---+--------+**
```

Spark 允许我调整发送给每个执行程序的记录数量，以及我想要(或负担得起)多少个执行程序。我可以决定我希望每个执行者拥有多少内存(对于上面的例子，我需要很少的内存，但是如果我分发整个机器学习模型，有时我需要很多)。

当我们从思考代码转移到思考代码系统时，我认为可伸缩性归结为三个主要部分:资源管理、集成和实现创造力。

## **资源管理**

我发现从内存、计算能力和磁盘空间的角度来考虑资源是很有用的。我不认为这些事情有任何类比不会因为过于简单而给我带来麻烦，但我要冒险一试:如果你试图从一个州向另一个州运送大量货物，你的卡车的数量和大小将是你的内存，你的卡车的速度和马力将是你的计算能力，你的仓库的大小和数量将是你的磁盘空间。每个部分都制约着其他部分。如果货物到达目的地后没有地方存放，那么你有多少辆卡车或者这些卡车开得多快都没有用。如果你的货物装不下你的卡车，你的仓库再大也没用。如果卡车没有燃料，你的仓库和卡车的大小都不重要。你明白了。

尽管我们实际上只管理三种基本的资源，但是管理每一种资源的方法有很多很多。想想一辆卡车可能发生故障的所有方式。有很多方法可以打乱你的内存使用。对于 spark 作业，我通常必须使用以下参数:

*   我希望可用于运行作业的执行者总数
*   每个执行器上的内核数量(这允许额外的并行化)
*   每个执行器上的内存量
*   为了在执行器上移动数据，我愿意承担的内存开销
*   我愿意移入和移出执行者的最大数据量
*   当将数据集连接在一起或聚合时，我希望将数据拆分成的“无序”分区的数量

还有 10-20 个我更喜欢的默认参数，它们几乎适用于任何工作。还有几十个其他参数我甚至不知道，因为我从来没有必要去弄乱它们。可伸缩性与获得更多资源关系不大。主要是学习如何明智地分配可用资源。

## **集成**

有时，流程中的瓶颈并不是技术资源分配的结果。换句话说，有时候人类会把你的东西弄乱。我见过的一些最常见的方式是:

*   **需求的意外增长。**你习惯于在被要求时给出结果。一开始你每周都会收到一些请求。现在你每小时都会收到一些请求。
*   **供应商延误。直到公司的另一个部门给你他们最新的数据，你才能运行你的流程。他们忘记发送邮件(或者他们降低了工作优先级，或者通常发送邮件的人生病了，等等。)你被阻止，直到他们给你他们的拼图。**
*   **随时间自然降解。**你训练一个模型，每个人对结果都很满意。随着时间的推移，人们开始变得不那么快乐。你把旧的模型拿出来重新评估，发现它的性能远不如投入使用时那么好。所以几个月(或几年)以来，你一直在糟糕的预测基础上运作。

所有这些问题的解决方案是，首先要问一个人是否真的需要参与进来。每次请求报告时，单个分析师是否需要手动滚动每个报告？财务部门的一个人真的需要把来自几个系统的数据汇编成 Excel 报表，然后通过电子邮件发给你吗？你真的需要在你的日程表上安排一个老型号的重新评估吗？所有这些事情都可以部分或全部自动化。有几种常见的方法可以做到这一点:

*   **数据仓库**。厌倦了不得不向别人要你的数据？将所有数据存储在同一个地方。
*   **原料药**。无法在一个地方获取数据？教一台计算机去要求它(并教其他计算机去递送它)，这样递送就可以按照设定的时间表进行。
*   **仪表盘。厌倦了回应数据请求？把它挂在网站上，自动更新，让它成为自助服务。**

集成允许您更明确、更有效地管理不受您直接控制的资源。它们还允许您获得警报和报告的好处。

## **实现创意**

通常，可伸缩性更多地与您如何考虑问题有关，而不是与您如何管理技术资源和合作关系有关。让我参考我自己工作中的一个例子来解释我的意思(我已经在这里[更详细地描述了这个例子](/deploy-a-python-model-more-efficiently-over-spark-497fc03e0a8d))。在我目前的工作中，我们查看位置数据的一种方式是将手机定位信号与地块叠加。宗地来自市或县估价员办公室，用于划分建筑红线。它们通常看起来像这样:

![](img/76bb5f6cf7c5adc2aeffe5ec60f2148d.png)

当我们看到位置信号位于住宅区或宜家缴税的地块内时，这对我们非常有用。这有助于我们理解为什么那些信号会在它们所在的地方。并不是所有的包裹都有很好的标签，所以我们需要为没有标签的包裹推断标签。特别是，我们希望区分住宅和非住宅地块。为居住性建模的方法非常简单:我们有数千万个被标注的地块，我们有许多与这些地块相关联的元数据，我们可以根据这些元数据训练模型来识别正确的标签。

当我们想要实际生成标签时，困难就来了。我们有大约 1.4 亿个包裹需要决定。我们已经使用 Python 的 scikit-learn 库训练了这个模型。然而，通过 PySpark 用户定义函数调用 scikit-learn ' predict '方法会产生一些问题。首先，它导致了序列化模型对象以传输到执行器，然后反序列化该对象以在执行器上实际使用的开销。它必须为作业中使用的每个执行者都这样做。第二，它未能利用 scikit-learn 的优化，这主要是因为它依赖于另一个包 numpy，以便一次对整个数据数组快速执行计算，而不是遍历单个记录。

使用 spark 逐行应用模型的开销如此之大，以至于我们甚至无法完成这个过程。我们最终发现，解决方案是创造性地思考如何应用经过训练的模型:

上面的代码做了几件事:

*   将唯一的宗地 id 映射到模型进行预测所需的要素列表。
*   将这些映射的项目分组到合理大小的组中。我们发现大约 50，000 人的小组运作良好。
*   重写用户定义的函数，使其按组而不是按记录应用。所以不需要移动和应用模型 1.4 亿次，只需要发生(1.4 亿/ 5 万)= 2800 次。

正如我所说的，即使运行了 48 小时，最初的实现也没有完成。新的实施在 30 分钟内完成。教训:谈到可伸缩性，有时我们需要新奇的新技术；有时候我们只需要五分钟的散步，这样我们就能获得一个新的视角。有时候我们两者都需要。

可伸缩性是为了让您的工作面向未来。你真的无法预测有多少人最终会想要你所构建的东西，或者他们想要它的时间框架。您可以等到需求意外增长时再做决定，也可以构建自己的系统来限制需求变化对工作的瓶颈影响。