<html>
<head>
<title>Publishing Keras Model API with TensorFlow Serving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TensorFlow 服务发布 Keras 模型 API</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/publishing-keras-model-api-with-tensorflow-serving-efb5b6d96c36?source=collection_archive---------18-----------------------#2019-12-24">https://towardsdatascience.com/publishing-keras-model-api-with-tensorflow-serving-efb5b6d96c36?source=collection_archive---------18-----------------------#2019-12-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2cb2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">TensorFlow Serving 是一个面向机器学习模型的高性能服务系统。我们提供了一个端到端示例来帮助您入门。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/02fb30811c1c4337f6eec21cbad072d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DOp9iuVsEQ_RqXn0o9hIDA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: Pixabay</figcaption></figure><p id="d134" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">构建 ML 模型是一项至关重要的任务。在生产中运行 ML 模型是一项复杂而重要的任务。我以前有一个关于通过 Flask REST API 服务 ML 模型的帖子— <a class="ae lu" rel="noopener" target="_blank" href="/publishing-machine-learning-api-with-python-flask-98be46fb2440">用 Python Flask </a>发布机器学习 API。虽然这种方法可行，但它肯定缺少一些要点:</p><ul class=""><li id="154f" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">模型版本控制</li><li id="652a" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">请求批处理</li><li id="e684" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">多线程操作</li></ul><p id="7c03" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">TensorFlow 附带了一套工具来帮助您在生产中运行 ML 模型。其中一个工具——tensor flow 服务。有一个很好的教程描述了如何配置和运行它——<a class="ae lu" href="https://www.tensorflow.org/tfx/serving/docker#top_of_page" rel="noopener ugc nofollow" target="_blank">tensor flow Serving with Docker</a>。我将在我的例子中遵循相同的步骤。</p><p id="f8e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">TensorFlow 服务提供模型版本控制功能。默认情况下，客户端可以访问特定的模型版本或获取最新版本。当模型被保存时，我们可以使用当前的时间戳生成一个模型版本:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="4d5a" class="mo mp it mk b gy mq mr l ms mt">import calendar;<br/>import time;<br/>ts = calendar.timegm(time.gmtime())</span><span id="2ba7" class="mo mp it mk b gy mu mr l ms mt">tf.saved_model.save(model, "./model_report_exec_time/" + str(ts))</span></pre><p id="d07d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是生成的文件夹结构，模型的版本带有时间戳:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/77a5d24a79b999e4e3fb71bbd4ce9d66.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*XaShJ1TusWvnZUoxqdgR_w.png"/></div></figure><p id="830c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我更喜欢在自己的 Docker 容器中运行 TensorFlow 服务。根据说明，可以选择将 ML 模型复制到 TensorFlow 服务容器中。首先，从 tensorflow/serving image 创建一个基本容器:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="b84b" class="mo mp it mk b gy mq mr l ms mt">docker run -d --name serving_base tensorflow/serving</span></pre><p id="008f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将 ML 模型从本地文件夹复制到基本容器中(在我的例子中，<em class="mw"> model_folder </em>和<em class="mw"> model_name </em>都被设置为<em class="mw"> model_report_exec_time </em>):</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="cbae" class="mo mp it mk b gy mq mr l ms mt">docker cp &lt;model_folder&gt; serving_base:/models/&lt;model_name&gt;</span></pre><p id="4835" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在您可以仔细检查模型是否复制成功。进入容器:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="ebd0" class="mo mp it mk b gy mq mr l ms mt">docker exec -it serving_base bash</span></pre><p id="a476" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">导航到<em class="mw"> models </em>文件夹，您应该在那里看到您的 ML 模型:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/13e5b614998c692afa31fbe3e531b835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*65iLtu6MxuZHmeBbTOiANA.png"/></div></div></figure><p id="09e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在创建一个新的 Docker 容器，并将您的 ML 模型名称设置为环境变量，这样模型将在下一次容器启动时提供服务:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="b14c" class="mo mp it mk b gy mq mr l ms mt">docker commit --change "ENV MODEL_NAME &lt;model_name&gt;" serving_base katanaml/core-serving:v19.8</span></pre><p id="e5e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你不再需要基本容器，移除它:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="8f59" class="mo mp it mk b gy mq mr l ms mt">docker kill serving_base<br/>docker rm serving_base</span></pre><p id="3378" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从您新创建的映像启动容器(REST 端点运行在端口 8501 上):</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="576c" class="mo mp it mk b gy mq mr l ms mt">docker run -d -p 8500:8500 -p 8501:8501 --name katana-ml-serving katanaml/core-serving:v19.8</span></pre><p id="2f1d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">检查容器日志，以确保 TensorFlow 服务启动时没有错误:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="74f1" class="mo mp it mk b gy mq mr l ms mt">docker logs -f katana-ml-serving</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/ac48abb0777f71f83c6b18f204521a86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gVsYNujQ3_O-4OH3plAEgg.png"/></div></div></figure><p id="8391" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我建议通过<a class="ae lu" href="https://www.tensorflow.org/tfx/serving/api_rest#start_modelserver_with_the_rest_api_endpoint" rel="noopener ugc nofollow" target="_blank"> RESTful API 指南</a>了解 TensorFlow 服务。您应该检查模型端点是否可用，执行 GET:<em class="mw">http://localhost:8501/v1/models/&lt;model _ name&gt;</em>。这将返回模型版本状态:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/a8d34016f4a6a2e786448d8284804fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9X5KRCWo8SdLYxTCWzd47w.png"/></div></div></figure><p id="b7ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果获得响应成功，我们可以进一步执行 ML 模型预测功能。使用 TensorFlow 服务，可以通过 POST 请求直接调用 predict 函数，参数可以通过一个名为<em class="mw"> instances </em>的变量传递。ML 模型接受规范化数据，这意味着数据应该在调用预测端点之前规范化。在这个例子中，数据在 Python 中被规范化(检查本文中描述的 ML 模型— <a class="ae lu" rel="noopener" target="_blank" href="/report-time-execution-prediction-with-keras-and-tensorflow-8c9d9a889237">用 Keras 和 TensorFlow </a>报告时间执行预测):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/fc7f398262c85c805fe005d66bd2c43d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hK43IdX6UTLtlxlkJJKIZA.png"/></div></div></figure><p id="d256" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用 CURL 执行预测请求:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="d0b7" class="mo mp it mk b gy mq mr l ms mt">curl -d '{"instances": [[ 3.19179609,  2.05277296, -0.51536518, -0.4880486,  -0.50239337, -0.50629114, -0.74968743, -0.68702182,  1.45992522]]}' \<br/>     -X POST <a class="ae lu" href="http://localhost:8501/v1/models/model_report_exec_time:predict" rel="noopener ugc nofollow" target="_blank">http://localhost:8501/v1/models/model_report_exec_time:predict</a></span></pre><p id="08d6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">响应返回预测值:424.9289</p><p id="8cf5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">资源:</p><ul class=""><li id="2184" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">带有源代码的 GitHub <a class="ae lu" href="https://github.com/abaranovskis-redsamurai/automation-repo/tree/master/tf-serving" rel="noopener ugc nofollow" target="_blank">回购</a></li><li id="e27b" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/report-time-execution-prediction-with-keras-and-tensorflow-8c9d9a889237">用 Keras 和 TensorFlow 报告时间执行预测</a></li><li id="067d" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae lu" href="https://www.tensorflow.org/tfx/serving/docker" rel="noopener ugc nofollow" target="_blank"> TensorFlow 与 Docker </a>一起发球</li></ul></div></div>    
</body>
</html>