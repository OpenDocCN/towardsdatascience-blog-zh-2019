<html>
<head>
<title>Learning Reinforcement Learning: REINFORCE with PyTorch!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习强化学习:用 PyTorch 强化！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0?source=collection_archive---------5-----------------------#2019-12-05">https://towardsdatascience.com/learning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0?source=collection_archive---------5-----------------------#2019-12-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5a51" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">策略梯度入门</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8d44a9531a33686b35abcb9b595dfb9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T060nuXjl4umxc5J"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@vantorin?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nikita Vantorin</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="589a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强化算法是强化学习中最早的<em class="lv">策略梯度</em>算法之一，也是进入更高级方法的一个很好的起点。策略梯度不同于 Q 值算法，因为 PG 试图学习参数化的策略，而不是估计状态-动作对的 Q 值。因此，策略输出被表示为动作的概率分布，而不是一组 Q 值估计。如果有任何困惑或不清楚的地方，不要担心，我们会一步一步地分解它！</p><h1 id="4919" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="a31e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在这篇文章中，我们将看看加强算法，并使用 OpenAI 的 CartPole 环境和 PyTorch 对其进行测试。我们假设对强化学习有一个基本的理解，所以如果你不知道状态、动作、环境等是什么意思，在这里查看一些<a class="ae ky" href="https://www.datahubbs.com/reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">到其他文章的链接</a>或者在这里查看关于这个主题的<a class="ae ky" href="https://www.datahubbs.com/intro-to-q-learning/" rel="noopener ugc nofollow" target="_blank">简单入门</a>。</p><h1 id="8748" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">政策与行动价值观</h1><p id="802c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们可以将策略梯度算法与 Q 值方法(例如，深度 Q 网络)区分开来，因为策略梯度在不参考动作值的情况下进行动作选择。一些策略梯度学习值的估计以帮助找到更好的策略，但是这个值估计不是选择动作所必需的。DQN 的输出将是价值估计的向量，而政策梯度的输出将是行动的概率分布。</p><p id="442a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，假设我们有两个网络，一个政策网络和一个 DQN 网络，它们已经学会了用两个动作(左和右)做横翻筋斗的任务。如果我们将状态<em class="lv"> s </em>传递给每一个，我们可能会从 DQN 得到以下内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/607675979a39547a9589b0729aa223e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*o85g7XYqh254FUNO-_95EQ.png"/></div></figure><p id="da53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这来自政策梯度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/2fc3e86cee1b77ca4e8850c94e8b06e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*zvdeh60XgaboYBezVUJ8Vg.png"/></div></figure><p id="e8d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DQN 为我们提供了该州未来奖励折扣的估计值，我们基于这些值进行选择(通常根据一些ϵ-greedy 规则取最大值)。另一方面，政策梯度给了我们行动的概率。在这种情况下，我们做出选择的方式是在 28%的情况下选择行动 0，在 72%的情况下选择行动 1。随着网络获得更多的经验，这些概率将会改变。</p><h1 id="a752" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">概率值</h1><p id="360e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">为了得到这些概率，我们在输出层使用了一个简单的函数叫做<em class="lv"> softmax </em>。该函数如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a138047fd1b7912ec809aa85cdbd468d.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*shJDaBuopfJDa580d_tG0w.png"/></div></figure><p id="991f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将所有值压缩到 0 和 1 之间，并确保所有输出总和为 1(σσ(x)= 1)。因为我们使用 exp(x)函数来缩放我们的值，所以最大的值往往占主导地位，并获得更多分配给它们的概率。</p><h1 id="ac19" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">强化算法</h1><p id="8d40" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在谈谈算法本身。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/f4e5757f3d9606598de45dc2dc9661aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4RncZNj1ij5A5eMJpexhrw.png"/></div></div></figure><p id="7114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你已经关注了<a class="ae ky" href="https://www.datahubbs.com/reinforce-with-pytorch/" rel="noopener ugc nofollow" target="_blank">之前的一些帖子，</a>这看起来应该不会太令人生畏。然而，为了清楚起见，我们还是要走一遍。</p><p id="4cf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要求相当简单，我们需要一个可区分的策略，为此我们可以使用一个神经网络，以及一些超参数，如我们的步长(α)、折扣率(γ)、批量(K)和最大集数(N)。从那里，我们初始化我们的网络，并运行我们的剧集。每集之后，我们会对我们的奖励进行折扣，这是从该奖励开始的所有折扣奖励的总和。我们将在每一批(例如 K 集)完成后更新我们的政策。这有助于为训练提供稳定性。</p><p id="32d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保单损失(L(θ))起初看起来有点复杂，但如果仔细观察，不难理解。回想一下，策略网络的输出是一个概率分布。我们对π(a | s，θ)所做的，只是得到我们网络在每个状态下的概率估计。然后，我们将其乘以折扣奖励的总和(G ),得到网络的期望值。</p><p id="2f3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，假设我们处于状态<em class="lv"> s </em>网络分为两个行动，那么选择 a=0 的概率是 50%，选择 a=1 的概率也是 50%。网络随机选择 a=0，我们得到 1 的奖励，剧集结束(假设贴现因子为 1)。当我们回过头来更新我们的网络时，这个状态-动作对给我们(1)(0.5)=0.5，这转化为在该状态下采取的动作的网络期望值。从这里，我们取概率的<a class="ae ky" href="https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability" rel="noopener ugc nofollow" target="_blank">对数，并对我们这一批剧集中的所有步骤求和。最后，我们将其平均，并采用该值的梯度进行更新。</a></p><p id="a247" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快速复习一下，<a class="ae ky" href="https://gym.openai.com/envs/CartPole-v0/" rel="noopener ugc nofollow" target="_blank">推车杆</a>的目标是尽可能长时间地保持杆在空中。您的代理需要决定是向左还是向右推购物车，以保持平衡，同时不要越过左右两边的边缘。如果你还没有安装 OpenAI 的库，只要运行<code class="fe mx my mz na b">pip install gym</code>就应该设置好了。此外，如果你还没有的话，请抓住 pytorch.org 的最新消息。</p><p id="13c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">继续并导入一些包:</p><pre class="kj kk kl km gt nb na nc nd aw ne bi"><span id="ecdd" class="nf lx it na b gy ng nh l ni nj">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import gym<br/>import sys</span><span id="5e6d" class="nf lx it na b gy nk nh l ni nj">import torch<br/>from torch import nn<br/>from torch import optim</span><span id="090a" class="nf lx it na b gy nk nh l ni nj">print("PyTorch:\t{}".format(torch.__version__))</span><span id="c260" class="nf lx it na b gy nk nh l ni nj">PyTorch:	1.1.0</span></pre><h1 id="42e9" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">PyTorch 中的实现</h1><p id="efe2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">导入我们的包后，我们将建立一个名为<code class="fe mx my mz na b">policy_estimator</code>的简单类，它将包含我们的神经网络。它将有两个隐藏层，具有 ReLU 激活功能和 softmax 输出。我们还将为它提供一个名为 predict 的方法，使我们能够在网络中向前传递。</p><pre class="kj kk kl km gt nb na nc nd aw ne bi"><span id="21a9" class="nf lx it na b gy ng nh l ni nj">class policy_estimator():<br/>    def __init__(self, env):<br/>        self.n_inputs = env.observation_space.shape[0]<br/>        self.n_outputs = env.action_space.n<br/>        <br/>        # Define network<br/>        self.network = nn.Sequential(<br/>            nn.Linear(self.n_inputs, 16), <br/>            nn.ReLU(), <br/>            nn.Linear(16, self.n_outputs),<br/>            nn.Softmax(dim=-1))<br/>    <br/>    def predict(self, state):<br/>        action_probs = self.network(torch.FloatTensor(state))<br/>        return action_probs</span></pre><p id="7df0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，调用<code class="fe mx my mz na b">predict</code>方法需要我们将状态转换成一个<code class="fe mx my mz na b">FloatTensor</code>，供 PyTorch 使用。实际上，<code class="fe mx my mz na b">predict</code>方法本身在 PyTorch 中有些多余，因为张量可以直接传递给我们的<code class="fe mx my mz na b">network</code>来得到结果，但是为了清楚起见，我把它包含在这里。</p><p id="e15f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要的另一件事是我们的贴现函数，根据我们使用的贴现因子γ来贴现未来的奖励。</p><pre class="kj kk kl km gt nb na nc nd aw ne bi"><span id="b061" class="nf lx it na b gy ng nh l ni nj">def discount_rewards(rewards, gamma=0.99):<br/>    r = np.array([gamma**i * rewards[i] <br/>        for i in range(len(rewards))])<br/>    # Reverse the array direction for cumsum and then<br/>    # revert back to the original order<br/>    r = r[::-1].cumsum()[::-1]<br/>    return r — r.mean()</span></pre><p id="03c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在这里做了一件有点不标准的事情，就是减去最后奖励的平均值。这有助于稳定学习，特别是在这种情况下，所有奖励都是正的，因为与奖励没有像这样标准化相比，负奖励或低于平均水平的奖励的梯度变化更大。</p><p id="07cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在谈谈增强算法本身。</p><pre class="kj kk kl km gt nb na nc nd aw ne bi"><span id="1a6a" class="nf lx it na b gy ng nh l ni nj">def reinforce(env, policy_estimator, num_episodes=2000,<br/>              batch_size=10, gamma=0.99):</span><span id="a706" class="nf lx it na b gy nk nh l ni nj">    # Set up lists to hold results<br/>    total_rewards = []<br/>    batch_rewards = []<br/>    batch_actions = []<br/>    batch_states = []<br/>    batch_counter = 1<br/>    <br/>    # Define optimizer<br/>    optimizer = optim.Adam(policy_estimator.network.parameters(), <br/>                           lr=0.01)<br/>    <br/>    action_space = np.arange(env.action_space.n)<br/>    ep = 0<br/>    while ep &lt; num_episodes:<br/>        s_0 = env.reset()<br/>        states = []<br/>        rewards = []<br/>        actions = []<br/>        done = False<br/>        while done == False:<br/>            # Get actions and convert to numpy array<br/>            action_probs = policy_estimator.predict(<br/>                s_0).detach().numpy()<br/>            action = np.random.choice(action_space, <br/>                p=action_probs)<br/>            s_1, r, done, _ = env.step(action)<br/>            <br/>            states.append(s_0)<br/>            rewards.append(r)<br/>            actions.append(action)<br/>            s_0 = s_1<br/>            <br/>            # If done, batch data<br/>            if done:<br/>                batch_rewards.extend(discount_rewards(<br/>                    rewards, gamma))<br/>                batch_states.extend(states)<br/>                batch_actions.extend(actions)<br/>                batch_counter += 1<br/>                total_rewards.append(sum(rewards))<br/>                <br/>                # If batch is complete, update network<br/>                if batch_counter == batch_size:<br/>                    optimizer.zero_grad()<br/>                    state_tensor = torch.FloatTensor(batch_states)<br/>                    reward_tensor = torch.FloatTensor(<br/>                        batch_rewards)<br/>                    # Actions are used as indices, must be <br/>                    # LongTensor<br/>                    action_tensor = torch.LongTensor(<br/>                       batch_actions)<br/>                    <br/>                    # Calculate loss<br/>                    logprob = torch.log(<br/>                        policy_estimator.predict(state_tensor))<br/>                    selected_logprobs = reward_tensor * \  <br/>                        torch.gather(logprob, 1, <br/>                        action_tensor).squeeze()<br/>                    loss = -selected_logprobs.mean()<br/>                    <br/>                    # Calculate gradients<br/>                    loss.backward()<br/>                    # Apply gradients<br/>                    optimizer.step()<br/>                    <br/>                    batch_rewards = []<br/>                    batch_actions = []<br/>                    batch_states = []<br/>                    batch_counter = 1<br/>                    <br/>                avg_rewards = np.mean(total_rewards[-100:])<br/>                # Print running average<br/>                print("\rEp: {} Average of last 100:" +   <br/>                     "{:.2f}".format(<br/>                     ep + 1, avg_rewards), end="")<br/>                ep += 1<br/>                <br/>    return total_rewards</span></pre><p id="f160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于算法，我们传递我们的<code class="fe mx my mz na b">policy_estimator</code>和<code class="fe mx my mz na b">env</code>对象，设置一些超参数，然后我们就可以开始了。</p><p id="2803" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于实现的几点，在将值传递给<code class="fe mx my mz na b">env.step()</code>或像<code class="fe mx my mz na b">np.random.choice()</code>这样的函数之前，一定要确保将 PyTorch 的输出转换回 NumPy 数组，以避免错误。此外，我们使用<code class="fe mx my mz na b">torch.gather()</code>将实际采取的行动与行动概率分开，以确保我们如上所述正确计算损失函数。最后，你可以改变结局，让算法在环境一旦“解出”就停止运行，而不是运行预设的步数(连续 100 集平均得分 195 分以上就解出 CartPole)。</p><p id="c1b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要运行这个程序，我们只需要几行代码就可以完成。</p><pre class="kj kk kl km gt nb na nc nd aw ne bi"><span id="5efd" class="nf lx it na b gy ng nh l ni nj">env = gym.make('CartPole-v0')<br/>policy_est = policy_estimator(env)<br/>rewards = reinforce(env, policy_est)</span></pre><p id="61d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制结果，我们可以看到它工作得相当好！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/f9c82d1b8379cba84eeee96f69234403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XzS6rKI1HEQI-9krJYqSsA.png"/></div></div></figure><p id="f20c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即使是简单的策略梯度算法也可以很好地工作，并且它们比 DQN 算法的负担更少，后者通常采用像记忆回放这样的附加功能来有效地学习。</p><p id="c828" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看看您可以在更具挑战性的环境中使用该算法做些什么！</p></div></div>    
</body>
</html>